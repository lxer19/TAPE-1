URL: ftp://ftp.wins.uva.nl/pub/computer-systems/aut-sys/reports/KroDam97.ps.gz
Refering-URL: http://www.fwi.uva.nl/research/neuro/publications/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Neural vehicles  
Author: Ben Krose and Joris van Dam 
Date: June 6, 1997  
Address: Kruislaan 403, 1098 SJ Amsterdam  
Affiliation: Department of Computer Science, University of Amsterdam  
Abstract: A review is given on the use of neural networks for mobile robots and autonomous vehicles. We focus on neural methods for navigation, making a distinction between sensor based `reactive' navigation and planned navigation methods.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Arkin R.C. </author> <title> "Motor Schema Based Mobile Robot Navigation",Int. </title> <journal> Journal of Robotics Research, </journal> <year> (1989), </year> <pages> 92-112. </pages>
Reference-contexts: Navigation is seen as sensor driven "reflexes". Successful applications of (non-neural) sensor based control for vehicles have been presented for road following tasks [6] and goal directed collision-free behaviour <ref> [1, 12] </ref>. In order to obtain behaviours which are more complex than can be obtained with a single sense-act controller, a distributed/parallel control architecture consisting of a set of dedicated sense-act modules with inhibiting connections has been suggested [5].
Reference: [2] <author> Ethem Alpaydin. </author> <title> "GAL : Networks that grow when they learn and shrink when they forget", </title> <type> TR 91-032, </type> <institution> International Computer Sci. Institute, </institution> <month> (May </month> <year> 1991). </year>
Reference: [3] <author> Barto, A.G., R.S. Sutton en C.W. Anderson, </author> <title> "Neuronlike adaptive elements that can solve difficult learning control problems", </title> <journal> IEEE Trans. on Systems, Man and Cybernetics, </journal> <volume> 13 (1983), </volume> <pages> 834-846 </pages>
Reference-contexts: The question is whether a network can be trained with such poor and delayed training data. A number of algorithms have been developed that make this form of learning possible. Specifically, Barto, Sutton and Anderson <ref> [3] </ref> have formulated `reinforcement learning' as a learning strategy which does not need a set of examples provided by a `teacher.' The system described by Barto et al. explores the space of alternative input-output mappings and uses an evaluative feedback (reinforcement signal) on the consequences of the control signal (network output) <p> A constant speed is maintained, and no other tasks are imposed on the vehicle.The only feedback is given upon collision, when a negative reinforcement signal is generated.The input of the controller is the 8-dimensional range vector, and the output is a binary steering signal. Similar to <ref> [3] </ref> a discrete representation of the input space is used. However, the problem is to find a good quantization of the input space. Self-organising schemes for quantization of the input space have been presented [33] to overcome this problem.
Reference: [4] <author> Berns, K.,R. Dillmann, U. Zachmann. </author> <title> "Reinforcement learning for the control of an autonomous mobile robot", </title> <booktitle> In: Proc of IEEE International Conference on Intelligent Robots and System (1992), </booktitle> <pages> 1808-1815. </pages>
Reference-contexts: This method will result in a navigating scheme which maximizes the expected distance to the obstacles. Reinforcement learning algorithms can also be used for goal-directed navigation. Berns et al. <ref> [4] </ref> describe the use of a reinforcement learning approach to a problem where a robot vehicle has to track another vehicle. The sensor data are provided by three ultrasonic sensors at the front of the vehicle.
Reference: [5] <author> Brooks, R.A. </author> <title> "A robust layered control system for a mobile robot", </title> <journal> IEEE Journal on Robotics and Automation, </journal> <volume> 2 (1), </volume> <year> (1986), </year> <pages> 4-10. 9 </pages>
Reference-contexts: In order to obtain behaviours which are more complex than can be obtained with a single sense-act controller, a distributed/parallel control architecture consisting of a set of dedicated sense-act modules with inhibiting connections has been suggested <ref> [5] </ref>. Conventional reactive systems use an explicit mathematical model of the system to find the correct mapping between sensory input and motor output. However, such a model is often very difficult and sometimes impossible to find.
Reference: [6] <author> Dickmanns, E.D., A. Zapp, </author> <title> "A curvature-based scheme for improving road vehicle guid-ance by computer vision", </title> <booktitle> SPIE Vol 727 Mobile Robots, </booktitle> <pages> (1986) pp 161-168. </pages>
Reference-contexts: Navigation is seen as sensor driven "reflexes". Successful applications of (non-neural) sensor based control for vehicles have been presented for road following tasks <ref> [6] </ref> and goal directed collision-free behaviour [1, 12]. In order to obtain behaviours which are more complex than can be obtained with a single sense-act controller, a distributed/parallel control architecture consisting of a set of dedicated sense-act modules with inhibiting connections has been suggested [5]. <p> The neural network controls the CMU Navlab system on a road-following task. In contrast to other road following algorithms on this CMU vehicle [9] and in contrast with for example the work of Dickmanns <ref> [6] </ref>, ALVINN does not have a model of the road. It learns associations between visual patterns and steering commands.
Reference: [7] <author> A. Elfes, </author> <title> "Using Occupancy Grids for Mobile Robot Perception and Navigation", </title> <journal> IEEE Computerr, </journal> <month> June </month> <year> 1989, </year> <pages> pp. 46-57 </pages>
Reference: [8] <author> Glasius, R., Komoda, A., Gielen, S. </author> <title> Biologically Inspired Neural Networks for Trajectory Formation and Obstacle Avoidance. </title> <editor> in: S. Gielen and B. Kappen (eds) ICANN'93, </editor> <booktitle> Proceedings of theInternational Conference on Artificial Neural Networks.Springer- Verlag, </booktitle> <pages> 281-284. </pages>
Reference-contexts: Examples of how such connections are used are given in the next section. 3.3 Examples of neural network approaches In the work of <ref> [8] </ref> a neural network is used to represent the configuration space of the robot. This is the space spanned by the degrees of freedom of the robot. The configuration space is represented by a fixed lattice of neurons distributed evenly over the configuration space. <p> When the wave of activity reaches the neuron at the starting position, a path is found by following the gradient of the wave to the target neuron. It is shown in <ref> [8] </ref> that this network is guaranteed to converge to an optimal solution.
Reference: [9] <author> Kluge, K. and C. </author> <title> Thorpe "Explicit models for road following". </title> <booktitle> IEEE Conference on Robotics and Automation, </booktitle> <year> 1989 </year>
Reference-contexts: The neural network controls the CMU Navlab system on a road-following task. In contrast to other road following algorithms on this CMU vehicle <ref> [9] </ref> and in contrast with for example the work of Dickmanns [6], ALVINN does not have a model of the road. It learns associations between visual patterns and steering commands.
Reference: [10] <author> Kohonen, T. </author> <title> Self-Organization and AssociativeMemory. </title> <publisher> Springer Verlag, </publisher> <year> 1984. </year>
Reference-contexts: However, the problem is to find a good quantization of the input space. Self-organising schemes for quantization of the input space have been presented [33] to overcome this problem. A self-organising topology preserving quantization as introduced by Kohonen <ref> [10] </ref>, was used for quantizing the input space [13] and showed an improvement relative to the fixed, a-priori determined quantization of the input space.
Reference: [11] <author> Kontoravdis, D, A. Likas and A. </author> <title> Stafylopatis "Collision-free movement of an autonomous vehicle using reinforcement learning", </title> <editor> In: B. Neumann (ed), </editor> <booktitle> ECAI 92, </booktitle> <publisher> John Wiley & Sons Ltd, </publisher> <year> 1992, </year> <pages> 666-670. </pages>
Reference-contexts: A more explicit reinforcement signal was used by Kontoravdis et al. <ref> [11] </ref>, in the context of Esprit project ANNIE.
Reference: [12] <author> Koren, Y and J. Borenstein, </author> <title> "Analysis of contol methods for mobile robot obstacle avoidance", </title> <booktitle> IEEE Int. Workshop on Intelligent Motion Control, Isanbul (1990), </booktitle> <pages> pp 457-463 </pages>
Reference-contexts: Navigation is seen as sensor driven "reflexes". Successful applications of (non-neural) sensor based control for vehicles have been presented for road following tasks [6] and goal directed collision-free behaviour <ref> [1, 12] </ref>. In order to obtain behaviours which are more complex than can be obtained with a single sense-act controller, a distributed/parallel control architecture consisting of a set of dedicated sense-act modules with inhibiting connections has been suggested [5].
Reference: [13] <author> Krose, B.J.A. and J.W.M. van Dam, </author> <title> "Learning to avoid collision: a reinforcement learning paradigm for mobile robot navigation" IFAC/IFIP/IMACS International Symposium on Artificial Intelligence in Real-Time Control, </title> <address> Delft, </address> <year> 1992, </year> <note> pp 295- 300 </note> . 
Reference-contexts: However; training is slow because generalization is bad, and other approaches have been suggested [23],Schram ||||-insert figure 4 about here || Reinforcement learning is used for learning collision-free navigation as well as goal-directed navigation. Krose and Dam <ref> [13, 14] </ref> describe a controller which learns to navigate a mobile robot without collisions in a complex environment. The robot is equipped with 8 range sensors distributed on a semicircle on the front of the vehicle and a collision sensor. <p> However, the problem is to find a good quantization of the input space. Self-organising schemes for quantization of the input space have been presented [33] to overcome this problem. A self-organising topology preserving quantization as introduced by Kohonen [10], was used for quantizing the input space <ref> [13] </ref> and showed an improvement relative to the fixed, a-priori determined quantization of the input space.
Reference: [14] <author> Krose, B.J.A. and J.W.M. van Dam, </author> <title> "Adaptive state space quantization for reinforcement learning of collision-free navigation". </title> <booktitle> In: Proc of IEEE International Conference on Intelligent Robots and System , (1992) pp 1327-1332 </booktitle>
Reference-contexts: However; training is slow because generalization is bad, and other approaches have been suggested [23],Schram ||||-insert figure 4 about here || Reinforcement learning is used for learning collision-free navigation as well as goal-directed navigation. Krose and Dam <ref> [13, 14] </ref> describe a controller which learns to navigate a mobile robot without collisions in a complex environment. The robot is equipped with 8 range sensors distributed on a semicircle on the front of the vehicle and a collision sensor. <p> In a different architecture a self-building network was used, which is able to have a larger density of neurons in regions where collisions are likely <ref> [14] </ref>. A similar approach for learning to drive without collisions was used by Prescott and Mayhew [32]. They also describe a reinforcement learning technique for a mobile robot which navigates on the basis of 3 range sensors. <p> If no winner at all is found, a (non-neural) planning mechanism will induce an action, and a new category is created. In contrast to the work of for example <ref> [14] </ref>, clustering is not only done in the input space, but in a space spanned by input and output.
Reference: [15] <author> B.J.A. Krose, M.Eecen, </author> <title> "A Self-Organizing Representation of Sensor Space for Mobile Robot Navigation", </title> <booktitle> Proc. of the 1994 IEEE IEEE/RSJ Int. Conf. on intelligent Robots and Systems, </booktitle> <address> Munich, Germany, </address> <year> 1994, </year> <pages> pp. 9-14. </pages>
Reference: [16] <author> Kurtz, A. </author> <title> (1992) "Building maps for path-planning and navigation using learning classification of external sensor data", </title> <editor> I. Aleksander and J. Taylor (eds): </editor> <booktitle> Artificial Neural Networks 2, </booktitle> <publisher> North-Holland/Elsevier Science Publishers, Amsterdam, </publisher> <pages> 587-591. </pages>
Reference: [17] <editor> A. Kurz, B. Wiedemann, B. Steinmann, "Lernende Klassification von Ultraschalldis-tanzmessungen durch selbstorganisierende Merkmalskarten und Aufbau einer Umwel-reprasentation zu Navigationszwecken", </editor> <booktitle> Autonome Mobile Systeme 7 (1991). </booktitle>
Reference: [18] <author> V.J. Lumelsky, A.A. Stepanov, </author> <title> "Dynamic Path Planning for a Mobile Automaton with Limited Information on the Environment", </title> <journal> IEEE Transactions on Automatic Control, Vol.Ac-31, </journal> <volume> No.11, </volume> <month> November </month> <year> 1986, </year> <pages> pp. 1058-1063 </pages>
Reference-contexts: An example of one of the results presented by the authors is given in figure 9 ||| insert fig. 9 about here |||||- In the work of [24] a path planning algorithm is suggested which is reminiscent of the algorithm presented by Lumelsky and Stepanov in <ref> [18] </ref>. According to this algorithm, the mobile robot moves in a straight line towards its goal while it is in free space. Once it detects the presence of an obstacle on its path, it moves around this obstacle until it can resume the path towards the goal. In [18] the authors <p> Stepanov in <ref> [18] </ref>. According to this algorithm, the mobile robot moves in a straight line towards its goal while it is in free space. Once it detects the presence of an obstacle on its path, it moves around this obstacle until it can resume the path towards the goal. In [18] the authors show the robot is guaranteed to find its goal. For this planning algorithm, a representation of the robot's environment is needed which 8 can accurately discern free space from occupied space. In [24] a neural network is described which learns such a representation.
Reference: [19] <author> Marko, K.A. </author> <title> "Neural network application to diagnostics and control of vehicle control systems", </title> <booktitle> NIPS, </booktitle> <year> 1991, </year> <pages> 537-543 10 </pages>
Reference-contexts: For robot vehicles the majority of the neural applications are found in the field of path planning and navigation. Some research has been reported on related topics such as neural networks for diagnostics and control of active suspension systems <ref> [19] </ref>, but in the review we restrict ourselves to neural approaches for navigation.
Reference: [20] <author> M.J. Mataric, </author> <title> "Integration of Representation Into Goal-Driven Behavior-Based Robots", </title> <journal> IEEE Trans. on Robotics and Automation, Vol.8, </journal> <volume> No. 3, </volume> <month> June </month> <year> 1992, </year> <month> pp.304-312. </month>
Reference: [21] <author> Michie, D and R.A. Chambers. </author> <title> "Boxes: An experiment in adaptive control", </title> <editor> In: E. Dale and D. </editor> <booktitle> Michie (ed.),Machine Intelligence 2, </booktitle> <publisher> Oliver and Boyd, </publisher> <year> 1968, </year> <pages> 137-152. </pages>
Reference: [22] <author> Millan, J. </author> <title> del R.,"Reinforcement learning of goal-directed obstacle avoiding reaction strategies in an autonomous mobile robot", </title> <type> Technical note, </type> <month> March 93. </month>
Reference-contexts: One of the major drawbacks of reinforcement learning is that it is a slowprocess. In particular the stochastic nature of the output unit (needed to explore the space and generate sufficient examples) does not contribute to fast learning.A system which solves this problem is described by Millan <ref> [22] </ref>. Similar to other approaches, a given input vector is classified in a given category, resulting in a (learnable) action. However, there is not a fixed tessellation of the input space, and a given input vector may activate multiple "neurons", which may belong to different categories.
Reference: [23] <author> Moody, J and V. Tresp. </author> <title> "A trivial but fast reinforcement controller', </title> <journal> Neural Computation, </journal> <volume> 6, </volume> <year> 1984. </year>
Reference: [24] <author> Morasso, P., Vercelli, G. and Zaccaria, R. </author> <title> (1992) Hybrid systems for robot planning.in: </title> <editor> I. Aleksander and J. Taylor (eds): </editor> <booktitle> Artificial Neural Networks 2, </booktitle> <publisher> North-Holland/Elsevier Science Publishers, Amsterdam, </publisher> <pages> 691-697. </pages>
Reference-contexts: Results show, that indeed with this accurate representation of configuration space complex, collision-free paths can be found for a moving object. An example of one of the results presented by the authors is given in figure 9 ||| insert fig. 9 about here |||||- In the work of <ref> [24] </ref> a path planning algorithm is suggested which is reminiscent of the algorithm presented by Lumelsky and Stepanov in [18]. According to this algorithm, the mobile robot moves in a straight line towards its goal while it is in free space. <p> In [18] the authors show the robot is guaranteed to find its goal. For this planning algorithm, a representation of the robot's environment is needed which 8 can accurately discern free space from occupied space. In <ref> [24] </ref> a neural network is described which learns such a representation. Again, a number of neurons is evenly distributed over the robot's environment, classifying the areas as `safe' or `unsafe'. <p> The vector quantization method introduced in [?] is used to change the position of the prototype vectors and to dynamically add new prototypes where necessary. The difference between this approach and the one described above is the heuristic used in the vector quantization method. In the approach of <ref> [24] </ref>, a heuristic is used such that the neurons move towards the boundaries of the obstacles, which is more suitable to their path planning method.
Reference: [25] <author> Narendra, </author> <title> K.S. "Adaptive control using neural networks", </title> <note> in: </note> <author> W. Thomas Miller III, Richard Sutton and Paul Werbos, </author> <title> Neural Networks for Control, </title> <publisher> Bradford Book, MIT Press, </publisher> <year> 1990, </year> <pages> pp 115-143. </pages>
Reference-contexts: A problem is how to map the error of the vehicle (expressed in the sensor or world domain) to an error signal in the control domain. |||||-insert figure 3 about here|||- Narendra <ref> [25] </ref> describes an adaptive control scheme using neural networks, where a neural network is trained to identify the system which is to be controlled. <p> Only backward motion steps are allowed and the control variable is sharp left or sharp right. Similar to <ref> [25] </ref> a neural network T was trained as identification model of the system. The truck was put in an arbitrary position and the system was controlled by a neural network C until the system was stuck.
Reference: [26] <author> Neusser, S. Nijhuis, J.A.G., Spaanenburg, L., Ho*inger, B., Franke, U., and Fritz, H. </author> <title> "Neurocontrol for lateral vehicle guidance", </title> <journal> IEEE Micro 13, </journal> <note> No.1 (February 1993) pp. 57 - 66. </note>
Reference-contexts: This speed is nearly twice as fast as a non-neural algorithm running on the same vehicle. Instead of using the entire image as input to the network, image features have been used for a neural road follower in <ref> [26] </ref>. A transputer-based image processing system was developed, giving a state vector which is used to control the vehicle.
Reference: [27] <author> Nguyen, D. and B. Widrow. </author> <title> "The Truck Backer-Upper: an example of self-learning in neural networks", </title> <note> in: </note> <author> W. Thomas Miller III, Richard Sutton and Paul Werbos, </author> <title> Neural Networks for Control, </title> <publisher> Bradford Book, MIT Press, </publisher> <year> 1990, </year> <pages> pp 287-301. </pages>
Reference-contexts: If such a model is available, the error in the state domain can be back-propagated though this network in order to compute the error in the controller output, and the controller can be adjusted (Figure 2.2). This principle was also used by Nguyn and Widrow <ref> [27] </ref> in their Truck Backer-Upper.
Reference: [28] <author> M. Niizuma, M. Tomizawa, Y. Kawano, M. Sugiyama, T. Oikawa, S. Misono, S. De-gawa, </author> <title> "Action-oriented Sensor Data Integration and its Application to Control of an Autonomous Vehicle", </title> <booktitle> Proc. of the 1994 IEEE Int. Conf. on Multisensor Fusion and Integration for Intelligent Systems (MFI'94), </booktitle> <address> Las Vegas, NV, </address> <year> 1994, </year> <pages> pp. 175-182. </pages>
Reference: [29] <author> Nijhuis, J.A.G., Neusser, S., Spaanenburg, L., Heller, J., and Sponnemann, J. </author> <title> Evaluation of Fuzzy and Neural Vehicle Control, reprint from: </title> <booktitle> Proceedings CompEuro'92 (The Hague, </booktitle> <address> The Netherlands, </address> <note> May 1992) pp. 447 - 452, in: </note> <editor> R.J.Marks (Ed.), </editor> <booktitle> Fuzzy Logic Technology and Applications, </booktitle> <publisher> IEEE Technology Update Series (IEEE Press, </publisher> <address> Piscataway), </address> <year> 1994 </year>
Reference: [30] <author> Opitz, R. Das Lernfahrzeug, </author> <title> Neural network application for autonomous mobile robots. </title> <editor> in: R.Eckmiller (ed): </editor> <booktitle> Advanced neural Computers, </booktitle> <publisher> Elseviers (North Holland), </publisher> <year> 1990, </year> <pages> 373-379. </pages>
Reference-contexts: Collision-free navigation Instead of using vision for road following, similar approaches have been presented which use ultrasonic range measurements in order to navigate without collisions. An example is the work carried out within the Esprit project ANNIE <ref> [30] </ref>. In this paper a simulated vehicle is equipped with sensors which measure the range to objects which are nearest within their field of view. The corresponding range values are assigned logarithmically in correspondence with the sensitivity to near obstacles and the 16 range values are coded in 4 bits.
Reference: [31] <author> Pomerleau, D.A. </author> <title> "Efficient Training of Artificial neural networks for autonomous navigation". </title> <booktitle> Neural Computation 3, (1991) pp. </booktitle> <pages> 88-97. </pages>
Reference-contexts: A large number of such applications has been described. |||||-insert figure 1 about here|||- Road-following One of the most appealing examples of such a sensor-motor mapping is ALVINN (Autonomous Land Vehicle in a Neural Net), described by Pomerleau <ref> [35, 31] </ref>. The neural network controls the CMU Navlab system on a road-following task. In contrast to other road following algorithms on this CMU vehicle [9] and in contrast with for example the work of Dickmanns [6], ALVINN does not have a model of the road.
Reference: [32] <author> T. Prescott and J. </author> <title> Mayhew. Adaptive local navigation. in: Active Vision, </title> <editor> ed. A. Blake and A. Yuille, </editor> <publisher> MIT Press 1992 </publisher>
Reference-contexts: In a different architecture a self-building network was used, which is able to have a larger density of neurons in regions where collisions are likely [14]. A similar approach for learning to drive without collisions was used by Prescott and Mayhew <ref> [32] </ref>. They also describe a reinforcement learning technique for a mobile robot which navigates on the basis of 3 range sensors.
Reference: [33] <author> Rosen, </author> <title> B.E., </title> <editor> J.M.Goodwin and J.J.Vidal, "Adaptiverange coding", In: D. Touretzky (Ed.) </editor> <booktitle> Neural Information Processing Systems 3. </booktitle> <publisher> Morgan Kauffman (1990). </publisher> <pages> 11 </pages>
Reference-contexts: Similar to [3] a discrete representation of the input space is used. However, the problem is to find a good quantization of the input space. Self-organising schemes for quantization of the input space have been presented <ref> [33] </ref> to overcome this problem. A self-organising topology preserving quantization as introduced by Kohonen [10], was used for quantizing the input space [13] and showed an improvement relative to the fixed, a-priori determined quantization of the input space.
Reference: [34] <author> J. Tani, N. Fukumura, </author> <title> "Embedding task-based behavior into internal sensory-based at-tractor dynamics in navigation of a mobile robot", </title> <booktitle> Proc. of the 1994 IEEE IEEE/RSJ Int. Conf. on intelligent Robots and Systems, </booktitle> <address> Munich, Germany, </address> <year> 1994, </year> <pages> pp. 9-14. </pages>
Reference: [35] <author> Touretzky, D.S. and D.A. Pomerleau, </author> <title> "What's hidden in the hidden layers?",Byte, </title> <month> August </month> <year> 1989, </year> <pages> pp 227-233. </pages>
Reference-contexts: A large number of such applications has been described. |||||-insert figure 1 about here|||- Road-following One of the most appealing examples of such a sensor-motor mapping is ALVINN (Autonomous Land Vehicle in a Neural Net), described by Pomerleau <ref> [35, 31] </ref>. The neural network controls the CMU Navlab system on a road-following task. In contrast to other road following algorithms on this CMU vehicle [9] and in contrast with for example the work of Dickmanns [6], ALVINN does not have a model of the road.
Reference: [36] <author> P. Tournassoud, </author> <title> "Motion Planning for a Mobile Robot with a Kinematic Constraint", </title> <editor> Journee Geometrie et Robotique, LNRS LAAS, </editor> <year> 1988, </year> <pages> pp. 1-24. </pages>
Reference-contexts: It is decomposed into a number of areas within which the robot can move freely without danger of collision and which are connected if the robot can execute a move between these areas without collision. Such an approach is described in <ref> [36] </ref>. The free space is represented as a graph of connected cones. The path planning algorithm can move freely within these cones and the connections between the cones specify how to plan a path from one cone to the other.
Reference: [37] <author> J. Vandorpe, H. van Brussel, </author> <title> "A Reflexive Navigation Algorithm for an Autonomous Mobile Robot", </title> <booktitle> Proc. of the 1994 IEEE Int. Conf. on Multisensor Fusion and Integration for Intelligent Systems (MFI'94), </booktitle> <address> Las Vegas, NV, </address> <year> 1994, </year> <month> pp.251-258. </month>

References-found: 37

