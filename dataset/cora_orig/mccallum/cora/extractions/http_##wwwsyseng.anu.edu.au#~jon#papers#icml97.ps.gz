URL: http://wwwsyseng.anu.edu.au/~jon/papers/icml97.ps.gz
Refering-URL: 
Root-URL: 
Title: The Canonical Distortion Measure for Vector Quantization and Function Approximation  
Author: Jonathan Baxter 
Address: Canberra 0200, Australia  
Affiliation: Department of systems Engineering Australian National University  
Abstract: To measure the quality of a set of vector quantization points a means of measuring the distance between a random point and its quantization is required. Common metrics such as the Hamming and Euclidean metrics, while mathematically simple, are inappropriate for comparing natural signals such as speech or images. In this paper it is shown how an environment of functions on an input space X induces a canonical distortion measure (CDM) on X. The depiction canonical is justified because it is shown that optimizing the reconstruction error of X with respect to the CDM gives rise to optimal piecewise constant approximations of the functions in the environment. The CDM is calculated in closed form for several different function classes. An algorithm for training neural networks to implement the CDM is presented along with some en couraging experimental results. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Jonathan Baxter. </author> <title> Learning Internal Representations. </title> <booktitle> In Proceedings of the Eighth International Conference on Computational Learning Theory, </booktitle> <address> Santa Cruz, California, 1995. </address> <publisher> ACM Press. </publisher>
Reference-contexts: From now on will be referred to as the Canonical Distortion Measure or CDM for the environment (F ; Q). In relation to the character transmission problem, F would consist of all character-like classifiers, Y would be the set <ref> [0; 1] </ref> and we could take (y; y 0 ) = (y y 0 ) 2 . <p> linear functions to images, or speech signals (face classifiers are not linear maps on image space, nor are word classifiers linear maps on speech signals), the use of squared Euclidean distortion in these environments is not the best thing to do. 3.2 A QUADRATIC ENVIRONMENT Let X = Y = <ref> [1; 1] </ref>, (y; y 0 ) = jy y 0 j for all y; y 0 2 Y and let F = ff : x 7! ax 2 g with a uniformly distributed in the range [1; 1]. <p> the best thing to do. 3.2 A QUADRATIC ENVIRONMENT Let X = Y = <ref> [1; 1] </ref>, (y; y 0 ) = jy y 0 j for all y; y 0 2 Y and let F = ff : x 7! ax 2 g with a uniformly distributed in the range [1; 1]. With this environment, (x; y) = 1 = jx yjjx + yj: Note that (x; y) = 0 if x = y and if x = y, so that x and x are zero distance apart under . <p> The idea is to train a set of real-valued classifiers, f 1 ; : : : ; f N , for a domain of prototype objects (so that f i (x) 2 <ref> [0; 1] </ref> is interpreted as the probability that x is an example of object i). All objects (not just the prototypes) are then represented by the vector of activations they induce at the output of the prototype classifiers. <p> Suppose that k is minimal. Note that the f take values outside the range <ref> [0; 1] </ref> and so cannot be inter preted as probabilities. However we can still interpret the output of f as a degree of classificationlarge positive values being high confidence and large negative values low confidence. In this case the environmental distribution Q is a distribution over the weights w. <p> Thus, f r 1 ;r 2 ( 1 ; 2 ) = r 2 2 + 2r 1 r 2 cos ( 1 2 ). The link lengths r 1 and r 2 were chosen uniformly in the range <ref> [0; 1] </ref>, so that F = ff r 1 ;r 2 : (r 1 ; r 2 ) 2 [0; 1] 2 g. The goal was to train a neural network to correctly implement the CDM (( 1 ; 2 ); ( 0 2 )). <p> The link lengths r 1 and r 2 were chosen uniformly in the range <ref> [0; 1] </ref>, so that F = ff r 1 ;r 2 : (r 1 ; r 2 ) 2 [0; 1] 2 g. The goal was to train a neural network to correctly implement the CDM (( 1 ; 2 ); ( 0 2 )). <p> Training sets were generated by first sampling M times from the environment to generate f 1 ; : : : ; f M , which in this case meant generating M pairs (r 1 ; r 2 ) uniformly at random in the square <ref> [0; 1] </ref> 2 .
Reference: [2] <author> Richard Caruana. </author> <title> Learning Many Related Tasks at the Same Time with Backpropagation. </title> <booktitle> In Advances in Neural Information Processing 5, </booktitle> <year> 1993. </year>
Reference: [3] <author> T M Cover and J A Thomas. </author> <title> Elements of Information Theory. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: distortion between x and its quantization q d (x) is minimal, i.e ~x are chosen to minimize the reconstruction error E d (~x) = X A common approach to minimizing the reconstruction error is Lloyd's algorithm which iteratively improves a set of quantization points based on a centroidal update (see <ref> [6, 3, 5] </ref>).
Reference: [4] <author> Shimon Edelman. </author> <title> Representation, similarity, and the chorus of prototypes. </title> <journal> Minds and Machines, </journal> <volume> 5:, </volume> <year> 1995. </year>
Reference-contexts: They achieved a notable improvement in performance using this measure in a nearest neighbour classifier. The concept of similarity to a chorus of prototypes, introduced by Edelman (see e.g. <ref> [4] </ref>) is closely related to the canonical distortion measure introduced here. In fact, under certain assumptions about the environment, one can show that the chorus of prototypes similarity measure is identical to the CDM. <p> x 0 ) + (y; y 0 ) (x 0 ; y 0 ): Running the same argument with x and x 0 and y and y 0 interchanged shows that (x; y) = (x 0 ; y 0 ) always. 5.2 THE CDM AND EDELMAN'S CHORUS OF PROTOTYPES In <ref> [4] </ref>, Edelman introduced a concept of representation he called the Chorus of Prototypes. <p> The CDM was shown to be a unifying concept for many seemingly disparate threads of research <ref> [10, 4, 7, 9] </ref>. We proved that generating an optimal quantization set for the input space using as the distortion measure automatically produces Voronoi regions that are optimal for forming piecewise-constant approximations to (1-NN estimates of) the func tions in the environment.
Reference: [5] <author> A Gersho and R M Gray. </author> <title> Vector Quantization and Signal Processing. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1992. </year>
Reference-contexts: 1 INTRODUCTION Consider the problems What are appropriate distortion measures for images of handwritten characters, or images of faces, or representations of speech signals? Simple measures such as squared Euclidean distance, while widely used in vector quantization applications <ref> [5] </ref>, do not correlate well with our own subjective notion of distance in these problems. <p> distortion between x and its quantization q d (x) is minimal, i.e ~x are chosen to minimize the reconstruction error E d (~x) = X A common approach to minimizing the reconstruction error is Lloyd's algorithm which iteratively improves a set of quantization points based on a centroidal update (see <ref> [6, 3, 5] </ref>).
Reference: [6] <author> S P Lloyd. </author> <title> Least squares quantization in pcm. </title> <type> Technical report, </type> <institution> Bell Laboratories, </institution> <year> 1975. </year>
Reference-contexts: distortion between x and its quantization q d (x) is minimal, i.e ~x are chosen to minimize the reconstruction error E d (~x) = X A common approach to minimizing the reconstruction error is Lloyd's algorithm which iteratively improves a set of quantization points based on a centroidal update (see <ref> [6, 3, 5] </ref>). <p> After the network had been trained, an initial quantization set q 1 ; : : : ; q m of size m t N was chosen uniformly at random from fx 1 ; : : : ; x N g and then the empirical Lloyd algorithm <ref> [6] </ref> was used to optimize the positions of the quantization points. The trained neural networksuitably sym-metrised via (13)was used as the distortion measure.
Reference: [7] <author> Karen L Oehler and Robert M Gray. </author> <title> Combining image compression and classification using vector quan-tozation. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 17(5):461473, </volume> <month> May </month> <year> 1995. </year>
Reference-contexts: The authors of [10] also proposed a similarity measure on images that has a close relationship to the CDM defined here, again under certain restrictions on the functions in the environment. In work that is also close to the spirit of the present paper, the authors of <ref> [7] </ref> considered vector quantization in a Bayes classifier environment. It can be show that the optimal Voronoi regions generated by their Bayes risk reconstruction error are the same as the optimal Voronoi regions generated by the CDM for the same classifier environment. <p> The CDM was shown to be a unifying concept for many seemingly disparate threads of research <ref> [10, 4, 7, 9] </ref>. We proved that generating an optimal quantization set for the input space using as the distortion measure automatically produces Voronoi regions that are optimal for forming piecewise-constant approximations to (1-NN estimates of) the func tions in the environment.
Reference: [8] <author> Lori Y Pratt. </author> <title> Discriminability-based transfer between neural networks. </title> <editor> In Stephen J Hanson, Jack D Cowan, and C Lee Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, pages 204211, </booktitle> <address> San Mateo, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [9] <author> Patrice Simard, Yann LeCun, and John Denker. </author> <title> Efficient pattern recognition using a new transformation distance. </title> <editor> In S J Hanson, J D Cowan, and C Lee Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 5158, </pages> <address> San Mateo, 1993. </address> <publisher> Morgan Kauffman. </publisher>
Reference-contexts: Learning with the CDM gives far better generalisation from small training sets than learning without the CDM. 1.1 RELATED WORK Other authors have investigated the possibility for using specially tailored distance functions in both machine learning and vector quantization contexts. The authors of <ref> [9] </ref> used a measure of distance that takes into account invari-ance with respect to affine transformations and thickness transformations of handwritten characters. They achieved a notable improvement in performance using this measure in a nearest neighbour classifier. <p> far as generating piecewise constant approximations to the functions in the environment is concerned, there is no better partition of the input space than that induced by the CDM and its optimal quantization set. 5 RELATIONSHIP BETWEEN THE CDM AND OTHER DISTANCE MEASURES 5.1 TRANSFORMATION INVARIANT DISTANCE The authors of <ref> [9] </ref> introduced a technique for comparing handwritten characters called Transformation Distance. They observed that images of characters are invariant under transformations such as rotation, dilation, shift, line thickening and so on. <p> In order to simplify the computation, in <ref> [9] </ref> D (x; x 0 ) was approximated by a linearised version. However we will concentrate on the exact expression (7). <p> The CDM was shown to be a unifying concept for many seemingly disparate threads of research <ref> [10, 4, 7, 9] </ref>. We proved that generating an optimal quantization set for the input space using as the distortion measure automatically produces Voronoi regions that are optimal for forming piecewise-constant approximations to (1-NN estimates of) the func tions in the environment.
Reference: [10] <author> Sebastian Thrun and Tom M Mitchell. </author> <title> Learning One More Thing. </title> <type> Technical Report CMU-CS-94-184, CMU, </type> <year> 1994. </year>
Reference-contexts: In fact, under certain assumptions about the environment, one can show that the chorus of prototypes similarity measure is identical to the CDM. The authors of <ref> [10] </ref> also proposed a similarity measure on images that has a close relationship to the CDM defined here, again under certain restrictions on the functions in the environment. <p> Then f = w 0 W h = P i f i (x). Thus any classifier in the environment is representable as a linear combination of prototype classifiers, as required for the chorus of prototypes idea. 5.3 SIMILARITY MEASURE OF THRUN AND MITCHELL The authors of <ref> [10] </ref> defined an invariance function : X fi X ! f0; 1g for a finite environment F with the property that if there exists f 2 F such that f (x) = 1 then f 0 (x) = 0 for all other f 0 2 F : (x; x 0 ) <p> The CDM was shown to be a unifying concept for many seemingly disparate threads of research <ref> [10, 4, 7, 9] </ref>. We proved that generating an optimal quantization set for the input space using as the distortion measure automatically produces Voronoi regions that are optimal for forming piecewise-constant approximations to (1-NN estimates of) the func tions in the environment.
References-found: 10

