URL: http://charm.cs.uiuc.edu/version2/papers/CharmSys1TPDS94.ps
Refering-URL: http://charm.cs.uiuc.edu/version2/papers/CharmSys1TPDS94.html
Root-URL: http://www.cs.uiuc.edu
Email: E-mail: kale,sinha,gursoy@cs.uiuc.edu E-mail: ramkumar@eng.uiowa.edu  
Title: The Charm Parallel Programming Language and System: Part I Description of Language Features  
Author: L. V. Kale B. Ramkumar flfl A. B. Sinha A. Gursoy 
Note: This research was supported in part by the National Science Foundation grants CCR-90-07195 and CCR-91-06608. Dr. Ramkumar's work is supported in part by the National Science Foundation grant NSF-CCR-9308108.  
Address: Urbana, IL 61801 Iowa City, IA 52242  
Affiliation: Dept. of Computer Science flfl Dept. of Electrical and Computer Engineering University of Illinois University of Iowa  
Abstract: We describe a parallel programming system for developing machine independent programs for all MIMD machines. Many useful approaches to this problem are seen to require a common base of support, which can be encapsulated in a language that abstracts over resource management decisions and machine specific details. This language can be used for implementing other high level approaches as well as for efficient application programming. The requirements for such a language are defined, and the language supported by the Charm system is described, and illustrated with examples. Charm is one of the first languages to support message driven execution, and embodies unique abstractions such as branch office chares and specifically shared variables. In Part II of this paper, we talk about the runtime support system for Charm. The system thus provides ease of programming on MIMD platforms without sacrificing performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> High performance FORTRAN forum. </author> <title> High Performance FORTRAN specification. </title> <type> Technical report, </type> <institution> Rice University, Houston, TX. </institution>
Reference-contexts: Since this difference is irreconcilable, except for data parallel languages, we restrict our attention to MIMD parallel machines. We believe that portability across SIMD and MIMD machines can be accomplished by implementing a common data parallel language such as High Performance Fortran <ref> [1, 2, 3] </ref> on SIMD machines using native machine primitives and on MIMD machines using a system such as Charm. 1 1.1 Requirements of a parallel programming system A general technique for dealing with complexity involves employing a hierarchical structure. <p> If the mechanisms are well-matched for the application at hand, and if the programmer is experienced in using them, they work very well. 3. In both of the above approaches, the programmer specifies the decomposition, the mapping, and scheduling. The High Performance Fortran <ref> [1, 2] </ref> standard is an example of approaches which free the programmer from the task of scheduling. <p> As experimental evidence, conditional packing led to three-fold improvements in speed on iPSC/2 for a parallel prolog interpreter [36] implemented using Charm. 18 message f int a; varSize float b []; g MSG; ... MSG *msg; int i, n, sizes <ref> [1] </ref>; CkScanf ("%d", &n); sizes [0] = n; msg = (MSG *) CkAllocMsg (MSG, sizes); for (i=0; i&lt;n; i++) CkScanf ("%f", &(msg!b [i])); g The special case of variable size arrays occurs very frequently in many applications. Charm provides a varSize array field in message definitions for this special case.
Reference: [2] <author> H. Zima, P. Brezany, B. Chapman, P. Mehrotra, and A. Schwald. </author> <title> Vienna fortran | a language specification. </title> <type> Technical report, ACPC technical report series, </type> <institution> University of Vienna, Vienna, Austria, </institution> <year> 1992. </year>
Reference-contexts: Since this difference is irreconcilable, except for data parallel languages, we restrict our attention to MIMD parallel machines. We believe that portability across SIMD and MIMD machines can be accomplished by implementing a common data parallel language such as High Performance Fortran <ref> [1, 2, 3] </ref> on SIMD machines using native machine primitives and on MIMD machines using a system such as Charm. 1 1.1 Requirements of a parallel programming system A general technique for dealing with complexity involves employing a hierarchical structure. <p> If the mechanisms are well-matched for the application at hand, and if the programmer is experienced in using them, they work very well. 3. In both of the above approaches, the programmer specifies the decomposition, the mapping, and scheduling. The High Performance Fortran <ref> [1, 2] </ref> standard is an example of approaches which free the programmer from the task of scheduling.
Reference: [3] <author> Edward Kornkven and Laxmikant Kale. </author> <title> Dynamic adaptive scheduling in an implementation of a data parallel language. </title> <type> Technical Report 92-10, </type> <institution> Parallel Programming Laboratory, Department of Computer Science, University of Illinois, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: Since this difference is irreconcilable, except for data parallel languages, we restrict our attention to MIMD parallel machines. We believe that portability across SIMD and MIMD machines can be accomplished by implementing a common data parallel language such as High Performance Fortran <ref> [1, 2, 3] </ref> on SIMD machines using native machine primitives and on MIMD machines using a system such as Charm. 1 1.1 Requirements of a parallel programming system A general technique for dealing with complexity involves employing a hierarchical structure.
Reference: [4] <author> J. Flower, A. Kolawa, and S. Bharadwaj. </author> <title> The Express way to distributed processing. </title> <booktitle> In Supercomputing Review, </booktitle> <pages> pages 54-55, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: The approaches in this class are characterized by their generality. Any computation that can be expressed on a MIMD machine can be expressed directly, with similar primitives. In Express <ref> [4] </ref> and in PVM [5, 6, 7], they are the primitives of a non-shared memory computer. <p> Static Load Balancing: Even without its public functions, the fact that the BOCs are replicated processes allows them to be used for simple statically mapped SPMD style programming. In comparison with the traditional mechanisms for supporting SPMD style programming, such as Express <ref> [4] </ref> or the send/receive primitives of the native operating systems on the distributed memory machines, BOCs provide the advantages of message-driven execution. Also, one may have as many BOC instances in a single program as needed. This separates and simplifies the flow of control in many programs. 2.
Reference: [5] <author> V. S. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency: Practice & Experience, </journal> <volume> 2, 4 </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: The approaches in this class are characterized by their generality. Any computation that can be expressed on a MIMD machine can be expressed directly, with similar primitives. In Express [4] and in PVM <ref> [5, 6, 7] </ref>, they are the primitives of a non-shared memory computer. These approaches are very useful toward achieving portability, and their utility is demonstrated by the popularity of systems such as PVM, and by the emergence of a evolving message passing interface standard (MPI) [8, 9, 10].
Reference: [6] <author> G. A. Geist and V. S. Sunderam. </author> <title> The PVM system: Supercomputing level concurrent computations on a heterogeneous network of workstations. </title> <booktitle> Sixth Distributed Memory Computing Conference Proceedings, </booktitle> <pages> pages 258-261, </pages> <year> 1991. </year>
Reference-contexts: The approaches in this class are characterized by their generality. Any computation that can be expressed on a MIMD machine can be expressed directly, with similar primitives. In Express [4] and in PVM <ref> [5, 6, 7] </ref>, they are the primitives of a non-shared memory computer. These approaches are very useful toward achieving portability, and their utility is demonstrated by the popularity of systems such as PVM, and by the emergence of a evolving message passing interface standard (MPI) [8, 9, 10].
Reference: [7] <author> J. Dongarra et al. </author> <title> Integrated PVM framework supports heterogeneous network computing. </title> <journal> Computers in Physics, </journal> <volume> 7, No. 2 </volume> <pages> 166-175, </pages> <year> 1993. </year>
Reference-contexts: The approaches in this class are characterized by their generality. Any computation that can be expressed on a MIMD machine can be expressed directly, with similar primitives. In Express [4] and in PVM <ref> [5, 6, 7] </ref>, they are the primitives of a non-shared memory computer. These approaches are very useful toward achieving portability, and their utility is demonstrated by the popularity of systems such as PVM, and by the emergence of a evolving message passing interface standard (MPI) [8, 9, 10].
Reference: [8] <author> M. Snir, W. Gropp, and E. Lusk. </author> <title> Document for a standard message-passing interface: point to point communication. </title> <type> draft, </type> <year> 1993. </year>
Reference-contexts: These approaches are very useful toward achieving portability, and their utility is demonstrated by the popularity of systems such as PVM, and by the emergence of a evolving message passing interface standard (MPI) <ref> [8, 9, 10] </ref>. However, beyond portability, such approaches do little to control the complexity of parallel programming. 2. The next class of approaches define their own communication and synchronization mechanisms, which are independent of the machine primitives.
Reference: [9] <author> A. Geist and M. Snir. </author> <title> Document for a standard message-passing interface: collective communication. </title> <type> draft, </type> <year> 1993. </year>
Reference-contexts: These approaches are very useful toward achieving portability, and their utility is demonstrated by the popularity of systems such as PVM, and by the emergence of a evolving message passing interface standard (MPI) <ref> [8, 9, 10] </ref>. However, beyond portability, such approaches do little to control the complexity of parallel programming. 2. The next class of approaches define their own communication and synchronization mechanisms, which are independent of the machine primitives.
Reference: [10] <author> L. Clarke et al. </author> <title> Document for a standard message-passing interface: groups, contexts, </title> <journal> communications. </journal> <volume> draft, </volume> <year> 1993. </year>
Reference-contexts: These approaches are very useful toward achieving portability, and their utility is demonstrated by the popularity of systems such as PVM, and by the emergence of a evolving message passing interface standard (MPI) <ref> [8, 9, 10] </ref>. However, beyond portability, such approaches do little to control the complexity of parallel programming. 2. The next class of approaches define their own communication and synchronization mechanisms, which are independent of the machine primitives.
Reference: [11] <author> D. H. </author> <title> Gelernter . Generative Communication in Linda. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(1) </volume> <pages> 80-112, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: However, beyond portability, such approaches do little to control the complexity of parallel programming. 2. The next class of approaches define their own communication and synchronization mechanisms, which are independent of the machine primitives. Linda <ref> [11, 12] </ref> is an example of such a system | here processes communicate by depositing, fetching, or copying tuples from a common tuple space.
Reference: [12] <author> N. Carriero and D. </author> <title> Gelernter . How to Write Parallel Programs: A Guide to the Perplexed. </title> <journal> ACM Computing Surveys, </journal> <pages> pages 323-357, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: However, beyond portability, such approaches do little to control the complexity of parallel programming. 2. The next class of approaches define their own communication and synchronization mechanisms, which are independent of the machine primitives. Linda <ref> [11, 12] </ref> is an example of such a system | here processes communicate by depositing, fetching, or copying tuples from a common tuple space.
Reference: [13] <author> I. Foster and S. Taylor. Strand: </author> <title> New Concepts in Parallel Programming. </title> <publisher> Prentice Hall, </publisher> <year> 1990. </year>
Reference-contexts: The next class of approaches define their own communication and synchronization mechanisms, which are independent of the machine primitives. Linda [11, 12] is an example of such a system | here processes communicate by depositing, fetching, or copying tuples from a common tuple space. In Strand <ref> [13] </ref> and Concurrent Prolog [14], processes, expressed as recursive clauses of a logic program, communicate by setting values to, and blocking for values of shared variables in a stream (represented as a list) of logic variables.
Reference: [14] <author> Taylor S., Safra S. and Shapiro E. </author> <title> A Parallel Implementation of Flat Concurrent Prolog. </title> <type> Technical Report CS87-04, </type> <institution> Weizmann Institute of Science, </institution> <month> October </month> <year> 1986. </year>
Reference-contexts: Linda [11, 12] is an example of such a system | here processes communicate by depositing, fetching, or copying tuples from a common tuple space. In Strand [13] and Concurrent Prolog <ref> [14] </ref>, processes, expressed as recursive clauses of a logic program, communicate by setting values to, and blocking for values of shared variables in a stream (represented as a list) of logic variables.
Reference: [15] <author> P. Hudak. </author> <title> Para-functional programming: a paradigm for programming multi-processor systems. </title> <booktitle> In 12th ACM Symposium on Principles of programming languages, </booktitle> <pages> pages 243-254, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: Because it applies only to data parallel computations, this approach is not as "general-purpose" as the ones above. Another example of this class is Hudak's explicitly parallel functional programming <ref> [15, 16] </ref>, where each function call can be mapped to specific processor under the programmer's control. 2 4. Systems such as Multilisp [17] and QLISP [18, 19, 20] allow programmers to explicitly specify the decompositions, while taking over both the tasks of mapping and scheduling.
Reference: [16] <author> P. Hudak. </author> <title> Para-functional programming. </title> <booktitle> In Computer, </booktitle> <volume> volume 19, Number 8, </volume> <month> August </month> <year> 1986. </year>
Reference-contexts: Because it applies only to data parallel computations, this approach is not as "general-purpose" as the ones above. Another example of this class is Hudak's explicitly parallel functional programming <ref> [15, 16] </ref>, where each function call can be mapped to specific processor under the programmer's control. 2 4. Systems such as Multilisp [17] and QLISP [18, 19, 20] allow programmers to explicitly specify the decompositions, while taking over both the tasks of mapping and scheduling.
Reference: [17] <author> R. Halstead. </author> <title> Multilisp: a language for concurrent symbolic computation. </title> <journal> In ACM Transactions on Programming Languages and Systems, </journal> <volume> volume Volume 4, No. 4, </volume> <month> October </month> <year> 1985. </year>
Reference-contexts: Another example of this class is Hudak's explicitly parallel functional programming [15, 16], where each function call can be mapped to specific processor under the programmer's control. 2 4. Systems such as Multilisp <ref> [17] </ref> and QLISP [18, 19, 20] allow programmers to explicitly specify the decompositions, while taking over both the tasks of mapping and scheduling. In Multilisp, for example, a computation may fork off a subcomputation to compute a value to be stored in a structure called a future.
Reference: [18] <author> R. P. Gabriel and J. McCarthy. </author> <booktitle> Qlisp, </booktitle> <pages> pages 63-90. </pages> <publisher> Kulwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1988. </year>
Reference-contexts: Another example of this class is Hudak's explicitly parallel functional programming [15, 16], where each function call can be mapped to specific processor under the programmer's control. 2 4. Systems such as Multilisp [17] and QLISP <ref> [18, 19, 20] </ref> allow programmers to explicitly specify the decompositions, while taking over both the tasks of mapping and scheduling. In Multilisp, for example, a computation may fork off a subcomputation to compute a value to be stored in a structure called a future.
Reference: [19] <author> Qlisp: </author> <title> Experience and New Directions, </title> <journal> volume SIGPLAN Notices, V. </journal> <volume> 2, No. 9, </volume> <month> September </month> <year> 1988. </year>
Reference-contexts: Another example of this class is Hudak's explicitly parallel functional programming [15, 16], where each function call can be mapped to specific processor under the programmer's control. 2 4. Systems such as Multilisp [17] and QLISP <ref> [18, 19, 20] </ref> allow programmers to explicitly specify the decompositions, while taking over both the tasks of mapping and scheduling. In Multilisp, for example, a computation may fork off a subcomputation to compute a value to be stored in a structure called a future.
Reference: [20] <author> I. Mason, J. Pehoushek, C. Talcott, and J. Weening. </author> <title> Programming in qlisp. </title> <type> Technical Report stan-cs-90-1340, </type> <institution> Stanford University, </institution> <year> 1990. </year>
Reference-contexts: Another example of this class is Hudak's explicitly parallel functional programming [15, 16], where each function call can be mapped to specific processor under the programmer's control. 2 4. Systems such as Multilisp [17] and QLISP <ref> [18, 19, 20] </ref> allow programmers to explicitly specify the decompositions, while taking over both the tasks of mapping and scheduling. In Multilisp, for example, a computation may fork off a subcomputation to compute a value to be stored in a structure called a future.
Reference: [21] <author> D. J. Kuck et al. </author> <title> The effects of program restructuring, algorithm change, and archictecture choice on program performance. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 129-138, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: Parallelizing compilers, such as those for Fortran <ref> [21, 22, 23] </ref> aim at a very high level in the hierarchy: they attempt to take over the task of decomposition, mapping, scheduling, as well as machine dependent expression. They let the programmers write programs in their usual sequential languages (such as Fortran). <p> A parallelizing compiler is used to detect the parallelism, and transform the program to a parallel program, which is then translated further for specific target machines. This approach has its obvious attractions: there is almost no additional parallel programming complexity. The pioneering work done at the University of Illinois <ref> [21, 22] </ref> and Rice [23] has demonstrated the viability of this approach for specific machines.
Reference: [22] <author> D. A. Padua and M. J. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> In Communications of the ACM, </journal> <volume> volume Volume 29, No. 12C, </volume> <month> December </month> <year> 1986. </year>
Reference-contexts: Parallelizing compilers, such as those for Fortran <ref> [21, 22, 23] </ref> aim at a very high level in the hierarchy: they attempt to take over the task of decomposition, mapping, scheduling, as well as machine dependent expression. They let the programmers write programs in their usual sequential languages (such as Fortran). <p> A parallelizing compiler is used to detect the parallelism, and transform the program to a parallel program, which is then translated further for specific target machines. This approach has its obvious attractions: there is almost no additional parallel programming complexity. The pioneering work done at the University of Illinois <ref> [21, 22] </ref> and Rice [23] has demonstrated the viability of this approach for specific machines.
Reference: [23] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic transformation of fortran programs to vector form. </title> <journal> In ACM Transactions on Programming Languages and Systems, </journal> <volume> volume Volume 9, No. 4, </volume> <pages> pages 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: Parallelizing compilers, such as those for Fortran <ref> [21, 22, 23] </ref> aim at a very high level in the hierarchy: they attempt to take over the task of decomposition, mapping, scheduling, as well as machine dependent expression. They let the programmers write programs in their usual sequential languages (such as Fortran). <p> This approach has its obvious attractions: there is almost no additional parallel programming complexity. The pioneering work done at the University of Illinois [21, 22] and Rice <ref> [23] </ref> has demonstrated the viability of this approach for specific machines. However, this approach often fails to achieve the best possible results from a parallel machine for particular problems because the parallelizing compiler is often unable to specify the most efficient decomposition of a computation into parallel actions. 6.
Reference: [24] <author> R. Nikhil, K. Pingali, and Arvind. </author> <title> Id nouveau. </title> <type> Technical Report Computational structure group memo 256, </type> <institution> MIT Laboratory for Computer Science, </institution> <year> 1986. </year>
Reference-contexts: However, this approach often fails to achieve the best possible results from a parallel machine for particular problems because the parallelizing compiler is often unable to specify the most efficient decomposition of a computation into parallel actions. 6. A related category of approaches involves languages such as Id <ref> [24, 25] </ref> and Sisal [26, 27], which can be thought of as providing sequentializing compilers for implicitly parallel languages.
Reference: [25] <author> R. Nikhil. </author> <title> Id version 88.1 reference manual. </title> <type> Technical Report Computational structure group memo 284, </type> <institution> MIT Laboratory for Computer Science, </institution> <year> 1988. </year>
Reference-contexts: However, this approach often fails to achieve the best possible results from a parallel machine for particular problems because the parallelizing compiler is often unable to specify the most efficient decomposition of a computation into parallel actions. 6. A related category of approaches involves languages such as Id <ref> [24, 25] </ref> and Sisal [26, 27], which can be thought of as providing sequentializing compilers for implicitly parallel languages.
Reference: [26] <author> D. C. Cann, C-C Lee, R. R. Oldehoeft, and S. K. Skedzielwski. </author> <title> Sisal multiprocessing support. </title> <type> Technical Report UCID-21115, </type> <institution> University of California, Lawrence Livermore National Laboratory, </institution> <month> July </month> <year> 1987. </year>
Reference-contexts: A related category of approaches involves languages such as Id [24, 25] and Sisal <ref> [26, 27] </ref>, which can be thought of as providing sequentializing compilers for implicitly parallel languages. Programs specify potential decompositions of computations into parallel subcomputations, while the compiler choses which decompositions to ignore, and keeps track of data and control dependences to generate parallel tasks from the chosen decompositions.
Reference: [27] <author> D. C. Cann and J. Feo. </author> <title> Sisal versus fortran: A comparison using the livermore loops. </title> <booktitle> In Supercomputing, </booktitle> <month> November </month> <year> 1990. </year>
Reference-contexts: A related category of approaches involves languages such as Id [24, 25] and Sisal <ref> [26, 27] </ref>, which can be thought of as providing sequentializing compilers for implicitly parallel languages. Programs specify potential decompositions of computations into parallel subcomputations, while the compiler choses which decompositions to ignore, and keeps track of data and control dependences to generate parallel tasks from the chosen decompositions.
Reference: [28] <author> A. A. Chien and W. J. Dally. </author> <title> Concurrent aggregates. </title> <booktitle> In The second ACM Symp. on Principles & Practice of Parallel Programming, </booktitle> <pages> pages 187-196, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Programs specify potential decompositions of computations into parallel subcomputations, while the compiler choses which decompositions to ignore, and keeps track of data and control dependences to generate parallel tasks from the chosen decompositions. Fine-grained concurrent languages, such as Concurrent Aggregates <ref> [28, 29] </ref> and ABCL [30] also fall in this category. 7. Finally, domain specific packages allow the users to specify their problems, and possibly select solution strategies from the system's repertoire. These are also as high in the hierarchy as the automatic parallelizing compilers. <p> One of the first implementations of the Actor model on stock multicomputers was carried out using Charm by Houck and Agha [35]. The notion of concurrent aggregates was developed at MIT by Chien and Dally <ref> [28, 29] </ref> at the same time that the branch-office chares were implemented in Charm [54]. Concurrent aggregates were designed for fine-grained machines, and were implemented in a simulator.
Reference: [29] <author> A. Chien. </author> <title> Concurrent Aggregates: an object-oriented language for fine-grained message-passing machines. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: Programs specify potential decompositions of computations into parallel subcomputations, while the compiler choses which decompositions to ignore, and keeps track of data and control dependences to generate parallel tasks from the chosen decompositions. Fine-grained concurrent languages, such as Concurrent Aggregates <ref> [28, 29] </ref> and ABCL [30] also fall in this category. 7. Finally, domain specific packages allow the users to specify their problems, and possibly select solution strategies from the system's repertoire. These are also as high in the hierarchy as the automatic parallelizing compilers. <p> One of the first implementations of the Actor model on stock multicomputers was carried out using Charm by Houck and Agha [35]. The notion of concurrent aggregates was developed at MIT by Chien and Dally <ref> [28, 29] </ref> at the same time that the branch-office chares were implemented in Charm [54]. Concurrent aggregates were designed for fine-grained machines, and were implemented in a simulator.
Reference: [30] <author> Akinori Yonezawa. </author> <title> ABCL: an object oriented concurrent system. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Programs specify potential decompositions of computations into parallel subcomputations, while the compiler choses which decompositions to ignore, and keeps track of data and control dependences to generate parallel tasks from the chosen decompositions. Fine-grained concurrent languages, such as Concurrent Aggregates [28, 29] and ABCL <ref> [30] </ref> also fall in this category. 7. Finally, domain specific packages allow the users to specify their problems, and possibly select solution strategies from the system's repertoire. These are also as high in the hierarchy as the automatic parallelizing compilers.
Reference: [31] <author> L. V. Kale and W. Shu. </author> <title> The Chare Kernel language for parallel programming: A perspective. </title> <type> Technical Report UIUCDCS-R-88-1451, </type> <institution> Department of Computer Science, University of Illinois, </institution> <month> August </month> <year> 1988. </year>
Reference-contexts: Looking at this spectrum of approaches, it seemed apparent to us early in our research that there was a need for a general purpose language at a higher level in the hierarchy <ref> [31] </ref>. We need a language and system that satisfies the following requirements: R1 General purpose: The system shouldn't be restricted to narrow classes of application domains, or to narrow parallel programming paradigms. <p> Charm is one of the first languages to employ message-driven execution in stock multi-computers <ref> [31] </ref>. The idea of message-driven execution is clearly implicit in earlier work on Data Flow machines, which depended on special-purpose hardware to support it. Special purpose hardware for message-driven execution was also the focus of projects such as the J-Machine [47] and Mosaic [48].
Reference: [32] <author> L. V. Kale and S. Krishnan. </author> <title> Charm++ : A portable concurrent object oriented system based on C++. </title> <booktitle> In Proceedings of OOPSLA-93., </booktitle> <month> March </month> <year> 1993. </year> <note> (Also: Technical Report UIUCDCS-R-93-1796, </note> <month> March </month> <year> 1993, </year> <institution> University of Illinois, Urbana, IL. </institution>
Reference-contexts: In addition, any sequential threads of control in the parallel constructs must be expressible in C. With this decision, it becomes possible to retain large portions of sequential 3 A C++ based version of Charm, called Charm++, was recently designed and implemented by Krishnan and Kale <ref> [32] </ref>. 6 application code while parallelizing them. C is also a pragmatic choice as it is available on all parallel machines, and has an acceptable performance. Next, we must define the parallel constructs in the language. Parallelism entails the existence of multiple focii of control.
Reference: [33] <author> Wegner, P. </author> <title> Conceptual Evolution of Object-Oriented Programming. </title> <booktitle> In Proceedings of OOP-SLA, </booktitle> <year> 1989. </year> <note> keynote talk. </note>
Reference-contexts: At runtime, a single instance of a special chare, called main, exists. Execution of a Charm program begins with the 4 Therefore, following Peter Wegner's <ref> [33] </ref> terminology, Charm can be thought of as an object-based system. Note that Charm++, the C++ based version supports inheritance and virtual functions. 7 creation of an instance of the main chare and the execution of a special entry function in the main chare called CharmInit.
Reference: [34] <author> Attila Gursoy. </author> <title> Simplified Expression of Message-Driven Programs and Quantification of their Impact on Performance. </title> <type> PhD thesis, </type> <institution> Departmant of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: In many cases, such local rearrangement of communication can increase the utilization of processors. However, this strategy cannot handle cases with more complex dependences and unpredictable latencies, nor can it handle global operations <ref> [34] </ref>. In accordance with our latency tolerance requirement (R3), we want to avoid the idling of processors under this condition. This is accomplished in Charm by allowing multiple chares to exist on each processor and by employing a message driven 6 execution model. In this model: 1. <p> The request is therefore split across two phases: make request and receive data. A message-driven execution model allows one to adaptively schedule computations within and across modules as illustrated below. A more detailed exposition of the advantages of message driven execution can be found in <ref> [34] </ref>. 3.1 Latency tolerance within a module Consider the following example abstracted and modified from a real application | a core routine in parallelized version of a molecular mechanics code, CHARMM. Each processor has an array A of size n. <p> Consider the computation (taken from Figure <ref> [34] </ref>) shown in Figure 5: module A invokes two other modules B and C. In the SPMD model, module A cannot activate B and C concurrently even if the computations in B and C are independent of each other. <p> As a result, the processor time is not fully utilized, as illustrated in the same figure. In a message-driven paradigm, the idle times on a processor can be utilized by another module if it has some work to do. Such a scenario is illustrated in Figure 6 (taken from <ref> [34] </ref>). Module C gets processor time (by virtue of having its message selected by the scheduler) while B waits for some data, and vice versa, thus 10 achieving a better overlap than the SPMD program. Libraries constitute an important part of the software development process.
Reference: [35] <author> C. Houck and G. Agha. Hal: </author> <title> A high-level actor language and its implementation. </title> <type> Technical Report UIUCDCS-R-92-1728, </type> <institution> Department of Computer Science, University of Illinois, </institution> <month> September </month> <year> 1992. </year> <month> 24 </month>
Reference-contexts: BOCs can be used to implement and provide local "services" of various kinds. For example, the memory manager in the Chare Kernel (the runtime support system for Charm) is written as a BOC, and so is the message-receptionist in the parallel implementation of Actors reported by Agha and Houck <ref> [35] </ref>. In this use, the BOC does not send any messages to other branches, but simply acts as a local sequential object, with a globally unique instance address. 4. BOCs can also be used to implement services that require communication. <p> also been attained as is substantiated by the fact that Charm has been used as a back-end for a data parallel language called DP [43], a parallel Prolog compiler [44], a high-level synchronization language called Dagger [45], a domain specific language called Divide-and-conquer [46], and an Actor language called Hal <ref> [35] </ref>. Charm is one of the first languages to employ message-driven execution in stock multi-computers [31]. The idea of message-driven execution is clearly implicit in earlier work on Data Flow machines, which depended on special-purpose hardware to support it. <p> The Actor model provides a theoretical background that is applicable to a system such as Charm. One of the first implementations of the Actor model on stock multicomputers was carried out using Charm by Houck and Agha <ref> [35] </ref>. The notion of concurrent aggregates was developed at MIT by Chien and Dally [28, 29] at the same time that the branch-office chares were implemented in Charm [54]. Concurrent aggregates were designed for fine-grained machines, and were implemented in a simulator.
Reference: [36] <author> L . V. Kale and B. Ramkumar. </author> <title> Implementing a parallel Prolog interpreter on multiprocessors. </title> <booktitle> In 5th International Parallel Processing Symposium, </booktitle> <pages> pages 543-548, </pages> <address> Anaheim, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: This feature is instrumental in satisfying the requirement R3 about ensuring competitive efficiency on shared memory machines. As experimental evidence, conditional packing led to three-fold improvements in speed on iPSC/2 for a parallel prolog interpreter <ref> [36] </ref> implemented using Charm. 18 message f int a; varSize float b []; g MSG; ...
Reference: [37] <author> A. B. Sinha, L. V. Kale, and B. Ramkumar. </author> <title> Quiescence detection in dynamic process executions. </title> <note> submitted, </note> <year> 1993. </year>
Reference-contexts: In general, quiescence detection can be used to detect the end of a phase of computation which involves arbitrary and unpredictable amount of communication per processor. Quiescence detection has been efficiently implemented <ref> [37] </ref>, so that (a) the condition is detected very quickly after it occurs, and (b) the overhead of the algorithm is very small. 10 Prioritized Execution and load balancing In many computations, the order in which available tasks are selected for execution [38] can affect various performance metrics.
Reference: [38] <author> L . V. Kale, B. Ramkumar, V. Saletore, and A. B. Sinha. </author> <title> Prioritization in parallel symbolic computing. </title> <editor> In T. Ito and R. Halstead, editors, </editor> <booktitle> Lecture Notes in Comp. Science, </booktitle> <pages> pages 146-181. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: Quiescence detection has been efficiently implemented [37], so that (a) the condition is detected very quickly after it occurs, and (b) the overhead of the algorithm is very small. 10 Prioritized Execution and load balancing In many computations, the order in which available tasks are selected for execution <ref> [38] </ref> can affect various performance metrics. In Charm, the programmer can assign priorities to messages; the message-queuing strategy chosen by the user will then schedule the highest priority member of the queue. The priority can be an integer, or an arbitrarily long bit-vector, depending on the queuing strategy option chosen.
Reference: [39] <author> V. Saletore and L. V. Kale. </author> <title> Obtaining first solutions fast in parallel problem solving. </title> <booktitle> In The North American Conference on Logic Programming, </booktitle> <pages> pages 390-408, </pages> <month> Oct </month> <year> 1989. </year>
Reference-contexts: The priority can be an integer, or an arbitrarily long bit-vector, depending on the queuing strategy option chosen. Bit-vector priorities are especially useful for obtaining good and consistent speedups in state-space search and related problems <ref> [39, 40] </ref> which involve speculative work. Integer priorities are useful in many seemingly regular computations which may have critical paths [41] that must be prioritized, particularly in the presence of message-driven execution. Charm has many different load balancing strategies for message passing machines.
Reference: [40] <author> L. V. Kale and V. Saletore. </author> <title> Parallel state-space search for a first solution. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 19 </volume> <pages> 251-293, </pages> <year> 1990. </year>
Reference-contexts: The priority can be an integer, or an arbitrarily long bit-vector, depending on the queuing strategy option chosen. Bit-vector priorities are especially useful for obtaining good and consistent speedups in state-space search and related problems <ref> [39, 40] </ref> which involve speculative work. Integer priorities are useful in many seemingly regular computations which may have critical paths [41] that must be prioritized, particularly in the presence of message-driven execution. Charm has many different load balancing strategies for message passing machines.
Reference: [41] <author> A. B. Sinha and L. V. Kale. </author> <title> A load balancing strategy for prioritized execution of tasks. In Workshop on Dynamic Object Placement and Load Balancing, in co-operation with ECOOP's 92, </title> <address> Utrecht, The Netherlands, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Bit-vector priorities are especially useful for obtaining good and consistent speedups in state-space search and related problems [39, 40] which involve speculative work. Integer priorities are useful in many seemingly regular computations which may have critical paths <ref> [41] </ref> that must be prioritized, particularly in the presence of message-driven execution. Charm has many different load balancing strategies for message passing machines. <p> In particular, performance feedback and debugging tools can be built that provide the user with application-level feedback allowing them to home onto the trouble-spots in their source program easily. A preliminary step in this direction is represented by Projections <ref> [41] </ref>, a graphical performance display tool, which exploits the specificity only minimally by distinguishing between different kinds of messages.
Reference: [42] <author> A. B. Sinha and L. V. Kale. </author> <title> A load balancing strategy for prioritized execution of tasks. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <address> New Port Beach, CA., </address> <month> April </month> <year> 1993. </year>
Reference-contexts: Typically, for reasons of scalability and to avoid bottlenecks, it is desirable that load balancing strategies be distributed in nature. However, experimental results <ref> [42] </ref> have shown that existing fully distributed load balancing strategies do not balance priorities well resulting in the concentrations of low and high priority work neighborhoods. Therefore, in addition to distributed strategies, Charm also provides fully and partly centralized load balancing strategies.
Reference: [43] <author> E.A. Kornkven. </author> <title> Compiling data parallel languages for multi-threaded execution. </title> <type> Technical Report 90-03-01, </type> <institution> Parallel Programming Laboratory, Department of Computer Science, University of Illinois, Urbana, </institution> <address> IL 61801, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: This objective has also been attained as is substantiated by the fact that Charm has been used as a back-end for a data parallel language called DP <ref> [43] </ref>, a parallel Prolog compiler [44], a high-level synchronization language called Dagger [45], a domain specific language called Divide-and-conquer [46], and an Actor language called Hal [35]. Charm is one of the first languages to employ message-driven execution in stock multi-computers [31].
Reference: [44] <author> B. Ramkumar and L. V. </author> <title> Kale . Compiled Execution of the REDUCE-OR Process Model on Multiprocessors. </title> <booktitle> In North American Conference on Logic Programming, </booktitle> <month> October </month> <year> 1989. </year>
Reference-contexts: This objective has also been attained as is substantiated by the fact that Charm has been used as a back-end for a data parallel language called DP [43], a parallel Prolog compiler <ref> [44] </ref>, a high-level synchronization language called Dagger [45], a domain specific language called Divide-and-conquer [46], and an Actor language called Hal [35]. Charm is one of the first languages to employ message-driven execution in stock multi-computers [31].
Reference: [45] <author> A. Gursoy and L.V. Kale. Dagger: </author> <title> Combining the benefits of synchronous and asynchronous communication styles. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <address> Cancun, Mexico, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: This objective has also been attained as is substantiated by the fact that Charm has been used as a back-end for a data parallel language called DP [43], a parallel Prolog compiler [44], a high-level synchronization language called Dagger <ref> [45] </ref>, a domain specific language called Divide-and-conquer [46], and an Actor language called Hal [35]. Charm is one of the first languages to employ message-driven execution in stock multi-computers [31].
Reference: [46] <author> A. Gursoy and L . V. Kale. </author> <title> High-level support for divide-and-conquer parallelism. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 283-292, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: This objective has also been attained as is substantiated by the fact that Charm has been used as a back-end for a data parallel language called DP [43], a parallel Prolog compiler [44], a high-level synchronization language called Dagger [45], a domain specific language called Divide-and-conquer <ref> [46] </ref>, and an Actor language called Hal [35]. Charm is one of the first languages to employ message-driven execution in stock multi-computers [31]. The idea of message-driven execution is clearly implicit in earlier work on Data Flow machines, which depended on special-purpose hardware to support it.
Reference: [47] <author> W. Dally et al. </author> <title> The j-machine: a fine-grained computer. </title> <booktitle> In IFIP Congress, </booktitle> <year> 1989. </year>
Reference-contexts: The idea of message-driven execution is clearly implicit in earlier work on Data Flow machines, which depended on special-purpose hardware to support it. Special purpose hardware for message-driven execution was also the focus of projects such as the J-Machine <ref> [47] </ref> and Mosaic [48]. The work on macro data flow [49] has focussed on bringing these concepts on general purpose hardware (stock multicomputers). The work on Active Messages [50] is more recent than Charm.
Reference: [48] <author> W. C. Athas and C. L. Seitz. </author> <title> Multicomputers: Message-passing concurrent computers. </title> <journal> Computer, </journal> <volume> 21, No. 8 </volume> <pages> 9-24, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: The idea of message-driven execution is clearly implicit in earlier work on Data Flow machines, which depended on special-purpose hardware to support it. Special purpose hardware for message-driven execution was also the focus of projects such as the J-Machine [47] and Mosaic <ref> [48] </ref>. The work on macro data flow [49] has focussed on bringing these concepts on general purpose hardware (stock multicomputers). The work on Active Messages [50] is more recent than Charm. In this model, a message interrupts the recipient process, and invokes the handler rountine specified in the message.
Reference: [49] <author> A. S. Grimshaw. </author> <title> Mentat: an object-oriented macro data flow system. </title> <type> PhD thesis, </type> <institution> University of Illinois, Urbana-Champaign, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: The idea of message-driven execution is clearly implicit in earlier work on Data Flow machines, which depended on special-purpose hardware to support it. Special purpose hardware for message-driven execution was also the focus of projects such as the J-Machine [47] and Mosaic [48]. The work on macro data flow <ref> [49] </ref> has focussed on bringing these concepts on general purpose hardware (stock multicomputers). The work on Active Messages [50] is more recent than Charm. In this model, a message interrupts the recipient process, and invokes the handler rountine specified in the message.
Reference: [50] <author> T. vonEicken, D. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> ACM, </booktitle> <pages> pages 256-266, </pages> <year> 1992. </year>
Reference-contexts: Special purpose hardware for message-driven execution was also the focus of projects such as the J-Machine [47] and Mosaic [48]. The work on macro data flow [49] has focussed on bringing these concepts on general purpose hardware (stock multicomputers). The work on Active Messages <ref> [50] </ref> is more recent than Charm. In this model, a message interrupts the recipient process, and invokes the handler rountine specified in the message. Active messages only provides a low-level mechanism for writing message driven programs.
Reference: [51] <author> D. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. vonEicken, and K. Yelick. </author> <title> Parallel programming in split-c. </title> <type> Technical report, </type> <institution> University of California, Berkeley. </institution>
Reference-contexts: Active messages implementations carried out at the operating system level can deliver messages even faster than the vendor's send-receive primitives on some machines. In fact, the communication layer of Charm has recently been implemented on the CM-5 using Active Messages. Split-C <ref> [51] </ref> is a programming language that provides a global data space, where accesses to global data are through unique split-phase operators, which separates the request for data from its use (this aspect is similar to "futures").
Reference: [52] <author> J. K. Bennett, J. B. Carter, and W. Zwaenepoel. Munin: </author> <title> distributed shared memory based on type-specific memory coherence. </title> <booktitle> In The second ACM Symp. on Principles & Practice of Parallel Programming, </booktitle> <month> March </month> <year> 1990. </year>
Reference-contexts: The primary differences with Charm are that it is also a single-process model and that it provides a global address space, and its primitives do not permit an adaptive overlap of computation and communication. The observation that information is shared in many specific modes was also made independently in <ref> [52] </ref>. However, they used it for annotations and optimizations in a parallelizing compiler. The 21 dataflow community also developed similar notions | often called "sideways" communication prim-itives | in the context of functional programs.
Reference: [53] <author> G. A. Agha. </author> <title> Actors: A Model of Concurrent Computation in Distributed Systems. </title> <publisher> MIT press, </publisher> <year> 1986. </year>
Reference-contexts: In Linda, the accesses to tuples are blocking whereas in Charm an entry from a distributed table is accessed in a non-blocking split-phase manner. Moreover, tuples are the only information sharing mechanism in Linda, whereas tables are one of many in Charm. Actors <ref> [53] </ref> , a construct proposed by Hewitt and developed by Gul Agha, embodied one of the early proposals for message driven execution. Each actor has a behavior associated with it. Actors do not issue "receive" statements, but execute only when triggered by a message.
Reference: [54] <author> L. V. Kale et al. </author> <title> The Charm Programming Language Manual (4.1), </title> <booktitle> 1994. </booktitle> <pages> 26 </pages>
Reference-contexts: One of the first implementations of the Actor model on stock multicomputers was carried out using Charm by Houck and Agha [35]. The notion of concurrent aggregates was developed at MIT by Chien and Dally [28, 29] at the same time that the branch-office chares were implemented in Charm <ref> [54] </ref>. Concurrent aggregates were designed for fine-grained machines, and were implemented in a simulator. The members of a concurrent aggregate are analogous to a branch of a branch-office chare, except that they do not necessarily have a member on every processor.
References-found: 54

