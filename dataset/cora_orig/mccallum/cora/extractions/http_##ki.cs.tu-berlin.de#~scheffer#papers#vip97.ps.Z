URL: http://ki.cs.tu-berlin.de/~scheffer/papers/vip97.ps.Z
Refering-URL: http://ki.cs.tu-berlin.de/~scheffer/publications.html
Root-URL: 
Email: email: ralfh|scheffer@cs.tu-berlin.de  
Title: Generation of task-specific segmentation procedures as a model selection task  
Author: Ralf Herbrich and Tobias Scheffer 
Keyword: model based image segmentation, automatic model selection, learning pixel classifier, texture segmentation  
Date: December 1, 1997  
Address: Sekr. FR 5-8, Franklinstr. 28/29. D-10587 Berlin, Germany  
Affiliation: Technische Universitat Berlin, Artificial Intelligence Research Group,  
Abstract: In image segmentation problems, there is usually a vast amount of filter operations available, a subset of which has to be selected and instantiated in order to obtain a satisfactory segmentation procedure for a particular domain. In supervised segmentation, a mapping from features, such as filter outputs for individual pixels, to classes is induced automatically. However, since the sample size required for supervised learning grows exponentially in the number of features it is not feasible to learn a segmentation procedure from a large amount of possible filters. But we argue that automatic model selection methods are able to select a region model in terms of some filters. We propose a wrapper algorithm that performs this task. We present results on artificial textured images (Brodatz) and report on our experiences with x-ray images. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Baxter, J. </author> <year> (1997). </year> <title> A model for bias learning. </title> <journal> Journal of the ACM. </journal> <note> submitted. 9 Blumer, </note> <author> A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. </author> <year> (1987). </year> <title> Occam's razor. </title> <journal> Infor--mation processing Letters, </journal> <volume> 24, </volume> <pages> 377380. </pages>
Reference-contexts: One way to overcome this problem is to learn an appropriate model selection bias for a class of similar problems <ref> (Baxter, 1997) </ref>. While the Gabor filters which we use as attributes are fixed and the task is to select a subset of them to form a well discriminating hypothesis, Weldon and Higgins (1997) propose a method for optimizing Gabor filters for a specific classification task.
Reference: <author> Bovik, A. C., Clark, M., & Geisler, W. S. </author> <year> (1990). </year> <title> Multichannel texture analysis using localized spatial filters. </title> <journal> IEEE Transactions on PAMI, </journal> <volume> 12(1), </volume> <pages> 5573. </pages>
Reference-contexts: Known supervised segmentation algorithms therefore require a restricted region model in terms of few, relevant features, e.g., based on the frequency spectrum <ref> (Bovik et al., 1990) </ref>, the second order statistics of the image (Houzelle & Giraudon, 1992), or the assumption that the image is a MRF (Cross & Jain, 1983).
Reference: <author> Brodatz, P. </author> <year> (1966). </year> <title> Textures: A Photographic Album for Artists and Designers. </title> <publisher> Dover. </publisher>
Reference-contexts: Texture segmentation In these experiments, the task is to learn the segmentation of a patchwork image consisting of Brodatz textures taken from <ref> (Brodatz, 1966) </ref>. We use the two images in Figure 1 (above tables (a) and (b)). In the feature extraction phase, we convolved the image with a bank of Gabor filters to compute the amplitudes.
Reference: <author> Cross, G. R., & Jain, A. K. </author> <year> (1983). </year> <title> Markov random field texture models. </title> <journal> IEEE Transaction on PAMI, </journal> <volume> 5(1), </volume> <pages> 2539. </pages>
Reference-contexts: Known supervised segmentation algorithms therefore require a restricted region model in terms of few, relevant features, e.g., based on the frequency spectrum (Bovik et al., 1990), the second order statistics of the image (Houzelle & Giraudon, 1992), or the assumption that the image is a MRF <ref> (Cross & Jain, 1983) </ref>. Automatic model selection (for an overview, see, e.g., Kearns et al., 1997) deals with the question how complex an optimal model should be, depending on the problem and the available sample size.
Reference: <author> Greenspan, H. </author> <year> (1996). </year> <title> learning textures. In Early Visual Learning. </title> <publisher> Oxford University Press. </publisher>
Reference: <author> Hansen, L., & Salamon, P. </author> <year> (1990). </year> <title> Neural network ensembles. </title> <journal> IEEE Transactions on PAMI, </journal> <volume> 12(10), </volume> <pages> 9931001. </pages>
Reference-contexts: Exemplary, Figure 2 shows the results on one image, without feature subset selection (b), with forward feature subset selection (c) and, additionally, the en 7 hancement obtained by combining the learning algorithm with a majority voting technique (d) <ref> (Hansen & Salamon, 1990) </ref>. (a) (b) (c) (d) forward feature subset selection; (d) with ensembles of 10 hypotheses If we induce a segmentation procedure without model selection, the results are unacceptably poor. (Figure 2 (b)).
Reference: <author> Houzelle, S., & Giraudon, G. </author> <year> (1992). </year> <title> Model based region segmentation using cooccur-rence matrices. </title> <booktitle> In IEEE Conference on CVPR, </booktitle> <pages> pp. 636639. </pages>
Reference-contexts: Known supervised segmentation algorithms therefore require a restricted region model in terms of few, relevant features, e.g., based on the frequency spectrum (Bovik et al., 1990), the second order statistics of the image <ref> (Houzelle & Giraudon, 1992) </ref>, or the assumption that the image is a MRF (Cross & Jain, 1983). Automatic model selection (for an overview, see, e.g., Kearns et al., 1997) deals with the question how complex an optimal model should be, depending on the problem and the available sample size.
Reference: <author> Kearns, M., Mansour, Y., Ng, A. Y., & Ron, D. </author> <year> (1997). </year> <title> An experimental and theoretical comparsion of model selection methods. </title> <journal> Machine Learning, </journal> <volume> 27, </volume> <pages> 750. </pages>
Reference-contexts: Automatic model selection <ref> (for an overview, see, e.g., Kearns et al., 1997) </ref> deals with the question how complex an optimal model should be, depending on the problem and the available sample size.
Reference: <author> Kohavi, R., & John, G. </author> <year> (1997). </year> <title> Wrappers for feature subset selection. </title> <journal> Journal of AI, </journal> <note> Special Issue on Relevance. to appear. </note>
Reference-contexts: Too small a model is unlikely to contain any good hypothesis, while too rich a model will inevitably incur over-fitting this problem is often referred to as the bias-variance tradeoff. Wrapper algorithms <ref> (Kohavi & John, 1997) </ref> find optimal models by repeatedly testing the best hypotheses of different models on hold-out sets. In Section 2, we describe the model selection problem briefly. Then, we discuss wrappers for feature subset selection and show how they can be applied for image segmentation. <p> The more hypotheses one assesses on a sample, the more optimistically assessed the apparently best hypothesis will be which increases the chance of selecting an apparently good hypothesis with a high true error. 3 which model incurs an optimally low error: hold-out testing e.g., <ref> (Kohavi & John, 1997) </ref> and complexity penalization based approaches, such as Minimum Description Length (Rissanen, 1985), or Guaranteed Risk Minimization (Vapnik, 1982). 3 Wrappers for model selection In order to find the best model we need to estimate the true error E D of the best hypothesis h min i of
Reference: <author> Ng, A. </author> <year> (1997). </year> <title> Preventing overfitting of cross validation data. </title> <booktitle> In Proceedings of the International Conference on Machine Learning, </booktitle> <pages> pp. 245253. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: But it will yield a high gain if the available feature set consists of a large library 8 of filters, as is often the case in pattern recognition tasks. Unfortunately, model selection incurs a higher-level over-fitting of the data, similar to the over-fitting caused by too large hypothesis spaces <ref> (Ng, 1997) </ref>. If the number of models that are compared by cross validation is large, chances are that one model matches the hold-out sets particularly well without actually being a good model.
Reference: <author> Rissanen, J. </author> <year> (1985). </year> <title> Minimumdescriptionlength principle. </title> <journal> Annuals of the Statistics, </journal> <volume> 6, </volume> <pages> 461464. </pages>
Reference-contexts: the more optimistically assessed the apparently best hypothesis will be which increases the chance of selecting an apparently good hypothesis with a high true error. 3 which model incurs an optimally low error: hold-out testing e.g., (Kohavi & John, 1997) and complexity penalization based approaches, such as Minimum Description Length <ref> (Rissanen, 1985) </ref>, or Guaranteed Risk Minimization (Vapnik, 1982). 3 Wrappers for model selection In order to find the best model we need to estimate the true error E D of the best hypothesis h min i of each model H i .
Reference: <author> Scheffer, T. </author> <year> (1996). </year> <title> Algebraic foundation and improved methods of induction of ripple down rules. </title> <booktitle> In Proccedings of the Pacific Rim Workshop on Knowledge Acquisition, </booktitle> <pages> pp. 279292. </pages>
Reference-contexts: When performing feature subset selection we use 10-fold cross validation. In our implementation, we use a learning algorithm that induces ripple down rules <ref> (Scheffer, 1996) </ref> which are rules with hierarchically nested exceptions. Ripple down rules incur a language bias similar to decision trees, the only difference being that every node in a ripple down rules contains a conjunction of interval constraints rather than a single test.
Reference: <author> Vapnik, V. </author> <year> (1982). </year> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer Verlag, </publisher> <address> New York. </address>
Reference-contexts: best hypothesis will be which increases the chance of selecting an apparently good hypothesis with a high true error. 3 which model incurs an optimally low error: hold-out testing e.g., (Kohavi & John, 1997) and complexity penalization based approaches, such as Minimum Description Length (Rissanen, 1985), or Guaranteed Risk Minimization <ref> (Vapnik, 1982) </ref>. 3 Wrappers for model selection In order to find the best model we need to estimate the true error E D of the best hypothesis h min i of each model H i .
Reference: <author> Weldon, T. P., & Higgins, W. E. </author> <year> (1997). </year> <title> Algorithm for designing multiple gabor filters for segmenting multi-textured images. </title> <journal> unpublished. </journal> <volume> 10 </volume>
References-found: 14

