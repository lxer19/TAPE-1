URL: http://www.ai.univie.ac.at/~bernhard/diss.ps.gz
Refering-URL: http://www.ai.univie.ac.at/cgi-bin/biblio_ora?sort_by_author=yes&tailor=1&loc=0&format=ml/ml&keyword=Publications&keyword=WWW_ML&relop=/
Root-URL: 
Title: Practical Uses of the Minimum Description Length Principle in Inductive Learning ausgefuhrt zum Zwecke der
Author: eingereicht an der von Dipl.-Ing. Bernhard Pfahringer Otto Bauergasse 
Date: April 1995  
Affiliation: Technischen Universitat Wien Technisch-Naturwissenschaftliche Fakultat  
Note: Dissertation  8/2, A-1060 Wien Matr.Nr. 8115321 Geboren am 3.Oktober 1962 in Hall in Tirol Wien, im  
Abstract-found: 0
Intro-found: 1
Reference: [Agrawal et al. 93] <author> Rakesh Agrawal, Tomasz Imielinski and Arun Swami: </author> <title> Mining Association Rules between Sets of Items in Large Databases. </title> <booktitle> Proceedings of the 1993 ACM SIGMOD Conference, </booktitle> <address> Washington D.C., </address> <year> 1993. </year>
Reference: [Agrawal & Srikant 94] <author> Rakesh Agrawal and Ramakrishnan Srikant: </author> <title> Fast Algorithms for Mining Association Rules. </title> <booktitle> Proceedings of the 20 th VLDB Conference, </booktitle> <address> Santiago, Chile, </address> <year> 1994. </year>
Reference-contexts: Let R = fI 1 ; I 2 ; :::; I m g be a relation schema with attributes over a binary domain f0; 1g. <ref> [Agrawal & Srikant 94] </ref> call those 5.3. EVALUATING PARTIAL DETERMINATIONS 48 attributes items.) Let r = ft 1 ; t 2 ; :::; t n g be a relation over the relation schema R.
Reference: [Almuallim & Dietterich 91] <author> Almuallim H., Dietterich T.G.: </author> <title> Learning with Many Irrelevant Features, </title> <booktitle> in Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> Menlo Park, </address> <booktitle> Vol. II, </booktitle> <address> pp.547-552, </address> <year> 1991. </year>
Reference-contexts: It seems to be a more efficient alternative to standard cross-validation, which optimizes solely predictive accuracy for selecting good subsets of features in inductive learning. Alternative methods for judging attribute relevance are e.g. FOCUS <ref> [Almuallim & Dietterich 91] </ref> and RELIEF [Kira & Rendell 92]. Their respective shortcomings are detailed in [John et al. 94].
Reference: [Angluin & Laird 87] <author> Angluin D., Laird P.: </author> <title> Learning from Noisy Examples, </title> <journal> Machine Learning, </journal> <volume> 2(4), pp.343-370, </volume> <year> 1987. </year>
Reference-contexts: About half of the entries of table 2.3 are empty, meaning no compression was achieved. We therefore suspect that their coding scheme for theories and proofs is inefficient in an information-theoretic sense: too many bits 1 We are using the so-called Classification Noise Process <ref> [Angluin & Laird 87] </ref> throughout this thesis. A class noise level of means that for every training example the classification is reversed with probability . This corresponds to a class noise level of 2 for the model incorporated by [Quinlan 94]. <p> This over-abundance in the representation space may quickly result in unwieldy, overly complex induced rule sets or attributes when learning without appropriate control. Such rule sets can be difficult for the user to comprehend and may yield mediocre results when classifying unseen examples. In analogy to noise fitting <ref> [Angluin & Laird 87] </ref> this phenomenon can be called language fitting. Typical examples of such behavior are published in the section on AQ17-HCI in the Monk report [Thrun et al. 91], which describes three artificial learning problems for evaluating and comparing different algorithms.
Reference: [Bloedorn et al. 93] <author> Bloedorn E., Wnek J., Michalski R.S.: </author> <title> Multistrategy Constructive Induction: </title> <editor> AQ17-MCI, in Michalski R.S. and Tecuci G.(eds.), </editor> <booktitle> Proceedings of the Second International Workshop on Multistrategy Learning (MSL-93), </booktitle> <address> Harpers Ferry, W.VA., pp.188-206, </address> <year> 1993. </year>
Reference-contexts: That would make it applicable to all kinds of propositional as well as first-order learning problems. * Furthermore the cost of additional attributes induced by means of constructive induction <ref> [Bloedorn et al. 93, Pfahringer 94b] </ref> must be accounted for. In chapter 6 we show how constructive induction can help the learning process in cases of an inadequate initial representation language. <p> This would give such a system a kind of case-based reasoning flavor [Kolodner 93]. * Compression-based feature subset selection could also be used as an input filter for constructive induction <ref> [Bloedorn et al. 93, Pfahringer 94b] </ref>, which can help the learning process in cases of an inadequate initial representation language. <p> CiPF is a true instance of the generic architecture for constructive induction described above in that it realizes all the boxes and pathways. CiPF's components will be detailed in the following. 6.2.1 Constructive Induction in CiPF Just like the multi-strategy system AQ17-MCI <ref> [Bloedorn et al. 93] </ref>, CiPF takes an operator-based approach to constructive induction. It supplies a (still growing) list of generally useful CI operators plus an interface allowing for user-supplied special operators. For instance, these operators might encode possibly relevant background knowledge. <p> Anyway it being a very useful operator we have chosen to include it in the above list. Furthermore the terminology used in <ref> [Bloedorn et al. 93] </ref> defines the set of constructive induction operators as the union of constructors and destructors. 6.2. A GENERIC ARCHITECTURE FOR CONSTRUCTIVE INDUCTION62 * Only the fittest attributes will be allowed to survive.
Reference: [Breiman et al. 84] <author> Breiman L., Friedman J.H., Olshen R.A., Stone C.J.: </author> <title> Classification and Regression Trees, </title> <booktitle> Wadsworth International Group, </booktitle> <address> Belmont, CA, </address> <publisher> The Wadsworth Statistics/Probability Series, </publisher> <year> 1984. </year>
Reference-contexts: Rule sets induced by C4.5 typically consist of the correct rules (or specializations thereof) and some additional spurious rules, thus clearly showing a bit of overfitting. * DIGIDAT. This example is essentially the same as used in <ref> [Breiman et al. 84] </ref> as a running example. Each data record is generated by simulating a faulty liquid crystal display in which digits are displayed by setting bars on or off. There are seven bars, each of which may be on or off. <p> Our simplifications allow for efficient implementation with tightly limited search (linear in the number of pairs of values), but still seem to yield useful (though maybe not always immediately the absolute best) abstractions. This operator can also be seen as a generalization of the technique described in <ref> [Breiman et al. 84] </ref> for computing optimal binary splits for single attributes: we handle combinations of two attributes.
Reference: [Caruana & Freitag 94] <author> Caruana R., Freitag D.: </author> <title> Greedy Attribute Selection, </title> <editor> in Cohen W.W. and Hirsh H.(eds.), </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference (ML94), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1994. </year>
Reference-contexts: As especially predictive accuracy cannot reliably be estimated from the complete set of training examples, usually some kind of cross-validation has to be used <ref> [John et al. 94, Kohavi 95, Caruana & Freitag 94] </ref>. In this chapter we describe the application of the Minimum Description Length (MDL) principle as an objective function for judging goodness of a feature subset. Feature subsets are used to construct so called simple decision tables [Kohavi 95]. <p> Best-first search is only one of a few possible search strategies. Clearly exhaustive enumeration of all possible subsets is out of the question except for the simplest domains. <ref> [Caruana & Freitag 94] </ref> compare various forms of greedy hill-climbing: * Forward Selection (FS): start with an empty set of attributes and greedily add attributes one at a time. * Backward Elimination (BE): start with the complete set of attributes and greedily delete attributes one at a time. * Forward Stepwise
Reference: [Catlett 91] <author> Catlett J.: </author> <title> On Changing Continuous Attributes into Ordered Discrete Attributes, </title> <editor> in Kodratoff Y.(ed.), EWSL-91, </editor> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1991. </year>
Reference-contexts: Any of the above mentioned class-driven discretization algorithms could be chosen for such a heuristic pre-selection of split-points, especially as they all tend to produce too fine a discretization. For efficiency reasons we employ a simplified version of D-2 <ref> [Catlett 91] </ref>, as it is the only top-down method. The other three methods basically all merge intervals bottom-up, starting from the smallest possible intervals. Therefore their complexity is O (N 2 ) for N being the total number of possible split-points. <p> All the domains used in this empirical evaluation are 2-class problems (but see above comments regarding N-class problems). The main test-bed is a simple artificial domain introduced by <ref> [Catlett 91] </ref>. Its artificial nature allows for extensive experiments with various sizes of training sets and levels of class noise. <p> Additionally we also performed some experiments with so-called natural domains mostly available from the UC Irvine Machine Learning Repository [Murphy & Aha 94]. 4 These experiments also support our claims, but to a lesser degree, as these databases tend to be rather small. 3.4.1 An Artificial Domain <ref> [Catlett 91] </ref> introduces the artificial Demon domain as a kind of worst case scenario. Every example has three continuous attributes A1, A2, and A3 drawn randomly from the interval [0; 1). A1 acts as a kind of switch (and as such A1 is not directly correlated to an example's class). <p> The bad results of D-2 in this domain are not surprising. Even for the original D-2 reported results are significantly worse compared to using just raw data <ref> [Catlett 91] </ref>. Allowing for a more fine-grained discretization does not seem to solve the basic problem: discretizations for the switch attribute A1 tend to be poor. The search is misled by small localized random perturbations of an ideally equal class distribution.
Reference: [Cheeseman et al. 88] <author> Cheeseman P., Self M., Kelly J., Taylor W., Freeman D., Stutz J.: </author> <title> Bayesian Classification, </title> <booktitle> in Proceedings of the Seventh National 88 BIBLIOGRAPHY 89 Conference on Artificial Intelligence (AAAI 88), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, pp.607-611, </address> <year> 1988. </year>
Reference-contexts: THEORETICAL WORK 82 7.4 Theoretical Work In this section we just shortly reference two interesting papers, one defining a measure for the interestingness of single rules closely related to the MDL principle, the other one detailing the relationship between the MDL principle and Bayesian inference (see also <ref> [Cheeseman et al. 88] </ref>). 7.4.1 Smyth [Smyth & Goodman 91] describe an interesting schema for judging the goodness of single rules. Their aim is to rank rules with respect to each other. Their formula is also based on information theory.
Reference: [Cheeseman 90] <author> Cheeseman P.: </author> <title> On Finding the Most Probable Model, </title> <editor> in Shrager J., Langley P.(eds.): </editor> <title> Computational Models of Scientific Discovery and Theory Formation, </title> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <year> 1990. </year>
Reference-contexts: The MDL principle states that the best theory derivable from the training data will be the one requiring the minimum number of bits. A very good further introduction to the MDL principle and also its close relation to Bayesian theory can be found in <ref> [Cheeseman 90] </ref>. He defines the message length of a rule set (called model in his article) as: Total message length = Message length to describe the model + Message length to describe the data, given the model.
Reference: [Chiu et al. 91] <author> Chiu D., Wong A., Cheung B.: </author> <title> Information Discovery through Hierarchical Maximum Entropy Discretization and Synthesis, </title> <editor> in Piatetsky-Shapiro G. & Matheus C.J., </editor> <title> Knowledge Discovery in Databases:, </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: INTRODUCTION 19 3. Accuracy: in the presence of noise good discretizations can sometimes im prove predictive accuracy. Simple discretization methods are class-blind , i.e. they group training examples into intervals without taking into account the respective classes of the training examples. <ref> [Chiu et al. 91] </ref> compares three such methods: equal-width discretiza-tion, equal-frequency discretization and maximum marginal entropy discretiza-tion.
Reference: [Conklin & Witten 94] <author> Conklin D., Witten I.H.: </author> <title> Complexity-Based Induction, </title> <booktitle> Machine Learning, </booktitle> <address> 16,3, </address> <year> 1994. </year>
Reference-contexts: Only the latter would ensure a bias towards efficiently computable programs! 1 This coding schema does not take into account any (first) argument indexing used by most Prolog implementations. 7.2. ILP USAGES OF MDL 77 <ref> [Conklin & Witten 94] </ref> note a further related problem with this proof complexity measure: Even if the respective theory is complete and correct, still proofs are transmitted for all examples. <p> Whether it really does is hard to tell because of the ad-hoc nature of controlled overfitting. A more thorough investigation of controlled overfitting and its interaction with different coding schemes may be worthwhile. 7.2.3 Conklin & Witten <ref> [Conklin & Witten 94] </ref> apply the MDL principle to the evaluation of alternative inductive logic theories. They are especially concerned with learning situations 7.2. ILP USAGES OF MDL 78 where negative examples of concepts are either scarce or not available at all. <p> Each field of Machine Learning not touched in the previous chapters offers opportunities for potential application of the MDL principle. The most prominent fields are: 1. Inductive Logic Programming: in chapter 7 we have mentioned <ref> [Conklin & Witten 94] </ref> and [Muggleton et al. 92]; neither is able to give really satisfactory coding schemas. Even more, the evaluation of pre-specified and possibly even automatically derived background knowledge is a demanding open problem.
Reference: [Cook & Holder 94] <author> Diane J. Cook, Lawrence B. Holder: </author> <title> Substructure Discovery Using Minimum Description Length and Background Knowledge. </title> <journal> Journal of Artificial Intelligence Research 1 (1994) 231-255, </journal> <year> 1994. </year>
Reference-contexts: Especially accuracy yields bad predictive accuracy as it overfits the training data. 7.3.4 Cook & Holder In <ref> [Cook & Holder 94] </ref> the MDL principle is used for discovering conceptually interesting and repetitive substructures in structural data. Data is represented by a graph structure. Example domains include chemical compound analysis, CAD circuit analysis, scene analysis, and analysis of artificially-generated graphs. <p> first-order data in a graph-based way (as e.g. is done when using so-called conceptual structures [Sowa 83] as a representation language), this measure might prove useful for such graph-based inductive logic programming. 7.3.5 Djoko The work described in [Djoko 94] seems to be closely related to the work reported in <ref> [Cook & Holder 94] </ref> (see above). At least they use exactly the same graph-based domains and are also concerned with extraction of useful syb-graphs. They too add heuristics taking into account user-supplied background knowledge in addition to the MDL principle. <p> They too add heuristics taking into account user-supplied background knowledge in addition to the MDL principle. Unfortunately exact formulas are missing from this paper, therefore it is not possible to assess its true relationship to either <ref> [Cook & Holder 94] </ref> or our own work. 7.4.
Reference: [Derthick 91] <author> Derthick M.: </author> <title> A Minimal Encoding Approach to Feature Discovery, </title> <booktitle> in Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> Menlo Park, </address> <booktitle> Vol. II, </booktitle> <address> pp.565-571, </address> <year> 1991. </year>
Reference-contexts: Note that the error coding used here is global. Reported experimental results confirm the applicability and usefulness of this simple coding schema. 7.3.2 Derthick <ref> [Derthick 91] </ref> uses the MDL principle for discovering orthogonal features on relational data. This work was done in the context of the CYC project [Guha & Lenat 91]. Therefore it is not surprising that all data used is represented by binary relations.
Reference: [Dietterich & Bakiri 95] <author> Dietterich T.G., Bakiri G.: </author> <title> Solving Multiclass Learning Problems via Error- Correcting Output Codes, </title> <journal> Journal of AI Research, </journal> <volume> Vol.2, </volume> <pages> pp. 263-286, </pages> <year> 1995. </year>
Reference-contexts: So we define the cost (bit size) of a discretization as follows (for the sake of simplicity we restrict ourselves to two-class problems, but all formulas can be straightforwardly generalized to multi-class problems, or alternatively multi-class problems could also be cast as two-class problems <ref> [Dietterich & Bakiri 95] </ref>): cost (Disc) = cost (DiscDef ) + cost (Examples) (3.1) cost (DiscDef ) = cost (Split) + cost (M ajorityClasses) (3.2) cost (Split) = cost (choose (U sedSplits; P ossibleSplits))(3.3) cost (M ajorityClasses) = cost (choose (P ositive; Intervals)) (3.4) cost (Examples) = cost (Examples=Interval i )
Reference: [Dietterich & Michalski 81] <author> Dietterich T.G., Michalski R.S.: </author> <title> Inductive Learning of Structural Descriptions: Evaluation Criteria and Comparative Review of Selected Methods, </title> <journal> Artificial Intelligence, </journal> <volume> 16(3), pp.257-294, </volume> <year> 1981. </year>
Reference-contexts: Alternative methods like cross-validation or the use of some arbitrary quality criteria (e.g. the lexicographical evaluation function LEF of <ref> [Dietterich & Michalski 81] </ref>) need to split the training set into two parts: one will be used for induction, the other one will be used for evaluation. Splitting can be problematic for small training sets: only regularities present in both sub-sets can be found and kept. <p> For example, decision trees encode axis-parallel nested hyper-rectangles. Two different problems may cause irregular distributions of learning examples in the original representation space: noise and/or an inadequate description language. As a remedy for the latter problem constructive induction has been introduced, e.g. in <ref> [Dietterich & Michalski 81] </ref> and [Mehra et al. 89]. The basic idea is to somehow transform the original representation space into a space where the learning examples exhibit (more) regularities. Usually this is done by introducing new attributes and forgetting old ones.
Reference: [Djoko 94] <author> Djoko S.: </author> <title> Substructure Discovery Using Minimum Description Length Principle and Background Knowledge, </title> <booktitle> in Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> Cam-bridge, MA, p.1442, </address> <year> 1994. </year>
Reference-contexts: But reported results are very interesting. Furthermore, if one would represent first-order data in a graph-based way (as e.g. is done when using so-called conceptual structures [Sowa 83] as a representation language), this measure might prove useful for such graph-based inductive logic programming. 7.3.5 Djoko The work described in <ref> [Djoko 94] </ref> seems to be closely related to the work reported in [Cook & Holder 94] (see above). At least they use exactly the same graph-based domains and are also concerned with extraction of useful syb-graphs.
Reference: [Fayyad & Irani 93] <author> Fayyad U.M., </author> <title> Irani K.B.: Multi-Interval Discretization of Continuous-Valued Attributes for Classification Learning, in Bajcsy R.(ed.), </title> <booktitle> Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, pp.1022-1029, </address> <year> 1993. </year>
Reference-contexts: They also cannot distinguish truly uncorrelated (irrelevant) attributes from what [Kerber 92] calls second-order correlated attributes, i.e. attributes correlating only in the presence of some other condition. An alternative approach to discretization is described in <ref> [Fayyad & Irani 93] </ref>. They use the MDL principle to decide, during tree induction, whether a numerical attribute should be split into two intervals (the standard approach used by C4.5) or more. In the latter case interval boundaries will also be determined with the help of the MDL principle.
Reference: [Flach 90] <author> Peter A. Flach: </author> <title> Inductive Characterisation of Database Relations. </title> <booktitle> In: Methodologies for Intelligent Systems, </booktitle> <volume> 5, </volume> <editor> Z.W. Ras, M. Zemankova and M.L. Emrich (eds.), </editor> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1990. </year>
Reference-contexts: We also describe three different kinds of search using the new measure. Preliminary empirical results in a few boolean domains are favorable. This chapter is based on joint work reported in [Pfahringer & Kramer 95]. Recently, researchers in KDD paid much attention to the search for functional dependencies <ref> [Flach 90, Savnik & Flach 93, Mannila & Raiha 94] </ref> and association rules [Agrawal et al. 93, Agrawal & Srikant 94, Mannila et al. 94]. Functional dependencies are essentially relational and do not allow for the possibility of 45 5.2. PARTIAL DETERMINATIONS 46 exceptions.
Reference: [Flach 93] <author> Peter A. Flach: </author> <title> Predicate Invention in Inductive Data Engineering. </title> <booktitle> Proceedings of the 1 st European Conference on Machine Learning, </booktitle> <year> 1993. </year> <note> BIBLIOGRAPHY 90 </note>
Reference: [Forsyth 93] <author> Forsyth R.S.: </author> <title> Overfitting Revisited: An Information-Theoretic Approach to Simplifying Discrimination Trees, </title> <booktitle> in JETAI 6(3), </booktitle> <year> 1994. </year>
Reference-contexts: Now the problem for practical machine learning consists of finding the appropriate coding schemes for the particular kinds of models induced, be it decision trees, propositional rules, Prolog programs, or neural networks. In the following chapters we will: * Develop a generalization of a known decision tree coding schema <ref> [Forsyth 93] </ref> to be usable for the induction of propositional rule-sets. * Adapt the schema to the problem of discretizing continuous attributes. * Adapt the schema for selecting subsets of relevant attributes. * Adapt the schema for finding partial determinations in databases. * Adapt the schema for evaluating new features created <p> AN ALTERNATIVE MDL FORMULA 9 are needed for encoding the theory and proofs, which in turn causes theories to produce no compression for small training sets. An additional discussion of this special coding schema can be found in section 7.2.1. 2.3 An alternative MDL formula <ref> [Forsyth 93] </ref> introduces a well-performing formula for encoding decision trees: cost (tree) = cost (leaf i ) cost (leaf i ) = d i + n i fl entropy (leaf i ) where d i is the depth of the leaf in the tree, n i is the number of examples <p> It may also account for the better results reported below when applying our system to some of the domains used by <ref> [Forsyth 93] </ref> (besides the principle advantages of learning rules instead of trees). So what are the properties of the new coding schema with respect to the problems enumerated in section 2.2: 1. <p> We always report the accuracy of both Knopf and C4.5rules [Quinlan 93] and sometimes quote additional results from the literature. 2.5.1 Various Boolean Concepts The following three data sets have been used by <ref> [Forsyth 93] </ref> for comparing his Treemin program with various other ways of pruning. We reproduce his de 2.5. EMPIRICAL RESULTS 13 scription of the sets here: * RAND. <p> In chapter 6 we show how constructive induction can help the learning process in cases of an inadequate initial representation language. In summary, the new MDL measure proposed in this chapter is a generalization of the formula given in <ref> [Forsyth 93] </ref> applicable to sets of rules, it overcomes deficiences of the formula used in C4.5, and it is simpler (and may also be more reliable for small training sets) than the coding scheme used by [Muggleton et al. 92].
Reference: [Furnkranz 93] <author> Furnkranz J.: </author> <title> A numerical analysis of the KRK domain. </title> <note> Working Note, 1993. Available upon request. </note>
Reference-contexts: Though the differences might not appear to be large, one has to keep in mind the following: a correct and complete theory consists of at least 8 rules, whereas good approximate theories with 4 rules using a total of only 6 tests already yield more than 98% accuracy <ref> [Furnkranz 93] </ref>. Knopf almost always induces some of these approximations, but is also able to construct a complete and correct theory for all ten runs given 10000 examples and misses such a theory in only 1 out of 10 runs given 5000 examples (at noise level 0). <p> Section 6.3 describes the simplified Kramer operator. Experiments in two artificial domains the Monk's Problems [Thrun et al. 91] and illegal king-rook-king chess positions <ref> [Furnkranz 93] </ref> are summarized in section 6.4. <p> As such it has been used intensively in inductive logic programming. There are a few hundred thousand different possible examples. <ref> [Furnkranz 93] </ref> is a theoretical study including various approximate theories and showing a test-set size of 5000 to be sufficient for estimating accuracies of induced theories. KRK is very easily represented for CiPF. <p> Consequences of this transformation are further explained in appendix A. Induced theories (with no noise present) usually resemble the approximate theories given in <ref> [Furnkranz 93] </ref>. A sample theory derived by CiPF from 100 training examples is given in figure 6.1. This approximate theory was tested with 5000 test examples yielding an accuracy of 98.4%. This is consistent with [Furnkranz 93] which proves a theory consisting of the first three clauses 1,2,3 to be 98.451% <p> Induced theories (with no noise present) usually resemble the approximate theories given in <ref> [Furnkranz 93] </ref>. A sample theory derived by CiPF from 100 training examples is given in figure 6.1. This approximate theory was tested with 5000 test examples yielding an accuracy of 98.4%. This is consistent with [Furnkranz 93] which proves a theory consisting of the first three clauses 1,2,3 to be 98.451% correct.
Reference: [Furnkranz 94] <author> Furnkranz J.: </author> <title> Efficient Pruning Methods for Relational Learning, </title> <type> TU Wien, Dissertation, </type> <year> 1994. </year>
Reference-contexts: Even complete and consistent theories show varying degrees of generality and complexity. A common machine learning approach is to (1) restrict the set of possible models inducible at all and to (2) use various forms of pruning <ref> [Furnkranz 94] </ref> to construct a theory with hopefully good predictive accuracy. Alternatively complexity-based induction theory [Solomonoff 64] motivates a fresh interpretation of Occam's Razor based on the idea of data compression. On this basis [Rissanen 78] formulates the so-called minimum description length (MDL) principle.
Reference: [Furnkranz & Widmer 94] <author> Furnkranz J., Widmer G.: </author> <title> Incremental Reduced Error Pruning. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning (ML-94), </booktitle> <address> New Brunswick, N.J., </address> <year> 1994. </year>
Reference-contexts: The second strategy just 2.5. EMPIRICAL RESULTS 12 maximizes the difference p n of positive and negative examples covered by the rule. This strategy for single rule induction is inspired by the IREP-algorithm <ref> [Furnkranz & Widmer 94] </ref>. One difference is that we always use all of the available data both for constructing and pruning single rules no splitting into separate training and pruning sets ever takes place. <p> Results reported here are also better than those given in <ref> [Furnkranz & Widmer 94] </ref> for the case of 10% noise. Generally C4.5 again induces significantly more rules in all cases, thus producing overly complex theories. <p> Also, the strategy includes no user-settable parameters, and it does not require a secondary training set (train-test set ) to evaluate the quality of constructed attributes, like e.g. AQ17-MCI or various forms of reduced error pruning <ref> [Furnkranz & Widmer 94] </ref> do. 6.3 The Simplified Kramer Operator In [Kramer 94] a new, general constructive induction operator is introduced, which essentially abstracts the extensional product of the set of possible values of two given attributes to a new boolean attribute. <p> Using only 100 training examples at a noise level of 10%, CiPF 2.0 significantly outperforms all approaches compared in <ref> [Furnkranz & Widmer 94] </ref>. When using 250 training examples, it performs slightly worse than the best approach (incremental reduced error pruning - IREP) cited in [Furnkranz & Widmer 94]. For 500 training example CiPF 2.0 in turn outperforms IREP by an even smaller margin: 98.48% vs. 98.9%. <p> Using only 100 training examples at a noise level of 10%, CiPF 2.0 significantly outperforms all approaches compared in <ref> [Furnkranz & Widmer 94] </ref>. When using 250 training examples, it performs slightly worse than the best approach (incremental reduced error pruning - IREP) cited in [Furnkranz & Widmer 94]. For 500 training example CiPF 2.0 in turn outperforms IREP by an even smaller margin: 98.48% vs. 98.9%. These small differences for training set sizes of 250 or 500 examples may not be statistically significant, though.
Reference: [Gao & Li 89] <author> Gao Q., Li M.: </author> <title> The Minimum Description Length Principle and Its Application to Online Learning of Handprinted Characters, </title> <booktitle> in Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (IJCAI-89), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, 843-848, </address> <year> 1989. </year>
Reference-contexts: VARIOUS APPLICATIONS 79 7.3 Various Applications This section will discuss the use of the MDL principle in diverse applications including character recognition, the discovery of patterns in frame knowledge bases, the discovery of partial equations, and the discovery of sub-structures in graph-based knowledge bases. 7.3.1 Gao & Li <ref> [Gao & Li 89] </ref> apply the MDL principle to the task of recognizing hand-written characters. They have implemented a learning system dealing with both English and (several thousands) Chinese characters.
Reference: [Guha & Lenat 91] <author> Guha R.V., </author> <title> Lenat D.B.: CYC: A Mid-Term Approach, </title> <journal> Applied Artificial Intelligence, </journal> <pages> 5(1)45-86, </pages> <year> 1991. </year>
Reference-contexts: Note that the error coding used here is global. Reported experimental results confirm the applicability and usefulness of this simple coding schema. 7.3.2 Derthick [Derthick 91] uses the MDL principle for discovering orthogonal features on relational data. This work was done in the context of the CYC project <ref> [Guha & Lenat 91] </ref>. Therefore it is not surprising that all data used is represented by binary relations. CYC (back then) used a frame representation language and binary relations fit the Frame.Slot.Value paradigm perfectly.
Reference: [Huffman 52] <author> Huffman D.A.: </author> <title> A Method for the Construction of Minimum-Redundancy Codes, </title> <booktitle> Proc. </booktitle> <institution> Inst. Electr. Radio Eng. </institution> <address> 40(9), pp.1098-1101, </address> <year> 1952. </year>
Reference: [John et al. 94] <author> John G.H., Kohavi R., Pfleger K.: </author> <title> Irrelevant Features and the Subset Selection Problem, </title> <editor> in Cohen W.W. and Hirsh H.(eds.), </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference (ML94), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1994. </year>
Reference-contexts: As especially predictive accuracy cannot reliably be estimated from the complete set of training examples, usually some kind of cross-validation has to be used <ref> [John et al. 94, Kohavi 95, Caruana & Freitag 94] </ref>. In this chapter we describe the application of the Minimum Description Length (MDL) principle as an objective function for judging goodness of a feature subset. Feature subsets are used to construct so called simple decision tables [Kohavi 95]. <p> As these latter methods perform bidirectional hill-climbing and as they are equipped with a mechanism for preventing cycles, they can theoretically explore all possible subsets (given enough time). [Skalak 94] successfully applies random mutation hill-climbing . <ref> [John et al. 94, Kohavi 95] </ref> make good use of best-first search for all experiments they report. <p> Alternative methods for judging attribute relevance are e.g. FOCUS [Almuallim & Dietterich 91] and RELIEF [Kira & Rendell 92]. Their respective shortcomings are detailed in <ref> [John et al. 94] </ref>. RELIEF is only able to delete irrelevant attributes, but it cannot delete redundant attributes, whereas FOCUS might be trapped into selecting a single attribute having a distinct value for every single training example, which may result in poor predictive accuracy.
Reference: [Kerber 92] <author> Kerber R.: ChiMerge: </author> <title> Discretization of Numeric Attributes, </title> <booktitle> in Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> Menlo Park, pp.123-128, </address> <year> 1992. </year>
Reference-contexts: This restricted local analysis, which is motivated by efficiency reasons, sometimes misses the opportunity of forming larger uniform intervals. Consequently these algorithms may produce superfluous intervals, which may have a detrimental influence on both efficiency and accuracy. 2. They also cannot distinguish truly uncorrelated (irrelevant) attributes from what <ref> [Kerber 92] </ref> calls second-order correlated attributes, i.e. attributes correlating only in the presence of some other condition. An alternative approach to discretization is described in [Fayyad & Irani 93]. <p> This is of course not a pre-processing method, and will therefore not improve induction runtimes. Quite on the contrary, it may increase total runtimes considerably. On the other hand, predictive accuracy improves on average. 1 See <ref> [Kerber 92] </ref> or [Lee & Shin 94] for both convincing examples and some comparisons of class-blind methods to more sophisticated methods for discretization. 3.2. AN MDL MEASURE FOR DISCRETIZATIONS 20 Therefore in this chapter we will introduce a new global information-theoretical measure for judging discretizations as a whole.
Reference: [Klemettinen et al. 94] <author> Mika Klemettinen, Heikki Mannila, Pirjo Ronkainen, Hannu Toivonen, and A. Inkeri Verkamo: </author> <title> Finding Interesting Rules from Large Sets of Discovered Association Rules. </title> <booktitle> Proceedings of the Third International Conference on Information and Knowledge Management, </booktitle> <publisher> ACM Press, </publisher> <year> 1994. </year>
Reference: [Kira & Rendell 92] <author> Kira K., Rendell L.A.: </author> <title> A Practical Approach to Feature Selection, </title> <editor> in Sleeman D. and Edwards P.(eds.), </editor> <booktitle> Machine Learning: Proceedings of the Ninth International Workshop (ML92), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, pp.249-256, </address> <year> 1992. </year>
Reference-contexts: It seems to be a more efficient alternative to standard cross-validation, which optimizes solely predictive accuracy for selecting good subsets of features in inductive learning. Alternative methods for judging attribute relevance are e.g. FOCUS [Almuallim & Dietterich 91] and RELIEF <ref> [Kira & Rendell 92] </ref>. Their respective shortcomings are detailed in [John et al. 94].
Reference: [Kohavi 95] <author> Kohavi R.: </author> <title> The Power of Decision Tables, </title> <note> Paper, submitted. (also available electronically as ftp://starry.stanford.edu/pub/ronnyk/tables.ps) BIBLIOGRAPHY 91 </note>
Reference-contexts: As especially predictive accuracy cannot reliably be estimated from the complete set of training examples, usually some kind of cross-validation has to be used <ref> [John et al. 94, Kohavi 95, Caruana & Freitag 94] </ref>. In this chapter we describe the application of the Minimum Description Length (MDL) principle as an objective function for judging goodness of a feature subset. Feature subsets are used to construct so called simple decision tables [Kohavi 95]. <p> In this chapter we describe the application of the Minimum Description Length (MDL) principle as an objective function for judging goodness of a feature subset. Feature subsets are used to construct so called simple decision tables <ref> [Kohavi 95] </ref>. The MDL principle takes into account both the simplicity and the accuracy of the 32 4.2. SIMPLE DECISION TABLES 33 simple decision tables induced by particular feature subsets. All of the training data is used at once, thus eliminating the need for costly cross-validation runs. <p> In section 4.3 we will describe a new MDL coding schema for simple decision tables. Experimental considerations and setup, and empirical results using this new coding schema are reported in section 4.4. Section 4.5 discusses open problems and further research directions. 4.2 Simple Decision Tables <ref> [Kohavi 95] </ref> describes simple decision tables and their usage for classification as follows: * For any feature subset construct a decision table by simply projecting all given training examples on the feature subset. * For all after projection identical examples count class frequencies and assign the majority class to every entry. <p> If there is an entry found, return the appropriate majority class of that entry as the classification result, else return the global majority class. Tables 4.1 and 4.2 exemplify simple decision tables, or DTM (for Decision Table Majority) as <ref> [Kohavi 95] </ref> calls them. Table 4.1 lists some training examples for an arbitrary 2-class learning task involving 4 boolean attributes. Table 4.2 depicts the respective decision table for the feature subset fA 1 ; A 2 g. <p> As these latter methods perform bidirectional hill-climbing and as they are equipped with a mechanism for preventing cycles, they can theoretically explore all possible subsets (given enough time). [Skalak 94] successfully applies random mutation hill-climbing . <ref> [John et al. 94, Kohavi 95] </ref> make good use of best-first search for all experiments they report. <p> Incremental k-fold cross-validation is described in <ref> [Kohavi 95] </ref>. If it is possible to incrementally add examples to and delete examples from the data-structures maintained by the learning algorithm, cross-validation can be done incrementally, which saves a lot of computational work.
Reference: [Kolodner 93] <author> Kolodner J.: </author> <title> Case-Based Reasoning, </title> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <year> 1993. </year>
Reference-contexts: This would give such a system a kind of case-based reasoning flavor <ref> [Kolodner 93] </ref>. * Compression-based feature subset selection could also be used as an input filter for constructive induction [Bloedorn et al. 93, Pfahringer 94b], which can help the learning process in cases of an inadequate initial representation language.
Reference: [Kovacic 94] <author> Kovacic M.: </author> <title> MDL-Heuristics in ILP Revisited, ML/COLT94 Workshop on Applications of Descriptional Complexity to Inductive, Statistical, and Visual Inference, </title> <year> 1994. </year>
Reference-contexts: So this case probably tells us that even rather inefficient coding schemas can yield acceptable theories, as long as both the theory and the examples are encoded somehow. 7.2.2 Kovacic The deficiencies of [Muggleton et al. 92] are the starting point for the work reported in <ref> [Kovacic 94] </ref>. He also identifies the proof complexity measure as the culprit for bad generalization performance given small (100 examples) noisy training-sets. As a remedy he proposes to use [Quinlan 93]'s schema for exception coding instead. <p> While doing the experiments reported in chapter 2, we too have tried using the idea of controlled overfitting. But typically the predictive accuracy of the most accurate and still compressive theory was equal or worse than the predictive accuracy of the most compressive theory. Unfortunately <ref> [Kovacic 94] </ref> does not give samples of induced theories, therefore it is hard to investigate these empirical differences. Probably the C4.5 exception coding schema is too coarse-grained and thus over-estimates compressiveness of small theories.
Reference: [Koza 92] <editor> Koza J.R.: </editor> <booktitle> Genetic Programming, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference-contexts: Even more, the evaluation of pre-specified and possibly even automatically derived background knowledge is a demanding open problem. An appropriate MDL-based evaluation function might also serve well for ranking competing theories, thus building a foundation for what could in analogy to Genetic Programming <ref> [Koza 92] </ref> be called genetic inductive logic programming. 2. Neural Networks: choosing the appropriate network architecture for a given learning task is still a kind of magic. An adequate MDL-based measure encoding neural networks would allow for a principled comparison of different network architectures and sizes for a given task.
Reference: [Kramer 94] <author> Kramer S.: CN2-MCI: </author> <title> A Two-Step Method for Constructive Induction, </title> <type> OeFAI Tech Report TR-94-13, </type> <institution> Vienna, </institution> <year> 1994. </year>
Reference-contexts: Also, the strategy includes no user-settable parameters, and it does not require a secondary training set (train-test set ) to evaluate the quality of constructed attributes, like e.g. AQ17-MCI or various forms of reduced error pruning [Furnkranz & Widmer 94] do. 6.3 The Simplified Kramer Operator In <ref> [Kramer 94] </ref> a new, general constructive induction operator is introduced, which essentially abstracts the extensional product of the set of possible values of two given attributes to a new boolean attribute. <p> We take care of immediately rejecting trivial constructions like tautologies or projections of one of the arguments. CiPF 2.0 introduces the following simplifications. In <ref> [Kramer 94] </ref> a heuristic chooses a few good rules and from these rules a few pairs of co-occurring attributes 6.4. EXPERIMENTS 64 are taken as input for an involved A*-search for the best split according to another heuristic estimating split values.
Reference: [Langley et al. 87] <author> Langley P., Simon H.A., Bradshaw G.L., Zytkow J.M.: </author> <title> Scientific Discovery: Computational Explorations of the Creative Processes, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: KDD is, contrary to classical supervised concept learning, concerned with the discovery of interesting dependencies in the data of a given database [Piatetsky-Shapiro 91]. Examples for dependencies are so-called association rules and functional dependencies. KDD can be interpreted as an application of Learning by Discovery <ref> [Langley et al. 87] </ref> to practical databases. Partial determinations can be viewed as generalizations of both functional dependencies and association rules, in that they are relational in nature and may have exceptions.
Reference: [Lavrac et al. 92] <author> Lavrac N., Cestnik B., Dzeroski S.: </author> <title> Search heuristics in empirical Inductive Logic Programming, </title> <booktitle> in Workshop W18, Logical Approaches to Machine Learning, ECAI-92, </booktitle> <address> Vienna, </address> <year> 1992 </year>
Reference-contexts: Different rule sets are induced by combining various standard search heuristics for single rule induction with two different forms of pruning as discussed below. Single rules are constructed top-down by using standard search heuristics <ref> [Lavrac et al. 92] </ref>: info or accuracy gain, weighted or not, approximating probabilities using either relative frequencies, the Laplace estimate or the m-estimate (for different values of m, currently 0:5; 1; 2; 4).
Reference: [Lee & Shin 94] <author> Lee C., Shin D.-G.: </author> <title> A Context-Sensitive Discretization of Numeric Attributes for Classification Learning, </title> <booktitle> in Cohn A.G.(ed.), Proceedings of the 11th European Conference on Artificial Intelligence (ECAI94), </booktitle> <publisher> John Wiley & Sons, </publisher> <address> Chichester, pp.428-432, </address> <year> 1994. </year>
Reference-contexts: This is of course not a pre-processing method, and will therefore not improve induction runtimes. Quite on the contrary, it may increase total runtimes considerably. On the other hand, predictive accuracy improves on average. 1 See [Kerber 92] or <ref> [Lee & Shin 94] </ref> for both convincing examples and some comparisons of class-blind methods to more sophisticated methods for discretization. 3.2. AN MDL MEASURE FOR DISCRETIZATIONS 20 Therefore in this chapter we will introduce a new global information-theoretical measure for judging discretizations as a whole.
Reference: [Mannilla et al. 93] <author> Heikki Mannila, Hannu Toivonen, and A. Inkeri Verkamo: </author> <title> Improved Methods for Finding Association Rules. </title> <type> Technical Report C-1993-65, </type> <institution> University Helsinki, Computer Science Department, </institution> <year> 1993. </year>
Reference: [Mannila et al. 94] <author> Heikki Mannila, Hannu Toivonen, and A. Inkeri Verkamo: </author> <title> Efficient Algorithms for Discovering Association Rules. </title> <booktitle> Proceedings of the AAAI-94 Workshop on Knowledge Discovery in Databases, </booktitle> <year> 1994. </year>
Reference: [Mannila & Raiha 94] <author> Heikki Mannila and Kari-Jouko Raiha: </author> <title> Algorithms for Inferring Functional Dependencies From Relations. </title> <booktitle> Data & Knowledge Engineering 12 (1994) 83-99, </booktitle> <year> 1994. </year>
Reference-contexts: We also describe three different kinds of search using the new measure. Preliminary empirical results in a few boolean domains are favorable. This chapter is based on joint work reported in [Pfahringer & Kramer 95]. Recently, researchers in KDD paid much attention to the search for functional dependencies <ref> [Flach 90, Savnik & Flach 93, Mannila & Raiha 94] </ref> and association rules [Agrawal et al. 93, Agrawal & Srikant 94, Mannila et al. 94]. Functional dependencies are essentially relational and do not allow for the possibility of 45 5.2. PARTIAL DETERMINATIONS 46 exceptions.
Reference: [Matheus & Rendell 89] <author> Matheus C.J., Rendell L.A.: </author> <title> Constructive Induction On Decision Trees, </title> <booktitle> in Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (IJCAI-89), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, 645-650, </address> <year> 1989. </year>
Reference-contexts: CI operators in CiPF 2.0: * Compare attributes of the same type: is attribute A1 Equal to/Different from attribute A2. * Conjoin possible values of nominal attributes occurring in good rules into a subset of useful values for the respective attribute. * Conjoin two attributes occurring in a good rule <ref> [Matheus & Rendell 89] </ref>. We use the simplified Kramer operator for this task as will be described in Section 3. 6.2.
Reference: [Mehra et al. 89] <author> Mehra P., Rendell L.A., Wah B.W.: </author> <title> Principled Constructive Induction, </title> <booktitle> in Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (IJCAI-89), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, 651-656, </address> <year> 1989. </year> <note> BIBLIOGRAPHY 92 </note>
Reference-contexts: For example, decision trees encode axis-parallel nested hyper-rectangles. Two different problems may cause irregular distributions of learning examples in the original representation space: noise and/or an inadequate description language. As a remedy for the latter problem constructive induction has been introduced, e.g. in [Dietterich & Michalski 81] and <ref> [Mehra et al. 89] </ref>. The basic idea is to somehow transform the original representation space into a space where the learning examples exhibit (more) regularities. Usually this is done by introducing new attributes and forgetting old ones.
Reference: [Michalski 83] <author> Ryszard S. Michalski: </author> <title> A Theory and Methodology of Inductive Learning. </title> <booktitle> In: Machine Learning: An Artificial Intelligence Approach. </booktitle> <publisher> Tioga Publishing Company, </publisher> <address> Palo Alto, CA, </address> <year> 1983. </year>
Reference-contexts: Alternatively, hierarchies could be utilized by operators during the search for dependencies. For instance, 'climbing the hierarchy'-operator of <ref> [Michalski 83] </ref> could be adapted: The operator could restrict or extend the domain of an attribute at the left-hand side of a partial determination. However, the next step will be to perform experiments with large databases from the real world.
Reference: [Muggleton et al. 92] <author> Muggleton S., Srinivasan A., Bain M.: </author> <title> Compression, Significance, and Accuracy, </title> <editor> in Sleeman D. and Edwards P.(eds.), </editor> <booktitle> Machine Learning: Proceedings of the Ninth International Workshop (ML92), </booktitle> <publisher> Mor-gan Kaufmann, </publisher> <address> San Mateo, CA, pp.338-347, </address> <year> 1992. </year>
Reference-contexts: The Minimum Description Length (MDL) principle [Rissanen 78], sometimes also called the Minimum Message Length (MML) principle, has been used successfully for e.g. inducing decision trees [Quinlan & Rivest 89, Quinlan 93, Wallace & Patrick 93, Forsyth 93], or in ILP <ref> [Muggleton et al. 92] </ref>. But recently also some problems with the particular MDL measures used so far were discovered [Quinlan 94]. We will try to explain the origin of these problems in section 2.2. <p> This explains problem 2 and also partly problem 3. In the following section we will introduce a different way for estimating L (EjT ) which yields better empirical results. Alternatively, <ref> [Muggleton et al. 92] </ref> describe a (rather complicated) scheme for encoding theories and data in an ILP setting in a way such that both the theory and a single proof for every example are encoded. <p> MDL measure proposed in this chapter is a generalization of the formula given in [Forsyth 93] applicable to sets of rules, it overcomes deficiences of the formula used in C4.5, and it is simpler (and may also be more reliable for small training sets) than the coding scheme used by <ref> [Muggleton et al. 92] </ref>. Chapter 3 Discretization of Continuous Attributes 3.1 Introduction Discretization of continuous attributes is concerned with transformation of continuous information into some ordered nominal representation. <p> More closely related work was already discussed in the previous chapters at the appropriate places. 7.2 ILP Usages of MDL In this section we will shortly discuss three different attempts of incorporating the MDL principle into inductive logic programming. 7.2.1 Muggleton In <ref> [Muggleton et al. 92] </ref> a schema is described for encoding both logical programs and proofs of training examples. It is claimed that (p.341) by encoding proofs, the model incorporates aspects of time-complexity in the same units (bits) as the program description. <p> So this case probably tells us that even rather inefficient coding schemas can yield acceptable theories, as long as both the theory and the examples are encoded somehow. 7.2.2 Kovacic The deficiencies of <ref> [Muggleton et al. 92] </ref> are the starting point for the work reported in [Kovacic 94]. He also identifies the proof complexity measure as the culprit for bad generalization performance given small (100 examples) noisy training-sets. As a remedy he proposes to use [Quinlan 93]'s schema for exception coding instead. <p> They are especially concerned with learning situations 7.2. ILP USAGES OF MDL 78 where negative examples of concepts are either scarce or not available at all. They define model complexity as a measure for the coding size of the data given a logical model. Contrary to <ref> [Muggleton et al. 92] </ref>'s proof complexity measure this model complexity is concerned with the size of the least Herbrand model of a logic program T. <p> Each field of Machine Learning not touched in the previous chapters offers opportunities for potential application of the MDL principle. The most prominent fields are: 1. Inductive Logic Programming: in chapter 7 we have mentioned [Conklin & Witten 94] and <ref> [Muggleton et al. 92] </ref>; neither is able to give really satisfactory coding schemas. Even more, the evaluation of pre-specified and possibly even automatically derived background knowledge is a demanding open problem.
Reference: [Murphy & Aha 94] <author> Murphy P.M., Aha D.W.: </author> <title> UCI repository of machine learning databases. For information contact ml-repository@ics.uci.edu. </title>
Reference-contexts: The main test-bed is a simple artificial domain introduced by [Catlett 91]. Its artificial nature allows for extensive experiments with various sizes of training sets and levels of class noise. Additionally we also performed some experiments with so-called natural domains mostly available from the UC Irvine Machine Learning Repository <ref> [Murphy & Aha 94] </ref>. 4 These experiments also support our claims, but to a lesser degree, as these databases tend to be rather small. 3.4.1 An Artificial Domain [Catlett 91] introduces the artificial Demon domain as a kind of worst case scenario. <p> Monk2loc is an interesting alternative representation for the Monk2 problem translating the 6 nominal attributes into 17 (partially redundant) boolean attributes. * Mushroom: This is a large example database taken from the UCI repository of machine learning databases <ref> [Murphy & Aha 94] </ref>. 8124 different examples are described by 22 nominal attributes having up to 12 different possible values. Training sets of size 1000 are drawn randomly, all 8124 examples are used for testing. * Parity 5/27: The target concept is the parity of 5 bits.
Reference: [Natarajan 91] <author> Natarajan B.K.: </author> <title> Machine Learning ATheoretical Approach, </title> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <year> 1991. </year>
Reference-contexts: William Ockham (1285-1349), an english philospher, is credited with this principle, which has become known as Occam's Razor. Literally it means "entities should not be multiplied unnecessarily", but commonly it is interpreted to mean "the simplest explanation of the observed phenomena is most likely to be the correct one" <ref> [Natarajan 91] </ref>. Theory simplicity certainly is a very intuitive notion, but it must be carefully interpreted if it is to yield a useful preference criterion (or a bias) for machine learning.
Reference: [Pagallo & Haussler 90] <author> Pagallo G., Haussler D.: </author> <title> Boolean Feature Discovery in Empirical Learning, </title> <journal> Machine Learning, </journal> <volume> 5(1), pp.71-99, </volume> <year> 1990. </year>
Reference-contexts: This does not seem to hurt in terms of classification accuracy, though. Treemin induced trees with 7 leaves, which would correspond to a set of seven rules. Two other artificial boolean functions, DNF3 and DNF4, introduced in <ref> [Pagallo & Haussler 90] </ref> turned out to be a hard problem for decision tree learning even though training set sizes are large - 1650 and 2650 respectively. <p> The 27 additional boolean attributes are random (and therefore irrelevant). The training set size is 4000 each, and 2000 examples each are used for testing. This particular domain was first used by <ref> [Pagallo & Haussler 90] </ref> and is especially difficult to learn for decision tree algorithms. This should not be surprising, 4.4. EXPERIMENTS AND EMPIRICAL RESULTS 40 as only relatively few subsets of the large number of all possible subsets (2 32 ) yield a classification performance significantly better than random guessing. <p> EXPERIMENTS 70 Noise% First Best 0 63.37 99.51 10 50.05 50.05 Table 6.4: PARITY-5: accuracies (percentages) for CiPF 2.0 after the first and after the best cycle of induction for various levels of noise. 6.4.3 PARITY-5 and other Boolean Problems PARITY-5 is one of the boolean functions used in <ref> [Pagallo & Haussler 90] </ref> to compare several learning methods. These functions are also used in [Wnek & Michalski 94] to test AQ17-HCI, where one can find more detailed descriptions, too. <p> The inherent problems encountered when trying to learn parity concepts is investigated in [Thornton 95]. We have also experimented with the other three boolean functions used in <ref> [Pagallo & Haussler 90] </ref>. the MUX-11, DNF-3, and DNF-4. Both MUX-11 and DNF-3 show a more graceful degradation of classification performance in the presence of noise. On the other hand the improvements due to constructive induction are not as spectacular as for PARITY-5.
Reference: [Pednault 91] <author> Edwin P.D. Pednault: </author> <title> Minimal Length Encoding and Inductive Inference. In: Knowledge Discovery in Databases, </title> <editor> G. Piatetsky-Shapiro and W.J. Frawley (eds.), </editor> <publisher> AAAI Press, </publisher> <year> 1991. </year>
Reference-contexts: It is however not clear how complete rule-sets could be judged by this J-measure. But this is not a concern for the authors, because they just want to search for the N best-ranked size-bounded rules inducible from a given database. 7.4. THEORETICAL WORK 83 7.4.2 Pednault <ref> [Pednault 91] </ref> is concerned with surface reconstruction by fitting polynomials to possibly noisy data. But what is much more important is the very useful analysis of the relationship between the MDL principle and Bayesian inference given in this paper.
Reference: [Pfahringer 94a] <author> Pfahringer B.: </author> <title> Controlling Constructive Induction in CiPF: An MDL Approach, </title> <booktitle> in Proceedings of the European Conference on Machine Learning (ECML94), </booktitle> <year> 1994. </year>
Reference-contexts: This phenomenon can be called overfitting the representation language in analogy to fitting the noise. The presence of both noise and an inadequate language obviously increases the possibilities for overfitting even further. A preliminary version of CiPF was described in <ref> [Pfahringer 94a] </ref> The system CiPF 2.0 described in this chapter has been considerably improved. These modifications allow for robust constructive induction handling both noise and an inadequate language. This chapter is based partly on [Pfahringer 94b].
Reference: [Pfahringer 94b] <author> Pfahringer B.: CiPF 2.0: </author> <title> A Robust Constructive Induction System, </title> <booktitle> Proceedings of the Workshop on Constructive Induction and Change of Representation, 11th International Conference on Machine Learning (ML-94/COLT-94), </booktitle> <address> New Brunswick, New Jersey., </address> <year> 1994. </year>
Reference-contexts: That would make it applicable to all kinds of propositional as well as first-order learning problems. * Furthermore the cost of additional attributes induced by means of constructive induction <ref> [Bloedorn et al. 93, Pfahringer 94b] </ref> must be accounted for. In chapter 6 we show how constructive induction can help the learning process in cases of an inadequate initial representation language. <p> This would give such a system a kind of case-based reasoning flavor [Kolodner 93]. * Compression-based feature subset selection could also be used as an input filter for constructive induction <ref> [Bloedorn et al. 93, Pfahringer 94b] </ref>, which can help the learning process in cases of an inadequate initial representation language. <p> A preliminary version of CiPF was described in [Pfahringer 94a] The system CiPF 2.0 described in this chapter has been considerably improved. These modifications allow for robust constructive induction handling both noise and an inadequate language. This chapter is based partly on <ref> [Pfahringer 94b] </ref>. Section 6.2 briefly describes a generic architecture for constructive induction and discusses CiPF 2.0 in these terms. Especially we will focus on how the problem of controlling search for useful changes of representation is solved in CiPF 2.0 by means of the MDL principle.
Reference: [Pfahringer 94c] <author> Pfahringer B.: </author> <title> Robust Constructive Induction, </title> <booktitle> in Proceedings der 18.Deutsche Jahrestagung fuer Kuenstliche Intelligenz (KI-94), </booktitle> <address> Saar-bruecken, Germany., </address> <year> 1994. </year>
Reference: [Pfahringer 95a] <author> Pfahringer B.: </author> <title> A New MDL Measure for Robust Rule Induction (Extended Abstract), </title> <booktitle> 8th European Conference on Machine Learning (ECML95), </booktitle> <year> 1995. </year>
Reference-contexts: We report on favorable results in various purely symbolic propositional domains. Both rule quality in terms of simplicity (and syntactic closeness to the respective underlying theory where known) and predictive accuracy of induced theories are convincing. This chapter is based on work reported in <ref> [Pfahringer 95a] </ref>. The Minimum Description Length (MDL) principle [Rissanen 78], sometimes also called the Minimum Message Length (MML) principle, has been used successfully for e.g. inducing decision trees [Quinlan & Rivest 89, Quinlan 93, Wallace & Patrick 93, Forsyth 93], or in ILP [Muggleton et al. 92].
Reference: [Pfahringer 95b] <author> Pfahringer B.: </author> <title> Compression-Based Discretization of Continuous Attributes, </title> <booktitle> accepted for publication at the Twelfth International Conference on Machine Learning (ICML95), </booktitle> <publisher> in press. </publisher>
Reference-contexts: The new method solves some problems of alternative local measures used for discretization. Empirical results in a few natural domains and extensive experiments in an artificial domain show that MDL-Disc scales up well to large learning problems involving noise. This chapter is based in part on <ref> [Pfahringer 95b] </ref>. Discretization groups continuous numeric values into discrete intervals. Originally the motivation for discretization was the inability of handling continuous values exhibited by early propositional learning algorithms. Even though most propositional algorithms nowadays can handle continuous values, discretization can still be beneficial for various reasons: 1.
Reference: [Pfahringer 95c] <author> Pfahringer B.: </author> <title> Compression-based Feature Subset Selection, </title> <booktitle> submitted to the IJCAI Workshop on Data Engineering for Machine Learning, 14th International Conference on Artificial Intelligence (IJCAI95), </booktitle> <year> 1995. </year> <note> BIBLIOGRAPHY 93 </note>
Reference-contexts: Domains with both a large number of training examples and a large number of possible features yield the biggest gains in efficiency. Thus our new approach seems to scale up better to large learning problems than previous methods. This chapter is based on work reported in <ref> [Pfahringer 95c] </ref>. The problem of feature subset selection involves finding a good subset of all given features. Some of these attributes may be irrelevant or redundant.
Reference: [Pfahringer & Kramer 95] <author> Pfahringer B., Kramer S.: </author> <title> Compression-Based Evaluation of Partial Determinations, </title> <booktitle> submitted to the First International Conference on Knowledge Discovery and Data Mining (KDD-95), </booktitle> <year> 1995. </year>
Reference-contexts: We also describe three different kinds of search using the new measure. Preliminary empirical results in a few boolean domains are favorable. This chapter is based on joint work reported in <ref> [Pfahringer & Kramer 95] </ref>. Recently, researchers in KDD paid much attention to the search for functional dependencies [Flach 90, Savnik & Flach 93, Mannila & Raiha 94] and association rules [Agrawal et al. 93, Agrawal & Srikant 94, Mannila et al. 94].
Reference: [Piatetsky-Shapiro 91] <author> Gregory Piatetsky-Shapiro: </author> <title> Discovery, Analysis, and Presentation of Strong Rules. In: Knowledge Discovery in Databases, </title> <editor> G. Piatetsky-Shapiro and W.J. Frawley (eds.), </editor> <publisher> AAAI Press, </publisher> <year> 1991. </year>
Reference-contexts: Partial determinations are of particular interest to researchers in the field of knowledge discovery and data mining in databases (KDD). KDD is, contrary to classical supervised concept learning, concerned with the discovery of interesting dependencies in the data of a given database <ref> [Piatetsky-Shapiro 91] </ref>. Examples for dependencies are so-called association rules and functional dependencies. KDD can be interpreted as an application of Learning by Discovery [Langley et al. 87] to practical databases. <p> According to [Russell 89], we will call those non-strict functional dependencies partial determinations. Partial determinations can be viewed as generalizations of both functional dependencies and association rules, in that they are relational in nature and may have exceptions. Unfortunately, the measures developed by <ref> [Piatetsky-Shapiro 91] </ref> for association rules, support and confidence, are not suited to evaluate partial determinations. We therefore develop an alternative, compression-based measure that is useful and conforms to the intuition. <p> This bound2 also tells us that specializing a total determination (a determination already having zero exceptions) will not yield a better determination, as its respective cost will be strictly larger. 5.3.4 Rule-Interest Measures The function satisfies the three principles of rule-interest measures as stated by <ref> [Piatetsky-Shapiro 91] </ref>. The conditions were originally formulated for propositional rules and can easily be reformulated for partial determinations: 5.4. EMPIRICAL RESULTS 51 1. If the attributes at the left-hand side are statistically independent of the attributes at the right-hand side, the value of the rule-interest function shall be zero. 2.
Reference: [Quinlan 87] <author> Quinlan J.R.: </author> <title> Simplifying Decision Trees, </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27, </volume> <pages> pp. 221-234, </pages> <year> 1987. </year>
Reference-contexts: This artificial data set was designed to model a task where only probabilistic classification is possible and which also contains disjunctions. It is effectively the same as Quinlan's `Prob-Disj' data-set <ref> [Quinlan 87] </ref> and consists of ten random binary variables (v1 to v10).
Reference: [Quinlan & Rivest 89] <author> Quinlan J.R, Rivest R.L.: </author> <title> Inferring Decision Trees using the Minimum Description Length Principle, </title> <booktitle> in Information and Computation, </booktitle> <volume> 80 </volume> <pages> 227-248, </pages> <year> 1989. </year>
Reference-contexts: They give an MDL schema for coding the adjacency matrix of graph. Assuming that the graphs they are interested in are rather sparse, they devise a coding schema similar to that given for decision trees by <ref> [Quinlan & Rivest 89] </ref>. Additionally they also introduce three domain-independent heuristics for evaluating graphs. These heuristics are supposed to measure compactness, connectivity, and coverage of sub-graph structure. These additional heuristics make it difficult to assess the contribution of the MDL principle in this work. But reported results are very interesting.
Reference: [Quinlan & Cameron-Jones 93] <author> Quinlan J.R., Cameron-Jones R.M.: </author> <title> FOIL: A Midterm Report, in Brazdil P.B.(ed.), Machine Learning: </title> <publisher> ECML-93, Springer, </publisher> <address> Berlin, pp.3-20, </address> <year> 1993. </year>
Reference-contexts: Furthermore this formula still is symmetric with respect to negative theories. 2.4 Algorithmic Usage of the new Formula For empirical testing of the new formula we have implemented a kind of propositional Foil <ref> [Quinlan & Cameron-Jones 93] </ref> called Knopf. Right now Knopf is restricted to purely symbolic 2-class learning problems. It is completely free of user-settable parameters. <p> SUMMARY AND FURTHER RESEARCH 74 with structured objects, hopefully giving CiPF some of the representational power found in inductive logic programming systems like FOIL <ref> [Quinlan & Cameron-Jones 93] </ref>. The simplified Kramer operator will also be generalized to construct multi-valued attributes in addition to binary attributes. We experienced the necessity for such more diverse abstractions especially in the context of multiple-class learning problems.
Reference: [Quinlan 93] <author> Quinlan J.R.: C4.5: </author> <title> Programs for Machine Learning, </title> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: This way a more complex theory will need more bits to be encoded, but might save bits when encoding more data correctly. The precise MDL formula used by <ref> [Quinlan 93] </ref> for simplifying rule sets is: Cost = T heoryCost + log 2 F P + log 2 N C !! In this formula T heoryCost is an estimate for the number of bits needed to encode the theory (L (T )). <p> The class attribute was randomly reversed (from yes to no or vice versa) for N % of the training examples, when the noise level was set to N . We always report the accuracy of both Knopf and C4.5rules <ref> [Quinlan 93] </ref> and sometimes quote additional results from the literature. 2.5.1 Various Boolean Concepts The following three data sets have been used by [Forsyth 93] for comparing his Treemin program with various other ways of pruning. We reproduce his de 2.5. <p> On the other hand, if we limit D-2 to a fixed depth D, its complexity is only O (N ). 2 The original D-2 recursively splits the training examples using the info-gain heuristic of ID3 (or its successor C4.5 <ref> [Quinlan 93] </ref>) until one of a set of predefined stopping criteria is fulfilled. Our simplification replaces this set of criteria by a simple depth limit. For all experiments reported here we used D = 5 which leads to the selection of at most 31 split-points (2 5 1). <p> Special provisions (escape to a class-blind method) are made for the degenerate case of a resulting discretization of just a single interval. 3.4 Experiments and Empirical Results For empirical testing of MDL-Disc we have coupled it with C4.5 <ref> [Quinlan 93] </ref>, a well-known induction algorithm for decision trees. According to our aims for MDL-Disc (efficiency, accuracy, and intelligibility) we report for all experiments average C4.5 runtimes, average error rates on unseen examples, and the average 3.4. EXPERIMENTS AND EMPIRICAL RESULTS 24 number of nodes of the resulting decision trees. <p> EXPERIMENTS AND EMPIRICAL RESULTS 37 incremental k-fold cross-validation. For further comparisons C4.5 <ref> [Quinlan 93] </ref> was used on both the original complete sets of attributes and on the best subsets returned by both heuristics for feature subset selection. Best-first search is only one of a few possible search strategies. <p> by a good rule: compute subsets for the respective base-level attributes, so that these subsets exactly cover these positive examples. * Drop attributes not used by any of the good rules. 1 Recursive application of these operators may yield complex new attributes. 6.2.2 CiPF's Selective Learner CiPF 2.0 incorporates C4.5 <ref> [Quinlan 93] </ref> as its selective learner, a sophisticated decision tree algorithm well able to deal with noise. C4.5 also includes a rule-generator transforming decision trees into sets of production rules. These rules are then processed and analyzed by CiPF 2.0's module for constructive induction. <p> He also identifies the proof complexity measure as the culprit for bad generalization performance given small (100 examples) noisy training-sets. As a remedy he proposes to use <ref> [Quinlan 93] </ref>'s schema for exception coding instead. Furthermore he suggests not using the most compressive theory as the final induction result, but to allow for controlled overfitting: for small training-sets the most accurate but still compressive theory shall be used as the final induction result. <p> This will increase T 's coding size (see below). 3. E Q (T ). Theory T is complete, but covers a few additional atoms. In this case one must transmit information for decoding the false-positive atoms. False-positives are encoded using <ref> [Quinlan 93] </ref>'s method of encoding exceptions: L MC (EjT ) = log 2 jQ (T )j !! The theory itself is encoded straightforwardly. An unambiguous probabilistic context-free grammar for well-formed logic programs [Wetherell 80] is used to compute the coding length of a theory.
Reference: [Quinlan 94] <author> Quinlan J.R.: </author> <title> The Minimum Description Length Principle and Categorical Theories, </title> <editor> in Cohen W.W. and Hirsh H.(eds.), </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference (ML94), </booktitle> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, CA, </address> <year> 1994. </year>
Reference-contexts: But recently also some problems with the particular MDL measures used so far were discovered <ref> [Quinlan 94] </ref>. We will try to explain the origin of these problems in section 2.2. A new MDL measure applicable to propositional rule learning aimed at overcoming these problems will be introduced in section 2.3. <p> In summary this formula approximates the total cost in the number of bits for encoding a theory and its exceptions. So what are the problems with that formula? <ref> [Quinlan 94] </ref> states two and we would like to add an additional one: 2.2. MDL IN RULE LEARNING AND ITS PROBLEMS 7 1. <p> When learning from lots of examples in the presence of noise, there is still a tendency to fit the noise. This behavior is evident in empirical results and will be explained below. <ref> [Quinlan 94] </ref> tries to remedy complaints 1 and 2 by introducing an ad-hoc factor for penalizing theories that predict distributions too far off the distribution found in the training data. He also briefly investigates two alternatives for encoding exceptions. <p> A class noise level of means that for every training example the classification is reversed with probability . This corresponds to a class noise level of 2 for the model incorporated by <ref> [Quinlan 94] </ref>. Note that neither model preserves the original distribution of positive and negative examples in the undisturbed data (unless there are exactly 50% positive examples). 2 We can estimate: log 2 2k ~ 2log 2 n 2.3. <p> Unfortunately [Kovacic 94] does not give samples of induced theories, therefore it is hard to investigate these empirical differences. Probably the C4.5 exception coding schema is too coarse-grained and thus over-estimates compressiveness of small theories. Additionally this approach may also exhibit the same problems that are reported in <ref> [Quinlan 94] </ref> for C4.5. Whether it really does is hard to tell because of the ad-hoc nature of controlled overfitting. <p> are encoded like C4.5's false-positives! Due to the close similarities of this coding schema and the one used by C4.5 we conjecture that this approach will show the same deficiences (namely consistent over-generalization for minority classes and vice versa) for small, noisy, and unevenly distributed training sets as C4.5 does <ref> [Quinlan 94] </ref>. 7.3.
Reference: [Rao & Lu 92] <author> Rao R.B., Lu S.C.-Y.: </author> <title> Learning Engineering Models with the Minimum Description Length Principle, </title> <booktitle> in Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> Menlo Park, pp.717-722, </address> <year> 1992. </year>
Reference-contexts: In comparison, the results we report in chapter 4 on compression-based feature subset selection (a task that seems comparable to the task described in this paper) are comparable to if not better than results for standard feature subset selection methods. 7.3.3 Rao & Lu <ref> [Rao & Lu 92] </ref> describe the use of the MDL principle in the context of their Knowledge-based Equation Discovery System. KEDS discovers mathematical models from engineering data sets by iteratively identifying sub-regions of the problem space that can be modeled by a single equation.
Reference: [Richeldi & Rossotto 95] <author> Richeldi M., Rossotto M.: </author> <title> Class-Driven Statistical Dis-cretization of Continuous Attributes (Extended Abstract), </title> <address> ECML95, Irak-lion, Greece, </address> <publisher> (in press), </publisher> <year> 1995. </year>
Reference: [Rissanen 78] <author> J. Rissanen: </author> <title> Modeling by Shortest Data Description. In: </title> <journal> Automat-ica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference-contexts: Alternatively complexity-based induction theory [Solomonoff 64] motivates a fresh interpretation of Occam's Razor based on the idea of data compression. On this basis <ref> [Rissanen 78] </ref> formulates the so-called minimum description length (MDL) principle. In a nutshell, the MDL principle is a concept from information theory that takes into account both a theory's simplicity and a theory's predictive accuracy simultaneously. <p> This way a more complex theory will need more bits to be encoded, but might save bits when encoding more data correctly. The theory with the minimal total message length is also the most probable theory explaining the data <ref> [Rissanen 78] </ref>. Important and useful properties of the MDL principle for Machine Learning include: * The MDL principle can be used to discard low-quality theories in a principled way. <p> Both rule quality in terms of simplicity (and syntactic closeness to the respective underlying theory where known) and predictive accuracy of induced theories are convincing. This chapter is based on work reported in [Pfahringer 95a]. The Minimum Description Length (MDL) principle <ref> [Rissanen 78] </ref>, sometimes also called the Minimum Message Length (MML) principle, has been used successfully for e.g. inducing decision trees [Quinlan & Rivest 89, Quinlan 93, Wallace & Patrick 93, Forsyth 93], or in ILP [Muggleton et al. 92]. <p> If the number of tuples in the database increases and the support stays constant, the value of the rule interest function shall decrease. The first condition is usually satisfied by MDL-based measures because of the way they are derived from probability theory <ref> [Rissanen 78] </ref>. This is empirically supported by experiments with random data. The second condition is obviously true for our measure, since the number of exceptions gets less.
Reference: [Russell 89] <author> Stuart J. Russell: </author> <title> The Use of Knowledge in Analogy and Induction. </title> <publisher> Pitman Publishing, </publisher> <address> London, </address> <year> 1989. </year>
Reference-contexts: Non-strict functional dependencies are a form of knowledge that is more expressive than the propositional association rules, but also well-suited for real-world data, since exceptions are possible. According to <ref> [Russell 89] </ref>, we will call those non-strict functional dependencies partial determinations. Partial determinations can be viewed as generalizations of both functional dependencies and association rules, in that they are relational in nature and may have exceptions. <p> Given a depth bound of only three, this was to be expected. 5.5 Related Work In this section we briefly describe how our work relates to the closest work found in the literature: * <ref> [Russell 89] </ref> introduced the term 'partial determination' and described algorithms for the induction of partial determinations from facts. Briefly, Russell estimates the proportion of tuples in the database for which the determination holds through sampling.
Reference: [Savnik & Flach 93] <author> Iztok Savnik and Peter A. Flach: </author> <title> Bottom-up Induction of Functional Dependencies from Relations. </title> <booktitle> Proceedings of the AAAI-93 Workshop on Knowledge Discovery in Databases, Report WS-02, </booktitle> <publisher> AAAI Press, </publisher> <year> 1993. </year> <note> BIBLIOGRAPHY 94 </note>
Reference-contexts: We also describe three different kinds of search using the new measure. Preliminary empirical results in a few boolean domains are favorable. This chapter is based on joint work reported in [Pfahringer & Kramer 95]. Recently, researchers in KDD paid much attention to the search for functional dependencies <ref> [Flach 90, Savnik & Flach 93, Mannila & Raiha 94] </ref> and association rules [Agrawal et al. 93, Agrawal & Srikant 94, Mannila et al. 94]. Functional dependencies are essentially relational and do not allow for the possibility of 45 5.2. PARTIAL DETERMINATIONS 46 exceptions.
Reference: [Schlimmer 93] <author> Jeffery C. Schlimmer: </author> <title> Efficiently Inducing Determinations: A Complete and Systematic Search Algorithm that Uses Optimal Pruning. </title> <booktitle> Proceedings of the 10 th International Conference on Machine Learning, </booktitle> <year> 1993. </year>
Reference-contexts: Furthermore, the determinations of interest are like association rules in that they have binary attributes. The algorithm generates such simple determinations and returns them if the support is bigger than the counter-support and if a statistical test suggests their significance. * <ref> [Schlimmer 93] </ref> proposes an algorithm that returns every 'reliable' partial determination with a complexity lower than a user-defined threshold. The reliability measure is supposed to measure the functional degree of the map given subsequent data.
Reference: [Shannon & Weaver 49] <author> Shannon C.E. and Weaver W.: </author> <title> The Mathematical Theory of Communication, </title> <publisher> University of Illinois Press, </publisher> <year> 1949. </year>
Reference-contexts: For estimating the cost of encoding (5.5) the selection of E elements out of N possible elements we just use the theoretical entropy-based bound provided by <ref> [Shannon & Weaver 49] </ref> (see also appendix B). Now according to the MDL principle the one partial determination which minimizes the above cost-function, i.e. the one with the smallest bitcost (also-called the most compressive theory) is the most probable theory given the tuples of the relation. 5.3.
Reference: [Shen 91] <author> Wei-Min Shen: </author> <title> Discovering Regularities from Large Knowledge Bases. </title> <booktitle> Proceedings of the 8 th International Workshop on Machine Learning, </booktitle> <year> 1991. </year>
Reference-contexts: Briefly, Russell estimates the proportion of tuples in the database for which the determination holds through sampling. In other words, he estimates nothing else but the confidence of the dependency given a set of tuples. Therefore the same argument as for support and confidence applies. * <ref> [Shen 91] </ref> describes an algorithm that searches for three kinds of regularities in a large knowledge base. One of those regularities are determinations, and they are restricted to those having a single attribute at the left-hand side.
Reference: [Skalak 94] <author> Skalak D.B.: </author> <title> Prototype and Feature Selection by Sampling and Random Mutation Hill Climbing Algorithms, </title> <editor> in Cohen W.W. and Hirsh H.(eds.), </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference (ML94), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1994. </year>
Reference-contexts: They report favorable results, especially for the latter three methods (FSS, BSE, and BSE-Slash), for their domain of calendar scheduling tasks. As these latter methods perform bidirectional hill-climbing and as they are equipped with a mechanism for preventing cycles, they can theoretically explore all possible subsets (given enough time). <ref> [Skalak 94] </ref> successfully applies random mutation hill-climbing . [John et al. 94, Kohavi 95] make good use of best-first search for all experiments they report.
Reference: [Smyth & Goodman 91] <author> Smyth P., Goodman R.M.: </author> <title> Rule Induction Using Information Theory, </title> <editor> in Piatetsky- Shapiro G. & Matheus C.J., </editor> <title> Knowledge Discovery in Databases:, </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: 7.4 Theoretical Work In this section we just shortly reference two interesting papers, one defining a measure for the interestingness of single rules closely related to the MDL principle, the other one detailing the relationship between the MDL principle and Bayesian inference (see also [Cheeseman et al. 88]). 7.4.1 Smyth <ref> [Smyth & Goodman 91] </ref> describe an interesting schema for judging the goodness of single rules. Their aim is to rank rules with respect to each other. Their formula is also based on information theory.
Reference: [Solomonoff 64] <author> Solomonoff R.J.: </author> <title> A Formal Theory of Inductive Inference, Information and Control 7, </title> <editor> pp.1-22 and pp.224-254, </editor> <year> 1964. </year>
Reference-contexts: A common machine learning approach is to (1) restrict the set of possible models inducible at all and to (2) use various forms of pruning [Furnkranz 94] to construct a theory with hopefully good predictive accuracy. Alternatively complexity-based induction theory <ref> [Solomonoff 64] </ref> motivates a fresh interpretation of Occam's Razor based on the idea of data compression. On this basis [Rissanen 78] formulates the so-called minimum description length (MDL) principle.
Reference: [Sowa 83] <author> Sowa J.F.(ed.): </author> <title> Conceptual Structures: Information Processing in Mind and Machine, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1983. </year>
Reference-contexts: These additional heuristics make it difficult to assess the contribution of the MDL principle in this work. But reported results are very interesting. Furthermore, if one would represent first-order data in a graph-based way (as e.g. is done when using so-called conceptual structures <ref> [Sowa 83] </ref> as a representation language), this measure might prove useful for such graph-based inductive logic programming. 7.3.5 Djoko The work described in [Djoko 94] seems to be closely related to the work reported in [Cook & Holder 94] (see above).
Reference: [Srinivasan et al. 92] <author> Srinivasan A., Muggleton S., Bain M.: </author> <title> Distinguishing Exceptions from Noise in Non-Monotonic Learning, </title> <booktitle> in Proceedings of the 2nd International Workshop on Inductive Logic Programming (ILP), </booktitle> <address> Tokyo, </address> <year> 1992. </year>
Reference-contexts: Their argument is that such a scheme minimizes the sizes of proofs, thus leading to the induction of efficient theories. But we think the essential property of their formula is just to encode every example in terms of the theory. In <ref> [Srinivasan et al. 92] </ref> they report impressive results (which are reproduced below in table 2.3 of section 5) for large enough training sets in the KRK domain, a standard ILP testbed. <p> We varied noise levels from 0% to 40% and used training set sizes between 100 and 10000. Testing was always performed on a set of 5000 test examples. Results for Knopf and C4.5 are averages of 10 runs each. The results reported for Golem are as given in <ref> [Srinivasan et al. 92] </ref> and are probably results of only one run. Furthermore Golem had to construct new predicates, too. Empty entries represent the fact that no compression was achieved, i.e. the empty theory resulted from induction. <p> All results reported are averages of ten runs each using different random samples for learning, giving the mean value and the standard deviation (if it is different from zero). * Illegal King-Rook-King Chess Positions (KRK): This domain is used as a standard testbed in ILP <ref> [Srinivasan et al. 92] </ref> for experiments involving various amounts of noise and varying sizes of training sets. It is also easily transformed into a propositional learning problem.
Reference: [Thornton 95] <author> Thornton C.: </author> <title> Why Empirical Learners can't do Parity Generalization, </title> <type> Technical Report, </type> <institution> University of Sussex, </institution> <year> 1995, </year> <note> also available as http://www.cogs.sussex.ac.uk/users/christ/papers/neutral-mappings.html. </note>
Reference-contexts: The inherent problems encountered when trying to learn parity concepts is investigated in <ref> [Thornton 95] </ref>. We have also experimented with the other three boolean functions used in [Pagallo & Haussler 90]. the MUX-11, DNF-3, and DNF-4. Both MUX-11 and DNF-3 show a more graceful degradation of classification performance in the presence of noise.
Reference: [Thrun et al. 91] <author> Thrun S.B., et.al.: </author> <title> The MONK's Problems: A Performance Comparison of Different Learning Algorithms, </title> <type> CMU Tech Report, </type> <institution> CMU-CS-91-197, </institution> <year> 1991. </year>
Reference-contexts: The 5 additional boolean attributes are random (and therefore irrelevant). The training set contains 100 examples, and all 1024 possible examples are used for testing ([John et al. 94] uses the same settings). * Monk1, Monk2, Monk2loc, Monk3: These data sets are taken from <ref> [Thrun et al. 91] </ref>. Monk1, Monk2, and Monk3 all have six attributes yielding 432 possible different examples. Ten sets each were drawn randomly from these 432 examples for learning, testing used all 432 examples (as is done originally in [Thrun et al. 91]). <p> * Monk1, Monk2, Monk2loc, Monk3: These data sets are taken from <ref> [Thrun et al. 91] </ref>. Monk1, Monk2, and Monk3 all have six attributes yielding 432 possible different examples. Ten sets each were drawn randomly from these 432 examples for learning, testing used all 432 examples (as is done originally in [Thrun et al. 91]). Training set sizes are 124, 169, and 122 respectively. For Monk3 class noise is artificially added by switching the class values of 5% of the training examples. <p> Especially we will focus on how the problem of controlling search for useful changes of representation is solved in CiPF 2.0 by means of the MDL principle. Section 6.3 describes the simplified Kramer operator. Experiments in two artificial domains the Monk's Problems <ref> [Thrun et al. 91] </ref> and illegal king-rook-king chess positions [Furnkranz 93] are summarized in section 6.4. <p> In analogy to noise fitting [Angluin & Laird 87] this phenomenon can be called language fitting. Typical examples of such behavior are published in the section on AQ17-HCI in the Monk report <ref> [Thrun et al. 91] </ref>, which describes three artificial learning problems for evaluating and comparing different algorithms. We have made similar experiences with early versions of CiPF lacking sophisticated control. <p> Nonetheless the final result is most often better than the initial result, and for the rare cases where it is not, differences are marginal, e.g. an accuracy of 80% instead of 82%. 6.4. EXPERIMENTS 65 6.4.1 MONK's Problems The Monk's problems <ref> [Thrun et al. 91] </ref> are three artificially constructed problems in a space formed by six nominal attributes having from two to four possible values. There is a total of 432 different possible examples. The three problems are abbreviated to Monk1, Monk2, and Monk3 in the following.
Reference: [Wallace & Patrick 93] <author> Wallace C.S., Patrick J.D.: </author> <title> Coding Decision Trees, </title> <journal> Machine Learning, </journal> <volume> 11(1), </volume> <year> 1993. </year>
Reference: [Wetherell 80] <author> Wetherell C.S.: </author> <title> Probabilistic Languages: A Review and Some Open Questions, </title> <journal> ACM Computing Surveys, </journal> <volume> 12(4) </volume> <pages> 361-379, </pages> <year> 1980. </year>
Reference-contexts: In this case one must transmit information for decoding the false-positive atoms. False-positives are encoded using [Quinlan 93]'s method of encoding exceptions: L MC (EjT ) = log 2 jQ (T )j !! The theory itself is encoded straightforwardly. An unambiguous probabilistic context-free grammar for well-formed logic programs <ref> [Wetherell 80] </ref> is used to compute the coding length of a theory. The paper does not give any algorithm for finding good theories, it just shows applicability of the above coding scheme for the well-known ILP domains KRK and CAN-REACH.
Reference: [Witten et al. 87] <author> Witten I.A., Radford M.N., Cleary J.G.: </author> <title> Arithmetic Coding for Data Compression, </title> <journal> Communications of the ACM, </journal> <volume> 30(6), </volume> <pages> pp. 520-540, </pages> <year> 1987. </year> <note> BIBLIOGRAPHY 95 </note>
Reference: [Wnek & Michalski 94] <author> Wnek J., Michalski R.S.: </author> <title> Hypothesis-Driven Constructive Induction in AQ17-HCI: A Method and Experiments, </title> <journal> Machine Learning, </journal> <volume> 14(2), pp.139-168, </volume> <year> 1994. </year>
Reference-contexts: Therefore constructive induction is only occasionally able to improve the scores marginally. But in every test-run the initially induced theory was rewritten into a concise and easily comprehensible form like exemplified by the above given sample rule-set. reported in <ref> [Wnek & Michalski 94] </ref>. Still this approach does not seem to be able to handle considerable levels of noise. 6.4. <p> These functions are also used in <ref> [Wnek & Michalski 94] </ref> to test AQ17-HCI, where one can find more detailed descriptions, too. For PARITY-5 the concept to be learned is even/odd parity of the first five boolean attributes of a given example in the presence of 27 irrelevant random boolean attributes per example.
Reference: [Wnek & Michalski 94] <author> Wnek J., Michalski R.S.: </author> <title> Discovering Representation Space Transformations for Learning Concept Descriptions Combining DNF and M-of-N Rules, Workshop on Constructive Induction and Change of Representation, </title> <institution> ML-COLT94, Rutgers, </institution> <year> 1994. </year>
Reference-contexts: Therefore constructive induction is only occasionally able to improve the scores marginally. But in every test-run the initially induced theory was rewritten into a concise and easily comprehensible form like exemplified by the above given sample rule-set. reported in <ref> [Wnek & Michalski 94] </ref>. Still this approach does not seem to be able to handle considerable levels of noise. 6.4. <p> These functions are also used in <ref> [Wnek & Michalski 94] </ref> to test AQ17-HCI, where one can find more detailed descriptions, too. For PARITY-5 the concept to be learned is even/odd parity of the first five boolean attributes of a given example in the presence of 27 irrelevant random boolean attributes per example.
References-found: 83

