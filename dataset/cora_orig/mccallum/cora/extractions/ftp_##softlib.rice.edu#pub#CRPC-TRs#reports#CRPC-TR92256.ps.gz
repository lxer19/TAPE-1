URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR92256.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Fortran 90D Intrinsic Functions on Distributed Memory Machines: Implementation and Scalability  
Author: Ishfaq Ahmad Rajesh Bordawekar Zeki Bozkus Alok Choudhary Geoffrey Fox Kanchana Parasuram Ravi Ponnusamy Sanjay Ranka Rajeev Thakur 
Affiliation: Northeast Parallel Architectures Center, Syracuse University  
Abstract: We are developing a Fortran 90D compiler, which converts Fortran 90D code into Fortran 77 plus message passing node programs for distributed memory machines. This paper presents the implementation and performance results of Fortran 90D intrinsic functions on the Intel iPSC/860 hypercube. Our implementation is portable and scalable. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ahmad, I., A. Choudhary, G. Fox, K. Parasuram, R. Ponnusamy, S. Ranka, and R. Thakur, </author> <title> Implementation and Scalability of Fortran 90D Intrinsic Functions on Distributed Memory Machines, </title> <type> Technical Report SCCS-256, </type> <month> March 92. </month>
Reference-contexts: Because of space constraints, we describe the implementation of only a few representative intrinsics and although we have performance results on iPSC/2, iPSC/860, NCUBE and a network of workstations, we give only the performance on iPSC/860. A more detailed discussion can be found in <ref> [1] </ref>. 2 Fortran 90D Intrinsic Functions The intrinsic functions that we have implemented fall into four main categories :- * Array Reduction Functions: ALL, ANY, COUNT, MAXVAL, MINVAL, PRODUCT, SUM. * Array Manipulation Functions: CSHIFT, EOSHIFT, TRANSPOSE. * Array Location Functions: MAXLOC, MINLOC. * Vector and Matrix Multiplication Functions: DOT PRODUCT,
Reference: [2] <author> Berntsen. J, </author> <title> Communication Efficient Matrix Multiplication on a Hypercube, </title> <booktitle> Parallel Computing, </booktitle> <year> 1989, </year> <pages> pp 335-342. </pages>
Reference-contexts: For large array sizes, the computation time is much higher than the communication time and hence as the number of processors increases, the speedup increases. 4 Matrix Multiplication We have implemented two algorithms for MAT-MUL by Fox et al [4] and Berntsen <ref> [2] </ref>. The former method requires a particular multiplicand sub-matrix to be broadcast to all the processors which are in the same row of the processor configuration, followed by multiplication and neighbor communication along the columns.
Reference: [3] <author> Choudhary, A., G. Fox, S. Ranka, S. Hiranan-dani, K. Kennedy, C. Koelbel, C. Tseng, </author> <title> Compiling Fortran 77D and 90D for MIMD Distributed-Memory Machines, </title> <booktitle> in Proc. of Frontiers '92. </booktitle>
Reference-contexts: Fortran 90D is a data parallel language which can be efficiently implemented on both SIMD as well as MIMD machines. We are developing a compiler which converts Fortran 90D to Fortran 77 plus message passing node programs for a distributed memory MIMD machine <ref> [3] </ref>. In order to make the programs portable, we are implementing all the communication using EXPRESS, a portable parallel programming environment developed by Parasoft Corporation.
Reference: [4] <author> Fox, G., S. Otto, and A. Hey, </author> <title> Matrix Algorithms on a Hypercube I: Matrix multiplication, </title> <booktitle> Parallel Computing, </booktitle> <year> 1987, </year> <pages> pp. 17-31. </pages>
Reference-contexts: For large array sizes, the computation time is much higher than the communication time and hence as the number of processors increases, the speedup increases. 4 Matrix Multiplication We have implemented two algorithms for MAT-MUL by Fox et al <ref> [4] </ref> and Berntsen [2]. The former method requires a particular multiplicand sub-matrix to be broadcast to all the processors which are in the same row of the processor configuration, followed by multiplication and neighbor communication along the columns.
References-found: 4

