URL: ftp://ftpipr.ira.uka.de/pub/projects/BRA7274_B-Learn_II/papers/lims95mn.ps.gz
Refering-URL: http://wwwipr.ira.uka.de/projects/blearn/blearnpub.html
Root-URL: 
Email: e-mail nuttin@mech.kuleuven.ac.be  
Phone: Tel: 32-16-322480  
Title: Learning the Peg-into-Hole Assembly Operation with a Connectionist Reinforcement Technique  
Author: M. Nuttin, H. Van Brussel, J. Peirs, A. S. Soembagijo, and S. Sonck 
Address: Celestijnenlaan 300B, B-3001 Heverlee, Belgium  
Affiliation: Katholieke Universiteit Leuven Department of Mechanical Engineering, Division PMA  
Abstract: The paper presents a learning controller that is capable of increasing insertion speed during consecutive peg-into-hole operations, without increasing the contact force level. Our aim is to find a better relationship between measured forces and the controlled velocity, without using a complicated (human generated) model. We followed a connectionist approach. Two learning phases are distinguished. First the learning controller is trained (or initialised) in a supervised way by a suboptimal task frame controller. Then a reinforcement learning phase follows. The controller consists of two networks: (1) the policy network and (2) the exploration network. On-line robotic exploration plays a crucial role in obtaining a better policy. Optionally, this architecture can be extended with a third network: the reinforcement network. The learning controller is implemented on a CAD-based contact force simulator. In contrast with most other related work, the experiments are simulated in 3D with 6 degrees of freedom. Performance of a peg-into-hole task is measured in insertion time and average/maximum force level. The fact that a better performance can be obtained in this way, demonstrates the importance of model-free learning techniques for repetitive robotic assembly tasks. The paper presents the approach and simulation results. Keywords: robotic assembly, peg-into-hole, artificial neural networks, reinforcement learning.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Asada. </author> <title> Teaching and learning of compliance using neural nets: Representation and generation of nonlinear compliance. </title> <booktitle> In Proceedings of the 1990 IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 1237 - 1244, </pages> <year> 1990. </year>
Reference-contexts: The best results are obtained using an action network with two hidden layers, each containing 8 hidden units. Although optimal control of a peg-into-hole insertion was proved to be non-linear <ref> [1] </ref>, it appears to be a hard task to learn a non-linear relationship with a multi-layer ANN. It's quite possible that the nature of the unknown optimal non-linear controller is equivalent to a set of several nearly-linear controllers. <p> In about 50% of these experiments, the RL-controller is able to converge to an excellent final performance. In the opinion of the authors it is more pragmatic and safer to exploit available knowledge. 5 Evaluation Similar approaches have been reported in <ref> [1, 2, 7, 13] </ref>. Simons et al. initiated the idea to learn a relationship between contact forces and manipulator commands. They used a learning stochastic automaton with an optimized quantification level of the input variables [13]. <p> Simons et al. initiated the idea to learn a relationship between contact forces and manipulator commands. They used a learning stochastic automaton with an optimized quantification level of the input variables [13]. In <ref> [1] </ref> Asada used an ANN to generate non-linear compliance for a two-dimensional version of the peg-into-hole task. Position and force uncertainty are not included in his simulations nor does [1] report performance results for the insertion operations. <p> They used a learning stochastic automaton with an optimized quantification level of the input variables [13]. In <ref> [1] </ref> Asada used an ANN to generate non-linear compliance for a two-dimensional version of the peg-into-hole task. Position and force uncertainty are not included in his simulations nor does [1] report performance results for the insertion operations. A training set of velocity commands must be available so that the ANN can be trained directly in a supervised way. Gullapalli e.a. [7] also used a RL approach to avoid the latter requirement. Their experiments concerned a two-dimensional peg-into-hole problem.
Reference: [2] <author> B. Borovac and D. Katic. </author> <title> Connectionist reactive control for robotic assembly tasks by soft sensored grippers. </title> <booktitle> In Proceedings of the 24th ISIR, </booktitle> <year> 1993. </year>
Reference-contexts: In about 50% of these experiments, the RL-controller is able to converge to an excellent final performance. In the opinion of the authors it is more pragmatic and safer to exploit available knowledge. 5 Evaluation Similar approaches have been reported in <ref> [1, 2, 7, 13] </ref>. Simons et al. initiated the idea to learn a relationship between contact forces and manipulator commands. They used a learning stochastic automaton with an optimized quantification level of the input variables [13]. <p> Gullapalli e.a. [7] also used a RL approach to avoid the latter requirement. Their experiments concerned a two-dimensional peg-into-hole problem. They didn't compare the resulting performance with a classical non-learning approach. In <ref> [2] </ref> pressure sensors are 9 used on soft fingers. The authors concentrated on off-line learning in a knowledge acquisition phase, but didn't report performance results in terms of insertion time and force level.
Reference: [3] <author> J. De Schutter and H. Van Brussel. </author> <title> Compliant robot motion I, a formalism for specifying compliant motion tasks. </title> <journal> The International Journal of Robotics Research, </journal> <volume> 7(4), </volume> <month> August </month> <year> 1988. </year>
Reference-contexts: Two classes of active compliant techniques can be distinguished: fine-motion planning and reactive control, and it is in a reactive control approach that we have integrated reinforcement learning techniques and neural nets. More information on compliant motion and the peg-into-hole task can be found in <ref> [3, 7, 10, 9, 14, 15] </ref>. 2 The reinforcement learning (RL) system 2.1 Introduction Reinforcement learning (RL) is an universal applicable but slowly converging optimisation technique [8, 11, 16]. <p> The control architecture of the RL phase is shown in Figure 4 on the right hand side. The contact stiffness, the task kinematics and its evolution, are not explicitly taken into account. Our RL algorithm is summarised in Figure 5. A command specification language similar to that of <ref> [3] </ref> was developed and is shown in Figure 6. 4 Simulation results The learning controller was implemented on a CAD based contact force simulator [17]. The set-up is shown in Figure 7.
Reference: [4] <author> J. De Schutter and H. Van Brussel. </author> <title> Compliant robot motion II, a control approach based on external control loops. </title> <journal> The International Journal of Robotics Research, </journal> <volume> 7(4), </volume> <month> August </month> <year> 1988. </year>
Reference-contexts: In the KA phase, the agent is trained in a supervised way by a task frame controller <ref> [4] </ref> (Figure 4, left hand side). We used the quick backpropagation 5 acquisition (KA) phase; on the right: the reinforcement learning (RL) phase. algorithm [5]. The agent receives as training input X, the measured forces and the cartesian end-effector position; as target output it receives the cartesian end-effector velocity.
Reference: [5] <author> S. E. Fahlman. </author> <title> An empirical study of learning speed in back propagation networks. </title> <type> Technical Report CMU-CS-88-162, Carnegie-Mellon Universtity Technical Report, </type> <year> 1988. </year>
Reference-contexts: In the KA phase, the agent is trained in a supervised way by a task frame controller [4] (Figure 4, left hand side). We used the quick backpropagation 5 acquisition (KA) phase; on the right: the reinforcement learning (RL) phase. algorithm <ref> [5] </ref>. The agent receives as training input X, the measured forces and the cartesian end-effector position; as target output it receives the cartesian end-effector velocity.
Reference: [6] <author> R. Ghijsen and K. Sterckx. </author> <title> Leerstrategieen met neurale netwerken voor een krachtgecontroleerde robot. </title> <type> Master's thesis, </type> <institution> Katholieke Universiteit Leuven, Department of Mechanical Engineering, Division PMA, </institution> <address> Celestijnenlaan 300 B, B-3001 Heverlee, Belgium, </address> <year> 1994. </year>
Reference-contexts: The set-up is shown in Figure 7. The simulations with a 6 degrees of freedom manipulator include sensor noise, finite encoder resolution and initial position uncertainty. Simulation results are shown in Figure 8, and more results are reported in <ref> [12, 6] </ref>. Figure 8a shows the evolution of the (dimensionless) quality measure Q 2 in equation (3) over 300 insertion operations. From Figures 8b and 8c it can be seen that the average contact force level decreases and that the insertion time decreases substantially.
Reference: [7] <author> V. Gullapalli, R.A. Grupen, and A.G. Barto. </author> <title> Learning reactive admittance control. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <address> Nice, France, </address> <year> 1992. </year>
Reference-contexts: Two classes of active compliant techniques can be distinguished: fine-motion planning and reactive control, and it is in a reactive control approach that we have integrated reinforcement learning techniques and neural nets. More information on compliant motion and the peg-into-hole task can be found in <ref> [3, 7, 10, 9, 14, 15] </ref>. 2 The reinforcement learning (RL) system 2.1 Introduction Reinforcement learning (RL) is an universal applicable but slowly converging optimisation technique [8, 11, 16]. <p> In about 50% of these experiments, the RL-controller is able to converge to an excellent final performance. In the opinion of the authors it is more pragmatic and safer to exploit available knowledge. 5 Evaluation Similar approaches have been reported in <ref> [1, 2, 7, 13] </ref>. Simons et al. initiated the idea to learn a relationship between contact forces and manipulator commands. They used a learning stochastic automaton with an optimized quantification level of the input variables [13]. <p> Position and force uncertainty are not included in his simulations nor does [1] report performance results for the insertion operations. A training set of velocity commands must be available so that the ANN can be trained directly in a supervised way. Gullapalli e.a. <ref> [7] </ref> also used a RL approach to avoid the latter requirement. Their experiments concerned a two-dimensional peg-into-hole problem. They didn't compare the resulting performance with a classical non-learning approach. In [2] pressure sensors are 9 used on soft fingers.
Reference: [8] <author> Long-Ji Lin. </author> <title> Self-improving reactive agents based on reinforcement learning, planning and teaching. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 293-321, </pages> <year> 1992. </year> <month> 10 </month>
Reference-contexts: More information on compliant motion and the peg-into-hole task can be found in [3, 7, 10, 9, 14, 15]. 2 The reinforcement learning (RL) system 2.1 Introduction Reinforcement learning (RL) is an universal applicable but slowly converging optimisation technique <ref> [8, 11, 16] </ref>. Typically RL is applied when 1) a model isn't available, and 2) only a scalar evaluation of the true performance is available on-line. The main difference with the supervised learning paradigm is the absence of gradient information on the performance surface.
Reference: [9] <author> B. J. McCarragher and H. Asada. </author> <title> A discrete event approach to the control of robotic assem-bly tasks. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <volume> volume 1, </volume> <pages> pages 331-336, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Two classes of active compliant techniques can be distinguished: fine-motion planning and reactive control, and it is in a reactive control approach that we have integrated reinforcement learning techniques and neural nets. More information on compliant motion and the peg-into-hole task can be found in <ref> [3, 7, 10, 9, 14, 15] </ref>. 2 The reinforcement learning (RL) system 2.1 Introduction Reinforcement learning (RL) is an universal applicable but slowly converging optimisation technique [8, 11, 16].
Reference: [10] <author> B. J. McCarragher and H. Asada. </author> <title> Qualitative template matching using dynamic process models for state transition recognition of robotic assembly. Journal of Dynamic Systems, </title> <booktitle> Measurement, and Control, </booktitle> <pages> pages 261-269, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Two classes of active compliant techniques can be distinguished: fine-motion planning and reactive control, and it is in a reactive control approach that we have integrated reinforcement learning techniques and neural nets. More information on compliant motion and the peg-into-hole task can be found in <ref> [3, 7, 10, 9, 14, 15] </ref>. 2 The reinforcement learning (RL) system 2.1 Introduction Reinforcement learning (RL) is an universal applicable but slowly converging optimisation technique [8, 11, 16].
Reference: [11] <author> W. T. Miller, S. R. Sutton, and J. P. Werbos, </author> <title> editors. Neural Networks for Control. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: More information on compliant motion and the peg-into-hole task can be found in [3, 7, 10, 9, 14, 15]. 2 The reinforcement learning (RL) system 2.1 Introduction Reinforcement learning (RL) is an universal applicable but slowly converging optimisation technique <ref> [8, 11, 16] </ref>. Typically RL is applied when 1) a model isn't available, and 2) only a scalar evaluation of the true performance is available on-line. The main difference with the supervised learning paradigm is the absence of gradient information on the performance surface. <p> The critic produces an (internal) reinforcement that the agent will use to modify its policy. The critic and the agent are described in the next sections. 2.2 The critic In general, the critic consists of two elements (Figure 2): the external critic and the adaptive heuristic critic (AHC) <ref> [11] </ref>. The external critic contains the global function to be optimised. 2 This global function can only be evaluated after the assembly operation is completed. The agent needs an evaluation after every action: the internal reinforcement. The AHC can learn the internal reinforcement function using information in the external critic.
Reference: [12] <author> J. Peirs and S. Sonck. Leerstrategieen voor krachtgecontroleerde montagebewerkingen. </author> <type> Master's thesis, </type> <institution> Katholieke Universiteit Leuven, Department of Mechanical Engineering,Division PMA, </institution> <address> Celestijnenlaan 300 B, B-3001 Heverlee, Belgium, </address> <year> 1993. </year>
Reference-contexts: The set-up is shown in Figure 7. The simulations with a 6 degrees of freedom manipulator include sensor noise, finite encoder resolution and initial position uncertainty. Simulation results are shown in Figure 8, and more results are reported in <ref> [12, 6] </ref>. Figure 8a shows the evolution of the (dimensionless) quality measure Q 2 in equation (3) over 300 insertion operations. From Figures 8b and 8c it can be seen that the average contact force level decreases and that the insertion time decreases substantially.
Reference: [13] <author> J. Simons, H. Van Brussel, J. De Schutter, and J. Verhaert. </author> <title> A self-learning automaton with variable resolution for high precision assembly by industrial robots. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> AC-27(5), </volume> <month> October </month> <year> 1982. </year>
Reference-contexts: In about 50% of these experiments, the RL-controller is able to converge to an excellent final performance. In the opinion of the authors it is more pragmatic and safer to exploit available knowledge. 5 Evaluation Similar approaches have been reported in <ref> [1, 2, 7, 13] </ref>. Simons et al. initiated the idea to learn a relationship between contact forces and manipulator commands. They used a learning stochastic automaton with an optimized quantification level of the input variables [13]. <p> Simons et al. initiated the idea to learn a relationship between contact forces and manipulator commands. They used a learning stochastic automaton with an optimized quantification level of the input variables <ref> [13] </ref>. In [1] Asada used an ANN to generate non-linear compliance for a two-dimensional version of the peg-into-hole task. Position and force uncertainty are not included in his simulations nor does [1] report performance results for the insertion operations.
Reference: [14] <author> H. Van Brussel and J. Simons. </author> <title> The adaptable compliance concept and its use for automatic assembly by active force feedback accommodations. </title> <booktitle> In Proceedings of the 9th International Symposium on Industrial Robots, </booktitle> <month> March </month> <year> 1979. </year>
Reference-contexts: Two classes of active compliant techniques can be distinguished: fine-motion planning and reactive control, and it is in a reactive control approach that we have integrated reinforcement learning techniques and neural nets. More information on compliant motion and the peg-into-hole task can be found in <ref> [3, 7, 10, 9, 14, 15] </ref>. 2 The reinforcement learning (RL) system 2.1 Introduction Reinforcement learning (RL) is an universal applicable but slowly converging optimisation technique [8, 11, 16].
Reference: [15] <author> D. E. Whitney. </author> <title> Quasi-static assembly of compliantly supported rigid parts. Journal of Dynamic Systems, Measurement, </title> <journal> and Control, </journal> <volume> 104 </volume> <pages> 65-77, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: Two classes of active compliant techniques can be distinguished: fine-motion planning and reactive control, and it is in a reactive control approach that we have integrated reinforcement learning techniques and neural nets. More information on compliant motion and the peg-into-hole task can be found in <ref> [3, 7, 10, 9, 14, 15] </ref>. 2 The reinforcement learning (RL) system 2.1 Introduction Reinforcement learning (RL) is an universal applicable but slowly converging optimisation technique [8, 11, 16].
Reference: [16] <author> Ronald J. Williams. </author> <title> Simple statistical gradient-following algorithms for connectionist reinforcement learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 229-256, </pages> <year> 1992. </year>
Reference-contexts: More information on compliant motion and the peg-into-hole task can be found in [3, 7, 10, 9, 14, 15]. 2 The reinforcement learning (RL) system 2.1 Introduction Reinforcement learning (RL) is an universal applicable but slowly converging optimisation technique <ref> [8, 11, 16] </ref>. Typically RL is applied when 1) a model isn't available, and 2) only a scalar evaluation of the true performance is available on-line. The main difference with the supervised learning paradigm is the absence of gradient information on the performance surface. <p> The exploration module performs in fact a kind of on-line system identification. The learning module adjusts the policy parameters in order to maximise the expected reinforcement over all actions. The algorithm we used is essentially a REINFORCE algorithm, proposed by R. J. Williams <ref> [16] </ref>.
Reference: [17] <author> W. Witvrouw, P. Van De Poel, H. Bruyninckx, and J. De Schutter. Rosi: </author> <title> a task specification and simulation tool for force sensor based robot control. </title> <booktitle> In 24th International Symposium on Industrial Robots, </booktitle> <month> November </month> <year> 1993. </year> <month> 11 </month>
Reference-contexts: Our RL algorithm is summarised in Figure 5. A command specification language similar to that of [3] was developed and is shown in Figure 6. 4 Simulation results The learning controller was implemented on a CAD based contact force simulator <ref> [17] </ref>. The set-up is shown in Figure 7. The simulations with a 6 degrees of freedom manipulator include sensor noise, finite encoder resolution and initial position uncertainty. Simulation results are shown in Figure 8, and more results are reported in [12, 6].
References-found: 17

