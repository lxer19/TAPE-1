URL: http://www.cs.unm.edu/~kapur/abstracts/stoc.96.ps.gz
Refering-URL: http://www.cs.unm.edu/~kapur/abstracts/stoc.96.html
Root-URL: http://www.cs.unm.edu
Author: Deepak Kapur and Tushar Saxena 
Date: September 23, 1995  
Address: Albany, NY 12222  
Affiliation: Institute for Programming and Logics Department of Computer Science State University of New York at Albany  
Abstract: Sparsity Considerations in Dixon Resultants fl Abstract New results relating the sparsity of non-homogeneous polynomial systems and computation of their projection operator (i.e., a non-trivial multiple of multivariate resultant) using Dixon's method are developed. It is proved that the Dixon formulation of resultants, despite being classical, implicitly exploits the structure of the Newton polytopes of input polynomials; the complexity of computing Dixon resultant is not determined by the total degree of the polynomial system. Bound on the size of the Dixon matrix of unmixed polynomial systems is derived in terms of their Newton polytopes. This bound is used to prove that for a multi-homogeneous system, the size of its Dixon matrix is of a smaller order than its n-fold mixed volume. Using dense multivariate polynomial interpolation techniques, it is shown that for a fixed number of variables, Dixon matrix of multi-homogeneous polynomial systems can be constructed using O(M 3 ) arithmetic operations, where M is the n-fold mixed volume of the input system. The Dixon matrix is found to be smaller than the sparse and Macaulay resultant matrices by a factor exponential in the number of variables, and the Dixon formulation yields a faster algorithm to compute the resultant. This work links the classical Dixon formulation (developed in 1908) to the modern line of sparsity analysis based on Newton polytopes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bernshtein D.N., </author> <title> The Number of Roots of a System of Equations, </title> <journal> Funktsional'nyi Analiz i Ego Prilozheniya, </journal> <volume> 9(3) </volume> <pages> 1-4. </pages>
Reference-contexts: The size of this matrix depends on the total degree of the input polynomials. However, recently, sharper bounds have been derived on the number fl Supported in part by a grant from United States Air Force Office of Scientific Research AFOSR-91-0361. of solutions, known as BKK bounds <ref> [1] </ref>, which exploit the structure and sparsity of the polynomials as determined by their Newton polytopes. The sparse resultant formulation is a modification of the Macaulay formulation which exploits the modern BKK bound and Newton polytopes of the input polynomials to construct smaller matrices [7].
Reference: [2] <author> Canny J., </author> <title> Generalized Characteristic Polynomials, </title> <journal> Journal of Symbolic Computation, </journal> <volume> 9 </volume> <pages> 241-250. </pages>
Reference-contexts: For Macaulay formulation, we give two timings, one using the Generalized Characteristic Method of Canny <ref> [2] </ref>, and the other using a method similar to one in Theorem 2.2. As can be observed, Dixon matrix is much smaller and the Dixon formulation performs much better than others and was able to solve all 14 problems reasonably well.
Reference: [3] <author> Canny J., Pedersen P., </author> <title> An Algorithm for Newton Resultant, </title> <type> Technical Report, </type> <institution> Cornell University, CornellCS:TR93-1394, </institution> <year> 1993. </year>
Reference: [4] <author> Dixon A.L., </author> <title> The eliminant of three quantics in two independent variables. </title> <journal> Proc. London Mathematical Society, </journal> <volume> 6, </volume> <year> 1908, </year> <pages> 468-478. </pages>
Reference-contexts: There are three major methods to compute the resultant or a projection operator the classical formulations by Dixon <ref> [4] </ref> and Macaulay [12] developed early this century, and the recently developed sparse resultant formulation [11, 6]. All three methods construct matrices whose determinant is either the resultant, or a projection operator. <p> However, such failure seems very rare; in fact, it has never occurred on the numerous problems that we have tried from different application domains. (For systems with two variables, it can be proved that this method always works <ref> [4] </ref>.) Below we establish a relationship between the Newton polytopes of the input polynomials and the size of the Dixon matrix, and compare it with resultant matrices in other formulations. 3 Newton Polytopes and the Dixon Matrix Let the convex hull of a set A ae ZZ n be denoted by
Reference: [5] <author> Ehrart E. </author> <title> Sur un probleme de geometrie diophantienne, I. </title> <journal> Polyedres et reseaux., J. Reine Angew. Math., </journal> <volume> 226 </volume> <pages> 1-29, </pages> <year> 1967. </year>
Reference-contexts: The cardinality of an integer point set is asymptotically bounded by the volume of their convex hull <ref> [5] </ref>. Let vol (A) denote the n-dimensional volume of a convex set A ae IR n . We get the following bound on the size of Dixon matrix as a function of the Newton polytopes of input polynomials.
Reference: [6] <author> Gelfand I.M., </author> <title> Kapranov M.M., Zelevinsky A.V., Discriminants, Resultants and Multidimensional Determinants, </title> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1994. </year>
Reference-contexts: There are three major methods to compute the resultant or a projection operator the classical formulations by Dixon [4] and Macaulay [12] developed early this century, and the recently developed sparse resultant formulation <ref> [11, 6] </ref>. All three methods construct matrices whose determinant is either the resultant, or a projection operator. The Macaulay formulation uses the traditional Bezout bound on the number of solutions of a polynomial system to construct the Macaulay matrix.
Reference: [7] <author> Emiris I., </author> <title> Sparse Elimination and Applications in Kinematics, </title> <type> Doctoral dissertation thesis, </type> <institution> Department of Computer Science, U of Calif., Berkeley, </institution> <year> 1994. </year>
Reference-contexts: The sparse resultant formulation is a modification of the Macaulay formulation which exploits the modern BKK bound and Newton polytopes of the input polynomials to construct smaller matrices <ref> [7] </ref>. The size of the sparse resultant matrix depends on the volume of the polytopes of input polynomials, and is not governed by their total degree. <p> This is in contrast to the sparse resultant formulation which, if the polynomials are full 2 , then for a fixed number of variables, takes O (M 6:5 ) arithmetic operations <ref> [7] </ref>. If the polynomials are hollow 3 , the construction time of the sparse resultant reduces considerably and the determinant computation dominates, but, it still takes longer than the Dixon formulation because the matrix size is larger (Equation 2). <p> The Dixon formulation does consider the sparsity in the input polynomials and implicitly exploits the structure of the Newton polytopes of input polynomials. 3.2 Comparison with Sparse Resultant Matrix A result analogous to Corollary 3.5 exists for the sparse resultant matrix <ref> [7] </ref>: Fact 3.6 Size of the sparse resultant matrix of an unmixed set, P ae Q [X], of n + 1 polynomials is O vol i=0 !! The reader will notice the surprising similarity between Fact 3.6 and Corollary 3.5. <p> First is the mixed volume, which we have used throughout this paper, and the other is the number of terms with nonzero coefficients in the polynomials. The following theorem from <ref> [7] </ref> gives the cost of constructing the sparse resultant matrix.
Reference: [8] <author> Hoffman C.M., </author> <title> Algebraic and Numerical Techniques for Offsets and Blends, Computation of Curves and Surfaces W. </title> <editor> Dahmen et. al. (eds.), </editor> <publisher> Kluwer Acad. Pub., </publisher> <year> 1990, </year> <pages> 499-528. </pages>
Reference-contexts: 1 Introduction Eliminating n variables from n+1 multivariate polynomials results in the resultant a quantity which has proved useful in many applications <ref> [8] </ref>. Most efficient ways to compute the resultant express it as a determinant.
Reference: [9] <author> Kapur D., Saxena T., Yang L., </author> <title> Algebraic and Geometric Reasoning using Dixon Resultants, </title> <booktitle> Proc. ACM ISSAC 94, </booktitle> <address> Oxford, England, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: Methods based on the Dixon formulation have been empirically found to be more efficient than the Macaulay and the sparse resultant formulations <ref> [9, 10] </ref>. However, being a classical approach (developed in 1908), the relationship between the Dixon formulation and the modern BKK bounds, or the structure of the input polytopes, is not well understood. <p> However, if the Dixon matrix is rectangular then the determinant (and the projection operator) cannot be directly computed. Even if it is square, it may be singular, resulting in a trivial projection operator, which is undesirable. The following theorem given in <ref> [9] </ref>, provides a method to extract a non-trivial projection operator in such instances. Theorem 2.2 If D P has a column which is linearly independent of all other columns, then the determinant of any non-singular rank submatrix of D P is a non-trivial projection operator.
Reference: [10] <author> Kapur D., Saxena T., </author> <title> Comparison of Various Multivariate Resultants, </title> <booktitle> To appear in Proc. ACM ISSAC 95, </booktitle> <address> Montreal, Canada, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Methods based on the Dixon formulation have been empirically found to be more efficient than the Macaulay and the sparse resultant formulations <ref> [9, 10] </ref>. However, being a classical approach (developed in 1908), the relationship between the Dixon formulation and the modern BKK bounds, or the structure of the input polytopes, is not well understood. <p> Theorem 2.2 If D P has a column which is linearly independent of all other columns, then the determinant of any non-singular rank submatrix of D P is a non-trivial projection operator. This method of computing the projection operator is quite efficient in practice as demonstrated in <ref> [10] </ref>. As can be noticed from the theorem, this method may fail if there is no column in the Dixon matrix which is linearly independent of all other columns. <p> Because of larger size of the sparse resultant matrix, the Dixon formulation again computes the resultant faster. In both cases, more so in the full case, the Dixon formulation is less expensive than the sparse resultant formulation. To see typical timings in practice, see Table 4.2 (from <ref> [10] </ref>) in which the timings of Dixon, sparse and Macaulay formulations are given on 14 examples, mostly from robotics, vision, geometry and graphics. The coefficients in these systems are polynomials in parameters and not just integers.
Reference: [11] <author> Sturmfels B., </author> <title> Sparse Elimination Theory, </title> <booktitle> Proc. Computat. Algebraic Geom. and Commut. Algebra, </booktitle> <editor> D. Eisenbud and L. Robbiano, eds., </editor> <address> Cortona , Italy, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: There are three major methods to compute the resultant or a projection operator the classical formulations by Dixon [4] and Macaulay [12] developed early this century, and the recently developed sparse resultant formulation <ref> [11, 6] </ref>. All three methods construct matrices whose determinant is either the resultant, or a projection operator. The Macaulay formulation uses the traditional Bezout bound on the number of solutions of a polynomial system to construct the Macaulay matrix. <p> However, being a classical approach (developed in 1908), the relationship between the Dixon formulation and the modern BKK bounds, or the structure of the input polytopes, is not well understood. In <ref> [11] </ref>, Sturmfels gave the bracket representation of the Dixon resultant for bi-homogeneous systems; however the Dixon resultant was not analyzed in its full generality. Moreover, it has been unclear whether the Dixon formulation exploits the sparsity of input polynomials, and if so, to what extent (quantitatively).
Reference: [12] <author> Macaulay F.S., </author> <title> The Algebraic Theory of Modular Systems, </title> <journal> Cambridge Tracts in Math. and Math. Phys., </journal> <volume> 19, </volume> <year> 1916. </year>
Reference-contexts: There are three major methods to compute the resultant or a projection operator the classical formulations by Dixon [4] and Macaulay <ref> [12] </ref> developed early this century, and the recently developed sparse resultant formulation [11, 6]. All three methods construct matrices whose determinant is either the resultant, or a projection operator.
Reference: [13] <author> Weyman J. and Zelevinsky A., </author> <title> Determinantal Formulas for Multigraded Resultants, </title> <journal> Journal of Algebraic Geometry, </journal> <pages> pp 569-597, </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction Eliminating n variables from n+1 multivariate polynomials results in the resultant a quantity which has proved useful in many applications [8]. Most efficient ways to compute the resultant express it as a determinant. However, a pure determinantal expression for the resultant is rare (see <ref> [13] </ref> for some classes of polynomials which allow a pure determinantal formulation), and in these cases, one can still express some nontrivial multiple of the resultant (called a projection operator) as a determinant.
Reference: [14] <author> Zippel R., </author> <title> Effective Polynomial Computation, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1993. </year> <month> 10 </month>
Reference-contexts: Since the entries of the cancellation matrix are multivariate polynomials, 7 direct expansion can be quite expensive, and give rise to large intermediate expressions. This can be avoided by using dense multivariate polynomial interpolation <ref> [14] </ref>. Interpolation works in stages. In each stage, polynomial is interpolated in one more variable using bounds on the number of terms in the polynomial (when viewed as a polynomial in the variables which have already been lifted) and its degree in the variable being lifted.
References-found: 14

