URL: http://www.cs.toronto.edu/~greiner/PAPERS/th-rev-update.ps
Refering-URL: http://www.cs.toronto.edu/~greiner/PAPERS/
Root-URL: 
Email: E-mail: fdrastal, bharat, greinerg@scr.siemens.com  
Phone: Phone: (609) 734-3627, Fax: (609) 734-6565  
Title: Theory Revision in Fault Hierarchies  
Author: Pat Langley, George Drastal, R. Bharat Rao, and Russell Greiner 
Keyword: machine learning, theory refinement, expert systems  
Address: 755 College Road East, Princeton, NJ 08540 USA  
Affiliation: Adaptive Information and Signal Processing, Siemens Corporate Research  
Abstract: The fault hierarchy representation is widely used in expert systems for the diagnosis of complex mechanical devices. This paper describes the theory revision algorithm that revises such fault hierarchies. This task presents several challenges: (1) typical training instances are missing most feature values; (2) the pattern of missing features is significant, rather than merely an effect of noise; and (3) the quality of a candidate theory depends both on the correctness of the diagnoses it returns, and on the set of tests it uses to reach those diagnoses. This paper describes how addresses these challenges and reports on experiments that use to improve the performance of two fielded diagnostic systems. fl This is an extended version of a paper that appeared in Proceedings of the Fifth International Workshop on Principles of Diagnosis (Dx94), New York, October 1994. y We gratefully acknowledge receiving helpful comments from Cheoung-Nam Lee, Glenn Meredith, and Chandra Mouleeswaran. z Current address: Robotics Laboratory, Computer Science Department, Stanford University, Stanford, CA 94305. e-mail: langley@flamingo.stanford.edu, phone: (415) 725-8813, fax: (415) 725-1449. 
Abstract-found: 1
Intro-found: 1
Reference: [Ask94] <author> L. Asker. </author> <title> Improving accuracy of incorrect domain theories. </title> <booktitle> In Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 19-27, </pages> <year> 1994. </year>
Reference-contexts: its behavior will be robust even when given a large fault hierarchy that contains many errors and training instances that omit the values of many features. 4 Discussion 4.1 Related Research on Theory Revision The approach we have taken with has close connections with other work on theory revision, including <ref> [GWP88, OM90, CS90, TS94, Cai91, BM91, WP93, Ask94] </ref>; these methods also start from an initial domain theory obtained from experts, and iteratively modify that theory to obtain a theory with improved accuracy on a set of training cases.
Reference: [BM91] <author> B.Richards and R. Mooney. </author> <title> First-order theory revision. </title> <booktitle> In ML-91 , pages 447-451, </booktitle> <year> 1991. </year>
Reference-contexts: its behavior will be robust even when given a large fault hierarchy that contains many errors and training instances that omit the values of many features. 4 Discussion 4.1 Related Research on Theory Revision The approach we have taken with has close connections with other work on theory revision, including <ref> [GWP88, OM90, CS90, TS94, Cai91, BM91, WP93, Ask94] </ref>; these methods also start from an initial domain theory obtained from experts, and iteratively modify that theory to obtain a theory with improved accuracy on a set of training cases.
Reference: [Cai91] <author> T. Cain. </author> <title> The Ductor: A theory revision system for propositional domains. </title> <booktitle> In ML-91 , pages 485-489, </booktitle> <year> 1991. </year>
Reference-contexts: its behavior will be robust even when given a large fault hierarchy that contains many errors and training instances that omit the values of many features. 4 Discussion 4.1 Related Research on Theory Revision The approach we have taken with has close connections with other work on theory revision, including <ref> [GWP88, OM90, CS90, TS94, Cai91, BM91, WP93, Ask94] </ref>; these methods also start from an initial domain theory obtained from experts, and iteratively modify that theory to obtain a theory with improved accuracy on a set of training cases.
Reference: [CF94] <author> R. Caruana and D. Freitag. </author> <title> Greedy attribute selection. </title> <booktitle> In Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 28-36, </pages> <year> 1994. </year>
Reference-contexts: We can use this same "trick" to acquire new repairs, by adding a link from the some node in KB cc to a node with a novel repair. (Of course, we still require an expert to specify these auxiliary tests and repairs.) A third extension concerns 's "wrapper" approach <ref> [JKP94, CF94] </ref>, which generates and evaluates all possible revisions, viewing each resulting hierarchy as a black box. Although this scheme is easy to describe, it is inefficient for large knowledge bases and large training sets. Even in our controlled experiments, generated approximately 10; 000 neighbors for each current hierarchy.
Reference: [CS90] <author> S. Craw and D. Sleeman. </author> <title> Automating the refinement of knowledge-based systems. </title> <booktitle> In Proceedings of ECAI 90. </booktitle> <publisher> Pitman, </publisher> <year> 1990. </year>
Reference-contexts: its behavior will be robust even when given a large fault hierarchy that contains many errors and training instances that omit the values of many features. 4 Discussion 4.1 Related Research on Theory Revision The approach we have taken with has close connections with other work on theory revision, including <ref> [GWP88, OM90, CS90, TS94, Cai91, BM91, WP93, Ask94] </ref>; these methods also start from an initial domain theory obtained from experts, and iteratively modify that theory to obtain a theory with improved accuracy on a set of training cases.
Reference: [DRC89] <author> G. Drastal, S. Raatz, and G. Czako. </author> <title> Induction in an abstraction space: A form of constructive induction. </title> <booktitle> In IJCAI-89, </booktitle> <pages> pages 708-712, </pages> <year> 1989. </year>
Reference-contexts: The bias toward modifying the initial domain theory as little as possible, implemented in by a preference ordering on transformations, can also be found in <ref> [DRC89] </ref>. One can also view incremental methods for the induction of decision trees [SF86, Utg89] as carrying out a form of theory revision. These approaches perform a hill-climbing search using the current knowledge base as the starting point.
Reference: [GWP88] <author> A. Ginsberg, S. Weiss, and P. Politakis. </author> <title> Automatic knowledge base refine ment for classification systems. </title> <journal> Artificial Intelligence, </journal> <volume> 35 </volume> <pages> 197-226, </pages> <year> 1988. </year>
Reference-contexts: its behavior will be robust even when given a large fault hierarchy that contains many errors and training instances that omit the values of many features. 4 Discussion 4.1 Related Research on Theory Revision The approach we have taken with has close connections with other work on theory revision, including <ref> [GWP88, OM90, CS90, TS94, Cai91, BM91, WP93, Ask94] </ref>; these methods also start from an initial domain theory obtained from experts, and iteratively modify that theory to obtain a theory with improved accuracy on a set of training cases.
Reference: [JKP94] <author> G. John, R. Kohavi, and K. Pfleger. </author> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In ML-94, </booktitle> <pages> pages 121-29, </pages> <year> 1994. </year>
Reference-contexts: We can use this same "trick" to acquire new repairs, by adding a link from the some node in KB cc to a node with a novel repair. (Of course, we still require an expert to specify these auxiliary tests and repairs.) A third extension concerns 's "wrapper" approach <ref> [JKP94, CF94] </ref>, which generates and evaluates all possible revisions, viewing each resulting hierarchy as a black box. Although this scheme is easy to describe, it is inefficient for large knowledge bases and large training sets. Even in our controlled experiments, generated approximately 10; 000 neighbors for each current hierarchy.
Reference: [OM90] <author> D. Ourston and R.J. Mooney. </author> <title> Changing the rules: A comprehensive approach to theory refinement. </title> <booktitle> In AAAI-90, </booktitle> <pages> pages 815-820, </pages> <year> 1990. </year>
Reference-contexts: its behavior will be robust even when given a large fault hierarchy that contains many errors and training instances that omit the values of many features. 4 Discussion 4.1 Related Research on Theory Revision The approach we have taken with has close connections with other work on theory revision, including <ref> [GWP88, OM90, CS90, TS94, Cai91, BM91, WP93, Ask94] </ref>; these methods also start from an initial domain theory obtained from experts, and iteratively modify that theory to obtain a theory with improved accuracy on a set of training cases.
Reference: [RGH94] <author> R. B. Rao, R. Greiner, and T. Hancock. </author> <title> Exploiting the absence of irrele vant information. </title> <booktitle> In AAAI Fall Symposium on `Relevance', </booktitle> <year> 1994. </year>
Reference-contexts: However, the selective nature of the diagnostic process means that only a few of the many possible feature values are specified in any instance; moreover, the set of features that is present is strongly correlated with the correct diagnosis <ref> [RGH94] </ref>. A practical approach to knowledge base maintenance must deal with both of these issues. In the next section we explain the structure of a fault hierarchy, describe a procedure that uses the hierarchy to perform diagnosis, and introduce the 2 in a CT scanner.
Reference: [SF86] <author> J.C. Schlimmer and D. Fisher. </author> <title> A case study of incremental concept in duction. </title> <booktitle> In AAAI-86, </booktitle> <pages> pages 496-501, </pages> <year> 1986. </year>
Reference-contexts: The bias toward modifying the initial domain theory as little as possible, implemented in by a preference ordering on transformations, can also be found in [DRC89]. One can also view incremental methods for the induction of decision trees <ref> [SF86, Utg89] </ref> as carrying out a form of theory revision. These approaches perform a hill-climbing search using the current knowledge base as the starting point. The standard operators here (e.g., extending the decision tree, pruning the tree, and reversing the order of two tests) are different from the operators uses.
Reference: [TS94] <author> G. G. Towell and J. W. Shavlik. </author> <title> Knowledge-based artificial neural net works. </title> <journal> Artificial Intelligence, </journal> <volume> 70 </volume> <pages> 119-165, </pages> <year> 1994. </year>
Reference-contexts: its behavior will be robust even when given a large fault hierarchy that contains many errors and training instances that omit the values of many features. 4 Discussion 4.1 Related Research on Theory Revision The approach we have taken with has close connections with other work on theory revision, including <ref> [GWP88, OM90, CS90, TS94, Cai91, BM91, WP93, Ask94] </ref>; these methods also start from an initial domain theory obtained from experts, and iteratively modify that theory to obtain a theory with improved accuracy on a set of training cases.
Reference: [Tur95] <author> Peter D. Turney. </author> <title> Cost-sensitive classification: Empirical evaluation of a hybrid genetic decision tree induction algorithm. </title> <journal> Journal of Artificial Intelligence Research, </journal> <note> (accepted subject to revision), </note> <year> 1995. </year>
Reference-contexts: scales well as the distance between the initial and target 9 Note that, in general, this "preference" is a function of many variables, including time and equipment needed to perform a test, risk of causing damage by testing, reliability of the test result, and information gain of the test; see <ref> [Tur95] </ref>. 14 hierarchy increases.
Reference: [Utg89] <author> P.E. Utgoff. </author> <title> Incremental induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 4 </volume> <pages> 161-186, </pages> <year> 1989. </year>
Reference-contexts: The bias toward modifying the initial domain theory as little as possible, implemented in by a preference ordering on transformations, can also be found in [DRC89]. One can also view incremental methods for the induction of decision trees <ref> [SF86, Utg89] </ref> as carrying out a form of theory revision. These approaches perform a hill-climbing search using the current knowledge base as the starting point. The standard operators here (e.g., extending the decision tree, pruning the tree, and reversing the order of two tests) are different from the operators uses.
Reference: [WP93] <author> J. Wogulis and M. Pazzani. </author> <title> A methodology for evaluating theory revision systems: Results with Audrey II. </title> <booktitle> In IJCAI-93, </booktitle> <pages> pages 1128-1134, </pages> <year> 1993. </year> <month> 15 </month>
Reference-contexts: its behavior will be robust even when given a large fault hierarchy that contains many errors and training instances that omit the values of many features. 4 Discussion 4.1 Related Research on Theory Revision The approach we have taken with has close connections with other work on theory revision, including <ref> [GWP88, OM90, CS90, TS94, Cai91, BM91, WP93, Ask94] </ref>; these methods also start from an initial domain theory obtained from experts, and iteratively modify that theory to obtain a theory with improved accuracy on a set of training cases.
References-found: 15

