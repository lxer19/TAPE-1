URL: http://www.cs.Helsinki.fi/research/cosco/Articles/nwga96.ps.gz
Refering-URL: 
Root-URL: 
Title: Chapter 4 Empirical comparison of stochastic algorithms Empirical comparison of stochastic algorithms in a graph
Author: Jussi Lahtinen, Petri Myllymaki, Tomi Silander, and Henry Tirri 
Keyword: stochastic algorithms, empirical comparisons, graph optimiza tion  
Address: P.O.Box  FIN-00014 University of Helsinki, Finland  
Affiliation: Complex Systems Computation Group (CoSCo)  Department of Computer Science  
Note: Pp. 45-59 in Proceedings of the Second Nordic Workshop on Genetic Algorithms and their Applications (Vaasa, Finland,  edited by J. Alander. University of Vaasa and the Finnish Artificial Intelligence Society, Vaasa 1996.  The complete title of this article:  
Email: E-mail: Firstname.Lastname@cs.Helsinki.FI  
Phone: 26,  
Date: August 1996),  
Abstract: There are several stochastic methods that can be used for solving NP-hard optimization problems approximatively. Examples of such algorithms include (in order of increasing computational complexity) stochastic greedy search methods, simulated annealing, and genetic algorithms. We investigate which of these methods is likely to give best performance in practice, with respect to the computational effort each requires. We study this problem empirically by selecting a set of stochastic algorithms with varying computational complexity, and by experimentally evaluating for each method how the goodness of the results achieved improves with increasing computational time. For the evaluation, we use a graph optimization problem, which is closely related to several real-world practical problems. To get a wider perspective of the goodness of the achieved results, the stochastic methods are also compared against special-case greedy heuristics. This investigation suggests that although genetic algorithms can provide good results, simpler stochastic algorithms can achieve similar performance more quickly. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Aarts and J. Korst. </author> <title> Simulated Annealing and Boltzmann Machines: A Stochastic Approach to Combinatorial Optimization and Neural Computing. </title> <publisher> John Wiley & Sons, </publisher> <address> Chichester, </address> <year> 1989. </year>
Reference-contexts: In this work we are interested in the wide class of stochastic optimization methods especially such method families as simulated annealing <ref> [1] </ref> and genetic algorithms [4, 2]. <p> Unfortunately, this interesting theoretical result requires an exponential number of iterations, but empirical results show that near-optimal minima can be obtained rather easily with a polynomial number of iterations (see e.g. <ref> [10, 1] </ref>). It should also be noted, that the formulation given above, which was introduced in [9], is not the only possible way to select the generating and acceptance probabilities | for alternative solutions, see e.g. [10]. Algorithm 4.2.2 Genetic algorithm (GA) 1.
Reference: [2] <author> J. Alander. </author> <title> An indexed bibliography of genetic algorithms. </title> <editor> In L. Chambers, editor, </editor> <booktitle> Practical Handbook of Genetic Algorithms: New Frontiers, chapter Appendix 1, </booktitle> <pages> pages 333-427. </pages> <publisher> CRC Press, </publisher> <address> Boca Raton, FL, </address> <year> 1995. </year>
Reference-contexts: In this work we are interested in the wide class of stochastic optimization methods especially such method families as simulated annealing [1] and genetic algorithms <ref> [4, 2] </ref>. <p> In genetic algorithms, the state of the search process at time t consists of a set (population) of configuration vectors (chromosomes). There are many variations of genetic algorithms (see e.g. <ref> [4, 2] </ref>). We have experimented with several different versions of GA, but the results reported in this paper are obtained by using Lahtinen et al: Empirical comparison of stochastic algorithms . 51 the generic form of Algorithm 4.2.2.
Reference: [3] <author> J.L. Casti. Complexification: </author> <title> Explaining a Paradoxical World through the Science of Surprise. </title> <address> HarperCollins, New York, NY, </address> <year> 1994. </year>
Reference-contexts: 4.1 Introduction Real-world phenomena are complex, as indicated by increased interest in the "Science of complexity" <ref> [3] </ref>. It is also evident that the computational problems encountered in real environments differ from text-book optimization problems, which in many cases only serve as tractable examples to illustrate the nature of problems.
Reference: [4] <author> L. Davis. </author> <title> Handbook of genetic algorithms. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, NY, </address> <year> 1991. </year>
Reference-contexts: In this work we are interested in the wide class of stochastic optimization methods especially such method families as simulated annealing [1] and genetic algorithms <ref> [4, 2] </ref>. <p> In genetic algorithms, the state of the search process at time t consists of a set (population) of configuration vectors (chromosomes). There are many variations of genetic algorithms (see e.g. <ref> [4, 2] </ref>). We have experimented with several different versions of GA, but the results reported in this paper are obtained by using Lahtinen et al: Empirical comparison of stochastic algorithms . 51 the generic form of Algorithm 4.2.2.
Reference: [5] <author> S. Geman and D. Geman. </author> <title> Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6 </volume> <pages> 721-741, </pages> <year> 1984. </year>
Reference-contexts: With large T , the simulated annealing process resembles random walk, and as T approaches zero, the process starts to behave like the greedy search. It can be shown <ref> [5] </ref> that if the temperature is decreased slowly enough (and the probabilities G ij and A ij fulfill certain requirements), the SA process will converge to the global minimum with probability one. <p> Since convergence of the simulated annealing algorithms is guaranteed if the generating probabilities are symmetric, i.e., G ij = G ji (see e.g. <ref> [5] </ref>), we wish our tree neighborhood relation to be symmetric too.
Reference: [6] <author> L. Ingber and B. Rosen. </author> <title> Genetic algorithms and very fast simulated re-annealing: a comparison. </title> <journal> Mathematical and Computer Modelling, </journal> <volume> 16(11) </volume> <pages> 87-100, </pages> <year> 1992. </year>
Reference-contexts: The results suggest that although it is possible to get good results with genetic algorithms, a similar performance can be achieved more quickly by using stochastic algorithms with simpler structures. Similar results has been Lahtinen et al: Empirical comparison of stochastic algorithms . 59 obtained earlier in <ref> [12, 7, 6] </ref>. In the future, we will continue to investigate this question and extend our ongoing experimentation in different problem domains. Acknowledgments This research has been supported by the Technology De velopment Center (TEKES) and Nokia Research Center.
Reference: [7] <author> A. Juels and M. Wattenberg. </author> <title> Stochastic hillclimbing as a baseline method for evaluating genetic algorithms. </title> <type> Technical Report CSD-94-834, </type> <institution> Department of Computer Science, University of California at Berkeley, </institution> <year> 1994. </year>
Reference-contexts: The results suggest that although it is possible to get good results with genetic algorithms, a similar performance can be achieved more quickly by using stochastic algorithms with simpler structures. Similar results has been Lahtinen et al: Empirical comparison of stochastic algorithms . 59 obtained earlier in <ref> [12, 7, 6] </ref>. In the future, we will continue to investigate this question and extend our ongoing experimentation in different problem domains. Acknowledgments This research has been supported by the Technology De velopment Center (TEKES) and Nokia Research Center.
Reference: [8] <author> J.B. Kruskal. </author> <title> On the shortest spanning subtree of a graph and the travelling salesman problem. </title> <journal> Proc. Amer. Math. Soc., </journal> <volume> 7(1) </volume> <pages> 48-50, </pages> <year> 1956. </year>
Reference-contexts: The best results achieved were obtained with the algorithm instances shown below. To get a wider perspective of the results achieved, we also used Kruskal's deterministic algorithm <ref> [8] </ref> for generating a minimum spanning tree (MST) solution with respect to Euclidean metric of connecting arcs.
Reference: [9] <author> N. Metropolis, A.W. Rosenbluth, M.N. Rosenbluth, M.N. Teller, and E. Teller. </author> <title> Equations of state calculations by fast computing machines. </title> <journal> Journal of Chem. Phys., </journal> <volume> 21 </volume> <pages> 1087-1092, </pages> <year> 1953. </year>
Reference-contexts: Unfortunately, this interesting theoretical result requires an exponential number of iterations, but empirical results show that near-optimal minima can be obtained rather easily with a polynomial number of iterations (see e.g. [10, 1]). It should also be noted, that the formulation given above, which was introduced in <ref> [9] </ref>, is not the only possible way to select the generating and acceptance probabilities | for alternative solutions, see e.g. [10]. Algorithm 4.2.2 Genetic algorithm (GA) 1. Set crossover probability p c and mutation probability p m ; 2.
Reference: [10] <author> P. Myllymaki. </author> <title> Mapping Bayesian Networks to Stochastic Neural Networks: A Foundation for Hybrid Bayesian-Neural Systems. </title> <type> PhD thesis, Report A-1995-1, </type> <institution> Department of Computer Science, University of Helsinki, </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: Unfortunately, this interesting theoretical result requires an exponential number of iterations, but empirical results show that near-optimal minima can be obtained rather easily with a polynomial number of iterations (see e.g. <ref> [10, 1] </ref>). It should also be noted, that the formulation given above, which was introduced in [9], is not the only possible way to select the generating and acceptance probabilities | for alternative solutions, see e.g. [10]. Algorithm 4.2.2 Genetic algorithm (GA) 1. <p> It should also be noted, that the formulation given above, which was introduced in [9], is not the only possible way to select the generating and acceptance probabilities | for alternative solutions, see e.g. <ref> [10] </ref>. Algorithm 4.2.2 Genetic algorithm (GA) 1. Set crossover probability p c and mutation probability p m ; 2. Initialize a population P , consisting of m randomly chosen config uration vectors ~x 1 ; : : : ; ~x m ; 3.
Reference: [11] <author> C. Palmer and A. Kershenbaum. </author> <title> An approach to a problem in network design using genetic algorithms. </title> <journal> Networks, </journal> <volume> 26 </volume> <pages> 151-163, </pages> <year> 1995. </year>
Reference-contexts: Genetic algorithm (GA) Genetic algorithms require the trees (the solutions) to be coded as chromosomes. In this work, we use the following coding scheme, which is a modification of the methods suggested in <ref> [11] </ref>. Each tree is represented as a real vector ~c of length 2 , with one real-valued component for each pair of nodes in V .
Reference: [12] <author> K. Park and B. Carter. </author> <title> On the effectiveness of genetic search in combinatorial optimization. </title> <type> Technical Report BU-CS-94-010, </type> <institution> Computer Science Department, Boston University, </institution> <year> 1994. </year> <month> 311 </month>
Reference-contexts: The results suggest that although it is possible to get good results with genetic algorithms, a similar performance can be achieved more quickly by using stochastic algorithms with simpler structures. Similar results has been Lahtinen et al: Empirical comparison of stochastic algorithms . 59 obtained earlier in <ref> [12, 7, 6] </ref>. In the future, we will continue to investigate this question and extend our ongoing experimentation in different problem domains. Acknowledgments This research has been supported by the Technology De velopment Center (TEKES) and Nokia Research Center.
References-found: 12

