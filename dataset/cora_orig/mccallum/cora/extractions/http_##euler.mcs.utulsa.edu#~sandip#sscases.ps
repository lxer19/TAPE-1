URL: http://euler.mcs.utulsa.edu/~sandip/sscases.ps
Refering-URL: http://euler.mcs.utulsa.edu/~sandip/DAI.html
Root-URL: 
Email: e-mail: [haynes,laukit,sandip]@euler.mcs.utulsa.edu  
Title: Learning Cases to Compliment Rules for Conflict Resolution in Multiagent Systems  
Author: Thomas Haynes, Kit Lau Sandip Sen 
Address: Tulsa  
Affiliation: Department of Mathematical Computer Sciences, The University of  
Abstract: Groups of agents following fixed behavioral rules can be limited in performance and efficiency. Adaptability and flexibility are key components of intelligent behavior which allow agent groups to improve performance in a given domain using prior problem solving experience. We motivate the usefulness of individual learning by group members in the context of overall group behavior. We propose a framework in which individual group members learn cases to improve their model of other group members. We utilize a testbed problem from the distributed AI literature to show that simultaneous learning by group members can lead to significant improvement in group performance and efficiency over groups following static behavioral rules. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Golding, A. R., and Rosenbloom, P. S. </author> <year> 1991. </year> <title> Improving rule-based systems through case-based reasoning. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> 22-27. </pages>
Reference-contexts: Our cases are negative in the sense they tell the agents what not to do. A positive case would tell the agent what to do in a certain situation <ref> (Golding & Rosenbloom 1991) </ref>.
Reference: <author> Haynes, T.; Sen, S.; Schoenefeld, D.; and Wainwright, R. </author> <year> 1995a. </year> <title> Evolving multiagent coordination strategies with genetic programming. </title> <journal> Artificial Intelligence. </journal> <note> (submitted for review). </note>
Reference-contexts: The goal of the four predator agents is to try to capture a prey agent. Agents can only make orthogonal moves on a grid world. We have reported the varying degrees of success of predator agents in capturing a prey against a variety of prey movement algorithms <ref> (Haynes et al. 1995a) </ref>. Of particular interest were the algorithms which caused the prey to pick a direction and always move along it (Linear) and the one in which it did not move at all (Still).
Reference: <author> Haynes, T.; Wainwright, R.; Sen, S.; and Schoene-feld, D. </author> <year> 1995b. </year> <title> Strongly typed genetic programming in evolving cooperation strategies. </title> <editor> In Eshelman, L., ed., </editor> <booktitle> Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <pages> 271-278. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference: <author> Kolodner, J. L. </author> <year> 1993. </year> <title> Case-Based Reasoning. </title> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: This explains the surprising lack of captures for a prey which does not move. The question that arises from these findings is how do the agents manage conflict resolution? An answer can be found in the ways we as humans manage conflict resolution: with cases <ref> (Kolodner 1993) </ref>.
Reference: <author> Korf, R. E. </author> <year> 1992. </year> <title> A simple solution to pursuit games. </title> <booktitle> In Working Papers of the 11th International Workshop on Distributed Artificial Intelligence, </booktitle> <pages> 183-194. </pages>
Reference-contexts: The algorithms examine the metrics from the set of possible moves, fN,E,S,W,Hg, and select a move corresponding to the minimal distance. All ties are randomly broken. The original MN algorithm, as described by Korf <ref> (Korf 1992) </ref>, does not allow the predators to move to the cell occupied by the prey. (In his research, the prey moves first, followed by the predators in order. Thus conflicts are resolved between predators and prey by serialization.) Figure 1 illustrates a problem with this restriction.
Reference: <author> Sen, S.; Sekaran, M.; and Hale, J. </author> <year> 1994. </year> <title> Learning to coordinate without sharing information. </title> <booktitle> In Proceedings of the Twelth National Conference on Artificial Intelligence, </booktitle> <pages> 426-431. </pages>
Reference: <author> Stephens, L. M., and Merx, M. B. </author> <year> 1990. </year> <title> The effect of agent control strategy on the performance of a DAI pursuit problem. </title> <booktitle> In Proceedings of the 1990 Distributed AI Workshop. </booktitle> <pages> 6 </pages>
References-found: 7

