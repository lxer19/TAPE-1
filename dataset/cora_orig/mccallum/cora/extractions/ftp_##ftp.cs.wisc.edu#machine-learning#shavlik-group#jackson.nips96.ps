URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/jackson.nips96.ps
Refering-URL: http://www.cs.wisc.edu/~shavlik/abstracts/jackson.nips96.ps.abstract.html
Root-URL: 
Email: jackson@mathcs.duq.edu  craven@cs.wisc.edu  
Title: Learning Sparse Perceptrons  
Author: Jeffrey C. Jackson Mark W. Craven 
Address: 600 Forbes Ave Pittsburgh, PA 15282  1210 West Dayton St. Madison, WI 53706  
Affiliation: Mathematics Computer Science Dept. Duquesne University  Computer Sciences Dept. University of Wisconsin-Madison  
Note: Appears in Advances in Neural Information Processing Systems, Vol. 8. MIT Press, Cambridge, MA, 1996.  
Abstract: We introduce a new algorithm designed to learn sparse perceptrons over input representations which include high-order features. Our algorithm, which is based on a hypothesis-boosting method, is able to PAC-learn a relatively natural class of target concepts. Moreover, the algorithm appears to work well in practice: on a set of three problem domains, the algorithm produces classifiers that utilize small numbers of features yet exhibit good generalization performance. Perhaps most importantly, our algorithm generates concept descriptions that are easy for humans to understand.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aslam, J. A. & Decatur, S. E. </author> <year> (1993). </year> <title> General bounds on statistical query learning and PAC learning with noise via hypothesis boosting. </title> <booktitle> In Proc. of the 34th Annual Annual Symposium on Foundations of Computer Science, </booktitle> <pages> (pp. 282-291). </pages>
Reference: <author> Bruck, J. </author> <year> (1990). </year> <title> Harmonic analysis of polynomial threshold functions. </title> <journal> SIAM Journal of Discrete Mathematics, </journal> <volume> 3(2) </volume> <pages> 168-177. </pages>
Reference: <author> Craven, M. W. & Shavlik, J. W. </author> <year> (1993). </year> <title> Learning to represent codons: A challenge problem for constructive induction. </title> <booktitle> In Proc. of the 13th International Joint Conf. on Artificial Intelligence, </booktitle> <pages> (pp. 1319-1324), </pages> <address> Chambery, France. </address>
Reference-contexts: fl Table 2: Hypothesis complexity (# weights). perceptrons domain boosting multi-layer ordinary 2nd-order pruned voting 12 651 30 450 12 promoters 41 2267 228 25764 59 protein coding 52 4270 60 1740 37 UC-Irvine one; and a data set in which the task is to recognize protein-coding regions in DNA <ref> (Craven & Shavlik, 1993) </ref>. We remove the physician-fee-freeze feature from the voting data set to make the problem more difficult. We conduct our experiments using a 10-fold cross validation methodology, except for in the protein-coding domain. <p> The input representation for this problem consists of 15 nominal features representing 15 consecutive bases in a DNA sequence. In the regions of DNA that encode proteins (the positive examples in our task), non-overlapping triplets of consecu tive bases represent meaningful "words" called codons. In previous work <ref> (Craven & Shavlik, 1993) </ref>, it has been found that a feature set that explicitly represents codons results in better generalization than a representation of just bases. However, we used the bases representation in our experiments in order to investigate the ability of our algorithm to select the "right" second-order features.
Reference: <author> Freund, Y. </author> <year> (1993). </year> <title> Data Filtering and Distribution Modeling Algorithms for Machine Learning. </title> <type> PhD thesis, </type> <institution> University of California at Santa Cruz. </institution>
Reference-contexts: This claim is based on results of Aslam and Decatur (1993), who present a noise-tolerant boosting algorithm. In fact, several different boosting algorithms could be used to learn P k;s <ref> (e.g., Freund, 1993) </ref>. We have chosen to use AdaBoost because it seems to offer significant practical advantages, particularly in terms of efficiency. Also, our empirical results to date indicate that our algorithm works very well on difficult (presumably "noisy") real-world problems.
Reference: <author> Freund, Y. & Schapire, R. E. </author> <year> (1995). </year> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <booktitle> In Proc. of the 2nd Annual European Conf. on Computational Learning Theory. </booktitle>
Reference-contexts: We present an approach to learning understandable, yet accurate, classifiers. Specifically, our algorithm constructs sparse perceptrons, i.e., single-layer perceptrons that have relatively few non-zero weights. Our algorithm for learning sparse perceptrons is based on a new hypothesis boosting algorithm <ref> (Freund & Schapire, 1995) </ref>. Although our algorithm was initially developed from a learning-theoretic point of view and retains certain theoretical guarantees (it PAC-learns the class of sparse perceptrons), it also works well in practice. <p> Our algorithm for finding sparse perceptrons is, as indicated earlier, based on the notion of hypothesis boosting. The specific boosting algorithm we use (Figure 1) is a version of the recent AdaBoost algorithm <ref> (Freund & Schapire, 1995) </ref>. In the next section we apply AdaBoost to "boost" a weak learning algorithm for P k;s into a strong learner for P k;s . <p> If the weak learner succeeds in producing a ( 1 2 fl)-approximator at each stage then AdaBoost's final hypothesis is guaranteed to be consistent with the training set <ref> (Freund & Schapire, 1995) </ref>. 3.2 PAC-Learning Sparse k-Perceptrons We now show that sparse k-perceptrons are PAC learnable by real-weighted k-perceptrons having relatively few nonzero weights. Specifically, ignoring log factors, P k;s is learnable by P R k;O (s 2 ) for any constant k.
Reference: <author> Goldmann, M., Hastad, J., & Razborov, A. </author> <year> (1992). </year> <title> Majority gates vs. general weighted threshold gates. </title> <booktitle> In Proc. of the 7th IEEE Conf. on Structure in Complexity Theory. </booktitle>
Reference-contexts: Our proof is implicit in somewhat more general work by Freund (1993), although he did not actually present a learning algorithm for this class or analyze the sample size needed to ensure *-approximation, as we do. Following Freund, we begin our development with the following lemma <ref> (Goldmann et al., 1992) </ref>: Lemma 1 (Goldmann Hastad Razborov) For f : f0; 1g n ! f1; +1g and H, any set of functions with the same domain and range, if f can be represented as f (x) = sign ( P s i=1 h i (x)), where h i 2
Reference: <author> Haussler, D. </author> <year> (1988). </year> <title> Quantifying inductive bias: AI learning algorithms and Valiant's learning framework. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> (pp. 177-221). </pages>
Reference-contexts: We can convert this consistency algorithm to a PAC learning algorithm as follows. First, given a finite set of functions F , it is straightforward to show the following <ref> (see, e.g., Haussler, 1988) </ref>: Lemma 2 Let F be a finite set of functions over a domain X.
Reference: <author> Le Cun, Y., Denker, J. S., & Solla, S. A. </author> <year> (1989). </year> <title> Optimal brain damage. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 2). </booktitle>
Reference-contexts: To test the hypothesis that the performance of our algorithm can be explained by its use of relatively few weights, we consider ordinary perceptrons which have been pruned using a variant of the Optimal Brain Damage (OBD) algorithm <ref> (Le Cun et al., 1989) </ref>. In our version of OBD, we train a perceptron until the stopping criteria are met, prune the weight with the smallest salience, and then iterate the process. We use a validation set to decide when to stop pruning weights.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Our experiments in a number of real-world domains indicate that our algorithm produces perceptrons that are relatively comprehensible, and that exhibit generalization performance comparable to that of backprop-trained MLP's (Rumelhart et al., 1986) and better than decision trees learned using C4.5 <ref> (Quinlan, 1993) </ref>. We contend that sparse perceptrons, unlike MLP's, are comprehensible because they have relatively few parameters, and each parameter describes a simple (i.e. linear) relationship. <p> We compare our algorithm to ordinary perceptrons trained using backpropagation (Rumelhart et al., 1986), multi-layer perceptrons trained using backpropagation, and decision trees induced using the C4.5 system <ref> (Quinlan, 1993) </ref>. We use C4.5 in our experiments as a representative of "symbolic" learning algorithms. Symbolic algorithms are widely believed to learn hypotheses that are more comprehensible than neural networks.
Reference: <author> Rumelhart, D., Hinton, G., & Williams, R. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. & McClelland, J., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the microstructure of cognition. </booktitle> <volume> Volume 1. </volume> <publisher> MIT Press. </publisher>
Reference-contexts: Our experiments in a number of real-world domains indicate that our algorithm produces perceptrons that are relatively comprehensible, and that exhibit generalization performance comparable to that of backprop-trained MLP's <ref> (Rumelhart et al., 1986) </ref> and better than decision trees learned using C4.5 (Quinlan, 1993). We contend that sparse perceptrons, unlike MLP's, are comprehensible because they have relatively few parameters, and each parameter describes a simple (i.e. linear) relationship. <p> We compare our algorithm to ordinary perceptrons trained using backpropagation <ref> (Rumelhart et al., 1986) </ref>, multi-layer perceptrons trained using backpropagation, and decision trees induced using the C4.5 system (Quinlan, 1993). We use C4.5 in our experiments as a representative of "symbolic" learning algorithms. Symbolic algorithms are widely believed to learn hypotheses that are more comprehensible than neural networks.
Reference: <author> Spackman, K. A. </author> <year> (1988). </year> <title> Learning categorical decision criteria. </title> <booktitle> In Proc. of the 5th International Conf. on Machine Learning, </booktitle> <pages> (pp. 36-46), </pages> <address> Ann Arbor, MI. </address>
Reference-contexts: We contend that sparse perceptrons, unlike MLP's, are comprehensible because they have relatively few parameters, and each parameter describes a simple (i.e. linear) relationship. As evidence that sparse perceptrons are comprehensible, consider that such linear functions are commonly used to express domain knowledge in fields such as medicine <ref> (Spackman, 1988) </ref> and molecular biology (Stormo, 1987). 2 Sparse Perceptrons A perceptron is a weighted threshold over the set of input features and over higher-order features consisting of functions operating on only a limited number of the input features.
Reference: <author> Stormo, G. </author> <year> (1987). </year> <title> Identifying coding sequences. </title> <editor> In Bishop, M. J. & Rawlings, C. J., editors, </editor> <title> Nucleic Acid and Protein Sequence Analysis: A Practical Approach. </title> <publisher> IRL Press. </publisher>
Reference-contexts: As evidence that sparse perceptrons are comprehensible, consider that such linear functions are commonly used to express domain knowledge in fields such as medicine (Spackman, 1988) and molecular biology <ref> (Stormo, 1987) </ref>. 2 Sparse Perceptrons A perceptron is a weighted threshold over the set of input features and over higher-order features consisting of functions operating on only a limited number of the input features. Informally, a sparse perceptron is any perceptron that has relatively few non-zero weights.
Reference: <author> Valiant, L. G. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Comm. of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142. </pages>
References-found: 13

