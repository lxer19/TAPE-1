URL: http://www.ics.uci.edu/~pazzani/Publications/ali-pazzani-tai-journal.ps
Refering-URL: http://www.ics.uci.edu/~mlearn/MLPapers.html
Root-URL: 
Email: fali,pazzanig@ics.uci.edu  
Title: HYDRA-MM: Learning Multiple Descriptions to Improve Classification Accuracy  
Author: Kamal Ali Michael Pazzani 
Address: Irvine, CA, 92717  
Affiliation: Department of Information and Computer Science, University of California,  
Abstract: For learning tasks with few examples, greater classification accuracy can be achieved by learning several concept descriptions for each class in the data and producing a classification that combines evidence from multiple descriptions. Stochastic (randomized) search can be used to generate many concept descriptions for each class. Here we use a tractable approximation to the optimal Bayesian method for combining evidence from multiple descriptions. Learning multiple descriptions is very useful when additional data is difficult to obtain. The primary result of this paper is that multiple concept descriptions are particularly helpful for improving accuracy in hypothesis spaces in which there are many equally good rules to learn. Another result is experimental evidence that learning multiple rule sets yields more accurate classifications than learning multiple rules for concepts containing many disjuncts.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Ali and M. Pazzani, </author> <title> "Reducing the small disjuncts problem by learning probabilistic concept descriptions," </title> <editor> in T. Petsche, S. Judd and S. Hanson (ed.s), </editor> <booktitle> Computational Learning Theory and Natural Learning Systems, </booktitle> <volume> Vol. 3, </volume> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference: [2] <author> K. Ali and M. Pazzani, "HYDRA: </author> <title> A Noise-tolerant Relational Concept Learning Algorithm," </title> <booktitle> Proc. 13th International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Mor-gan Kaufmann Press, </publisher> <address> Chambery, France, </address> <pages> pp. 1064-1070, </pages> <year> 1993. </year>
Reference-contexts: HYDRA differs from FOCL in three important ways. 1. HYDRA learns a set of rules for each class so that each set can compete to classify test examples. In <ref> [2] </ref> we show that this allows HYDRA to learn more accurate descriptions from noisy data. 2. HYDRA attaches the degree of logical sufficiency (LS a measure of classifi cation reliability) to each rule. 3.
Reference: [3] <author> K. Ali and M. Pazzani, </author> <title> "On Learning Multiple Descriptions of a Concept," </title> <booktitle> Proc. on Sixth International Conference on Tools with Artificial Intelligence, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 476-483. </pages> <year> 1994. </year>
Reference: [4] <author> K. Ali and M. Pazzani, </author> <title> "Error Reduction through Learning Multiple Descriptions", </title> <note> submitted to Machine Learning. </note>
Reference: [5] <author> W. Buntine, </author> <title> A Theory of Learning Classification Rules, </title> <type> doctoral dissertation, </type> <institution> School of Computing Science, University of Technology, </institution> <address> Sydney, Australia, </address> <year> 1990. </year>
Reference-contexts: Although previous work has shown that learning multiple concept descriptions increases accuracy for some kinds of concept descriptions (e.g. decision trees, <ref> [5] </ref>) there has been no prior work in learning multiple concept descriptions where each description is a rule set or in learning multiple concept descriptions for relational concepts.
Reference: [6] <author> P. Clark and R. Boswell, </author> <title> "Rule Induction with CN2: Some Recent Improvements," </title> <booktitle> Proceedings of the European Working Session on Learning, </booktitle> <year> 1991. </year>
Reference: [7] <author> R. Duda, J. Gaschnig and Hart P, </author> <title> "Model design in the Prospector consultant system for mineral exploration", </title> <editor> in D. Michie (ed.), </editor> <booktitle> Expert Systems in the Micro-electronic Age, </booktitle> <publisher> Edinburgh University Press, </publisher> <year> 1979. </year> <month> 21 </month>
Reference-contexts: Rules that are not completely sufficient are modeled in HYDRA by attaching a degree of logical sufficiency (LS, <ref> [7] </ref>) to each rule. The primary goal of this research is to characterize the conditions under which learning multiple descriptions is beneficial to accuracy. <p> Briefly, we will use the odds form of Bayes rule because it is consistent with degree of logical sufficiency (LS, <ref> [7] </ref>), the type of uncertainty measure used in HYDRA. The degree of logical sufficiency for a class C is also called an odds multiplier because it is multiplied by the prior odds of C to produce the posterior odds of C.
Reference: [8] <author> S. Dzeroski and I. Bratko, </author> <title> "Handling noise in Inductive Logic Programming," </title> <booktitle> Proc. International Workshop on Inductive Logic Programming, </booktitle> <publisher> ICOT, </publisher> <address> Tokyo, Japan, </address> <year> 1992. </year>
Reference: [9] <author> F. Esposito, D. Malerba, G. Semeraro and M. Pazzani M, </author> <title> "A machine learning approach to document understanding," </title> <booktitle> Proc. 2nd International Workshop on Multi-strategy Learning., </booktitle> <address> Harpers Ferry, WV., </address> <year> 1993. </year>
Reference: [10] <author> U. Fayyad and K. Irani, </author> <title> "The Attribute Selection Problem in Decision Tree Generation," </title> <booktitle> Proc. 10th National Conference on Artificial Intelligence, </booktitle> <publisher> AAAI Press, </publisher> <address> San Jose, CA., </address> <pages> pp. 104-110, </pages> <year> 1992. </year>
Reference: [11] <author> M. </author> <title> Gams, "New Measurements Highlight the Importance of Redundant Knowledge," </title> <booktitle> Proc. 4th European Working Session on Learning, </booktitle> <publisher> Pitman Press, </publisher> <address> Montpeiller, France, </address> <year> 1989. </year>
Reference: [12] <author> I. Kononenko and M. </author> <title> Kovacic M, "Learning as Optimization: Stochastic Generation of Multiple Knowledge," </title> <booktitle> Proc. 9th International Workshop on Machine Learning, </booktitle> <publisher> Morgan Kaufmann Press, Aberdeen, UK, </publisher> <pages> pp. 257-262, </pages> <year> 1992. </year>
Reference-contexts: In practice, however, it is only possible to use a small set of descriptions (hypotheses) so we aim to find the N most probable descriptions. The third goal of this research is to show that using multiple rule sets is a better approach than using multiple rules (figure 2, <ref> [12] </ref>). These approaches differ in that the multiple rules approach tries to model each class with a single, conjunctive rule. However, there are many classes that cannot be accurately modeled with just a single rule with respect to the given set of background concepts.
Reference: [13] <author> S. Kwok and C. Carter, </author> <title> "Multiple decision trees," </title> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <volume> Vol. 4, </volume> <year> 1990, </year> <pages> pp. 327-335, </pages> <year> 1990. </year>
Reference: [14] <author> S. Muggleton, M. Bain, J. Hayes-Michie and D. Michie, </author> <title> "An experimental comparison of human and machine-learning formalisms," </title> <booktitle> Proc. 6th International Workshop on Machine Learning, </booktitle> <publisher> Morgan Kaufmann Press, </publisher> <address> Ithaca, NY., </address> <pages> pp. 113-118, </pages> <year> 1989. </year>
Reference-contexts: We present results on the following relational problems: predicting finite-element mesh granularity ([8]), learning the concept of "illegality" in the King-Rook-King domain (KRK, <ref> [14] </ref>), deciding whether a part of an optically-scanned document contains the date of the document ([9]) and predicting whether a person is required to make payments on their student loan ([16]).
Reference: [15] <author> S. Muggleton, A. Srinivasan and Bain M, </author> <title> "Compression, Significance and Accuracy," </title> <booktitle> Proc. 9th International Workshop on Machine Learning, </booktitle> <publisher> Morgan Kaufmann Press, Aberdeen, Scotland, </publisher> <pages> pp. 338-347, </pages> <year> 1992. </year>
Reference: [16] <author> M. Pazzani and C. Brunk, </author> <title> "Detecting and correcting errors in rule-based expert systems: an integration of empirical and explanation-based learning," </title> <journal> Knowledge Acquisition, </journal> <volume> Vol. 3, </volume> <year> 1991, </year> <pages> pp. 157-173. </pages>
Reference: [17] <author> M. Pazzani and D. Kibler, </author> <title> "The utility of knowledge in inductive learning," </title> <journal> Machine Learning, </journal> <volume> Vol. 9, No. 1, </volume> <year> 1991, </year> <pages> pp. 57-94. </pages>
Reference: [18] <author> R. Quinlan, </author> <title> "Learning logical definitions from relations," </title> <journal> Machine Learning, </journal> <volume> Vol. 5, No. 3, </volume> <year> 1990. </year>
Reference-contexts: Figure 3 illustrates a concept containing a major disjunct (large dark circle) and a minor disjunct (small dark circle). Light lines indicate the coverage of learned rules that try to approximate the underlying disjuncts. The leftmost figure illustrates what is learned using a separate and conquer technique (e.g. FOIL, <ref> [18] </ref>) which learns an approximation 5 Single Multiple Multiple Model Rules Rulesets inside dark circles) consists of two disjuncts (dark circles). The area outside the dark circles corresponds to the other class.
Reference: [19] <author> P. Smyth and R. Goodman, </author> <title> "Rule Induction Using Information Theory," </title> <editor> in G. Piatetsky-Shapiro (ed.) </editor> <title> Knowledge Discovery in Databases, </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference: [20] <author> G. Towell, J. Shavlik and M. Noordewier, </author> <title> "Refinement of Approximate Domain Theories by Knowledge-Based Artificial Neural Networks," </title> <booktitle> Proc. 8th National Conference on Artificial Intelligence, </booktitle> <publisher> AAAI Press, </publisher> <address> Boston, MA., </address> <pages> pp. 861-866, </pages> <year> 1990. </year> <month> 22 </month>
References-found: 20

