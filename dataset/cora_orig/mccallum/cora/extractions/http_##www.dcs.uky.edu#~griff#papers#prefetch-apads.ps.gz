URL: http://www.dcs.uky.edu/~griff/papers/prefetch-apads.ps.gz
Refering-URL: 
Root-URL: 
Title: Automatic Prefetching in a  
Author: WAN James Griffioen Randy Appleton 
Address: Lexington, KY 40506 Lexington, KY 40506  
Affiliation: Department of Computer Science Department of Computer Science University of Kentucky University of Kentucky  
Note: (Appeared in the IEEE Workshop on Advances in Parallel and Distributed Systems, Oct '93)  
Pubnum: Technical Report #CS243-93  
Abstract: File systems have become slow relative to processor, memory and network speeds. The high latencies of distributed file systems (particularly wide area file systems) have only exacerbated the problem. We believe that file system prefetching provides an effective method for alleviating the latency bottleneck. File system prefetching requires knowledge of future file accesses. We are currently investigating efficient methods for accurately predicting future file activity automatically without application intervention. We have designed and implement a system which uses past file activity to predict future file system needs. Our initial results indicate that reasonable accuracy can be obtain even when relatively simple algorithms are employed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Randy Appleton. </author> <title> Automatic File System Prefetching. </title> <type> Technical report, </type> <institution> University of Ken-tucky, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Experience shows that standard caching is unable to capitalize on the availability of very large memories. It has been noted by many people that the relative benefit of caching decreases as cache size (and cache cost) increases <ref> [8, 1] </ref>. For example, measured performance of a standard Unix 1 workstation in our lab shows that approximately 72% of all disk access can be met from a two megabyte cache, but doubling the size of the cache only provides an additional 7% improvement. Further doubling provides even smaller improvements. <p> Further doubling provides even smaller improvements. Moreover, for large cache sizes standard LRU cache replacement is reasonably close to optimal <ref> [1, 2] </ref>. Therefore, we can expect little additional improvement in performance from new cache replacement strategies, since the standard caching techniques already perform at a rate close to optimal (see Figure 1).
Reference: [2] <author> M. Baker, S. Asami, E. Deprit, J. Ousterhout, and M. Seltzer. </author> <title> Non-Volatile Memory for Fast, Reliable File Systems. </title> <booktitle> In Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 10-22, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Disk access times seem unlikely to improve at any significant rate, and the trend toward wide area distributed file systems and mobile (wireless) distributed systems with high latencies will only increase file access times. Traditionally, file caching has been used to partially solve the latency problem <ref> [8, 2] </ref>. However, the performance gains of file caching have already been realized. <p> Further doubling provides even smaller improvements. Moreover, for large cache sizes standard LRU cache replacement is reasonably close to optimal <ref> [1, 2] </ref>. Therefore, we can expect little additional improvement in performance from new cache replacement strategies, since the standard caching techniques already perform at a rate close to optimal (see Figure 1).
Reference: [3] <author> Mary G. Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirriff, and John K. Ouster-hout. </author> <title> Measurements of a distributed file system. </title> <booktitle> In Proceedings of 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 198-212. </pages> <institution> Association for Computing Machinery SIGOPS, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: For larger cache sizes, LRU approaches the optimal hit rate. if it were the first access with latency dominating the overall access time. * Most files are quite small. In fact, measurements of existing distributed file systems show that the average file is only a few kilobytes long <ref> [9, 3] </ref>. For files of this size, transfer time is minimal when compared to the access latency across a WAN.
Reference: [4] <author> D. Kotz and C. Ellis. </author> <title> Prefetching in file systems for MIMD multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1 </volume> <pages> 218-230, </pages> <year> 1990. </year>
Reference: [5] <author> Balachander Krishnamurthy. </author> <title> A Uniform Model of Interaction In Interactive Systems. </title> <type> PhD thesis, </type> <institution> Purdue University, West Lafayette, Indiana, </institution> <month> December </month> <year> 1987. </year>
Reference-contexts: Fortunately our experience and others indicates that users tend to execute the same small set of operations repeatedly <ref> [5] </ref>. As a result, all of our traces have produced probability graphs with surprisingly high accuracy values. Establishing some minimum accuracy requirement is crucial to avoid wasting system resources.
Reference: [6] <author> J. Morris, M. Satyanarayanan, M. Conner, J. Howard, D. Rosenthal, and F. Smith. </author> <note> Andrew: </note>
Reference-contexts: One important reason is the prevalence of distributed and networked file systems, which have an added network latency in addition to standard disk access latency. As distributed systems grow in popularity and geographic size, like those envisioned by the Andrew File System <ref> [6] </ref>, latency and network delays will become the dominant factor in overall file system performance for normal applications. Unfortunately, there seems little hope on the horizon for substantially improved file system performance.
References-found: 6

