URL: http://www-anw.cs.umass.edu/People/singh/Papers/single-sided.ps.Z
Refering-URL: http://www-anw.cs.umass.edu/People/singh/Papers/
Root-URL: 
Email: singh@psyche.mit.edu  vijay@cs.umass.edu  
Title: Asynchronous Modified Policy Iteration with Single-sided Updates  
Author: Satinder P. Singh Vijaykumar Gullapalli 
Keyword: Markov Decision Problems, Asynchronous, Single-sided.  
Note: The research presented here was done while the first author was a graduate student at the  This material is based upon work supported by funding provided to Prof. Andrew G. Barto by the AFOSR, Bolling AFB, under Grant AFOSR-F49620 93-1-0269 and by the NSF under Grant ECS-92-14866.  
Date: December 20, 1993  
Address: Cambridge, MA 02139  Amherst, MA 01003  
Affiliation: Department of Brain and Cognitive Sciences MIT  Computer Science Department University of Massachusetts  Department of Computer Science at the University of Massachusetts.  
Abstract: We present a new algorithm for solving Markov decision problems that extends the modified policy iteration algorithm of Puterman and Shin [6] in two important ways: 1) The new algorithm is asynchronous in that it allows the values of states to be updated in arbitrary order, and it does not need to consider all actions in each state while updating the policy. 2) The new algorithm converges under more general initial conditions than those required by modified policy iteration. Specifically, the set of initial policy-value function pairs for which our algorithm guarantees convergence is a strict superset of the set for which modified policy iteration converges. This generalization was obtained by making a simple and easily implementable change to the policy evaluation operator used in updating the value function. Both the asynchronous nature of our algorithm and its convergence under more general conditions expand the range of problems to which our algorithm can be applied. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.G. Barto. </author> <type> personal communication. </type>
Reference-contexts: Puterman and Shin proved that under certain initial conditions, any synchronous modified policy iteration method is guaranteed to yield an optimal policy. Recently, Williams and Baird [9] derived an asynchronous algorithm that can be viewed as a form of asynchronous modified policy iteration (Barto <ref> [1] </ref>). Williams and Baird proved convergence of their algorithm, hereafter called the W&B algorithm, under initial conditions similar to those specified by Puterman and Shin (P&S ). <p> restrictive initial conditions than both the P&S and the W&B algorithms. 2 Solving Markovian Decision Processes Consider an MDP with a finite state set X, a finite set of actions A, a payoff function R : X fiA ! R, and a state transition function P : X fiAfiX ! <ref> [0; 1] </ref>. A stationary policy, : X ! A, is a function that assigns an action to each state. The optimal decision problem requires finding a policy that maximizes some given objective functional defined over policy space.
Reference: [2] <author> R.E. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1957. </year>
Reference-contexts: 1 Introduction The problem of finding optimal policies in finite Markovian decision processes (MDPs) can be reduced to the problem of solving a finite system of non-linear equations known as the Bellman optimality equation (Bellman <ref> [2] </ref>, see also Bertsekas [3]). There are two classical iterative dynamic programming (DP) methods for solving the Bellman equation: policy iteration (Howard [5]), and value iteration (see Ross [7]).
Reference: [3] <author> D.P. Bertsekas. </author> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1987. </year>
Reference-contexts: 1 Introduction The problem of finding optimal policies in finite Markovian decision processes (MDPs) can be reduced to the problem of solving a finite system of non-linear equations known as the Bellman optimality equation (Bellman [2], see also Bertsekas <ref> [3] </ref>). There are two classical iterative dynamic programming (DP) methods for solving the Bellman equation: policy iteration (Howard [5]), and value iteration (see Ross [7]).
Reference: [4] <author> D.P. Bertsekas and J.N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: Then if V l 2 V, jjV l+h V fl jj 1 fljjV l V fl jj 1 . Proof: This is a simple extension of a result in Bertsekas and Tsitsiklis <ref> [4] </ref>. Fact 2: Consider the iteration (V 0 k+1 ; k+1 ) = U k (V 0 k ; k ), where U k 2 fC x j x 2 Xg.
Reference: [5] <author> R. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1960. </year>
Reference-contexts: There are two classical iterative dynamic programming (DP) methods for solving the Bellman equation: policy iteration (Howard <ref> [5] </ref>), and value iteration (see Ross [7]). Puterman and Shin [6] have shown that it is more appropriate to think of these two classical methods as two extremes of a continuum of iterative methods, consisting of what they call modified policy iteration algorithms.
Reference: [6] <author> M.L. Puterman and M.C. Shin. </author> <title> Modified policy iteration algorithms for discounted markov decision problems. </title> <journal> Management Science, </journal> <volume> 24(11), </volume> <month> July </month> <year> 1978. </year>
Reference-contexts: There are two classical iterative dynamic programming (DP) methods for solving the Bellman equation: policy iteration (Howard [5]), and value iteration (see Ross [7]). Puterman and Shin <ref> [6] </ref> have shown that it is more appropriate to think of these two classical methods as two extremes of a continuum of iterative methods, consisting of what they call modified policy iteration algorithms.
Reference: [7] <author> S. Ross. </author> <title> Introduction to Stochastic Dynamic Programming. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: There are two classical iterative dynamic programming (DP) methods for solving the Bellman equation: policy iteration (Howard [5]), and value iteration (see Ross <ref> [7] </ref>). Puterman and Shin [6] have shown that it is more appropriate to think of these two classical methods as two extremes of a continuum of iterative methods, consisting of what they call modified policy iteration algorithms. <p> Bellman equation: V (x) = max 2 X P a (x; y)V (y) 5 where P a (x; y) is the probability of a transition to state y when action a is executed in 1 For finite MDPs it is known that there is a stationary optimal policy (e.g., Ross <ref> [7] </ref>). 2 state x.
Reference: [8] <author> S. P. Singh. </author> <title> Learning to Solve Markovian Decision Processes. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <year> 1993. </year>
Reference-contexts: Moreover, it is known that there exists an * &gt; 0 such that if jjV V fl jj 1 * then the optimal policy with respect to V is also optimal with respect to V fl (e.g., Singh <ref> [8] </ref>). Because the sequence fV k g is non-decreasing and V k V fl , it can 8 be concluded that 1 2 f fl g. Q.E.D.
Reference: [9] <author> R. J. Williams and L. C. Baird. </author> <title> Analysis of some incremental variants of policy iteration: First steps toward understanding actor-critic learning systems. </title> <type> Technical Report NU-CCS-93-11, </type> <institution> Northeastern University, College of Computer Science, </institution> <address> Boston, MA 02115, </address> <month> September </month> <year> 1993. </year> <month> 9 </month>
Reference-contexts: Modified policy iteration is a synchronous algorithm, i.e., it updates information about every state of the MDP in each iteration. Puterman and Shin proved that under certain initial conditions, any synchronous modified policy iteration method is guaranteed to yield an optimal policy. Recently, Williams and Baird <ref> [9] </ref> derived an asynchronous algorithm that can be viewed as a form of asynchronous modified policy iteration (Barto [1]). Williams and Baird proved convergence of their algorithm, hereafter called the W&B algorithm, under initial conditions similar to those specified by Puterman and Shin (P&S ).
References-found: 9

