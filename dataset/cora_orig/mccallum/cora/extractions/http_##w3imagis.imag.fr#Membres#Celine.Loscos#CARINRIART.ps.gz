URL: http://w3imagis.imag.fr/Membres/Celine.Loscos/CARINRIART.ps.gz
Refering-URL: http://w3imagis.imag.fr/Publications/index_fr.html
Root-URL: http://www.imag.fr
Title: ISSN 0249-0803 a p p o r t  Interactive Virtual Relighting of Real Scenes  
Author: t e c h n i q u e Celine Loscos, George Drettakis, Luc Robert 
Note: TH OE EME 3  
Date: Novembre 1998  
Affiliation: INSTITUT NATIONAL DE RECHERCHE EN INFORMATIQUE ET EN AUTOMATIQUE  
Pubnum: No RT-0225  
Abstract-found: 0
Intro-found: 1
Reference: [Azu97] <author> Ronald T. Azuma. </author> <title> A survey of augmented reality. In Presence: </title> <booktitle> Teleoperators and Virtual Environments 6, </booktitle> <pages> pages 355385, </pages> <month> August </month> <year> 1997. </year> <note> Earlier version in Course Notes #9: Developing Advanced Virtual Reality Applications, ACM SIGGRAPH (LA, 1995), 20-1 to 20-38. </note>
Reference-contexts: The focus of previous research in the domain has mainly been on registration and calibration <ref> [Azu97] </ref>. Our interest however focuses on common illumination between real and virtual objects, i.e. the interaction of light (shadows, reflections etc.) between real lights and objects and virtual lights and objects. <p> These two approaches are presented in detail in the survey of Azuma <ref> [Azu97] </ref> which also provides extensive references to related literature. The first approach involves the calibration, registration and display of virtual objects in real time to avoid delays between projected images and the perceived real world.
Reference: [CCWG88] <author> Michael F. Cohen, Shenchang Eric Chen, John R. Wallace, and Donald P. Greenberg. </author> <title> A progressive refinement approach to fast radiosity image generation. </title> <editor> In John Dill, editor, </editor> <booktitle> Computer Graphics (SIGGRAPH '88 Proceedings), </booktitle> <volume> volume 22, </volume> <pages> pages 7584, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Using matrix solution techniques, radiosity values are updated at each new iteration. This full-matrix approach is costly in memory (n 2 where n is the number of patches) and in computation time. A first improvement was the progressive radiosity method <ref> [CCWG88] </ref> in which radiosity is first shot from the most significant patches, that is primarily light sources and patches that have the greatest radiosity values. <p> The emittance E i of each source is estimated from the following equation: E i A i = (1 r) B A A i (2) with A i being the area of patch i. This approximation is based on the estimated ambient term in the progressive radiosity algorithm <ref> [CCWG88] </ref>. To simplify, and as it is approximately the case for our scenes, we consider that all the sources have the same intensity. However a system of equations could be solved for non-homogeneous intensities.
Reference: [DBY98] <author> Paul Debevec, George Borshukov, and Yizhou Yu. </author> <title> Efficient view-dependent image-based rendering with projective texture-mapping. </title> <booktitle> In Rendering Techniques '98, 9th EG workshop on Rendering, </booktitle> <address> Vienna, Austria, June 1998. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: View-dependent texture mapping [DTM96], combines RT n0225 6 Cline Loscos, George Drettakis, Luc Robert several original images from nearby viewpoints, by weighting them appropriately. A novel algorithm was recently presented to achieve view dependent texture mapping in real time <ref> [DBY98] </ref>. The first step of this algorithm is a clipping algorithm which splits the model into completely visible polygons for each view. Holes are then filled with Gouraud shading, and a view map is built for each polygon. <p> Debevec et al. <ref> [DBY98] </ref> use a similar approach when combining several weighted textures to create a single one, while Soler and Sillion [SS98] also used a multi-pass display to correctly modulate textures with direct illumination. 5.4 Final relighting Recall that the radiosity system has been previously set up, and that real shadows were removed <p> Building on recent work <ref> [POF98, DBY98] </ref> we believe that we could develop a solution at least for a limited set of viewpoints. Another interesting research direction is to allow the removal or replacement of real objects.
Reference: [Deb98] <author> Paul Debevec. </author> <title> Rendering synthetic objects into real scenes: Bridging traditional and image-based graphics with global illumination and high dynamic range photography. </title> <booktitle> In Computer Graphics (ACM SIGGRAPH '98 Proceedings), </booktitle> <year> 1998. </year>
Reference-contexts: Our interest however focuses on common illumination between real and virtual objects, i.e. the interaction of light (shadows, reflections etc.) between real lights and objects and virtual lights and objects. Previous work in common illumination <ref> [SHC + 96, NHIN86, YM98, JNP + 95, Deb98, FGR93, DRB97] </ref> has provided solutions which handle certain cases of light interactions between real and virtual objects, and especially the case of virtual objects casting shadows onto real objects. <p> In this case, two passive links shown in green were maintained. The corresponding shaft is outlined in grey. 2.4 Common illumination in augmented reality The retrieval and simulation of common illumination between virtual and real objects has been treated by several researchers in previous work <ref> [SHC + 96, NHIN86, YM98, JNP + 95, Deb98, FGR93, DRB97] </ref>. All use some form of a 3D representation of the real scene. State et al. [SHC + 96] use a composition of vision-based and magnetic tracking methods for accurate registration of the real environment. <p> This allows the creation of video sequences, with animated virtual objects such as a cloth, and the modification of the reflective properties of real objects. The final rendering is performed using a ray-tracing system, and images are merged using a masking algorithm. Debevec <ref> [Deb98] </ref> also simulates common illumination effects using RADIANCE [War94], a ray tracing based global illumination system. In this work, the real environment is decomposed into the distant scene and the local scene. The distant scene is used to evaluate the global radiance, and the source emittance [DM97].
Reference: [DM97] <author> Paul E. Debevec and Jitendra Malik. </author> <title> Recovering high dynamic range radiance maps from photographs. </title> <editor> In Turner Whitted, editor, </editor> <booktitle> SIGGRAPH 97 Conference Proceedings, Annual Conference Series, </booktitle> <pages> pages 369378. </pages> <publisher> ACM SIGGRAPH, Addison Wesley, </publisher> <month> August </month> <year> 1997. </year> <note> ISBN 0-89791-896-7. </note>
Reference-contexts: There are two measured incident radiances, one for the sun and one for the sky and the surrounding environment. Radiance is recovered by adapting the high-dynamic range method presented by Debevec et al. <ref> [DM97] </ref>. Diffuse and specular reflectances are retrieved using multiple images from multiple viewpoints. From various virtual positions of the sun and modified sky and environment illumination, the new outdoors illumination is performed pixel by pixel for each reconstructed triangle. <p> Debevec [Deb98] also simulates common illumination effects using RADIANCE [War94], a ray tracing based global illumination system. In this work, the real environment is decomposed into the distant scene and the local scene. The distant scene is used to evaluate the global radiance, and the source emittance <ref> [DM97] </ref>. An approximate geometric model of the local scene is built using the methods previously developed by the same author [DTM96]. Since radiance is accurately retrieved from images, rendering with mixed images is done by using the difference of the wanted effects and the original image value.
Reference: [DRB97] <author> George Drettakis, Luc Robert, and Sylvain Bougnoux. </author> <title> Interactive common illumination for computer augmented reality. </title> <editor> In J. Dorsey and P. Slusallek, editors, </editor> <booktitle> Rendering Techniques '97, pages 4556, 8th EG workshop on Rendering, </booktitle> <address> Saint Etienne, France, June 1997. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: Our interest however focuses on common illumination between real and virtual objects, i.e. the interaction of light (shadows, reflections etc.) between real lights and objects and virtual lights and objects. Previous work in common illumination <ref> [SHC + 96, NHIN86, YM98, JNP + 95, Deb98, FGR93, DRB97] </ref> has provided solutions which handle certain cases of light interactions between real and virtual objects, and especially the case of virtual objects casting shadows onto real objects. <p> Previous solutions [NHIN86, YM98] allow the virtual modification of the sun position in outdoors real environments, but are based on inherently non-interactive algorithms. We build on the only previous interactive common illumination approach <ref> [DRB97] </ref>, which was restricted in the effects it could treat to virtual objects casting shadows on reconstructed real surfaces. Our solution has three main steps. We first reconstruct a 3D representation of a real scene, using advanced vision-based techniques. <p> In this case, two passive links shown in green were maintained. The corresponding shaft is outlined in grey. 2.4 Common illumination in augmented reality The retrieval and simulation of common illumination between virtual and real objects has been treated by several researchers in previous work <ref> [SHC + 96, NHIN86, YM98, JNP + 95, Deb98, FGR93, DRB97] </ref>. All use some form of a 3D representation of the real scene. State et al. [SHC + 96] use a composition of vision-based and magnetic tracking methods for accurate registration of the real environment. <p> Digital prototyping or mock-ups require this type of interactive capability; for a final high-quality animation, one of the previous methods can always be used. Radiosity-based systems for common illumination The most closely related previous work is that of Fournier et al. [FGR93] and its interactive extension <ref> [DRB97] </ref>. The system presented permits the retrieval of radiosity parameters from the textures extracted from the real scene images. In our approach, we use Fournier et al.'s basic derivations for the extraction of the quantities required to initialise the radiosity equations. We thus describe this work in more detail. <p> Since a ratio is being used (which is 1 if there is no change), the only requirement is that the modifications to B i be consistent. Note that ray-casting is used for rendering in [FGR93]. This approach was adapted in <ref> [DRB97] </ref>, in the context of a hierarchical radiosity system, which allows common illumination between a dynamic virtual object and a real scene. The interactive update of the illumination when the virtual object moves uses the dynamic hierarchical radiosity solution decribed in [DS97]. <p> The interactive update of the illumination when the virtual object moves uses the dynamic hierarchical radiosity solution decribed in [DS97]. An example of the results obtained by the method of <ref> [DRB97] </ref> is shown in Figure 4, where a red dynamic virtual object was inserted into a real scene, on the top of the desk. The shadows are virtually projected onto the table, using the display ratio described above (Eq. (3)). (a) (b) (c) in 5.65 seconds. <p> Shadows are projected onto the table using the display ratio of Eq. (3). (b) and (c) The dynamic object moves above the table. Links and radiosity are updated interactively in 3 frames per seconds. 3 The Common Illumination System The interactive common illumination system of Drettakis et al. <ref> [DRB97] </ref> was limited to the insertion of a virtual object into the scene, and the casting of shadows by the virtual object onto a real (reconstructed) surface; modification of the real or virtual lights was not possible (see Section 4.1). <p> The entire algorithm is summarised in Figure 5. 3.1 Representation of the real scene The real scene is represented in our system with an approximation of its geometry and with projected textures. The model of the scene is built semi-automatically, using advanced vision techniques <ref> [FLR + 97, DRB97] </ref> as described in Section 2.2. This process allows the reconstruction of the basic 3D model visible in the captured images (for example the mosaics shown in Figure 6). <p> INRIA Interactive Virtual Relighting of Real Scenes 11 (a) (b) (A, B, C, D). The preprocess begins by setting up all necessary parameters for the estimation of the real scene illumination as in <ref> [DRB97] </ref>. An improved, texture-based refinement follows (described in Section 4.2), allowing better delimitation of real shadow boundaries (see Section 4.3). Real shadows are then removed from real world textures using a radiosity-based algorithm (see Section 4.4), and virtually reprojected using radiosity values (see Section 4.5). <p> Real shadows are then removed from real world textures using a radiosity-based algorithm (see Section 4.4), and virtually reprojected using radiosity values (see Section 4.5). Virtual objects and virtual light sources can then be inserted if desired. The insertion of dynamic objects is performed using the method of <ref> [DRB97] </ref>. In Section 5, implementation issues are explained which enable the insertion of virtual light sources and texture modulation when the virtually modified illumination is brighter than the original real illumination. Such effects could not be simulated by the algorithm described in [DRB97]. <p> dynamic objects is performed using the method of <ref> [DRB97] </ref>. In Section 5, implementation issues are explained which enable the insertion of virtual light sources and texture modulation when the virtually modified illumination is brighter than the original real illumination. Such effects could not be simulated by the algorithm described in [DRB97]. <p> modification algorithm described in the previous section. (a) (b) (c) illuminate the wall, causing the shadow of the table to be cast on the wall. (b) Light B (on the left) is turned off and the intensity of light A on the right is doubled. (c) Using the method of <ref> [DRB97] </ref>, pre-existing shadows are not removed and the quality of the relighting is unsatisfactory: the shadow due to light B is still clearly perceptible. This is however far from being the case. Recall that the reflectance values were estimated from the texture values in the original images. <p> As a consequence, using the method of <ref> [DRB97] </ref>, even after turning off light B, we still see the corresponding shadow (Figure 9 (c)). To solve this problem, we need first to remove pre-existing shadows from the textures thus estimating more consistent reflectance values for the regions previously in shadow. <p> Moreover, we need to be able to brighten textures when the illumination is increased. Our solutions to these issues, as well as the interactive modification process, are described next. 5.2 Modified refinement to insert virtual sources We adapt the method of <ref> [DRB97] </ref>, in which a virtual object can be inserted into the real scene and interactively manipulated [DS97], to also treat the insertion of a virtual light source. This results in the projection of the shadows due to the virtual source on the real objects. <p> Compare the new result with that of the method of <ref> [DRB97] </ref> previously shown in Figure 9 (c), which was inexact, since real shadows were not removed from textures. We now switch on the left light with double the original intensity and switch off the right light (see Figure 22 (d)).
Reference: [DS97] <author> George Drettakis and Franois Sillion. </author> <title> Interactive update of global illumination using A line-space hierarchy. </title> <editor> In Turner Whitted, editor, </editor> <booktitle> SIGGRAPH 97 Conference Proceedings, Annual Conference Series, </booktitle> <pages> pages 5764. </pages> <publisher> ACM SIGGRAPH, Addison Wesley, </publisher> <month> August </month> <year> 1997. </year> <note> ISBN 0-89791-896-7. </note>
Reference-contexts: We have chosen to use the hierarchical radiosity approach with clustering [HSA91, Sil95] with the extensions to dynamic environments <ref> [DS97] </ref>. We next introduce certain basic concepts of radiosity methods. The radiosity method is based on energy exchanges, and has been used in computer graphics to simulate light interactions in synthetic environments [SP94], including indirect illumination. <p> The attenuation factor of a link is typically estimated by shooting rays between the two patches, and counting the percentage of rays blocked by occluders. The hierarchical representation with links can be adapted to allow fast radiosity modification <ref> [DS97] </ref>, by augmenting the links with a shaft data structure [HW91]. In addition, previously subdivided links, called passive links are maintained. The passive links contain all the necessary information allowing them to be reactivated at no cost, if it is required by a geometry change. <p> This approach was adapted in [DRB97], in the context of a hierarchical radiosity system, which allows common illumination between a dynamic virtual object and a real scene. The interactive update of the illumination when the virtual object moves uses the dynamic hierarchical radiosity solution decribed in <ref> [DS97] </ref>. An example of the results obtained by the method of [DRB97] is shown in Figure 4, where a red dynamic virtual object was inserted into a real scene, on the top of the desk. <p> The resulting scene after texture modification is shown in Figure 18 (f), which is very similar to the original image shown in Figure 18 (c). 5 Implementation Issues 5.1 Insertion of virtual light sources and virtual objects Insertion and dynamic modification are performed using the algorithm presented in <ref> [DS97] </ref>, with an adaptation for virtual light sources. Moreover, we need to be able to brighten textures when the illumination is increased. <p> Our solutions to these issues, as well as the interactive modification process, are described next. 5.2 Modified refinement to insert virtual sources We adapt the method of [DRB97], in which a virtual object can be inserted into the real scene and interactively manipulated <ref> [DS97] </ref>, to also treat the insertion of a virtual light source. This results in the projection of the shadows due to the virtual source on the real objects. <p> We therefore perform the modification with the subdivision used for real shadow removal, which is undoubtedly excessive. The multi-pass display also takes time, and it could also be optimised. The radiosity steps could also be optimized by avoiding the complete traversal of the hierarchy in the spirit of <ref> [DS97] </ref>. The quality of the shadow removal is directly related to the subdivision effected by the hierarchical radiosity algorithm. Our texture-based refinement has greatly improved the resulting quadtrees compared with traditional refinement approaches, but it is still susceptible to problems mainly due to inaccurate geometric reconstruction.
Reference: [DTM96] <author> Paul E. Debevec, Camillo J. Taylor, and Jitendra Malik. </author> <title> Modeling and rendering architecture from photographs: A hybrid geometry- and image-based approach. </title> <editor> In Holly Rushmeier, editor, </editor> <booktitle> SIG-GRAPH 96 Conference Proceedings, Annual Conference Series, </booktitle> <pages> pages 1120. </pages> <publisher> ACM SIGGRAPH, Addison Wesley, </publisher> <month> August </month> <year> 1996. </year> <title> held in New Orleans, </title> <address> Louisiana, </address> <month> 04-09 August </month> <year> 1996. </year> <title> INRIA Interactive Virtual Relighting of Real Scenes 29 </title>
Reference-contexts: This drawback can be removed by taking into account additional user input, as presented in the work of Debevec et al. <ref> [DTM96] </ref> or Poulin et al. [POF98]. In the work by Debevec et al. [DTM96], reconstruction is based on a hierarchy of blocks. The main idea is to build polyhedra which include geometric constraints, such as parallelism, orthogonality, and size aspects. <p> This drawback can be removed by taking into account additional user input, as presented in the work of Debevec et al. <ref> [DTM96] </ref> or Poulin et al. [POF98]. In the work by Debevec et al. [DTM96], reconstruction is based on a hierarchy of blocks. The main idea is to build polyhedra which include geometric constraints, such as parallelism, orthogonality, and size aspects. Polyhedra provide good approximations of many objects of the real world, especially for outdoor architectural scenes. <p> If we use the projection of textures presented in [FLR + 97] we are limited to a single point of view, i.e. one of the original views of the camera. View-dependent texture mapping <ref> [DTM96] </ref>, combines RT n0225 6 Cline Loscos, George Drettakis, Luc Robert several original images from nearby viewpoints, by weighting them appropriately. A novel algorithm was recently presented to achieve view dependent texture mapping in real time [DBY98]. <p> The distant scene is used to evaluate the global radiance, and the source emittance [DM97]. An approximate geometric model of the local scene is built using the methods previously developed by the same author <ref> [DTM96] </ref>. Since radiance is accurately retrieved from images, rendering with mixed images is done by using the difference of the wanted effects and the original image value. This method can be adapted for indoors or outdoors environments.
Reference: [FGR93] <author> Alain Fournier, Atjeng S. Gunawan, and Chris Romanzin. </author> <title> Common illumination between real and computer generated scenes. </title> <booktitle> In Proceedings of Graphics Interface '93, </booktitle> <pages> pages 254262, </pages> <address> Toronto, Ontario, Canada, </address> <month> May </month> <year> 1993. </year> <booktitle> Canadian Information Processing Society. </booktitle>
Reference-contexts: Our interest however focuses on common illumination between real and virtual objects, i.e. the interaction of light (shadows, reflections etc.) between real lights and objects and virtual lights and objects. Previous work in common illumination <ref> [SHC + 96, NHIN86, YM98, JNP + 95, Deb98, FGR93, DRB97] </ref> has provided solutions which handle certain cases of light interactions between real and virtual objects, and especially the case of virtual objects casting shadows onto real objects. <p> In this case, two passive links shown in green were maintained. The corresponding shaft is outlined in grey. 2.4 Common illumination in augmented reality The retrieval and simulation of common illumination between virtual and real objects has been treated by several researchers in previous work <ref> [SHC + 96, NHIN86, YM98, JNP + 95, Deb98, FGR93, DRB97] </ref>. All use some form of a 3D representation of the real scene. State et al. [SHC + 96] use a composition of vision-based and magnetic tracking methods for accurate registration of the real environment. <p> Digital prototyping or mock-ups require this type of interactive capability; for a final high-quality animation, one of the previous methods can always be used. Radiosity-based systems for common illumination The most closely related previous work is that of Fournier et al. <ref> [FGR93] </ref> and its interactive extension [DRB97]. The system presented permits the retrieval of radiosity parameters from the textures extracted from the real scene images. In our approach, we use Fournier et al.'s basic derivations for the extraction of the quantities required to initialise the radiosity equations. <p> Since a ratio is being used (which is 1 if there is no change), the only requirement is that the modifications to B i be consistent. Note that ray-casting is used for rendering in <ref> [FGR93] </ref>. This approach was adapted in [DRB97], in the context of a hierarchical radiosity system, which allows common illumination between a dynamic virtual object and a real scene. The interactive update of the illumination when the virtual object moves uses the dynamic hierarchical radiosity solution decribed in [DS97].
Reference: [FLR + 97] <author> Olivier Faugeras, Stphane Laveau, Luc Robert, Gabriella Csurka, Cyril Zeller, Cyrille Gauclin, and Imed Zoghlami. </author> <title> 3-d reconstruction of urban scenes from image sequences. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <year> 1997. </year>
Reference-contexts: Much research on the subject exists in the field of Computer Vision; we have chosen to use an advanced vision-based technique, which allows semi-automatic reconstruction based on multiple views. The approach we use is presented in <ref> [FLR + 97] </ref>. <p> Polygonal regions are next manually selected by a user from point correspondences, and the system provides 3D coordinates of these polygons from the projection equations. Finally, textures are projected to allow correct perspective effects for a fixed viewpoint <ref> [FLR + 97] </ref>. For each reconstructed polygon, a texture image is computed by de-warping the original image (from a given viewpoint), and mapping it to the plane of the polygon. epipolar geometry. <p> Another approach is described in [POF98] in which the primitives are points, lines and polygons, and constraints such as parallelism, orthogonality, or co-planarity are determined by the user. If we use the projection of textures presented in <ref> [FLR + 97] </ref> we are limited to a single point of view, i.e. one of the original views of the camera. View-dependent texture mapping [DTM96], combines RT n0225 6 Cline Loscos, George Drettakis, Luc Robert several original images from nearby viewpoints, by weighting them appropriately. <p> The entire algorithm is summarised in Figure 5. 3.1 Representation of the real scene The real scene is represented in our system with an approximation of its geometry and with projected textures. The model of the scene is built semi-automatically, using advanced vision techniques <ref> [FLR + 97, DRB97] </ref> as described in Section 2.2. This process allows the reconstruction of the basic 3D model visible in the captured images (for example the mosaics shown in Figure 6).
Reference: [GH96] <author> S. Gibson and R. J. Hubbold. </author> <title> Efficient hierarchical refinement and clustering for radiosity in complex environments. </title> <journal> Computer Graphics Forum, </journal> <volume> 15(5):297310, </volume> <year> 1996. </year> <pages> ISSN 0167-7055. </pages>
Reference-contexts: We then present the details of each step of the shadow removal process. 4.2 Texture-based refinement for shadow boundaries In order to remove shadows from original images, shadow boundaries need to be accurately defined. If we use usual refinement criteria, such as BF refinement [HSA91] or error-driven refinement <ref> [GH96] </ref> we do not obtain the desired result. The main problem is that these approaches do not always guarantee good shadow boundaries (even when using the visibility factor of [HSA91]). Since the geometry reconstruction and the visibility computation via ray-casting are not completely accurate we often get inappropriate subdivision.
Reference: [GTGB84] <author> Cindy M. Goral, Kenneth E. Torrance, Donald P. Greenberg, and Bennett Battaile. </author> <title> Modelling the interaction of light between diffuse surfaces. </title> <booktitle> In Computer Graphics (SIGGRAPH '84 Proceedings), </booktitle> <volume> volume 18, </volume> <pages> pages 21222, </pages> <month> July </month> <year> 1984. </year>
Reference-contexts: Since the radiosity method is a finite-element approach, a mesh representation of the scene is required, which is usually constructed with quadtrees. The basic method consists in iteratively solving a system of equations using finite-element methods <ref> [GTGB84] </ref>. Form factors are first computed between elements or patches, and with the reflectance given for each element, a matrix is constructed. Using matrix solution techniques, radiosity values are updated at each new iteration.
Reference: [HSA91] <author> Pat Hanrahan, David Salzman, and Larry Aupperle. </author> <title> A rapid hierarchical radiosity algorithm. </title> <editor> In Thomas W. Sederberg, editor, </editor> <booktitle> Computer Graphics (SIGGRAPH '91 Proceedings), </booktitle> <volume> volume 25, </volume> <pages> pages 197206, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: As a result, when the point of view changes, the resulting model looks less realistic. 2.3 Hierarchical radiosity To achieve interactive relighting, we need an efficient description of light exchanges in the scene, including shadow information. We have chosen to use the hierarchical radiosity approach with clustering <ref> [HSA91, Sil95] </ref> with the extensions to dynamic environments [DS97]. We next introduce certain basic concepts of radiosity methods. The radiosity method is based on energy exchanges, and has been used in computer graphics to simulate light interactions in synthetic environments [SP94], including indirect illumination. <p> The two methods converge to the same result, but we can have a more visually convincing approximation of the final solution before the convergence with progressive radiosity. Hierarchical radiosity <ref> [HSA91] </ref> uses a multi-resolution representation of light, by creating a hierarchy of patches on each surface. Light exchanges are established at the appropriate levels at the patch hierarchy via a link data structure, resulting in an efficient solution. <p> If the light exchange is not sufficiently well-represented, the link is refined and the patches or clusters are then subdivided. When links are established, the incoming irradiance is gathered at each patch, followed by a push-pull step performed to maintain a coherent multi-resolution representation of radiant exchanges <ref> [HSA91] </ref>. The cluster-based hierarchical radiosity starts with the root cluster linked to itself. The algorithm described by Sillion [Sil95] performs a refinement step, establishing links at appropriate levels followed by the gather and push-pull steps. <p> We then present the details of each step of the shadow removal process. 4.2 Texture-based refinement for shadow boundaries In order to remove shadows from original images, shadow boundaries need to be accurately defined. If we use usual refinement criteria, such as BF refinement <ref> [HSA91] </ref> or error-driven refinement [GH96] we do not obtain the desired result. The main problem is that these approaches do not always guarantee good shadow boundaries (even when using the visibility factor of [HSA91]). <p> If we use usual refinement criteria, such as BF refinement <ref> [HSA91] </ref> or error-driven refinement [GH96] we do not obtain the desired result. The main problem is that these approaches do not always guarantee good shadow boundaries (even when using the visibility factor of [HSA91]). Since the geometry reconstruction and the visibility computation via ray-casting are not completely accurate we often get inappropriate subdivision. Discontinuity meshing [LTG93] is unsuitable for the same reasons, since discontinuity lines would be geometrically inaccurate. As a consequence, we use standard quadtree subdivision, with texture-refinement based on new criteria.
Reference: [HW91] <author> Eric Haines and John Wallace. </author> <title> Shaft culling for efficient ray-traced radiosity. </title> <booktitle> In Eurographics Workshop on Rendering, </booktitle> <year> 1991. </year>
Reference-contexts: The attenuation factor of a link is typically estimated by shooting rays between the two patches, and counting the percentage of rays blocked by occluders. The hierarchical representation with links can be adapted to allow fast radiosity modification [DS97], by augmenting the links with a shaft data structure <ref> [HW91] </ref>. In addition, previously subdivided links, called passive links are maintained. The passive links contain all the necessary information allowing them to be reactivated at no cost, if it is required by a geometry change. See Figure 3 for an example. (a) (b) (c) the hierarchy of elements and links.
Reference: [JNP + 95] <author> Pierre Jancne, Fabrice Neyret, Xavier Provot, Jean-Philippe Tarel, Jean-Marc Vzien, Christophe Meilhac, and Anne Vrroust. </author> <title> Res: computing the interactions between real and virtual objects in video sequences. </title> <booktitle> In Second IEEE Workshop on Networked Realities, </booktitle> <pages> pages 2740, </pages> <address> Boston, Massachusetts (USA), </address> <month> October </month> <year> 1995. </year> <note> http://www-rocq.inria.fr/syntim/textes/nr95-eng.html. </note>
Reference-contexts: Our interest however focuses on common illumination between real and virtual objects, i.e. the interaction of light (shadows, reflections etc.) between real lights and objects and virtual lights and objects. Previous work in common illumination <ref> [SHC + 96, NHIN86, YM98, JNP + 95, Deb98, FGR93, DRB97] </ref> has provided solutions which handle certain cases of light interactions between real and virtual objects, and especially the case of virtual objects casting shadows onto real objects. <p> In this case, two passive links shown in green were maintained. The corresponding shaft is outlined in grey. 2.4 Common illumination in augmented reality The retrieval and simulation of common illumination between virtual and real objects has been treated by several researchers in previous work <ref> [SHC + 96, NHIN86, YM98, JNP + 95, Deb98, FGR93, DRB97] </ref>. All use some form of a 3D representation of the real scene. State et al. [SHC + 96] use a composition of vision-based and magnetic tracking methods for accurate registration of the real environment. <p> However, for certain applications, an approximation of only the diffuse reflectance is sufficient. RT n0225 8 Cline Loscos, George Drettakis, Luc Robert For indoors environments, Jancne et al. <ref> [JNP + 95] </ref> used vision-based techniques to retrieve the geometry of the real scene from a video sequence. Common illumination between virtual and real objects is simulated.
Reference: [LTG93] <author> Daniel Lischinski, Filippo Tampieri, and Donald P. Greenberg. </author> <title> Combining hierarchical radiosity and discontinuity meshing. </title> <booktitle> In Computer Graphics Proceedings, Annual Conference Series, </booktitle> <year> 1993, </year> <pages> pages 199208, </pages> <year> 1993. </year>
Reference-contexts: The main problem is that these approaches do not always guarantee good shadow boundaries (even when using the visibility factor of [HSA91]). Since the geometry reconstruction and the visibility computation via ray-casting are not completely accurate we often get inappropriate subdivision. Discontinuity meshing <ref> [LTG93] </ref> is unsuitable for the same reasons, since discontinuity lines would be geometrically inaccurate. As a consequence, we use standard quadtree subdivision, with texture-refinement based on new criteria.
Reference: [NHIN86] <author> Eihachiro Nakamae, Koichi Harada, Takao Ishizaki, and Tomoyuki Nishita. </author> <title> A montage method: The overlaying of the computer generated images onto a background photograph. </title> <editor> In David C. Evans and Russell J. Athay, editors, </editor> <booktitle> Computer Graphics (SIGGRAPH '86 Proceedings), </booktitle> <volume> volume 20, </volume> <pages> pages 207214, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Our interest however focuses on common illumination between real and virtual objects, i.e. the interaction of light (shadows, reflections etc.) between real lights and objects and virtual lights and objects. Previous work in common illumination <ref> [SHC + 96, NHIN86, YM98, JNP + 95, Deb98, FGR93, DRB97] </ref> has provided solutions which handle certain cases of light interactions between real and virtual objects, and especially the case of virtual objects casting shadows onto real objects. <p> Modification of real lights is a difficult problem because real shadows are already present in textures representing the real world, which are mapped onto the reconstructed models of real objects. Previous solutions <ref> [NHIN86, YM98] </ref> allow the virtual modification of the sun position in outdoors real environments, but are based on inherently non-interactive algorithms. We build on the only previous interactive common illumination approach [DRB97], which was restricted in the effects it could treat to virtual objects casting shadows on reconstructed real surfaces. <p> In this case, two passive links shown in green were maintained. The corresponding shaft is outlined in grey. 2.4 Common illumination in augmented reality The retrieval and simulation of common illumination between virtual and real objects has been treated by several researchers in previous work <ref> [SHC + 96, NHIN86, YM98, JNP + 95, Deb98, FGR93, DRB97] </ref>. All use some form of a 3D representation of the real scene. State et al. [SHC + 96] use a composition of vision-based and magnetic tracking methods for accurate registration of the real environment. <p> Virtual objects are inserted into a real scene and common illumination is performed, with a moving (real) point light source. Shadow maps are used allowing updates in real time, but only for direct illumination and sharp shadows from point sources. Nakamae et al. <ref> [NHIN86] </ref> developed a solution for merging virtual objects into background photographs, and estimated the sun location to simulate common illumination effects in outdoors environments. More recently Yu [YM98] proposed a solution to virtually modify the illumination with different virtual positions of the sun in outdoors scenes.
Reference: [POF98] <author> Pierre Poulin, Mathieu Ouimet, and Marie-Claude Frasson. </author> <title> Interactively modeling with pho-togrammetry. </title> <booktitle> In Rendering Techniques '98, 9th EG workshop on Rendering, </booktitle> <address> Vienna, Austria, June 1998. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: This drawback can be removed by taking into account additional user input, as presented in the work of Debevec et al. [DTM96] or Poulin et al. <ref> [POF98] </ref>. In the work by Debevec et al. [DTM96], reconstruction is based on a hierarchy of blocks. The main idea is to build polyhedra which include geometric constraints, such as parallelism, orthogonality, and size aspects. <p> Polyhedra provide good approximations of many objects of the real world, especially for outdoor architectural scenes. This also allows the reconstruction of vertices which are invisible in the original images, but correspond to hidden vertices of the polyhedra. Another approach is described in <ref> [POF98] </ref> in which the primitives are points, lines and polygons, and constraints such as parallelism, orthogonality, or co-planarity are determined by the user. <p> Holes are then filled with Gouraud shading, and a view map is built for each polygon. For each new viewpoint, a new texture is created by modulating weighted textures from the three closest views. In <ref> [POF98] </ref>, a method is proposed for view-independent textures, that weights and modulates textures from different views. There is thus a single texture per polygon, for any view point. <p> Building on recent work <ref> [POF98, DBY98] </ref> we believe that we could develop a solution at least for a limited set of viewpoints. Another interesting research direction is to allow the removal or replacement of real objects.
Reference: [Rob95] <author> L Robert. </author> <title> Camera calibration without feature extraction. Computer Vision, Graphics, </title> <booktitle> and Image Processing, </booktitle> <address> 63(2):314325, </address> <month> March </month> <year> 1995. </year> <note> also INRIA Technical Report 2204. </note>
Reference-contexts: The first step is the calibration of the camera which consists in retrieving the intrinsic parameters from a non-planar calibration pattern image using an automatic algorithm <ref> [Rob95] </ref>. The user provides approximate positions of 6 reference points. From this, the system retrieves intrinsic and extrinsic parameters of the camera. Then, four sets of three photographs each are taken, and a mosaic is built automatically for each set as presented in [ZFD97].
Reference: [SAG94] <author> Brian Smits, James Arvo, and Donald Greenberg. </author> <title> A clustering algorithm for radiosity in complex environments. </title> <editor> In Andrew Glassner, editor, </editor> <booktitle> Proceedings of SIGGRAPH '94 (Orlando, </booktitle> <address> Florida, </address> <month> July 2429, </month> <year> 1994), </year> <booktitle> Computer Graphics Proceedings, Annual Conference Series, </booktitle> <pages> pages 435442. </pages> <publisher> ACM SIGGRAPH, ACM Press, </publisher> <month> July </month> <year> 1994. </year> <note> ISBN 0-89791-667-0. </note>
Reference-contexts: Light exchanges are established at the appropriate levels at the patch hierarchy via a link data structure, resulting in an efficient solution. A generalisation of this approach can be achieved using clusters <ref> [SAG94, SDS95] </ref>, which represent groups of objects. The entire scene is contained in a single, root cluster. Clusters and patches can be linked at the appropriate level, depending on the refinement criterion, which decides whether the link represents the light transfer at a suitable, user defined, level of accuracy.
Reference: [SDS95] <author> F. Sillion, G. Drettakis, and C. Soler. </author> <title> A clustering algorithm for radiance calculation in general environments. </title> <booktitle> In Eurographics Rendering Workshop 1995. Eurographics, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Light exchanges are established at the appropriate levels at the patch hierarchy via a link data structure, resulting in an efficient solution. A generalisation of this approach can be achieved using clusters <ref> [SAG94, SDS95] </ref>, which represent groups of objects. The entire scene is contained in a single, root cluster. Clusters and patches can be linked at the appropriate level, depending on the refinement criterion, which decides whether the link represents the light transfer at a suitable, user defined, level of accuracy.
Reference: [SHC + 96] <author> Andrei State, Gentaro Hirota, David T. Chen, Bill Garrett, and Mark Livingston. </author> <title> Superior augmented reality registration by integrating landmark tracking and magnetic tracking. </title> <editor> In Holly Rushmeier, editor, </editor> <booktitle> SIGGRAPH 96 Conference Proceedings, Annual Conference Series, </booktitle> <pages> pages 429438. </pages> <publisher> ACM SIGGRAPH, Addison Wesley, </publisher> <month> August </month> <year> 1996. </year> <title> held in New Orleans, </title> <address> Louisiana, </address> <month> 04-09 August </month> <year> 1996. </year>
Reference-contexts: Our interest however focuses on common illumination between real and virtual objects, i.e. the interaction of light (shadows, reflections etc.) between real lights and objects and virtual lights and objects. Previous work in common illumination <ref> [SHC + 96, NHIN86, YM98, JNP + 95, Deb98, FGR93, DRB97] </ref> has provided solutions which handle certain cases of light interactions between real and virtual objects, and especially the case of virtual objects casting shadows onto real objects. <p> In this case, two passive links shown in green were maintained. The corresponding shaft is outlined in grey. 2.4 Common illumination in augmented reality The retrieval and simulation of common illumination between virtual and real objects has been treated by several researchers in previous work <ref> [SHC + 96, NHIN86, YM98, JNP + 95, Deb98, FGR93, DRB97] </ref>. All use some form of a 3D representation of the real scene. State et al. [SHC + 96] use a composition of vision-based and magnetic tracking methods for accurate registration of the real environment. <p> All use some form of a 3D representation of the real scene. State et al. <ref> [SHC + 96] </ref> use a composition of vision-based and magnetic tracking methods for accurate registration of the real environment. Virtual objects are inserted into a real scene and common illumination is performed, with a moving (real) point light source.
Reference: [Sil95] <author> F. X. Sillion. </author> <title> A unified hierarchical algorithm for global illumination with scattering volumes and object clusters. </title> <journal> IEEE Transactions on Visualization and Computer Graphics, </journal> <volume> 1(3):240254, </volume> <month> September </month> <year> 1995. </year> <pages> ISSN 1077-2626. </pages>
Reference-contexts: As a result, when the point of view changes, the resulting model looks less realistic. 2.3 Hierarchical radiosity To achieve interactive relighting, we need an efficient description of light exchanges in the scene, including shadow information. We have chosen to use the hierarchical radiosity approach with clustering <ref> [HSA91, Sil95] </ref> with the extensions to dynamic environments [DS97]. We next introduce certain basic concepts of radiosity methods. The radiosity method is based on energy exchanges, and has been used in computer graphics to simulate light interactions in synthetic environments [SP94], including indirect illumination. <p> When links are established, the incoming irradiance is gathered at each patch, followed by a push-pull step performed to maintain a coherent multi-resolution representation of radiant exchanges [HSA91]. The cluster-based hierarchical radiosity starts with the root cluster linked to itself. The algorithm described by Sillion <ref> [Sil95] </ref> performs a refinement step, establishing links at appropriate levels followed by the gather and push-pull steps. Irradiance is pushed down to the leaves of the patch hierarchy, and radiosity is pulled up by averaging [Sil95]. This is repeated until convergence. Links store visibility information and form factors. <p> The algorithm described by Sillion <ref> [Sil95] </ref> performs a refinement step, establishing links at appropriate levels followed by the gather and push-pull steps. Irradiance is pushed down to the leaves of the patch hierarchy, and radiosity is pulled up by averaging [Sil95]. This is repeated until convergence. Links store visibility information and form factors. The visibility information can be of three types: VISIBLE, INVISIBLE or PARTIAL. When computing radiosity exchanges between two patches, the incoming irradiance is multiplied by the form factor and an attenuation factor.
Reference: [SP94] <author> Franois Sillion and Claude Puech. </author> <title> Radiosity and Global Illumination. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, </address> <year> 1994. </year> <title> excellent coverage of radiosity and global illumination algorithms. </title>
Reference-contexts: We next introduce certain basic concepts of radiosity methods. The radiosity method is based on energy exchanges, and has been used in computer graphics to simulate light interactions in synthetic environments <ref> [SP94] </ref>, including indirect illumination. Since the radiosity method is a finite-element approach, a mesh representation of the scene is required, which is usually constructed with quadtrees. The basic method consists in iteratively solving a system of equations using finite-element methods [GTGB84].
Reference: [SS98] <author> Cyril Soler and Francois Sillion. </author> <title> Automatic calculation of soft shadow textures for fast, high-quality radiosity. </title> <booktitle> In Rendering Techniques '98, 9th EG workshop on Rendering, </booktitle> <address> Vienna, Austria, RT n0225 30 Cline Loscos, George Drettakis, Luc Robert June 1998. </address> <publisher> Springer Verlag. </publisher> <address> [Tot] http://www.inria.fr/robotvis/personnel/sbougnou/TotalCalib/. </address>
Reference-contexts: Debevec et al. [DBY98] use a similar approach when combining several weighted textures to create a single one, while Soler and Sillion <ref> [SS98] </ref> also used a multi-pass display to correctly modulate textures with direct illumination. 5.4 Final relighting Recall that the radiosity system has been previously set up, and that real shadows were removed from real world textures.
Reference: [War94] <author> Gregory J. Ward. </author> <title> The RADIANCE lighting simulation and rendering system. </title> <editor> In Andrew Glassner, editor, </editor> <booktitle> Proceedings of SIGGRAPH '94 (Orlando, </booktitle> <address> Florida, </address> <month> July 2429, </month> <year> 1994), </year> <booktitle> Computer Graphics Proceedings, Annual Conference Series, </booktitle> <pages> pages 459472. </pages> <publisher> ACM SIGGRAPH, ACM Press, </publisher> <month> July </month> <year> 1994. </year> <note> ISBN 0-89791-667-0. </note>
Reference-contexts: The final rendering is performed using a ray-tracing system, and images are merged using a masking algorithm. Debevec [Deb98] also simulates common illumination effects using RADIANCE <ref> [War94] </ref>, a ray tracing based global illumination system. In this work, the real environment is decomposed into the distant scene and the local scene. The distant scene is used to evaluate the global radiance, and the source emittance [DM97].
Reference: [YM98] <author> Yizhou Yu and Jitendra Malik. </author> <title> Recovering photometric properties of architectural scenes from photographs. </title> <booktitle> In Computer Graphics (ACM SIGGRAPH '98 Proceedings), </booktitle> <year> 1998. </year>
Reference-contexts: Our interest however focuses on common illumination between real and virtual objects, i.e. the interaction of light (shadows, reflections etc.) between real lights and objects and virtual lights and objects. Previous work in common illumination <ref> [SHC + 96, NHIN86, YM98, JNP + 95, Deb98, FGR93, DRB97] </ref> has provided solutions which handle certain cases of light interactions between real and virtual objects, and especially the case of virtual objects casting shadows onto real objects. <p> Modification of real lights is a difficult problem because real shadows are already present in textures representing the real world, which are mapped onto the reconstructed models of real objects. Previous solutions <ref> [NHIN86, YM98] </ref> allow the virtual modification of the sun position in outdoors real environments, but are based on inherently non-interactive algorithms. We build on the only previous interactive common illumination approach [DRB97], which was restricted in the effects it could treat to virtual objects casting shadows on reconstructed real surfaces. <p> In this case, two passive links shown in green were maintained. The corresponding shaft is outlined in grey. 2.4 Common illumination in augmented reality The retrieval and simulation of common illumination between virtual and real objects has been treated by several researchers in previous work <ref> [SHC + 96, NHIN86, YM98, JNP + 95, Deb98, FGR93, DRB97] </ref>. All use some form of a 3D representation of the real scene. State et al. [SHC + 96] use a composition of vision-based and magnetic tracking methods for accurate registration of the real environment. <p> Nakamae et al. [NHIN86] developed a solution for merging virtual objects into background photographs, and estimated the sun location to simulate common illumination effects in outdoors environments. More recently Yu <ref> [YM98] </ref> proposed a solution to virtually modify the illumination with different virtual positions of the sun in outdoors scenes. A pseudo-BRDF is first estimated, which is a function of the incident radiance on the reflected differential radiance.
Reference: [ZDFL95] <author> Z. Zhang, R. Deriche, O. Faugeras, and Q.-T. Luong. </author> <title> A robust technique for matching two uncalibrated images through the recovery of the unknown epipolar geometry. </title> <journal> Artificial Intelligence Journal, </journal> <volume> 78:87119, </volume> <month> October </month> <year> 1995. </year>
Reference-contexts: Point correspondences are provided by a user, who clicks on one image to create a reference point. The matched points on the 3 other mosaics are given automatically by the system. From about 30 point correspondences, fundamental matrices are computed using a non-linear method <ref> [ZDFL95] </ref>. Polygonal regions are next manually selected by a user from point correspondences, and the system provides 3D coordinates of these polygons from the projection equations. Finally, textures are projected to allow correct perspective effects for a fixed viewpoint [FLR + 97].

References-found: 28

