URL: http://s2k-ftp.cs.berkeley.edu:8000/sequoia/tech-reports/s2k-92-16/s2k-92-16.ps.Z
Refering-URL: http://s2k-ftp.cs.berkeley.edu:8000/sequoia/tech-reports/s2k-92-16/
Root-URL: http://www.cs.berkeley.edu
Title: HighLight: Using a Log-structured File System for Tertiary Storage Management project Sequoia 2000: Large Capacity
Author: John T. Kohl Carl Staelin Michael Stonebraker 
Note: This research was sponsored in part by the University of California and Digital Equipment Corporation under Digital's flagship research  Research. Other industrial and government partners include the California Department of Water Resources, United States Geological Survey, MCI, ESL, Hewlett Packard, RSI, SAIC, PictureTel, Metrum Information Storage, and Hughes Aircraft Corporation. This work was also supported in part by Digital Equipment Corporation's Graduate Engineering Education Program.  
Date: November 20, 1992  
Affiliation: University of California, Berkeley and Digital Equipment Corporation  Hewlett-Packard Laboratories  University of California, Berkeley  
Abstract: Robotic storage devices offer huge storage capacity at a low cost per byte, but with large access times. Integrating these devices into the storage hierarchy presents a challenge to file system designers. Log-structured file systems (LFSs) were developed to reduce latencies involved in accessing disk devices, but their sequential write patterns match well with tertiary storage characteristics. Unfortunately, existing versions only manage memory caches and disks, and do not support a broader storage hierarchy. HighLight extends 4.4BSD LFS to incorporate both secondary storage devices (disks) and tertiary storage devices (such as robotic tape jukeboxes), providing a hierarchy within the file system that does not require any application support. This paper presents the design of HighLight, proposes various policies for automatic migration of file data between the hierarchy levels, and presents initial migration mechanism performance figures. y Permission has been granted by the USENIX Association to reprint the above article. This article was originally published in the USENIX Association Conference Proceedings, January 1993. Copyright c flUSENIX Association, 1993. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Mary G. Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirriff, and John K. Ousterhout. </author> <title> Measurements of a Distributed File System. </title> <journal> Operating Systems Review, </journal> <volume> 25(5) </volume> <pages> 198-212, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: However, in some environments whole file migration may be inadequate. In UNIX-like distributed file system environments, most files are accessed sequentially and many of those are read completely <ref> [1] </ref>. We expect scientific application checkpoints to be read completely and sequentially. In these cases, whole file migration makes sense. However, database files tend to be large, may be accessed randomly and incompletely (depending on the application's queries), 5 and in some systems are never overwritten [13].
Reference: [2] <author> General Atomics/DISCOS Division. </author> <title> The UniTree Virtual Disk System: An Overview. </title> <note> Technical report available from DISCOS, </note> <institution> P.O. </institution> <address> Box 85608, San Diego, CA 92186, </address> <year> 1991. </year>
Reference-contexts: Unlike whole-file migration schemes such as Strange's or UniTree's <ref> [2] </ref>, we want to allow migration of portions of files rather than whole files. Our partial-file migration mechanism can support whole file migration, if desired for a particular policy.
Reference: [3] <author> D. H. Lawrie, J. M. Randal, and R. R. Barton. </author> <title> Experiments with Automatic File Migration. </title> <journal> IEEE Computer, </journal> <volume> 15(7) </volume> <pages> 45-55, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: All the possible policy components discussed below require some additional mechanism support beyond that provided by the basic 4.4BSD LFS. They require some basic migration bookkeeping and data transfer mechanisms, which are described in the next section. 4.1. File Size-based rankings Earlier studies <ref> [3, 12] </ref> conclude that file size alone does not work well for selecting files as migration candidates; they recommend using a space-time product (STP) ranking metric (time since last access, raised to a small power, times file size).
Reference: [4] <author> Marshall Kirk McKusick, William N. Joy, Samuel J. Leffler, and Robert S. Fabry. </author> <title> A Fast File System for UNIX. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(3) </volume> <pages> 181-197, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: LFS is optimized for writing data, whereas most file systems (e.g. the BSD Fast File System <ref> [4] </ref>) are optimized for reading data. LFS divides the disk into 512KB or 1MB segments, and writes data sequentially within each segment. The segments are threaded together to form a log, so recovery is quick; it entails a roll-forward of the log from the last checkpoint. <p> Obviously, as data are deleted or replaced, the log contains blocks of invalid or obsolete data, and the system must coalesce this wasted space to generate new, empty segments for the log. 4.4BSD LFS shares much of its implementation with the Berkeley Fast File System (FFS) <ref> [4] </ref>. It has two auxiliary data structures not found in FFS: the segment summary table and the inode map. The segment summary table contains information describing the state of each segment in the file system.
Reference: [5] <author> Ethan L. Miller, Randy H. Katz, and Stephen Strange. </author> <title> An Analysis of File Migration in a UNIX Supercomputing Environment. </title> <booktitle> In USENIX Association Winter 1993 Conference Proceedings, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1993. </year> <booktitle> The USENIX Association. </booktitle>
Reference-contexts: Migration policies may be considered in two parts, writing to tertiary storage, and caching from tertiary storage. Before describing our migration policies, we must first state our initial assumptions regarding file access patterns, which are based on previous analyses of systems <ref> [5, 16, 11] </ref>. Our basic assumptions are that file access patterns are skewed, and that most archived data are never re-read. However, some archived data will be accessed, and once archived data became active again, they will be accessed many times before becoming inactive again.
Reference: [6] <author> James W. Mott-Smith. </author> <title> The Jaquith Archive Server. </title> <type> UCB/CSD Report 92-701, </type> <institution> University of California, Berkeley, Berkeley, </institution> <address> CA, </address> <month> September </month> <year> 1992. </year>
Reference-contexts: HighLight is one of several file management avenues under 1 exploration as a supporting technology for this research. Other storage management efforts include the Inversion support in the POSTGRES database system [7] and the Jaquith manual archive system <ref> [6] </ref> (which was developed for other uses, but is under consideration for Sequoia's use). The bulk of the on-line storage for Sequoia will be provided by a 600-cartridge Metrum robotic tape unit; each cartridge has a capacity of 14.5 gigabytes for a total of nearly 9 terabytes.
Reference: [7] <author> Michael Olson. </author> <title> The Design and Implementation of the Inversion File System. </title> <booktitle> In USENIX Association Winter 1993 Conference Proceedings, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1993. </year> <booktitle> The USENIX Association. </booktitle>
Reference-contexts: HighLight is one of several file management avenues under 1 exploration as a supporting technology for this research. Other storage management efforts include the Inversion support in the POSTGRES database system <ref> [7] </ref> and the Jaquith manual archive system [6] (which was developed for other uses, but is under consideration for Sequoia's use).
Reference: [8] <author> Sean Quinlan. </author> <title> A Cached WORM File System. </title> <journal> SoftwarePractice and Experience, </journal> <volume> 21(12) </volume> <pages> 1289-1299, </pages> <month> December </month> <year> 1991. </year> <month> 14 </month>
Reference-contexts: In particular, the database reference patterns will be query-dependent, and will most likely be random accesses within a file rather than sequential access. Our migration scheme is most similar to that described by Quinlan <ref> [8] </ref> for the Plan 9 file system. He provides a disk cache as a front for a WORM device which stores all permanent data.
Reference: [9] <author> Mendel Rosenblum and John K. Ousterhout. </author> <title> The Design and Implementation of a Log-Structured File System. </title> <journal> Operating Systems Review, </journal> <volume> 25(5) </volume> <pages> 1-15, </pages> <month> Oc-tober </month> <year> 1991. </year>
Reference-contexts: 1. Introduction HighLight combines both conventional disk secondary storage and robotic tertiary storage into a single file system. It builds upon the 4.4BSD LFS [10], which derives directly from the Sprite Log-structured File System (LFS) <ref> [9] </ref>, developed at the University of California at Berke-ley by Mendel Rosenblum and John Ousterhout as part of the Sprite operating system. LFS is optimized for writing data, whereas most file systems (e.g. the BSD Fast File System [4]) are optimized for reading data.
Reference: [10] <author> Margo Seltzer, Keith Bostic, Marshall Kirk McKu-sick, and Carl Staelin. </author> <title> An Implementation of a Log-structured File System for UNIX. </title> <booktitle> In USENIX Association Winter 1993 Conference Proceedings, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1993. </year> <booktitle> The USENIX Association. </booktitle>
Reference-contexts: 1. Introduction HighLight combines both conventional disk secondary storage and robotic tertiary storage into a single file system. It builds upon the 4.4BSD LFS <ref> [10] </ref>, which derives directly from the Sprite Log-structured File System (LFS) [9], developed at the University of California at Berke-ley by Mendel Rosenblum and John Ousterhout as part of the Sprite operating system.
Reference: [11] <author> Alan Jay Smith. </author> <title> Analysis of Long-Term File Reference Patterns for Application to File Migration Algorithms. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-7(4):403-417, </volume> <year> 1981. </year>
Reference-contexts: We also desire to allow file system metadata, such as directories, inode blocks or indirect pointer blocks, to migrate to tertiary storage. 2 A final reason why existing systems may not be applicable to Sequoia's needs lies with the expected access patterns. Smith <ref> [11, 12] </ref> studied file references based mostly on editing tasks; Strange [16] studied a networked workstation environment used for software development in a university environment. <p> Migration policies may be considered in two parts, writing to tertiary storage, and caching from tertiary storage. Before describing our migration policies, we must first state our initial assumptions regarding file access patterns, which are based on previous analyses of systems <ref> [5, 16, 11] </ref>. Our basic assumptions are that file access patterns are skewed, and that most archived data are never re-read. However, some archived data will be accessed, and once archived data became active again, they will be accessed many times before becoming inactive again.
Reference: [12] <author> Alan Jay Smith. </author> <title> Long-Term File Migration: Development and Evaluation of Algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 24(8) </volume> <pages> 521-532, </pages> <month> August </month> <year> 1981. </year>
Reference-contexts: We also desire to allow file system metadata, such as directories, inode blocks or indirect pointer blocks, to migrate to tertiary storage. 2 A final reason why existing systems may not be applicable to Sequoia's needs lies with the expected access patterns. Smith <ref> [11, 12] </ref> studied file references based mostly on editing tasks; Strange [16] studied a networked workstation environment used for software development in a university environment. <p> All the possible policy components discussed below require some additional mechanism support beyond that provided by the basic 4.4BSD LFS. They require some basic migration bookkeeping and data transfer mechanisms, which are described in the next section. 4.1. File Size-based rankings Earlier studies <ref> [3, 12] </ref> conclude that file size alone does not work well for selecting files as migration candidates; they recommend using a space-time product (STP) ranking metric (time since last access, raised to a small power, times file size).
Reference: [13] <author> Michael Stonebraker. </author> <title> The POSTGRES Storage System. </title> <booktitle> In Proceedings of the 1987 VLDB Conference, </booktitle> <address> Brighton, England, </address> <month> September </month> <year> 1987. </year>
Reference-contexts: We expect scientific application checkpoints to be read completely and sequentially. In these cases, whole file migration makes sense. However, database files tend to be large, may be accessed randomly and incompletely (depending on the application's queries), 5 and in some systems are never overwritten <ref> [13] </ref>. Con--sequently, block-based information is useful, since old, unreferenced data may migrate to tertiary storage while active data remain on secondary storage.
Reference: [14] <author> Michael Stonebraker. </author> <title> An Overview of the Sequoia 2000 Project. </title> <type> Technical Report 91/5, </type> <institution> University of California, Project Sequoia, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: However, system performance will depend on optimizing read performance, since LFS already optimizes write performance. Therefore, migration policies and mechanisms should arrange the data on tertiary storage to improve read performance. HighLight was developed to provide a data storage file system for use by Sequoia researchers. Project Sequoia 2000 <ref> [14] </ref> is a collaborative project between computer scientists and earth science researchers to develop the necessary support structure to enable global change research on a larger scale than current systems can support. HighLight is one of several file management avenues under 1 exploration as a supporting technology for this research. <p> Unfortunately, those results may not be directly applicable for our environment, since we expect Sequoia's file system references to be generated by database, simulation, image processing, visualization, and other I/O intensive-processes <ref> [14] </ref>. In particular, the database reference patterns will be query-dependent, and will most likely be random accesses within a file rather than sequential access. Our migration scheme is most similar to that described by Quinlan [8] for the Plan 9 file system.
Reference: [15] <author> Michael Stonebraker and Michael Olson. </author> <title> Large Object Support in POSTGRES. </title> <booktitle> In Proc. 9th Int'l Conf. on Data Engineering, </booktitle> <address> Vienna, Austria, </address> <month> April </month> <year> 1993. </year> <note> To appear. </note>
Reference-contexts: Such media swaps can take many seconds to complete. 6.1. Large object performance To test performance with large objects, we used the benchmark of Stonebraker and Olson <ref> [15] </ref> to measure I/O performance on relatively large transfers. It starts with a 51.2MB file, considered a collection of 12,500 frames of 4096 bytes each (these could be database data pages, compressed images in an animation, etc). The buffer cache is flushed before each phase of the benchmark.

References-found: 15

