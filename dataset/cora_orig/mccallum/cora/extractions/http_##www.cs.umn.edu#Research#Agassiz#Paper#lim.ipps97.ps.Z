URL: http://www.cs.umn.edu/Research/Agassiz/Paper/lim.ipps97.ps.Z
Refering-URL: http://www.cs.umn.edu/Research/Agassiz/agassiz_pubs.html
Root-URL: http://www.cs.umn.edu
Email: hblim@csrd.uiuc.edu  yew@cs.umn.edu  
Title: A Compiler-Directed Cache Coherence Scheme Using Data Prefetching  
Author: Hock-Beng Lim Pen-Chung Yew 
Address: Urbana, IL 61801  Minneapolis, MN 55455  
Affiliation: Center for Supercomputing R D University of Illinois  Dept. of Computer Science University of Minnesota  
Abstract: Cache coherence enforcement and memory latency reduction and hiding are very important problems in the design of large-scale shared-memory multiprocessors. In this paper, we propose a compiler-directed cache coherence scheme which makes use of data prefetching. The Cache Coherence with Data Prefetching (CCDP) scheme uses compiler analysis techniques to identify potentially-stale data references, which are references to invalid copies of cached data. The key idea of the CCDP scheme is to enforce cache coherence by prefetching the up-to-date data corresponding to these potentially-stale references from the main memory. Application case studies were conducted to gain a quantitative idea of the performance potential of the CCDP scheme on a real system. We applied the CCDP scheme on four benchmark programs from the SPEC CFP95 and CFP92 suites, and executed them on the Cray T3D. The experimental results show that for the programs studied, our scheme provides significant performance improvements by caching shared data and reducing the remote shared-memory access penalty incurred by the programs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Arpaci, D. Culler, A. Krishnammurthy, S. Steinberg, and K. Yelick. </author> <title> Empirical evaluation of the Cray T3D : A compiler perspective. </title> <booktitle> In Proceedings of the 22th International Symposium on Computer Architecture, </booktitle> <pages> pages 320-331, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Each prefetch instruction transfers one 64-bit word of data from the memory of a remote PE to the local PE's prefetch queue. The processor then extracts the prefetched word from the queue when it is needed. The prefetch queue can only store 16 words of data. Previous studies <ref> [1, 9] </ref> indicated that the overhead of interacting with the DTB Annex and the prefetch queue is significant. Software support for shared address space and data prefetching is provided.
Reference: [2] <author> D. Bernstein, D. Cohen, A. Freund, and D. Maydan. </author> <title> Compiler techniques for data prefetching on the PowerPC. </title> <booktitle> In Proceedings of the 1995 International Conference on Parallel Architectures and Compilation Techniques, </booktitle> <pages> pages 19-26, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Software-initiated data prefetching is also an active research area. Several prefetching algorithms have been proposed and implemented using experimental compilers [8, 10, 13]. The performances of these prefetching algorithms were evaluated by simulations. Recent efforts have focused on the implementation and performance evaluation of prefetching on real systems <ref> [2] </ref>. <p> As the algorithm proceeds, the potentially-stale references which should not be prefetched are removed from the set. Thus, when the algorithm terminates, the resulting set will contain the potentially-stale references which should be prefetched. The algorithm is shown in Figure 1. Like several previous prefetching algorithms <ref> [2, 10] </ref>, our prefetch target analysis algorithm focuses on the potentially-stale references in the inner loops, where prefetching is most likely to be beneficial. Our algorithm also exploits spatial reuses to eliminate some unnecessary prefetch operations. <p> Note that it is possible to further reduce the number of unnecessary prefetch operations by also exploiting group-temporal, self-spatial, and self-temporal localities. However, this would require additional compiler analyses and transformations, such as the estimation of loop volume and loop unrolling <ref> [2, 10] </ref>. Some arbitrary decisions and approximations must be made to handle complications such as unknown loop bounds. 4.3 Prefetch Scheduling Our prefetch scheduling algorithm can generate two types of prefetch operations : vector prefetches and cache-line prefetches.
Reference: [3] <author> L. Choi. </author> <title> Hardware and Compiler Support for Cache Coherence in Large-Scale Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, Center for Supercomputing R & D, </institution> <month> Mar. </month> <year> 1996. </year>
Reference-contexts: Extensive algorithms for these techniques were previously developed by Choi and Yew <ref> [3] </ref>, and they have been implemented using the Polaris parallelizing compiler [12]. We make use of these algorithms in the CCDP scheme. The interested reader can refer to [3] for the details of these algorithms. 4.2 Prefetch Target Analysis Our prefetch target analysis algorithm makes use of simple heuristics which are <p> Extensive algorithms for these techniques were previously developed by Choi and Yew <ref> [3] </ref>, and they have been implemented using the Polaris parallelizing compiler [12]. We make use of these algorithms in the CCDP scheme. The interested reader can refer to [3] for the details of these algorithms. 4.2 Prefetch Target Analysis Our prefetch target analysis algorithm makes use of simple heuristics which are easy to implement and are likely to be effective. The algorithm starts by including all potentially-stale references of the program in the target set for prefetching. <p> Speedups over sequential execution time. doshared directive, each CCDP code assigns loop iterations directly to the PEs using a similar loop scheduling policy as the corresponding BASE code. Finally, we use a Polaris implementation of the stale reference analysis algorithms <ref> [3] </ref> to identify the potentially-stale references in the programs.
Reference: [4] <author> L. Choi, H.-B. Lim, and P.-C. Yew. </author> <title> Techniques for compiler-directed cache coherence. </title> <booktitle> IEEE Parallel & Distributed Technology, </booktitle> <pages> pages 23-34, </pages> <month> Winter </month> <year> 1996. </year>
Reference-contexts: Additional support is provided by a gift from Cray Research, Inc and by a gift from Intel Corporation. The computing resources are provided in part by a grant from the Pittsburgh Supercomputing Center through the National Science Foundation and by Cray Research, Inc. cache coherence schemes <ref> [4] </ref> offer a viable solution to the cache coherence problem for large-scale shared-memory multiprocessors. Although compiler-directed cache coherence schemes can improve multiprocessor cache performance, they cannot totally eliminate main memory accesses. Several techniques have been developed to hide memory latency in multiprocessors. <p> Finally, we conclude in Section 6 and outline the future directions of our research. 2 Related Work Several hardware-supported compiler-directed (HSCD) cache coherence schemes, which require hardware support to keep track of local cache states at run time, have been developed recently <ref> [4] </ref>. Although the HSCD schemes require less hardware support and are more scalable than the hardware directory cache coherence schemes, none of them have been implemented on real systems using off-the-shelf components yet. <p> In several existing large-scale multiprocessors, the cache coherence problem is solved by not caching shared data at all or by caching shared data only when it is safe to do so <ref> [4] </ref>. However, this approach is too conservative and does not deliver very good performance. Software-initiated data prefetching is also an active research area. Several prefetching algorithms have been proposed and implemented using experimental compilers [8, 10, 13]. The performances of these prefetching algorithms were evaluated by simulations.
Reference: [5] <author> Cray Research, Inc. </author> <title> Cray T3D System Architecture Overview, </title> <month> Mar. </month> <year> 1993. </year>
Reference-contexts: (1), (2), (3), or (4), but only prefetch within the if-part or else-part of the if-statement; endswitch endfor 5 Application Case Studies We have conducted application case studies to obtain a quantitative measure of the performance improvements which can be obtained by using the CCDP scheme on the Cray T3D <ref> [5] </ref>. We adapted the CCDP scheme to suit the hardware constraints and architectural parameters of the system. 5.1 Target Platform The Cray T3D is physically distributed memory MPP system which provides hardware support for a shared address space [11].
Reference: [6] <author> Cray Research, Inc. </author> <title> Cray MPP Fortran Reference Manual, </title> <note> Version 6.1, </note> <month> June </month> <year> 1994. </year>
Reference-contexts: Previous studies [1, 9] indicated that the overhead of interacting with the DTB Annex and the prefetch queue is significant. Software support for shared address space and data prefetching is provided. The programmer can use a compiler directive in the Cray MPP Fortran (CRAFT) language <ref> [6] </ref> to declare shared data and to specify their distribution pattern amongst the PEs. A directive called doshared is used to specify a parallel loop. In order to avoid the cache coherence problem, the shared data are not cached. <p> The parallel loop iterations are block distributed accordingly. TOMCATV is a highly vectorizable mesh generation program. The main data structures it used are 7 shared matrices of size 513 fi 513. We use the generalized distribution <ref> [6] </ref> directive to distribute the shared matrices and loop iterations in the BASE version of TOMCATV. The CCDP version of TOMCATV in turn follows a similar data and loop distribution method. Last but not least, SWIM solves a system of shallow water equations using finite difference approximations.
Reference: [7] <institution> Cray Research, Inc. </institution> <note> SHMEM User's Guide, Revision 2.0, </note> <month> May </month> <year> 1994. </year>
Reference-contexts: Second, the programmer can explicitly perform remote memory reads and writes in a cached or non-cached manner. Finally, the Cray T3D also provides a high-level shared-memory communication library called the SHMEM library <ref> [7] </ref>. The shmem get primitive in the library provides similar functionality as a vector prefetch. 5.2 Methodology We first parallelize the application codes using the Polaris compiler [12]. Then, we manually convert the parallelized codes into two versions of CRAFT programs.
Reference: [8] <author> E. Gornish. </author> <title> Compile time analysis for data prefetching. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, Center for Supercomputing R & D, </institution> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: However, this approach is too conservative and does not deliver very good performance. Software-initiated data prefetching is also an active research area. Several prefetching algorithms have been proposed and implemented using experimental compilers <ref> [8, 10, 13] </ref>. The performances of these prefetching algorithms were evaluated by simulations. Recent efforts have focused on the implementation and performance evaluation of prefetching on real systems [2]. <p> Finally, the prefetch scheduling algorithm should try to minimize the prefetch overhead. 4.3.2 Scheduling techniques Our prefetch scheduling algorithm makes use of three scheduling techniques : vector prefetch generation, software pipelining, and moving back prefetches. Vector prefetch generation Gornish <ref> [8] </ref> developed an algorithm for conservatively determining the earliest point in a program during which a block of data can be prefetched. It examines the array references in each loop to see if they could be pulled out of the loop and still satisfy the control and data dependences. <p> Even if array references can be pulled out of multiple loop levels, the prefetched data might not remain in the cache by the time the data are referenced. We adapt Gornish's approach to generate vector prefetches. The basic algorithm for pulling out array references is described in <ref> [8] </ref>. We modify the algorithm by imposing a restriction on the number of loop levels that array references should be pulled out from, in order to maximize the effectiveness of the vector prefetches. Our algorithm pulls out an array reference one loop level at a time. <p> Third, we can use this technique for prefetch targets in serial code sections, where vector prefetch generation and software pipelining are not applicable. We adapt Gornish's algorithm for pulling back references <ref> [8] </ref>. The original algorithm tries to move references as far back as possible subjected to control and data dependence constraints. However, the algorithm might move a prefetch operation so far back that the prefetched data might be replaced from the cache by the time it is used.
Reference: [9] <author> V. Karamcheti and A. Chien. </author> <title> A comparison of architectural support for messaging in the TMC CM-5 and the Cray T3D. </title> <booktitle> In Proceedings of the 22th International Symposium on Computer Architecture, </booktitle> <pages> pages 298-307, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Each prefetch instruction transfers one 64-bit word of data from the memory of a remote PE to the local PE's prefetch queue. The processor then extracts the prefetched word from the queue when it is needed. The prefetch queue can only store 16 words of data. Previous studies <ref> [1, 9] </ref> indicated that the overhead of interacting with the DTB Annex and the prefetch queue is significant. Software support for shared address space and data prefetching is provided.
Reference: [10] <author> T. Mowry. </author> <title> Tolerating Latency Through Software-Controlled Data Prefetching. </title> <type> PhD thesis, </type> <institution> Stanford University, Dept. of Electrical Engineering, </institution> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: However, this approach is too conservative and does not deliver very good performance. Software-initiated data prefetching is also an active research area. Several prefetching algorithms have been proposed and implemented using experimental compilers <ref> [8, 10, 13] </ref>. The performances of these prefetching algorithms were evaluated by simulations. Recent efforts have focused on the implementation and performance evaluation of prefetching on real systems [2]. <p> As the algorithm proceeds, the potentially-stale references which should not be prefetched are removed from the set. Thus, when the algorithm terminates, the resulting set will contain the potentially-stale references which should be prefetched. The algorithm is shown in Figure 1. Like several previous prefetching algorithms <ref> [2, 10] </ref>, our prefetch target analysis algorithm focuses on the potentially-stale references in the inner loops, where prefetching is most likely to be beneficial. Our algorithm also exploits spatial reuses to eliminate some unnecessary prefetch operations. <p> Note that it is possible to further reduce the number of unnecessary prefetch operations by also exploiting group-temporal, self-spatial, and self-temporal localities. However, this would require additional compiler analyses and transformations, such as the estimation of loop volume and loop unrolling <ref> [2, 10] </ref>. Some arbitrary decisions and approximations must be made to handle complications such as unknown loop bounds. 4.3 Prefetch Scheduling Our prefetch scheduling algorithm can generate two types of prefetch operations : vector prefetches and cache-line prefetches. <p> The vector prefetch operation will be issued only if these hardware constraints are satisfied and that the array reference should not be pulled further out. The compiler then inserts the prefetch operation into the code just before the appropriate loop. Software pipelining In Mowry's approach <ref> [10] </ref>, software pipelining is used to schedule cache-line prefetch operations. Software pipelining is an effective scheduling strategy to hide memory latency by overlapping the prefetches for a future iteration of a loop with the computation of the current iteration. We adapt the software pipelining algorithm to suit the CCDP scheme.
Reference: [11] <author> R. </author> <title> Numrich. The Cray T3D address space and how to use it. </title> <type> Tech. report, </type> <institution> Cray Research, Inc., </institution> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: We adapted the CCDP scheme to suit the hardware constraints and architectural parameters of the system. 5.1 Target Platform The Cray T3D is physically distributed memory MPP system which provides hardware support for a shared address space <ref> [11] </ref>. A special circuitry in each Processing Element (PE), called the DTB Annex, helps to translate a global logical address into its actual physical address, which is composed of the PE number and the local memory address within the PE. The Cray T3D provides simple hardware support for data prefetching. <p> Data prefetching and data transfer can be performed in several ways. First, the programmer can use an assembly library which provides functions to set the DTB Annex entries, issue prefetch requests, and extract prefetched data from the prefetch queue <ref> [11] </ref>. Second, the programmer can explicitly perform remote memory reads and writes in a cached or non-cached manner. Finally, the Cray T3D also provides a high-level shared-memory communication library called the SHMEM library [7].
Reference: [12] <author> D. A. Padua, R. Eigenmann, J. Hoeflinger, P. Peterson, P. Tu, S. Weatherford, and K. Faigin. </author> <title> Polaris: A new-generation parallelizing compiler for MPPs. </title> <type> CSRD Tech. Report 1306, </type> <institution> Univ. of Illinois at Urbana-Champaign, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Extensive algorithms for these techniques were previously developed by Choi and Yew [3], and they have been implemented using the Polaris parallelizing compiler <ref> [12] </ref>. We make use of these algorithms in the CCDP scheme. The interested reader can refer to [3] for the details of these algorithms. 4.2 Prefetch Target Analysis Our prefetch target analysis algorithm makes use of simple heuristics which are easy to implement and are likely to be effective. <p> Finally, the Cray T3D also provides a high-level shared-memory communication library called the SHMEM library [7]. The shmem get primitive in the library provides similar functionality as a vector prefetch. 5.2 Methodology We first parallelize the application codes using the Polaris compiler <ref> [12] </ref>. Then, we manually convert the parallelized codes into two versions of CRAFT programs. The baseline version, which we call the BASE version, makes use of the default software support for shared address space available in CRAFT.
Reference: [13] <author> A. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: However, this approach is too conservative and does not deliver very good performance. Software-initiated data prefetching is also an active research area. Several prefetching algorithms have been proposed and implemented using experimental compilers <ref> [8, 10, 13] </ref>. The performances of these prefetching algorithms were evaluated by simulations. Recent efforts have focused on the implementation and performance evaluation of prefetching on real systems [2].
References-found: 13

