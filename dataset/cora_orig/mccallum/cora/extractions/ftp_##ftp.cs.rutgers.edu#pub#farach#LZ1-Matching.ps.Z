URL: ftp://ftp.cs.rutgers.edu/pub/farach/LZ1-Matching.ps.Z
Refering-URL: http://www.cs.rutgers.edu/~farach/index.html
Root-URL: http://www.cs.rutgers.edu
Title: String Matching in Lempel-Ziv Compressed Strings  
Author: Martin Farach Mikkel Thorup 
Note: farach@cs.rutgers.edu;http://www.cs.rutgers.edu/~farach; Part of this work was done while the author was visiting the University of Copenhagen; Supported in part by DIMACS (Center for Discrete Mathematics and Theoretical Computer Science), a National Science Foundation Science and Technology Center under NSF contract STC-8809648. mthorup@diku.dk; This research was initiated while visiting DIMACS  
Affiliation: Rutgers University University of Copenhagen  
Abstract: String matching and Compression are two widely studied areas of computer science. The theory of string matching has a long association with compression algorithms. Data structures from string matching can be used to derive fast implementations of many important compression schemes, most notably the Lempel-Ziv (LZ1) algorithm. Intuitively, once a string has been compressed and therefore its repetitive nature has been elucidated one might be tempted to exploit this knowledge to speed up string matching. The Compressed Matching Problem is that of performing string matching in a compressed text, without un-compressing it. More formally, let T be a text, let Z be the compressed string representing T , and let P be a pattern. The Compressed Matching Problem is that of deciding if P occurs in T , given only P and Z. Compressed matching algorithms have been given for several compression schemes, such as LZW. In this paper, we give the first non-trivial compressed matching algorithm for the classic compression scheme, the LZ1 algorithm. In practice, the LZ1 algorithm is known to compresses more than other compression schemes, such as LZ2 and LZW, though for strings with constant per bit entropy, all these schemes compress optimally in the limit.However, for strings with o(1) per bit entropy, while it was recently shown that the LZ1 gives compression to within a constant factor of optimal, schemes such as LZ2 and LZW may deviate from optimality by an exponential factor. Asymptotically, compressed matching is only relevant if jZj = o(jT j), i.e. if the compression ratio jT j=jZj is more than a constant. These results show that LZ1 is the appropriate compression method in such setttings. We present an LZ1 compressed matching algorithm which runs in time O(n log 2 u=n + p) where n = jZj, u = jT j, and p = jPj. Compare with the nave "decompresion" algorithm, which takes time fi(u + p) to decide if P occurs in T . Writing u + p as n u=n + p, we see that we have improved the complexity replacing the compression factor u=n by a factor log 2 u=n. Our algorithm is competitive in the sense that O(n log 2 u=n+p) = O(u+p), and opportunistic in the sense that O(n log 2 u=n+p) = o(u + p) if n = o(u) and p = o(u). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.V. Aho, J.E. Hopcroft, and J.D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <year> 1974. </year>
Reference-contexts: In order to facilitate a more uniform treatment, we simplify the scheme by the following trivial transformation. First, map into <ref> [1; jj] </ref>. Then take each triple (P i ; L i ; C i ) and replace it by the pairs (P i ; L i )(C i ; 1). <p> A.1.3. Merge L 0 with L 1 into L, removing informers that are duplicate with respect to u-values. Using any standard balanced tree data structure <ref> [1] </ref> to implement the lists, all steps but the merge step (A.1.3) can be implemented in time O (log n), hence in time O (n log n) over all iterations.
Reference: [2] <author> A. Amir and G. Benson. </author> <title> Efficient two dimensional compressed matching. </title> <booktitle> Proc. of the 2nd IEEE Data Compression Conference, </booktitle> <pages> pages 279-288, </pages> <month> Mar </month> <year> 1992. </year>
Reference-contexts: The elimination of repetition is the key to compression. Intuitively, once a string has been compressed and therefore its repetitive nature has been elucidated one might be tempted to exploit this knowledge to speed up string matching. This idea is inherent in the work of Amir and Benson <ref> [2] </ref>, where they introduced the Compressed Matching Problem. They were motivated by the practical consideration that, increasingly, files are stored in a compressed format, and that current string matching technology requires the files be uncompressed before string matching takes place. <p> In <ref> [2, 4] </ref>, an optimal O (n + p 2 ) algorithm was derived for finding a p fi p two dimensional pattern within a file of length n.
Reference: [3] <author> A. Amir, G. Benson, and M. Farach. </author> <title> Let sleeping files lie: Pattern matching in Z-compressed files. </title> <booktitle> Proc. of the 5th Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <year> 1994. </year>
Reference-contexts: In this paper, we give a pseudo-optimal compressed matching algorithm for the classic compression scheme, the so-called LZ1 algorithm [10]. To the best of our knowledge this is the first nontrivial LZ1 compressed matching algorithm, and the search for such an algorithm was advocated in <ref> [3] </ref>, which deals with LZW compressed matching. <p> In one dimension, perhaps the most commonly used compression algorithm is the LZW algorithm [15] as implemented by the UNIX compress program [14]. In <ref> [3] </ref>, it was shown that a length p pattern can be found in a length n text in time O (n + p 2 ), or in O (n log p + p). In [9], Kosaraju improved this complexity to O (n + p 1+* ). The techniques in [3] show <p> In <ref> [3] </ref>, it was shown that a length p pattern can be found in a length n text in time O (n + p 2 ), or in O (n log p + p). In [9], Kosaraju improved this complexity to O (n + p 1+* ). The techniques in [3] show that it is trivial to perform linear time compressed matching on LZW files if the pattern is of constant length. <p> The ideas presented therein can be used directly to arrive at a deterministic algorithm with run-ning time O (n log (u=n)(log (u=n) + log p). Alternatively, combining with ideas from <ref> [3, 9] </ref>, we get an O (n (log 2 u=n) + p 1+* ) deterministic algorithm. Neither of these deterministic bounds is fully competitive. They both perform worse than the nave decompressing fi (u + p) algorithm if the pattern is large and the compression ratio is low. <p> Thus we get an algorithm which is both opportunistic and competitive. In fact this last technique also applies to the best known compressed matching algorithm for LZW <ref> [3] </ref> to yield a O (n log u=n+p) time algorithm. Thus a side-effect of this paper is the first competitive and opportunistic LZW compressed matching algorithm. 1.3 THE LZ1 ALGORITHM The LZ1 algorithm gives a very natural way of representing a string and is defined as follows.
Reference: [4] <author> A. Amir, G. Benson, and M. Farach. </author> <title> Optimal two-dimensional compressed matching. </title> <booktitle> Proc. of 21st International Colloquium on Automata Languages and Programming, </booktitle> <year> 1994. </year>
Reference-contexts: In <ref> [2, 4] </ref>, an optimal O (n + p 2 ) algorithm was derived for finding a p fi p two dimensional pattern within a file of length n.
Reference: [5] <author> Corman, Leiserson, and Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: But then F [if; (i + k)f 1] can be found in constant time since F [if; (i+k)f1] = (F [0; (i+k)f1]F [0; if1]) 1if mod -. Computing the needed powers of (mod -) is standard (see <ref> [5, Chapter 33] </ref>). We are now ready to describe the unwinding. Besides the above information, an informer at po sition j will contain the fingerprint of the character T [j].
Reference: [6] <author> M. Gu, M. Farach, and R. Beigel. </author> <title> An efficient algorithm for dynamic text indexing. </title> <booktitle> Proc. of the 5th Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <year> 1994. </year>
Reference-contexts: Having applied an O (p) pattern preprocessing technique from <ref> [6] </ref>, each of these operations are done in O (log p) time, giving a total complexity for the compressed pattern matching problem of O (p + n log (u=n)(log (u=n) + log p)). <p> We will use a function LeftAppend (ff; fi) which, given substrings ff; fi of the pattern, returns the longest suffix of fffi which is a substring of the pattern. In <ref> [6] </ref>, an algorithm for LeftAppend was given which preprocesses a string of length p in O (p) time and answers such queries in O (log p) time. The basic idea is to log the winding of Algorithm A, so that we can reverse it directly in the unwinding. <p> In conclusion, the unwinding is done with O (n log u=n) list operations and calls to LeftAppend. Theorem 4.3 We can load all informers in time O (n log (u=n)(log u=n + log p)). In <ref> [6] </ref>, an algorithm was given to compute if a pattern occurs within the concatenation of two of its substrings. This operation take O (p) preprocessing on a string of length p and answers such queries in O (log p) time. This result suffices for the following.
Reference: [7] <author> R.M. Karp and M.O. Rabin. </author> <title> Efficient randomized pattern-matching algorithms. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 31 </volume> <pages> 249-260, </pages> <year> 1987. </year>
Reference-contexts: Neither of these deterministic bounds is fully competitive. They both perform worse than the nave decompressing fi (u + p) algorithm if the pattern is large and the compression ratio is low. However, we give a novel application of Karp and Rabin's fingerprint method <ref> [7] </ref>, which gives us a randomized O (n log 2 (u=n) + p) time algorithm for LZ1 compressed matching. Thus we get an algorithm which is both opportunistic and competitive. <p> So the real question is how to compute the M i , which we do as follows. Rather than compute the M i themselves, we will show how to compute the fingerprints of the M i . The notion of a string fingerprint was introduced by Karp and Rabin <ref> [7] </ref>. They showed that if a string over an alphabet of size is taken to be a base number, and fur ther taken modulo some random prime -, then two distinct strings have the same such fingerprint with probability something like 1=-.
Reference: [8] <author> S. Rao Kosaraju. </author> <title> On the entropy estimation of low entropy sources. </title> <type> Personal Communication, </type> <year> 1994. </year>
Reference-contexts: In practice, the LZ1 algorithm is known to compresses more than other compression schemes, such as LZ2 and LZW, though for strings with constant per bit entropy, all these schemes compress optimally in the limit.However, for strings with o (1) per bit entropy, while Kosaraju <ref> [8] </ref> recently showed that the LZ1 gives compression to within a constant factor of optimal, schemes such as LZ2 and LZW can deviate from optimality by an exponential factor. 1.1 PREVIOUS RESULTS The theory of compressed matching was initiated with the study of two dimensions run-length compression, the compression algorithm used
Reference: [9] <author> S. Rao Kosaraju. </author> <title> Pattern matching in compressed texts. </title> <type> Manuscript, </type> <year> 1994. </year>
Reference-contexts: In [3], it was shown that a length p pattern can be found in a length n text in time O (n + p 2 ), or in O (n log p + p). In <ref> [9] </ref>, Kosaraju improved this complexity to O (n + p 1+* ). The techniques in [3] show that it is trivial to perform linear time compressed matching on LZW files if the pattern is of constant length. <p> The ideas presented therein can be used directly to arrive at a deterministic algorithm with run-ning time O (n log (u=n)(log (u=n) + log p). Alternatively, combining with ideas from <ref> [3, 9] </ref>, we get an O (n (log 2 u=n) + p 1+* ) deterministic algorithm. Neither of these deterministic bounds is fully competitive. They both perform worse than the nave decompressing fi (u + p) algorithm if the pattern is large and the compression ratio is low.
Reference: [10] <author> A. Lempel and J. Ziv. </author> <title> On the complexity of finite sequences. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 22 </volume> <pages> 75-81, </pages> <year> 1976. </year>
Reference-contexts: Each of these fields has its own very rich theory. The theory of string matching has a long association with compression algorithms. Data structures from string matching can be used to derive fast implementations of many important compression schemes [12, 13], most notably the Lempel-Ziv algorithm <ref> [10] </ref>. The reason for this connection is clear; string algorithms and data structures most commonly detect repetitions of various sorts within strings. The elimination of repetition is the key to compression. <p> In this paper, we give a pseudo-optimal compressed matching algorithm for the classic compression scheme, the so-called LZ1 algorithm <ref> [10] </ref>. To the best of our knowledge this is the first nontrivial LZ1 compressed matching algorithm, and the search for such an algorithm was advocated in [3], which deals with LZW compressed matching.
Reference: [11] <author> D. Sleator and R. Tarjan. </author> <title> A data structure for dynamic trees. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 24, </volume> <year> 1983. </year>
Reference-contexts: See cutting and linking trees in <ref> [11] </ref>. As a result, the work connected with maintaining pref and char is O (n log f).
Reference: [12] <author> L. Stauffer and D. Hirschberg. </author> <title> Pram algorithms for static dictionary compression. </title> <booktitle> 8th International Parallel Processing Symposium, </booktitle> <year> 1994. </year>
Reference-contexts: Each of these fields has its own very rich theory. The theory of string matching has a long association with compression algorithms. Data structures from string matching can be used to derive fast implementations of many important compression schemes <ref> [12, 13] </ref>, most notably the Lempel-Ziv algorithm [10]. The reason for this connection is clear; string algorithms and data structures most commonly detect repetitions of various sorts within strings. The elimination of repetition is the key to compression.
Reference: [13] <author> J. Storer. </author> <title> Data compression: methods and theory. </title> <publisher> Computer Science Press, </publisher> <address> Rockville, Maryland, </address> <year> 1988. </year>
Reference-contexts: Each of these fields has its own very rich theory. The theory of string matching has a long association with compression algorithms. Data structures from string matching can be used to derive fast implementations of many important compression schemes <ref> [12, 13] </ref>, most notably the Lempel-Ziv algorithm [10]. The reason for this connection is clear; string algorithms and data structures most commonly detect repetitions of various sorts within strings. The elimination of repetition is the key to compression.
Reference: [14] <author> T. A. Welch. </author> <title> A technique for high--performance data compression. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 17 </volume> <pages> 8-19, </pages> <year> 1984. </year>
Reference-contexts: In one dimension, perhaps the most commonly used compression algorithm is the LZW algorithm [15] as implemented by the UNIX compress program <ref> [14] </ref>. In [3], it was shown that a length p pattern can be found in a length n text in time O (n + p 2 ), or in O (n log p + p). In [9], Kosaraju improved this complexity to O (n + p 1+* ).
Reference: [15] <author> J. Ziv and A. Lempel. </author> <title> A universal algorithm for sequential data compression. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-23(3):337-343, </volume> <year> 1977. </year>
Reference-contexts: In [2, 4], an optimal O (n + p 2 ) algorithm was derived for finding a p fi p two dimensional pattern within a file of length n. In one dimension, perhaps the most commonly used compression algorithm is the LZW algorithm <ref> [15] </ref> as implemented by the UNIX compress program [14]. In [3], it was shown that a length p pattern can be found in a length n text in time O (n + p 2 ), or in O (n log p + p).
References-found: 15

