URL: http://www.cs.umn.edu/Users/dept/users/yew/Benchmarks/splash2/splash.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/yew/Benchmarks/splash2/
Root-URL: http://www.cs.umn.edu
Title: SPLASH: Stanford Parallel Applications for Shared-Memory  
Author: Jaswinder Pal Singh, Wolf-Dietrich Weber and Anoop Gupta 
Address: CA 94305  
Affiliation: Computer Systems Laboratory Stanford University,  
Abstract: We present the Stanford Parallel Applications for Shared-Memory (SPLASH), a set of parallel applications for use in the design and evaluation of shared-memory multiprocessing systems. Our goal is to provide a suite of realistic applications that will serve as a well-documented and consistent basis for evaluation studies. We describe the applications currently in the suite in detail, discuss and compare some of their important characteristics|such as data locality, granularity, synchronization, etc.|and explore their behavior by running them on a real multiprocessor as well as on a simulator of an idealized parallel architecture. We expect the current set of applications to act as a nucleus for a suite that will grow with time.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.J. Dongarra, J.L. Martin and J. Worlton, </author> <title> "Evaluating Computers and Their Performance: Perspectives, Pitfalls, and Paths," </title> <type> IBM Research Report 12904, </type> <month> April, </month> <year> 1987. </year> <title> [2] "SPEC Benchmark Suite Release 1.0," </title> <month> October, </month> <year> 1989. </year>
Reference-contexts: Drawn from several scientific and engineering problem domains, the applications are intended as a design aid for architects and software people working in the area of shared-memory multiprocessing. The use of real applications for studying system performance, however is not without pitfalls. Dongarra et al. <ref> [1] </ref> discuss some of these in the context of sequential and vector computing. <p> This information should help the user to select the applications appropriate for her studies, and it provides some idea of the coverage achieved with SPLASH. Some basic information about the different applications is presented in Table 1. Following the terminology of Dongarra et al. <ref> [1] </ref>, our applications can be characterized as whole applications.
Reference: [3] <editor> E.L. Lusk and R.A. Overbeek, </editor> <title> "Use of Monitors in FORTRAN: A Tutorial on the Barrier, Self-scheduling DO-Loop, and Askfor Monitors," </title> <type> Tech. Report No. ANL-84-51, Rev. 1, </type> <institution> Argonne National Laboratory, </institution> <month> June </month> <year> 1987. </year>
Reference-contexts: and people who want to be added to or deleted from the mailing list for updates should send electronic mail to that account. 3 The Programming Model Most of the programs in this suite are written in C (one is in FORTRAN), using the parmacs macros from Ar-gonne National Laboratory <ref> [3] </ref> for parallel constructs. The programs assume a number of tasks (Unix processes) operating on a single shared address space. Typically, the initial or parent process spawns off a number of child processes, one per additional processor to be used 1 .
Reference: [4] <author> J.P. Singh, J.L. Hennessy and A. Gupta, </author> <title> "Scaling Parallel Programs for Multiprocessors: Methodology and Examples," </title> <note> submitted for publication. [5] "Using the Encore Multimax," Tech. Mem. No. 65, Rev. 1, </note> <institution> Math. and Comp. Sci. Division, Argonne National Laboratory, </institution> <month> Feb. </month> <year> 1987. </year>
Reference-contexts: In general, we find that scaling under a constraint on execution time is most realistic for these applications, and that under realistic forms of time-constrained scaling (in which other application parameters are scaled along with the input data set size <ref> [4, 21] </ref>) the communication to computation ratio grows slowly and the granularity between synchronization points decreases as larger problems are run on larger machines. Also, the input data set size grows less than proportionally to the number of processors. Table 3 summarizes the first four behavioral characteristics for every application.
Reference: [6] <author> J.J. Dongarra, J. Bunch, C. Moler and G. Stewart, </author> <title> "LINPACK Users' Guide," </title> <publisher> SIAM Pub., </publisher> <address> Philadelphia, </address> <year> 1976. </year>
Reference: [7] <author> H. Davis, S. Goldschmidt and J.L. Hennessy, </author> <title> "Tango: a Multiprocessor Simulation and Tracing System," </title> <type> Tech. Report No. </type> <institution> CSL-TR-90-439, Stanford University, </institution> <year> 1990. </year> <month> 40 </month>
Reference-contexts: Timing measurements were made with no other user applications running on the machine. 5.2 The Simulator There are two parts to the simulator we use: the Tango reference generator <ref> [7] </ref> which runs the application and produces a parallel memory reference stream, and a memory system simulator which processes these references and feeds timing information back to the reference generator. The simulator runs on a DECstation 5000.
Reference: [8] <author> J.P. Singh and J.L. Hennessy, </author> <title> "Finding and Exploiting Parallelism in an Ocean Simulation Program: Ex--perience, Results and Implications," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 15, No. 1, </volume> <month> May </month> <year> 1992, </year> <pages> pp. 27-48. </pages> <note> Preliminary version available as Tech. Report No. </note> <institution> CSL-TR-89-388, Stanford University, </institution> <month> Aug. </month> <year> 1989. </year>
Reference-contexts: However, the grid-based application is well suited to parallelism. The work done every time-step essentially involves setting up and solving a set of spatial partial differential equations, details of which can be found in <ref> [8] </ref>. The continuous functions are transformed into discrete counterparts by second-order finite-differencing, and the resulting difference equations set up and solved on two-dimensional fixed-size grids representing horizontal cross-sections of the ocean basin. <p> We simulate a square grid of size 98-by-98 points, and we use the same (constant) resolution in both dimensions. The original sequential program used a block cyclic reduction algorithm to solve the elliptic equations (see <ref> [8] </ref>); the parallel programs use an iterative method: Gauss-Seidel with Successive Over Relaxation (SOR) [8, 9]. This iterative solver works well for coarse grid resolutions such as the one we use. For finer resolutions, a more sophisticated solver may be necessary. We have developed a multigrid solver for this purpose. <p> We simulate a square grid of size 98-by-98 points, and we use the same (constant) resolution in both dimensions. The original sequential program used a block cyclic reduction algorithm to solve the elliptic equations (see [8]); the parallel programs use an iterative method: Gauss-Seidel with Successive Over Relaxation (SOR) <ref> [8, 9] </ref>. This iterative solver works well for coarse grid resolutions such as the one we use. For finer resolutions, a more sophisticated solver may be necessary. We have developed a multigrid solver for this purpose. <p> After this, the outermost loop of the program iterates over a fixed number of time-steps, each performing a number of computations on entire grids. Parallelism is afforded at a hierarchy of levels, both across and within grid computations (see <ref> [8] </ref>). The high-level structure of the parallel program within a time-step is shown in Figure 1. Grid computations in the same horizontal section in this figure are independent of one another. Those in the same vertical section follow a thread of dependence. <p> The Jacobians and Laplacians are near-neighbor computations (9-point and 5-point stencils, respectively) with different input and output arrays, while the equation solution is an in-place near-neighbor iteration to convergence with a 5-point stencil. An understanding of the equation system and solution method can be obtained from <ref> [8] </ref>. 2 Geostrophic: relating to the deflective forces caused by the rotation of the earth. 8 Initialize and g g Put Laplacian of in Y 1 W1 1 Put Laplacian of in Y Put - Y Y 3 Put computed values in W3 Y 2 Copy , into , Y Y
Reference: [9] <author> G.H. Golub and C.F. Van Loan, </author> <title> Matrix Computations, Second Edition, </title> <journal> Chap. </journal> <volume> 10, </volume> <publisher> The Johns Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: We simulate a square grid of size 98-by-98 points, and we use the same (constant) resolution in both dimensions. The original sequential program used a block cyclic reduction algorithm to solve the elliptic equations (see [8]); the parallel programs use an iterative method: Gauss-Seidel with Successive Over Relaxation (SOR) <ref> [8, 9] </ref>. This iterative solver works well for coarse grid resolutions such as the one we use. For finer resolutions, a more sophisticated solver may be necessary. We have developed a multigrid solver for this purpose.
Reference: [10] <author> C.W.Gear, </author> <title> Numerical Initial Value Problems in Ordinary Differential Equations, </title> <publisher> Prentice-Hall, </publisher> <address> New Jersey, </address> <year> 1971. </year>
Reference-contexts: The computation is performed over a user-specified number of time-steps, hopefully allowing the system to reach a steady state. Every time-step involves setting up and solving the Newtonian equations of motion for water molecules in a cubical box with periodic boundary conditions, using Gear's sixth-order predictor-corrector method <ref> [10] </ref>. The total potential is computed as the sum of intra- and intermolecular potentials. To avoid computing all the n 2 2 pairwise interactions among molecules, a spherical cutoff range is used with radius equal to half the box length.
Reference: [11] <author> J.P. Singh and J.L. Hennessy, </author> <title> "Data Locality and Memory System Performance in the Parallel Simulation of Ocean Eddy Currents," </title> <booktitle> Proceedings of the Second Symposium on High Performance Computing, </booktitle> <address> Mont-pelier, France, </address> <month> October </month> <year> 1991. </year> <note> Also Tech. Report. No. </note> <institution> CSL-TR-91-490, Stanford University, </institution> <month> Aug. </month> <year> 1991. </year> <note> Available by anonymous ftp from samay.stanford.edu (file papers/ocean-locality.ps). </note>
Reference-contexts: In fact, as long as the grid data structures are traversed in the appropriate way (column-major in FORTRAN), the prefetching advantages of long cache lines overwhelm the disadvantages of false-sharing and fragmentation in this application, and provide substantial performance benefits <ref> [11] </ref>. This is primarily because the organization of data structures (arrays each representing the values of a variable over all grid points, rather than an array of records, each representing all the variables for a grid point) matches the access patterns of the program. <p> if the machine's caches cannot hold their partitions of the entire data set, owing to either limited size or cache mapping collisions among the many distinct grids (which can substantially degrade performance unless care is taken to minimize such collisions by adjusting the layout of grids relative to one another <ref> [11] </ref>). Since grid points within a processor's partition are never written by any other processor, misses to these points (which constitute most of the capacity misses) can always be satisfied locally. <p> Finally, the near-neighbor communication patterns also allow convenient exploitation of geographic locality in an interconnection network with nonuniform distances between processors (for example, a mesh or hypercube). A more detailed discussion of data locality and memory system performance in this application can be found in <ref> [11] </ref>. 7.2.2 Synchronization and Granularity Mutual exclusion, enforced with locks, is required in obtaining a process identifier and in two other situations in this application: when every process accumulates its private sum into a shared sum in computing a matrix integral, and when processors communicate through a shared convergence flag in <p> Since we expect this ratio to typically be large, the fraction of references that access synchronization variables is very small and we call the program large-grained. Even 4 If partitioning is done in square subgrids, data distribution in units of pages might not be quite so easy (see <ref> [11] </ref>). 10 when barriers are used, the cost of each is at worst proportional to the number of processors, not to the size of the grid. The number of barriers per time-step scales with the problem size only to the extent that the number of iterations to convergence does.
Reference: [12] <author> J.P. Singh and J.L. Hennessy, </author> <title> "Automatic and Explicit Parallelization of an N-body Simulation," </title> <note> submitted for publication. </note>
Reference-contexts: The box length is computed by the program to be large enough to hold all the molecules. Double-precision accuracy is required for this simulation, which can be used to predict a variety of static and dynamic properties of liquid water. Further documentation of the program can be found in <ref> [12, 13] </ref>, and details of the physical models in [14, 15, 16]. The sequential program, written in FORTRAN, is one of the Perfect Club set of supercomputing bench marks [17].
Reference: [13] <author> J.P. Singh and J.L. Hennessy, </author> <title> "An Empirical Investigation of the Effectiveness and Limitations of Automatic Parallelization," </title> <booktitle> Proceedings of the International Symposium on Shared Memory Multiprocessing, </booktitle> <address> Tokyo, </address> <month> April </month> <year> 1991. </year> <note> Also Tech. Report. No. </note> <institution> CSL-TR-91-462 Stanford University, </institution> <year> 1991. </year> <note> Available by anonymous ftp from samay.stanford.edu (file papers/compilers.ps). </note>
Reference-contexts: The box length is computed by the program to be large enough to hold all the molecules. Double-precision accuracy is required for this simulation, which can be used to predict a variety of static and dynamic properties of liquid water. Further documentation of the program can be found in <ref> [12, 13] </ref>, and details of the physical models in [14, 15, 16]. The sequential program, written in FORTRAN, is one of the Perfect Club set of supercomputing bench marks [17].
Reference: [14] <author> G.C. Lie and E.Clementi, </author> <title> "Molecular-Dynamics Simulation of Liquid Water with an ab initio Flexible Water-Water Interaction Potential," </title> <journal> Physical Review, </journal> <volume> Vol. A33, </volume> <pages> pp. 2679 ff., </pages> <year> 1986. </year>
Reference-contexts: Double-precision accuracy is required for this simulation, which can be used to predict a variety of static and dynamic properties of liquid water. Further documentation of the program can be found in [12, 13], and details of the physical models in <ref> [14, 15, 16] </ref>. The sequential program, written in FORTRAN, is one of the Perfect Club set of supercomputing bench marks [17].
Reference: [15] <author> O. Matsuoka, E.Clementi and M. Yoshimine, </author> <title> "CI Study of the Water Dimer Potential Surface," </title> <journal> Journal of Chemical Physics, </journal> <volume> Vol. 64, No. 4, </volume> <pages> pp. 1351-61, </pages> <month> Feb. </month> <year> 1976. </year>
Reference-contexts: Double-precision accuracy is required for this simulation, which can be used to predict a variety of static and dynamic properties of liquid water. Further documentation of the program can be found in [12, 13], and details of the physical models in <ref> [14, 15, 16] </ref>. The sequential program, written in FORTRAN, is one of the Perfect Club set of supercomputing bench marks [17].
Reference: [16] <author> R. Bartlett, I. Shavitt and G. Purvis, </author> <title> "The Quartic Force Field of H 2 O Determined by Many-Body Methods that Include Quadruple Excitation Effects," </title> <journal> Journal of Chemical Physics, </journal> <volume> Vol. 71, No. 1, </volume> <pages> pp. 281-291, </pages> <month> July </month> <year> 1979. </year>
Reference-contexts: Double-precision accuracy is required for this simulation, which can be used to predict a variety of static and dynamic properties of liquid water. Further documentation of the program can be found in [12, 13], and details of the physical models in <ref> [14, 15, 16] </ref>. The sequential program, written in FORTRAN, is one of the Perfect Club set of supercomputing bench marks [17].
Reference: [17] <author> M. Berry et. al., </author> <title> "The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers," </title> <type> CSRD Report No. 827, </type> <institution> Center for Supercomputing Research and Develpment, Urbana, Illinois, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Further documentation of the program can be found in [12, 13], and details of the physical models in [14, 15, 16]. The sequential program, written in FORTRAN, is one of the Perfect Club set of supercomputing bench marks <ref> [17] </ref>. The parallel program is written in C, with significantly modified data structures. 8.1 Principal Data Structures The main data structure used in the Perfect Club benchmark is a large, one-dimensional array called VAR.
Reference: [18] <author> J.E. Barnes and P. Hut, </author> <title> "A Hierarchical O(N log N) Force Calculation Algorithm", </title> <journal> Nature, </journal> <volume> Vol. 324, No. 4, </volume> <pages> pp. 446-449, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Since an O (n 2 ) complexity makes simulating 18 large systems impractical, hierarchical tree-based methods have been developed that reduce the complexity to O (n log n) <ref> [18] </ref> for general distributions, or even O (n) for uniform distributions [22]. This application uses the O (n log n) Barnes-Hut algorithm 8 . The Barnes-Hut algorithm is based on a hierarchical octree representation of space in three dimensions (in two dimensions, a quadtree representation is used) 9 .
Reference: [19] <author> G.C. Fox, </author> <title> "A Graphical Approach to Load Balancing and Sparse Matrix Vector Multiplication on the Hypercube", in Numerical Algorithms for Modern Parallel Computer Architectures, </title> <editor> ed. M. Schultz, </editor> <publisher> Springer-Verlag, </publisher> <year> 1988, </year> <pages> pp. 37-62. </pages>
Reference-contexts: Enough locality is obtained within each of these cells, and the number of transitions between them is negligible in comparison. Orthogonal Recursive Bisection (ORB) ORB <ref> [19] </ref> is a more robust technique for preserving physical locality, because its actually partitions space rather than the Barnes-Hut tree. The tree is not used in the partitioning process at all.
Reference: [20] <author> J.K. Salmon, </author> <title> "Parallel Hierarchical N-body Methods", </title> <type> Ph.D. Thesis, </type> <institution> California Insitute of Technology, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Further details of implementing ORB are omitted for reasons of space; a more detailed description of its application to this problem is provided by Salmon <ref> [20] </ref>. ORB introduces several new data structures, including a separate binary ORB tree whose nodes are the recursively divided subspaces with their processor subsets, and whose leaves are the final spatial partitions.
Reference: [21] <author> J.P. Singh, J.L. Hennessy and A. Gupta, </author> <title> "Implications of Hierarchical N-Body Techniques for Multiprocessor Architecture", </title> <type> Technical Report CSL-TR-92-506, </type> <institution> Stanford University, </institution> <month> February </month> <year> 1992. </year> <note> Updated version available by anonymous ftp from samay.stanford.edu (file nbody-arch.ps). </note>
Reference-contexts: In general, we find that scaling under a constraint on execution time is most realistic for these applications, and that under realistic forms of time-constrained scaling (in which other application parameters are scaled along with the input data set size <ref> [4, 21] </ref>) the communication to computation ratio grows slowly and the granularity between synchronization points decreases as larger problems are run on larger machines. Also, the input data set size grows less than proportionally to the number of processors. Table 3 summarizes the first four behavioral characteristics for every application. <p> In fact, it is the temporal locality provided by caching that is the key form of locality in the application <ref> [21] </ref>. <p> Three parameters are likely to be scaled to use more computing power: the number of bodies n, the force-calculation accuracy , and the physical time-step t. These parameters make the following contributions to the total simulation error <ref> [21] </ref>: * n: The error from the increased relaxation rate due to Monte Carlo sampling scales as 1 p n ; thus, an increase in n by a factor of k leads to a decrease in simulation error by a factor of p * t: The leap-frog method used to integrate <p> However, starting from a point where the errors are balanced, this scaling rule applies. Results for how communication and computation scale under different scaling models using this rule can be found in <ref> [21] </ref>. Essentially, under the most appropriate method of scaling (scaling all application parameters appropriately under a constraint on execution time), the communication to computation ratio increases, just as in the Ocean application.
Reference: [22] <author> L. Greengard and V. Rokhlin, </author> <title> "A Fast Algorithm for Particle Simulation", </title> <journal> Journal of Computational Physics, </journal> <volume> Vol. 73, No. 325, </volume> <year> 1987. </year>
Reference-contexts: Since an O (n 2 ) complexity makes simulating 18 large systems impractical, hierarchical tree-based methods have been developed that reduce the complexity to O (n log n) [18] for general distributions, or even O (n) for uniform distributions <ref> [22] </ref>. This application uses the O (n log n) Barnes-Hut algorithm 8 . The Barnes-Hut algorithm is based on a hierarchical octree representation of space in three dimensions (in two dimensions, a quadtree representation is used) 9 . <p> We look forward to expanding the set with other parallel applications from the user community. In the near future, we will add the following applications or versions of applications: * A galactic simulation using the Fast Multipole Method <ref> [22] </ref>. * A radiosity application from computer graphics that uses a hierarchical solution method. * A version of the Ocean application with a multigrid solver instead of SOR. * A version of the Water application with the spatial data structure to make the intermolecular force calculation algorithm O (n).
Reference: [23] <author> J.P. Singh, C. Holt, T. Totsuka, A. Gupta and J.L. Hennessy, </author> <title> "Load Balancing and Data Locality in Hierarchical N-body Methods", </title> <type> Technical Report CSL-TR-92-505, </type> <institution> Stanford University, </institution> <month> February </month> <year> 1992. </year> <note> Updated version available by anonymous ftp from samay.stanford.edu (file nbody-sched.ps). </note>
Reference-contexts: the nonuniformity of the domain, which leads to highly nonuniform distributions of workload and communication among the units of parallelism (particles); the dynamically changing nature of the particle distribution; the need for unstructured, long-range communication; and the fact that different phases of computation (see Figure 7) have different preferred partitionings <ref> [23] </ref>. This program focuses its partitioning efforts on the force-computation phase, since it is the most time-consuming. The partitioning is not modified for other phases since the overhead of doing so (both in partitioning and in the loss of locality) outweighs the potential benefits. <p> Such partitions minimize interprocessor communication and maximize data reuse. The program uses one of the following two partitioning techniques (determined by a compile-time flag, see Section 9.3) to provide both load balancing and data locality <ref> [23] </ref>. Costzones The Barnes-Hut algorithm already has a representation of the spatial distribution encoded in its tree data structure. In the costzones partitioning scheme, the tree is conceptually laid out in a two-dimensional plane, with a cell's children laid out from left to right in increasing order of child number.
Reference: [24] <author> David R. Cheriton, Hendrik A. Goosen, and Philip Machanick, </author> <title> "Restructuring a parallel simulation to improve cache behavior in a shared-memory multiprocessor: A first experience, </title> <booktitle> 1990," to appear in Proc. International Symposium on Shared-Memory Multiprocessing, </booktitle> <month> April </month> <year> 1991. </year> <month> 41 </month>
Reference-contexts: MP3D was developed and initially parallelized in the Aeronautics and Astronautics department at Stanford. Several enhanced versions have since been developed, and a restructuring study is described in <ref> [24] </ref>. 10.1 Principal Data Structures Two large arrays of structures account for more than 99% of the static data space used by MP3D. The first one stores the state information for each molecule, and occupies 36 bytes per molecule. <p> In the 64-processor run, the overall miss rate was 22.1%. These misses are almost entirely invalidation misses. Since each molecule is assigned to a fixed processor, the space array is responsible for most of the misses. Assigning regions of the space array to different processors, as was done in <ref> [24] </ref>, is one approach to reducing the number of space array misses. With a realistic problem, we would expect the total data space used by the application to be much larger than the available cache space.
Reference: [25] <author> Jeffrey D. McDonald, </author> " <title> A direct particle simulation method for hypersonic rarified flow," </title> <type> CS 411 Final Project Report, </type> <institution> Stanford University, </institution> <month> March </month> <year> 1988. </year>
Reference-contexts: After a steady-state is reached, statistical analysis of the trajectory data produces an estimated flow field for the configuration under study. To obtain accurate results, such methods require large amounts of computation. Vectorized and parallelized codes have, therefore, been developed <ref> [25] </ref>. MP3D employs five degree-of-freedom simulation of idealized diatomic molecules in a three-dimensional active space. There are three translational freedoms and two rotational energy modes. The active space is a rectangular tunnel with openings at each end and reflecting walls on the remaining sides.
Reference: [26] <author> J.S. Rose, "LocusRoute: </author> <title> a parallel global router for standard cells," </title> <booktitle> Proc. 25th Design Automation Conference, </booktitle> <pages> pages 189-195, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: With a realistic problem, we would expect the total data space used by the application to be much larger than the available cache space. This means that the application basically sweeps the caches during each time-step, resulting in high miss rates due to replacement. 11 LocusRoute LocusRoute <ref> [26, 27, 28] </ref> is a commercial quality VLSI standard cell router. It is used to evaluate standard cell circuit placements by routing them efficiently and determining the area of the resulting layout.
Reference: [27] <author> J.S. Rose, </author> <title> "The parallel decomposition and implementation of an integrated circuit global router," </title> <booktitle> ACM Sigplan Symposium on Parallel Programming: Experience with Applications, Languages and Systems, </booktitle> <pages> pages 138-145, </pages> <month> July </month> <year> 1988. </year> <month> Sep. </month> <year> 1990. </year>
Reference-contexts: With a realistic problem, we would expect the total data space used by the application to be much larger than the available cache space. This means that the application basically sweeps the caches during each time-step, resulting in high miss rates due to replacement. 11 LocusRoute LocusRoute <ref> [26, 27, 28] </ref> is a commercial quality VLSI standard cell router. It is used to evaluate standard cell circuit placements by routing them efficiently and determining the area of the resulting layout.
Reference: [28] <author> J.S. Rose, </author> <title> "Parallel global routing for standard cells", </title> <journal> IEEE Trans. Computer-Aided Design of Circuits and Systems, </journal> <month> September </month> <year> 1990. </year>
Reference-contexts: With a realistic problem, we would expect the total data space used by the application to be much larger than the available cache space. This means that the application basically sweeps the caches during each time-step, resulting in high miss rates due to replacement. 11 LocusRoute LocusRoute <ref> [26, 27, 28] </ref> is a commercial quality VLSI standard cell router. It is used to evaluate standard cell circuit placements by routing them efficiently and determining the area of the resulting layout.
Reference: [29] <author> K. M. Chandy and J. Misra, </author> <title> "Asynchronous Distributed Simulation Via a Sequence of Parallel Computations," </title> <journal> Comm of the ACM, </journal> <volume> 24:11, </volume> <pages> pages 198-206, </pages> <month> April </month> <year> 1981. </year>
Reference-contexts: Number of Miss Rate Synchronization Processors (%) Waiting Time (%) 1 0.20 0.00 4 1.42 0.00 16 3.17 0.00 64 5.08 0.02 PTHOR uses a variant of the Chandy-Misra <ref> [29] </ref> distributed-time algorithm (denoted CM). The CM algorithm will be described only briefly here; for an in-depth treatment see [30].
Reference: [30] <author> Larry Soule and Anoop Gupta. </author> <title> "Analysis of parallelism and deadlocks in distributed-time logic simulation," </title> <type> Technical Report CSL-TR-89-378, </type> <institution> Stanford University, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: Number of Miss Rate Synchronization Processors (%) Waiting Time (%) 1 0.20 0.00 4 1.42 0.00 16 3.17 0.00 64 5.08 0.02 PTHOR uses a variant of the Chandy-Misra [29] distributed-time algorithm (denoted CM). The CM algorithm will be described only briefly here; for an in-depth treatment see <ref> [30] </ref>. While the standard event-driven algorithm maintains a common value of the current simulated time for the entire circuit, CM allows every element to advance its own value of time independently of other elements. As a result, different elements might have different notions of the current simulated time.
Reference: [31] <author> I. Duff, R. Grimes, and J. Lewis, </author> <title> "Sparse matrix test problems," </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 15 </volume> <pages> 1-14, </pages> <year> 1989. </year>
Reference-contexts: The program also verifies that the computed factor is correct once the factorization is complete. Two input matrices are provided. Both come from the Boeing/Harwell sparse matrix test set <ref> [31] </ref>. bcsttk14 is a 1806-by-1806 matrix with 30,824 non-zeros in the matrix and 110,461 in the factor; it has 503 distinct supernodes, the largest of which contains 135 columns. The corresponding numbers for the larger matrix bcsttk15 are 3948-by-3948, 56934, 647274, 1295 and 211, respectively.
Reference: [32] <author> A. George, M. Heath, J. Liu, and E. Ng, </author> <title> "Solution of sparse positive definite systems on a hypercube," </title> <type> Technical Report TM-10865, </type> <institution> Oak Ridge National Laboratory, </institution> <year> 1988. </year>
Reference-contexts: This step is typically the most time-consuming, and is parallelized in our program. The numerical factorization approach we use is very efficient, due to the use of supernodal elimination techniques. The approach is a dynamic version of the supernodal fan-out method [34], an enhancement of the fan-out method of <ref> [32] </ref>. Supernodes are sets of columns with nearly identical non-zero structures, and a factor matrix will typically contain a number of often very large supernodes. 13.1 Principal Data Structures The primary data structure in this program is the representation of the sparse matrix itself.
Reference: [33] <author> A. George and J. Liu, </author> <title> Computer Solution of Large Sparse Positive Definite Systems, </title> <publisher> Prentice-Hall Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: The row number of a particular non-zero is available through the row [] and startrow [] fields. Row numbers are stored in a compressed manner in order to conserve space. Details of the compression and other issues relating to the data structure can be found in <ref> [33] </ref>. <p> The corresponding numbers for the larger matrix bcsttk15 are 3948-by-3948, 56934, 647274, 1295 and 211, respectively. Both matrices have been reordered using the minimum degree heuristic <ref> [33] </ref>. 38 The program prints some numbers describing the matrix and the execution. It also outputs the execution time and MFLOPS rate of the factorization. 13.5 Results The results we present are for the factorization of the two Boeing/Harwell matrices included with the program.
Reference: [34] <author> E. Rothberg and A. Gupta, </author> <title> "Techniques for improving the performance of sparse factorization on multiprocessor workstations," </title> <booktitle> Proceedings of Supercomputing '90, </booktitle> <month> November, </month> <year> 1990. </year> <month> 42 </month>
Reference-contexts: This step is typically the most time-consuming, and is parallelized in our program. The numerical factorization approach we use is very efficient, due to the use of supernodal elimination techniques. The approach is a dynamic version of the supernodal fan-out method <ref> [34] </ref>, an enhancement of the fan-out method of [32].
References-found: 32

