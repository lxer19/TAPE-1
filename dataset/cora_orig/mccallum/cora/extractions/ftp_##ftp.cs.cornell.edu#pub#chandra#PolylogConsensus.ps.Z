URL: ftp://ftp.cs.cornell.edu/pub/chandra/PolylogConsensus.ps.Z
Refering-URL: http://www.cs.cornell.edu/Info/People/chandra/PolylogConsensus.html
Root-URL: 
Title: Polylog Randomized Wait-Free Consensus  
Author: Tushar Deepak Chandra 
Abstract: I present the first randomized wait-free implementation of consensus from multiple writer multiple reader register in which each process takes poly-log (O(log 2 n)) expected steps. To achieve this result, I assume a non-standard type of adversary (from [Abr88]). I argue that this type of adversary (which is more powerful than the oblivious adversary, but weaker than the strong adversary) is powerful enough to model practical systems. 
Abstract-found: 1
Intro-found: 1
Reference: [AB96] <author> Yonatan Aumann and Micheal Bender. </author> <title> Efficient asynchronous consensus with a value-oblivious adversary scheduler. </title> <booktitle> In Proceedings of the 23rd International Conference on Automata, Languages, and Programming, </booktitle> <month> July </month> <year> 1996. </year>
Reference-contexts: The existence of a randomized implementation of consensus that runs in polylog time against a strong adversary remains an open problem. In an independent work Aumann et. al. make many of the key observations that are made in this paper <ref> [ABZ94, AB96] </ref>. They use multiple writer multiple reader register to improve the execution time of randomized implementations of consensus. Further they discovered a similar adversary that is powerful enough to model practical systems, yet weak enough to enable efficient randomized implementations of consensus [AB96]. <p> They use multiple writer multiple reader register to improve the execution time of randomized implementations of consensus. Further they discovered a similar adversary that is powerful enough to model practical systems, yet weak enough to enable efficient randomized implementations of consensus <ref> [AB96] </ref>. Unlike this paper, Aumann et. al. focus on minimizing the total running time of their consensus implementations. <p> They present two randomized implementations of consensus: * An implementation with O (n log n) total running time against an oblivious adversary [ABZ94]. * An implementation with O (n log 2 n) total running time against their intermediate adversary <ref> [AB96] </ref>. Since the per process expected execution time of my consensus implementation is O (log 2 n), the expected total running time of my implementation is O (n log 2 n). The results from Aumann et. al. are weaker than the result presented in this paper in the following sense. <p> In particular, if only one process out of n actually executes their implementation, this process will take O (n) steps. The result in this paper, along with those of Au-mann et. al. <ref> [ABZ94, AB96] </ref> indicate that multiple writer multiple reader register is an important primitive object type for fault-tolerant parallel computing; in light of this I believe that architectures that provide the shared memory abstraction should provide fast implementations of multiple writer multiple reader register. <p> this in mind, I consider several different types of adversaries, some of which are restricted from learning the outcome of the coin flip whilst building the adversarial schedule: Oblivious adversary: The oblivious adversary is the weakest type of adversary considered here. 3 Aumann and Bender independently make the same observation <ref> [AB96] </ref>. Informally, an oblivious adversary fixes the schedule before any coins are flipped. More precisely, the adversary selects an infinite list of processes before the implementation begins to execute. <p> Since the intermediate adversary model is "reasonable", my polylog randomized wait-free implementation of consensus from multiple writer multiple reader registers is "practical". Aumann and Bender independently considered a very similar type of adversary <ref> [AB96] </ref>. The result of a coin toss is revealed to their adversary only when a process makes a decision based on the coin toss. Though their intermediate adversary is slightly less powerful than the intermediate adversary considered in this paper, this difference appears to have no practical consequence.
Reference: [Abr88] <author> Karl Abrahamson. </author> <title> On achieving consensus using a shared memory. </title> <booktitle> In Proceedings of the 7th Annual Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 291-302, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Consensus is known to be a fundamental problem in wait-free shared memory systems. Since there can be no deterministic implementation of consensus from registers [LA87], there has been extensive work in the literature giving randomized implementations of consensus from register (for example, see <ref> [CIL87, Abr88, AH90, Asp93, AW92] </ref>). Unfortunately, all previously known randomized implementations of consensus (from register) have polynomial expected per process running times (for the rest of this paper, unless specified otherwise, when I refer to execution time, I mean per process execution time). <p> Another point of departure between the "usual" model and my model is the adversary. The literature primarily studies two types of adversaries: 1) the oblivious adversary, and 2) the strong adversary. In this paper I also look at a third type of adversary (that first appeared in <ref> [Abr88] </ref>) which I shall refer to as the intermediate adversary. I argue that the oblivious adversary is too weak to model practical 1 Like consensus, test&set does not have a deterministic implementation from register [LA87]. <p> Loui and Abu-Amara showed that there is no algorithm that satisfies these requirements and implements n-process binary consensus for n &gt; 1 [LA87]. In order to circumvent this impossibility result, several researchers developed randomized implementations of n-process binary consensus (for example, see <ref> [CIL87, Abr88, AH90, Asp93, AW92] </ref>). Such implementations use the outcome of random events (such as the outcome of a coin flip) to determine the direction to take. Clearly, such implementations must assume that the adversary is unable to control or predict the outcome of the random events. <p> Note that in the above example, the behavior of an adversary is affected by the outcome of a coin toss only after the coin toss affects the schedule. This observation motivates the next type of ad versary considered in this paper. Intermediate adversary <ref> [Abr88] </ref>: Informally, this adversary is not permitted to see the outcome of a coin flip until some process makes a choice based on that coin flip. In this paper I model the intermediate adversary as follows.
Reference: [ABZ94] <author> Yonatan Aumann, Micheal Bender, and L. H. Zhang. </author> <title> Asynchronous consensus with an oblivious adversary scheduler. </title> <type> Unpublished manuscript, </type> <month> November </month> <year> 1994. </year>
Reference-contexts: The existence of a randomized implementation of consensus that runs in polylog time against a strong adversary remains an open problem. In an independent work Aumann et. al. make many of the key observations that are made in this paper <ref> [ABZ94, AB96] </ref>. They use multiple writer multiple reader register to improve the execution time of randomized implementations of consensus. Further they discovered a similar adversary that is powerful enough to model practical systems, yet weak enough to enable efficient randomized implementations of consensus [AB96]. <p> Unlike this paper, Aumann et. al. focus on minimizing the total running time of their consensus implementations. They present two randomized implementations of consensus: * An implementation with O (n log n) total running time against an oblivious adversary <ref> [ABZ94] </ref>. * An implementation with O (n log 2 n) total running time against their intermediate adversary [AB96]. Since the per process expected execution time of my consensus implementation is O (log 2 n), the expected total running time of my implementation is O (n log 2 n). <p> In particular, if only one process out of n actually executes their implementation, this process will take O (n) steps. The result in this paper, along with those of Au-mann et. al. <ref> [ABZ94, AB96] </ref> indicate that multiple writer multiple reader register is an important primitive object type for fault-tolerant parallel computing; in light of this I believe that architectures that provide the shared memory abstraction should provide fast implementations of multiple writer multiple reader register.
Reference: [AGTV92] <author> Yehuda Afek, Eli Gafni, John Tromp, and Paul M.B. Vitanyi. </author> <title> Wait-free test-and-set. </title> <booktitle> In Proceedings of the Sixth International Workshop on Distributed Algorithms, </booktitle> <pages> pages 85-94, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: The fastest known randomized implementation of test&set from single writer multiple reader register has an expected running time that is linear in the number of participating processes <ref> [AGTV92] </ref>. 2 Note that multiple writer multiple reader register can be implemented from single writer multiple reader register.
Reference: [AH90] <author> James Aspnes and M.P. Herlihy. </author> <title> Fast randomized consensus using shared memory. </title> <journal> Journal of Algorithms, </journal> <volume> 11 </volume> <pages> 441-461, </pages> <year> 1990. </year>
Reference-contexts: Consensus is known to be a fundamental problem in wait-free shared memory systems. Since there can be no deterministic implementation of consensus from registers [LA87], there has been extensive work in the literature giving randomized implementations of consensus from register (for example, see <ref> [CIL87, Abr88, AH90, Asp93, AW92] </ref>). Unfortunately, all previously known randomized implementations of consensus (from register) have polynomial expected per process running times (for the rest of this paper, unless specified otherwise, when I refer to execution time, I mean per process execution time). <p> Loui and Abu-Amara showed that there is no algorithm that satisfies these requirements and implements n-process binary consensus for n &gt; 1 [LA87]. In order to circumvent this impossibility result, several researchers developed randomized implementations of n-process binary consensus (for example, see <ref> [CIL87, Abr88, AH90, Asp93, AW92] </ref>). Such implementations use the outcome of random events (such as the outcome of a coin flip) to determine the direction to take. Clearly, such implementations must assume that the adversary is unable to control or predict the outcome of the random events.
Reference: [Asp93] <author> James Aspnes. </author> <title> Time- and space-efficient randomized consensus. </title> <journal> Journal of Algorithms, </journal> <volume> 14(3) </volume> <pages> 414-431, </pages> <year> 1993. </year> <note> An earlier version appeared in 9th ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing, </note> <month> August </month> <year> 1990, </year> <pages> pp. 325-331. </pages>
Reference-contexts: Consensus is known to be a fundamental problem in wait-free shared memory systems. Since there can be no deterministic implementation of consensus from registers [LA87], there has been extensive work in the literature giving randomized implementations of consensus from register (for example, see <ref> [CIL87, Abr88, AH90, Asp93, AW92] </ref>). Unfortunately, all previously known randomized implementations of consensus (from register) have polynomial expected per process running times (for the rest of this paper, unless specified otherwise, when I refer to execution time, I mean per process execution time). <p> Loui and Abu-Amara showed that there is no algorithm that satisfies these requirements and implements n-process binary consensus for n &gt; 1 [LA87]. In order to circumvent this impossibility result, several researchers developed randomized implementations of n-process binary consensus (for example, see <ref> [CIL87, Abr88, AH90, Asp93, AW92] </ref>). Such implementations use the outcome of random events (such as the outcome of a coin flip) to determine the direction to take. Clearly, such implementations must assume that the adversary is unable to control or predict the outcome of the random events. <p> All such processes will revert to some well known polynomial time randomized implementation of n-process binary consensus (see Line 15, Figure 1); in this paper I use the randomized n-process binary consensus implementation of Aspnes <ref> [Asp93] </ref> with expected running time O (n 4 log n) per process. By ensuring that the probability that any process reverts to Aspnes' implementation is o ( 1 n 4 logn ), I ensure that the expected running time of the overall implementation is not affected.
Reference: [AW92] <author> James Aspnes and Orli Waarts. </author> <title> Randomized consensus in expected O(n log 2 n) operations per processor. </title> <booktitle> In Proceedings of the Thirty-Third Symposium on Foundations of Computer Science, </booktitle> <pages> pages 137-146. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> Oc-tober </month> <year> 1992. </year>
Reference-contexts: Consensus is known to be a fundamental problem in wait-free shared memory systems. Since there can be no deterministic implementation of consensus from registers [LA87], there has been extensive work in the literature giving randomized implementations of consensus from register (for example, see <ref> [CIL87, Abr88, AH90, Asp93, AW92] </ref>). Unfortunately, all previously known randomized implementations of consensus (from register) have polynomial expected per process running times (for the rest of this paper, unless specified otherwise, when I refer to execution time, I mean per process execution time). <p> Unfortunately, this results in implementations that are slow: i.e., their execution time is a polynomial in the number of participating processes. In particular, the fastest known randomized implementation of consensus from single writer multiple reader register has an expected running time of O (n log 2 per process <ref> [AW92] </ref>. Further, it is easy to show that the expected running time of any randomized implementation of consensus from single writer multiple reader register, is at least linear in the number of participating processes. <p> Loui and Abu-Amara showed that there is no algorithm that satisfies these requirements and implements n-process binary consensus for n &gt; 1 [LA87]. In order to circumvent this impossibility result, several researchers developed randomized implementations of n-process binary consensus (for example, see <ref> [CIL87, Abr88, AH90, Asp93, AW92] </ref>). Such implementations use the outcome of random events (such as the outcome of a coin flip) to determine the direction to take. Clearly, such implementations must assume that the adversary is unable to control or predict the outcome of the random events.
Reference: [CD92] <author> Tushar D. Chandra and Cynthia Dwork. </author> <title> Personal communication. In this, we give a randomized implementation of test&set from multi reader multi writer register for a strong adversary with O(log n) expected running time., </title> <month> Oc-tober </month> <year> 1992. </year>
Reference-contexts: Dwork and I showed that by using multiple writer multiple reader register as the primitive, we might be able to improve the running time of known implementations to polylog time <ref> [CD92] </ref>.
Reference: [CHP71] <author> P.J. Courtois, F. Heymans, and D.L. Par-nas. </author> <title> Concurrent control with readers and writers. </title> <journal> Communications of the ACM, </journal> <volume> 14(10) </volume> <pages> 667-668, </pages> <year> 1971. </year>
Reference-contexts: Request permissions from Publications Dept, ACM Inc., fax +1 (212) 869-0481, or permissions@acm.org. order. More precisely, the behavior of a shared object must be linearizable [HW90]. One way to ensure linearizability is to implement shared objects using critical sections <ref> [CHP71] </ref>. This approach, however, is not fault-tolerant: The crash of a process while in the critical section of a shared object can permanently prevent the rest of the processes from accessing that object. This lack of fault-tolerance led to the concept of wait-free implementations of shared objects.
Reference: [CIL87] <author> Benny Chor, Amos Israeli, and Ming Li. </author> <title> On processor coordination using asynchronous hardware. </title> <booktitle> In Proceedings of the Sixth ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 222-231, </pages> <month> August </month> <year> 1987. </year>
Reference-contexts: Consensus is known to be a fundamental problem in wait-free shared memory systems. Since there can be no deterministic implementation of consensus from registers [LA87], there has been extensive work in the literature giving randomized implementations of consensus from register (for example, see <ref> [CIL87, Abr88, AH90, Asp93, AW92] </ref>). Unfortunately, all previously known randomized implementations of consensus (from register) have polynomial expected per process running times (for the rest of this paper, unless specified otherwise, when I refer to execution time, I mean per process execution time). <p> Loui and Abu-Amara showed that there is no algorithm that satisfies these requirements and implements n-process binary consensus for n &gt; 1 [LA87]. In order to circumvent this impossibility result, several researchers developed randomized implementations of n-process binary consensus (for example, see <ref> [CIL87, Abr88, AH90, Asp93, AW92] </ref>). Such implementations use the outcome of random events (such as the outcome of a coin flip) to determine the direction to take. Clearly, such implementations must assume that the adversary is unable to control or predict the outcome of the random events.
Reference: [HW90] <author> M.P. Herlihy and J.M. Wing. Linearizabil-ity: </author> <title> A correctness condition for concurrent objects. </title> <journal> ACM TOPLAS, </journal> <volume> 12(3) </volume> <pages> 463-492, </pages> <year> 1990. </year>
Reference-contexts: To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Publications Dept, ACM Inc., fax +1 (212) 869-0481, or permissions@acm.org. order. More precisely, the behavior of a shared object must be linearizable <ref> [HW90] </ref>. One way to ensure linearizability is to implement shared objects using critical sections [CHP71]. This approach, however, is not fault-tolerant: The crash of a process while in the critical section of a shared object can permanently prevent the rest of the processes from accessing that object. <p> Some processes may crash by halting prematurely. Further, processes may experience arbitrary variations in speed. A process cannot determine whether another process is correct, nor can a process determine when another process will take its next step. The behavior of an object in a concurrent system must be linearizable <ref> [HW90] </ref>, i.e., each operation must appear to take place at some time instant during its execution. One object type considered in this paper is multiple writer multiple reader register which supports two primitive operations write v and read.
Reference: [LA87] <author> M.C. Loui and Abu-Amara. </author> <title> Memory requirements for agreement among unreliable asynchronous processes. </title> <booktitle> Advances in computing research, </booktitle> <volume> 4 </volume> <pages> 163-183, </pages> <year> 1987. </year>
Reference-contexts: In the rest of this paper, I refer to "wait-free implementations" as "implementations", i.e., I will omit mentioning wait-free. Consensus is known to be a fundamental problem in wait-free shared memory systems. Since there can be no deterministic implementation of consensus from registers <ref> [LA87] </ref>, there has been extensive work in the literature giving randomized implementations of consensus from register (for example, see [CIL87, Abr88, AH90, Asp93, AW92]). <p> I argue that the oblivious adversary is too weak to model practical 1 Like consensus, test&set does not have a deterministic implementation from register <ref> [LA87] </ref>. The fastest known randomized implementation of test&set from single writer multiple reader register has an expected running time that is linear in the number of participating processes [AGTV92]. 2 Note that multiple writer multiple reader register can be implemented from single writer multiple reader register. <p> These programs must run even against the most unfavorable schedule picked by an "adversary". Loui and Abu-Amara showed that there is no algorithm that satisfies these requirements and implements n-process binary consensus for n &gt; 1 <ref> [LA87] </ref>. In order to circumvent this impossibility result, several researchers developed randomized implementations of n-process binary consensus (for example, see [CIL87, Abr88, AH90, Asp93, AW92]). Such implementations use the outcome of random events (such as the outcome of a coin flip) to determine the direction to take.
Reference: [Lam86] <author> Leslie Lamport. </author> <title> On interprocess communication, parts i and ii. </title> <journal> Distributed Computing, </journal> <volume> 1 </volume> <pages> 77-101, </pages> <year> 1986. </year>
Reference-contexts: This is of concern since there has been extensive work on implementing one type of register from another following the seminal work of Lamport in <ref> [Lam86] </ref>. For example, the following mechanism which occurs repeatedly in single writer register implementations behaves differently when implementing write 0 and write 1: Step 1: If old value 6= value, then old value value, write value on register, etc. Step 1': Else do nothing.
References-found: 13

