URL: http://polaris.cs.uiuc.edu/reports/1189.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Phone: (217) 333-6233  
Title: Life Span Strategy A Compiler-Based Approach to Cache Coherence  
Author: by Hoichi Cheong 
Address: Street Urbana, Illinois 61801  
Affiliation: University of Illinois at Urbana-Champaign 104 S. Wright  University of Illinois  
Note: Center for Supercomputing Research and Development  Copyright 1992, Board of Trustees of the  
Abstract: CSRD Report No. 1189 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. J. Smith, </author> <title> "CPU cache consistency with software support and using `One Time Identifiers'," </title> <booktitle> Proceedings Pacific Computer Communications Symposium, </booktitle> <pages> pp. 153-161, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: Let processor P 1 be scheduled to tasks t N 1 , t 2 , t 2 , and 1 on task levels N, N+1, N+2, and N+3 respectively. Table ? shows the status bits of X <ref> [1] </ref> and X [2] in P 1 's cache and also the cache responses. For clarity of presentation, let us assume a cache coherence block of one word and a prefetched line of one word. <p> The status bits affected by the operations will be represented by two numbers separated by a slash (old value/new value), and the unaffected status bits are represented by a single number, such as the status bits for X [2] on rows 1, 2, and 11, and those for X <ref> [1] </ref> on rows 4, 5, 7, 8, and 9. The Invalidate operation affects the status bits of all cache copies, as shown on rows 3, 6, 10, and 12. On level N, P 1 executes task t N 1 and it misses on X [1]. <p> and 11, and those for X <ref> [1] </ref> on rows 4, 5, 7, 8, and 9. The Invalidate operation affects the status bits of all cache copies, as shown on rows 3, 6, 10, and 12. On level N, P 1 executes task t N 1 and it misses on X [1]. The change of the Stale bit on row 1 records the result of the MRRS. On the other hand, because P 1 does not access its cache copy of X [2], the Stale bit of X [2] does not change. <p> The Invalidate operation is depicted as a separate task that is executed once by each processor before going on to the next level. The resultant zero Change bit of X <ref> [1] </ref> indicates its up-to-date status, but the Change bit of X [2] (value 1) indicates a stale copy. In our Doall program model, the execution of task t N 2 on level N by another processor will very likely write the up-to-date copy of X [2] in a different cache. <p> In our Doall program model, the execution of task t N 2 on level N by another processor will very likely write the up-to-date copy of X [2] in a different cache. Therefore, the Change bits of X <ref> [1] </ref> and X [2] in P 1 's cache correctly signal the status of the cache copies. The resultant Change bits on row 10 can be explained similarly with the roles of task 1 (t N+2 1 ) and task 2 (t N+2 1 ) reversed. <p> On rows 5 and 9, the fetches result in a cache hit due to the zero Change bits previously reset in the current level. To see the advantage of the Life Span strategy, notice that the fetch from X <ref> [1] </ref> on row 7 is also a new fetch. The FSI strategy will generate a Memory Read for this access. Without the Stale Table 1. Example of Life Span strategy: case 1. X [1] cache status bits X [2] cache status bits Task no. <p> To see the advantage of the Life Span strategy, notice that the fetch from X <ref> [1] </ref> on row 7 is also a new fetch. The FSI strategy will generate a Memory Read for this access. Without the Stale Table 1. Example of Life Span strategy: case 1. X [1] cache status bits X [2] cache status bits Task no. Operations that affect status bits Cache response Row 1/0 1/0 1 1 MRRS (X [1]) miss 1 t 1 N INV Task 0/1 0/0 1/1 1/1 INV 3 1 0 1/0 1/0 MRRS (X [2]) miss 4 t 2 N+1 <p> The FSI strategy will generate a Memory Read for this access. Without the Stale Table 1. Example of Life Span strategy: case 1. X <ref> [1] </ref> cache status bits X [2] cache status bits Task no. Operations that affect status bits Cache response Row 1/0 1/0 1 1 MRRS (X [1]) miss 1 t 1 N INV Task 0/1 0/0 1/1 1/1 INV 3 1 0 1/0 1/0 MRRS (X [2]) miss 4 t 2 N+1 1 0 0/0 0/0 MRRS (X [2]) hit 5 INV Task 1/1 0/1 0/1 0/0 INV 6 1 1 1/0 0/0 MRRS (X [2]) hit <p> INV Task 1/1 0/1 0/1 0/0 INV 6 1 1 1/0 0/0 MRRS (X [2]) hit 7 1 1 0/0 0/0 W (X [2]) 8 N+2 1 1 0/0 0/0 MRRS (X [2]) hit 9 INV Task 1/1 1/1 0/1 0/0 INV 10 N+3 1/0 1/0 1 0 MRRS (X <ref> [1] </ref>) miss 11 INV Task 0/1 0/0 1/1 0/1 INV 12 bit, this fetch will find the Change bit set. (The Change bits are always set to 1 by INV in the FSI strategy.) Therefore, a cache miss will be issued. <p> For variable elements with full inter-task-level temporal locality, this is a remarkable cost-saving advantage over the fixed-length version numbers or time stamps. Table 2. Example of Life Span strategy: case 2. X <ref> [1] </ref> cache status bits Task no. S C Operations that affect status bits Cache response Row 1/0 1/0 MRRS (X [1]) miss 1 t 1 N INV Task 0/1 0/0 INV 3 1/0 0/0 MRRS (X [1]) hit 4 t 1 N+1 0/0 0/0 MRRS (X [1]) hit 5 INV Task <p> Table 2. Example of Life Span strategy: case 2. X <ref> [1] </ref> cache status bits Task no. S C Operations that affect status bits Cache response Row 1/0 1/0 MRRS (X [1]) miss 1 t 1 N INV Task 0/1 0/0 INV 3 1/0 0/0 MRRS (X [1]) hit 4 t 1 N+1 0/0 0/0 MRRS (X [1]) hit 5 INV Task 0/1 0/0 INV 6 1/0 0/0 MRRS (X [1]) hit 7 0/0 0/0 W (X [1]) 8 N+2 0/0 0/0 <p> Table 2. Example of Life Span strategy: case 2. X <ref> [1] </ref> cache status bits Task no. S C Operations that affect status bits Cache response Row 1/0 1/0 MRRS (X [1]) miss 1 t 1 N INV Task 0/1 0/0 INV 3 1/0 0/0 MRRS (X [1]) hit 4 t 1 N+1 0/0 0/0 MRRS (X [1]) hit 5 INV Task 0/1 0/0 INV 6 1/0 0/0 MRRS (X [1]) hit 7 0/0 0/0 W (X [1]) 8 N+2 0/0 0/0 MRRS (X [1]) hit 9 INV Task 0/1 0/0 INV 10 N+3 1/0 0/0 MRRS (X <p> X <ref> [1] </ref> cache status bits Task no. S C Operations that affect status bits Cache response Row 1/0 1/0 MRRS (X [1]) miss 1 t 1 N INV Task 0/1 0/0 INV 3 1/0 0/0 MRRS (X [1]) hit 4 t 1 N+1 0/0 0/0 MRRS (X [1]) hit 5 INV Task 0/1 0/0 INV 6 1/0 0/0 MRRS (X [1]) hit 7 0/0 0/0 W (X [1]) 8 N+2 0/0 0/0 MRRS (X [1]) hit 9 INV Task 0/1 0/0 INV 10 N+3 1/0 0/0 MRRS (X [1]) hit 11 INV Task 0/1 0/0 INV 12 5 <p> that affect status bits Cache response Row 1/0 1/0 MRRS (X <ref> [1] </ref>) miss 1 t 1 N INV Task 0/1 0/0 INV 3 1/0 0/0 MRRS (X [1]) hit 4 t 1 N+1 0/0 0/0 MRRS (X [1]) hit 5 INV Task 0/1 0/0 INV 6 1/0 0/0 MRRS (X [1]) hit 7 0/0 0/0 W (X [1]) 8 N+2 0/0 0/0 MRRS (X [1]) hit 9 INV Task 0/1 0/0 INV 10 N+3 1/0 0/0 MRRS (X [1]) hit 11 INV Task 0/1 0/0 INV 12 5 Compiler Support The compiler algorithm to identify and generate code for different kinds <p> 1/0 1/0 MRRS (X <ref> [1] </ref>) miss 1 t 1 N INV Task 0/1 0/0 INV 3 1/0 0/0 MRRS (X [1]) hit 4 t 1 N+1 0/0 0/0 MRRS (X [1]) hit 5 INV Task 0/1 0/0 INV 6 1/0 0/0 MRRS (X [1]) hit 7 0/0 0/0 W (X [1]) 8 N+2 0/0 0/0 MRRS (X [1]) hit 9 INV Task 0/1 0/0 INV 10 N+3 1/0 0/0 MRRS (X [1]) hit 11 INV Task 0/1 0/0 INV 12 5 Compiler Support The compiler algorithm to identify and generate code for different kinds of fetches is based on the one <p> t 1 N INV Task 0/1 0/0 INV 3 1/0 0/0 MRRS (X <ref> [1] </ref>) hit 4 t 1 N+1 0/0 0/0 MRRS (X [1]) hit 5 INV Task 0/1 0/0 INV 6 1/0 0/0 MRRS (X [1]) hit 7 0/0 0/0 W (X [1]) 8 N+2 0/0 0/0 MRRS (X [1]) hit 9 INV Task 0/1 0/0 INV 10 N+3 1/0 0/0 MRRS (X [1]) hit 11 INV Task 0/1 0/0 INV 12 5 Compiler Support The compiler algorithm to identify and generate code for different kinds of fetches is based on the one introduced in the FSI strategy. <p> hit 4 t 1 N+1 0/0 0/0 MRRS (X <ref> [1] </ref>) hit 5 INV Task 0/1 0/0 INV 6 1/0 0/0 MRRS (X [1]) hit 7 0/0 0/0 W (X [1]) 8 N+2 0/0 0/0 MRRS (X [1]) hit 9 INV Task 0/1 0/0 INV 10 N+3 1/0 0/0 MRRS (X [1]) hit 11 INV Task 0/1 0/0 INV 12 5 Compiler Support The compiler algorithm to identify and generate code for different kinds of fetches is based on the one introduced in the FSI strategy.
Reference: [2] <author> A. Veidenbaum, </author> <title> "A compiler-assisted cache coherence solution for multiprocessors," </title> <booktitle> Proceedings 1986 International Conference on Parallel Processing, </booktitle> <pages> pp. 1029-1036, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Let processor P 1 be scheduled to tasks t N 1 , t 2 , t 2 , and 1 on task levels N, N+1, N+2, and N+3 respectively. Table ? shows the status bits of X [1] and X <ref> [2] </ref> in P 1 's cache and also the cache responses. For clarity of presentation, let us assume a cache coherence block of one word and a prefetched line of one word. <p> The status bits affected by the operations will be represented by two numbers separated by a slash (old value/new value), and the unaffected status bits are represented by a single number, such as the status bits for X <ref> [2] </ref> on rows 1, 2, and 11, and those for X [1] on rows 4, 5, 7, 8, and 9. The Invalidate operation affects the status bits of all cache copies, as shown on rows 3, 6, 10, and 12. <p> On level N, P 1 executes task t N 1 and it misses on X [1]. The change of the Stale bit on row 1 records the result of the MRRS. On the other hand, because P 1 does not access its cache copy of X <ref> [2] </ref>, the Stale bit of X [2] does not change. After task t N 1 , the processor executes the Invalidate operation (row 3) on its entire cache the Change bit takes the value of the Stale bit, and the Stale bit becomes 1. <p> The change of the Stale bit on row 1 records the result of the MRRS. On the other hand, because P 1 does not access its cache copy of X <ref> [2] </ref>, the Stale bit of X [2] does not change. After task t N 1 , the processor executes the Invalidate operation (row 3) on its entire cache the Change bit takes the value of the Stale bit, and the Stale bit becomes 1. <p> The Invalidate operation is depicted as a separate task that is executed once by each processor before going on to the next level. The resultant zero Change bit of X [1] indicates its up-to-date status, but the Change bit of X <ref> [2] </ref> (value 1) indicates a stale copy. In our Doall program model, the execution of task t N 2 on level N by another processor will very likely write the up-to-date copy of X [2] in a different cache. Therefore, the Change bits of X [1] and X [2] in P <p> zero Change bit of X [1] indicates its up-to-date status, but the Change bit of X <ref> [2] </ref> (value 1) indicates a stale copy. In our Doall program model, the execution of task t N 2 on level N by another processor will very likely write the up-to-date copy of X [2] in a different cache. Therefore, the Change bits of X [1] and X [2] in P 1 's cache correctly signal the status of the cache copies. <p> of X <ref> [2] </ref> (value 1) indicates a stale copy. In our Doall program model, the execution of task t N 2 on level N by another processor will very likely write the up-to-date copy of X [2] in a different cache. Therefore, the Change bits of X [1] and X [2] in P 1 's cache correctly signal the status of the cache copies. The resultant Change bits on row 10 can be explained similarly with the roles of task 1 (t N+2 1 ) and task 2 (t N+2 1 ) reversed. <p> Also, since the fetch is an MRRS, the Stale bit is reset. The zero Stale bit indicates that the copy is an up-to-date one from the global memory. And the copy of X <ref> [2] </ref> will be known to be up-to-date on task level N+2. Therefore, the new fetch by P 1 on level N+2 is a hit (row 7). On level N+1 and N+2, intra-task-level temporal locality is also preserved by the zero Change bit. <p> The FSI strategy will generate a Memory Read for this access. Without the Stale Table 1. Example of Life Span strategy: case 1. X [1] cache status bits X <ref> [2] </ref> cache status bits Task no. Operations that affect status bits Cache response Row 1/0 1/0 1 1 MRRS (X [1]) miss 1 t 1 N INV Task 0/1 0/0 1/1 1/1 INV 3 1 0 1/0 1/0 MRRS (X [2]) miss 4 t 2 N+1 1 0 0/0 0/0 MRRS <p> X [1] cache status bits X <ref> [2] </ref> cache status bits Task no. Operations that affect status bits Cache response Row 1/0 1/0 1 1 MRRS (X [1]) miss 1 t 1 N INV Task 0/1 0/0 1/1 1/1 INV 3 1 0 1/0 1/0 MRRS (X [2]) miss 4 t 2 N+1 1 0 0/0 0/0 MRRS (X [2]) hit 5 INV Task 1/1 0/1 0/1 0/0 INV 6 1 1 1/0 0/0 MRRS (X [2]) hit 7 1 1 0/0 0/0 W (X [2]) 8 N+2 1 1 0/0 0/0 MRRS (X [2]) hit 9 INV <p> Operations that affect status bits Cache response Row 1/0 1/0 1 1 MRRS (X [1]) miss 1 t 1 N INV Task 0/1 0/0 1/1 1/1 INV 3 1 0 1/0 1/0 MRRS (X <ref> [2] </ref>) miss 4 t 2 N+1 1 0 0/0 0/0 MRRS (X [2]) hit 5 INV Task 1/1 0/1 0/1 0/0 INV 6 1 1 1/0 0/0 MRRS (X [2]) hit 7 1 1 0/0 0/0 W (X [2]) 8 N+2 1 1 0/0 0/0 MRRS (X [2]) hit 9 INV Task 1/1 1/1 0/1 0/0 INV 10 N+3 1/0 1/0 1 0 <p> (X [1]) miss 1 t 1 N INV Task 0/1 0/0 1/1 1/1 INV 3 1 0 1/0 1/0 MRRS (X <ref> [2] </ref>) miss 4 t 2 N+1 1 0 0/0 0/0 MRRS (X [2]) hit 5 INV Task 1/1 0/1 0/1 0/0 INV 6 1 1 1/0 0/0 MRRS (X [2]) hit 7 1 1 0/0 0/0 W (X [2]) 8 N+2 1 1 0/0 0/0 MRRS (X [2]) hit 9 INV Task 1/1 1/1 0/1 0/0 INV 10 N+3 1/0 1/0 1 0 MRRS (X [1]) miss 11 INV Task 0/1 0/0 1/1 0/1 INV 12 bit, this fetch will <p> 0/1 0/0 1/1 1/1 INV 3 1 0 1/0 1/0 MRRS (X <ref> [2] </ref>) miss 4 t 2 N+1 1 0 0/0 0/0 MRRS (X [2]) hit 5 INV Task 1/1 0/1 0/1 0/0 INV 6 1 1 1/0 0/0 MRRS (X [2]) hit 7 1 1 0/0 0/0 W (X [2]) 8 N+2 1 1 0/0 0/0 MRRS (X [2]) hit 9 INV Task 1/1 1/1 0/1 0/0 INV 10 N+3 1/0 1/0 1 0 MRRS (X [1]) miss 11 INV Task 0/1 0/0 1/1 0/1 INV 12 bit, this fetch will find the Change bit set. (The Change bits are <p> 1/0 MRRS (X <ref> [2] </ref>) miss 4 t 2 N+1 1 0 0/0 0/0 MRRS (X [2]) hit 5 INV Task 1/1 0/1 0/1 0/0 INV 6 1 1 1/0 0/0 MRRS (X [2]) hit 7 1 1 0/0 0/0 W (X [2]) 8 N+2 1 1 0/0 0/0 MRRS (X [2]) hit 9 INV Task 1/1 1/1 0/1 0/0 INV 10 N+3 1/0 1/0 1 0 MRRS (X [1]) miss 11 INV Task 0/1 0/0 1/1 0/1 INV 12 bit, this fetch will find the Change bit set. (The Change bits are always set to 1 by INV in the FSI <p> The simple index expressions in the example below are used for the purpose of clarity. The example in Figure ? shows a simplified view of such a Doacross loop. Notice, by the Change bits, that X <ref> [2] </ref> in P 1 's cache will be treated as stale after the task level whereas the up-to-date copy in P 2 's cache will be recognized as a fresh copy. 8.2 Sink-fetches For a sink-fetch, the dependence is a flow-dependence (read-after-write) with a source-write in a previous iteration in the <p> For a fetch that is both a sink and a source, it is marked Memoryread. fetches. The snapshot of cache status shows the Stale and the Change bit corresponding to cache copy X <ref> [2] </ref>. Due to the sink-fetch on X [i], the write and the fetch to X in the last task level (corresponding to the Doall loop) are marked MR and Writesetstale (WSS) which result in the set Change bit of X [2] in P 2 's cache after the middle level boundary. <p> Stale and the Change bit corresponding to cache copy X <ref> [2] </ref>. Due to the sink-fetch on X [i], the write and the fetch to X in the last task level (corresponding to the Doall loop) are marked MR and Writesetstale (WSS) which result in the set Change bit of X [2] in P 2 's cache after the middle level boundary. Therefore, the fetch by P 2 from X [2] in the Doacross loop will result in a miss. Our assumption of cross-iteration flow-dependence guarantees that P 1 has written the new value of X [2] into the global memory before <p> write and the fetch to X in the last task level (corresponding to the Doall loop) are marked MR and Writesetstale (WSS) which result in the set Change bit of X <ref> [2] </ref> in P 2 's cache after the middle level boundary. Therefore, the fetch by P 2 from X [2] in the Doacross loop will result in a miss. Our assumption of cross-iteration flow-dependence guarantees that P 1 has written the new value of X [2] into the global memory before the synchronization operation. The miss by P 2 will then be satisfied by the up-to-date copy of X [2] <p> set Change bit of X <ref> [2] </ref> in P 2 's cache after the middle level boundary. Therefore, the fetch by P 2 from X [2] in the Doacross loop will result in a miss. Our assumption of cross-iteration flow-dependence guarantees that P 1 has written the new value of X [2] into the global memory before the synchronization operation. The miss by P 2 will then be satisfied by the up-to-date copy of X [2] in the global memory. <p> <ref> [2] </ref> in the Doacross loop will result in a miss. Our assumption of cross-iteration flow-dependence guarantees that P 1 has written the new value of X [2] into the global memory before the synchronization operation. The miss by P 2 will then be satisfied by the up-to-date copy of X [2] in the global memory. In the illustration, we assume that the next task level after the Doacross loop contains no sink-fetch of X in a cross-iteration flow-dependence.
Reference: [3] <author> K. P. McAuliffe, </author> <title> "Analysis of cache memories in highly parallel systems," </title> <type> Tech. Rep. No. 269, Ph.D. dissertation, </type> <institution> Courant Institute of Mathematical Sciences, NYU, </institution> <year> 1986. </year>
Reference: [4] <author> R. L. Lee, </author> <title> "The effectiveness of caches and data prefetch buffers in large-scale shared memory multiprocessors," </title> <type> Tech. Rep. No. 670, Ph.D. dissertation, </type> <institution> University of Illinois, Urbana, IL, </institution> <month> August </month> <year> 1987. </year>
Reference: [5] <author> R. Lee, P. Yew, and D. Lawrie, </author> <title> "Multiprocessor cache design considerations," </title> <booktitle> Proceedings of the 14th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 253-262, </pages> <month> June </month> <year> 1987. </year>
Reference: [6] <author> H. Cheong and A. Veidenbaum, </author> <title> "A cache coherence scheme with fast selective invalidation," </title> <booktitle> Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <address> p. 299, </address> <month> June </month> <year> 1988. </year>
Reference: [7] <author> H. Cheong and A. V. Veidenbaum, </author> <title> "Stale data detection and coherence enforcement using flow analysis," </title> <booktitle> Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <volume> vol. I, </volume> <pages> pp. 138-145, </pages> <month> August </month> <year> 1988. </year>
Reference: [8] <author> R. Cytron, S. Karlovsky, and K. P. McAuliffe, </author> <title> "Automatic management of programmable caches," </title> <booktitle> Proceedings the 1988 International Conference on Parallel Processing, </booktitle> <volume> vol. II, </volume> <pages> pp. 229-238, </pages> <month> August </month> <year> 1988. </year>
Reference: [9] <author> C. K. Tang, </author> <title> "Cache system design in the tightly coupled multiprocessor system," </title> <booktitle> Proceedings NCC, </booktitle> <volume> vol. 45, </volume> <pages> pp. 749-753, </pages> <year> 1976. </year>
Reference: [10] <author> L. M. Censier and P. Feautrier, </author> <title> "A new solution to coherence problems in multicache systems," </title> <journal> IEEE Transations on Computers, </journal> <volume> vol. C-27, no. 12, </volume> <pages> pp. 1112-1118, </pages> <month> December, </month> <year> 1978. </year>
Reference: [11] <author> J. Archibald and J.-L. Baer, </author> <title> "An economical solution to the cache coherence problem," </title> <booktitle> Proceedings of the 11th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 355-362, </pages> <month> June 5-7, </month> <year> 1984. </year>
Reference: [12] <author> J. R. Goodman, </author> <title> "Using cache memory to reduce processor-memory traffic," </title> <booktitle> Proceedings of the 10th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 124-131, </pages> <year> 1983. </year>
Reference: [13] <author> E. McCreight, </author> <title> "The dragon computer system: An early overview," </title> <type> tech. rep., </type> <institution> Xerox Corp., </institution> <month> September </month> <year> 1984. </year>
Reference: [14] <author> M. S. Papamarcos and J. H. Patel, </author> <title> "A low-overhead coherence solution for multiprocessors with private cache memories," </title> <booktitle> Proceedings of the 11th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 348-354, </pages> <month> June </month> <year> 1984. </year>
Reference: [15] <author> L. Rudolph and Z. Segall, </author> <title> "Dynamic decentralized cache schemes for MIMD parallel processors," </title> <booktitle> Proceedings of the 11th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 340-347, </pages> <month> June </month> <year> 1984. </year>
Reference: [16] <author> R. H. Katz, S. J. Eggers, D. A. Wood, C. L. Perkins, and R. G. Sheldon, </author> <title> "Implementing a cache consistency protocol," </title> <booktitle> Proceedings of the 12th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 276-283, </pages> <month> June </month> <year> 1985. </year>
Reference: [17] <author> Y.-C. Chen and A. V. Veidenbaum, </author> <title> "A software coherence scheme with the assistance of directories," </title> <booktitle> Proceedings 1989 International Conference on Supercomputing, </booktitle> <month> June </month> <year> 1991. </year>
Reference: [18] <author> H. Cheong and A. Veidenbaum, </author> <title> "A version control approach to cache coherence," </title> <booktitle> Proceedings International Conference of Supercomputing 89, </booktitle> <pages> pp. 322-330, </pages> <month> June </month> <year> 1989. </year>
Reference: [19] <author> S. L. Min and J.-L. Baer, </author> <title> "A timestamp-based cache coherence scheme," </title> <booktitle> Proceedings 1989 International Conference on Parallel Processing, I, </booktitle> <address> Architecture:23-32, </address> <month> August </month> <year> 1989. </year>
Reference: [20] <author> J. Fang and M. Lu, </author> <title> "A solution of cache ping-pong problem in RISC based parallel processing systems," </title> <booktitle> Proceedings 1991 International Conference On Parallel Processing, I, </booktitle> <address> Architecture:238-245, </address> <month> August </month> <year> 1991. </year>
Reference: [21] <author> H. Cheong, </author> <title> "Compiler-directed cache coherence strategies for large-scale shared-memory multiprocessor systems," </title> <type> Tech. Rep. CSRD No. 953, Ph.D. dissertation, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> January </month> <year> 1990. </year>
Reference: [22] <author> J. R. Goodman, </author> <title> "Coherency for multiprocessor virtual address caches," </title> <booktitle> Proceedings of the Second International Conference on Architecture Support for Programming Languages and Operating Systems (ASPLOS II), </booktitle> <pages> pp. 72-81, </pages> <year> 1987. </year>
Reference: [23] <author> U. Banerjee, </author> <title> "Data dependence in ordinary programs," </title> <type> Tech. Rep. No. 76-837, M.S. thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> November </month> <year> 1976. </year>
Reference: [24] <author> S. L. Min and J.-L. Baer, </author> <title> "Design and analysis of a scalable cache coherence scheme basaed on clocks and timestamps," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 3, no. 1, </volume> <month> January </month> <year> 1992. </year>
Reference: [25] <author> M. Wolfe, </author> <title> "Optimizing compilers for supercomputers," </title> <type> Tech. Rep. No. UIUCDCS-R-82-1105, Ph.D. dissertation, </type> <institution> University of Illinois, Urbana, IL, </institution> <month> October </month> <year> 1982. </year>
Reference: [26] <author> J.-K. Peir, K. So, and J.-H. Tang, </author> <title> "Inter-section locality of shared data in parallel programs," </title> <booktitle> Proceedings 1991 International Conference On Parallel Processing, I, </booktitle> <address> Architecture:278-286, </address> <month> August </month> <year> 1991. </year> <month> 10 </month>
References-found: 26

