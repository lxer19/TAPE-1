URL: http://www-anw.cs.umass.edu/People/singh/Papers/singh-AAAI92.ps.Z
Refering-URL: http://www-anw.cs.umass.edu/People/singh/Papers/
Root-URL: 
Email: singh@cs.umass.edu  
Title: Reinforcement Learning with a Hierarchy of Abstract Models  
Author: Satinder P. Singh 
Address: Amherst, MA 01003  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: Reinforcement learning (RL) algorithms have traditionally been thought of as trial and error learning methods that use actual control experience to incrementally improve a control policy. Sutton's DYNA architecture demonstrated that RL algorithms can work as well using simulated experience from an environment model, and that the resulting computation was similar to doing one-step lookahead planning. Inspired by the literature on hierarchical planning, I propose learning a hierarchy of models of the environment that abstract temporal detail as a means of improving the scalability of RL algorithms. I present H-DYNA (Hierarchical DYNA), an extension to Sutton's DYNA architecture that is able to learn such a hierarchy of abstract models. H-DYNA differs from hierarchical planners in two ways: first, the abstract models are learned using experience gained while learning to solve other tasks in the same environment, and second, the abstract models can be used to solve stochastic control tasks. Simulations on a set of compositionally-structured navigation tasks show that H-DYNA can learn to solve them faster than conventional RL algorithms. The abstract models also serve as mechanisms for achieving transfer of learning across multiple tasks. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A.G. and Singh, S.P. </author> <year> 1990. </year> <title> On the computational economics of reinforcement learning. </title> <booktitle> In Proc. of the 1990 Connectionist Models Summer School, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: I present a RL-based control architecture that learns a hierarchy of abstract models that, like hierarchical planners, abstract temporal detail. Scaling Reinforcement Learning Algorithms Unlike planning-based controllers, RL-based controllers are embedded in an optimal control framework <ref> (Barto et al. 1990) </ref>. Thus, the RL agent has to learn a sequence of actions that not only transforms an external dynamic environment to a desired goal state 2 , but also improves performance with respect to an objective function. <p> Learning Abstract Models for Multiple Tasks If the agent is to simply learn to solve a single task, the computational cost of constructing a VTRM may not be worthwhile <ref> (see Barto and Singh 1990) </ref>. My approach to the scaling problem is to consider problem-solving agents that have to learn to solve multiple tasks and to use repeated experience at solving these tasks to construct a hierarchy of VTRMs that could then be used to accelerate learning of subsequent tasks.
Reference: <author> Barto, A. G.; Sutton, R. S.; and Watkins, C. </author> <year> 1990. </year> <title> Learning and sequential decision making. </title> <editor> In Gabriel, M. and Moore, J. W., editors 1990, </editor> <title> Learning and Computational Neuroscience. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: I present a RL-based control architecture that learns a hierarchy of abstract models that, like hierarchical planners, abstract temporal detail. Scaling Reinforcement Learning Algorithms Unlike planning-based controllers, RL-based controllers are embedded in an optimal control framework <ref> (Barto et al. 1990) </ref>. Thus, the RL agent has to learn a sequence of actions that not only transforms an external dynamic environment to a desired goal state 2 , but also improves performance with respect to an objective function. <p> Learning Abstract Models for Multiple Tasks If the agent is to simply learn to solve a single task, the computational cost of constructing a VTRM may not be worthwhile <ref> (see Barto and Singh 1990) </ref>. My approach to the scaling problem is to consider problem-solving agents that have to learn to solve multiple tasks and to use repeated experience at solving these tasks to construct a hierarchy of VTRMs that could then be used to accelerate learning of subsequent tasks.
Reference: <author> Barto, A.G.; Bradtke, S.J.; and Singh, S.P. </author> <year> 1991. </year> <title> Real-time learning and control using asynchronous dynamic programming. </title> <type> Technical Report 91-57, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA. </address> <note> Also submitted to AI Journal. </note>
Reference-contexts: Sut-ton's (1990) DYNA architecture is one such controller that learns a control policy as well as a model of the environment. Whenever time permits, simulated experience with the model is used to adapt the control policy <ref> (also see Barto et al. 1991) </ref>. As noted by Sutton (1991), the computation performed by the RL algorithm on simulated experience is similar to executing a one-step lookahead planning algorithm. <p> The extension to the case where different sets of actions are available in different states is straightforward. 4 Algorithms based on policy iteration are an exception. the convergence of some DP algorithms <ref> (also see Barto et al. 1991) </ref> to the optimal value function.
Reference: <author> Bertsekas, D. P. and Tsitsiklis, J. N. </author> <year> 1989. </year> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: <author> Chapman, D. and Kaelbling, L. P. </author> <year> 1991. </year> <title> Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. </title> <booktitle> In Proceedings of the 1991 International Joint Conference on Artificial Intelligence. </booktitle>
Reference-contexts: They first plan in the highest level, and then move down the hierarchy (and if necessary back up) to successively refine the abstract plan until it is expressed solely in terms of primitive actions. Although using abstract models is not new to RL <ref> (e.g., Chapman & Kaelbling 1991) </ref>, such research has focussed on abstracting structural detail. I present a RL-based control architecture that learns a hierarchy of abstract models that, like hierarchical planners, abstract temporal detail.
Reference: <author> Dean, T. and Boddy, M. </author> <year> 1988. </year> <title> An analysis of time dependent planning. </title> <booktitle> In Proceedings AAAI-88. </booktitle> <pages> 49-54. </pages>
Reference-contexts: Some researchers have proposed reactive controllers (e.g., Schoppers 1987) that dispense with planning altogether and determine actions directly as a function of the state or sensations. Others <ref> (e.g., Dean & Boddy 1988) </ref> have proposed control architectures that use anytime algorithms, i.e., use the results of partial planning to determine the action in a given state.
Reference: <author> Iba, G. A. </author> <year> 1989. </year> <title> A heuristic approach to the discovery of macro-operators. </title> <booktitle> Machine Learning 3 </booktitle> <pages> 285-317. </pages>
Reference: <author> Kaelbling, L. P. </author> <year> 1990. </year> <title> Learning in Embedded Systems. </title> <type> Ph.D. Dissertation, </type> <institution> Stanford University, Department of Computer Science, Stanford, CA. </institution> <note> Technical Report TR-90-04. </note>
Reference: <author> Korf, R.E. </author> <year> 1985. </year> <title> Learning to Solve Problems by Searching for Macro-Operators. </title> <publisher> Pitman Publishers, </publisher> <address> Massachusetts. </address>
Reference: <author> Ross, S. </author> <year> 1983. </year> <title> Introduction to Stochastic Dynamic Programming. </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference-contexts: A closed loop control policy, which is a function assigning actions to states, that maximizes the agents objective function is an optimal control policy. If a model of the environment is available, i.e., the transition probabilities and the payoff function are known, conventional dynamic programming (DP) algorithms <ref> (e.g., Ross 1983) </ref> can be used to find an optimal policy. If a model of the environment is not available to the agent, RL algorithms that approximate DP, such as Sutton's (1988) temporal differences (TD), can be used to approximate an optimal policy.
Reference: <author> Sacerdoti, E. D. </author> <year> 1973. </year> <title> Planning in a hierarchy of abstraction spaces. </title> <booktitle> In Advance Papers of the Third International Joint Conference on Artificial Intelligence. </booktitle>
Reference-contexts: The inability of RL-based controllers to scale well to control tasks with large state or action spaces has limited their application to simple tasks (see Tesauro 1992 for an exception). An approach to scaling RL algorithms can be derived from the research on hierarchical planning <ref> (e.g., Sacerdoti 1973) </ref>. Most hierarchical planners assume access to a hierarchy of abstract models of the problem state-space.
Reference: <author> Schoppers, M. J. </author> <year> 1987. </year> <title> Universal plans for reactive robots in unpredictable domains. </title> <booktitle> In Proceedings of the IJCAI-87. </booktitle>
Reference-contexts: Recent work on building real-time controllers has highlighted the shortcomings of 1 In this paper I will use the terms agent and controller interchangeably. planning algorithms: their inability to deal with uncer-tainity, stochasticity, and model imperfection without extensive recomputation. Some researchers have proposed reactive controllers <ref> (e.g., Schoppers 1987) </ref> that dispense with planning altogether and determine actions directly as a function of the state or sensations. Others (e.g., Dean & Boddy 1988) have proposed control architectures that use anytime algorithms, i.e., use the results of partial planning to determine the action in a given state.
Reference: <author> Singh, S.P. </author> <year> 1992a. </year> <title> Scaling reinforcement learning algorithms by learning variable temporal resolution models. </title> <booktitle> In Proceedings of the Ninth International Machine Learning conference. </booktitle> <publisher> Forthcoming. </publisher>
Reference: <author> Singh, S.P. </author> <year> 1992b. </year> <title> Transfer of learning by composing solutions for elemental sequential tasks. </title> <booktitle> Machine Learning. </booktitle>
Reference-contexts: Besides, building sophisticated autonomous agents will require the ability to handle multiple tasks/goals <ref> (Singh 1992b) </ref>. Determining the useful abstract actions for an arbitrary set of tasks is difficult, if not impossible.
Reference: <author> Sutton, R. S. </author> <year> 1988. </year> <title> Learning to predict by the methods of temporal differences. </title> <booktitle> Machine Learning 3 </booktitle> <pages> 9-44. </pages>
Reference: <author> Sutton, R. S. </author> <year> 1990. </year> <title> Integrating architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proc. of the Seventh International Conference on Machine Learning, </booktitle> <address> San Ma-teo, CA. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 216-224. </pages>
Reference: <author> Sutton, R. S. </author> <year> 1991. </year> <title> Planning by incremental dynamic programming. </title> <editor> In Birnbaum, L. and Collins, G., editors 1991, </editor> <booktitle> Machine Learning: Proceedings of the Eighth International Workshop, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 353-357. </pages>
Reference: <author> Tesauro, G. J. </author> <year> 1992. </year> <title> Practical issues in temporal difference learning. </title> <booktitle> Machine Learning. </booktitle>
Reference-contexts: The inability of RL-based controllers to scale well to control tasks with large state or action spaces has limited their application to simple tasks <ref> (see Tesauro 1992 for an exception) </ref>. An approach to scaling RL algorithms can be derived from the research on hierarchical planning (e.g., Sacerdoti 1973). Most hierarchical planners assume access to a hierarchy of abstract models of the problem state-space.
Reference: <author> Whitehead, S. D. </author> <year> 1991. </year> <title> Complexity and cooperation in Q-learning. </title> <editor> In Birnbaum, L. A. and Collins, G. C., editors 1991, </editor> <booktitle> Maching Learning: Proceedings of the Eighth International Workshop, </booktitle> <address> San Mateo, CA. </address> <publisher> Mor-gan Kaufmann. </publisher> <pages> 363-367. </pages>
References-found: 19

