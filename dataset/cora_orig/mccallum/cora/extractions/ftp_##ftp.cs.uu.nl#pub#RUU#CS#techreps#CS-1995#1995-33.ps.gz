URL: ftp://ftp.cs.uu.nl/pub/RUU/CS/techreps/CS-1995/1995-33.ps.gz
Refering-URL: http://www.cs.ruu.nl/docs/research/publication/TechList1.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Motion Planning in Environments with Low Obstacle Density  
Author: A. Frank van der Stappen Mark H. Overmars 
Address: P.O. Box 80.089, 3508 TB Utrecht, The Netherlands.  
Affiliation: Department of Computer Science, Utrecht University,  
Abstract: We present a simple and efficient paradigm for computing the exact solution of the motion planning problem in environments with a low obstacle density. Such environments frequently occur in practical instances of the motion planning problem. The complexity of the free space for such environments is known to be linear in the number of obstacles. Our paradigm is a new cell decomposition approach to motion planning and exploits properties that follow from the low density of the obstacles in the robot's workspace. These properties allow us to decompose the workspace, subject to some constraints, rather than to decompose the higher-dimensional free configuration space directly. A sequence of uniform steps transforms the workspace decomposition into a free space decomposition of asymptotically the same size. The approach applies to robots with any fixed number of degrees of freedom and turns out to be efficient in many cases: it leads to nearly optimal O(n log n) algorithms for motion planning in 2D, for motion planning in 3D amidst obstacles of comparable size, and for motion planning on a planar workfloor in 3D. In addition, we obtain algorithms for planning 3D motions among polyhedral obstacles, running in O(n 2 log n) time, and among arbitrary obstacles, running in time O(n 3 ). Several other interesting instances of the motion planning seem adequately solvable by the paradigm as well.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P.K. Agarwal, M.J. Katz, and M. Sharir, </author> <title> Computing depth orders and related problems, </title> <booktitle> Proc. 4th Scandinavian Workshop on Algorithm Theory (SWAT'94) (E.M. </booktitle> <editor> Schmidt and S. Skyum Eds.) </editor> <booktitle> Lecture Notes in Computer Science 824 (1994), </booktitle> <pages> pp. 1-12. </pages>
Reference-contexts: A collision-free path or motion for a robot B from an initial placement Z 0 to a final placement Z 1 is a continuous map: t : <ref> [0; 1] </ref> ! FP, with t (0) = Z 0 and t (1) = Z 1 . Hence, solving the motion planning problem boils down to finding a continuous curve in FP connecting Z 0 and Z 1 . <p> Several papers present surprising combinatorial complexity reductions [2, 10, 16, 19, 33] and efficiency gains for algorithms <ref> [1, 6, 14, 22, 23] </ref> if the objects under consideration have a certain fatness. Fat objects are compact to some extent, rather than long and thin. Fatness is a realistic assumption, since in many practical instances of geometric problems the considered objects are fat. <p> bounds on the complexity of the union of certain fat figures (e.g. triangles, wedges) in the plane [2, 10, 16, 19], a linear bound on the complexity 7 of the free space for motion planning amidst fat obstacles [33], and efficient algorithms for computing depth orders on certain fat objects <ref> [1] </ref>, binary space partitions for scenes of nonintersecting fat objects in the plane [6], hidden surface removal for fat horizontal triangles [14], and range searching and point location among fat objects [22, 23]. <p> Contrary to many other definitions of fatness in literature <ref> [1, 2, 10, 14, 16, 19] </ref>, the notion introduced in [33], and recaptured below, applies to general objects in arbitrary dimension d. The definition involves a parameter k, supplying a qualitative measure of the fatness of an object: the smaller the value of k, the fatter the object must be.
Reference: [2] <author> H. Alt, R. Fleischer, M. Kaufmann, K. Mehlhorn, S. N aher, S. Schirra, and C. Uhrig, </author> <title> Approximate motion planning and the complexity of the boundary of the union of simple geometric figures, </title> <booktitle> Algorithmica 8 (1992), </booktitle> <pages> pp. 391-406. 32 </pages>
Reference-contexts: Simple queries allow to replace the robot by an outer approximation with fewer degrees of freedom. Non-simple and non-impossible queries require the application of an exact method to the original problem. Alt et al. <ref> [2] </ref> introduce the tightness of a motion planning problem for a rectangle among polygonal obstacles as a measure for its complexity. The tightness of a problem is closely related to the scaling factor for the rectangular robot to make a solvable problem unsolvable, or an unsolvable problem solvable. <p> An immediate consequence of the preceding results will be the linear free space complexity for problems in this class. 2.2 Fatness Fatness has turned out to be an interesting phenomenon in computational geometry. Several papers present surprising combinatorial complexity reductions <ref> [2, 10, 16, 19, 33] </ref> and efficiency gains for algorithms [1, 6, 14, 22, 23] if the objects under consideration have a certain fatness. Fat objects are compact to some extent, rather than long and thin. <p> The achievements of the study of fatness so far include near-linear bounds on the complexity of the union of certain fat figures (e.g. triangles, wedges) in the plane <ref> [2, 10, 16, 19] </ref>, a linear bound on the complexity 7 of the free space for motion planning amidst fat obstacles [33], and efficient algorithms for computing depth orders on certain fat objects [1], binary space partitions for scenes of nonintersecting fat objects in the plane [6], hidden surface removal for <p> Contrary to many other definitions of fatness in literature <ref> [1, 2, 10, 14, 16, 19] </ref>, the notion introduced in [33], and recaptured below, applies to general objects in arbitrary dimension d. The definition involves a parameter k, supplying a qualitative measure of the fatness of an object: the smaller the value of k, the fatter the object must be.
Reference: [3] <author> B. Aronov and M. Sharir, </author> <title> Triangles in space or building (and analyzing) castles in the air, </title> <booktitle> Combinatorica 10 (1990), </booktitle> <pages> pp. 137-173. </pages>
Reference-contexts: Moreover, the existing results (see, for example, papers by Aronov and Sharir <ref> [3] </ref>, Chazelle [9], and De Berg, Guibas, and Halperin [7]) do not apply to arbitrary arrangements but instead only hold for arrangements of planar faces, which makes their application to the arrangement of (arbitrary) grown obstacle boundaries impossible.
Reference: [4] <author> F. Avnaim, J.-D. Boissonnat, and B. Faverjon, </author> <title> A practical exact motion planning algorithm for polygonal objects amidst polygonal obstacles, </title> <booktitle> Proc. Geometry and Robotics Workshop (J.-D. </booktitle> <editor> Boissonnat and J.-P. Laumond Eds.), </editor> <booktitle> Lecture Notes in Computer Science 391 (1988), </booktitle> <pages> pp. 67-86. </pages>
Reference-contexts: Some algorithms have a hidden sensitivity to the complexity of the free space [36]. For example, the boundary cell decomposition algorithm by Avnaim, Boissonnat, and Faverjon <ref> [4] </ref>, running in time O (n 3 log n) for a constant complexity polygonal robot amidst arbitrary polygonal obstacles, can be shown to run in O (n log n) time in the low obstacle density setting. <p> A solution to this type of path-finding for a polyhedral robot amidst polyhedral obstacles is given by Wentink and Schwarzkopf in [37]. Their algorithm, which is a generalization of the boundary cell decomposition algorithm by Avnaim, Boissonnat, and Faverjon <ref> [4] </ref>, runs in time O (n 3 log n). Under our assumptions, the free space can be decomposed into O (n) simple subcells in time O (n log n), using the general ideas of Section 3.1, yielding an O (n log n) motion planning algorithm.
Reference: [5] <author> J.L. Bentley and T.A. Ottman, </author> <title> Algorithms for reporting and counting geometric intersections, </title> <journal> IEEE Transactions on Computers 28 (1979), </journal> <pages> pp. 643-647. </pages>
Reference-contexts: In a second step we compute the vertical decomposition of the arrangement A (G) of grown obstacle boundaries, by sweeping the plane <ref> [5] </ref> with the arcs with a vertical line, meanwhile extending walls in upward and downward vertical direction from all fi (n) arc endpoints (known in advance) and all O (n) arc intersections (to be determined during the sweep).
Reference: [6] <author> M. de Berg, M. de Groot, and M. Overmars, </author> <title> New results on binary space partitions in the plane, </title> <booktitle> Proc. 4th Scandinavian Workshop on Algorithm Theory (SWAT'94) (E.M. </booktitle> <editor> Schmidt and S. Skyum Eds.) </editor> <booktitle> Lecture Notes in Computer Science 824 (1994), </booktitle> <pages> pp. 61-72. </pages>
Reference-contexts: Several papers present surprising combinatorial complexity reductions [2, 10, 16, 19, 33] and efficiency gains for algorithms <ref> [1, 6, 14, 22, 23] </ref> if the objects under consideration have a certain fatness. Fat objects are compact to some extent, rather than long and thin. Fatness is a realistic assumption, since in many practical instances of geometric problems the considered objects are fat. <p> wedges) in the plane [2, 10, 16, 19], a linear bound on the complexity 7 of the free space for motion planning amidst fat obstacles [33], and efficient algorithms for computing depth orders on certain fat objects [1], binary space partitions for scenes of nonintersecting fat objects in the plane <ref> [6] </ref>, hidden surface removal for fat horizontal triangles [14], and range searching and point location among fat objects [22, 23].
Reference: [7] <author> M. de Berg, L.J. Guibas, and D. Halperin, </author> <title> Vertical decompositions for triangles in 3-space, </title> <booktitle> Proc. 10th Ann. ACM Symp. on Computational Geometry (1994), </booktitle> <pages> pp. 1-10. </pages>
Reference-contexts: Moreover, the existing results (see, for example, papers by Aronov and Sharir [3], Chazelle [9], and De Berg, Guibas, and Halperin <ref> [7] </ref>) do not apply to arbitrary arrangements but instead only hold for arrangements of planar faces, which makes their application to the arrangement of (arbitrary) grown obstacle boundaries impossible. <p> The partition is obtained by computing a constrained triangulation of some polyhedral (outer) approximation of the grown obstacle boundaries, then replacing the triangles by flat non-intersecting tetrahedra, and subsequently applying a full vertical decomposition <ref> [7] </ref> to the triangular faces of the tetrahedra. The entire computation takes T (n) = O (n 2 log n) time. The substitution of the flat tetrahedra for the triangles serves as a means of obtaining a provable quadratic number of adjacencies. <p> The addition of 22 disjoint triangles to the partition can only lead to a refinement of the regions into smaller regions, with smaller or equally-sized coverages. De Berg, Guibas, and Halperin <ref> [7] </ref> present an algorithm for computing a full vertical decomposition of an arrangement of triangles in general position in 3-space.
Reference: [8] <author> J.F. Canny, </author> <title> The complexity of robot motion planning, </title> <publisher> MIT Press, </publisher> <address> Cambridge MA (1988). </address>
Reference-contexts: As a result, the size of the query structure the connectivity graph CG and the time to compute it depend on the complexity of FP. Retraction methods (see e.g. <ref> [8, 17, 20, 21, 32] </ref>) aim at capturing the structure and connectivity of the free space in some one-dimensional network of curves in the free space, the roadmap. <p> General approaches to motion planning (e.g. by Schwartz and Sharir [26] with running time O (n 2 f+6 ) and Canny <ref> [8] </ref> with running time O (n f log n)) are computationally expensive, even under our beneficial circumstances. These rigorous methods do not take advantage of any accidental structure of the free space (as is present in our case). <p> It has been noted that the existing planar motion planning algorithms are not easily extendible towards other in particular spatial problems. Moreover, the existing general approaches to motion planning (like those by Schwartz and Sharir [26] and Canny <ref> [8] </ref>) are computationally expensive, even for problems from the special class that we consider here. Motion planning problems in Euclidean workspaces of dimension three normally imply at least three-dimensional configuration spaces. <p> The two methods that apply to robots with an arbitrary number f 3 of degrees of freedom are the general O (n 2 f+6 ) cell decomposition algorithm by Schwartz and Sharir [26] and the O (n f log n) roadmap method by Canny <ref> [8] </ref>. More specific results include O (n 11 ) [28] and O (n 6 log n) [15] algorithms for a (5-DOF) ladder among polyhedral obstacles, and an O (n 15 ) algorithm [28] for a polyhedral robot in the same environment. <p> The only general methods that could solve such a problem are those by Schwartz and Sharir [25] and Canny <ref> [8] </ref>. Here, it is shown that an instance of the algorithm LDMot for a bounded-size robot in a low density workspace with arbitrary obstacles exists with running time fi (n 3 ), independent of the actual number f of degrees of freedom of the robot.
Reference: [9] <author> B. Chazelle, </author> <title> Convex partitions of polyhedra: a lower bound and worst-case optimal algorithm, </title> <journal> SIAM Journal on Computing 13 (1984), </journal> <pages> pp. 488-507. </pages>
Reference-contexts: Moreover, the existing results (see, for example, papers by Aronov and Sharir [3], Chazelle <ref> [9] </ref>, and De Berg, Guibas, and Halperin [7]) do not apply to arbitrary arrangements but instead only hold for arrangements of planar faces, which makes their application to the arrangement of (arbitrary) grown obstacle boundaries impossible.
Reference: [10] <author> A. Efrat, G. Rote, and M. Sharir, </author> <title> On the union of fat wedges and separating a collection of segments by a line, Computational Geometry: </title> <booktitle> Theory and Applications 3 (1993), </booktitle> <pages> pp. 277-288. </pages>
Reference-contexts: An immediate consequence of the preceding results will be the linear free space complexity for problems in this class. 2.2 Fatness Fatness has turned out to be an interesting phenomenon in computational geometry. Several papers present surprising combinatorial complexity reductions <ref> [2, 10, 16, 19, 33] </ref> and efficiency gains for algorithms [1, 6, 14, 22, 23] if the objects under consideration have a certain fatness. Fat objects are compact to some extent, rather than long and thin. <p> The achievements of the study of fatness so far include near-linear bounds on the complexity of the union of certain fat figures (e.g. triangles, wedges) in the plane <ref> [2, 10, 16, 19] </ref>, a linear bound on the complexity 7 of the free space for motion planning amidst fat obstacles [33], and efficient algorithms for computing depth orders on certain fat objects [1], binary space partitions for scenes of nonintersecting fat objects in the plane [6], hidden surface removal for <p> Contrary to many other definitions of fatness in literature <ref> [1, 2, 10, 14, 16, 19] </ref>, the notion introduced in [33], and recaptured below, applies to general objects in arbitrary dimension d. The definition involves a parameter k, supplying a qualitative measure of the fatness of an object: the smaller the value of k, the fatter the object must be.
Reference: [11] <author> L. Guibas and J. Stolfi, </author> <title> Primitives for the manipulation of general subdivisions and the computation of Voronoi diagrams, </title> <journal> ACM Transactions on Graphics 4 (1985), </journal> <pages> pp. 74-123. </pages>
Reference-contexts: The algorithm stores the walls and the triangle arrangements in a quad-edge structure <ref> [11] </ref> to facilitate future navigating through the decomposition. Let us now bound the size of the set E W = f (R; R 0 ) 2 V W fi V W j@R " @R 0 6= ;g.
Reference: [12] <author> D. Halperin and M.H. Overmars, </author> <title> Spheres, molecules, and hidden surface removal, </title> <booktitle> Proc. 10th Ann. ACM Symp. on Computational Geometry (1994), </booktitle> <pages> pp. 113-122. </pages>
Reference-contexts: So, under the circumstances sketched in Theorem 2.7, the boundary of the union of all wrappings has complexity O (n). 9 The result of the theorem is, for example, applicable to the molecule model in the paper by Halperin and Overmars <ref> [12] </ref>. The atoms that constitute a molecule are assumed to satisfy the hard sphere model. The hard sphere model describes atoms by spheres and forbids any sphere center to penetrate another sphere too far.
Reference: [13] <author> J. Hershberger, </author> <title> Finding the upper envelope of n line segments in O(n log n) time, </title> <booktitle> Information Processing Letters 33 (1989), </booktitle> <pages> pp. 169-174. </pages>
Reference: [14] <author> M.J. Katz, M.H. Overmars, and M. Sharir, </author> <title> Efficient hidden surface removal for objects with small union size, Computational Geometry: </title> <booktitle> Theory and Applications 2 (1992), </booktitle> <pages> pp. 223-234. </pages>
Reference-contexts: Several papers present surprising combinatorial complexity reductions [2, 10, 16, 19, 33] and efficiency gains for algorithms <ref> [1, 6, 14, 22, 23] </ref> if the objects under consideration have a certain fatness. Fat objects are compact to some extent, rather than long and thin. Fatness is a realistic assumption, since in many practical instances of geometric problems the considered objects are fat. <p> a linear bound on the complexity 7 of the free space for motion planning amidst fat obstacles [33], and efficient algorithms for computing depth orders on certain fat objects [1], binary space partitions for scenes of nonintersecting fat objects in the plane [6], hidden surface removal for fat horizontal triangles <ref> [14] </ref>, and range searching and point location among fat objects [22, 23]. Contrary to many other definitions of fatness in literature [1, 2, 10, 14, 16, 19], the notion introduced in [33], and recaptured below, applies to general objects in arbitrary dimension d. <p> Contrary to many other definitions of fatness in literature <ref> [1, 2, 10, 14, 16, 19] </ref>, the notion introduced in [33], and recaptured below, applies to general objects in arbitrary dimension d. The definition involves a parameter k, supplying a qualitative measure of the fatness of an object: the smaller the value of k, the fatter the object must be.
Reference: [15] <author> Y. Ke and J. O'Rourke, </author> <title> Moving a ladder in three dimensions: upper and lower bounds, </title> <booktitle> Proc. 3rd Ann. ACM Symp. on Computational Geometry (1987), </booktitle> <pages> pp. 136-145. </pages>
Reference-contexts: Although there essentially exist two different approaches to exact motion planning (cell decomposition and retraction), the time spent in processing the free space and the size of the resulting query structure clearly depend on the complexity of the free space. Cell decomposition algorithms (see e.g. <ref> [15, 18, 25, 26, 27, 28, 31] </ref>) partition the free space into a finite number of simple connected subcells, such that planning a motion between two placements in a single subcell is straightforward and such that uniform crossing rules can be defined for B crossing from one cell into another. <p> More specific results include O (n 11 ) [28] and O (n 6 log n) <ref> [15] </ref> algorithms for a (5-DOF) ladder among polyhedral obstacles, and an O (n 15 ) algorithm [28] for a polyhedral robot in the same environment. This section presents an instance of the algorithm LDMot with running time O (n 2 log n) for the following class of problems.
Reference: [16] <author> M. van Kreveld, </author> <title> On fat partitioning, fat covering and the union size of polygons, </title> <type> Technical Report RUU-CS-93-36, </type> <institution> Dept. of Computer Science, Utrecht University (1993). </institution>
Reference-contexts: An immediate consequence of the preceding results will be the linear free space complexity for problems in this class. 2.2 Fatness Fatness has turned out to be an interesting phenomenon in computational geometry. Several papers present surprising combinatorial complexity reductions <ref> [2, 10, 16, 19, 33] </ref> and efficiency gains for algorithms [1, 6, 14, 22, 23] if the objects under consideration have a certain fatness. Fat objects are compact to some extent, rather than long and thin. <p> The achievements of the study of fatness so far include near-linear bounds on the complexity of the union of certain fat figures (e.g. triangles, wedges) in the plane <ref> [2, 10, 16, 19] </ref>, a linear bound on the complexity 7 of the free space for motion planning amidst fat obstacles [33], and efficient algorithms for computing depth orders on certain fat objects [1], binary space partitions for scenes of nonintersecting fat objects in the plane [6], hidden surface removal for <p> Contrary to many other definitions of fatness in literature <ref> [1, 2, 10, 14, 16, 19] </ref>, the notion introduced in [33], and recaptured below, applies to general objects in arbitrary dimension d. The definition involves a parameter k, supplying a qualitative measure of the fatness of an object: the smaller the value of k, the fatter the object must be.
Reference: [17] <author> D. Leven and M. Sharir, </author> <title> Planning a purely translational motion for a convex object in two-dimensional space using generalized Voronoi diagrams, </title> <booktitle> Discrete & Computational Geometry 2 (1987), </booktitle> <pages> pp. 9-31. 33 </pages>
Reference-contexts: As a result, the size of the query structure the connectivity graph CG and the time to compute it depend on the complexity of FP. Retraction methods (see e.g. <ref> [8, 17, 20, 21, 32] </ref>) aim at capturing the structure and connectivity of the free space in some one-dimensional network of curves in the free space, the roadmap.
Reference: [18] <author> D. Leven and M. Sharir, </author> <title> An efficient and simple motion planning algorithm for a ladder amidst polygonal barriers, </title> <booktitle> Journal of Algorithms 8 (1987), </booktitle> <pages> pp. 192-215. </pages>
Reference-contexts: Although there essentially exist two different approaches to exact motion planning (cell decomposition and retraction), the time spent in processing the free space and the size of the resulting query structure clearly depend on the complexity of the free space. Cell decomposition algorithms (see e.g. <ref> [15, 18, 25, 26, 27, 28, 31] </ref>) partition the free space into a finite number of simple connected subcells, such that planning a motion between two placements in a single subcell is straightforward and such that uniform crossing rules can be defined for B crossing from one cell into another.
Reference: [19] <author> J. Matou sek, J. Pach, M. Sharir, S. Sifrony, and E. Welzl, </author> <title> Fat triangles determine linearly many holes, </title> <journal> SIAM Journal on Computing 23 (1994), </journal> <pages> pp. 154-169. </pages>
Reference-contexts: An immediate consequence of the preceding results will be the linear free space complexity for problems in this class. 2.2 Fatness Fatness has turned out to be an interesting phenomenon in computational geometry. Several papers present surprising combinatorial complexity reductions <ref> [2, 10, 16, 19, 33] </ref> and efficiency gains for algorithms [1, 6, 14, 22, 23] if the objects under consideration have a certain fatness. Fat objects are compact to some extent, rather than long and thin. <p> The achievements of the study of fatness so far include near-linear bounds on the complexity of the union of certain fat figures (e.g. triangles, wedges) in the plane <ref> [2, 10, 16, 19] </ref>, a linear bound on the complexity 7 of the free space for motion planning amidst fat obstacles [33], and efficient algorithms for computing depth orders on certain fat objects [1], binary space partitions for scenes of nonintersecting fat objects in the plane [6], hidden surface removal for <p> Contrary to many other definitions of fatness in literature <ref> [1, 2, 10, 14, 16, 19] </ref>, the notion introduced in [33], and recaptured below, applies to general objects in arbitrary dimension d. The definition involves a parameter k, supplying a qualitative measure of the fatness of an object: the smaller the value of k, the fatter the object must be.
Reference: [20] <author> C. O'D unlaing, M. Sharir, and C.-K. Yap, Retraction: </author> <title> A new approach to motion planning, </title> <booktitle> Proc. 15th Ann. ACM Symp. on the Theory of Computing (1983), </booktitle> <pages> pp. 207-220. </pages>
Reference-contexts: As a result, the size of the query structure the connectivity graph CG and the time to compute it depend on the complexity of FP. Retraction methods (see e.g. <ref> [8, 17, 20, 21, 32] </ref>) aim at capturing the structure and connectivity of the free space in some one-dimensional network of curves in the free space, the roadmap.
Reference: [21] <author> C. O'D unlaing and C.-K. Yap, </author> <title> A retraction method for planning the motion of a disc, </title> <booktitle> Journal of Algorithms 6 (1985), </booktitle> <pages> pp. 104-111. </pages>
Reference-contexts: As a result, the size of the query structure the connectivity graph CG and the time to compute it depend on the complexity of FP. Retraction methods (see e.g. <ref> [8, 17, 20, 21, 32] </ref>) aim at capturing the structure and connectivity of the free space in some one-dimensional network of curves in the free space, the roadmap.
Reference: [22] <author> M.H. Overmars, </author> <title> Point location in fat subdivisions, </title> <booktitle> Information Processing Letters 44 (1992), </booktitle> <pages> pp. 261-265. </pages>
Reference-contexts: Several papers present surprising combinatorial complexity reductions [2, 10, 16, 19, 33] and efficiency gains for algorithms <ref> [1, 6, 14, 22, 23] </ref> if the objects under consideration have a certain fatness. Fat objects are compact to some extent, rather than long and thin. Fatness is a realistic assumption, since in many practical instances of geometric problems the considered objects are fat. <p> space for motion planning amidst fat obstacles [33], and efficient algorithms for computing depth orders on certain fat objects [1], binary space partitions for scenes of nonintersecting fat objects in the plane [6], hidden surface removal for fat horizontal triangles [14], and range searching and point location among fat objects <ref> [22, 23] </ref>. Contrary to many other definitions of fatness in literature [1, 2, 10, 14, 16, 19], the notion introduced in [33], and recaptured below, applies to general objects in arbitrary dimension d.
Reference: [23] <author> M.H. Overmars and A.F. van der Stappen, </author> <title> Range searching and point location among fat objects, </title> <journal> Journal of Algorithms, </journal> <note> to appear. (Also available as: </note> <institution> Tech. </institution> <type> Rep. </type> <institution> UU-CS-1994-30, Dept. of Computer Science, Utrecht University (1994).) </institution>
Reference-contexts: Several papers present surprising combinatorial complexity reductions [2, 10, 16, 19, 33] and efficiency gains for algorithms <ref> [1, 6, 14, 22, 23] </ref> if the objects under consideration have a certain fatness. Fat objects are compact to some extent, rather than long and thin. Fatness is a realistic assumption, since in many practical instances of geometric problems the considered objects are fat. <p> space for motion planning amidst fat obstacles [33], and efficient algorithms for computing depth orders on certain fat objects [1], binary space partitions for scenes of nonintersecting fat objects in the plane [6], hidden surface removal for fat horizontal triangles [14], and range searching and point location among fat objects <ref> [22, 23] </ref>. Contrary to many other definitions of fatness in literature [1, 2, 10, 14, 16, 19], the notion introduced in [33], and recaptured below, applies to general objects in arbitrary dimension d.
Reference: [24] <author> Ph. Pignon, </author> <title> Structuration de l' espace pour une planification hierarchisee des trajec-toires de robots mobiles, </title> <type> Ph.D. Thesis, </type> <institution> LAAS-CNRS and Universite Paul Sabatier de Toulouse, </institution> <note> Rapport LAAS N o 93395 (1993) (in French). </note>
Reference-contexts: It is though trivial to extend it to sets that satisfy the low obstacle density property.) Circumstances that resemble the low obstacle density have also been studied by Schwartz and Sharir [29] who refer to it as bounded local complexity and by Pignon <ref> [24] </ref> who calls it sparsity. A question that immediately comes to mind when considering the combinatorial result of [33] is whether this reduced complexity opens the way to efficient motion planning algorithms for such realistic environments. <p> Any (imaginary) ball with radius r in such a workspace intersects no more than a constant number of obstacles. The property resembles our notion of low obstacle density. The authors give directions on 5 how to solve the motion planning problem in such workspaces. Pignon <ref> [24] </ref> structures two--dimensional workspaces with polygonal obstacles and a polygonal robot using Minkowski differences to easily detect simple and impossible path-finding queries. Simple queries allow to replace the robot by an outer approximation with fewer degrees of freedom.
Reference: [25] <author> J.T. Schwartz and M. Sharir, </author> <title> On the piano movers' problem: I. The case of a two-dimensional rigid polygonal body moving amidst polygonal boundaries, </title> <journal> Communications on Pure and Applied Mathematics 36 (1983), </journal> <pages> pp. 345-398. </pages>
Reference-contexts: A direction of research in computational geometry, initiated by a series of papers known as the Piano Movers' series <ref> [25, 26, 27, 28, 31] </ref> by Schwartz and Sharir in the early 80s, studies the exact solution of the motion planning problem. Exact methods for solving the fl Research is supported by the Dutch Organization for Scientific Research (N.W.O.) and by the ESPRIT III BRA Project 6546 (PROMotion). <p> Although there essentially exist two different approaches to exact motion planning (cell decomposition and retraction), the time spent in processing the free space and the size of the resulting query structure clearly depend on the complexity of the free space. Cell decomposition algorithms (see e.g. <ref> [15, 18, 25, 26, 27, 28, 31] </ref>) partition the free space into a finite number of simple connected subcells, such that planning a motion between two placements in a single subcell is straightforward and such that uniform crossing rules can be defined for B crossing from one cell into another. <p> The O (n 5 ) algorithm by Schwartz and Sharir <ref> [25] </ref> for planning the motion of a ladder or a polygonal robot amidst polygonal obstacles can be shown to run, unmodified, in time O (n 2 ) if the obstacle density is low, whereas a minor modification enhances the efficiency to a running time of O (n log n) (see also <p> The only general methods that could solve such a problem are those by Schwartz and Sharir <ref> [25] </ref> and Canny [8]. Here, it is shown that an instance of the algorithm LDMot for a bounded-size robot in a low density workspace with arbitrary obstacles exists with running time fi (n 3 ), independent of the actual number f of degrees of freedom of the robot.
Reference: [26] <author> J.T. Schwartz and M. Sharir, </author> <title> On the piano movers' problem: II. General techniques for computing topological properties of real algebraic manifolds, </title> <booktitle> Advances in Applied Mathematics 4 (1983), </booktitle> <pages> pp. 298-351. </pages>
Reference-contexts: A direction of research in computational geometry, initiated by a series of papers known as the Piano Movers' series <ref> [25, 26, 27, 28, 31] </ref> by Schwartz and Sharir in the early 80s, studies the exact solution of the motion planning problem. Exact methods for solving the fl Research is supported by the Dutch Organization for Scientific Research (N.W.O.) and by the ESPRIT III BRA Project 6546 (PROMotion). <p> Although there essentially exist two different approaches to exact motion planning (cell decomposition and retraction), the time spent in processing the free space and the size of the resulting query structure clearly depend on the complexity of the free space. Cell decomposition algorithms (see e.g. <ref> [15, 18, 25, 26, 27, 28, 31] </ref>) partition the free space into a finite number of simple connected subcells, such that planning a motion between two placements in a single subcell is straightforward and such that uniform crossing rules can be defined for B crossing from one cell into another. <p> Algorithms for efficient motion planning in 3D workspaces are scarce: approaches in contact space, like the algorithms mentioned above by Sifrony and Sharir, and by Avnaim, Boissonnat, and Faverjon, were never shown to generalize to higher dimensions. General approaches to motion planning (e.g. by Schwartz and Sharir <ref> [26] </ref> with running time O (n 2 f+6 ) and Canny [8] with running time O (n f log n)) are computationally expensive, even under our beneficial circumstances. These rigorous methods do not take advantage of any accidental structure of the free space (as is present in our case). <p> It has been noted that the existing planar motion planning algorithms are not easily extendible towards other in particular spatial problems. Moreover, the existing general approaches to motion planning (like those by Schwartz and Sharir <ref> [26] </ref> and Canny [8]) are computationally expensive, even for problems from the special class that we consider here. Motion planning problems in Euclidean workspaces of dimension three normally imply at least three-dimensional configuration spaces. <p> The first for-loop computes a decomposition of FP " (R fi D) into O (1) (constant-complexity) subcells and gathers these in a set V C . One possible way to perform this computation in constant time is by applying (in step 2) the rigorous techniques by Schwartz and Sharir <ref> [26] </ref> to the constant number of constraint hypersurfaces intersecting the cylinder RfiD. The outcome is a subdivision of the arrangement cells into constant-complexity subcells. <p> The techniques by Schwartz and Sharir from <ref> [26] </ref> may be useful to compute a decomposition of the free part FP " (R fi D) of a cylinder R fi D. The refinement of step 1 of the first for-loop verifies the running time of O (jV W j) for the first for-loop of the transformation. <p> The number of algorithms for motion planning problems in a three-dimensional workspace with polyhedral obstacles is limited. The two methods that apply to robots with an arbitrary number f 3 of degrees of freedom are the general O (n 2 f+6 ) cell decomposition algorithm by Schwartz and Sharir <ref> [26] </ref> and the O (n f log n) roadmap method by Canny [8].
Reference: [27] <author> J.T. Schwartz and M. Sharir, </author> <title> On the piano movers' problem: III. Coordinating the motion of several independent bodies: the special case of circular bodies moving amidst polygonal barriers, </title> <journal> International Journal of Robotics Research 2 (1983), </journal> <pages> pp. 46-75. </pages>
Reference-contexts: A direction of research in computational geometry, initiated by a series of papers known as the Piano Movers' series <ref> [25, 26, 27, 28, 31] </ref> by Schwartz and Sharir in the early 80s, studies the exact solution of the motion planning problem. Exact methods for solving the fl Research is supported by the Dutch Organization for Scientific Research (N.W.O.) and by the ESPRIT III BRA Project 6546 (PROMotion). <p> Although there essentially exist two different approaches to exact motion planning (cell decomposition and retraction), the time spent in processing the free space and the size of the resulting query structure clearly depend on the complexity of the free space. Cell decomposition algorithms (see e.g. <ref> [15, 18, 25, 26, 27, 28, 31] </ref>) partition the free space into a finite number of simple connected subcells, such that planning a motion between two placements in a single subcell is straightforward and such that uniform crossing rules can be defined for B crossing from one cell into another.
Reference: [28] <author> J.T. Schwartz and M. Sharir, </author> <title> On the piano movers' problem: V. The case of a rod moving in three-dimensional space amidst polyhedral obstacles, </title> <journal> Communications on Pure and Applied Mathematics 37 (1984), </journal> <pages> pp. 815-848. </pages>
Reference-contexts: A direction of research in computational geometry, initiated by a series of papers known as the Piano Movers' series <ref> [25, 26, 27, 28, 31] </ref> by Schwartz and Sharir in the early 80s, studies the exact solution of the motion planning problem. Exact methods for solving the fl Research is supported by the Dutch Organization for Scientific Research (N.W.O.) and by the ESPRIT III BRA Project 6546 (PROMotion). <p> Although there essentially exist two different approaches to exact motion planning (cell decomposition and retraction), the time spent in processing the free space and the size of the resulting query structure clearly depend on the complexity of the free space. Cell decomposition algorithms (see e.g. <ref> [15, 18, 25, 26, 27, 28, 31] </ref>) partition the free space into a finite number of simple connected subcells, such that planning a motion between two placements in a single subcell is straightforward and such that uniform crossing rules can be defined for B crossing from one cell into another. <p> More specific results include O (n 11 ) <ref> [28] </ref> and O (n 6 log n) [15] algorithms for a (5-DOF) ladder among polyhedral obstacles, and an O (n 15 ) algorithm [28] for a polyhedral robot in the same environment. <p> More specific results include O (n 11 ) <ref> [28] </ref> and O (n 6 log n) [15] algorithms for a (5-DOF) ladder among polyhedral obstacles, and an O (n 15 ) algorithm [28] for a polyhedral robot in the same environment. This section presents an instance of the algorithm LDMot with running time O (n 2 log n) for the following class of problems.
Reference: [29] <author> J.T. Schwartz and M. Sharir, </author> <title> Efficient motion planning algorithms in environments of bounded local complexity, </title> <type> Report 164, </type> <institution> Department of Computer Science, Courant Inst. Math. Sci., </institution> <address> New York NY (1985). </address>
Reference-contexts: It is though trivial to extend it to sets that satisfy the low obstacle density property.) Circumstances that resemble the low obstacle density have also been studied by Schwartz and Sharir <ref> [29] </ref> who refer to it as bounded local complexity and by Pignon [24] who calls it sparsity. A question that immediately comes to mind when considering the combinatorial result of [33] is whether this reduced complexity opens the way to efficient motion planning algorithms for such realistic environments. <p> This number gives some idea of how cluttered the obstacles in the workspace are and is closely related to the complexity of the free space. Schwartz and Sharir <ref> [29] </ref> consider workspaces with obstacles of so-called bounded local complexity. Any (imaginary) ball with radius r in such a workspace intersects no more than a constant number of obstacles. The property resembles our notion of low obstacle density.
Reference: [30] <author> M. Sharir, </author> <title> Efficient algorithms for planning purely translational collision-free motion in two and three dimensions, </title> <booktitle> Proc. of the IEEE Int. Conf. on Robotics and Automation, </booktitle> <address> Raleigh NC (1987), </address> <pages> pp. 1326-1331. </pages>
Reference-contexts: These bounds generally remain close to one order of magnitude, i.e., a factor n, below the (n f ) bound (see e.g. <ref> [30, 38] </ref>). Hence, even in such more specific cases, the theoretical worst-case bounds are high.
Reference: [31] <author> M. Sharir and E. Ariel-Sheffi, </author> <title> On the piano movers' problem: IV. Various decomposable two-dimensional motion planning problems, </title> <journal> Communications on Pure and Applied Mathematics 37 (1984), </journal> <pages> pp. 479-493. 34 </pages>
Reference-contexts: A direction of research in computational geometry, initiated by a series of papers known as the Piano Movers' series <ref> [25, 26, 27, 28, 31] </ref> by Schwartz and Sharir in the early 80s, studies the exact solution of the motion planning problem. Exact methods for solving the fl Research is supported by the Dutch Organization for Scientific Research (N.W.O.) and by the ESPRIT III BRA Project 6546 (PROMotion). <p> Although there essentially exist two different approaches to exact motion planning (cell decomposition and retraction), the time spent in processing the free space and the size of the resulting query structure clearly depend on the complexity of the free space. Cell decomposition algorithms (see e.g. <ref> [15, 18, 25, 26, 27, 28, 31] </ref>) partition the free space into a finite number of simple connected subcells, such that planning a motion between two placements in a single subcell is straightforward and such that uniform crossing rules can be defined for B crossing from one cell into another.
Reference: [32] <author> S. Sifrony and M. Sharir, </author> <title> A new efficient motion planning algorithm for a rod in two-dimensional polygonal space, </title> <booktitle> Algorithmica 2 (1987), </booktitle> <pages> pp. 367-402. </pages>
Reference-contexts: As a result, the size of the query structure the connectivity graph CG and the time to compute it depend on the complexity of FP. Retraction methods (see e.g. <ref> [8, 17, 20, 21, 32] </ref>) aim at capturing the structure and connectivity of the free space in some one-dimensional network of curves in the free space, the roadmap. <p> The vast majority of motion planning algorithms have no reported sensitivity to the complexity of the free space. A clear exception is the boundary-vertices retraction algorithm by Sifrony and Sharir <ref> [32] </ref> for a ladder moving in a planar workspace with polygonal obstacles. The algorithm runs in time O (K log n), where K is the number of pairs of obstacle corners that lie less than the length of the ladder apart. <p> We are aware of only few (related) results on exact motion planning methods with provable efficiency or free space complexity-sensitive behavior for realistic motion planning problems (with low complexity workspaces or free spaces). The running time of Sifrony and Sharir's algorithm <ref> [32] </ref> depends on the number of pairs of obstacle corners that lie less than the length of the ladder apart. This number gives some idea of how cluttered the obstacles in the workspace are and is closely related to the complexity of the free space.
Reference: [33] <author> A.F. van der Stappen, D. Halperin, M.H. Overmars, </author> <title> The complexity of the free space for a robot moving amidst fat obstacles, Computational Geometry: </title> <booktitle> Theory and Applications 3 (1993), </booktitle> <pages> pp. 353-373. </pages>
Reference-contexts: What natural mild assumptions would for example lead to the relative low obstacle density of the above example, in which the robot is unable to touch more than a constant number of obstacles simultaneously? Van der Stappen, Halperin, and Overmars <ref> [33] </ref> show that the combinatorial complexity of the free space is linear in the number of obstacles if the robot is not too large compared to the obstacles and if any workspace region of size proportional to the size of the smallest obstacle intersects no more than a constant number of <p> We shall refer to the last property as the low obstacle density property of the workspace. (Actually, in <ref> [33] </ref> the linear bound is only proven for the more restricted assumption of fatness of the obstacles. <p> A question that immediately comes to mind when considering the combinatorial result of <ref> [33] </ref> is whether this reduced complexity opens the way to efficient motion planning algorithms for such realistic environments. The vast majority of motion planning algorithms have no reported sensitivity to the complexity of the free space. <p> The results are basically reformulations of results previously reported in <ref> [33] </ref>, but we repeat them because they are fundamental to this paper. As the relative sizes of the robot and the obstacles play a crucial role throughout the paper, we first give convenient measures for the size of an obstacle and a robot. <p> In the specific case that IR d is the workspace W of a robot, and E is the set of obstacles in W, we will refer to W as a low obstacle density workspace. Theorem 2.3 states the linear complexity result. The reader is referred to <ref> [33] </ref> for a proof. Theorem 2.3 The free space for a constant-complexity robot B with reach B b moving in a low obstacle density workspace W with n constant-complexity obstacles E 2 E with minimal enclosing hypersphere radii at least , for some constant b 0, has complexity O (n). <p> An immediate consequence of the preceding results will be the linear free space complexity for problems in this class. 2.2 Fatness Fatness has turned out to be an interesting phenomenon in computational geometry. Several papers present surprising combinatorial complexity reductions <ref> [2, 10, 16, 19, 33] </ref> and efficiency gains for algorithms [1, 6, 14, 22, 23] if the objects under consideration have a certain fatness. Fat objects are compact to some extent, rather than long and thin. <p> The achievements of the study of fatness so far include near-linear bounds on the complexity of the union of certain fat figures (e.g. triangles, wedges) in the plane [2, 10, 16, 19], a linear bound on the complexity 7 of the free space for motion planning amidst fat obstacles <ref> [33] </ref>, and efficient algorithms for computing depth orders on certain fat objects [1], binary space partitions for scenes of nonintersecting fat objects in the plane [6], hidden surface removal for fat horizontal triangles [14], and range searching and point location among fat objects [22, 23]. <p> Contrary to many other definitions of fatness in literature [1, 2, 10, 14, 16, 19], the notion introduced in <ref> [33] </ref>, and recaptured below, applies to general objects in arbitrary dimension d. The definition involves a parameter k, supplying a qualitative measure of the fatness of an object: the smaller the value of k, the fatter the object must be. <p> An intuitive explanation lies in the observation that it is impossible to have a very large number of fat obstacles of a certain minimum size intersecting a small region. A more formal proof follows from <ref> [33] </ref>. Here, we confine ourselves to reporting the result. Theorem 2.5 A space IR d with non-intersecting k-fat objects is a low object density space. 2.3 Object wrappings This subsection studies the complexity implications of expanding the objects in a scene satisfying Property 2.2 for the arrangement of object boundaries. <p> The mild assumptions provide a realistic framework for many practical motion planning problems. The complexity of the free space for problems that satisfy the assumptions was proven to be O (n) <ref> [33] </ref>, whereas the complexity can easily be as high as (n f ) when both assumptions are dropped. Besides having a low combinatorial complexity, the free space for a motion planning problem that fits in our framework also has a beneficial structure.
Reference: [34] <author> A.F. van der Stappen, </author> <title> The complexity of the free space for motion planning amidst fat obstacles, </title> <journal> Journal of Intelligent and Robotic Systems 11 (1994), </journal> <pages> pp. 21-44. </pages>
Reference-contexts: for planning the motion of a ladder or a polygonal robot amidst polygonal obstacles can be shown to run, unmodified, in time O (n 2 ) if the obstacle density is low, whereas a minor modification enhances the efficiency to a running time of O (n log n) (see also <ref> [34] </ref>). Hence, there exist (planar) motion planning algorithms that do benefit from low free space complexities, even though several other algorithms do not.
Reference: [35] <author> A.F. van der Stappen and M.H. Overmars, </author> <title> Motion planning amidst fat obstacles, </title> <booktitle> Proc. 10th Ann. ACM Symp. on Computational Geometry (1994), </booktitle> <pages> pp. 31-40. </pages>
Reference: [36] <author> A.F. van der Stappen, </author> <title> Motion planning amidst fat obstacles, </title> <type> Ph.D. Thesis, </type> <institution> Dept. of Computer Science, Utrecht University (1994). </institution>
Reference-contexts: The low 4 obstacle density causes K to be only O (n), whereas it would be O (n 2 ) for arbitrary workspaces with obstacles. Some algorithms have a hidden sensitivity to the complexity of the free space <ref> [36] </ref>. For example, the boundary cell decomposition algorithm by Avnaim, Boissonnat, and Faverjon [4], running in time O (n 3 log n) for a constant complexity polygonal robot amidst arbitrary polygonal obstacles, can be shown to run in O (n log n) time in the low obstacle density setting. <p> The constant number of additional constraint hypersurfaces induced by the self-collisions of the constant-complexity robot does not increase the asymptotic complexity of the arrangement inside any cylinder R fi D. Therefore, the combinatorial and algorithmic considerations of this section apply without restrictions. The reader is referred to <ref> [36] </ref> for further details. 4 Small and efficiently computable base partitions This objective in this section is to find instances of the general paradigm presented in Section 3 for a handful of different settings of the motion planning problem. <p> Lemma 4.10 states an upper bound on the number of region adjacencies in the cc-partition. The reader is referred to <ref> [36] </ref> for a proof of the result. Lemma 4.10 E W = f (R; R 0 ) 2 V W fi V W j@R " @R 0 6= ;g has size O (n). <p> Below we give an informal explanation for the correctness of this claim. The reader is referred to <ref> [36] </ref> for a more thorough explanation. The grown obstacle boundaries partition the workspace W into regions of constant-size coverage. When the robot's reference point is restricted to such a region, the robot can only touch a constant number of different obstacles.
Reference: [37] <author> C. Wentink and O. Schwarzkopf, </author> <title> Motion planning for vacuum cleaner robots, </title> <booktitle> Proc. 6th Canadian Conf. on Computational Geometry (1994), </booktitle> <pages> pp. 51-56. </pages>
Reference-contexts: Instead, many robots move in three-dimensional workspaces amidst spatial obstacles while their motion is confined to a (planar) workfloor. An example of such a setting is a vacuum cleaner moving in a room in which objects hang from the ceiling and stand on the floor <ref> [37] </ref>. Sometimes, the nature and positions of the obstacles do not allow to reduce such problem to purely planar motion planning. The vacuum cleaner, for example, can easily pass under a table. <p> A solution to this type of path-finding for a polyhedral robot amidst polyhedral obstacles is given by Wentink and Schwarzkopf in <ref> [37] </ref>. Their algorithm, which is a generalization of the boundary cell decomposition algorithm by Avnaim, Boissonnat, and Faverjon [4], runs in time O (n 3 log n).
Reference: [38] <author> A. Wiernik and M. Sharir, </author> <title> Planar realizations of nonlinear Davenport-Schinzel sequences by segments, </title> <booktitle> Discrete & Computational Geometry 3 (1988), </booktitle> <pages> pp. 15-47. 35 </pages>
Reference-contexts: These bounds generally remain close to one order of magnitude, i.e., a factor n, below the (n f ) bound (see e.g. <ref> [30, 38] </ref>). Hence, even in such more specific cases, the theoretical worst-case bounds are high.
References-found: 38

