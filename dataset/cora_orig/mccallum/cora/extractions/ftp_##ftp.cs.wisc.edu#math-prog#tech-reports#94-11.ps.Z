URL: ftp://ftp.cs.wisc.edu/math-prog/tech-reports/94-11.ps.Z
Refering-URL: http://www.cs.wisc.edu/~chunhui/chunhui.html
Root-URL: 
Title: A Class of Smoothing Functions for Nonlinear and Mixed Complementarity Problems Dedicated to  
Author: Richard W. Cottle, friend and colleague, Chunhui Chen O. L. Mangasarian 
Note: on the occasion of his sixtieth birthday  
Abstract: Mathematical Programming Technical Report 94-11 August 1994 / Revised February 1995 Abstract We propose a class of parametric smooth functions that approximate the fundamental plus function, (x) + =maxf0; xg, by twice integrating a probability density function. This leads to classes of smooth parametric nonlinear equation approximations of nonlinear and mixed complementarity problems (NCPs and MCPs). For any solvable NCP or MCP, existence of an arbitrarily accurate solution to the smooth nonlinear equation as well as the NCP or MCP, is established for sufficiently large value of a smoothing parameter ff. Newton-based algorithms are proposed for the smooth problem. For strongly monotone NCPs, global convergence and local quadratic convergence are established. For solvable monotone NCPs, each accumulation point of the proposed algorithms solves the smooth problem. Exact solutions of our smooth nonlinear equation for various values of the parameter ff, generate an interior path, which is different from the central path for interior point method. Computational results for 52 test problems compare favorably with those for another Newton-based method. The smooth technique is capable of solving efficiently the test problems solved by Dirkse & Ferris [8], Harker & Xiao [13] and Pang & Gabriel [30]. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Ben-Tal and M. Teloulle. </author> <title> A smoothing technique for nondifferentiable optimization problems. </title> <editor> In S. Deldeki, editor, </editor> <booktitle> Optimization, </booktitle> <pages> pages 1-11, </pages> <address> Berlin, </address> <year> 1989. </year> <note> Springer-Verlag. Lecture Notes in Mathematics 1405. </note>
Reference-contexts: ffx ) 1 ) = 1 + e ffx = t (~; ff)d~ 1 ) = (1 + e ffx ) 2 = ffs (x; ff)(1 s (x; ff)) The function p (x; ff) above can also be obtained by using an appropriate recession function given in Example 2.2 of <ref> [1] </ref>, or by an exponential penalty function as in [3, p. 313, equation (25)]. Figures 4 to 6 depict the functions p (x; 5); s (x; 5) and t (x; 5) respectively. Following are several other smooth plus functions based on probability density functions proposed by other authors. <p> [17] and [4] Let 2 3 Here D 1 = 1; D 2 = 0; suppfd (x)g = R and ^p (x; fi) = p 2 Example 2.3 Pinar-Zenios Smooth Plus Function [31] Let 0 otherwise Here D 1 = 0; D 2 = 1 2 ; suppfd (x)g = <ref> [0; 1] </ref> and ^p (x; fi) = &gt; &lt; 0 if x &lt; 0 2fi if 0 x fi 2 if x &gt; fi This function can also be obtained by applying the Moreau-Yosida regularization [15, p.13] to the plus function. 8 p (x; ff) ff log (1 + e ffx <p> By Proposition 3.1 we get that x satisfies (10). If y is another solution of (10), then 0 = (x y)(R (x) R (y)) = (x y) t=0 for some t 2 <ref> [0; 1] </ref>. Since rR is positive definite by Proposition 3.1, it follows that x = y. Therefore equation (10) has a unique solution. Let x (fi) be a solution of (10). Then x (fi) = ^p (x (fi) F (x (fi)); fi). <p> also grateful to Jong-Shi Pang for providing us with the generalized von Thunen model and an initial starting point, and to Marc Teboulle for pointing out that our function p (x; ff) of Example 2.1 can also be obtained by using an appropriate recession function given in Example 2.2 of <ref> [1] </ref>.
Reference: [2] <author> Dimitri P. Bertsekas. </author> <title> Projected Newton methods for optimization problems with simple constraints. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 20 </volume> <pages> 221-246, </pages> <year> 1982. </year>
Reference-contexts: Figures 9 to 12 indicate that our smooth algorithm is faster than PATH solver for the larger problems, whereas PATH solver is faster on smaller problems. The newest version of PATH (PATH 2.7) that uses a Newton method on the active set <ref> [2] </ref> as a preprocessor, improves solution times on the larger problems. Our smooth method can be similarly improved by adding the projected Newton preprocessor. We have compared PATH and SMOOTH with a Newton preprocessor on a Sun SPARCstation 20. The results are given in Figures 13 to 16.
Reference: [3] <author> D.P. Bertsekas. </author> <title> Constrained Optimization and Lagrange Multiplier Methods. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: = t (~; ff)d~ 1 ) = (1 + e ffx ) 2 = ffs (x; ff)(1 s (x; ff)) The function p (x; ff) above can also be obtained by using an appropriate recession function given in Example 2.2 of [1], or by an exponential penalty function as in <ref> [3, p. 313, equation (25)] </ref>. Figures 4 to 6 depict the functions p (x; 5); s (x; 5) and t (x; 5) respectively. Following are several other smooth plus functions based on probability density functions proposed by other authors.
Reference: [4] <author> Bintong Chen and P. T. Harker. </author> <title> A non-interior-point continuation method for linear complementarity problems. </title> <journal> SIAM Journal on Matrix Analysis and Applicatons, </journal> <volume> 14 </volume> <pages> 1168-1190, </pages> <year> 1993. </year>
Reference-contexts: This is much simpler than solving a mixed linear complementarity problem or a quadratic program. Smoothing techniques have already been applied to different problems, such as, l 1 minimization problems [21], multi-commodity flow problems [31], nonsmooth programming [41, 20], linear and convex inequalities [5], and linear complementarity problems <ref> [4] </ref>, [5] and [17]. These successful techniques motivate a systematic study of the smoothing approach. Questions we wish to address include the following. <p> Figures 4 to 6 depict the functions p (x; 5); s (x; 5) and t (x; 5) respectively. Following are several other smooth plus functions based on probability density functions proposed by other authors. Example 2.2 Chen-Harker-Kanzow-Smale Smooth Plus Function [38], [17] and <ref> [4] </ref> Let 2 3 Here D 1 = 1; D 2 = 0; suppfd (x)g = R and ^p (x; fi) = p 2 Example 2.3 Pinar-Zenios Smooth Plus Function [31] Let 0 otherwise Here D 1 = 0; D 2 = 1 2 ; suppfd (x)g = [0; 1] and <p> Hence an exact solution of (10) is interior to the feasible region. However the iterates of the smooth method, which are only approximate solutions of (10), are not necessarily feasible. For the function ^p defined in Example 2.2 <ref> [38, 17, 4] </ref> , the exact solution x of the equation (10) satisfies x &gt; 0; F (x) &gt; 0; x i F i (x) = fi 2 ; i = 1; ; n which is precisely the central path of the interior point method for solving NCP. <p> Methods that trace this path but allow iterates to be exterior to the feasible region have been proposed in [38], <ref> [4] </ref> and [17]. In [18], the relation between Smale's method [38] and the central path was pointed out.
Reference: [5] <author> Chunhui Chen and O.L. Mangasarian. </author> <title> Smoothing methods for convex inequalities and linear complementarity problems. </title> <type> Technical Report 1191, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin 53706, </institution> <month> November </month> <year> 1993. </year> <note> Mathematical Programming, to appear. </note>
Reference-contexts: This is much simpler than solving a mixed linear complementarity problem or a quadratic program. Smoothing techniques have already been applied to different problems, such as, l 1 minimization problems [21], multi-commodity flow problems [31], nonsmooth programming [41, 20], linear and convex inequalities <ref> [5] </ref>, and linear complementarity problems [4], [5] and [17]. These successful techniques motivate a systematic study of the smoothing approach. Questions we wish to address include the following. <p> This is much simpler than solving a mixed linear complementarity problem or a quadratic program. Smoothing techniques have already been applied to different problems, such as, l 1 minimization problems [21], multi-commodity flow problems [31], nonsmooth programming [41, 20], linear and convex inequalities <ref> [5] </ref>, and linear complementarity problems [4], [5] and [17]. These successful techniques motivate a systematic study of the smoothing approach. Questions we wish to address include the following. <p> The properties of the function ^p (x; fi) are given in Proposition 2.2 above. We now give examples of smooth plus functions. The first example, which will be used throughout this paper, is based on the sigmoid function of neural networks <ref> [14, 22, 5] </ref> and defined as follows: s (x; ff) = 1 + e ffx ; ff &gt; 0 This function approximates the step function (x) as ff tends to infinity. <p> Since the derivative with respect to x of this function tends to the Dirac delta function as ff tends to infinity, it follows that ff plays the role of 1 fi and we shall therefore take ff = fi 7 Example 2.1 Neural Networks Smooth Plus Function <ref> [5] </ref> Let e x Here D 1 = log 2; D 2 = 0 and suppfd (x)g = R, where D 1 and D 2 are defined by (7) and (8). <p> as follows: ^p (x; fi) ^s (x; fi) ^ t (x; fi) = 1 fi ) (x) + (x) ffi (x) p (x; ff) = ^p (x; 1 R ff ) t (x; ff) = ^ t (x; 1 Because of our favorable experience with the function p (x; ff) <ref> [5] </ref> on linear complementarity problems and linear and convex inequalities, we chose it for our numerical experiments.
Reference: [6] <author> R.W. Cottle, F. Giannessi, and J.-L. Lions. </author> <title> Variational Inequalities and Complementariy Problems. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: Many problems can be formulated by using this complementarity condition. For example, most optimality conditions of mathematical programming [26] as well as variational inequalities <ref> [6] </ref> and extended complementarity problems [23, 11, 40] can be so formulated.
Reference: [7] <author> J.E. Dennis and R.B. Schnabel. </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1983. </year>
Reference-contexts: The above algorithm is well defined for a monotone NCP with a continuously differentiable F (x). We will state the following convergence theorem <ref> [7] </ref>. We omit the proof that is similar to the proof of Theorem 4.3. Theorem 3.4 Consider a solvable monotone nonlinear complementarity problem (9) with F (x) 2 LC 1 K (R n ). <p> According to the standard result of local quadratic convergence for Newton Method, Theorem 5.2.1 in <ref> [7] </ref>, the conclusion follows. (4) Let ff i ; i = 0; 1; , be the sequence of different parameters ff used in Algorithm 4.1.
Reference: [8] <author> S.P. Dirkse and M.C. Ferris. </author> <title> The path solver: A non-monotone stabilization scheme for mixed complementarity problems. </title> <institution> Computer Sciences Department Technical Report 1179, University of Wisconsin, Madison, Wisconsin, </institution> <year> 1993. </year> <month> 35 </month>
Reference-contexts: Since then, several approaches based on B-differentiable equations were investigated in [13], [28] and [29]. In addition, an algorithm based on nonsmooth equations and successive quadratic programming was given [30], as well as a Newton method with a path following technique <ref> [32, 8] </ref>, and a trust region Newton method for solving a nonlinear least squares reformulation of the NCP [24]. With the exception of [24], a feature common to all these methods is that the subproblem at each Newton iteration is still a combinatorial problem. <p> In Section 6, encouraging numerical testing results are given for 2 52 problems from the MCPLIB [9] which includes all the problems attempted in [13], [30] and <ref> [8] </ref>. These problems range in size of up to 8192 variables. These examples include the difficult von Thunen NCP model [30, 39] which is solved here to an accuracy of 1.0e-7. A few words about our notation. <p> sequence fx k g converges to x quadratically. (4) If, in addition, F is strongly monotone and Lipschitz continuous, then the sequence fx k g converges to x, the solution of (10), at a quadratic rate. 4 The Mixed Complementarity Problem The mixed complementarity problem (MCP) is defined as follows <ref> [8] </ref>: Given a differentiable F : R n ! R n ; l; u 2 R n ; l &lt; u, where R = R [ f+1; 1g, find x; w; v 2 R n , such that F (x) w + v = 0 0 v ? u x 0 <p> The details of implementing the smooth algorithm are given in Appendix 2. For comparison, we also give the results for the PATH solver <ref> [8] </ref>. Both algorithms were run on a DECstation 5000/125. Among the 52 test problems, which includes all the problems attempted in [13], [30] and [8], 51 problems are from the MCPLIB [9], and one is the generalized von Thunen model from [30] and [39]. <p> The details of implementing the smooth algorithm are given in Appendix 2. For comparison, we also give the results for the PATH solver <ref> [8] </ref>. Both algorithms were run on a DECstation 5000/125. Among the 52 test problems, which includes all the problems attempted in [13], [30] and [8], 51 problems are from the MCPLIB [9], and one is the generalized von Thunen model from [30] and [39]. Our smooth algorithm was run using one set of default parameters and so was the PATH solver. <p> Figures 10, 11 and 12 depict the CPU times for all remaining problems except the von Thunen model. We note that the PATH solver <ref> [8] </ref> is faster than Josephy's Newton method [16] and Rutherford's GAMS [10] mixed inequality and linear equation solver (MILES) [37] which is also Newton-based. Figures 9 to 12 indicate that our smooth algorithm is faster than PATH solver for the larger problems, whereas PATH solver is faster on smaller problems.
Reference: [9] <author> S.P. Dirkse and M.C. Ferris. MCPLIB: </author> <title> A collection of nonlinear mixed complementarity problems. </title> <institution> Computer Sciences Department Technical Report 1215, University of Wisconsin, Madison, Wisconsin, </institution> <year> 1994. </year>
Reference-contexts: We compare the two paths on a simple example and show that our path gives a smaller error for the same value of the smoothing parameter fi. In Section 6, encouraging numerical testing results are given for 2 52 problems from the MCPLIB <ref> [9] </ref> which includes all the problems attempted in [13], [30] and [8]. These problems range in size of up to 8192 variables. These examples include the difficult von Thunen NCP model [30, 39] which is solved here to an accuracy of 1.0e-7. A few words about our notation. <p> For comparison, we also give the results for the PATH solver [8]. Both algorithms were run on a DECstation 5000/125. Among the 52 test problems, which includes all the problems attempted in [13], [30] and [8], 51 problems are from the MCPLIB <ref> [9] </ref>, and one is the generalized von Thunen model from [30] and [39]. Our smooth algorithm was run using one set of default parameters and so was the PATH solver. The smooth algorithm is written in the C language and implemented by using the GAMS/CPLIB [10]. <p> A MINOS routine [25] was used to perform a sparse LU decomposition for solving sparse linear equations. Both algorithms use the same convergence tolerance of * = 1:0e 6. Table 1 gives a simple description of the test problems <ref> [9] </ref>. 25 the smooth nonlinear equation (10) versus the central path for Example 5.1 5.1 26 Table 1: MCPLIB Problems Model origin GAMS file Size Distillation column modeling (NLE) hydroc20.gms 99 Distillation column modeling (NLE) hydroc06.gms 29 Distillation column modeling (NLE) methan08.gms 31 NLP problem form Powell (NLP) powell mcp.gms 8
Reference: [10] <author> S.P. Dirkse, M.C. Ferris, P.V. Preckel, and T. Rutherford. </author> <title> The GAMS callable program library for variational and complementarity solvers. </title> <type> Manuscript, </type> <institution> University of Wisconsin, </institution> <year> 1993. </year>
Reference-contexts: Our smooth algorithm was run using one set of default parameters and so was the PATH solver. The smooth algorithm is written in the C language and implemented by using the GAMS/CPLIB <ref> [10] </ref>. A MINOS routine [25] was used to perform a sparse LU decomposition for solving sparse linear equations. Both algorithms use the same convergence tolerance of * = 1:0e 6. <p> Figures 10, 11 and 12 depict the CPU times for all remaining problems except the von Thunen model. We note that the PATH solver [8] is faster than Josephy's Newton method [16] and Rutherford's GAMS <ref> [10] </ref> mixed inequality and linear equation solver (MILES) [37] which is also Newton-based. Figures 9 to 12 indicate that our smooth algorithm is faster than PATH solver for the larger problems, whereas PATH solver is faster on smaller problems.
Reference: [11] <author> M.S. Gowda. </author> <title> On the extended linear complementarity problem. </title> <type> Technical report, </type> <institution> Department of Mathematics & Statistics, University of Maryland Baltimore County, Baltimore, Maryland, </institution> <year> 1994. </year>
Reference-contexts: Many problems can be formulated by using this complementarity condition. For example, most optimality conditions of mathematical programming [26] as well as variational inequalities [6] and extended complementarity problems <ref> [23, 11, 40] </ref> can be so formulated. It is obvious that the vectors x and y satisfy complementarity condition if and only if x = (x y) + ; fl Computer Sciences Department, University of Wisconsin, 1210 West Dayton Street, Madison, WI 53706, email: chunhui@cs.wisc.edu, olvi@cs.wisc.edu.
Reference: [12] <author> P. T. Harker and J.-S. Pang. </author> <title> Finite-dimensional variational inequality and nonlinear complementarity problems: A survey of theory, algorithms and applications. </title> <journal> Mathematical Programming, </journal> <volume> 48 </volume> <pages> 161-220, </pages> <year> 1990. </year>
Reference-contexts: The basic idea of this paper is to use a smooth function approximation to the plus function. With this approximation, many efficient algorithms, such as the Newton method, can be easily employed. There are many Newton-based algorithms for solving nonlinear complementarity problems, variational inequalities and mixed complementarity problems. In <ref> [12] </ref> a good summary and references up to 1988 are given. Generalizations of the Newton method to nonsmooth equations can be found in [34], [35] and [36]. Since then, several approaches based on B-differentiable equations were investigated in [13], [28] and [29]. <p> We first consider the strongly monotone NCP, that is there exists a k &gt; 0 such that for any x; y 2 R n Since the NCP is strongly monotone, it has a unique solution <ref> [12] </ref>. The following error bound for the strongly monotone NCP is given as Theorem 3.2.1 in [33]. Lemma 3.2 Let the NCP be strongly monotone and let F (x) be Lipschitz continuous.
Reference: [13] <author> P.T. Harker and B. Xiao. </author> <title> Newton's method for the nonlinear complementarity problem: A B-differentiable equation approach. </title> <journal> Mathematical Programming, </journal> <volume> 48 </volume> <pages> 339-357, </pages> <year> 1990. </year>
Reference-contexts: In [12] a good summary and references up to 1988 are given. Generalizations of the Newton method to nonsmooth equations can be found in [34], [35] and [36]. Since then, several approaches based on B-differentiable equations were investigated in <ref> [13] </ref>, [28] and [29]. In addition, an algorithm based on nonsmooth equations and successive quadratic programming was given [30], as well as a Newton method with a path following technique [32, 8], and a trust region Newton method for solving a nonlinear least squares reformulation of the NCP [24]. <p> In Section 6, encouraging numerical testing results are given for 2 52 problems from the MCPLIB [9] which includes all the problems attempted in <ref> [13] </ref>, [30] and [8]. These problems range in size of up to 8192 variables. These examples include the difficult von Thunen NCP model [30, 39] which is solved here to an accuracy of 1.0e-7. A few words about our notation. <p> The details of implementing the smooth algorithm are given in Appendix 2. For comparison, we also give the results for the PATH solver [8]. Both algorithms were run on a DECstation 5000/125. Among the 52 test problems, which includes all the problems attempted in <ref> [13] </ref>, [30] and [8], 51 problems are from the MCPLIB [9], and one is the generalized von Thunen model from [30] and [39]. Our smooth algorithm was run using one set of default parameters and so was the PATH solver.
Reference: [14] <author> J. Hertz, A. Krogh, and R. G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, California, </address> <year> 1991. </year>
Reference-contexts: The properties of the function ^p (x; fi) are given in Proposition 2.2 above. We now give examples of smooth plus functions. The first example, which will be used throughout this paper, is based on the sigmoid function of neural networks <ref> [14, 22, 5] </ref> and defined as follows: s (x; ff) = 1 + e ffx ; ff &gt; 0 This function approximates the step function (x) as ff tends to infinity.
Reference: [15] <author> J.-B. Hiriart-Urruty and C. </author> <title> Lemarechal. Convex Analysis and Minimization Algorithms I. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1993. </year>
Reference-contexts: In fact, this is the same as defining ^p (x; fi) = 1 Z x (x t) ^ t (t; fi)dt: (4) This formulation was given in [20] and <ref> [15, p.12] </ref> for a density (kernel) function with finite support. We will give our results in terms of a density function with arbitrary support. This includes the finite support density function as a special case. <p> otherwise Here D 1 = 0; D 2 = 1 2 ; suppfd (x)g = [0; 1] and ^p (x; fi) = &gt; &lt; 0 if x &lt; 0 2fi if 0 x fi 2 if x &gt; fi This function can also be obtained by applying the Moreau-Yosida regularization <ref> [15, p.13] </ref> to the plus function. 8 p (x; ff) ff log (1 + e ffx ) with ff = 5 s (x; ff) 1+e ffx with ff = 5 t (x; ff) (1+e ffx ) 2 with ff = 5 9 Example 2.4 Zang Smooth Plus Function [41] Let 2
Reference: [16] <author> N.H. Josephy. </author> <title> Newton's method for generalized equations. </title> <type> Technical Summary Report 1965, </type> <institution> Mathematics Research Center, University of Wisconsin, Madison, Wisconsin, </institution> <year> 1979. </year>
Reference-contexts: Figures 10, 11 and 12 depict the CPU times for all remaining problems except the von Thunen model. We note that the PATH solver [8] is faster than Josephy's Newton method <ref> [16] </ref> and Rutherford's GAMS [10] mixed inequality and linear equation solver (MILES) [37] which is also Newton-based. Figures 9 to 12 indicate that our smooth algorithm is faster than PATH solver for the larger problems, whereas PATH solver is faster on smaller problems.
Reference: [17] <author> C. Kanzow. </author> <title> Some tools allowing interior-point methods to become noninterior. </title> <type> Technical report, </type> <institution> Institue of Applied Mathematics, Unversity of Hamburg, Germany, </institution> <year> 1994. </year>
Reference-contexts: Smoothing techniques have already been applied to different problems, such as, l 1 minimization problems [21], multi-commodity flow problems [31], nonsmooth programming [41, 20], linear and convex inequalities [5], and linear complementarity problems [4], [5] and <ref> [17] </ref>. These successful techniques motivate a systematic study of the smoothing approach. Questions we wish to address include the following. <p> Figures 4 to 6 depict the functions p (x; 5); s (x; 5) and t (x; 5) respectively. Following are several other smooth plus functions based on probability density functions proposed by other authors. Example 2.2 Chen-Harker-Kanzow-Smale Smooth Plus Function [38], <ref> [17] </ref> and [4] Let 2 3 Here D 1 = 1; D 2 = 0; suppfd (x)g = R and ^p (x; fi) = p 2 Example 2.3 Pinar-Zenios Smooth Plus Function [31] Let 0 otherwise Here D 1 = 0; D 2 = 1 2 ; suppfd (x)g = [0; <p> Hence an exact solution of (10) is interior to the feasible region. However the iterates of the smooth method, which are only approximate solutions of (10), are not necessarily feasible. For the function ^p defined in Example 2.2 <ref> [38, 17, 4] </ref> , the exact solution x of the equation (10) satisfies x &gt; 0; F (x) &gt; 0; x i F i (x) = fi 2 ; i = 1; ; n which is precisely the central path of the interior point method for solving NCP. <p> Methods that trace this path but allow iterates to be exterior to the feasible region have been proposed in [38], [4] and <ref> [17] </ref>. In [18], the relation between Smale's method [38] and the central path was pointed out.
Reference: [18] <author> M. Kojima and N. Megiddo. </author> <title> The relation between the path of centers and smale's regularization of the linear programming problem. </title> <journal> Linear Algrbra and Its Applications, </journal> <volume> 152 </volume> <pages> 135-139, </pages> <year> 1991. </year>
Reference-contexts: Methods that trace this path but allow iterates to be exterior to the feasible region have been proposed in [38], [4] and [17]. In <ref> [18] </ref>, the relation between Smale's method [38] and the central path was pointed out.
Reference: [19] <author> M. Kojima, S. Mizuno, and A. Yoshise. </author> <title> A polynomial-time algorithm for a class of linear complementarity problems. </title> <journal> Mathematical Programming, </journal> <volume> 44 </volume> <pages> 1-27, </pages> <year> 1989. </year>
Reference-contexts: In Section 5 we show that exact solutions of our smooth nonlinear equation, for various values of the smoothing parameter fi generate an interior path to the feasible region, different from the central path of the interior point method <ref> [19] </ref>. We compare the two paths on a simple example and show that our path gives a smaller error for the same value of the smoothing parameter fi. <p> By (2) and (3), the whole sequence converges to an *accurate solution of MCP (21). We shall give our numerical test results for Algorithm 4.1 in Section 6, after relating our smooth approach to the central path of the interior point method <ref> [19] </ref> in Section 5. 5 Relation to the Interior Point Method In this section, we consider the NCP (9). Let the density function d (x) satisfy (A1)-(A3) and D 2 = 0, and let let ^p (x; fi) be defined by (4).
Reference: [20] <author> J. Kreimer and R.Y. Rubinstein. </author> <title> Nondifferentiable optimization via smooth approximation: General analytical approach. </title> <journal> Annals of Operations Research, </journal> <volume> 39 </volume> <pages> 97-119, </pages> <year> 1992. </year>
Reference-contexts: This is much simpler than solving a mixed linear complementarity problem or a quadratic program. Smoothing techniques have already been applied to different problems, such as, l 1 minimization problems [21], multi-commodity flow problems [31], nonsmooth programming <ref> [41, 20] </ref>, linear and convex inequalities [5], and linear complementarity problems [4], [5] and [17]. These successful techniques motivate a systematic study of the smoothing approach. Questions we wish to address include the following. <p> In fact, this is the same as defining ^p (x; fi) = 1 Z x (x t) ^ t (t; fi)dt: (4) This formulation was given in <ref> [20] </ref> and [15, p.12] for a density (kernel) function with finite support. We will give our results in terms of a density function with arbitrary support. This includes the finite support density function as a special case.
Reference: [21] <author> K. Madsen and H.B. Nielsen. </author> <title> A finite smoothing algorithm for linear l 1 estimation. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 3(2) </volume> <pages> 223-235, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Consequently, at each Newton step, we only need to solve a linear equation. This is much simpler than solving a mixed linear complementarity problem or a quadratic program. Smoothing techniques have already been applied to different problems, such as, l 1 minimization problems <ref> [21] </ref>, multi-commodity flow problems [31], nonsmooth programming [41, 20], linear and convex inequalities [5], and linear complementarity problems [4], [5] and [17]. These successful techniques motivate a systematic study of the smoothing approach. Questions we wish to address include the following.
Reference: [22] <author> O.L. Mangasarian. </author> <title> Mathematical programming in neural networks. </title> <journal> ORSA Journal on Computing, </journal> <volume> 5(4) </volume> <pages> 349-360, </pages> <year> 1993. </year>
Reference-contexts: The properties of the function ^p (x; fi) are given in Proposition 2.2 above. We now give examples of smooth plus functions. The first example, which will be used throughout this paper, is based on the sigmoid function of neural networks <ref> [14, 22, 5] </ref> and defined as follows: s (x; ff) = 1 + e ffx ; ff &gt; 0 This function approximates the step function (x) as ff tends to infinity.
Reference: [23] <author> O.L. Mangasarian and J.-S. Pang. </author> <title> The extended linear complementarity problem. </title> <type> Technical Report 1188, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin 53706, </institution> <month> November </month> <year> 1993. </year> <note> SIAM Journal on Matrix Analysis and Applications, to appear. </note>
Reference-contexts: Many problems can be formulated by using this complementarity condition. For example, most optimality conditions of mathematical programming [26] as well as variational inequalities [6] and extended complementarity problems <ref> [23, 11, 40] </ref> can be so formulated. It is obvious that the vectors x and y satisfy complementarity condition if and only if x = (x y) + ; fl Computer Sciences Department, University of Wisconsin, 1210 West Dayton Street, Madison, WI 53706, email: chunhui@cs.wisc.edu, olvi@cs.wisc.edu.
Reference: [24] <author> Jorge J. </author> <title> More. Global methods for nonlinear complementarity problems. </title> <type> Technical Report Preprint MCS-P429-0494, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Illinois, </institution> <year> 1994. </year> <month> 36 </month>
Reference-contexts: In addition, an algorithm based on nonsmooth equations and successive quadratic programming was given [30], as well as a Newton method with a path following technique [32, 8], and a trust region Newton method for solving a nonlinear least squares reformulation of the NCP <ref> [24] </ref>. With the exception of [24], a feature common to all these methods is that the subproblem at each Newton iteration is still a combinatorial problem. <p> In addition, an algorithm based on nonsmooth equations and successive quadratic programming was given [30], as well as a Newton method with a path following technique [32, 8], and a trust region Newton method for solving a nonlinear least squares reformulation of the NCP <ref> [24] </ref>. With the exception of [24], a feature common to all these methods is that the subproblem at each Newton iteration is still a combinatorial problem. In contrast, by using the smooth technique proposed here, we avoid this combinatorial difficulty by approximately reformulating the nonlinear or mixed complementarity problem as a smooth nonlinear equation.
Reference: [25] <author> B.A. Murtagh and M.A. Saunders. </author> <title> MINOS 5.0 user's guide. </title> <type> Technical Report SOL 83.20, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1983. </year>
Reference-contexts: Our smooth algorithm was run using one set of default parameters and so was the PATH solver. The smooth algorithm is written in the C language and implemented by using the GAMS/CPLIB [10]. A MINOS routine <ref> [25] </ref> was used to perform a sparse LU decomposition for solving sparse linear equations. Both algorithms use the same convergence tolerance of * = 1:0e 6.
Reference: [26] <author> K.G. Murty. </author> <title> Linear Complementarity, Linear and Nonlinear Programming. </title> <address> Helderman-Verlag, Berlin, </address> <year> 1988. </year>
Reference-contexts: 1 Introduction The complementarity condition 0 x ? y 0; where x and y are vectors in R n and the symbol ? denotes orthogonality, plays a fundamental role in mathematical programming. Many problems can be formulated by using this complementarity condition. For example, most optimality conditions of mathematical programming <ref> [26] </ref> as well as variational inequalities [6] and extended complementarity problems [23, 11, 40] can be so formulated.
Reference: [27] <author> J.M. Ortega. </author> <title> Numerical Analysis, a Second Course. </title> <publisher> Academic Press, </publisher> <year> 1972. </year>
Reference-contexts: (x F (x)) + k p kx ^p (x F (x); fi) + ^p (x F (x); fi) (x F (x)) + k p kx ^p (x F (x); fi)k p + fl p maxfD 1 ; D 2 gfi: The above result is also true for any monotone norm <ref> [27] </ref>. We first consider the strongly monotone NCP, that is there exists a k &gt; 0 such that for any x; y 2 R n Since the NCP is strongly monotone, it has a unique solution [12]. <p> Since F 2 LC 1 K (R n ), for a compact set S whose interior contains fy k i g and y, we have that R (y) 2 LC 1 K 1 (S) for some K 1 . By Quadratic Bound Lemma <ref> [27, p.144] </ref>, we have kf (y k i + k i d k i ) f (y k i ) rf (y k i ) T k i d k i k 2 2 2 : Since rR (y) is nonsingular, on the compact S, there exists K (S) and k
Reference: [28] <author> J.-S. Pang. </author> <title> Newton's method for B-differentiable equations. </title> <journal> Mathematics of Operations Research, </journal> <volume> 15 </volume> <pages> 331-341, </pages> <year> 1990. </year>
Reference-contexts: In [12] a good summary and references up to 1988 are given. Generalizations of the Newton method to nonsmooth equations can be found in [34], [35] and [36]. Since then, several approaches based on B-differentiable equations were investigated in [13], <ref> [28] </ref> and [29]. In addition, an algorithm based on nonsmooth equations and successive quadratic programming was given [30], as well as a Newton method with a path following technique [32, 8], and a trust region Newton method for solving a nonlinear least squares reformulation of the NCP [24].
Reference: [29] <author> J.-S. Pang. </author> <title> A B-differentiable equation based, globally and locally quadratically convergent algorithm for nonlinear programs, complementarity and variational inequality problems. </title> <journal> Mathematical Programming, </journal> <volume> 51 </volume> <pages> 101-131, </pages> <year> 1991. </year>
Reference-contexts: In [12] a good summary and references up to 1988 are given. Generalizations of the Newton method to nonsmooth equations can be found in [34], [35] and [36]. Since then, several approaches based on B-differentiable equations were investigated in [13], [28] and <ref> [29] </ref>. In addition, an algorithm based on nonsmooth equations and successive quadratic programming was given [30], as well as a Newton method with a path following technique [32, 8], and a trust region Newton method for solving a nonlinear least squares reformulation of the NCP [24].
Reference: [30] <author> J.-S. Pang and S.A. Gabriel. NE/SQP: </author> <title> A robust algorithm for the nonlinear complementarity problem. </title> <journal> Mathematical Programming, </journal> <volume> 60 </volume> <pages> 295-337, </pages> <year> 1993. </year>
Reference-contexts: Generalizations of the Newton method to nonsmooth equations can be found in [34], [35] and [36]. Since then, several approaches based on B-differentiable equations were investigated in [13], [28] and [29]. In addition, an algorithm based on nonsmooth equations and successive quadratic programming was given <ref> [30] </ref>, as well as a Newton method with a path following technique [32, 8], and a trust region Newton method for solving a nonlinear least squares reformulation of the NCP [24]. <p> In Section 6, encouraging numerical testing results are given for 2 52 problems from the MCPLIB [9] which includes all the problems attempted in [13], <ref> [30] </ref> and [8]. These problems range in size of up to 8192 variables. These examples include the difficult von Thunen NCP model [30, 39] which is solved here to an accuracy of 1.0e-7. A few words about our notation. <p> In Section 6, encouraging numerical testing results are given for 2 52 problems from the MCPLIB [9] which includes all the problems attempted in [13], [30] and [8]. These problems range in size of up to 8192 variables. These examples include the difficult von Thunen NCP model <ref> [30, 39] </ref> which is solved here to an accuracy of 1.0e-7. A few words about our notation. <p> The details of implementing the smooth algorithm are given in Appendix 2. For comparison, we also give the results for the PATH solver [8]. Both algorithms were run on a DECstation 5000/125. Among the 52 test problems, which includes all the problems attempted in [13], <ref> [30] </ref> and [8], 51 problems are from the MCPLIB [9], and one is the generalized von Thunen model from [30] and [39]. Our smooth algorithm was run using one set of default parameters and so was the PATH solver. <p> Both algorithms were run on a DECstation 5000/125. Among the 52 test problems, which includes all the problems attempted in [13], <ref> [30] </ref> and [8], 51 problems are from the MCPLIB [9], and one is the generalized von Thunen model from [30] and [39]. Our smooth algorithm was run using one set of default parameters and so was the PATH solver. The smooth algorithm is written in the C language and implemented by using the GAMS/CPLIB [10]. <p> The results are given in Figures 13 to 16. It can be seen that with a Newton preprocessor, the solution times are very similar for PATH and SMOOTH for larger problems, whereas PATH is still better for the smaller problems. As mentioned in <ref> [30] </ref>, the generalized von Thunen model is an NCP with 106 variables. This is a very difficult problem that has challenged many of the recently proposed algorithms [30, 39]. <p> As mentioned in [30], the generalized von Thunen model is an NCP with 106 variables. This is a very difficult problem that has challenged many of the recently proposed algorithms <ref> [30, 39] </ref>. In order to guarantee that the function F (x) is well defined, we added a lower bound of 1.0e-7 to variables x 1 to x 26 as suggested by Jong-Shi Pang. We used three starting points.
Reference: [31] <author> M. C. Pinar and S. A. Zenios. </author> <title> On smoothing exact penalty functions for convex constrained optimization. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 4 </volume> <pages> 486-511, </pages> <year> 1994. </year>
Reference-contexts: Consequently, at each Newton step, we only need to solve a linear equation. This is much simpler than solving a mixed linear complementarity problem or a quadratic program. Smoothing techniques have already been applied to different problems, such as, l 1 minimization problems [21], multi-commodity flow problems <ref> [31] </ref>, nonsmooth programming [41, 20], linear and convex inequalities [5], and linear complementarity problems [4], [5] and [17]. These successful techniques motivate a systematic study of the smoothing approach. Questions we wish to address include the following. <p> Example 2.2 Chen-Harker-Kanzow-Smale Smooth Plus Function [38], [17] and [4] Let 2 3 Here D 1 = 1; D 2 = 0; suppfd (x)g = R and ^p (x; fi) = p 2 Example 2.3 Pinar-Zenios Smooth Plus Function <ref> [31] </ref> Let 0 otherwise Here D 1 = 0; D 2 = 1 2 ; suppfd (x)g = [0; 1] and ^p (x; fi) = &gt; &lt; 0 if x &lt; 0 2fi if 0 x fi 2 if x &gt; fi This function can also be obtained by applying the
Reference: [32] <author> D. Ralph. </author> <title> Global convergence of damped newton's method for nonsmooth equations via the path search. </title> <institution> Mathematics of Operations Research, </institution> <year> 1993. </year>
Reference-contexts: Since then, several approaches based on B-differentiable equations were investigated in [13], [28] and [29]. In addition, an algorithm based on nonsmooth equations and successive quadratic programming was given [30], as well as a Newton method with a path following technique <ref> [32, 8] </ref>, and a trust region Newton method for solving a nonlinear least squares reformulation of the NCP [24]. With the exception of [24], a feature common to all these methods is that the subproblem at each Newton iteration is still a combinatorial problem.
Reference: [33] <author> J. Ren. </author> <title> Computable error bounds in mathematical programming. </title> <type> Technical Report 1173, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin 53706, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: The following error bound for the strongly monotone NCP is given as Theorem 3.2.1 in <ref> [33] </ref>. Lemma 3.2 Let the NCP be strongly monotone and let F (x) be Lipschitz continuous. Then for any x 2 R n where x is the unique solution of the NCP and C p is a condition constant of F independent of x.
Reference: [34] <author> S.M. Robinson. </author> <title> Generalized equations and their solution: Part I: Basic thoery. </title> <journal> Mathematical Programming Study, </journal> <volume> 10 </volume> <pages> 128-140, </pages> <year> 1979. </year>
Reference-contexts: There are many Newton-based algorithms for solving nonlinear complementarity problems, variational inequalities and mixed complementarity problems. In [12] a good summary and references up to 1988 are given. Generalizations of the Newton method to nonsmooth equations can be found in <ref> [34] </ref>, [35] and [36]. Since then, several approaches based on B-differentiable equations were investigated in [13], [28] and [29].
Reference: [35] <author> S.M. Robinson. </author> <title> Strongly regular generalized equations. </title> <journal> Mathematics of Operations Research, </journal> <volume> 5 </volume> <pages> 43-62, </pages> <year> 1980. </year>
Reference-contexts: There are many Newton-based algorithms for solving nonlinear complementarity problems, variational inequalities and mixed complementarity problems. In [12] a good summary and references up to 1988 are given. Generalizations of the Newton method to nonsmooth equations can be found in [34], <ref> [35] </ref> and [36]. Since then, several approaches based on B-differentiable equations were investigated in [13], [28] and [29].
Reference: [36] <author> S.M. Robinson. </author> <title> Newton's method for a class of nonsmooth functions. </title> <journal> Set-Valued Analysis, </journal> <volume> 2 </volume> <pages> 291-305, </pages> <year> 1994. </year>
Reference-contexts: There are many Newton-based algorithms for solving nonlinear complementarity problems, variational inequalities and mixed complementarity problems. In [12] a good summary and references up to 1988 are given. Generalizations of the Newton method to nonsmooth equations can be found in [34], [35] and <ref> [36] </ref>. Since then, several approaches based on B-differentiable equations were investigated in [13], [28] and [29].
Reference: [37] <author> Thomas F. Rutherford. MILES: </author> <title> A mixed inequality and nonlinear equation solver. </title> <type> Working Paper, </type> <institution> Department of Economics, University of Colorado, Boulder, </institution> <year> 1993. </year>
Reference-contexts: Figures 10, 11 and 12 depict the CPU times for all remaining problems except the von Thunen model. We note that the PATH solver [8] is faster than Josephy's Newton method [16] and Rutherford's GAMS [10] mixed inequality and linear equation solver (MILES) <ref> [37] </ref> which is also Newton-based. Figures 9 to 12 indicate that our smooth algorithm is faster than PATH solver for the larger problems, whereas PATH solver is faster on smaller problems.
Reference: [38] <author> S. Smale. </author> <title> Algorithms for solving equations. </title> <booktitle> In Proceedings of the International Congress of Mathematicians, </booktitle> <pages> pages 172-195. </pages> <address> Ameri. </address> <publisher> Math. Soc., </publisher> <address> Providence, </address> <year> 1987. </year>
Reference-contexts: Figures 4 to 6 depict the functions p (x; 5); s (x; 5) and t (x; 5) respectively. Following are several other smooth plus functions based on probability density functions proposed by other authors. Example 2.2 Chen-Harker-Kanzow-Smale Smooth Plus Function <ref> [38] </ref>, [17] and [4] Let 2 3 Here D 1 = 1; D 2 = 0; suppfd (x)g = R and ^p (x; fi) = p 2 Example 2.3 Pinar-Zenios Smooth Plus Function [31] Let 0 otherwise Here D 1 = 0; D 2 = 1 2 ; suppfd (x)g = <p> Hence an exact solution of (10) is interior to the feasible region. However the iterates of the smooth method, which are only approximate solutions of (10), are not necessarily feasible. For the function ^p defined in Example 2.2 <ref> [38, 17, 4] </ref> , the exact solution x of the equation (10) satisfies x &gt; 0; F (x) &gt; 0; x i F i (x) = fi 2 ; i = 1; ; n which is precisely the central path of the interior point method for solving NCP. <p> Methods that trace this path but allow iterates to be exterior to the feasible region have been proposed in <ref> [38] </ref>, [4] and [17]. In [18], the relation between Smale's method [38] and the central path was pointed out. <p> Methods that trace this path but allow iterates to be exterior to the feasible region have been proposed in <ref> [38] </ref>, [4] and [17]. In [18], the relation between Smale's method [38] and the central path was pointed out.
Reference: [39] <author> B. Xiao. </author> <title> Global newton methods for nonlinear programs and variational inequalities. </title> <type> Technical report, Ph. D. thesis, </type> <institution> Department of Decision Sciences, The Wharton School, University of Pennsylvania, </institution> <address> Philadelphia, PA, </address> <year> 1990. </year>
Reference-contexts: In Section 6, encouraging numerical testing results are given for 2 52 problems from the MCPLIB [9] which includes all the problems attempted in [13], [30] and [8]. These problems range in size of up to 8192 variables. These examples include the difficult von Thunen NCP model <ref> [30, 39] </ref> which is solved here to an accuracy of 1.0e-7. A few words about our notation. <p> Both algorithms were run on a DECstation 5000/125. Among the 52 test problems, which includes all the problems attempted in [13], [30] and [8], 51 problems are from the MCPLIB [9], and one is the generalized von Thunen model from [30] and <ref> [39] </ref>. Our smooth algorithm was run using one set of default parameters and so was the PATH solver. The smooth algorithm is written in the C language and implemented by using the GAMS/CPLIB [10]. <p> As mentioned in [30], the generalized von Thunen model is an NCP with 106 variables. This is a very difficult problem that has challenged many of the recently proposed algorithms <ref> [30, 39] </ref>. In order to guarantee that the function F (x) is well defined, we added a lower bound of 1.0e-7 to variables x 1 to x 26 as suggested by Jong-Shi Pang. We used three starting points. <p> We used three starting points. In the first, we 27 28 29 Problem (bert oc.gms) 30 Problem (opt cont.gms) 31 set all variables to 1, as suggested by Michael C. Ferris; the second one is a starting point suggested in <ref> [39] </ref>, while the third is the point suggested in [39] and modified by Jong-Shi Pang. SMOOTH, with or without the Newton preprocessor, solved the problem from all the three starting points. Solution times did not change by adding the Newton preprocessor. We report times for SMOOTH with the preprocessor. <p> We used three starting points. In the first, we 27 28 29 Problem (bert oc.gms) 30 Problem (opt cont.gms) 31 set all variables to 1, as suggested by Michael C. Ferris; the second one is a starting point suggested in <ref> [39] </ref>, while the third is the point suggested in [39] and modified by Jong-Shi Pang. SMOOTH, with or without the Newton preprocessor, solved the problem from all the three starting points. Solution times did not change by adding the Newton preprocessor. We report times for SMOOTH with the preprocessor.
Reference: [40] <author> Y. Ye. </author> <title> A fully polynomial-time approximation algorithm for computing a stationary point of the general linear complementarity problem. </title> <journal> Mathematics of Operations Research, </journal> <volume> 18 </volume> <pages> 334-345, </pages> <year> 1993. </year>
Reference-contexts: Many problems can be formulated by using this complementarity condition. For example, most optimality conditions of mathematical programming [26] as well as variational inequalities [6] and extended complementarity problems <ref> [23, 11, 40] </ref> can be so formulated. It is obvious that the vectors x and y satisfy complementarity condition if and only if x = (x y) + ; fl Computer Sciences Department, University of Wisconsin, 1210 West Dayton Street, Madison, WI 53706, email: chunhui@cs.wisc.edu, olvi@cs.wisc.edu.
Reference: [41] <author> Israel Zang. </author> <title> A smoothing-out technique for min-max optimization. </title> <journal> Mathematical Programming, </journal> <volume> 19 </volume> <pages> 61-77, </pages> <year> 1980. </year> <month> 37 </month>
Reference-contexts: This is much simpler than solving a mixed linear complementarity problem or a quadratic program. Smoothing techniques have already been applied to different problems, such as, l 1 minimization problems [21], multi-commodity flow problems [31], nonsmooth programming <ref> [41, 20] </ref>, linear and convex inequalities [5], and linear complementarity problems [4], [5] and [17]. These successful techniques motivate a systematic study of the smoothing approach. Questions we wish to address include the following. <p> Moreau-Yosida regularization [15, p.13] to the plus function. 8 p (x; ff) ff log (1 + e ffx ) with ff = 5 s (x; ff) 1+e ffx with ff = 5 t (x; ff) (1+e ffx ) 2 with ff = 5 9 Example 2.4 Zang Smooth Plus Function <ref> [41] </ref> Let 2 x 1 0 otherwise Here D 1 = 1 8 ; D 2 = 0; suppfd (x)g = [ 1 2 ; 1 ^p (x; fi) = &gt; &lt; 0 if x &lt; fi 1 2 ) 2 if jxj fi x if x &gt; fi Note that
References-found: 41

