URL: http://www.cs.ucsd.edu/classes/wi98/cse291/tme.ps.gz
Refering-URL: http://www.cs.ucsd.edu/classes/wi98/cse291/
Root-URL: http://www.cs.ucsd.edu
Email: fswallace,calder,tullseng@cs.ucsd.edu  
Title: Threaded Multiple Path Execution  
Author: Steven Wallace Brad Calder Dean M. Tullsen 
Address: San Diego  
Affiliation: Department of Computer Science and Engineering University of California,  
Note: .........DRAFT........ PLEASE DO NOT DISTRIBUTE..........DRAFT......  
Abstract: This paper presents Threaded Multi-Path Execution (TME), which exploits existing hardware on a Simultaneous Multi-threading (SMT) processor to speculatively execute multiple paths of execution. When there are fewer threads in an SMT processor than hardware contexts, threaded multi-path execution uses spare contexts to fetch and execute code along the less likely path of hard-to-predict branches. This paper describes the hardware mechanisms needed to enable an SMT processor to efficiently spawn speculative threads for threaded multi-path execution. The Mapping Synchronization Bus is described, which enables the spawning of these multiple paths. Policies are examined for deciding which branches to fork, and for managing competition between primary and alternate path threads for critical resources. Our results show that TME increases the single program performance of an SMT with eight thread contexts by 14%-23% on average, depending on the misprediction penalty, for programs with a high misprediction rate. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. Calder and D. Grunwald. </author> <title> Fast and accurate instruction fetch and branch prediction. </title> <booktitle> In 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 211, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Conflict-free miss penalties are 6 cycles to the L2 cache, another 12 cycles to the L3 cache, and another 62 cycles to memory. Branch prediction is provided by a decoupled branch target buffer (BTB) and pattern history table (PHT) scheme <ref> [1] </ref>. We use a 256-entry, four-way set associative BTB. The 2K x 2-bit PHT is accessed by the XOR of the lower bits of the address and the global history register [5, 13]. Return destinations are predicted with a 12-entry return stack (per context).
Reference: [2] <author> B. Calder, D. Grunwald, and B. Zorn. </author> <title> Quantifying behavioral differences between C and C++ programs. </title> <journal> Journal of Programming Languages, </journal> <volume> 2(4), </volume> <year> 1994. </year>
Reference-contexts: 1 Introduction A primary impediment to high throughput in superscalar processors is the branch problem. Integer programs have on average 4 to 5 instructions between each branch instruction <ref> [2] </ref>. On today's deeply pipelined processors, the CPU typically has many unresolved branches in the machine at once. This compounds the branch problem, particularly early in the pipeline the fetch unit will only be fetching useful instructions if all unresolved branches were predicted correctly.
Reference: [3] <author> T.H. Heil and J.E. Smith. </author> <title> Selective dual path execution. </title> <institution> University of Wisconsin - Madison, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: As the results in this paper have shown, the SMT architecture allows for an efficient extension for threaded multi-path execution. Our work also draws from the two studies by Heil and Smith <ref> [3] </ref> and Tyson, Lick and Farrens [10] on restricted dual path execution. Identifying candidate branches for forking through different branch confidience prediction schemes was examined in great detail in both of these studies.
Reference: [4] <author> E. Jacobsen, E. Rotenberg, and J.E. Smith. </author> <title> Assigning confidence to conditional branch predictions. </title> <booktitle> In 29th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 142152. </pages> <publisher> IEEE, </publisher> <month> December </month> <year> 1996. </year>
Reference-contexts: This prevents forking from an alternate path, which simplifies the architecture. Branches along alternate paths are predicted in the normal fashion. Uht and Sindagi call this disjoint eager execution [11]. Second, to determine which primary-path branches to fork, we added branch confidence prediction (as described in <ref> [4] </ref>) to the SMT architecture. A 2048 entry table of n-bit counters, shared among all hardware contexts, is used to keep track of the predictability of a branch.
Reference: [5] <author> S. McFarling. </author> <title> Combining branch predictors. </title> <type> Technical Report TN-36, </type> <institution> DEC-WRL, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Branch prediction is provided by a decoupled branch target buffer (BTB) and pattern history table (PHT) scheme [1]. We use a 256-entry, four-way set associative BTB. The 2K x 2-bit PHT is accessed by the XOR of the lower bits of the address and the global history register <ref> [5, 13] </ref>. Return destinations are predicted with a 12-entry return stack (per context). The instruction fetch mechanism we assume is the ICOUNT.2.8 mechanism from [8]. It fetches up to eight instructions from up to two threads.
Reference: [6] <author> M.D. Smith, M. Johnson, and M.A. Horowitz. </author> <title> Limits on multiple instruction issue. </title> <booktitle> In Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 290302, </pages> <year> 1989. </year>
Reference-contexts: The fetch mechanism is the most logical place to control the allocation of resources in the processor for two reasons. First, the fetch bandwidth has often been identified as the bottleneck resource both in single-thread <ref> [6] </ref> and multiple-thread execution [8] processors. Second, the fetch unit is the gateway to the processor giving a thread priority access to the fetch unit gives it priority access to the entire machine. Tullsen et al. [8] showed that assigning fetch priorities for threads led to significant performance gains.
Reference: [7] <author> D.M. Tullsen. </author> <title> Simulation and modeling of a simultaneous multi-threading processor. </title> <booktitle> In 22nd Annual Computer Measurement Group Conference, </booktitle> <month> December </month> <year> 1996. </year>
Reference-contexts: Our simulator is derived from the betaSMT simulator <ref> [7] </ref>. It uses emulation-based, instruction-level simulation. The simulator executes unmodified Alpha object code and models the execution pipelines, memory hierarchy, TLBs, and the branch prediction logic of the processor described in Section 2. This simulator accurately models execution following a branch misprediction.
Reference: [8] <author> D.M. Tullsen, S.J. Eggers, J.S. Emer, H.M. Levy, J.L. Lo, and R.L. Stamm. </author> <title> Exploiting choice: Instruction fetch and issue on an implementable simultaneous multithreading processor. </title> <booktitle> In 23nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 191202, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: However, other branches will never fit into this category because they are data-dependent on relatively random data. For those branches, prediction is not a sufficient solution. This paper examines a new speculative execution technique called Threaded Multi-Path Execution (TME). TME uses resources already available on a simultaneous multithreading <ref> [8, 9] </ref> processor to achieve higher instruction level parallelism when there are only one or a few processes running. <p> Multithreading is effective when multiple threads share the system, but does nothing to improve single-thread performance; TME is used for this purpose. Tullsen, et al. <ref> [8] </ref>, showed that SMT can help relieve the branch problem by making the system more tolerant of branch mispredictions, but again, only in a multiple-thread scenario. <p> The ability to combine instructions from multiple threads in the same cycle allows simultaneous multithreading to both hide latencies and more fully utilize the issue width of a wide superscalar processor. The baseline simulataneous multithreading architecture we use is the SMT processor proposed by Tullsen et al. <ref> [8] </ref>. This processor has the ability to fetch up to eight instructions from the instruction cache each cycle. Those instructions, after decoding and register renaming, find their way to one of two 32-entry instruction queues. <p> The 2K x 2-bit PHT is accessed by the XOR of the lower bits of the address and the global history register [5, 13]. Return destinations are predicted with a 12-entry return stack (per context). The instruction fetch mechanism we assume is the ICOUNT.2.8 mechanism from <ref> [8] </ref>. It fetches up to eight instructions from up to two threads. As many instructions as possible are fetched from the first thread; the second thread is then allowed to use any remaining slots from the 8-instruction fetch bandwidth. <p> We assume a 9-stage instruction pipeline, which is based on the Alpha 21264 pipeline, but includes extra cycles for accessing a large register file, as in <ref> [8] </ref>. This results in a mis-prediction penalty of 7 cycles plus the number of cycles the branch instruction stalled in the processor pipeline. Instruction latencies are also based on the Alpha. <p> The fetch mechanism is the most logical place to control the allocation of resources in the processor for two reasons. First, the fetch bandwidth has often been identified as the bottleneck resource both in single-thread [6] and multiple-thread execution <ref> [8] </ref> processors. Second, the fetch unit is the gateway to the processor giving a thread priority access to the fetch unit gives it priority access to the entire machine. Tullsen et al. [8] showed that assigning fetch priorities for threads led to significant performance gains. <p> the fetch bandwidth has often been identified as the bottleneck resource both in single-thread [6] and multiple-thread execution <ref> [8] </ref> processors. Second, the fetch unit is the gateway to the processor giving a thread priority access to the fetch unit gives it priority access to the entire machine. Tullsen et al. [8] showed that assigning fetch priorities for threads led to significant performance gains. With TME, the variances between the usefulness of threads are even greater. In our baseline simultaneous multithreading processor, the ICOUNT [8] fetch scheme gives highest priority to threads with the fewest un-issued instructions in the machine. <p> Tullsen et al. <ref> [8] </ref> showed that assigning fetch priorities for threads led to significant performance gains. With TME, the variances between the usefulness of threads are even greater. In our baseline simultaneous multithreading processor, the ICOUNT [8] fetch scheme gives highest priority to threads with the fewest un-issued instructions in the machine. It uses a counter associated with each thread to determine which two threads get to fetch, and which of those have higher priority.
Reference: [9] <author> D.M. Tullsen, S.J. Eggers, and H.M. Levy. </author> <title> Simultaneous multithread-ing: Maximizing on-chip parallelism. </title> <booktitle> In 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 392403, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: However, other branches will never fit into this category because they are data-dependent on relatively random data. For those branches, prediction is not a sufficient solution. This paper examines a new speculative execution technique called Threaded Multi-Path Execution (TME). TME uses resources already available on a simultaneous multithreading <ref> [8, 9] </ref> processor to achieve higher instruction level parallelism when there are only one or a few processes running.
Reference: [10] <author> G. Tyson, K. Lick, and M. Farrens. </author> <title> Limited dual path execution. </title> <type> Technical Report CSE-TR 346-97, </type> <institution> University of Michigan, </institution> <year> 1997. </year>
Reference-contexts: As the results in this paper have shown, the SMT architecture allows for an efficient extension for threaded multi-path execution. Our work also draws from the two studies by Heil and Smith [3] and Tyson, Lick and Farrens <ref> [10] </ref> on restricted dual path execution. Identifying candidate branches for forking through different branch confidience prediction schemes was examined in great detail in both of these studies.
Reference: [11] <author> A. Uht and V. Sindagi. </author> <title> Disjoint eager execution: An optimal form of speculative execution. </title> <booktitle> In 28th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 313325. </pages> <publisher> IEEE, </publisher> <month> December </month> <year> 1995. </year>
Reference-contexts: First, alternate paths are forked only along the primary path. This prevents forking from an alternate path, which simplifies the architecture. Branches along alternate paths are predicted in the normal fashion. Uht and Sindagi call this disjoint eager execution <ref> [11] </ref>. Second, to determine which primary-path branches to fork, we added branch confidence prediction (as described in [4]) to the SMT architecture. A 2048 entry table of n-bit counters, shared among all hardware contexts, is used to keep track of the predictability of a branch. <p> Figure 9 shows that for the mix of programs we examined, and this repartitioning algorithm, that dynamic allocation did not perform better than the default static partitioning. 11 6 Related Work This research was motivated by the speculative execution technique called Disjoint Eager Execution (DEE) proposed by Uht et al. <ref> [11] </ref>. Instead of speculatively predicting a single-path, DEE in hardware contains an elaborate structure which allows the processor to speculatively execute down multiple paths in a program.
Reference: [12] <author> K.C. Yeager. </author> <title> The mips r10000 superscalar microprocessor. </title> <journal> IEEE Micro, </journal> <volume> 16(2), </volume> <month> April </month> <year> 1996. </year>
Reference-contexts: Instruction latencies are also based on the Alpha. The next section goes into more detail on the register renaming scheme, since it is the most effected by TME. 2.1 Register Mapping Architecture For register renaming, the SMT architecture uses a mapping scheme (derived from the MIPS R10000 <ref> [12] </ref>) extended for simultaneous multithreading as shown in Figure 1 (except for the mapping synchronization bus, which is added for TME and described in Section 3.2).
Reference: [13] <author> T.-Y. Yeh and Y. Patt. </author> <title> Alternative implementations of two-level adaptive branch prediction. </title> <booktitle> In 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 124134, </pages> <month> May </month> <year> 1992. </year> <month> 12 </month>
Reference-contexts: Branch prediction is provided by a decoupled branch target buffer (BTB) and pattern history table (PHT) scheme [1]. We use a 256-entry, four-way set associative BTB. The 2K x 2-bit PHT is accessed by the XOR of the lower bits of the address and the global history register <ref> [5, 13] </ref>. Return destinations are predicted with a 12-entry return stack (per context). The instruction fetch mechanism we assume is the ICOUNT.2.8 mechanism from [8]. It fetches up to eight instructions from up to two threads.
References-found: 13

