URL: http://www.eecs.umich.edu/techreports/cse/1997/CSE-TR-331-97.ps.gz
Refering-URL: http://www.eecs.umich.edu/home/techreports/cse97.html
Root-URL: http://www.eecs.umich.edu
Title: Communication Characterization of a Cray T3D  
Author: Stevan Vlaovic 
Date: February 13, 1997  
Abstract: The Cray T3D is a multiprocessor that has a high speed 3D torus interconnect, and special communications hardware. By utilizing this hardware properly, a program has the potential to minimize its communications overhead. This study investigates three different libraries that provide communication on the T3D: PVM, MPI, and SMA. Both PVM and MPI are portable, whereas SMA is native. The latency, bandwidth, and collective communication costs of the communication primitives implemented in these libraries are characterized and compared. These results are also briefly contrasted with the results of related studies on the IBM SP2 and the Convex SPP-1000. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Abandah. Ford group presentation, </author> <month> November </month> <year> 1995. </year>
Reference-contexts: Note that for pvm reduce the data is not packed. 6 Comparison to other Multiprocessors This section presents of a simple comparison between the Cray T3D, the IBM SP2, and the Convex SPP-1000. The results for the SP2 are from [2], while the results from the Convex are from <ref> [1] </ref>. The data for Point-to-point communication is listed in the table below. The data represents a single region curve fit for Point-to-point communication.
Reference: [2] <author> G. Abandah and E. Davidson. </author> <title> Modeling the communication performance of the ibm sp2. </title> <booktitle> In 10th International Parallel Processing Symposium, page ??, 1996. </booktitle>
Reference-contexts: They have some models that predict throughput; and for accesses other than contiguous (e.g stride n) chained transfers (supplying data directly to the network) seemed to perform better than buffer packing. This is simply because it avoids additional copying overhead. Our work is most like <ref> [2] </ref>. In this paper, Abandah develops communication models for the IBM SP2. Like his paper, the goals of this research effort are to explain the achieved performance of message-passing applications, develop efficient message-passing applications and compare the performance of different multicomputers and application code implementations. Similar to the work in [2], <p> <ref> [2] </ref>. In this paper, Abandah develops communication models for the IBM SP2. Like his paper, the goals of this research effort are to explain the achieved performance of message-passing applications, develop efficient message-passing applications and compare the performance of different multicomputers and application code implementations. Similar to the work in [2], our approach to developing communication performance models for a particular message passing library is to measure and model the performance of a set of common communication patterns. One difference between our work and [2], is that we look at three message-passing libraries (PVM, MPI, and SMA) and their characterization on <p> Similar to the work in <ref> [2] </ref>, our approach to developing communication performance models for a particular message passing library is to measure and model the performance of a set of common communication patterns. One difference between our work and [2], is that we look at three message-passing libraries (PVM, MPI, and SMA) and their characterization on the Cray T3D. 10 5 Message Passing The time required to send messages processors to in a multiprocessor system is dependent on the processing nodes, the interconnection network, the message-passing library used, the communication <p> Note that for pvm reduce the data is not packed. 6 Comparison to other Multiprocessors This section presents of a simple comparison between the Cray T3D, the IBM SP2, and the Convex SPP-1000. The results for the SP2 are from <ref> [2] </ref>, while the results from the Convex are from [1]. The data for Point-to-point communication is listed in the table below. The data represents a single region curve fit for Point-to-point communication.
Reference: [3] <author> R. Arpaci, D. Culler, A. Krishnamurthy, S. Steinberg, and K. Yelick. </author> <title> Empirical evaluation of the cray t3d: A compiler persective. </title> <booktitle> In International Conference on Computer Architecture, </booktitle> <pages> pages 298-307, </pages> <year> 1995. </year>
Reference-contexts: Since the remote writes were to be sent to many processors and it is more straightforward to measure the time on one processor, this method was not employed. The graph in figure 3 shows these differences in the techniques for small data sizes. 4 Related Work In Culler's work <ref> [3] </ref>, the Cray T3D's raw hardware performance is investigated. By carefully crafting mi-crobenchmarks, he is able to inspect each aspect of the communications hardware shell. <p> In Chien's experiments, when the standard deviation of the compute time is twice the mean, send overheads are 300% higher. This supports the notion of receiving the processor from the responsibility of processing each message. This work also corroborates the work done in <ref> [3] </ref>. Their results also suggest that when output contention is a problem (fan-in), a pull based message system would work better than a push based one. By doing "lazy" communication, the processor that needs the data can grab it as fast as it can. <p> This is consistent with the data reported in <ref> [3, 9] </ref>. shmem get has slightly worse latency at 4.64 s, and the effective bandwidth is consistently worse than shmem put, due to the additional packet required. MPI Send initial latency is 71.6 s, and a peak effective bandwidth of 107 MB/s.
Reference: [4] <author> K. Cameron, L. Clarke, and A. Smith. </author> <title> Cri/epcc mpi for cray t3d. </title> <type> Technical report, </type> <institution> The University of Edinburgh, </institution> <year> 1995. </year>
Reference-contexts: The MPI version that is available on the Cray T3D, was developed by Edinburgh Parallel Computing Centre at the University of Edinburgh. In <ref> [4] </ref>, the architecture of this implementation is discussed. In particular, the point-to-point and collective communications are discussed. Point-to-point communication is facilitated by a speculative transfer of envelopes (packets), based on a shared queue for each process. Messages and queue slots correspond to data cache lines to facilitate cache coherence maintenance. <p> In Standard mode, if the message can be accommodated within a protocol message, the T protocol is used; the system memory used is within the protocol message slot at the receiver <ref> [4] </ref>. Since messages and queue slots correspond to data cache lines to facilitate cache coherence [4], the amount of data that can be accommodated by the send envelope is 4 cache lines, or 128 bytes. <p> In Standard mode, if the message can be accommodated within a protocol message, the T protocol is used; the system memory used is within the protocol message slot at the receiver <ref> [4] </ref>. Since messages and queue slots correspond to data cache lines to facilitate cache coherence [4], the amount of data that can be accommodated by the send envelope is 4 cache lines, or 128 bytes. For messages up to 128 bytes essential comes for free (fixed latency) and marks our first region. <p> The MPI SEND call then returns, so it is the receiver's responsibility to fetch the message copy using get-based transfers. If there is no MPI buffer space for holding a copy of the message, the sender can advertise the actual message, but must block until the acknowledgment is received <ref> [4] </ref>. This corresponds to a Request Transfer Acknowledge (RTA) protocol. In this protocol, the receiver drives message data transfer, following a 16 10 1000 100000 1 10 100 1000 10000 100000 1000000 10000000 Latency (us) Transfer size (bytes) Many-to-one SMA MPI PVM 17 successful send/receive match. <p> The discontinuity associated with MPI at 256 bytes is associated with the T protocol described previously. The implementation of the MPI BCAST uses a different protocol method to coordinate data transfer; however, the actual data transfer is done using the same module as the point-to-point communication <ref> [4] </ref> Broadcast bandwidth and latency measures from the graph follow.
Reference: [5] <author> Message Passing Interface Forum. </author> <title> The MPI message passing interface standard. </title> <institution> University of Tennessee, Knoxville, </institution> <year> 1994. </year>
Reference-contexts: The goal of MPI was to develop a widely used standard for writing message passing programs. As such, the goals of the steering committee that defined MPI were as follows <ref> [5] </ref>: * design an application programming interface. * Allow efficient communication, i.e. avoid memory-to-memory copying, allow overlap of computation and communication, and o*oad to communication coprocessor, where available. * Allow for implementations that can be used in a heterogeneous environment. * Allow convenient C and Fortran bindings for the interface. *
Reference: [6] <author> G. Geist and V. Sunderam. </author> <title> The pvm system: Supercomputer level concurrent computation on a heterogeneous network of workstations. </title> <booktitle> In Proceedings of the Sixth Distributed Memory Computers Conference, </booktitle> <pages> pages 111-122, </pages> <year> 1992. </year>
Reference-contexts: One library supported by Cray is the Parallel Virtual Machine, or PVM. PVM was first developed as a software system to allow a network of heterogeneous Unix computers to be used as a single computer <ref> [6] </ref>. In version 3.0, the dependence on Unix sockets and TCP/IP software is relaxed. This allows multiprocessor systems to exploit the native calls in order to increase performance. Convex, DEC, KSR, and IBM have also decided to supply PVM 3.0 with their multiprocessors.
Reference: [7] <author> R. Hockney. </author> <title> Performance parameters and benchmarking of supercomputers. </title> <journal> Parallel Computing, </journal> <volume> 17(10) </volume> <pages> 1111-1130, </pages> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: The point-to-point communication time can often be expressed by a few simple parameters as T (n) = t 0 + n r 1 is the asymptotic transfer rate in MB/s, and t 0 is the asymptotic zero length message latency in s <ref> [7] </ref>. For small messages, t 0 is dominant, while for large messages the transfer time is dominant.
Reference: [8] <author> V. Karamcheti and A. Chien. </author> <title> A comparison of architectural support for messaging in the tmc cm-5 and cray t3d. </title> <booktitle> In International Conference on Computer Architecture, </booktitle> <pages> pages 298-307, </pages> <year> 1995. </year>
Reference-contexts: If a run time table of Annex entries is kept, performing the lookup in software would likely cause greater delay than simply updating the Annex register (23 cycles). So it is their conclusion that only one Annex register is required. This is corroborated by work in <ref> [8, 10] </ref>. The performance of cached reads, uncached reads, and remote writes was also presented. An uncached read costs roughly 610 ns, while a cached read required 765 ns to complete. A remote write needs roughly 880 ns. <p> From Culler's measurements, for reads of more than 16 KB, the BLT mechanism achieves better bandwidth than the other methods. Another inspection of the Cray T3D hardware shell is done in <ref> [8] </ref>. In this work, Chien compares the T3D's communication hardware to that of the Thinking Machines CM-5. Because the CM-5 is focused on supporting the data parallel programming model, it requires processor intervention to process incoming messages.
Reference: [9] <author> M. Pahud and T. Cornu. </author> <title> Measurement of the contention phenomena on the cray t3d architecture. </title> <note> 1995, submitted. </note>
Reference-contexts: Since the T3D has a separate communication processor, the function may return (and usually does) before the data actually arrives at the destination processor. This allows for many different ways of timing the shmem put routines. In <ref> [9] </ref>, four different measurement methods are mentioned. <p> This is consistent with the data reported in <ref> [3, 9] </ref>. shmem get has slightly worse latency at 4.64 s, and the effective bandwidth is consistently worse than shmem put, due to the additional packet required. MPI Send initial latency is 71.6 s, and a peak effective bandwidth of 107 MB/s.
Reference: [10] <author> T. Stricker and T. Gross. </author> <title> Optimizing memory system performance for communication in parallel computers. </title> <booktitle> In International Conference on Computer Architecture, </booktitle> <pages> pages 308-319, </pages> <year> 1995. </year>
Reference-contexts: If a run time table of Annex entries is kept, performing the lookup in software would likely cause greater delay than simply updating the Annex register (23 cycles). So it is their conclusion that only one Annex register is required. This is corroborated by work in <ref> [8, 10] </ref>. The performance of cached reads, uncached reads, and remote writes was also presented. An uncached read costs roughly 610 ns, while a cached read required 765 ns to complete. A remote write needs roughly 880 ns. <p> A simulation study is presented which explores the advantage of making this hardware available on chip. In Stricker and Gross's paper they stress that the memory system performance is the key element in overall MPP performance <ref> [10] </ref>. They use the Cray T3D and the Intel Paragon as examples with which to demonstrate some proposed memory reference pattern models. The two models presented are buffer packing and chained transfers.
Reference: [11] <author> T. Tabe. Ford group presentation, </author> <month> December </month> <year> 1995. </year> <month> 27 </month>
Reference-contexts: Both of these machines have separate communication engines, so such overlap is possible in theory at least. One concern, however, is that the overlap of communication with computation on the IBM SP2 has been measured at only around 4 percent for one large application <ref> [11] </ref>. While the Cray T3D has more sophisticated communication support circuitry, the question is whether it can be effectively used concurrently with the processor.
References-found: 11

