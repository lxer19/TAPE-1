URL: http://www.cs.berkeley.edu/~russell/papers/ijcai95-cnt.ps
Refering-URL: http://www.cs.berkeley.edu/~russell/full-log.html
Root-URL: 
Title: Rationality and Intelligence  
Author: Stuart Russell 
Address: Berkeley, CA 94720, USA  
Affiliation: Computer Science Division University of California  
Abstract: The long-term goal of our field is the creation and understanding of intelligence. Productive research in AI, both practical and theoretical, benefits from a notion of intelligence that is precise enough to allow the cumulative development of robust systems and general results. This paper outlines a gradual evolution in our formal conception of intelligence that brings it closer to our informal conception and simultaneously reduces the gap between theory and practice.
Abstract-found: 1
Intro-found: 1
Reference: [ Agre and Chapman, 1987 ] <author> Philip E. Agre and David Chapman. Pengi: </author> <title> an implementation of a theory of activity. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence (IJCAI-87), </booktitle> <pages> pages 268-272, </pages> <address> Milan, Italy, August 1987. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: for example) helps in three ways: first, it allows us to view such cognitive faculties as planning and reasoning as occurring in the service of finding the right thing to do; second, it encompasses rather than excludes the position that systems can do the right thing without such cognitive faculties <ref> [ Agre and Chapman, 1987; Brooks, 1989 ] </ref> ; third, it allows more freedom to consider various specifications, boundaries, and interconnections of subsystems.
Reference: [ Breese and Fehling, 1990 ] <author> J. S. Breese and M. R. Fehling. </author> <title> Control of problem-solving: Principles and architecture. </title> <editor> In R. D. Shachter, T. Levitt, L. Kanal, and J. Lemmer, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence 4. </booktitle> <address> Elsevier/North-Holland, Amsterdam, London, New York, </address> <year> 1990. </year>
Reference: [ Brooks, 1989 ] <author> R. A. Brooks. </author> <title> Engineering approach to building complete, </title> <booktitle> intelligent beings. Proceedings of the SPIEThe International Society for Optical Engineering, </booktitle> <volume> 1002 </volume> <pages> 618-625, </pages> <year> 1989. </year>
Reference-contexts: for example) helps in three ways: first, it allows us to view such cognitive faculties as planning and reasoning as occurring in the service of finding the right thing to do; second, it encompasses rather than excludes the position that systems can do the right thing without such cognitive faculties <ref> [ Agre and Chapman, 1987; Brooks, 1989 ] </ref> ; third, it allows more freedom to consider various specifications, boundaries, and interconnections of subsystems.
Reference: [ Carnap, 1950 ] <author> Rudolf Carnap. </author> <title> Logical Foundations of Probability. </title> <publisher> University of Chicago Press, </publisher> <address> Chicago, Illinois, </address> <year> 1950. </year>
Reference-contexts: This is especially true when other fields are simultaneously embracing the insights derived within AI. knowledge level. In the decision-theoretic view, Bayesian up-dating provides a model for rational learning, but this pushes the question back to the prior <ref> [ Carnap, 1950 ] </ref> . The question of rational priors remains unsettled. Another aspect of perfect rationality that is lacking is the development of a suitable body of techniques for the specification of utility functions.
Reference: [ Cherniak, 1986 ] <author> C. Cherniak. </author> <title> Minimal Rationality. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1986. </year>
Reference: [ Davis, 1980 ] <author> Randall Davis. </author> <title> Meta-rules: reasoning about control. </title> <journal> Artificial Intelligence, </journal> <volume> 15(3) </volume> <pages> 179-222, </pages> <month> December </month> <year> 1980. </year>
Reference-contexts: The metalevel is a second decision-making process whose application domain consists of the object-level computations themselves and the computational objects and states that they affect. Metareasoning has a long history in AI, going back at least to the early 1970s. TEIRESIAS <ref> [ Davis, 1980 ] </ref> established the idea that explicit, domain-specific metaknowledge was an important aspect of expert system creation.
Reference: [ Dean and Boddy, 1988 ] <author> Thomas Dean and Mark Boddy. </author> <title> An analysis of time-dependent planning. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence (AAAI-88), </booktitle> <pages> pages 49-54, </pages> <address> St. Paul, Minnesota, </address> <month> 21-26 August </month> <year> 1988. </year> <note> Morgan Kauf-mann. </note>
Reference-contexts: This method was implemented for two-player games, two-player games with chance nodes, and single-agent search. In each case, the same general metareasoning scheme resulted in efficiency improvements of roughly an order of magnitude over traditional, highly-engineered algorithms. Another general class of metareasoning problems arises with anytime <ref> [ Dean and Boddy, 1988 ] </ref> or flexible [ Horvitz, 1987 ] algorithms, which are algorithms designed to return results whose quality varies with the amount of time allocated to computation.
Reference: [ Dean and Kanazawa, 1989 ] <author> Thomas Dean and Keiji Kanazawa. </author> <title> A model for reasoning about persistence and causation. </title> <journal> Computational Intelligence, </journal> <volume> 5(3) </volume> <pages> 142-150, </pages> <year> 1989. </year>
Reference-contexts: Since most real-world applications are partially observable, nondeterministic, dynamic, and continuous, the lack of emphasis is somewhat surprising. There are, however, several new bricks under construction. For example, dynamic probabilistic networks <ref> [ Dean and Kanazawa, 1989 ] </ref> provide a mechanism to maintain beliefs about the current state of a dynamic, partially observable, nondeterministic environment, and to project forward the effects of actions.
Reference: [ Dean et al., 1995 ] <editor> Thomas L. Dean, John Aloimonos, and James F. Allen. </editor> <booktitle> Artificial Intelligence: Theory and Practice. </booktitle> <address> Ben-jamin/Cummings, Redwood City, California, </address> <year> 1995. </year>
Reference-contexts: The agent-based view of AI has moved quickly from workshops on situatedness and embeddedness to mainstream textbooks <ref> [ Russell and Norvig, 1995; Dean et al., 1995 ] </ref> and buzzwords in Newsweek. Rational agents, loosely speaking, are agents whose actions make sense from the point of view of the information possessed by the agent and its goals (or, the task for which it was designed).
Reference: [ Dennett, 1986 ] <author> Daniel C. Dennett. </author> <title> The moral first aid manual. Tanner lectures on human values, </title> <institution> University of Michigan, </institution> <year> 1986. </year>
Reference-contexts: The requirement that policies be feasible for limited agents was discussed extensively by Cherniak [ 1986 ] and Harman [ 1983 ] . A philosophical proposal generally consistent with the notion of bounded optimality can be found in the Moral First Aid Manual <ref> [ Dennett, 1986 ] </ref> . Dennett explicitly discusses the idea of reaching an optimum within the space of feasible decision procedures, using as an example the Ph.D. admissions procedure of a philosophy department.
Reference: [ Forbes et al., 1995 ] <author> Jeff Forbes, Tim Huang, Keiji Kanazawa, and Stuart Russell. </author> <title> The BATmobile: Towards a Bayesian automated taxi. </title> <note> Submitted to IJCAI-95, </note> <year> 1995. </year>
Reference-contexts: Finally, learning methods for static and dynamic probabilistic networks with hidden variables (i.e., for partially observable environments) may make it possible to acquire the necessary environment models [ Lauritzen, 1995; Russell et al., 1995 ] . The Bayesian Automated Taxi (a.k.a. BATmobile) project <ref> [ Forbes et al., 1995 ] </ref> is an attempt to combine all these new bricks to solve an interesting application problem, namely driving a car on a freeway.
Reference: [ Harman, 1983 ] <author> Gilbert H. Harman. </author> <title> Change in View: </title> <booktitle> Principles of Reasoning. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1983. </year>
Reference: [ Horvitz, 1987 ] <author> E. J. Horvitz. </author> <title> Problem-solving design: Reasoning about computational value, trade-offs, and resources. </title> <booktitle> In Proceedings of the Second Annual NASA Research Forum, </booktitle> <pages> pages 26-43, </pages> <address> Moffett Field, California, </address> <year> 1987. </year> <institution> NASA Ames Research Center. </institution>
Reference-contexts: In each case, the same general metareasoning scheme resulted in efficiency improvements of roughly an order of magnitude over traditional, highly-engineered algorithms. Another general class of metareasoning problems arises with anytime [ Dean and Boddy, 1988 ] or flexible <ref> [ Horvitz, 1987 ] </ref> algorithms, which are algorithms designed to return results whose quality varies with the amount of time allocated to computation.
Reference: [ Howard, 1966 ] <author> Ronald A. Howard. </author> <title> Information value theory. </title> <journal> IEEE Transactions on Systems Science and Cybernetics, </journal> <volume> SSC-2:22-26, </volume> <year> 1966. </year>
Reference-contexts: thinking finesses one major puzzle of AI: if what is required for AI is incredibly devious and superbly efficient algorithms far surpassing the best efforts of computer scientists, how did evolution (and how will machine learning) ever get there? Rational metareasoning has as a precursor the theory of information value <ref> [ Howard, 1966 ] </ref> the notion that one can calculate the decision-theoretic value of acquiring an additional piece of information by simulating the decision process that would be followed given each possible outcome of the information request, thereby estimating the expected improvement in decision quality averaged over those outcomes.
Reference: [ Kalman, 1960 ] <author> R. E. </author> <title> Kalman. A new approach to linear filtering and prediction problems. </title> <journal> Journal of Basic Engineering, </journal> <pages> pages 35-46, </pages> <month> March </month> <year> 1960. </year>
Reference-contexts: Also, the rapid improvement in the speed and accuracy of computer vision systems has made interfacing with continuous physical environments more practical. In particular, the application of Kalman filtering <ref> [ Kalman, 1960 ] </ref> , a widely used technique in control theory, allows robust and efficient tracking of moving objects. Reinforcement learning, together with inductive learning methods for continuous function representations such as neural networks, allow learning from delayed rewards in continuous, nondeterministic environments.
Reference: [ Kearns et al., 1992 ] <author> M. Kearns, R. Schapire, , and L. Sellie. </author> <title> Toward efficient agnostic learning. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory (COLT-92), </booktitle> <address> Pittsburgh, Pennsylvania, July 1992. </address> <publisher> ACM Press. </publisher>
Reference-contexts: It is to be expected that the topic of agnostic learning <ref> [ Kearns et al., 1992 ] </ref> , which analyses the convergence of inductive learning algorithms working in arbitrary environments within a fixed hypothesis language, will be an important adjunct to the theory of bounded optimal agent design.
Reference: [ Keeney and Raiffa, 1976 ] <author> Ralph L. Keeney and Howard Raiffa. </author> <title> Decisions with Multiple Objectives: Preferences and Value Tradeoffs. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: Another aspect of perfect rationality that is lacking is the development of a suitable body of techniques for the specification of utility functions. In economics, many results have been derived on the decomposition of overall utility into attributes that can be combined in various ways <ref> [ Keeney and Raiffa, 1976 ] </ref> , yet such methods have made few inroads into AI (but see [ Wellman, 1985 ] ).
Reference: [ Lauritzen, 1995 ] <author> S. L. Lauritzen. </author> <title> The EM algorithm for graphical association models with missing data. </title> <journal> Computational Statistics and Data Analysis, </journal> <volume> 19 </volume> <pages> 191-201, </pages> <year> 1995. </year>
Reference-contexts: Recently, Parr and Rus-sell [ 1995 ] , among others, have had some success in adapting reinforcement learning to partially observable environments. Finally, learning methods for static and dynamic probabilistic networks with hidden variables (i.e., for partially observable environments) may make it possible to acquire the necessary environment models <ref> [ Lauritzen, 1995; Russell et al., 1995 ] </ref> . The Bayesian Automated Taxi (a.k.a. BATmobile) project [ Forbes et al., 1995 ] is an attempt to combine all these new bricks to solve an interesting application problem, namely driving a car on a freeway.
Reference: [ Matheson, 1968 ] <author> J. E. Matheson. </author> <title> The economic value of analysis and computation. </title> <journal> IEEE Transactions on Systems Science and Cybernetics, </journal> <volume> SSC-4(3):325-332, </volume> <year> 1968. </year>
Reference: [ Megiddo and Wigderson, 1986 ] <author> N. Megiddo and A. Wigderson. </author> <title> On play by means of computing machines. </title> <editor> In Joseph Y. Halpern, editor, </editor> <booktitle> Theoretical Aspects of Reasoning about Knowledge: Proceedings of the 1986 Conference (TARK-86), </booktitle> <pages> pages 259-274, </pages> <address> Monterey, California, March 19-22 1986. </address> <publisher> IBM and AAAI, Mor-gan Kaufmann. </publisher>
Reference-contexts: This leads to different results because it limits the ability of each agent to do unlimited simulation of the other, who is also doing unlimited simulation of the first, and so on. Even the requirement of computability makes a significant difference <ref> [ Megiddo and Wigderson, 1986 ] </ref> . Bounds on the complexity of players have also become a topic of intense interest.
Reference: [ Newell, 1982 ] <author> Allen Newell. </author> <title> The knowledge level. </title> <journal> Artificial Intelligence, </journal> <volume> 18(1) </volume> <pages> 82-127, </pages> <year> 1982. </year>
Reference-contexts: The performance measure evaluates the state history to arrive at the value of the agent. The theoretical role of perfect rationality within AI is well-described by Newell's paper on the Knowledge Level <ref> [ Newell, 1982 ] </ref> . Knowledge-level analysis of AI systems relies on an assumption of perfect rationality. It can be used to establish an upper bound on the performance of any possible system, by establishing what a perfectly rational agent would do given the same knowledge.
Reference: [ Papadimitriou and Yannakakis, 1994 ] <author> C. H. Papadimitriou and M. Yannakakis. </author> <title> On complexity as bounded rationality. </title> <booktitle> In Symposium on Theory of Computation (STOC-94), </booktitle> <year> 1994. </year>
Reference: [ Parr and Russell, 1995 ] <author> Ronald Parr and Stuart Russell. </author> <title> Approximating optimal policies for partially observable stochastic domains. </title> <note> Submitted to IJCAI-95, </note> <year> 1995. </year>
Reference: [ Russell and Norvig, 1995 ] <author> Stuart J. Russell and Peter Norvig. </author> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice-Hall, </publisher> <address> Engle-wood Cliffs, New Jersey, </address> <year> 1995. </year>
Reference-contexts: The agent-based view of AI has moved quickly from workshops on situatedness and embeddedness to mainstream textbooks <ref> [ Russell and Norvig, 1995; Dean et al., 1995 ] </ref> and buzzwords in Newsweek. Rational agents, loosely speaking, are agents whose actions make sense from the point of view of the information possessed by the agent and its goals (or, the task for which it was designed). <p> It is possible to define some basic properties of task environments that, together with the complexity of the problem, lead to identifiable requirements on the corresponding rational agent designs <ref> [ Russell and Norvig, 1995, Ch. 2 ] </ref> . The principal properties are whether the environment is fully observable or partially observable, whether it is deterministic or stochastic, whether it is static (i.e., does not change except when the agent acts) or dynamic, and whether it is discrete or continuous.
Reference: [ Russell and Subramanian, 1995 ] <author> Stuart J. Russell and Devika Sub-ramanian. </author> <title> Provably bounded-optimal agents. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 3, </volume> <month> May </month> <year> 1995. </year>
Reference-contexts: real and desirable solutions, and also because it satisfies some sides into a smorgasbord of fieldsintelligence as chess playing, intelligence as vehicle control, intelligence as medical diagnosis. 2 In doing so I shall draw heavily on previous work with Eric We-fald [ Russell and Wefald, 1991a ] and Devika Subramanian <ref> [ Russell and Subramanian, 1995 ] </ref> . The latter paper contains a much more rigorous analysis of the concepts presented here. essential intuitions about the nature of intelligence. Some im-portant questions about intelligence can only be formulated and answered within the framework of bounded optimality or some relative thereof. <p> Specifying that actions or computations be rational is of no use if no real agents can fulfill the specification. The designer controls the program. In <ref> [ Russell and Subramanian, 1995 ] </ref> , the notion of feasibility for a given machine is introduced to describe the set of all agent functions that can be implemented by some agent program running on that machine. <p> The O () notation provided a much more robust, relatively machine-independent way to describe complexity and allowed results to develop cumulatively. In <ref> [ Russell and Subramanian, 1995 ] </ref> , the corresponding notion is asymptotic bounded optimality (ABO). As with classical complexity, we can define both average-case and worst-case ABO, where case here means the environment. <p> It is also important to apply those methods such as partial-order planning and abstraction that have been so effective in extending the reach of classical planners. 7.2 Directions for Bounded Optimality Ongoing research on bounded optimality aims to extend the initial results of <ref> [ Russell and Subramanian, 1995 ] </ref> to more interesting agent designs. The general idea is that the space of agent designs can be divided up into architectural classes such that in each class the structural variation is sufficiently limited.
Reference: [ Russell and Wefald, 1989 ] <author> Stuart J. Russell and Eric H. Wefald. </author> <title> On optimal game-tree search using rational meta-reasoning. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (IJCAI-89), </booktitle> <pages> pages 334-340, </pages> <address> Detroit, Michigan, August 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Russell and Wefald, 1991a ] <author> Stuart J. Russell and Eric H. Wefald. </author> <title> Do the Right Thing: Studies in Limited Rationality. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1991. </year>
Reference-contexts: P 3 because it is a real problem with real and desirable solutions, and also because it satisfies some sides into a smorgasbord of fieldsintelligence as chess playing, intelligence as vehicle control, intelligence as medical diagnosis. 2 In doing so I shall draw heavily on previous work with Eric We-fald <ref> [ Russell and Wefald, 1991a ] </ref> and Devika Subramanian [ Russell and Subramanian, 1995 ] . The latter paper contains a much more rigorous analysis of the concepts presented here. essential intuitions about the nature of intelligence.
Reference: [ Russell and Wefald, 1991b ] <author> Stuart J. Russell and Eric H. </author> <title> Wefald. </title> <booktitle> Principles of metareasoning. Artificial Intelligence, </booktitle> <address> 49(1-3):361-395, </address> <month> May </month> <year> 1991. </year>
Reference: [ Russell and Zilberstein, 1991 ] <author> S. Russell and S. Zilberstein. </author> <title> Composing real-time systems. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence (IJCAI-91), </booktitle> <address> Sydney, August 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In this way, we can construct a complex system that can handle arbitrary and unexpected real-time demands exactly as if it knew the exact time available in advance, with just a small ( 4) constant factor penalty in speed <ref> [ Russell and Zilberstein, 1991 ] </ref> . Second, one has to allocate the available computation optimally among the components to maximize the total output quality. <p> As an illustration of how ABO is a useful abstraction, one can show that under certain restrictions one can construct universal ABO programs which are ABO for any time variation in the utility function, using the iteration construction from <ref> [ Russell and Zilberstein, 1991 ] </ref> . Further directions for bounded optimality research are discussed below. 7 What Is To Be Done? This section describes some of the research activities that will, I hope, help to turn bounded optimality into a creative tool for AI system design.
Reference: [ Russell et al., 1995 ] <author> Stuart Russell, John Binder, Daphne Koller, and Keiji Kanazawa. </author> <title> Local learning in probabilistic networks with hidden variables. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <address> Montreal, Canada, August 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Recently, Parr and Rus-sell [ 1995 ] , among others, have had some success in adapting reinforcement learning to partially observable environments. Finally, learning methods for static and dynamic probabilistic networks with hidden variables (i.e., for partially observable environments) may make it possible to acquire the necessary environment models <ref> [ Lauritzen, 1995; Russell et al., 1995 ] </ref> . The Bayesian Automated Taxi (a.k.a. BATmobile) project [ Forbes et al., 1995 ] is an attempt to combine all these new bricks to solve an interesting application problem, namely driving a car on a freeway.
Reference: [ Simon, 1955 ] <author> Herbert A. Simon. </author> <title> A behavioral model of rational choice. </title> <journal> Quarterly Journal of Economics, </journal> <volume> 69 </volume> <pages> 99-118, </pages> <year> 1955. </year>
Reference-contexts: The simplest type of metareasoning trades off the expected increase in decision quality for a single algorithm, as measured by a performance profile, against the cost of time <ref> [ Simon, 1955 ] </ref> . A greedy termination condition is optimal if the second derivative of the performance profile is negative. More complex problems arise if one wishes to build complex real-time systems from anytime components.
Reference: [ Simon, 1958 ] <author> Herbert A. Simon. </author> <title> Rational choice and the structure of the environment. In Models of Bounded Rationality, volume 2. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1958. </year>
Reference: [ Sondik, 1971 ] <author> E. J. Sondik. </author> <title> The Optimal Control of Partially Observable Markov Decision Processes. </title> <type> PhD thesis, </type> <institution> Stanford University, Stanford, California, </institution> <year> 1971. </year>
Reference: [ Tash and Russell, 1994 ] <author> Jonathan K. Tash and Stuart J. Russell. </author> <title> Control strategies for a stochastic planner. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94), </booktitle> <address> Seattle, Washington, </address> <month> August </month> <year> 1994. </year> <note> AAAI Press. </note>
Reference: [ Wellman, 1985 ] <author> Michael P. Wellman. </author> <title> Reasoning about preference models. </title> <type> Technical Report MIT/LCS/TR-340, </type> <institution> Laboratory for Computer Science, MIT, Cambridge, Massachusetts, </institution> <year> 1985. </year>
Reference-contexts: In economics, many results have been derived on the decomposition of overall utility into attributes that can be combined in various ways [ Keeney and Raiffa, 1976 ] , yet such methods have made few inroads into AI (but see <ref> [ Wellman, 1985 ] </ref> ). We also have little idea how to specify utility over time, and although the question has been raised often, we do not have a satisfactory understanding of the relationship between goals and utility.
Reference: [ Zilberstein and Russell, 1995 ] <author> Shlomo Zilberstein and Stuart Rus-sell. </author> <title> Optimal composition of real-time systems. </title> <journal> Artificial Intelligence, </journal> <volume> 79(2), </volume> <year> 1995. </year>
Reference-contexts: Second, one has to allocate the available computation optimally among the components to maximize the total output quality. Although this is NP-hard for the general case, it can be solved in time linear in program size when the call graph of the components is tree-structured <ref> [ Zilberstein and Russell, 1995 ] </ref> . Thus, although these results are derived in the relatively clean context of anytime algorithms with well-defined performance profiles, there is reason to expect that the general problem of robust real-time decision-making in complex systems can be handled in practice.
References-found: 36

