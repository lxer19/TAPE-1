URL: http://www.eecs.harvard.edu/~vino/web/server.cache/icdcs.ps
Refering-URL: http://www.eecs.harvard.edu/~vino/web/
Root-URL: 
Title: An Analysis of Geographical Push-Caching  
Author: James Gwertzman, Microsoft Corporation Margo Seltzer, 
Affiliation: Harvard University  
Abstract: Most caching schemes in wide-area, distributed systems are client-initiated. Decisions of when and where to cache information are made without the benefit of the server's global knowledge of the usage patterns. In this paper, we present a new caching strategy: geographical push-caching. Using the server's global knowledge and a derived network topology, we distribute data to cooperating servers. The World Wide Web is an example of a wide-area system that will benefit from distance-sensitive caching, and we present an architecture that allows a Web server to autonomously replicate HTML pages. We use a trace-driven simulation to evaluate several competing caching strategies. Our results show that geographical push-caching reduces bandwidth consumption and sever load by the same amount as web proxy caching, but with a savings in global cache space of almost two orders of magnitude. More importantly, servers that wish to reduce Internet bandwidth consumption and their load can do so without waiting for web proxies to be implemented world-wide. Furthermore, geographical push-caching helps distribute server load for all web servers, not just the most popular as is the case with proxy caching.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Berners-Lee, R. Cailliau, J-F. Groff, and B. Pollermann. </author> <title> World-wide web: The information universe. </title> <journal> Electronic Networking Research, Applications and Policy, </journal> <volume> 2(1) </volume> <pages> 52-58, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction The World-Wide Web <ref> [1] </ref> operates as a cacheless distributed system. When two clients retrieve a document from the same server, the document is transmitted twice, regardless of the proximity of the two clients.
Reference: [2] <author> Azer Bestavros. </author> <title> Demand-based document dissemination to reduce traffic and balance load in distributed information systems. </title> <booktitle> In Proceedings of the 1995 Seventh IEEE Symposium on Parallel and Distributed Processing, </booktitle> <address> San Antonio. </address> <publisher> IEEE, </publisher> <month> October </month> <year> 1995. </year>
Reference-contexts: The two types of systems have very different underlying assumptions. Web objects are singly-mastered (files change in only one location) and web access patterns (browsing across many different files on many different servers) are very different than file system access patterns (limited browsing on a few different servers). Bestavros <ref> [2] </ref> et al are also examining server-side caching; the Demand-based Document Dissemination 2 allows servers to initiate replication of documents. On the client side, the Harvest system [4] includes an object caching subsystem that provides a hierarchically organized means for efficiently retrieving Internet objects such as FTP and HTML files. <p> While the average number of unique objects per site was fourteen, the median was only five objects per site, and the mode was one object per site. This implies that a few sites were browsed extensively, while most sites were only skimmed. Bestavros confirms these results <ref> [2] </ref>, adding that the more globally popular a server, the smaller the fraction of pages that account for most of its accesses.
Reference: [3] <author> Matthew A. </author> <title> Blaze. Caching in large-scale distributed file systems. </title> <type> Technical Report TR-397-92, </type> <institution> Prince ton University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: In addition to investigating caching, the Harvest group has examined the resource location problem [9] and the consistency problem [19]. Blaze <ref> [3] </ref> addressed caching in a large-scale system for his PhD thesis. His research focused on distributed file systems, but can also be applied to the Web. His research is similar to ours in that clients share cache space.
Reference: [4] <author> C. Mic Bowman, Peter B. Danzig, Darren R. Hardy, Udi Manber, and Mich ael F. Schwartz. Harvest: </author> <title> A scalable, customizable discovery and access system. </title> <type> Technical Report CU-CS-732-94, </type> <institution> University of Colorado, Boulder, </institution> <year> 1994. </year>
Reference-contexts: The problem with both of these solutions is that they are myopic. A client cache does nothing to reduce traffic to a neighboring computer, and a web proxy does not help neighboring proxies. One suggested solution to this problem, hierarchical caching, appears to increase latency without significantly improving performance <ref> [4] </ref> since if an object is not already cached in a leaf node it typically isn't cached in a higher level node either. <p> Bestavros [2] et al are also examining server-side caching; the Demand-based Document Dissemination 2 allows servers to initiate replication of documents. On the client side, the Harvest system <ref> [4] </ref> includes an object caching subsystem that provides a hierarchically organized means for efficiently retrieving Internet objects such as FTP and HTML files. In addition to investigating caching, the Harvest group has examined the resource location problem [9] and the consistency problem [19].
Reference: [5] <author> C. Mic Bowman, Peter B. Danzig, Udi Manber, and Michael E. Schwartz. </author> <title> Scalable Internet resource discovery: Research problems and approaches. </title> <journal> Communications of the ACM, </journal> <volume> 37(8) </volume> <pages> 98-107, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: Other groups are working on approaches to local resource discovery <ref> [5, 9] </ref>. For the purposes of this work, we had clients contact the original server on the first request to a page. The server then sent the client a redirect message instructing it to use a different cache if a closer replica existed.
Reference: [6] <author> Hans-Werner Braun and Kimberly Claffy. </author> <title> Web traffic characterization: an assessment of the im pact of caching documents from ncsa's web server. </title> <booktitle> In Electronic Proceedings of the Second World Wide Web Conference '94: Mosaic and the Web, </booktitle> <address> Chicago, IL, </address> <month> October </month> <year> 1994. </year> <note> Available from http://www.ncsa.uiuc.edu/SDG/IT94/Proceedings/DDay/claffy/main.html. </note>
Reference-contexts: Viles and French show that average Web server latency is too slow due to the nature of the Internet [18], and Braun and Claffy show that geographical caching will help reduce network traffic, asserting that a solution must be found that deals with server load on the Web <ref> [6] </ref>. They temper this statement by warning that any solution to load balancing must be sure to distribute the load so as to reduce the network bandwidth requirements. <p> We hypothesized that geographical information could be used to hint at which servers were network topologically close. We surveyed the Internet using the traceroute [12] program, to measure Internet topology, and a file maintained by Merit <ref> [17, 6] </ref>, listing the address of each subnetwork administrator, to determine geographic distance. The critical datum in the Merit file is the zip code listed in the address; in conjunction with a geography server [13], this provides enough information to establish the latitude and longitude of each network administrator.
Reference: [7] <author> Vincent Cate. </author> <title> Alex A global filesystem. </title> <booktitle> In USENIX File Systems Workshop Proceedings, </booktitle> <pages> pages 1-12, </pages> <address> Ann Arbor, MI, May 21 - 22 1992. </address> <publisher> USENIX. </publisher>
Reference-contexts: His research is similar to ours in that clients share cache space. The work differs from ours in that distance-sensitive server-initiated caching focuses on an optimal, topological-based selection of cache location. We use a form of cache consistency [11] similar to that used by the Alex system <ref> [7] </ref>. The consistency model is based on the observation that the older a file is the less likely it is to have changed, and therefore the less often the cache needs poll the server.
Reference: [8] <author> James D. Guyton and Michael F. Schwartz. </author> <title> Experiences with a survey tool for discovering network time protocol servers. </title> <booktitle> In 1994 Summer USENIX, </booktitle> <pages> pages 257-265. </pages> <publisher> USENIX, </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: Instead we used an Internet topology database gathered for a Network Time Protocol server survey <ref> [8] </ref>. Using this topology we were able to accurately measure the number of hops between two arbitrary hosts. We were unable, however, to distinguish backbone hops from other hops with this model or to calculate the latency between two hosts.
Reference: [9] <author> James D. Guyton and Michael F. Schwartz. </author> <title> Locating nearby copies of replicated internet servers. </title> <type> Technical Report CU-CS-762-95, </type> <institution> University of Colorado at Boulder, </institution> <year> 1995. </year>
Reference-contexts: On the client side, the Harvest system [4] includes an object caching subsystem that provides a hierarchically organized means for efficiently retrieving Internet objects such as FTP and HTML files. In addition to investigating caching, the Harvest group has examined the resource location problem <ref> [9] </ref> and the consistency problem [19]. Blaze [3] addressed caching in a large-scale system for his PhD thesis. His research focused on distributed file systems, but can also be applied to the Web. His research is similar to ours in that clients share cache space. <p> Other groups are working on approaches to local resource discovery <ref> [5, 9] </ref>. For the purposes of this work, we had clients contact the original server on the first request to a page. The server then sent the client a redirect message instructing it to use a different cache if a closer replica existed.
Reference: [10] <author> James Gwertzman. </author> <title> Autonomous replication in wide-area internetworks. </title> <type> Technical Report TR-17-95, </type> <institution> Harvard University, </institution> <year> 1995. </year> <month> 19 </month>
Reference-contexts: Fortunately, we did find a slight correlation between geographical distance and network distance. Nearby hosts showed the greatest correlation, but as the geographical distance exceeded 500 miles, the correlation decreased. Full details on this survey may be found in <ref> [10] </ref> We hypothesized that if we limited our analysis to hosts on the same backbone network, we would find stronger correlation between geographical distance and network distance.
Reference: [11] <author> James Gwertzman and Margo Seltzer. </author> <title> World-wide web cache consistency. </title> <booktitle> In 1996 USENIX Conference Proceedings. USENIX, </booktitle> <month> January </month> <year> 1996. </year>
Reference-contexts: His research is similar to ours in that clients share cache space. The work differs from ours in that distance-sensitive server-initiated caching focuses on an optimal, topological-based selection of cache location. We use a form of cache consistency <ref> [11] </ref> similar to that used by the Alex system [7]. The consistency model is based on the observation that the older a file is the less likely it is to have changed, and therefore the less often the cache needs poll the server.
Reference: [12] <author> V. </author> <type> Jacobsen. </type> <institution> Traceroute software. Lawrence Berkeley Laboratories, </institution> <month> December </month> <year> 1988. </year> <note> Available at ftp://ftp.ee.lbl.gov/pub/traceroute.tar.Z. </note>
Reference-contexts: We hypothesized that geographical information could be used to hint at which servers were network topologically close. We surveyed the Internet using the traceroute <ref> [12] </ref> program, to measure Internet topology, and a file maintained by Merit [17, 6], listing the address of each subnetwork administrator, to determine geographic distance.
Reference: [13] <author> Tom Libert. </author> <month> telnet://martini.eecs.umich.edu:3000/. </month>
Reference-contexts: The critical datum in the Merit file is the zip code listed in the address; in conjunction with a geography server <ref> [13] </ref>, this provides enough information to establish the latitude and longitude of each network administrator. It is then a simple calculation to compute the distance between two arbitrary hosts 6 on the Internet, accurate to within a zip-code and the size of the subnet.
Reference: [14] <author> Ari Luotonen and Kevin Altis. </author> <title> World-wide web proxies. </title> <booktitle> In Computer Networks and ISDN systems. First International Conference on the World-Wide Web, </booktitle> <publisher> Elsevier Science BV, </publisher> <year> 1994. </year> <note> available from 'http://www.cern.ch/ PapersWWW94/ luotonen.ps'. </note>
Reference-contexts: The vast majority of Web browsers have begun to address this problem by adding local client caches. These caches enable the client to avoid retrieving the same document twice. An increasing number of sites are also attempting to reduce server load and network bandwidth usage by deploying Web proxies <ref> [14, 16] </ref> that saves two clients in the same domain from having to retrieve the same document twice. The problem with both of these solutions is that they are myopic.
Reference: [15] <author> Inc. Lycos. Lycos, </author> <title> the catalog of the internet. </title> <address> http://www.lycos.com/. </address>
Reference: [16] <author> Mosaic-x@ncsa.uiuc.edu. </author> <title> Using proxy gateways. World-Wide Web. </title> <note> available from 'http://www.ncsa.uiuc.edu/ SDG/Software/ Mosaic/ Docs/ proxy-gateways.html'. [17] nic.merit.edu. ftp://nic.merit.edu/nsfnet/ announced.networks/nets.unl.now. </note>
Reference-contexts: The vast majority of Web browsers have begun to address this problem by adding local client caches. These caches enable the client to avoid retrieving the same document twice. An increasing number of sites are also attempting to reduce server load and network bandwidth usage by deploying Web proxies <ref> [14, 16] </ref> that saves two clients in the same domain from having to retrieve the same document twice. The problem with both of these solutions is that they are myopic.
Reference: [18] <author> Charles L. Viles and James C. </author> <title> French. Availability and latency of world wide web information servers. </title> <journal> Computing Systems, </journal> <volume> 8(1) </volume> <pages> 61-91, </pages> <year> 1995. </year>
Reference-contexts: Viles and French show that average Web server latency is too slow due to the nature of the Internet <ref> [18] </ref>, and Braun and Claffy show that geographical caching will help reduce network traffic, asserting that a solution must be found that deals with server load on the Web [6].
Reference: [19] <author> Kurt Jeffery Worrell. </author> <title> Invalidation in Large Scale Network Object Caches. </title> <type> PhD thesis, </type> <institution> University of Colorado-Boulder, </institution> <year> 1994. </year>
Reference-contexts: On the client side, the Harvest system [4] includes an object caching subsystem that provides a hierarchically organized means for efficiently retrieving Internet objects such as FTP and HTML files. In addition to investigating caching, the Harvest group has examined the resource location problem [9] and the consistency problem <ref> [19] </ref>. Blaze [3] addressed caching in a large-scale system for his PhD thesis. His research focused on distributed file systems, but can also be applied to the Web. His research is similar to ours in that clients share cache space.
Reference: [20] <author> Yahoo. Yahoo. </author> <note> http://www.yahoo.com/. 20 </note>
Reference-contexts: Server-initiated caching is especially applicable to applications such as video-on-demand that require large files to be efficiently and autonomously distributed around a wide-area network such that bandwidth and latency are minimized. Server-initiated caching can also help solve the replication problems plaguing information services such as Yahoo <ref> [20] </ref> or Lycos.[15] The rest of this paper is organized as follows. Section 2 presents related works. Section 3 presents the results of our trace analysis.
References-found: 19

