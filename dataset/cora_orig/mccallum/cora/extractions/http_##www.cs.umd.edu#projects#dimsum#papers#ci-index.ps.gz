URL: http://www.cs.umd.edu/projects/dimsum/papers/ci-index.ps.gz
Refering-URL: http://www.cs.umd.edu/projects/dimsum/papers/
Root-URL: 
Title: Cache Investment for Indexes  
Author: Gerhard Drasch Donald Kossmann fl Michael J. Franklin y 
Date: February 20, 1998  
Note: Draft  Submitted to the 1998 VLDB Conference.  
Address: D-94030 Passau College Park, MD 20742  
Affiliation: 1 Universitat Passau 2 University of Maryland FMI Department of Computer Science  
Abstract: Modern distributed information systems depend on caching to dynamically place data close to where it is needed. Caching can have a dramatic effect on the performance of distributed queries. Likewise, query execution plans dictate the shipment of data and hence, help determine what can be cached where. Because of this interdependency, performing query optimization without concern for caching can have disastrous consequences for both short-term and long-term performance. Cache investment extends the query optimizer of a distributed database system to account for such concerns. In this paper, we examine how cache investment can be used to make the right decisions about the caching of indexes. We develop two alternative cache investment policies for indexes and implement them in the SHORE database system. Using this implementation, we conduct a detailed study of the performance tradeoffs of investment for indexes. The experimental results show that cache investment for indexes can improve the performance of applications by as much as a factor of ten compared to traditional distributed query processing techniques such as query shipping or data shipping.
Abstract-found: 1
Intro-found: 1
Reference: [BEG96] <author> R. Buck-Emden and J. Galimow. </author> <title> SAP R/3 System, A Client/Server Technology. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, USA, </address> <year> 1996. </year>
Reference-contexts: SAP R/3 has a three-tier architecture in which the top tier carries out presentation services, the middle tier carries out the SAP R/3 application programs, and the back-end stores the database <ref> [BEG96, DHKK97] </ref>.
Reference: [CDF + 94] <author> M. Carey, D. DeWitt, M. Franklin, N. Hall, M. McAuliffe, J. Naughton, D. Schuh, M. Solomon, C. Tan, O. Tsatalos, S. White, and M. Zwilling. </author> <title> Shoring up persistent applications. </title> <booktitle> In Proc. of the ACM SIGMOD Conf. on Management of Data, </booktitle> <pages> pages 383-394, </pages> <address> Minneapolis, MI, USA, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: In this paper, we describe the cache investment framework and show how it can be extended to deal with indexes. We present two approaches to cache investment for indexes and study their performance by implementing them in the SHORE database system <ref> [CDF + 94] </ref>. Using a workload based on the benchmark used by Zaharioudakis and Carey to study alternative concurrency control protocols for indexes in client-server systems [ZC97], we show that our approach outperforms the best "traditional" approaches by as much as a factor of ten in some situations. <p> For space reasons, we do not discuss the tuning of this parameter any further in this paper. 8 system and since SHORE is based on callback locking, we briefly describe how cache investment considers updates in this particular environment <ref> [CDF + 94, Fra96] </ref>. 2 Before a page (or object) is modified in SHORE, the SHORE server that owns that page sends a callback message to all clients that have a copy of that page in their cache. <p> HOTCOLD UNIFORM Table 2: Workload Parameter Settings 4.2 Software and Hardware Used We used Version 1.0 of SHORE, enhanced by code for client-side index caching which was described in [ZC97]; this code is not part of the official SHORE release. 3 A detailed description of SHORE can be found in <ref> [CDF + 94] </ref> or on the SHORE project's web pages (http://www.cs.wisc.edu/shore). SHORE provides state-of-the art implementations for record management, B + -trees, concurrency control and recovery, client-side caching, etc.
Reference: [CKV93] <author> K. M. Curewitz, P. Krishnan, and J. S. Vitter. </author> <title> Practical prefetching via data compression. </title> <booktitle> In Proc. of the ACM SIGMOD Conf. on Management of Data, </booktitle> <pages> pages 43-53, </pages> <address> Washington, DC, USA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Of course, there already has been a large body of related work about various aspects of caching: e.g., view or semantic caching [RCK + 95, KB94, DFJ + 96], caching for data warehouses [SSV96], cost models for caching [SW96], prefetching <ref> [PZ91, CKV93] </ref>, etc. All previous work on caching, however, ignored the circular dependency between caching and query optimization. Using cache investment, we are the first to effectively break this circular dependency thereby allowing any kind of system to take the best advantage of caching.
Reference: [CN97] <author> S. Chaudhuri and V. Narasayya. </author> <title> An efficient, cost-driven index selection tool for microsoft SQL server. </title> <booktitle> In Proc. of the Conf. on Very Large Data Bases (VLDB), </booktitle> <pages> pages 146-155, </pages> <address> Athens, Greece, </address> <month> August </month> <year> 1997. </year>
Reference-contexts: Due to space considerations, however, we keep the exposition short; the details of the no-index case can be found in [FK97]. 3.1.1 The Profitable Policy for Tables The profitable policy is based on an approach that has been used for the physical design of relational database systems <ref> [SFT88, CN97] </ref>). The idea is to do a "what-if" analysis and compute v i;d (c) as the reduction in response time of query i if table d is cached at the client prior to the execution of query i.
Reference: [DFJ + 96] <author> S. Dar, M. Franklin, B. Jonsson, D. Srivastava, and M. Tan. </author> <title> Semantic data caching and replacement. </title> <booktitle> In VLDB [VLD96], </booktitle> <pages> pages 330-341. </pages>
Reference-contexts: Of course, there already has been a large body of related work about various aspects of caching: e.g., view or semantic caching <ref> [RCK + 95, KB94, DFJ + 96] </ref>, caching for data warehouses [SSV96], cost models for caching [SW96], prefetching [PZ91, CKV93], etc. All previous work on caching, however, ignored the circular dependency between caching and query optimization.
Reference: [DHKK97] <author> J. Doppelhammer, T. Hoppler, A. Kemper, and D. Kossmann. </author> <title> Database performance in the real world: TPC-D and SAP R/3. </title> <booktitle> In SIGMOD [SIG97], </booktitle> <pages> pages 123-134. </pages>
Reference-contexts: SAP R/3 has a three-tier architecture in which the top tier carries out presentation services, the middle tier carries out the SAP R/3 application programs, and the back-end stores the database <ref> [BEG96, DHKK97] </ref>.
Reference: [EH84] <author> W. Effelsberg and T. </author> <title> Harder. Principles of database buffer management. </title> <journal> ACM Trans. on Database Systems, </journal> <volume> 9(4) </volume> <pages> 560-595, </pages> <year> 1984. </year>
Reference-contexts: More specifically, V d (c) is computed from the v i;d (c)'s using a technique called periodic aging by division that was originally proposed for database buffer replacement <ref> [EH84] </ref>.
Reference: [FJK96] <author> M. Franklin, B. Jonsson, and D. Kossmann. </author> <title> Performance tradeoffs for client-server query processing. </title> <booktitle> In Proc. of the ACM SIGMOD Conf. on Management of Data, </booktitle> <pages> pages 149-160, </pages> <address> Montreal, Canada, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: Thus, a system that supports only one of these policies is likely to have sub-optimal performance 3 under certain workload and/or system conditions. In previous work, we have shown that for complex queries, a hybrid-shipping (HY) approach can often outperform both pure policies <ref> [FJK96] </ref>. Hybrid-shipping provides the flexibility to execute query operators on client and server machines, and allows the caching of data at clients.
Reference: [FK97] <author> M. Franklin and D. Kossmann. </author> <title> Cache investment strategies. </title> <type> Technical Report CS-TR-3803, </type> <institution> Uni versity of Maryland, College Park, MD 20742, </institution> <month> May </month> <year> 1997. </year> <note> Submitted for journal publication. http://www.db.fmi.uni-passau.de/ kossmann. </note>
Reference-contexts: We have proposed Cache Investment as a technique for integrating caching knowledge and decision making into a query optimization framework <ref> [FK97] </ref>. Cache investment is based on the realization that there is a circular dependency between query optimization and caching. <p> We have found that a setting of ff = 0:9 usually provides good performance for cache investment, so we use that value throughout this paper. A sensitivity analysis of the policies to the setting of ff can be found in <ref> [FK97] </ref>. In the following, we describe the profitable and reference counting policies in the no-index case and show how cache investment deals with updates in the no-index case. Due to space considerations, however, we keep the exposition short; the details of the no-index case can be found in [FK97]. 3.1.1 The <p> found in <ref> [FK97] </ref>. In the following, we describe the profitable and reference counting policies in the no-index case and show how cache investment deals with updates in the no-index case. Due to space considerations, however, we keep the exposition short; the details of the no-index case can be found in [FK97]. 3.1.1 The Profitable Policy for Tables The profitable policy is based on an approach that has been used for the physical design of relational database systems [SFT88, CN97]). <p> Since we implemented cache investment in the SHORE database 1 The sensitivity of T is analyzed in <ref> [FK97] </ref>. <p> For workloads with more complex queries, our previous work <ref> [FK97] </ref> shows that the overhead of the profitable policy 18 becomes significant (more than a second) for queries involving five or more tables: given the benefits that can be achieved, this overhead might, however, still be tolerable in many situations.
Reference: [Fra96] <author> M. Franklin. </author> <title> Client Data Caching: A Foundation for High Performance Object Database Systems. </title> <publisher> Kluwer Academic Press, </publisher> <year> 1996. </year>
Reference-contexts: For space reasons, we do not discuss the tuning of this parameter any further in this paper. 8 system and since SHORE is based on callback locking, we briefly describe how cache investment considers updates in this particular environment <ref> [CDF + 94, Fra96] </ref>. 2 Before a page (or object) is modified in SHORE, the SHORE server that owns that page sends a callback message to all clients that have a copy of that page in their cache.
Reference: [HF86] <author> R. Hagmann and D. Ferrari. </author> <title> Performance analysis of several back-end database architectures. </title> <journal> ACM Trans. on Database Systems, </journal> <volume> 11(1) </volume> <pages> 1-26, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: The current version of SHORE, however, lacks support for query processing so we had to integrate a query optimizer and basic relational query operators for joins, sorting, etc. In a client-server architecture, there is a question of where to optimize a query <ref> [HF86] </ref> and where to keep the statistics for cache investment: at the client or at the server. Since we wanted to change the SHORE server code as little as possible, we decided to run an instance of the optimizer and also to carry out cache investment at every client.
Reference: [HS76] <author> E. Horowitz and S. Sahni. </author> <title> Fundamentals of Data Structures. </title> <publisher> Computer Science Press, </publisher> <address> Rockville, MD, USA, </address> <year> 1976. </year>
Reference-contexts: Essentially, this is an (NP-hard) knapsack problem in which the goal is to maximize the sum of the values of all cached data items at every client; to find a good solution, the obvious heuristics of packing those data items with the highest V d (c)=size ratio can be used <ref> [HS76] </ref>. So, just as an investor on Wall Street decides which kinds of stock to keep in his/her portfolio, cache investment decides which data items to buy (i.e., bring into a client's cache), and the value of a data item corresponds to the expected return on investment of a share.
Reference: [KB94] <author> A. Keller and J. Basu. </author> <title> A predicate-based caching scheme for client-server database architectures. </title> <booktitle> In Proc. of the Intl. IEEE Conf. on Parallel and Distributed Information Systems, </booktitle> <pages> pages 229-238, </pages> <address> Austin, TX, USA, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: Of course, there already has been a large body of related work about various aspects of caching: e.g., view or semantic caching <ref> [RCK + 95, KB94, DFJ + 96] </ref>, caching for data warehouses [SSV96], cost models for caching [SW96], prefetching [PZ91, CKV93], etc. All previous work on caching, however, ignored the circular dependency between caching and query optimization.
Reference: [Loh88] <author> G. M. Lohman. </author> <title> Grammar-like functional rules for representing query optimization alternatives. </title> <booktitle> In Proc. of the ACM SIGMOD Conf. on Management of Data, </booktitle> <pages> pages 18-27, </pages> <address> Chicago, IL, USA, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: Making an optimizer cache-aware is quite simple and, in fact, distributed query optimizers such as the System R fl optimizer provide all the required functionality <ref> [Loh88] </ref>. Unfortunately, making an optimizer cache-aware is not enough. Initially, all client caches are empty. In this case, a cache-aware optimizer would likely place all scan operators at servers. Thus, client caches would remain empty, and caching would never be exploited.
Reference: [PZ91] <author> M. L. Palmer and S. B. Zdonik. </author> <title> FIDO: A cache that learns to fetch. </title> <booktitle> In Proc. of the Conf. on Very Large Data Bases (VLDB), </booktitle> <pages> pages 255-264, </pages> <address> Barcelona, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: Of course, there already has been a large body of related work about various aspects of caching: e.g., view or semantic caching [RCK + 95, KB94, DFJ + 96], caching for data warehouses [SSV96], cost models for caching [SW96], prefetching <ref> [PZ91, CKV93] </ref>, etc. All previous work on caching, however, ignored the circular dependency between caching and query optimization. Using cache investment, we are the first to effectively break this circular dependency thereby allowing any kind of system to take the best advantage of caching.
Reference: [RCK + 95] <author> N. Roussopoulos, C. Chen, S. Kelley, A. Delis, and Y. Papakonstantinou. </author> <title> The ADMS Project: Views r us. </title> <journal> IEEE Data Engeneering Bulletin, </journal> <volume> 18(2) </volume> <pages> 19-28, </pages> <month> June </month> <year> 1995. </year> <month> 21 </month>
Reference-contexts: Of course, there already has been a large body of related work about various aspects of caching: e.g., view or semantic caching <ref> [RCK + 95, KB94, DFJ + 96] </ref>, caching for data warehouses [SSV96], cost models for caching [SW96], prefetching [PZ91, CKV93], etc. All previous work on caching, however, ignored the circular dependency between caching and query optimization.
Reference: [SAL + 96] <author> M. Stonebraker, P. M. Aoki, W. Litwin, A. Pfeffer, A. Sah, J. Sidell, C. Staelin, and A. Yu. Mariposa: </author> <title> A wide-area distributed database system. </title> <journal> The VLDB Journal, </journal> <volume> 5(1) </volume> <pages> 48-63, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: Initiating caching as a by-product of executing a suboptimal query plan is likely to have lower overhead than establishing a new replica of data in a distributed database system like Mariposa <ref> [SAL + 96] </ref>. <p> Cache investment would also help "truly" distributed systems such as System R fl [WDH + 81] or Mariposa <ref> [SAL + 96] </ref>. These systems provide the flexibility to execute query operators at every site (like hybrid shipping), but usually provide no support for caching. Instead of caching, these systems support replication which is more static and coarser-grained.
Reference: [SFT88] <author> M. Schkolnick, S. Finkelstein, and P. Tiberio. </author> <title> Physical database design for relational databases. </title> <journal> ACM Trans. on Database Systems, </journal> <volume> 13(1) </volume> <pages> 91-128, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Due to space considerations, however, we keep the exposition short; the details of the no-index case can be found in [FK97]. 3.1.1 The Profitable Policy for Tables The profitable policy is based on an approach that has been used for the physical design of relational database systems <ref> [SFT88, CN97] </ref>). The idea is to do a "what-if" analysis and compute v i;d (c) as the reduction in response time of query i if table d is cached at the client prior to the execution of query i.
Reference: [Sha86] <author> L. D. Shapiro. </author> <title> Join processing in database systems with large main memories. </title> <journal> ACM Trans. on Database Systems, </journal> <volume> 11(9) </volume> <pages> 239-264, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: For example, consider a query that asks for all employees that work in a research department; i.e., Employee 1 (Dept). One way to process this query at a client would be to use an index join with, say, a main-memory hashtable on Dept (i.e., a classic hash join <ref> [Sha86] </ref>). If we build a hashtable for the whole Dept table rather than for the research Depts only, then we can keep this hashtable at the client for subsequent queries.
Reference: [SIG97] <editor> Proc. </editor> <booktitle> of the ACM SIGMOD Conf. on Management of Data, </booktitle> <address> Tucson, AZ, USA, </address> <month> May </month> <year> 1997. </year>
Reference: [SSV96] <author> P. Scheuermann, J. Shim, and R. Vingralek. Watchman: </author> <title> A data warehouse intelligent cache man ager. </title> <booktitle> In VLDB [VLD96], </booktitle> <pages> pages 51-62. </pages>
Reference-contexts: Of course, there already has been a large body of related work about various aspects of caching: e.g., view or semantic caching [RCK + 95, KB94, DFJ + 96], caching for data warehouses <ref> [SSV96] </ref>, cost models for caching [SW96], prefetching [PZ91, CKV93], etc. All previous work on caching, however, ignored the circular dependency between caching and query optimization.
Reference: [SW96] <author> M. Sinnwell and G. Weikum. </author> <title> A cost-model-based online method for distributed caching. </title> <booktitle> In Proc. IEEE Conf. on Data Engineering, </booktitle> <address> Birmingham, GB, </address> <year> 1996. </year>
Reference-contexts: Of course, there already has been a large body of related work about various aspects of caching: e.g., view or semantic caching [RCK + 95, KB94, DFJ + 96], caching for data warehouses [SSV96], cost models for caching <ref> [SW96] </ref>, prefetching [PZ91, CKV93], etc. All previous work on caching, however, ignored the circular dependency between caching and query optimization. Using cache investment, we are the first to effectively break this circular dependency thereby allowing any kind of system to take the best advantage of caching.
Reference: [VLD96] <editor> Proc. </editor> <booktitle> of the Conf. on Very Large Data Bases (VLDB), </booktitle> <address> Bombay, India, </address> <month> September </month> <year> 1996. </year>
Reference: [WDH + 81] <author> R. Williams, D. Daniels, L. Haas, G. Lapis, B. Lindsay, P. Ng, R. Obermarck, P. Selinger, A. Walker, P. Wilms, and R. Yost. </author> <title> R fl : An overview of the architecture. </title> <institution> IBM Research, </institution> <address> San Jose, CA, RJ3325, </address> <month> December </month> <year> 1981. </year> <note> Reprinted in: </note> <editor> M. Stonebraker (ed.), </editor> <booktitle> Readings in Database Systems, </booktitle> <publisher> Morgan Kaufmann publ, </publisher> <year> 1994, </year> <pages> pp. 515-536. </pages>
Reference-contexts: Other examples include all database systems that are based on pure query shipping (e.g., relational systems such as Oracle, DB2, etc.) or pure data shipping (e.g., OODBs such as O 2 , etc.). Cache investment would also help "truly" distributed systems such as System R fl <ref> [WDH + 81] </ref> or Mariposa [SAL + 96]. These systems provide the flexibility to execute query operators at every site (like hybrid shipping), but usually provide no support for caching. Instead of caching, these systems support replication which is more static and coarser-grained.
Reference: [Yao77] <author> S. B. Yao. </author> <title> Approximating block accesses in database organizations. </title> <journal> Communications of the ACM, </journal> <volume> 20(4) </volume> <pages> 260-261, </pages> <month> April </month> <year> 1977. </year>
Reference-contexts: If the B + -tree is an unclustered index, then the number of base table pages that need to be accessed can be estimated using Yao's formula <ref> [Yao77] </ref>. Now, let us consider a situation in which we have estimated that all past queries involving an index could have been executed with S i index pages and S t base table pages.
Reference: [ZC97] <author> M. Zaharioudakis and M. Carey. </author> <title> Highly concurrent cache consistency for indices in client-server database systems. </title> <booktitle> In SIGMOD [SIG97], </booktitle> <pages> pages 50-61. 22 </pages>
Reference-contexts: We present two approaches to cache investment for indexes and study their performance by implementing them in the SHORE database system [CDF + 94]. Using a workload based on the benchmark used by Zaharioudakis and Carey to study alternative concurrency control protocols for indexes in client-server systems <ref> [ZC97] </ref>, we show that our approach outperforms the best "traditional" approaches by as much as a factor of ten in some situations. We also extend our results to a particular kind of index that we call a private index. <p> SHORE uses callback locking (i.e., invalidation) for cached pages of base tables and for cached leaf pages of indexes <ref> [ZC97] </ref> so we can continue to reduce the value of an index proportionally in the presence of updates. <p> The workloads used in the experiments shown here were derived from the workloads used by Zaharioudakis and Carey in their study on indexes in client-server database systems <ref> [ZC97] </ref>. Our version of the benchmark is slightly simplified; the main difference is that we had to implement the update transactions differently in our prototype, but we did carry out the same kinds of operations in our experiments as Zaharioudakis and Carey did in theirs. <p> However, both kinds of update operations modify objects and indexes. We also studied a third workload that we call the POINTRANGE workload, which was not described in <ref> [ZC97] </ref>. This workload uses the HOTCOLD database and involves point and range queries; here, point queries read one object using an index and range queries read 100 objects using an index, as before. <p> 80% 50% 100% FreqPrivQueries 20% 50% | KindSharedQueries range point range KindPrivQueries range range range FreqUpdates 0%-80% 0%-80% 0%-80% Database HOTCOLD HOTCOLD UNIFORM Table 2: Workload Parameter Settings 4.2 Software and Hardware Used We used Version 1.0 of SHORE, enhanced by code for client-side index caching which was described in <ref> [ZC97] </ref>; this code is not part of the official SHORE release. 3 A detailed description of SHORE can be found in [CDF + 94] or on the SHORE project's web pages (http://www.cs.wisc.edu/shore). SHORE provides state-of-the art implementations for record management, B + -trees, concurrency control and recovery, client-side caching, etc. <p> The following four approaches are compared: 3 Specifically, we used the hybrid caching approach of <ref> [ZC97] </ref>. 15 1. hybrid shipping and cache investment with the profitable policy; 2. hybrid shipping and cache investment with the reference counting policy; 3. query shipping; as explained in Section 2.3, query shipping is, in these experiments, equivalent to a hybrid-shipping approach without cache investment; 4. data shipping, which carries out <p> Our data-shipping results are comparable to the best approach studied in <ref> [ZC97] </ref> (i.e., hybrid caching). We first show results obtained for the HOTCOLD, POINTRANGE, and UNIFORM workloads.
References-found: 26

