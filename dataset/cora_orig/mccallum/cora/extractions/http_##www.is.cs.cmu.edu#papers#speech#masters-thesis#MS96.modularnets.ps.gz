URL: http://www.is.cs.cmu.edu/papers/speech/masters-thesis/MS96.modularnets.ps.gz
Refering-URL: http://www.is.cs.cmu.edu/ISL.speech.publications.html
Root-URL: 
Email: Supervisors:  
Title: Modular Neural Networks for Speech Recognition  
Author: Jurgen Fritsch Prof. Dr. Alex Waibel Michael Finke 
Degree: Diploma thesis  
Address: Germany  7/31/96 Pittsburgh, PA  
Affiliation: Interactive Systems Laboratories Carnegie Mellon University USA University of Karlsruhe  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Bengio, Y., De Mori, R., Flammia, G. & Kompe, R. </author> <title> (1992) Global optimization of a neural network Hidden Markov Model Hybrid. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> Vol. 3, No. 2, </volume> <pages> 252-259, </pages> <year> 1992. </year>
Reference-contexts: IY IH EH AE AH AA AO UH UW ER We did not preprocess the data in any way, except that we normalized each of the four formant frequencies in the data set independently to the range <ref> [0; 1] </ref>. Fig. 4.3 shows the complete data set in the normalized (F1,F2) feature space. 52 CHAPTER 4. HIERARCHICAL MIXTURES OF EXPERTS Syrdal and Gopal [50] performed classification on this dataset using a quantitative perceptual model of human vowel recognition. <p> Fig. 4.5 shows the class boundaries imposed on a 2-dimensional feature space (F1,F2) by an HME (depth 3,branching factor 2) and an MLP (24 hidden units), respectively. HME and MLP were trained until convergence on the 2-dimensional feature. The plots in Fig. 4.5 were computed by sampling the interval <ref> [0; 1] </ref> 2 , coloring the class with highest output activation in different shades of gray. The MLP seems to prefer nonlinear curvy class boundaries, whereas the HME imposes almost linear ones. <p> HME FOR VOWEL CLASSIFICATION 55 Fig.4.6 shows the evolution of the activation regions of the experts while training the architecture. The plots are sampled in the same region <ref> [0; 1] </ref> 2 as before, coloring the expert with the highest cummulative gating probability in different shades of gray. Obviously, as the training proceeds, the HME shuts off 5 of its 8 experts completely. A combination of 3 experts seems to be enough to solve the given task. <p> Fig. 5.5 and Fig. 5.6 compare the regions of activation for each of the 8 experts in both architectures. Each plot was obtained by sampling the expert's activation (product of gating probabilities along the path from root to expert node) in the region <ref> [0; 1] </ref> 2 . White color indicates high activation, whereas black color indicates low activation. 5.3.2 Pruning Fig. 5.7 shows the effect of different pruning factors during training on the final classification performance.
Reference: [2] <author> Bourlard, H., Konig, Y., Morgan, N. </author> <title> (1994) REMAP: recursive estimation and maximization of a posteriori probabilities Application to transition-based connectionist speech recognition. </title> <type> Technical Report, </type> <institution> International Computer Science Institute, ICSI TR-94-064, </institution> <year> 1994. </year>
Reference-contexts: We can compute scaled likelihoods by dividing the network outputs by the prior state probabilities. It should be noted that in theory, HMM's could also be trained using local posterior probabilities as emission probabilities. In <ref> [2] </ref>, an iterative procedure based on the EM algorithm is used to compute local estimates of posterior class probabilities which can be used as 'soft' targets for neural networks. This approach aims at optimizing the global posterior probability for the sequence of word models, instead of maximizing the likelihood.
Reference: [3] <author> Bourlard, H., Morgan, N. </author> <title> (1994) Connectionist Speech Recognition A Hybrid Approach. </title> <publisher> Kluwer Academic Press, </publisher> <year> 1994. </year>
Reference-contexts: Furthermore, as we will see in the next chapter, our approach of factoring posteriors allows to make use of the same context clustering trees that are used in mixture-of-Gaussian based HMM systems. Yet another approach was adopted by Bourlard and Morgan at ICSI <ref> [3] </ref>. Their method factors the posterior phone-in-context probability in the same way as we presented it. However, their system uses only one MLP to estimate context posteriors instead of a set of context experts as proposed earlier in this thesis.
Reference: [4] <author> Bourlard, H., Morgan, N. </author> <title> (1993) Continuous Speech Recognition by Connectionist Statistical Methods. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> Vol. 4, No. 6, </volume> <month> Nov. </month> <year> 1993. </year>
Reference: [5] <author> Bourlard, H., Morgan, N. </author> <title> (1992) A context dependent neural network for continuous speech recognition. </title> <booktitle> IEEE Proc. Intl. Conf. on Acoustics, Speech and Signal Processing, </booktitle> <address> II:349-352, San Francisco, CA. </address>
Reference: [6] <author> Breiman, L., Friedman, J. H., Olshen, R. A. & Stone, C. J. </author> <title> (1984) Classification and Regression Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <address> Belmont, CA. </address>
Reference-contexts: There are existing similar tree-structured divide-and-conquer models in statistics, 38 CHAPTER 4. HIERARCHICAL MIXTURES OF EXPERTS namely CART by Breiman et. al. <ref> [6] </ref>, MARS of Friedman [15] and ID3 by Quinlan [44]. However, these algorithms solve function approximation or classification problems by explicitly dividing the input space into subregions, such that only one single 'expert' is contributing to the overall output of the model. <p> Because of the inherent tree structure of the HME, it is very appealing to derive a growing algorithm for this architecture. The machine learning literature offers a wide variety of growing algorithms for classification and decision trees [44], [45], <ref> [6] </ref>. Unfortunately, these algorithms require the evaluation of the gain of all possible node splits, using (mostly) entropy or likelihood based criterions, to eventually realize the best split and discard all the others. Waterhouse and Robinson [56] presented such an algorithm 57 58 CHAPTER 5.
Reference: [7] <author> Bridle, J. </author> <title> (1989) Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. In Neurocomputing: Algorithms, Architectures, and Applications, </title> <editor> F. Fogelman-Soulie and J. Herault, eds. </editor> <publisher> Springer Verlag, </publisher> <address> New York. </address>
Reference: [8] <author> Buntine, W. </author> <title> (1991) Learning classification trees. </title> <institution> NASA Ames Research Center Tech. </institution> <type> Rep. </type> <institution> FIA-90-12-19-01, Moffett Field, </institution> <address> CA. </address>
Reference: [9] <author> Cohen, M., Murveit, H., Bernstein, J., Price, P. & Weintraub, M. </author> <title> (1990) The DECIPHER speech recognition system. </title> <booktitle> in Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing, </booktitle> <pages> 77-80, </pages> <address> Albuquerque, NM. 105 106 BIBLIOGRAPHY </address>
Reference: [10] <author> Dempster, A. P., Laird, N. M. & Rubin D. B. </author> <title> (1977) Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> J. R. Statist. Soc. </journal> <volume> B 39, </volume> <pages> 1-38. </pages>
Reference-contexts: The diagram also shows all possible state transitions at one specific time point. The last problem, also called the training problem, can be solved by the Forward-Backward or Baum-Welch algorithm, which is essentially a version of the Expectation-Maximization (EM) <ref> [10] </ref> algorithm. <p> We will postpone the evaluation of the gradient ascent learning rule until after the next two sections, where we will derive a more efficient learning algorithm for the HME architecture. 4.3 EM Learning The Expectation Maximization (EM) algorithm of Dempster et. al. <ref> [10] </ref> is a general technique for maximum likelihood estimation. It is mainly applied to unsupervised learning, 44 CHAPTER 4. HIERARCHICAL MIXTURES OF EXPERTS i.e. clustering and mixture density estimation. <p> This corresponds to minimizing the distortion of a discrete vector quantized distribution where the codebook vectors are the means. (3) Maximum Likelihood Iteratively reestimate the mixture coefficients ff i , the means i and the covariance matrices i according to the EM algorithm for Gaussian mixtures <ref> [10] </ref>. The possibility to initalize the gate parameters to near optimal solutions and the single-loop EM re-estimation algorithm render the Gaussian parameterization a powerful extension to the standard HME architecture. 7.2.3 Combining Multiple Classifiers There is one other application of Gaussian gates, namely the task of combining multiple classifiers (CMC).
Reference: [11] <author> Duda, R. O. & Hart, P. E. </author> <title> (1973) Pattern Classification and Scene Analysis. </title> <publisher> John Wiley, </publisher> <address> New York. </address>
Reference: [12] <author> Finney, D. J. </author> <title> (1973) Statistical Methods in Biological Assay. Hafner, </title> <address> New York. </address>
Reference-contexts: This process is iteratively repeated until no further improvement can be obtained. This section describes the IRLS algorithm that can be used to solve the maximization problems within the M-step. The IRLS algorithm is a special case of the Fisher scoring method <ref> [12] </ref>. In order to maximize the log likelihood l (fi; X ) with respect to the parameter vector fi, the Fisher scoring method updates fi according to 4.3.
Reference: [13] <author> Franco, H., Cohen, M., Morgan, N., Rumelhart, D. & Abrash V. </author> <title> (1994) Context-dependent connectionist probability estimation in a hybrid Hidden Markov Model - Neural Net speech recognition system. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> Vol. 8, No 3, </volume> <pages> 211-222, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Every context specific network performs a simpler task than a context-independent network. This approach is adopted by SRI <ref> [13] </ref>.
Reference: [14] <author> Franzini, M., Lee, K. F., Waibel, A. </author> <title> (1990) Connectionist Viterbi Training: a new hybrid method for continuous speech recognition. </title> <booktitle> IEEE Proc. Intl. Conf. on Acoustics, Speech and Signal Processing, </booktitle> <pages> 425-428, </pages> <address> Albuquerque, NM, </address> <year> 1990. </year>
Reference: [15] <author> Friedman, J. H. </author> <title> (1991) Multivariate adaptive regression splines. </title> <journal> Ann. Statistics 19, </journal> <pages> 1-141. </pages>
Reference-contexts: There are existing similar tree-structured divide-and-conquer models in statistics, 38 CHAPTER 4. HIERARCHICAL MIXTURES OF EXPERTS namely CART by Breiman et. al. [6], MARS of Friedman <ref> [15] </ref> and ID3 by Quinlan [44]. However, these algorithms solve function approximation or classification problems by explicitly dividing the input space into subregions, such that only one single 'expert' is contributing to the overall output of the model.
Reference: [16] <author> Fritsch, J. & Rogina, I. </author> <title> (1996) The Bucket Box Intersection (BBI) Algorithm for Fast Approximative Evaluation of Diagonal Mixture Gaussians IEEE Intl. </title> <booktitle> Conf. on Acoustics, Speeech and Signal Processing, </booktitle> <address> May 1996, Atlanta, GA. </address>
Reference-contexts: However, we will show, that the combination of this algorithm with the initalization technique presented above yield very fast convergence in practice. 7.3.3 BBI Trees for Pruning In <ref> [16] </ref>, we presented a binary tree based space partioning algorithm which is very effective in speeding up the evaluation of Gaussian mixtures with diagonal covariance matrices.
Reference: [17] <author> Gish, H. </author> <title> (1990) A probabilistic approach to the understanding and training of neural network classifiers. </title> <booktitle> Proc. IEEE Intl. Conf. on Acoustics, Speech and Signal Processing 1361-1364, </booktitle> <address> Albuquerque, NM. </address>
Reference: [18] <author> Hampshire II, J. B., Waibel A. H. </author> <title> (1989) The Meta-Pi Network: Building Distributed Knowledge Representations for Robust Pattern Recognition Tech. </title> <type> Rep. </type> <institution> CMU-CS-89-166, Carnegie Mellon University, </institution> <address> Pittsburgh PA, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: This renders the learning process computationally very expensive and takes orders of magnitude longer than training traditional density estimators for speech recognition. Recently, modular and hierarchically organized neural networks have been studied extensively in the neural network and machine learning community (e.g. Meta-Pi networks <ref> [18] </ref>, Hierarchical Mixtures of Experts [26],[27]). In these networks, the overall recognition task is divided among several small sub-networks, so called experts. The experts decisions are integrated in a hierarchical way, yielding the overall network output. <p> After training, the two gender dependent HME's were combined to a new HME, introducing an additional top-level gate. The whole architecture was then retrained for one additional iteration. This form of initalizing an HME resembles the Meta-Pi paradigm, as introduced in <ref> [18] </ref>.
Reference: [19] <author> Haykin, S. </author> <title> (1991) Adaptive Filter Theory. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: [20] <author> Haykin, S. </author> <title> (1994) Neural networks: a comprehensive foundation. </title> <publisher> Macmillan, </publisher> <address> New York. </address>
Reference: [21] <author> Hochberg, M. M., Cook, G. D., Renals, S. J., Robinson, A. J. & Schechtman, R. S. </author> <title> (1995) The 1994 ABBOT Hybrid Connectionist-HMM Large-Vocabulary Recognition System. </title> <booktitle> In Spoken Language Systems Technology Workshop, </booktitle> <pages> 170-176, ARPA, </pages> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: To reduce training times even further, the network was initialized by training on a hand-labeled phonetic database (TIMIT) before training it on the larger target task. 3.4.2 A RNN based Hybrid The group at Cambridge University Engineering Department (CUED) has developed a hybrid connectionist/HMM speech recognition system called ABBOT <ref> [21] </ref>, which uses recurrent neural networks to compute emission probabilities. The network is depicted in Fig. 3.6. It uses a set of state units that have recurrent connections from their outputs back to their inputs (these units also have connections to the input nodes).
Reference: [22] <author> Jacobs, R. A., Jordan, M. I., Nowlan, S. J. & Hinton, G. E. </author> <title> (1991) Adaptive mixtures of local experts. </title> <booktitle> Neural Comp. </booktitle> <pages> 3 , 79-87. BIBLIOGRAPHY 107 </pages>
Reference: [23] <author> Jelinek, F. </author> <title> (1976) Continuous speech recognition by statistical methods. </title> <journal> Proc. of IEEE, </journal> <volume> Vol. 64, No. 4, </volume> <pages> 532-555. </pages>
Reference: [24] <author> Jelinek, F. </author> <title> (1990) Self-organized modelling for speech recognition. In Readings in Speech Recognition, </title> <editor> A. Waibel and K. F. Lee (eds), </editor> <address> 450-503, </address> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference: [25] <author> Jiang, L., Barnard, E. </author> <title> (1994) Choosing contexts for neural networks. </title> <institution> Oregon Graduate Institute Technial Report, </institution> <year> 1994. </year>
Reference: [26] <author> Jordan, M. I., Jacobs, R. A. </author> <title> (1992) Hierarchies of adaptive experts. </title> <booktitle> In Advances in Neural Information Processing Systems 4, </booktitle> <editor> J. Moody, S. Hanson & R. Lippmann, </editor> <booktitle> eds., </booktitle> <pages> pp. 985-993. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: GRADIENT ASCENT LEARNING 41 and treat the learning problem as a maximum likelihood problem. This kind of learning algorithm for HMEs was introduced by Jordan and Jacobs <ref> [26] </ref>. <p> One can prove easily that the posterior probabilities h i , h jji and h ij can be used as the expected values for the unknown indicator variables z i , z jji and z ij , respectively (see <ref> [26] </ref> for a proof). Using this fact, we can define the Q-function for the E-step of the EM algorithm: Q (; (k) ) = t i j (t) ij flog g (t) (t) 46 CHAPTER 4.
Reference: [27] <author> Jordan, M. I., Jacobs, R. A. </author> <title> (1994) Hierarchical Mixtures of Experts and the EM algorithm., </title> <booktitle> Neural Computation 6, </booktitle> <pages> 181-214, </pages> <publisher> MIT Press. </publisher>
Reference-contexts: HYBRID SPEECH RECOGNITION Chapter 4 Hierarchical Mixtures of Experts This chapter introduces Hierarchical Mixtures of Experts as a modular and hierarchical neural network for supervised learning. It closely follows the presentation by Jordan and Jacobs <ref> [27] </ref>, yet focussing on classification instead of regression.
Reference: [28] <author> Jordan, M. I., Xu, L. </author> <title> (1993) Convergence Properties of the EM Approach to Learning in Mixture-of-Experts Architectures. </title> <journal> Computational Cognitive Science Tech. Rep. </journal> <volume> 9301, </volume> <publisher> MIT, </publisher> <address> Cambridge, MA. </address>
Reference: [29] <author> Jordan, M. I., Jacobs, R. A. </author> <title> (1996) Modular and hierarchical learning systems. </title> <editor> In press: M. Arbib, ed., </editor> <booktitle> The Handbook of Brain Theory and Neural Networks. </booktitle> <address> Cam-bridge, MA. </address>
Reference: [30] <author> Kershaw, D. J., Hochberg, M. M. & Robinson, A. J. </author> <title> (1995) Context-Dependent Classes in a Hybrid Recurrent Network-HMM Speech Recognition System. </title> <type> Tech. Rep. </type> <institution> CUED/F-INFENG/TR217, Cambridge University Engineering Department, </institution> <address> Cam-bridge, England. </address>
Reference-contexts: that the context-dependent hybrid connectionist system can easily be switched back to context-independent (CI) mode by turning off the context expert networks, a feature not available in mixture-of-Gaussians based systems. 6.2.3 Related Work The modeling of context dependent likelihoods as presented in this thesis most closely resembles the work in <ref> [30] </ref> and [31], with the noteable difference, that we have generalized context-dependent posteriors to multi-state HMM models. 70 CHAPTER 6. CONTEXT MODELING There are other ways of factoring a conditional posterior probability. <p> 8.6 CD Smoothing In our context-dependent hybrid HMM system, we estimate scaled acoustic model likelihoods the following way: ^p (xjc j ; ! i ; s k ) = P (c j j! i ; s k ) P (! i js k ) P (s k ) As in <ref> [30] </ref>, we introduce a smoothing factor for the context dependent posteriors in order to compensate different dynamic ranges of context-independent and context dependent posteriors.
Reference: [31] <author> Kershaw, D. J., Robinson, T., Hochberg, M. </author> <title> (1995) Context-Dependent Classes in a Hybrid Recurrent Network-HMM Speech Recognition System. </title> <booktitle> In Advances in Neural Information Processing Systems 8. </booktitle>
Reference-contexts: context-dependent hybrid connectionist system can easily be switched back to context-independent (CI) mode by turning off the context expert networks, a feature not available in mixture-of-Gaussians based systems. 6.2.3 Related Work The modeling of context dependent likelihoods as presented in this thesis most closely resembles the work in [30] and <ref> [31] </ref>, with the noteable difference, that we have generalized context-dependent posteriors to multi-state HMM models. 70 CHAPTER 6. CONTEXT MODELING There are other ways of factoring a conditional posterior probability.
Reference: [32] <author> Lee, K. F. </author> <title> (1988) Large vocabulary, speaker-independent continuous speech recognition: The SPHINX system. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: Other forms of parameter sharing include state clustering and/or decision tree clustering. Another issue is the modeling of context-dependency on the HMM level. It was shown (see for example <ref> [32] </ref>) that the explicit modeling of phonemes in different contexts by different HMMs yields a vast improvement over context-independent systems. Current systems model biphone, triphone or even polyphone contexts to account for the variability of speech sounds in different contexts. <p> Usually, phonetic contexts are hierarchically clustered according to a distance measure between two parametric distributions. The most popular example are generalized triphones <ref> [32] </ref>. Systems that use this kind of modeling cluster the set of all possible/observed monophone triples ( 125000) into a set of about 5000 10000 models. This approach, however, considers only the left and right neighbors of a monophone.
Reference: [33] <author> Lippman, R. P. </author> <title> (1989) Review of neural networks for speech recognition. </title> <journal> Neural Computation, </journal> <volume> Vol. 1, No. 1, </volume> <pages> 1-38, </pages> <year> 1989. </year>
Reference: [34] <author> McCullagh, P., Nelder, J. A. </author> <title> (1983) Generalized Linear Models. </title> <publisher> Chapman and Hall, London. </publisher>
Reference-contexts: The vector mu ij is considered to be the mean of a probability density that models the generation of output vectors. The gating networks parameterization corresponds to a multinomial logit probability model, which is a special case of a Generalized Linear Model (GLIM) <ref> [34] </ref>. <p> Since all of the above maximization problems are based on likelihoods for generalized linear models, we can apply an algorithm called Iteratively Reweighted Least Squares (IRLS) <ref> [34] </ref> that solves such likelihood problems. 4.3.3 Iteratively Reweighted Least Squares (IRLS) Applying the EM algorithm to the HME architecture requires the computation of posterior probabilities h i , h jji and h ij for each input vector x in the E-step, and the maximization of independent maximum likelihood problems for
Reference: [35] <author> Moody, J. </author> <title> (1989) Fast learning in multi-resolution hierarchies. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <editor> D.S. Touretzky, ed. </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. 108 BIBLIOGRAPHY </address>
Reference: [36] <author> Moody, J., Darken, C. J. </author> <title> (1989) Fast learning in networks of locally tuned processing units. </title> <booktitle> Neural computation 1, </booktitle> <pages> 281-294. </pages>
Reference: [37] <author> Morgan, N., Bourlard, H. </author> <title> (1992) Factoring Networks by a Statistical Method. </title> <journal> Neural Computation, </journal> <volume> Vol 4, No. 6, </volume> <pages> 835-838, </pages> <year> 1992. </year>
Reference: [38] <author> Morgan, N., Bourlard, H. </author> <title> (1995) An Introduction to Hybrid HMM/Connectionist Continuous Speech Recognition. </title> <journal> Signal Processing Magazine, </journal> <pages> 25-42, </pages> <month> May </month> <year> 1995. </year>
Reference: [39] <author> Morgan, N. </author> <title> (1994) Big Dumb Neural Nets (BDNN): a working brute force approach to speech recognition. </title> <booktitle> Proc. of the ICNN, </booktitle> <volume> Vol. 7, </volume> <pages> 4462-4465, </pages> <year> 1994. </year>
Reference: [40] <author> Nowlan, S. J. </author> <title> (1990) Maximum likelihood competitive learning. </title> <booktitle> In Advances in Neural Information Processing Systems 2, </booktitle> <editor> D.S. Touretzky, ed. </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: [41] <author> Nowlan, S. J. </author> <title> (1991) Soft Competitive Adaptation: Neural Network Learning Algorithms Based on Fitting Statistical Mixtures. </title> <type> Tech. Rep. </type> <address> CMU-CS-91-126, CMU, Pittsburgh, PA. </address>
Reference: [42] <author> Peterson, G. E., Barney, H. </author> <title> (1952) Control methods used in a study of the vowels. </title> <journal> Journal Acoust. Soc. </journal> <volume> America 24, </volume> <pages> 175-184. </pages>
Reference-contexts: The integration of HME's into a hybrid speech recognition framework will be evaluated later in a separate chapter. 4.5 HME for Vowel Classification We will demonstrate the properties of the HME architecture and its learning algorithms on Peterson and Barneys vowel classification data set <ref> [42] </ref>. We chose this dataset because it is non-artificial, speech recognition related and relatively small, allowing to explore and analyze the space of learning parameters. Another advantage of this dataset is its low dimensionality.
Reference: [43] <institution> Proceedings of LVCSR Hub 5 workshop, </institution> <month> Apr. </month> <note> 29 May 1 (1996) MITAGS, </note> <institution> Linthicum Heights, Maryland. </institution>
Reference: [44] <author> Quinlan, J. R. </author> <title> (1986) Induction of decision trees. </title> <journal> Machine Learn. </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: The solution to this problem is the use of decision tree's with a set of phonetic context questions to cluster the polyphonic contexts into a reasonably small set of context classes, which are then modelled by separate HMM's. See <ref> [44] </ref> for an introduction to decision trees. 3.1.5 Decoding/Search The decoder is the essential recognition part of a speech recognizer. It uses locally computed emission probabilities to find the most likely sequence of words in a dynamic programming fashion. <p> There are existing similar tree-structured divide-and-conquer models in statistics, 38 CHAPTER 4. HIERARCHICAL MIXTURES OF EXPERTS namely CART by Breiman et. al. [6], MARS of Friedman [15] and ID3 by Quinlan <ref> [44] </ref>. However, these algorithms solve function approximation or classification problems by explicitly dividing the input space into subregions, such that only one single 'expert' is contributing to the overall output of the model. <p> Because of the inherent tree structure of the HME, it is very appealing to derive a growing algorithm for this architecture. The machine learning literature offers a wide variety of growing algorithms for classification and decision trees <ref> [44] </ref>, [45], [6]. Unfortunately, these algorithms require the evaluation of the gain of all possible node splits, using (mostly) entropy or likelihood based criterions, to eventually realize the best split and discard all the others. Waterhouse and Robinson [56] presented such an algorithm 57 58 CHAPTER 5.
Reference: [45] <author> Quinlan, J. R., Rivest, R. L. </author> <title> (1989) Inferring decision trees using the Minimum Description Length Principle. </title> <booktitle> Information and Computation 80, </booktitle> <pages> 227-248. </pages>
Reference-contexts: Because of the inherent tree structure of the HME, it is very appealing to derive a growing algorithm for this architecture. The machine learning literature offers a wide variety of growing algorithms for classification and decision trees [44], <ref> [45] </ref>, [6]. Unfortunately, these algorithms require the evaluation of the gain of all possible node splits, using (mostly) entropy or likelihood based criterions, to eventually realize the best split and discard all the others. Waterhouse and Robinson [56] presented such an algorithm 57 58 CHAPTER 5.
Reference: [46] <author> Richard, M. D., Lippman, R. P. </author> <title> (1991) Neural network classifiers estimate Bayesian a posteriori probabilities. </title> <booktitle> Neural Computation 3, </booktitle> <pages> 461-483, </pages> <year> 1991. </year>
Reference: [47] <author> Ripley, B. D. </author> <title> (1993) Statistical Aspects of Neural Networks. </title> <note> In Barndorff-Nielsen, </note> <author> O. E., Jensen, J. L. & Kendall, W. S. </author> <title> (eds) Networks and Chaos: Statistical and Probabilistic Aspects, </title> <publisher> London: Chapman & Hall. </publisher>
Reference-contexts: Recently, statisticians published works which established ties between statistics and neural networks, sometimes showing the equivalence of statistical and neural network models. Sarle [48] shows relationships between many neural networks and statistical models and translates the jargons in the two fields. Ripley <ref> [47] </ref> provides a very interesting overview of the similarities of neural networks and statistical models. 2.4.1 Perceptrons A perceptron with a linear output function computes a linear combination of the input features.
Reference: [48] <author> Sarle, W. S. </author> <title> (1994) Neural Networks and Statistical Models. </title> <booktitle> In Proc. Nineteenth Annual SAS Users Group Internat. Conference, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: Additionally, statistical methods provide diagnostic tools such as confidence intervals and hypothesis testing which are missing in the field of neural networks. Recently, statisticians published works which established ties between statistics and neural networks, sometimes showing the equivalence of statistical and neural network models. Sarle <ref> [48] </ref> shows relationships between many neural networks and statistical models and translates the jargons in the two fields. <p> Sarle <ref> [48] </ref> categorizes MLP's into the following three groups: * Small number of hidden neurons. MLP can be considered as a parametric model such as polynomial regression. * Moderate number of hidden neurons. MLP can be considered a quasi-parametric model similar to projection pursuit regression. 2.4.
Reference: [49] <author> Stromberg, J. E., Zrida, J. & Isaksson, A. </author> <title> (1991) Neural trees using neural nets in a tree classifier structure. </title> <booktitle> IEEE Internat. Conf. on Acoustics, Speech and Signal Proc., </booktitle> <pages> 137-140. BIBLIOGRAPHY 109 </pages>
Reference: [50] <author> Syrdal, A. K., Gopal, H. S. </author> <title> (1986) A perceptual model of vowel recognition based on the auditory representation of American English vowels. </title> <journal> Journal Acoust. Soc. </journal> <pages> America 79(4) 1086-1100 </pages>
Reference-contexts: Fig. 4.3 shows the complete data set in the normalized (F1,F2) feature space. 52 CHAPTER 4. HIERARCHICAL MIXTURES OF EXPERTS Syrdal and Gopal <ref> [50] </ref> performed classification on this dataset using a quantitative perceptual model of human vowel recognition. They reported classification rates between 82.3% and 85.9% for their classifier based on bark scale differences and linear discriminant analysis (LDA).
Reference: [51] <author> Tebelskis, J. </author> <title> (1995) Speech Recognition using Neural Networks Ph. D. </title> <type> Thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh PA. </address>
Reference: [52] <author> Waibel, A., Lee, K. F. </author> <title> (eds) (1990) Readings in Speech Recognition. </title> <publisher> Morgan Kauf-mann 1990. </publisher>
Reference: [53] <author> Waibel, A., Finke, M., Gates, D., Gavalda, M., Kemp, T., Lavie, A., Levin, L., Maier, M., Mayfield, L., McNair, A., Rogina, I., Shima, A., Sloboda, T., Woszczyna, M., Zeppenfeld, T., Zhan, P. </author> <title> (1996) JANUS-II Advances in Spontaneous Speech Translation. </title> <booktitle> Internat. Conf. Acoustics, Speech and Signal Proc., </booktitle> <month> May </month> <year> 1996, </year> <institution> Atlanta, Georgia. </institution>
Reference: [54] <author> Waterhouse, S. R. </author> <title> (1993) Speech Recognition using Hierarchical Mixtures of Experts. M. </title> <type> Phil. Thesis, </type> <institution> University of Cambridge, </institution> <address> England. </address>
Reference: [55] <author> Waterhouse, S. R., Robinson, A. J. </author> <title> (1994) Classification using Hierarchical Mixtures of Experts. </title> <booktitle> IEEE Workshop on Neural Networks for Signal Processing IV, </booktitle> <pages> 177-186. </pages>
Reference: [56] <author> Waterhouse, S. R., Robinson, A. J. </author> <title> (1995) Constructive Algorithms for Hierachical Mixtures of Experts. </title> <booktitle> In Advances in Neural Information Processing Systems 8. </booktitle>
Reference-contexts: Unfortunately, these algorithms require the evaluation of the gain of all possible node splits, using (mostly) entropy or likelihood based criterions, to eventually realize the best split and discard all the others. Waterhouse and Robinson <ref> [56] </ref> presented such an algorithm 57 58 CHAPTER 5. CONSTRUCTIVE METHODS for the HME architecture. They evaluated their growing algorithm on a relatively small data set. In the case of very large speech data sets, their approach is no longer applicable in a reasonable amount of time. <p> Continue with step (2) until desired tree size is reached. The number of tree growing phases may either be pre-determined, or based on difference in the likelihoods before and after splitting a node. In contrast to the growing algorithm in <ref> [56] </ref>, our algorithm does not hypothesize all possible node splits, but determines the expansion node (s) directly, which is much faster, especially when dealing with large hierarchies. 5.2.2 Pruning Furthermore, we implemented a path pruning technique similar to the one proposed in [56], which speeds up training and testing times significantly. <p> In contrast to the growing algorithm in <ref> [56] </ref>, our algorithm does not hypothesize all possible node splits, but determines the expansion node (s) directly, which is much faster, especially when dealing with large hierarchies. 5.2.2 Pruning Furthermore, we implemented a path pruning technique similar to the one proposed in [56], which speeds up training and testing times significantly. During the recursive depth-first traversal of the tree (needed for forward evaluation, posterior probability computation and accumulation of node statistics) a path is pruned temporarily if the current node's probability of activation falls below a certain threshold.
Reference: [57] <author> Xu, L., Jordan, M. I. & Hinton, G. E. </author> <title> (1995) An Alternative Model for Mixtures of Experts. </title> <booktitle> In Advances in Neural Information Processing Systems 8. </booktitle>
Reference-contexts: In the case of classification, however, the models for gates and experts have to fullfil the constraint, that their output activations sum up to one for each input frame. Recently, Xu, Jordan and Hinton <ref> [57] </ref> have proposed to use a parametric form based on Gaussian kernels for the gates. We will further develop their work, showing that the same parametric form can be used for experts as well. <p> The problem can be treated as a special case of a mixture of experts, where the experts parameters remain fixed and only the gates are iteratively adapted. The single-loop EM algorithm can therefore be directly used to estimate the gate parameters. It was shown in <ref> [57] </ref> that this can increase overall performance considerably, while avoiding the costly re-estimation of the expert classifiers. <p> Also, we would like to 7.3. MIXTURE OF GAUSSIAN EXPERTS 79 generalize the technique to hierarchical mixtures with more than one gate. Unfortunately, the solution to the EM learning problem proposed in <ref> [57] </ref> does not generalize to experts.
Reference: [58] <author> Zavaliagkos, G., Zhao, Y., Schwartz, R. & Makhoul, J. </author> <title> A Hybrid Segmental Neural Net/Hidden Markov Model System for Continuous Speech Recognition., </title> <journal> IEEE Trans. on Speech and Audio Processing, </journal> <volume> Vol. 2, No. 1, </volume> <pages> 151-160, </pages> <year> 1994. </year>
Reference: [59] <author> Zhao, Y., Schwartz, R., Sroka, J. & Makhoul, J. </author> <title> (1995) Hierarchical Mixtures of Experts Methodology Applied to Continuous Speech Recognition. </title> <booktitle> Internat. Conf. Acoustics Speech and Signal Proc., </booktitle> <volume> Vol 5, </volume> <pages> 3443-3446, </pages> <month> May </month> <year> 1995. </year>
References-found: 59

