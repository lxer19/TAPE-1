URL: http://www.cs.cornell.edu/Info/People/divakar/papers/ran.ps.gz
Refering-URL: 
Root-URL: 
Title: Condition Numbers of Random Triangular Matrices  
Author: D. Viswanath and L.N. Trefethen 
Note: n 2 almost surely n p n 1:305683410: almost surely  
Date: July 9, 1997  
Abstract: Let L n be a lower triangular matrix of dimension n each of whose nonzero entries is an independent N(0; 1) variable, i.e., a random normal variable of mean 0 and variance 1. It is shown that n , the 2-norm condition number of L n , satisfies as n ! 1. This exponential growth of n with n is in striking contrast to the linear growth of the condition numbers of random dense matrices with n that is already known. This phenomenon is not due to small entries on the diagonal (i.e., small eigenvalues) of L n . Indeed, it is shown that a lower triangular matrix of dimension n whose diagonal entries are fixed at 1 with the subdiagonal entries taken as independent N(0; 1) variables is also exponentially ill-conditioned with the 2-norm condition number n of such a matrix satisfying as n ! 1. A similar pair of results about complex random triangular matrices is established. The results for real triangular matrices are generalized to triangular matrices with entries from any symmetric, strictly stable distribution. n p
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> M. Abramowitz and I.A. Stegun, eds., </editor> <title> Handbook of Mathematical Functions, </title> <publisher> Dover Publications, </publisher> <address> New York, </address> <year> 1970. </year>
Reference-contexts: Our results are similar in spirit to results obtained by Silverstein for random dense matrices [16]. Consider a matrix of dimension n fi (yn), where y 2 <ref> [0; 1] </ref>, each of whose n 2 y entries is an independent N (0; 1) variable. Denote its largest and smallest singular values by max and min respectively. It is shown in [16] that max n p min n p y almost surely as n ! 1. <p> The relevant expression for the beta function B (x; y) is Equation (6.2.1) in <ref> [1] </ref>. Also, if x is chosen from the standard Cauchy distribution, then fl ~ = E ((1 + x 2 ) ~=2 ). We do not need fl ~ in terms of the beta function, however; the integral expression (3.4) is more suitable for our purposes.
Reference: [2] <author> P. Billingsley, </author> <title> Probability and Measure, 2nd ed., </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: Since ~ &gt; 0, P (T k &lt; m k ) = P (T ~ k &gt; m ~k ). Use Lemma 3.2 with ~ = ~ to obtain an expression for E (T ~ k ) and apply Markov's inequality <ref> [2] </ref>. Lemma 3.4. For k 1, 0 &lt; ~ &lt; 1 and M &gt; 0, P (T k &gt; M k ) &lt; C ~ (fl ~ =M ) ~k : Proof. <p> In the sequel, a.s. means almost surely as n ! 1. The definition of almost sure convergence for a sequence of random variables can be found in most textbooks on probability, for example <ref> [2] </ref>. Roughly, it means that the convergence holds for a set of sequences of measure 1. 8 Lemma 4.1. kL n k 1=n 2 ! 1 almost surely as n ! 1. Proof. The proof is easy. We provide only an outline. <p> The Frobenius norm of L n , kL n k 2 F , is a sum of n (n + 1)=2 independent 2 variables of mean 1. By an argument exactly analogous to the proof of the strong law of large numbers with finite fourth moment assumption <ref> [2, p. 80] </ref>, kL n k 2 n (n + 1)=2 The proof can be completed using the inequalities n 1=2 kL n k F kL n k 2 kL n k F . <p> Since jp * j &lt; 1, P 1 * is finite. The first Borel-Cantelli lemma <ref> [2] </ref> can be applied to obtain P ( q n k 2 &lt; fl ~ * infinitely often as n ! 1) = 0: Taking the union of the sets in the above equation over all rational * in (0; 1) and considering the complement of that union, we obtain P <p> Theorem 4.3. For random triangular matrices with N (0; 1) entries, as n ! 1, n ! 2 almost surely. Proof. By an inequality sometimes called Lyapunov's [13, p. 144] <ref> [2] </ref>, fl fi &lt; fl 1=ff for any real fi &lt; ff. Thus the bounding intervals [fl 1=~ 1=~ ~ ] in Lemma 4.2 shrink as ~ decreases from 1 to 0.
Reference: [3] <author> J.B. Conway, </author> <title> Functions of One Complex Variable, </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: says that these intervals actually shrink to the following point: lim fl ~ = lim 1 Z 1 1 1=~ 1 Z 1 log (1 + x 2 ) The exact value of the limit can be evaluated to 2 using the substitution x = tan followed by complex integration <ref> [3, p. 121] </ref>. Thus n p n ! 2 a.s.
Reference: [4] <author> A. Edelman, </author> <title> Eigenvalues and Condition Numbers of Random Matrices, </title> <type> PhD dissertation and Numerical Analysis Report 89-7, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1989. </year>
Reference-contexts: Denote its largest and smallest singular values by max and min respectively. It is shown in [16] that max n p min n p y almost surely as n ! 1. The complex analogues of these results can be found in <ref> [4] </ref>. The technique used in [16] is a beautiful combination of what is now known as the Golub-Kahan bidiagonalization step in computing the singular value decomposition with the Gerschgorin circle theorem and the Marcenko-Pastur semicircle law. The techniques used in this paper are more direct.
Reference: [5] <author> A. Edelman, </author> <title> Eigenvalues and condition numbers of random matrices, </title> <journal> SIAM J. Matrix Anal. Appl. </journal> <volume> 9 (1988), </volume> <pages> 543-560. 21 </pages>
Reference-contexts: In the limit n ! 1, the cdfs converge to Heaviside step functions with jumps at the dashed lines. the probability density function (pdf) of n =n, where n is the 2-norm condition number of such a matrix, converges pointwise to the function 2x + 4 as n ! 1 <ref> [5] </ref>. Since the distribution of n =n is independent of n in the limit n ! 1, we can say that the condition numbers of random dense matrices grow only linearly with n. <p> Using this pdf, it can be shown, for example, that E (log ( n )) = log (n) + 1:537 : : : + o (1) <ref> [5] </ref>. In striking contrast, the condition number of a random lower triangular matrix L n , a matrix of dimension n all of whose diagonal and subdiagonal entries are independent N (0; 1) variables, grows exponentially with n.
Reference: [6] <author> W. Feller, </author> <title> An Introduction to Probability Theory and Its Applications, </title> <journal> Vol. </journal> <volume> 2, </volume> <editor> 2nd ed., </editor> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: distribution is said to be stable, if for X i chosen independently from that distribution, n X X i has the same distribution as c n X + d n , where X has the same distribution as X i and c n &gt; 0 and d n are constants <ref> [6, p. 170] </ref>. If d n = 0, the distribution is said to be strictly stable. As usual, the distribution is symmetric if X has the same distribution as X. A symmetric, strictly stable distribution has exponent a if c n = n 1=a . <p> A symmetric, strictly stable distribution has exponent a if c n = n 1=a . A standard result of probability theory says that any stable distribution has an exponent 0 &lt; a 2. The normal distribution is stable with exponent a = 2 <ref> [6] </ref>. The techniques used for triangular matrices with normal entries work more generally when the entries are drawn from a symmetric, strictly stable distri bution. Let L n be a unit lower triangular matrix with entries chosen from a 16 symmetric, strictly stable distribution. <p> Another notable symmetric, strictly stable distribution is the Cauchy distribution with the den sity function (x) = 1 + x 2 : The exponent a for the Cauchy distribution is 1 <ref> [6] </ref>. Using Theorem 8.4 we obtain, Theorem 8.5. For random unit triangular matrices with entries from the stan dard Cauchy distribution, as n ! 1, n p 1 Z +1 log (1 + jxj) almost surely. Numerical integration shows the constant to be 2:533737279 : : : .
Reference: [7] <author> L.V. Foster, </author> <title> Gaussian elimination with partial pivoting can fail in practice, </title> <journal> SIAM J. Matrix Anal. Appl. </journal> <volume> 15 (1994), </volume> <pages> 1354-1362. </pages>
Reference-contexts: In fact, it is not clear that a single matrix problem has ever led to an instability in this algorithm, except for the ones produced by numerical analysts with that end in mind, although Foster <ref> [7] </ref> and Wright [20] have devised problems leading to instability that plausibly "might have arisen" in applications.
Reference: [8] <author> I.S. Gradshteyn and I.W. Ryzhik, </author> <title> Table of Integrals, Series, and Products, 4th ed., </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1965. </year>
Reference-contexts: Proof. The constant K is given by K = exp ( 2 Z 1 log x exp (x 2 =2)dx): To evaluate K, we used integral 4:333 of <ref> [8] </ref>. In contrast to the situation in Theorem 4.3, the constant that n p n converges to in Theorem 5.3 depends on . This is because changing scales only the subdiagonal entries of the unit triangular matrix L n while leaving the diagonal entries fixed at one. <p> Proof. To obtain K, we evaluated K = exp ( 4 0 using the Laplace transform of log (x) given by integral 4.331.1 of <ref> [8] </ref>. The explicit formula involving Ei ( 2 ) was obtained using integral 4.337.2 of [8]. <p> Proof. To obtain K, we evaluated K = exp ( 4 0 using the Laplace transform of log (x) given by integral 4.331.1 of <ref> [8] </ref>. The explicit formula involving Ei ( 2 ) was obtained using integral 4.337.2 of [8].
Reference: [9] <author> H.H. Goldstine and J. von Neumann, </author> <title> Numerical inverting of matrices of high order, </title> <journal> Amer. Math. Soc. Bull. </journal> <volume> 53 (1947), </volume> <pages> 1021-1099. </pages>
Reference-contexts: This algorithm generates an "LU factorization" P A = LU , where P is a permutation matrix, L is unit lower triangular with subdiagonal entries 1 in absolute value, and U is upper triangular. 12 In the mid-1940s it was predicted by Hotelling [14] and von Neumann <ref> [9] </ref> that rounding errors must accumulate exponentially in elimination algorithms of this kind, causing instability for all but small dimensions.
Reference: [10] <author> G.H. Golub and C.F. Van Loan, </author> <title> Matrix Computations, </title> <publisher> 3rd ed., John Hop-kins Univeristy Press, </publisher> <address> Baltimore, </address> <year> 1996. </year>
Reference-contexts: Figure 1a illustrates this result. The matrices that arise in the experiments reported in Figure 1 are so ill-conditioned that the standard, normwise stable method of finding the condition number using the SVD <ref> [10] </ref> fails owing to rounding errors. The method used to generate the figures finds the inverse of the triangular matrix explicitly using the standard algorithm for triangular inversion, and then computes the norms of the matrix and its inverse independently.
Reference: [11] <author> A.K. Gupta and Z. Govindarajulu, </author> <title> Distribution of the quotient of two independent Hotelling T2-variates, </title> <journal> Comm. Statist. </journal> <volume> 4 (1975), </volume> <pages> 449-453. </pages>
Reference-contexts: The (x) corresponding to N (0; 2 ) is the standard Cauchy distribution. To apply Theorem 8.6 for the Cauchy distribu tion, we note that (x) = 2 x 2 1 is the density function of the quotient if the numerator and the denominator are independent Cauchy variables <ref> [11] </ref>. Therefore, Theorem 8.6 implies Theorem 8.7. For random triangular matrices with entries from the standard Cauchy distribution, as n ! 1, n p 2 Z 1 log (1 + jxj) x 2 1 dx) almost surely.
Reference: [12] <author> N.J. Higham, </author> <title> Accuracy and Stability of Numerical Algorithms, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1996. </year>
Reference-contexts: This works because the computation of each column of the inverse by the standard triangular inversion algorithm is componentwise backward stable <ref> [12] </ref>. 2 The exponential growth of n with n is not due to small entries on the diagonal since the probability of a diagonal entry being exponentially small is also exponentially small.
Reference: [13] <author> G. Hardy, J.E. Littlewood and G. Polya, </author> <title> Inequalities, 2nd ed., </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1988. </year>
Reference-contexts: Theorem 4.3. For random triangular matrices with N (0; 1) entries, as n ! 1, n ! 2 almost surely. Proof. By an inequality sometimes called Lyapunov's <ref> [13, p. 144] </ref> [2], fl fi &lt; fl 1=ff for any real fi &lt; ff. Thus the bounding intervals [fl 1=~ 1=~ ~ ] in Lemma 4.2 shrink as ~ decreases from 1 to 0. <p> Proof. By an inequality sometimes called Lyapunov's [13, p. 144] [2], fl fi &lt; fl 1=ff for any real fi &lt; ff. Thus the bounding intervals [fl 1=~ 1=~ ~ ] in Lemma 4.2 shrink as ~ decreases from 1 to 0. A classical theorem <ref> [13, p. 139] </ref> says that these intervals actually shrink to the following point: lim fl ~ = lim 1 Z 1 1 1=~ 1 Z 1 log (1 + x 2 ) The exact value of the limit can be evaluated to 2 using the substitution x = tan followed by
Reference: [14] <author> H. Hotelling, </author> <title> Some new methods in matrix calculation, </title> <journal> Ann. Math. Statist. </journal> <volume> 14 (1943), </volume> <pages> 1-34. </pages>
Reference-contexts: This algorithm generates an "LU factorization" P A = LU , where P is a permutation matrix, L is unit lower triangular with subdiagonal entries 1 in absolute value, and U is upper triangular. 12 In the mid-1940s it was predicted by Hotelling <ref> [14] </ref> and von Neumann [9] that rounding errors must accumulate exponentially in elimination algorithms of this kind, causing instability for all but small dimensions.
Reference: [15] <author> M.L. Mehta, </author> <title> Random Matrices and the Statistical Theory of Energy Levels, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1967. </year>
Reference-contexts: ; Im (t k1 )]; The independence of Re (t k ) and Im (t k ) is a consequence of the orthogonality of v and w, i.e., (v; w) = vw 0 = 0, and the invariance of the jpdf of independent, identically distributed normal variables under orthogonal transformation <ref> [15] </ref>. Thus for fixed t 1 ; : : : ; t k1 , the real and imaginary parts of t k are independent normal variables of mean 0 and variance R k1 =2.
Reference: [16] <author> J.W. Silverstein, </author> <title> The smallest eigenvalue of a large-dimensional Wishart matrix, </title> <journal> Ann. Prob. </journal> <volume> 13 (1985), </volume> <pages> 1364-1368. </pages>
Reference-contexts: Thus, unit triangular matrices with complex nor-mal entries tend to have slightly bigger condition numbers than unit triangular matrices with real normal entries. Our results are similar in spirit to results obtained by Silverstein for random dense matrices <ref> [16] </ref>. Consider a matrix of dimension n fi (yn), where y 2 [0; 1], each of whose n 2 y entries is an independent N (0; 1) variable. Denote its largest and smallest singular values by max and min respectively. It is shown in [16] that max n p min n <p> by Silverstein for random dense matrices <ref> [16] </ref>. Consider a matrix of dimension n fi (yn), where y 2 [0; 1], each of whose n 2 y entries is an independent N (0; 1) variable. Denote its largest and smallest singular values by max and min respectively. It is shown in [16] that max n p min n p y almost surely as n ! 1. The complex analogues of these results can be found in [4]. The technique used in [16] is a beautiful combination of what is now known as the Golub-Kahan bidiagonalization step in computing the singular value decomposition <p> Denote its largest and smallest singular values by max and min respectively. It is shown in <ref> [16] </ref> that max n p min n p y almost surely as n ! 1. The complex analogues of these results can be found in [4]. The technique used in [16] is a beautiful combination of what is now known as the Golub-Kahan bidiagonalization step in computing the singular value decomposition with the Gerschgorin circle theorem and the Marcenko-Pastur semicircle law. The techniques used in this paper are more direct.
Reference: [17] <author> L.N. Trefethen and D. Bau, III, </author> <title> Numerical Linear Algebra, </title> <publisher> SIAM, </publisher> <address> Philadel-phia, </address> <year> 1997. </year>
Reference-contexts: From a comparison of Theorem 5.3 with half a century of the history of Gaussian elimination, then, one may conclude that unit triangular factors of random dense matrices are very different from random unit triangular matrices. An explanation of this difference is offered in <ref> [17] </ref> along the following lines. If A is random, then its successive column spaces are randomly oriented in n-space in the sense that the first column of A is oriented in a random direction, the span 13 of the first two columns is a random 2-dimensional space, and so on.
Reference: [18] <author> L.N. Trefethen and R.S. Schreiber, </author> <title> Average-case stability of Gaussian elimination, </title> <journal> SIAM J. Matrix Anal. Appl. </journal> <volume> 11 (1990), </volume> <pages> 335-360. </pages>
Reference-contexts: The reason appears to be statistical: the matrices A for which kL 1 k is large occupy an exponentially small proportion of the space of all matrices, so small that such matrices "never" arise in practice. Experimental evidence of this phenomenon is presented in <ref> [18] </ref>. This raises the question, why are matrices A for which kL 1 k is large so rare? It is here that the behavior of random unit triangular matrices is relevant. <p> The signs of the entries of these matrices are correlated in special ways that have the effect of keeping kL 1 k almost always very small. For example, it is reported in <ref> [18] </ref> that a certain random matrix A with n = 256 led to kL 1 k = 33:2, whereas if ~ L was taken to be the same matrix but with the signs of its subdiagonal entries randomized, the result became k ~ L 1 k = 2:7 fi 10 8
Reference: [19] <author> J.H. Wilkinson, </author> <title> Error Analysis of direct methods of matrix inversion, </title> <journal> J. Assoc. Comput. Mach. </journal> <volume> 8 (1961), </volume> <pages> 281-330. </pages>
Reference-contexts: In the 1950s, Wilkinson developed a beautiful theory based on backward error analysis that, while it explained a great deal about Gaussian elimination, confirmed that for certain matrices, exponential instability does indeed occur <ref> [19] </ref>. He showed that amplification of rounding errors by factors on the order of kL 1 k may take place, and that for certain matrices, kL 1 k is of order 2 n .
Reference: [20] <author> S.J. Wright, </author> <title> A collection of problems for which Gaussian elimination with partial pivoting is unstable, </title> <journal> SIAM J. Sci. Comput. </journal> <volume> 14 (1993), </volume> <pages> 231-238. </pages>
Reference-contexts: In fact, it is not clear that a single matrix problem has ever led to an instability in this algorithm, except for the ones produced by numerical analysts with that end in mind, although Foster [7] and Wright <ref> [20] </ref> have devised problems leading to instability that plausibly "might have arisen" in applications. The reason appears to be statistical: the matrices A for which kL 1 k is large occupy an exponentially small proportion of the space of all matrices, so small that such matrices "never" arise in practice.
Reference: [21] <author> D. Viswanath and L.N. Trefethen, </author> <title> Random Fibonacci Sequences and the Number 1:13198824 : : : , manuscript. </title>
Reference-contexts: The implementation is explained in the text. n &gt; 2, t n = t n1 t n2 where each sign is independent and either + or with probability 1=2, then n p jt n j ! 1:13198824 : : : almost surely <ref> [21] </ref>. Thus the condition number increases exponentially even for some random triangular matrices that are banded.
Reference: [22] <author> M.C. Yeung and T.F. Chan, </author> <title> Probabilistic analysis of Gaussian elimination without pivoting, </title> <journal> SIAM J. Matrix Anal. Appl. </journal> <volume> 18 (1997), </volume> <pages> 499-517. 22 </pages>
Reference-contexts: In fact, even unpivoted Gaussian elimination does not produce triangular matrices as severely ill-conditioned as random triangular matrices <ref> [22] </ref>. From a comparison of Theorem 5.3 with half a century of the history of Gaussian elimination, then, one may conclude that unit triangular factors of random dense matrices are very different from random unit triangular matrices. An explanation of this difference is offered in [17] along the following lines.
References-found: 22

