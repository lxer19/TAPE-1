URL: http://csg-www.lcs.mit.edu:8001/Users/vivek/ps/GOST92.ps
Refering-URL: http://csg-www.lcs.mit.edu:8001/Users/vivek/sark_pub.html
Root-URL: 
Title: Abstract  
Abstract: In this paper we propose a loop fusion algorithm specifically designed to increase opportunities for array contraction. Array contraction is an optimization that transforms array variables into scalar variables within a loop nest. In contrast to array elements, scalar variables have better cache behavior and can be allocated to registers. In past work we investigated loop interchange and loop reversal as optimizations that increase opportunities for array contraction [13]. This paper extends this work by including the loop fusion optimization. The fusion method discussed in this paper uses the maxflow-mincut algorithm to do loop clustering. Our collective loop fusion algorithm is efficient, and we demonstrate its usefulness for array contraction with a simple example.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. R. Allen and K. Kennedy. </author> <title> Vector register allocation. </title> <type> Technical Report TR86-45, </type> <institution> Rice University, Houston, TX, </institution> <month> December </month> <year> 1986. </year>
Reference-contexts: Notice that in some instances the code was rewritten to assist the compiler with array-element recognition. The particular modification is shown in the array-contracted original collection, Figure 5 (b), where scalar references are used for array elements to avoid redundant array accesses. 1 The greedy algorithms described in <ref> [1] </ref> and [7] would result in the naive partitioning depicted by Figure 5 (c). The random method of picking loops to be fused, as described by [15], would yield an unpredictable clustering. <p> The effects of loop fusion upon virtual memory management and register assignment has been studied by Kuck et al. [9]. Loop fusion for vector machines has been studied in <ref> [2, 1] </ref>. Warren describes a fusion algorithm which merges a collection of loops, allowing the replacement of temporary arrays by scalars [15]. His method however, does not indicate how to perform fusion to minimize the cost of array accesses using scalar replacement.
Reference: [2] <author> John R. Allen. </author> <title> Dependence Analysis for Subscripted Variables and its Application to Program Transformation. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <year> 1983. </year>
Reference-contexts: A node in the LDG represents a loop nest [17]; an edge in the LDG represents a loop-independent data dependence from the source loop nest to the destination loop nest <ref> [2] </ref>. Each edge in the LDG is marked as being fusible or nonfusible. The source and destination loop nests of a nonfusible LDG edge cannot be fused because this would violate the data dependence test for loop fusion [17]. Fusible edges are further classified as contractable and noncontractable. <p> The effects of loop fusion upon virtual memory management and register assignment has been studied by Kuck et al. [9]. Loop fusion for vector machines has been studied in <ref> [2, 1] </ref>. Warren describes a fusion algorithm which merges a collection of loops, allowing the replacement of temporary arrays by scalars [15]. His method however, does not indicate how to perform fusion to minimize the cost of array accesses using scalar replacement. <p> Wolfe uses the term "array contraction" to denote reducing the size of a compiler-generated temporary array created by scalar expansion [17]. Allen discusses the problem of minimizing temporary array storage in a single loop nest by a technique called sectioning <ref> [2] </ref>. Register allocation of subscripted variables (arrays) has been recognized as a challenging optimization problem and studied by Callahan et al., but their work focused on single loop nests as well [4].
Reference: [3] <author> R. Allen and K. Kennedy. </author> <title> Automatic translation of FORTRAN programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9 </volume> <pages> 491-542, </pages> <year> 1987. </year>
Reference-contexts: The benefit derives not from loop fusion or array contraction alone, but from a combination of the two transformations. 6 Related Work A lot of the past work in optimizing the performance of loops has focused on individual loops rather than on collections of loops <ref> [9, 3, 17] </ref>. Loop distribution and loop fusion are two well known loop transformations which deal with multiple loop nests [17]. Loop fusion is useful in 1) reducing loop overhead, 2) increasing the size of loop bodies, which can affect instruction-scheduling, and 3) for better register allocation and memory performance.
Reference: [4] <author> David Callahan, Steve Carr, and Ken Kennedy. </author> <title> Improving register allocation for subscripted variables. </title> <booktitle> Proceedings of the SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <address> June 1990. White Plains, NY. </address>
Reference-contexts: Array contraction replaces an array variable by a scalar variable or by a buffer containing a small number of scalar variables. This replacement usually eliminates many load/store instructions because scalar variables are good candidates for register allocation <ref> [4] </ref>. A secondary benefit arises from the fact that loop fusion increases the number of instructions in a loop body. <p> loops in Figure 5 (e) showed little improvement for the same 1 The compilers we used failed to recognize such references as scalar identities and therefore unnecessarily reloaded the previously written array element again from memory, rather than using the value which is still in register, see Callahan et al. <ref> [4] </ref>. <p> Register allocation of subscripted variables (arrays) has been recognized as a challenging optimization problem and studied by Callahan et al., but their work focused on single loop nests as well <ref> [4] </ref>. In our previous work we investigated the problem of performing loop interchange and loop reversal on individual loop nests so as to maximize the number of array accesses that are potential candidates for array contraction [13].
Reference: [5] <author> J. Ferrante, K. J. Ottenstein, and J. D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Section 6 discusses related work, and Section 7 the conclusion and future work. 2 Program Representation The program representation assumed for performing collective loop fusion is a Loop Dependence Graph (LDG). The loop dependence graph represents a single-entry, single-exit region consisting of k identically control dependent perfect loop nests <ref> [5] </ref>, with conformable loop bounds. A node in the LDG represents a loop nest [17]; an edge in the LDG represents a loop-independent data dependence from the source loop nest to the destination loop nest [2]. Each edge in the LDG is marked as being fusible or nonfusible.
Reference: [6] <author> Jeanne Ferrante, Vivek Sarkar, and Wendy Thrash. </author> <title> On estimating and enhancing cache effectiveness. </title> <booktitle> Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1991. </year> <note> To appear in Springer Verlag's Lecture Notes in Computer Science series. </note>
Reference-contexts: 1 Introduction Loop optimization plays a critical role in the compiler optimization of scientific programs. Loops are primarily used to read and write large arrays of values. The cost of array accesses can be reduced by transforming loop nests for cache locality <ref> [11, 16, 6] </ref>. However, inspite of high cache hit rates, array accesses can incur a large overhead.
Reference: [7] <author> Allen Goldberg and Robert Paige. </author> <title> Stream processing. </title> <booktitle> In 1984 ACM Symposium on Lisp and Functional Programming, </booktitle> <pages> pages 53-62, </pages> <address> Austin, TX, </address> <month> August </month> <year> 1984. </year>
Reference-contexts: The particular modification is shown in the array-contracted original collection, Figure 5 (b), where scalar references are used for array elements to avoid redundant array accesses. 1 The greedy algorithms described in [1] and <ref> [7] </ref> would result in the naive partitioning depicted by Figure 5 (c). The random method of picking loops to be fused, as described by [15], would yield an unpredictable clustering. The second algorithm described in [7] is more sophisticated, and generates a clustering close to optimal, as shown in The results <p> array elements to avoid redundant array accesses. 1 The greedy algorithms described in [1] and <ref> [7] </ref> would result in the naive partitioning depicted by Figure 5 (c). The random method of picking loops to be fused, as described by [15], would yield an unpredictable clustering. The second algorithm described in [7] is more sophisticated, and generates a clustering close to optimal, as shown in The results of partitioning based upon timings taken on the SPARC server are shown in Table 1 (a). The table shows both optimized and nonoptimized versions for integer, single-precision floating point, and double-precision floating-point arrays. <p> His method however, does not indicate how to perform fusion to minimize the cost of array accesses using scalar replacement. Goldberg and Paige describe loop fusion as an optimization technique to find an "optimal schedule" by minimizing the number of fusible loop clusters <ref> [7] </ref>. The loop fusion problem studied in this paper goes beyond simple loop-cluster minimization and instead tries to minimize the data-access cost function. A recent study [8], uses loop fusion to maximize loop parallelism and to improve data locality.
Reference: [8] <author> Ken Kennedy and Kathryn S. McKinley. </author> <title> Maximizing loop parallelism and improving data locality via loop fusion and distribution. </title> <type> Technical report, </type> <institution> Rice University, </institution> <month> August </month> <year> 1992. </year> <institution> Rice COMP TR92-189. </institution>
Reference-contexts: Goldberg and Paige describe loop fusion as an optimization technique to find an "optimal schedule" by minimizing the number of fusible loop clusters [7]. The loop fusion problem studied in this paper goes beyond simple loop-cluster minimization and instead tries to minimize the data-access cost function. A recent study <ref> [8] </ref>, uses loop fusion to maximize loop parallelism and to improve data locality. In this independent study, they use the maxflow algorithm to do graph partitioning for loop fusion. They use edge weights, similar to our approach, to denote the reuse of arrays across different loop nests.
Reference: [9] <author> D. J. Kuck, Kuhn R., D. Padua, B. Leasure, and M. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Proceedings of the Eighth ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 207-218, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: The benefit derives not from loop fusion or array contraction alone, but from a combination of the two transformations. 6 Related Work A lot of the past work in optimizing the performance of loops has focused on individual loops rather than on collections of loops <ref> [9, 3, 17] </ref>. Loop distribution and loop fusion are two well known loop transformations which deal with multiple loop nests [17]. Loop fusion is useful in 1) reducing loop overhead, 2) increasing the size of loop bodies, which can affect instruction-scheduling, and 3) for better register allocation and memory performance. <p> Loop fusion is useful in 1) reducing loop overhead, 2) increasing the size of loop bodies, which can affect instruction-scheduling, and 3) for better register allocation and memory performance. The effects of loop fusion upon virtual memory management and register assignment has been studied by Kuck et al. <ref> [9] </ref>. Loop fusion for vector machines has been studied in [2, 1]. Warren describes a fusion algorithm which merges a collection of loops, allowing the replacement of temporary arrays by scalars [15].
Reference: [10] <author> Russell Olsen. </author> <title> Analysis and transformation of loop clusters. </title> <type> Master's thesis, </type> <institution> McGill University, Montreal, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: The loop nests have two nonfusible data dependences which can be detected by the compiler, from loop 1 to loop 3 and from loop 4 to loop 6 <ref> [13, 10] </ref>.
Reference: [11] <author> Allan K. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year> <note> COMP TR89-93. </note>
Reference-contexts: 1 Introduction Loop optimization plays a critical role in the compiler optimization of scientific programs. Loops are primarily used to read and write large arrays of values. The cost of array accesses can be reduced by transforming loop nests for cache locality <ref> [11, 16, 6] </ref>. However, inspite of high cache hit rates, array accesses can incur a large overhead.
Reference: [12] <author> Vivek Sarkar. </author> <title> The PTRAN parallel programming system. </title> <editor> In B. Szymanski, editor, </editor> <booktitle> Parallel Functional Programming Languages and Environments. McGraw-Hill Series in Supercomputing and Parallel Processing, </booktitle> <year> 1990. </year>
Reference-contexts: However, it is possible for either a store or load operation to be contained within a conditional. This case can be more accurately modeled by using an edge cost &lt;1, based upon the execution probabilities for the conditionals <ref> [12] </ref>. For the sake of simplicity, we approximate this case by a unit cost as well. The cost model can be extended to take into account conditional probabilities.
Reference: [13] <author> Vivek Sarkar and Guang R. Gao. </author> <title> Optimization of array accesses by collective loop transformations. </title> <booktitle> Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <pages> pages 194-205, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: This limitation on instruction parallelism is a serious problem for modern processor architectures that depend heavily on instruction parallelism to achieve high performance. Our approach is based upon performing loop fusion to increase the opportunities for array contraction <ref> [13] </ref>. Array contraction replaces an array variable by a scalar variable or by a buffer containing a small number of scalar variables. This replacement usually eliminates many load/store instructions because scalar variables are good candidates for register allocation [4]. <p> The cost model can be extended to take into account conditional probabilities. We also implicitly assume that suitable loop transformations have already been performed on the individual loop nests before applying the fusion algorithm (see <ref> [13] </ref> or other related work). 4 A Solution based upon Network-Flow A graph that has no nonfusible nodes or edges can be trivially fused into a single cluster. Let G = [V; E] be an acyclic Loop Dependence Graph with at least one nonfusible edge or node. <p> The loop nests have two nonfusible data dependences which can be detected by the compiler, from loop 1 to loop 3 and from loop 4 to loop 6 <ref> [13, 10] </ref>. <p> In our previous work we investigated the problem of performing loop interchange and loop reversal on individual loop nests so as to maximize the number of array accesses that are potential candidates for array contraction <ref> [13] </ref>.
Reference: [14] <author> R. E. Tarjan. </author> <title> Data Structures and Network Algorithms. </title> <institution> Society for Industrial and Applied Mathematics, </institution> <address> Philadelphia, PA, </address> <year> 1983. </year>
Reference-contexts: A secondary benefit arises from the fact that loop fusion increases the number of instructions in a loop body. The algorithm for collective loop fusion presented in this paper is novel in its use of a cluster numbering scheme followed by one or more applications of the maxflow-mincut algorithm <ref> [14] </ref>. Unlike other loop fusion algorithms that are based upon fusing pairs of loops, our algorithm operates collectively on all loop nests. Our approach is provably optimal for the special case of two-cluster partitioning for which the max-flow algorithm is known to yield a cut set of minimum size. <p> A flow through the graph can never exceed the capacity of any cut. And, for a given maximum flow through the graph, there must exist a cut whose capacity is equal to the maximum flow; such a cut is called the minimum cut <ref> [14] </ref>. A reduced graph with only one cluster node can assign all its vertex nodes to this cluster. But when there are multiple cluster nodes, then the maxflow algorithm is applied. <p> Given a maximum flow, the set of edges that comprise the minimum cut is easily found <ref> [14] </ref>. The maxflow algorithm is applied repeatedly until there remain no unassigned vertex node. This algorithm terminates since each application of the maxflow algorithm partitions the graph into two components that are each smaller than the original. Sometimes the graph can be partitioned into more than two components. <p> In the worst case, each application of the maxflow algorithm isolates only one vertex, therefore requiring n r 1 applications of maxflow. If we assume that the maxflow algorithm on a graph takes time O (n r m r log n r ) <ref> [14] </ref>, the total time taken for Step 2 is O (n 2 r m r log n r ). Thus the total time for the algorithm is O (nm) + O (n 2 r m r log n r ).
Reference: [15] <author> Joe Warren. </author> <title> A hierarchical basis for reordering transformations. </title> <booktitle> Eleventh ACM Principles of Programming Languages Symposium, </booktitle> <pages> pages 272-282, </pages> <address> January 1984. Salt Lake City, UT. </address>
Reference-contexts: The random method of picking loops to be fused, as described by <ref> [15] </ref>, would yield an unpredictable clustering. The second algorithm described in [7] is more sophisticated, and generates a clustering close to optimal, as shown in The results of partitioning based upon timings taken on the SPARC server are shown in Table 1 (a). <p> Loop fusion for vector machines has been studied in [2, 1]. Warren describes a fusion algorithm which merges a collection of loops, allowing the replacement of temporary arrays by scalars <ref> [15] </ref>. His method however, does not indicate how to perform fusion to minimize the cost of array accesses using scalar replacement. Goldberg and Paige describe loop fusion as an optimization technique to find an "optimal schedule" by minimizing the number of fusible loop clusters [7].
Reference: [16] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <month> June 26-28 </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Loop optimization plays a critical role in the compiler optimization of scientific programs. Loops are primarily used to read and write large arrays of values. The cost of array accesses can be reduced by transforming loop nests for cache locality <ref> [11, 16, 6] </ref>. However, inspite of high cache hit rates, array accesses can incur a large overhead.
Reference: [17] <author> Michael J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> Pitman, London and MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year> <title> In the series, Research Monographs in Parallel and Distributed Computing. </title> <note> Revised version of the author's Ph.D. dissertation, Published as Technical Report UIUCDCS-R-82-1105, </note> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1982. </year>
Reference-contexts: The loop dependence graph represents a single-entry, single-exit region consisting of k identically control dependent perfect loop nests [5], with conformable loop bounds. A node in the LDG represents a loop nest <ref> [17] </ref>; an edge in the LDG represents a loop-independent data dependence from the source loop nest to the destination loop nest [2]. Each edge in the LDG is marked as being fusible or nonfusible. <p> Each edge in the LDG is marked as being fusible or nonfusible. The source and destination loop nests of a nonfusible LDG edge cannot be fused because this would violate the data dependence test for loop fusion <ref> [17] </ref>. Fusible edges are further classified as contractable and noncontractable. A contractable edge is one where loop fusion will yield a savings in memory cost because a data computed by a source loop nest will be available in the processor's registers for use by a destination loop nest. <p> The benefit derives not from loop fusion or array contraction alone, but from a combination of the two transformations. 6 Related Work A lot of the past work in optimizing the performance of loops has focused on individual loops rather than on collections of loops <ref> [9, 3, 17] </ref>. Loop distribution and loop fusion are two well known loop transformations which deal with multiple loop nests [17]. Loop fusion is useful in 1) reducing loop overhead, 2) increasing the size of loop bodies, which can affect instruction-scheduling, and 3) for better register allocation and memory performance. <p> Loop distribution and loop fusion are two well known loop transformations which deal with multiple loop nests <ref> [17] </ref>. Loop fusion is useful in 1) reducing loop overhead, 2) increasing the size of loop bodies, which can affect instruction-scheduling, and 3) for better register allocation and memory performance. The effects of loop fusion upon virtual memory management and register assignment has been studied by Kuck et al. [9]. <p> Wolfe uses the term "array contraction" to denote reducing the size of a compiler-generated temporary array created by scalar expansion <ref> [17] </ref>. Allen discusses the problem of minimizing temporary array storage in a single loop nest by a technique called sectioning [2].
References-found: 17

