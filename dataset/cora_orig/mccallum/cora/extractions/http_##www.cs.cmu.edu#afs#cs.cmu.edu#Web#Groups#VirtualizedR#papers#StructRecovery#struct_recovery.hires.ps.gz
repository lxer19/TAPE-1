URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/Groups/VirtualizedR/papers/StructRecovery/struct_recovery.hires.ps.gz
Refering-URL: 
Root-URL: 
Title: laser range finders. In addition, exposure time and sampling rate are unimportant because no motion
Author: Peter W. Rander, P.J. Narayanan, and Takeo Kanade 
Date: Dec. 8-11, 1996, pp.305-312.  
Address: Washington D.C.,  Pittsburgh, PA 15221, USA  
Affiliation: Fusion and Integration for Intelligent Systems,  Robotics Institute Carnegie Mellon University  
Note: Proc. of the 1996 Intl Conf. on Multisensor  1.0 Introduction sensors such as  
Abstract: Despite significant progress in automatic recovery of static scene structure from range images, little effort has been made toward extending these approaches to dynamic scenes. This disparity is in large part due to the lack of range sensors with the high sampling rates needed to accurately capture dynamic scenes. We have developed a system that overcomes this problem by exploiting video cameras, which easily capture images of dynamic scenes, and image-based stereo, which estimates scene structure based on correspondences among the images from different cameras. Our system uses a synchronized multi-camera recording system to capture live video of the scene and a software implementation of image-based stereo to compute range images off-line. By combining this system with multi-image fusion, we created a novel system for dynamic structure recovery, with many applications including telepresence, training, and entertainment. Development of this system has also revealed the potential use of fusion as both a multi-view and multi-resolution integration process for stereo. Recovering 3D structure of static scenes has been the focus of much research [2][4][6][7][17][19][21]. Good solutions to this problem have applications in a number of domains, especially reverse engineering for virtual reality and manufacturing. Dynamic scene structure recovery, on the other hand, has received little attention from the research community, despite broad usefulness in traditional telepres-ence and virtual reality, as well as in such new areas as Virtu-alized Reality [10][11] and Multiple-Perspective Interactive Video [8]. This lack of work on the dynamic problem is due in large part to the lack of range sensors capable of meeting the high demands of the dynamic scenes. In static structure recovery, multiple range images are fused into a single 3D model. Usually the range images are collected with a single range sensor, either by moving the sensor around the scene or by moving the scene around the sensor. This approach eliminates any interaction between sensors at different viewpoints, allowing the use of active In dynamic structure recovery, multiple range image sequences are fused into a sequence of 3D models. Because the scene changes, multiple viewpoints require multiple sensors operating simultaneously. Using a collection of active sensors is not possible because the illumination (the active signal transmitted) from one sensor would interfere with that of another sensor. In addition, the exposure time must be short to reduce blur and the sampling rate must be high to avoid missing parts of the scene motion -- that is, to avoid temporal aliasing. Existing sensors are simply too slow to achieve these requirements for many interesting scenes. Dynamic scene range imaging, then, requires sensors which have high sampling rates and which are capable of coexisting with other sensors. One way of satisfying these constraints is to apply image-based stereo to images acquired from simple video cameras. A video cameras sampling rate is high enough to capture significant motion (for NTSC video, 30Hz) and since video cameras are passive sensors, they easily co-exist with one another in a common environment. Recent efforts have, in fact, demonstrated the utility of this approach: Kanade et. al. [9] have developed a real-time stereo machine that provides depth images with 240x256 pixels at 30 Hz while searching up to 60 depth levels. This stereo machine performs both image capture and stereo computation in dedicated hardware at video rate. We apply the same concept in our system, but separate real-time image recording from off-line stereo computation. This approach allows us to use inexpensive recording equipment for image capture and general purpose computers for stereo processing. Given sequences of range images from many perspectives, one then must fuse the images to form 3D models that accurately represent the 3D scene structure at each time instant. In general, this would require pose estimation for each sensor position (assuming fixed sensor location) as well as temporal synchronization of the sequences. We eliminate both problems, though, by exploiting the information required for stereo processing. Stereo requires camera pose Recovery of Dynamic Scene Structure from Multiple Image Sequences 
Abstract-found: 1
Intro-found: 0
Reference: [1] <author> R. Collins, </author> <title> A Space-Sweep Approach to True Multi-Image Matching, </title> <note> Proceedings of IEEE CVPR96,to appear. </note>
Reference-contexts: In addition, this approach assumes that the intensity observed in the image is independent of the viewpoint. Physically, this would require uniformly illuminated, perfectly-lambertian scenes and identical cameras, neither of which is likely to exist in the real world. Transforming from intensity to edges, as Collins <ref> [1] </ref> does, may overcome some of these difficulties since edges are less dependent on intensity variations from viewpoint changes and imaging hardware inconsistencies. Since edges do not provide dense structure, though, significantly more effort must go into constructing the model from the edges, which itself is a non-trivial issue.
Reference: [2] <author> B. Curless and M. Levoy, </author> <title> A Volumetric Method for Building Complex Models from Range Images, </title> <booktitle> Computer Graphics SIGGRAPH 96, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: With these added constraints, we approximate dynamic scenes with a sequence of static scenes, reducing the dynamic structure recovery problem to static structure recovery at each time sample. For this problem, we apply an algorithm very similar to Curless and Levoy <ref> [2] </ref>, integrating multiple range images into a single 3D grid of voxels, then extracting a 3D triangle mesh surface description from the voxel space. If desired, the triangle mesh can be decimated to achieve significantly smaller meshes with little loss in structural accuracy. <p> Based on its resilience to noise, simplicity of design, and non-iterative operation, we use an algorithm very similar to the voxel-space merging strategy of Curless and Levoy <ref> [2] </ref>, which itself bears similarity to several other algorithms [6][7]. Their algorithm accumulates, at each voxel, the signed distance to the surfaces in the range images, weighted by any reliability estimates for the sample.
Reference: [3] <author> D. Eberly, R. Gardner, B. Morse, S. Pizer, and C. Scharlach, </author> <title> Ridges for Image Analysis, </title> <journal> Journal of Mathematical Imaging and Vision, </journal> <volume> 4(4) </volume> <pages> 353-373, </pages> <month> Dec. </month> <year> 1994. </year>
Reference-contexts: Unlike isosurface extraction, ridge finding is a problem without robust solutions <ref> [3] </ref>. Our approach is similar to that of Curless and Levoy, but with one noteworthy change.
Reference: [4] <author> P. Fua, </author> <title> Reconstructing Complex Surfaces from Multiple Stereo Views, </title> <booktitle> Proceedings of International Conference on Computer Vision (ICCV95), </booktitle> <pages> 1078-1085, </pages> <month> June, </month> <year> 1995. </year>
Reference: [5] <author> H. Fuchs, G. Bishop, K. Arthur, L. McMillan, R. Bajcsy, S.W. Lee, H. Farid, and T. Kanade. </author> <title> Virtual Space Teleconferencing using a Sea of Cameras, </title> <booktitle> In Proceedings of the First International Symposium on Medical Robotics and Computer Assisted Surgery, </booktitle> <address> pp.161-167, </address> <year> 1994 </year>
Reference: [6] <author> A. Hilton, </author> <title> On Reliable Surface Reconstruction From Multiple Range Images, </title> <type> Technical Report VSSP-TR-5/ 95, </type> <institution> University of Surrey, </institution> <month> Oct. </month> <year> 1995. </year>
Reference: [7] <author> H. Hoppe, T. DeRose, T. Duchamp, M. Halstead, H. Jin, J. McDonald, J. Schweitzer, and W. Stuetzle, </author> <title> Piecewise Smooth Surface Reconstruction, </title> <journal> Computer Graphics SIGGRAPH94, </journal> <pages> 295-302, </pages> <year> 1994. </year>
Reference: [8] <author> R. Jain and K. Wakimoto, </author> <title> Multiple perspective interactive video, </title> <booktitle> In Proceedings of IEEE Conference on Multimedia Systems, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: Dynamic scene structure recovery, on the other hand, has received little attention from the research community, despite broad usefulness in traditional telepres-ence and virtual reality, as well as in such new areas as Virtu-alized Reality [10][11] and Multiple-Perspective Interactive Video <ref> [8] </ref>. This lack of work on the dynamic problem is due in large part to the lack of range sensors capable of meeting the high demands of the dynamic scenes. In static structure recovery, multiple range images are fused into a single 3D model.
Reference: [9] <author> T. Kanade, H. Kano, S. Kimura, A. Yoshida, K. </author> <title> Oda, Development of a Video-Rate Stereo Machine, </title> <booktitle> Proceedings of International Robotics and Systems Conference (IROS95), </booktitle> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: A video cameras sampling rate is high enough to capture significant motion (for NTSC video, 30Hz) and since video cameras are passive sensors, they easily co-exist with one another in a common environment. Recent efforts have, in fact, demonstrated the utility of this approach: Kanade et. al. <ref> [9] </ref> have developed a real-time stereo machine that provides depth images with 240x256 pixels at 30 Hz while searching up to 60 depth levels. This stereo machine performs both image capture and stereo computation in dedicated hardware at video rate. <p> We then present experiments using the system and evaluate the potential benefits of using multi-image fusion to address common breakdowns in stereo. 2.0 Intensity and Range Image Acquisition Although it would be simplest to generate range images using hardware like that of the stereo machine <ref> [9] </ref>, practical limitations (especially cost) prevent us from directly implementing this strategy.
Reference: [10] <author> T. Kanade, P. J. Narayanan, and P. W. Rander. </author> <title> Virtual-ized Reality: Being Mobile in a Visual Scene, </title> <booktitle> International Conference on Artificial Reality and Tele-Existence and Conference on VIrtual Reality Software and Technology, </booktitle> <address> Japan, </address> <month> Nov </month> <year> 1995. </year>
Reference: [11] <author> T. Kanade, P. J. Narayanan, and P. W. Rander. </author> <title> Virtual-ized Reality: Concept and Early Results, </title> <booktitle> IEEE Workshop on the Representation of Visual Scenes, </booktitle> <address> Boston, </address> <month> June, </month> <year> 1995. </year>
Reference: [12] <author> P. Khalili, </author> <title> Forming a Three-Dimensional Environment Model for Autonomous Navigation Using a Sequence of Images, </title> <institution> University of Michigan Ph.D. </institution> <type> thesis, </type> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: We discuss this possibility more completely in Section 5.0. There are a few alternative approaches that attempt to bypass the range image construction phase, mapping more directly from intensity images to full 3D models. Khalili <ref> [12] </ref> proposes an intensity-based voxel stereo algorithm in which each voxel is projected into all images to acquire a set of intensity samples. Assuming the voxel is actually occupied, the variance of these samples should be low, since they are all images of the same point.
Reference: [13] <author> W. Lorensen and H. Cline, </author> <title> Marching Cubes: a High Resolution 3D surface Construction Algorithm, </title> <journal> Computer Graphics SIGGRAPH87, </journal> <pages> 163-170, </pages> <month> July </month> <year> 1987 </year>
Reference-contexts: This simple strategy provides nearly linear speedups with the number of computers used. 4.5 Surface Extraction We next extracted the 3D object surfaces by applying an implementation [22] of the Marching Cubes algorithm <ref> [13] </ref>. We decimated the resulting mesh using a mesh simplification program written by Andrew Johnson. The sequence of meshes is shown in Figure 11, where we have removed the background so that the person is more clearly visible.
Reference: [14] <author> P. J. Narayanan, P. Rander, and T. Kanade, </author> <title> Synchronizing and Capturing Every Frame from Multiple Cameras, </title> <type> Robotics Institute Technical Report, </type> <institution> CMU-RI-TR-95-25, </institution> <year> 1995. </year>
Reference-contexts: A 50-camera system would require 0.5 GBytes/sec bandwidth and more that 1 TeraByte of storage for an hour of recording time. Clearly, this approach is impractical using current technology. The approach we take (discussed much more completely in <ref> [14] </ref>) is to split the image capture problem into two stages: synchronous real-time recording and off-line digitization. In the recording system, shown schematically in Figure 2, video cameras are externally synchronized. The video signals from Range Images Voxel Merging 3D ModelCameras FIGURE 1. Conceptual overview of structure recovery process.
Reference: [15] <author> M. Okutomi and T. Kanade. </author> <title> A multiple-baseline stereo, </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 15(4) </volume> <pages> 353-363, </pages> <year> 1993. </year>
Reference-contexts: This approach can be considered a two-stage fusion process, since stereo itself is a low-level fusion process. In this context, fusion of multiple stereo views aims to increase precision and to reduce ambiguity -- the same reasons to use multi-camera stereo rather than two-camera (binocular) stereo <ref> [15] </ref>. In stereo, it is common to use multi-resolution processing to improve speed and reliability, but fusing the multi-resolution information in stereo has proven to be problematic. <p> base cost covers a computer, a digitizer, and computer-controllable VCR, while each channel requires a camera, VITC inserter, and VCR -- all available commercially. 2.2 Image to Depth Conversion To compute range images from the multi-camera images collected with our video capture system, we use the Multi-Baseline Stereo (MBS) technique <ref> [15] </ref>, extended to handle non-parallel cameras. The choice of the MBS algorithm was motivated primarily by two factors. First, MBS recovers dense depth maps that is, a depth estimate for every pixel in the intensity images which is useful for surface reconstruction.
Reference: [16] <author> K. Satoh and Y. Ohta. </author> <title> Passive Depth Acquisition for 3D Image Displays. </title> <journal> In IEICE Transactions on Information and Systems, </journal> <volume> E77-D(9), </volume> <month> Sep. </month> <year> 1994. </year>
Reference-contexts: This error can make surfaces appear to be wider or narrower than they should be, as shown in Figure 4. While some efforts have been made to improve performance near these discontinuities (e.g., <ref> [16] </ref>), this problem is still an open research issue. Third, precision of the depth estimates is limited by the sampling resolution and noise of the images, which limits the resolu tion with which a region of one image can be aligned with a region in another image.
Reference: [17] <author> H.Y. Shum, M. Hebert, K. Ikeuchi and R. Reddy, </author> <title> An integral approach to free-form object modeling, </title> <booktitle> to appear in IEEE Transactions on Pattern Analysis and Machine Intelligence. Also appeared in Proceedings of International Conference on Computer Vision (ICCV95), </booktitle> <pages> 870-875, </pages> <address> Boston, USA, </address> <month> June, </month> <year> 1995. </year>
Reference: [18] <editor> Society of Motion Picture and Television Engineers. </editor> <title> American National Standard for Television -- Time and Control Code, </title> <journal> SMPTE Journal, </journal> <month> June </month> <year> 1986. </year>
Reference-contexts: Stereo Stereo Stereo Stereo Proc. of the 1996 Intl Conf. on Multisensor Fusion and Integration for Intelligent Systems, Washington D.C., Dec. 8-11, 1996, pp.305-312. the cameras are time-stamped with the industry-standard Vertical Interval Time Code (VITC) <ref> [18] </ref> before being recorded by consumer-grade VCRs. The digitization system (Figure 3) contains a computer-controllable VCR, a digitizer, and a host computer. The computer instructs the VCR to play a videotape while the digitizer captures as many frames as it can.
Reference: [19] <author> M. Soucy and D. Laurendeau, </author> <title> Multi-Resolution Surface Modelling from Multiple Range Views, </title> <booktitle> Proceedings of IEEE CVPR92, </booktitle> <pages> 348-353, </pages> <year> 1992. </year>
Reference: [20] <author> R. Tsai, </author> <title> A versatile camera calibration technique for high-accuracy 3D machine vision metrology using off-the-shelf tv cameras and lenses, </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> RA-3(4):323-344, </volume> <year> 1987. </year>
Reference-contexts: Before we can apply MBS, we must first calibrate each camera in the multi-camera system. We use an approach from Tsai <ref> [20] </ref>, implemented by Reg Willson, that calibrates an 11-parameter camera model to each camera.
Reference: [21] <author> G. Turk and M. Levoy, </author> <title> Zippered Polygon Meshes from Range Images, </title> <journal> Computer Graphics SIGGRAPH94, </journal> <pages> 311-318, </pages> <year> 1994. </year>
Reference: [22] <author> J. Bloomenthal, </author> <title> An Implicit Surface Polygonizer, Graphics Gems IV, </title> <editor> ed. P. </editor> <booktitle> Heckbert, </booktitle> <pages> 324-349, </pages> <note> 1994 (ftp:/ </note> <author> /ftp-graphics.stanford.edu/pub/Graphics/Graphics-Gems/GemsIV/GGemsIV.tar.Z). </author> <title> FIGURE 12. Fusion algorithm applied near a depth discontinuity. Each of the estimated surfaces from stereo extend the edge, but always parallel to the image plane. real object </title>
Reference-contexts: This simple strategy provides nearly linear speedups with the number of computers used. 4.5 Surface Extraction We next extracted the 3D object surfaces by applying an implementation <ref> [22] </ref> of the Marching Cubes algorithm [13]. We decimated the resulting mesh using a mesh simplification program written by Andrew Johnson. The sequence of meshes is shown in Figure 11, where we have removed the background so that the person is more clearly visible.
References-found: 22

