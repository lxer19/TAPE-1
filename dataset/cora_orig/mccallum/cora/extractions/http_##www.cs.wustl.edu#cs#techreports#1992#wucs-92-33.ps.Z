URL: http://www.cs.wustl.edu/cs/techreports/1992/wucs-92-33.ps.Z
Refering-URL: http://www.cs.wustl.edu/cs/cs/publications.html
Root-URL: 
Title: On the Sample Complexity of Weakly Learning  
Author: Sally A. Goldman Michael J. Kearns Robert E. Schapire 
Keyword: p  
Note: Most of this research was carried out while all three authors were at MIT Laboratory for Computer Science with support provided by ARO Grant DAAL03-86-K-0171, DARPA Contract N00014-89-J-1988, NSF Grant CCR-88914428, and a grant from the Siemens Corporation. S. Goldman is currently supported in part by a G.E. Foundation Junior Faculty Grant and NSF Grant CCR-9110108.  
Address: St. Louis, MO 63130  Murray Hill, NJ 07974  Murray Hill, NJ 07974  
Affiliation: Department of Computer Science Washington University  AT&T Bell Laboratories  AT&T Bell Laboratories  
Pubnum: WUCS-92-33  
Email: sg@cs.wustl.edu  mkearns@research.att.com  schapire@research.att.com  
Date: September 22, 1992  
Abstract: In this paper, we study the sample complexity of weak learning. That is, we ask how much data must be collected from an unknown distribution in order to extract a small but significant advantage in prediction. We show that it is important to distinguish between those learning algorithms that output deterministic hypotheses and those that output randomized hypotheses. We prove that in the weak learning model, any algorithm using deterministic hypotheses to weakly learn a class of Vapnik-Chervonenkis dimension d(n) requires ( d(n)) examples. In contrast, when randomized hypotheses are allowed, we show that fi(1) examples suffice in some cases. We then show that there exists an efficient algorithm using deterministic hypotheses that weakly learns against any distribution on a set of size d(n) with only O(d(n) 2=3 ) examples. Thus for the class of symmetric Boolean functions over n variables, where the strong learning sample complexity is fi(n), the sample complexity for weak learning using deterministic hypotheses is ( n) and O(n 2=3 ), and the sample complexity for weak learning using randomized hypotheses is fi(1). Next we prove the existence of classes for which the distribution-free sample size required to obtain a slight advantage in prediction over random guessing is essentially equal to that required to obtain arbitrary accuracy. Finally, for a class of small circuits, namely all parity functions of subsets of n Boolean variables, we prove a weak learning sample complexity of fi(n). This bound holds even if the weak learning algorithm is allowed to replace random sampling with membership queries, and the target distribution is uniform on f0; 1g n . p
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dana Angluin and Leslie G. Valiant. </author> <title> Fast probabilistic algorithms for Hamiltonian circuits and matchings. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 18(2) </volume> <pages> 155-193, </pages> <month> April </month> <year> 1979. </year>
Reference-contexts: Finally, to compute the sample sizes needed for several of our algorithms we use the following versions of Chernoff bounds. The first bound stated, Hoeffding's inequality [11], will be used whenever p 1=4. However, when p &lt; 1=4 the last two bounds as stated by Angluin and Valiant <ref> [1] </ref> give better bounds. (See also Chernoff [3], and Erdos and Spencer [5].) 5 Lemma 1 (Chernoff Bounds) Let X 1 ; : : : ; X m be a sequence of m independent Bernoulli trials, each succeeding with probability p. <p> With respect to the N points in T , any Boolean concept c is represented by characteristic vector ~v c 2 f0; 1g N on the N -dimensional Boolean hypercube and any randomized hypothesis h is represented by a vector ~v h 2 <ref> [0; 1] </ref> N in the N -dimensional real cube. In both cases we regard the ith components (~v c ) i and (~v h ) i as the probability that 1 is output when the input is the ith point of T . <p> The next lemma shows that any ball b h contains less than half of f0; 1g N ; thus any randomized hypothesis h is a weak learning hypothesis for at most half of all concepts over T . Lemma 13 For any randomized hypothesis ~v h 2 <ref> [0; 1] </ref> N , at most 1=2 of the ~v c 2 f0; 1g N satisfy ~v c 2 b h . <p> We want this statement to hold simultaneously for all randomized hypotheses h. Since there are infinitely many such hypotheses, we need a uniform convergence result for the class of concept classes B = fb h : h 2 <ref> [0; 1] </ref> N g. <p> Finally, we sum the probability of failure over all choices for S. Lemma 14 Fix 0 &lt; fi 1=2. The probability (over the random choice of the class C n ) that there exists ~v h 2 <ref> [0; 1] </ref> N such that d N (~v c ; ~v h ) &lt; 1=2 for a fraction 1=2 + fi of the ~v c 2 C n consistent with S is at most c 0 e 2 n for some constant c 0 &gt; 0. <p> The number of such samples is at most 2 n 2 +n ; thus the probability (over the random choice of C n ) that there is some labeled sample S of n points such that there exists ~v h 2 <ref> [0; 1] </ref> N satisfying d N (~v c ; ~v h ) &lt; 1=2 for a fraction 1=2 + fi of the concepts in ~v c 2 C n consistent with S is at most c 0 2 n 2 +n =e 2 n .
Reference: [2] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: We begin by observing that if the Vapnik-Chervonenkis dimension of a concept class C n is super-polynomial in n, then the lower bound proofs for the strong learning model <ref> [2] </ref> are easily adapted to give super-polynomial lower bounds on the sample size required for weak learning. Thus, we focus on classes C n whose Vapnik-Chervonenkis dimension is polynomial in n. <p> fi p and 0 fl 1, the following inequalities hold: Pr [S fim] e 2m (pfi) 2 Pr [S mp (1 + fl)] e fl 2 mp=3 3 Previous Work In the strong learning model, a major contribution to the understanding of sample complexity was made by Blumer et al. <ref> [2] </ref>. Building on the work of Vapnik and Chervonenkis [17], they proved that the number of examples required for strongly learning a concept class C n is (vcd (C n )) (ignoring dependence on * and ffi). <p> We now demonstrate that for concept classes with polynomial Vapnik-Chervonenkis dimension, the lower bound of Blumer et al. <ref> [2] </ref> breaks down in the weak learning model. <p> Thus, for fixed ffi, O (1) examples suffice for weak learning against target distributions over small support sets; this should be contrasted with the lower bound of (vcd (C n )) for the same class of distributions in the strong learning model <ref> [2] </ref>.
Reference: [3] <author> H. Chernoff. </author> <title> A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 23 </volume> <pages> 493-509, </pages> <year> 1952. </year>
Reference-contexts: The first bound stated, Hoeffding's inequality [11], will be used whenever p 1=4. However, when p &lt; 1=4 the last two bounds as stated by Angluin and Valiant [1] give better bounds. (See also Chernoff <ref> [3] </ref>, and Erdos and Spencer [5].) 5 Lemma 1 (Chernoff Bounds) Let X 1 ; : : : ; X m be a sequence of m independent Bernoulli trials, each succeeding with probability p.
Reference: [4] <author> Bonnie Eisenberg and Ronald L. Rivest. </author> <title> On the sample complexity of pac-learning using random and chosen examples. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 154-162. </pages> <publisher> Morgan Kaufmann, </publisher> <month> August </month> <year> 1990. </year>
Reference-contexts: Similar issues have recently been investigated in Euclidean domains by Eisenberg and Rivest <ref> [4] </ref>. 11 Toward a Characterization of Weak Sample Complexity As we have mentioned, it is well-known that the sample size required for strong learning is characterized by the Vapnik-Chervonenkis dimension.
Reference: [5] <author> P. Erdos and J. Spencer. </author> <title> Probabilistic Methods in Combinatorics. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1974. </year>
Reference-contexts: The first bound stated, Hoeffding's inequality [11], will be used whenever p 1=4. However, when p &lt; 1=4 the last two bounds as stated by Angluin and Valiant [1] give better bounds. (See also Chernoff [3], and Erdos and Spencer <ref> [5] </ref>.) 5 Lemma 1 (Chernoff Bounds) Let X 1 ; : : : ; X m be a sequence of m independent Bernoulli trials, each succeeding with probability p. Let S = X 1 + + X m be the random variable describing the total number of successes.
Reference: [6] <author> William Feller. </author> <title> An Introduction to Probability and Its Applications, volume 1. </title> <publisher> John Wiley and Sons, </publisher> <address> third edition, </address> <year> 1968. </year>
Reference-contexts: But the probability that at least d (n)=2 tails occur in a sequence of d (n) q d (n) coin flips is at least ffi 0 for some constant 0 &lt; ffi 0 &lt; 1 (for example, see Feller <ref> [6] </ref>). Letting tails represent points in T S on which h is incorrect, and applying an averaging argument, we see that there must exist some c 2 C 0 n for which A has probability at least ffi 0 of failing to output a hypothesis of accuracy 1=2 on D.
Reference: [7] <author> Yoav Freund. </author> <title> Boosting a weak learning algorithm by majority. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 202-216. </pages> <publisher> Morgan Kaufmann, </publisher> <month> August </month> <year> 1990. </year> <month> 22 </month>
Reference-contexts: More precisely, he gives an efficient strong learning algorithm for C that uses an efficient weak learning algorithm for C as a subroutine. Subsequently, Freund <ref> [7, 8] </ref> has given a different technique for converting a weak learning algorithm into a strong learning algorithm. Combining this result with the lower bound provided by Blumer et al., one obtains an initial lower bound on weak learning sample complexity. <p> Under this encoding, the size s (n) of the output hypothesis in bits is m (n) times the number of bits needed to encode each example. Schapire [14] and Freund <ref> [7, 8] </ref> describe techniques for converting this weak learning algorithm into a strong learning algorithm A 0 outputting hypotheses of size O s (n) (p (n)) ff (log (1=*)) fi (1) for some constants ff and fi.
Reference: [8] <author> Yoav Freund. </author> <title> An improved boosting algorithm and its implications on learning complex-ity. </title> <booktitle> In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 391-398, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: More precisely, he gives an efficient strong learning algorithm for C that uses an efficient weak learning algorithm for C as a subroutine. Subsequently, Freund <ref> [7, 8] </ref> has given a different technique for converting a weak learning algorithm into a strong learning algorithm. Combining this result with the lower bound provided by Blumer et al., one obtains an initial lower bound on weak learning sample complexity. <p> Under this encoding, the size s (n) of the output hypothesis in bits is m (n) times the number of bits needed to encode each example. Schapire [14] and Freund <ref> [7, 8] </ref> describe techniques for converting this weak learning algorithm into a strong learning algorithm A 0 outputting hypotheses of size O s (n) (p (n)) ff (log (1=*)) fi (1) for some constants ff and fi.
Reference: [9] <author> David Haussler, Michael Kearns, Nick Littlestone, and Manfred K. Warmuth. </author> <title> Equivalence of models for polynomial learnability. </title> <journal> Information and Computation, </journal> <volume> 25(2) </volume> <pages> 129-161, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: This technique is described by Haussler et al. <ref> [9] </ref>.) Note that the output hypothesis can be encoded by the m (n) examples on which A was successfully trained. Under this encoding, the size s (n) of the output hypothesis in bits is m (n) times the number of bits needed to encode each example. <p> Next we use hypothesis testing (as described by Haussler et al. <ref> [9] </ref>) to estimate the error of each hypothesis and output the one with the lowest error.
Reference: [10] <author> David Helmbold, Robert Sloan, and Manfred K. Warmuth. </author> <title> Learning integer lattices. </title> <journal> SIAM Journal on Computing, </journal> <volume> 21(2) </volume> <pages> 240-266, </pages> <year> 1992. </year>
Reference-contexts: Thus, each concept is just the parity function computed on a subset of the n variables. It is known that P n is learnable in polynomial time <ref> [10, 15] </ref>. It is not hard to show that vcd (P n ) = n. Also, note that each concept in P n can be represented by a vector in f0; 1g n .
Reference: [11] <author> Wassily Hoeffding. </author> <title> Probability inequalities for sums of bounded random variables. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 58(301) </volume> <pages> 13-30, </pages> <month> March </month> <year> 1963. </year>
Reference-contexts: Finally, to compute the sample sizes needed for several of our algorithms we use the following versions of Chernoff bounds. The first bound stated, Hoeffding's inequality <ref> [11] </ref>, will be used whenever p 1=4.
Reference: [12] <author> Michael Kearns and Leslie Valiant. </author> <title> Cryptographic limitations on learning Boolean formulae and finite automata. </title> <booktitle> In Proceedings of the Twenty First Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 433-444, </pages> <month> May </month> <year> 1989. </year> <note> To appear in JACM. </note>
Reference-contexts: In this setting polynomial time means polynomial in n, 1=* and 1=ffi. The support set of a distribution D is the set of all x such that D (x) &gt; 0. In the related weak learning model <ref> [12] </ref>, we drop the demand for accuracy 1* and simply ask that the hypothesis h have accuracy at least 1 2 + 1 p (n) for some polynomial p (n). Thus we ask only for a small correlation in the underlying distribution.
Reference: [13] <author> Leonard Pitt and Leslie G. Valiant. </author> <title> Computational limitations on learning from examples. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 35(4) </volume> <pages> 965-984, </pages> <year> 1988. </year>
Reference-contexts: This should be contrasted with results showing that the computational complexity of learning can sometimes be reduced by using a hypothesis space that is more powerful than the target class (see for example Pitt and Valiant <ref> [13] </ref>). 7 Almost Every Class Has Weak Sample Complexity (vcd (C n )) We have seen that the power of using a randomized hypothesis may be dramatic in some cases for weak learning sample size.
Reference: [14] <author> Robert E. Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-227, </pages> <year> 1990. </year>
Reference-contexts: A second motivation for our study is the recent result of Schapire <ref> [14] </ref> showing that a concept class is weakly learnable in polynomial time if and only if it is strongly learnable in polynomial time. <p> However, we use a probabilistic construction to obtain this result, and the resulting class C n , while having small Vapnik-Chervonenkis dimension, does not have small (size polynomial in n) circuits, and thus is not learnable in polynomial time by results of Schapire <ref> [14] </ref>. <p> In the polynomial-time setting, Schapire <ref> [14] </ref> proved that a concept class C can be weakly learned in polynomial time if and only if it can be strongly learned in polynomial time. More precisely, he gives an efficient strong learning algorithm for C that uses an efficient weak learning algorithm for C as a subroutine. <p> Under this encoding, the size s (n) of the output hypothesis in bits is m (n) times the number of bits needed to encode each example. Schapire <ref> [14] </ref> and Freund [7, 8] describe techniques for converting this weak learning algorithm into a strong learning algorithm A 0 outputting hypotheses of size O s (n) (p (n)) ff (log (1=*)) fi (1) for some constants ff and fi.
Reference: [15] <author> Robert Hal Sloan. </author> <title> Computational Learning Theory: New Models and Algorithms. </title> <type> PhD thesis, </type> <institution> MIT Dept. of Electrical Engineering and Computer Science, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Thus, each concept is just the parity function computed on a subset of the n variables. It is known that P n is learnable in polynomial time <ref> [10, 15] </ref>. It is not hard to show that vcd (P n ) = n. Also, note that each concept in P n can be represented by a vector in f0; 1g n .
Reference: [16] <author> Leslie Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: A necessary and sufficient characterization of weak learning sample complexity remains an interesting open problem. 4 2 Definitions We begin by describing the distribution-free learning model introduced by Valiant <ref> [16] </ref>. The learner is attempting to infer an unknown target concept c chosen from some known concept class C.
Reference: [17] <author> V. N. Vapnik and A. Ya. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and Its Applications, </title> <address> XVI(2):264-280, </address> <year> 1971. </year> <month> 23 </month>
Reference-contexts: Building on the work of Vapnik and Chervonenkis <ref> [17] </ref>, they proved that the number of examples required for strongly learning a concept class C n is (vcd (C n )) (ignoring dependence on * and ffi). <p> then the probability that there is some ~v h such that d N (~v c ; ~v h ) &lt; 1=2 for a fraction 1=2 + fi of the ~v c drawn is at most 4 (2 (k1)nc 1 N )e fi 2 2 (k1)n1 =8 by Vapnik and Chervonenkis <ref> [17] </ref>. Note that this is a generalized use of the Vapnik-Chervonenkis dimension, which is usually used to prove uniform convergence of some concept class over a domain set.
References-found: 17

