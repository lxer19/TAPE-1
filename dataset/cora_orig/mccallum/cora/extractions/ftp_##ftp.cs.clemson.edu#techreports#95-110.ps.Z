URL: ftp://ftp.cs.clemson.edu/techreports/95-110.ps.Z
Refering-URL: http://www.cs.clemson.edu/html/research/techrpt.shtml
Root-URL: http://www.cs.clemson.edu
Title: Psychological Complexity Measure at the Domain Analysis Phase of an Object-Oriented System  
Author: Sanjay Kamath 
Date: December 6, 1994  
Abstract: A psychological complexity measure developed at Clemson University, called the Permitted Interactions (PI) measure, uses design information to calculate the psychological complexity as a measure of effort. However there is a general demand for measures that can use information present at earlier phases. Following this requirement the measure reported here estimates complexity at the domain analysis phase which is the earliest development phase in an object-oriented software process. Psychological complexity relates to the cognitive load imposed on the developers of the software system which is in turn directly related to the time to completion of the development process. We define a measure called Permitted Collaborations (PC) which measures the psycholgical complexity from a set of CRC cards that represents a domain. The measure can however be used on any object-oriented representation like OMT diagrams that provide equivalent information. The measure has been found to be responsive to system changes and we show that the measure can aid the developers in selecting the right class decomposition when there are conflicting choices. Empirical validation is done to correlate the values given by PI and PC for a set of classes and the result show positive correlation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. H. Abbott, T. D. Korson, and J. D. McGregor. </author> <title> A proposed design complexity metric for object-oriented development. </title> <type> Technical Report TR 94-105, </type> <institution> Clemson University, </institution> <year> 1994. </year>
Reference-contexts: The result of this effort is a measure model [16] that offers an approach to managing measures of the 7 o-o software process and product. As part of this measurement model, a complexity measure called Permitted Interactions (PI) has been developed in Clemson <ref> [1] </ref>, and it uses class specifications available at the design phase as its input. PI defines the concept of data interactions to model the complexity of a class. <p> Thus at the outset we make an assumption that the experts can be used as oracle in predicting the relative complexities. We utilize a panel to be certain that the results generalize. For this empirical investigation we use the data from Abbott <ref> [1] </ref>. For the validation of PI, he created several example designs, see Appendix B (The designs are given as C++ class headers for the classes of the design.) The designs are categorized based on their domains of knowledge. There are 8 such categories and and 9 sets of alternatives. <p> There are 20 such comparisons in total. The second has the collective opinion of the panel of o-o experts. For more information on the experts used and the strategy of calculation of the collective opinion from the individual opinions the reader is referred to <ref> [1] </ref>. The third and the fourth columns refer to the preference given by PC and PI respectively. The check marks near the PC and PI preferences indicate the times when the their preference matched or did not match that given by the experts.
Reference: [2] <author> A. J. Albrecht and J. E. Gaffney. </author> <title> Software function, source lines of code, and development effort prediction. </title> <journal> IEEE Trans. on Software Eng., </journal> <volume> 9(6) </volume> <pages> 639-647, </pages> <month> November </month> <year> 1983. </year>
Reference-contexts: However, even these measures suffer from severe criticisms as cited in Sellers [19], Weyuker [22] and Shepperd, et al. [20]. Other researchers have tried to measure effort, error rates and size using statistical models. Boehm [5] and Albrecht <ref> [2] </ref> are notable examples. A recent compendium on software measures based on a critical viewpoint is found in Roche [18]. Fenton, et. al. [11] and Baker, et al. [3] have stressed the importance of basing measures on scientific ground rules.
Reference: [3] <author> A. L. Baker and et. al. </author> <title> A philosophy for software measurement. </title> <journal> J. Systems and Software, </journal> <volume> 12(99) </volume> <pages> 277-281, </pages> <year> 1990. </year>
Reference-contexts: Other researchers have tried to measure effort, error rates and size using statistical models. Boehm [5] and Albrecht [2] are notable examples. A recent compendium on software measures based on a critical viewpoint is found in Roche [18]. Fenton, et. al. [11] and Baker, et al. <ref> [3] </ref> have stressed the importance of basing measures on scientific ground rules.
Reference: [4] <author> W. Beck and K. Cuningham. </author> <title> A laboratory for teaching object-oriented thinking. </title> <booktitle> In OOPSLA, </booktitle> <pages> pages 1-6, </pages> <year> 1989. </year>
Reference-contexts: The CRC card approach (Beck, et al. <ref> [4] </ref>), is one way of documenting the result of the domain analysis phase, and we will be using CRC cards as the source input to the measure reported in this paper. Measurement theory is a field whose tenets are uniformly applicable irrespective of the field of application. <p> In o-o analysis, the entities are more fully defined and grouped into classes. The name mechanism loosely defines the data structures and algorithms that supports a particular responsibility. As mentioned earlier domain analysis information can be recorded using a set of index cards called CRC cards (See Figure 2) <ref> [4] </ref>. CRC is an acronym for Class-Responsibilities-Collaborators. Each card presents information about an entity in the domain of knowledge. Most of these en 8 tities will correspond to implemented classes in the resulting software system. The card lists the responsibilities of the entity and the list of collaborators if any.
Reference: [5] <author> B. W. </author> <title> Boehm. </title> <journal> Software engineering economics. IEEE Trans. on Software Eng., </journal> <volume> 10(1) </volume> <pages> 4-21, </pages> <year> 1984. </year>
Reference-contexts: These measures have been hailed as "classical" and extensively cited in journals. However, even these measures suffer from severe criticisms as cited in Sellers [19], Weyuker [22] and Shepperd, et al. [20]. Other researchers have tried to measure effort, error rates and size using statistical models. Boehm <ref> [5] </ref> and Albrecht [2] are notable examples. A recent compendium on software measures based on a critical viewpoint is found in Roche [18]. Fenton, et. al. [11] and Baker, et al. [3] have stressed the importance of basing measures on scientific ground rules.
Reference: [6] <author> D. N. Card and W. W. </author> <title> Agresti. Measuring software design complexity. </title> <journal> J. Systems and Software, </journal> <volume> 8(1) </volume> <pages> 185-197, </pages> <year> 1988. </year>
Reference-contexts: We arbitrarily choose m = 2 and thus the interface complexity of a class is given by R 2 . At least two such occasions exist in literature, (Henry and Kafura [13], Card and Agresti <ref> [6] </ref>) when a term has been squared to preserve the kind of relationship that exists here between interface complexity and R. At this point we choose to call the interface complexity calculated this way as the interface size.
Reference: [7] <author> J. C. Cherniavsky and C. H. Smith. </author> <title> On weyuker's axioms for software complexity measures. </title> <journal> IEEE Trans. on Software Eng., </journal> <volume> 17(6) </volume> <pages> 636-638, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Axiomatic Properties Weyuker [22] listed a set of properties (necessary but not sufficient) that are desired of a complexity measure for procedural programs. Although these properties have been criticized 4 , Cherniavsky <ref> [7] </ref>, they allow for rigorous theoretical validation. As shown by Weyuker, with such properties it is possible to filter out measurements with undesirable properties. In this section we will define as set of properties which have to be compulsorily satisfied by an ideal complexity measure.
Reference: [8] <author> S. R. Chidamber and C. F. Kemerer. </author> <title> A metrics suite for object-oriented design. </title> <journal> IEEE Trans. on Software Eng., </journal> <volume> 20(6) </volume> <pages> 476-493, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Halstead [12] estimated programming effort by studying the cognitive processes of programmers based on the findings of classical psychology. Henry and Kafura [13] used the concept of fan-in and fan-out on procedures to measure the information flow complexity. Chidamber and Kemerer <ref> [8] </ref> used Bunge's ontology as a model to define complexity based on the numerosity of properties. 3.3 Collaborations Model Our objective is to measure the psychological complexity of classes given analysis level information of the type presented on CRC cards.
Reference: [9] <author> S. D. Conte, H. E. Dunsmore, and V. Y. Shen. </author> <title> Software Engineering Metrics and Models. </title> <publisher> Benjamin Cummings Publishing Company, Inc., </publisher> <address> Memlo Park, CA, USA, </address> <year> 1986. </year>
Reference-contexts: The technique used is similar to that axiomatic techniques in Fenton, et al. [11]. We make clear the applications of the measure and invite users of the measure to develop estimation models based on their own environment. Our measure is designed to determine the psychological complexity (Conte, et al. <ref> [9] </ref>) of a system representation at the analysis phase in an attempt to estimate the effort. More precisely, it 6 is a ordinal measure of the complexity based on which we can impose a ranking on a set of classes or set of systems.
Reference: [10] <author> N. Fenton. </author> <title> Software measurement: A necessary scientific basis. </title> <journal> IEEE Trans. on Software Eng., </journal> <volume> 20(3) </volume> <pages> 199-206, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Measurement theory is a field whose tenets are uniformly applicable irrespective of the field of application. Any measure that is developed must fulfill the basic requirements of measurement theory or else it is not a valid measure. Fenton <ref> [10] </ref> and Roche [18] notice that there are many software measures available in literature that do not satisfy these basic principles. Fenton refers to such "measures" as metrics to make the difference clear. Software measures have not been successful estimators and software continues to be expensive and bug ridden. <p> A general and all encompassing view of software complexity is not a good attribute to measure since it is not always possible to come up with empirical relations between software products that are measured <ref> [10] </ref>. It would be a sounder proposition to measure a specific complexity like the complexity of testing a product or the complexity of development of a product, or, the complexity of maintenance of a product.
Reference: [11] <author> N. Fenton and A. Melton. </author> <title> Deriving structurally based measures. </title> <journal> J. Systems and Software, </journal> <volume> 12(99) </volume> <pages> 177-187, </pages> <year> 1990. </year>
Reference-contexts: We endeavor to properly address all of these issues. Our measure is based on the principles of measurements and on a model where the assumptions have been stated as clearly as possible. The technique used is similar to that axiomatic techniques in Fenton, et al. <ref> [11] </ref>. We make clear the applications of the measure and invite users of the measure to develop estimation models based on their own environment. <p> Other researchers have tried to measure effort, error rates and size using statistical models. Boehm [5] and Albrecht [2] are notable examples. A recent compendium on software measures based on a critical viewpoint is found in Roche [18]. Fenton, et. al. <ref> [11] </ref> and Baker, et al. [3] have stressed the importance of basing measures on scientific ground rules.
Reference: [12] <author> M. H. Halstead. </author> <title> Elements of Software Science. </title> <publisher> Elsevier North-Holland Pub. Co., </publisher> <address> New York, NY, </address> <year> 1977. </year>
Reference-contexts: The above phases are based on more traditional software processes, and for o-o process we could have measures based on other phases, like the domain analysis phase. McCabe [15], Halstead <ref> [12] </ref>, and Henry, et al. [13] have developed complexity measures for traditional programs based on different complexity models. These measures have been hailed as "classical" and extensively cited in journals. However, even these measures suffer from severe criticisms as cited in Sellers [19], Weyuker [22] and Shepperd, et al. [20]. <p> Thus if A 1 ; : : : ; A n represent that attributes and the output attribute is O, the measure is some mathematical function f on the attributes. O = f (A 1 ; : : : ; A n ) As an example, Halstead's <ref> [12] </ref> measure is based on the count and occurrence of operators and operands. They form the direct attributes. These direct attributes are believed to be relevant and contributing significantly to the indirect attribute in quest, which is the cognitive effort taken by the programmers in developing a program. <p> For almost two decades researchers have been trying to model various viewpoints of complexity using different complexity models. McCabe [15] modeled the complexity of testing of FORTRAN programs using the cyclomatic number which computes the number of independent paths in a program. Halstead <ref> [12] </ref> estimated programming effort by studying the cognitive processes of programmers based on the findings of classical psychology. Henry and Kafura [13] used the concept of fan-in and fan-out on procedures to measure the information flow complexity.
Reference: [13] <author> S. Henry and D. Kafura. </author> <title> Software structure metrics based on information flow. </title> <journal> IEEE Trans. on Software Eng., </journal> <volume> 7(5) </volume> <pages> 510-518, </pages> <month> September </month> <year> 1981. </year>
Reference-contexts: The above phases are based on more traditional software processes, and for o-o process we could have measures based on other phases, like the domain analysis phase. McCabe [15], Halstead [12], and Henry, et al. <ref> [13] </ref> have developed complexity measures for traditional programs based on different complexity models. These measures have been hailed as "classical" and extensively cited in journals. However, even these measures suffer from severe criticisms as cited in Sellers [19], Weyuker [22] and Shepperd, et al. [20]. <p> McCabe [15] modeled the complexity of testing of FORTRAN programs using the cyclomatic number which computes the number of independent paths in a program. Halstead [12] estimated programming effort by studying the cognitive processes of programmers based on the findings of classical psychology. Henry and Kafura <ref> [13] </ref> used the concept of fan-in and fan-out on procedures to measure the information flow complexity. <p> Accordingly we choose to measure the interface complexity as R raised to a constant power m. We arbitrarily choose m = 2 and thus the interface complexity of a class is given by R 2 . At least two such occasions exist in literature, (Henry and Kafura <ref> [13] </ref>, Card and Agresti [6]) when a term has been squared to preserve the kind of relationship that exists here between interface complexity and R. At this point we choose to call the interface complexity calculated this way as the interface size.
Reference: [14] <author> K. Lieberherr, I. Holland, and A. Riel. </author> <title> Object-oriented programming: A objective sense of style. </title> <booktitle> In OOPSLA, </booktitle> <pages> pages 323-334, </pages> <year> 1988. </year>
Reference-contexts: Then, in the rest of the section we will define some axiomatic properties which have to be satisfied by any psychological complexity measure. PC is then validated against these properties. Object-Oriented Features Object-oriented designs have certain features that have been found to reduce the complexity of the software [21], <ref> [14] </ref>. Below we list the features and explain the role of the measure in those situations. 1. Inheritance: Whenever a class has to be developed we may have a choice of inheriting from an existing class or creating one from scratch.
Reference: [15] <author> T. J. McCabe. </author> <title> A complexity measure. </title> <journal> IEEE Trans. on Software Eng., </journal> <volume> 2(4) </volume> <pages> 308-320, </pages> <month> December </month> <year> 1976. </year>
Reference-contexts: Product measures are further classified based on the phase at which they are computed, namely code-based, design-based and specification-based measures. The above phases are based on more traditional software processes, and for o-o process we could have measures based on other phases, like the domain analysis phase. McCabe <ref> [15] </ref>, Halstead [12], and Henry, et al. [13] have developed complexity measures for traditional programs based on different complexity models. These measures have been hailed as "classical" and extensively cited in journals. <p> The definitions of the viewpoint of complexity may be different for different organizations but nevertheless they have to be defined before the measure is searched for. For almost two decades researchers have been trying to model various viewpoints of complexity using different complexity models. McCabe <ref> [15] </ref> modeled the complexity of testing of FORTRAN programs using the cyclomatic number which computes the number of independent paths in a program. Halstead [12] estimated programming effort by studying the cognitive processes of programmers based on the findings of classical psychology.
Reference: [16] <author> J. D. McGregor. </author> <title> Managing metrics in an iterative incremental development environment. </title> <type> Technical Report TR 94-110, </type> <institution> Clemson University, </institution> <year> 1994. </year>
Reference-contexts: At Clemson investigation has been going on to come up with a suite of measures for o-o software developers. This research is due to the co-operative effort between COMSOFT, IBM, BNR and Clemson University. The result of this effort is a measure model <ref> [16] </ref> that offers an approach to managing measures of the 7 o-o software process and product. As part of this measurement model, a complexity measure called Permitted Interactions (PI) has been developed in Clemson [1], and it uses class specifications available at the design phase as its input.
Reference: [17] <author> J. D. McGregor and D. A. Sykes. </author> <title> Object-oriented Software Development: Engineering Software for Reuse. </title> <publisher> Van Nostrand-Reinhold, </publisher> <address> New York, NY, </address> <year> 1992. </year> <month> 36 </month>
Reference-contexts: Get the current balance. 1. Set and get the account object. Responsibilities: TransactionLog Class: CheckingAccount, SavingsAccount TransactionSuperclass (es) : Collaborator (s) : BalanceInquiryTransaction The CRC card representation of a system shows classes arranged in a inheritance hierarchy. Inheritance mechanism of the o-o paradigm is used to represent type-subtype relationships <ref> [17] </ref> existing between entities. A subclass or a child class inherits from another class which is called its superclass or parent class. In Figure 2, BalanceInquiryTransaction is the child class inheriting from the parent Transaction class. <p> For our measurement model the number of responsibilities and the number of collaborators of classes form the direct attributes. 11 Classes relate to each other via three kinds of relationships: composition, association and in-heritance <ref> [17] </ref>. CRC cards record all these three relationships, but under only two categories: collaborations and inheritance. Both compositions and associations are recorded as collaborations. A typical class and its relationships is depicted in Figure 4. Class E collaborates with three classes E 1 , E 2 and E 3 .
Reference: [18] <author> J. M. Roche. </author> <title> Software metrics and measurement principles. </title> <journal> ACM SIGSOFT Software Eng. Notes, </journal> <volume> 19(1) </volume> <pages> 77-85, </pages> <month> Jan </month> <year> 1994. </year>
Reference-contexts: Measurement theory is a field whose tenets are uniformly applicable irrespective of the field of application. Any measure that is developed must fulfill the basic requirements of measurement theory or else it is not a valid measure. Fenton [10] and Roche <ref> [18] </ref> notice that there are many software measures available in literature that do not satisfy these basic principles. Fenton refers to such "measures" as metrics to make the difference clear. Software measures have not been successful estimators and software continues to be expensive and bug ridden. <p> Other researchers have tried to measure effort, error rates and size using statistical models. Boehm [5] and Albrecht [2] are notable examples. A recent compendium on software measures based on a critical viewpoint is found in Roche <ref> [18] </ref>. Fenton, et. al. [11] and Baker, et al. [3] have stressed the importance of basing measures on scientific ground rules.
Reference: [19] <author> B. H. Sellers. </author> <title> Modularization and mccabe's cyclomatic complexity. </title> <journal> Comm. of the ACM, </journal> <volume> 35(12) </volume> <pages> 17-19, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: McCabe [15], Halstead [12], and Henry, et al. [13] have developed complexity measures for traditional programs based on different complexity models. These measures have been hailed as "classical" and extensively cited in journals. However, even these measures suffer from severe criticisms as cited in Sellers <ref> [19] </ref>, Weyuker [22] and Shepperd, et al. [20]. Other researchers have tried to measure effort, error rates and size using statistical models. Boehm [5] and Albrecht [2] are notable examples. A recent compendium on software measures based on a critical viewpoint is found in Roche [18].
Reference: [20] <author> M. Shepperd and D. Ince. </author> <title> Derivation and Validation of Software Metrics. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1993. </year>
Reference-contexts: These measures have been hailed as "classical" and extensively cited in journals. However, even these measures suffer from severe criticisms as cited in Sellers [19], Weyuker [22] and Shepperd, et al. <ref> [20] </ref>. Other researchers have tried to measure effort, error rates and size using statistical models. Boehm [5] and Albrecht [2] are notable examples. A recent compendium on software measures based on a critical viewpoint is found in Roche [18]. <p> As shown by Weyuker, with such properties it is possible to filter out measurements with undesirable properties. In this section we will define as set of properties which have to be compulsorily satisfied by an ideal complexity measure. Shepperd <ref> [20] </ref> proposed that the concatenation operations (operations used to recursively build a system) used in the properties have to be formally specified before the properties are specified. We already have provided for the algebraic specifications of the concatenation operations uparrow and ! in Appendix A.
Reference: [21] <author> W. T. Tsai. </author> <title> A perspective on software maintenance. </title> <booktitle> In OOPSLA, </booktitle> <pages> pages 365-366, </pages> <year> 1988. </year>
Reference-contexts: Then, in the rest of the section we will define some axiomatic properties which have to be satisfied by any psychological complexity measure. PC is then validated against these properties. Object-Oriented Features Object-oriented designs have certain features that have been found to reduce the complexity of the software <ref> [21] </ref>, [14]. Below we list the features and explain the role of the measure in those situations. 1. Inheritance: Whenever a class has to be developed we may have a choice of inheriting from an existing class or creating one from scratch.
Reference: [22] <author> E. J. Weyuker. </author> <title> Evaluating software complexity measures. </title> <journal> IEEE Trans. on Software Eng., </journal> <volume> 14(9) </volume> <pages> 1357-1365, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: McCabe [15], Halstead [12], and Henry, et al. [13] have developed complexity measures for traditional programs based on different complexity models. These measures have been hailed as "classical" and extensively cited in journals. However, even these measures suffer from severe criticisms as cited in Sellers [19], Weyuker <ref> [22] </ref> and Shepperd, et al. [20]. Other researchers have tried to measure effort, error rates and size using statistical models. Boehm [5] and Albrecht [2] are notable examples. A recent compendium on software measures based on a critical viewpoint is found in Roche [18]. <p> Strong Cohesion: In section 5.1 we have shown a case when partitioning of a weakly cohesive class has resulted in a lower overall complexity value. Axiomatic Properties Weyuker <ref> [22] </ref> listed a set of properties (necessary but not sufficient) that are desired of a complexity measure for procedural programs. Although these properties have been criticized 4 , Cherniavsky [7], they allow for rigorous theoretical validation.
Reference: [23] <author> R. Wirfs-Brock, B. Wilkerson, and L. Weiner. </author> <title> Designing Object-Oriented Software. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1990. </year> <month> 37 </month>
Reference-contexts: We then used Kendall's t statistic to determine the correlation between the values. We conducted the same experiment on another set of sample classes. These classes were borrowed from Wirfs-Brock's <ref> [23] </ref> book, and represent a possible analysis model of the Automated Teller Machine (ATM) system. Each of the classes and their PI and PC values are listed in Table 4.
References-found: 23

