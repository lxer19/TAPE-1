URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P534.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts95.htm
Root-URL: http://www.mcs.anl.gov
Email: @mcs.anl.gov  
Title: I/O Characterization of a Portable Astrophysics Application on the IBM SP and Intel Paragon  
Author: Rajeev Thakur Ewing Lusk William Gropp fthakur, lusk, groppg 
Date: Revised October 1995  
Address: 9700 S. Cass Avenue Argonne, IL 60439  
Affiliation: Mathematics and Computer Science Division Argonne National Laboratory  
Pubnum: Preprint MCS-P534-0895  
Abstract: Many large-scale applications on parallel machines are bottlenecked by the I/O performance rather than the CPU or communication performance of the system. To improve the I/O performance, it is first necessary for system designers to understand the I/O requirements of various applications. This paper presents the results of a study of the I/O characteristics and performance of a real, I/O-intensive, portable, parallel application in astrophysics, on two different parallel machines|the IBM SP and the Intel Paragon. We instrumented the source code to record all I/O activity, and analyzed the resulting trace files. Our results show that, for this application, the I/O consists of fairly large writes, and writing data to files is faster on the Paragon, whereas opening and closing files are faster on the SP. We also discuss how the I/O performance of this application could be improved; particularly, we believe that this application would benefit from using collective I/O. fl This work was supported by the Mathematical, Information, and Computational Sciences Division subprogram of the Office of Computational and Technology Research, U.S. Department of Energy, under Contract W-31-109-Eng-38; and by the Scalable I/O Initiative, a multiagency project funded by the Advanced Research Projects Agency (contract number DABT63-94-C-0049), the Department of Energy, the National Aeronautics and Space Administration, and the National Science Foundation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Applications Working Group of the Scalable I/O Initiative. </institution> <note> Preliminary Survey of I/O Intensive Applications. Scalable I/O Initiative Working Paper Number 1. On World-Wide Web at http://www.ccsf.caltech.edu/SIO/SIO apps.ps, </note> <year> 1994. </year>
Reference-contexts: Many such applications also deal with large quantities of data, and hence have significant I/O requirements. The Applications Working Group of the Scalable I/O Initiative has compiled a list of several data-intensive applications and their I/O requirements <ref> [1] </ref>. The I/O performance of parallel computers has always been orders of magnitude lower than the CPU and communication performance because improvements in the I/O subsystem have not kept pace with improvements in the CPU and communication subsystems.
Reference: [2] <author> R. Aydt. </author> <title> The Pablo Self-Defining Data Format. </title> <type> Technical report, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: The Pablo environment includes instrumentation libraries that can record timestamped event traces, a performance data meta-format called Self-Defining Data Format (SDDF) <ref> [2] </ref> and associated library, a parser for generating instrumented SPMD source code, and tools for analyzing and graphically displaying the traces. Pablo also includes a special set of tools for capturing and analyzing I/O performance data [3]. <p> Upshot expects the logfiles to be in a specific format described in [16]. The traces created by Pablo are in a different format called Self-Defining Data Format (SDDF) <ref> [2] </ref>. Hence, we had to first convert Pablo trace files to the format required by Upshot.
Reference: [3] <author> R. Aydt. </author> <title> A User's Guide to Pablo I/O Instrumentation. </title> <type> Technical report, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: Pablo also includes a special set of tools for capturing and analyzing I/O performance data <ref> [3] </ref>. The I/O extension to the Pablo trace library provides the user with a set of high-level routines that generate trace events corresponding to the I/O operations taking place in a program.
Reference: [4] <author> M. Baker, J. Hartman, M. Kupfer, K. Shirriff, and J. Ousterhout. </author> <title> Measurements of a Distributed File System. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 198-212, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Ousterhout et al. [24] and Floyd and Ellis [12] studied file access patterns on 1 Unix workstations. Baker et al. <ref> [4] </ref> analyzed the user-level file access patterns and caching behavior of the Sprite distributed file system. Smith [32] studied the file access patterns on IBM mainframes at the Stanford Linear Accelerator Center.
Reference: [5] <author> S. Baylor and C. Wu. </author> <title> Parallel I/O Workload Characteristics Using Vesta. </title> <booktitle> In IPPS '95 Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 16-29, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: They found a wide variety of access patterns, including both read-intensive and write-intensive phases, large as well as small request sizes, and both sequential and irregular access patterns. Baylor and Wu <ref> [5] </ref> performed a study of the I/O characteristics of four parallel applications on an IBM SP using the Vesta parallel file system. They found I/O request rates on the order of hundreds of requests per second, mainly small request sizes, and strong temporal and spatial locality.
Reference: [6] <author> T. Bogdan, F. Cattaneo, and A. Malagoli. </author> <title> On the Generation of Sound by Turbulent Convection: I. A Numerical Experiment. </title> <journal> The Astrophysical Journal, </journal> <volume> 407 </volume> <pages> 316-329, </pages> <year> 1993. </year>
Reference-contexts: The computational algorithm consists mainly of two independent components|a finite difference higher-order Godunov method for compressible hydrodynamics, and a Crank-Nicholson-based multigrid scheme. The two algorithms are combined by using a time-splitting technique. Further details about the application are given in <ref> [21, 6, 7] </ref>. The code has been implemented in Fortran 77 and C. The Chameleon library is used for both message passing and I/O [15]. Chameleon provides a uniform interface to whatever native message-passing library is available on a given machine, and also provides a portable high-level interface for I/O.
Reference: [7] <author> F. Cattaneo, N. Brummel, J. Toomre, A. Malagoli, and N. Hurlburt. </author> <title> Turbulent Compressible Convection. </title> <journal> The Astrophysical Journal, </journal> <volume> 370 </volume> <pages> 282-294, </pages> <year> 1991. </year> <month> 18 </month>
Reference-contexts: The computational algorithm consists mainly of two independent components|a finite difference higher-order Godunov method for compressible hydrodynamics, and a Crank-Nicholson-based multigrid scheme. The two algorithms are combined by using a time-splitting technique. Further details about the application are given in <ref> [21, 6, 7] </ref>. The code has been implemented in Fortran 77 and C. The Chameleon library is used for both message passing and I/O [15]. Chameleon provides a uniform interface to whatever native message-passing library is available on a given machine, and also provides a portable high-level interface for I/O.
Reference: [8] <author> P. Crandall, R. Aydt, A. Chien, and D. Reed. </author> <title> Input-Output Characteristics of Scalable Parallel Applications. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: The results of these two studies are compared and contrasted in [23]. They found that file sizes were large, request sizes were fairly small, data was accessed in sequence but with strides, and I/O was dominated by writes. Crandall et al. <ref> [8] </ref> instrumented and analyzed the I/O characteristics of three parallel applications on the Intel Paragon at Caltech. They found a wide variety of access patterns, including both read-intensive and write-intensive phases, large as well as small request sizes, and both sequential and irregular access patterns.
Reference: [9] <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina. </author> <title> Architectural Requirements of Parallel Scientific Applications with Explicit Communication. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <year> 1993. </year>
Reference-contexts: Reddy and Banerjee [29] studied the I/O activity of five applications from the Perfect benchmark suite on an Alliant multiprocessor, and found only sequential accesses. Del Rosario and Choudhary [11] provide an informal summary of the I/O requirements of several grand-challenge applications. Cypher et al. <ref> [9] </ref> discuss the I/O requirements of eight parallel scientific applications. Our work differs from previous efforts in that we have performed a comparative study of the I/O characteristics and performance of one portable parallel application on two different parallel machines.
Reference: [10] <author> J. del Rosario, R. Bordawekar, and A. Choudhary. </author> <title> Improved Parallel I/O via a Two-Phase Runtime Access Strategy. </title> <booktitle> In Proceedings of the Workshop on I/O in Parallel Computer Systems at IPPS '93, </booktitle> <pages> pages 56-70, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: This would make better use of the available I/O bandwidth and also eliminate the communication bottleneck. The I/O access pattern of this application suggests that it would benefit from using collective I/O, in the form of either two-phase I/O <ref> [10] </ref>, disk-directed I/O [18], or server-directed collective I/O [31]. In this application, arrays are distributed in a block fashion in two dimensions, resulting in each processor getting a sub-block of the array. The sub-block from each processor needs to written to the file containing the entire array. <p> In this application, arrays are distributed in a block fashion in two dimensions, resulting in each processor getting a sub-block of the array. The sub-block from each processor needs to written to the file containing the entire array. It has been shown in <ref> [10, 18, 31] </ref> that this can be done efficiently using collective I/O. To study the benefits of using collective I/O for this application, we have implemented the Chameleon I/O routines PIWriteDistributedArray and PIReadDistributedArray using two-phase I/O.
Reference: [11] <author> J. del Rosario and A. Choudhary. </author> <title> High Performance I/O for Parallel Computers: Problems and Prospects. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 59-68, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Reddy and Banerjee [29] studied the I/O activity of five applications from the Perfect benchmark suite on an Alliant multiprocessor, and found only sequential accesses. Del Rosario and Choudhary <ref> [11] </ref> provide an informal summary of the I/O requirements of several grand-challenge applications. Cypher et al. [9] discuss the I/O requirements of eight parallel scientific applications.
Reference: [12] <author> R. Floyd and C. Ellis. </author> <title> Directory Reference Patterns in Hierarchical File Systems. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 1(2) </volume> <pages> 238-247, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: In Section 7, we discuss how the I/O performance of this application could be improved, and draw overall conclusions in Section 8. 2 Related Work The file access characteristics of applications on uniprocessor and vector machines have been studied quite extensively. Ousterhout et al. [24] and Floyd and Ellis <ref> [12] </ref> studied file access patterns on 1 Unix workstations. Baker et al. [4] analyzed the user-level file access patterns and caching behavior of the Sprite distributed file system. Smith [32] studied the file access patterns on IBM mainframes at the Stanford Linear Accelerator Center.
Reference: [13] <author> N. Galbreath, W. Gropp, and D. Levine. </author> <title> Applications-Driven Parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 462-471, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: In this case, distributed arrays are read from previously stored files. All I/O is performed using the Chameleon I/O library, which provides a portable set of routines for high-performance I/O that hide the details of the actual implementation from the user <ref> [13] </ref>. The 3 user can select at run-time whether a file is stored as a simple Unix file (for compatibility with other tools) or as a parallel file readable only with the Chameleon I/O library routines (for maximum performance).
Reference: [14] <author> W. Gropp and E. Lusk. </author> <title> User's Guide for the ANL IBM SPx. </title> <note> On World-Wide Web at ftp://info.mcs.anl.gov/pub/ibm sp1/guide-r2.ps.Z, </note> <month> March </month> <year> 1995. </year>
Reference-contexts: The operating system on each node is IBM AIX 3.2.5. The nodes are interconnected by a high-performance omega switch with approximately 63 sec latency and 35 Mbytes/sec bandwidth <ref> [14] </ref>. The peak performance of each node is 125 Mflops/sec [14]. The parallel I/O system on the SP consists of four RAID-5 disk arrays, each of 50 GBytes. <p> The operating system on each node is IBM AIX 3.2.5. The nodes are interconnected by a high-performance omega switch with approximately 63 sec latency and 35 Mbytes/sec bandwidth <ref> [14] </ref>. The peak performance of each node is 125 Mflops/sec [14]. The parallel I/O system on the SP consists of four RAID-5 disk arrays, each of 50 GBytes. In addition to the regular Unix (AIX) file system on each node, the SP supports two other file systems|NSL Unitree and IBM's parallel file system PIOFS.
Reference: [15] <author> W. Gropp and B. Smith. </author> <title> Chameleon Parallel Programming Tools User's Manual. </title> <type> Technical Report ANL-93/23, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: The two algorithms are combined by using a time-splitting technique. Further details about the application are given in [21, 6, 7]. The code has been implemented in Fortran 77 and C. The Chameleon library is used for both message passing and I/O <ref> [15] </ref>. Chameleon provides a uniform interface to whatever native message-passing library is available on a given machine, and also provides a portable high-level interface for I/O. As a result, the code is directly portable across a wide range of parallel machines.
Reference: [16] <author> V. Herrarte and E. Lusk. </author> <title> Studying Parallel Program Behavior with Upshot. </title> <type> Technical Report ANL-91/15, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: We instrumented the source code using the Pablo performance analysis environment [30] and collected trace files on both systems. The traces were analyzed and visualized using Upshot <ref> [16] </ref>. The objectives of this study were to understand the I/O behavior of the application on each individual machine and to compare the I/O performance of the two machines for the same application. Note that we instrumented the existing parallel code for the application. <p> Other Pablo tools, such as IOStats and IOtotalsByPE, can be used to analyze the trace file, and provide detailed information about the I/O operations recorded. 4 We visualized and analyzed the trace files using Upshot <ref> [16] </ref>, a tool for studying parallel program behavior developed at Argonne National Laboratory. Upshot takes as input a logfile, which is basically a list of the events of interest in a parallel program, in the order in which they occurred during the execution of the program. <p> Upshot thus provides a global view of the trace data across all processors, which can often reveal interesting patterns and peculiarities in the performance of the code. Upshot expects the logfiles to be in a specific format described in <ref> [16] </ref>. The traces created by Pablo are in a different format called Self-Defining Data Format (SDDF) [2]. Hence, we had to first convert Pablo trace files to the format required by Upshot.
Reference: [17] <author> D. Jensen and D. Reed. </author> <title> File Archive Activity in a Supercomputing Environment. </title> <booktitle> In Proceedings of the 7th ACM International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: Smith [32] studied the file access patterns on IBM mainframes at the Stanford Linear Accelerator Center. Ramakrishnan et al. [28] performed a study of the I/O access patterns in a commercial computing environment on a VAX/VMS system. Jensen and Reed <ref> [17] </ref> studied the patterns of access to the file archive of the National Center for Supercomputing Applications. Miller and Katz [22] traced certain I/O-intensive applications on the Cray Y-MP at NASA Ames Research Center.
Reference: [18] <author> D. Kotz. </author> <title> Disk-directed I/O for MIMD Multiprocessors. </title> <booktitle> In Proceedings of the 1994 Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 61-74, </pages> <month> November </month> <year> 1994. </year> <note> Updated as Technical Report PCS-TR94-226, </note> <institution> Dept. of Computer Science, Dartmouth College. </institution>
Reference-contexts: This would make better use of the available I/O bandwidth and also eliminate the communication bottleneck. The I/O access pattern of this application suggests that it would benefit from using collective I/O, in the form of either two-phase I/O [10], disk-directed I/O <ref> [18] </ref>, or server-directed collective I/O [31]. In this application, arrays are distributed in a block fashion in two dimensions, resulting in each processor getting a sub-block of the array. The sub-block from each processor needs to written to the file containing the entire array. <p> In this application, arrays are distributed in a block fashion in two dimensions, resulting in each processor getting a sub-block of the array. The sub-block from each processor needs to written to the file containing the entire array. It has been shown in <ref> [10, 18, 31] </ref> that this can be done efficiently using collective I/O. To study the benefits of using collective I/O for this application, we have implemented the Chameleon I/O routines PIWriteDistributedArray and PIReadDistributedArray using two-phase I/O.
Reference: [19] <author> D. Kotz and N. Nieuwejaar. </author> <title> File-System Workload on a Scientific Multiprocessor. </title> <booktitle> IEEE Parallel and Distributed Technology, </booktitle> <pages> pages 51-60, </pages> <month> Spring </month> <year> 1995. </year>
Reference-contexts: Pasquale and Polyzos [25, 26] studied the file access characteristics of scientific applications on the Cray Y-MP and Cray C90 at San Diego Supercomputer Center. Only recently have there been any efforts to characterize the file access patterns on parallel machines. Kotz and Nieuwejaar <ref> [19] </ref> present the results of a three-week tracing study in which all file-related activity on the Intel iPSC/860 at NASA Ames Research Center was recorded.
Reference: [20] <author> A. </author> <title> Malagoli. </title> <type> Personal communication, </type> <year> 1995. </year>
Reference-contexts: The main data structures in the code are two-dimensional arrays of double-precision floating-point numbers. The application performs a significant amount of I/O and is bottlenecked by its I/O requirements <ref> [20] </ref>. The authors would like to solve three-dimensional problems, but are constrained by the I/O performance [20]. To reduce the amount of data transfer, I/O is performed in single-precision though computation is done in double-precision. All arrays fit in main memory, so the application is essentially in-core. <p> The main data structures in the code are two-dimensional arrays of double-precision floating-point numbers. The application performs a significant amount of I/O and is bottlenecked by its I/O requirements <ref> [20] </ref>. The authors would like to solve three-dimensional problems, but are constrained by the I/O performance [20]. To reduce the amount of data transfer, I/O is performed in single-precision though computation is done in double-precision. All arrays fit in main memory, so the application is essentially in-core. <p> We did not run the instrumented code using the PIO AS PARALLEL mode because the authors of the application never run it in that mode <ref> [20] </ref>. Clearly, the I/O performance could be improved by having several processors write to a common file in parallel. This would make better use of the available I/O bandwidth and also eliminate the communication bottleneck. <p> To reach a more definite conclusion, we plan to study the I/O characteristics of several other parallel applications. We also plan to instrument and characterize the three-dimensional version of this application being developed <ref> [20] </ref>, which is much more I/O-intensive than the present two-dimensional version. Acknowledgments We thank Andrea Malagoli for giving us the source code of the application and helping us understand it. We also thank Ruth Aydt for helping us understand how to use Pablo.
Reference: [21] <author> A. Malagoli, A. Dubey, F. Cattaneo, and D. Levine. </author> <title> A Portable and Efficient Parallel Algorithm for Astrophysical Fluid Dynamics. </title> <booktitle> In Proceedings of Parallel CFD '95, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: This paper presents the results of a study of the I/O characteristics of a real, large-scale, I/O-intensive, portable, parallel application in astrophysics <ref> [21] </ref>, on two different parallel computers|the IBM SP and the Intel Paragon. We instrumented the source code using the Pablo performance analysis environment [30] and collected trace files on both systems. The traces were analyzed and visualized using Upshot [16]. <p> our knowledge, there has been no other study of the I/O characteristics of any application on different machines. 2 3 Application Overview The application we benchmarked is an astrophysics code developed at the University of Chicago as part of a NASA/ESS-funded grand-challenge project on convective turbulence and mixing in astrophysics <ref> [21] </ref>. This application focuses on the study of the highly turbulent convective layers of late-type stars, like the sun, in which turbulent mixing plays a fundamental role in the redistribution of many physical ingredients of the star, such as angular momentum, chemical contaminants, and magnetic fields. <p> The computational algorithm consists mainly of two independent components|a finite difference higher-order Godunov method for compressible hydrodynamics, and a Crank-Nicholson-based multigrid scheme. The two algorithms are combined by using a time-splitting technique. Further details about the application are given in <ref> [21, 6, 7] </ref>. The code has been implemented in Fortran 77 and C. The Chameleon library is used for both message passing and I/O [15]. Chameleon provides a uniform interface to whatever native message-passing library is available on a given machine, and also provides a portable high-level interface for I/O.
Reference: [22] <author> E. Miller and R. Katz. </author> <title> Input/Output Behavior of Supercomputer Applications. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 567-576, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Ramakrishnan et al. [28] performed a study of the I/O access patterns in a commercial computing environment on a VAX/VMS system. Jensen and Reed [17] studied the patterns of access to the file archive of the National Center for Supercomputing Applications. Miller and Katz <ref> [22] </ref> traced certain I/O-intensive applications on the Cray Y-MP at NASA Ames Research Center. Pasquale and Polyzos [25, 26] studied the file access characteristics of scientific applications on the Cray Y-MP and Cray C90 at San Diego Supercomputer Center.
Reference: [23] <author> N. Nieuwejaar, D. Kotz, A. Purakayastha, C. Ellis, and M. </author> <title> Best. File-Access Characteristics of Parallel Scientific Workloads. </title> <type> Technical Report PCS-TR95-263, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> August </month> <year> 1995. </year> <month> 19 </month>
Reference-contexts: A similar study on the CM-5 at the National Center for Supercomputing Applications was performed by Purakayastha et al. [27]. The results of these two studies are compared and contrasted in <ref> [23] </ref>. They found that file sizes were large, request sizes were fairly small, data was accessed in sequence but with strides, and I/O was dominated by writes. Crandall et al. [8] instrumented and analyzed the I/O characteristics of three parallel applications on the Intel Paragon at Caltech.
Reference: [24] <author> J. Ousterhout, H. Da Costa, D. Harrison, J. Kunze, M. Kupfer, and J. Thompson. </author> <title> A Trace Driven Analysis of the UNIX 4.2 BSD File System. </title> <booktitle> In Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 15-24, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: In Section 7, we discuss how the I/O performance of this application could be improved, and draw overall conclusions in Section 8. 2 Related Work The file access characteristics of applications on uniprocessor and vector machines have been studied quite extensively. Ousterhout et al. <ref> [24] </ref> and Floyd and Ellis [12] studied file access patterns on 1 Unix workstations. Baker et al. [4] analyzed the user-level file access patterns and caching behavior of the Sprite distributed file system. Smith [32] studied the file access patterns on IBM mainframes at the Stanford Linear Accelerator Center.
Reference: [25] <author> B. Pasquale and G. Polyzos. </author> <title> A Static Analysis of I/O Characteristics of Scientific Applications in a Production Workload. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 388-397, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Jensen and Reed [17] studied the patterns of access to the file archive of the National Center for Supercomputing Applications. Miller and Katz [22] traced certain I/O-intensive applications on the Cray Y-MP at NASA Ames Research Center. Pasquale and Polyzos <ref> [25, 26] </ref> studied the file access characteristics of scientific applications on the Cray Y-MP and Cray C90 at San Diego Supercomputer Center. Only recently have there been any efforts to characterize the file access patterns on parallel machines.
Reference: [26] <author> B. Pasquale and G. Polyzos. </author> <title> Dynamic I/O Characterization of I/O Intensive Scientific Applications. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 660-669, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Jensen and Reed [17] studied the patterns of access to the file archive of the National Center for Supercomputing Applications. Miller and Katz [22] traced certain I/O-intensive applications on the Cray Y-MP at NASA Ames Research Center. Pasquale and Polyzos <ref> [25, 26] </ref> studied the file access characteristics of scientific applications on the Cray Y-MP and Cray C90 at San Diego Supercomputer Center. Only recently have there been any efforts to characterize the file access patterns on parallel machines.
Reference: [27] <author> A. Purakayastha, C. Ellis, D. Kotz, N. Nieuwejaar, and M. </author> <title> Best. Characterizing Parallel File-Access Patterns on a Large-Scale Multiprocessor. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium, </booktitle> <pages> pages 165-172, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: They did not instrument individual applications; instead, they instrumented the Concurrent File System (CFS) library routines called by all applications, and studied the file-related activity of all applications together. A similar study on the CM-5 at the National Center for Supercomputing Applications was performed by Purakayastha et al. <ref> [27] </ref>. The results of these two studies are compared and contrasted in [23]. They found that file sizes were large, request sizes were fairly small, data was accessed in sequence but with strides, and I/O was dominated by writes.
Reference: [28] <author> K. Ramakrishnan, P. Biswas, and R. Karedla. </author> <title> Analysis of File I/O Traces in Commercial Computing Environments. </title> <booktitle> In Proceedings of ACM SIGMETRICS and PERFORMANCE, </booktitle> <pages> pages 78-90, </pages> <year> 1992. </year>
Reference-contexts: Baker et al. [4] analyzed the user-level file access patterns and caching behavior of the Sprite distributed file system. Smith [32] studied the file access patterns on IBM mainframes at the Stanford Linear Accelerator Center. Ramakrishnan et al. <ref> [28] </ref> performed a study of the I/O access patterns in a commercial computing environment on a VAX/VMS system. Jensen and Reed [17] studied the patterns of access to the file archive of the National Center for Supercomputing Applications.
Reference: [29] <author> A. L. N. Reddy and P. Banerjee. </author> <title> A Study of I/O Behavior of Perfect Benchmarks on a Multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 312-321, </pages> <year> 1990. </year>
Reference-contexts: They found I/O request rates on the order of hundreds of requests per second, mainly small request sizes, and strong temporal and spatial locality. Reddy and Banerjee <ref> [29] </ref> studied the I/O activity of five applications from the Perfect benchmark suite on an Alliant multiprocessor, and found only sequential accesses. Del Rosario and Choudhary [11] provide an informal summary of the I/O requirements of several grand-challenge applications.
Reference: [30] <author> D. Reed, R. Aydt, R. Noe, P. Roth, K. Shields, B. Schwartz, and L. Tavera. </author> <title> Scalable Performance Analysis: The Pablo Performance Analysis Environment. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 104-113, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: This paper presents the results of a study of the I/O characteristics of a real, large-scale, I/O-intensive, portable, parallel application in astrophysics [21], on two different parallel computers|the IBM SP and the Intel Paragon. We instrumented the source code using the Pablo performance analysis environment <ref> [30] </ref> and collected trace files on both systems. The traces were analyzed and visualized using Upshot [16]. The objectives of this study were to understand the I/O behavior of the application on each individual machine and to compare the I/O performance of the two machines for the same application. <p> The user can either specify the cache size, or the library will use a default value. 4 Details of Experiments We describe how the application code was instrumented and analyzed, and the various experiments that were performed. 4.1 Instrumentation and Analysis We instrumented the source code using Pablo <ref> [30] </ref>, a comprehensive performance analysis environment developed at the University of Illinois at Urbana-Champaign.
Reference: [31] <author> K. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server-Directed Collective I/O in Panda. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: This would make better use of the available I/O bandwidth and also eliminate the communication bottleneck. The I/O access pattern of this application suggests that it would benefit from using collective I/O, in the form of either two-phase I/O [10], disk-directed I/O [18], or server-directed collective I/O <ref> [31] </ref>. In this application, arrays are distributed in a block fashion in two dimensions, resulting in each processor getting a sub-block of the array. The sub-block from each processor needs to written to the file containing the entire array. <p> In this application, arrays are distributed in a block fashion in two dimensions, resulting in each processor getting a sub-block of the array. The sub-block from each processor needs to written to the file containing the entire array. It has been shown in <ref> [10, 18, 31] </ref> that this can be done efficiently using collective I/O. To study the benefits of using collective I/O for this application, we have implemented the Chameleon I/O routines PIWriteDistributedArray and PIReadDistributedArray using two-phase I/O.
Reference: [32] <author> A. Smith. </author> <title> Analysis of Long Term File Reference Patterns for Application to File Migration Algorithms. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-7(4):403-417, </volume> <month> July </month> <year> 1981. </year> <month> 20 </month>
Reference-contexts: Ousterhout et al. [24] and Floyd and Ellis [12] studied file access patterns on 1 Unix workstations. Baker et al. [4] analyzed the user-level file access patterns and caching behavior of the Sprite distributed file system. Smith <ref> [32] </ref> studied the file access patterns on IBM mainframes at the Stanford Linear Accelerator Center. Ramakrishnan et al. [28] performed a study of the I/O access patterns in a commercial computing environment on a VAX/VMS system.
References-found: 32

