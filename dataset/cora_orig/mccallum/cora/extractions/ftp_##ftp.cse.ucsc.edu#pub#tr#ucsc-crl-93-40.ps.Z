URL: ftp://ftp.cse.ucsc.edu/pub/tr/ucsc-crl-93-40.ps.Z
Refering-URL: ftp://ftp.cse.ucsc.edu/pub/tr/README.html
Root-URL: http://www.cse.ucsc.edu
Title: A study of the reliability of hosts on the Internet  
Author: by K. B. Sriram Prof. Darrell D. E. Long Prof. David H. Haussler Prof. Charles E. M c Dowell Dean 
Degree: A thesis submitted in partial satisfaction of the requirements for the degree of Master of Science in  The thesis of K. B. Sriram is approved:  
Date: June 1993  
Affiliation: University of California Santa Cruz  Computer and Information Sciences  of Graduate Studies and Research  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> P. J. Bickel and K. A. Doksum. </author> <title> Mathematical Statistics (1977). </title> <publisher> Holden-Day Inc. </publisher>
Reference-contexts: A full presentation of these results from classic statistical theory can be found in the references <ref> [18, 5, 4, 1, 9] </ref>. For the most part, this chapter confines itself to quoting appropriate results from various references and showing how they are applied. <p> A good estimate for the mean inter-event renewal time is simply the mean of the observations X = ( P n Standard statistical results (see for instance <ref> [1, 8, 18] </ref>) show that this estimate is unbiased, consistent and sufficient. The following is a useful result from these references that we state without proof. <p> In practice, the equation is taken to be true for n &gt; 30 no matter what 9 distribution X follows, which leads to this confidence estimate (again from <ref> [1, 8, 18] </ref>) for the mean. Result 2.2 A (1 ff) confidence interval for the mean is (X t ff=2 p ; X + t ff=2 p ) where if T is a Student T distribution with n 1 degrees of freedom, P [T &gt; t ff=2 ] = ff=2. <p> the Weibull distribution, we have f U (u) = exp (x ff ) 14 If u 1 ; u 2 ; : : : ; u N are the observed samples of U , then we can derive a maximum likelihood estimate (MLE estimate) for and ff as follows (see <ref> [1] </ref> for a treatment of the MLE technique to derive estimates). The following is a result we derive to obtain a maximum likelihood estimate for and ff based in this technique. <p> Once ff is available, can be obtained from the first equation. 15 2.3.3 The Gamma distribution model The Gamma distribution is a different generalization of the exponential distribution that has the two-parameter density function e x (x) ff1 (ff) . We use the method of moments (see <ref> [18, 1] </ref> for an explanation of this method) to obtain estimates for and ff. The method of moments equates the sample moments with their expected values, and solves for the parameters of the model. I now derive estimates for and ff based on the method of moments technique.
Reference: [2] <author> F. K. C. Pu and R. C. Lehman. </author> <title> A Measurement Methodology for Wide Area Internets. </title> <type> Tech. Rep. </type> <month> CUCS-044-90 (Mar </month> <year> 1991). </year> <institution> Columbia University. </institution>
Reference-contexts: The Internet is used to support a number of wide area network distributed applications. It would be useful to determine the reliability of these applications, and be able to predict their behavior. While there have been studies about network latency, response time and path length <ref> [2] </ref> for typical operations in such applications, there has been less work on studying the reliability of such applications on the Internet. There is also little published data on measured failure statistics of hosts on the Internet. <p> Other studies about the Internet have focused on network latency, message path, network load and the types of protocols used. For instance Pu, Korz, and Lehman <ref> [2] </ref> propose a general methodology to measure network routing and latency for applications over the Internet, and present some example applications of the technique. The focus of this work is on application response time, which is different from our study which estimates failure times of hosts on the Internet.
Reference: [3] <author> J. L. Carroll and D. D. E. </author> <title> Long. The Effect of Failure and Repair Distributions on Consistency Protocols for Replicated Data Objects. </title> <booktitle> Proceedings of the Twenty Second Annual Simulation Symposium (Tampa), </booktitle> <month> pages 47-60 (Mar </month> <year> 1989). </year>
Reference-contexts: This work focuses on analyzing the kinds of failures that happen for a particular computer, and is not based on measurements through the Internet. As examples of work where measurements made in our study can be used, Long, Carroll and Stewart <ref> [14, 3] </ref> study replicated databases, and model their reliability with failure models of hosts that can be validated with the results in this study. Resource discovery protocols have been proposed and implemented on the Internet (for instance by Schwartz and Tsirigotis [19]).
Reference: [4] <author> D. R. Cox. </author> <title> Renewal Theory (1962). </title> <publisher> Methuen and Co. </publisher>
Reference-contexts: A full presentation of these results from classic statistical theory can be found in the references <ref> [18, 5, 4, 1, 9] </ref>. For the most part, this chapter confines itself to quoting appropriate results from various references and showing how they are applied. <p> The final section evaluates the appropriateness of each of these estimates. 7 2.1 Renewal Processes Renewal processes have been studied for a long while <ref> [5, 4, 9] </ref> and form a convenient framework in which to describe our observations. Results from renewal theory apply to deriving estimates about failure rates from our observations. We first describe renewal processes, and show how our observations fit into this model. <p> We are interested in situations where t is large, in other words when the renewal process has been running for a long time. The distribution of U t is independent of t for large t <ref> [4] </ref>, so we will just refer to the distribution of U instead of U t in the discussion that follows. In the experiments that we conducted, this sample corresponds to measuring the uptimes of machines (viz. how long it has been running) at some instant of time. <p> In the experiments that we conducted, this sample corresponds to measuring the uptimes of machines (viz. how long it has been running) at some instant of time. We now derive results for estimates (see <ref> [4] </ref> for this approach) for the mean inter-event arrival time given a sample of backward occurrence times. Let the random variable U denote the age of the component in use at the present time. We first obtain the distribution of U in terms of the distribution of X as follows. <p> Therefore N t =t is an estimate for . Furthermore, N t is approximately normal for 19 large values of t. We can apply results from section 1.3.1 to get estimates and confidence intervals about E [X]. A more detailed analysis of N t is shown in <ref> [4] </ref>. This analysis shows a stronger result, viz. E [N t ] = t= for all t (However, the normality assumption for N t is still valid only for large values of t). <p> The second problem is that getting the data involves obtaining the permission of the user of the machine, since our method of obtaining the data changes some data on the queried machine. Again, this restricts the amount of data we can collect anonymously over the Internet. 3 Cox <ref> [4] </ref> presents arguments that about three times the mean value of the distribution is a good "large" number. 21 Chapter 3 Implementation In this chapter we describe the three experiments that were performed to collect observations about hosts on the Internet.
Reference: [5] <author> D. R. Cox and P. A. Lewis. </author> <title> The Statistical Analysis of Series of Events (1966). </title> <publisher> Chapman and Hall. </publisher>
Reference-contexts: A full presentation of these results from classic statistical theory can be found in the references <ref> [18, 5, 4, 1, 9] </ref>. For the most part, this chapter confines itself to quoting appropriate results from various references and showing how they are applied. <p> The final section evaluates the appropriateness of each of these estimates. 7 2.1 Renewal Processes Renewal processes have been studied for a long while <ref> [5, 4, 9] </ref> and form a convenient framework in which to describe our observations. Results from renewal theory apply to deriving estimates about failure rates from our observations. We first describe renewal processes, and show how our observations fit into this model.
Reference: [6] <author> D. D. E. Long, J. L. Carroll and C. J. Park. </author> <title> A study of the reliability of Internet sites. </title> <booktitle> Proceedings of the Tenth Symposium on Reliable Distributed Systems (Pisa, </booktitle> <address> Italy), </address> <month> pages 177-186 (Sep </month> <year> 1991). </year>
Reference-contexts: a particular kind of daemon, but as it changes some data on the host, we decided to obtain the permission of the systems administrators before collecting this data. 1.2 Previous Studies and Related Work This work is a continuation of a study of the reliability of hosts using the Internet <ref> [6] </ref>. In this study, a large population of hosts were queried to obtain a sample of uptimes of machines. 4 From these observations, an estimate of time to failure was obtained. This estimate involved assuming that the underlying data followed an exponential distribution. <p> We can then use the results in section 2.3 to obtain estimates for the mean inter-renewal time, which in this instance is the mean time between reboots. 3.3.1 Selecting and obtaining the sample Such an experiment was first performed by Long, Carroll and Park <ref> [6] </ref>. All the data was collected anonymously, and a vast amount of data was retrieved from this experiment. The list of hosts that were queried was obtained by traversing the entire Internet name-tree. <p> A subset of this collection of hosts that were likely to be running the network service were chosen. Calls to the network service were made to hosts in this collection to determine the length of time the host was running. We refer to this paper <ref> [6] </ref> for additional details about the experiment and the conditions under which the data was collected. 30 In [6], the data from this experiment was used to provide estimates about the mean time to failure of hosts on the Internet. <p> Calls to the network service were made to hosts in this collection to determine the length of time the host was running. We refer to this paper <ref> [6] </ref> for additional details about the experiment and the conditions under which the data was collected. 30 In [6], the data from this experiment was used to provide estimates about the mean time to failure of hosts on the Internet. Estimates about the mean time to failure were derived based on the assumption that the lifetimes of machines were drawn from an exponential distribution. <p> As described in section 2.4, we analyzed this data in four different ways to reach estimates about the mean time to failure. The first estimate is identical to the estimate made in <ref> [6] </ref>, and assumes that the underlying data follows an exponential distribution. The formulae used in this estimate are presented in section 2.4.1. <p> The small sample size leads to a large 90% confidence interval for the mean, which lies between between 8 and 14 days. In comparison, estimates from Carroll, Long and Park <ref> [6] </ref> are significantly higher. For instance, the estimate for the mean time to reboot for Sun4/60 machines is about 18 days. Other systems too, are estimated to have similar mean times to failure. One explanation for this discrepancy is from the following interesting observation about our data. <p> The distribution of the data from the wtmp data indicates that the underlying 35 distribution for the time between reboots is one that has a high failure rate for small time intervals, and a nearly constant failure rate for larger time intervals. The estimate from <ref> [6] </ref> assumes the underlying distribution is exponential, and therefore, that the backward occurrence distribution is also exponential. However, from our observation of the sample, the underlying data does not seem to be exponential, and we believe that part of the discrepancy in the two estimates comes from this fact. <p> The predicted Weibull model also had a decreasing failure rate, which validates our observations about the high initial failure rate of the observed data based on the wtmp experiment. In comparison with the estimates made in <ref> [6] </ref>, all of our estimates yield lower mean times between failure. However, it is interesting to note that same method used in [6] (the rup experiment with the exponential hypothesis) produced an estimate that was closest to those in [6]. <p> In comparison with the estimates made in <ref> [6] </ref>, all of our estimates yield lower mean times between failure. However, it is interesting to note that same method used in [6] (the rup experiment with the exponential hypothesis) produced an estimate that was closest to those in [6]. <p> In comparison with the estimates made in <ref> [6] </ref>, all of our estimates yield lower mean times between failure. However, it is interesting to note that same method used in [6] (the rup experiment with the exponential hypothesis) produced an estimate that was closest to those in [6]. The fact that they are still different is probably due to the fact that these two experiments were taken on different sample populations. 4.5 Future directions The ideal observation about hosts on the Internet would involve continuous monitoring to accurately track failures.
Reference: [7] <author> K. A. Doksum and B. S. Yandell. </author> <title> Handbook of Statistics, </title> <booktitle> volume 4 (1984). </booktitle> <publisher> Elsevier. </publisher>
Reference-contexts: The following result follows directly from the Central Limit Theorem (see for instance <ref> [7] </ref>), and is stated without proof. Result 2.4 S r is asymptotically normally distributed with mean r and variance r 2 , where = E [X] and is the standard deviation of X.
Reference: [8] <author> W. Feller. </author> <title> Introduction to Probability Theory, </title> <booktitle> volume II (1972). </booktitle> <publisher> John Wiley and Sons. </publisher>
Reference-contexts: A good estimate for the mean inter-event renewal time is simply the mean of the observations X = ( P n Standard statistical results (see for instance <ref> [1, 8, 18] </ref>) show that this estimate is unbiased, consistent and sufficient. The following is a useful result from these references that we state without proof. <p> Here S is the standard deviation estimator S = r P n n1 . Empirical studies (see <ref> [8] </ref> for instance) suggest that if n is large, this result holds even when X differs considerably from normal. <p> In practice, the equation is taken to be true for n &gt; 30 no matter what 9 distribution X follows, which leads to this confidence estimate (again from <ref> [1, 8, 18] </ref>) for the mean. Result 2.2 A (1 ff) confidence interval for the mean is (X t ff=2 p ; X + t ff=2 p ) where if T is a Student T distribution with n 1 degrees of freedom, P [T &gt; t ff=2 ] = ff=2.
Reference: [9] <author> B. V. Gnedenko. </author> <title> Mathematical Methods in Reliability Theory, Moscow: English Translation (1968). </title> <publisher> Academic Press. </publisher>
Reference-contexts: A full presentation of these results from classic statistical theory can be found in the references <ref> [18, 5, 4, 1, 9] </ref>. For the most part, this chapter confines itself to quoting appropriate results from various references and showing how they are applied. <p> The final section evaluates the appropriateness of each of these estimates. 7 2.1 Renewal Processes Renewal processes have been studied for a long while <ref> [5, 4, 9] </ref> and form a convenient framework in which to describe our observations. Results from renewal theory apply to deriving estimates about failure rates from our observations. We first describe renewal processes, and show how our observations fit into this model.
Reference: [10] <author> R. A. Golding. </author> <title> Weak-consistency Group Communication and Membership. </title> <type> Tech. Rep. </type> <institution> UCSC-CRL-92-52 (Dec 1992). University of California at Santa Cruz. </institution>
Reference-contexts: A system of tattlers behaves essentially as a distributed database of information. The consistency of this 42 database in the face of failures of tattlers is ensured by exchanging information sufficiently often, through a protocol called the time-stamped anti-entropy protocol <ref> [10] </ref>. This protocol guarantees that the database is "eventually consistent." This means that if all monitoring were to stop at any particular instant, (but tattlers still continue to contact one another) the probability of any two tattlers having non-identical databases approaches zero.
Reference: [11] <author> J. Gray. </author> <title> A Census of Tandem System Availability Between 1985 and 1990. </title> <type> Tech. Rep. </type> <month> 90.1 (Jan </month> <year> 1990). </year> <title> Tandem Computers. </title> <type> 48 </type>
Reference-contexts: Another study by the same group analyzes electronic mail traffic to determine people with shared interests. A different approach to measuring the reliability of computers was taken in a study made by Gray <ref> [11] </ref>, analyzing the performance of Tandem computers. In this study, numerous reports from customers were studied to get a breakdown of the different causes of failure and their frequency.
Reference: [12] <author> W. Hardle. </author> <title> Smoothing Techniques (1991). </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Therefore, f U (u) = e x . This is an interesting result since it shows that the backward occurrence time follows the same distribution as X when X is exponentially distributed. We state without proof standard results <ref> [12] </ref> that provide confidence intervals for the mean of an exponential distribution. <p> Dividing the fraction of samples in each bin by the width of the bin gives an estimate for the probability density function (pdf) for the midpoint of the bin <ref> [12] </ref>. This histogram is an approximation of the actual pdf f U (u). In fact, the histogram is also a consistent estimator [12] of the density f U (u) at the midpoint of each bin. <p> the fraction of samples in each bin by the width of the bin gives an estimate for the probability density function (pdf) for the midpoint of the bin <ref> [12] </ref>. This histogram is an approximation of the actual pdf f U (u). In fact, the histogram is also a consistent estimator [12] of the density f U (u) at the midpoint of each bin. We locate the first bin so that the midpoint of the first bin is at 0. Next, find out the value of f U (0) from the bin whose center is positioned on the origin.
Reference: [13] <author> D. D. E. </author> <title> Long. A Replicated Monitoring Tool. </title> <booktitle> Proceedings of the Second Workshop on the Management of Replicated Data (Monterey), </booktitle> <month> pages 96-99 (November </month> <year> 1992). </year>
Reference-contexts: Work is in progress on continuously monitoring hosts on the Internet without generating a voluminous amount of traffic. A prototype of a monitoring system called the tattler has been implemented. This monitoring system is described in <ref> [13] </ref>. A tattler is a monitoring station that periodically gathers information about a subset of hosts on the Internet. A tattler also periodically contacts another random tattler in the system and exchanges information that it has collected about hosts.
Reference: [14] <author> D. D. E. Long, J. L. Carroll, and K. Stewart. </author> <title> Estimating the Reliability of Regeneration-Based Replica Control Protocols. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <month> 1691-1702 (Dec </month> <year> 1989). </year>
Reference-contexts: Such comparisons can also be made to test the stability of a new operating system before its release. A third use, as mentioned before, is in modeling the reliability of distributed systems <ref> [14] </ref>. The results from our study can be used to validate the model used, and provide estimates for the reliability of the application. Measuring the failure rate is non-trivial as much of the information necessary to determine this parameter is difficult to collect anonymously over the Internet. <p> This work focuses on analyzing the kinds of failures that happen for a particular computer, and is not based on measurements through the Internet. As examples of work where measurements made in our study can be used, Long, Carroll and Stewart <ref> [14, 3] </ref> study replicated databases, and model their reliability with failure models of hosts that can be validated with the results in this study. Resource discovery protocols have been proposed and implemented on the Internet (for instance by Schwartz and Tsirigotis [19]).
Reference: [15] <author> Mark Lottor. </author> <title> Internet Domain Survey. </title> <type> Technical report (Jan 1993). </type> <institution> Network Information Systems Center, SRI Intl. </institution>
Reference-contexts: Introduction 1.1 Overview According to a recent study <ref> [15] </ref>, there are nearly 1.3 million hosts that are reachable on the Internet. The Internet is used to support a number of wide area network distributed applications. It would be useful to determine the reliability of these applications, and be able to predict their behavior.
Reference: [16] <author> S. J. R. Caceres, P. B. Danzig and D. J. Mitzel. </author> <title> Characteristics of Wide-Area TCP/IP Conversations. </title> <booktitle> Proceedings of the ACM SIGCOMM 91, </booktitle> <pages> pages 601-612 (1991). </pages>
Reference-contexts: The focus of this work is on application response time, which is different from our study which estimates failure times of hosts on the Internet. A large number of studies have been made about the types of protocols used in typical Internet traffic <ref> [16] </ref>. Other studies involve analyzing the usage of particular protocols to determine characteristics of the Internet. For instance, one study analyzes files transferred in the File Transfer Protocol (FTP) to determine the size of files transferred and occurrence of duplicate file transfers.
Reference: [17] <author> Sun Microsystems, </author> <title> Incorporated. RPC: Remote Procedure Call Protocol specification version 2. </title> <type> Tech. Rep. </type> <month> RFC-1057 (Jun </month> <year> 1988). </year> <institution> USC Information Sciences Institute. </institution>
Reference-contexts: This service is performed by a daemon called rpc.statd that runs using the Sun RPC protocol <ref> [17] </ref>. This daemon offers Sun RPC based remote procedure calls that can be made to obtain information about the status of the system.
Reference: [18] <author> K. S. Trivedi. </author> <title> Probability & Statistics with Reliability, </title> <booktitle> Queuing and Computer Science Applications (1982). </booktitle> <publisher> Prentice-Hall. </publisher>
Reference-contexts: A full presentation of these results from classic statistical theory can be found in the references <ref> [18, 5, 4, 1, 9] </ref>. For the most part, this chapter confines itself to quoting appropriate results from various references and showing how they are applied. <p> A good estimate for the mean inter-event renewal time is simply the mean of the observations X = ( P n Standard statistical results (see for instance <ref> [1, 8, 18] </ref>) show that this estimate is unbiased, consistent and sufficient. The following is a useful result from these references that we state without proof. <p> In practice, the equation is taken to be true for n &gt; 30 no matter what 9 distribution X follows, which leads to this confidence estimate (again from <ref> [1, 8, 18] </ref>) for the mean. Result 2.2 A (1 ff) confidence interval for the mean is (X t ff=2 p ; X + t ff=2 p ) where if T is a Student T distribution with n 1 degrees of freedom, P [T &gt; t ff=2 ] = ff=2. <p> Therefore, E [U i ] = 0 + (i + 1)E [X] 0 = (i + 1)E [X] In particular, this gives us the well-known result <ref> [18] </ref> E [U ] = E [X 2 ]=2E [X] as a special case of our lemma when i = 1. The next four subsections describe four estimates for E [X], the mean inter-event time, and the assumptions that underlie the assumptions. <p> Once ff is available, can be obtained from the first equation. 15 2.3.3 The Gamma distribution model The Gamma distribution is a different generalization of the exponential distribution that has the two-parameter density function e x (x) ff1 (ff) . We use the method of moments (see <ref> [18, 1] </ref> for an explanation of this method) to obtain estimates for and ff. The method of moments equates the sample moments with their expected values, and solves for the parameters of the model. I now derive estimates for and ff based on the method of moments technique.
Reference: [19] <author> M. F. Schwartz P. G. Tsirigotis. </author> <title> Experience with a Semantically Cognizant Internet White Pages Directory Tool. </title> <journal> Journal of Internetworking Research and Experience, </journal> <month> pages 23-50 (Mar </month> <year> 1991). </year>
Reference-contexts: Resource discovery protocols have been proposed and implemented on the Internet (for instance by Schwartz and Tsirigotis <ref> [19] </ref>). Analyzing the performance of these protocols in the face of failures of hosts can benefit from the estimates available from our study. General techniques in 5 building wide area distributed applications have also been suggested [20].
Reference: [20] <author> M. F. Schwartz D. C. M. Wood. </author> <title> A Measurement Study of Organizational Properties in the Global Electronic Mail Community. </title> <type> Tech. Rep. </type> <month> CU-CS-482-90 (Aug </month> <year> 1990). </year> <institution> University of Colorado. </institution>
Reference-contexts: Analyzing the performance of these protocols in the face of failures of hosts can benefit from the estimates available from our study. General techniques in 5 building wide area distributed applications have also been suggested <ref> [20] </ref>. Analyses of the performance of such systems can also use the results from our study. 1.2.1 Organization of thesis Chapter 2 discusses the theory behind the formulae used to estimate the average failure times from the observations.
References-found: 20

