URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/towell.aij94.ps
Refering-URL: http://www.cs.wisc.edu/~shavlik/abstracts/towell.aij94.ps.abstract.html
Root-URL: 
Title: Knowledge-Based Artificial Neural Networks  
Author: Geoffrey G. Towell Jude W. Shavlik 
Keyword: machine learning, connectionism, explanation-based learning, hybrid algorithms, theory refinement, computational biology  
Address: 1210 West Dayton St. Madison, WI 53706  College Road East, Princeton, NJ,  
Affiliation: University of Wisconsin  
Note: Appears in Artificial Intelligence, volume 69 or 70. Submitted 1/92, Final pre-publication revisions 8/94  Running Head: Knowledge-Based Artificial Neural Networks Current address is: Siemens Corporate Research, 755  08540. Please direct all correspondence to this address.  
Email: towell@learning.scr.siemens.com shavlik@cs.wisc.edu  
Phone: (609) 321-0065 (608) 262-7784  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. Ahmad, </author> <title> A Study of Scaling and Generalization in Neural Networks, </title> <type> Technical Report CCSR-88-13, </type> <institution> University of Illinois, Center for Complex Systems Research (1988). </institution>
Reference-contexts: However, they have a set of problems unique to their style of empirical learning. Among these problems are: * Training times are lengthy [50]. * The initial parameters of the network can greatly affect how well concepts are learned <ref> [1] </ref>. * There is not yet a problem-independent way to choose a good network topology, although there has been considerable research in this direction (e.g., [10]). * After training, neural networks are often very difficult to interpret [56]. 2.4.
Reference: [2] <author> L. Atlas, R. Cole, Y. Muthusamy, A. Lippman, J. Connor, D. Park, M. El-Sharkawi, and R. J. Marks, </author> <title> A peerformance comparison of trained multi-layer perceptrons and trained classification trees, </title> <booktitle> Proceedings of IEEE 78 (1990) 1614-1619. </booktitle>
Reference-contexts: Artificial neural networks Artificial neural networks (ANNs), which form the basis of KBANN, are a particular method for empirical learning. ANNs have proven to be equal, or superior, to other empirical learning systems over a wide range of domains, when evaluated in terms of their generalization ability <ref> [50, 2] </ref>. However, they have a set of problems unique to their style of empirical learning. <p> Figure 16 compares backpropagation, ID3, and Cobweb using the same experimental methodology as ability of backpropagation. In addition, a number of empirical studies <ref> [50, 2] </ref> suggest that, over a wide range of conditions, backpropagation is as good or better than other empirical learning systems at classifying examples not seen during training. Therefore, it is reasonable to conclude that some of the power of KBANN is due simply to its underlying empirical learning algorithm.
Reference: [3] <author> J. Bachant and J. McDermott, </author> <title> R1 Revisited: Four Years in the Trenches, </title> <journal> AI Magazine 5 (1984) 21-32. </journal>
Reference-contexts: To make a domain theory as complete and correct as possible, it may be necessary to write thousands of interacting, possibly recursive, rules. Use of such rule sets may be intolerably slow. * Domain theories can be difficult to modify <ref> [3] </ref>. As interactions proliferate in a rule set, it becomes difficult to predict all of the changes resulting from modifying a single rule. 2.2. Empirical learning Empirical learning systems inductively generalize specific examples.
Reference: [4] <author> E. Barnard and R. A. Cole, </author> <title> A Neural-Net Training Program based on Conjugate-Gradient Optimization, </title> <type> Technical Report CSE 89-014, </type> <institution> Oregon Graduate Institute, Beaverton, </institution> <address> OR (1989). </address>
Reference-contexts: Networks created in this step make the same classifications as the rules upon which they are based. The second algorithm of KBANN, labeled Neural Learning, refines networks using the backpropagation learning algorithm [47]. (Although all of our experiments use backpropagation, any method for supervised weight revision e.g., conjugate gradient <ref> [4] </ref> would work.) While the learning mechanism is essentially standard backpropagation, the network being trained is not standard. Instead, the first algorithm of KBANN constructs and initializes the network. This has implications for training that 4 are discussed in Section 3.7.
Reference: [5] <author> H. R. Berenji, </author> <title> Refinement of Approximate Reasoning-Based Controllers by Reinforcement Learning, </title> <booktitle> in: Proceedings of the Eighth International Machine Learning Workshop, </booktitle> <address> Evanston, IL (1991), </address> <pages> 475-479. </pages>
Reference-contexts: The focus of these systems has varied from reducing the number of examples required for robot learning [29], to probabilistic logics [24], to the propagation of Mycin-like certainty factors [21], to the refinement of systems for fuzzy logic <ref> [5] </ref>. Rather than trying to review these diverse system, we focus on two systems developed at about the same time as KBANN which took slightly different paths.
Reference: [6] <editor> L. A. Birnbaum and G. C. Collins, eds, </editor> <booktitle> Proceedings of the Eighth International Machine Learning Workshop, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Evanston, IL (1991). </address>
Reference-contexts: Perhaps the most important of these trends is the realization that knowledge-intensive (e.g., [28]) and knowledge-free learning are just two ends of a spectrum along 3 which an intelligent system may operate. This realization has lead to recent specialized workshops (e.g., <ref> [6, 26] </ref>). Staying at one end or the other of the spectrum of possible learning systems simplifies the learning problem by allowing strong assumptions to be made about the nature of what needs to be learned. <p> Neither focusing attention nor topology selection suffice alone. 6. Other Approaches to Hybrid Learn ing We stated earlier that there has been much recent work in the field of hybrid learning (e.g., <ref> [6, 26, 30] </ref>). Rather than trying to review the whole field, in this section we will closely compare our approach to several representative approaches. The initial systems compared to KBANN are all-symbolic approaches. EITHER [36] and Labyrinth-k [54] will be used as examples of this approach.
Reference: [7] <author> W. W. Cohen, </author> <title> Compiling Prior Knowledge into an Explicit Bias, </title> <booktitle> in: Proceedings of the Ninth International Machine Learning Workshop, Aberdeen, Scotland (1992), </booktitle> <pages> 102-110. </pages>
Reference-contexts: It has been used to test inductive theory-refinement systems (e.g., LABYRINTH [54] and EITHER [36]) and inductive logic-programming systems (e.g., FOCL and GRENDEL <ref> [7] </ref>). It has also been used as a test problem for exemplar-based learning systems (e.g., [8]). However, to our knowledge no system has exceeded the performance we report in this article. 4.3. <p> However, FOCL is unable to directly handle the promoter domain theory because that theory classifies every training example in the same way. After a slight modification, FOCL is able to handle this problem, but its generalization is inferior to that of KBANN <ref> [7] </ref>. (Using 80 examples for training, FOCL has an error rate of 14.2% [7]. With the same number of examples, KBANN's error rate is 7.5%.) 6.3. Systems based on Neural Networks Many hybrid systems have been developed recently that use neural networks as their basis. <p> After a slight modification, FOCL is able to handle this problem, but its generalization is inferior to that of KBANN <ref> [7] </ref>. (Using 80 examples for training, FOCL has an error rate of 14.2% [7]. With the same number of examples, KBANN's error rate is 7.5%.) 6.3. Systems based on Neural Networks Many hybrid systems have been developed recently that use neural networks as their basis.
Reference: [8] <author> S. Cost and S. Salzberg, </author> <title> A Weighted Nearest Neighbor Algorithm for Learning With Symbolic Features, </title> <note> Machine Learning 10 (1993) 57-78. </note>
Reference-contexts: It has been used to test inductive theory-refinement systems (e.g., LABYRINTH [54] and EITHER [36]) and inductive logic-programming systems (e.g., FOCL and GRENDEL [7]). It has also been used as a test problem for exemplar-based learning systems (e.g., <ref> [8] </ref>). However, to our knowledge no system has exceeded the performance we report in this article. 4.3. <p> Third, we compare the learning and generalization of KBANN and backpropagation in terms of training effort. Seven-way comparison of algorithms In this section, KBANN is compared to six empirical learning algorithms: standard backpropagation [47], ID3 [42], nearest neighbor, PEBLS <ref> [8] </ref>, Perceptron [46], and Cobweb [11] 4 . <p> In most cases, differences are statistically significant with 99.5% confidence based on a one-tailed t-test. The only exceptions to this generalization 10 Cost and Salzburg <ref> [8] </ref> report making only four errors on the promoter testbed. However, we were unable to reproduce their results with our implementation of PEBLS. 19 with 1000 training examples. are that KBANN is only marginally better than standard backpropagation and PEBLS on the splice-junction problem.
Reference: [9] <author> T. G. Dietterich, </author> <title> Learning at the Knowledge Level, </title> <note> Machine Learning 1 (1986) 287-316. </note>
Reference-contexts: Hand-built classifiers Hand-built classifiers are non-learning systems (except insofar as they are later altered by hand). They simply do what they are told; they do not learn at the knowledge level <ref> [9] </ref>. Despite their apparent simplicity, such systems pose many problems for those that build them. * Typically, hand-built classifiers assume that their domain theory is complete and correct. However, for most real-world tasks, completeness and correctness are extremely difficult, if not impossible, to achieve.
Reference: [10] <author> S. E. Fahlman and C. Lebiere, </author> <booktitle> The Cascade-Correlation Learning Architecture, in: Advances in Neural Information Processing Systems, volume 2, </booktitle> <address> Denver, CO (1989), </address> <pages> 524-532. </pages>
Reference-contexts: Among these problems are: * Training times are lengthy [50]. * The initial parameters of the network can greatly affect how well concepts are learned [1]. * There is not yet a problem-independent way to choose a good network topology, although there has been considerable research in this direction (e.g., <ref> [10] </ref>). * After training, neural networks are often very difficult to interpret [56]. 2.4. Hybrid Learning Systems There is a significant gap between the knowledge-intensive, learning-by-being-told approach of hand-built classifiers and the virtually knowledge-free approach of empirical learning. <p> The network is classifying at least 90% of the training examples correctly but has not improved it ability to classify the training examples for five epochs. (This implements the patience stopping criterion <ref> [10] </ref>.) In general, networks trained for promoter recognition stopped training on the first criterion they are able to perfectly learn the training data. On the other hand, 7 For an incremental system like Cobweb, it may not be reasonable to count promoter errors following the method described.
Reference: [11] <author> D. H. Fisher, </author> <title> Knowledge Acquisition via Incremental Conceptual Clustering, </title> <note> Machine Learning 2 (1987) 139-172. </note>
Reference-contexts: Third, we compare the learning and generalization of KBANN and backpropagation in terms of training effort. Seven-way comparison of algorithms In this section, KBANN is compared to six empirical learning algorithms: standard backpropagation [47], ID3 [42], nearest neighbor, PEBLS [8], Perceptron [46], and Cobweb <ref> [11] </ref> 4 . <p> The first system is EITHER [36]. This system uses ID3 to make minimal corrections to domain theories. The second system is Labyrinth-k [54], an extension of Labyrinth that allows the insertion of domain knowledge. Labyrinth is, in turn, based on Cobweb <ref> [11] </ref>. (See Section 6.1 for a more detailed description of these algorithms.) Like KBANN, these systems are able to use initial knowledge that is both incorrect and incomplete as the basis for learning. Method. <p> As a result, the underlying empirical learner directly modifies the initial knowledge rather than a rerepresentation of that knowledge. Hence, after learning is complete, the modified domain theory is directly observable. Labyrinth-k uses the initial knowledge to create a structure that Cobweb 15 <ref> [11] </ref>, its underlying empirical algorithm, might create if given an appropriate set of examples.
Reference: [12] <author> N. S. Flann and T. G. Dietterich, </author> <title> A Study of Explanation-Based Methods for Inductive Learning, </title> <note> Machine Learning 4 (1989) 187-226. </note>
Reference-contexts: Both systems allow the initial knowledge to have arbitrary types of errors. This is a significant step forward form earlier systems (e.g., <ref> [22, 12] </ref>) which required that the domain theory have only certain classes of mistakes. Moreover, the systems are able to create knowledge (in the form of rules) when supplied with no initial information. <p> Specifically, constraints on the quality and form of information have been eased. Early systems such as UNIMEM [22] required correct theoretical information. While subsequent systems eased the correctness requirement, they introduced new constraints such as strict over-generality of the domain theory <ref> [12] </ref>. More recent systems such as KBANN allow arbitrary imperfections in theories. On the data side, earlier systems required that data be noise free while more recent systems allow noisy examples.
Reference: [13] <author> L. M. Fu, </author> <title> Integration of Neural Heuristics into Knowledge-Based Inference, </title> <note> Connection Science 1 (1989) 325-340. </note>
Reference-contexts: EITHER [36] and Labyrinth-k [54] will be used as examples of this approach. Following the all-symbolic approaches, we compare KBANN to inductive logic programming, using FOCL [37] as a representative of this field. We lastly compare KBANN to the work of Fu <ref> [13] </ref> and Katz [18] which we use as representatives of systems that, like KBANN, base their hybrid systems on neural networks. 6.1. All-symbolic approaches EITHER [36] and Labyrinth-k [54] take a similar approach to forming a hybrid system, one that is in many ways similar to KBANN. <p> Rather than trying to review these diverse system, we focus on two systems developed at about the same time as KBANN which took slightly different paths. Fu <ref> [13] </ref> described a system very similar to KBANN in that it uses symbolic knowledge in the form of rules to establish the structure and connection weights in a neural network. However, Fu's system uses non-differentiable activation functions to handle disjunctions in rule sets. <p> This point cuts in two ways. First, KBANN has no mechanism through for assigning different weights to antecedents. (See [24] for an approach.) Second, KBANN has no mechanism for expressing the partial truth of consequents. Mycin-like certainty factors [52] could be integrated into the rules-to-network translator <ref> [13, 21] </ref>. These factors would adjust link weights in the resulting network. Hence, links would have weights that depend both upon a global link weight and a certainty factor. This change in the definition of link weights would require modification of the method for setting the bias.
Reference: [14] <author> J. Gennari, P. Langley, and D. Fisher, </author> <title> Models of incremental concept formation, </title> <booktitle> Artificial Intelligence 40 (1989) 11-61. </booktitle>
Reference-contexts: Rather than 4 These tests all use implementations written at Wisconsin that are based on the descriptions in the cited references. The lone exception is Cobweb, for which we used Gennari's CLASSIT code <ref> [14] </ref>. 5 Note that we use the term cross-validation in the sense used by Weiss and Kulikowski [61] to indicate a testing methodology in which the set of examples are permuted and divided into N sets.
Reference: [15] <author> G. E. Hinton, </author> <title> Connectionist Learning Procedures, </title> <booktitle> Artificial Intelligence 40 (1989) 185-234. </booktitle>
Reference-contexts: However, when the outputs of networks are completely incorrect, this property makes it very difficult to correct the aspects of the networks that cause the errors. In the general context of neural networks, several solutions that require only minor adjustments to backpropagation have been proposed for this problem <ref> [15] </ref>. (Other, more significant changes to backpropagation also address the problem [59].) The results for KBANN-nets described in this article use the cross-entropy error function - Eq. 4 suggested by Hinton [15] rather than the standard error function - Eq. 3. <p> context of neural networks, several solutions that require only minor adjustments to backpropagation have been proposed for this problem <ref> [15] </ref>. (Other, more significant changes to backpropagation also address the problem [59].) The results for KBANN-nets described in this article use the cross-entropy error function - Eq. 4 suggested by Hinton [15] rather than the standard error function - Eq. 3. The cross-entropy function interprets the training signal and network outputs as conditional probabilities and attempts to minimize the difference between these probabilities.
Reference: [16] <author> R. C. Holte, L. E. Acker, and B. W. Porter, </author> <title> Concept Learning and the Problem of Small Disjuncts, </title> <booktitle> in: Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, MI (1989), </address> <pages> 813-819. </pages>
Reference-contexts: However, feature construction is a difficult, error-prone, enterprise. * Even when a large set of examples are available, small sets of exceptions may be either unrepresented or very poorly represented <ref> [16] </ref>. As a result, uncommon cases may be very difficult to correctly handle. 2.3. Artificial neural networks Artificial neural networks (ANNs), which form the basis of KBANN, are a particular method for empirical learning. <p> Moreover, these derived features can capture contextual dependencies in an example's description. In addition, the rules can refer to arbitrarily small regions of feature space. Hence, the rules can reduce the need for the empirical portion of a hybrid system to learn about uncommon cases <ref> [16] </ref>. This procedure also indirectly addresses many problems of hand-built classifiers. For instance, the problem of intractable domain theories is reduced because approximately-correct theories are often quite brief.
Reference: [17] <author> IUB Nomenclature Committee, </author> <title> Ambiguity Codes, </title> <note> European Journal of Biochemistry 150 (1985) 1-5. </note>
Reference-contexts: Meaning Code Meaning Code Meaning M A or C R A or G W A or T V A or C or G H A or C or T D A or G or T a standard notation for referring to all possible combinations of `nucleotides' using a single letter <ref> [17] </ref>. They are compatible with the codes used by the EMBL, GenBank, and PIR data libraries, three major collections of data for molecular biology. 4.2.
Reference: [18] <author> B. F. Katz, EBL and SBL: </author> <title> A Neural Network Synthesis, </title> <booktitle> in: Proceedings of the Eleventh Annual Conference of the Cognitive Science Society, </booktitle> <address> Ann Arbor, MI (1989), </address> <pages> 683-689. </pages>
Reference-contexts: EITHER [36] and Labyrinth-k [54] will be used as examples of this approach. Following the all-symbolic approaches, we compare KBANN to inductive logic programming, using FOCL [37] as a representative of this field. We lastly compare KBANN to the work of Fu [13] and Katz <ref> [18] </ref> which we use as representatives of systems that, like KBANN, base their hybrid systems on neural networks. 6.1. All-symbolic approaches EITHER [36] and Labyrinth-k [54] take a similar approach to forming a hybrid system, one that is in many ways similar to KBANN. <p> An empirical study in medical diagnosis showed this technique to be quite effective. However, the method is closely tied to its particular learning mechanism. Hence, his system is unable to use developments in the methods of training neural networks, (e.g., [59]). Katz <ref> [18] </ref> describes a system which, much like KBANN, uses a knowledge base that has been mapped (apparently by hand) into a neural network. The focus of Katz's work is on improving the time required to classify an object rather than generalization.
Reference: [19] <author> R. D. King, S. Muggleton, R. A. Lewis, and J. E. Sternberg, </author> <title> Drug Design by Machine Learning: The use of Inductive Logic Programming to Model the StructureActivity Relationships of Trimethoprim Analogues Binding to Dihydro-folate Reductase, </title> <booktitle> Proceeding of the National Academy of Sciences, </booktitle> <address> USA 89 (1992) 11322-11326. </address>
Reference-contexts: This has been the subject of much research [38, 62] which affects work in machine learning. Finally, there is a purely practical consideration; hybrid systems have proven effective on several real-world problems <ref> [57, 33, 36, 54, 19] </ref> (also Section 5). 3. KBANN This section describes the KBANN methodology, which Figure 1 depicts as a pair of algorithms (on the arcs) that form a system for learning from both theory and examples. The first algorithm, labeled Rules-to-Network, is detailed in Section 3.3. <p> In addition, like KBANN, inductive logic programming has been applied to several problems in computational biology <ref> [31, 19] </ref>. Inductive logic programming systems are closely related to EITHER and Labyrinth-k except that they are not limited to propositional rules. As an example of these systems, we use FOCL [37]. <p> However, it is unlikely that in the near future KBANN will be able to handle quantified variables in its rules, as there are currently no appropriate techniques for binding variables within neural networks. Inductive logic programming systems such as FOIL [43], FOCL [37] and GOLEM <ref> [19] </ref> allow relational rule sets; hence, they avoid the restrictions inherent to KBANN. * There is no mechanism for handling uncertainty in rules. This point cuts in two ways.
Reference: [20] <author> S. C. Kleene, </author> <title> Representation of Events in Nerve Nets and Finite Automata, in: Automata Studies, </title> <editor> C. E. Shannon and J. McCarthy (Eds.), </editor> <address> 3-41, </address> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ (1956). </address>
Reference-contexts: If the 3 The translation of rules into neural structures has been described by McCulloch and Pitts [25] and Kleene <ref> [20] </ref> for units with threshold functions.
Reference: [21] <author> R. C. Lacher, S. I. Hruska, and D. C. Kuncicky, </author> <title> Backpropagation Learning in Expert Networks, </title> <booktitle> IEEE Transactions on Neural Networks 3 (1992) 62-72. </booktitle>
Reference-contexts: Systems based on Neural Networks Many hybrid systems have been developed recently that use neural networks as their basis. The focus of these systems has varied from reducing the number of examples required for robot learning [29], to probabilistic logics [24], to the propagation of Mycin-like certainty factors <ref> [21] </ref>, to the refinement of systems for fuzzy logic [5]. Rather than trying to review these diverse system, we focus on two systems developed at about the same time as KBANN which took slightly different paths. <p> This point cuts in two ways. First, KBANN has no mechanism through for assigning different weights to antecedents. (See [24] for an approach.) Second, KBANN has no mechanism for expressing the partial truth of consequents. Mycin-like certainty factors [52] could be integrated into the rules-to-network translator <ref> [13, 21] </ref>. These factors would adjust link weights in the resulting network. Hence, links would have weights that depend both upon a global link weight and a certainty factor. This change in the definition of link weights would require modification of the method for setting the bias.
Reference: [22] <author> M. Lebowitz, </author> <title> Integrated Learning: Controlling Explanation, </title> <booktitle> Cognitive Science 10 (1986) 219-240. </booktitle>
Reference-contexts: Both systems allow the initial knowledge to have arbitrary types of errors. This is a significant step forward form earlier systems (e.g., <ref> [22, 12] </ref>) which required that the domain theory have only certain classes of mistakes. Moreover, the systems are able to create knowledge (in the form of rules) when supplied with no initial information. <p> Trends in hybrid systems research There has been a pronounced trend in the development of hybrid systems. Specifically, constraints on the quality and form of information have been eased. Early systems such as UNIMEM <ref> [22] </ref> required correct theoretical information. While subsequent systems eased the correctness requirement, they introduced new constraints such as strict over-generality of the domain theory [12]. More recent systems such as KBANN allow arbitrary imperfections in theories.
Reference: [23] <author> R. Maclin and J. W. Shavlik, </author> <title> Refining Algorithms with Knowledge-Based Neural Networks: Improving th Chou-Fasman Algorithm for Protein Folding, </title> <note> Machine Learning 11 (1993) 195-213. </note>
Reference-contexts: This no cycles constraint simplifies the training of the resulting networks. However, it does not represent a fundamental limitation on KBANN, as there exist algorithms based upon backpropagation that can be used to train networks with cycles [40]. Moreover, others have extended KBANN to handle recursive finite-state grammars <ref> [23] </ref>. In addition to these constraints, the rule sets provided to KBANN are usually hierarchically structured. That is, rules do not commonly map directly from inputs to outputs. Rather, at least some of the rules provide intermediate conclusions that describe useful conjunctions of the input features. <p> As tests on small, artificial domains show qualitatively similar results [51, 35, 56], we are optimistic that future tests will continue to show the success of the KBANN algorithm. Indeed, work by us and our colleagues on another biological domain (protein secondary-structure prediction) <ref> [23] </ref> and a process control problem [49] have shown the generality of the approach. Finally, our tests show that KBANN's value is due to both the identification of informative input features and useful derived features (thereby establishing a good network topology). Neither focusing attention nor topology selection suffice alone. 6. <p> Also, a network-to-rules translator that we have developed [55] directly addresses these problems. * KBANN's rule syntax is limited. KBANN can not handle rules with cycles or variables. A research area is the expansion of KBANN to admit certain type of cyclic rules <ref> [23] </ref>. However, it is unlikely that in the near future KBANN will be able to handle quantified variables in its rules, as there are currently no appropriate techniques for binding variables within neural networks. <p> These DNA sequence-analysis domains, promoter recognition and splice-junction determination, are two small problems from a field that is growing rapidly as a result of the Human Genome Project. The ideas underlying KBANN have also been used for protein folding <ref> [23] </ref> and process control [49]. While much work remain to be done on KBANN, as well as the more general topic of learning from both theory and data, the KBANN system represents a first step along a significant research path.
Reference: [24] <author> J. J. Mahoney and R. J. Mooney, </author> <title> Combining connectionist and symbolic learning to refine certainty-factor rule-bases, </title> <note> Connection Science 5 (1993) 339-364. </note>
Reference-contexts: Systems based on Neural Networks Many hybrid systems have been developed recently that use neural networks as their basis. The focus of these systems has varied from reducing the number of examples required for robot learning [29], to probabilistic logics <ref> [24] </ref>, to the propagation of Mycin-like certainty factors [21], to the refinement of systems for fuzzy logic [5]. Rather than trying to review these diverse system, we focus on two systems developed at about the same time as KBANN which took slightly different paths. <p> This point cuts in two ways. First, KBANN has no mechanism through for assigning different weights to antecedents. (See <ref> [24] </ref> for an approach.) Second, KBANN has no mechanism for expressing the partial truth of consequents. Mycin-like certainty factors [52] could be integrated into the rules-to-network translator [13, 21]. These factors would adjust link weights in the resulting network.
Reference: [25] <author> W. S. McCulloch and W. A. Pitts, </author> <title> A Logical Calculus of Ideas Immanent in Nervous Activity, </title> <note> Bulletin of Mathematical Biophysics 5 (1943) 115-133. </note>
Reference-contexts: If the 3 The translation of rules into neural structures has been described by McCulloch and Pitts <ref> [25] </ref> and Kleene [20] for units with threshold functions.
Reference: [26] <editor> R. S. Michalski and G. Tecuci, eds, </editor> <booktitle> Proceedings of the First International Workshop on Multistrategy Learning, </booktitle> <institution> George Mason University, </institution> <address> Harpers Ferry, </address> <publisher> W. </publisher> <address> Va. </address> <year> (1991). </year>
Reference-contexts: Perhaps the most important of these trends is the realization that knowledge-intensive (e.g., [28]) and knowledge-free learning are just two ends of a spectrum along 3 which an intelligent system may operate. This realization has lead to recent specialized workshops (e.g., <ref> [6, 26] </ref>). Staying at one end or the other of the spectrum of possible learning systems simplifies the learning problem by allowing strong assumptions to be made about the nature of what needs to be learned. <p> Neither focusing attention nor topology selection suffice alone. 6. Other Approaches to Hybrid Learn ing We stated earlier that there has been much recent work in the field of hybrid learning (e.g., <ref> [6, 26, 30] </ref>). Rather than trying to review the whole field, in this section we will closely compare our approach to several representative approaches. The initial systems compared to KBANN are all-symbolic approaches. EITHER [36] and Labyrinth-k [54] will be used as examples of this approach.
Reference: [27] <author> J. Mingers, </author> <title> An Empirical Comparison of Pruning Methods for Decision Tree Induction, </title> <booktitle> Machine Learning 4 (1989) 227-243. </booktitle> <pages> 37 </pages>
Reference-contexts: Although generalization often peaked before the end of training, we never observed a significant or consistent decline. Because we were unable to detect overfitting, we did not pursue any techniques for its prevention. For example, we did not prune the decision trees created by ID3 <ref> [27] </ref>. Nor did we use a tuning [cross-validation] set [63] to decide when to stop training our neural networks. Results and discussion. Figures 10 and 11 present the comparisons of KBANN to the six empirical-learning algorithms listed above 10 .
Reference: [28] <author> T. M. Mitchell, R. Keller, and S. Kedar-Cabelli, </author> <title> Explanation-Based Generalization: </title>
Reference-contexts: Section 3 describes the KBANN algorithm. Empirical tests in Section 5, using the DNA 1 In machine learning, a domain theory <ref> [28] </ref> is a collection of rules that describes task-specific inferences that can be drawn from the given facts. <p> Despite their apparent simplicity, such systems pose many problems for those that build them. * Typically, hand-built classifiers assume that their domain theory is complete and correct. However, for most real-world tasks, completeness and correctness are extremely difficult, if not impossible, to achieve. In fact, in explanation-based learning <ref> [28] </ref> one of the major issues is dealing with incomplete and incorrect domain theories. 2 * Domain theories can be intractable to use [28]. To make a domain theory as complete and correct as possible, it may be necessary to write thousands of interacting, possibly recursive, rules. <p> However, for most real-world tasks, completeness and correctness are extremely difficult, if not impossible, to achieve. In fact, in explanation-based learning <ref> [28] </ref> one of the major issues is dealing with incomplete and incorrect domain theories. 2 * Domain theories can be intractable to use [28]. To make a domain theory as complete and correct as possible, it may be necessary to write thousands of interacting, possibly recursive, rules. Use of such rule sets may be intolerably slow. * Domain theories can be difficult to modify [3]. <p> Some of this gap is filled by hybrid learning methods, which use both hand-constructed rules and classified examples during learning. Several trends have made the development of such systems an active area in machine learning. Perhaps the most important of these trends is the realization that knowledge-intensive (e.g., <ref> [28] </ref>) and knowledge-free learning are just two ends of a spectrum along 3 which an intelligent system may operate. This realization has lead to recent specialized workshops (e.g., [6, 26]).
References-found: 28

