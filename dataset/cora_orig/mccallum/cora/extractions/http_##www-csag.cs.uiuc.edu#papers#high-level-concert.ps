URL: http://www-csag.cs.uiuc.edu/papers/high-level-concert.ps
Refering-URL: http://www-csag.cs.uiuc.edu/papers/index.html
Root-URL: http://www.cs.uiuc.edu
Title: High Level Parallel Programming: The Illinois Concert System  
Author: Andrew Chien Julian Dolby Bishwaroop Ganguly Vijay Karamcheti Xingbin Zhang 
Keyword: concurrent languages, concurrent object-oriented programming, compiler optimization, runtime systems, object-oriented optimization  
Web: concert@red-herring.cs.uiuc.edu  
Address: Urbana, Illinois 61801  
Affiliation: Department of Computer Science University of Illinois  
Abstract: Programmers of concurrent applications are faced with complex performance trade-offs, since data distribution and concurrency management exacerbate the difficulty of building large, complex applications. To address these challenges, the Illinois Concert system provides a global namespace, implicit concurrency control and granularity management, implicit storage management, and object-oriented programming features. These features are embodied in a language ICC++ (derived from C++) which has been used to build a number of kernels and applications. As high level features can potentially incur overhead, the Concert system employs a range of compiler and runtime optimization techniques to efficiently support the high level programming model. The compiler techniques include type inference, inlining and specialization; and the runtime techniques include caching, prefetching and hybrid stack/heap multithreading. The effectiveness of these techniques permits the construction of complex parallel applications that are portable and flexible, enabling convenient application modification or tuning. We present performance results for a number of application programs which attain good speedups and absolute performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Pierre America. POOL-T: </author> <title> A parallel object-oriented language. </title> <editor> In Aki Yonezawa and Mario Tokoro, editors, </editor> <booktitle> Object-Oriented Concurrent Programming, </booktitle> <pages> pages 199-220. </pages> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Actor-based languages <ref> [1, 17, 36, 25] </ref> are most similar in terms of high-level programming support, but have focused less [33, 23] on efficient implementation.
Reference: [2] <author> Peter Beckman, Dennis Gannon, and Elizabeth Johnson. </author> <title> Portable parallel programming in HPC++. </title> <note> Available online at http://www.extreme.indiana.edu/hpc%2b%2b/docs/ppphpc++/icpp.ps, 1996. </note>
Reference-contexts: Task-parallel object-oriented languages, mostly based on C++ extensions [13, 19, 6], support irregular parallelism and some location independence, but require programmer management of con-currency, storage-management, and task granularity which limits scalability and portability. Data-parallel object-oriented languages, such as pC++ [24], provide little support for expressing task-level parallelism. HPC++ <ref> [2] </ref> is similar, expressing concurrency primarily as parallel operations across homogenous collections. ICC++ expresses data parallelism as task-level concurrency, providing greater programming power, of an optimization results in a 35-80% performance drop. but making efficient implementation significantly more challenging.
Reference: [3] <author> Brad Calder, Dirk Grunwald, and Benjamin Zorn. </author> <title> Quantifying differences between C and C++ programs. </title> <type> Technical Report CU-CS-698-94, </type> <institution> University of Colorado, Boulder, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: The effect of object-oriented abstraction is to hide implementation details needed for efficient code - e.g. concrete types of variables and object lifetimes beneath abstract interfaces, requiring program analysis to discover them. Additionally, the many interfaces in object-oriented code tend to break programs down into many small, dynamically-dispatched methods <ref> [3] </ref>. Two performance issues arise from these aspects of object-oriented code. The first is small, dynamic methods, which increase overhead by requiring function calls, and inhibit the use of standard intra-procedural optimizations because of their small block size.
Reference: [4] <author> S. Chakrabarti and K. Yelick. </author> <title> Implementing an irregular application on a distributed memory multiprocessor. </title> <booktitle> In Proceedings of the Fourth ACM/SIGPLAN Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 169-179, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The applications exhibit good speedups ranging from 8.5 on 16 nodes for Grobner to 54.8 on 64 nodes for the force phase of FMM. These speedups compare favorably with the best speedups reported elsewhere for hand-optimized codes <ref> [4, 30, 31, 18, 32] </ref>. <p> Program Input Grobner pavelle5 <ref> [4] </ref> Grobner basis IC-Cedar Myoglobin Molecular dynamics Radiosity Room [35] Hierarchical radiosity Barnes 16K bodies Hierarchical N-body FMM 32K bodies Hierarchical N-body FMM are only for the force phases. The speedup numbers are comparable to the best reported for low-level programming approaches.
Reference: [5] <author> Craig Chambers. </author> <title> The Design and Implementation of the Self Compiler, an Optimizing Compiler for Object-Oriented Programming Languages. </title> <type> PhD thesis, </type> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: Concert differs from all the above systems in its focus on supporting high-level programming features with efficient implementation techniques. This focus can be found in the context of sequential object-oriented languages <ref> [10, 16, 5, 9] </ref>, but our system additionally tackles the problems associated with concurrency, distribution and parallelism. With respect to parallel systems in general, a wide variety of high-level approaches to portable programming are being actively pursued. Global address-space languages [12] minimally extend a low-level language with global pointers.
Reference: [6] <author> K. Mani Chandy and Carl Kesselman. </author> <title> Compositional C++: Compositional parallel programming. </title> <booktitle> In Proceedings of the Fifth Workshop on Compilers and Languages for Parallel Computing, </booktitle> <address> New Haven, Connecticut, </address> <year> 1992. </year> <note> YALEU/DCS/RR-915, Springer-Verlag Lecture Notes in Computer Science, </note> <year> 1993. </year>
Reference-contexts: Actor-based languages [1, 17, 36, 25] are most similar in terms of high-level programming support, but have focused less [33, 23] on efficient implementation. Task-parallel object-oriented languages, mostly based on C++ extensions <ref> [13, 19, 6] </ref>, support irregular parallelism and some location independence, but require programmer management of con-currency, storage-management, and task granularity which limits scalability and portability. Data-parallel object-oriented languages, such as pC++ [24], provide little support for expressing task-level parallelism.
Reference: [7] <author> Chen and Cowie. </author> <title> Prototyping FORTRAN-90 compilers for massively parallel machines. </title> <booktitle> In Proceedings of SIGPLAN PLDI, </booktitle> <year> 1992. </year>
Reference-contexts: With respect to parallel systems in general, a wide variety of high-level approaches to portable programming are being actively pursued. Global address-space languages [12] minimally extend a low-level language with global pointers. While efficiently implementable, they require programmer control of distribution, concurrency, and task granularity. Data parallel approaches <ref> [34, 7, 15] </ref> express parallelism across arrays, collections, or program constructs such as loops in the context of a single control flow model. Such programs achieve efficiency by grouping and scheduling operations on colocated data elements. However, they cannot easily express task-level or irregular concurrency. <p> Such programs achieve efficiency by grouping and scheduling operations on colocated data elements. However, they cannot easily express task-level or irregular concurrency. Further, with the exception of Fortran 90 <ref> [7] </ref>, data parallel languages provide no support for encapsulation and modularity. 7 Conclusions We have described the Concert System, an optimizing implementation for a concurrent object-oriented programming model. We detailed the features of our language, ICC++, that supports fine-grained concurrency and concurrent abstractions.
Reference: [8] <author> Andrew Chien, Julian Dolby, Bishwaroop Ganguly, Vijay Karamcheti, and Xingbin Zhang. </author> <title> Supporting high level programming with high performance: The Illinois Concert system. </title> <booktitle> In Proceedings of the Second International Workshop on High-level Parallel Programming Models and Supportive Environments, </booktitle> <month> April </month> <year> 1997. </year>
Reference-contexts: These demonstrations have repeatedly confirmed the benefits of high-level programming constructs and aggressive implementation techniques for achieving high performance, flexible application software <ref> [8] </ref>. Application demonstrations are used to illustrate the good speedups and high absolute performance levels achieved, and also to assess the impact of various optimizations on program performance. 1.1 Organization The rest of this paper is structured as below.
Reference: [9] <author> Jeffrey Dean, Craig Chambers, and David Grove. </author> <title> Selective specialization for object-oriented languages. </title> <booktitle> In Proceedings of the ACM SIGPLAN '95 Conference on Programmin g Language Design and Implementation, </booktitle> <pages> pages 93-102, </pages> <address> La Jolla, CA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Concert differs from all the above systems in its focus on supporting high-level programming features with efficient implementation techniques. This focus can be found in the context of sequential object-oriented languages <ref> [10, 16, 5, 9] </ref>, but our system additionally tackles the problems associated with concurrency, distribution and parallelism. With respect to parallel systems in general, a wide variety of high-level approaches to portable programming are being actively pursued. Global address-space languages [12] minimally extend a low-level language with global pointers.
Reference: [10] <author> L. Peter Deutsch and Allan M. Schiffman. </author> <title> Efficient implementation of the Smalltalk-80 system. </title> <booktitle> In Eleventh Symposium on Principles of Programming Languages, </booktitle> <pages> pages 297-302. </pages> <publisher> ACM, </publisher> <year> 1984. </year>
Reference-contexts: Concert differs from all the above systems in its focus on supporting high-level programming features with efficient implementation techniques. This focus can be found in the context of sequential object-oriented languages <ref> [10, 16, 5, 9] </ref>, but our system additionally tackles the problems associated with concurrency, distribution and parallelism. With respect to parallel systems in general, a wide variety of high-level approaches to portable programming are being actively pursued. Global address-space languages [12] minimally extend a low-level language with global pointers.
Reference: [11] <author> Julian Dolby. </author> <title> Automatic inline allocation of objects. </title> <booktitle> In Proceedings of the 1997 ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: This next pass of analysis results in the graph in Figure 1 (b). 4.2 Static Optimizations The Concert system implements three interprocedural static optimizations <ref> [11, 29] </ref> to reduce object access overhead and enlarge thread granularity: object inlining, method inlining and access region expansion. First, we apply object inlining to inline allocate object within other objects. Inline allocation lowers object access costs because the inlined objects consistency can be managed by the container object.
Reference: [12] <editor> A. Krishnamuthy et al. </editor> <booktitle> Parallel programming in Split-C. In Proceedings of Supercomputing, </booktitle> <pages> pages 262-273, </pages> <year> 1993. </year>
Reference-contexts: With respect to parallel systems in general, a wide variety of high-level approaches to portable programming are being actively pursued. Global address-space languages <ref> [12] </ref> minimally extend a low-level language with global pointers. While efficiently implementable, they require programmer control of distribution, concurrency, and task granularity. Data parallel approaches [34, 7, 15] express parallelism across arrays, collections, or program constructs such as loops in the context of a single control flow model.
Reference: [13] <author> A. Grimshaw. </author> <title> Easy-to-use object-oriented parallel processing with Mentat. </title> <journal> IEEE Computer, </journal> <volume> 5(26) </volume> <pages> 39-51, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Actor-based languages [1, 17, 36, 25] are most similar in terms of high-level programming support, but have focused less [33, 23] on efficient implementation. Task-parallel object-oriented languages, mostly based on C++ extensions <ref> [13, 19, 6] </ref>, support irregular parallelism and some location independence, but require programmer management of con-currency, storage-management, and task granularity which limits scalability and portability. Data-parallel object-oriented languages, such as pC++ [24], provide little support for expressing task-level parallelism.
Reference: [14] <institution> Concurrent Systems Architecture Group. </institution> <note> The ICC++ reference manual. Concurrent Systems Architecture Group Memo. Available from http://www-csag.cs.uiuc.edu/, May 1996. </note>
Reference-contexts: Language support consists of four parts: a general high level model, the expression of concurrency, concurrency control (synchronization), and management of large scale concurrency (via highly parallel object collections). We cover the salient features of ICC++ <ref> [14] </ref> in order. 2.1 General Features ICC++ supports flexible, modular programming for concurrent programs in a style similar to sequential programming.
Reference: [15] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiler optimizations for FORTRAN D on MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <month> August </month> <year> 1992. </year>
Reference-contexts: With respect to parallel systems in general, a wide variety of high-level approaches to portable programming are being actively pursued. Global address-space languages [12] minimally extend a low-level language with global pointers. While efficiently implementable, they require programmer control of distribution, concurrency, and task granularity. Data parallel approaches <ref> [34, 7, 15] </ref> express parallelism across arrays, collections, or program constructs such as loops in the context of a single control flow model. Such programs achieve efficiency by grouping and scheduling operations on colocated data elements. However, they cannot easily express task-level or irregular concurrency.
Reference: [16] <author> Urs Holzle. </author> <title> Adaptive Optimization for SELF: Reconciling High Performance with Exporatory Programming. </title> <type> PhD thesis, </type> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Concert differs from all the above systems in its focus on supporting high-level programming features with efficient implementation techniques. This focus can be found in the context of sequential object-oriented languages <ref> [10, 16, 5, 9] </ref>, but our system additionally tackles the problems associated with concurrency, distribution and parallelism. With respect to parallel systems in general, a wide variety of high-level approaches to portable programming are being actively pursued. Global address-space languages [12] minimally extend a low-level language with global pointers.
Reference: [17] <author> C. Houck and G. Agha. HAL: </author> <title> A high-level actor language and its distributed implementation. </title> <booktitle> In Proceedings of the 21st International Conference on Parallel Processing, </booktitle> <pages> pages 158-165, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Actor-based languages <ref> [1, 17, 36, 25] </ref> are most similar in terms of high-level programming support, but have focused less [33, 23] on efficient implementation.
Reference: [18] <author> Yuan-Shin Hwang, Raja Das, Joel Saltz, Bernard Brooks, and Milan Hodoscek. </author> <title> Parallelizing molecular dynamics programs for distributed memory machines. </title> <journal> IEEE Computational Science and Engineering, </journal> <pages> pages 18-29, </pages> <month> Summer </month> <year> 1995. </year>
Reference-contexts: The applications exhibit good speedups ranging from 8.5 on 16 nodes for Grobner to 54.8 on 64 nodes for the force phase of FMM. These speedups compare favorably with the best speedups reported elsewhere for hand-optimized codes <ref> [4, 30, 31, 18, 32] </ref>.
Reference: [19] <author> L. V. Kale and Sanjeev Krishnan. CHARM++: </author> <title> A portable concurrent object oriented system based on C++. </title> <booktitle> In Proceedings of OOPSLA'93, </booktitle> <pages> pages 91-108, </pages> <year> 1993. </year>
Reference-contexts: Actor-based languages [1, 17, 36, 25] are most similar in terms of high-level programming support, but have focused less [33, 23] on efficient implementation. Task-parallel object-oriented languages, mostly based on C++ extensions <ref> [13, 19, 6] </ref>, support irregular parallelism and some location independence, but require programmer management of con-currency, storage-management, and task granularity which limits scalability and portability. Data-parallel object-oriented languages, such as pC++ [24], provide little support for expressing task-level parallelism.
Reference: [20] <author> Vijay Karamcheti and Andrew A. Chien. </author> <title> A comparison of architectural support for messaging on the TMC CM-5 and the Cray T3D. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1995. </year> <note> Available from http://www-csag.cs.uiuc.edu/papers/cm5-t3d-messaging.ps. </note>
Reference-contexts: lazily create heap context on block Continuation passing Extension of may-block which allows forwarding on the stack Table 1: Various thread interaction schemas in the hybrid stack-heap execution model. 4.5 Fast Communication and Thread Scheduling To support fine-grained, distributed programs efficiently, the Concert implementation is built atop Fast Messages (FM) <ref> [20] </ref>, which utilizes novel implementation techniques such as receiver-initiated data transfer to support high-performance messaging in the face of irregular communication that is unsynchronized with ongoing computation (a consequence of our dynamic programming model).
Reference: [21] <author> Vijay Karamcheti and Andrew A. Chien. </author> <title> View caching: Efficient software shared memory for dynamic computations. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium, </booktitle> <year> 1997. </year>
Reference-contexts: and lift access regions above loops and conditionals, as shown in Figure 2 (c), to create regions of optimized sequential code with the efficiency of a sequential uniprocessor implementation. 4.3 Locality Optimizations Since global pointer-based data structures are fundamental for many dynamic (e.g. data-dependent) computations, Concert supports two locality optimizations <ref> [21, 37] </ref> to efficiently implement such structures on modern architectures with deep memory hierarchies, such as NUMA machines, whether cache-coherent or not. When static coarse-grained aliasing information is available, we apply dynamic pointer alignment, a generalization of static loop tiling and communication optimizations. <p> At run time, the program concurrency structure allows these iterations to be reordered dynamically, guided by runtime data access information, to maximize data reuse and hide communication latency. View caching <ref> [21] </ref> supports efficient runtime object caching in dynamic computations, relying on application knowledge of data access semantics to construct customized latency-tolerant coherence protocols that require reduced message traffic and synchronization.
Reference: [22] <author> Vijay Karamcheti, John Plevyak, and Andrew A. Chien. </author> <title> Runtime mechanisms for efficient dynamic multithreading. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 37(1) </volume> <pages> 21-40, </pages> <year> 1996. </year> <note> Available from http://www-csag.cs.uiuc.edu/papers/rtperf.ps. </note>
Reference-contexts: Our hybrid stack-heap execution model <ref> [28, 22] </ref> provides a flexible runtime interface to the compiler, shown in Table 1, allowing it to generate code which optimistically executes a logical thread sequentially on its caller's stack, lazily creating a different thread only when the callee computation needs to suspend or be scheduled separately. <p> For example, robust communication is important at small numbers of processors when communication traffic is high, and load-balancing is essential for large numbers of processors. Space limitations prevent us from a detailed analysis for the other applications; the reader is referred elsewhere <ref> [38, 22] </ref> for additional details. 6 Related Work The Concert system is related to a wide variety of work on concurrent object-oriented languages that can be loosely classified as actor-based, task-parallel, and data-parallel.
Reference: [23] <author> Woo Young Kim and Gul Agha. </author> <title> Efficient support for location transparency in concurrent object-oriented programming languages. </title> <booktitle> In Proceedings of the Supercomputing '95 Conference, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Actor-based languages [1, 17, 36, 25] are most similar in terms of high-level programming support, but have focused less <ref> [33, 23] </ref> on efficient implementation. Task-parallel object-oriented languages, mostly based on C++ extensions [13, 19, 6], support irregular parallelism and some location independence, but require programmer management of con-currency, storage-management, and task granularity which limits scalability and portability.
Reference: [24] <author> J. Lee and D. Gannon. </author> <title> Object oriented parallel programming. </title> <booktitle> In Proceedings of the ACM/IEEE Conference on Supercomputing. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: Task-parallel object-oriented languages, mostly based on C++ extensions [13, 19, 6], support irregular parallelism and some location independence, but require programmer management of con-currency, storage-management, and task granularity which limits scalability and portability. Data-parallel object-oriented languages, such as pC++ <ref> [24] </ref>, provide little support for expressing task-level parallelism. HPC++ [2] is similar, expressing concurrency primarily as parallel operations across homogenous collections. ICC++ expresses data parallelism as task-level concurrency, providing greater programming power, of an optimization results in a 35-80% performance drop. but making efficient implementation significantly more challenging.
Reference: [25] <author> Stephan Murer, Jerome A. Feldman, Chu-Cheow Lim, and Martina-Maria Seidel. pSather: </author> <title> Layered extensions to an object-oriented language for efficient parallel computation. </title> <type> Technical Report TR-93-028, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> CA, </address> <month> June </month> <year> 1993 </year> <month> November </month> <year> 1993. </year>
Reference-contexts: Actor-based languages <ref> [1, 17, 36, 25] </ref> are most similar in terms of high-level programming support, but have focused less [33, 23] on efficient implementation.
Reference: [26] <author> John Plevyak. </author> <title> Optimization of Object-Oriented and Concurrent Programs. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, Urbana, Illinois, </institution> <year> 1996. </year>
Reference-contexts: The optimized generated code then leverages Concert's efficient runtime mechanisms for communication and thread scheduling, to execute concurrently. 4.1 Program Analysis The Concert compiler implements global program analysis <ref> [26, 27] </ref> to obtain a variety of information: types of variables to resolve dynamic disptach, relative locality of objects, and container objects for storage optimizations. The analysis is context sensitive and adapts, in a demand-driven manner, to program structure.
Reference: [27] <author> John Plevyak and Andrew A. Chien. </author> <title> Precise concrete type inference of object-oriented programs. </title> <booktitle> In Proceedings of OOPSLA'94, Object-Oriented Programming Systems, Languages and Architectures, </booktitle> <pages> pages 324-340, </pages> <year> 1994. </year>
Reference-contexts: The optimized generated code then leverages Concert's efficient runtime mechanisms for communication and thread scheduling, to execute concurrently. 4.1 Program Analysis The Concert compiler implements global program analysis <ref> [26, 27] </ref> to obtain a variety of information: types of variables to resolve dynamic disptach, relative locality of objects, and container objects for storage optimizations. The analysis is context sensitive and adapts, in a demand-driven manner, to program structure.
Reference: [28] <author> John Plevyak, Vijay Karamcheti, Xingbin Zhang, and Andrew Chien. </author> <title> A hybrid execution model for fine-grained languages on distributed memory multicomputers. </title> <booktitle> In Proceedings of Supercomputing'95, </booktitle> <year> 1995. </year>
Reference-contexts: Our hybrid stack-heap execution model <ref> [28, 22] </ref> provides a flexible runtime interface to the compiler, shown in Table 1, allowing it to generate code which optimistically executes a logical thread sequentially on its caller's stack, lazily creating a different thread only when the callee computation needs to suspend or be scheduled separately.
Reference: [29] <author> John Plevyak, Xingbin Zhang, and Andrew A. Chien. </author> <title> Obtaining sequential efficiency in concurrent object-oriented programs. </title> <booktitle> In Proceedings of the ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 311-321, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: This next pass of analysis results in the graph in Figure 1 (b). 4.2 Static Optimizations The Concert system implements three interprocedural static optimizations <ref> [11, 29] </ref> to reduce object access overhead and enlarge thread granularity: object inlining, method inlining and access region expansion. First, we apply object inlining to inline allocate object within other objects. Inline allocation lowers object access costs because the inlined objects consistency can be managed by the container object.
Reference: [30] <author> Daniel J. Scales and Monica S. Lam. </author> <title> The design and evaluation of a shared object system for distributed memory machines. </title> <booktitle> In First Symposium on Operating Systems Design and Implementation, </booktitle> <year> 1994. </year>
Reference-contexts: The applications exhibit good speedups ranging from 8.5 on 16 nodes for Grobner to 54.8 on 64 nodes for the force phase of FMM. These speedups compare favorably with the best speedups reported elsewhere for hand-optimized codes <ref> [4, 30, 31, 18, 32] </ref>.
Reference: [31] <author> Jaswinder Pal Singh, Anoop Gupta, and Marc Levoy. </author> <title> Parallel visualization algorithms: Performance and architectural implications. </title> <journal> IEEE Computer, </journal> <volume> 27(7) </volume> <pages> 45-56, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: The applications exhibit good speedups ranging from 8.5 on 16 nodes for Grobner to 54.8 on 64 nodes for the force phase of FMM. These speedups compare favorably with the best speedups reported elsewhere for hand-optimized codes <ref> [4, 30, 31, 18, 32] </ref>. <p> These speedups compare favorably with the best speedups reported elsewhere for hand-optimized codes [4, 30, 31, 18, 32]. For example, the Radiosity speedup of 23 on 32 T3D processors compares well with the previously reported speedup of 26 on 32 processors of the DASH machine <ref> [31] </ref>, despite hardware support for cache-coherent shared memory and an order of magnitude faster communication (in terms of processor clocks) in the DASH which better facilitates scalable performance.
Reference: [32] <author> Jaswinder Pal Singh, Chris Holt, John L. Hennessy, and Anoop Gupta. </author> <title> A parallel adaptive fast multipole method. </title> <booktitle> In Proceedings of Supercomputing Conference, </booktitle> <pages> pages 54-65, </pages> <year> 1993. </year>
Reference-contexts: The applications exhibit good speedups ranging from 8.5 on 16 nodes for Grobner to 54.8 on 64 nodes for the force phase of FMM. These speedups compare favorably with the best speedups reported elsewhere for hand-optimized codes <ref> [4, 30, 31, 18, 32] </ref>.
Reference: [33] <author> Kenjiro Taura, Satoshi Matsuoka, and Akinori Yonezawa. StackThreads: </author> <title> An abstract machine for scheduling fine-grain threads on stock CPUs. </title> <booktitle> In Joint Symposium on Parallel Processing, </booktitle> <year> 1994. </year>
Reference-contexts: Actor-based languages [1, 17, 36, 25] are most similar in terms of high-level programming support, but have focused less <ref> [33, 23] </ref> on efficient implementation. Task-parallel object-oriented languages, mostly based on C++ extensions [13, 19, 6], support irregular parallelism and some location independence, but require programmer management of con-currency, storage-management, and task granularity which limits scalability and portability.
Reference: [34] <author> Thinking Machines Corporation. </author> <title> Getting Started in CM Fortran, </title> <year> 1990. </year>
Reference-contexts: With respect to parallel systems in general, a wide variety of high-level approaches to portable programming are being actively pursued. Global address-space languages [12] minimally extend a low-level language with global pointers. While efficiently implementable, they require programmer control of distribution, concurrency, and task granularity. Data parallel approaches <ref> [34, 7, 15] </ref> express parallelism across arrays, collections, or program constructs such as loops in the context of a single control flow model. Such programs achieve efficiency by grouping and scheduling operations on colocated data elements. However, they cannot easily express task-level or irregular concurrency.
Reference: [35] <author> Steven Cameron Woo, Moriyoshi Ohara, Evan Torrie, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> The SPLASH-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <year> 1995. </year>
Reference-contexts: Program Input Grobner pavelle5 [4] Grobner basis IC-Cedar Myoglobin Molecular dynamics Radiosity Room <ref> [35] </ref> Hierarchical radiosity Barnes 16K bodies Hierarchical N-body FMM 32K bodies Hierarchical N-body FMM are only for the force phases. The speedup numbers are comparable to the best reported for low-level programming approaches.
Reference: [36] <author> A. Yonezawa, E. Shibayama, T. Takada, and Y. Honda. </author> <title> Object-oriented concurrent programming - modelling and programming in an object-oriented concurrent language ABCL/1. </title> <editor> In Aki Yonezawa and Mario Tokoro, editors, </editor> <booktitle> Object-Oriented Concurrent Programming, </booktitle> <pages> pages 55-89. </pages> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Actor-based languages <ref> [1, 17, 36, 25] </ref> are most similar in terms of high-level programming support, but have focused less [33, 23] on efficient implementation.
Reference: [37] <author> Xingbin Zhang and Andrew A. Chien. </author> <title> Dynamic pointer alignment: Tiling and communication optimizations for parallel pointer-based computations. </title> <booktitle> In Proceedings of ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Las Vegas, Nevada, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: and lift access regions above loops and conditionals, as shown in Figure 2 (c), to create regions of optimized sequential code with the efficiency of a sequential uniprocessor implementation. 4.3 Locality Optimizations Since global pointer-based data structures are fundamental for many dynamic (e.g. data-dependent) computations, Concert supports two locality optimizations <ref> [21, 37] </ref> to efficiently implement such structures on modern architectures with deep memory hierarchies, such as NUMA machines, whether cache-coherent or not. When static coarse-grained aliasing information is available, we apply dynamic pointer alignment, a generalization of static loop tiling and communication optimizations.
Reference: [38] <author> Xingbin Zhang, Vijay Karamcheti, Tony Ng, and Andrew Chien. </author> <title> Optimizing COOP languages: Study of a protein dynamics program. </title> <booktitle> In IPPS'96, </booktitle> <year> 1996. </year>
Reference-contexts: For example, robust communication is important at small numbers of processors when communication traffic is high, and load-balancing is essential for large numbers of processors. Space limitations prevent us from a detailed analysis for the other applications; the reader is referred elsewhere <ref> [38, 22] </ref> for additional details. 6 Related Work The Concert system is related to a wide variety of work on concurrent object-oriented languages that can be loosely classified as actor-based, task-parallel, and data-parallel.
References-found: 38

