URL: http://cobar.cs.umass.edu/pubfiles/callancroftipm94.ps.gz
Refering-URL: http://cobar.cs.umass.edu/pubfiles/
Root-URL: 
Email: E-mail: fcallan, croft, brogliog@cs.umass.edu  
Title: TREC and TIPSTER Experiments With INQUERY  
Author: James P. Callan, W. Bruce Croft and John Broglio 
Note: To appear in Information Processing and Management.  
Date: July 8, 1994  
Address: Amherst, MA 01003-4610, USA  
Affiliation: Computer Science Department University of Massachusetts  
Abstract: INQUERY is a probablistic information retrieval system based upon a Bayesian inference network model. This paper describes recent improvements to the system as a result of participation in the TIPSTER project and the TREC-2 conference. Improvements include transforming forms-based specifications of information needs into complex structured queries, automatic query expansion, automatic recognition of features in documents, relevance feedback, and simulated document routing. Experiments with one and two gigabyte document collections are also described. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. R. Caid, S. T. Dumais, and S. I. Gallant. </author> <title> Learned vector-space models for document retrieval. Information Processing and Management, </title> <note> (this issue). </note>
Reference-contexts: Although very different in implementation, the approach is similar in spirit to the distributed representations employed by the MatchPlus and Bellcore systems <ref> [1] </ref>. The usual document retrieval algorithms (discussed in Section 2.4) are used to retrieve the pseudo-documents that represent concepts. Thus, INQUERY can use any structured query to retrieve a ranked list of concepts. <p> has sometimes proved to be effective in increasing the quality of retrieval results. 3.2 Query Evaluation The formula that determines belief in a document due to the occurrence of a term (Equation 1) scales the log of tf by the log of max tf , producing values in the range <ref> [0; 1] </ref>. One consequence of this approach is that it favors long documents. For example, if a term occurs 3 times in a document and the most frequent term occurs 6 times, the result is log (3+0:5) log (6+1) = 0:644.
Reference: [2] <author> J. P. Callan. </author> <title> Passage-level evidence in document retrieval. </title> <booktitle> In Proceedings of the Seventeenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 302-310, </pages> <address> Dublin, Ireland, </address> <year> 1994. </year> <institution> Association for Computing Machinery. </institution>
Reference-contexts: However, we have recently experienced more success with overlapping fixed-length passages of 200-300 words <ref> [2] </ref>. The apparent explanations are that heuristics for identifying paragraphs are imperfect, and that authors are not consistent in their use of paragraphs. Acknowledgements We thank Bob Krovetz, David Haines, Stephen Harding, Yufeng Jing, Michelle LaMar, Dan Nachbar and Margie Connell for their assistance in the work described here.
Reference: [3] <author> J. P. Callan and W. B. Croft. </author> <title> An evaluation of query processing strategies using the TIPSTER collection. </title> <editor> In R. Korfhage, E. Rasmussen, and P. Willett, editors, </editor> <booktitle> Proceedings of the Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 347-356, </pages> <address> Pittsburgh, PA, </address> <month> June </month> <year> 1993. </year> <institution> Association for Computing Machinery. </institution>
Reference-contexts: Natural language queries are transformed incrementally into complex structured queries in the INQUERY query language by a series of query text processing modules <ref> [3] </ref>. Query text processing must minimally mirror the indexing text processing. But because query texts are much shorter than document collections, it is practical to experiment with more thorough textual analysis at the research and development stage. <p> All query text processing is experimental and the sequence of operations is adjusted frequently as more is learned about the effects of this processing. Currently, INQUERY has a small number of internal query text processors <ref> [3] </ref>.
Reference: [4] <author> J. P. Callan, W. B. Croft, and S. M. Harding. </author> <title> The INQUERY retrieval system. </title> <booktitle> In Proceedings of the Third International Conference on Database and Expert Systems Applications, </booktitle> <pages> pages 78-83, </pages> <address> Valencia, Spain, 1992. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Learning techniques are used to modify the initial queries both for short-term and long-term information needs (relevance feedback and routing, respectively). This approach, generally known as the inference net model and implemented in the INQUERY system <ref> [4] </ref>, emphasizes retrieval based on combination of evidence. Different text representations (such as words, phrases, paragraphs, or manually assigned keywords) and different versions of the query (such as natural language and Boolean) can be combined in a consistent probabilistic framework. <p> In general, it is more effective (as well as efficient) to analyze short query texts rather than millions of document texts. The results of the query analysis are represented in the INQUERY query language which contains a number of operators, such as #SUM, #AND, #OR, #NOT, #PHRASE, and #SYN <ref> [13; 4] </ref>. These operators implement different methods of combining evidence. <p> john davenport 52 year old appoint chief execut offic intern telecommun concern u.s. #USA subsidiar cabl wireless north america inc #COMPANY davenport succee john zrno current gener manag group oper bermuda #FOREIGNCOUNTRY 2 The INQUERY System The INQUERY document retrieval and routing system is based on the inference network model <ref> [13; 4] </ref>. The main processes in INQUERY are document indexing, query processing, query expansion, query evaluation, and relevance feedback. <p> The belief in a document due to a given query language operator depends on the type of operator and the belief in its arguments. The query language operators have been discussed in detail elsewhere <ref> [13; 4] </ref>, so we merely provide several examples to illustrate their general operation. #WSUM is a weighted sum operator, #UWn is an unordered window proximity operator, and #PROXn is an ordered interword proximity operator. bel wsum (w 1 Q 1 ; : : : ; w m Q m ) =
Reference: [5] <author> Kenneth Church. </author> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Proceedings of the 2nd Conference on Applied Natural Language Processing, </booktitle> <pages> pages 136-143, </pages> <year> 1988. </year>
Reference-contexts: These highlighted words would then be treated as key concepts in the query processing. Natural language query fields are tagged for syntactic category by a part-of-speech (POS) tagger <ref> [5] </ref>. Additionally, we change operator phrases to single words in order to simplify later processing. An example of this simplification is replacing the phrase in order to with the infinitive particle to or replacing with respect to with the word regarding.
Reference: [6] <author> W. Bruce Croft and Howard R. </author> <title> Turtle. Text retrieval and inference. </title> <editor> In P. Jacobs, editor, </editor> <booktitle> Text-Based Intelligent Systems, </booktitle> <pages> pages 127-156. </pages> <publisher> Lawrence Erlbaum, </publisher> <year> 1992. </year>
Reference-contexts: Our approach has been to use improved representations of document text and queries in the framework of the inference network model of retrieval. This model uses Bayesian networks to describe how text and queries should be used to identify relevant documents <ref> [11; 6; 12] </ref>. Document retrieval and routing are viewed as probabilistic inference processes that compare text representations based on different forms of linguistic and statistical evidence to representations of information needs based on similar evidence from natural language queries and user interaction.
Reference: [7] <author> David Haines and W. B. Croft. </author> <title> Relevance feedback and inference networks. </title> <editor> In R. Ko-rfhage, E. Rasmussen, and P. Willett, editors, </editor> <booktitle> Proceedings of the Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 2-11, </pages> <address> Pittsburgh, PA, </address> <month> June </month> <year> 1993. </year> <institution> Association for Computing Machinery. </institution>
Reference-contexts: The general approach is for the system to select terms from relevant documents, add them to the query, and then reweight all of the query terms. 6 Early experiments <ref> [7] </ref> showed that ranking terms by the product of their frequency in relevant documents (rdf ) and their inverse document frequency (idf ) was best on small and medium-sized collections with relatively small numbers of relevance judgements. The number of terms added was set empirically to 5. <p> INQ003: Created automatically from TIPSTER topics and relevance judgements from Volumes 1 and 2. Baseline queries (from a previous TIPSTER evaluation) were modified by reweighting and adding single-word terms. The term weighting and selection function used was df.idf, as described in <ref> [7] </ref>. Only the top 120 relevant documents found by INQUERY were used for feedback, and 30 terms were added to each query. INQ004: Formed by combining (using the #SUM operator) INQ003 queries and IN-QRYP queries (used in TIPSTER 18 month evaluation).
Reference: [8] <editor> D. Harman, editor. </editor> <booktitle> The Second Text REtrieval Conference (TREC2). National Institute of Standards and Technology Special Publication 500-215, </booktitle> <address> Gaithersburg, MD, </address> <year> 1994. </year>
Reference-contexts: It prevented INQUERY from being biased unduly towards long documents, but still allowed them to be retrieved. Only four of the TREC-2 systems retrieved more relevant Federal Register documents than did INQUERY <ref> [8] </ref>. Each of those systems also retrieved at least twice as many non-relevant Federal Register documents as did INQUERY. 3.3 Routing Our approach to the routing portion of our TIPSTER and TREC work was based initially upon our existing relevance feedback mechanisms.
Reference: [9] <author> Y. Jing and W. B. Croft. </author> <title> An association thesaurus for information retrieval. </title> <booktitle> In RIAO 4 Conference Proceedings, </booktitle> <address> New York, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: The current implementation of PhraseFinder shows promise on the TIPSTER data, but more work is necessary to build a PhraseFinder that would be effective on a variety of document collections <ref> [9] </ref>. 2.4 Query Evaluation The query evaluation process uses the inverted files and the query represented as an inference net to produce a document ranking.
Reference: [10] <editor> B. Sundheim, editor. </editor> <booktitle> Proceedings of the Third Message Understanding Evaluation and Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <year> 1991. </year>
Reference-contexts: In theory, every word in the document collection will be indexed. In practice, it is helpful to identify very common words, such as operators or closed-class words, which do not carry any meaningful information for retrieval purposes (although they may offer significant information for text extraction <ref> [10] </ref>). These stopwords are usually not indexed, although they are retained in the text so that subsequent textual analysis (syntactic analysis, feature recognition) may make use of them.
Reference: [11] <author> H. R. Turtle and W. B. Croft. </author> <title> Evaluation of an inference network-based retrieval model. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 9(3) </volume> <pages> 187-222, </pages> <year> 1991. </year>
Reference-contexts: Our approach has been to use improved representations of document text and queries in the framework of the inference network model of retrieval. This model uses Bayesian networks to describe how text and queries should be used to identify relevant documents <ref> [11; 6; 12] </ref>. Document retrieval and routing are viewed as probabilistic inference processes that compare text representations based on different forms of linguistic and statistical evidence to representations of information needs based on similar evidence from natural language queries and user interaction.
Reference: [12] <author> H. R. Turtle and W. B. Croft. </author> <title> A comparison of text retrieval models. </title> <journal> Computer Journal, </journal> <year> 1992. </year>
Reference-contexts: Our approach has been to use improved representations of document text and queries in the framework of the inference network model of retrieval. This model uses Bayesian networks to describe how text and queries should be used to identify relevant documents <ref> [11; 6; 12] </ref>. Document retrieval and routing are viewed as probabilistic inference processes that compare text representations based on different forms of linguistic and statistical evidence to representations of information needs based on similar evidence from natural language queries and user interaction.
Reference: [13] <author> Howard R. Turtle and W. Bruce Croft. </author> <title> Efficient probabilistic inference for text retrieval. </title> <booktitle> In RIAO 3 Conference Proceedings, </booktitle> <pages> pages 644-661, </pages> <address> Barcelona, Spain, </address> <month> April </month> <year> 1991. </year> <month> 21 </month>
Reference-contexts: In general, it is more effective (as well as efficient) to analyze short query texts rather than millions of document texts. The results of the query analysis are represented in the INQUERY query language which contains a number of operators, such as #SUM, #AND, #OR, #NOT, #PHRASE, and #SYN <ref> [13; 4] </ref>. These operators implement different methods of combining evidence. <p> john davenport 52 year old appoint chief execut offic intern telecommun concern u.s. #USA subsidiar cabl wireless north america inc #COMPANY davenport succee john zrno current gener manag group oper bermuda #FOREIGNCOUNTRY 2 The INQUERY System The INQUERY document retrieval and routing system is based on the inference network model <ref> [13; 4] </ref>. The main processes in INQUERY are document indexing, query processing, query expansion, query evaluation, and relevance feedback. <p> The belief in a document due to a given query language operator depends on the type of operator and the belief in its arguments. The query language operators have been discussed in detail elsewhere <ref> [13; 4] </ref>, so we merely provide several examples to illustrate their general operation. #WSUM is a weighted sum operator, #UWn is an unordered window proximity operator, and #PROXn is an ordered interword proximity operator. bel wsum (w 1 Q 1 ; : : : ; w m Q m ) =
References-found: 13

