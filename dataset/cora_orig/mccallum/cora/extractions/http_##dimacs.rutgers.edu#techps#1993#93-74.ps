URL: http://dimacs.rutgers.edu/techps/1993/93-74.ps
Refering-URL: http://dimacs.rutgers.edu/TechnicalReports/1993.html
Root-URL: http://www.cs.rutgers.edu
Title: Brownian Bandits  where the supremum is over all stopping times t of B i Put  
Author: Avi Mandelbaum Robert J. Vanderbei M i fx IR d i i max 
Keyword: Key words: Gittins indices, Brownian motion, optimal stopping, optimal switching  
Note: i i sup  AMS 1991 Subject Classification: Primary 49L05, Secondary 60J65 Research partially supported by The Fund for the Promotion of Research at the Technion and AT&T Bell Labs. Research partially supported by the Air Force through grants AFOSR-91-0359 and AFOSR F49620-93 1-0098 and AT&T Bell Labs.  
Address: New Brunswick, NJ 08903  i  i (B i E i  
Affiliation: Rutgers University  E  e t r  e t dt  
Pubnum: DIMACS  Technical Report  
Phone: 0  0  
Date: November 8, 1993  93-74  
Abstract: We consider a multi-armed bandit whose arms are driven by Brownian motions: the state of arm i is modeled as a one-dimensional Brownian motion B i , i = 1; : : : ; d. At each instant in time, a gambler must decide to pull some subset of these d arms, holding the others fixed at their current state. If arm i is pulled when B i is in state x i , the gambler accumulates reward at rate r i (x i ). The goal is to find a strategy that maximizes the accumulated discounted reward over an infinite horizon (with discount rate ). Let j (x j )g: From prior work, one expects that an optimal control is to pull arm i when the state of the bandit belongs to M i . Equivalently, the optimal strategy follows the leader among the processes i (B i ), i = 1; : : : ; d. Such results have been established for bounded monotone reward functions. In this paper we extend the scope to cover certain unimodal functions. At the same time, we develop a framework within which general rewards and diffusions can be studied.
Abstract-found: 1
Intro-found: 1
Reference: [Dal90] <author> R.C. Dalang. </author> <title> Randomization in the two-armed bandit problem. </title> <journal> Ann. Prob., </journal> <volume> 18 </volume> <pages> 218-225, </pages> <year> 1990. </year>
Reference-contexts: The function v is called the value function for the multi-armed bandit problem. This problem was studied by Karatzas [Kar84], Mandelbaum [Man87] and Dalang <ref> [Dal90] </ref> as a continuous time generalization of Gittins' index theorem for Markov chains (see, e.g., [Whi82] Chapter 14).
Reference: [GS68] <author> B.I. </author> <title> Grigelionis and A.N. Shiryaev. Controllable Markov processes and Stefan's problem. </title> <journal> Problemy Peredachi Informatsii, </journal> <volume> 4 </volume> <pages> 60-72, </pages> <year> 1968. </year>
Reference-contexts: The principle of smooth fit (see, e.g., <ref> [GS68] </ref>) says that D is determined by the condition that the function v be "smooth" at the boundary of D. The following theorem makes this precise. (3.4) Theorem Suppose there exists an open set D and a function w satisfying (3.2) and (3.3).
Reference: [Kar84] <author> I. Karatzas. </author> <title> Gittins indices in the dynamic allocation problem for diffusion processes. </title> <journal> Ann. Prob., </journal> <volume> 12 </volume> <pages> 173-192, </pages> <year> 1984. </year> <month> 19 </month>
Reference-contexts: The function v is called the value function for the multi-armed bandit problem. This problem was studied by Karatzas <ref> [Kar84] </ref>, Mandelbaum [Man87] and Dalang [Dal90] as a continuous time generalization of Gittins' index theorem for Markov chains (see, e.g., [Whi82] Chapter 14).
Reference: [KM93] <author> H. Kaspi and A. Mandelbaum. Levy bandits: </author> <title> Multi-armed bandits driven by levy processes, </title> <note> 1993. In preparation. </note>
Reference-contexts: The existence of such a strategy was established in [Man87], for continuous i . (An alternative construction for d = 2 was described in [MSV90].) An extension to ^ i whose sample paths are right-continuous with left limits is carried out in <ref> [KM93] </ref>. Switching among Brownian motions is naturally quantified in terms of local times. To see that, consider for example a multi-armed bandit whose reward functions are monotone increasing and equal to each other.
Reference: [Man87] <author> A. Mandelbaum. </author> <title> Continuous multi-armed bandits and multi-parameter processes. </title> <journal> Ann. Prob., </journal> <volume> 15 </volume> <pages> 1527-1556, </pages> <year> 1987. </year>
Reference-contexts: The function v is called the value function for the multi-armed bandit problem. This problem was studied by Karatzas [Kar84], Mandelbaum <ref> [Man87] </ref> and Dalang [Dal90] as a continuous time generalization of Gittins' index theorem for Markov chains (see, e.g., [Whi82] Chapter 14). <p> Along the way certain technical assumptions will be required. In Sections 7 and 8 we analyze certain classes of monotone and unimodal reward functions and verify the technical assumptions in those cases. As is clear from <ref> [Man87] </ref>, the optimality of Gittins' index strategies holds in great generality. In this paper we analyze certain classes of monotone and unimodal functions, but our framework seems appropriate to accomodate general rewards. However, technical issues remain open and we hope that others will be inspired to join in their resolution. <p> The existence of such a strategy was established in <ref> [Man87] </ref>, for continuous i . (An alternative construction for d = 2 was described in [MSV90].) An extension to ^ i whose sample paths are right-continuous with left limits is carried out in [KM93]. Switching among Brownian motions is naturally quantified in terms of local times. <p> Then an optimal strategy T fl simply follows the leader among B i ; i = 1; : : : ; d. Assume for simplicity of exposition that B i (0) = B j (0) for all i; j. Then, as explained in <ref> [Man87] </ref>, the strategy T fl is uniquely determined by the relations B 1 1 T fl (t) ; t 0: Introduce the excursion process ~ = (~ 1 ; : : : ; ~ d ) by ~ i T fl (t) B i i 11 One can then show that
Reference: [MSV90] <author> A. Mandelbaum, L.A. Shepp, and R.J. Vanderbei. </author> <title> Optimal switching between a pair of Brownian motions. </title> <journal> Ann. Prob., </journal> <volume> 18 </volume> <pages> 1010-1033, </pages> <year> 1990. </year>
Reference-contexts: Then w is the value function v, and ~ T is an optimal switching strategy. 3 Proof. Let Z t = e t w (B T (t)). In Section 3.1 of <ref> [MSV90] </ref> it was shown that, for any switching strategy T , the quadratic variation of B i T is T i , T i t = T i (t); and that the quadratic covariation between B i T and B T vanishes (for i 6= j). <p> The existence of such a strategy was established in [Man87], for continuous i . (An alternative construction for d = 2 was described in <ref> [MSV90] </ref>.) An extension to ^ i whose sample paths are right-continuous with left limits is carried out in [KM93]. Switching among Brownian motions is naturally quantified in terms of local times.
Reference: [Whi82] <author> P. Whittle. </author> <title> Optimization over Time: Dynamic Programming and Stochastic Control. </title> <publisher> Wiley, </publisher> <year> 1982. </year> <month> 20 </month>
Reference-contexts: The function v is called the value function for the multi-armed bandit problem. This problem was studied by Karatzas [Kar84], Mandelbaum [Man87] and Dalang [Dal90] as a continuous time generalization of Gittins' index theorem for Markov chains (see, e.g., <ref> [Whi82] </ref> Chapter 14). Assuming each of the r i are bounded strictly increasing functions, it was shown that there exist index functions i which determine the following optimal strategy; run the process with the largest i (B i ) (this will be made precise momentarily).
References-found: 7

