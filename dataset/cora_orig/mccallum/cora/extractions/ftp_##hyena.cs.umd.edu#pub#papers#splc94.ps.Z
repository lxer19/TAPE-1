URL: ftp://hyena.cs.umd.edu/pub/papers/splc94.ps.Z
Refering-URL: http://www.cs.umd.edu/projects/hpsl/hpio/papers/splc94.html
Root-URL: 
Email: frobertb,ksb,als,raja,saltzg@cs.umd.edu  
Title: Jovian: A Framework for Optimizing Parallel I/O  
Author: Robert Bennett Kelvin Bryant Alan Sussman Raja Das Joel Saltz 
Address: College Park, MD 20742  
Affiliation: Department of Computer Science and UMIACS University of Maryland  
Abstract: There has been a great deal of recent interest in parallel I/O. In this paper, we discuss the design and implementation of the Jovian library, which is intended to optimize the I/O performance of multiprocessor architectures that include multiple disks or disk arrays. We also present preliminary performance measurements from benchmarking the Jovian I/O library on the IBM SP1 distributed memory parallel machine for two application templates. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> Compiler and runtime support for structured and block structured applications. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 578-587. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: Access to a compact global descriptor will allow the Jovian library to skip the collective communication stage that is needed when performing collective local I/O optimization. The collective global I/O optimizations in the Jovian library will be similar to the optimizations carried out by the multiblock Parti library <ref> [1, 12] </ref>; we will be able to use many ideas from multiblock Parti to construct this part of the Jovian library. The paper is organized as follows. <p> In that case, the user program is responsible for translating local data structure addresses into the correct global addresses for accessing the entire out-of-core data structure (perhaps through calls to a runtime library such as CHAOS [5] or multiblock PARTI <ref> [1] </ref>). A global view of out-of-core data structures is the natural view for a compiler for a parallelizing compiler for languages such as HPF or Fortran D. <p> For communication, our techniques have been validated for irregular, block structured and structured data access patterns <ref> [1, 4, 11, 10] </ref>. Extending these techniques to handle out-of-core data structures requires using the local memories as "caches" for out-of-core data. <p> Each C/P was responsible for N P C=P rows of N data elements. In this case each data element is a double, which is 8bytes on the RS6000. The application was parallelized using the Multiblock PARTI runtime library <ref> [1] </ref>. The library provides the functionality to lay out distributed arrays. Using Multiblock PARTI, the grid was distributed by block in both dimensions across the A/Ps. Each sub-grid was of size N= p p Also, the library provided the functionality by which the range request information was generated.
Reference: [2] <author> Rajesh Bordawekar, Rajeev Thakur, and Alok Choudhary. </author> <title> Efficient compilation of out-of-core data parallel programs. </title> <type> Technical Report SCCS 622, </type> <institution> NPAC, </institution> <month> April </month> <year> 1994. </year> <note> Submitted to Supercomputing '94. </note>
Reference-contexts: The library can make use of a varying number of coalescing processes (fixed before program execution) to carry out this aggregation process. We will call this kind of optimization collective local I/O optimization. There are at least two other projects that aim to perform collective local I/O optimizations. Choud-hary <ref> [2, 6] </ref> assumes that each processor has access to either a physical or logical I/O device. They partition parallel I/O into two phases, where processors first read data in a layout that corresponds to the logical disk layout, and then perform in-memory permutations to lay out the data as required. <p> Strip mining algorithms for out-of-core compilation for regular data distributions and regular access patterns are described by Choud-hary <ref> [2] </ref>. The main goal of these optimizations is to minimize both the number of I/O operations (since each one is a collective operation) and the total I/O volume. 4 Jovian I/O runtime library The model that was used to develop the Jovian Runtime Library is illustrated in Figure 1.
Reference: [3] <author> P.F. Corbett and D.G. Feitelson. </author> <title> Design and implementation of the Vesta parallel file system. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-94), </booktitle> <pages> pages 63-70. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1994. </year>
Reference-contexts: The C/Ps then perform the necessary disk I/O operations and service the I/O requests. In this model, C/Ps can potentially access disjoint data files; that is, in the best possible scenario, each C/P has access to a local disk or to a parallel file system (e.g. IBM's Vesta <ref> [3] </ref>) that supports parallel access to disjoint partitions (subfiles of a global file). The basic phases employed in the library to handle I/O requests are: 1. All application processes send I/O requests to predetermined coalescing processes (each application process sends requests to one coalescing process). 2.
Reference: [4] <author> Raja Das, Joel Saltz, and Reinhard von Hanxle-den. </author> <title> Slicing analysis and indirect access to distributed arrays. </title> <booktitle> In Proceedings of the 6th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 152-168. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1993. </year> <note> Also available as University of Maryland Technical Report CS-TR-3076 and UMIACS-TR-93-42. </note>
Reference-contexts: For communication, our techniques have been validated for irregular, block structured and structured data access patterns <ref> [1, 4, 11, 10] </ref>. Extending these techniques to handle out-of-core data structures requires using the local memories as "caches" for out-of-core data.
Reference: [5] <author> Raja Das, Mustafa Uysal, Joel Saltz, and Yuan-Shin Hwang. </author> <title> Communication optimizations for irregular scientific computations on distributed memory architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 462-479, </pages> <month> September </month> <year> 1994. </year> <note> Also available as University of Maryland Technical Report CS-TR-3163 and UMIACS-TR-93-109. </note>
Reference-contexts: In that case, the user program is responsible for translating local data structure addresses into the correct global addresses for accessing the entire out-of-core data structure (perhaps through calls to a runtime library such as CHAOS <ref> [5] </ref> or multiblock PARTI [1]). A global view of out-of-core data structures is the natural view for a compiler for a parallelizing compiler for languages such as HPF or Fortran D. <p> For data that is irregularly distributed across processes, a data structure analogous to the CHAOS distributed translation table <ref> [5] </ref>, can serve to store the locations (process and local address) of data currently available in the local memory of some process. We call this data structure an out-of-core data access descriptor. All other data will reside only in secondary storage.
Reference: [6] <author> Juan Miguel del Rosario, Rajesh Bordawekar, and Alok Choudhary. </author> <title> Improved parallel I/O via a two-phase run-time access strategy. </title> <journal> ACM Computer Architecture News, </journal> <volume> 21(5) </volume> <pages> 31-38, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: The library can make use of a varying number of coalescing processes (fixed before program execution) to carry out this aggregation process. We will call this kind of optimization collective local I/O optimization. There are at least two other projects that aim to perform collective local I/O optimizations. Choud-hary <ref> [2, 6] </ref> assumes that each processor has access to either a physical or logical I/O device. They partition parallel I/O into two phases, where processors first read data in a layout that corresponds to the logical disk layout, and then perform in-memory permutations to lay out the data as required.
Reference: [7] <author> Christos Faloutsos, M. Ranganathan, and Yan-nis Manolopoulos. </author> <title> Fast subsequence matching in time-series databases. </title> <booktitle> In Proceedings of the 1994 ACM SIGMOD Conference, </booktitle> <month> May </month> <year> 1994. </year> <note> Also available as University of Maryland Technical Report CS-TR-3190 and UMIACS-TR-93-131. </note>
Reference-contexts: The Jovian library will be used to implement out-of-core algorithms for accessing spatial databases. Data Mining: The Jovian I/O library can be used to support data mining applications being developed at the University of Maryland <ref> [7] </ref>. In a database system, similarities exist among the resident objects. Groups of similar objects form classes.
Reference: [8] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Regular sections with strides provide a compact representation for accessing arbitrary sized, n-dimensional data structures <ref> [8] </ref>. For each dimension, a lower bound, upper bound and stride (a single tuple) must be specified. Therefore, for an n-dimensional array, only n tuples are necessary. The library supports both regular and irregular data access patterns by utilizing a set of ranges.
Reference: [9] <author> David Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <type> Technical Report PCS-TR94-226, </type> <institution> Department of Computer Science, Dartmouth College, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: The goal is to do the preprocessing needed so that disk requests access data in large contiguous chunks. Kotz <ref> [9] </ref> modifies this approach by having compute processors pass their disk access requests to I/O processors. In Kotz's approach, the I/O processors are responsible for coordinating the collective requests to the file system.
Reference: [10] <author> Ravi Ponnusamy, Yuan-Shin Hwang, Joel Saltz, Alok Choudhary, and Geoffrey Fox. </author> <title> Supporting irregular distributions in FORTRAN 90D/HPF compilers. </title> <institution> Technical Report CS-TR-3268 and UMIACS-TR-94-57, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> May </month> <year> 1994. </year> <note> Submitted to IEEE Parallel and Distributed Technology. </note>
Reference-contexts: For communication, our techniques have been validated for irregular, block structured and structured data access patterns <ref> [1, 4, 11, 10] </ref>. Extending these techniques to handle out-of-core data structures requires using the local memories as "caches" for out-of-core data.
Reference: [11] <author> Ravi Ponnusamy, Joel Saltz, and Alok Choud-hary. </author> <title> Runtime-compilation techniques for data partitioning and communication schedule reuse. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 361-370. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year> <note> Also available as University of Maryland Technical Report CS-TR-3055 and UMIACS-TR-93-32. </note>
Reference-contexts: For communication, our techniques have been validated for irregular, block structured and structured data access patterns <ref> [1, 4, 11, 10] </ref>. Extending these techniques to handle out-of-core data structures requires using the local memories as "caches" for out-of-core data.
Reference: [12] <author> Alan Sussman, Gagan Agrawal, and Joel Saltz. </author> <title> A manual for the multiblock PARTI runtime primitives, revision 4.1. </title> <institution> Technical Report CS-TR-3070.1 and UMIACS-TR-93-36.1, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: Access to a compact global descriptor will allow the Jovian library to skip the collective communication stage that is needed when performing collective local I/O optimization. The collective global I/O optimizations in the Jovian library will be similar to the optimizations carried out by the multiblock Parti library <ref> [1, 12] </ref>; we will be able to use many ideas from multiblock Parti to construct this part of the Jovian library. The paper is organized as follows.
References-found: 12

