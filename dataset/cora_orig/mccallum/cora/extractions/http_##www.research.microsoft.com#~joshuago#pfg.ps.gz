URL: http://www.research.microsoft.com/~joshuago/pfg.ps.gz
Refering-URL: http://www.research.microsoft.com/~joshuago/
Root-URL: http://www.research.microsoft.com
Email: Email: goodman@eecs.harvard.edu  
Title: Probabilistic Feature Grammars  
Author: Joshua Goodman 
Address: 40 Oxford St. Cambridge, MA 02138  
Affiliation: Harvard University  
Abstract: We present a new formalism, probabilistic feature grammar (PFG). PFGs combine most of the best properties of several other formalisms, including those of Collins, Magerman, and Charniak, and in experiments have comparable or better performance. PFGs generate features one at a time, probabilistically, conditioning the probabilities of each feature on other features in a local context. Because the conditioning is local, efficient polynomial time parsing algorithms exist for computing inside, outside, and Viterbi parses. PFGs can produce probabilities of strings, making them potentially useful for language modeling. Precision and recall results are comparable to the state of the art with words, and the best reported without words. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Abney, Steve. </author> <year> 1996. </year> <title> Stochastic attribute-value grammars. </title> <note> Available as cmp-lg/9610003, </note> <month> October. </month>
Reference: <author> Baker, J.K. </author> <year> 1979. </year> <title> Trainable grammars for speech recognition. </title> <booktitle> In Proceedings of the Spring Conference of the Acoustical Society of America, </booktitle> <pages> pages 547-550, </pages> <address> Boston, MA, </address> <month> June. </month>
Reference: <author> Black, Ezra, George Garside, and Geoffrey Leech. </author> <year> 1993. </year> <title> Statistically-Driven Computer Grammars of English: the IBM/Lancaster Approach, </title> <booktitle> volume 8 of Language and Computers: Studies in Practical Linguistics. </booktitle> <address> Rodopi, Amsterdam. </address>
Reference: <author> Black, Ezra, Frederick Jelinek, John Lafferty, David M. Magerman, Robert Mercer, and Salim Roukos. </author> <year> 1992. </year> <title> Towards history-based grammars: Using printer models for probabilistic parsing. </title> <booktitle> In Proceedings of the February 1992 DARPA Speech and Natural Language Workshop. </booktitle>
Reference: <author> Black, Ezra, John Lafferty, and Salim Roukos. </author> <year> 1992. </year> <title> Development and evaluation of a broad-coverage probabilistic grammar of english-language computer manuals. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the ACL, </booktitle> <pages> pages 185-192. </pages>
Reference: <author> Brew, Chris. </author> <year> 1995. </year> <title> Stochastic HPSG. </title> <booktitle> In Proceedings of the Seventh Conference of the European Chapter of the ACL, </booktitle> <pages> pages 83-89, </pages> <address> Dublin, Ireland, </address> <month> March. </month>
Reference: <author> Briscoe, Ted and John Carroll. </author> <year> 1993. </year> <title> Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars. </title> <journal> Computational Linguistics, </journal> <volume> 19 </volume> <pages> 25-59. </pages>
Reference: <author> Carroll, John and Ted Briscoe. </author> <year> 1992. </year> <title> Probabilistic normalisation and unpacking of packed parse forests for unification-based grammars. </title> <booktitle> In Proceedings of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language, </booktitle> <pages> pages 33-38, </pages> <address> Cambridge, MA. </address>
Reference: <author> Charniak, Eugene. </author> <year> 1996. </year> <title> Tree-bank grammars. </title> <type> Technical Report CS-96-02, </type> <institution> Department of Computer Science, Brown University. </institution> <note> Available from ftp://ftp.cs.brown.edu/pub/techreports/96/cs96-02.ps.Z </note> . 
Reference-contexts: For a reasonably large treebank, probabilities estimated in this way would be reliable enough to be useful <ref> (Charniak, 1996) </ref>. On the other hand, it is not unlikely that we would never have seen any counts at all of C ((S; singular; dies) ! (NP; singular; man)(VP; singular; dies)) C ((S; singular; dies)) which is the estimated probability of the corresponding production in our grammar augmented with features.
Reference: <author> Collins, Michael. </author> <year> 1996. </year> <title> A new statistical parser based on bigram lexical dependencies. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the ACL, </booktitle> <pages> pages 184-191, </pages> <address> Santa Cruz, CA, </address> <month> June. </month>
Reference-contexts: than comparable parsers. 2 Motivation PFG can be regarded in several different ways: as a way to make history-based grammars (Magerman, 1995) more context free, and thus amenable to dynamic programming; as a way to generalize the work of Black et al. (1992); as a way to turn Collins' parser <ref> (Collins, 1996) </ref> into a generative probabilistic language model; or as an extension of language-modeling techniques to stochastic grammars. The resulting formalism, which is relatively simple and elegant, has most of the advantages of each of the systems from which it is derived. <p> PFGs in general use memory which is only linear. 4.2 Generative Lexicalized Parsing Collins (1997) worked independently from us to construct a model similar to ours. In particular, Collins wished to adapt his previous parser <ref> (Collins, 1996) </ref> to a generative model. In this he succeeded.
Reference: <author> Collins, Michael. </author> <year> 1997. </year> <title> Three generative, lexicalised models for statistical parsing. </title> <booktitle> In Proceedings of the 35th Annual Meeting of the ACL, </booktitle> <pages> pages 16-23, </pages> <address> Madrid, Spain. </address>
Reference: <author> Eisele, Andreas. </author> <year> 1994. </year> <title> Towards probabilistic extensions of constraint-based grammars. </title> <booktitle> DYANA-2 Deliverable R1.2.B, </booktitle> <month> September. </month> <note> Available from ftp://ftp.ims.uni-stuttgart.de/pub/papers/DYANA2/R1.2.B. </note>
Reference: <author> Goodman, Joshua. </author> <year> 1996. </year> <title> Parsing algorithms and metrics. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the ACL, </booktitle> <pages> pages 177-183, </pages> <address> Santa Cruz, CA, </address> <month> June. </month>
Reference-contexts: PFGs use joint probabilites, so can be used both for language modeling and as part of an integrated model. Furthermore, unlike all but one of the comparable systems, PFGs can compute outside probabilities, which are useful for grammar induction, some parsing algorithms <ref> (Goodman, 1996) </ref>, and, as we will show, pruning (Goodman, 1997). 4.1 Bigram Lexical Dependency Parsing Collins (1996) introduced a parser with extremely good performance. From this parser, we take many of the particular conditioning features that we will use in PFGs.
Reference: <author> Goodman, Joshua. </author> <year> 1997. </year> <title> Global thresholding and multiple-pass parsing. </title> <booktitle> In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing. </booktitle>
Reference-contexts: Furthermore, unlike all but one of the comparable systems, PFGs can compute outside probabilities, which are useful for grammar induction, some parsing algorithms (Goodman, 1996), and, as we will show, pruning <ref> (Goodman, 1997) </ref>. 4.1 Bigram Lexical Dependency Parsing Collins (1996) introduced a parser with extremely good performance. From this parser, we take many of the particular conditioning features that we will use in PFGs. As noted, this model cannot be used for language modeling. <p> We then remove those entries whose combined probability is too much lower than the best entry of the cell. In speech recognition, multiple-pass recognizers (Zavaliagkos et al., 1994) have been very successful. We can use an analogous technique, multiple-pass parsing <ref> (Goodman, 1997) </ref> with either PCFGs or PFGs. We use a simple, fast grammar for the first pass, which approximates the later pass. We then remove any events whose combined inside-outside product is too low: essentially those events that are unlikely given the complete sentence.
Reference: <author> Lari, K. and S.J. Young. </author> <year> 1990. </year> <title> The estimation of stochastic context-free grammars using the inside-outside algorithm. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 4 </volume> <pages> 35-56. </pages>
Reference: <author> Magerman, David. </author> <year> 1994. </year> <title> Natural Language Parsing as Statistical Pattern Recognition. </title> <type> Ph.D. thesis, </type> <institution> Stanford University University, </institution> <month> February. </month>
Reference: <author> Magerman, David. </author> <year> 1995. </year> <title> Statistical decision-models for parsing. </title> <booktitle> In Proceedings of the 33rd Annual Meeting of the ACL, </booktitle> <pages> pages 276-283, </pages> <address> Cambridge, MA. </address>
Reference-contexts: When we run using part-of-speech (POS) tags alone as input, we perform significantly better than comparable parsers. 2 Motivation PFG can be regarded in several different ways: as a way to make history-based grammars <ref> (Magerman, 1995) </ref> more context free, and thus amenable to dynamic programming; as a way to generalize the work of Black et al. (1992); as a way to turn Collins' parser (Collins, 1996) into a generative probabilistic language model; or as an extension of language-modeling techniques to stochastic grammars.
Reference: <author> Miller, Scott, David Stallard, Robert Bobrow, and Richard Schwartz. </author> <year> 1996. </year> <title> A fully statistical approach to natural language interfaces. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the ACL, </booktitle> <pages> pages 55-61, </pages> <address> Santa Cruz, CA, </address> <month> June. </month>
Reference-contexts: Also, PFGs can be parsed using efficient polynomial-time dynamic programming algorithms, and learned quickly from a treebank. Finally, unlike most other formalisms, PFGs are potentially useful for language modeling or as one part of an integrated statistical system <ref> (e.g. Miller et al., 1996) </ref> or for use with algorithms requiring outside probabilities. Empirical results are encouraging: our best parser is comparable to those of Magerman (1995) and Collins (1996) when run on the same data.
Reference: <author> Stolcke, Andreas. </author> <year> 1993. </year> <title> An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. </title> <type> Technical Report TR-93-065, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> CA. </address>
Reference-contexts: Thus, our model can be used to capture a wider variety of grammatical theories, simply by changing the choice of features. 1 Unary branching events are in general difficult to deal with <ref> (Stolcke, 1993) </ref>. We introduce an additional EventProb for unary events, and do not allow more than one unary event in a row. Finally, there are some subtle interesting differences with respect to the distance metric.
Reference: <author> Zavaliagkos, G., T. Anastasakos, G. Chou, C. Lapre, F. Kubala, J. Makhoul, L. Nguyen, R. Schwartz, and Y. Zhao. </author> <year> 1994. </year> <title> Improved search, acoustic and language modeling in the BBN Byblos large vocabulary CSR system. </title> <booktitle> In Proceedings of the ARPA Workshop on Spoken Language Technology, </booktitle> <pages> pages 81-88, </pages> <address> Plainsboro, New Jersey. </address>
Reference-contexts: We then remove those entries whose combined probability is too much lower than the best entry of the cell. In speech recognition, multiple-pass recognizers <ref> (Zavaliagkos et al., 1994) </ref> have been very successful. We can use an analogous technique, multiple-pass parsing (Goodman, 1997) with either PCFGs or PFGs. We use a simple, fast grammar for the first pass, which approximates the later pass.
References-found: 20

