URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR93338.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Advanced Compilation Techniques for Fortran D  
Author: Semma Hiranandani, Ken Kennedy, John M. Crummy, and Ajay Sethi 
Address: P.O. Box 1892 Houston, TX 77251-1892  
Affiliation: Rice University  
Note: Center for Research on Parallel Computation  
Date: October 1993  
Pubnum: CRPC-TR93338  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: The first step is partitioning the data and computation among the available set of processors. The second is introducing communication operations to transfer values as necessary. A simple compilation technique known as run-time resolution yields code that explicitly calculates the ownership and communication for each reference at run time <ref> [1, 14, 18] </ref>, but resulting programs are likely to execute significantly slower than the original sequential code. By using aggressive compile-time analysis and optimization, the Fortran 77D compiler can generate much more efficient programs. <p> Alignment and distribution statements are used to calculate the array section owned by each processor. 3) Partition computation The compiler partitions computation among the processors using the "owner computes" rule|each processor only computes values of data it owns <ref> [1, 14, 18] </ref>. The left-hand side (lhs) of each assignment statement in a loop nest is used to calculate the set of loop iterations that cause a processor to assign to local data.
Reference: [2] <author> S. Chatterjee, J. Gilbert, F. Long, R. Schreiber, and S. Teng. </author> <title> Generating local addresses and communication sets for data-parallel programs. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Similarly, 51 is the third element after 36. We wish to compute these distances (which would be kept in the array M indexed by the offset). As mentioned in <ref> [2] </ref>, the offset of an element determines the offset of the next element on the same processor. Since the offsets range between 0 and (block-size-1), by pigeon hole principle, at least two of the first (block-size+1) local memory locations on any particular processor must have the same offset. <p> We could then sort the array to list all the array elements accessed by the processor and hence find out the memory access gaps. Using this idea, Chatterjee, et al give algorithms for computing the memory access gap sequence for loops with arbitrary array alignments and step size <ref> [2] </ref>. The running time of the algorithm using this approach is O (log min (s; P fl b) + b log b) which reduces to O (min (b log b + log s; b log b + log P )).
Reference: [3] <author> K. Cooper, M. W. Hall, R. T. Hood, K. Kennedy, K. S. McKinley, J. M. Mellor-Crummey, L. Torczon, and S. K. Warren. </author> <title> The ParaScope parallel programming environment. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 244-263, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: To evaluate the Fortran D programming model, we implemented a prototype compiler based on the software infrastructure developed for the ParaScope programming environment <ref> [3] </ref>. While the current compiler prototype has enabled validation of the importance of communication and parallelism optimizations, it has many limitations that prevent it from successfully compiling many data-parallel programs. The restrictions in the current prototype are as follows.
Reference: [4] <author> R. Cytron and J. Ferrante. </author> <title> What's in a name? or the value of renaming for parallelism detection and storage allocation. </title> <booktitle> In Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1987. </year>
Reference-contexts: While storage-related dependences are unsafe, flow dependences do not cause safety problems. Fortunately, storage-related dependences can always be removed by using additional storage <ref> [4] </ref>; one of the ways being privatization. The Fortran D compiler recognizes that statements which cause either input dependences in a loop or output dependences impede parallelism.
Reference: [5] <author> R. Cytron, J. Lipkis and E. Schonberg. </author> <title> A Compiler-Assisted approach to SPMD execution. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: In such cases, we resort to the inspector-executor model [15] for irregular loops. 7 More General SPMD Code Replication Checking As mentioned elsewhere in the literature <ref> [5] </ref>, a sequential code segment can be converted to SPMD code if there are no storage-related dependences in the segment. While storage-related dependences are unsafe, flow dependences do not cause safety problems. Fortunately, storage-related dependences can always be removed by using additional storage [4]; one of the ways being privatization.
Reference: [6] <author> M. Gerndt. </author> <title> Updating distributed variables in local computations. </title> <journal> Concurrency: Practice & Experience, </journal> <volume> 2(3) </volume> <pages> 171-193, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: For RSDs representing array elements contiguous to the local array section, the compiler reserves storage using overlaps created by extending the local array bounds <ref> [6] </ref>. Otherwise, temporary buffers or hash tables are used for storing instances of non-local data. 7) Generate code Finally, the Fortran D compiler uses the results of previous analysis and optimization to generate a single-program, multiple-data (SPMD) program that uses message passing to communicate values as necessary.
Reference: [7] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Regular section descriptors (RSDs) are built for the sections of data to be communicated. RSDs compactly represent rectangular array sections and their higher dimension analogs <ref> [7] </ref> 6) Manage storage The compiler identifies the extent and type of non-local data accesses represented by RSDs to calculate the storage required for non-local data.
Reference: [8] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Previous work has described algorithms for partitioning data and computation in the Rice Fortran 77D compiler, as well as its optimization and validation strategy [9]. Internal representations, program analysis, message vectorization, pipelining, and code generation algorithms for block distributions were presented elsewhere <ref> [8] </ref>. The principal contribution of this paper is to provide algorithms and techniques to compile complex Fortran D programs that may contain variable number of processors, symbolic loop bounds and array sizes, loops with non unit strides and multidimensional block, cyclic and block cyclic distributions. <p> By using aggressive compile-time analysis and optimization, the Fortran 77D compiler can generate much more efficient programs. Below, we briefly review the sequence of steps performed by the Rice Fortran 77D compiler; details of the compilation process are described elsewhere <ref> [8, 9] </ref>: 1) Analyze program The compiler performs scalar data-flow analysis, symbolic analysis, and dependence testing to determine the type and level of all data dependences [11]. 2) Partition data The compiler analyzes Fortran D data decomposition specifications to determine the decomposition of each array in a program. <p> Despite these shortcomings, we use the notation mentioned above for the sake of convenience with the understanding that the compiler will handle these cases by using appropriate data-structures to store boundary conditions. The current compiler successfully stores boundary conditions for block distributions <ref> [8] </ref>.
Reference: [9] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: Previous work has described algorithms for partitioning data and computation in the Rice Fortran 77D compiler, as well as its optimization and validation strategy <ref> [9] </ref>. Internal representations, program analysis, message vectorization, pipelining, and code generation algorithms for block distributions were presented elsewhere [8]. <p> By using aggressive compile-time analysis and optimization, the Fortran 77D compiler can generate much more efficient programs. Below, we briefly review the sequence of steps performed by the Rice Fortran 77D compiler; details of the compilation process are described elsewhere <ref> [8, 9] </ref>: 1) Analyze program The compiler performs scalar data-flow analysis, symbolic analysis, and dependence testing to determine the type and level of all data dependences [11]. 2) Partition data The compiler analyzes Fortran D data decomposition specifications to determine the decomposition of each array in a program. <p> For the following canonical loop nest, 2 do ~ k = ~ l to ~m by ~s A (g ( ~ k)) = B (h ( ~ k)) (equation 1) enddo we define the sets (formal definitions of these sets are presented elsewhere <ref> [9] </ref>): * image set B (t p ) is the set of indices of array B that cause a reference to a data element owned by processor t p . * iter set A (t p ) is the set of loop iterations that cause reference A to access data owned <p> Algorithms to construct IN and OUT index sets in the general case are described in <ref> [9] </ref>. Here, we present algorithms to construct specialized in index, out index, receive p and send p sets. These sets are constructed based on the communication type classified by the algorithm in Figure 2.
Reference: [10] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Preliminary experiences with the Fortran D compiler. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Note that we use a relaxed owner-computes rule to replicate some computation and to avoid expensive and unnecessary communication <ref> [10] </ref>. ffl Original Program flg ffl Compiler Output flg REAL B (L:U) REAL B ((U-L+1)/n$p) do i = L i ,U i lb$ = LowerLoopBound (L j ) S 1 B (i) = F 1 (i) ub$ = UpperLoopBound (U j ) do j = L j ,U j do i
Reference: [11] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Analysis and transformation in the ParaScope Editor. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Below, we briefly review the sequence of steps performed by the Rice Fortran 77D compiler; details of the compilation process are described elsewhere [8, 9]: 1) Analyze program The compiler performs scalar data-flow analysis, symbolic analysis, and dependence testing to determine the type and level of all data dependences <ref> [11] </ref>. 2) Partition data The compiler analyzes Fortran D data decomposition specifications to determine the decomposition of each array in a program.
Reference: [12] <author> C. Koelbel. </author> <title> Compiling Programs for Nonshared Memory Machines. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Purdue University, West Lafayette, IN, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: This is equivalent to c 1 fl j n fl P = t p (lb k l k ) We can use extended Euclid algorithm to find the general solution of this linear Diophantine equa tion. (Details of this solution can be found in the literature <ref> [12, 13] </ref>.) During the iteration set construction phase, we also classify each iteration set as one of the following: Iter simple, Iter symbolic or Iter complex. The compiler uses the iteration type information to optimize the code generated.
Reference: [13] <author> C. Koelbel. </author> <title> Compile-time generation of regular communications patterns. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 101-110, </pages> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: This is equivalent to c 1 fl j n fl P = t p (lb k l k ) We can use extended Euclid algorithm to find the general solution of this linear Diophantine equa tion. (Details of this solution can be found in the literature <ref> [12, 13] </ref>.) During the iteration set construction phase, we also classify each iteration set as one of the following: Iter simple, Iter symbolic or Iter complex. The compiler uses the iteration type information to optimize the code generated.
Reference: [14] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the 30 SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: The first step is partitioning the data and computation among the available set of processors. The second is introducing communication operations to transfer values as necessary. A simple compilation technique known as run-time resolution yields code that explicitly calculates the ownership and communication for each reference at run time <ref> [1, 14, 18] </ref>, but resulting programs are likely to execute significantly slower than the original sequential code. By using aggressive compile-time analysis and optimization, the Fortran 77D compiler can generate much more efficient programs. <p> Alignment and distribution statements are used to calculate the array section owned by each processor. 3) Partition computation The compiler partitions computation among the processors using the "owner computes" rule|each processor only computes values of data it owns <ref> [1, 14, 18] </ref>. The left-hand side (lhs) of each assignment statement in a loop nest is used to calculate the set of loop iterations that cause a processor to assign to local data.
Reference: [15] <author> J. Saltz, K. Crowley, R. Mirchandaney, and H. Berryman. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(4) </volume> <pages> 303-312, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: For more complicated patterns, for example in case of stride changes, there does not exist any simple lookup technique for generating the communication sets because the pattern of destination processors can have period longer than the block-size b. In such cases, we resort to the inspector-executor model <ref> [15] </ref> for irregular loops. 7 More General SPMD Code Replication Checking As mentioned elsewhere in the literature [5], a sequential code segment can be converted to SPMD code if there are no storage-related dependences in the segment. While storage-related dependences are unsafe, flow dependences do not cause safety problems.
Reference: [16] <author> J. Stichnoth, D. O'Hallaron, and T. Gross. </author> <title> Generating communication for array statements: Design, implementation, and evaluation. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: But, as the example in Figure 8 demonstrates, block-cyclic sets are not closed under intersection. Stichnoth <ref> [16] </ref> treats the block-cyclic sets as a union of disjoint cyclic sets. Since the cyclic sets are closed under intersection, the intersection of the two block-cyclic sets can be determined by intersecting all possible pairs of the cyclic sets. <p> Hence the time required to compute the intersection is O (LCM (Block-size (A), Block-size (B))*P) which compares favorably to the time complexity of the method suggested in <ref> [16] </ref>. In case the number of processors or the loop bounds are unknown at compile time, the compiler needs to perform the intersection at run time. Figure 9 gives the algorithm for computing the array elements which need to be sent from one processor to another.
Reference: [17] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berryman. </author> <title> Runtime compilation methods for multicomputers. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: It simply lists the type of functions we will have in our run time library 8.1.3 Inspector/Executor and Run Time Resolution When complex index sets are generated, the compiler will not be able to optimize the code and will have to resort to generating inspector/executors described in <ref> [17] </ref> in the case where true loop carried cross processor dependences do not exist and run time resolution otherwise.
Reference: [18] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year> <month> 31 </month>
Reference-contexts: The first step is partitioning the data and computation among the available set of processors. The second is introducing communication operations to transfer values as necessary. A simple compilation technique known as run-time resolution yields code that explicitly calculates the ownership and communication for each reference at run time <ref> [1, 14, 18] </ref>, but resulting programs are likely to execute significantly slower than the original sequential code. By using aggressive compile-time analysis and optimization, the Fortran 77D compiler can generate much more efficient programs. <p> Alignment and distribution statements are used to calculate the array section owned by each processor. 3) Partition computation The compiler partitions computation among the processors using the "owner computes" rule|each processor only computes values of data it owns <ref> [1, 14, 18] </ref>. The left-hand side (lhs) of each assignment statement in a loop nest is used to calculate the set of loop iterations that cause a processor to assign to local data.
References-found: 18

