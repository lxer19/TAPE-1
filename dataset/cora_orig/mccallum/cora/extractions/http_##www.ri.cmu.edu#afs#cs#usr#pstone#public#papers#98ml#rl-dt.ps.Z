URL: http://www.ri.cmu.edu/afs/cs/usr/pstone/public/papers/98ml/rl-dt.ps.Z
Refering-URL: http://www.ri.cmu.edu/afs/cs/usr/pstone/mosaic/pstone-papers.html
Root-URL: 
Title: Team-Partitioned, Opaque-Transition Reinforcement Learning  
Author: Peter Stone and Manuela Veloso 
Date: April 24, 1998  
Address: Pittsburgh, PA 15213  
Affiliation: Computer Science Department Carnegie Mellon University  
Abstract: In this paper, we present a novel multi-agent learning paradigm called team-partitioned, opaque-transition reinforcement learning (TPOT-RL). TPOT-RL introduces the concept of using action-dependent features to generalize the state space. In our work, we use a learned action-dependent feature space. TPOT-RL is an effective technique to allow a team of agents to learn to cooperate towards the achievement of a specific goal. It is an adaptation of traditional RL methods that is applicable in complex, non-Markovian, multi-agent domains with large state spaces and limited training opportunities. Multi-agent scenarios are opaque-transition, as team members are not always in full communication with one another and adversaries may affect the environment. Hence, each learner cannot rely on having knowledge of future state transitions after acting in the world. TPOT-RL enables teams of agents to learn effective policies with very few training examples even in the face of a large state space with large amounts of hidden state. The main responsible features are: dividing the learning task among team members, using a very coarse, action-dependent feature space, and allowing agents to gather reinforcement directly from observation of the environment. TPOT-RL is fully implemented and has been tested in the robotic soccer domain, a complex, multi-agent framework. This paper presents the algorithmic details of TPOT-RL as well as empirical results demonstrating the effectiveness of the developed multi-agent learning approach with learned features.
Abstract-found: 1
Intro-found: 1
Reference: [ Arai, Miyazaki, & Kobayashi, 1997 ] <author> Arai, S.; Miyazaki, K.; and Kobayashi, S. </author> <year> 1997. </year> <title> Generating cooperative behavior by multi-agent reinforcement learning. </title> <booktitle> In Sixth European Workshop on Learning Robots. </booktitle>
Reference-contexts: Littman uses Markov games to learn stochastic policies in a very abstract version of 1-on-1 robotic soccer [ Littman, 1994 ] . There have also been a number of studies of multi-agent reinforcement learning in the pursuit domain <ref> [ Arai, Miyazaki, & Kobayashi, 1997, Tan, 1993 ] </ref> .
Reference: [ Asada et al., 1996 ] <author> Asada, M.; Noda, S.; Tawaratumida, S.; and Hosoda, K. </author> <year> 1996. </year> <title> Purposive behavior acquisition for a real robot by vision-based reinforcement learning. </title> <booktitle> Machine Learning 23:279303. </booktitle>
Reference-contexts: Thus unlike TPOT-RL agents, the nodes are able to use dynamic programming. In other soccer systems, there have been a number of learning techniques that have been explored. However, most have learned low-level, individual skills as opposed to team-based policies <ref> [ Asada et al., 1996, Stone & Veloso, 1998b ] </ref> .
Reference: [ Boyan & Littman, 1994 ] <author> Boyan, J. A., and Littman, M. L. </author> <year> 1994. </year> <title> Packet routing in dynamically changing networks: A reinforcement learning approach. </title> <editor> In Cowan, J. D.; Tesauro, G.; and Alspector, J., eds., </editor> <booktitle> Advances In Neural Information Processing Systems 6. </booktitle> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: The effects of opponent actions are captured in the reward function. Another team-partitioned, opaque transition domain is network routing as considered in <ref> [ Boyan & Littman, 1994 ] </ref> . Each network node is considered as a separate agent which cannot see a packet's route beyond its own action.
Reference: [ Kaelbling, Cassandra, & Littman, 1994 ] <author> Kaelbling, L. P.; Cassandra, A. R.; and Littman, M. L. </author> <year> 1994. </year> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence. </booktitle>
Reference-contexts: Worse than being model-free, multi-agent RL must deal with the inability to even track the team's state trajectory. Thus we use Equation 1, which doesn't rely on knowing s 0 . Notice that the opaque-transition characteristic also does not fit into the partially observable Markov decision process (POMDP) framework <ref> [ Kaelbling, Cassandra, & Littman, 1994 ] </ref> . While POMDPs deal with hidden state, they do assume that the agent at least knows when it has transitioned to a new state and may act again.
Reference: [ Kaelbling, Littman, & Moore, 1996 ] <author> Kaelbling, L. P.; Littman, M. L.; and Moore, A. W. </author> <year> 1996. </year> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research 4:237285. </journal>
Reference-contexts: 1 Introduction Reinforcement learning (RL) is an effective paradigm for training an artificial agent to act in its environment in pursuit of a goal. RL techniques rely on the premise that an agent's action policy affects its overall reward over time. As surveyed in <ref> [ Kaelbling, Littman, & Moore, 1996 ] </ref> , several popular RL techniques use dynamic programming to enable a single agent to learn an effective control policy as it traverses a stationary (Markovian) environment. <p> This section highlights the components of TPOT-RL and relates them to previous work. Typical RL paradigms update the value of a state-action pair based upon the value of the subsequent state (or state distribution). As presented in <ref> [ Kaelbling, Littman, & Moore, 1996 ] </ref> , the typical update function in Q-learning is Q (s; a) = Q (s; a) + ff (r + flmax a Q (s 0 ; a) Q (s; a)) where s 0 is the state next reached after executing action a in state s
Reference: [ Kitano et al., 1997 ] <author> Kitano, H.; Kuniyoshi, Y.; Noda, I.; Asada, M.; Matsubara, H.; and Osawa, E. </author> <year> 1997. </year> <title> RoboCup: A challenge problem for AI. </title> <journal> AI Magazine 18(1):7385. </journal> <volume> 9 </volume>
Reference-contexts: We implemented TPOT-RL as the current highest layer of a layered learning system in the RoboCup soccer server [ Noda, Matsubara, & Hiraki, 1996 ] . The soccer server used at RoboCup-97 <ref> [ Kitano et al., 1997 ] </ref> is a much more complex domain than has previously been used for studying multi-agent policy learning. <p> Using the C4.5 DT algorithm [ Quinlan, 1993 ] , the classifications were learned with associated confidence factors. The learned behaviors proved effective both in controlled testing scenarios [ Stone & Veloso, 1998a, Stone & Veloso, 1998c ] and against other previously-unseen opponents in an international tournament setting <ref> [ Kitano et al., 1997 ] </ref> . These two previously-learned behaviors were both trained off-line in limited, controlled training situations.
Reference: [ Littman, 1994 ] <author> Littman, M. L. </author> <year> 1994. </year> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, 157163. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: This work was in a completely collaborative, as opposed to our adversarial, setting. Previous multi-agent reinforcement learning systems have typically dealt with much simpler tasks than the one presented here. Littman uses Markov games to learn stochastic policies in a very abstract version of 1-on-1 robotic soccer <ref> [ Littman, 1994 ] </ref> . There have also been a number of studies of multi-agent reinforcement learning in the pursuit domain [ Arai, Miyazaki, & Kobayashi, 1997, Tan, 1993 ] .
Reference: [ Luke et al., 1998 ] <author> Luke, S.; Hohn, C.; Farris, J.; Jackson, G.; and Hendler, J. </author> <year> 1998. </year> <title> Co-evolving soccer softbot team coordination with genetic programming. </title> <editor> In Kitano, H., ed., RoboCup-97: </editor> <title> The First Robot World Cup Soccer Games and Conferences. </title> <publisher> Berlin: Springer Verlag. In Press. </publisher>
Reference-contexts: In other soccer systems, there have been a number of learning techniques that have been explored. However, most have learned low-level, individual skills as opposed to team-based policies [ Asada et al., 1996, Stone & Veloso, 1998b ] . Interestingly, <ref> [ Luke et al., 1998 ] </ref> uses genetic programming to evolve team behaviors from scratch as opposed to our layered learning approach. 6 Conclusion TPOT-RL is an adaptation of RL to non-Markovian multi-agent domains with opaque transitions, large state spaces, hidden state and limited training opportunities.
Reference: [ Mataric, 1994 ] <author> Mataric, M. J. </author> <year> 1994. </year> <title> Interaction and intelligent behavior. </title> <type> MIT EECS PhD Thesis AITR-1495, </type> <institution> MIT AI Lab. </institution>
Reference-contexts: For an extensive survey, see [ Stone & Veloso, 1997 ] . This section highlights the work most related to TPOT-RL. The internal reinforcement in the reward function R is similar to Mataric's progress estimators <ref> [ Mataric, 1994 ] </ref> . There, the short-term real-world effects of actions are used as an intermediate reward to help robots reach the ultimate goal location. Mataric's conditions also play a similar role to the features used here, reducing the size of the evaluation function domain.
Reference: [ Noda, Matsubara, & Hiraki, 1996 ] <author> Noda, I.; Matsubara, H.; and Hiraki, K. </author> <year> 1996. </year> <title> Learning cooperative behavior in multi-agent environment: a case study of choice of play-plans in soccer. </title> <booktitle> In PRICAI'96: Topics in Artificial Intelligence (Proc. of 4th Pacific Rim International Conference on Artificial Intelligence, Cairns, Australia), </booktitle> <volume> 570 579. </volume>
Reference-contexts: Instead, intermediate domain-dependent skills should be learned in a bottom-up hierarchical fashion [ Stone & Veloso, 1998a ] . We implemented TPOT-RL as the current highest layer of a layered learning system in the RoboCup soccer server <ref> [ Noda, Matsubara, & Hiraki, 1996 ] </ref> . The soccer server used at RoboCup-97 [ Kitano et al., 1997 ] is a much more complex domain than has previously been used for studying multi-agent policy learning.
Reference: [ Quinlan, 1993 ] <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Based on almost 200 continuous-valued attributes describing teammate and opponent positions on the field, players learned to classify the pass as a likely success (ball reaches its destination or a teammate gets it) or likely failure (opponent intercepts the ball). Using the C4.5 DT algorithm <ref> [ Quinlan, 1993 ] </ref> , the classifications were learned with associated confidence factors. The learned behaviors proved effective both in controlled testing scenarios [ Stone & Veloso, 1998a, Stone & Veloso, 1998c ] and against other previously-unseen opponents in an international tournament setting [ Kitano et al., 1997 ] .
Reference: [ Salustowicz, Wiering, & Schmidhuber, 1998 ] <author> Salustowicz, R. P.; Wiering, M. A.; and Schmidhuber, J. </author> <year> 1998. </year> <title> Learning team strategies: Soccer case studies. </title> <journal> Machine Learning. </journal> <note> To appear. </note>
Reference-contexts: While POMDPs deal with hidden state, they do assume that the agent at least knows when it has transitioned to a new state and may act again. The construction of feature space V can have a huge effect on the nature of Q. For example, in <ref> [ Salustowicz, Wiering, & Schmidhuber, 1998 ] </ref> , a grid-like discretization is used for V . Since too many states result for a lookup-table, a neural network is used as the value function approximator.
Reference: [ Stone & Veloso, 1997 ] <author> Stone, P., and Veloso, M. </author> <year> 1997. </year> <title> Multiagent systems: A survey from a machine learning perspective. </title> <type> Technical Report CMU-CS-97-193, </type> <institution> Computer Science Department, Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: We can do so by relying on our layered learning approach to learn the state generalization function. The use of machine learning in multi-agent systems has recently been receiving a good deal of attention. For an extensive survey, see <ref> [ Stone & Veloso, 1997 ] </ref> . This section highlights the work most related to TPOT-RL. The internal reinforcement in the reward function R is similar to Mataric's progress estimators [ Mataric, 1994 ] .
Reference: [ Stone & Veloso, 1998a ] <author> Stone, P., and Veloso, M. </author> <year> 1998a. </year> <title> A layered approach to learning client behaviors in the RoboCup soccer server. </title> <journal> Applied Artificial Intelligence 12. </journal> <note> In Press. </note>
Reference-contexts: Our general approach, called layered learning, is based on the premise that realistic domains are too complex for learning mappings directly from sensor inputs to actuator outputs. Instead, intermediate domain-dependent skills should be learned in a bottom-up hierarchical fashion <ref> [ Stone & Veloso, 1998a ] </ref> . We implemented TPOT-RL as the current highest layer of a layered learning system in the RoboCup soccer server [ Noda, Matsubara, & Hiraki, 1996 ] . <p> Using the C4.5 DT algorithm [ Quinlan, 1993 ] , the classifications were learned with associated confidence factors. The learned behaviors proved effective both in controlled testing scenarios <ref> [ Stone & Veloso, 1998a, Stone & Veloso, 1998c ] </ref> and against other previously-unseen opponents in an international tournament setting [ Kitano et al., 1997 ] . These two previously-learned behaviors were both trained off-line in limited, controlled training situations.
Reference: [ Stone & Veloso, 1998b ] <author> Stone, P., and Veloso, M. </author> <year> 1998b. </year> <title> Towards collaborative and adversarial learning: A case study in robotic soccer. </title> <journal> International Journal of Human-Computer Systems 48. </journal> <note> In Press. </note>
Reference-contexts: Thus unlike TPOT-RL agents, the nodes are able to use dynamic programming. In other soccer systems, there have been a number of learning techniques that have been explored. However, most have learned low-level, individual skills as opposed to team-based policies <ref> [ Asada et al., 1996, Stone & Veloso, 1998b ] </ref> .
Reference: [ Stone & Veloso, 1998c ] <author> Stone, P., and Veloso, M. </author> <year> 1998c. </year> <title> Using decision tree confidence factors for multiagent control. </title> <editor> In Kitano, H., ed., RoboCup-97: </editor> <title> The First Robot World Cup Soccer Games and Conferences. </title> <publisher> Berlin: Springer Verlag. In Press. </publisher>
Reference-contexts: Using the C4.5 DT algorithm [ Quinlan, 1993 ] , the classifications were learned with associated confidence factors. The learned behaviors proved effective both in controlled testing scenarios <ref> [ Stone & Veloso, 1998a, Stone & Veloso, 1998c ] </ref> and against other previously-unseen opponents in an international tournament setting [ Kitano et al., 1997 ] . These two previously-learned behaviors were both trained off-line in limited, controlled training situations. <p> Since a player may not be able to tell the results of other players' actions, or even when they can act, the domain is opaque-transition. A team formation is divided into 11 positions (m = 11), as also shown in Figure 1 (a) <ref> [ Stone & Veloso, 1998c ] </ref> . Thus, the partition function P (s) returns the player's position. Using our layered learning approach, we use the previously trained DT as e. Each possible pass is classified as either a likely success or a likely failure with a confidence factor.
Reference: [ Tan, 1993 ] <author> Tan, M. </author> <year> 1993. </year> <title> Multi-agent reinforcement learning: Independent vs. </title> <booktitle> cooperative agents. In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> 330337. </pages>
Reference-contexts: Littman uses Markov games to learn stochastic policies in a very abstract version of 1-on-1 robotic soccer [ Littman, 1994 ] . There have also been a number of studies of multi-agent reinforcement learning in the pursuit domain <ref> [ Arai, Miyazaki, & Kobayashi, 1997, Tan, 1993 ] </ref> .
Reference: [ Veloso et al., 1998 ] <author> Veloso, M.; Stone, P.; Han, K.; and Achim, S. </author> <year> 1998. </year> <title> The CMUnited-97 small-robot team. </title> <editor> In Kitano, H., ed., RoboCup-97: </editor> <title> The First Robot World Cup Soccer Games and Conferences. </title> <publisher> Berlin: Springer Verlag. </publisher>
Reference-contexts: The DT even helps TPOT-RL more than a hand-coded heuristic pass-evaluation function (e = heur) based on one that we successfully used on our real robot team <ref> [ Veloso et al., 1998 ] </ref> (Figure 2 (b.5)). Final score is the ultimate performance measure. However, we examined learning more closely in the best case experiment (e = DT, jW j = 1 Figure 2 (b.2)).
Reference: [ Zhao & Schmidhuber, 1996 ] <author> Zhao, J., and Schmidhuber, J. </author> <year> 1996. </year> <title> Incremental self-improvement for life-time multi-agent reinforcement learning. </title> <booktitle> In Proceedings of the 4th International Conference of Simulation of Adaptive Behaviros (SAB 4), </booktitle> <volume> 363372. </volume> <publisher> MIT Press. </publisher> <pages> 10 </pages>
Reference-contexts: There have also been a number of studies of multi-agent reinforcement learning in the pursuit domain [ Arai, Miyazaki, & Kobayashi, 1997, Tan, 1993 ] . In this domain, four predators chase a single prey in a small grid-like world. 8 Also in a predator-like task, <ref> [ Zhao & Schmidhuber, 1996 ] </ref> uses a single run to deal with the opponents' shifting policies and ignore the opponents' policies just as we do. The effects of opponent actions are captured in the reward function.
References-found: 19

