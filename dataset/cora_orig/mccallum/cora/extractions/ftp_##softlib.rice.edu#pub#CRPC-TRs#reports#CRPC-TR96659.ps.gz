URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR96659.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: P ARPACK: An Efficient Portable Large Scale Eigenvalue Package for Distributed Memory Parallel Architectures  
Author: K. J. Maschhoff and D. C. Sorensen 
Affiliation: Rice University  
Abstract: P ARPACK is a parallel version of the ARPACK software. ARPACK is a package of Fortran 77 subroutines which implement the Implicitly Restarted Arnoldi Method used for solving large sparse eigenvalue problems. A parallel implementation of ARPACK is presented which is portable across a wide range of distributed memory platforms and requires minimal changes to the serial code. The communication layers used for message passing are the Basic Linear Algebra Communication Subprograms (BLACS) developed for the ScaLAPACK project and Message Passing Interface(MPI).
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D. C. Sorensen, </author> <title> Implicit application of polynomial filters in a k-step Arnoldi method, </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 13(1) </volume> <pages> 357-385, </pages> <month> Jan-uary </month> <year> 1992. </year>
Reference-contexts: n fi n matrix H k , k fi k projected matrix ( Upper Hessenberg ) V k , n fi k matrix, k t n Set of Arnoldi vectors f k , residual vector, length n, V T k f k = 0 coupled with an implicit restarting mechanism <ref> [1] </ref> is the basis of the ARPACK codes. <p> The explicit steps of the process responsible for the j block. Since H is replicated on each processor, the parallelization of the implicit restart mechanism described in <ref> [1, 2] </ref> remains untouched. The only difference is that the local block V (j) is in place of the full matrix V. All operations on the matrix H are replicated on each processor.
Reference: 2. <author> D. C. Sorensen, </author> <title> Implicitly-Restarted Arnoldi/Lanczos Methods for Large Scale Eigenvalue Calculations, (invited paper), in Parallel Numerical Algorithms: </title> <booktitle> Proceedings of an ICASE/LaRC Workshop, </booktitle> <month> May 23-25, </month> <year> 1994, </year> <note> Hampton, VA, </note> <author> D. E. Keyes, A. Sameh, and V. Venkatakrishnan, eds., </author> <note> Kluwer, 1995 (to appear). </note>
Reference-contexts: The reverse communication interface is one of the most important aspects in the design of ARPACK and this feature lends itself to a simplified SPMD parallelization strategy. This approach was used for previous parallel implementations of ARPACK <ref> [2] </ref> and is simple for the the user to implement. The reverse communication interface feature of ARPACK allows the P ARPACK codes to be parallelized internally without imposing a fixed parallel decomposition on the matrix or the user supplied matrix-vector product. <p> The explicit steps of the process responsible for the j block. Since H is replicated on each processor, the parallelization of the implicit restart mechanism described in <ref> [1, 2] </ref> remains untouched. The only difference is that the local block V (j) is in place of the full matrix V. All operations on the matrix H are replicated on each processor.
Reference: 3. <author> R.B. Lehoucq, D.C. Sorensen, P.A. Vu, and C. Yang, ARPACK: </author> <title> Fortran subroutines for solving large scale eigenvalue problems, Release 2.1 </title>
Reference-contexts: The current implementation of the PBLAS (ScaLAPACK version 1.1) assumes the matrix operands to be distributed in a block-cyclic decomposition scheme. 2.2 Message Passing One objective for the development and maintenance of a parallel version of the ARPACK <ref> [3] </ref> package was to construct a parallelization strategy whose implementation required as few changes as possible to the current serial version.
Reference: 4. <author> R. B. Lehoucq and D.C. </author> <note> Sorensen Deflation Techniques for an Implicitly Re-started Arnoldi Iteration , To appear in SIAM Journal of Matrix Analysis </note>
Reference: 5. <author> J. Daniel, W.B. Gragg, L. Kaufman, and G.W. </author> <title> Stewart Reorthogonalization and stable algorithms for updating the Gram-Schmidt QR factorization , Mathematics of Computation, </title> <booktitle> 30 </booktitle> <pages> 772-795, </pages> <year> 1976 </year>
Reference-contexts: With this approach there are only two communication points within the construction of the Arnoldi factorization inside P ARPACK: computation of the 2-norm of the distributed vector f k and the orthogonalization of f k to V k using Classical Gram Schmidt with DGKS correction <ref> [5] </ref>. Additional communication will typically occur in the user supplied matrix-vector product operation as well. Ideally, this product will only require nearest neighbor communication among the processes. Typically the blocking of V is commensurate with the parallel decomposition of the matrix A.
Reference: 6. <author> M.P. Debicki, P. Jedrzejewski, J. Mielewski, P. Przybyszewski, and M. </author> <title> Mrozowski Application of the Arnoldi Method to the Solution of Electromagnetic Eigenprob-lems on the Multiprocessor Power Challenge Architecture, </title> <type> Technical Report Number 19/95, </type> <institution> Department of Electronics, Technical University of Gdansk, Poland. </institution>
Reference-contexts: PFA is an optimizing Fortran preprocessor that discovers parallelism in Fortran code and converts those programs to parallel code. A brief discussion of implementation details for ARPACK using PFA preprocessing may be found in <ref> [6] </ref>. The effectiveness of this preprocessing step is still dependent on how suitable the source code is for parallelization.
Reference: 7. <author> J. J. Dongarra and R. C. </author> <note> Whaley LAPACK Working Note 94, A User's Guide to the BLACS v1.0, , June 7, </note> <year> 1995 </year>
Reference-contexts: Inclusion of the context (or communicator) is necessary for global communication as well as managing I/O. The addition of the context is new to this implementation and reflects the improvements and standardizations being made in message passing <ref> [9, 7] </ref>. 2.1 Data Distribution of the Arnoldi Factorization The numerically stable generation of the Arnoldi factorization AV k = V k H k + f k e T where A, n fi n matrix H k , k fi k projected matrix ( Upper Hessenberg ) V k , n <p> In addition, for the parallel code to be portable, the communication interface used for message passing must be supported on a wide range of parallel machines and platforms. For /small P ARPACK, this portability is achieved via the Basic Linear Algebra Communication Subprograms (BLACS) <ref> [7] </ref> developed for the ScaLAPACK project and Message Passing Interface (MPI) [9]. 3 Parallel Performance To illustrate the potential scalability of Parallel ARPACK on distributed memory architectures some example problems have been run on the Maui HPCC SP2.
Reference: 8. <author> J. Choi, J. Dongarra, S. Ostrouchov, A. Petitet, D. Walker, and R. C. </author> <title> Whaley LAPACK Working Note 100, A Proposal for a Set of Parallel Basic Linear Algebra Subprograms , May 1995 </title>
Reference-contexts: 1, * zero, h (1,j), 1) to call sgemv ('T', n, j, one, v, ldv, workd (ipj), 1, * zero, h (1,j), 1) call sgsum2d ( comm, 'All', ' ', j, 1, h (1,j), j, * -1, -1 ) Another strategy which was tested was to use Parallel BLAS (PBLAS) <ref> [8] </ref> software developed for the ScaLAPACK project to achieve parallelization. The function of the PBLAS is to simplify the parallelization of serial codes implemented on top of the BLAS.
Reference: 9. <author> Message Passing Interface Forum, </author> <title> MPI: A Message-Passing Interface Standard , International Journal of Supercomputer Applications and High Performance Computing, 8(3/4), 1994 This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: Inclusion of the context (or communicator) is necessary for global communication as well as managing I/O. The addition of the context is new to this implementation and reflects the improvements and standardizations being made in message passing <ref> [9, 7] </ref>. 2.1 Data Distribution of the Arnoldi Factorization The numerically stable generation of the Arnoldi factorization AV k = V k H k + f k e T where A, n fi n matrix H k , k fi k projected matrix ( Upper Hessenberg ) V k , n <p> For /small P ARPACK, this portability is achieved via the Basic Linear Algebra Communication Subprograms (BLACS) [7] developed for the ScaLAPACK project and Message Passing Interface (MPI) <ref> [9] </ref>. 3 Parallel Performance To illustrate the potential scalability of Parallel ARPACK on distributed memory architectures some example problems have been run on the Maui HPCC SP2.
References-found: 9

