URL: http://www.eecs.harvard.edu/~smith/papers/asplos94.ps
Refering-URL: http://www.eecs.harvard.edu/~smith/papers.html
Root-URL: 
Date: October 1994  
Note: Published in Proc. of ASPLOS-VI,  
Abstract: Recent work in history-based branch prediction uses novel hardware structures to capture branch correlation and increase branch prediction accuracy. We present a profile-based code transformation that exploits branch correlation to improve the accuracy of static branch prediction schemes. Our general method encodes branch history information in the program counter through the duplication and placement of program basic blocks. For correlation histories of eight branches, our experimental results achieve up to a 14.7% improvement in prediction accuracy over conventional profile-based prediction without any increase in the dynamic instruction count of our benchmark applications. In the majority of these applications, code duplication increases code size by less than 30%. For the few applications with code segments that exhibit exponential branching paths and no branch correlation, simple compile-time heuristics can eliminate these branches as code-transformation candidates. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Ball and J. Larus, </author> <title> Branch Prediction for Free, </title> <booktitle> Proc. ACM SIGPLAN 1993 Conf. on Prog. Lang. Design and Implementation, </booktitle> <month> Jun. </month> <year> 1993. </year>
Reference-contexts: This work could then be applied to schemes that perform static prediction based only on program structure (no profiling) <ref> [1] </ref>. Some execution paths do not always lead to taken or fall-through branch execution, but consistently favor one direction over several workloads for the same program. This indicates some type of statistical correlation between a history path and a branch prediction.

Reference: [3] <author> P. Chang, S. Mahlke, and W. Hwu, </author> <title> Using Profile Informa tion to Assist Classic Code Optimizations, </title> <institution> Center for Reliable and High-Performance Computing Report CRHC-91-13, Univ. of Illinois Urbana-Champaign, Urbana, IL, </institution> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: This paper introduces and evaluates a compile-time, profile-based algorithm for static branch prediction that improves prediction accuracies without the use of special hardware structures. Greater accuracy in the compile-time prediction of conditional branches is important because it improves the effectiveness of global optimiz-ers <ref> [3] </ref> and global instruction schedulers [7,12,14,19]. Before the recent branch correlation schemes, other schemes predicted branch direction using either the static properties of a branch (e.g. the opcode or branch direction) or the past execution history of a branch (e.g. this branch was taken on its last execution).
Reference: [4] <author> J. Fisher and S. Freudenberger, </author> <title> Predicting Conditional Branch Directions From Previous Runs of a Program, </title> <booktitle> Proc. 5th Annual Intl. Conf. on Architectural Support for Prog. Lang. and Operating Systems, </booktitle> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: As a result, one might assume that dynamic schemes are inherently better than a static history-based prediction scheme such as profiling [13]. On the contrary, Fisher and Freudenberger <ref> [4] </ref> have empirically shown that many applications are dominated by statically-predictable branches, and thus static history-based prediction schemes are effective across the different data sets of most applications.
Reference: [5] <author> L. Gwennap, </author> <title> PPC 604 Powers Past Pentium, </title> <type> Microproces sor Report, 8(5), </type> <month> Apr. 18, </month> <year> 1994. </year>
Reference-contexts: The hardware schemes are based on a fixed bit budget of 1 kilobyte for the branch prediction table (cost of the history shift registers is ignored). This table is 8-times bigger than the 2-bit branch prediction table in the PowerPC 604 <ref> [5] </ref>, but it is consistent with what Pan et al. and Yeh and Patt recommend for near-term future microprocessors.
Reference: [6] <author> W. Hwu and P. Chang, </author> <title> Inlining Function Expansion for Compiling C Programs, </title> <booktitle> Proc. ACM SIGPLAN 1989 Conf. on Prog. Lang. Design and Implementation, </booktitle> <month> Jun. </month> <year> 1989. </year>
Reference-contexts: A number of other researchers also have used code duplication in the optimization and scheduling phases of a compiler. For example, Chang and Hwu <ref> [6] </ref> have used procedure inlining, which duplicates the code for entire functions, A M Y B (a) CFG with overlaid history trees for branch blocks Y and Z.
Reference: [7] <author> W. Hwu et al., </author> <title> The Superblock: An Effective Technique for VLIW and Superscalar Compilation, </title> <journal> The Journal of Supercomputing, </journal> <volume> 7(1/2), </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: Mueller and Whal-ley [15] describe a technique that duplicates the target blocks of unconditional branches. This technique removes unconditional branches from the instruction stream and thus decreases the total dynamic instruction count for a program. Both Bernstein et al. [2] and Hwu et al. <ref> [7] </ref> use code duplication techniques to increase the number of global code motions that they can perform during instruction scheduling. The compile-time code duplication techniques that are the closest to our work are the studies in loop unrolling and loop peeling [16], and a very recent study by Krall [8].
Reference: [8] <author> A. Krall, </author> <title> Improving Semi-static Branch Prediction by Code Replication, </title> <booktitle> Proc. ACM SIGPLAN 94 Conf. on Prog. Lang. Design and Implementation, </booktitle> <month> Jun. </month> <year> 1994. </year>
Reference-contexts: The compile-time code duplication techniques that are the closest to our work are the studies in loop unrolling and loop peeling [16], and a very recent study by Krall <ref> [8] </ref>. The loop techniques use code duplication to construct correlated static branches from a single static branch. For example in loop peeling, a single iteration of a loop is peeled off if the loop frequently executes only one loop iteration. <p> The loop branch for the peeled code is predicted to exit while the loop branch in the loop is predicted to stay in the loop. Kralls study <ref> [8] </ref>, like ours, attempts to improve static branch prediction accuracies through code duplication. His approach describes code duplication in terms of state machines that either encode the history of a single branch (his loop branch strategy) or the history of a path of branches (his correlated branch strategy).
Reference: [9] <author> M. Lam and R. Wilson, </author> <title> Limits of Control Flow on Parallel ism, </title> <booktitle> Proc. 19th Annual Intl. Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: 1 Introduction To achieve high performance, processors with deep pipelines and wide instruction issue widths require accurate methods to predict the direction of conditional branches <ref> [9] </ref>. Though many schemes have been proposed for branch prediction [10,11,13,20], two recent studies [17,22] have obtained higher prediction accuracies by observing when the outcome of a particular branch is affected by the outcomes of its neighboring branches in the dynamic instruction stream.
Reference: [10] <author> J. Lee and A. Smith, </author> <title> Branch Prediction Strategies and Branch Target Buffer Design, </title> <journal> Computer, </journal> <volume> 17(1), </volume> <month> Jan. </month> <year> 1984. </year>
Reference: [11] <author> D. Lilja, </author> <title> Reducing the Branch Penalty in Pipelined Proces sors, </title> <journal> Computer, </journal> <volume> 21(7), </volume> <month> Jul. </month> <year> 1988. </year>
Reference: [12] <author> P. Lowney et al., </author> <title> The Multiow Trace Scheduling Com piler, </title> <journal> Journal of Supercomputing, </journal> <volume> 7(1/2), </volume> <month> May </month> <year> 1993. </year>
Reference: [13] <author> S. McFarling and J. Hennessy, </author> <title> Reducing the Cost of Branches, </title> <booktitle> Proc. of 13th Annual Intl. Symp. on Computer Architecture, </booktitle> <month> Jun. </month> <year> 1986. </year>
Reference-contexts: As a result, one might assume that dynamic schemes are inherently better than a static history-based prediction scheme such as profiling <ref> [13] </ref>. On the contrary, Fisher and Freudenberger [4] have empirically shown that many applications are dominated by statically-predictable branches, and thus static history-based prediction schemes are effective across the different data sets of most applications. <p> Prediction schemes usually associate a predictor with each conditional branch (or set of conditional branches) in a program; each predictor then summarizes the past actions of that specific branch (or set of branches). In static profiling, the self-history of each branch predicts its future actions <ref> [13] </ref>. A particular branch prediction is based on the historical preference of the branch (to take or to not take). In contrast, a popular dynamic scheme with this self-history characteristic uses a branch prediction table (BPT) of two-bit, up/down counters which act as predictors [20].
Reference: [14] <author> S. Moon and K. Ebcioglu, </author> <title> An Efficient Resource-Con strained Global Scheduling Technique for Superscalar and VLIW Processors, </title> <booktitle> Proc. 25th Annual ACM/IEEE Intl. Symp. on Microarchitecture, </booktitle> <month> Dec. </month> <year> 1992 </year>
Reference: [15] <author> F. Mueller and D. Whalley, </author> <title> Avoiding Unconditional Jumps by Code Replication, </title> <booktitle> Proc. ACM SIGPLAN 92 Conf. on Prog. Lang. Design and Implementation, </booktitle> <month> Jun. </month> <year> 1992. </year>
Reference-contexts: Mueller and Whal-ley <ref> [15] </ref> describe a technique that duplicates the target blocks of unconditional branches. This technique removes unconditional branches from the instruction stream and thus decreases the total dynamic instruction count for a program.
Reference: [16] <author> D. Padua and M. Wolfe, </author> <title> Advanced Compiler Optimizations for Supercomputers, </title> <journal> Comm. of the ACM, </journal> <volume> 29(12), </volume> <month> Dec. </month> <year> 1986. </year>
Reference-contexts: The compile-time code duplication techniques that are the closest to our work are the studies in loop unrolling and loop peeling <ref> [16] </ref>, and a very recent study by Krall [8]. The loop techniques use code duplication to construct correlated static branches from a single static branch. For example in loop peeling, a single iteration of a loop is peeled off if the loop frequently executes only one loop iteration.
Reference: [17] <author> S. Pan, K. So, and J. Rahmeh, </author> <title> Improving the Accuracy of Dynamic Branch Prediction Using Branch Correlation, </title> <booktitle> Proc. 5th Annual Intl. Conf. on Architectural Support for Prog. Lang. and Operating Systems, </booktitle> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: This linking of execution histories is called branch correlation <ref> [17] </ref>. Both of the previous studies describe run-time schemes for exploiting the correlation between conditional branches, and they report prediction accuracies over 90%. This paper introduces and evaluates a compile-time, profile-based algorithm for static branch prediction that improves prediction accuracies without the use of special hardware structures. <p> Studies have shown that the property-based predictors are not consistently accurate [10,20]. On the other hand, the history based schemes work quite well, especially when the vast majority of the dynamic branches in a program are loop branches. In work-loads with more general branching, Pan et al. <ref> [17] </ref> and Yeh and Patt [21] both found that the outcome of a particular conditional branch can be strongly affected by the outcomes of earlier branches. Figure 1 contains a code fragment from the SPEC integer benchmark eqntott that Pan et al. [17] used to exemplify branch correlation. <p> work-loads with more general branching, Pan et al. <ref> [17] </ref> and Yeh and Patt [21] both found that the outcome of a particular conditional branch can be strongly affected by the outcomes of earlier branches. Figure 1 contains a code fragment from the SPEC integer benchmark eqntott that Pan et al. [17] used to exemplify branch correlation. In this code, the outcome of branch b3 depends upon the comparison of variables checked and modified by the bodies of branches b1 and b2 (i.e. b3 is correlated with b1 and b2). <p> If branches b1 and b2 both evaluate to TRUE, the evaluation of b3 will always be FALSE. The history-based branch correlation schemes described by Pan et al. <ref> [17] </ref> and by Yeh and Patt [22] add hardware to capture branch correlation. <p> The intuition behind multiple predictors per branch is that specific subhistories of a particular branch can provide more accurate predictions (because of branch correlation) than the averaged history of that branch. 2.1 Exploiting Branch Correlation at Run-time The dynamic, correlation-based branch predictor proposed by Pan et al. <ref> [17] </ref> fits nicely into the model described above. In their scheme, the prediction table is organized as 2 j x2 k two-bit counters. <p> Though it is feasible to use other address bits, our simulations (like the simulations by Pan et al. <ref> [17] </ref>) only use the low-order bits. scheme (i = 0, j and k non-zero). Yeh and Patts PAs variant extends GAs to include a set of 2 i branch history registers; we refer to this variant as the Michigan scheme (i = j and k non-zero). <p> By setting i = k = 0 and j to some non-zero value, we construct the typical self-history BPT organization discussed earlier. By setting i = j = 0 and k non-zero, we derive the degenerate correlation case discussed in Pan et al. <ref> [17] </ref>. They claim that this case is interesting because it allows the hardware to determine the prediction of the next conditional branch immediately after the execution of the previous branch. <p> All of these cases are further examples of branching with little if any correlation. 4.3 Comparison with Dynamic Schemes In Tables 4 and 5, we compare the prediction accuracy of our static history-based branch prediction scheme with the most effective hardware methods proposed by Pan et al. <ref> [17] </ref> (what we call the IBM scheme) and by Yeh and Patt [21] (what we call the Michigan scheme). The hardware schemes are based on a fixed bit budget of 1 kilobyte for the branch prediction table (cost of the history shift registers is ignored).
Reference: [18] <author> M. Smith, </author> <title> Tracing with pixie, </title> <institution> Computer Systems Labora tory Technical Report CSL-TR-91-497, Stanford Univ., </institution> <address> Stan-ford, CA, </address> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: Section 4.2 then presents prediction accuracy and code expansion results obtained with our implementation. Section 4.3 compares our prediction accuracies with those obtained by two hardware-based branch correlation prediction schemes. 4.1 Environment and Simulator We have implemented the algorithm of Section 3 within the framework of pixie <ref> [18] </ref>. Figure 10 shows the organization of our system. The pixie program modifies an executable so that it generates an instruction trace when run. The analysis and simulation program gathers branch profile information from this trace and generates statistics of prediction accuracy 4 and code expansion.
Reference: [19] <author> M. Smith, </author> <title> Architectural Support for Compile-Time Specula tion, The Interaction of Compilation Technology and Computer Architecture, edited by David Lilja and Peter Bird, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1994. </year>
Reference: [20] <author> J. Smith, </author> <title> A Study of Branch Prediction Strategies, </title> <booktitle> Proc. 8th Annual Intl. Symp. on Computer Architecture, </booktitle> <month> Jun. </month> <year> 1981. </year>
Reference-contexts: A particular branch prediction is based on the historical preference of the branch (to take or to not take). In contrast, a popular dynamic scheme with this self-history characteristic uses a branch prediction table (BPT) of two-bit, up/down counters which act as predictors <ref> [20] </ref>. To associate a predictor with a conditional branch, the low-order bits of the branchs address are used as the index into the BPT. Correlation-based branch prediction schemes differ from the schemes above because they associate multiple predictors with each conditional branch (or set of conditional branches) in a program.
Reference: [21] <author> T. Yeh and Y. Patt, </author> <title> Two-Level Adaptive Branch Prediction, </title> <booktitle> Proc. 24th Annual ACM/IEEE Intl. Symp. and Workshop on Microarchitecture, </booktitle> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: On the other hand, the history based schemes work quite well, especially when the vast majority of the dynamic branches in a program are loop branches. In work-loads with more general branching, Pan et al. [17] and Yeh and Patt <ref> [21] </ref> both found that the outcome of a particular conditional branch can be strongly affected by the outcomes of earlier branches. Figure 1 contains a code fragment from the SPEC integer benchmark eqntott that Pan et al. [17] used to exemplify branch correlation. <p> with little if any correlation. 4.3 Comparison with Dynamic Schemes In Tables 4 and 5, we compare the prediction accuracy of our static history-based branch prediction scheme with the most effective hardware methods proposed by Pan et al. [17] (what we call the IBM scheme) and by Yeh and Patt <ref> [21] </ref> (what we call the Michigan scheme). The hardware schemes are based on a fixed bit budget of 1 kilobyte for the branch prediction table (cost of the history shift registers is ignored).
Reference: [22] <author> T. Yeh and Y. Patt, </author> <title> A Comparison of Dynamic Branch Pre dictors that use Two Levels of Branch History, </title> <booktitle> Proc. 20th Annual Intl. Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: If branches b1 and b2 both evaluate to TRUE, the evaluation of b3 will always be FALSE. The history-based branch correlation schemes described by Pan et al. [17] and by Yeh and Patt <ref> [22] </ref> add hardware to capture branch correlation. <p> For the purposes of discussion, we refer to this type of an organization with a single k-bit history register as the IBM scheme. Yeh and Patt <ref> [22] </ref> explore a richer set of dynamic, correlation-based branch prediction schemes beyond those described by the IBM scheme. As illustrated in Figure 2, their organization allows for multiple branch history registers (i.e. multiple shift registers). <p> As illustrated in Figure 2, their organization allows for multiple branch history registers (i.e. multiple shift registers). In Figure 2, i contiguous bits from the branchs address select one of the 2 i branch history registers. Yeh and Patt <ref> [22] </ref> conclude that the most cost-effective hardware schemes for the logical organization presented in Figure 2 are their GAs and PAs variations. Yeh and Patts GAs variant corresponds to what we call the IBM 1. <p> Brief descriptions of the benchmarks and their input data sets appear in Table 1. We compiled the benchmark programs with gcc (version 2.5.4, -O3) on a MIPS DECstation 3100 running ULTRIX V4.2A (Rev. 47). We ran only integer benchmarks since Yeh and Patts work <ref> [22] </ref> suggests that these benchmarks benefit more from correlation-based branch prediction. 4. We report prediction accuracy for only conditional branches; the prediction accu-racy of unconditional branches is not included in our calculations. 5.
Reference: [23] <author> T. Yeh, </author> <title> Two-Level Adaptive Branch Prediction and Instruc tion Fetch Mechanisms for High Performance Superscalar Processors, </title> <institution> Computer Science and Engineering Div. Tech. Report CSE-TR-182-93, Univ. of Michigan, </institution> <address> Ann Arbor, MI, </address> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: If branches b1 and b2 both evaluate to TRUE, the evaluation of b3 will always be FALSE. The history-based branch correlation schemes described by Pan et al. [17] and by Yeh and Patt [22] add hardware to capture branch correlation. As summarized by Yeh <ref> [23] </ref>, these hardware structures are able to adapt to changes in branch behavior due to changes in the characteristics of the data set, and they are able to capture the run-time history patterns of the preceding branches in the dynamic instruction stream.
References-found: 22

