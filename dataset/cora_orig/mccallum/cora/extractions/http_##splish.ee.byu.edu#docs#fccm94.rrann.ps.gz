URL: http://splish.ee.byu.edu/docs/fccm94.rrann.ps.gz
Refering-URL: http://www.cs.wustl.edu/~sds/research/html/hga_conf/node18.html
Root-URL: 
Email: Email: hutch@ee.byu.edu  
Phone: Tel: (801) 378-2667  
Title: Density Enhancement of a Neural Network Using FPGAs and Run-Time Reconfiguration of hardware neurons a
Author: James G. Eldredge and Brad L. Hutchings 
Note: Using reconfigurability in this way increases the number  has been measured.  
Date: April 11, 1994  
Address: Provo, UT 84602  
Affiliation: Dept. of Electrical and Computer Eng. Brigham Young University  
Abstract: Run-time reconfiguration is a way of more fully exploiting the flexibility of reconfigurable FPGAs. The Run-Time Reconfiguration Artificial Neural Network (RRANN) uses run-time reconfiguration to increase the hardware density of FPGAs. The RRANN architecture also allows large amounts of parallelism to be used and is very scalable. RRANN divides the backpropagation algorithm into three sequentially executed stages and configures the FPGAs to execute only one stage at a time. The FPGAs are reconfigured as part of normal execution in order to change stages. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. H. E. Weste and K. Eshraghian, </author> <title> Principles of CMOS VLSI Design, A Systems Perspective, </title> <booktitle> 2 nd ed., </booktitle> <pages> pp. 399-403, </pages> <address> Reading, MA, </address> <publisher> Addison-Wesley Publishing Co., </publisher> <year> 1993. </year>
Reference-contexts: Because of this overhead, an FPGA provides less functionality per unit area of silicon and longer communication delay times than gate-array or full-custom implementations <ref> [1] </ref>. In a conventional static design (one where the FPGA's configuration does not change) this flexibility is wasted, while its overhead remains. The efficient use of FPGAs beyond prototyping and low-volume production requires that the FPGA's flexibility be used directly. One way of exploiting this flexibility is through run-time reconfiguration.
Reference: [2] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams, </author> <title> "Learning Internal Representations by Error Propagation," </title> <booktitle> in Parallel and Distributed Processing, </booktitle> <volume> Vol. </volume> <pages> 1, </pages> <address> Cambridge, MA, </address> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Next, the performance of the 1 IEEE Workshop on FPGAs for Custom Computing Machines, Napa, CA, April 10-13, 1994, pg. 180-188. 2 RRANN architecture is given, and then some conclusions are made about the project. 2 The Backpropagation Algo rithm The backpropagation learning algorithm <ref> [2] </ref> provides a way to train a multilayered feed-forward neural network such as the one shown in Figure 1.
Reference: [3] <author> T. Watanabe, et. al., </author> <title> Neural Network Simulation on a Massively Parallel Cellular Array Processor: </title> <booktitle> AAP-2, IEEE International Joint Conference on Neural Networks, </booktitle> <volume> 2 </volume> <pages> 155-61, </pages> <address> Washington DC, </address> <year> 1989. </year>
Reference-contexts: This RAM is used to store weight and target output data and as a scratch pad to buffer net and error values. In order to use hardware efficiently, RRANN utilizes bit-serial hardware to do mathematical computations. Also, as with other projects <ref> [3, 4] </ref>, RRANN uses lookup tables to implement the activation function (Equation (2)) and its derivative. 3.1 The Feed-Forward Stage The feed-forward stage computes Equations (1) and (2), and the feed-forward neural processor, shown in Figure 3, contains the hardware for these computations.
Reference: [4] <author> D. Hammerstrom, </author> <title> A VLSI Architecture for High Performance, Low-Cost, On-chip Learning, </title> <booktitle> IEEE International Joint Conference on Neural Networks, </booktitle> <volume> 2 </volume> <pages> 537-544, </pages> <address> San Diego CA, </address> <year> 1990. </year>
Reference-contexts: This RAM is used to store weight and target output data and as a scratch pad to buffer net and error values. In order to use hardware efficiently, RRANN utilizes bit-serial hardware to do mathematical computations. Also, as with other projects <ref> [3, 4] </ref>, RRANN uses lookup tables to implement the activation function (Equation (2)) and its derivative. 3.1 The Feed-Forward Stage The feed-forward stage computes Equations (1) and (2), and the feed-forward neural processor, shown in Figure 3, contains the hardware for these computations. <p> Finally, the SynCon and ErrCon state machines implement subroutines that control the evaluation of the network equations. To begin the feed-forward stage, the global controller broadcasts an input value to the neural processors. This broadcast is done over a time-multiplexed bus one input value at a time <ref> [4] </ref>. As each input value is placed on the bus, the neural processors read it and do the appropriate weight multiplication for neurons in the first hidden layer.
Reference: [5] <author> S. S. Erdogan and T. H. Hong, </author> <title> Massively Parallel Com putation of Back-Propagation Algorithm Using The Reconfigurable Machine, </title> <booktitle> World Congress on Neural Networks '93, </booktitle> <volume> 4 </volume> <pages> 861-4, </pages> <address> Portland OR, </address> <year> 1993. </year>
Reference-contexts: This operation can be executed with a large degree of parallelism, is purely local, and, therefore, does not require any communication of data between hardware neurons. Erdogan and Hong <ref> [5] </ref> point out a stumbling block to parallelism in the calculation of (4). This problem occurs when error values are broadcast back through the network in a similar manner as activation values during the feed-forward stage. <p> Computing Equation (4) in this manner achieves the same level of parallel execution as the feed-forward and update stages. This is an important advantage that overcomes the problem of non-local weight distribution discussed by Erdogan and Hong <ref> [5] </ref> without requiring weight duplication or broadcasting. However, a small amount of scalability is lost as each neural processor requires its own line of communication. As the number of neural processors grow, the number of error communication lines grow.
Reference: [6] <author> J. Eldredge, </author> <title> FPGA Density Enhancement of a Neu ral Network Through Run-Time Reconfiguration, </title> <type> Masters Thesis, </type> <institution> Dept. Electrical and Computer Engineering, Brigham Young University, </institution> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: Part utilization (in Xilinx XC3000 CLBs) for the feed-forward, backpropagation, and update stages are summarized in Table 1. The RRANN circuitry was tested on neural networks requiring four XC3090s. For these networks, weight convergence was observed, and the networks made useful generalizations about the training patterns presented them <ref> [6] </ref>. For comparison, two other options for implementing the RRANN architecture have been investigated. The first option combines all three stages of execution into the same circuit module, and this circuit module is then configured onto the FPGAs.
References-found: 6

