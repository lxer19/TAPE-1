URL: http://www.neci.nj.nec.com/homepages/lawrence/papers/results-tnn97/results-tnn97-letter.ps.gz
Refering-URL: http://www.neci.nj.nec.com/homepages/lawrence/papers/results-tnn97/results-tnn97-20.html
Root-URL: 
Email: flawrence,gilesg@research.nj.nec.com, back@zoo.riken.go.jp,  Tsoi@uow.edu.au  
Title: On the Distribution of Performance from Multiple Neural Network Trials, On the Distribution of Performance
Author: Steve Lawrence, Andrew D. Back, Ah Chung Tsoi, C. Lee Giles, Steve Lawrence Andrew D. Back, Ah Chung Tsoi, C. Lee Giles Ah Chung Steve Lawrence C. Lee Giles 
Keyword: neural networks, gradient training, backpropagation, error analysis, convergence, gaussian distribution, probability distributions, statistical methods, box whiskers, kolmogorov smirnov test, mackey-glass, phoneme classification.  
Web: http://www.neci.nj.nec.com/homepages/giles.html  
Address: St. Lucia, Australia.  4 Independence Way, Princeton, NJ 08540.  St. Lucia, Australia.  Northfields Avenue, Wollongong NSW 2522, Australia.  4 Independence Way, Princeton, NJ  College Park, MD 20742.  
Affiliation: Department of Electrical and Computer Engineering, University of Queensland.  NEC Research,  Department of Electrical and Computer Engineering, University of Queensland.  of Informatics, University of Wollongong,  NEC Research Institute,  Institute for Advanced Computer Studies, University of Maryland,  
Note: IEEE Transactions on Neural Networks, Volume 8, Number 6, pp. 15071517. Copyright IEEE.  was with the  He is now with  Ah Chung Tsoi was with the  He is now with the Faculty  is with the  08540. He is also with the  http://www.neci.nj.nec.com/homepages/lawrence  
Abstract: Andrew D. Back was with the Department of Electrical and Computer Engineering, University of Queensland. St. Lucia, Australia. He is now with the Brain Information Processing Group, Frontier Research Program, RIKEN, The Institute of Physical and Chemical Research, 2-1 Hirosawa, Wako-shi, Saitama 351-01, Japan Abstract The performance of neural network simulations is often reported in terms of the mean and standard deviation of a number of simulations performed with different starting conditions. However, in many cases, the distribution of the individual results does not approximate a Gaussian distribution, may not be symmetric, and may be multimodal. We present the distribution of results for practical problems and show that assuming Gaussian distributions can significantly affect the interpretation of results, especially those of comparison studies. For a controlled task which we consider, we find that the distribution of performance is skewed towards better performance for smoother target functions and skewed towards worse performance 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Auer, M. Herbster, and M.K. Warmuth. </author> <title> Exponentially many local minima for single neurons. </title> <editor> In D. Touretzky, M. Mozer, and M. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 8. </volume> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Blum and Rivest [5] have proven that training even a three node network can be NP-complete. These results are for threshold networks. For sigmoid networks, Auer, Herbster and Warmuth <ref> [1] </ref> have shown that the number of local minima may grow exponentially in the number of parameters. It is conjectured that training may also be NP-complete for sigmoid networks.
Reference: [2] <author> A.D. </author> <title> Back. New Techniques for Nonlinear System Identification: A Rapprochement Between Neural Networks and Linear Systems. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering, University of Queensland, </institution> <year> 1992. </year>
Reference-contexts: The MLP networks used an input window of 6. Each network had 5 hidden nodes and was trained for 200,000 updates. There were 1,000 training patterns and 1,000 test patterns. The FIR and IIR networks were tested both with and without synaptic gains <ref> [2] </ref>. It is interesting to observe the difference in the distribution of results in this case. When using synaptic gains an extra parameter is inserted into each synapse which multiplies the weighted sum of the individual filter outputs. <p> Altering a synaptic gain is equivalent to altering all of the weights corresponding to the filter taps. The addition of synaptic gains does not affect the representational power of the networks, however it does affect the error surface and the extra degrees of freedom may make optimization easier <ref> [2] </ref>. the distribution varies significantly across the various models 3 and that the distributions are often highly skewed and several are multimodal. Figure 6 shows box-whiskers plots and the usual mean and standard deviation plots for these models.
Reference: [3] <author> A.D. Back and A.C. Tsoi. </author> <title> FIR and IIR synapses, a new neural network architecture for time series modeling. </title> <booktitle> Neural Computation, </booktitle> <address> 3(3):375385, </address> <year> 1991. </year>
Reference-contexts: For the Mackey-Glass problem, the results of a number of architectures are compared: MLP, FIR MLP, and IIR 8 MLP <ref> [3, 4] </ref>. The FIR and IIR MLP networks are similar to the standard MLP except each synapse is replaced by FIR and IIR 2 filters respectively. For the FIR MLP, the FIR filters were order 5 (6 taps). For the IIR MLP the IIR filters were order (5, 5).
Reference: [4] <author> A.D. Back, E. Wan, Steve Lawrence, and A.C. Tsoi. </author> <title> A unifying view of some training algorithms for multilayer perceptrons with FIR filter synapses. </title> <editor> In J. Vlontzos, J. Hwang, and E. Wilson, editors, </editor> <booktitle> Neural Networks for Signal Processing 4, </booktitle> <pages> pages 146154. </pages> <publisher> IEEE Press, </publisher> <year> 1995. </year>
Reference-contexts: For the Mackey-Glass problem, the results of a number of architectures are compared: MLP, FIR MLP, and IIR 8 MLP <ref> [3, 4] </ref>. The FIR and IIR MLP networks are similar to the standard MLP except each synapse is replaced by FIR and IIR 2 filters respectively. For the FIR MLP, the FIR filters were order 5 (6 taps). For the IIR MLP the IIR filters were order (5, 5).
Reference: [5] <author> A.L. Blum and R.L. Rivest. </author> <title> Training a 3-node neural network is NP-complete. Neural Networks, </title> <address> 5(1):117127, </address> <year> 1992. </year>
Reference-contexts: Researchers in computational learning theory have investigated the complexity of neural network learning <ref> [9, 5, 15, 19] </ref>. Judd [16] showed that even under very restrictive assumptions, the general problem of finding a set of weights consistent with a set of examples is NP-complete. Blum and Rivest [5] have proven that training even a three node network can be NP-complete. <p> Researchers in computational learning theory have investigated the complexity of neural network learning [9, 5, 15, 19]. Judd [16] showed that even under very restrictive assumptions, the general problem of finding a set of weights consistent with a set of examples is NP-complete. Blum and Rivest <ref> [5] </ref> have proven that training even a three node network can be NP-complete. These results are for threshold networks. For sigmoid networks, Auer, Herbster and Warmuth [1] have shown that the number of local minima may grow exponentially in the number of parameters.
Reference: [6] <author> M.L. Brady, R. Raghavan, and J. Slawny. </author> <title> Back propagation fails to separate where perceptrons succeed. </title> <journal> IEEE Transactions on Circuits and Systems, </journal> <volume> 36:665674, </volume> <year> 1989. </year> <month> 18 </month>
Reference-contexts: A typical compromise is to use an iterative optimization technique [9] such as backpropagation. In most cases, such techniques are only guaranteed to find a local minimum of the cost function. Backpropagation can fail in very simple cases <ref> [6, 17, 18] </ref>, resulting in a local minimum significantly worse than the global minimum. When the problem and the training algorithm make it hard to find a globally optimal solution, it may be difficult to predict the expected quality of the solutions found.
Reference: [7] <author> Roger Crane, Charles Fefferman, Scott Markel, and John Pearson. </author> <title> Characterizing neural network error surfaces with a sequential quadratic programming algorithm. In Machines That Learn, </title> <address> Snowbird, </address> <year> 1995. </year>
Reference-contexts: This dataset S forms the training data for 4 The task is similar to the procedure used in <ref> [7] </ref>. 10 MLP and FIR no gains cases are compressed due to the relatively poor performance of the other cases. Therefore, the plot has been repeated in the lower two graphs with a different scale on the ordinate. 11 Training normalized mean squared error Model Mean Median Std. Dev.
Reference: [8] <author> C. Darken and J.E. Moody. </author> <title> Note on learning rate schedules for stochastic optimization. </title> <editor> In R.P. Lippmann, J.E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 832838. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: equation is a time delay differential equation first proposed as a model of white blood cell production [20]: dx = [1 + x c (t t )] 1 We have found this to result in similar performance to the search then converge learning rate schedules proposed by Darken and Moody <ref> [8] </ref>. 7 shows the training distribution and the right hand graph shows the test distribution. The abscissa corresponds to the percentage of examples incorrectly classified and the ordinate represents the percentage of individual results falling within each section of the histogram.
Reference: [9] <author> Andras Farag o and Gabor Lugosi. </author> <title> Strong universal consistency of neural network classifiers. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 39(4):11461151, </volume> <year> 1993. </year>
Reference-contexts: Researchers in computational learning theory have investigated the complexity of neural network learning <ref> [9, 5, 15, 19] </ref>. Judd [16] showed that even under very restrictive assumptions, the general problem of finding a set of weights consistent with a set of examples is NP-complete. Blum and Rivest [5] have proven that training even a three node network can be NP-complete. <p> Therefore, in general, there may be no algorithm capable of finding the optimal set of parameters which has computation time that is bounded by a polynomial in d, the input dimension. A typical compromise is to use an iterative optimization technique <ref> [9] </ref> such as backpropagation. In most cases, such techniques are only guaranteed to find a local minimum of the cost function. Backpropagation can fail in very simple cases [6, 17, 18], resulting in a local minimum significantly worse than the global minimum.
Reference: [10] <author> J. D. Farmer. </author> <title> Chaotic attractors of an infinite-dimensional dynamical system. </title> <journal> Physica, </journal> <volume> 4D:366, </volume> <year> 1982. </year>
Reference-contexts: The delay parameter t determines the behavior of the system <ref> [10] </ref>. For t &lt; 4:53 there is a stable fixed point attractor. For 4:53 &lt; t &lt; 13:3 there is a stable limit cycle attractor. Period doubling begins at t = 13:3 and continues until t = 16:8. For t &gt; 16:8 the system produces a chaotic attractor.
Reference: [11] <author> Arthur Flexer. </author> <title> Statistical evaluation of neural network experiments: Minimum requirements and current practice. </title> <type> Technical Report OEFAI-TR-95-16, </type> <institution> The Austrian Research Institute for Artificial Intelligence, Schottengasse 3, A-1010 Vienna, Austria, </institution> <year> 1995. </year>
Reference-contexts: 1 Introduction 1.1 Performance Measures It is common in neural network simulations to report results using the mean and standard deviation of a number of trials with different starting conditions <ref> [11] </ref>. However, in many cases, the distribution of the individual results does not approximate a Gaussian distribution, may not be symmetric, and may be multimodal. For non-Gaussian distributions, more informative methods can be used to present the data (such as plotting the distribution or box-whiskers plots (see section 2.2)). <p> Other papers recommend reporting confidence intervals using Gaussian or t-distributions (for a number of runs less than 100) and testing the significance of comparisons using the t-test <ref> [11] </ref>. However, these assume symmetric distributions. The distribution of results for neural network simulations can vary widely depending on the network architecture, the data, and the training algorithm.
Reference: [12] <author> T.G. Gonzalez, S. Sahni, and W.R. Franta. </author> <title> An efficient algorithm for the Kolmogorov-Smirnov and Lilliefors tests. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 3(1):6064, </volume> <year> 1977. </year>
Reference-contexts: Outliers are sometimes plotted separately (e.g. points greater than 1.5 IQR from the ends of the box may be considered to be outliers and plotted separately [23]). 2.3 Kolmogorov-Smirnov Test The Kolmogorov-Smirnov (K-S) test can be used to test the normality of a distribution. The K-S D statistic is <ref> [25, 12, 22] </ref>: 1&lt;x<1 where S (x) is an estimator of the cumulative distribution function of the distribution to test and N (x) is the cumulative distribution function for (in this case) the normal distribution (erf ( x p 2 )=2 + 0:5 for mean 0 and variance 1).
Reference: [13] <author> S. Haykin. </author> <title> Neural Networks, A Comprehensive Foundation. </title> <publisher> Macmillan, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: Except when specified, all networks are MLPs. All inputs were normalized to zero mean and unit variance. The quadratic cost function was used <ref> [13] </ref>. The learning rate was reduced linearly to zero over the training period from an 6 initial value of 0.1 1 . <p> They are initialized on a node by node basis as uniformly distributed random numbers in the range (2:4=F i ; 2:4=F i ) where F i is the fan-in of neuron i <ref> [13] </ref>. Each network was trained for 200,000 updates. It is difficult to visualize the function we are trying to approximate.
Reference: [14] <author> B.A. Huberman, R.M. Lukose, and T. Hogg. </author> <title> An economics approach to hard computational problems. </title> <booktitle> Science, </booktitle> <address> 275:5154, </address> <year> 1997. </year>
Reference-contexts: In some cases, it may be desirable to trade lower average performance for an increased certainty of obtaining a given performance level. Depending on the distribution of results, a portfolio approach <ref> [14] </ref> may be used to reduce, for example, the expected time for training to reach a given target performance (in a portfolio approach, multiple networks are trained in parallel and the fraction of time spent training each network can be adjusted in order to optimize a given training criterion, such as
Reference: [15] <author> J.S. Judd. </author> <title> On the complexity of loading shallow neural networks. </title> <journal> Journal of Complexity, </journal> <volume> 4:177192, </volume> <year> 1988. </year>
Reference-contexts: Researchers in computational learning theory have investigated the complexity of neural network learning <ref> [9, 5, 15, 19] </ref>. Judd [16] showed that even under very restrictive assumptions, the general problem of finding a set of weights consistent with a set of examples is NP-complete. Blum and Rivest [5] have proven that training even a three node network can be NP-complete.
Reference: [16] <author> J.S. Judd. </author> <title> Neural Network Design and the Complexity of Learning. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: Researchers in computational learning theory have investigated the complexity of neural network learning [9, 5, 15, 19]. Judd <ref> [16] </ref> showed that even under very restrictive assumptions, the general problem of finding a set of weights consistent with a set of examples is NP-complete. Blum and Rivest [5] have proven that training even a three node network can be NP-complete. These results are for threshold networks.
Reference: [17] <author> Steve Lawrence, C. Lee Giles, and A.C. Tsoi. </author> <title> What size neural network gives optimal generalization? Convergence properties of backpropagation. </title> <institution> Technical Report UMIACS-TR-96-22 and CS-TR-3617, Institute for Advanced Computer Studies, University of Maryland, College Park MD 20742, </institution> <month> April </month> <year> 1996. </year>
Reference-contexts: A typical compromise is to use an iterative optimization technique [9] such as backpropagation. In most cases, such techniques are only guaranteed to find a local minimum of the cost function. Backpropagation can fail in very simple cases <ref> [6, 17, 18] </ref>, resulting in a local minimum significantly worse than the global minimum. When the problem and the training algorithm make it hard to find a globally optimal solution, it may be difficult to predict the expected quality of the solutions found.
Reference: [18] <author> Steve Lawrence, C. Lee Giles, and A.C. Tsoi. </author> <title> Lessons in neural network training: Overfitting may be harder than expected. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Artificial Intelligence, AAAI-97, </booktitle> <pages> pages 540545. </pages> <publisher> AAAI Press, </publisher> <address> Menlo Park, California, </address> <year> 1997. </year>
Reference-contexts: A typical compromise is to use an iterative optimization technique [9] such as backpropagation. In most cases, such techniques are only guaranteed to find a local minimum of the cost function. Backpropagation can fail in very simple cases <ref> [6, 17, 18] </ref>, resulting in a local minimum significantly worse than the global minimum. When the problem and the training algorithm make it hard to find a globally optimal solution, it may be difficult to predict the expected quality of the solutions found.
Reference: [19] <author> J.H. Lin and J.S. Vitter. </author> <title> Complexity results on learning by neural nets. </title> <booktitle> Machine Learning, </booktitle> <address> 6:211230, </address> <year> 1991. </year>
Reference-contexts: Researchers in computational learning theory have investigated the complexity of neural network learning <ref> [9, 5, 15, 19] </ref>. Judd [16] showed that even under very restrictive assumptions, the general problem of finding a set of weights consistent with a set of examples is NP-complete. Blum and Rivest [5] have proven that training even a three node network can be NP-complete.
Reference: [20] <author> M.C. Mackey and L. Glass. </author> <title> Oscillation and chaos in physiological control systems. </title> <booktitle> Science, </booktitle> <address> 197:287, </address> <year> 1977. </year>
Reference-contexts: indication of how often a trial meets a given performance criterion and can therefore be useful in determining how many trials are required in a given situation. 3.3 Mackey-Glass Time Series The Mackey-Glass equation is a time delay differential equation first proposed as a model of white blood cell production <ref> [20] </ref>: dx = [1 + x c (t t )] 1 We have found this to result in similar performance to the search then converge learning rate schedules proposed by Darken and Moody [8]. 7 shows the training distribution and the right hand graph shows the test distribution.
Reference: [21] <author> A. Papoulis. </author> <title> Probability, Random Variables, and Stochastic Processes. </title> <publisher> McGraw-Hill, 3rd edition, </publisher> <year> 1991. </year> <month> 19 </month>
Reference-contexts: The 4 distributions are a) a Gaussian distribution, b) a Cauchy distribution <ref> [21] </ref>, c) a Beta distribution [21], and d) a distribution created from the summation of two Gaussian distributions. <p> The 4 distributions are a) a Gaussian distribution, b) a Cauchy distribution <ref> [21] </ref>, c) a Beta distribution [21], and d) a distribution created from the summation of two Gaussian distributions.
Reference: [22] <author> W.H. Press, S.A. Teukolsky, W.T. Vetterling, and B.P. Flannery. </author> <title> Numerical Recipes. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <note> second edition, </note> <year> 1992. </year>
Reference-contexts: When the number of points is even, it is conventional to estimate the median as the mean of the two central values, i.e. the median is defined as <ref> [22] </ref>: x m = &lt; x (N+1)=2 ; N odd 2 x N=2 + x (N=2)+1 ; N even (2) where the data is in order from the smallest value, x 1 , to the largest value, x N . <p> Outliers are sometimes plotted separately (e.g. points greater than 1.5 IQR from the ends of the box may be considered to be outliers and plotted separately [23]). 2.3 Kolmogorov-Smirnov Test The Kolmogorov-Smirnov (K-S) test can be used to test the normality of a distribution. The K-S D statistic is <ref> [25, 12, 22] </ref>: 1&lt;x<1 where S (x) is an estimator of the cumulative distribution function of the distribution to test and N (x) is the cumulative distribution function for (in this case) the normal distribution (erf ( x p 2 )=2 + 0:5 for mean 0 and variance 1).
Reference: [23] <author> J.W. Tukey. </author> <title> Exploratory Data Analysis. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1977. </year>
Reference-contexts: More specifically, the statistic of interest may be the probability that a trial will meet a given performance criterion. The quartiles can provide more information about the distribution of the results and, consequently, the nature of the optimization process. 2.2 Box-Whiskers Plots Box-whiskers plots <ref> [23] </ref> use the median and IQR as measures of central tendency and spread rather than the mean and standard deviation. Additionally, they also show the maximum and minimum values of the distribution. The IQR is shown with a box and the median is represented with a bar across the box. <p> Whiskers extend from the ends of the box to the minimum and maximum values. Outliers are sometimes plotted separately (e.g. points greater than 1.5 IQR from the ends of the box may be considered to be outliers and plotted separately <ref> [23] </ref>). 2.3 Kolmogorov-Smirnov Test The Kolmogorov-Smirnov (K-S) test can be used to test the normality of a distribution. <p> Distributions can be significantly multimodal and neither the mean plus standard deviation nor box-whisker plots show the complete picture. 2. Use the median, interquartile range, minimum and maximum values as well as the mean and standard devi ation for interpreting results. When plotting results, use box-whiskers plots <ref> [23] </ref>. 3. In certain cases it may be possible to approximate a normal distribution by removing outliers. For the case where a relatively small number of trials result in comparatively poor convergence, the practice of removing 16 Training normalized mean squared error K Mean Median Std. Dev.
Reference: [24] <author> M. Verleysen, J.L. Voz, P. Thissen, and J.D. Legat. </author> <title> A statistical neural network for high-dimensional vector classification. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks, ICNN 95, </booktitle> <address> Perth, Western Australia, 1995. </address> <publisher> IEEE. </publisher>
Reference-contexts: The aim of the task is to distinguish between nasal and oral vowels <ref> [24] </ref>. There are 3600 training patterns, 1800 test patterns, five inputs provided by cochlear spectra, and two outputs. Using 10 hidden nodes and 250,000 iterations per trial, the distribution of results is shown in figure 3.
Reference: [25] <author> R. </author> <title> von Mises. Mathematical Theory of Probability and Statistics. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1964. </year>
Reference-contexts: Outliers are sometimes plotted separately (e.g. points greater than 1.5 IQR from the ends of the box may be considered to be outliers and plotted separately [23]). 2.3 Kolmogorov-Smirnov Test The Kolmogorov-Smirnov (K-S) test can be used to test the normality of a distribution. The K-S D statistic is <ref> [25, 12, 22] </ref>: 1&lt;x<1 where S (x) is an estimator of the cumulative distribution function of the distribution to test and N (x) is the cumulative distribution function for (in this case) the normal distribution (erf ( x p 2 )=2 + 0:5 for mean 0 and variance 1).
Reference: [26] <author> A.S. Weigend and N.A. Gershenfeld. </author> <title> Results of the time series prediction competition at the Santa Fe Institute. </title> <booktitle> In IEEE International Conference on Neural Networks, </booktitle> <pages> pages 17861793. </pages> <publisher> IEEE Press, </publisher> <address> Piscataway, NJ, </address> <year> 1993. </year>
Reference-contexts: The learning rate was reduced linearly to zero over the training period from an 6 initial value of 0.1 1 . Performance is reported in terms of the percentage of examples incorrectly classified (for the classification problem) or normalized mean squared error (NMSE) which is defined as <ref> [26] </ref>: Definition 1 NMSE = P N p P N o 2 P N p P N o P N p P N o 2 =(N p N o ) where d is the desired or target value, y is the predicted value, N p is the number of patterns, and
Reference: [27] <author> N.A. Weiss and M.J. Hassett. </author> <title> Introductory Statistics. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <note> second edition, 1987. 20 </note>
Reference-contexts: and standard deviation for describing a distribution, and the Kolmogorov-Smirnov test which we use to test a distribution for normality. 2.1 Median and Interquartile Range The median and the interquartile range (IQR) are simple statistics which are not as sensitive to outliers as the commonly used mean and standard deviation <ref> [27] </ref>.
References-found: 27

