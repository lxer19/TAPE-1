URL: http://www.cs.columbia.edu/~andreas/publications/ICDCS97.ps.gz
Refering-URL: http://www.cs.columbia.edu/~andreas/publications/publications.html
Root-URL: http://www.cs.columbia.edu
Title: Extensible Resource Management For Cluster Computing  
Author: Nayeem Islam Andreas L. Prodromidis Mark S. Squillante Liana L. Fong Ajei S. Gopal 
Abstract: Advanced general-purpose parallel systems should be able to support diverse applications with different resource requirements without compromising effectiveness and efficiency. We present a new resource management model for cluster computing that allows multiple scheduling policies to co-exist dynamically. In particular, we have built Octopus, an extensible and distributed hierarchical scheduler that implements new space-sharing, gang-scheduling and load-sharing strategies. A series of experiments performed on an IBM SP2 suggest that Octopus can effectively match application requirements to available resources, and improve the performance of a variety of parallel applications within a cluster. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Campbell, N. Islam, D. Raila, and P. Madany. </author> <title> Designing and Implementing Choices:an Object-Oriented System in C++. </title> <journal> CACM, </journal> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: Scheduler Extensibility Octopus is an extensible system, designed using object-oriented frameworks <ref> [8, 7, 1, 9] </ref>, that supports multiple scheduling paradigms and permits the easy implementation and incorporation of new paradigms.
Reference: [2] <author> S.-H. Chiang, R. K. Mansharamani, and M. K. Vernon. </author> <title> Use of application characteristics and limited preemption for run-to-completion parallel processor scheduling policies. </title> <booktitle> In Proc. ACM SIGMETRICS Conf. Meas. Mod. Comp. Sys., </booktitle> <pages> pp 33-44, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: The first two are unable to adjust scheduling decisions in response to workload changes <ref> [14, 2] </ref>, while the latter incurs increased overheads [3, 13]. Similarly, in time-sharing, certain conditions can force the system into low resource utilization.
Reference: [3] <author> K. Dussa, B. Carlson, L. Dowdy, and K-H. Park. </author> <title> Dynamic partitioning in transputer environments. </title> <booktitle> In Proc. ACM SIGMETRICS Conf. Meas. Mod. Comp. Sys., </booktitle> <pages> pp 203-213, </pages> <year> 1990. </year>
Reference-contexts: The first two are unable to adjust scheduling decisions in response to workload changes [14, 2], while the latter incurs increased overheads <ref> [3, 13] </ref>. Similarly, in time-sharing, certain conditions can force the system into low resource utilization. On the other hand, several strategies that combine space-sharing and time-sharing, in what is often called gang scheduling [15, 4], provide the benefits of both space-sharing and time-sharing.
Reference: [4] <author> D. G. Feitelson and L. Rudolph. </author> <title> Distributed hierarchical control for parallel processing. </title> <booktitle> Computer, </booktitle> <pages> pp 65-77, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Similarly, in time-sharing, certain conditions can force the system into low resource utilization. On the other hand, several strategies that combine space-sharing and time-sharing, in what is often called gang scheduling <ref> [15, 4] </ref>, provide the benefits of both space-sharing and time-sharing. Gang Scheduling can be effective with small/interactive jobs, and can reduce the mean job response time (for a given partitioning of the nodes) [16].
Reference: [5] <author> B. Ford and S. Susarla. </author> <title> CPU Inheritance Scheduling. </title> <booktitle> OSDI 96, </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Advanced parallel computer systems must be capable of supporting a variety of applications with diverse resource requirements and different scheduling needs. Several hierarchical schedulers have already been discussed in the literature <ref> [5, 6] </ref>, but they all focus on special application domains. Many scheduling strategies for distributed memory parallel systems exist, but they are not general enough to accommodate diverse applications. Furthermore, most batch schedulers, e.g LoadLeveler, are single paradigm schedulers and is not designed to be extensible.
Reference: [6] <author> P. Goyal, X. Gao, and H. Vin. </author> <title> A Hierarchical CPU Sched-uler for Multimedia Operating Systems. </title> <booktitle> OSDI 96, </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Advanced parallel computer systems must be capable of supporting a variety of applications with diverse resource requirements and different scheduling needs. Several hierarchical schedulers have already been discussed in the literature <ref> [5, 6] </ref>, but they all focus on special application domains. Many scheduling strategies for distributed memory parallel systems exist, but they are not general enough to accommodate diverse applications. Furthermore, most batch schedulers, e.g LoadLeveler, are single paradigm schedulers and is not designed to be extensible.
Reference: [7] <author> N. Islam. </author> <title> Distributed Objects: Methodologies for Customizing System Software. </title> <booktitle> IEEE Computer 1996. </booktitle>
Reference-contexts: Scheduler Extensibility Octopus is an extensible system, designed using object-oriented frameworks <ref> [8, 7, 1, 9] </ref>, that supports multiple scheduling paradigms and permits the easy implementation and incorporation of new paradigms.
Reference: [8] <author> N. Islam. </author> <title> Customizing System Software Using OO Frameworks. </title> <booktitle> In IEEE Computer, </booktitle> <month> Feb. </month> <year> 1997. </year>
Reference-contexts: Scheduler Extensibility Octopus is an extensible system, designed using object-oriented frameworks <ref> [8, 7, 1, 9] </ref>, that supports multiple scheduling paradigms and permits the easy implementation and incorporation of new paradigms.
Reference: [9] <author> N. Islam and M. Devarakonda. </author> <title> An Essential Design Pattern for Fault-Tolerant Distributed State Sharing. </title> <journal> In CACM, </journal> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Scheduler Extensibility Octopus is an extensible system, designed using object-oriented frameworks <ref> [8, 7, 1, 9] </ref>, that supports multiple scheduling paradigms and permits the easy implementation and incorporation of new paradigms.
Reference: [10] <author> N. Islam, A. L. Prodromidis, and M. S. Squillante. </author> <title> Dynamic partitioning in different distributed-memory environments. In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp 244-270. </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year> <booktitle> Lecture Notes in Comp. Sci. </booktitle> <volume> Vol. </volume> <pages> 1162. </pages>
Reference-contexts: Space Sharing The dynamic form of space sharing can provide the best system performance for a wide variety of application workloads, especially those with lower efficiencies and/or those with less variable service time requirements (as our results show). Octopus uses the dynamic partitioning with smoothing approach <ref> [10] </ref> to determine the number of partitions in the system, denoted by K. According to this approach, the resource management system attempts to balance the node utilizations by reallocating the resources among the applications.
Reference: [11] <author> N. Islam, A. L. Prodromidis, M. S. Squillante, A. S. Gopal, and L. L. Fong. </author> <title> Extensible resource management for cluster computing. </title> <type> Tech. Rep. </type> <institution> RC 20526, IBM Research Division, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: The smoothing interval between repartition epochs is larger than that for space-sharing, since repartition-ing in this case implies a reconstruction of (portions of) the gang-scheduling matrix. This plus a number of other optimizations <ref> [11] </ref> can reduce substantially the undesirable cost of repartitioning. During each smoothing interval (in which K is fixed), the system maintains a high-level of performance by adjusting the values of D k in response to changing load conditions (subject to memory constraints and partition utilization). <p> Gang Scheduling We have performed several experiments associated with the gang scheduling partition, including measuring overheads (context switching, clock drifting between the node clocks, matrix downloading) and parameter tuning. Detailed results can be found in <ref> [11] </ref>. Here, we examined how the different values of K and D k (degree of multiprogramming for partition k) can affect performance with respect to system load.
Reference: [12] <author> S. T. Leutenegger and M. K. Vernon. </author> <title> The performance of multiprogrammed multiprocessor scheduling policies. </title> <booktitle> In Proc. ACM SIGMETRICS Conf. Meas. Mod. Comp. Sys., </booktitle> <pages> pp 226-236, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: For example, there are three partitioning policies for space sharing: (1) static, where partitions are fixed in size, (2) adaptive where the number of nodes allocated to a job is decided upon its arrival and depends upon the system state <ref> [12, 14] </ref>, and (3) dynamic where partition sizes can change dynamically, and all of them have disadvantages under certain conditions. The first two are unable to adjust scheduling decisions in response to workload changes [14, 2], while the latter incurs increased overheads [3, 13].
Reference: [13] <author> C. McCann and J. Zahorjan. </author> <title> Processor allocation policies for message-passing parallel computers. </title> <booktitle> In Proc. ACM SIGMETRICS Conf. Meas. Mod. Comp. Sys., </booktitle> <pages> pp 19-32, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: The first two are unable to adjust scheduling decisions in response to workload changes [14, 2], while the latter incurs increased overheads <ref> [3, 13] </ref>. Similarly, in time-sharing, certain conditions can force the system into low resource utilization. On the other hand, several strategies that combine space-sharing and time-sharing, in what is often called gang scheduling [15, 4], provide the benefits of both space-sharing and time-sharing. <p> the benefits of our dynamic partitioning with smoothing policy (see Section 3), we compared the instance used within the space-sharing PLS against two other previously proposed strategies for dynamic space sharing, the greedy dynamic partitioning policy (the smoothing interval was set to 0), and the basic folding policy without rotation <ref> [13] </ref>. In these experiments, we measured the mean response times of the jobs of workload W2, which performs best under space-sharing due to its inefficiencies (see Section 4), at different utilization levels and we reported the normalized response times in Figure 3.
Reference: [14] <author> V. K. Naik, S. K. Setia, and M. S. Squillante. </author> <title> Performance analysis of job scheduling policies in parallel supercomputing environments. </title> <booktitle> In Proc. Supercomputing, </booktitle> <pages> pp 824-833, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: For example, there are three partitioning policies for space sharing: (1) static, where partitions are fixed in size, (2) adaptive where the number of nodes allocated to a job is decided upon its arrival and depends upon the system state <ref> [12, 14] </ref>, and (3) dynamic where partition sizes can change dynamically, and all of them have disadvantages under certain conditions. The first two are unable to adjust scheduling decisions in response to workload changes [14, 2], while the latter incurs increased overheads [3, 13]. <p> The first two are unable to adjust scheduling decisions in response to workload changes <ref> [14, 2] </ref>, while the latter incurs increased overheads [3, 13]. Similarly, in time-sharing, certain conditions can force the system into low resource utilization.
Reference: [15] <author> J. K. Ousterhout. </author> <title> Scheduling techniques for concurrent systems. </title> <booktitle> In Proc. Third Int. Conf. Dist. Comp. Sys., </booktitle> <pages> pp 22-30, </pages> <month> Oct. </month> <year> 1982. </year>
Reference-contexts: In one possible categorization, parallel applications can be classified as tightly-synchronized or loosely-synchronized. Tightly-synchronized applications are applications with threads that need to interact, synchronize and communicate frequently, hence they need to be scheduled on different nodes simultaneously <ref> [15] </ref>. Three main scheduling paradigms support applications with such requirements: those which space-share the available nodes by partitioning them among the different parallel jobs; those which time-share the nodes by rotating them among a set of jobs; and those which combine space-sharing and time-sharing. <p> Similarly, in time-sharing, certain conditions can force the system into low resource utilization. On the other hand, several strategies that combine space-sharing and time-sharing, in what is often called gang scheduling <ref> [15, 4] </ref>, provide the benefits of both space-sharing and time-sharing. Gang Scheduling can be effective with small/interactive jobs, and can reduce the mean job response time (for a given partitioning of the nodes) [16]. <p> Gang Scheduling Time and Space Sharing Gang-scheduling combines the advantages of space-sharing and time-sharing. Our approach can be conceptually viewed as a generalization of the global matrix originally proposed by Ousterhout <ref> [15] </ref>. Each column represents one node and each row represents one timeslice. In our approach, the columns are divided into K disjoint partitions , or groups.
Reference: [16] <author> M. S. Squillante. </author> <title> On the benefits and limitations of dynamic partitioning in parallel computer systems. In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feit-elson and L. </editor> <booktitle> Rudolph (eds.), </booktitle> <pages> pp 219-238. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <booktitle> Lecture Notes Comp. Sci. </booktitle> <volume> Vol. </volume> <pages> 949. </pages>
Reference-contexts: Gang Scheduling can be effective with small/interactive jobs, and can reduce the mean job response time (for a given partitioning of the nodes) <ref> [16] </ref>. These advantages are very important especially under highly variable workloads although they come with the price of increased overhead for context switching among the parallel jobs and fragmentation of node allocations.
Reference: [17] <author> M. S. Squillante and K. P. Tsoukatos. </author> <title> Optimal scheduling of coarse-grained parallel applications. </title> <booktitle> In Proc. Eighth SIAM Conf. Par. Processing Sci. Comp., </booktitle> <month> Mar. </month> <year> 1997. </year>
Reference-contexts: Load-Sharing Paradigm Octopus adopts a new practical scheduling strategy for the load-sharing paradigm that is based on previous theoretical scheduling results <ref> [17] </ref>. It outperforms alternative approaches for a class of loosely-synchronized applications. Upon the arrival of a job J the scheduling policy first determines the best set of nodes on which to schedule the tasks of the job.
References-found: 17

