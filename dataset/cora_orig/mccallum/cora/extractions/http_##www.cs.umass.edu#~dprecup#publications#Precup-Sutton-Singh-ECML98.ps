URL: http://www.cs.umass.edu/~dprecup/publications/Precup-Sutton-Singh-ECML98.ps
Refering-URL: http://www-anw.cs.umass.edu/~rich/publications.html
Root-URL: 
Phone: 2  
Title: Temporally Abstract Options  
Author: Doina Precup Richard S. Sutton and Satinder Singh 
Web: http://www.cs.umass.edu/fdprecupjrichg  http://www.cs.colorado.edu/baveja  
Address: Amherst, MA 01003-4610  Boulder, CO 80309-0430  
Affiliation: University of Massachusetts  Department of Computer Science University of Colorado  
Abstract: Theoretical Results on Reinforcement Learning with Abstract. We present new theoretical results on planning within the framework of temporally abstract reinforcement learning (Precup & Sutton, 1997; Sutton, 1995). Temporal abstraction is a key step in any decision making system that involves planning and prediction. In temporally abstract reinforcement learning, the agent is allowed to choose among options, whole courses of action that may be temporally extended, stochastic, and contingent on previous events. Examples of options include closed-loop policies such as picking up an object, as well as primitive actions such as joint torques. Knowledge about the consequences of options is represented by special structures called multi-time models. In this paper we focus on the theory of planning with multi-time models. We define new Bellman equations that are satisfied for sets of multi-time models. As a consequence, multi-time models can be used interchangeably with models of primitive actions in a variety of well-known planning methods including value iteration, policy improvement and policy iteration.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Dimitri P. Bertsekas. </author> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1987. </year>
Reference-contexts: Therefore, T o (V) T o (V 0 ) * o k V V 0 k 1 The result follows from the contraction mapping theorem <ref> [1] </ref>. ut So far we have established that the value functions of Markov option policies have similar properties with the Markov policies that use only primitive actions. Now we establish similar results for the optimal value function that can be obtained when planning with a set of options. <p> Therefore T O is a contraction with constant * O . The results follow from the contraction mapping theorem <ref> [1] </ref>. So far we have shown that the optimal value function V fl O is the unique bounded solution of the Bellman optimality equations. Now we will show that this value function can be achieved by a deterministic Markov policy: Theorem 6 (Value Achievement). <p> Theoretical results similar to the ones presented in this paper are also available for planning in optimal stopping tasks <ref> [1] </ref>, as well as for a different regime of executing options, which allows early termination. Further research will be devoted to integrating temporal and state abstraction, and to the issue of discovering useful options.
Reference: 2. <author> Peter Dayan. </author> <title> Improving generalization for temporal difference learning: </title> <booktitle> The successor representation. Neural Computation, </booktitle> <address> 5:613624, </address> <year> 1993. </year>
Reference-contexts: In order to use such actions in planning, an agent needs the ability to create and handle models at a variety of different, interrelated levels of temporal abstraction. Sutton [19] introduced an approach to modeling at different time scales, based on prior work by Singh [17], Dayan <ref> [2] </ref> and by Sutton and Pinette [21]. This approach enables models of the environment at different temporal scales to be intermixed, producing temporally abstract models. In previous work [14], we generalized this approach from the prediction case, to the full control case.
Reference: 3. <author> Peter Dayan and Geoff E. Hinton. </author> <title> Feudal reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 5, </volume> <pages> pages 271278, </pages> <address> Cambridge, MA, 1993. </address> <publisher> MIT Press. </publisher>
Reference: 4. <author> Thomas G. Dietterich. </author> <title> Hierarchical reinforecement learning with maxq value function decomposition. </title> <type> Technical report, </type> <institution> Computer Science Department, Oregon State University, </institution> <year> 1997. </year>
Reference: 5. <author> Manfred Huber and Roderic A. Grupen. </author> <title> Learning to coordinate controllers reinforcement learning on a control basis. </title> <booktitle> In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, IJCAI-97, </booktitle> <address> San Francisco, CA, 1997. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: 6. <author> Leslie P. Kaelbling. </author> <title> Hierarchical learning in stochastic domains: Preliminary results. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning ICML'93, </booktitle> <pages> pages 167173, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: 7. <author> Richard E. Korf. </author> <title> Learning to Solve Problems by Searching for Macro-Operators. </title> <publisher> Pitman Publishing Ltd, </publisher> <address> London, </address> <year> 1985. </year>
Reference-contexts: In this paper, we summarize the framework of temporally abstract reinforcement learning and present new theoretical results on planning with general options and temporally abstract models of options. Options are similar to AI's classical macro operators <ref> [7, 8, 16] </ref>, in that they can take control for some period of time, determining the actions during that time, and in that one can choose among options much as one originally chose among primitive actions.
Reference: 8. <author> John E. Laird, Paul S. Rosenbloom, and Allan Newell. </author> <title> Chunking in SOAR: The anatomy of a general learning mechanism. </title> <booktitle> Machine Learning, </booktitle> <address> 1:1146, </address> <year> 1986. </year>
Reference-contexts: In this paper, we summarize the framework of temporally abstract reinforcement learning and present new theoretical results on planning with general options and temporally abstract models of options. Options are similar to AI's classical macro operators <ref> [7, 8, 16] </ref>, in that they can take control for some period of time, determining the actions during that time, and in that one can choose among options much as one originally chose among primitive actions.
Reference: 9. <author> Sridhar Mahadevan and Jonathan Connell. </author> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <journal> Artificial Intelligence, </journal> <volume> 55(2-3):311365, </volume> <year> 1992. </year>
Reference-contexts: However, conventional model-based reinforcement learning uses one-step models [11, 13, 18], that cannot represent common-sense, higher-level actions, such as picking an object or traveling to a specified location. Several researchers have proposed extending reinforcement learning to a higher level by treating entire closed-loop policies as actions <ref> [36, 9, 10, 12, 17] </ref>. In order to use such actions in planning, an agent needs the ability to create and handle models at a variety of different, interrelated levels of temporal abstraction.
Reference: 10. <author> Amy McGovern, Richard S. Sutton, and Andrew H. Fagg. </author> <title> Roles of macro-actions in accelerating reinforcement learning. </title> <booktitle> In Grace Hopper Celebration of Women in Computing, </booktitle> <pages> pages 1318, </pages> <year> 1997. </year>
Reference-contexts: However, conventional model-based reinforcement learning uses one-step models [11, 13, 18], that cannot represent common-sense, higher-level actions, such as picking an object or traveling to a specified location. Several researchers have proposed extending reinforcement learning to a higher level by treating entire closed-loop policies as actions <ref> [36, 9, 10, 12, 17] </ref>. In order to use such actions in planning, an agent needs the ability to create and handle models at a variety of different, interrelated levels of temporal abstraction.
Reference: 11. <author> Andrew W. Moore and Chris G. Atkeson. </author> <title> Prioritized sweeping: Reinforcement learning with less data and less real time. </title> <booktitle> Machine Learning, </booktitle> <address> 13:103130, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction Model-based reinforcement learning offers a possible solution to the problem of integrating planning with real-time learning and decision-making [20]. However, conventional model-based reinforcement learning uses one-step models <ref> [11, 13, 18] </ref>, that cannot represent common-sense, higher-level actions, such as picking an object or traveling to a specified location. Several researchers have proposed extending reinforcement learning to a higher level by treating entire closed-loop policies as actions [36, 9, 10, 12, 17].
Reference: 12. <author> Ronald Parr and Stuart Russell. </author> <title> Reinforcement learning with hierarchies of machines. </title> <booktitle> In Advances in Neural Information Processing Systems, volume 10, </booktitle> <address> Cambridge, MA, 1998. </address> <publisher> MIT Press. </publisher>
Reference-contexts: However, conventional model-based reinforcement learning uses one-step models [11, 13, 18], that cannot represent common-sense, higher-level actions, such as picking an object or traveling to a specified location. Several researchers have proposed extending reinforcement learning to a higher level by treating entire closed-loop policies as actions <ref> [36, 9, 10, 12, 17] </ref>. In order to use such actions in planning, an agent needs the ability to create and handle models at a variety of different, interrelated levels of temporal abstraction.
Reference: 13. <author> Jing Peng and John Williams. </author> <title> Efficient learning and planning within the Dyna framework. Adaptive Behavior, </title> <address> 4:323 334, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction Model-based reinforcement learning offers a possible solution to the problem of integrating planning with real-time learning and decision-making [20]. However, conventional model-based reinforcement learning uses one-step models <ref> [11, 13, 18] </ref>, that cannot represent common-sense, higher-level actions, such as picking an object or traveling to a specified location. Several researchers have proposed extending reinforcement learning to a higher level by treating entire closed-loop policies as actions [36, 9, 10, 12, 17].
Reference: 14. <author> Doina Precup and Richard S. Sutton. </author> <title> Multi-Time models for temporally abstract planning. </title> <booktitle> In Advances in Neural Information Processing Systems, volume 10, </booktitle> <address> Cambridge, MA, 1998. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Sutton [19] introduced an approach to modeling at different time scales, based on prior work by Singh [17], Dayan [2] and by Sutton and Pinette [21]. This approach enables models of the environment at different temporal scales to be intermixed, producing temporally abstract models. In previous work <ref> [14] </ref>, we generalized this approach from the prediction case, to the full control case. In this paper, we summarize the framework of temporally abstract reinforcement learning and present new theoretical results on planning with general options and temporally abstract models of options. <p> These generalizations are required when the environment is stochastic and uncertain with general goals, as in reinforcement learning and Markov decision processes (MDP). The predictive knowledge needed in order to plan using options can be represented through multi-time models <ref> [14] </ref>. Such models summarize several time scales and have the ability to predict events that can happen at various unknown moments. In this paper, we focus on the theoretical properties of multi-time models. <p> The multi-time model of a option characterizes the states that result upon the option's completion and the truncated return received along the way when the option is executed in various states <ref> [14] </ref>. Let p be an n-vector and r a scalar.
Reference: 15. <author> Martin L. Puterman. </author> <title> Markov Decision Processes. </title> <publisher> Wiley-Interscience, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: The options map the low-level MDP in a higher-level semi-Markov decision process (SMDP) <ref> [15] </ref>, which we can solve using dynamic programming methods.
Reference: 16. <author> Earl D. Sacerdoti. </author> <title> A Structure for Plans and Behavior. </title> <publisher> Elsevier, North-Holland, </publisher> <address> NY, </address> <year> 1977. </year>
Reference-contexts: In this paper, we summarize the framework of temporally abstract reinforcement learning and present new theoretical results on planning with general options and temporally abstract models of options. Options are similar to AI's classical macro operators <ref> [7, 8, 16] </ref>, in that they can take control for some period of time, determining the actions during that time, and in that one can choose among options much as one originally chose among primitive actions.
Reference: 17. <author> Satinder P. Singh. </author> <title> Scaling reinforcement learning by learning variable temporal resolution models. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning ICML'92, pages 202207, </booktitle> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, conventional model-based reinforcement learning uses one-step models [11, 13, 18], that cannot represent common-sense, higher-level actions, such as picking an object or traveling to a specified location. Several researchers have proposed extending reinforcement learning to a higher level by treating entire closed-loop policies as actions <ref> [36, 9, 10, 12, 17] </ref>. In order to use such actions in planning, an agent needs the ability to create and handle models at a variety of different, interrelated levels of temporal abstraction. <p> In order to use such actions in planning, an agent needs the ability to create and handle models at a variety of different, interrelated levels of temporal abstraction. Sutton [19] introduced an approach to modeling at different time scales, based on prior work by Singh <ref> [17] </ref>, Dayan [2] and by Sutton and Pinette [21]. This approach enables models of the environment at different temporal scales to be intermixed, producing temporally abstract models. In previous work [14], we generalized this approach from the prediction case, to the full control case.
Reference: 18. <author> Richard S. Sutton. </author> <title> Integrating architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning ICML'90, </booktitle> <pages> pages 216 224, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1 Introduction Model-based reinforcement learning offers a possible solution to the problem of integrating planning with real-time learning and decision-making [20]. However, conventional model-based reinforcement learning uses one-step models <ref> [11, 13, 18] </ref>, that cannot represent common-sense, higher-level actions, such as picking an object or traveling to a specified location. Several researchers have proposed extending reinforcement learning to a higher level by treating entire closed-loop policies as actions [36, 9, 10, 12, 17].
Reference: 19. <author> Richard S. Sutton. </author> <title> TD models: Modeling the world as a mixture of time scales. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning ICML'95, </booktitle> <pages> pages 531539, </pages> <address> San Mateo, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In order to use such actions in planning, an agent needs the ability to create and handle models at a variety of different, interrelated levels of temporal abstraction. Sutton <ref> [19] </ref> introduced an approach to modeling at different time scales, based on prior work by Singh [17], Dayan [2] and by Sutton and Pinette [21]. This approach enables models of the environment at different temporal scales to be intermixed, producing temporally abstract models. <p> These locations have been chosen randomly inside the environment. Accurate models for all the options are also available. Both the options and their models have been learned during a prior random walk in the environment, using Q-learning [22] and the fi-model learning algorithm <ref> [19] </ref>. The agent is repeatedly given new goal positions and it needs to compute optimal paths to these positions as quickly as possible. In this experiment, we considered all possible goal positions.
Reference: 20. <author> Richard S. Sutton and Andrew G. Barto. </author> <title> Reinforcement Learning. An Introduction. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1998. </year>
Reference-contexts: 1 Introduction Model-based reinforcement learning offers a possible solution to the problem of integrating planning with real-time learning and decision-making <ref> [20] </ref>. However, conventional model-based reinforcement learning uses one-step models [11, 13, 18], that cannot represent common-sense, higher-level actions, such as picking an object or traveling to a specified location.
Reference: 21. <author> Richard S. Sutton and Brian Pinette. </author> <title> The learning of world models by connectionist networks. </title> <booktitle> In Proceedings of the Seventh Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 5464, </pages> <year> 1985. </year>
Reference-contexts: Sutton [19] introduced an approach to modeling at different time scales, based on prior work by Singh [17], Dayan [2] and by Sutton and Pinette <ref> [21] </ref>. This approach enables models of the environment at different temporal scales to be intermixed, producing temporally abstract models. In previous work [14], we generalized this approach from the prediction case, to the full control case.
Reference: 22. <author> Christopher J. C. H. Watkins. </author> <title> Learning with Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University, </institution> <year> 1989. </year>
Reference-contexts: These locations have been chosen randomly inside the environment. Accurate models for all the options are also available. Both the options and their models have been learned during a prior random walk in the environment, using Q-learning <ref> [22] </ref> and the fi-model learning algorithm [19]. The agent is repeatedly given new goal positions and it needs to compute optimal paths to these positions as quickly as possible. In this experiment, we considered all possible goal positions.
References-found: 22

