URL: ftp://crl.ucsd.edu/pub/neuralnets/wiles_elman.ps.Z
Refering-URL: http://crl.ucsd.edu/~elman/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: janetw@cs.uq.oz.au  elman@cogsci.ucsd.edu  
Title: Learning to count without a counter: A case study of dynamics and activation landscapes in
Author: Janet Wiles Jeff Elman 
Note: =1 to 12. After training, it generalized to ing of the  
Address: 4072, Australia  La Jolla, California 92093-0515  
Affiliation: Departments of Computer Science and Psychology University of Queensland Queensland  Department of Cognitive Science, 0515 University of California, San Diego  
Abstract: The broad context of this study is the investigation of the nature of computation in recurrent networks (RNs). The current study has two parts. The first is to show that a RN can solve a problem that we take to be of interest (a counting task), and the second is to use the solution as a platform for developing a more general understanding of RNs as computational mechanisms. We begin by presenting the empirical results of training RNs on the counting task. The task ( ) is the simplest possible grammar that requires a PDA or counter. A RN was trained to predict the deterministic elements in sequences of the form * where =18. Contrary to our expectations, on analyzing the hidden unit dynamics, we find no evidence of units acting like counters. Instead, we find an oscillator. We then explore the possible range of behaviors of oscillators using iterated maps and in the second part of the paper we describe the use of iterated maps for understanding RN mechanisms in terms of activation landscapes. This analysis leads to used an understand 
Abstract-found: 1
Intro-found: 1
Reference: <author> Cleeremans, A., Servan-Schreiber, D., & McClelland, J. </author> <year> (1989). </year> <title> Finite state automata and simple recurrent networks. Neural Computation Elman, </title> <address> J.L. </address> <year> (1991). </year> <title> Distributed representations, simple recurrent networks, and grammatical structure. </title> <journal> Machine Learning, </journal> <note> Giles, </note> <author> C.L., Miller, C.B., Chen, D., Chen, H.H., Sun, G.Z., & Lee, Y.C. </author> <year> (1992). </year> <title> Learning and extracting finite state automata with second-order recurrent networks. Neural Computation, </title> <type> Kolen, </type> <institution> J.F. </institution> <year> (1994). </year> <title> Recurrent networks: state machines or iterated function systems? In M. </title> <editor> Mozer, P. S. Smolensky, D. Touretzky, J. Elman, & A. Weigend (Eds.), </editor> <booktitle> Proceedings of the 1993 Connectionist Models Summer School 210). </booktitle> <address> Boulder, </address> <publisher> CO: Lawrence Erlbaum Assoc. </publisher>
Reference: <author> Manolios, P. & Fanelli, R. </author> <year> (1994). </year> <title> First order recurrent neural networks and deterministic finite state automata. Neural Computation Pollack, </title> <address> J.B. </address> <year> (1991). </year> <title> The induction of dynamical recognizers. Machine Learning Seligmann, </title> <editor> H.T., & Sontag, E.D. </editor> <year> (1992). </year> <title> Neural networks with real weights: Analog computational complexity. </title> <type> Report SYCON-92-05. </type> <institution> Rutgers Center for Systems and Control, Rutgers University. </institution>
Reference: <author> Watrous, R.J., & Kuhn, G.M. </author> <year> (1992). </year> <title> Induction of finite-state languages using second-order recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 4, </volume> <pages> 406-414. </pages>
References-found: 3

