URL: ftp://ftp.cogsci.indiana.edu/pub/wang.inheritance_nal.ps
Refering-URL: http://www.cogsci.indiana.edu/farg/peiwang/papers.html
Root-URL: 
Title: From Inheritance Relation to Non-Axiomatic Logic  
Author: Pei Wang 
Keyword: Insufficient knowledge and resources, non-axiomatic logic and reasoning system, term logic, experience-grounded semantics, measurements of uncertainty, revision, deduction, abduction, induction.  
Affiliation: Indiana University  
Note: Center for Research on Concepts and Cognition  
Abstract: At the beginning of the paper, three binary term logics are defined. The first is based only on an inheritance relation. The second and the third suggest a novel way to process extension and intension, and they also have interesting relations with Aristotle's syllogistic logic. Based on the three simple systems, a Non-Axiomatic Logic is defined. It has a term-oriented language and an experience-grounded semantics. It can uniformly represents and processes randomness, fuzziness, and ignorance. It can also uniformly carries out deduction, abduction, induction, and revision. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Aristotle. </author> <title> Prior Analytics. </title> <publisher> Hackett Publishing Company, </publisher> <address> Indianapolis, Indiana, </address> <year> 1989. </year> <note> Translated by R. Smith. </note>
Reference-contexts: Since (E S E P = ;) () (E S E P ), we have (S &lt; a P ) () (S &lt; P ). Though described differently, ETL turns out to be isomorphic with Aris-totle's syllogistic logic. For each property of Aristotle's logic <ref> [1, 24, 28] </ref>, there is a corresponding one in ETL, and vice versa. <p> According to the practice of statistics, the most natural way to calculate the weight of evidence is to simply count such terms. In this way, weight of evidence take its values in <ref> [0; 1] </ref>, and is additive when combining two pieces of evidence from distinct sources [32, 41]. If the experience of the system is represented by a string of propositions in IL, then, by using the string as premises set K, we can determine extension and intension for each term. <p> Instead of using "absolute measurements", we often prefer "relative measurements" of uncertainty, such as real numbers in <ref> [0; 1] </ref>. Fortunately, it is easy to define them as functions of weight of evidence. Definition 3.8. <p> For our current purpose, k can be any positive number. Though c is in <ref> [0; 1] </ref>, can be explained as a ratio, and is at a higher level than f in the sense that it indicates the stability of f, it cannot 16 be interpreted as a second-order probability in the sense that it is the probability of the judgment "the (real, or objective) probability <p> Now we have three functionally identical ways to represent a truth value: 1. as a pair of weights fw + , wg, where w w + 0; 2. as a pair of ratios &lt; f; c &gt;, where f 2 <ref> [0; 1] </ref>, and c 2 [0; 1]; or 3. as an interval [l; u], where 0 l u 1. Because NARS is designed under the assumption of insufficient knowledge and resources, all the judgments within the system are supported by finite evidence, that is, w is positive and finite. <p> Now we have three functionally identical ways to represent a truth value: 1. as a pair of weights fw + , wg, where w w + 0; 2. as a pair of ratios &lt; f; c &gt;, where f 2 <ref> [0; 1] </ref>, and c 2 [0; 1]; or 3. as an interval [l; u], where 0 l u 1. Because NARS is designed under the assumption of insufficient knowledge and resources, all the judgments within the system are supported by finite evidence, that is, w is positive and finite. <p> From "M P &lt; f 1 ; c 1 &gt;" and "S M &lt; f 2 ; c 2 &gt;" to get "S P &lt; f; c &gt;". This is Aristotle's first figure, and what Peirce called deduction <ref> [1, 30] </ref>. Let us refer to the function that calculate f and c from f 1 , c 1 , f 2 , and c 2 as F 1 . 2. <p> From "P M &lt; f 1 ; c 1 &gt;" and "S M &lt; f 2 ; c 2 &gt;" to get "S P &lt; f; c &gt;". This is Aristotle's second figure, and what Peirce called abduction (or hypothesis) <ref> [1, 30] </ref>. Let us refer to the function that calculate f and c from f 1 , c 1 , f 2 , and c 2 as F 2 . 3. <p> From "M P &lt; f 1 ; c 1 &gt;" and "M S &lt; f 2 ; c 2 &gt;" to get "S P &lt; f; c &gt;". This is Aristotle's third figure, and what Peirce called 24 induction <ref> [1, 30] </ref>. Let us refer to the function that calculate f and c from f 1 , c 1 , f 2 , and c 2 as F 3 . 4. <p> The functions in Table 4 can be built by considering the relations among the involved truth values in terms of Triangular norm (T-norm) and Triangular conorm (T-conorm). T-norm and T-conorm are function from <ref> [0; 1] </ref> fi [0; 1] to [0; 1] that are monotonic, commutative, associative, and with boundary conditions satisfying the truth tables of the logical operators AN D and OR, respectively [5, 6, 11]. They also can be extended to take more than two arguments. <p> The functions in Table 4 can be built by considering the relations among the involved truth values in terms of Triangular norm (T-norm) and Triangular conorm (T-conorm). T-norm and T-conorm are function from <ref> [0; 1] </ref> fi [0; 1] to [0; 1] that are monotonic, commutative, associative, and with boundary conditions satisfying the truth tables of the logical operators AN D and OR, respectively [5, 6, 11]. They also can be extended to take more than two arguments. <p> The functions in Table 4 can be built by considering the relations among the involved truth values in terms of Triangular norm (T-norm) and Triangular conorm (T-conorm). T-norm and T-conorm are function from <ref> [0; 1] </ref> fi [0; 1] to [0; 1] that are monotonic, commutative, associative, and with boundary conditions satisfying the truth tables of the logical operators AN D and OR, respectively [5, 6, 11]. They also can be extended to take more than two arguments.
Reference: 2. <author> S. Bai. </author> <title> A mathematical theory for evidence combination. </title> <type> Unpublished manuscript, </type> <year> 1991. </year>
Reference-contexts: We also collect all the functions, in all the three forms of truth value, in Table 5. It is possible to find direct intuitive justifications for a function in a form that is different from the previously discussed ones (for example, Bai Shuo in <ref> [2] </ref> also reached the revision rule in the ratio form from a different starting point), but such justifications are not always obvious. 5. An Example Up to now, we have completely defined a non-axiomatic logic, NAL1, with its grammar, semantics, and inference rules.
Reference: 3. <author> L. Birnbaum. Rigor mortis: </author> <title> a response to Nillsson's "logic and artificial intelligence". </title> <journal> Artificial Intelligence, </journal> <volume> 47 </volume> <pages> 57-77, </pages> <year> 1991. </year>
Reference-contexts: By naming it a "Non-Axiomatic Logic", I am trying to show that, from the viewpoint of artificial intelligence, the problems of the traditional "symbolic AI" <ref> [3, 25, 34] </ref> are not caused by the ideas like "formalization", "symbolization", "logical inferences", and so on, but by the ideas like "axiomatization", "computation", "binary logics", "consistent and complete system", and other concepts that explicitly or implicitly assume the sufficiency of knowledge and resources.
Reference: 4. <author> I. Bochenski. </author> <title> A History of Formal Logic. </title> <publisher> Chelsea Publishing Company, </publisher> <address> New York, </address> <year> 1970. </year> <title> Translated and edited by I. </title> <institution> Thomas. </institution>
Reference-contexts: THREE SIMPLE SYSTEMS 2.1. Inheritance Logic The four logics discussed in this paper are all term logics, which are different from predicate logics by having the following features: <ref> [4, 13] </ref> 1. Each proposition consists of a subject term and a predicate term, which are related by a copula; 2. The copula is intuitively interpreted as "to be"; 3. <p> Extension is usually defined as an object, or a set of objects, which is in a "physical world", and denoted by the term; intension is usually defined as a concept, which is in a "Platonic world", and denoting the term <ref> [4, 19] </ref>. In spite of the differences among the exact ways the two words are used by different authors, they indicate relations between a term in a language and something outside the language. <p> For each property of Aristotle's logic [1, 24, 28], there is a corresponding one in ETL, and vice versa. The square of opposition: The relations among the four types of extensional inheritance can be represented in Figure 1 <ref> [4] </ref>, where there are four types of relations: 3 Here "inheritance" is used for a logical relation between two terms, rather than an idea about the implementation of a knowledge base, by which storage space can be saved [7, 35]. 7 1. <p> Here, the quantifiers are applied to the properties (intension) of the predicate, rather than to the instances (extension) of the subject (as Aristotle did) or predicate (as Bentham and Hamilton did, see <ref> [4] </ref>). The propositions represented in this way are closely related to "typicalness" [31], "representativeness" [36], "normality" [20], and "fuzziness" [45]. All these concepts are proposed, from different standing points, to capture the phenomenon that an instance does (or doesn't) possess all (or some) properties of a category. <p> From "M P &lt; f 1 ; c 1 &gt;" and "S M &lt; f 2 ; c 2 &gt;" to get "P S &lt; f; c &gt;". This rule, not discussed by Aristotle and Peirce, was called the fourth figure by Aristotle's successors <ref> [4] </ref>. Let us refer to the function that calculate f and c from f 1 , c 1 , f 2 , and c 2 as F 4 . <p> In term logics, "conversion" is a inference from a single premiss to a conclusion by interchanging the subject term and the predicate term <ref> [4] </ref>. Now we can see it as a special case of abduction by taking "P S &lt; f 0 ; c 0 &gt;" and "S S &lt; 1; 1 &gt;" (a tautology) as premises.
Reference: 5. <author> P. Bonissone. </author> <title> Summarizing and propagating uncertain information with triangular norms. </title> <journal> International Journal of Approximate Reasoning, </journal> <volume> 1 </volume> <pages> 71-101, </pages> <year> 1987. </year>
Reference-contexts: Address correspondence to 510 North Fess, Bloomington, IN47408 pwang@cogsci.indiana.edu International Journal of Approximate Reasoning 1994 7:1-74 c fl 1994 Elsevier Science Inc. 655 Avenue of the Americas, New York, NY 10010 0888-613X/94/$7.00 1 2 A reasoning system, in its general form, has the following components <ref> [5, 34] </ref>: 1. A domain-independent formal language by which the system can communicate with its environment, that is, to get knowledge and ques tions, and to provide answers according to its knowledge; 2. <p> This measurement shares similar intuition with other interval approaches <ref> [5, 22, 43] </ref>. For example, "ignorance", i, can be represented by the width of the interval (here it happens to be 1 c, so ignorance and confidence are complement to each other). <p> T-norm and T-conorm are function from [0; 1] fi [0; 1] to [0; 1] that are monotonic, commutative, associative, and with boundary conditions satisfying the truth tables of the logical operators AN D and OR, respectively <ref> [5, 6, 11] </ref>. They also can be extended to take more than two arguments. The usage of T-norm and T-conorm in NAL1 is different from their usual usage [6, 11] in which they are used to determine the degree of certainty of the conjunction and disjunction of two propositions, respectively.
Reference: 6. <author> P. Bonissone and K. Decker. </author> <title> Selecting uncertain calculi and granularity. </title> <editor> In L. Kanal and J. Lemmer, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 217-247. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1986. </year>
Reference-contexts: T-norm and T-conorm are function from [0; 1] fi [0; 1] to [0; 1] that are monotonic, commutative, associative, and with boundary conditions satisfying the truth tables of the logical operators AN D and OR, respectively <ref> [5, 6, 11] </ref>. They also can be extended to take more than two arguments. The usage of T-norm and T-conorm in NAL1 is different from their usual usage [6, 11] in which they are used to determine the degree of certainty of the conjunction and disjunction of two propositions, respectively. <p> They also can be extended to take more than two arguments. The usage of T-norm and T-conorm in NAL1 is different from their usual usage <ref> [6, 11] </ref> in which they are used to determine the degree of certainty of the conjunction and disjunction of two propositions, respectively. <p> among f 1 , c 1 , f 2 , and c 2 , it is natural For NAL1 to use the "probabilistic" 8 In NAL1, the conjunction or disjunction of two judgments is not defined as a judgment. 25 operators (see the comparison of different T-norms and T-conorms in <ref> [6] </ref>): T (a; b) = ab; S (a; b) = a + b ab: T-norm and T-conorm with more than two arguments are defined as: T (x 1 ; : : : ; x n ) = T (T (x 1 ; : : : ; x n1 ); x n
Reference: 7. <author> R. Brachman. </author> <title> What is-a is and isn't: an analysis of taxonomic links in semantic networks. </title> <journal> IEEE Computer, </journal> <volume> 16 </volume> <pages> 30-36, </pages> <year> 1983. </year>
Reference-contexts: meaning of the inheritance relation is closely related to many well-known relations, for instance, "ISA" (in semantic network), "belongs to" (in Aristotle's syllogism), "subset" (in set theory), "inheritance assertion" (in inheritance network [35]), as well as many relations studied in psychology and philosophy, such as "type-token", "category-instance", "general-specific", and "superordinate-subordinate" <ref> [7] </ref>. What make it different from the others are: it is a relation between two terms, and the 4 relation is completely defined by the two properties: reflexivity and transitivity. 1 This logic (as well as the following two) can be interpreted in the usual model-theoretic way. <p> the four types of extensional inheritance can be represented in Figure 1 [4], where there are four types of relations: 3 Here "inheritance" is used for a logical relation between two terms, rather than an idea about the implementation of a knowledge base, by which storage space can be saved <ref> [7, 35] </ref>. 7 1.
Reference: 8. <author> R. Carnap. </author> <title> Logical Foundations of Probability. </title> <publisher> The University of Chicago Press, </publisher> <address> Chicago, </address> <year> 1950. </year> <month> 33 </month>
Reference-contexts: These problems are hard for predicate logics (as revealed by the famous "Raven Paradox" of Hempel <ref> [8] </ref>), but in a term logic like NAL1, we can find a natural solution of them. In summary, the truth value of a judgment in NAL1 is a numerical representation indicating the weights of positive and negative evidence, according (part of) the experience of the system.
Reference: 9. <author> R. Carnap. </author> <title> The Continuum of Inductive Methods. </title> <publisher> The University of Chicago Press, </publisher> <address> Chicago, </address> <year> 1952. </year>
Reference-contexts: This formula turns out to be closely related to Hardy's beta-form based continuum (with equally weighted positive evidence and negative evidence) [16], and Carnap's "- continuum" (with the logical factor, or the prior probability, to be 1/2) <ref> [9] </ref>. Though interpreted differently, the three continua share the same formula, and make identical predictions. <p> This parameter was called "flattening constant" by Good (see [16], where he also tried to estimate its value), and interpreted by him as a way to choose a prior probability distribution. The same parameter is interpreted by Carnap as "the relative weight" of the logical factor <ref> [9] </ref>. One reasonable alternative of the choice rule is to choose the conclusion probabilistically. The judgment with a higher confidence (for evaluation) or a higher expectation (for selection) is not always chosen as the answer, but is given a higher probability to be chosen.
Reference: 10. <author> B. Carpenter and T. Richmond. </author> <title> Inheritance theory and path based reasoning: an introduction. </title> <editor> In H. Kyburg, R. Loui, and H. Carlson, editors, </editor> <booktitle> Knowledge Representation and Defeasible Reasoning, </booktitle> <pages> pages 309-343. </pages> <publisher> Kluwer Academic Publishers, </publisher> <address> Dordrecht, Netherlands, </address> <year> 1990. </year>
Reference-contexts: If only the relations "S &lt; a P " and "S &lt; e P " are represented and processed, ETL will degenerate into a special case, which is identical with the 8 "Monotonic Inheritance Network" defined in <ref> [10] </ref>. 2.4. Intensional Term Logic Since extension and intension are defined as a "dual" in IL, we get an Intensional Term Logic (ITL) "for free", which is isomorphic with ETL. Definition 2.6.
Reference: 11. <author> D. Dubois and H. Prade. </author> <title> A class of fuzzy measures based on triangular norms. </title> <journal> International Journal of General Systems, </journal> <volume> 8 </volume> <pages> 43-61, </pages> <year> 1982. </year>
Reference-contexts: T-norm and T-conorm are function from [0; 1] fi [0; 1] to [0; 1] that are monotonic, commutative, associative, and with boundary conditions satisfying the truth tables of the logical operators AN D and OR, respectively <ref> [5, 6, 11] </ref>. They also can be extended to take more than two arguments. The usage of T-norm and T-conorm in NAL1 is different from their usual usage [6, 11] in which they are used to determine the degree of certainty of the conjunction and disjunction of two propositions, respectively. <p> They also can be extended to take more than two arguments. The usage of T-norm and T-conorm in NAL1 is different from their usual usage <ref> [6, 11] </ref> in which they are used to determine the degree of certainty of the conjunction and disjunction of two propositions, respectively.
Reference: 12. <author> H. Einhorn and R. Hogarth. </author> <title> Confidence in judgment: persistence of illusion of validity. </title> <journal> Psychological Review, </journal> <volume> 35 </volume> <pages> 395-416, </pages> <year> 1978. </year>
Reference-contexts: The higher the confidence, the harder the frequency can be changed by new evidence, but this does not mean that the judgment is "truer", or the more "accurate", as some psychologists means by the concept "confidence" <ref> [12] </ref>. It is easy to calculate w and w + from f and c, therefore the truth value of a judgment can also be represented as a pair of ratio &lt; f; c &gt; [38].
Reference: 13. <editor> G. Englebretsen. Three Logicians. Van Gorcum, Assen, </editor> <address> The Netherlands, </address> <year> 1981. </year>
Reference-contexts: THREE SIMPLE SYSTEMS 2.1. Inheritance Logic The four logics discussed in this paper are all term logics, which are different from predicate logics by having the following features: <ref> [4, 13] </ref> 1. Each proposition consists of a subject term and a predicate term, which are related by a copula; 2. The copula is intuitively interpreted as "to be"; 3.
Reference: 14. <author> R. Fung and C. Chong. </author> <title> Metaprobability and Dempster-Shafer in evidential reasoning. </title> <editor> In L. Kanal and J. Lemmer, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 295-302. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1986. </year>
Reference-contexts: The frequency defined above can be referred to as an estimation of "the first-order probability", and the second-order probability is used to represent how good the estimation is. Actually, several approaches are working along this line <ref> [14, 15, 27] </ref>. However, there are problems in how to interpret the second value, and how it helps in the related operations [23, 29].
Reference: 15. <author> H. Gaifman. </author> <title> A theory of higher order probabilities. </title> <editor> In J. Halpern, editor, </editor> <booktitle> Theoretical Aspects of Reasoning about Knowledge, </booktitle> <pages> pages 275-292. </pages> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, California, </address> <year> 1986. </year>
Reference-contexts: The frequency defined above can be referred to as an estimation of "the first-order probability", and the second-order probability is used to represent how good the estimation is. Actually, several approaches are working along this line <ref> [14, 15, 27] </ref>. However, there are problems in how to interpret the second value, and how it helps in the related operations [23, 29].
Reference: 16. <author> I. </author> <title> Good. The Estimation of Probabilities. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1965. </year>
Reference-contexts: To calculate e from &lt; f; c &gt;, we can see that under the assumption that the system make predictions according to its (past) experience, it is natural to use f as e's "first-order approximation". However, such a maximum-likelihood estimate is not good enough when c is small <ref> [16] </ref>. For example, if a hypothesis has been tested only once, nobody will take an expectation as 1 (if the test is a success) or 0 (if the test leads is failure). <p> Therefore, it is natural to define e = c (f 0:5) + 0:5: Identically, it can be written as c = (e 0:5)=(f 0:5) (when f 6= 0:5), so it says that c indicates the ratio that f is (to use Good's term in <ref> [16] </ref>) "squashed" to the "no preference point" to become e. When c = 1 (total evidence), e = f ; when c = 0 (null evidence), e = 0:5. To express the definition of e in the other two forms of truth value leads to interesting results. <p> When the truth value is represented as weights of evidence, from the mappings we get e = 2 which is a continuum with k as a parameter. This formula turns out to be closely related to Hardy's beta-form based continuum (with equally weighted positive evidence and negative evidence) <ref> [16] </ref>, and Carnap's "- continuum" (with the logical factor, or the prior probability, to be 1/2) [9]. Though interpreted differently, the three continua share the same formula, and make identical predictions. <p> The larger k is, the more "conservative" the system is, in the sense that given the same amount of evidence, it always make less change in e, compared with a system with a smaller k. This parameter was called "flattening constant" by Good (see <ref> [16] </ref>, where he also tried to estimate its value), and interpreted by him as a way to choose a prior probability distribution. The same parameter is interpreted by Carnap as "the relative weight" of the logical factor [9].
Reference: 17. <author> D. Hofstadter. </author> <title> From Euler to Ulam: discovery and dissection of a geometric gem. </title> <type> Technical Report 81, </type> <institution> Center for Research on Concepts and Cognition, Indiana University, Bloomington, Indiana, </institution> <year> 1993. </year>
Reference-contexts: Hofstadter discussed this opinion in <ref> [17] </ref>, and generalized it into "Ulam's thesis": "AS is the key to AI". NAL1 can be seen as a primary step in this direction.
Reference: 18. <author> D. Hofstadter. </author> <title> How could a copycat ever be creative? In Working Notes, </title> <booktitle> 1993 AAAI Spring Symposium Series, Symposium: AI and Creativity, </booktitle> <pages> pages 1-10, </pages> <year> 1993. </year>
Reference-contexts: The judgment with a higher confidence (for evaluation) or a higher expectation (for selection) is not always chosen as the answer, but is given a higher probability to be chosen. In this way, the decisions are more variable and indeterministic, so have some advantages in certain circumstances <ref> [18] </ref>. 4.4. Syllogisms The major inference rules in NAL1 are the (extended) syllogisms. When two judgments share a common term, they can be used as premises to infer the inheritance relations between the other two (unshared) terms.
Reference: 19. <author> B. Inhelder and J. Piaget. </author> <title> The Early Growth of Logic in the Child. </title> <editor> W. W. </editor> <publisher> Norton & Company, Inc., </publisher> <address> New York, </address> <year> 1969. </year> <title> Translated by E. </title> <editor> Lunzer and D. </editor> <publisher> Papert. </publisher>
Reference-contexts: Extension is usually defined as an object, or a set of objects, which is in a "physical world", and denoted by the term; intension is usually defined as a concept, which is in a "Platonic world", and denoting the term <ref> [4, 19] </ref>. In spite of the differences among the exact ways the two words are used by different authors, they indicate relations between a term in a language and something outside the language.
Reference: 20. <author> D. Kahneman and D. Miller. </author> <title> Norm theory: comparing reality to its alternatives. </title> <journal> Psychological Review, </journal> <volume> 93 </volume> <pages> 136-153, </pages> <year> 1986. </year>
Reference-contexts: Here, the quantifiers are applied to the properties (intension) of the predicate, rather than to the instances (extension) of the subject (as Aristotle did) or predicate (as Bentham and Hamilton did, see [4]). The propositions represented in this way are closely related to "typicalness" [31], "representativeness" [36], "normality" <ref> [20] </ref>, and "fuzziness" [45]. All these concepts are proposed, from different standing points, to capture the phenomenon that an instance does (or doesn't) possess all (or some) properties of a category. The related problems cannot be properly represented and processed by any extensional logic.
Reference: 21. <author> H. Kyburg. </author> <title> The Logical Foundations of Statistical Inference. </title> <address> D. </address> <publisher> Reidel Publishing Company, </publisher> <address> Boston, </address> <year> 1974. </year>
Reference-contexts: Obviously, this measurement is closely related to probability and statistics, and often appears is our everyday life. However, it is still different from probability under the traditional interpretations (logical, frequen-tist, and subjective, see <ref> [21] </ref>) because it is determined by finite empirical evidence. Another basic difference between probability and frequency is: probability is traditionally interpreted as about extensions of sets. For example, if we say "the probability of `S P ' is p", it is usually understood as jS"P j jSj = p. <p> Intuitively, similar to subjective probability <ref> [21] </ref>, e can be interpreted as the estimation about a future "inheritance frequency", or a bet the system will accept about a future "inheritance test". Under the assumption of insufficient knowledge, in NAL1 e only takes values in (0; 1), with 0 and 1 22 as limits.
Reference: 22. <author> H. Kyburg. </author> <title> Bayesian and non-Bayesian evidential updating. </title> <journal> Artificial Intelligence, </journal> <volume> 31 </volume> <pages> 271-293, </pages> <year> 1987. </year>
Reference-contexts: This measurement shares similar intuition with other interval approaches <ref> [5, 22, 43] </ref>. For example, "ignorance", i, can be represented by the width of the interval (here it happens to be 1 c, so ignorance and confidence are complement to each other).
Reference: 23. <author> H. Kyburg. </author> <title> Higher order probabilities. </title> <editor> In L. Kanal, T. Levitt, and J. Lem-mer, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence 3, </booktitle> <pages> pages 15-22. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1989. </year>
Reference-contexts: Actually, several approaches are working along this line [14, 15, 27]. However, there are problems in how to interpret the second value, and how it helps in the related operations <ref> [23, 29] </ref>. At least under the assumption of insufficient knowledge, it make little sense to talk about the "probability" that "the frequency is an accurate estimation of an `objective first-order probability' of the inheritance relation".
Reference: 24. <author> J. </author> <title> Lukasiewicz. Aristotle's Syllogistic: From the Standpoint of Modern Formal Logic. </title> <publisher> Oxford University Press, </publisher> <address> London, </address> <year> 1951. </year>
Reference-contexts: Therefore, the subject of a proposition cannot be a "singular term", such as "Tweety" or "Socrates". On the other hand, as in Aristo-tle's logic, "the same term may be used as a subject and as a predicate without any restriction" <ref> [24] </ref>. 5 questions are search problems either for the existence of a path from a given node to another given node (evaluation) or for a node in a path from (or to) a given node (selection). Up to now, we have got a complete reasoning system. <p> Since (E S E P = ;) () (E S E P ), we have (S &lt; a P ) () (S &lt; P ). Though described differently, ETL turns out to be isomorphic with Aris-totle's syllogistic logic. For each property of Aristotle's logic <ref> [1, 24, 28] </ref>, there is a corresponding one in ETL, and vice versa.
Reference: 25. <author> D. McDermott. </author> <title> A critique of pure reason. </title> <journal> Computational Intelligence, </journal> <volume> 3 </volume> <pages> 151-160, </pages> <year> 1987. </year>
Reference-contexts: 1 , c 1 , f 2 , c 2 to f and c, we have F 2 : f = f 2 f 1 c 1 c 2 +k c = f 2 c 1 c 2 +k Defined as above, abduction and induction are no longer "inversed deductions" <ref> [25, 30] </ref>, and the difference between them and deduction is still there: deductive conclusions are usually much more confident (with 1 as upper bound) than abductive and inductive conclusions (with 1 1+k as upper bound). 9 Using F 2 or F 3 , we can define NAL1's conversion rule. <p> By naming it a "Non-Axiomatic Logic", I am trying to show that, from the viewpoint of artificial intelligence, the problems of the traditional "symbolic AI" <ref> [3, 25, 34] </ref> are not caused by the ideas like "formalization", "symbolization", "logical inferences", and so on, but by the ideas like "axiomatization", "computation", "binary logics", "consistent and complete system", and other concepts that explicitly or implicitly assume the sufficiency of knowledge and resources.
Reference: 26. <author> M. Minsky. </author> <title> The Society of Mind. </title> <publisher> Simon and Schuster, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: To summarize information about evidence into truth values causes information loss, but it is absolutely necessary for the system, because qualitatively different evidence need to be treated in a unified manner <ref> [26] </ref>. This will lead to what I call "experience-grounded semantics", where the truth value of a judgment indicates the degree to which the judgment is supported by the system's experience. Defined in this way, truth value is system-dependent and time-dependent. <p> This "network interpretation" of NAL1 reminds us Minsky's comment <ref> [26] </ref>: For the purposes of psychology, we'd better to set aside the dubious ideal of faultless deduction and try, instead, to understand how people actually deal with what is usual or typical. To do this, we often think in terms of causes, similarities, and dependencies.
Reference: 27. <author> G. Paa. </author> <title> Second order probabilities for uncertain and conflicting evidence. </title> <editor> In P. Bonissone, M. Henrion, L. Kanal, and J. Lemmer, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence 6, </booktitle> <pages> pages 447-456. </pages> <publisher> North-Holland, </publisher> <address> Amster-dam, </address> <year> 1991. </year> <month> 34 </month>
Reference-contexts: The frequency defined above can be referred to as an estimation of "the first-order probability", and the second-order probability is used to represent how good the estimation is. Actually, several approaches are working along this line <ref> [14, 15, 27] </ref>. However, there are problems in how to interpret the second value, and how it helps in the related operations [23, 29].
Reference: 28. <author> G. Patzig. </author> <booktitle> Aristotle's Theory of the Syllogism. </booktitle> <address> D. </address> <publisher> Reidel Publishing Company, Dordrecht, Holland, </publisher> <year> 1968. </year> <note> Translated by J. Barnes. </note>
Reference-contexts: Since (E S E P = ;) () (E S E P ), we have (S &lt; a P ) () (S &lt; P ). Though described differently, ETL turns out to be isomorphic with Aris-totle's syllogistic logic. For each property of Aristotle's logic <ref> [1, 24, 28] </ref>, there is a corresponding one in ETL, and vice versa.
Reference: 29. <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1988. </year>
Reference-contexts: Actually, several approaches are working along this line [14, 15, 27]. However, there are problems in how to interpret the second value, and how it helps in the related operations <ref> [23, 29] </ref>. At least under the assumption of insufficient knowledge, it make little sense to talk about the "probability" that "the frequency is an accurate estimation of an `objective first-order probability' of the inheritance relation". <p> A direct implication of the above consequence is that all the inference rules are "local rules", in the sense that each rule only takes a constant number of premises to get conclusions. In NAL1, all rules take one or two judgment (s) as premises. Several authors, for instance Pearl <ref> [29] </ref>, have correctly pointed out that such local rules can cause problems, for examples ignoring related information, repeated using of correlated evidence, and so on. With insufficient resources, ignoring related information is inevitable in each inference step. <p> Therefore the "correlated-evidence recognition problem" cannot be completely solved with insufficient resource. Actually, this is also true for human beings: we simply cannot exactly remember all evidence that supports each judgment we made. On the other hand, the problem must be handled somehow, otherwise, as Pearl said in <ref> [29] </ref>: in a bidirectional reasoning system, "A cycle would be created where any slight evidence in favor of A would be amplified via B and fed back to A, quickly turning into a stronger conformation (of A and B), with no apparent factual justification." What NAL1 does for the problem is
Reference: 30. <author> C. </author> <title> Peirce. </title> <booktitle> Collected papers of Charles Sanders Peirce, </booktitle> <volume> volume 2. </volume> <publisher> Harvard University Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1931. </year>
Reference-contexts: From "M P &lt; f 1 ; c 1 &gt;" and "S M &lt; f 2 ; c 2 &gt;" to get "S P &lt; f; c &gt;". This is Aristotle's first figure, and what Peirce called deduction <ref> [1, 30] </ref>. Let us refer to the function that calculate f and c from f 1 , c 1 , f 2 , and c 2 as F 1 . 2. <p> From "P M &lt; f 1 ; c 1 &gt;" and "S M &lt; f 2 ; c 2 &gt;" to get "S P &lt; f; c &gt;". This is Aristotle's second figure, and what Peirce called abduction (or hypothesis) <ref> [1, 30] </ref>. Let us refer to the function that calculate f and c from f 1 , c 1 , f 2 , and c 2 as F 2 . 3. <p> From "M P &lt; f 1 ; c 1 &gt;" and "M S &lt; f 2 ; c 2 &gt;" to get "S P &lt; f; c &gt;". This is Aristotle's third figure, and what Peirce called 24 induction <ref> [1, 30] </ref>. Let us refer to the function that calculate f and c from f 1 , c 1 , f 2 , and c 2 as F 3 . 4. <p> 1 , c 1 , f 2 , c 2 to f and c, we have F 2 : f = f 2 f 1 c 1 c 2 +k c = f 2 c 1 c 2 +k Defined as above, abduction and induction are no longer "inversed deductions" <ref> [25, 30] </ref>, and the difference between them and deduction is still there: deductive conclusions are usually much more confident (with 1 as upper bound) than abductive and inductive conclusions (with 1 1+k as upper bound). 9 Using F 2 or F 3 , we can define NAL1's conversion rule.
Reference: 31. <author> E. Rosch and C. Mervis. </author> <title> Family resemblances: studies in the internal structure of categories. </title> <journal> Cognitive Psychology, </journal> <volume> 7 </volume> <pages> 573-605, </pages> <year> 1975. </year>
Reference-contexts: Here, the quantifiers are applied to the properties (intension) of the predicate, rather than to the instances (extension) of the subject (as Aristotle did) or predicate (as Bentham and Hamilton did, see [4]). The propositions represented in this way are closely related to "typicalness" <ref> [31] </ref>, "representativeness" [36], "normality" [20], and "fuzziness" [45]. All these concepts are proposed, from different standing points, to capture the phenomenon that an instance does (or doesn't) possess all (or some) properties of a category. The related problems cannot be properly represented and processed by any extensional logic.
Reference: 32. <author> G. Shafer. </author> <title> A Mathematical Theory of Evidence. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1976. </year>
Reference-contexts: According to the practice of statistics, the most natural way to calculate the weight of evidence is to simply count such terms. In this way, weight of evidence take its values in [0; 1], and is additive when combining two pieces of evidence from distinct sources <ref> [32, 41] </ref>. If the experience of the system is represented by a string of propositions in IL, then, by using the string as premises set K, we can determine extension and intension for each term.
Reference: 33. <author> G. Shafer. </author> <title> Perspectives on the theory and practice of belief functions. </title> <journal> International Journal of Approximate Reasoning, </journal> <volume> 4 </volume> <pages> 323-362, </pages> <year> 1990. </year>
Reference-contexts: Intuitively, confidence is the ratio that the weight of "current relevant evidence" to the weight of "relevant evidence in the near future". It indicates how much the system knows about the inheritance relation, so is similar to Shafer's "reliability" <ref> [33] </ref> or Yager's "credibility" [44].
Reference: 34. <author> H. Simon and C. Kaplan. </author> <booktitle> Foundations of cognitive science. In Posner, editor, Foundations of Cognitive Science, </booktitle> <pages> pages 1-47. </pages> <publisher> The MIT Press, </publisher> <address> Cam-bridge, Massachusetts, </address> <year> 1989. </year>
Reference-contexts: Address correspondence to 510 North Fess, Bloomington, IN47408 pwang@cogsci.indiana.edu International Journal of Approximate Reasoning 1994 7:1-74 c fl 1994 Elsevier Science Inc. 655 Avenue of the Americas, New York, NY 10010 0888-613X/94/$7.00 1 2 A reasoning system, in its general form, has the following components <ref> [5, 34] </ref>: 1. A domain-independent formal language by which the system can communicate with its environment, that is, to get knowledge and ques tions, and to provide answers according to its knowledge; 2. <p> By naming it a "Non-Axiomatic Logic", I am trying to show that, from the viewpoint of artificial intelligence, the problems of the traditional "symbolic AI" <ref> [3, 25, 34] </ref> are not caused by the ideas like "formalization", "symbolization", "logical inferences", and so on, but by the ideas like "axiomatization", "computation", "binary logics", "consistent and complete system", and other concepts that explicitly or implicitly assume the sufficiency of knowledge and resources.
Reference: 35. <author> D. Touretzky. </author> <title> The Mathematics of Inheritance Systems. </title> <publisher> Pitman Publishing, </publisher> <address> London, </address> <year> 1986. </year>
Reference-contexts: The intuitive meaning of the inheritance relation is closely related to many well-known relations, for instance, "ISA" (in semantic network), "belongs to" (in Aristotle's syllogism), "subset" (in set theory), "inheritance assertion" (in inheritance network <ref> [35] </ref>), as well as many relations studied in psychology and philosophy, such as "type-token", "category-instance", "general-specific", and "superordinate-subordinate" [7]. <p> the four types of extensional inheritance can be represented in Figure 1 [4], where there are four types of relations: 3 Here "inheritance" is used for a logical relation between two terms, rather than an idea about the implementation of a knowledge base, by which storage space can be saved <ref> [7, 35] </ref>. 7 1.
Reference: 36. <author> A. Tversky and D. Kahneman. </author> <title> Judgment under uncertainty: heuristics and biases. </title> <journal> Science, </journal> <volume> 185 </volume> <pages> 1124-1131, </pages> <year> 1974. </year>
Reference-contexts: Here, the quantifiers are applied to the properties (intension) of the predicate, rather than to the instances (extension) of the subject (as Aristotle did) or predicate (as Bentham and Hamilton did, see [4]). The propositions represented in this way are closely related to "typicalness" [31], "representativeness" <ref> [36] </ref>, "normality" [20], and "fuzziness" [45]. All these concepts are proposed, from different standing points, to capture the phenomenon that an instance does (or doesn't) possess all (or some) properties of a category. The related problems cannot be properly represented and processed by any extensional logic.
Reference: 37. <author> P. Wang. </author> <title> First ladies and fluid logics. </title> <type> Technical Report 62, </type> <institution> Center for Research on Concepts and Cognition, Indiana University, Bloomington, Indiana, </institution> <year> 1992. </year>
Reference-contexts: A control mechanism which chooses premises and rule (s) in each in ference step to answer the questions. The first three components can be called a "logic", in the broad sense of this term <ref> [37] </ref>. As an intelligent system, NARS is designed to be an adaptive system under the constraints that its knowledge and resources are usually insufficient to answer the questions proposed by the environment [40, 42]. <p> It is still a logic, however, in the sense that it uses a domain-independent formal language to represent knowledge, and uses formal rules to capture patterns appearing in human reasoning <ref> [37] </ref>.
Reference: 38. <author> P. Wang. </author> <title> Belief revision in probability theory. </title> <booktitle> In Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 519-526. </pages> <publisher> Mor-gan Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1993. </year>
Reference-contexts: To represent a truth value by a frequency value is not enough for NARS: we still need the information about the absolute value of w to manage the revision of the frequency <ref> [38] </ref>. Can we find a natural way to represent this information in the form of a 15 "relative measurements", or specially, as a ratio? An attractive idea is to define it as the "second-order probability". <p> It is easy to calculate w and w + from f and c, therefore the truth value of a judgment can also be represented as a pair of ratio &lt; f; c &gt; <ref> [38] </ref>. Amazingly, there is a third way to represent a truth value in NAL1: as an interval [41]. Let us first define two measurements. Definition 3.10. <p> No matter which form and interpretation is used, the information carried is actually the same. * It is easier to compare the measurements in NAL1 to various other approaches of uncertain representations, because different forms capture different intuitions about uncertainty. See <ref> [38, 39, 41] </ref> for examples. 18 3.3. Grammar As an extension of IL, ETL, and ITL.
Reference: 39. <author> P. Wang. </author> <title> The interpretation of fuzziness. </title> <type> Technical Report 86, </type> <institution> Center for Research on Concepts and Cognition, Indiana University, Bloomington, Indiana, </institution> <year> 1993. </year>
Reference-contexts: However, as described earlier, frequency (in NAL1) is about both extensional and intensional relations of the two terms. Therefore, it can be used to process the phenomena like fuzziness, typicalness, and so on. <ref> [39] </ref> is a detailed description about how to interpret fuzziness and represent it in NARS. To represent a truth value by a frequency value is not enough for NARS: we still need the information about the absolute value of w to manage the revision of the frequency [38]. <p> No matter which form and interpretation is used, the information carried is actually the same. * It is easier to compare the measurements in NAL1 to various other approaches of uncertain representations, because different forms capture different intuitions about uncertainty. See <ref> [38, 39, 41] </ref> for examples. 18 3.3. Grammar As an extension of IL, ETL, and ITL.
Reference: 40. <author> P. Wang. </author> <title> Non-axiomatic reasoning system (version 2.2). </title> <type> Technical Report 75, </type> <institution> Center for Research on Concepts and Cognition, Indiana University, Bloomington, Indiana, </institution> <year> 1993. </year>
Reference-contexts: 1. INTRODUCTION Non-Axiomatic Reasoning System (NARS) is proposed as a formal model of intelligent reasoning systems <ref> [40] </ref>. fl This work is supported by a research assistantship from Center for Research on Concepts and Cognition, Indiana University. <p> The first three components can be called a "logic", in the broad sense of this term [37]. As an intelligent system, NARS is designed to be an adaptive system under the constraints that its knowledge and resources are usually insufficient to answer the questions proposed by the environment <ref> [40, 42] </ref>. <p> K can be represented by a directed graph, where terms are nodes and inheritance relations are directed links (say, from the subject to the predicate). The 1 The membership relation "2" cannot be represented in IL, though it can be introduced in the extensions of IL <ref> [40] </ref>. Therefore, the subject of a proposition cannot be a "singular term", such as "Tweety" or "Socrates". <p> For an adaptive system, if it must make a choice between 7 In a recent implementation, a "postmark" mechanism is used for this purpose, and it works well. See <ref> [40] </ref> for details. 21 conflicting judgments, the one related to much experience has a higher priority. Choices are necessary in another situation: among competing answers. <p> To get a non-axiomatic reasoning system, we need to provide a memory and a control mechanism, which are adaptive and work with insufficient knowledge and resources. A description of the two components can be found in <ref> [40] </ref>. In this section, let us see how NAL1 works on an example. 5.1.
Reference: 41. <author> P. Wang. </author> <title> A defect in Dempster-Shafer theory. </title> <booktitle> In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <year> 1994. </year>
Reference-contexts: According to the practice of statistics, the most natural way to calculate the weight of evidence is to simply count such terms. In this way, weight of evidence take its values in [0; 1], and is additive when combining two pieces of evidence from distinct sources <ref> [32, 41] </ref>. If the experience of the system is represented by a string of propositions in IL, then, by using the string as premises set K, we can determine extension and intension for each term. <p> Amazingly, there is a third way to represent a truth value in NAL1: as an interval <ref> [41] </ref>. Let us first define two measurements. Definition 3.10. The lower frequency of a judgment, l, is defined as w + w+k ; the upper frequency of a judgment, u, is defined as w + +k w+k . Here k is the same constant introduced above. <p> No matter which form and interpretation is used, the information carried is actually the same. * It is easier to compare the measurements in NAL1 to various other approaches of uncertain representations, because different forms capture different intuitions about uncertainty. See <ref> [38, 39, 41] </ref> for examples. 18 3.3. Grammar As an extension of IL, ETL, and ITL. <p> Therefore, the conclusion derived from the two should be 1 + w + where the evidence from different sections of experience is summarized, or pooled <ref> [41] </ref>.
Reference: 42. <author> P. Wang. </author> <title> On the working definition of intelligence. </title> <type> Technical report, </type> <institution> Center for Research on Concepts and Cognition, Indiana University, Bloom-ington, Indiana, </institution> <year> 1994. </year> <note> In preparation. </note>
Reference-contexts: The first three components can be called a "logic", in the broad sense of this term [37]. As an intelligent system, NARS is designed to be an adaptive system under the constraints that its knowledge and resources are usually insufficient to answer the questions proposed by the environment <ref> [40, 42] </ref>.
Reference: 43. <author> K. Weichselberger and S. Pohlmann. </author> <title> A Methodology for Uncertainty in Knowledge-Based Systems. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1990. </year>
Reference-contexts: This measurement shares similar intuition with other interval approaches <ref> [5, 22, 43] </ref>. For example, "ignorance", i, can be represented by the width of the interval (here it happens to be 1 c, so ignorance and confidence are complement to each other).
Reference: 44. <author> R. Yager. </author> <title> Credibility discounting in the theory of approximate reasoning. </title> <editor> In P. Bonissone, M. Henrion, L. Kanal, and J. Lemmer, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence 6, </booktitle> <pages> pages 299-310. </pages> <publisher> North-Holland, </publisher> <address> Amster-dam, </address> <year> 1991. </year>
Reference-contexts: Intuitively, confidence is the ratio that the weight of "current relevant evidence" to the weight of "relevant evidence in the near future". It indicates how much the system knows about the inheritance relation, so is similar to Shafer's "reliability" [33] or Yager's "credibility" <ref> [44] </ref>.

References-found: 44

