URL: http://polaris.cs.uiuc.edu/newhome/ftp/Parafrase2/papers/icpp89.ps.Z
Refering-URL: http://polaris.cs.uiuc.edu/newhome/ftp/Parafrase2/papers/
Root-URL: http://www.cs.uiuc.edu
Title: The Structure of Parafrase-2 An Advanced Parallelizing Compiler for C and Fortran  
Author: C. D. Polychronopoulos, M. B. Girkar M. R. Haghighat, C. L. Lee B. P. Leung and D. A. Schouten 
Address: Urbana, IL 61801/USA  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. D. Ullman, </author> <booktitle> Compilers : Principles, Techniques and Tools, </booktitle> <publisher> Addison Wesley, </publisher> <month> march </month> <year> 1986. </year>
Reference-contexts: Data dependences are normally defined with respect to the set of variables which are used and modified by a statement; denoted by the IN set and the OUT set respectively <ref> [1] </ref>. The next section gives a formal definition of data dependences and other associated terms which are used throughout the paper. 2.1 Background and Definitions A program is a list of statements. S i denotes the i-th statement in a program (counting in lexicographic order). <p> j e ; j m+1 ; . . . ; j n )) 6= ;; and it is denoted by S i ffi o S j . 2.2 Data Dependence Computations Detecting scalar dependences among statements is straightforward: it involves taking the intersection of the corresponding IN and OUT sets <ref> [1] </ref>. The same strategy also works for arrays, but gives coarse dependence results. For more accurate information, subscript analysis of array variables needs to be performed. Testing for dependences then involves checking whether two subscript expressions could take on identical values during the execution of the program.
Reference: [2] <author> F. E. Allen, M. Burke, R. Cytron, J. Ferrante, W. Hsieh, and V. Sarkar, </author> <title> A framework for determining useful parallelism, </title> <booktitle> in International Con--ference on Supercomputing, </booktitle> <year> 1988. </year>
Reference-contexts: The initial implementation and evaluation is restricted to shared memory multiprocessors. The first is an idealized machine model with an unlimited number of processors and without memory access overheads or synchronization delays. By determining the maximum parallelism possible <ref> [2] </ref>, timing with this model will give the theoretical best time for execution of a program. This can be used as a basis for comparison with other models. The next simplified model is based on the hypothesis that memory access is the overriding factor in the execution of a program.
Reference: [3] <author> F. E. Allen and J. Cocke, </author> <title> A Catalogue of Optimizing Transformations, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1972, </year> <pages> pp. 1-30. </pages>
Reference-contexts: Restructuring compilers which facilitate automatic parallelization of serial programs become increasingly popular; most of the parallel computer vendors supply such (more or less powerful) compilers with their parallel systems. However, unlike automatic program vectorization which is a well-researched and well-understood subject <ref> [3] </ref> [4] [6] [21] [23] [22] [41], program parallelization (or concurrentization), particularly for hierarchical architectures [5], is still at an early and fast developing stage.
Reference: [4] <author> J. R. Allen and K. Kennedy, </author> <title> PFC: A Program to Convert Fortran to Parallel Form, </title> <type> Tech. Rep. </type> <institution> MASC-TR82-6, Rice University, Houston, Texas, </institution> <month> March </month> <year> 1982. </year>
Reference-contexts: Restructuring compilers which facilitate automatic parallelization of serial programs become increasingly popular; most of the parallel computer vendors supply such (more or less powerful) compilers with their parallel systems. However, unlike automatic program vectorization which is a well-researched and well-understood subject [3] <ref> [4] </ref> [6] [21] [23] [22] [41], program parallelization (or concurrentization), particularly for hierarchical architectures [5], is still at an early and fast developing stage.
Reference: [5] <author> R. Allen, </author> <title> Dependence analysis for subscripted variables and its application to program transformations, </title> <type> PhD thesis, </type> <institution> Rice University, Houston, Texas, </institution> <month> April </month> <year> 1983. </year>
Reference-contexts: However, unlike automatic program vectorization which is a well-researched and well-understood subject [3] [4] [6] [21] [23] [22] [41], program parallelization (or concurrentization), particularly for hierarchical architectures <ref> [5] </ref>, is still at an early and fast developing stage. For example, little is known on program restructuring and compiling for message-passing multiprocessors; many existing compilers use approaches which are direct analogs of schemes used by vectorizing compilers. <p> The compiler can discover this by examining the condition in the IF statement. In Parafrase-2, quick and inexpensive heuristics which capture most of the cases are used instead of doing an extensive symbolic data dependence analysis, which will usually give more information but may be cost-prohibitive <ref> [5] </ref> [26]. First, the set of loop invariant variables is found, that is, those variables whose value do not change as long as control stays within the loop. All symbolic expressions are then evaluated in terms of the loop invariant variables.
Reference: [6] <author> R. Allen and K. Kennedy, </author> <title> Automatic translation of fortran programs to vector form, </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <month> 9 </month> <year> (1987). </year>
Reference-contexts: Restructuring compilers which facilitate automatic parallelization of serial programs become increasingly popular; most of the parallel computer vendors supply such (more or less powerful) compilers with their parallel systems. However, unlike automatic program vectorization which is a well-researched and well-understood subject [3] [4] <ref> [6] </ref> [21] [23] [22] [41], program parallelization (or concurrentization), particularly for hierarchical architectures [5], is still at an early and fast developing stage. <p> Section 5 discusses a particular transformation involving the generation of multiple version loops. Section 6 examines auto-scheduling by the compiler and Section 7 deals with the user interface. 2 Data Dependences Data dependence testing has been extensively studied <ref> [6] </ref> [8] [9] [41] in the context of automatic parallelization of sequential programs. Data dependences give information about the flow of data in a program.
Reference: [7] <editor> Allen, F. E., et. al., </editor> <title> An overview of the PTRAN analysis system, </title> <booktitle> in Proceedings of the 1987 International Conference on Supercomputing, </booktitle> <publisher> Springer-Verlag, LNCS, </publisher> <month> February </month> <year> 1988. </year>
Reference-contexts: For example, little is known on program restructuring and compiling for message-passing multiprocessors; many existing compilers use approaches which are direct analogs of schemes used by vectorizing compilers. Recent efforts in this direction are underway in a few research centers; PTRAN at IBM, PTOOL at Rice and Parafrase-2 [34] <ref> [7] </ref> [13] [17] [27] [28] [29] [31] [35] [32]. This paper describes the Parafrase-2 project which is aimed at developing a source to source multilingual restructuring compiler. It provides a reliable, portable and efficient research tool for experimentation with program transformations and other compiler techniques for parallel supercomputers.
Reference: [8] <author> U. Banerjee, </author> <title> Data Dependence in Ordinary Programs, </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> November </month> <year> 1976. </year> <title> [9] , Speedup of Ordinary Programs, </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1979. </year> <title> [10] , Dependence Analysis for Supercomputing, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: Section 5 discusses a particular transformation involving the generation of multiple version loops. Section 6 examines auto-scheduling by the compiler and Section 7 deals with the user interface. 2 Data Dependences Data dependence testing has been extensively studied [6] <ref> [8] </ref> [9] [41] in the context of automatic parallelization of sequential programs. Data dependences give information about the flow of data in a program. <p> In other words, I and I 0 must satisfy the following n inequalities. L 1 i 1 ; i 0 . . . (2) n U n Two of the most common data dependence tests are the gcd and bounds tests <ref> [8] </ref>. In the gcd test one checks whether (1) has a solution in integers ignoring the loop limits. In the bounds test one checks whether the above system has a solution in reals (instead of integers) within the loop limits. For example, consider the following loop.
Reference: [11] <author> J. Banning, </author> <title> A Method for Determining the Side Effects of Procedure Calls, </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> August </month> <year> 1978. </year>
Reference-contexts: Alias information is stored as pairs of symbol table entries. Two objects a and b are aliased if and only if there exists a pair ha; bi in the set of alias pairs. Aliasing information is gathered by an iterative analysis of the callgraph, according to Banning's algorithm <ref> [11] </ref>. This algorithm is limited by the number of possible aliasing pairs, and runs with time complexity O (number of aliases). Banning's algorithm has been implemented in Parafrase-2. A sample run is shown in Figure 3.
Reference: [12] <author> C. S. Beckman-Davies, </author> <title> Improving Parallelism in LINPACK, </title> <type> Tech. Rep. </type> <institution> UIUCDCS-R-83-1138, Dept. of Computer Science, UIUC, Urbana, Illinois 61801, </institution> <month> May </month> <year> 1983. </year>
Reference-contexts: Hence, this loop can be either transformed to a DOALL loop or to the following vector statements: A (2*LOW+N:2*HIGH+N:2) = B (LOW:HIGH) A (2*LOW+N+1:2*HIGH+N+1:2) = C (LOW:HIGH) To see how symbolic bounds test works, consider the following code which has been extracted from a block tridiagonal solver <ref> [12] </ref>: DO J = 1, M F ((P-1)*Q+J) = F ((P-1)*Q+J) - V (K,J,P-1) END DO A simple symbolic manipulation of subscripts of array F shows that the inner loop forms a recurrence and the outer loop can be executed in parallel.
Reference: [13] <author> M. Burke and R. Cytron, </author> <title> Interprocedural dependence analysis and paralleliza-tion, </title> <booktitle> ACM SIGPLAN `86 Symposium on Compiler Construction, </booktitle> <year> (1986), </year> <pages> pp. 162-175. 21(7). </pages>
Reference-contexts: Recent efforts in this direction are underway in a few research centers; PTRAN at IBM, PTOOL at Rice and Parafrase-2 [34] [7] <ref> [13] </ref> [17] [27] [28] [29] [31] [35] [32]. This paper describes the Parafrase-2 project which is aimed at developing a source to source multilingual restructuring compiler. It provides a reliable, portable and efficient research tool for experimentation with program transformations and other compiler techniques for parallel supercomputers.
Reference: [14] <author> D. Callahan, K. D. Cooper, K. Kennedy, and L. Torczon, </author> <title> Interprocedural constant propagation, </title> <journal> Journal of the ACM, </journal> <year> (1986), </year> <pages> pp. 152-161. </pages>
Reference-contexts: indicate identical objects or if they overlap, as with two arrays with different offsets from a common location. * Execution Contexts | propagating execution contexts (values and ranges of various formal parameters) from procedure calls to the procedures, which can be used for more accurate dependence analysis within the procedure <ref> [14] </ref>.
Reference: [15] <author> D. Callahan and K. Kennedy, </author> <title> Analysis of interprocedural side effects in a parallel programming environment, </title> <booktitle> in Lecture Notes in Computer Science Vol. 297 : 1st International Conference on Supercomputing, </booktitle> <editor> E. N. Houstis, T. S. Papatheodorou, and C. D. Polychronopoulos, eds., </editor> <publisher> Springer-Verlag, </publisher> <month> June </month> <year> 1987, </year> <pages> pp. 138-171. </pages>
Reference-contexts: Second, it fails in the presence of recursion. However, it is useful in many cases and is useful as a standard against which other methods may be compared. Another approach is to summarize reference information for a procedure in terms of the formal parameters at a call site [39] <ref> [15] </ref> [25]. The goal is to collect a set of all the objects (or parts of objects) that may be defined or used as a result of a call to a given procedure.
Reference: [16] <author> M. Chastain, G. Gostin, J. Mankovich, and S. Wallach, </author> <title> The Convex C240 architecture, </title> <booktitle> in Supercomputing 88, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1988. </year>
Reference-contexts: Hence it is possible for some of these "optimizations" to actually slow down the program. By using quantitative estimates, the compiler can decide when to do certain transformations and when vectorization is better than parallelization or vice versa. At least one commercial compiler features some level of cost/performance analysis <ref> [16] </ref>. For example, each iteration of the outer loop requires a fork and join for the doall. This could involve a fair amount of overhead.
Reference: [17] <author> R. G. Cytron, </author> <title> Doacross: beyond vectorization for multiprocessors (extended ab-stract), </title> <booktitle> in Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1986, </year> <pages> pp. 836-844. </pages>
Reference-contexts: Recent efforts in this direction are underway in a few research centers; PTRAN at IBM, PTOOL at Rice and Parafrase-2 [34] [7] [13] <ref> [17] </ref> [27] [28] [29] [31] [35] [32]. This paper describes the Parafrase-2 project which is aimed at developing a source to source multilingual restructuring compiler. It provides a reliable, portable and efficient research tool for experimentation with program transformations and other compiler techniques for parallel supercomputers.
Reference: [18] <author> K. Gallivan, D. Gannon, W. Jalby, A. Malony, and H. Wijshoff, </author> <title> Behavioral Characterization of Multiprocessor Memory Systems: A Case Study, </title> <type> Tech. Rep. 808, </type> <institution> CSRD, </institution> <year> 1988. </year>
Reference-contexts: The next simplified model is based on the hypothesis that memory access is the overriding factor in the execution of a program. By counting the various types of memory references, we can achieve a close approximation (within some constant factor) to the actual execution time <ref> [18] </ref>. Another model assumes that all memory references take unit time. This has limited usefulness. However, if actual timing experiments are performed on the target architecture, this model can provide some measure of how much time is involved with memory conflicts.
Reference: [19] <author> V. A. Guarna, D. Gannon, Y. Gaur, and D. Jablonowski, </author> <title> Faust : an environment for programming parallel scientific applications, </title> <booktitle> in Supercomputing 88, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1988. </year>
Reference-contexts: In cases where the order of loop transformations applied by the compiler is unsatisfactory, the user has the option of applying a different order via the menu options in the graphic interface. 7.3 Implementation Details The graphical interface of Parafrase-2 uses a subset of the Faust <ref> [19] </ref> library routines. Faust provides a programming environment for parallel computing and has utilities to display general purpose graphs and manipulate windows, menus, nodes and arcs in a convenient form under the X windows environment. dependence graph for the body of the do loop is displayed in the graphics window.
Reference: [20] <author> M. D. Guzzi, </author> <title> CEDAR Fortran Programmers Handbook, </title> <type> Tech. Rep. 601, </type> <month> June </month> <year> 1987. </year>
Reference-contexts: Node and edge information is displayed in the text window whenever the user clicks on a node and an edge respectively. "DOALL TRANSFORMATION" option is selected, Parafrase-2 checks to see if the loop can be made into a doall loop. If so, the loop is marked as a CDOALL <ref> [20] </ref> loop. On the other hand, if the loop is not parallelizable, the user is asked if the loop should be forced into a doall loop.
Reference: [21] <author> D. J. Kuck, </author> <title> The Structure of Computers and Computations, Volume I, </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: Restructuring compilers which facilitate automatic parallelization of serial programs become increasingly popular; most of the parallel computer vendors supply such (more or less powerful) compilers with their parallel systems. However, unlike automatic program vectorization which is a well-researched and well-understood subject [3] [4] [6] <ref> [21] </ref> [23] [22] [41], program parallelization (or concurrentization), particularly for hierarchical architectures [5], is still at an early and fast developing stage. For example, little is known on program restructuring and compiling for message-passing multiprocessors; many existing compilers use approaches which are direct analogs of schemes used by vectorizing compilers.
Reference: [22] <author> D. J. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. Wolfe, </author> <title> Dependence graphs and compiler optimizations, </title> <booktitle> in Proceedings of the 8-th ACM Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1981, </year> <pages> pp. 207-218. </pages>
Reference-contexts: Restructuring compilers which facilitate automatic parallelization of serial programs become increasingly popular; most of the parallel computer vendors supply such (more or less powerful) compilers with their parallel systems. However, unlike automatic program vectorization which is a well-researched and well-understood subject [3] [4] [6] [21] [23] <ref> [22] </ref> [41], program parallelization (or concurrentization), particularly for hierarchical architectures [5], is still at an early and fast developing stage. For example, little is known on program restructuring and compiling for message-passing multiprocessors; many existing compilers use approaches which are direct analogs of schemes used by vectorizing compilers.
Reference: [23] <author> D. J. Kuck, R. H. Kuhn, B. Leasure, and M. Wolfe, </author> <title> The structure of an advanced vectorizer for pipelined processors, </title> <booktitle> in Fourth International Computer Software and Applications Conference, </booktitle> <month> October </month> <year> 1980. </year>
Reference-contexts: Restructuring compilers which facilitate automatic parallelization of serial programs become increasingly popular; most of the parallel computer vendors supply such (more or less powerful) compilers with their parallel systems. However, unlike automatic program vectorization which is a well-researched and well-understood subject [3] [4] [6] [21] <ref> [23] </ref> [22] [41], program parallelization (or concurrentization), particularly for hierarchical architectures [5], is still at an early and fast developing stage. For example, little is known on program restructuring and compiling for message-passing multiprocessors; many existing compilers use approaches which are direct analogs of schemes used by vectorizing compilers.
Reference: [24] <author> J. T. Kuehn and B. J. Smith, </author> <title> The Horizon supercomputing system: </title> <booktitle> architecture and software, in Supercomputing 88, </booktitle> <month> nov </month> <year> 1988, </year> <pages> pp. 28-34. </pages>
Reference-contexts: We consider here three possible schemes for scheduling of loops. The first scheme is commonly referred to as self-scheduling. An idle processor picks a single iteration of a parallel loop by incrementing the loop indices in a synchronized way <ref> [24] </ref> [38]. Thus if N is the total number of iterations of a loop, self-scheduling involves N dispatch operations. Let B be the average iteration execution time and the overhead involved with each dispatch.
Reference: [25] <author> Z. Li and P. C. Yew, </author> <title> Interprocedural Analysis and Program Restructuring for Parallel Programs, </title> <type> Tech. Rep. </type> <institution> CSRD Rpt No. </institution> <address> 720, UIUC, </address> <month> January </month> <year> 1988. </year>
Reference-contexts: However, it is useful in many cases and is useful as a standard against which other methods may be compared. Another approach is to summarize reference information for a procedure in terms of the formal parameters at a call site [39] [15] <ref> [25] </ref>. The goal is to collect a set of all the objects (or parts of objects) that may be defined or used as a result of a call to a given procedure.
Reference: [26] <author> A. Lichnewsky and F. Thomasset, </author> <title> Introducing symbolic problem solving techniques in the dependence testing phases of a vectorizer, </title> <booktitle> in Proceedings of the 1988 International Conference on Supercomputing, </booktitle> <year> 1988. </year>
Reference-contexts: The compiler can discover this by examining the condition in the IF statement. In Parafrase-2, quick and inexpensive heuristics which capture most of the cases are used instead of doing an extensive symbolic data dependence analysis, which will usually give more information but may be cost-prohibitive [5] <ref> [26] </ref>. First, the set of loop invariant variables is found, that is, those variables whose value do not change as long as control stays within the loop. All symbolic expressions are then evaluated in terms of the loop invariant variables.
Reference: [27] <author> S. P. Midkiff and D. A. Padua, </author> <title> Compiler generated synchronization for do loops, </title> <booktitle> in Proc. of the 1986 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1986, </year> <pages> pp. 544-551. </pages>
Reference-contexts: Recent efforts in this direction are underway in a few research centers; PTRAN at IBM, PTOOL at Rice and Parafrase-2 [34] [7] [13] [17] <ref> [27] </ref> [28] [29] [31] [35] [32]. This paper describes the Parafrase-2 project which is aimed at developing a source to source multilingual restructuring compiler. It provides a reliable, portable and efficient research tool for experimentation with program transformations and other compiler techniques for parallel supercomputers.
Reference: [28] <author> A. Nicolau, </author> <title> Parallelism, Memory Anti-Aliasing and Correctness for Trace Scheduling Compilers, </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> June </month> <year> 1984. </year>
Reference-contexts: Recent efforts in this direction are underway in a few research centers; PTRAN at IBM, PTOOL at Rice and Parafrase-2 [34] [7] [13] [17] [27] <ref> [28] </ref> [29] [31] [35] [32]. This paper describes the Parafrase-2 project which is aimed at developing a source to source multilingual restructuring compiler. It provides a reliable, portable and efficient research tool for experimentation with program transformations and other compiler techniques for parallel supercomputers.
Reference: [29] <author> D. A. Padua, D. J. Kuck, and D. H. Lawrie, </author> <title> High-speed multiprocessors and compilation techniques, </title> <journal> IEEE Transactions on Computers, </journal> <month> C-29 </month> <year> (1980). </year>
Reference-contexts: Recent efforts in this direction are underway in a few research centers; PTRAN at IBM, PTOOL at Rice and Parafrase-2 [34] [7] [13] [17] [27] [28] <ref> [29] </ref> [31] [35] [32]. This paper describes the Parafrase-2 project which is aimed at developing a source to source multilingual restructuring compiler. It provides a reliable, portable and efficient research tool for experimentation with program transformations and other compiler techniques for parallel supercomputers.
Reference: [30] <author> D. A. Padua and M. Wolfe, </author> <title> Advanced compiler optimizations for supercomputers, </title> <journal> Communications of the ACM, </journal> <volume> 29 (1986), </volume> <pages> pp. 1184-1201. </pages>
Reference-contexts: Thus, a similar analysis shows that the outer loop can be executed in parallel, and the inner loop forms a recurrence. Besides using information about loop bounds, global analysis may also help to eliminate some of the dependences which may be assumed otherwise. For instance, consider the following loop <ref> [30] </ref>: IF (M &gt; 0) THEN DO I = LOW, HIGH A (I) = B (I) + A (I+M) END IF The loop cannot be executed unless M &gt; 0; therefore, it can be vectorized. The compiler can discover this by examining the condition in the IF statement.
Reference: [31] <author> J. K. Peir, </author> <title> Program Partitioning and Synchronization on Multiprocessor Systems, </title> <type> Tech. Rep. </type> <institution> UIUCCDCS-R-86-1259, Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> March </month> <year> 1986. </year>
Reference-contexts: Recent efforts in this direction are underway in a few research centers; PTRAN at IBM, PTOOL at Rice and Parafrase-2 [34] [7] [13] [17] [27] [28] [29] <ref> [31] </ref> [35] [32]. This paper describes the Parafrase-2 project which is aimed at developing a source to source multilingual restructuring compiler. It provides a reliable, portable and efficient research tool for experimentation with program transformations and other compiler techniques for parallel supercomputers.
Reference: [32] <author> C. D. Polychronopoulos, </author> <title> Parallel Programming and Compilers, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year> <title> [33] , Toward auto-scheduling compilers, </title> <journal> The Journal of Supercomputing, </journal> <year> (1988), </year> <pages> pp. 297-330. </pages>
Reference-contexts: Recent efforts in this direction are underway in a few research centers; PTRAN at IBM, PTOOL at Rice and Parafrase-2 [34] [7] [13] [17] [27] [28] [29] [31] [35] <ref> [32] </ref>. This paper describes the Parafrase-2 project which is aimed at developing a source to source multilingual restructuring compiler. It provides a reliable, portable and efficient research tool for experimentation with program transformations and other compiler techniques for parallel supercomputers. <p> Each idle processor tries to dispatch the next available task from the queue (if any). Also, tasks are queued and thus are qualified for execution as soon as they become "ready". This environment can be realized through an auto-scheduling compiler which generates control code to implement the above procedure <ref> [32] </ref>. Loop (or parallel task) scheduling: Upon queueing, a serial task is dispatched at once as soon as a processor becomes idle. However, a parallel task can draw several processors to execute it and thus it remains queued until exhausted. <p> Intuitively, the CPS is the minimum size of a process whose execution time is equal to the overhead that it incurs during scheduling (dispatching) at run-time. The compiler can roughly estimate the CPS for each task in the program as shown in <ref> [32] </ref>. The effect of this pass is to merge small tasks into larger ones and serialize certain loops. 6.2 Self-Drive Code Generation An auto-scheduling compiler would eliminate most (if not all) of the functions of the operating system pertaining to task creation, scheduling, execution, and deletion.
Reference: [34] <author> C. D. Polychronopoulos, M. B. Girkar, M. R. Haghighat, C. L. Lee, B. Leung, and D. A. Schouten, </author> <title> Parafrase-2: an environment for parallelizing, partitioning, synchronizing, and scheduling programs on multiprocessors, </title> <booktitle> in Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <address> Penn State, St. Charles, IL, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: For example, little is known on program restructuring and compiling for message-passing multiprocessors; many existing compilers use approaches which are direct analogs of schemes used by vectorizing compilers. Recent efforts in this direction are underway in a few research centers; PTRAN at IBM, PTOOL at Rice and Parafrase-2 <ref> [34] </ref> [7] [13] [17] [27] [28] [29] [31] [35] [32]. This paper describes the Parafrase-2 project which is aimed at developing a source to source multilingual restructuring compiler. It provides a reliable, portable and efficient research tool for experimentation with program transformations and other compiler techniques for parallel supercomputers.
Reference: [35] <author> C. D. Polychronopoulos and D. J. Kuck, </author> <title> Guided self-scheduling: a practical scheduling scheme for parallel supercomputers, </title> <journal> IEEE Trans. on Computers, </journal> <month> 36 </month> <year> (1987). </year>
Reference-contexts: Recent efforts in this direction are underway in a few research centers; PTRAN at IBM, PTOOL at Rice and Parafrase-2 [34] [7] [13] [17] [27] [28] [29] [31] <ref> [35] </ref> [32]. This paper describes the Parafrase-2 project which is aimed at developing a source to source multilingual restructuring compiler. It provides a reliable, portable and efficient research tool for experimentation with program transformations and other compiler techniques for parallel supercomputers.
Reference: [36] <author> R. W. Scheifler, </author> <title> An analysis of inline substitution for a structured programming language, </title> <journal> Communications of the ACM, </journal> <year> (1977). </year>
Reference-contexts: It allows for complete analysis of the procedure, taking into account the complete calling environment, and is thus as close to ideal as possible. This has two main drawbacks. First, it is time and space inefficient, potentially increasing each exponentially <ref> [36] </ref>. Second, it fails in the presence of recursion. However, it is useful in many cases and is useful as a standard against which other methods may be compared.
Reference: [37] <author> Z. Shen, Z. Li, and P. C. Yew, </author> <title> An empirical study on array subscripts and data dependences, </title> <booktitle> in Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1989, </year> <pages> pp. 145-152. </pages>
Reference-contexts: Some of these cases can be handled by interprocedural constant propagation, yet many of them would remain unresolved. For a more precise treatment of the subject see <ref> [37] </ref>. For the unresolved cases we propose a set of simple heuristics.
Reference: [38] <author> P. Tang and P. C. Yew, </author> <title> Processor self-scheduling for multiple-nested parallel loops, </title> <booktitle> in Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1986. </year>
Reference-contexts: We consider here three possible schemes for scheduling of loops. The first scheme is commonly referred to as self-scheduling. An idle processor picks a single iteration of a parallel loop by incrementing the loop indices in a synchronized way [24] <ref> [38] </ref>. Thus if N is the total number of iterations of a loop, self-scheduling involves N dispatch operations. Let B be the average iteration execution time and the overhead involved with each dispatch.
Reference: [39] <author> R. Triolet, F. Irigoin, and P. Feautrier, </author> <title> Direct parallelization of call statements, </title> <booktitle> ACM SIGPLAN 86 Symposium on Compiler Construction, </booktitle> <year> (1986), </year> <pages> pp. 176-185. </pages>
Reference-contexts: Second, it fails in the presence of recursion. However, it is useful in many cases and is useful as a standard against which other methods may be compared. Another approach is to summarize reference information for a procedure in terms of the formal parameters at a call site <ref> [39] </ref> [15] [25]. The goal is to collect a set of all the objects (or parts of objects) that may be defined or used as a result of a call to a given procedure.
Reference: [40] <author> E. W. Weihl, </author> <title> Interprocedural data flow analysis in the presence of pointers, procedure variables and label variables, </title> <booktitle> Seventh Annual ACM Symposium on Principles of Programming Languages, </booktitle> <year> (1980), </year> <pages> pp. 83-94. </pages>
Reference-contexts: This requires detailed flow analysis and even then such aliases can not always be determined. These problems have yet to be investigated in Parafrase-2, though some work has been done in this area <ref> [40] </ref>. 4.1.1 Aliasing Due to Duplicate Parameters Parameter aliasing in Parafrase-2 determines which parameters may be aliased due to aliasing of reference parameters. Alias information is stored as pairs of symbol table entries.
Reference: [41] <author> M. J. Wolfe, </author> <title> Optimizing Supercompilers for Supercomputers, </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1982. </year>
Reference-contexts: Restructuring compilers which facilitate automatic parallelization of serial programs become increasingly popular; most of the parallel computer vendors supply such (more or less powerful) compilers with their parallel systems. However, unlike automatic program vectorization which is a well-researched and well-understood subject [3] [4] [6] [21] [23] [22] <ref> [41] </ref>, program parallelization (or concurrentization), particularly for hierarchical architectures [5], is still at an early and fast developing stage. For example, little is known on program restructuring and compiling for message-passing multiprocessors; many existing compilers use approaches which are direct analogs of schemes used by vectorizing compilers. <p> Section 5 discusses a particular transformation involving the generation of multiple version loops. Section 6 examines auto-scheduling by the compiler and Section 7 deals with the user interface. 2 Data Dependences Data dependence testing has been extensively studied [6] [8] [9] <ref> [41] </ref> in the context of automatic parallelization of sequential programs. Data dependences give information about the flow of data in a program. This can be used to restructure the program and find blocks of code which do not reference the same data and hence can be executed in parallel. <p> Thus when checking for intersection one not only checks whether the variable name is the same but also goes through a list of its qualifications and checks that each component name and subscript expression matches. 2.3 Dependence Directions and Distances Data dependence directions were first introduced in <ref> [41] </ref> and were shown to be useful in carrying out many loop transformations. <p> Once the data dependence graph is computed there are several well known loop transformations <ref> [41] </ref> which can be used to extract parallelism from the program.
References-found: 38

