URL: http://www.cs.utexas.edu/users/ring/AllGoals.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ring/
Root-URL: 
Email: Mark.Ring@GMD.de  
Title: Finding Promising Exploration Regions by Weighting Expected Navigation Costs  
Author: Mark B. Ring 
Date: April 11, 1996  
Address: Schlo Birlinghoven, D-53 754 Sankt Augustin Germany  
Affiliation: GMD German National Research Center for Information Technology  
Abstract: In many learning tasks, data-query is neither free nor of constant cost. Often the cost of a query depends on the distance from the current location in state space to the desired query point. Much can be gained in these instances by keeping track of (1) the length of the shortest path from each state to every other, and (2) the first action to take on each of these paths. With this information, a learning agent can efficiently explore its environment, calculating at every step the action that will move it towards the region of greatest estimated exploration benefit by balancing the exploration potential of all reachable states encountered so far against their currently estimated distances. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman. </author> <title> Data Structures and Algorithms. </title> <booktitle> Addison-Wesley Series in Computer Science and Information Processing. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1983. </year>
Reference-contexts: It is also possible to keep best known estimates of the distances from each state to each other (see Kaelbling, 1993). Kaelbling's DG-learning algorithm is based on Floyd's all-pairs shortest-path algorithm <ref> [1] </ref> and is just slightly different from that used here. These "all-goals" algorithms (after Kaelbling) can provide a highly satisfying representation of the distance/benefit tradeoff. The following describes a minor variation of the DG algorithm that is perhaps a bit more direct.
Reference: [2] <author> David Cohn. </author> <title> Neural network exploration using optimal experiment design. </title> <editor> In Jack D. Cowan, Gerald Tesauro, and Joshua Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 679-686, </pages> <address> San Mateo, California, 1994. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Querying possibly optimal regions of state space in these environments is inadvisable if the path to the query point intersects a region of known volatility. In continuous environments, some first-order approximations to such distance-dependent active learning has already been done <ref> [2, 5, 7, 8] </ref>. In these cases, the learning agent follows a gradient towards promising learning areas by taking the action at each step that maximizes a local ignorance measure.
Reference: [3] <author> Leslie Pack Kaelbling. </author> <title> Hierarchical learning in stochastic domains: Preliminary re-sults. </title> <booktitle> In Machine Learning: Proceedings of the tenth International Conference, </booktitle> <pages> pages 167-173. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <month> June </month> <year> 1993. </year>
Reference-contexts: Unfortunately, neither of these schemes could be said to require less than O (N 2 ) space, since the complete D matrix is needed from the outset. So far, none of the attempts to find an incremental version of the algorithm have yielded a satisfactory result. Kaelbling <ref> [3] </ref> has introduced some methods that use less space while approximating the D matrix using neural networks and hierarchies of distances. Though promising, this approach has not yet yielded a practical solution to the problem.
Reference: [4] <author> Leslie Pack Kaelbling. </author> <title> Learning to achieve goals. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1094-1098, </pages> <address> Chambery, France, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [5] <author> Alexander Linden and Frank Weber. </author> <title> Implementing inner drive through competence reflection. </title> <editor> In J. A. Meyer, H. Roitblat, and S. Wilson, editors, </editor> <booktitle> From Animals to Animats 2: Proceedings of the Second International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 321-326. </pages> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: Querying possibly optimal regions of state space in these environments is inadvisable if the path to the query point intersects a region of known volatility. In continuous environments, some first-order approximations to such distance-dependent active learning has already been done <ref> [2, 5, 7, 8] </ref>. In these cases, the learning agent follows a gradient towards promising learning areas by taking the action at each step that maximizes a local ignorance measure.
Reference: [6] <author> Ronald L. Rivest and Robert E. Schapire. </author> <title> Inference of finite automata using homing sequences. </title> <journal> Information and Computation, </journal> <volume> 103(2) </volume> <pages> 299-347, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: y is deducible and therefore (from above) such a path is also known, i.e., y 2 K: As a result, the agent will always know a path to an underexplored state until all reachable states have been explored. (The existence of this proof was also implied by Rivest and Schapire <ref> [6] </ref>.) 4 Locality and Exploration Benefit Though the strategy just given will guarantee complete exploration, it does not imply efficient exploration when certain common properties about the environment might be known in advance. One of these is the property of locality.
Reference: [7] <author> Jurgen Schmidhuber. </author> <title> Adaptive confidence and adaptive curiosity. </title> <type> Technical Report FKI-149-91 (revised), </type> <institution> Technische Universitat Munchen, Institut fur Informatik, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: Querying possibly optimal regions of state space in these environments is inadvisable if the path to the query point intersects a region of known volatility. In continuous environments, some first-order approximations to such distance-dependent active learning has already been done <ref> [2, 5, 7, 8] </ref>. In these cases, the learning agent follows a gradient towards promising learning areas by taking the action at each step that maximizes a local ignorance measure.
Reference: [8] <author> Sebastian B. Thrun and Knut Moller. </author> <title> Active exploration in dynamic environments. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippman, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 531-538, </pages> <address> San Mateo, California, 1992. </address> <publisher> Morgan Kaufmann Publishers. </publisher> <pages> 9 </pages>
Reference-contexts: Querying possibly optimal regions of state space in these environments is inadvisable if the path to the query point intersects a region of known volatility. In continuous environments, some first-order approximations to such distance-dependent active learning has already been done <ref> [2, 5, 7, 8] </ref>. In these cases, the learning agent follows a gradient towards promising learning areas by taking the action at each step that maximizes a local ignorance measure.
References-found: 8

