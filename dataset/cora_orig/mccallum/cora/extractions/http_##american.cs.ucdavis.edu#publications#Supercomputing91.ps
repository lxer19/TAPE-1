URL: http://american.cs.ucdavis.edu/publications/Supercomputing91.ps
Refering-URL: http://american.cs.ucdavis.edu/ArchLabPersonnel/Farrens/PubList.html
Root-URL: http://www.cs.ucdavis.edu
Email: email: farrens@cs.ucdavis.edu email: wetmore@cs.ucdavis.edu email: woodruff@cs.ucdavis.edu  
Phone: tel: (916) 752-9678 tel: (916) 752-2149 tel: (916) 752-8878  
Title: d d Alleviation of Tree Saturation in Multistage Interconnection Networks  
Author: Matthew Farrens Brad Wetmore Allison Woodruff 
Address: Davis, CA 95616 Davis, CA 95616 Davis, CA 95616  
Affiliation: Division of Computer Science Division of Computer Science Division of Computer Science University of California University of California University of California  
Abstract: This paper presents an examination of two distinct but complementary extensions of previous work on hot spot contention in multistage interconnection networks. The first extension focuses on the use of larger queues at the memory modules than are traditionally studied. The second extension explores a simple feedback damping scheme, which we refer to as bleeding, which allows selected processors to ignore feedback information. The impact of memory queue size, feedback threshold value, and bleeding on system performance (specifically maximum bandwidth per processor) is evaluated by analyzing the results of extensive network simulations. These results indicate that combining these approaches can significantly improve the effective bandwidth of a multistage interconnection network. 
Abstract-found: 1
Intro-found: 1
Reference: [DiJu81] <author> D. M. Dias and J. R. </author> <title> Jump, ``Analysis and Simulation of Buffered Delta Networks'', </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. </volume> <month> C-30:4 (April </month> <year> 1981), </year> <pages> pp. 273-282. </pages>
Reference-contexts: The maximum bandwidth of a network can only approach 1.0, even if there are no hot spots, because collisions in both the nodes and the memory modules occur even under uniform traffic <ref> [DiJu81] </ref>. In these simulations, the term "relative bandwidth" refers to the performance of the modified network under study compared to that of a standard Omega network with no modifications.
Reference: [GGKM83] <author> A. Gottlieb, R. Grishman, C. P. Kruskal, K. P. McAuliffe, L. Randolph and M. Snir, </author> <title> ``The NYU Ultracomputer Designing an MIMD Shared Memory Parallel Computer'', </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. </volume> <month> C-34:10 (February </month> <year> 1983), </year> <pages> pp. 175-189. </pages>
Reference-contexts: Previous Approaches Several methods of controlling tree saturation have been proposed. Most, however, have been impractical in terms of technology or expense. Three of these methods are described below. 2.1. Combining One commonly discussed approach to reducing tree saturation is to combine requests to the same address. Both hardware <ref> [GGKM83, LeK86, PfNo85] </ref> and software [TaYe90, YeT87] solutions have been proposed. These approaches reduce the number of requests to a hot memory location, as well as minimize network traffic. <p> Stone has suggested using large queues at the memory modules in regular multistage interconnection networks (i.e. networks without feedback) [Ston87]. Although it has been shown that simply increasing memory queue sizes is not an effective method of controlling tree saturation <ref> [GGKM83, Lee85] </ref>, this paper shows that this technique is significantly more useful when used in conjunction with feedback. 3.2. Damping Because hardware costs and complexity of a global arbiter are prohibitive, some other form of controlling requests to a hot module is desirable. <p> A queue length of four is the same length selected by several previous studies [KuPf86, PfNo85, ScSo90]. The appropriateness of a small queue size is shown by both Lee and Gottlieb, who suggest increasing the queue size beyond size five [Lee85] or size eight <ref> [GGKM83] </ref> does not help alleviate tree saturation. (However, neither study takes feedback into account.) The model also assumes two complete interconnection networks, one to propagate requests from the processors to the memory modules, and one for the reverse trip. <p> As expected, increasing the size of the memory queue provides virtually no improvement in bandwidth. Without feedback, a large queue experiences the same problems that smaller queues do; the only difference is the amount of time before saturation occurs. As shown in [Lee85] and <ref> [GGKM83] </ref>, employing larger memory queues without supplying some kind of feedback mechanism does not eliminate saturation conditions (although it might be useful for reducing the effects of transient hot spots).
Reference: [KuPf86] <author> M. Kumar and G. F. Pfister, </author> <title> ``The Onset of Hot Spot Contention'', </title> <booktitle> International Conference on Parallel Processing(1986), </booktitle> <pages> pp. 28-34. </pages>
Reference-contexts: This degradation is called "tree saturation" and is compounded by the fact that its onset can be rapid <ref> [KuPf86] </ref> and may take a long time to alleviate. <p> The queue that lies between the final stage of the network and the memory modules is called the memory queue (mq) and has a variable length. A queue length of four is the same length selected by several previous studies <ref> [KuPf86, PfNo85, ScSo90] </ref>.
Reference: [LaKu90] <author> T. Lang and L. Kurisaki, </author> <title> ``Nonuniform Traffic Spots (NUTS) in Multistage Interconnection Networks'', </title> <journal> Journal of Parallel and Distributed Computing(1990), </journal> <pages> pp. 55-67. </pages>
Reference-contexts: by events such as references to shared or global variables (e.g. for barrier synchronization or operations on semaphores [Lee85, TaYe90]); block transfers [Thom86]; a coincidental concentration of requests to a single memory module [Lee85, NoPf85]; or a coincidental concentration of traffic through an internal switch (nonuniform traffic spots, or NUTS <ref> [LaKu90] </ref>) which can occur even if the requests to memory are evenly distributed. The model used in this study assumes that queues exist between processors and switches, switches and switches, and switches and memory modules, in order to buffer requests (see Figure 1).
Reference: [Lee85] <author> R. Lee, </author> <title> ``On `Hot Spot' Contention'', </title> <journal> ACM Computer Architecture News, </journal> <volume> vol. </volume> <month> 13:5 </month> <year> (1985), </year> <pages> pp. 15-20. </pages>
Reference-contexts: Hot spots are locations (nodes within the network, memory modules, or specific addresses) which receive a nonuniform distribution of traffic. This concentration of traffic can be caused by events such as references to shared or global variables (e.g. for barrier synchronization or operations on semaphores <ref> [Lee85, TaYe90] </ref>); block transfers [Thom86]; a coincidental concentration of requests to a single memory module [Lee85, NoPf85]; or a coincidental concentration of traffic through an internal switch (nonuniform traffic spots, or NUTS [LaKu90]) which can occur even if the requests to memory are evenly distributed. <p> This concentration of traffic can be caused by events such as references to shared or global variables (e.g. for barrier synchronization or operations on semaphores [Lee85, TaYe90]); block transfers [Thom86]; a coincidental concentration of requests to a single memory module <ref> [Lee85, NoPf85] </ref>; or a coincidental concentration of traffic through an internal switch (nonuniform traffic spots, or NUTS [LaKu90]) which can occur even if the requests to memory are evenly distributed. <p> Stone has suggested using large queues at the memory modules in regular multistage interconnection networks (i.e. networks without feedback) [Ston87]. Although it has been shown that simply increasing memory queue sizes is not an effective method of controlling tree saturation <ref> [GGKM83, Lee85] </ref>, this paper shows that this technique is significantly more useful when used in conjunction with feedback. 3.2. Damping Because hardware costs and complexity of a global arbiter are prohibitive, some other form of controlling requests to a hot module is desirable. <p> A queue length of four is the same length selected by several previous studies [KuPf86, PfNo85, ScSo90]. The appropriateness of a small queue size is shown by both Lee and Gottlieb, who suggest increasing the queue size beyond size five <ref> [Lee85] </ref> or size eight [GGKM83] does not help alleviate tree saturation. (However, neither study takes feedback into account.) The model also assumes two complete interconnection networks, one to propagate requests from the processors to the memory modules, and one for the reverse trip. <p> As expected, increasing the size of the memory queue provides virtually no improvement in bandwidth. Without feedback, a large queue experiences the same problems that smaller queues do; the only difference is the amount of time before saturation occurs. As shown in <ref> [Lee85] </ref> and [GGKM83], employing larger memory queues without supplying some kind of feedback mechanism does not eliminate saturation conditions (although it might be useful for reducing the effects of transient hot spots).
Reference: [LeK86] <author> G. Lee, C. P. Kruskal and D. J. Kuck, </author> <title> ``The Effectiveness of Combining in Shared Memory Parallel Computers in the Presence of `Hot Spots''', </title> <booktitle> International Conference on Parallel Processing(1986), </booktitle> <pages> pp. 35-41. </pages>
Reference-contexts: Previous Approaches Several methods of controlling tree saturation have been proposed. Most, however, have been impractical in terms of technology or expense. Three of these methods are described below. 2.1. Combining One commonly discussed approach to reducing tree saturation is to combine requests to the same address. Both hardware <ref> [GGKM83, LeK86, PfNo85] </ref> and software [TaYe90, YeT87] solutions have been proposed. These approaches reduce the number of requests to a hot memory location, as well as minimize network traffic. <p> When a queue's threshold is reached, all processors are notified to hold hhhhhhhhhhhhhhhhhhhhhhhhhhhhh 1 Lee et al found that the hardware cost is even higher than originally suspected, because suggested methods of combining are ineffective, necessitating even more sophisticated hardware to achieve favorable results <ref> [LeK86] </ref>. requests to the corresponding (hot) memory module. Once the queue length falls below the threshold, processors are informed that they may once again issue requests to the formerly hot module. <p> Each node is capable of receiving one request from each node input per cycle, and will place that request in a FIFO queue in time for the next cycle; queues can accept up to two inputs per cycle, as in <ref> [LeK86, Lee89] </ref>. Each queue feeds the next stage of the network, and can hold up to four elements. The queue that lies between the final stage of the network and the memory modules is called the memory queue (mq) and has a variable length. <p> This module is called the "hot" module, although it may be either hot or cold at any point during the simulation. The model does not differentiate the percentage of time spent in hot spot versus uniform traffic conditions. It has been noted that hot spots are transient <ref> [LeK86] </ref>, but the amount of time spent in different states has not been d d fully explored, and is not examined here. We assume a steady-state representation. Further, the model is somewhat optimistic in forcing only one hot spot at a time. <p> Future Work There are a number of topics yet to be investigated. Hot spots on the return network should be examined in non-combining networks; in <ref> [LeK86] </ref>, it was discovered that with combining, return networks needed larger queues because requests came out in bursts.
Reference: [Lee89] <author> G. Lee, </author> <title> ``A Performance Bound of Multistage Combining Networks'', </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. </volume> <month> C-38:10 (October </month> <year> 1989), </year> <pages> pp. 1387-1395. </pages>
Reference-contexts: Unfortunately, sharing memory leads to memory contention. Furthermore, the larger the number of processors, the greater the degree of contention <ref> [Lee89] </ref>. The result in multistage interconnection networks is an undesirable phenomenon known as hot spots [PfNo85]. Hot spots are locations (nodes within the network, memory modules, or specific addresses) which receive a nonuniform distribution of traffic. <p> Both hardware [GGKM83, LeK86, PfNo85] and software [TaYe90, YeT87] solutions have been proposed. These approaches reduce the number of requests to a hot memory location, as well as minimize network traffic. However, combining has several disadvantages: large hardware expense 1 ; lack of scalability in practical applications <ref> [Lee89] </ref>; and failure to alleviate tree saturation arising from causes other than excessive competition for a single address. 2.2. Feedback The technique of feedback can be used to control tree saturation as follows: a threshold value for the queues at all memory modules is chosen. <p> Each node is capable of receiving one request from each node input per cycle, and will place that request in a FIFO queue in time for the next cycle; queues can accept up to two inputs per cycle, as in <ref> [LeK86, Lee89] </ref>. Each queue feeds the next stage of the network, and can hold up to four elements. The queue that lies between the final stage of the network and the memory modules is called the memory queue (mq) and has a variable length.
Reference: [NoPf85] <author> A. Norton and G. F. Pfister, </author> <title> ``A Methodology for Predicting Multiprocessor Performance'', </title> <booktitle> International Conference on Parallel Processing(1985), </booktitle> <pages> pp. 772-881. </pages>
Reference-contexts: This concentration of traffic can be caused by events such as references to shared or global variables (e.g. for barrier synchronization or operations on semaphores [Lee85, TaYe90]); block transfers [Thom86]; a coincidental concentration of requests to a single memory module <ref> [Lee85, NoPf85] </ref>; or a coincidental concentration of traffic through an internal switch (nonuniform traffic spots, or NUTS [LaKu90]) which can occur even if the requests to memory are evenly distributed.
Reference: [PfNo85] <author> G. F. Pfister and V. A. Norton, </author> <title> ```Hot Spot' Contention and Combining in Multistage Interconnection Networks'', </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. </volume> <month> C-34:10 (October </month> <year> 1985), </year> <pages> pp. 943-948. </pages>
Reference-contexts: Unfortunately, sharing memory leads to memory contention. Furthermore, the larger the number of processors, the greater the degree of contention [Lee89]. The result in multistage interconnection networks is an undesirable phenomenon known as hot spots <ref> [PfNo85] </ref>. Hot spots are locations (nodes within the network, memory modules, or specific addresses) which receive a nonuniform distribution of traffic. <p> This degradation is called "tree saturation" and is compounded by the fact that its onset can be rapid [KuPf86] and may take a long time to alleviate. In addition, it takes very little nonuniform traffic to induce this condition <ref> [PfNo85, Ston87] </ref>. d d Switch 2x2 Switch 2x2 Switch 2x2 Switch P I N P I N Processor Processor Processor Processor Memory Memory Memory Memory mq mq q q q q Components of a Multistage Interconnection Network 2. Previous Approaches Several methods of controlling tree saturation have been proposed. <p> Previous Approaches Several methods of controlling tree saturation have been proposed. Most, however, have been impractical in terms of technology or expense. Three of these methods are described below. 2.1. Combining One commonly discussed approach to reducing tree saturation is to combine requests to the same address. Both hardware <ref> [GGKM83, LeK86, PfNo85] </ref> and software [TaYe90, YeT87] solutions have been proposed. These approaches reduce the number of requests to a hot memory location, as well as minimize network traffic. <p> The queue that lies between the final stage of the network and the memory modules is called the memory queue (mq) and has a variable length. A queue length of four is the same length selected by several previous studies <ref> [KuPf86, PfNo85, ScSo90] </ref>. <p> In practice there may be nonuniform requests to several modules simultaneously, although Pfister and Norton suggest that typically there are only one or two memory modules hot simultaneously <ref> [PfNo85] </ref>. 5. Simulation Results The simulator was run under a variety of conditions, varying parameters such as memory queue size, feedback threshold, hot rates, degree of bleeding, etc. <p> Simulation Results The simulator was run under a variety of conditions, varying parameters such as memory queue size, feedback threshold, hot rates, degree of bleeding, etc. In discussing our results, we use the following variables, chosen to allow comparison with previous models <ref> [PfNo85, ScSo89, ScSo90] </ref>: N: The number of processors and memory modules. n: The number of levels in the network. (n = log 2 N). <p> Hardware Requirements One of the goals of this study was to examine tree saturation limiting schemes which could be implemented easily and inexpensively (unlike combining, which has been estimated to increase hardware cost by a factor of between 6 and 32 <ref> [PfNo85] </ref>, or global limiting, which not only increases hardware cost and design difficulty, but also potentially lengthens the critical path for submitting requests to the network). A brief estimate of the hardware complexity of the approaches examined in this paper are presented below. 6.1. <p> Further, the effects of caching have yet to be examined, although Pfister and Norton suggest that caching does not help to alleviate tree saturation related to global variables (since they can not be cached) <ref> [PfNo85] </ref>. In addition, an examination of the literature reveals that the frequency of occurrence of hot spots and degree of hot rate have not been explored in detail.
Reference: [PoHa89] <author> A. Pombortis and C. </author> <title> Halatsis, ``Behaviour of Circuit-Switched Multistage Networks in Presence of Memory Hot Spot'', </title> <journal> Electronic Letters, </journal> <volume> vol. </volume> <month> 25:13 (June, </month> <year> 1989), </year> <pages> pp. 833-834. </pages>
Reference-contexts: Requests to cold memory modules ("cold" requests) can encounter full queues; therefore, the latency of the cold requests is increased, and the level of congestion in the network is heightened <ref> [PoHa89, ScSo90] </ref>. This degradation is called "tree saturation" and is compounded by the fact that its onset can be rapid [KuPf86] and may take a long time to alleviate.
Reference: [ScSo89] <author> S. L. Scott and G. S. Sohi, </author> <title> ``Using Feedback to Control Tree Saturation in Multistage Interconnection Networks'', </title> <booktitle> Proceedings of the 15th Annual Symposium on Computer Architecture(1989), </booktitle> <pages> pp. 167-176. </pages>
Reference-contexts: When the processors are notified that the module is again accepting requests, the stored requests will be simultaneously released by the processors, possibly flooding the network. This re-creation of tree saturation is termed overshoot <ref> [ScSo89, ScSo90] </ref>. 2.3. Limiting A limiting scheme assumes the presence of a global arbiter which controls the number of requests to each memory module that enter the network per cycle. <p> This paper explores a simplification of the limiting scheme proposed by Scott and Sohi, who suggest that whenever a module is hot, at most 1 request to a hot module and 2 requests to a cold module be allowed to enter the network each cycle <ref> [ScSo89] </ref>. Requests are damped as follows: at each network cycle, a number of processors are selected and allowed to submit a request to the hot module, if they have one pending. Cold requests are not restricted from entering the network. We call this technique bleeding. <p> It also provides a more gradual release of blocked requests into the network, reducing overshoot. 4. The Simulator In this study we used a slightly modified version of the model-driven simulator used by Scott and Sohi, thus allowing our results to be directly compared to their work <ref> [ScSo89, ScSo90] </ref>. 4.1. The Simulation Model The simulation model assumes an equal number of processors (P) and memory modules (M); in this study M and P are set to 256. The processors and memory modules are connected by a standard Omega network with 22 crossbar blocking switches. <p> Simulation Results The simulator was run under a variety of conditions, varying parameters such as memory queue size, feedback threshold, hot rates, degree of bleeding, etc. In discussing our results, we use the following variables, chosen to allow comparison with previous models <ref> [PfNo85, ScSo89, ScSo90] </ref>: N: The number of processors and memory modules. n: The number of levels in the network. (n = log 2 N).
Reference: [ScSo90] <author> S. L. Scott and G. S. Sohi, </author> <title> ``The Use of Feedback in Multiprocessors and Its Application to Tree Saturation Control'', </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. </volume> <month> 1:4 (October </month> <year> 1990), </year> <pages> pp. 385-399. </pages>
Reference-contexts: Requests to cold memory modules ("cold" requests) can encounter full queues; therefore, the latency of the cold requests is increased, and the level of congestion in the network is heightened <ref> [PoHa89, ScSo90] </ref>. This degradation is called "tree saturation" and is compounded by the fact that its onset can be rapid [KuPf86] and may take a long time to alleviate. <p> Unfortunately, because of the inherent latencies involved in the network, a new batch of requests may not arrive before the hot module services all of its queued requests (leaving the memory module idle). Scott and Sohi label this undershoot, an analogy from classic control systems theory <ref> [ScSo90] </ref>. Another problem with feedback occurs when several of the processors have saved a number of requests for a hot module. When the processors are notified that the module is again accepting requests, the stored requests will be simultaneously released by the processors, possibly flooding the network. <p> When the processors are notified that the module is again accepting requests, the stored requests will be simultaneously released by the processors, possibly flooding the network. This re-creation of tree saturation is termed overshoot <ref> [ScSo89, ScSo90] </ref>. 2.3. Limiting A limiting scheme assumes the presence of a global arbiter which controls the number of requests to each memory module that enter the network per cycle. <p> Damping Because hardware costs and complexity of a global arbiter are prohibitive, some other form of controlling requests to a hot module is desirable. A better solution <ref> [ScSo90] </ref> is to allow each processor to independently limit its requests to hot and/or cold memory modules, using some method that (with a high degree of probability) will reduce tree saturation. <p> It also provides a more gradual release of blocked requests into the network, reducing overshoot. 4. The Simulator In this study we used a slightly modified version of the model-driven simulator used by Scott and Sohi, thus allowing our results to be directly compared to their work <ref> [ScSo89, ScSo90] </ref>. 4.1. The Simulation Model The simulation model assumes an equal number of processors (P) and memory modules (M); in this study M and P are set to 256. The processors and memory modules are connected by a standard Omega network with 22 crossbar blocking switches. <p> The queue that lies between the final stage of the network and the memory modules is called the memory queue (mq) and has a variable length. A queue length of four is the same length selected by several previous studies <ref> [KuPf86, PfNo85, ScSo90] </ref>. <p> Simulation Results The simulator was run under a variety of conditions, varying parameters such as memory queue size, feedback threshold, hot rates, degree of bleeding, etc. In discussing our results, we use the following variables, chosen to allow comparison with previous models <ref> [PfNo85, ScSo89, ScSo90] </ref>: N: The number of processors and memory modules. n: The number of levels in the network. (n = log 2 N). <p> Effects of Increasing the Memory Module Queue Size queue at the memory module (the mq) with no feedback mechanism provided. The dashed line represents the theoretical degradation which would occur if tree saturation did not affect the bandwidth <ref> [ScSo90] </ref>, i.e. if all degradation could be attributed to the fact that the hot memory module can process only one request per cycle. As expected, increasing the size of the memory queue provides virtually no improvement in bandwidth. <p> We chose to simulate this higher hot rate on the basis of current trends in multiprocessing systems. As the number of processors in a system becomes larger, it is anticipated that the severity of tree saturation caused by a given hot rate will increase, leading to decreased network performance <ref> [ScSo90] </ref>. <p> A bus to allow the memory/network interface to communicate with the processor/network interface (PNI) each time a module changes temperature is slightly more complicated; since multiple transitions can occur per cycle <ref> [ScSo90] </ref>, the memory/network interface may have to wait for the bus. However, the bus will only require O (logN) wires to uniquely identify the memory module whose temperature has changed.
Reference: [Sieg85] <author> H. J. Siegel, </author> <title> Interconnection Networks for Large-Scale Parallel Processing: Theory and Case Studies, </title> <publisher> Lexington Books, </publisher> <address> Lexington, MA, </address> <year> (1985). </year>
Reference-contexts: Since these networks are hhhhhhhhhhhhhhhhhhhhhhhhhhhhh This work was supported by the National Science Foundation under Grant CCR-90-11535. inexpensive enough to be feasible, and provide sufficient bandwidth for many processors and memory modules to communicate simultaneously <ref> [Sieg85] </ref>, they will be the focus of this paper. Unfortunately, sharing memory leads to memory contention. Furthermore, the larger the number of processors, the greater the degree of contention [Lee89]. The result in multistage interconnection networks is an undesirable phenomenon known as hot spots [PfNo85].
Reference: [Ston87] <author> H. S. Stone, </author> <title> High-Performance Computer Architecture, </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> (1987). </year>
Reference-contexts: This degradation is called "tree saturation" and is compounded by the fact that its onset can be rapid [KuPf86] and may take a long time to alleviate. In addition, it takes very little nonuniform traffic to induce this condition <ref> [PfNo85, Ston87] </ref>. d d Switch 2x2 Switch 2x2 Switch 2x2 Switch P I N P I N Processor Processor Processor Processor Memory Memory Memory Memory mq mq q q q q Components of a Multistage Interconnection Network 2. Previous Approaches Several methods of controlling tree saturation have been proposed. <p> Larger Queues at the Memory Modules Several authors have suggested that larger queues at the memory modules could reduce the negative effects of nonuniform traffic. Stone has suggested using large queues at the memory modules in regular multistage interconnection networks (i.e. networks without feedback) <ref> [Ston87] </ref>. Although it has been shown that simply increasing memory queue sizes is not an effective method of controlling tree saturation [GGKM83, Lee85], this paper shows that this technique is significantly more useful when used in conjunction with feedback. 3.2.
Reference: [TaYe90] <author> P. Tang and P. Yew, </author> <title> ``Software Combining Algorithms for Distributing Hot-Spot Addressing'', </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 10 (1990), </volume> <pages> pp. 130-139. </pages>
Reference-contexts: Hot spots are locations (nodes within the network, memory modules, or specific addresses) which receive a nonuniform distribution of traffic. This concentration of traffic can be caused by events such as references to shared or global variables (e.g. for barrier synchronization or operations on semaphores <ref> [Lee85, TaYe90] </ref>); block transfers [Thom86]; a coincidental concentration of requests to a single memory module [Lee85, NoPf85]; or a coincidental concentration of traffic through an internal switch (nonuniform traffic spots, or NUTS [LaKu90]) which can occur even if the requests to memory are evenly distributed. <p> Most, however, have been impractical in terms of technology or expense. Three of these methods are described below. 2.1. Combining One commonly discussed approach to reducing tree saturation is to combine requests to the same address. Both hardware [GGKM83, LeK86, PfNo85] and software <ref> [TaYe90, YeT87] </ref> solutions have been proposed. These approaches reduce the number of requests to a hot memory location, as well as minimize network traffic.
Reference: [Thom86] <author> R. H. Thomas, </author> <title> ``Behavior of the Butterfly Parallel Processor in the Presence of Memory Hot Spots'', </title> <booktitle> International Conference on Parallel Processing(1986), </booktitle> <pages> pp. 46-50. </pages>
Reference-contexts: Hot spots are locations (nodes within the network, memory modules, or specific addresses) which receive a nonuniform distribution of traffic. This concentration of traffic can be caused by events such as references to shared or global variables (e.g. for barrier synchronization or operations on semaphores [Lee85, TaYe90]); block transfers <ref> [Thom86] </ref>; a coincidental concentration of requests to a single memory module [Lee85, NoPf85]; or a coincidental concentration of traffic through an internal switch (nonuniform traffic spots, or NUTS [LaKu90]) which can occur even if the requests to memory are evenly distributed.
Reference: [YeT87] <author> P. Yew, N. Tzeng and D. H. Lawrie, </author> <title> ``Distributing Hot-Spot Addressing in Large-Scale Multiprocessors'', </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. </volume> <month> C-36:4 (April </month> <year> 1987), </year> <pages> pp. 388-395. </pages> <address> d d </address>
Reference-contexts: Most, however, have been impractical in terms of technology or expense. Three of these methods are described below. 2.1. Combining One commonly discussed approach to reducing tree saturation is to combine requests to the same address. Both hardware [GGKM83, LeK86, PfNo85] and software <ref> [TaYe90, YeT87] </ref> solutions have been proposed. These approaches reduce the number of requests to a hot memory location, as well as minimize network traffic.
References-found: 17

