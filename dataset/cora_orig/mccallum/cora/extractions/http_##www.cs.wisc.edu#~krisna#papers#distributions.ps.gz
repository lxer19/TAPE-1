URL: http://www.cs.wisc.edu/~krisna/papers/distributions.ps.gz
Refering-URL: http://www.cs.wisc.edu/~krisna/papers/papers.html
Root-URL: 
Email: krishna@cs.wisc.edu bart@cs.wisc.edu  
Title: --Optimizing Array Distributions in Data-Parallel Programs  
Author: Krishna Kunchithapadam Barton P. Miller 
Address: 1210 W. Dayton Street Madison, WI 53706  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract: Data parallel programs are sensitive to the distribution of data across processor nodes. We formulate the reduction of inter-node communication as an optimization on a colored graph. We present a technique that records the run time inter-node communication caused by the movement of array data between nodes during execution and builds the colored graph, and provide a simple algorithm that optimizes the coloring of this graph to describe new data distributions that would result in less inter-node communication. From the distribution information, we write compiler pragmas to be used in the application program. Using these techniques, we traced the execution of a real data-parallel application (written in CM Fortran) and collected the array access information. We computed new distributions that should provide an overall reduction in program execution time. However, compiler optimizations and poor interfaces between the compiler and runtime systems counteracted any potential benefit from the new data layouts. In this context, we provide a set of recommendations for compiler writers that we think are needed to both write efficient programs and to build the next generation of tools for parallel systems. The techniques that we have developed form the basis for future work in monitoring array access patterns and generate on-the-fly redistributions of arrays. hhhhhhhhhhhhhhhhhh This work is supported in part by Wright Laboratory Avionics Directorate, Air Force Material Command, USAF, under grant F33615-94-1-1525 (ARPA order no. B550), NSF Grants CCR-9100968 and CDA-9024618, Department of Energy Grant DE-FG02-93ER25176, and Office of Naval Research Grant N00014-89-J-1222. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Wright Laboratory Avionics Directorate or the U.S. Government. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> W. D. Hillis and G. L. Steele, </author> <title> Data Parallel Algorithms, </title> <journal> Communications of the ACM, </journal> <month> December </month> <year> 1986, </year> <pages> 1170-1183. </pages>
Reference-contexts: Government. - --1. INTRODUCTION Parallel programming languages provide a programmer with abstractions that ease the development of code. Data parallel programming languages <ref> [1] </ref> allow the programmer to reason about a single thread of control that executes in parallel on a large collection of data. The mechanisms of parallel execution, communication and synchronization are all handled automatically by the compiler; the details are hidden from the programmer.
Reference: 2. <institution> CMFortran Reference Manual (Online document), Thinking Machines Corp. </institution> <note> Version 2.2.1-2 </note> . 
Reference: 3. <author> C*: </author> <title> C-star Reference Manual (Online document), </title> <institution> Thinking Machines Corp. </institution> <note> Version 7.1 </note> . 
Reference: 4. <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kramer and C. Tseng, </author> <title> Fortran-D Language Specification, </title> <type> Technical Report, </type> <institution> Computer TR90-141, Rice University, </institution> <year> 1990. </year>
Reference-contexts: We develop our initial work in the context of parallel arrays. Data distribution pragmas specify the details of the layout of a parallel array onto the nodes of a parallel machine. For example, CM Fortran (which we used in our study) provides block-structured layout pragmas. A language like Fortran-D <ref> [4] </ref> supports both block and cyclic layouts while the HPF language definition mentions arbitrary and dynamically changeable permutations of arrays. Usually, a programmer chooses a simple data layout scheme or lets the compiler choose a default layout.
Reference: 5. <institution> High Performance Fortran Language Specification, </institution> <note> High Performance Fortran Forum Version 1.0 (May 1993). </note> - <author> --6. U. Kremer, J. Mellor-Crummey, K. Kennedy and A. Carle, </author> <title> Automatic Data Layout for Distributed-Memory Machines in the D Programming Environment, </title> <type> Technical Report CRPC-TR93-298-S, </type> <institution> Rice University, </institution> . 
Reference-contexts: Our current analysis of array accesses is post-morten, but our goal is to be able to monitor and evaluate access patterns on-the-fly (while the program is running). In systems that support execution time data redistributions (such as described in the HPF specification <ref> [5] </ref>), programmers would be able to improve their array distributions automatically. 1.1. Data Distribution in Parallel programs Almost all parallel programming environments require programmers to specify the distribution of their data structures across the target hardware.
Reference: 7. <author> A. Rogers and K. Pingali, </author> <title> Process Decomposition Through Locality of Reference, </title> <booktitle> Proc. of the 1989 Conf. on Programming Language Design and Implementation, </booktitle> <address> Portland, Oregon, </address> <month> June </month> <year> 1989, </year> <pages> 69-80. </pages>
Reference-contexts: Edges of the graph represent assignments of values arising from part of one or more arrays to part of another array (assuming an owner-computes <ref> [7] </ref> model). This notion is made more precise below. Our approach to finding good data distributions is based on graph coloring with a specific conservation property on the graph. Our approach also weights communication costs over computation costs.
Reference: 8. <author> U. Kremer, </author> <title> NP-Completeness of Dynamic Remapping, </title> <booktitle> Proceedings of the Fourth International Workshop on Compilers for Parallel Computers, </booktitle> <month> December </month> <year> 1993, </year> <pages> 135-141. </pages>
Reference-contexts: The assumption is that reduced communication should result in improved program performance. While computing the optimal coloring of the graph is NP-complete <ref> [8] </ref> there are many possible heuristics that we can use. Simple greedy algorithms are preferable because they are fast. Other possibilities include genetic algorithms [9], simulated annealing [10] and algorithms based on optimization problems [11].
Reference: 9. <author> L. D. Whitley, </author> <title> Foundations of Genetic Algorithms, </title> <editor> M. </editor> <publisher> Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1993. </year>
Reference-contexts: The assumption is that reduced communication should result in improved program performance. While computing the optimal coloring of the graph is NP-complete [8] there are many possible heuristics that we can use. Simple greedy algorithms are preferable because they are fast. Other possibilities include genetic algorithms <ref> [9] </ref>, simulated annealing [10] and algorithms based on optimization problems [11]. The basic tradeoff is between the complexity of the algorithm and the quality of the solution. A simple, fast algorithm is more appropriate in the context of application steering.
Reference: 10. <author> D. S. Johnson, C. R. Aragon, L. A. McGeoch and C. Schevon, </author> <title> Optimization by Simulated Annealing: An Experimental Evaluation, </title> <booktitle> Operations Research 39, 3 (May-June 1991), </booktitle> <pages> 378-406. </pages>
Reference-contexts: The assumption is that reduced communication should result in improved program performance. While computing the optimal coloring of the graph is NP-complete [8] there are many possible heuristics that we can use. Simple greedy algorithms are preferable because they are fast. Other possibilities include genetic algorithms [9], simulated annealing <ref> [10] </ref> and algorithms based on optimization problems [11]. The basic tradeoff is between the complexity of the algorithm and the quality of the solution. A simple, fast algorithm is more appropriate in the context of application steering.
Reference: 11. <author> J. R. Evans and E. Minieka, </author> <title> Optimization Algorithms for Networks and Graphs, </title> <editor> M. </editor> <publisher> Dekker, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: While computing the optimal coloring of the graph is NP-complete [8] there are many possible heuristics that we can use. Simple greedy algorithms are preferable because they are fast. Other possibilities include genetic algorithms [9], simulated annealing [10] and algorithms based on optimization problems <ref> [11] </ref>. The basic tradeoff is between the complexity of the algorithm and the quality of the solution. A simple, fast algorithm is more appropriate in the context of application steering.
Reference: 12. <author> B. H. McCormick, T. A. DeFanti and M. D. Brown, </author> <title> Visualization in Scientific Computing, </title> <booktitle> Computer Graphics 21, </booktitle> <month> 6 (November </month> <year> 1987). </year>
Reference-contexts: The enumerations that result in weights closest to the computed coloring are candidates for further investigation. A third approach is to simply animate or visualize the data distributions produced by the graph coloring algorithm <ref> [12] </ref>. The programmer may be able to identify global patterns in a picture and map them to a array distribution pragma; block and cyclic distribution patterns may be especially easy to recognize in this manner. We expect that a practical tool would use some combination of the above techniques.
Reference: 13. <author> J. R. Larus and T. Ball, </author> <title> Rewriting Executable Files to Measure Program Behavior, </title> <journal> SoftwarePractice & Experience 24, </journal> <month> 2 (Feb, </month> <year> 1994), </year> <pages> 197-218. </pages>
Reference-contexts: Hand-instrumentation is tedious, but was used only for this initial study. With the availability of def/use information from the compiler, the instrumentation step is straight-forward to automate. Ultimately, we expect to use binary rewriting <ref> [13] </ref> or dynamic instrumentation [14]. All program versions were run on a 32-node partition of a CM-5 computer (without using the vector units). The applications were compiled with CM Fortran (version 2.1.1-2) with optimization enabled.
Reference: 14. <author> J. K. Hollingsworth, B. P. Miller and J. Cargille, </author> <title> Dynamic Program Instrumentation for Scalable Performance Tools, </title> <booktitle> 1994 Scalable High-Performance Computing Conf., </booktitle> <address> Knoxville, Tenn., </address> <year> 1994. </year>
Reference-contexts: Hand-instrumentation is tedious, but was used only for this initial study. With the availability of def/use information from the compiler, the instrumentation step is straight-forward to automate. Ultimately, we expect to use binary rewriting [13] or dynamic instrumentation <ref> [14] </ref>. All program versions were run on a 32-node partition of a CM-5 computer (without using the vector units). The applications were compiled with CM Fortran (version 2.1.1-2) with optimization enabled. The trace data collected from the instrumented runs was used to build our proximity graph. 4.2.
Reference: 15. <author> B. Kernighan and S. Lin, </author> <title> An efficient heuristic procedure for partitioning graphs, </title> <journal> Bell Systems Technical Journal 49 (1970), </journal> <pages> 291-307. </pages> - - 
Reference-contexts: Our coloring algorithm was written to be as fast as possible. Its heuristic is very similar to a restricted version of the Kernighan-Lin <ref> [15] </ref> optimization algorithm; we examine only 2-neighbors while the Kernighan-Lin algorithm examines arbitrary n-neighbors. Realizing a Distribution: Mapping a Coloring to Language Pragmas To map the distribution given by the optimized proximity graphs, we used a simple visualization approach to assist the programmer in identifying regular patterns in the distributions.
References-found: 14

