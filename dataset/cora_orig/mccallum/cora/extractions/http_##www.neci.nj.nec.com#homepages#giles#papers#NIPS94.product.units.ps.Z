URL: http://www.neci.nj.nec.com/homepages/giles/papers/NIPS94.product.units.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/html/CLG_pub.html
Root-URL: 
Email: laurens@sedal.su.oz.au  giles@research.nj.nec.com  horne@research.nj.nec.com  marwan@sedal.su.oz.au  
Title: Learning with Product Units  
Author: Laurens R. Leerink C. Lee Giles Bill G. Horne Marwan A. Jabri 
Address: Sydney NSW 2006, Australia  4 Independence Way Princeton, NJ 08540, USA  4 Independence Way Princeton, NJ 08540, USA  Sydney NSW 2006, Australia  
Affiliation: Department of Electrical Engineering The University of  NEC Research Institute  NEC Research Institute  Department of Electrical Engineering The University of  
Date: 1995  
Note: Published in Advances in Neural Information Processing Systems 7, p. 537, MIT Press,  
Abstract: Product units provide a method of automatically learning the higher-order input combinations required for efficient learning in neural networks. However, we show that problems are encountered when using backpropagation to train networks containing these units. This paper examines these problems, and proposes some atypical heuristics to improve learning. Using these heuristics a constructive method is introduced which solves well-researched problems with significantly less neurons than previously reported. Secondly, product units are implemented as candidate units in the Cascade Correlation ( Fahlman & Lebiere, 1990 ) system. This resulted in smaller networks which trained faster than when using sigmoidal or Gaussian units.
Abstract-found: 1
Intro-found: 1
Reference: <author> Ash, T. </author> <year> (1989). </year> <title> Dynamic node creation in backpropagation networks. </title> <journal> Connection Science, </journal> <volume> 1 (4), </volume> <pages> 365-375. </pages>
Reference-contexts: Freezing a subset of the weights restricts the new solution to an affine subset of the existing weight space, often resulting in non-minimal networks <ref> ( Ash, 1989 ) </ref> . For this reason a simple incremental method (SIM) was implemented which retains the global search for all weights during the whole training process. <p> Reinitialize weights and continue training with the BP-RSA combination. * Repeat process until a solution is found or the network has grown a prede termined maximum size. The method of <ref> ( Ash, 1989 ) </ref> was also evaluated, where neurons with small weights were added to a network according to certain criteria. The SIM performed better, possibly because of the global search performed by the RSA step.
Reference: <author> Cover, T. </author> <year> (1965). </year> <title> Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> 14, </volume> <pages> 326-334. </pages>
Reference-contexts: 1 Introduction It is well-known that supplementing the inputs to a neural network with higher-order combinations of the inputs both increases the capacity of the network <ref> ( Cover, 1965 ) </ref> and the the ability to learn geometrically invariant properties ( Giles & Maxwell, Published in Advances in Neural Information Processing Systems 7, p. 537, MIT Press, 1995 1987 ) . <p> An additional advantage of PUs is the increased information capacity of these units compared to standard summation networks. It is approximately 3N ( Durbin & Rumelhart, 1989 ) , compared to 2N for a single threshold logic function <ref> ( Cover, 1965 ) </ref> , where N is the number of inputs to the unit. The larger capacity means that the same functions can be implemented by networks containing less units.
Reference: <author> Dawson, M. & Schopflocher, D. </author> <year> (1992). </year> <title> Modifying the generalized delta rule to train networks of nonmonotonic processors for pattern classification. </title> <journal> Connection Science, </journal> <volume> 4, </volume> <pages> 19-31. </pages>
Reference-contexts: Thirdly, although the error surface of networks with PUs contains more local minima than when using standard transfer functions, the surface is locally smooth. This allows effective use of higher-order error derivatives, resulting in fast convergence by the quickprop algorithm. In <ref> ( Dawson & Schopflocher, 1992 ) </ref> it was shown that networks with Gaussian units train faster and require less units than networks with standard sigmoidal units. This is supported by our results shown in Table 3.
Reference: <author> Durbin, R. & Rumelhart, D. </author> <year> (1989). </year> <title> Product units: A computationally powerful and biologically plausible extension to backpropagation networks. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 133-142. </pages>
Reference-contexts: Yet in order to implement a certain logical function, in most cases only a few of these higher order terms are required ( Redding et al., 1993 ) . The product units (PUs) introduced by <ref> ( Durbin & Rumelhart, 1989 ) </ref> attempt to make use of this fact. These networks have the advantage that, given an appropriate training algorithm, the units can automatically learn the higher order terms that are required to implement a specific logical function. <p> An additional advantage of PUs is the increased information capacity of these units compared to standard summation networks. It is approximately 3N <ref> ( Durbin & Rumelhart, 1989 ) </ref> , compared to 2N for a single threshold logic function ( Cover, 1965 ) , where N is the number of inputs to the unit. The larger capacity means that the same functions can be implemented by networks containing less units. <p> This is important for certain applications such as speech recognition where the data bandwidth is high or if realtime implementations are desired. When PUs are used to process Boolean inputs, best performance is obtained <ref> ( Durbin & Rumelhart, 1989 ) </ref> by using inputs of f+1; 1g. If the imaginary component is ignored, with these inputs, the activation function is equivalent to a cosine summation function with f1; +1g inputs mapped f1; 0g. <p> This assumes that Boolean mappings are being implemented and the appropriate f1; +1g ! f1; 0g mapping has been performed on the input vectors. However, if we then attempt to train a network on on the parity-6 problem shown in <ref> ( Durbin & Rumelhart, 1989 ) </ref> , it is found that the standard backpropagation (BP) algorithm simply does not work. We have found two main reasons for this. The first is weight initialization. A typical first step in the backpropagation procedure is to initialize all weights to small random values. <p> The algorithms were avaluated on two problems, the parity problem and learning all logical functions of 2 and 3 inputs. The infamous parity problem is (for the product unit at least) an appropriate task. As illustrated by <ref> ( Durbin & Rumelhart, 1989 ) </ref> , this problem can be solved by one product unit. The question is whether the training algorithms can find a solution. The target values are f1; +1g, and the output is taken to be correct if it has the correct sign.
Reference: <author> Fahlman, S. & Lebiere, C. </author> <year> (1990). </year> <title> The cascade-correlation learning architecture. </title>
Reference-contexts: The values for the tiling and upstart algorithms are approximate and were obtained through inspection from a similar graph in ( Frean, 1990 ) . 5 Using Cosine Candidate Units in Cascade Correlation Initially we wanted to compare the performance of SIM with the well-known `cascade-correlation' (CC) algorithm of <ref> ( Fahlman & Lebiere, 1990 ) </ref> . However, the network architectures differ and a direct comparison between the number of units in the respective architectures does not reflect the efficiency of the algorithms. Instead, it was decided to integrate PUs into the CC system as candidate units.
Reference: <editor> In Touretzky, D. (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> (pp. 524-532)., </pages> <address> San Mateo. (Denver 1989), </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Yet in order to implement a certain logical function, in most cases only a few of these higher order terms are required ( Redding et al., 1993 ) . The product units (PUs) introduced by <ref> ( Durbin & Rumelhart, 1989 ) </ref> attempt to make use of this fact. These networks have the advantage that, given an appropriate training algorithm, the units can automatically learn the higher order terms that are required to implement a specific logical function. <p> An additional advantage of PUs is the increased information capacity of these units compared to standard summation networks. It is approximately 3N <ref> ( Durbin & Rumelhart, 1989 ) </ref> , compared to 2N for a single threshold logic function ( Cover, 1965 ) , where N is the number of inputs to the unit. The larger capacity means that the same functions can be implemented by networks containing less units. <p> This is important for certain applications such as speech recognition where the data bandwidth is high or if realtime implementations are desired. When PUs are used to process Boolean inputs, best performance is obtained <ref> ( Durbin & Rumelhart, 1989 ) </ref> by using inputs of f+1; 1g. If the imaginary component is ignored, with these inputs, the activation function is equivalent to a cosine summation function with f1; +1g inputs mapped f1; 0g. <p> This assumes that Boolean mappings are being implemented and the appropriate f1; +1g ! f1; 0g mapping has been performed on the input vectors. However, if we then attempt to train a network on on the parity-6 problem shown in <ref> ( Durbin & Rumelhart, 1989 ) </ref> , it is found that the standard backpropagation (BP) algorithm simply does not work. We have found two main reasons for this. The first is weight initialization. A typical first step in the backpropagation procedure is to initialize all weights to small random values. <p> The algorithms were avaluated on two problems, the parity problem and learning all logical functions of 2 and 3 inputs. The infamous parity problem is (for the product unit at least) an appropriate task. As illustrated by <ref> ( Durbin & Rumelhart, 1989 ) </ref> , this problem can be solved by one product unit. The question is whether the training algorithms can find a solution. The target values are f1; +1g, and the output is taken to be correct if it has the correct sign.
Reference: <author> Frean, M. </author> <year> (1990). </year> <title> The upstart algorithm: A method for constructing and training feedforward neural networks. </title> <journal> Neural Computation, </journal> <volume> 2, </volume> <pages> 198-209. </pages>
Reference-contexts: The method of ( Ash, 1989 ) was also evaluated, where neurons with small weights were added to a network according to certain criteria. The SIM performed better, possibly because of the global search performed by the RSA step. The `upstart' <ref> ( Frean, 1990 ) </ref> and `tiling' ( Mezard & Nadal, 1989 ) constructive algorithms were chosen as benchmarks. A constructive PU network was trained on two problems described in these papers, namely the parity problem and the random mapping problem. In ( Frean, 1990 ) it was reported that the upstart <p> The `upstart' <ref> ( Frean, 1990 ) </ref> and `tiling' ( Mezard & Nadal, 1989 ) constructive algorithms were chosen as benchmarks. A constructive PU network was trained on two problems described in these papers, namely the parity problem and the random mapping problem. In ( Frean, 1990 ) it was reported that the upstart algorithm Published in Advances in Neural Information Processing Systems 7, p. 537, MIT Press, 1995 required N units for all parity N problems, and 1,000 training epochs were sufficient for all values of N except N = 10, which required 10,000. <p> The number of units required by SIM is plotted in Figure 1. The values for the tiling and upstart algorithms are approximate and were obtained through inspection from a similar graph in <ref> ( Frean, 1990 ) </ref> . 5 Using Cosine Candidate Units in Cascade Correlation Initially we wanted to compare the performance of SIM with the well-known `cascade-correlation' (CC) algorithm of ( Fahlman & Lebiere, 1990 ) .
Reference: <author> Giles, C. & Maxwell, T. </author> <year> (1987). </year> <title> Learning, invariance, and generalization in high-order neural networks. </title> <journal> Applied Optics, </journal> <volume> 26 (23), </volume> <pages> 4972-4978. </pages>
Reference: <author> Jacobs, R. </author> <year> (1988). </year> <title> Increased rates of convergence through learning rate adaptation. </title> <booktitle> Neural Networks, </booktitle> <volume> 1, </volume> <pages> 295-307. </pages>
Reference-contexts: BP was used as a benchmark and for use in combination with the other algorithms. The Delta-Bar-Delta learning rate adaptation rule <ref> ( Jacobs, 1988 ) </ref> was used along with the batch version of BP to accelerate convergence, with the parameters were set to = 0:35; = 0:05 and = 0:90. RSA is a global search method (i.e. the whole weight space is explored during training).
Reference: <author> Karmarkar, N. & Karp, R. </author> <year> (1982). </year> <title> The differencing method of set partitioning. </title> <type> Technical Report UCB/CSD 82/113, </type> <institution> Computer Science Division, University of California, Berkeley, California. </institution>
Reference-contexts: As discussed earlier, this puts the PUs at a slight disadvantage as their optimum range is [2; +2]. 6 Discussion The BP-RSA combination is in effect equivalent to the `local optimization with random restarts' process discussed by <ref> ( Karmarkar & Karp, 1982 ) </ref> , where the local optimization is this case is performed by the BP algorithm. They reported that for certain problems where the error surface was `exceedingly mountainous', multiple random-start local optimization outperformed more sophisticated methods. <p> This again is in accordance with the results of <ref> ( Karmarkar & Karp, 1982 ) </ref> discussed above. When used in CC we hypothesize that there are three main reasons for the choice of PUs above any of the other types during the competitive learning phase.
Reference: <author> Kirkpatrick, S., Jr., C. G., , & Vecchi, M. </author> <year> (1983). </year> <title> Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220. </volume>
Reference-contexts: RSA is a global search method (i.e. the whole weight space is explored during training). Weights are randomly chosen from a predefined distribution, and replaced if this results in an error decrease. SA <ref> ( Kirkpatrick et al., 1983 ) </ref> is a standard optimization method. The operation of SA is similar to RSA, with the difference that with a decreasing probability solutions are accepted which increase the training error.
Reference: <author> Lapedes, A. & Farber, R. </author> <year> (1987). </year> <title> Nonlinear signal processing using neural networks: Prediction and system modelling. </title> <type> Technical Report LA-UR-87-2662, </type> <institution> Los Alamos National Laboratory, </institution> <address> Los Alamos, NM. </address>
Reference-contexts: In our simulations weights were initialized randomly in the range [2; 2]. In fact, learning seems insensitive to the size of the weights, as long as they are large enough. The second problem is local minima. Previous reports have mentioned this problem, <ref> ( Lapedes & Farber, 1987 ) </ref> commented that "using sin's often leads to numerical problems, and nonglobal minima, whereas sigmoids seemed to avoid such problems". This comment summarizes our experience of training with PUs. For small problems (less than 3 inputs) backpropagation provides satisfactory training.
Reference: <author> Mezard, M. & Nadal, J.-P. </author> <year> (1989). </year> <title> Learning in feedforward layered networks: The tiling algorithm. </title> <journal> Journal of Physics A, </journal> <volume> 22, </volume> <pages> 2191-2204. </pages>
Reference-contexts: The method of ( Ash, 1989 ) was also evaluated, where neurons with small weights were added to a network according to certain criteria. The SIM performed better, possibly because of the global search performed by the RSA step. The `upstart' ( Frean, 1990 ) and `tiling' <ref> ( Mezard & Nadal, 1989 ) </ref> constructive algorithms were chosen as benchmarks. A constructive PU network was trained on two problems described in these papers, namely the parity problem and the random mapping problem.
Reference: <author> Redding, N., Kowalczyk, A., & Downs, T. </author> <year> (1993). </year> <title> A constructive higher order network algorithm that is polynomial-time. </title> <booktitle> Neural Networks, </booktitle> <volume> 6, </volume> <pages> 997. </pages>
Reference-contexts: However, there is a combinatorial explosion of higher order terms as the number of inputs to the network increases. Yet in order to implement a certain logical function, in most cases only a few of these higher order terms are required <ref> ( Redding et al., 1993 ) </ref> . The product units (PUs) introduced by ( Durbin & Rumelhart, 1989 ) attempt to make use of this fact.
Reference: <author> White, M. </author> <year> (1993). </year> <title> A public domain C implemention of the Cascade Correlation algorithm. </title> <institution> Department of Computer Science, Carnegie Mellon University, </institution> <address> Pitts-burgh, PA. </address>
Reference-contexts: Instead, it was decided to integrate PUs into the CC system as candidate units. For these simulations a public domain version of CC was used <ref> ( White, 1993 ) </ref> which supports four different candidate types; the asymmetric sigmoid, symmetric sigmoid, variable sigmoid and gaussian units. Facilities exist for either constructing homogeneous networks by selecting one unit type, or training with a pool of different units allowing the construction of hybrid networks.
References-found: 15

