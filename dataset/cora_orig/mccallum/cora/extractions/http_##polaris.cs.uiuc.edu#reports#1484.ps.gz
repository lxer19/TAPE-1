URL: http://polaris.cs.uiuc.edu/reports/1484.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: HARDWARE AND COMPILER SUPPORT FOR CACHE COHERENCE IN LARGE-SCALE SHARED-MEMORY MULTIPROCESSORS  
Author: BY LYNN CHOI 
Degree: 1986 M.S., Seoul National University, 1988 THESIS Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Computer Science in the Graduate College of the  
Address: 1996 Urbana, Illinois  
Affiliation: B.S., Seoul National University,  University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. V. Adve, V. S. Adve, M. D. Hill, and M. K. Vernon. </author> <title> Comparison of Hardware and Software Cache Coherence Schemes. </title> <booktitle> Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 298-308, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Lilja [37] compared the performance of the version control scheme [15] and directory schemes, and analyzed the directory overhead of several implementations. Both studies showed that the performance of those HSCD schemes can be comparable to that of directory schemes. Agarwal and Adve <ref> [1] </ref> used an analytic model to compare the performance of compiler-directed and directory-based techniques. They concluded that the performance of compiler-directed schemes depends on the characteristics of the workloads.
Reference: [2] <author> A. Agarwal et al. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <booktitle> Proceedings of Workshop on Scalable Shared Memory Multiprocessors, </booktitle> <year> 1991. </year>
Reference-contexts: Having multiple cached copies of a shared memory location, however, can lead to erroneous program behavior unless they are maintained coherent. Existing solutions for large-scale multiprocessors include hardware directory-based coherence protocols, which have been studied in many research machines <ref> [2, 35, 36] </ref>. Although these hardware schemes can precisely identify stale data by maintaining sharing information at runtime, they substantially increase hardware cost for directory storage and require complex directory and cache controllers. <p> Since the address tag overhead is much more expensive than the data storage, 8 the extra hardware overhead for TPI scheme is reasonably small. Table 3.1 compares the storage overhead of our TPI scheme with a full-map directory scheme [9] as well as the LimitLess directory scheme <ref> [2] </ref>. It shows both the cache (SRAM) and memory overhead (DRAM) in terms of the cache line size (L), node cache size (C), node memory size (M) and number of processors (P).
Reference: [3] <author> J. Archibald and J. Baer. </author> <title> An Economical Solution to the Cache Coherence Problem. </title> <booktitle> Proceedings of The 11th Annual International Symposium on Computer Architectur, </booktitle> <pages> pages 355-362, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: This coherence mechanism is similar to that of the Cray T3D, where the compiler generates noncacheable loads for shared memory references and cacheable loads for private references. 2. Full-map directory scheme (HW) This scheme uses a simple, three-state (invalid, read-shared, write-exclusive) invalidation-based protocol with a full-map directory <ref> [9, 3] </ref> but without broadcasting.
Reference: [4] <author> R. Ballance, A. Maccabe, and K. Ottenstein. </author> <title> The Program Dependence Web: a Representation Supporting Control Data- and Demand-Driven Interpretation of Imperative Languages. </title> <booktitle> Proceedings of the SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 257-271, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: We use a combination of interval and data-flow analysis techniques to determine memory reference patterns that can lead to stale data accesses. To obtain more precise array access information, we compute the array region accessed by each array reference. Gated single assignment (GSA) <ref> [4] </ref> form is used to compute equality and to compare the array regions involving symbolic expressions. Two key analysis techniques are used to identify potentially stale references: stale reference pattern detection and locality preserving analysis. <p> We use the demand-driven symbolic analysis using GSA form <ref> [4] </ref>. * Interprocedural analysis Previous HSCD schemes invalidate the entire cache at procedure boundaries to avoid side effects caused by procedure calls. We use a complete interprocedural analysis to avoid such invalidations and to exploit locality across procedure boundaries. <p> First, we construct a procedure call graph. Then, 42 based on the bottom-up scan of the call graph, we analyze each procedure and propagate its side effects to its callers. The per-procedure analysis is based on the following steps. First, we transform the source program into GSA form <ref> [4] </ref>. Then, we construct a modified flow graph, called the epoch flow graph [21]. It contains the epoch boundary information as well as the control flows of the program. <p> Static single assignment (SSA) [24] is a representation of a program in which each use of a variable is reached by exactly a single definition of the variable. It allows us to track the value of a variable by its name. Gated single assignment (GSA) <ref> [4] </ref> introduces three types of pseudo-assignment functions, which are extensions of the functions used in SSA: * fl (cond, value1, value2) : for function located immediately after an IF state ment.
Reference: [5] <author> M. Berry and others. </author> <title> The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall, </month> <year> 1989. </year>
Reference-contexts: We use six programs from the Perfect Club benchmark suite <ref> [5] </ref> as our target benchmarks. They are first parallelized by the Polaris compiler. In the parallelized code, the parallelism is expressed in terms of DOALL loops. We then mark the Time-Read operations in the parallelized source codes using our compiler algorithms, which are also implemented in the Polaris compiler. <p> The necessary compiler algorithms, including intra- and interprocedural array data flow analysis, have been implemented on the Polaris paralleling compiler [43]. The results of our simulation study using the Perfect Club Benchmarks <ref> [5] </ref> show that both hardware directory schemes and the TPI scheme have comparable number of unnecessary cache misses. In hardware schemes these misses result from the false-sharing 123 effect while in our proposed scheme they come from the conservative assumptions made by the compiler.
Reference: [6] <author> W. C. Brantley, K. P. McAuliffe, and J. Weiss. </author> <title> RP3 processor-memory element. </title> <booktitle> Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <pages> pages 782-789, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: Therefore, many commercially available large-scale multiprocessors, such as the Cray T3D [31] and the Intel Paragon [23], do not provide hardware-coherent caches. In several early multiprocessor systems, such as the CMU C.mmp [50], the NYU Ultracomputer [29], the IBM RP3 <ref> [6] </ref>, and the Illinois Cedar [34], compiler-directed techniques were used to solve the cache coherence problem. In this approach, cache coherence 1 is maintained locally without the need for interprocessor communication or hardware di-rectories. <p> The cache coherence scheme in the Ultracomputer is an improvement over that of the C.mmp. But it is still quite conservative. IBM RP3 The IBM RP3 <ref> [6] </ref> uses a similar strategy as the NYU Ultracomputer to enforce cache coherence. The high-level programming languages available on the RP3 16 allow the programmer to declare data as private or shared. The compiler determines the cacheability of data by assigning two special attributes to data: cacheability and volatility. <p> This dissertation investigates a hardware-supported, compiler-directed (HSCD) cache coherence scheme, called the two-phase invalidation (TPI) scheme, which relies mostly on compiler analysis, yet also provides a reasonable amount of hardware support. This approach has a long history of predecessors, including C.mmp [50], IBM's RP3 <ref> [6] </ref>, Illinois Cedar [34], and several recently proposed schemes [12, 14, 15, 21, 27, 38, 39]. 122 off-chip implementation of the TPI scheme, HW : full-map hardware directory scheme, SC: software cache-bypass scheme, BASE : base underlying machine with no coherence support.
Reference: [7] <institution> IBM Inc. </institution> <note> Edited by C. May, </note> <author> E. Silha, R. Simpson, and H. Warren. </author> <title> The PowerPC Architecture: A Specification for a New Family of RISC Processors. </title> <month> March </month> <year> 1995. </year>
Reference-contexts: Similarly, the Flush instruction (DCBF) can be used for the IBM PowerPC 600 series microprocessor <ref> [7] </ref>. 2 It is called two-phase invalidation because of its hardware reset mechanism on epoch counter overflow. 35 each epoch by every processor individually.
Reference: [8] <author> D. Callahan and K. Kennedy. </author> <title> Analysis of Interprocedural Side Effects in a Parallel Programming Environment. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 517-550, </pages> <year> 1988. </year>
Reference-contexts: A subarray consists of a subscripted variable and one or more ranges for some of the indices in its subscript expression. A range is represented by a lower bound, a upper bound and a stride. The notion of a subarray is an extension to the regular section used in <ref> [8] </ref>.
Reference: [9] <author> L. M. Censier and P. Feautrier. </author> <title> A New Solution to Coherence Problems in Multicache Systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-27(12):1112-1118, </volume> <month> December, </month> <year> 1978. </year>
Reference-contexts: since each access unit (such as a character, integer, or floating point data) is a distinct variable analyzed by the compiler. 7 Our experimental results in chapter 6 show that a 4-bit or 8-bit timetag is large enough to achieve a very good performance. 47 Full-map LimitLess Directory Two-phase Directory <ref> [9] </ref> (bits) (DIR NB i )[2] (bits) Invalidation (bits) Cache Overhead (SRAM) 2*C*P 2*C*P 8*L*C*P Memory Overhead (DRAM) (P+2)*M*P (i+2)*M*P None Total (P = 1024, i = 10) 4MB SRAM / 4MB SRAM / 64MB SRAM only 64.5GB DRAM 3GB DRAM Table 3.1: Cache and memory overhead of different hardware cache <p> Since the address tag overhead is much more expensive than the data storage, 8 the extra hardware overhead for TPI scheme is reasonably small. Table 3.1 compares the storage overhead of our TPI scheme with a full-map directory scheme <ref> [9] </ref> as well as the LimitLess directory scheme [2]. It shows both the cache (SRAM) and memory overhead (DRAM) in terms of the cache line size (L), node cache size (C), node memory size (M) and number of processors (P). <p> This coherence mechanism is similar to that of the Cray T3D, where the compiler generates noncacheable loads for shared memory references and cacheable loads for private references. 2. Full-map directory scheme (HW) This scheme uses a simple, three-state (invalid, read-shared, write-exclusive) invalidation-based protocol with a full-map directory <ref> [9, 3] </ref> but without broadcasting.
Reference: [10] <author> Y.-C. Chen and A. Veidenbaum. </author> <title> Comparison and Analysis of Software and Directory Coherence Schemes. </title> <booktitle> Proceedings Supercomputing'91, </booktitle> <pages> pages 818-829, </pages> <month> November </month> <year> 1991. </year> <month> 125 </month>
Reference: [11] <author> Yung-Chin Chen. </author> <title> Cache Design and Performance in a Large-Scale Shared-Memory Multiprocessor System. </title> <type> Technical report, </type> <institution> Univ. of Illinois, Dept. of Elec. Eng ., 1993. </institution> <type> Ph.D. Thesis. </type>
Reference-contexts: Agarwal and Adve [1] used an analytic model to compare the performance of compiler-directed and directory-based techniques. They concluded that the performance of compiler-directed schemes depends on the characteristics of the workloads. Chen <ref> [11] </ref> showed that a simple invalidation scheme [49] can achieve performance comparable to that of a directory scheme and discussed several different write policies. Most of these studies, however, assumed perfect compile-time memory disambiguation and complete control dependence information. <p> However, this will produce more redundant write traffic. By organizing the write buffer as a cache, as in the DEC Alpha 21164 processor [22], such redundant write traffic can be reduced effectively <ref> [11] </ref>. Note that ordinary write buffers can help hide latencies but cannot eliminate redundant write traffic. For a write-back cache configuration, TPI scheme should force all global writes to be written back to the main memory at synchronization points. <p> A write-through write-allocate policy is used for both TPI and SC schemes, while a write-back cache is used for the hardware directory protocol. These write policies are chosen to deliver the best performance for each type of coherence scheme <ref> [11] </ref>. A weak consistency model is used for all the coherence schemes. It is assumed that each processor can handle basic arithmetic and logical operations in one cycle and that it has synchronization operations to support parallel language constructs. <p> Therefore, redundant writes are not merged. The use of write buffers will decrease only the stall times of the CPU, and not the network traffic. The assumption of the infinite write buffer will decrease the CPU stall times during the simulation compared to a fixed size write buffer. Chen <ref> [11] </ref> studied the issue of write buffer design for the compiler-directed schemes and found that 8 words of write merging write buffers will reduce the traffic significantly, and that the write through with the write merging write buffer is a better choice than a write back cache implementation for compiler-directed schemes. <p> This additional write traffic can be eliminated 4 Although compiler-directed schemes can employ write-back at task boundaries, it increases the latency of the invalidation and results in more bursty traffic <ref> [11] </ref>. 104 Program Average Miss Latency Two-Phase Invalidation Hardware Directory 16 bytes 64 bytes 16 bytes 64 bytes SPEC77 136.2 356.3 136.4 355.5 OCEAN 136.2 354.3 136.4 353.6 FLO52 136.2 355.1 136.6 361.2 QCD 136.0 354.7 145.5 405.4 TRFD 136.0 352.4 149.1 418.6 Table 6.4: Average miss penalty of TPI and <p> 64 bytes 16 bytes 64 bytes SPEC77 136.2 356.3 136.4 355.5 OCEAN 136.2 354.3 136.4 353.6 FLO52 136.2 355.1 136.6 361.2 QCD 136.0 354.7 145.5 405.4 TRFD 136.0 352.4 149.1 418.6 Table 6.4: Average miss penalty of TPI and HW schemes. effectively by organizing a write buffer as a cache <ref> [11] </ref>. A similar technique can also be employed to remove redundant write traffic for update-based coherence protocols. The third type of network traffic is for coherence transactions in the directory protocol. This extra traffic is relatively small compared to the read and write traffic for the benchmarks considered.
Reference: [12] <author> H. Cheong. </author> <title> Life Span Strategy A Compiler-Based Approach to Cache Coherence. </title> <booktitle> Proceedings of the 1992 International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1992. </year>
Reference-contexts: The Cedar uses a shared cache to avoid coherence problems within each cluster. Data coherence is maintained among clusters by moving data explicitly between cluster and global memories using data movement instructions. Several compiler-directed cache coherence schemes <ref> [12, 14, 15, 16, 21, 27, 38, 39] </ref> have been proposed recently. These schemes give better performance, but demand more hardware and compiler support than the previous schemes. <p> Several compiler-directed cache coherence schemes [12, 14, 15, 16, 21, 27, 38, 39] have been proposed recently. These schemes give better performance, but demand more hardware and compiler support than the previous schemes. They require a more precise program analysis to maintain coherence on a reference basis <ref> [12, 13, 21] </ref> as opposed to the program region basis used by previous schemes. In addition, these schemes require hardware support to maintain local runtime cache states. In this regard, the terminology software cache coherence is a misnomer. It is a hardware approach with strong compiler support. <p> The fast selective invalidation scheme [14] combines the techniques of prevention and detection, and extends the simple invalidation scheme to exploit intertask locality for safe (e.g., read-only) accesses. The life span strategy <ref> [12] </ref>, Peir and So's scheme [44], and the generational algorithm [16] allow invalidation on a per-variable basis. In these schemes, a variable is not invalidated until the next write to that variable occurs in a different task. <p> (logN) epoch counter avoidance spatial increment SI : Simple Invalidation [49], FSI : Fast Selective Invalidation [14] CKM : Cytron, Karlovsky, and McAuliffe [25], CTV : Coherence Through Vectorization [26] VC : Version Control [15], TS : Timestamp-based [39] PEI : Parallel Explicit Invalidation [38], LSS : Life Span Strategy <ref> [12] </ref> GS : Generational Scheme [16], TS1 : Timestamp with 1 bit [27] TPI : Two-phase invalidation [21] Table 2.2: Comparison of existing HSCD cache coherence schemes. 20 detection. A flow analysis technique [13] is used to mark those potentially stale references at compile time. <p> Finally, it suffers the same problem as previous compiler-directed cache coherence schemes by making the unrealistic assumption of single-word cache lines. Life span strategy Cheong developed the Life Span Strategy (LSS) <ref> [12] </ref>, which can exploit intertask locality at a much lower hardware cost than the VC and TS schemes. <p> The guarded execution technique can be used to further optimize code generation. The compiler marking algorithms developed here are general enough to be applicable to other compiler-directed coherence schemes <ref> [12, 14] </ref>. 43 3.4 Hardware implementation issues Off-chip secondary cache implementation Since most of today's multiprocessors use off-the-shelf microprocessors, it would be more cost effective if the proposed TPI scheme can directly be implemented using existing microprocessors. cache. <p> This approach has a long history of predecessors, including C.mmp [50], IBM's RP3 [6], Illinois Cedar [34], and several recently proposed schemes <ref> [12, 14, 15, 21, 27, 38, 39] </ref>. 122 off-chip implementation of the TPI scheme, HW : full-map hardware directory scheme, SC: software cache-bypass scheme, BASE : base underlying machine with no coherence support.
Reference: [13] <author> H. Cheong and A. Veidenbaum. </author> <title> Stale Data Detection and Coherence Enforcement Using Flow Analysis. </title> <booktitle> Proceedings of the 1988 International Conference on Parallel Processing, I, </booktitle> <address> Architecture:138-145, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: Several compiler-directed cache coherence schemes [12, 14, 15, 16, 21, 27, 38, 39] have been proposed recently. These schemes give better performance, but demand more hardware and compiler support than the previous schemes. They require a more precise program analysis to maintain coherence on a reference basis <ref> [12, 13, 21] </ref> as opposed to the program region basis used by previous schemes. In addition, these schemes require hardware support to maintain local runtime cache states. In this regard, the terminology software cache coherence is a misnomer. It is a hardware approach with strong compiler support. <p> Compiler-directed coherence schemes maintain coherence through detection, prevention, or avoidance of stale references. The existing compiler-directed coherence schemes use combinations of these techniques (see Table 2.2). Stale reference detection refers to the use of compile time analysis to identify potential references to stale data <ref> [13] </ref>. By identifying these potential stale references at compile time, the system can be forced to get up-to-date data directly from the main memory instead of from the cache. Stale reference prevention techniques invalidate or update stale cache entries before stale references occur. <p> A flow analysis technique <ref> [13] </ref> is used to mark those potentially stale references at compile time. The compiler generates memory-reads for these references to access up-to-date data from the main memory directly. <p> For a target reference which does not have a reaching definition inside a procedure, we issue a Time-Read with the minimum offset, implying that the referenced data item can be potentially modified before entering the procedure. This is an improvement over previous algorithms <ref> [13, 17] </ref> that use cache invalidation at the beginning of a procedure since only global and formal variables are affected by the unknown context information. We propagate definitions through the flow graph and increment their offsets when they cross scheduling edges.
Reference: [14] <author> H. Cheong and A. Veidenbaum. </author> <title> A Cache Coherence Scheme with Fast Selective Invalidation. </title> <booktitle> Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: The Cedar uses a shared cache to avoid coherence problems within each cluster. Data coherence is maintained among clusters by moving data explicitly between cluster and global memories using data movement instructions. Several compiler-directed cache coherence schemes <ref> [12, 14, 15, 16, 21, 27, 38, 39] </ref> have been proposed recently. These schemes give better performance, but demand more hardware and compiler support than the previous schemes. <p> Simple invalidation [49] maintains cache coherence by invalidating the entire cache at task boundaries. If no read-write dependences exist among processors (e.g., during the parallel execution of a DOALL loop), no stale references will occur in the processors. The fast selective invalidation scheme <ref> [14] </ref> combines the techniques of prevention and detection, and extends the simple invalidation scheme to exploit intertask locality for safe (e.g., read-only) accesses. The life span strategy [12], Peir and So's scheme [44], and the generational algorithm [16] allow invalidation on a per-variable basis. <p> Although this indiscriminate invalidation is easy to implement efficiently, it is too conservative and leads to unnecessary cache misses since neither temporal nor spatial locality is preserved across task boundaries. Cheong and Veidenbaum proposed a Fast Selective Invalidation (FSI) scheme <ref> [14] </ref> which improves upon the simple invalidation scheme by using compile time stale reference 19 Scheme Coherence Intertask Storage Runtime Technique locality Overhead overhead SI prevention none O (1) invalidate FSI detection, read-only O (1) invalidate prevention data CKM detection, read-only O (1) update, flush, prevention data invalidate CTV detection, read-only <p> between arrays invalidate LSS detection, temporal O (N) invalidate prevention GS prevention temporal O (logN) VGN lookup and update TS1 prevention temporal 1 epoch bit per associative cache line invalidate TPI detection, temporal, O (logN) epoch counter avoidance spatial increment SI : Simple Invalidation [49], FSI : Fast Selective Invalidation <ref> [14] </ref> CKM : Cytron, Karlovsky, and McAuliffe [25], CTV : Coherence Through Vectorization [26] VC : Version Control [15], TS : Timestamp-based [39] PEI : Parallel Explicit Invalidation [38], LSS : Life Span Strategy [12] GS : Generational Scheme [16], TS1 : Timestamp with 1 bit [27] TPI : Two-phase invalidation <p> The guarded execution technique can be used to further optimize code generation. The compiler marking algorithms developed here are general enough to be applicable to other compiler-directed coherence schemes <ref> [12, 14] </ref>. 43 3.4 Hardware implementation issues Off-chip secondary cache implementation Since most of today's multiprocessors use off-the-shelf microprocessors, it would be more cost effective if the proposed TPI scheme can directly be implemented using existing microprocessors. cache. <p> This approach has a long history of predecessors, including C.mmp [50], IBM's RP3 [6], Illinois Cedar [34], and several recently proposed schemes <ref> [12, 14, 15, 21, 27, 38, 39] </ref>. 122 off-chip implementation of the TPI scheme, HW : full-map hardware directory scheme, SC: software cache-bypass scheme, BASE : base underlying machine with no coherence support.
Reference: [15] <author> H. Cheong and A. Veidenbaum. </author> <title> A Version Control Approach To Cache Coherence. </title> <booktitle> Proceedings of the 1989 ACM/SIGARCH International Conference on Supercomputing, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: The Cedar uses a shared cache to avoid coherence problems within each cluster. Data coherence is maintained among clusters by moving data explicitly between cluster and global memories using data movement instructions. Several compiler-directed cache coherence schemes <ref> [12, 14, 15, 16, 21, 27, 38, 39] </ref> have been proposed recently. These schemes give better performance, but demand more hardware and compiler support than the previous schemes. <p> Several studies have compared the performance of directory schemes and some recent HSCD schemes. Min and Baer [40] compared the performance of a directory scheme and a timestamp-based scheme, assuming infinite cache size and single-word cache lines. Lilja [37] compared the performance of the version control scheme <ref> [15] </ref> and directory schemes, and analyzed the directory overhead of several implementations. Both studies showed that the performance of those HSCD schemes can be comparable to that of directory schemes. Agarwal and Adve [1] used an analytic model to compare the performance of compiler-directed and directory-based techniques. <p> Stale reference avoidance is a more relaxed coherence model than the prevention technique. Stale reference avoidance allows the existence of stale data while avoiding stale accesses to them at runtime. Examples of this approach include the timestamp-based scheme [39] and the version control scheme <ref> [15] </ref>. In such schemes, stale data can exist until an access to the data occurs. At that time, by checking the status of the data in the cache, it is decided whether an up-to-date copy is brought in from the main memory. <p> update TS1 prevention temporal 1 epoch bit per associative cache line invalidate TPI detection, temporal, O (logN) epoch counter avoidance spatial increment SI : Simple Invalidation [49], FSI : Fast Selective Invalidation [14] CKM : Cytron, Karlovsky, and McAuliffe [25], CTV : Coherence Through Vectorization [26] VC : Version Control <ref> [15] </ref>, TS : Timestamp-based [39] PEI : Parallel Explicit Invalidation [38], LSS : Life Span Strategy [12] GS : Generational Scheme [16], TS1 : Timestamp with 1 bit [27] TPI : Two-phase invalidation [21] Table 2.2: Comparison of existing HSCD cache coherence schemes. 20 detection. <p> The result of applying the CTV scheme to the previous matrix multiply example is shown in Figure 2.2 (b). This scheme also assumes single-word cache lines. Version control and timestamp-based schemes The Version Control (VC) <ref> [15] </ref> and Timestamp-based (TS) [39] schemes use additional hardware support to improve the exploitation of intertask temporal locality. Although these two schemes were independently proposed, they are actually very similar in nature. <p> This approach has a long history of predecessors, including C.mmp [50], IBM's RP3 [6], Illinois Cedar [34], and several recently proposed schemes <ref> [12, 14, 15, 21, 27, 38, 39] </ref>. 122 off-chip implementation of the TPI scheme, HW : full-map hardware directory scheme, SC: software cache-bypass scheme, BASE : base underlying machine with no coherence support.
Reference: [16] <author> T. Chiueh. </author> <title> A Generational Approach to Software-Controlled Multiprocessor Cache Coherence. </title> <booktitle> Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: The Cedar uses a shared cache to avoid coherence problems within each cluster. Data coherence is maintained among clusters by moving data explicitly between cluster and global memories using data movement instructions. Several compiler-directed cache coherence schemes <ref> [12, 14, 15, 16, 21, 27, 38, 39] </ref> have been proposed recently. These schemes give better performance, but demand more hardware and compiler support than the previous schemes. <p> The fast selective invalidation scheme [14] combines the techniques of prevention and detection, and extends the simple invalidation scheme to exploit intertask locality for safe (e.g., read-only) accesses. The life span strategy [12], Peir and So's scheme [44], and the generational algorithm <ref> [16] </ref> allow invalidation on a per-variable basis. In these schemes, a variable is not invalidated until the next write to that variable occurs in a different task. The compiler identifies the task level at which the cache copy needs to be invalidated. <p> increment SI : Simple Invalidation [49], FSI : Fast Selective Invalidation [14] CKM : Cytron, Karlovsky, and McAuliffe [25], CTV : Coherence Through Vectorization [26] VC : Version Control [15], TS : Timestamp-based [39] PEI : Parallel Explicit Invalidation [38], LSS : Life Span Strategy [12] GS : Generational Scheme <ref> [16] </ref>, TS1 : Timestamp with 1 bit [27] TPI : Two-phase invalidation [21] Table 2.2: Comparison of existing HSCD cache coherence schemes. 20 detection. A flow analysis technique [13] is used to mark those potentially stale references at compile time. <p> In addition, these stale bits are updated by shifting during invalidations, which requires special cache hardware and incurs run time overhead. Furthermore, the LSS scheme still relies on single-word cache lines. Generational cache coherence scheme Chiueh developed the Generational Scheme (GS) <ref> [16] </ref> with the objective of overcoming the limitations of the VC and TS schemes. The GS scheme imposes a system-wide version number for all shared variables instead of allowing each shared variable to have its own current version number (CVN).
Reference: [17] <author> Lynn Choi and Pen-Chung Yew. </author> <title> Eliminating Stale Data References through Array Data-Flow Analysis. </title> <booktitle> Proceedings of the 10th IEEE International Parallel Processing Symposium, </booktitle> <month> April. </month> <year> 1996. </year>
Reference-contexts: We address several system related issues, including critical sections, inter-thread communication, and task migration. The cost of the required hardware support is small and proportional to the cache size. To study the compiler analysis techniques for the proposed scheme, we develop and implement both intraprocedural and interprocedural compiler algorithms <ref> [17, 18, 19] </ref> on the Polaris parallelizing compiler [43]. We use a combination of interval and data-flow analysis techniques to determine memory reference patterns that can lead to stale data accesses. To obtain more precise array access information, we compute the array region accessed by each array reference. <p> For a target reference which does not have a reaching definition inside a procedure, we issue a Time-Read with the minimum offset, implying that the referenced data item can be potentially modified before entering the procedure. This is an improvement over previous algorithms <ref> [13, 17] </ref> that use cache invalidation at the beginning of a procedure since only global and formal variables are affected by the unknown context information. We propagate definitions through the flow graph and increment their offsets when they cross scheduling edges. <p> Invalidation-based intraprocedural algorithm (ALG1) This algorithm performs stale reference detection on a per-procedure basis <ref> [17] </ref>. To avoid the complications caused by unknown side effects, cache invalidation operations are inserted after each call site and at the beginning of a procedure. 114 2.
Reference: [18] <author> Lynn Choi and Pen-Chung Yew. </author> <title> Interprocedural Array Data-Flow Analysis for Cache Coherence. </title> <booktitle> Lecture Notes in Computer Science, Proceedings of the Eighth International Workshop on Languages and Compilers for Parallel Computing '95, </booktitle> <pages> pages 81-95, </pages> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: We address several system related issues, including critical sections, inter-thread communication, and task migration. The cost of the required hardware support is small and proportional to the cache size. To study the compiler analysis techniques for the proposed scheme, we develop and implement both intraprocedural and interprocedural compiler algorithms <ref> [17, 18, 19] </ref> on the Polaris parallelizing compiler [43]. We use a combination of interval and data-flow analysis techniques to determine memory reference patterns that can lead to stale data accesses. To obtain more precise array access information, we compute the array region accessed by each array reference.
Reference: [19] <author> Lynn Choi and Pen-Chung Yew. </author> <title> Program Analysis for Cache Coherence: Beyond Procedural Boundaries. </title> <booktitle> Proceedings of the 1996 International Conference on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: We address several system related issues, including critical sections, inter-thread communication, and task migration. The cost of the required hardware support is small and proportional to the cache size. To study the compiler analysis techniques for the proposed scheme, we develop and implement both intraprocedural and interprocedural compiler algorithms <ref> [17, 18, 19] </ref> on the Polaris parallelizing compiler [43]. We use a combination of interval and data-flow analysis techniques to determine memory reference patterns that can lead to stale data accesses. To obtain more precise array access information, we compute the array region accessed by each array reference.
Reference: [20] <author> Lynn Choi and Pen-Chung Yew. </author> <title> Compiler and Hardware Support for Cache Coherence in Large-Sca le Multiprocessors: Design Considerations and Performance Study. </title> <booktitle> Proceedings of the 23rd Annual ACM International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: All of these compiler algorithms have been implemented in the Polaris parallelizing compiler [43]. Execution-driven simulations are used to verify the compiler marking and to demonstrate the performance of our scheme and the automatic compiler implementation against a hardware directory scheme <ref> [20] </ref>. Dissertation outline This dissertation is organized as follows. In chapter 2, we discuss basic concepts and issues that are helpful to understand compiler-directed cache coherence in general. <p> The purpose of this dissertation is to address these hardware and compiler implementation issues and to investigate the feasibility and performance of compiler-directed cache coherence approach as compared to hardware directory approaches. 31 Chapter 3 TWO-PHASE INVALIDATION SCHEME The Two-Phase Invalidation (TPI) <ref> [20] </ref> scheme removes most of the deficiencies in the earlier compiler-directed cache coherence schemes. It preserves much of the temporal and spatial locality across epoch boundaries but does not require excessive hardware support.
Reference: [21] <author> Lynn Choi and Pen-Chung Yew. </author> <title> A Compiler-Directed Cache Coherence Scheme with Improved Intertask Locality. </title> <booktitle> Proceedings of the ACM/IEEE Supercomputing'94, </booktitle> <pages> pages 773-782, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: The Cedar uses a shared cache to avoid coherence problems within each cluster. Data coherence is maintained among clusters by moving data explicitly between cluster and global memories using data movement instructions. Several compiler-directed cache coherence schemes <ref> [12, 14, 15, 16, 21, 27, 38, 39] </ref> have been proposed recently. These schemes give better performance, but demand more hardware and compiler support than the previous schemes. <p> Several compiler-directed cache coherence schemes [12, 14, 15, 16, 21, 27, 38, 39] have been proposed recently. These schemes give better performance, but demand more hardware and compiler support than the previous schemes. They require a more precise program analysis to maintain coherence on a reference basis <ref> [12, 13, 21] </ref> as opposed to the program region basis used by previous schemes. In addition, these schemes require hardware support to maintain local runtime cache states. In this regard, the terminology software cache coherence is a misnomer. It is a hardware approach with strong compiler support. <p> CKM : Cytron, Karlovsky, and McAuliffe [25], CTV : Coherence Through Vectorization [26] VC : Version Control [15], TS : Timestamp-based [39] PEI : Parallel Explicit Invalidation [38], LSS : Life Span Strategy [12] GS : Generational Scheme [16], TS1 : Timestamp with 1 bit [27] TPI : Two-phase invalidation <ref> [21] </ref> Table 2.2: Comparison of existing HSCD cache coherence schemes. 20 detection. A flow analysis technique [13] is used to mark those potentially stale references at compile time. The compiler generates memory-reads for these references to access up-to-date data from the main memory directly. <p> The per-procedure analysis is based on the following steps. First, we transform the source program into GSA form [4]. Then, we construct a modified flow graph, called the epoch flow graph <ref> [21] </ref>. It contains the epoch boundary information as well as the control flows of the program. Given a source program unit and its epoch flow graph, G, we identify the target references in each epoch to utilize both spatial and temporal reuses inside a task. <p> When there exists a redefining definition whose intersection with the read reference has nonempty subarrays, the read reference is marked as potentially stale. In Figure 4.5, we present a detailed algorithm for single-word cache lines. The previous scalar algorithm <ref> [21] </ref> may overestimate the potential stale references by summarizing the flow information from multiple flow paths. <p> This approach has a long history of predecessors, including C.mmp [50], IBM's RP3 [6], Illinois Cedar [34], and several recently proposed schemes <ref> [12, 14, 15, 21, 27, 38, 39] </ref>. 122 off-chip implementation of the TPI scheme, HW : full-map hardware directory scheme, SC: software cache-bypass scheme, BASE : base underlying machine with no coherence support.
Reference: [22] <author> Digital Equipment Corp. </author> <title> Alpha 21164 Microprocessor: Hardware Reference Manual. </title> <year> 1994. </year>
Reference-contexts: However, this will produce more redundant write traffic. By organizing the write buffer as a cache, as in the DEC Alpha 21164 processor <ref> [22] </ref>, such redundant write traffic can be reduced effectively [11]. Note that ordinary write buffers can help hide latencies but cannot eliminate redundant write traffic. For a write-back cache configuration, TPI scheme should force all global writes to be written back to the main memory at synchronization points.
Reference: [23] <author> Intel Corporation. </author> <title> Paragon XP/S Product Overview. </title> <year> 1991. </year>
Reference-contexts: Although these hardware schemes can precisely identify stale data by maintaining sharing information at runtime, they substantially increase hardware cost for directory storage and require complex directory and cache controllers. Therefore, many commercially available large-scale multiprocessors, such as the Cray T3D [31] and the Intel Paragon <ref> [23] </ref>, do not provide hardware-coherent caches. In several early multiprocessor systems, such as the CMU C.mmp [50], the NYU Ultracomputer [29], the IBM RP3 [6], and the Illinois Cedar [34], compiler-directed techniques were used to solve the cache coherence problem.
Reference: [24] <author> Ron Cytron, Jeanne Ferrante, and Barry K. Rosen. </author> <title> Efficiently Computing Static Single Assignment Form and the Control Dependence Graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: Static single assignment (SSA) <ref> [24] </ref> is a representation of a program in which each use of a variable is reached by exactly a single definition of the variable. It allows us to track the value of a variable by its name. <p> A backward demand-driven symbolic analysis is used next to 57 compute values and conditions across the confluence points of the control flow graph [47]. In addition to the above 3 functions, another function called ff (array, subscript, value) <ref> [24] </ref> is used to replace the array assignment statement. The semantics of the ff function is that a part of the array will take the value for the specified subscript while the rest of the array will remain as before. This representation maintains the single assignment property for the arrays.
Reference: [25] <author> Ron Cytron, Steve Karlovsky, and Kevin P. McAuliffe. </author> <title> Automatic Management of Programmable Caches. </title> <booktitle> Proceedings of the 1988 International Conference on Parallel Processing, II, </booktitle> <address> Software:229-238, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: (N) invalidate prevention GS prevention temporal O (logN) VGN lookup and update TS1 prevention temporal 1 epoch bit per associative cache line invalidate TPI detection, temporal, O (logN) epoch counter avoidance spatial increment SI : Simple Invalidation [49], FSI : Fast Selective Invalidation [14] CKM : Cytron, Karlovsky, and McAuliffe <ref> [25] </ref>, CTV : Coherence Through Vectorization [26] VC : Version Control [15], TS : Timestamp-based [39] PEI : Parallel Explicit Invalidation [38], LSS : Life Span Strategy [12] GS : Generational Scheme [16], TS1 : Timestamp with 1 bit [27] TPI : Two-phase invalidation [21] Table 2.2: Comparison of existing HSCD <p> For memory reads, if both the change bit and valid bit are set, then it results in a cache hit. The FSI scheme is still conservative and cannot fully exploit temporal locality across task boundaries for read-write shared data. Automatic management of programmable caches Cytron, Karlovsky, and McAuliffe <ref> [25] </ref> proposed compiler algorithms (let us call it the CKM scheme) which place cache management operations into the program to maintain cache coherence. Three cache management operations are used: update (referred to as post by the authors), invalidate, and flush. <p> Consider the simple matrix multiplication example shown in Figure 2.2 (a). According to an algorithm proposed in <ref> [25] </ref>, this scheme will invalidate variables A (i; k), B (k; j), and C (i; j) before they are read, and update C (i; j) after it is modified. Due to the invalidations, the CKM scheme also cannot exploit intertask locality, except for read-only or private variables.
Reference: [26] <author> E. Darnell and K. Kennedy. </author> <title> Automatic Software Cache Coherence through Vector-ization. </title> <booktitle> Proceedings of the 1992 International Conference on Supercomputing, </booktitle> <pages> pages 720-729, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: O (logN) VGN lookup and update TS1 prevention temporal 1 epoch bit per associative cache line invalidate TPI detection, temporal, O (logN) epoch counter avoidance spatial increment SI : Simple Invalidation [49], FSI : Fast Selective Invalidation [14] CKM : Cytron, Karlovsky, and McAuliffe [25], CTV : Coherence Through Vectorization <ref> [26] </ref> VC : Version Control [15], TS : Timestamp-based [39] PEI : Parallel Explicit Invalidation [38], LSS : Life Span Strategy [12] GS : Generational Scheme [16], TS1 : Timestamp with 1 bit [27] TPI : Two-phase invalidation [21] Table 2.2: Comparison of existing HSCD cache coherence schemes. 20 detection. <p> Due to the invalidations, the CKM scheme also cannot exploit intertask locality, except for read-only or private variables. Furthermore, the CKM scheme makes the unrealistic assumption of single-word cache lines. Coherence through vectorization Darnell, et. al. proposed the Coherence Through Vectorization (CTV) <ref> [26] </ref> scheme, which is an extension of the CKM scheme. The main idea of the CTV scheme is to reduce the run-time overheads of cache management operations, such as update and invalidate, by aggregating these operations with vectorization.
Reference: [27] <author> E. Darnell and K. Kennedy. </author> <title> Cache Coherence Using Local Knowledge. </title> <booktitle> Proceedings of the Supercomputing '93, </booktitle> <pages> pages 720-729, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: The Cedar uses a shared cache to avoid coherence problems within each cluster. Data coherence is maintained among clusters by moving data explicitly between cluster and global memories using data movement instructions. Several compiler-directed cache coherence schemes <ref> [12, 14, 15, 16, 21, 27, 38, 39] </ref> have been proposed recently. These schemes give better performance, but demand more hardware and compiler support than the previous schemes. <p> The scheme can invalidate the stale cache entries in only the part of an array that has been modified. This is done by allocating elements of an array in a structured form, and by generating a masking bit pattern for each write reference. Darnell and Kennedy <ref> [27] </ref> proposed an invalidation scheme that combines the idea of parallel explicit invalidation with a 1-bit timestamp scheme. Most hardware cache coherence schemes use prevention techniques. Stale reference avoidance is a more relaxed coherence model than the prevention technique. <p> : Fast Selective Invalidation [14] CKM : Cytron, Karlovsky, and McAuliffe [25], CTV : Coherence Through Vectorization [26] VC : Version Control [15], TS : Timestamp-based [39] PEI : Parallel Explicit Invalidation [38], LSS : Life Span Strategy [12] GS : Generational Scheme [16], TS1 : Timestamp with 1 bit <ref> [27] </ref> TPI : Two-phase invalidation [21] Table 2.2: Comparison of existing HSCD cache coherence schemes. 20 detection. A flow analysis technique [13] is used to mark those potentially stale references at compile time. The compiler generates memory-reads for these references to access up-to-date data from the main memory directly. <p> Also, this scheme uses code replication to handle conditional branches in the dynamic update of VGNs, which can lead to code expansion. Finally, the GS scheme also assumes single-word cache lines. TS1 scheme The Timestamping with 1 bit (TS1) scheme, developed by Darnell and Kennedy <ref> [27] </ref>, combines the idea of parallel explicit invalidation with the simple invalidation approach. The key idea of the TS1 scheme is that cache lines created in an epoch can remain valid until the next epoch without causing a stale reference 2 . <p> This approach has a long history of predecessors, including C.mmp [50], IBM's RP3 [6], Illinois Cedar [34], and several recently proposed schemes <ref> [12, 14, 15, 21, 27, 38, 39] </ref>. 122 off-chip implementation of the TPI scheme, HW : full-map hardware directory scheme, SC: software cache-bypass scheme, BASE : base underlying machine with no coherence support.
Reference: [28] <author> C. Dubnicki and T. LeBlanc. </author> <title> Adjustable Block Size Coherent Caches. </title> <booktitle> Proceedings the 19th Annual International Symposium on Computer Archtecture, </booktitle> <pages> pages 170-180, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Maintaining sharing information per cache word in directory schemes will increase directory storage significantly since the memory requirement is proportional to the total memory size instead of the total cache size. Optimizations to reduce the storage overhead may result in very complicated hardware protocols <ref> [28] </ref>. However, more fine-grained sharing information can be incorporated in this HSCD scheme more easily because the coherence enforcement is done locally. A cache memory has two components: data storage and address tag storage.
Reference: [29] <author> J. Edler, A. Gottlieb, C. P. Kruskal, K. P. McAuliffe, et al. </author> <title> Issues related to MIMD shared-memory computers: the NYU Ultracomputer approach. </title> <booktitle> Proceedings of the 12th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 126-135, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: Therefore, many commercially available large-scale multiprocessors, such as the Cray T3D [31] and the Intel Paragon [23], do not provide hardware-coherent caches. In several early multiprocessor systems, such as the CMU C.mmp [50], the NYU Ultracomputer <ref> [29] </ref>, the IBM RP3 [6], and the Illinois Cedar [34], compiler-directed techniques were used to solve the cache coherence problem. In this approach, cache coherence 1 is maintained locally without the need for interprocessor communication or hardware di-rectories. <p> Although this scheme is easy to implement, it is very conservative and limits the performance of 15 the system since a significant portion of memory references in real programs are accesses to read-write shared data. NYU Ultracomputer The NYU Ultracomputer <ref> [29] </ref> caches read-only shared data like the C.mmp. In addition, it also allows caching of read-write shared data during periods of read-only access or during periods of exclusive use by a single processor.
Reference: [30] <author> S. J. Eggers and R. H. Katz. </author> <title> A Characterization of Sharing in Parallel Programs and Its Application to Coherency Protocol Evaluation. </title> <booktitle> Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 373-383, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: It turns out that the use of multi-word cache lines leads to the false sharing effect if cache coherence is enforced on a per-cache-line basis <ref> [30] </ref>. Most compiler-directed coherence schemes generate coherence operations on a per-variable or per-reference basis. With multi-word cache lines, it is very difficult for the compiler to generate such operations since different variables can share the same cache line.
Reference: [31] <author> Cray Research Inc. </author> <title> Cray T3D System Architecture Overview. </title> <month> Mar. </month> <year> 1993. </year>
Reference-contexts: Although these hardware schemes can precisely identify stale data by maintaining sharing information at runtime, they substantially increase hardware cost for directory storage and require complex directory and cache controllers. Therefore, many commercially available large-scale multiprocessors, such as the Cray T3D <ref> [31] </ref> and the Intel Paragon [23], do not provide hardware-coherent caches. In several early multiprocessor systems, such as the CMU C.mmp [50], the NYU Ultracomputer [29], the IBM RP3 [6], and the Illinois Cedar [34], compiler-directed techniques were used to solve the cache coherence problem. <p> Furthermore, data movement instructions are provided so that the programmer can explicitly move data between the cluster and global memories. By using these software mechanisms, coherence can be maintained for globally shared data. Cray T3D The Cray T3D <ref> [31] </ref> is an MPP system which has a physically distributed memory but also provides support for a shared address space. Each T3D node contains a DEC Alpha microprocessor, which has an on-chip data and instruction cache. Each node also contains a local memory.
Reference: [32] <institution> MIPS Technology Inc. </institution> <note> R10000 User's Manual, Alpha Revision 2.0. </note> <month> March </month> <year> 1995. </year>
Reference-contexts: number is stored in an n-bit register in each processor, called the epoch counter (R counter ), and is incremented at the end of 1 This operation can be implemented in the MIPS R10000 processor with a cache block invalidate (Index Write Back Invalidate) followed by a regular load operation <ref> [32] </ref>. Similarly, the Flush instruction (DCBF) can be used for the IBM PowerPC 600 series microprocessor [7]. 2 It is called two-phase invalidation because of its hardware reset mechanism on epoch counter overflow. 35 each epoch by every processor individually.
Reference: [33] <author> C. P. Kruskal and M. Snir. </author> <title> The Performance of Multistage Interconnection Networks for Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-32(12):1091-1098, </volume> <month> Sept., </month> <year> 1987. </year>
Reference-contexts: size on-chip 64 KB, direct-mapped on epoch counter overflow two-phase reset mechanism line size 4 32-bit word cache line timetag size 8-bits number of processors 16 Cache and system organization ALU operations 1 CPU cycle cache hit 1 CPU cycle base miss latency 100 CPU cycles network delay analytic model <ref> [33] </ref> memory bus cycle 3 CPU cycles off-chip timetag access 2 CPU cycles off-chip timetag access penalty 8 (3 + 2 + 3) CPU cycles Table 6.1: Parameters used for typical simulations. miss latency, assuming no network load. <p> The network delays are simulated using an analytical delay model for indirect multistage networks <ref> [33] </ref>. The execution-driven simulator instruments the application codes to generate events that reflect the behavior of the codes executing on the target architecture. Simulated events include global and local memory accesses, parallel loop setup and scheduling op erations, and synchronization operations.
Reference: [34] <author> D. Kuck, E. Davidson, et al. </author> <title> The Cedar System and an Initial Performance Study. </title> <booktitle> Proceedings of the 20th Annual International Symposium on Com puter Architecture, </booktitle> <pages> pages 213-223, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Therefore, many commercially available large-scale multiprocessors, such as the Cray T3D [31] and the Intel Paragon [23], do not provide hardware-coherent caches. In several early multiprocessor systems, such as the CMU C.mmp [50], the NYU Ultracomputer [29], the IBM RP3 [6], and the Illinois Cedar <ref> [34] </ref>, compiler-directed techniques were used to solve the cache coherence problem. In this approach, cache coherence 1 is maintained locally without the need for interprocessor communication or hardware di-rectories. The C.mmp was the first to allow read-only shared data to be kept in private caches while leaving read-write data uncached. <p> Thus, the cache coherence mechanism on the RP3 is more flexible than that of the Ultracomputer, since the compiler can minimize the amount of over-invalidation by selecting the most suitable invalidation granularity. Illinois Cedar The Illinois Cedar <ref> [34] </ref> is a cluster-based, shared-memory multiprocessor. It consists of four clusters (each with eight processors) that are connected to a globally shared memory. In each cluster, the processors are connected to a 4-way interleaved shared cache, which is in turn connected to an interleaved cluster memory. <p> This dissertation investigates a hardware-supported, compiler-directed (HSCD) cache coherence scheme, called the two-phase invalidation (TPI) scheme, which relies mostly on compiler analysis, yet also provides a reasonable amount of hardware support. This approach has a long history of predecessors, including C.mmp [50], IBM's RP3 [6], Illinois Cedar <ref> [34] </ref>, and several recently proposed schemes [12, 14, 15, 21, 27, 38, 39]. 122 off-chip implementation of the TPI scheme, HW : full-map hardware directory scheme, SC: software cache-bypass scheme, BASE : base underlying machine with no coherence support.
Reference: [35] <author> J. Kuskin, D. Ofelt, M. Heinrich, and J. Heinlein et al. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> Proceedings of The 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> April 18-21, </month> <year> 1994. </year> <month> 127 </month>
Reference-contexts: Having multiple cached copies of a shared memory location, however, can lead to erroneous program behavior unless they are maintained coherent. Existing solutions for large-scale multiprocessors include hardware directory-based coherence protocols, which have been studied in many research machines <ref> [2, 35, 36] </ref>. Although these hardware schemes can precisely identify stale data by maintaining sharing information at runtime, they substantially increase hardware cost for directory storage and require complex directory and cache controllers.
Reference: [36] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory--Based Cache Coherence Protocol for the DASH Computer. </title> <booktitle> Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Having multiple cached copies of a shared memory location, however, can lead to erroneous program behavior unless they are maintained coherent. Existing solutions for large-scale multiprocessors include hardware directory-based coherence protocols, which have been studied in many research machines <ref> [2, 35, 36] </ref>. Although these hardware schemes can precisely identify stale data by maintaining sharing information at runtime, they substantially increase hardware cost for directory storage and require complex directory and cache controllers.
Reference: [37] <author> D. J. Lilja. </author> <title> Cache Coherence in Large-Scale Shared-Memory Multiprocessors: Issues and Comparisons. </title> <journal> ACM Computing Surveys, </journal> <volume> 25(3) </volume> <pages> 303-338, </pages> <month> Sep. </month> <year> 1993. </year>
Reference-contexts: Several studies have compared the performance of directory schemes and some recent HSCD schemes. Min and Baer [40] compared the performance of a directory scheme and a timestamp-based scheme, assuming infinite cache size and single-word cache lines. Lilja <ref> [37] </ref> compared the performance of the version control scheme [15] and directory schemes, and analyzed the directory overhead of several implementations. Both studies showed that the performance of those HSCD schemes can be comparable to that of directory schemes.
Reference: [38] <author> A. Louri and H. Sung. </author> <title> A Compiler Directed Cache Coherence Scheme with Fast and Parallel Explicit Invalidation. </title> <booktitle> Proceedings of the 1992 International Conference on Parallel Processing, I, </booktitle> <address> Architecture:2-9, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: The Cedar uses a shared cache to avoid coherence problems within each cluster. Data coherence is maintained among clusters by moving data explicitly between cluster and global memories using data movement instructions. Several compiler-directed cache coherence schemes <ref> [12, 14, 15, 16, 21, 27, 38, 39] </ref> have been proposed recently. These schemes give better performance, but demand more hardware and compiler support than the previous schemes. <p> The compiler identifies the task level at which the cache copy needs to be invalidated. This strategy allows more temporal intertask locality to 11 be exploited for shared read-write data. Parallel explicit invalidation <ref> [38] </ref> uses a more refined selective invalidation technique. The scheme can invalidate the stale cache entries in only the part of an array that has been modified. <p> line invalidate TPI detection, temporal, O (logN) epoch counter avoidance spatial increment SI : Simple Invalidation [49], FSI : Fast Selective Invalidation [14] CKM : Cytron, Karlovsky, and McAuliffe [25], CTV : Coherence Through Vectorization [26] VC : Version Control [15], TS : Timestamp-based [39] PEI : Parallel Explicit Invalidation <ref> [38] </ref>, LSS : Life Span Strategy [12] GS : Generational Scheme [16], TS1 : Timestamp with 1 bit [27] TPI : Two-phase invalidation [21] Table 2.2: Comparison of existing HSCD cache coherence schemes. 20 detection. <p> Parallel explicit invalidation scheme The Parallel Explicit Invalidation (PEI) scheme proposed by Louri and Sung <ref> [38] </ref> extends the invalidation approach used in the SI and FSI schemes. The key idea of the PEI scheme is to let each processor explicitly and 24 selectively invalidate stale cache copies instead of the entire cache. <p> This approach has a long history of predecessors, including C.mmp [50], IBM's RP3 [6], Illinois Cedar [34], and several recently proposed schemes <ref> [12, 14, 15, 21, 27, 38, 39] </ref>. 122 off-chip implementation of the TPI scheme, HW : full-map hardware directory scheme, SC: software cache-bypass scheme, BASE : base underlying machine with no coherence support.
Reference: [39] <author> S. L. Min and J.-L. Baer. </author> <title> A Timestamp-based Cache Coherence Scheme. </title> <booktitle> Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <address> I:23-32, </address> <year> 1989. </year>
Reference-contexts: The Cedar uses a shared cache to avoid coherence problems within each cluster. Data coherence is maintained among clusters by moving data explicitly between cluster and global memories using data movement instructions. Several compiler-directed cache coherence schemes <ref> [12, 14, 15, 16, 21, 27, 38, 39] </ref> have been proposed recently. These schemes give better performance, but demand more hardware and compiler support than the previous schemes. <p> Most hardware cache coherence schemes use prevention techniques. Stale reference avoidance is a more relaxed coherence model than the prevention technique. Stale reference avoidance allows the existence of stale data while avoiding stale accesses to them at runtime. Examples of this approach include the timestamp-based scheme <ref> [39] </ref> and the version control scheme [15]. In such schemes, stale data can exist until an access to the data occurs. At that time, by checking the status of the data in the cache, it is decided whether an up-to-date copy is brought in from the main memory. <p> 1 epoch bit per associative cache line invalidate TPI detection, temporal, O (logN) epoch counter avoidance spatial increment SI : Simple Invalidation [49], FSI : Fast Selective Invalidation [14] CKM : Cytron, Karlovsky, and McAuliffe [25], CTV : Coherence Through Vectorization [26] VC : Version Control [15], TS : Timestamp-based <ref> [39] </ref> PEI : Parallel Explicit Invalidation [38], LSS : Life Span Strategy [12] GS : Generational Scheme [16], TS1 : Timestamp with 1 bit [27] TPI : Two-phase invalidation [21] Table 2.2: Comparison of existing HSCD cache coherence schemes. 20 detection. <p> The result of applying the CTV scheme to the previous matrix multiply example is shown in Figure 2.2 (b). This scheme also assumes single-word cache lines. Version control and timestamp-based schemes The Version Control (VC) [15] and Timestamp-based (TS) <ref> [39] </ref> schemes use additional hardware support to improve the exploitation of intertask temporal locality. Although these two schemes were independently proposed, they are actually very similar in nature. <p> This approach has a long history of predecessors, including C.mmp [50], IBM's RP3 [6], Illinois Cedar [34], and several recently proposed schemes <ref> [12, 14, 15, 21, 27, 38, 39] </ref>. 122 off-chip implementation of the TPI scheme, HW : full-map hardware directory scheme, SC: software cache-bypass scheme, BASE : base underlying machine with no coherence support.
Reference: [40] <author> S. L. Min and J.-L. Baer. </author> <title> Design and Analysis of a Scalable Cache Coherence Scheme Based on Clocks and Timestamps. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(1) </volume> <pages> 25-44, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: We call them hardware-supported compiler-directed (HSCD) coherence schemes, 2 which is distinctly different from a pure hardware directory scheme or a pure software scheme. Several studies have compared the performance of directory schemes and some recent HSCD schemes. Min and Baer <ref> [40] </ref> compared the performance of a directory scheme and a timestamp-based scheme, assuming infinite cache size and single-word cache lines. Lilja [37] compared the performance of the version control scheme [15] and directory schemes, and analyzed the directory overhead of several implementations.
Reference: [41] <author> F. Mounes-Toussi and D. Lilja. </author> <title> The Potential of Compile-Time Analysis to Adapt the Cache Coherence Enforcement Strategy to the Data Sharing Characteristics. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <year> 1995. </year>
Reference: [42] <author> J. M. Mulder, N. T. Quach, and M. J. Flynn. </author> <title> An area model for on-chip memories and its application. </title> <journal> Journal of Solid State Circuits, </journal> <volume> 26 </volume> <pages> 98-106, </pages> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: Note that this is true for all the shared-memory multiprocessors. This can be accomplished by using write-through 8 As noted by <ref> [42, 51] </ref>, the tag overhead in the design of the on-chip caches are significant, occupying area comparable to the data portion, especially for set-associative caches with 64-bit addresses. 48 caches. However, this will produce more redundant write traffic.
Reference: [43] <author> D. A. Padua, R. Eigenmann, J. Hoeflinger, P. Peterson, P. Tu, S. Weatherford, and K. Faign. </author> <title> Polaris: A New-Generation Parallelizing Compiler for MPPs. In CSRD Rept. No. </title> <type> 1306. </type> <institution> Univ. of Illinois at Urbana-Champaign., </institution> <month> June, </month> <year> 1993. </year>
Reference-contexts: The cost of the required hardware support is small and proportional to the cache size. To study the compiler analysis techniques for the proposed scheme, we develop and implement both intraprocedural and interprocedural compiler algorithms [17, 18, 19] on the Polaris parallelizing compiler <ref> [43] </ref>. We use a combination of interval and data-flow analysis techniques to determine memory reference patterns that can lead to stale data accesses. To obtain more precise array access information, we compute the array region accessed by each array reference. <p> This algorithm eliminates cache invalidations at procedure boundaries, which are assumed by all previous compiler-directed coherence schemes, and allows the locality of programs to be preserved across procedure calls. All of these compiler algorithms have been implemented in the Polaris parallelizing compiler <ref> [43] </ref>. Execution-driven simulations are used to verify the compiler marking and to demonstrate the performance of our scheme and the automatic compiler implementation against a hardware directory scheme [20]. Dissertation outline This dissertation is organized as follows. <p> We will discuss the details of compiler algorithms in the following chapters 4 and 5. Compiler implementation All the compiler algorithms explained earlier have been implemented on the Polaris parallelizing compiler <ref> [43] </ref>. Figure 3.5 shows the flowchart of our reference marking algorithm. First, we construct a procedure call graph. Then, 42 based on the bottom-up scan of the call graph, we analyze each procedure and propagate its side effects to its callers. The per-procedure analysis is based on the following steps. <p> The cost is less than the Scalable Coherence Interface (SCI) because the tag size required is much smaller than that required in SCI. The necessary compiler algorithms, including intra- and interprocedural array data flow analysis, have been implemented on the Polaris paralleling compiler <ref> [43] </ref>. The results of our simulation study using the Perfect Club Benchmarks [5] show that both hardware directory schemes and the TPI scheme have comparable number of unnecessary cache misses.
Reference: [44] <author> J. K. Peir, K. So, and J. H. Tang. </author> <title> Techniques to Enhance Cache Performance Across Parallel Program Sections. </title> <booktitle> Proceedings of the 1993 International Conference on Parallel Processing, I, </booktitle> <address> Architecture:I-12-I-19, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: The fast selective invalidation scheme [14] combines the techniques of prevention and detection, and extends the simple invalidation scheme to exploit intertask locality for safe (e.g., read-only) accesses. The life span strategy [12], Peir and So's scheme <ref> [44] </ref>, and the generational algorithm [16] allow invalidation on a per-variable basis. In these schemes, a variable is not invalidated until the next write to that variable occurs in a different task. The compiler identifies the task level at which the cache copy needs to be invalidated. <p> The LSS scheme uses an additional bit (called change bit) to capture intratask locality. In addition, Cheong suggested using the stale reference detection technique to further refine code generation for the LSS scheme. Peir, So, and Tang <ref> [44] </ref> proposed a scheme very similar to Cheong's LSS scheme. To exploit N levels of intertask locality, both schemes require at least N stale bits per cached data item.
Reference: [45] <author> D. K. Poulsen and P.-C. Yew. </author> <title> Execution-Driven Tools for Parallel Simulation of Parallel Architectures and Applications. </title> <booktitle> Proceedings of the Supercomputing 93, </booktitle> <pages> pages 860-869, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: After compiler marking, we instrument the benchmarks to generate memory events, which are used for the simulation. Figure 6.1 shows the experimentation tools used in our simulations. 6.1.1 Simulation Execution-driven simulations <ref> [45] </ref> are used to verify the compiler algorithms and to evaluate the performance of our proposed coherence scheme. All the simulations assume a 16-processor, physically distributed shared-memory multiprocessor.
Reference: [46] <author> J. Torrellas, M. S. Lam, and J. L. Hennessy. </author> <title> False Sharing and Spatial Locality in Multiprocessor Caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-43(6):651-663, </volume> <month> June </month> <year> 1994. </year>
Reference-contexts: On the other hand, code generation, such as address calculation, will increase the frequency of private references due to spill code. Shared data consistency prevents optimization of the references to the shared data <ref> [46] </ref>. 99 6.2 Impact of hardware support In this section, we investigate how cache coherence support impacts the overall system performance. Four different hardware and compiler-directed coherence schemes are studied. They differ in caching strategy for shared memory accesses, and in the amount of hardware and compiler support required.
Reference: [47] <author> Peng Tu. </author> <title> Automatic Array Privatization and Demand-Driven Symbolic Analysis. </title> <type> Technical report, </type> <institution> Univ. of Illinois at Urbana-Champaign, Dept. of Computer Science, </institution> <year> 1995. </year> <type> Ph.D. Thesis. 128 </type>
Reference-contexts: The data values to be analyzed include scalar variables, subscripted variables, and subarrays <ref> [47] </ref>. A subscripted variable consists of an array identifier and a subscript expression, representing a single array element referenced. A subarray consists of a subscripted variable and one or more ranges for some of the indices in its subscript expression. <p> The notion of a subarray is an extension to the regular section used in [8]. A subarray can represent a triangular region, a banded region, as well as a strip, grid, column, row, or block of an array <ref> [47] </ref>. 56 4.1.1 Gated single assignment (GSA) form To perform an effective array flow analysis, the symbolic manipulation of expressions is necessary since the computation of array regions often involves the equality and comparison tests between symbolic expressions. <p> In the global symbolic forward substitution, information is propagated until it terminates at the confluence points in the control flow graph. A backward demand-driven symbolic analysis is used next to 57 compute values and conditions across the confluence points of the control flow graph <ref> [47] </ref>. In addition to the above 3 functions, another function called ff (array, subscript, value) [24] is used to replace the array assignment statement.
Reference: [48] <author> D. M. Tullsen and S. J. Eggers. </author> <title> Limitations of Cache Prefetching on a Bus-Based Multiprocessors. </title> <booktitle> Proceedings of The 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 278-288, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: However, the compiler algorithms are essential for coherence enforcement in both SC and TPI. 101 protocols. False sharing misses are identified during simulations using the method in <ref> [48] </ref>. If an invalidation is caused by an access to a word that the local processor had not used since getting the block into its cache, then it is a false sharing invalidation. Any subsequent invalidation miss on this block will be counted also as a false sharing miss.
Reference: [49] <author> A. V. Veidenbaum. </author> <title> A Compiler-Assisted Cache Coherence Solution for Multiprocessors. </title> <booktitle> Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 1029-1035, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Agarwal and Adve [1] used an analytic model to compare the performance of compiler-directed and directory-based techniques. They concluded that the performance of compiler-directed schemes depends on the characteristics of the workloads. Chen [11] showed that a simple invalidation scheme <ref> [49] </ref> can achieve performance comparable to that of a directory scheme and discussed several different write policies. Most of these studies, however, assumed perfect compile-time memory disambiguation and complete control dependence information. <p> By identifying these potential stale references at compile time, the system can be forced to get up-to-date data directly from the main memory instead of from the cache. Stale reference prevention techniques invalidate or update stale cache entries before stale references occur. Simple invalidation <ref> [49] </ref> maintains cache coherence by invalidating the entire cache at task boundaries. If no read-write dependences exist among processors (e.g., during the parallel execution of a DOALL loop), no stale references will occur in the processors. <p> Table 2.2 summarizes the important features of these schemes. Simple invalidation and selective invalidation schemes Veidenbaum proposed a Simple Invalidation (SI) <ref> [49] </ref> approach in which each processor invalidates its cache at task boundaries to enforce cache coherence. Assuming DOALL-style parallelism, each shared data item can be modified by at most one processor in each parallel section of code. <p> update PEI prevention temporal segmentation associative between arrays invalidate LSS detection, temporal O (N) invalidate prevention GS prevention temporal O (logN) VGN lookup and update TS1 prevention temporal 1 epoch bit per associative cache line invalidate TPI detection, temporal, O (logN) epoch counter avoidance spatial increment SI : Simple Invalidation <ref> [49] </ref>, FSI : Fast Selective Invalidation [14] CKM : Cytron, Karlovsky, and McAuliffe [25], CTV : Coherence Through Vectorization [26] VC : Version Control [15], TS : Timestamp-based [39] PEI : Parallel Explicit Invalidation [38], LSS : Life Span Strategy [12] GS : Generational Scheme [16], TS1 : Timestamp with 1 <p> We will first introduce a pure compiler scheme that can enforce cache coherence using stale reference detection, and then explain how the TPI scheme can improve cache utilization by using hardware support. 32 3.1 Stale reference sequence The following sequence of events creates a stale reference at runtime <ref> [49] </ref>: (1) Processor P i reads or writes to memory location x at time T 1 , and brings a copy of x in its cache; (2) Another processor, P j (j 6= i), later writes to x at time T 2 (&gt; T 1 ), and creates a new copy <p> In the hardware directory schemes, the coherence is guaranteed either by invalidation or update coherence transactions. For compiler-directed schemes, however, this might require careful handling for correct operations. For example, in the two-phase invalidation scheme <ref> [49] </ref>, assume that a task T 1 running on a processor P i in epoch e writes to a variable A, creating a new cache copy of A, then the task is migrated to a different processor P j in the same epoch.
Reference: [50] <author> C. G. Bell W. A. Wulf. </author> <title> C.mmp amulti-mini processor. </title> <booktitle> Proceedings of the Fall Joint Computer Conference, </booktitle> <pages> pages 765-777, </pages> <month> December </month> <year> 1972. </year>
Reference-contexts: Therefore, many commercially available large-scale multiprocessors, such as the Cray T3D [31] and the Intel Paragon [23], do not provide hardware-coherent caches. In several early multiprocessor systems, such as the CMU C.mmp <ref> [50] </ref>, the NYU Ultracomputer [29], the IBM RP3 [6], and the Illinois Cedar [34], compiler-directed techniques were used to solve the cache coherence problem. In this approach, cache coherence 1 is maintained locally without the need for interprocessor communication or hardware di-rectories. <p> Such an approach does not require any special hardware and has been used in several early multiprocessor systems. Table 2.1 summarizes the cache coherence schemes used in several existing shared-memory multiprocessor systems. CMU C.mmp The CMU C.mmp <ref> [50] </ref> caches read-only shared data, but does not cache read-write shared data. This is accomplished by using the relocation register associated with each page for address mapping. A special bit in each relocation register indicates whether the corresponding page is a read-only one and hence can be cached. <p> This dissertation investigates a hardware-supported, compiler-directed (HSCD) cache coherence scheme, called the two-phase invalidation (TPI) scheme, which relies mostly on compiler analysis, yet also provides a reasonable amount of hardware support. This approach has a long history of predecessors, including C.mmp <ref> [50] </ref>, IBM's RP3 [6], Illinois Cedar [34], and several recently proposed schemes [12, 14, 15, 21, 27, 38, 39]. 122 off-chip implementation of the TPI scheme, HW : full-map hardware directory scheme, SC: software cache-bypass scheme, BASE : base underlying machine with no coherence support.
Reference: [51] <author> H. Wang, T. Sun, and Q. Yang. </author> <title> CAT Caching Address Tags: A Technique for Reducing Area Cost of On-Chip Caches. </title> <booktitle> Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 381-390, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Note that this is true for all the shared-memory multiprocessors. This can be accomplished by using write-through 8 As noted by <ref> [42, 51] </ref>, the tag overhead in the design of the on-chip caches are significant, occupying area comparable to the data portion, especially for set-associative caches with 64-bit addresses. 48 caches. However, this will produce more redundant write traffic.

References-found: 51

