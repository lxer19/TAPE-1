URL: http://www.cs.cornell.edu/Info/People/prakas/papers/canpc97.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/prakas/
Root-URL: http://www.cs.brown.edu/
Title: Data Movement and Control Substrate for parallel scientific computing  
Author: Nikos Chrisochoides ?? Induprakas Kodukula and Keshav Pingali 
Address: Ithaca, NY 14853-3801  
Affiliation: Computer Science Department Cornell University,  
Abstract: In this paper, we describe the design and implementation of a data-movement and control substrate (DMCS) for network-based, homogeneous communication within a single multiprocessor. DMCS is an implementation of an API for communication and computation that has been proposed by the PORTS consortium. One of the goals of this consortium is to define an API that can support heterogeneous computing without undue performance penalties for homogeneous computing. Preliminary results in our implementation suggest that this is quite feasible. The DMCS implementation seeks to minimize the assumptions made about the homogeneous nature of its target architecture. Finally, we present some extensions to the API for PORTS that will improve the performance of sparse, adaptive and irregular type of numeric computations. Keywords: parallel processing, runtime systems, communication, threads, networks
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Thorsten von Eicken, Davin E. Culler, Seth Cooper Goldstein, and Klaus Erik Schauser, </author> <title> Active Messages: </title> <booktitle> a mechanism for integrated communication and computation Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <publisher> ACM Press, </publisher> <month> May </month> <year> 1992. </year>
Reference-contexts: Fig. 1. Architecture handlers are executed either as the message is being retrieved from the network interface 4 , or after the message retrieval has been completed <ref> [1] </ref>. Finally, the control module provides some limited support for simple load balancing by allowing associating a window within which load on any processor can be balanced. <p> Attributes currently implemented are the stack size of a thread, and whether the thread should use a pre-allocated stack, or do a fresh allocation for its stack. 3.2 Communication subpackage The communication subpackage is implemented on top of a generic active message implementation on the SP-2 <ref> [1, 21] </ref>. Active messages are a mechanism for asynchronous, low-overhead communication. The fundamental idea in Active Messages is that every message is sent along with a reference to a handler which is invoked on receipt of the message.
Reference: 2. <author> Matthew Haines, David Cronk, and Piyush Mehrotra, </author> <title> On the design of Chant : A talking threads package, NASA CR-194903 ICASE Report No. </title> <type> 94-25, </type> <institution> Institute for Computer Applications in Science and Engineering Mail Stop 132C, NASA Langley Research Center Hampton, </institution> <address> VA 23681-0001, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: An extremely fast implementation of ports timing is available from University of Oregon [24]. There is a proposed API for communication, and for the integration of communication with threads [17]. The PORTS consortium has been experimenting with four different approaches. 1. Thread-to-thread communication, supported by CHANT <ref> [2] </ref>. 2. Remote service request communication, supported by NEXUS [11]. 3. Hybrid communication, supported by TULIP [25]. 4. The DMCS approach outlined in this paper. In this context, our DMCS implementation accomplishes the following. <p> For portability, CHANT supports the first approach, since many thread packages do not allow their scheduler to be modified. Performance data in <ref> [2] </ref> indicate that there is little difference in performance between the first two polling approaches. NEXUS decouples the specification of the destination of communication from the specification of the thread of control that responds to it.
Reference: 3. <author> R.S. Nikhil, Cid: </author> <title> A Parallel, "Shared-Memory" C for Distributed Memory Machines. </title> <booktitle> In Lecture Notes in Computer Science, </booktitle> <volume> vol 892. </volume>
Reference-contexts: Finally, functionality similar to DMCS threads and communication and control modules are provided by a number of other runtime systems like Cid, Split-C, Cilk, and Multipol. Cid <ref> [3] </ref> and Split-C [22] are parallel extensions to C. Both systems support a global address space through the abstraction of the global pointer.
Reference: 4. <author> Christopher F. Joerg. </author> <title> The Cilk system for Parallel Multithreaded Computing. </title> <type> Ph.D. Thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> January, </month> <year> 1996. </year>
Reference-contexts: Both systems support a global address space through the abstraction of the global pointer. They also implement asynchronous, one-sided communication, and multithreading (either in the language as in Cid, or through extensions as in SplitThreads [23]), and have mechanisms for overlapping computation with communication and synchronization latencies. Cilk <ref> [4] </ref> is similar but it targets a more restricted class of computations (strict computations). The scheduling policy is fixed, and for a certain class of applications is provably efficient with respect to time, space and communication.
Reference: 5. <author> L.V. Kale and M. Bhandarkar and N. Jagathesan and S. Krishnan and J. Yelon, </author> <title> CONVERSE: An Interoperability Framework for Parallel Programming, </title> <note> Parallel Programming Laboratory Report #95-2, </note> <institution> Dept. of Computer Science, University of Illinois, </institution> <month> March </month> <year> 1995 </year>
Reference: 6. <author> Nikos Chrisochoides and Nikos Pitsianis, </author> <title> FFT Sensitive Messages, </title> <note> to appear as Cornell Theory Center Technical Report, </note> <year> 1996. </year>
Reference: 7. <author> Nikos Chrisochoides and Juan Miguel del Rosario, </author> <title> A Remote Service Protocol for Dynamic Load Balancing of Multithreaded Parallel Computations. </title> <note> Poster presentation in Frontiers'95. </note>
Reference-contexts: We are using the implementation provided by the Argonne group, PORTS0 [16], augmented by an extra routine: ports thread create atonce. The efficient implementation of this routine is necessary to minimize the scheduling latency of certain urgent, remote service requests <ref> [7] </ref>. Clearly, this extension can be implemented on top of the existing ports threads primitives provided by PORTS0 (for example, using the thread priority attributes), but for efficiency reasons, we choose to implement it, whenever possible, directly on the underlying thread package.
Reference: 8. <author> MPI Forum, </author> <title> Message-Passing Interface Standard, </title> <address> April 15, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction The portability of programs across supercomputers has been addressed very successfully by MPI <ref> [8] </ref> which is intended to be an easy-to-use and attractive interface for the application programmer and tool developer. <p> CHANT implements thread-to-thread communication on top of portable message passing software layers such as p4 [13], PVM [18], and MPI <ref> [8] </ref>. The efficiency of this mechanism depends critically on the implementation of message polling or message delivery interrupt.
Reference: 9. <institution> Runtime Support for Portable Distributed Data Structures C.-P. </institution> <note> Wen, </note> <author> S. Chakrabarti, E. Deprit, Chih-Po Wen, A. Krishnamurthy, and K. Yelick. </author> <booktitle> Workshop on Languages, Compilers, and Runtime Systems for Scalable Computers, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: Cilk [4] is similar but it targets a more restricted class of computations (strict computations). The scheduling policy is fixed, and for a certain class of applications is provably efficient with respect to time, space and communication. In contrast to the Cilk runtime system, Multipol <ref> [9] </ref> allows more flexibility to the programmer.
Reference: 10. <author> N. Sundaresan and L. Lee, </author> <title> An object-oriented thread model for parallel numerical applications. </title> <booktitle> Proceedings of the 2n Annual Object-Oriented Numerics Conference - OONSKI 94, Sunriver, Oregon, </booktitle> <pages> pp 291-308, </pages> <month> April 24-27 </month> <year> 1994. </year>
Reference-contexts: The communication module provides the necessary support for the implementation of a global address space over both shared and distributed memory machines. Collective communication primitives are not considered in this paper. In the future, we plan to evaluate and use the "rope" primitives introduced in <ref> [10] </ref> and [20]. As in Split-C, NEXUS, TULIP and Cid, our communication abstraction is the global pointer. A global pointer essentially consists of a context number and a pointer to the local address space. <p> In the runtime substrate, TULIP provides basic communication via global pointers and remote service requests. Then, at the pC++ language level, there is the concept of threads and the notion of group thread actions <ref> [10, 20] </ref>. Communication is one module, the basic threads functions (i.e., creation, thread synchronization, etc) are in another module, and the two are combined into the rope module.
Reference: 11. <author> I. Foster, Carl Kesselman, Steve Tuecke, </author> <title> Portable Mechanisms for Multithreaded Distributed Computations Argonne National Laboratory, </title> <publisher> MCS-P494-0195. </publisher>
Reference-contexts: There is a proposed API for communication, and for the integration of communication with threads [17]. The PORTS consortium has been experimenting with four different approaches. 1. Thread-to-thread communication, supported by CHANT [2]. 2. Remote service request communication, supported by NEXUS <ref> [11] </ref>. 3. Hybrid communication, supported by TULIP [25]. 4. The DMCS approach outlined in this paper. In this context, our DMCS implementation accomplishes the following. <p> The RSR driven communication paradigm is implemented in NEXUS which a portable multithreaded communication library for parallel language compilers and higher-level communication libraries <ref> [11] </ref>. TULIP's hybrid approach is essentially a combination of thread-to-thread and RSR driven communication paradigm [25]. The hybrid approach is essentially a combination of thread-to-thread and RSR driven communication paradigm and is supported by TULIP [25].
Reference: 12. <author> Ian Foster, Carl Kesselman and Steven Tuecke, </author> <title> The NEXUS approach to integrating multithreading and communication, </title> <institution> Argonne National Laboratory. </institution>
Reference: 13. <author> Ralph M. Butler, and Ewing L. Lusk, </author> <title> User's Guide to p4 Parallel Programming System Oct 1992, </title> <institution> Mathematics and Computer Science division, Argonne National Laboratory. </institution>
Reference-contexts: CHANT implements thread-to-thread communication on top of portable message passing software layers such as p4 <ref> [13] </ref>, PVM [18], and MPI [8]. The efficiency of this mechanism depends critically on the implementation of message polling or message delivery interrupt.
Reference: 14. <author> Nikos Chrisochoides, Florian Sukup, </author> <title> Task parallel implementation of the Bowyer-Watson algorithm, CTC96TR235, </title> <type> Technical Report, </type> <institution> Cornell Theory Center, </institution> <year> 1996. </year>
Reference-contexts: DMCS is being currently used to implement a task-parallel version of the Bowyer-Watson algorithm for mesh generation <ref> [14] </ref>. This algorithm provides an ideal mesh refinement strategy for a large class of unstructured mesh generation techniques on both sequential and parallel computers, by preventing the need for global mesh refinements. This application has been ported from an active message implementation to a PORTS implementation on top of DMCS.
Reference: 15. <institution> Portable Runtime System (PORTS) consortium, </institution> <note> http://www.cs.uoregon.edu/research/paracomp/ports/ </note>
Reference-contexts: Issues not addressed by MPI, such as dynamic resource management, concurrency at the uniprocessor level and interoperability at the language level, need to be addressed by such substrates. These issues are being addressed by a consortium called POrtable Run-time Systems (PORTS) <ref> [15] </ref> which consists of research universities, national laboratories, and computer vendors interested in advancing research for software ? This work supported by the Cornell Theory Center which receives major funding from the National Science Foundation, IBM corporation, New York State and members of the its Corporate Research Institute. ?? Chrisochoides' current
Reference: 16. <institution> PORTS Level 0 Thread Modules from Argonne/CalTech, ftp://ftp.mcs.anl.gov/pub/ports/ </institution>
Reference-contexts: The first API, ports threads, has already been agreed upon by the PORTS consortium. It comprises of a set of functions for lightweight thread management, modeled after a subset of the POSIX thread interface. An implementation of the ports threads interface is available from Argonne National Laboratory <ref> [16] </ref>. In addition, a set of functions have been specified for timing and event logging, using the high resolution, synchronized clocks available on many shared and distributed memory supercomputers. The timer package is thread safe, but not thread aware. <p> The threads module supports the primitives defined by the PORTS consortium, ports threads. We are using the implementation provided by the Argonne group, PORTS0 <ref> [16] </ref>, augmented by an extra routine: ports thread create atonce. The efficient implementation of this routine is necessary to minimize the scheduling latency of certain urgent, remote service requests [7].
Reference: 17. <institution> A Proposal for PORTS Level 1 Communication Routines, </institution> <note> http://www.cs.uoregon.edu/research/paracomp/ports </note>
Reference-contexts: An extremely fast implementation of ports timing is available from University of Oregon [24]. There is a proposed API for communication, and for the integration of communication with threads <ref> [17] </ref>. The PORTS consortium has been experimenting with four different approaches. 1. Thread-to-thread communication, supported by CHANT [2]. 2. Remote service request communication, supported by NEXUS [11]. 3. Hybrid communication, supported by TULIP [25]. 4. The DMCS approach outlined in this paper.
Reference: 18. <author> A. Belguelin, J. Dongarra, A. Geist, R. Manchek, S. Otto, and J. Walpore, </author> <title> PVM: Experiences, current status and future direction. </title> <booktitle> Supercomputing'93 Proceedings, </booktitle> <pages> pp 765-6. </pages>
Reference-contexts: CHANT implements thread-to-thread communication on top of portable message passing software layers such as p4 [13], PVM <ref> [18] </ref>, and MPI [8]. The efficiency of this mechanism depends critically on the implementation of message polling or message delivery interrupt.
Reference: 19. <author> David Keppel, </author> <title> Tools and Techniques for Building Fast Portable Threads Package, UW-CSE-93-05-06, </title> <type> Technical Report, </type> <institution> University of Washington at Seattle, </institution> <year> 1993. </year>
Reference-contexts: A prototype is implemented on top of the QuickThreads <ref> [19] </ref>, which has been ported to a wide variety of workstation and PC architectures. The communication module provides the necessary support for the implementation of a global address space over both shared and distributed memory machines. Collective communication primitives are not considered in this paper.
Reference: 20. <institution> Data Parallel Programming in a Multithreaded Environment, </institution> <note> (Need authors...)to appear i a Special Issue of Scientific Programming, </note> <year> 1996. </year>
Reference-contexts: The communication module provides the necessary support for the implementation of a global address space over both shared and distributed memory machines. Collective communication primitives are not considered in this paper. In the future, we plan to evaluate and use the "rope" primitives introduced in [10] and <ref> [20] </ref>. As in Split-C, NEXUS, TULIP and Cid, our communication abstraction is the global pointer. A global pointer essentially consists of a context number and a pointer to the local address space. <p> In the runtime substrate, TULIP provides basic communication via global pointers and remote service requests. Then, at the pC++ language level, there is the concept of threads and the notion of group thread actions <ref> [10, 20] </ref>. Communication is one module, the basic threads functions (i.e., creation, thread synchronization, etc) are in another module, and the two are combined into the rope module.
Reference: 21. <author> Chichao Chang, Grzegorz Czajkowski, Chris Hawblitzell and Thorsten von Eicken, </author> <title> Low-latency communication on the IBM risc system/6000 SP. </title> <note> To appear in Supercomputing '96. </note>
Reference-contexts: Attributes currently implemented are the stack size of a thread, and whether the thread should use a pre-allocated stack, or do a fresh allocation for its stack. 3.2 Communication subpackage The communication subpackage is implemented on top of a generic active message implementation on the SP-2 <ref> [1, 21] </ref>. Active messages are a mechanism for asynchronous, low-overhead communication. The fundamental idea in Active Messages is that every message is sent along with a reference to a handler which is invoked on receipt of the message. <p> The generic active message specification provides for small messages as well as bulk transfer routines. The implementation of this specification on the SP-2 <ref> [21] </ref> is optimized so that small messages are delivered as efficiently as possible; for sufficiently large messages, the bandwidth attained is very close to the peak hardware bandwidth. DMCS provides a homogeneous, data driven, asynchronous and efficient run-time environment.
Reference: 22. <author> David E. Culler, Andrea Dusseau, Seth Copen Goldstein, Arvind Krishnamurthy, Steven Lumetta, Thorsten von Eicken and Katherine Yelick. </author> <title> Parallel Programming in Split-C. </title> <publisher> Supercomputing'93. </publisher>
Reference-contexts: Finally, functionality similar to DMCS threads and communication and control modules are provided by a number of other runtime systems like Cid, Split-C, Cilk, and Multipol. Cid [3] and Split-C <ref> [22] </ref> are parallel extensions to C. Both systems support a global address space through the abstraction of the global pointer.
Reference: 23. <author> Veena Avula. </author> <title> SplitThreads - Split-C threads. </title> <type> Masters thesis, </type> <institution> Cornell University. </institution> <year> 1994. </year>
Reference-contexts: Finally, the control module provides some limited support for simple load balancing by allowing associating a window within which load on any processor can be balanced. This load-balancing support was chosen after experiments with the SplitThreads <ref> [23] </ref> system. 3 Implementation In this section, we discuss the thread, communication and control modules of DMCS. 3.1 Thread subpackage The underlying threads package consists of a user-level threads core called Quick-Threads, which is a non-preemptive user-level threads package for thread creation and initialization. <p> Cid [3] and Split-C [22] are parallel extensions to C. Both systems support a global address space through the abstraction of the global pointer. They also implement asynchronous, one-sided communication, and multithreading (either in the language as in Cid, or through extensions as in SplitThreads <ref> [23] </ref>), and have mechanisms for overlapping computation with communication and synchronization latencies. Cilk [4] is similar but it targets a more restricted class of computations (strict computations). The scheduling policy is fixed, and for a certain class of applications is provably efficient with respect to time, space and communication.
Reference: 24. <institution> Portable Clock and Timer Module from Oregon, </institution> <note> http://www.cs.uoregon.edu/research/paracomp/ports </note>
Reference-contexts: The timer package is thread safe, but not thread aware. In other words, a correct implementation of this specification can be used in a preemptive thread environment, however the specification does not require threads. An extremely fast implementation of ports timing is available from University of Oregon <ref> [24] </ref>. There is a proposed API for communication, and for the integration of communication with threads [17]. The PORTS consortium has been experimenting with four different approaches. 1. Thread-to-thread communication, supported by CHANT [2]. 2. Remote service request communication, supported by NEXUS [11]. 3.
Reference: 25. <author> Pete Beckman and Dennis Gannon, Tulip: </author> <title> Parallel Run-time Support System for pC++, http://www.extreme.indiana.edu. This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: There is a proposed API for communication, and for the integration of communication with threads [17]. The PORTS consortium has been experimenting with four different approaches. 1. Thread-to-thread communication, supported by CHANT [2]. 2. Remote service request communication, supported by NEXUS [11]. 3. Hybrid communication, supported by TULIP <ref> [25] </ref>. 4. The DMCS approach outlined in this paper. In this context, our DMCS implementation accomplishes the following. <p> The RSR driven communication paradigm is implemented in NEXUS which a portable multithreaded communication library for parallel language compilers and higher-level communication libraries [11]. TULIP's hybrid approach is essentially a combination of thread-to-thread and RSR driven communication paradigm <ref> [25] </ref>. The hybrid approach is essentially a combination of thread-to-thread and RSR driven communication paradigm and is supported by TULIP [25]. In the runtime substrate, TULIP provides basic communication via global pointers and remote service requests. <p> TULIP's hybrid approach is essentially a combination of thread-to-thread and RSR driven communication paradigm <ref> [25] </ref>. The hybrid approach is essentially a combination of thread-to-thread and RSR driven communication paradigm and is supported by TULIP [25]. In the runtime substrate, TULIP provides basic communication via global pointers and remote service requests. Then, at the pC++ language level, there is the concept of threads and the notion of group thread actions [10, 20].
References-found: 25

