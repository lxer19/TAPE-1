URL: ftp://ftp.cs.unc.edu/pub/users/jeffay/papers/CN-ISDN-94.ps.Z
Refering-URL: http://www.cs.unc.edu/Research/multimedia.html
Root-URL: http://www.cs.unc.edu
Email: -jeffay,stone,smithfd-@cs.unc.edu  
Title: Transport and Display Mechanisms For Multimedia Conferencing Across Packet-Switched Networks  
Author: K. Jeffay, D.L. Stone, F.D. Smith 
Note: Published in: Computer Networks and ISDN Systems, Vol. 26, No. 10, (July 1994) pp. 1281-1304. This work supported in parts by grants from the National Science Foundation (numbers CCR-9110938 and ICI 9015443), and by the Digital Equipment Corporation and the IBM Corporation.  
Date: September, 1993  
Address: Chapel Hill, NC 27599-3175 USA  
Affiliation: University of North Carolina at Chapel Hill Department of Computer Science  
Abstract: A transport protocol that supports real-time communication of audio/video frames across campus-area packet switched networks is presented. It is a best effort protocol that attempts to ameliorate the effect of jitter, load variation, and packet loss, to provide low latency, synchronized audio and video communications. This goal is realized through four transport and display mechanisms, and a real-time implementation of these mechanisms that integrates operating system services ( e.g., scheduling and resource allocation, and device management) with network communication services ( e.g., transport protocols), and with application code ( e.g., display routines). The four mechanisms are: a facility for varying the synchronization between audio and video to achieve continuous audio in the face of jitter, a network congestion monitoring mechanism that is used to control audio/video latency, a queueing mechanism at the sender that is used to maximize frame throughput without unnecessarily increasing latency, and a forward error correction mechanism for transmitting audio frames multiple times to ameliorate the effects of packet loss in the network. The effectiveness of these techniques is demonstrated by measuring the performance of the protocol when transmitting audio and video across congested networks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Abdel-Wahab, H., Feit, M. A., </author> <year> 1991. </year> <title> XTV: A Framework for Sharing X Window Clients in Remote Synchronous Collaboration , Proc., </title> <booktitle> IEEE Conf. on Communications Software: Communications for Distributed Applications & Systems, </booktitle> <address> Chapel Hill, NC, </address> <month> April </month> <year> 1991, </year> <pages> pp. 159-167. </pages>
Reference: [2] <author> Anderson, D.P., Herrtwich, R.G., Schaefer, C., </author> <year> 1990. </year> <title> SRP: A Resource Reservation Protocol for Guaranteed Performance Communication in the Internet, </title> <institution> University of California Berkeley, Dept. of Electrical Eng. and Computer Science Technical Report, TR-90-006, </institution> <month> February </month> <year> 1990. </year>
Reference-contexts: A stream is a totally ordered, continuous sequence of frames.) The work reported here does not consider the effect of special network services such as admission control and resource reservation <ref> [2] </ref>. While such services are clearly useful and important, they equally clearly imply additional costs to modify routing software at all network interconnection points, and possible reductions in the total available bandwidth.
Reference: [3] <author> Anderson, D.P., Tzou, S.-Y., Wahbe, R., Govindan, R., Andrews, M., </author> <year> 1990. </year> <title> Support for Continuous Media in the DASH System, </title> <booktitle> Proc. Tenth Intl. Conf. on Distributed Computing Systems, </booktitle> <address> Paris, France, </address> <month> May </month> <year> 1990, </year> <pages> pp. 54-61. </pages>
Reference-contexts: This means that new capabilities for processing digital audio and video are required for desktop computing environments, especially in operating system and network services. The problem of providing operating system support for these applications has been addressed by ourselves and others <ref> [3, 6, 11, 17] </ref>. The work described here assumes appropriate services, especially those for real-time scheduling of tasks, are available in the underlying operating system.
Reference: [4] <author> Ferrari, D., </author> <year> 1990. </year> <title> Client Requirements for Real-Time Communication Services, </title> <journal> IEEE Communications, </journal> <month> (November), </month> <pages> pp. 65-72. </pages>
Reference-contexts: End-to-end latency is the elapsed time between the acquisition of a frame at the sender and the display of the frame at a receiver. The end-to-end latency should be kept under 250 ms for most conferencing applications <ref> [4] </ref>. A discontinuity occurs when frame n + 1 is not displayed immediately after frame n. This occurs if a frame is lost from the stream or does not arrive at the receiver in time to be played when the previous frame is finished.
Reference: [5] <author> Ferrari, D., </author> <year> 1991. </year> <title> Design and Application of a Jitter Control Scheme for Packet-Switch Internetworks, </title> <booktitle> Proc. 2nd Intl. Workshop on Network and Operating System Support for Digital Audio and Video, </booktitle> <address> Heidelberg, Germany, </address> <pages> pp. 81-84. 28 </pages>
Reference: [6] <author> Govindan, R., Anderson, D.P., </author> <year> 1991. </year> <title> Scheduling and IPC Mechanisms for Continuous Media , Proc. </title> <booktitle> ACM Symp. on Operating Systems Principles, ACM Operating Systems Review, </booktitle> <volume> Vol. 25, No. 5, </volume> <month> (October), </month> <pages> pp. 68-80. </pages>
Reference-contexts: This means that new capabilities for processing digital audio and video are required for desktop computing environments, especially in operating system and network services. The problem of providing operating system support for these applications has been addressed by ourselves and others <ref> [3, 6, 11, 17] </ref>. The work described here assumes appropriate services, especially those for real-time scheduling of tasks, are available in the underlying operating system.
Reference: [7] <author> Harney, K., Keith, M., Lavelle, G., Ryan, L.D., Stark, D.J., </author> <year> 1991. </year> <title> The i750 Video Processor: A Total Multimedia Solution, </title> <journal> Comm. of the ACM, </journal> <volume> Vol. 34, No. </volume> <month> 4 (April), </month> <pages> pp. 64-79. </pages>
Reference-contexts: Experimental System and Metrics We have constructed a testbed for experimenting with the transmission of live digital audio and video across IP networks. For acquisition and display we use IBM-Intel ActionMedia 750 hardware <ref> [7, 11] </ref> and IBM PS/2 workstations. The PS/2s run a real-time operating system we have developed (YARTOS [10]) and support UDP/IP.
Reference: [8] <author> Hopper, A., </author> <year> 1990. </year> <title> Pandora An Experimental System For Multimedia Applications, </title> <journal> ACM Operating Systems Review, </journal> <volume> Vol. 24, No. 2, </volume> <month> (April), </month> <pages> pp. 19-34. </pages>
Reference-contexts: Computer-based conferencing systems attempt to create a feeling of virtual presence among team members not in the same physical location by providing shared access to computer-based materials along with links for human communications (e.g., <ref> [8, 9, 14, 20] </ref>). For example, a team of software designers might use a shared whiteboard (drawing) application to sketch the logical structure of a program, a shared editor to outline the documentation, and shared audio and video links to discuss alternatives and reach consensus (see Figure 1.1).
Reference: [9] <author> Ishii, H, Miyake, N., </author> <year> 1991. </year> <title> Toward an Open Shared Workspace: Computer and Video Fusion Approach of Team Workstation, </title> <journal> Comm. of the ACM, </journal> <volume> Vol. 34, No. 12 (December), </volume> <pages> pp. 37-50. </pages>
Reference-contexts: Computer-based conferencing systems attempt to create a feeling of virtual presence among team members not in the same physical location by providing shared access to computer-based materials along with links for human communications (e.g., <ref> [8, 9, 14, 20] </ref>). For example, a team of software designers might use a shared whiteboard (drawing) application to sketch the logical structure of a program, a shared editor to outline the documentation, and shared audio and video links to discuss alternatives and reach consensus (see Figure 1.1).
Reference: [10] <author> Jeffay, K., Stone, D., Poirier, D., </author> <year> 1992. </year> <title> YARTOS: Kernel support for efficient, predictable real-time systems , in Real-Time Programming, </title> <editor> W. Halang and K. Ramamritham, eds., </editor> <publisher> Pergamon Press, Oxford, </publisher> <address> UK. </address>
Reference-contexts: For acquisition and display we use IBM-Intel ActionMedia 750 hardware [7, 11] and IBM PS/2 workstations. The PS/2s run a real-time operating system we have developed (YARTOS <ref> [10] </ref>) and support UDP/IP.
Reference: [11] <author> Jeffay, K., Stone, D.L., and Smith, F.D., </author> <year> 1992. </year> <title> Kernel Support for Live Digital Audio and Video. </title> <journal> Computer Communications, </journal> <volume> Vol. 16, No. </volume> <month> 6 (July), </month> <pages> pp. 388-395. </pages>
Reference-contexts: This means that new capabilities for processing digital audio and video are required for desktop computing environments, especially in operating system and network services. The problem of providing operating system support for these applications has been addressed by ourselves and others <ref> [3, 6, 11, 17] </ref>. The work described here assumes appropriate services, especially those for real-time scheduling of tasks, are available in the underlying operating system. <p> Experimental System and Metrics We have constructed a testbed for experimenting with the transmission of live digital audio and video across IP networks. For acquisition and display we use IBM-Intel ActionMedia 750 hardware <ref> [7, 11] </ref> and IBM PS/2 workstations. The PS/2s run a real-time operating system we have developed (YARTOS [10]) and support UDP/IP. <p> The effective use of such hardware requires that real - time services be provided by the operating system <ref> [11] </ref>. The ActionMedia hardware provides a two stage pipeline for either acquisition or display of video data.
Reference: [12] <author> Jeffay, K., </author> <year> 1992. </year> <title> Scheduling Sporadic Tasks with Shared Resources in Hard-Real-Time Systems, </title> <booktitle> Proc. 13 th IEEE Real-Time Systems Symp., </booktitle> <address> Phoenix, AZ, </address> <month> December </month> <year> 1992, </year> <pages> pp. 89-99. </pages>
Reference-contexts: Each frame has a deadline equal to its generation time plus one frame time. Packets are then scheduled at the network interface using a deadline-based scheduling algorithm <ref> [12] </ref>. 20 Once an audio frame has been transmitted it is placed on a retransmission queue. Frames from this queue are used to fill the forward error correction portion of every MTP packet. Two parameters, a minimum and maximum transmission separation, drive this process.
Reference: [13] <author> Jeffay, K., Lin, J.K., Menges, J., Smith, F.D., Smith, J.B., </author> <year> 1992. </year> <title> Architecture of the Artifact-Based Collaboration System Matrix , CSCW 92, </title> <booktitle> Proc. of the Conf. on ComputerSupported Cooperative Work, </booktitle> <address> Toronto, Canada, </address> <publisher> ACM Press, </publisher> <month> November </month> <year> 1992, </year> <pages> pp. 195-202. </pages>
Reference: [14] <author> Katseff, H.P., Gaglianello, R.D., London, T.B., Robinson, B.S., Swicker. D.B., </author> <year> 1990. </year> <title> An Overview of the Liaison Network Multimedia Workstation, </title> <booktitle> Proc. First Intl. Workshop on Network and Operating System Support for Digital Audio and Video, </booktitle> <address> Berkeley, CA. </address>
Reference-contexts: Computer-based conferencing systems attempt to create a feeling of virtual presence among team members not in the same physical location by providing shared access to computer-based materials along with links for human communications (e.g., <ref> [8, 9, 14, 20] </ref>). For example, a team of software designers might use a shared whiteboard (drawing) application to sketch the logical structure of a program, a shared editor to outline the documentation, and shared audio and video links to discuss alternatives and reach consensus (see Figure 1.1).
Reference: [15] <author> Le Gall, D., </author> <year> 1991. </year> <title> MPEG: A Video Compression Standard for Multimedia Applications, </title> <journal> Comm. of the ACM, </journal> <volume> Vol. 34, No. 4, </volume> <month> (April), </month> <pages> pp. 46-58. </pages>
Reference: [16] <author> Liou M., </author> <year> 1991. </year> <title> Overview of the px64 kbit/s Video Coding Standard , Comm. </title> <journal> of the ACM, </journal> <volume> Vol. 34, No. 4, </volume> <month> (April), </month> <pages> pp. 59-63. </pages>
Reference: [17] <author> Rangan, P.V., Vin, H.M., </author> <year> 1991. </year> <title> Designing File Systems for Digital Video and Audio, </title> <booktitle> Proc. ACM Symp. on Operating Systems Principles, ACM Operating Systems Review, </booktitle> <volume> Vol. 25, No. 5, </volume> <month> (October), </month> <pages> pp. 81-94. </pages>
Reference-contexts: This means that new capabilities for processing digital audio and video are required for desktop computing environments, especially in operating system and network services. The problem of providing operating system support for these applications has been addressed by ourselves and others <ref> [3, 6, 11, 17] </ref>. The work described here assumes appropriate services, especially those for real-time scheduling of tasks, are available in the underlying operating system.
Reference: [18] <author> Steinmetz, R., Meyer, T., </author> <year> 1992. </year> <title> Multimedia Synchronization Techniques: Experiences Based on Different System Structures, </title> <booktitle> IEEE Multimedia Workshop, </booktitle> <address> Monterey, CA, </address> <month> April, </month> <year> 1992. </year>
Reference-contexts: Also related to users perception of the conference is the synchronization between audio and video frames (i.e., lip sync). For this work we assume that audio and video should never be more than 100 ms out of synchronization <ref> [18] </ref>. 2.2 System Structure The basic structure for computer-based conferencing systems using todays commodity technologies is shown in Figure 2.1. It is useful to view the processing components as a multistage distributed pipeline through which frames flow from source to destination. In individual frames are shown as squares. <p> Audio should be played ahead of video only to the extent that audio discontinuities are perceived as more annoying the than loss of audio/video synchronization. For full - motion video, we conjecture that users will tolerate audio being played by 100 ms ahead of video <ref> [18] </ref>. Informal user studies indicate that this is the level at which the loss of synchronization is first noticeable in our system (given a particular video compression algorithm and its resulting image quality).
Reference: [19] <author> Wallace, G.K., </author> <year> 1991. </year> <title> The JPEG Still Picture Compression Standard, </title> <journal> Comm. of the ACM, </journal> <volume> Vol. 34, No. 4, </volume> <month> (April), </month> <pages> pp. 30-44. </pages>
Reference: [20] <author> Watabe, K., Sakata S., Maeno K., Fukuoka H., Ohmori T., </author> <year> 1990. </year> <title> Distributed Multiparty Desktop Conferencing System: </title> <booktitle> MERMAID, Proceedings, CSCW 90 Conference on Computer-Supported Cooperative Work, </booktitle> <month> October </month> <year> 1990. </year>
Reference-contexts: Computer-based conferencing systems attempt to create a feeling of virtual presence among team members not in the same physical location by providing shared access to computer-based materials along with links for human communications (e.g., <ref> [8, 9, 14, 20] </ref>). For example, a team of software designers might use a shared whiteboard (drawing) application to sketch the logical structure of a program, a shared editor to outline the documentation, and shared audio and video links to discuss alternatives and reach consensus (see Figure 1.1).
References-found: 20

