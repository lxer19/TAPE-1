URL: http://www-cgi.cs.cmu.edu/afs/cs/project/cmcl/archive/CreditNet-papers/96ics.ps
Refering-URL: 
Root-URL: 
Title: Fine Grain Parallel Communication on General Purpose LANs  
Author: Todd Mummert, Corey Kosak, Peter Steenkiste, Allan Fisher 
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: In this paper we present a host-network interface architecture that supports efficient remote memory writes across standard ATM networks, bringing performance closer to that of special-purpose, tightly coupled systems for a large class of applications. We show that minimal hardware support is required on the adaptor, and that the required features are very similar to those already needed on adaptors for high-speed networks. We also describe an implementation of this architecture, and present measurements of communication performance indicating its effect on the breadth of applications that can use a general purpose network in an effective way. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Adams. </author> <title> Cray T3D system architecture overview. </title> <institution> Cray Research Inc., </institution> <month> September </month> <year> 1993. </year> <note> Revision 1.C. </note>
Reference-contexts: The main difference between workstation clusters and the more traditional tightly coupled distributed-memory systems (Paragon [15], T3D <ref> [1] </ref>) is communication performance and efficiency. Because of fundamental differences in their architecture (use of standard I/O bus, possibility of network errors), workstation clusters have communication overhead and latencies that can be as much as two orders of magnitude higher than tightly coupled systems. <p> Besides this common feature, remote memory access models can be quite diverse, both in their implementation (hardware versus software support) and in the details of their semantics. For example, in some cases the model supports both read and write (e.g. Cray T3D <ref> [1] </ref>), while in other implementations, only remote writes are supported (e.g., Shrimp [6] and deposit model [26]). The main advantage of the remote memory access model over the more traditional message passing model is that it decouples control and data transfers. <p> The remote memory access model has been implemented both completely in hardware (e.g. Cray T3D <ref> [1] </ref>), Shrimp [6] and in software (e.g. deposit model on iWarp [26]). The overheads and latency of a remote write are on the order of microseconds. In contrast, loosely-coupled systems are built from off-the-shelf nodes (workstations or PCs) and network hardware (e.g. FDDI or ATM). <p> An alternative is to check memory references explicitly [30]. The other columns present levels of support for remote PUT between these two extremes. The Cray T3D supports remote PUT and GET in hardware <ref> [1] </ref>. Note that in contrast to more traditional shared memory machines, the hardware does not provide memory consistency, i.e. the software has to implement an appropriate consistency protocol. Shrimp [6] uses custom adapters to connect PCs to a custom interconnect, specifically the Paragon interconnect.
Reference: [2] <author> Emmanuel Arnould, Francois Bitz, Eric Cooper, H. T. Kung, Robert Sansom, and Peter Steenkiste. </author> <title> The design of nectar: A network backplane for heterogeneous multicomputers. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, pages 205216, </booktitle> <address> Boston, </address> <month> April </month> <year> 1989. </year> <month> ACM/IEEE. </month>
Reference-contexts: A first approach is to rely on a powerful outboard processor to take over some of the communication tasks normally performed by the host, e.g. similar to the original Nectar systems <ref> [2] </ref>. An alternative is to provide dedicated hardware support, e.g. similar to the Shrimp interface [6].
Reference: [3] <author> Mary L. Bailey, Burra Gopal, Michael A. Pagels, Larry L. Pe-terson, and Prasenjit Sarkar. PATHFINDER: </author> <title> A Pattern-Based Packet Classifier. </title> <booktitle> In Proceeding Conference on Operating Systems Design and Implementation, volume Winter, </booktitle> <pages> pages 115124. </pages> <publisher> Usenix, </publisher> <month> November </month> <year> 1994. </year>
Reference-contexts: ATM VCIs provide an ideal demultiplexer. Although few if any current adapters for packet-based networks have the ability to interpret headers, early demultiplexing is widely recognized as a powerful mechanism, and both hardware and software solutions have been proposed <ref> [3] </ref>. intelligent buffer management, so that the adapter can manage multiple buffers simultaneously. Some of this functionality is required on any ATM adapter, since the cell-based nature of the network means that multiple packets can be in transit at the same time. <p> For example, a traditional send/receive model, although it involves recipient interaction for each individual message, will still benefit from the avoidance of copying [24, 16] and potential avoidance of context switching provided by HARPs support for demultiplexing. Similarly, conventional network level protocols like IP can benefit from automatic demultiplexing <ref> [3] </ref> and application-level access.
Reference: [4] <author> John Bennett, John Carter, and Willy Zwaenepoel. Munin: </author> <title> Distributed Shared Memory Based on Type-Specific Memory Coherence. </title> <booktitle> In Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 168176. </pages> <publisher> ACM, </publisher> <month> March </month> <year> 1990. </year>
Reference-contexts: The distributed shared memory column shows systems that implement the same programming model over workstations connected by a network. The main idea is to rely on the virtual memory system to catch memory accesses to memory regions currently allocated on a different node <ref> [4] </ref> [5]. An alternative is to check memory references explicitly [30]. The other columns present levels of support for remote PUT between these two extremes. The Cray T3D supports remote PUT and GET in hardware [1].
Reference: [5] <author> Brian N. Bershad, Matthew J. Zekauskas, and Wayne A. Saw-don. </author> <title> The midway distributed shared memory system. </title> <booktitle> In Proceedings of COMPCON. IEEE, </booktitle> <year> 1993. </year>
Reference-contexts: The distributed shared memory column shows systems that implement the same programming model over workstations connected by a network. The main idea is to rely on the virtual memory system to catch memory accesses to memory regions currently allocated on a different node [4] <ref> [5] </ref>. An alternative is to check memory references explicitly [30]. The other columns present levels of support for remote PUT between these two extremes. The Cray T3D supports remote PUT and GET in hardware [1].
Reference: [6] <author> Matthias Blumrich, Kai Li, Richard Alpert, Cesary Dubnicki, and Edward Felten. </author> <title> Virtual memory mapped network interface for the shrimp multicomputer. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 142153, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: For example, in some cases the model supports both read and write (e.g. Cray T3D [1]), while in other implementations, only remote writes are supported (e.g., Shrimp <ref> [6] </ref> and deposit model [26]). The main advantage of the remote memory access model over the more traditional message passing model is that it decouples control and data transfers. <p> The model can also be used to implement other programming models, such as message passing or coherent shared memory. Note that in the remote memory access model, memory consistency issues have to resolved explicitly by the higher level software. Some examples are discussed in <ref> [6] </ref>. 3 Implementation approach Tightly-coupled distributed-memory systems are characterized by a proprietary high-speed connect and a very tight coupling of the nodes and interconnect. <p> The remote memory access model has been implemented both completely in hardware (e.g. Cray T3D [1]), Shrimp <ref> [6] </ref> and in software (e.g. deposit model on iWarp [26]). The overheads and latency of a remote write are on the order of microseconds. In contrast, loosely-coupled systems are built from off-the-shelf nodes (workstations or PCs) and network hardware (e.g. FDDI or ATM). <p> A first approach is to rely on a powerful outboard processor to take over some of the communication tasks normally performed by the host, e.g. similar to the original Nectar systems [2]. An alternative is to provide dedicated hardware support, e.g. similar to the Shrimp interface <ref> [6] </ref>. While both approaches reduce communication overhead significantly, they have the disadvantage that they may lead to expensive adapters, and more importantly, since the adapter becomes more special purpose, they lose some of the advantages of the loosely-coupled systems architecture. <p> The Cray T3D supports remote PUT and GET in hardware [1]. Note that in contrast to more traditional shared memory machines, the hardware does not provide memory consistency, i.e. the software has to implement an appropriate consistency protocol. Shrimp <ref> [6] </ref> uses custom adapters to connect PCs to a custom interconnect, specifically the Paragon interconnect. Applications can set up mappings between memory areas in the private memories of different nodes.
Reference: [7] <author> Flavio Bonomi and Kerry Fendick. </author> <title> The rate-based ow control framework for the available bit rate atm service. </title> <journal> IEEE Network Magazine, </journal> <volume> 9(2):2539, </volume> <month> March/April </month> <year> 1995. </year>
Reference-contexts: The i960 has two major functions. First, it can serve as an accelerator for such tasks as link-by-link per-VC credit-based ow control [18, 22], ATM Forum-style rate control <ref> [7] </ref>, and cell scheduling algorithms on transmit that are beyond the capability of the scheduler available on the ASIC. Second, the i960 can provide direct application access to the device by validating network operations and carrying out a request-reply protocol with the host.
Reference: [8] <author> C. Brendan, S. Traw, and Jonathan M. Smith. </author> <title> Implementation and Performance of an ATM Host Interface for Workstations. In Workshop on the Architecture and Implementation of High Performance Communication Subsystems. </title> <publisher> IEEE, </publisher> <month> Febru-ary </month> <year> 1992. </year>
Reference-contexts: Several research and commercial adapters <ref> [10, 12, 8] </ref> follow this architecture. The adapter sits on the host I/O bus and has the following components: Xmit: a transmit VC table with VCs (virtual connections) open for transmit.
Reference: [9] <author> Jose Brustoloni. </author> <title> Exposed buffering and subdatagram ow control for ATM LANs. </title> <booktitle> In Proceedings of the 19th Conference on Local Computer Networks, </booktitle> <pages> pages 324334. </pages> <publisher> IEEE, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: Because of fundamental differences in their architecture (use of standard I/O bus, possibility of network errors), workstation clusters have communication overhead and latencies that can be as much as two orders of magnitude higher than tightly coupled systems. While this gap can be reduced by using optimized software <ref> [9] </ref>, a significant difference remains. In this paper we present a host interface architecture for workstations and PCs that reduces communication overhead.
Reference: [10] <author> Eric Cooper, Onat Menzilcioglu, Robert Sansom, and Francois Bitz. </author> <title> Host interface design for atm lans. </title> <booktitle> In Proceedings of the 16th Conference on Local Computer Networks, </booktitle> <pages> pages 247 258. </pages> <publisher> IEEE, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: Several research and commercial adapters <ref> [10, 12, 8] </ref> follow this architecture. The adapter sits on the host I/O bus and has the following components: Xmit: a transmit VC table with VCs (virtual connections) open for transmit.
Reference: [11] <author> Alan Cox, Sandhya Dwarkadas, Pete Keleher, Honghui Lu, Ramakrishnan Rajamony, and Willy Zwaenepoel. </author> <title> Software versus Hardware Shared-Memory Implementation: A Case Study. </title> <booktitle> In Proceedings of the 21th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 106117. </pages> <address> ACM/IEEE, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Commodity workstations connected by commodity networks are increasingly viewed as a viable alternative to traditional supercomputers and in recent years, many scientific computing applications have been able to make effective use of various types of workstation clusters <ref> [11, 19] </ref>. The main difference between workstation clusters and the more traditional tightly coupled distributed-memory systems (Paragon [15], T3D [1]) is communication performance and efficiency.
Reference: [12] <author> Bruce S. Davie. </author> <title> A Host-Network Interface Architecture for ATM. </title> <booktitle> In Proceedings of the SIGCOMM 91 Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 307315. </pages> <publisher> ACM, </publisher> <month> September </month> <year> 1991. </year>
Reference-contexts: Several research and commercial adapters <ref> [10, 12, 8] </ref> follow this architecture. The adapter sits on the host I/O bus and has the following components: Xmit: a transmit VC table with VCs (virtual connections) open for transmit.
Reference: [13] <author> The MPI Forum. </author> <title> MPI: A Message Passing Interface. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 878883, </pages> <address> Oregon, </address> <month> November </month> <year> 1993. </year> <month> ACM/IEEE. </month>
Reference-contexts: For example, this approach is well-suited to a parallelizing compiler or distributed object library, where the code is typically organized as a sequence of compute and communicate phases, and the communicate phases have the appropriate characteristics. Some message passing libraries, e.g. MPI <ref> [13] </ref>, also directly support collective communication operations that fit the requirements. Although our focus has been on collective communication, the features of the HARP architecture also provide efficient implementation for many other styles of communication.
Reference: [14] <author> Thomas Gross, Dave OHallaron, and Jaspal Subhlok. </author> <title> Task Parallelism in a High Performance Fortran Framework. </title> <booktitle> IEEE Parallel and Distributed Technology, </booktitle> <address> 2(3):1626, </address> <month> Fall </month> <year> 1994. </year>
Reference-contexts: The remote memory access model is typically not used directly by programmers, but is a target for programming tools such as par-allelizing compilers or runtime object libraries. An example is the Fx parallelizing FORTRAN compiler <ref> [14] </ref>, which uses the deposit model as a target. The deposit model has been implemented on the iWarp, Paragon and T3D systems [26]. The model can also be used to implement other programming models, such as message passing or coherent shared memory.
Reference: [15] <author> Intel Corporation. </author> <title> Paragon X/PS Product Overview, </title> <month> March </month> <year> 1991. </year>
Reference-contexts: The main difference between workstation clusters and the more traditional tightly coupled distributed-memory systems (Paragon <ref> [15] </ref>, T3D [1]) is communication performance and efficiency. Because of fundamental differences in their architecture (use of standard I/O bus, possibility of network errors), workstation clusters have communication overhead and latencies that can be as much as two orders of magnitude higher than tightly coupled systems.
Reference: [16] <author> Karl Kleinpaste, Peter Steenkiste, and Brian Zill. </author> <title> Software support for outboard buffering and checksumming. </title> <booktitle> In Proceedings of the SIGCOMM 95 Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 8798. </pages> <publisher> ACM, </publisher> <month> August/September </month> <year> 1995. </year>
Reference-contexts: Although our focus has been on collective communication, the features of the HARP architecture also provide efficient implementation for many other styles of communication. For example, a traditional send/receive model, although it involves recipient interaction for each individual message, will still benefit from the avoidance of copying <ref> [24, 16] </ref> and potential avoidance of context switching provided by HARPs support for demultiplexing. Similarly, conventional network level protocols like IP can benefit from automatic demultiplexing [3] and application-level access.
Reference: [17] <author> Corey Kosak, David Eckhardt, Todd Mummert, Peter Steenkiste, and Allan Fisher. </author> <title> Buffer management and ow control in the credit net ATM host interface. </title> <booktitle> In Proceedings of the 20th Conference on Local Computer Networks, </booktitle> <pages> pages 370378, </pages> <address> Minneapolis, </address> <month> October </month> <year> 1995. </year> <note> IEEE. </note>
Reference-contexts: This project has brought CMU, Intel, Harvard University and Bell Northern Research together to build a 622 Mbps ATM network that uses credit-based ow control for traffic management <ref> [17] </ref>. Adapters for standard 155 Mbps and 622 Mbps ATM/SONET links have been built by Intel [17]. Although the adapters are intended to be used as regular network interfaces, they support the HARP architecture. <p> This project has brought CMU, Intel, Harvard University and Bell Northern Research together to build a 622 Mbps ATM network that uses credit-based ow control for traffic management <ref> [17] </ref>. Adapters for standard 155 Mbps and 622 Mbps ATM/SONET links have been built by Intel [17]. Although the adapters are intended to be used as regular network interfaces, they support the HARP architecture.
Reference: [18] <author> H.T. Kung, T. Blackwell, and A. Chapman. </author> <title> Credit update protocol for ow-controlled ATM networks: Statistical mulit-plexing and adaptive credit allocation. </title> <booktitle> In Proceedings of the SIGCOMM 94 Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 101114. </pages> <publisher> ACM, </publisher> <month> August </month> <year> 1994. </year>
Reference-contexts: The i960 has its own set of request and response blocks that it can use to schedule cell transmission and to send/receive packets from/in the shared memory. The i960 has two major functions. First, it can serve as an accelerator for such tasks as link-by-link per-VC credit-based ow control <ref> [18, 22] </ref>, ATM Forum-style rate control [7], and cell scheduling algorithms on transmit that are beyond the capability of the scheduler available on the ASIC. Second, the i960 can provide direct application access to the device by validating network operations and carrying out a request-reply protocol with the host.
Reference: [19] <author> H.T. Kung, Peter Steenkiste, Marco Gubitoso, and Manpreet Khaira. </author> <title> Parallelizing a new class of large applications over high-speed networks. </title> <booktitle> In Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 167177. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Commodity workstations connected by commodity networks are increasingly viewed as a viable alternative to traditional supercomputers and in recent years, many scientific computing applications have been able to make effective use of various types of workstation clusters <ref> [11, 19] </ref>. The main difference between workstation clusters and the more traditional tightly coupled distributed-memory systems (Paragon [15], T3D [1]) is communication performance and efficiency.
Reference: [20] <author> Daniel Lenoski, James Laudon, Truman Joe, David Nakahira, Luis Stevens, Anoop Gupta, and John Hennessy. </author> <title> The DASH Prototype: Implementation and Performance. </title> <booktitle> In 19th Annual International Symposium in Computer Architecture, </booktitle> <pages> pages 92 102. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1992. </year>
Reference-contexts: Table 1 shows how the basic operations were implemented in some example systems that implement remote write on distributed memory hardware. The left-hand columns represent systems that implement a shared-memory programming model based on a weak consistency coherency protocol (e.g. <ref> [20] </ref>): remote memory operations are completely supported in hardware. The distributed shared memory column shows systems that implement the same programming model over workstations connected by a network.
Reference: [21] <author> Andreas Nowatzyk, Gunes Aybay, Michael Browne, Edmund Kelly, David Lee, and Michael Parkin. </author> <title> The S3.mp Scalable Shared Memory Multiprocessor. </title> <booktitle> In Proceedings of the Twenty-Seventh Hawaii International Conference on System Sciences - Vol. I: Architecture, </booktitle> <pages> pages 144153. </pages> <publisher> IEEE, </publisher> <month> January </month> <year> 1994. </year>
Reference-contexts: More recently, groups have proposed networks that sit in between these two extremes, i.e. they present a reliable interconnect that is exible enough to be used in a LAN environment (e.g. <ref> [21] </ref>). Table 1 shows how the basic operations were implemented in some example systems that implement remote write on distributed memory hardware. The left-hand columns represent systems that implement a shared-memory programming model based on a weak consistency coherency protocol (e.g. [20]): remote memory operations are completely supported in hardware.
Reference: [22] <author> K. K. Ramakrishnan and Peter Neuman. </author> <title> Integration of rate and credit schemes for atm ow control. </title> <journal> IEEE Network Magazine, </journal> <volume> 9(2):4956, </volume> <month> March/April </month> <year> 1995. </year>
Reference-contexts: The i960 has its own set of request and response blocks that it can use to schedule cell transmission and to send/receive packets from/in the shared memory. The i960 has two major functions. First, it can serve as an accelerator for such tasks as link-by-link per-VC credit-based ow control <ref> [18, 22] </ref>, ATM Forum-style rate control [7], and cell scheduling algorithms on transmit that are beyond the capability of the scheduler available on the ASIC. Second, the i960 can provide direct application access to the device by validating network operations and carrying out a request-reply protocol with the host.
Reference: [23] <author> Alfred Z. Spector. </author> <title> Performing Remote Operations Efficiently on a Local Computer Network. </title> <journal> Communications of the ACM, </journal> <volume> 25(4):246260, </volume> <month> April </month> <year> 1982. </year>
Reference-contexts: There were able to demonstrate a performance improvement over traditional message passing, mainly as a result of reduced context switch overhead on the receiving node <ref> [23, 27, 28] </ref>. It is possible to implement remote memory accesses more efficiently, i.e. reduce host overhead, by using a more powerful network adapter. <p> The deposit model on iWarp implements this model entirely in software [25]. Finally in the right-most column, a number of projects have explored the support of a remote PUT and GET for networks connected by LANs <ref> [23] </ref> [28] [27]. These systems typically rely on remote procedure calls to transfer commands and data kernel-to-kernel. They can be more efficient than traditional send-receive operations because the application does not have to be scheduled (i.e. data transfer without control transfer).
Reference: [24] <author> Peter A. Steenkiste, Brian D. Zill, H.T. Kung, Steven J. Schlick, Jim Hughes, Bob Kowalski, and John Mullaney. </author> <title> A host interface architecture for high-speed networks. </title> <booktitle> In Proceedings of the 4th IFIP Conference on High Performance Networks, </booktitle> <pages> pages A3 116, </pages> <address> Liege, Belgium, </address> <month> December </month> <year> 1992. </year> <title> IFIP, </title> <publisher> Elsevier. </publisher>
Reference-contexts: Although our focus has been on collective communication, the features of the HARP architecture also provide efficient implementation for many other styles of communication. For example, a traditional send/receive model, although it involves recipient interaction for each individual message, will still benefit from the avoidance of copying <ref> [24, 16] </ref> and potential avoidance of context switching provided by HARPs support for demultiplexing. Similarly, conventional network level protocols like IP can benefit from automatic demultiplexing [3] and application-level access.
Reference: [25] <author> J. Stichnoth, D. OHallaron, and T. Gross. </author> <title> Generating Communication for Array Statements: Design, Implementation, and Evaluation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1):150159, </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: Once these mappings have been established, applications can transfer data between these memory areas in a light weight fashion; the options are automatic updates, or updates under software control (i.e. the application issues a PUT). The deposit model on iWarp implements this model entirely in software <ref> [25] </ref>. Finally in the right-most column, a number of projects have explored the support of a remote PUT and GET for networks connected by LANs [23] [28] [27]. These systems typically rely on remote procedure calls to transfer commands and data kernel-to-kernel.
Reference: [26] <author> T. Stricker, J. Stichnoth, D. OHallaron, S. Hinrichs, and T. Gross. </author> <title> Decoupling synchronization and data transfer in message passing systems of parallel computers. </title> <booktitle> In Proc. Intl. Conf. on Supercomputing, </booktitle> <pages> page accepted, </pages> <address> Barcelona, </address> <month> July </month> <year> 1995. </year> <note> ACM. </note>
Reference-contexts: For example, in some cases the model supports both read and write (e.g. Cray T3D [1]), while in other implementations, only remote writes are supported (e.g., Shrimp [6] and deposit model <ref> [26] </ref>). The main advantage of the remote memory access model over the more traditional message passing model is that it decouples control and data transfers. <p> An example is the Fx parallelizing FORTRAN compiler [14], which uses the deposit model as a target. The deposit model has been implemented on the iWarp, Paragon and T3D systems <ref> [26] </ref>. The model can also be used to implement other programming models, such as message passing or coherent shared memory. Note that in the remote memory access model, memory consistency issues have to resolved explicitly by the higher level software. <p> The remote memory access model has been implemented both completely in hardware (e.g. Cray T3D [1]), Shrimp [6] and in software (e.g. deposit model on iWarp <ref> [26] </ref>). The overheads and latency of a remote write are on the order of microseconds. In contrast, loosely-coupled systems are built from off-the-shelf nodes (workstations or PCs) and network hardware (e.g. FDDI or ATM). This architecture differs in a number of fundamental ways.
Reference: [27] <author> Chandramohan A. Thekkath, Henry M. Levy, and Edward D. Lazowska. </author> <title> Efficient Support for Multicomputing on ATM Networks. </title> <type> Technical Report UW-CSE-93-04-03, </type> <institution> Computer Science and Engineering, University of Washington, </institution> <month> April 12 </month> <year> 1993. </year>
Reference-contexts: There were able to demonstrate a performance improvement over traditional message passing, mainly as a result of reduced context switch overhead on the receiving node <ref> [23, 27, 28] </ref>. It is possible to implement remote memory accesses more efficiently, i.e. reduce host overhead, by using a more powerful network adapter. <p> The deposit model on iWarp implements this model entirely in software [25]. Finally in the right-most column, a number of projects have explored the support of a remote PUT and GET for networks connected by LANs [23] [28] <ref> [27] </ref>. These systems typically rely on remote procedure calls to transfer commands and data kernel-to-kernel. They can be more efficient than traditional send-receive operations because the application does not have to be scheduled (i.e. data transfer without control transfer).
Reference: [28] <author> Chandramohan A. Thekkath, Henry M. Levy, and Edward D. Lazowska. </author> <title> Separating data and control transfer in distributed operating systems. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 211, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: There were able to demonstrate a performance improvement over traditional message passing, mainly as a result of reduced context switch overhead on the receiving node <ref> [23, 27, 28] </ref>. It is possible to implement remote memory accesses more efficiently, i.e. reduce host overhead, by using a more powerful network adapter. <p> The deposit model on iWarp implements this model entirely in software [25]. Finally in the right-most column, a number of projects have explored the support of a remote PUT and GET for networks connected by LANs [23] <ref> [28] </ref> [27]. These systems typically rely on remote procedure calls to transfer commands and data kernel-to-kernel. They can be more efficient than traditional send-receive operations because the application does not have to be scheduled (i.e. data transfer without control transfer).
Reference: [29] <author> Z. Wang and J. Crowcroft. </author> <title> Seal detects cell misordering. </title> <journal> IEEE Network Magazine, </journal> <volume> 6(4):89, </volume> <month> July </month> <year> 1992. </year>
Reference-contexts: If the entire interconnect is a single ATM network, we can rely on ATMs HEC and the AAL5 CRC for error detection <ref> [29] </ref>. The 32 bit CRC over the data is very strong. A cell with an incorrect header that passes the relatively weak HEC will create an incorrect AAL5 byte count and will also nearly always cause a CRC error.
Reference: [30] <author> Matthew J. Zekauskas, Wayne A. Sawdon, and Brian N. Ber-shad. </author> <title> Software Write Detection for a Distributed Shared Memory. </title> <booktitle> In Proceeding Conference on Operating Systems Design and Implementation, volume Winter, </booktitle> <pages> pages 87100. </pages> <publisher> Usenix, </publisher> <month> November </month> <year> 1994. </year>
Reference-contexts: The main idea is to rely on the virtual memory system to catch memory accesses to memory regions currently allocated on a different node [4] [5]. An alternative is to check memory references explicitly <ref> [30] </ref>. The other columns present levels of support for remote PUT between these two extremes. The Cray T3D supports remote PUT and GET in hardware [1].
References-found: 30

