URL: ftp://ftp.cag.lcs.mit.edu/pub/raw/documents/raw-tr.ps.Z
Refering-URL: http://cag-www.lcs.mit.edu/~mtaylor/paper.html
Root-URL: 
Title: Baring it all to Software: The Raw Machine  
Author: Elliot Waingold, Michael Taylor, Vivek Sarkar, Walter Lee, Victor Lee, Jang Kim, Matthew Frank, Peter Finch, Srikrishna Devabhaktuni, Rajeev Barua, Jonathan Babb, Saman Amarasinghe, Anant Agarwal 
Address: Cambridge, MA 02139  
Affiliation: MIT Laboratory for Computer Science  
Abstract: Rapid advances in technology force a quest for computer architectures that exploit new opportunities and shed existing mechanisms that do not scale. Current architectures, such as hardware scheduled superscalars, are already hitting performance and complexity limits and cannot be scaled indefinitely. The Reconfigurable Architecture Workstation (Raw) is a simple, wire-efficient architecture that scales with increasing VLSI gate densities and attempts to provide performance that is at least comparable to that provided by scaling an existing architecture, but that can achieve orders of magnitude more performance for applications in which the compiler can discover and statically schedule fine-grain parallelism. The Raw microprocessor chip comprises a set of replicated tiles, each tile containing a simple RISC like processor, a small amount of configurable logic, and a portion of memory for instructions and data. Each tile has an associated programmable switch which connects the tiles in a wide-channel point-to-point interconnect. The compiler statically schedules multiple streams of computations, with one program counter per tile. The interconnect provides register-to-register communication with very low latency and can also be statically scheduled. The compiler is thus able to schedule instruction-level parallelism across the tiles and exploit the large number of registers and memory ports. Of course, Raw provides backup dynamic support in the form of flow control for situations in which the compiler cannot determine a precise static schedule. The Raw architecture can be viewed as replacing the bus architecture of superscalar processors with a switched interconnect and accomplishing at compile time operations such as register renaming and instruction scheduling. This paper makes a case for the Raw architecture and provides early results on the plausibility of static compilation for several small benchmarks. We have implemented a prototype Raw processor (called RawLogic) and an associated compilation system by leveraging commercial FPGA based logic emulation technology. RawLogic supports many of the features of the Raw architecture and demonstrates that we can write compilers to statically orchestrate all the communication and computation in multiple threads, and that performance levels 10x-100x over workstations is achievable for many applications. fl Also appears as MIT Laboratory For Computer Science TR-709, March 1997.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, Mahadevan Ganapathi, and Steven W. K. Tjiang. </author> <title> Code Generation Using Tree Matching and Dynamic Programming. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 11(5) </volume> <pages> 491-516, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Our current Raw prototype system described in Section 6 requires that these special operators be identified by the programmer and specified in behavioral Verilog. The compiler will extend the dynamic programming algorithms used in tree pattern-matching 12 systems <ref> [1] </ref> to identify commonly occurring subtree patterns that might be good candidates for implementation as custom instructions. To support multigranular operation the compiler will search for important special cases, such as bitwise and multiple shortword (e.g., 8-bit) arithmetic operations that can be packed together into a single custom instruction.
Reference: [2] <author> J. Babb, R. Tessier, M. Dahl, S. Hanono, D. Hoki, and A. Agarwal. </author> <title> Logic emulation with virtual wires. </title> <note> In To appear in IEEE Transactions on CAD, </note> <year> 1997. </year>
Reference-contexts: The placement algorithm attempts to minimize a latency and bandwidth cost measure and is a variant of a VLSI cell placement algorithm. This algorithm is used in our current multi-FPGA compiler system <ref> [2] </ref>. The placement algorithm attempts to lay close together threads that communicate intensely or threads that are in timing critical paths. As shown in Figure 10, the Placement algorithm uses a dependence graph for the program to determine critical paths and the volume of communication between threads. <p> In our system, the output behavioral Verilog netlist is then automatically synthesized to a gate-level verilog netlist. This gate level netlist is then processed by a VirtualWires compiler, and includes the steps of partitioning, placement and scheduling <ref> [11, 2] </ref>. These steps are akin to the corresponding phases in our compiler flow described in Section 4 process the resulting verilog netlist and produce a binary that maps into the multi-FPGA hardware substrate.
Reference: [3] <author> S. Borkar, R. Cohn, G. Cox, T. Gross, H. T. Kung, M. Lam, M. Levine, B. Moore, W. Moore, C. Peterson, J. Susman, J. Sutton, J. Urbanski, and J. Webb. </author> <title> Supporting systolic and memory communication in iWarp. </title> <booktitle> In Proc. 17th Annual Symposium on Computer Architecture, </booktitle> <address> Seattle, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: One way to view configurable logic is as a mechanism to provide the compiler a means to create customized single or multicycle instructions without resorting to longer software sequences. 1.1 Comparison with other Architectures Raw builds on several previous architectures. IWarp <ref> [3] </ref> and NuMesh [4] both share with Raw the philosophy of building point-to-point networks that support static scheduling.
Reference: [4] <author> David Shoemaker, Frank Honore, Chris Metcalf, and Steve Ward. NuMesh: </author> <title> An Architecture Optimized for Scheduled Communication. </title> <journal> The Journal of Supercomputing, </journal> <volume> 10 </volume> <pages> 285-302, </pages> <year> 1996. </year>
Reference-contexts: One way to view configurable logic is as a mechanism to provide the compiler a means to create customized single or multicycle instructions without resorting to longer software sequences. 1.1 Comparison with other Architectures Raw builds on several previous architectures. IWarp [3] and NuMesh <ref> [4] </ref> both share with Raw the philosophy of building point-to-point networks that support static scheduling.
Reference: [5] <author> Joseph A. Fisher. </author> <title> Very Long Instruction Word Architectures and the ELI-512. </title> <booktitle> In Proceedings of the 10th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 140-150, </pages> <address> Stockholm, Sweden, </address> <month> June </month> <year> 1983. </year>
Reference-contexts: Binding of common mechanisms into hardware also yields significantly better execution speed, lower area, and better power efficiency than FPGA systems. Many features in Raw are inspired by the VLIW work. Like the VLIW Multiflow machine <ref> [5] </ref>, Raw has a large register name space and a distributed register file. Like VLIWs, Raw also provides multiple memory ports, and relies heavily on compiler technology to discover parallelism and statically schedule computations. Unlike traditional VLIWs, however, Raw uses multiple instruction streams.
Reference: [6] <author> Susan Hinrichs, Corey Kosak, David R. O'Hallaron, Thomas M. Stricker, </author> <title> and Riichiro Take. An architecture for optimal all-to-all personalized communication. </title> <institution> Computing Science Tech Report CMU-CS-94-140, </institution> <address> CMU, Pittsburgh, PA, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: This approach statically reserves channel bandwidth between nodes that may potentially communicate. In the worst case, this conservative approach will involve an all-to-all personal communication schedule <ref> [6] </ref> between the processing elements. All-to-all schedules effectively simulate global communication architectures, such as a cross-bar or bus, on our static mesh. This approach is effective for a small number of tiles. It is also effective when messages are short, so that the amount of communication bandwidth wasted is small.
Reference: [7] <author> Kunle Olukotun, Basem A. Nayfeh, Lance Hammond, Ken Wilson, and Kunyung Chang. </author> <title> The case for a single-chip multiprocessor. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 2-11, </pages> <address> Cambridge, Massachusetts, </address> <month> October 1-5, </month> <year> 1996. </year>
Reference-contexts: It is natural to compare this work with the natural evolution of multiprocessors, viz, integrate on a chip a multiprocessor that uses a simple RISC chip <ref> [7] </ref>. Like Raw, such a multiprocessor would use a simple replicated tile and provide distributed memory. However, the cost of message startup and synchronization would hamper its ability to exploit fine-grain instruction-level parallelism.
Reference: [8] <author> D. A. Patterson and D. R. Ditzel. </author> <title> The Case for the Reduced Instruction Set Computer. </title> <journal> Computer Architecture News, </journal> <volume> 8(6) </volume> <pages> 25-33, </pages> <month> October </month> <year> 1980. </year>
Reference-contexts: Of course, a simple architecture also results in faster time to market, and eliminates the verification nightmare that threatens modern day superscalars. The passage of time has not changed the benefits of simple architectures superbly articulated in the paper by Patterson and Ditzel <ref> [8] </ref> making a case for RISCs. Programmable, integrated interconnect The switched interconnect between the tiles provides high bandwidth, low latency, communication between neighboring tiles. Integrated interconnect allows channels with hundreds of wires instead of the traditional tens.
Reference: [9] <author> Vivek Sarkar and John Hennessy. </author> <title> Compile-time Partitioning and Scheduling of Parallel Programs. </title> <booktitle> Proceedings of the ACM SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <volume> 21(7) </volume> <pages> 17-26, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: The structure of the code after partitioning is similar to that of an single-program-multiple-data (SPMD) program. A single program is assumed for all threads, along with a my id primitive that is used to guide control flow on different threads. Along with traditional multiprocessor and vector loop parallelization techniques <ref> [9] </ref> this phase identifies and partitions for fine grain instruction level parallelism. The partitioner must balance the benefits of parallelism versus the overheads of communication and synchronization.
Reference: [10] <author> Daniel J. Scales, Kourosh Gharachorloo, and Chandramohan A. Thekkath. </author> <title> Shasta: A low overhead, software-only approach for supporting fine-grain shared memory. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 174-185, </pages> <address> Cambridge, Massachusetts, </address> <month> October 1-5, </month> <year> 1996. </year>
Reference-contexts: Higher level abstractions like caching, global shared memory and memory protection are implemented by the compiler <ref> [10] </ref>, and dynamic events, like cache misses, are handled entirely in software. Discussion The Raw hardware architecture provides a number of benefits over conventional superscalar designs. Because of its tiled distributed structure Raw can achieve large I-fetch, data memory and register file bandwidth. <p> This phase assumes that each tile's address space is divided into a private section and a shared section. An access to the private section is always local. The compiler implements a shared memory abstraction on top of the distributed physical memory using techniques similar to those described in <ref> [10] </ref>.
Reference: [11] <author> Charley Selvidge, Anant Agarwal, Matt Dahl, and Jonathan Babb. TIERS: </author> <title> Topology independent pipelined routing and scheduling for VirtualWire compilation. </title> <booktitle> In 1995 ACM International Workshop on Field-Programmable Gate Arrays, </booktitle> <pages> pages 12-14, </pages> <address> Berkeley, CA, </address> <month> February </month> <year> 1995. </year> <journal> ACM. </journal> <volume> 22 </volume>
Reference-contexts: The following is a brief summary of the TIERS (Topology Independent Pipelined Routing and Scheduling) routing and scheduling algorithm that we will use <ref> [11] </ref>. This algorithm is implemented in our emulation system, and we propose to adapt this for use in Raw. <p> In our system, the output behavioral Verilog netlist is then automatically synthesized to a gate-level verilog netlist. This gate level netlist is then processed by a VirtualWires compiler, and includes the steps of partitioning, placement and scheduling <ref> [11, 2] </ref>. These steps are akin to the corresponding phases in our compiler flow described in Section 4 process the resulting verilog netlist and produce a binary that maps into the multi-FPGA hardware substrate.
Reference: [12] <author> Dick Sites. </author> <title> Architects look to processors of future, applications, instruction sets, memory bandwidth are key issues. </title> <type> Microprocessor Report, </type> <pages> pages 18-24, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: Raw architectures also promise better balance than the processor in memory or IRAM architectures. Current proposals for the latter architectures combine within a DRAM chip a single processor, and as such suffer from the same problems as extant superscalar processors, although to a lesser extent. As Dick Sites notes <ref> [12] </ref>, extant superscalars spend three out of four CPU cycles waiting for memory for many applications. Long memory latencies are one of the causes for this and are not significantly reduced in the IRAM approach.
Reference: [13] <author> Michael D. Smith, Mike Johnson, and Mark A. Horowitz. </author> <title> Limits on multiple instruction issue. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 290-302, </pages> <address> Boston, Massachusetts, </address> <month> April 3-6, </month> <year> 1989. </year>
Reference-contexts: This overcomes a fundamental limitation of superscalar processors, that the hardware must extract instructions for concurrent execution from a sequential instruction stream. The issue logic that dynamically resolves instruction dependencies in superscalars is a system bottleneck, limiting available concurrency <ref> [13] </ref>. The Raw machine avoids the bottleneck resulting from a single sequential instruction stream by supporting a multi-sequential architecture in which multiple sequential instruction streams execute on multiple tiles (one stream per tile).
Reference: [14] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A Loop Transformation Theory and an Algorithm to Maximize Parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 452-471, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: High-Level Transformations This phase performs high-level program transformations to expose greater levels of instruction-level parallelism and register locality. The transformations include loop unrolling, procedure inlining, software pipelining, code duplication, scalar replacement, speculative execution, and iteration-reordering loop transformations such as loop interchange, loop tiling, and unimodular loop transformations <ref> [14] </ref>. As in high-level optimizers in today's compilers for high-performance processors, this phase includes a deep intraprocedural and interprocedural analysis of the program's control flow and data flow. Alias analysis is especially important for reducing the number of unpredictable (dynamic) memory references on the Raw machine.
References-found: 14

