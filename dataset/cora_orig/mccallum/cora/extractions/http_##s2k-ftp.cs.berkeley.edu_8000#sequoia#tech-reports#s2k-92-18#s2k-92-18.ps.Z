URL: http://s2k-ftp.cs.berkeley.edu:8000/sequoia/tech-reports/s2k-92-18/s2k-92-18.ps.Z
Refering-URL: http://s2k-ftp.cs.berkeley.edu:8000/sequoia/tech-reports/s2k-92-18/
Root-URL: http://www.cs.berkeley.edu
Email: (jkay@cs.ucsd.edu)  (pasquale@cs.ucsd.edu)  
Title: A Performance Analysis of TCP/IP and UDP/IP Networking Software for the DECstation 5000  
Author: Jonathan Kay Joseph Pasquale 
Note: 1.0 Introduction  
Address: La Jolla, CA 92093-0114  
Affiliation: Computer Systems Laboratory Department of Computer Science and Engineering University of California, San Diego  
Abstract: Modern workstations increasingly rely on distributed software such as NFS and NIS, yet the speed of networking software is not improving as rapidly as the workstation and networking hardware, leading to a network software bottleneck. We present detailed measurements of various components of the TCP/IP and UDP/IP protocol stacks on a DECstation 5000/200 running Ultrix 4.2a. Measurements are by layer (i.e. socket, transport, IP, data-link) and by function (i.e. checksum computation, data copying, buffer management, protocol processing, and operating system interaction), with further breakdowns within each category. We show that checksum computation and data transfers dominate component times for a real LAN workload, that using large packet MTUs (maximum transmission units) is very important to achieving high throughput, and that given the distribution of component times for small sized messages, it will be difficult to improve latency. TCP and UDP time breakdowns are shown to be quite similar, suggesting that lightweight transport protocols are not likely to greatly decrease processing time. Finally, analytical models for network software processing times are presented. Network software speed is not increasing as rapidly as that of workstation CPUs. The goal of this study is to determine how various components of network software contribute to this bottleneck. We instrument the TCP/IP and UDP/IP protocol stacks since almost all of the traffic on the networks we measured is comprised of messages based on these protocols. We analyze network software component processing time by layer and by operation. By operation, we present detailed measurements for processing times for major categories of operations, including checksum computation, data movement, protocol processing, buffer management, operating system functions, data structure manipulations, and error checking. Other studies have 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. T. Braden, </author> <title> Requirements for Internet Hosts -- Communication Layers, Internet RFC 1122, </title> <month> October </month> <year> 1989. </year>
Reference-contexts: Address Resolution Protocol) and Ethernet encapsulation. We are aware that this is somewhat different from the spirit of the link layer as described by the OSI [4], yet it is not altogether unusual within the Internet community <ref> [1] </ref>. 3.1 Analysis by Operation We present the total message processing time for various components of network software when receiving and then sending the same-sized message by operation.
Reference: [2] <editor> R. T. Braden, D. A. Borman, and C. Partridge, </editor> <title> Computing the Internet Checksum, Internet RFC 1071, </title> <month> Sep. </month> <year> 1988. </year> <month> 20 </month>
Reference-contexts: An additional category, Other, is the amount of processing time we have not been able to account for. Checksum is a single operation, the Internet checksum routine <ref> [2] </ref>. Data movement includes all operations that cause data to be moved from one place to another. Buffer management includes allocation and freeing of mbufs, the major data structure used to store messages for Berkeley UNIX-based network software [13].
Reference: [3] <author> L.-F. Cabrera, E. Hunter, M. J. Karels, D. A. </author> <title> Mosher, User-Process Communication Performance in Networks of Com puters, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 14(1), </volume> <pages> 38-53, </pages> <month> Jan. </month> <year> 1988. </year>
Reference-contexts: Other studies have 1. This work was supported in part by grants from DEC, IBM, NCR, NSF, TRW, and UC MICRO. 2 shown some of these operations to be expensive <ref> [3, 6-8, 11, 22] </ref>. By layer, we measure the individual processing times of the socket, TCP and UDP, IP, link, and device driver software. We perform this study for the following reasons. <p> We perform this study for the following reasons. First, the performance characteristics of current RISC-based workstations (e.g. the DECstation 5000) and fast highly reliable optic-fiber LANs (e.g. FDDI) are different from those used in past studies <ref> [3, 8] </ref>. Second, we wanted to experiment with a variety of parameters, allowing us to understand both latency and throughput performance characteristics using a wide range of packet sizes.
Reference: [4] <author> CCITT, </author> <title> Data Communication Networks - Open System Interconnection (OSI) Model and Notation, Service Definition, Blue Book Recommendations X.200 - X.219, </title> <booktitle> International Telecommunications Union, </booktitle> <year> 1989. </year>
Reference-contexts: Address Resolution Protocol) and Ethernet encapsulation. We are aware that this is somewhat different from the spirit of the link layer as described by the OSI <ref> [4] </ref>, yet it is not altogether unusual within the Internet community [1]. 3.1 Analysis by Operation We present the total message processing time for various components of network software when receiving and then sending the same-sized message by operation.
Reference: [5] <author> K. C. Claffy, G. C. Polyzos, H. W. Braun, </author> <title> Traffic Characteristics of the T1 NSFNET, </title> <type> UCSD Technical Report CS92 252, </type> <month> July </month> <year> 1992. </year>
Reference-contexts: We include latency, as most a large portion of LAN traffic using TCP and UDP protocols is comprised of messages smaller than 200 bytes <ref> [5, 10, 16] </ref>, and increasing numbers of RPC-based applications [19-21] are likely to be more constrained by latency than by bandwidth. Many high throughput applications are increasingly constrained by high latencies and low packet sizes.
Reference: [6] <author> D. D. Clark, </author> <title> Modularity and Efficiency in Protocol Implementation, Internet RFC, </title> <type> 817, </type> <year> 1982 </year>
Reference: [7] <author> D. D. Clark, </author> <title> The Structuring of Systems Using Upcalls, </title> <booktitle> Proceedings of the Tenth ACM Symposium on Operating Sys tem Principles, </booktitle> <pages> 171-180, </pages> <month> Dec. </month> <year> 1985 </year>
Reference: [8] <author> D. D. Clark, V. Jacobson, J. Romkey, H. Salwen, </author> <title> An Analysis of TCP Processing Overhead, </title> <journal> IEEE Communications Magazine, </journal> <pages> 23-29, </pages> <month> June </month> <year> 1989 </year>
Reference-contexts: We perform this study for the following reasons. First, the performance characteristics of current RISC-based workstations (e.g. the DECstation 5000) and fast highly reliable optic-fiber LANs (e.g. FDDI) are different from those used in past studies <ref> [3, 8] </ref>. Second, we wanted to experiment with a variety of parameters, allowing us to understand both latency and throughput performance characteristics using a wide range of packet sizes. <p> The Mbuf category is considerably more lopsided than expected; Mbuf Alloc is very large, reecting a large number of small mbufs, yet Mbuf Misc is smaller than it is for any message length greater than or equal to 204 bytes. 3.5 Throughputs In <ref> [8] </ref>, Clark et al. make use of a particularly interesting means of analysis. They discuss the performance of various components of network software in terms of throughput. We present a similar analysis here, except that we include message size as a parameter.
Reference: [9] <institution> Digital Equipment Corporation, TURBOchannel Hardware Specification, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: On the other hand, reaching true gigabit throughputs on this architecture would require elimination of all categories save Data Structs and Error Check; even though the I/O bus is Figure15a Figure 15b 17 theoretically capable of 0.8 gigabits/sec <ref> [9] </ref>, use of that amount of throughput by TCP/IP running on a DECstation 5000/200 does not seem likely. The curves look rather different for small messages. The throughput for messages of size 614 is 0.773 MB/s, not quite 2/3 Ethernet throughput, about 1/6 the figure at the FDDI MTU.
Reference: [10] <author> R. Gusella, </author> <title> A Characterization of the Variability of Packet Arrival Processes in Workstation Networks, </title> <type> Technical Report UCB/CSD 90/612, </type> <institution> UC Berkeley Computer Science Division, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: We include latency, as most a large portion of LAN traffic using TCP and UDP protocols is comprised of messages smaller than 200 bytes <ref> [5, 10, 16] </ref>, and increasing numbers of RPC-based applications [19-21] are likely to be more constrained by latency than by bandwidth. Many high throughput applications are increasingly constrained by high latencies and low packet sizes.
Reference: [11] <author> V. Jacobson, </author> <title> BSD TCP Ethernet Throughput, comp.protocols.tcp-ip, </title> <type> Usenet, </type> <year> 1988 </year>
Reference-contexts: Other studies have 1. This work was supported in part by grants from DEC, IBM, NCR, NSF, TRW, and UC MICRO. 2 shown some of these operations to be expensive <ref> [3, 6-8, 11, 22] </ref>. By layer, we measure the individual processing times of the socket, TCP and UDP, IP, link, and device driver software. We perform this study for the following reasons.
Reference: [12] <author> V. Jacobson, R. Braden, D. </author> <title> Borman, TCP Extensions for High Performance, Internet RFC 1323, </title> <month> May </month> <year> 1992. </year>
Reference-contexts: Many high throughput applications are increasingly constrained by high latencies and low packet sizes. High latencies, which often derive a large contribution from router network software, require larger TCP windows <ref> [12] </ref>. Furthermore, in the wide area Internet arena, the current maximal packet size across wide area networks is largely frozen by [18] at 527 bytes, a relatively small value when one compares this value to maximum Ethernet and FDDI packet sizes.
Reference: [13] <author> S. J. Lefer, M. K. Mckusick, M. J. Karels, J. S. Quarterman, </author> <title> The Design and Implementation of the 4.3 BSD UNIX Operating System, </title> <publisher> Addison-Wesley, </publisher> <month> November </month> <year> 1989. </year>
Reference-contexts: Checksum is a single operation, the Internet checksum routine [2]. Data movement includes all operations that cause data to be moved from one place to another. Buffer management includes allocation and freeing of mbufs, the major data structure used to store messages for Berkeley UNIX-based network software <ref> [13] </ref>. Protocol processing is protocol-specific work such as setting header fields and maintaining state. Operating system over 4 head includes synchronization overhead and other general OS support functions. Data structure manipulation comprises manipulations of various data structures other than mbufs.
Reference: [14] <author> J. C. Mogul, </author> <title> Network Locality at the Scale of Processes, </title> <booktitle> Proceedings of the SIGCOMM 91 Symposium on Commu nications Architectures and Protocols, </booktitle> <month> August </month> <year> 1991. </year>
Reference: [15] <author> J. C. Mogul, S. Deering, </author> <title> Path MTU Discovery, Internet RFC 1191, </title> <month> November </month> <year> 1990. </year>
Reference-contexts: The curves look rather different for small messages. The throughput for messages of size 614 is 0.773 MB/s, not quite 2/3 Ethernet throughput, about 1/6 the figure at the FDDI MTU. Fortunately, the MTU Discovery effort <ref> [15] </ref> would allow transmission of maximal sized messages over wide area networks; in light of the factor of six performance hit, such work could have a large effect on throughput of connections over T3 links.
Reference: [16] <author> S. Owicki, </author> <title> Autonet Performance, </title> <institution> lecture at UC Berkeley, </institution> <month> September 17, </month> <year> 1992. </year>
Reference-contexts: We include latency, as most a large portion of LAN traffic using TCP and UDP protocols is comprised of messages smaller than 200 bytes <ref> [5, 10, 16] </ref>, and increasing numbers of RPC-based applications [19-21] are likely to be more constrained by latency than by bandwidth. Many high throughput applications are increasingly constrained by high latencies and low packet sizes.
Reference: [17] <author> D. C. Plummer, </author> <title> Ethernet Address Resolution Protocol: Or converting network protocol addresses to 48.bit Ethernet address for transmission on Ethernet hardware, Internet RFC 826, </title> <month> November </month> <year> 1982. </year>
Reference-contexts: Interface Protl is in the device driver layer, Link Protl is part of the IEEE 802 encapsulation layer, IP Protl is in the IP, and TP Protl is within TCP. Arp is the entire Address Resolution Protocol <ref> [17] </ref>, Demux is the operation of finding a protocol control block, given a TCP header (in_pcblookup), and 8 PCB (Dis)connect is the operations of checking that a route already exists for a connection, setting up the protocol control block to reect current connection state properly (in_pcbconnect and in_pcbdisconnect).
Reference: [18] <author> J. B. Postel, </author> <title> Transmission Control Protocol, Internet RFC 793, </title> <month> September </month> <year> 1981. </year>
Reference-contexts: High latencies, which often derive a large contribution from router network software, require larger TCP windows [12]. Furthermore, in the wide area Internet arena, the current maximal packet size across wide area networks is largely frozen by <ref> [18] </ref> at 527 bytes, a relatively small value when one compares this value to maximum Ethernet and FDDI packet sizes. However, processing time for large packets is certainly important; large packets consume more resources than small packets, and large packets are frequently produced under situations of greater processing load, e.g. <p> However, most packets are actually rather small. Almost all packets sent over wide area networks are less than 576 bytes, the limit imposed by <ref> [18] </ref>. Even in our LAN packet trace, where no such limitation applies, most messages are indeed quite small, less than 200 bytes. We devote a section here to small packet performance, since performance characteristics and bottlenecks turn out to be different for small packets than for large ones.
Reference: [19] <author> Sun Microsystems, Inc., NFS: </author> <title> Network File System Protocol Specification, Internet RFC 1094, </title> <month> March </month> <year> 1989. </year>
Reference: [20] <author> Sun Microsystems, Inc., </author> <title> RPC: Remote Procedure Call specification: </title> <type> version 2, Internet RFC 1057, </type> <month> June </month> <year> 1988. </year>
Reference: [21] <author> Sun Microsystems, Inc., XDR: </author> <title> External Data Representation Standard, </title> <note> Intern et RFC 1014, </note> <month> June </month> <year> 1987. </year> <month> 21 </month>
Reference: [22] <author> R. W. Watson, S. A. Mamrak, </author> <title> Gaining Efficiency in Transport Services by Appropriate Design and Implementation Choices, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(2), </volume> <pages> 97-120, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: Other studies have 1. This work was supported in part by grants from DEC, IBM, NCR, NSF, TRW, and UC MICRO. 2 shown some of these operations to be expensive <ref> [3, 6-8, 11, 22] </ref>. By layer, we measure the individual processing times of the socket, TCP and UDP, IP, link, and device driver software. We perform this study for the following reasons.
Reference: [23] <author> J. Kay, J. Pasquale, </author> <title> Measurement, Analysis, and Improvement of UDP/IP Throughput for the DECstation 5000, to appear, </title> <booktitle> Proceedings of the Winter 1993 Usenix, </booktitle> <month> January </month> <year> 1983. </year>
Reference-contexts: This resulted in a doubling of the speed of the Checksum routine, and consequently an overall performance improvement of 20% in large packet throughput <ref> [23] </ref>. Note that the software tested in the experiments described in this paper do not include these performance optimizations. 6 Data Mvmnt is the next largest category of operations for large packets; see Figures 3a and 3b for a breakdown of this category.
References-found: 23

