URL: http://www.cs.kun.nl/eita/publications/reports/Vuurpijl-rep95.ps.gz
Refering-URL: http://www.cs.kun.nl/eita/publications/
Root-URL: 
Email: Vytopil  
Title: Performance of the GCel-512 and PowerXPlorer for parallel neural network simulations  
Author: Louis Vuurpijl, Theo Schouten and Jan 
Date: April 21, 1995  
Web: http://www.cs.kun.nl/eita/projects/preens  
Abstract: This report presents new results from work performed in the framework of the IC 3 A pro-gramme. Using the GCel-512 and the PowerXPlorer made available by the UvA, a performance prediction model for several neural network simulations could be validated quantitatively both for a larger processor grid and for a different target parallel processor configuration. The performance prediction model and its application on a popular neural network model | backpropagation | decomposed via network decomposition will be discussed here. Using the model, the suitability of the GCel-512 and PowerXPlorer are discussed in terms of performance, speedup, efficiency and scalability.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Kohonen. </author> <title> Self-Organization and Associative Memory. </title> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <note> second edition, </note> <year> 1988. </year>
Reference-contexts: In particular, it discusses the typical patterns of communication required for the implementation, which boil down to distributed broadcast and gather operations. In [4, 5], also the parallel implementation of Kohonen <ref> [1] </ref> neural networks is discussed. It appeared that for these (and other) neural networks, similar patterns of communication are required. A general method for predicting the performance of MIMD parallel processor systems which communicate through message passing channels was introduced in [4, 5].
Reference: [2] <author> D.E. Rumelhart and J.L. McClelland. </author> <title> Parallel Distributed Processing: </title> <journal> Explorations in the Microstruc ture of Cognition, </journal> <volume> volume 1. </volume> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: 1 Introduction In [3], the parallel implementation of the backpropagation neural network <ref> [2] </ref> on a parallel trans-puter network is presented. In particular, it discusses the typical patterns of communication required for the implementation, which boil down to distributed broadcast and gather operations. In [4, 5], also the parallel implementation of Kohonen [1] neural networks is discussed.
Reference: [3] <author> L.G. </author> <title> Vuurpijl and Th.E. Schouten. Suitability of Transputers for Neural Network Simulations. </title> <editor> In W. Joosen and E. Milgrom, editors, </editor> <booktitle> Parallel Computing: From Theory to Sound Practice, </booktitle> <pages> pages 528-537. </pages> <publisher> IOS Press, </publisher> <year> 1992. </year>
Reference-contexts: 1 Introduction In <ref> [3] </ref>, the parallel implementation of the backpropagation neural network [2] on a parallel trans-puter network is presented. In particular, it discusses the typical patterns of communication required for the implementation, which boil down to distributed broadcast and gather operations.
Reference: [4] <author> L.G. </author> <title> Vuurpijl and Th.E. Schouten. A Scalable Performance Prediction Model for Parallel Neural Network Simulations. </title> <booktitle> In High Performance Computing and Networking '94, </booktitle> <address> Munchen, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: 1 Introduction In [3], the parallel implementation of the backpropagation neural network [2] on a parallel trans-puter network is presented. In particular, it discusses the typical patterns of communication required for the implementation, which boil down to distributed broadcast and gather operations. In <ref> [4, 5] </ref>, also the parallel implementation of Kohonen [1] neural networks is discussed. It appeared that for these (and other) neural networks, similar patterns of communication are required. A general method for predicting the performance of MIMD parallel processor systems which communicate through message passing channels was introduced in [4, 5]. <p> In <ref> [4, 5] </ref>, also the parallel implementation of Kohonen [1] neural networks is discussed. It appeared that for these (and other) neural networks, similar patterns of communication are required. A general method for predicting the performance of MIMD parallel processor systems which communicate through message passing channels was introduced in [4, 5]. The total execution time for a parallel program is modeled as the sum of the total calculation and communication times.
Reference: [5] <author> L.G. Vuurpijl, Th.E. Schouten, and J. </author> <title> Vytopil. Performance Prediction of Large MIMD Systems for Parallel Neural Network Simulations. </title> <booktitle> Future Generation Computing Systems, </booktitle> <month> January </month> <year> 1995. </year> <note> To Appear. 5 </note>
Reference-contexts: 1 Introduction In [3], the parallel implementation of the backpropagation neural network [2] on a parallel trans-puter network is presented. In particular, it discusses the typical patterns of communication required for the implementation, which boil down to distributed broadcast and gather operations. In <ref> [4, 5] </ref>, also the parallel implementation of Kohonen [1] neural networks is discussed. It appeared that for these (and other) neural networks, similar patterns of communication are required. A general method for predicting the performance of MIMD parallel processor systems which communicate through message passing channels was introduced in [4, 5]. <p> In <ref> [4, 5] </ref>, also the parallel implementation of Kohonen [1] neural networks is discussed. It appeared that for these (and other) neural networks, similar patterns of communication are required. A general method for predicting the performance of MIMD parallel processor systems which communicate through message passing channels was introduced in [4, 5]. The total execution time for a parallel program is modeled as the sum of the total calculation and communication times. <p> Gathering in a tree is not discussed here. Two decomposition techniques were used to distribute a neural network or the patterns over the available processors, dataset decomposition and network decomposition (see <ref> [5] </ref>). Considered here 1 For non-square grids like the 16x8 and 16x32 width and height are used in our models. 2 In [5] this was implemented differently and required more communications. 2 is the training phase of backpropagation decomposed via the second technique. <p> Two decomposition techniques were used to distribute a neural network or the patterns over the available processors, dataset decomposition and network decomposition (see <ref> [5] </ref>). Considered here 1 For non-square grids like the 16x8 and 16x32 width and height are used in our models. 2 In [5] this was implemented differently and required more communications. 2 is the training phase of backpropagation decomposed via the second technique. For network decomposition, during the forward pass n neuron values are broadcasted and arrays of size n=P are gathered.
References-found: 5

