URL: http://www.math.tau.ac.il/~mansour/papers/97-ml.ps.gz
Refering-URL: 
Root-URL: 
Email: mansour@math.tau.ac.il  
Title: Pessimistic decision tree pruning based on tree size  
Author: Yishay Mansour 
Address: Tel-Aviv, ISRAEL  
Affiliation: Computer Science Dept. Tel-Aviv University  
Abstract: In this work we develop a new criteria to perform pessimistic decision tree pruning. Our method is theoretically sound and is based on theoretical concepts such as uniform convergence and the Vapnik-Chervonenkis dimension. We show that our criteria is very well motivated, from the theory side, and performs very well in practice. The accuracy of the new criteria is comparable to that of the current method used in C4.5.
Abstract-found: 1
Intro-found: 1
Reference: [AHM95] <author> Peter Auer, Robert C. Holte, and Wolfgang Maass. </author> <title> Theory and applications of agnostic PAC-leaning with small decision trees. </title> <booktitle> In The 12th Internetional Conference on Machine Learning, </booktitle> <pages> pages 21-30. </pages> <publisher> Morgan Kauf-mann, </publisher> <year> 1995. </year>
Reference-contexts: In <ref> [AHM95] </ref> the accuracy of C4.5 was compared to an algorithm that generates decision trees of depth two. There it was observed that decision trees of depth two achieve remarkably good results compared to C4.5, although in some cases the limitation to depth two forced reduced accuracy.
Reference: [BB94] <author> Marco Bohanec and Ivan Bratko. </author> <title> Trading accuracy for simplicity in decision trees. </title> <journal> Machine Learning, </journal> <volume> 15 </volume> <pages> 223-250, </pages> <year> 1994. </year>
Reference-contexts: They proposed using dynamic programming to find a sequence of trees that minimizes a combination of the error on the training sample and the tree size. Later, an independent sample is used to chose the best tree out of the sequence. In <ref> [BB94] </ref> a simple dynamic programming algorithm is given to find the smallest pruned decision tree that has a given error rate. Here again, one can generate a sequence of trees and use an independent sample to chose the best tree in the sequence.
Reference: [BFOS84] <author> Leo Breiman, Jerome H. Friedman, Richard A. Olshen, and Charles J. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <year> 1984. </year>
Reference-contexts: There are two different methodologies for performing pruning. The first is of generating a sequence of pruned trees, and later to chose one of them. This idea is the core idea behind the Minimal Cost-Complexity Pruning proposed by Breiman et. al. <ref> [BFOS84] </ref>. They proposed using dynamic programming to find a sequence of trees that minimizes a combination of the error on the training sample and the tree size. Later, an independent sample is used to chose the best tree out of the sequence.
Reference: [Die96] <author> Thomas G. Dietterich. </author> <title> Statistical tests for comparing supervised classification learning algorithms. </title> <type> manuscript, </type> <year> 1996. </year>
Reference-contexts: 1:2y 23:0 5:8 14:5 0:2 ? y 14:8 7:9 c=0.1 18:3 6:0 26:4 5:4 28:7 2:8y 21:9 4:0 13:9 0:2y 12:5 7:2 the standard C4.5 (c4.5 -c 25) and by y significant diffrences from the unprunned tree. (The significance level is 0.9 and it uses a difference of proportions test <ref> [Die96] </ref>) the parameter -m 1 and -c 1. The unpruned tree had size 4567 (and zero error on the training set). We ran C4.5 with the default confidence parameter (-c 25) which generated a pruned tree of size 3685 (and only 244 mislabels on the training set). <p> For our criteria we know the exact error, forty five percent. We tested the hypotheses generated by C4.5, and they had error 49.5% (for -c 25) and 47.1% (for -c 1). As expected, the results show that the superfluous nodes only increase the generalization error. Using test of proportions <ref> [Die96] </ref>, a 95% confidence interval around the best error (45%) gives the interval [44:3; 45:7], which implies that the deviations, as expected, are very significant. We did not limit ourself to artificial data, but considered also a variety of real databases. <p> In Figure 3 we plot the average tree size as a function of c for the database adult. The error rate of the different criteria appears in Figure 2. In order to test the significance of the results we used the test of proportions <ref> [Die96] </ref> with confidence 90%. We compared the results to the unpruned tree and the C4.5 with the default setting (i.e., c4.5 -c 25. The results show that the setting of our parameter to c=0.5 is too pessimistic, and it over-prunes.
Reference: [Koh96] <author> Ron Kohavi. </author> <title> Scaling up the accuracy of naive-bayes classifiers: a decision-tree hybrid. In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, </title> <note> page to appear, </note> <year> 1996. </year>
Reference: [MA94] <author> P. M. Murphy and D. W. Aha. </author> <title> Uci repository of machine learning databases. </title> <address> http://www.ics.uci.edu/ mlearn /MLRepository.html, </address> <year> 1994. </year>
Reference-contexts: We did not limit ourself to artificial data, but considered also a variety of real databases. In order to perform the comparison we chose six databases, four taken from the project StatLog [Sta] and two databases from UCI Repository <ref> [MA94] </ref>. Since we are interested in functions with boolean output, the databases all have to predict a boolean output. The databases are: 1. australian - Australian Credit database. The aim is to decide whether to approve a credit card application. There are 690 examples.
Reference: [Min87] <author> John Mingers. </author> <title> Expert systems rule induction with statistical data. </title> <journal> Jounal of the Operational Research Society, </journal> <volume> 38:39 - 47, </volume> <year> 1987. </year>
Reference-contexts: The approach of Reduced Error Pruning, proposed by Quinlan [Qui87], uses an independent sample to test the accuracy of each sub-tree compared to the case when it is pruned. Mingers <ref> [Min87] </ref> suggested the Critical Value Pruning, which performs the decision about pruning using on the "information-gain" that was achieved in growing the tree. (For each node it computes the maximum "information-gain" in its subtree, and prunes a node if this value is less than a certain threshold.) Under the category of
Reference: [Min89] <author> John Mingers. </author> <title> An empirical comparison of pruning methods for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 4:227 - 243, </volume> <year> 1989. </year>
Reference-contexts: This method of pessimistic pruning is the method that is used currently in C4.5. An excellent survey and empirical comparison of the different pruning methods was done by Mingers <ref> [Min89] </ref>, which also includes a survey of all the different methods. Our starting point can be motivated as follows. Assume you have a sub-tree that classifies 200 training examples and makes no error.
Reference: [Nib87] <author> T. Niblett. </author> <title> Constructing decision trees in noisy domains. </title> <editor> In I. Bratko and N. Lavrac, editors, </editor> <booktitle> Progress in Machine Learning-Proceedings of EWSL 87: 2nd European Working Session on Learning, </booktitle> <pages> pages 67-78, </pages> <address> Bled, Yugoslavia, </address> <month> May </month> <year> 1987. </year>
Reference-contexts: One method that falls into our category of pessimistic pruning is Minimum error pruning <ref> [Nib87] </ref>. This method tries to estimate the true error using the following formula, e + (r 1) where r is the number of classes, n the number of examples, and e is the number of errors. (The theoretical justification for the formula is simple.
Reference: [Qui86] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Assuming a uniform prior over the classes, the above is the expected error over the posterior distribution, given that in n examples there where e errors.) Pessimistic pruning of Quinlan <ref> [Qui86, Qui93] </ref> uses a complex method based on the number of errors and the size of the training sample.
Reference: [Qui87] <author> J. R. Quinlan. </author> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies, </journal> <year> 1987. </year>
Reference-contexts: The second methodology for pruning tries to directly prune the original decision tree. The pruning is done by replacing the sub-tree rooted at a node by either a leaf or by a sub-tree rooted at one of its children. The approach of Reduced Error Pruning, proposed by Quinlan <ref> [Qui87] </ref>, uses an independent sample to test the accuracy of each sub-tree compared to the case when it is pruned.
Reference: [Qui93] <author> J. R. Quinlan. C4.5: </author> <title> Programs for machine learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Assuming a uniform prior over the classes, the above is the expected error over the posterior distribution, given that in n examples there where e errors.) Pessimistic pruning of Quinlan <ref> [Qui86, Qui93] </ref> uses a complex method based on the number of errors and the size of the training sample. <p> However, the theoretical justification of this method is rather weak, or as Quinlan says in his book ": : : the reasoning should be taken with a large grain of salt" <ref> [Qui93] </ref>. This method of pessimistic pruning is the method that is used currently in C4.5. An excellent survey and empirical comparison of the different pruning methods was done by Mingers [Min89], which also includes a survey of all the different methods. Our starting point can be motivated as follows.
Reference: [Sta] <author> StatLog. </author> <title> Comparative testing and evaluation of statistical and logical learning algorithms for large-scale applications in classification, prediction and control. ftp:ftp.ncc.up.pt/pub/statlog /datasets, (See also: Machine Learning, Neural and Statistical Classification, </title> <editor> ed. Michie, </editor> <publisher> Spiegelhalter and Taylor). </publisher>
Reference-contexts: We did not limit ourself to artificial data, but considered also a variety of real databases. In order to perform the comparison we chose six databases, four taken from the project StatLog <ref> [Sta] </ref> and two databases from UCI Repository [MA94]. Since we are interested in functions with boolean output, the databases all have to predict a boolean output. The databases are: 1. australian - Australian Credit database. The aim is to decide whether to approve a credit card application.
Reference: [VC71] <author> V. N. Vapnik and A. Ya. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its applications, </title> <address> XVI(2):264-280, </address> <year> 1971. </year>
Reference-contexts: In this section we used very basic techniques to bound the deviations. More involved techniques may be used, and they could be based on the Vapnik Chervonenkis (VC) dimension <ref> [VC71] </ref>. For decision trees we where not able to determine the exact VC dimension as a function of their size, but we can show that it is between (k) and O (k log d), for a binary decision tree over d inputs with at most k nodes (proof omitted).
References-found: 14

