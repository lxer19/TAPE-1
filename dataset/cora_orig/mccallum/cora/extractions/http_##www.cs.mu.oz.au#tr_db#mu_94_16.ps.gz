URL: http://www.cs.mu.oz.au/tr_db/mu_94_16.ps.gz
Refering-URL: http://www.cs.mu.oz.au/tr_db/TR.html
Root-URL: 
Title: Practical Length-Limited Coding for Large Alphabets  
Author: Andrew Turpin Alistair Moffat 
Keyword: Text compression, Huffman code, prefix code, length-limited code, package-merge algorithm, optimal binary search tree.  
Date: August 1994  
Address: Parkville 3052, Australia.  
Affiliation: Department of Computer Science, The University of Melbourne,  
Abstract: The use of Huffman coding for economical representation of a stream of symbols drawn from a defined source alphabet is widely known. In this paper we consider the problems encountered when Huffman coding is applied to an alphabet containing millions of symbols. Conventional tree-based methods for generating the set of codewords require large amounts of main memory; and worse, the codewords generated may be longer than 32 bits, which can severely limit the usefulness of both software and hardware implementations. The solution to the second problem is to generate "length-limited" codes, but previous algorithms for this restricted problem have required even more memory space than Huffman's unrestricted method. Here we re-examine the "package-merge" algorithm for generating optimal length-limited prefix-free codes and show that with a considered reorganisation of the key steps and careful attention to detail it is possible to implement it to run quickly in modest amounts of memory. As evidence of the practical usefulness of the improved method, we describe experiments on an alphabet of over one million symbols, for which an optimal 32-bit limited code was constructed in 11 megabytes of memory and about 60 seconds of CPU time. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T.C. Bell, J.G. Cleary, and I.H. Witten. </author> <title> Text Compression. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1990. </year>
Reference-contexts: It is generally accepted that a compression system consists of two activities: modelling and coding <ref> [1] </ref>. The model estimates, for each possible symbol that might appear next, a probability of occurrence. The coder is then responsible for representing the actual stream of symbols with respect to those estimated probabilities. <p> The coder is then responsible for representing the actual stream of symbols with respect to those estimated probabilities. There are many ways to model text, and for an overview the reader is referred to Bell, Cleary and Witten <ref> [1] </ref>. Here we will focus on the coding side of this team, and ask what happens when the model contains a very large number|perhaps millions|of symbols. <p> Suppose that p is a set of n frequencies, and that p 1 p 2 p n . Let L be the length limit. 2. If L &lt; dlog 2 ne return with failure, no length-limited code is possible. 3. Set q <ref> [1] </ref> p. Set q [i + 1] merge (p; package (q [i])). 5. <p> These three packages are drawn from the first six items in q [2]; they in turn cause one packet| two items|to be expanded out of q <ref> [1] </ref>. Accumulating all of the individual increments on symbols, the final code is l = f4; 4; 3; 3; 3; 2; 2g, which results in B = 98. <p> If we assume that q [i] contains 2n1 or fewer items then there will be at most n1 packages produced by package (q [i]), and so at most 2n 1 items in list q [i + 1]. Since q <ref> [1] </ref> = p contains n 2n 1 items, by induction each list q [i] contains at most 2n 1 items. There are thus fewer than 2nL items in the entire structure. <p> Similarly, the three packages within the first 10 items of q [3] must correspond to the first six items in list q [2]; and the single package expanded from q [2] must have as its source the first two items in q <ref> [1] </ref>. Suppose then that we record this information in a bitmap M , and set M [i; j] to 1 if the j'th item in list q [i] is a package, and to 0 if it is a symbol. <p> In fact, if we allocate each row to be exactly the required number of bits, it can be made even smaller. For example, none of the items stored in q <ref> [1] </ref> are packages, and so M [1] need not be stored at all. Similarly, q [2] contains at most n=2 packages and is at most 3n=2 items long, and q [3] contains at most 3n=4 packages at is at most 7n=4 items long in total. <p> In fact, if we allocate each row to be exactly the required number of bits, it can be made even smaller. For example, none of the items stored in q <ref> [1] </ref> are packages, and so M [1] need not be stored at all. Similarly, q [2] contains at most n=2 packages and is at most 3n=2 items long, and q [3] contains at most 3n=4 packages at is at most 7n=4 items long in total.
Reference: [2] <author> A.S. Fraenkel and S.T. Klein. </author> <title> Bounding the depth of search trees. </title> <journal> The Computer Journal, </journal> <volume> 36(7) </volume> <pages> 668-678, </pages> <year> 1993. </year>
Reference-contexts: generating length-limited codes, or, equivalently, optimal binary search trees [3, 5, 13], but all of the early algorithms were intractable in terms of either space or time or both, or described efficient solutions that seemed useful in practice but did not guarantee that the resulting set of codewords was optimal <ref> [2, 11] </ref>. It was not until 1990 that an efficient solution to the problem of finding optimal length-limited codes was articulated, by Larmore and Hirschberg [7]. They described two algorithms. <p> Inspection of the list q [3] shows that to obtain these ten items, seven leaves must be traversed, 7 1 2 3 4 5 6 7 8 9 10 11 12 13 q <ref> [2] </ref> 1 1 (1,1) 3 5 6 (3,5) 11 13 (6,11) q [4] 1 1 (1,1) 3 (2,3) 5 6 (5,5) 11 13 (6,11) (11,13) (19,30) causing increments so that l i = 2 for all i, and three packages must be expanded. <p> These three packages are drawn from the first six items in q <ref> [2] </ref>; they in turn cause one packet| two items|to be expanded out of q [1]. Accumulating all of the individual increments on symbols, the final code is l = f4; 4; 3; 3; 3; 2; 2g, which results in B = 98. <p> Similarly, the three packages within the first 10 items of q [3] must correspond to the first six items in list q <ref> [2] </ref>; and the single package expanded from q [2] must have as its source the first two items in q [1]. <p> Similarly, the three packages within the first 10 items of q [3] must correspond to the first six items in list q <ref> [2] </ref>; and the single package expanded from q [2] must have as its source the first two items in q [1]. <p> In fact, if we allocate each row to be exactly the required number of bits, it can be made even smaller. For example, none of the items stored in q [1] are packages, and so M [1] need not be stored at all. Similarly, q <ref> [2] </ref> contains at most n=2 packages and is at most 3n=2 items long, and q [3] contains at most 3n=4 packages at is at most 7n=4 items long in total. <p> All values of L in the range 21 L 32 were tested. The results of these experiments are shown in Figure 7. We compared the new implementation against the O (n)-space recursive package-merge of Larmore and Hirschberg, the O (n)-space approximate length-limited coding method of by Fraenkel and Klein <ref> [2] </ref> (including all of the improvements they describe), and against the Huffman code generation method described in [15]. Because of space limitations (730 Mb is a good reason) we were unable to run the non-recursive package-merge method. allocated arrays and bitvectors.
Reference: [3] <author> M.R. Garey. </author> <title> Optimal binary search trees with restricted maximal depth. </title> <journal> SIAM Journal on Computing, </journal> <volume> 3 </volume> <pages> 101-110, </pages> <year> 1974. </year>
Reference-contexts: In such a system it is necessary for some length-limit to be imposed, an upper bound on the permitted length of the generated codewords. Many authors have considered the problem of generating length-limited codes, or, equivalently, optimal binary search trees <ref> [3, 5, 13] </ref>, but all of the early algorithms were intractable in terms of either space or time or both, or described efficient solutions that seemed useful in practice but did not guarantee that the resulting set of codewords was optimal [2, 11]. <p> This involves, not surprisingly, increments to all of the seven l i values, plus the expansion of five packages, which correspond in turn (by tracing back the packages) to the first ten items of q <ref> [3] </ref>. Inspection of the list q [3] shows that to obtain these ten items, seven leaves must be traversed, 7 1 2 3 4 5 6 7 8 9 10 11 12 13 q [2] 1 1 (1,1) 3 5 6 (3,5) 11 13 (6,11) q [4] 1 1 (1,1) 3 <p> This involves, not surprisingly, increments to all of the seven l i values, plus the expansion of five packages, which correspond in turn (by tracing back the packages) to the first ten items of q <ref> [3] </ref>. Inspection of the list q [3] shows that to obtain these ten items, seven leaves must be traversed, 7 1 2 3 4 5 6 7 8 9 10 11 12 13 q [2] 1 1 (1,1) 3 5 6 (3,5) 11 13 (6,11) q [4] 1 1 (1,1) 3 (2,3) 5 6 (5,5) 11 13 <p> Accumulating all of the individual increments on symbols, the final code is l = f4; 4; 3; 3; 3; 2; 2g, which results in B = 98. By taking the first 12 items from q <ref> [3] </ref> the same table can also be used to generate a 3-limited code; in this case the answer is l = f3; 3; 3; 3; 3; 3; 2g and B = 107. There is no 2-limited code possible for n = 7 symbols. <p> Consider again the example shown in Figure 2. In the first twelve items of list q [4] there are seven symbols and five packages; those packages must, of necessity, correspond to the first ten items of list q <ref> [3] </ref>. Similarly, the three packages within the first 10 items of q [3] must correspond to the first six items in list q [2]; and the single package expanded from q [2] must have as its source the first two items in q [1]. <p> In the first twelve items of list q [4] there are seven symbols and five packages; those packages must, of necessity, correspond to the first ten items of list q <ref> [3] </ref>. Similarly, the three packages within the first 10 items of q [3] must correspond to the first six items in list q [2]; and the single package expanded from q [2] must have as its source the first two items in q [1]. <p> For example, none of the items stored in q [1] are packages, and so M [1] need not be stored at all. Similarly, q [2] contains at most n=2 packages and is at most 3n=2 items long, and q <ref> [3] </ref> contains at most 3n=4 packages at is at most 7n=4 items long in total. That is, 3n=2 bits and 7n=4 bits respectively suffice for the next two rows, savings of roughly n=2 and n=4 bits.
Reference: [4] <author> D.K. Harman. </author> <title> Overview of the first text retrieval conference. </title> <editor> In D.K. Harman, editor, </editor> <booktitle> Proc. TREC Text Retrieval Conference, </booktitle> <pages> pages 1-20, </pages> <address> Washington, </address> <month> November </month> <year> 1992. </year> <note> National Institute of Standards Special Publication 500-207. </note>
Reference-contexts: In Figure 2 packages are shown as pairs of values to show their origin; the weight of the package is the sum of the two individual weights. To calculate the code, the first 12 items from q <ref> [4] </ref> must be expanded, where 12 = 2n 2 is the number of 0.5 "increments" necessary to reduce K from 7 to 1. <p> Inspection of the list q [3] shows that to obtain these ten items, seven leaves must be traversed, 7 1 2 3 4 5 6 7 8 9 10 11 12 13 q [2] 1 1 (1,1) 3 5 6 (3,5) 11 13 (6,11) q <ref> [4] </ref> 1 1 (1,1) 3 (2,3) 5 6 (5,5) 11 13 (6,11) (11,13) (19,30) causing increments so that l i = 2 for all i, and three packages must be expanded. <p> Consider again the example shown in Figure 2. In the first twelve items of list q <ref> [4] </ref> there are seven symbols and five packages; those packages must, of necessity, correspond to the first ten items of list q [3]. <p> is the most "pressure" on the length limit|the total amount of space required to devise a length-limited code is close to 2n words, and the running time is almost linear. 12 6 Experimental Results This investigation has been motivated by the work we have been undertaking with the TREC collection <ref> [4] </ref>, a large corpus of text drawn from several sources such as newspapers, government publications, and the US Patent Office. As originally processed, it consisted of 2 Gb of text.
Reference: [5] <author> T.C. Hu and K.C. Tan. </author> <title> Path length of binary search trees. </title> <journal> SIAM Journal of Applied Mathematics, </journal> <volume> 22(2) </volume> <pages> 225-234, </pages> <month> March </month> <year> 1972. </year>
Reference-contexts: In such a system it is necessary for some length-limit to be imposed, an upper bound on the permitted length of the generated codewords. Many authors have considered the problem of generating length-limited codes, or, equivalently, optimal binary search trees <ref> [3, 5, 13] </ref>, but all of the early algorithms were intractable in terms of either space or time or both, or described efficient solutions that seemed useful in practice but did not guarantee that the resulting set of codewords was optimal [2, 11].
Reference: [6] <author> D.A. Huffman. </author> <title> A method for the construction of minimum redundancy codes. </title> <journal> Proc. IRE, </journal> <volume> 40(9) </volume> <pages> 1098-1101, </pages> <month> September </month> <year> 1952. </year>
Reference-contexts: Probably the most widely-known of all coding techniques is Huffman's famous method <ref> [6] </ref>. It is described in textbooks covering both text compression and the more general area of algorithms and data structures. <p> An optimal code is always complete. Huffman's algorithm <ref> [6] </ref> generates optimal prefix-free codes.
Reference: [7] <author> L.L. Larmore and D.S. Hirschberg. </author> <title> A fast algorithm for optimal length-limited Huffman codes. </title> <journal> Journal of the ACM, </journal> <volume> 37(3) </volume> <pages> 464-473, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: It was not until 1990 that an efficient solution to the problem of finding optimal length-limited codes was articulated, by Larmore and Hirschberg <ref> [7] </ref>. They described two algorithms. The first|known as the "package-merge" technique|takes O (nL) time and O (nL) space to generate an L-bit length-limited code for an alphabet of n symbols.
Reference: [8] <author> D. Manstetten. </author> <title> Tight upper bounds on the redundancy of Huffman codes. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38(1) </volume> <pages> 144-151, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: It is described in textbooks covering both text compression and the more general area of algorithms and data structures. Moreover, the behaviour of Huffman codes is well understood, and in most practical situations Huffman codes give compression very close to the underlying model entropy <ref> [8] </ref> despite the fact that all of the codewords assigned by the algorithm are integral length, which means in effect that all symbol probabilities have been approximated by negative powers of two. There are two reasons why large alphabets pose problems for the usual textbook implementations of Huffman coding.
Reference: [9] <author> A. Moffat, N.B. Sharman, I.H. Witten, </author> <title> and T.C. Bell. An empirical evaluation of coding methods for multi-symbol alphabets. Information Processing & Management. </title> <note> To appear. </note>
Reference: [10] <author> A. Moffat and J. Zobel. </author> <title> Compression and fast indexing for multi-gigabyte text databases. </title> <journal> Australian Computer Journal, </journal> <volume> 26(1) </volume> <pages> 1-9, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: As originally processed, it consisted of 2 Gb of text. We have been compressing this using a zero-order word-based model and Huffman coding; there are about 350 million symbols in total, and nearly one million distinct symbols <ref> [10] </ref>. By luck, the Huffman code on this distribution yielded codewords that peaked at 30 bits, and so did not present a problem. However the collection has recently grown by another gigabyte, and it has become quite apparent that the existing method of generating codewords cannot be continued with indefinitely.
Reference: [11] <author> H. Murakami, S. Matsumoto, and H. Yamamoto. </author> <title> Algorithm for construction of variable length code with maximum word length. </title> <journal> IEEE Transactions on Communications, </journal> <volume> COM-32(10):1157-1159, </volume> <month> October </month> <year> 1984. </year>
Reference-contexts: generating length-limited codes, or, equivalently, optimal binary search trees [3, 5, 13], but all of the early algorithms were intractable in terms of either space or time or both, or described efficient solutions that seemed useful in practice but did not guarantee that the resulting set of codewords was optimal <ref> [2, 11] </ref>. It was not until 1990 that an efficient solution to the problem of finding optimal length-limited codes was articulated, by Larmore and Hirschberg [7]. They described two algorithms.
Reference: [12] <author> R. Sedgewick. </author> <title> Algorithms in C. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <note> second edition, </note> <year> 1990. </year>
Reference-contexts: By using an implicit tree structure rather than an explicit structure the space can be reduced. For example Sedgewick <ref> [12, page 328] </ref> gives a construction employing arrays and an implicit heap structure that uses 5n words of memory. Further memory savings result if codeword lengths are calculated rather than actual codeword bit-descriptions. This allows the memory requirement to be reduced to as little as 2n words [15].
Reference: [13] <author> D.C. Van Voorhis. </author> <title> Constructing codes with bounded codeword lengths. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-20(2):288-290, </volume> <month> March </month> <year> 1974. </year>
Reference-contexts: In such a system it is necessary for some length-limit to be imposed, an upper bound on the permitted length of the generated codewords. Many authors have considered the problem of generating length-limited codes, or, equivalently, optimal binary search trees <ref> [3, 5, 13] </ref>, but all of the early algorithms were intractable in terms of either space or time or both, or described efficient solutions that seemed useful in practice but did not guarantee that the resulting set of codewords was optimal [2, 11].
Reference: [14] <author> C.J. Van Wyk. </author> <title> Data Structures and C Programs. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1988. </year>
Reference-contexts: There are two reasons why large alphabets pose problems for the usual textbook implementations of Huffman coding. The first is the memory space required by the algorithm. Typical descriptions (see, for example, Van Wyk <ref> [14, page 238] </ref>) make use of linked data structures with multiple fields per node.
Reference: [15] <author> I.H. Witten, A. Moffat, </author> <title> and T.C. Bell. Managing Gigabytes: Compressing and Indexing Documents and Images. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: Further memory savings result if codeword lengths are calculated rather than actual codeword bit-descriptions. This allows the memory requirement to be reduced to as little as 2n words <ref> [15] </ref>. Although these improvements are in terms of constant factors only, the impact they have is important, and they make large-scale coding significantly more "affordable". <p> In reality, an alphabet of as few as 34 symbols can force a 33-bit codeword. However for this to happen the least probable symbol must turn up fewer than one time in ten million <ref> [15] </ref>, and this seems a safe bet, since reasonably accurate statistics for a small alphabet can be achieved after just a few thousand or tens of thousands of symbols have been processed. <p> For example, 2n + 2nL=w is less than 16n whenever L=w 7, that is, on current computers whenever L 224. In turn, a codeword of 225 bits can only be generated on an input stream of at least 2 fi 10 47 symbols <ref> [15] </ref>, and this is most unlikely. (At a million symbols a second, it would take 6 fi 10 30 centuries to perform the compression, somewhat greater than the mean time between failure of any conceivable computing equipment.) Moreover, the new approach is significantly faster than the recursive Larmore and Hirschberg variant. <p> One method for assigning codewords that leads to fast decoding is described by Witten, Moffat and Bell <ref> [15] </ref>. A code is complete if P n i=1 2 l i = 1. If a code is complete then there are no "gaps" in the assignment of codewords and every infinite string of zero- and one- bits corresponds to a message in the source alphabet. <p> We compared the new implementation against the O (n)-space recursive package-merge of Larmore and Hirschberg, the O (n)-space approximate length-limited coding method of by Fraenkel and Klein [2] (including all of the improvements they describe), and against the Huffman code generation method described in <ref> [15] </ref>. Because of space limitations (730 Mb is a good reason) we were unable to run the non-recursive package-merge method. allocated arrays and bitvectors. Figure 7b shows the corresponding amount of CPU time required when these programs were executed on a Sun SPARC 10 Model 402. <p> The improved implementation|using a bitmap, an in-place merge, and "complementary" information|requires substantially less space than the O (n)-space method of Larmore and Hirschberg, and only a little more space than unrestricted Huffman coding using the 2n-word implementation described in <ref> [15] </ref>. Moreover, it is also fast compared to the Larmore 13 (a) and Hirschberg approach, which performs a controlled but non-trivial amount of repeated computation.
Reference: [16] <author> I.H. Witten, R. Neal, and J.G. Cleary. </author> <title> Arithmetic coding for data compression. </title> <journal> Communications of the ACM, </journal> <volume> 30(6) </volume> <pages> 520-541, </pages> <month> June </month> <year> 1987. </year> <month> 16 </month>
Reference-contexts: This allows input and output operations to be performed as a single "mask and shift" sequence, and gives rise to the high speed of Huffman coders [9]|their principal advantage over arithmetic coders of the form described by Witten, Neal and Cleary <ref> [16] </ref>. Although 64-bit machines are beginning to make their presence felt, for most users the "one word" requirement means that none of the generated codewords exceed 32-bits long. For small alphabets of just a few hundred symbols codeword overflow is usually assumed to be a remote possibility.
References-found: 16

