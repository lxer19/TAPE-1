URL: http://www.cs.rice.edu:80/~greiner/thesis.ps.Z
Refering-URL: http://www.cs.rice.edu:80/~greiner/my_research.html
Root-URL: 
Title: Semantics-based parallel cost models and their use in provably efficient implementations  
Author: John Greiner Guy Blelloch, Chair Robert Harper Gary Miller Guy Steele, Jr., Sun Microsystems 
Degree: Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy. Thesis committee:  
Note: Copyright 1997 c John Greiner  
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Date: April 26, 1997  
Pubnum: CMU-CS-97-113  
Abstract: This research was sponsored in part by the Wright Laboratory, Aeronautical Systmes Center, Air Force Materiel Command, USAF, and the Advanced Research Projects Agency (ARPA) under grand number F33615-93-1-1330 and contract number F19628-91-C-0168. It was also supported in part by an NSF Young Investigator Award and by Finmeccanica. The views and conclusions contained in this document are those of the author and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of Wright Laboratory or the U.S. Government. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Samson Abramsky and R. Sykes. Secd-m: </author> <title> A virtual machine for applicative programming. </title> <editor> In Jean-Pierre Jouannaud, editor, </editor> <booktitle> Proceedings 2nd International Conference on Functional Programming Languages and Computer Architecture, number 201 in Lecture Notes in Computer Science, </booktitle> <pages> pages 81-98, </pages> <year> 1985. </year>
Reference-contexts: Goodrich and Kosaraju [44] introduced a parallel pointer machine (PPM), but this is quite different from our models since it assumes a fixed number of processors and allows side effecting of pointers. Abramsky and Sykes <ref> [1] </ref> introduced the Secd-m machine, which shares a similar basis as our intermediate machines, but is non-deterministic and uses fair merge. 2.2 Relating cost models Previous work on formally relating language-based models (languages with cost-augmented semantics) to machine models is sparse.
Reference: [2] <author> Shail Aditya, Arvind, Jan-Willem Maessen, Lennart Augustsson, and Rishiyur S. Nikhil. </author> <title> Semantics of pH: A parallel dialect of Haskell. </title> <type> Technical Report Computation Structures Group Memo 377-1, </type> <institution> Laboratory for Computer Science, Massachusetts Institute of Technology, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: This style of parallelism is closely related to the futures of Multilisp [50] (also known as promises [40])|Multilisp applies an application's function to futures which represent the arguments and which eventually receive the arguments' final values. Speculative parallelism also forms the core of languages such as Id and pH <ref> [87, 2, 88] </ref>. The Parallel Speculative -calculus (PSL) allows two threads to be spawned by an application expression, as in the PAL. Synchronization occurs only when looking up a variable's value.
Reference: [3] <author> Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <year> 1974. </year>
Reference-contexts: Within the context of this model, the constant costs for constant application, or the uniform cost criterion, is a reasonable assumption for implementation of these numeric functions on most real machines, assuming we ignore arbitrary precision arithmetic. The semantics could also accommodate the more precise logarithmic cost criterion <ref> [3] </ref>. 60 CHAPTER 5.
Reference: [4] <author> Boon S. Ang, Alejandro Caro, Stephem Glim, and Andrew Shaw. </author> <title> An introduction to the Id compiler. </title> <type> Technical Report Computation Structures Group Memo 328, </type> <institution> Laboratory for Computer Science, Massachusetts Institute of Technology, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: This work provides a general framework for such comparisons, although mainly targeted towards the use of more abstract language models of computation. 2.3 Implementations of dynamically parallel languages This section briefly overviews some related work in implementing languages with dynamic parallelism. Parallel implementations of Id and pH, e.g. <ref> [4, 86, 94, 93] </ref>, are generally based on assigning tasks to processors and minimizing the movement of tasks between processors. Each processor 34 CHAPTER 2. RELATED WORK has a queue of tasks waiting for a processor.
Reference: [5] <author> Andrew W. Appel. </author> <title> Compiling with continuations. </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: For example, tail recursion asymptotically affects stack space; an optimization in some versions of Standard ML of New Jersey to share the space for function environments keeps data accessible for too long, increasing space usage asymptotically <ref> [5] </ref>; and the implementations of some parallel languages needlessly serialize the synchronization of threads, asymptotically reducing the parallelization of some programs (cf. Chapter 8). While we restrict ourselves to such languages in this dissertation, the framework of provably efficient implementations is applicable to any language and its implementation.
Reference: [6] <author> Henry G. Baker, Jr. and Carl Hewitt. </author> <title> The incremental garbage collection of processes. </title> <booktitle> In Proceedings of Symposium on AI and Programming Languages, volume 12 of SIG-PLAN Notices, </booktitle> <pages> pages 55-59, </pages> <month> August </month> <year> 1977. </year>
Reference-contexts: To implement this scheme, we can use two active state stacks, one for each priority, with only a constant factor of overhead (e.g., each newly created or reactivated state checks to which of the two stacks it should be added). This priority scheme was proposed by Baker and Hewitt <ref> [6] </ref>. More general priority schemes can be based on the distinguishing degrees of "speculative-ness": * Threads created by APPC have the same priority as the original parent thread, because they are relevant (or irrelevant) if and only if the original thread is. 8.5. <p> Only following all the relevant pointers can tell us which threads are no longer accessible, and thus unnecessary. Several methods of garbage collection of processes has been previously described. Baker and Hewitt <ref> [6] </ref> and Hudak and Keller [54] used a mark-and-sweep approach, which is not asymptotically efficient since it traverses pointers too many times.
Reference: [7] <author> Amir M. Ben-Amram and Zvi Galil. </author> <title> On pointers versus addresses. </title> <journal> Journal of the ACM, </journal> <volume> 39(3) </volume> <pages> 617-648, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Unlike all of this work, 2.3. IMPLEMENTATIONS OF DYNAMICALLY PARALLEL LANGUAGES 33 which is driven simply to compare a few models, we also provide a general framework for comparisons of language-based models. Ben-Amram and Galil <ref> [7] </ref> described a serial computation model based on pointer-based access to memory (indirect addressing) rather than the usual representation of memory as a giant array (direct addressing). <p> There exist encoding schemes to encode groups (e.g., arrays) of integers as a single integer and operations on such groups as functions on their encodings. However, if we include division along with a standard set of operations on arbitrary-precision integers, the integers are compressible <ref> [7] </ref>, which means that such encodings can be asymptotically faster than the operations on the raw groups of data. <p> The simulation is optimal in terms of work for all the PRAM variants. This is because it takes logarithmic work to simulate each random access into memory (this is the same as for pointer machines <ref> [7] </ref>). Since we don't know how to do better for the weaker models, we will base our results on the most powerful model, the CRCW PRAM with unit-time multiprefix sums (MP PRAM).
Reference: [8] <author> Guy Blelloch, Phil Gibbons, and Yossi Matias. </author> <title> Provably efficient scheduling for languages with fine-grained parallelism. </title> <booktitle> In ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 1-12, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: The central concept of these implementations is executing parallel traversals of the profiling semantics' computation graphs. Previous work showed how to schedule some parallel computation graphs efficiently with respect to time and space <ref> [18, 8] </ref>. However, this work did not show how these graphs were obtained from or related to more concrete representations of computation, such as a programming language. <p> Its implementation is based on compiling into a dataflow model and discovering parallelism|what is evaluated in data- or task-parallel depends on a data dependency analysis which may vary between compilers. Theoretical work by Blumofe and Leiserson [18] and Blelloch, Gibbons, and Matias <ref> [8] </ref> shows how to efficiently schedule some parallel computations as described by computation graphs. Our implementations and efficiency proofs are built directly on this work, but also provide the missing link of showing how our language models relate to their graphs. <p> The following definitions and theorems about traversals are either standard graph terminology or are from Blumofe and Leiserson [18] or Blelloch, Gibbons, and Matias <ref> [8] </ref>. Note that the definition of a graph traversal is somewhat different than is standard in that it requires all nodes to be visited. <p> In other words, computation is scheduled at run time, not compile time. Definition 6.4 (Level-order traversal) A level-order traversal of a graph g is an 1-DFT based on a 1-DFT of the graph. Costs of scheduling Theorem 6.1 (Premature nodes of greedy traversal <ref> [8] </ref>) For any graph g and any 1-traversal T of the graph, the maximum number of premature nodes in the greedy q-traversal based on T is at most (D (g) 1)(q 1). <p> Then any newly ready nodes are pushed on the stack. Definition 6.6 (Depth-first q-traversal) A depth-first q-traversal (q-DFT) is a greedy q-traversal based on a 1-DFT. Theorem 6.3 (q-DFT <ref> [8] </ref>) For any series-parallel graph g, the following algorithm makes the q-DFT of g: Let StA be an array initially containing the root node of g. Repeat the following two steps until all nodes in g have been scheduled: 1. Schedule the first min (q; jStAj) nodes from StA. 2. <p> So we assume that the amount to deallocate is dependent on the traversal, e.g., the memory for a value can be deallocated only after the last referencing thread finishes. Thus, the last node referencing the memory is credited for its deallocation. Definition 6.7 (Space of traversal prefix <ref> [8] </ref>) For any prefix P = V 0 ; : : : ; V i1 of a q-traversal, the space of the traversal, written S P (P ), is the size of the program input plus the space allocated by the nodes in the traversal, P i1 P Definition 6.8 (Space <p> P = V 0 ; : : : ; V i1 of a q-traversal, the space of the traversal, written S P (P ), is the size of the program input plus the space allocated by the nodes in the traversal, P i1 P Definition 6.8 (Space complexity of traversal <ref> [8] </ref>) For a q-traversal T = V 0 ; : : : ; V k1 , the space complexity of the traversal, written S T (T ), is the maximum reachable space in use after any step of the traversal, max k1 j=0 S P (V 0 ; : : : <p> Theorem 6.4 (Space of q-traversal <ref> [8] </ref>) If s is the space complexity of the 1-traversal of graph g, then the space complexity of any q-DFT of the graph is bounded above by s + O (D (g)q), including all bookkeeping space. 6.2. <p> As mentioned, the idea behind the proof is to show that the P-CEK q PAL executes a q-DFT traversal of the computation graph returned by the semantics, then use the previous results on the number of nodes scheduled prematurely in a q-DFT <ref> [8] </ref> (cf. Section 6.1), and finally use these results to bound the space. By the machine traversing the graph we mean that there is a one-to-one correspondence between nodes in the graph and sets of a single step's computation, communication, and synchronization transitions for a given state.
Reference: [9] <author> Guy Blelloch and John Greiner. </author> <title> Parallelism in sequential functional languages. </title> <booktitle> In Proceedings 7th International Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 226-237, </pages> <month> June </month> <year> 1995. </year> <note> 217 218 BIBLIOGRAPHY </note>
Reference-contexts: Transitions on each of these states are independent and are to occur in parallel. 2 An earlier presentation of this implementation was not concerned with space-efficiency, and so selected all active states on each step, producing a level-order traversal <ref> [9] </ref>. 86 CHAPTER 6. INTERMEDIATE MODEL q PAL active states during an evaluation. It starts with one active state representing the entire program and ends with one active state representing the result value. The states are kept in a stack. At most q states are selected each step. <p> Without the assumption that environments are of constant size, our time bounds need to be generalized to account for the time for environment accesses and updates. E.g., representing environments as balanced binary trees, this adds a factor logarithmic in the number of distinct variables in the program <ref> [9] </ref>. We can then assume that the variables are renamed (e.g., via deBruijn indices) so as to minimize the number of variables. Moreover, translating other language features into the basic -calculus syntax adds at most a constant number of variables (e.g., Figure 5.20).
Reference: [10] <author> Guy Blelloch, Gary L. Miller, and Dafta Talmor. </author> <title> Developing a practical projection-based parallel Delaunay algorithm. </title> <booktitle> In Proceedings ACM Symposium on Computational Geometry, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: In parallel models, a program's execution time is dependent on (at least) its work and depth and the number of processors available. Work and depth are frequently used to describe parallelism, especially in teaching parallel algorithms [63, 59, 17, 15] and implementing various applications <ref> [46, 11, 10] </ref>. Let's examine the work and depth of quicksort on m data elements. Figure 1.4 gives pseudo-code for a parallel quicksort. First, recall that a serial quicksort algorithm requires O (m log m) time in the expected case. <p> Also, the work of Skillicorn, et al. overlaps with that of automatic complexity analysis [25, 116, 117]. In addition, their work also overlaps greatly with that in algorithm analysis using high-level functional data parallel languages, which also includes work using Nesl, e.g., <ref> [11, 10, 46] </ref>, and other work in the Bird-Meertens formalism. There has been some work on obtaining asymptotically efficient data structures using functional languages, e.g. [52, 30, 89]. These each approached traditional algorithmic analysis problems, but from the perspective of modern programming languages, using language 2.7.
Reference: [11] <author> Guy Blelloch and Girija Narlikar. </author> <title> A comparison of two n-body algorithms. </title> <booktitle> In DIMACS Implementation Challenge Workshop, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: In parallel models, a program's execution time is dependent on (at least) its work and depth and the number of processors available. Work and depth are frequently used to describe parallelism, especially in teaching parallel algorithms [63, 59, 17, 15] and implementing various applications <ref> [46, 11, 10] </ref>. Let's examine the work and depth of quicksort on m data elements. Figure 1.4 gives pseudo-code for a parallel quicksort. First, recall that a serial quicksort algorithm requires O (m log m) time in the expected case. <p> Also, the work of Skillicorn, et al. overlaps with that of automatic complexity analysis [25, 116, 117]. In addition, their work also overlaps greatly with that in algorithm analysis using high-level functional data parallel languages, which also includes work using Nesl, e.g., <ref> [11, 10, 46] </ref>, and other work in the Bird-Meertens formalism. There has been some work on obtaining asymptotically efficient data structures using functional languages, e.g. [52, 30, 89]. These each approached traditional algorithmic analysis problems, but from the perspective of modern programming languages, using language 2.7.
Reference: [12] <author> Guy E. Blelloch. </author> <title> Scans as primitive parallel operations. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-38(11):1526-1538, </volume> <month> November </month> <year> 1989. </year>
Reference-contexts: The simulation we use gives the same results for the EREW, CREW, and CRCW PRAM as well as for the multiprefix [100] and scan models <ref> [12] </ref>. The simulation is optimal in terms of work for all the PRAM variants. This is because it takes logarithmic work to simulate each random access into memory (this is the same as for pointer machines [7]).
Reference: [13] <author> Guy E. Blelloch. </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Roe used his model to analyze algorithms, but did not relate his model to more concrete models. On the other hand, Flanagan and Felleisen and Moreau related their semantics to very abstract machines, but provided no algorithmic analysis. Blelloch <ref> [13, 14] </ref> presented Nesl with an informal cost model of work and depth, but not space, that is used for algorithmic analysis. Also he did not give a formal cost mapping 31 32 CHAPTER 2.
Reference: [14] <author> Guy E. Blelloch. NESL: </author> <title> A nested data-parallel language (version 3.1). </title> <type> Technical Report CMU-CS-95-170, </type> <institution> Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: Instead, we introduce sequences as a datatype and an expression fe 00 : x in e 0 g which evaluates e 00 in parallel for each binding of x to a value in the sequence resulting from e 0 . This forms the core of languages such as Nesl <ref> [14] </ref>, HPF, and C* [107]. NESL provides a very flexible model of data-parallelism, where e 00 may be any general expression. In particular we allow nested data-parallelism, i.e., forked threads can fork additional threads (as in Nesl, but not HPF and C*). <p> Roe used his model to analyze algorithms, but did not relate his model to more concrete models. On the other hand, Flanagan and Felleisen and Moreau related their semantics to very abstract machines, but provided no algorithmic analysis. Blelloch <ref> [13, 14] </ref> presented Nesl with an informal cost model of work and depth, but not space, that is used for algorithmic analysis. Also he did not give a formal cost mapping 31 32 CHAPTER 2. <p> Additional continuation forms are needed to serialize the omitted pairing, conditional, and recursive binding expressions. We implement the semantics' nested sequences as nested sequences in the abstract machine, unlike in the current implementation of Nesl <ref> [14] </ref>. During compilation, Nesl flattens nested sequences and any code building or using nested sequences. As a result, Nesl's implementation (i.e., VCODE) uses only larger, one-dimensional sequences. This has the advantage of increasing the granularity for parallelism, which is very important in practice. <p> Similarly, the computation graphs in the NESL model are a more appropriate measure of parallelism than the maximum number of useful processors as defined by Zimmermann [128]. And this definition is formal, unlike that specified for NESL <ref> [14] </ref>. For each of these language models, we give a formal implementation onto the hypercube, butterfly, and PRAM machine models, and obtained the induced cost mapping. The speculative implementation is asymptotically faster than those used in practice, as it correctly parallelizes the suspension and reawakening of blocked threads.
Reference: [15] <author> Guy E. Blelloch. </author> <title> Programming parallel algorithms. </title> <journal> Communications of the ACM, </journal> <pages> pages 85-97, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: In parallel models, a program's execution time is dependent on (at least) its work and depth and the number of processors available. Work and depth are frequently used to describe parallelism, especially in teaching parallel algorithms <ref> [63, 59, 17, 15] </ref> and implementing various applications [46, 11, 10]. Let's examine the work and depth of quicksort on m data elements. Figure 1.4 gives pseudo-code for a parallel quicksort. First, recall that a serial quicksort algorithm requires O (m log m) time in the expected case. <p> Proof: The depth of a balanced tree is O (log m). Thus, this gives an expected depth of O (log 2 m). 2 Theorem 10.2 (Quicksort in NESL <ref> [15] </ref>) The quicksort algorithm quicksort shown in O (log m) depth on the NESL model. 10.2.2 Parallel Mergesort We first consider the problem of merging two sorted trees. We use m to refer to sum of the sizes of the two trees. <p> Thus, the entire computation is of O (log m) depth. 2 Theorem 10.8 (FFT in NESL) The FFT algorithm of Figure 10.7 when applied to balanced trees each with m leaves will execute in O (m log m) work and O (log m) depth on the NESL model <ref> [15] </ref>. 10.3 Comparing models Now that we have compared these three models on three specific algorithms, we make some more general statements about how the costs of these models relate.
Reference: [16] <author> Guy E. Blelloch and John Greiner. </author> <title> A parallel complexity model for functional languages. </title> <type> Technical Report CMU-CS-94-196, </type> <institution> Carnegie Mellon University, </institution> <month> October </month> <year> 1994. </year>
Reference-contexts: Theorem 10.4 (More efficient mergesort in PAL and PSL) A mergesort algorithm on m elements can execute in O (m log m) work and O (log 2 m) depth on the PAL and PSL models. Proof: The PAL case is shown by Blelloch and the author <ref> [16] </ref>.
Reference: [17] <author> Guy E. Blelloch and Jonathan C. Hardwick. </author> <title> Class notes: Programming parallel algorithms. </title> <type> Technical Report CMU-CS-93-115, </type> <institution> Carnegie Mellon University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: In parallel models, a program's execution time is dependent on (at least) its work and depth and the number of processors available. Work and depth are frequently used to describe parallelism, especially in teaching parallel algorithms <ref> [63, 59, 17, 15] </ref> and implementing various applications [46, 11, 10]. Let's examine the work and depth of quicksort on m data elements. Figure 1.4 gives pseudo-code for a parallel quicksort. First, recall that a serial quicksort algorithm requires O (m log m) time in the expected case. <p> Proof: The PAL case is shown by Blelloch and the author [16]. The PSL case then holds by Theorem 10.9. 2 Theorem 10.5 (Mergesort in NESL <ref> [17] </ref>) A mergesort algorithm on m elements can ex ecute in O (m log m) work and O (log 2 m) depth on the NESL model. 10.2.3 Fast Fourier Transform This section presents the standard FFT algorithm, adapted to each of the models in the most straightforward manner, as shown in
Reference: [18] <author> R. D. Blumofe and C. E. Leiserson. </author> <title> Space-efficient scheduling of multithreaded computations. </title> <booktitle> In Proceedings 25th ACM Symposium on Theory of Computing, </booktitle> <pages> pages 362-371, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The central concept of these implementations is executing parallel traversals of the profiling semantics' computation graphs. Previous work showed how to schedule some parallel computation graphs efficiently with respect to time and space <ref> [18, 8] </ref>. However, this work did not show how these graphs were obtained from or related to more concrete representations of computation, such as a programming language. <p> Its implementation is based on compiling into a dataflow model and discovering parallelism|what is evaluated in data- or task-parallel depends on a data dependency analysis which may vary between compilers. Theoretical work by Blumofe and Leiserson <ref> [18] </ref> and Blelloch, Gibbons, and Matias [8] shows how to efficiently schedule some parallel computations as described by computation graphs. Our implementations and efficiency proofs are built directly on this work, but also provide the missing link of showing how our language models relate to their graphs. <p> INTERMEDIATE MODEL 6.1 Parallel Graph Traversals Since we represent the computation as a graph of computation units, a specific traversal of the graph represents a scheduling of these computation units. The following definitions and theorems about traversals are either standard graph terminology or are from Blumofe and Leiserson <ref> [18] </ref> or Blelloch, Gibbons, and Matias [8]. Note that the definition of a graph traversal is somewhat different than is standard in that it requires all nodes to be visited. <p> Costs of scheduling Theorem 6.1 (Premature nodes of greedy traversal [8]) For any graph g and any 1-traversal T of the graph, the maximum number of premature nodes in the greedy q-traversal based on T is at most (D (g) 1)(q 1). Theorem 6.2 (Steps of greedy traversal <ref> [18] </ref>) For any graph g, a greedy q-traversal of g takes at most W (g)=q + D (g) steps. Note that Brent's Theorem [20] is a special case of Theorem 6.2 for level-order traversals.
Reference: [19] <author> E. Borger and I. Durdanovic. </author> <title> Correctness of compiling Occam to Transputer code. </title> <journal> The Computer Journal, </journal> <volume> 39(1) </volume> <pages> 52-92, </pages> <year> 1996. </year>
Reference-contexts: But our framework allows detailed semantics and provides a formalism which could be used as the basis for these semantics-based profilers. 36 CHAPTER 2. RELATED WORK 2.5 Provably correct implementations The general goal in provably correct implementation is obtaining compilers which produce efficient and provably correct code, e.g. <ref> [95, 71, 98, 24, 19] </ref>. At the core, this work provides a relatively abstract source code semantics, a very detailed object code semantics, and a provably correct compiler mapping between the two. These correspond to our language model, machine model, and mapping.
Reference: [20] <author> Richard P. Brent. </author> <title> The parallel evaluation of general arithmetic expressions. </title> <journal> Journal of the ACM, </journal> <volume> 21(2) </volume> <pages> 201-206, </pages> <year> 1974. </year>
Reference-contexts: Theorem 6.2 (Steps of greedy traversal [18]) For any graph g, a greedy q-traversal of g takes at most W (g)=q + D (g) steps. Note that Brent's Theorem <ref> [20] </ref> is a special case of Theorem 6.2 for level-order traversals.
Reference: [21] <author> Stephen Brookes and Shai Geva. </author> <title> Computational comonads and intensional semantics. </title> <type> Technical report, </type> <institution> Carnegie Mellon University, </institution> <year> 1991. </year>
Reference-contexts: But most of the work in intensional semantics concentrated on areas such as full abstraction (proving denotational and operational semantics equivalent) or traces of concurrent processes <ref> [21] </ref>. However, Gurr gave a categorical framework for defining language cost models and generalizing these to asymptotic complexity models [49]. While powerful, his framework does not correspond to typical programmers' intuition because of its use of category theory. And while very intriguing, the generalization to complexities is only partially successful.
Reference: [22] <author> F. W. Burton and D. J. Simpson. </author> <title> Space efficient execution of deterministic parallel programs. </title> <month> August </month> <year> 1996. </year>
Reference-contexts: Our implementations and efficiency proofs are built directly on this work, but also provide the missing link of showing how our language models relate to their graphs. Work by Burton [23] and Burton and Simpson <ref> [22] </ref> also described the space of deterministic parallel models. In particular, for series-parallel computation graphs with constant fan-in and fanout, they presented a scheduling algorithm using O (s p) space and within a constant factor of optimal in time for programs with sufficient parallelism.
Reference: [23] <author> F. Warren Burton. </author> <title> Guaranteeing good memory bounds for parallel programs. </title> <month> January </month> <year> 1996. </year> <note> BIBLIOGRAPHY 219 </note>
Reference-contexts: Our implementations and efficiency proofs are built directly on this work, but also provide the missing link of showing how our language models relate to their graphs. Work by Burton <ref> [23] </ref> and Burton and Simpson [22] also described the space of deterministic parallel models. In particular, for series-parallel computation graphs with constant fan-in and fanout, they presented a scheduling algorithm using O (s p) space and within a constant factor of optimal in time for programs with sufficient parallelism.
Reference: [24] <author> Bettina Buth, Karl-Heinz Buth, Martin Franzle, Burghard von Karger, Yassine Lakhneche, Hans Langmaack, and Markus Muller-Olm. </author> <title> Provably correct compiler development and implementation. </title> <editor> In U. Kastens and P. Pfahler, editors, </editor> <booktitle> Compiler Construction, number 641 in Lecture Notes in Computer Science, </booktitle> <pages> pages 141-155. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: But our framework allows detailed semantics and provides a formalism which could be used as the basis for these semantics-based profilers. 36 CHAPTER 2. RELATED WORK 2.5 Provably correct implementations The general goal in provably correct implementation is obtaining compilers which produce efficient and provably correct code, e.g. <ref> [95, 71, 98, 24, 19] </ref>. At the core, this work provides a relatively abstract source code semantics, a very detailed object code semantics, and a provably correct compiler mapping between the two. These correspond to our language model, machine model, and mapping.
Reference: [25] <author> Wentong Cai and David B. Skillicorn. </author> <title> Calculating recurrences using the Bird-Meertens formalism. </title> <journal> Parallel Processing Letters, </journal> <volume> 5(2) </volume> <pages> 179-190, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Note that one general technique used with skeletons is shape analysis which is a generalization of the size abstraction in automatic complexity analysis. Also, the work of Skillicorn, et al. overlaps with that of automatic complexity analysis <ref> [25, 116, 117] </ref>. In addition, their work also overlaps greatly with that in algorithm analysis using high-level functional data parallel languages, which also includes work using Nesl, e.g., [11, 10, 46], and other work in the Bird-Meertens formalism.
Reference: [26] <author> David Callahan and Burton Smith. </author> <title> A future-based parallel language for a general-purpose highly-parallel computer. </title> <editor> In David Galernter, Alexander Nicolau, and David Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Research Monographs in Parallel and Distributed Computing, chapter 6, </booktitle> <pages> pages 95-113. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: The latter offers a wide spectrum of implementations, depending on which computations it aborts and when. Speculative evaluation is most closely related to the futures of Multilisp and its many descendant languages <ref> [50, 51, 26, 31, 79, 69, 58, 125] </ref>, the lenient evaluation of Id and pH [120, 87, 88], and parallel graph reduction, e.g., [55, 96, 61]. * In Multilisp, any expression can be designated as a future that spawns a thread which may be executed in parallel.
Reference: [27] <author> William Clinger and Jonathan Rees. </author> <title> Revised 4 report on the algorithmic language Scheme. LISP Pointers, </title> <address> IV(3):1-55, </address> <month> July-September </month> <year> 1991. </year>
Reference-contexts: Many implicit implementation requirements are considered common sense and obvious, e.g., that adding two integers should take constant time. This example assumption is reasonable for fixed-precision arithmetic, but not for the arbitrary-precision arithmetic available in some languages, such as Scheme <ref> [27] </ref> and Mathematica. Since many intensional properties really are "obvious" in most commonly used languages, such as C or Fortran, explicitly defining these properties is not considered a priority. But not all intensional properties are "obvious", especially in modern programming languages that are more abstract than C, Fortran, etc. <p> But not all intensional properties are "obvious", especially in modern programming languages that are more abstract than C, Fortran, etc. Many constraints on languages can be considered required "optimizations". The best example of this is Scheme's explicit requirement for tail recursion <ref> [27] </ref>, i.e., tail calls in a function are implemented with a jump rather than a function call. The execution of a tail-recursive function reuses the current stack frame so that a sequence of tail calls requires only one stack frame. <p> These are the simplest of the models we consider and are appropriate for introducing the framework. Chapters 4 and 5 define the language and its profiling semantics, respectively. Syntactically, the language is based on the -calculus and thus most resembles languages such as Scheme, ML, Haskell, and Id <ref> [27, 81, 56, 87] </ref>. Chapters 6 and 7 relate the language model to traditional machine models of computation (hypercube, butterfly, and PRAM), staging this via an intermediate model for convenience (P-CEK). Part III uses this methodology for some other parallel language models.
Reference: [28] <author> Jacques Cohen. </author> <title> Computer-assisted microanalysis of programs. </title> <journal> Communications of the ACM, </journal> <volume> 25(10) </volume> <pages> 724-733, </pages> <month> October </month> <year> 1982. </year>
Reference-contexts: However, there has also been some work on worst-case analysis of for a PRAM-like parallel language [130], defining the depth of the computation and the maximum number of processes that the computation can employ. Also there has been some work on worst-case analysis of imperative languages <ref> [28] </ref> and average-case analysis of serial functional languages [36]. Converting the program into cost recurrence equations requires at least an informal definition of a language's costs, although some used formal definitions. <p> FUTURE WORK 215 11.2.3 More detailed models The models could also incorporate more detailed costs to accurately measure run-time costs, i.e., worrying about constant factors. I.e., provide a microanalysis, as opposed to our current macroanalysis <ref> [28] </ref>. At the simplest level, this could be distinguishing between the various costs here described as unit cost, e.g., assigning some constant applications to be twice as expensive as others. More generally, this requires incorporating into the semantics anything considered relevant that affects the costs.
Reference: [29] <author> Richard Cole and Uzi Vishkin. </author> <title> Deterministic coin tossing with applications to optimal parallel list ranking. </title> <journal> Information and Control, </journal> <volume> 70(1) </volume> <pages> 31-53, </pages> <year> 1986. </year>
Reference: [30] <author> James R. Driscoll, Neil Sarnak, Daniel D. K. Sleator, and Robert E. Tarjan. </author> <title> Making data structures persistent. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 38(1) </volume> <pages> 86-124, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: There has been some work on obtaining asymptotically efficient data structures using functional languages, e.g. <ref> [52, 30, 89] </ref>. These each approached traditional algorithmic analysis problems, but from the perspective of modern programming languages, using language 2.7. EXPRESSIVENESS 37 features such as higher-order functions and laziness. Many problems of interest here involve persistent data structures, where updates do not destroy the original data structure.
Reference: [31] <author> Marc Feeley. </author> <title> An Efficient and General Implementation of Futures on Large Scale Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Brandeis University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: The latter offers a wide spectrum of implementations, depending on which computations it aborts and when. Speculative evaluation is most closely related to the futures of Multilisp and its many descendant languages <ref> [50, 51, 26, 31, 79, 69, 58, 125] </ref>, the lenient evaluation of Id and pH [120, 87, 88], and parallel graph reduction, e.g., [55, 96, 61]. * In Multilisp, any expression can be designated as a future that spawns a thread which may be executed in parallel.
Reference: [32] <author> Matthias Felleisen. </author> <title> On the expressive power of programming languages. </title> <booktitle> Science of Computer Programming, </booktitle> <address> 17(1-3):35-75, </address> <month> December </month> <year> 1991. </year>
Reference-contexts: However, there is no single formal notion of what this means. For example, Felleisen <ref> [32] </ref> and Mitchell [82] compared languages based on different criteria|Felleisen observed when language features could equivalently be defined as macros, whereas Mitchell observed when features can be used as abstraction contexts. Both also discussed additional previous work on comparing languages.
Reference: [33] <author> Matthias Felleisen and Daniel P. Friedman. </author> <title> A calculus for assignments in higher-order languages. </title> <booktitle> In Proceedings 13th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 314-325, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: As in a compiler, staging the implementation via an intermediate language or model frequently simplifies the problem. Here we stage the implementation using a parallel abstract machine model called the P-CEK PAL , loosely based on the serial CESK abstract machine <ref> [33] </ref>. 1 It is relatively abstract since it uses many of the same semantic domains as the high-level language model, but it is more machine-like since it is based on a state transition relation. It also shows, at an abstract level, details such as how computation is scheduled onto processors.
Reference: [34] <author> John T. Feo, David C. Cann, and Rodney R. Oldehoeft. </author> <title> A report on the Sisal language project. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 10 </volume> <pages> 349-366, </pages> <year> 1990. </year>
Reference-contexts: Thus the limitation that only sequence-based operations are parallelized is sufficient. We examine only models of Nesl which do not flatten nested parallelism. A flattening model would be somewhat more complicated than the model of Chapter 9 because of the extra compilation step for flattening. Sisal <ref> [34] </ref> is an applicative language designed for use on serial and parallel computers. It includes data-parallelism based on a flexible for construct combining looping and data reduction on streams of data. It also includes task-parallelism. Streams are non-strict and single-assignment, similar to the I-structures of Id and pH 1 .
Reference: [35] <author> Philippe Flajolet, Bruno Salvy, and Paul Zimmermann. Lambda-Upsilon-Omega: </author> <title> An assistant algorithms analayzer. Applied Algebra, Algebraic Algorithms and Error-Correcting Codes, </title> <booktitle> 357 </booktitle> <pages> 201-212, </pages> <month> June </month> <year> 1989. </year> <note> 220 BIBLIOGRAPHY </note>
Reference-contexts: Thus, a logical step would be to use these models as the core of automatic complexity analysis tools such as Metric, ACE, COMPLEXA, and fl <ref> [124, 77, 35, 126] </ref>, Kishon's profiling tool [66, 65], or compiler analyses. 216 CHAPTER 11. CONCLUSIONS
Reference: [36] <author> Philippe Flajolet, Bruno Salvy, and Paul Zimmermann. </author> <title> Automatic average-case analysis of algorithms. </title> <journal> Theoretical Computer Science, </journal> <volume> 79(1) </volume> <pages> 37-109, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: Also there has been some work on worst-case analysis of imperative languages [28] and average-case analysis of serial functional languages <ref> [36] </ref>. Converting the program into cost recurrence equations requires at least an informal definition of a language's costs, although some used formal definitions.
Reference: [37] <author> Cormac Flanagan and Matthias Felleisen. </author> <title> The semantics of future and its use in program optimization. </title> <booktitle> In Proceedings 22nd ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 209-220, </pages> <year> 1995. </year>
Reference-contexts: Although significantly more complicated, they present semantics (or parts thereof) corresponding to the PAL and PSLf models. However, they did not provide implementations or otherwise relate their model to other models of parallelism or describe how it would effect algorithms. Roe [105, 106], Flanagan and Felleisen <ref> [37] </ref>, and Moreau [83, 84, 85] provided cost models of speculative evaluation. Roe tracks only the depth of the computation, whereas Flanagan and Felleisen and Moreau track only the work. Roe used his model to analyze algorithms, but did not relate his model to more concrete models. <p> Any such technique should be based on a formal model, such as provided by this work, so that the resulting optimization can be verified and quantified. For example, Knopp [67, 68] and Flanagan and Felleisen <ref> [37] </ref> both used language models somewhat similar to the PAL model in analyses to avoid runtime checks. 2.4.3 Profiling tools A profiling tool (or profiler) instruments source or object code to keep track of run-time costs. It is used for run-time debugging and performance analysis and for guiding optimization. <p> This is consistent with Hudak and Anderson's call-by-speculation [53] terminology, which they contrasted with call-by-value and call-by-need. Second, it is speculative relative to a call-by-need evaluation, as it at least starts the evaluation of an argument even if it is irrelevant. This contrasts with some descriptions of speculativeness <ref> [90, 37, 83] </ref> that are speculative relative to a call-by-value evaluation. By definition of those descriptions, the parallel execution of a program must be extensionally equivalent to the serial execution, even in the presence of control escapes or side-effects. <p> These tags are subsumed here by * tagging values in environments with their computation graphs (recall that the encoding of a data structure is a closure, which contains an environment), together with * the evaluation judgment resulting in a value and its graph. Flanagan and Felleisen <ref> [37] </ref> and Moreau [83, 84, 85] also provided semantics for speculative languages that were augmented with costs. Both used small-step contextual-style operational semantics and included continuations or escapes in the language. Moreau also included side-effects. <p> The definition of the PSL model is simpler than Roe's similar model, while more useful for describing parallelism. The PSL defines computation graphs, and thus the amount of parallelism available, unlike those of Roe [105, 106], Flanagan and Felleisen <ref> [37] </ref>, and Moreau [84, 85], which only define the work of a computation. Similarly, the computation graphs in the NESL model are a more appropriate measure of parallelism than the maximum number of useful processors as defined by Zimmermann [128].
Reference: [38] <author> Steven Fortune and James Wyllie. </author> <title> Parallelism in random access machines. </title> <booktitle> In Proceedings 10th ACM Symposium on Theory of Computing, </booktitle> <pages> pages 114-118, </pages> <year> 1978. </year>
Reference-contexts: The operational style matches our needs better, since we are interested in the costs of the evaluation process. 1.2.6 Implementations and their cost mappings This dissertation uses three standard parallel machine models: the butterfly, the hypercube, and the Parallel Random-Access Machine (PRAM) <ref> [38] </ref>. Each of these uses a collection of processors connected by a different style of communication network, as illustrated in Figure 1.5. The butterfly and hypercube are commonly used in practical networks, while the PRAM is a common abstraction of parallel machine models. <p> The implicit side-effect in implementing call-by-need substitutes for the explicit side-effects used by Pippenger. Other parallel work used the PRAM <ref> [38] </ref>. While the PRAM is often considered a general model of parallelism useful for designing algorithms, it is also acknowledged as an abstract model which doesn't correspond to an actual machine. It abstractness stems from the unrealistic assumption of constant time communication between arbitrary processors.
Reference: [39] <author> Daniel P. Friedman and D. S. Wise. </author> <title> The impact of applicative programming on multiprocessing. </title> <booktitle> In Proceedings International Conference on Parallel Processing, </booktitle> <pages> pages 263-272, </pages> <year> 1976. </year>
Reference-contexts: Thus, these languages are deterministic and not concurrent. Side-effect-free applicative languages are a natural candidate for modeling parallelism since it is always safe to evaluate subexpressions in parallel in these languages <ref> [39, 40] </ref>. * We examine asymptotic costs because this allows us to simplify many issues by ignoring constant factors. Even the asymptotic cost bounds of languages and their implementations are not well understood, and many implementation decisions affect the run-time costs in an asymptotically significant manner.
Reference: [40] <author> Daniel P. Friedman and D. S. Wise. </author> <title> Aspects of applicative programming for parallel processing. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 27(4) </volume> <pages> 289-296, </pages> <month> April </month> <year> 1978. </year>
Reference-contexts: Thus, these languages are deterministic and not concurrent. Side-effect-free applicative languages are a natural candidate for modeling parallelism since it is always safe to evaluate subexpressions in parallel in these languages <ref> [39, 40] </ref>. * We examine asymptotic costs because this allows us to simplify many issues by ignoring constant factors. Even the asymptotic cost bounds of languages and their implementations are not well understood, and many implementation decisions affect the run-time costs in an asymptotically significant manner. <p> In the PSL model, an expression might result in a value before the expression finishes evaluating. If the program evaluates without ever needing a subexpression's value, as is the case with e in previous example, that subexpression is irrelevant <ref> [40, 48] </ref>. Here we describe two variants of the model: full speculation, which evaluates all expressions, even if irrelevant, and partial speculation, which can abort and discard irrelevant computations. The latter offers a wide spectrum of implementations, depending on which computations it aborts and when.
Reference: [41] <author> Joseph Gil and Yossi Matias. </author> <title> Fast and efficient simulations among CRCW PRAMs. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 23(2) </volume> <pages> 135-148, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Our cost bounds are parameterized by the time cost of this operation, T F (p), as Figure 8.1 shows for the same machine models as in Section 7.1. These bounds hold with high probability (w.h.p., for short) <ref> [100, 73, 41, 74] </ref>. 8.1.
Reference: [42] <author> Joseph Gil, Yossi Matias, and Uzi Vishkin. </author> <title> Towards a theory of nearly constant time parallel algorithms. </title> <booktitle> In IEEE Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 698-710, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Gil, Matias, and Vishkin <ref> [42] </ref> have shown that the linear approximate compaction problem can be solved on a p processor CRCW PRAM (arbitrary) in O (m=p + log fl p) expected time, using a randomized solution.
Reference: [43] <author> T. Goldberg and U. Zwick. </author> <title> Optimal deterministic processor allocation. </title> <booktitle> In Proceedings 4th ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 220-228, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: Gil, Matias, and Vishkin [42] have shown that the linear approximate compaction problem can be solved on a p processor CRCW PRAM (arbitrary) in O (m=p + log fl p) expected time, using a randomized solution. Goldberg and Zwick <ref> [43] </ref> have shown that the problem can be solved deterministically in O (m=p + log log p) time.
Reference: [44] <author> Michael T. Goodrich and S. Rao Kosaraju. </author> <title> Sorting on a parallel pointer machine with applications to set expression evaluation. </title> <booktitle> In Proceedings 30th IEEE Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 190-195, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: It was not shown, however, whether the model exactly modeled the PRAM. In particular since it is not known until execution how many processors are needed, it is not clear whether the scheduling could be done on the fly. Goodrich and Kosaraju <ref> [44] </ref> introduced a parallel pointer machine (PPM), but this is quite different from our models since it assumes a fixed number of processors and allows side effecting of pointers.
Reference: [45] <author> Allan Gottlieb, B. D. Lubachevsky, and Larry Rudolph. </author> <title> Basic techniques for the efficient coordination of very large numbers of cooperating sequential processors. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(2), </volume> <month> April </month> <year> 1983. </year>
Reference: [46] <author> John Greiner. </author> <title> A comparison of parallel algorithms for connected components. </title> <booktitle> In Proceedings 6th ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 16-25, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: In parallel models, a program's execution time is dependent on (at least) its work and depth and the number of processors available. Work and depth are frequently used to describe parallelism, especially in teaching parallel algorithms [63, 59, 17, 15] and implementing various applications <ref> [46, 11, 10] </ref>. Let's examine the work and depth of quicksort on m data elements. Figure 1.4 gives pseudo-code for a parallel quicksort. First, recall that a serial quicksort algorithm requires O (m log m) time in the expected case. <p> Also, the work of Skillicorn, et al. overlaps with that of automatic complexity analysis [25, 116, 117]. In addition, their work also overlaps greatly with that in algorithm analysis using high-level functional data parallel languages, which also includes work using Nesl, e.g., <ref> [11, 10, 46] </ref>, and other work in the Bird-Meertens formalism. There has been some work on obtaining asymptotically efficient data structures using functional languages, e.g. [52, 30, 89]. These each approached traditional algorithmic analysis problems, but from the perspective of modern programming languages, using language 2.7.
Reference: [47] <author> John Greiner and Guy E. Blelloch. </author> <title> A provably time-efficient parallel implementation of full speculation. </title> <booktitle> In Proceedings 23rd ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 309-321, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: It represents the application of the function value and a placeholder for its argument. It could be omitted with a resulting constant factor difference in the work and depth. 1 1 This extra node was not included in the previous version of this work <ref> [47] </ref>. Thus, the costs of this presentation are a constant factor greater than those of that earlier presentation. 8.1. LANGUAGE AND PROFILING SEMANTICS 121 elements in order and the argument branch creates the list. To access each element, the function must first access the cons-cell containing the element. <p> Comparisons to similar semantics By using computation graphs as our costs, we have been able to simplify the semantics as compared to those by Roe [105, 106] and by the author and Blelloch <ref> [47] </ref>. Similar to here, they 128 CHAPTER 8. SPECULATIVE MODELS 8.1. LANGUAGE AND PROFILING SEMANTICS 129 included depths in an environment to describe when values had been computed. <p> By this we mean that the argument does not result in some intermediate state used for communication, as it does in the PAL model. Thus we do not 3 In the previous presentation of this work <ref> [47] </ref>, states blocked in the computation substep, rather than in a "pre-fetching"-like suspension substep. Thus active states did not correspond to ready nodes, and each state could be active on two steps|once when blocking and once when reactivated. <p> As used here, pattern matching can be encoded in any of the models with 1 Note that an alternative way to define the PSL model is based on defining the depths of data objects rather than defining computation graphs <ref> [47] </ref>, effectively including these time-stamps in the semantics. 10.2.
Reference: [48] <author> Dale H. Grit and Rex L. </author> <title> Page. Deleting irrelevant tasks in an expression-oriented multiprocessor system. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 3(1) </volume> <pages> 49-59, </pages> <month> January </month> <year> 1981. </year> <note> BIBLIOGRAPHY 221 </note>
Reference-contexts: In the PSL model, an expression might result in a value before the expression finishes evaluating. If the program evaluates without ever needing a subexpression's value, as is the case with e in previous example, that subexpression is irrelevant <ref> [40, 48] </ref>. Here we describe two variants of the model: full speculation, which evaluates all expressions, even if irrelevant, and partial speculation, which can abort and discard irrelevant computations. The latter offers a wide spectrum of implementations, depending on which computations it aborts and when. <p> Several methods of garbage collection of processes has been previously described. Baker and Hewitt [6] and Hudak and Keller [54] used a mark-and-sweep approach, which is not asymptotically efficient since it traverses pointers too many times. Grit and Page <ref> [48] </ref> and Partridge [94, 93] used a reference counting approach which can be efficient if we don't spend too much effort garbage collecting on each step. We discuss this option in more detail. For each thread we maintain a count of the references to this thread from environments.
Reference: [49] <author> Douglas J. Gurr. </author> <title> Semantic Frameworks for Complexity. </title> <type> PhD thesis, </type> <institution> University of Edinburgh, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: But most of the work in intensional semantics concentrated on areas such as full abstraction (proving denotational and operational semantics equivalent) or traces of concurrent processes [21]. However, Gurr gave a categorical framework for defining language cost models and generalizing these to asymptotic complexity models <ref> [49] </ref>. While powerful, his framework does not correspond to typical programmers' intuition because of its use of category theory. And while very intriguing, the generalization to complexities is only partially successful. Also, Talcott provided an intensional theory similar to those used for automatic cost analysis [119].
Reference: [50] <author> Robert H. Halstead, Jr. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-538, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: This allows "pipelined" and "producer-consumer" parallelism to be expressed. Since synchronization is more relaxed than in fork-and-join parallelism, this can allow faster programs, but since synchronization is data-dependent, it is more difficult to formally define and implement. This style of parallelism is closely related to the futures of Multilisp <ref> [50] </ref> (also known as promises [40])|Multilisp applies an application's function to futures which represent the arguments and which eventually receive the arguments' final values. Speculative parallelism also forms the core of languages such as Id and pH [87, 2, 88]. <p> The latter offers a wide spectrum of implementations, depending on which computations it aborts and when. Speculative evaluation is most closely related to the futures of Multilisp and its many descendant languages <ref> [50, 51, 26, 31, 79, 69, 58, 125] </ref>, the lenient evaluation of Id and pH [120, 87, 88], and parallel graph reduction, e.g., [55, 96, 61]. * In Multilisp, any expression can be designated as a future that spawns a thread which may be executed in parallel.
Reference: [51] <author> Robert H. Halstead, Jr. </author> <title> New ideas in Parallel Lisp: Language design, implementation, and programming tools. </title> <editor> In T. Ito and R. H. Halstead, Jr., editors, </editor> <booktitle> Parallel Lisp: Languages and Systems, US/Japan Workshop on Parallel Lisp, number 441 in Lecture Notes in Computer Science, </booktitle> <pages> pages 2-51. </pages> <publisher> Springer-Verlag, </publisher> <month> June </month> <year> 1989. </year>
Reference-contexts: The latter offers a wide spectrum of implementations, depending on which computations it aborts and when. Speculative evaluation is most closely related to the futures of Multilisp and its many descendant languages <ref> [50, 51, 26, 31, 79, 69, 58, 125] </ref>, the lenient evaluation of Id and pH [120, 87, 88], and parallel graph reduction, e.g., [55, 96, 61]. * In Multilisp, any expression can be designated as a future that spawns a thread which may be executed in parallel.
Reference: [52] <author> Robert Hood. </author> <title> The Efficient Implementation of Very-High-Level Programming Language Constructs. </title> <type> PhD thesis, </type> <institution> Cornell University, </institution> <year> 1982. </year>
Reference-contexts: There has been some work on obtaining asymptotically efficient data structures using functional languages, e.g. <ref> [52, 30, 89] </ref>. These each approached traditional algorithmic analysis problems, but from the perspective of modern programming languages, using language 2.7. EXPRESSIVENESS 37 features such as higher-order functions and laziness. Many problems of interest here involve persistent data structures, where updates do not destroy the original data structure.
Reference: [53] <author> Paul Hudak and Steve Anderson. </author> <title> Pomset interpretations of parallel functional programs. </title> <booktitle> In Proceedings 3rd International Conference on Functional Programming Languages and Computer Architecture, number 274 in Lecture Notes in Computer Science, </booktitle> <pages> pages 234-256. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1987. </year>
Reference-contexts: All data structures are pointer-based, so all data must be stored in tree- or list-like structures. In many programs, a balanced binary tree leads to the most efficient algo rithm. * Speculative parallelism (or call-by-speculation <ref> [53] </ref>), as used here, also allows a bounded number of threads to be spawned at once. However, it synchronizes only as necessary for data dependencies, i.e., arguments are evaluated in parallel with function application and evaluation of its body. This allows "pipelined" and "producer-consumer" parallelism to be expressed. <p> However, none of this previous work has been targeted to or fully addresses our goals. Here we present a general overview of the related work|further details are included as relevant in the remainder of the dissertation. Hudak and Anderson <ref> [53] </ref> suggested modeling parallelism in functional languages using an extended operational semantics based on partially ordered multi-sets (pomsets). The semantics can be thought of as keeping a trace of the computation as a partial order specifying what had to be computed before what else. <p> The PSL model is "speculative" in two senses. First, it is speculatively parallel relative to the PAL model, as it allows a function body and argument to be evaluated in parallel when possible. This is consistent with Hudak and Anderson's call-by-speculation <ref> [53] </ref> terminology, which they contrasted with call-by-value and call-by-need. Second, it is speculative relative to a call-by-need evaluation, as it at least starts the evaluation of an argument even if it is irrelevant.
Reference: [54] <author> Paul Hudak and Robert M. Keller. </author> <title> Garbage collection and task deletion in distributed applicative processing systems. </title> <booktitle> In Proceedings ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 168-178, </pages> <year> 1982. </year>
Reference-contexts: Only following all the relevant pointers can tell us which threads are no longer accessible, and thus unnecessary. Several methods of garbage collection of processes has been previously described. Baker and Hewitt [6] and Hudak and Keller <ref> [54] </ref> used a mark-and-sweep approach, which is not asymptotically efficient since it traverses pointers too many times. Grit and Page [48] and Partridge [94, 93] used a reference counting approach which can be efficient if we don't spend too much effort garbage collecting on each step.
Reference: [55] <author> Paul Hudak and Eric Mohr. </author> <title> Graphinators and the duality of SIMD and MIMD. </title> <booktitle> In Proceedings ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 224-234, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: Speculative evaluation is most closely related to the futures of Multilisp and its many descendant languages [50, 51, 26, 31, 79, 69, 58, 125], the lenient evaluation of Id and pH [120, 87, 88], and parallel graph reduction, e.g., <ref> [55, 96, 61] </ref>. * In Multilisp, any expression can be designated as a future that spawns a thread which may be executed in parallel. If its value is needed and not yet computed, the thread requesting the future's value blocks until the value is available.
Reference: [56] <editor> Paul Hudak et al. </editor> <title> Report on the functional programming language Haskell, version 1.2. </title> <journal> SIGPLAN Notices, </journal> <volume> 27(5), </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: These are the simplest of the models we consider and are appropriate for introducing the framework. Chapters 4 and 5 define the language and its profiling semantics, respectively. Syntactically, the language is based on the -calculus and thus most resembles languages such as Scheme, ML, Haskell, and Id <ref> [27, 81, 56, 87] </ref>. Chapters 6 and 7 relate the language model to traditional machine models of computation (hypercube, butterfly, and PRAM), staging this via an intermediate model for convenience (P-CEK). Part III uses this methodology for some other parallel language models.
Reference: [57] <author> Lorenz Huelsbergen, James R. Larus, and Alexander Aiken. </author> <title> Using the run-time sizes of data structures to guide parallel-thread creation. </title> <booktitle> In Proceedings ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 79-90, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: In terms of the computation graph, this means that on each step the machine visits some set of ready nodes and then immediately visits the ready children of those nodes. This is the same basic idea as work examining heuristics for building large sequential blocks of code, e.g., <ref> [57, 96] </ref>. As long as each step performs constant work per selected state, this does not effect our asymptotic time bounds, but reduces load-balancing costs and other overhead by a constant factor.
Reference: [58] <author> Takayasu Ito and Manabu Matsui. </author> <title> A parallel lisp language PaiLisp and its kernal specification. </title> <editor> In T. Ito and R. H. Halstead, Jr., editors, </editor> <booktitle> Parallel Lisp: Languages and Systems, US/Japan Workshop on Parallel Lisp, number 441 in Lecture Notes in Computer Science, </booktitle> <pages> pages 58-100. </pages> <publisher> Springer-Verlag, </publisher> <month> June </month> <year> 1989. </year>
Reference-contexts: The latter offers a wide spectrum of implementations, depending on which computations it aborts and when. Speculative evaluation is most closely related to the futures of Multilisp and its many descendant languages <ref> [50, 51, 26, 31, 79, 69, 58, 125] </ref>, the lenient evaluation of Id and pH [120, 87, 88], and parallel graph reduction, e.g., [55, 96, 61]. * In Multilisp, any expression can be designated as a future that spawns a thread which may be executed in parallel.
Reference: [59] <author> Joseph JaJa. </author> <title> An Introduction to Parallel Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1992. </year>
Reference-contexts: In parallel models, a program's execution time is dependent on (at least) its work and depth and the number of processors available. Work and depth are frequently used to describe parallelism, especially in teaching parallel algorithms <ref> [63, 59, 17, 15] </ref> and implementing various applications [46, 11, 10]. Let's examine the work and depth of quicksort on m data elements. Figure 1.4 gives pseudo-code for a parallel quicksort. First, recall that a serial quicksort algorithm requires O (m log m) time in the expected case.
Reference: [60] <author> Neil D. Jones. </author> <title> Constant time factors do matter (extended abstract). </title> <booktitle> In Proceedings 25th ACM Symposium on Theory of Computing, </booktitle> <pages> pages 602-611, </pages> <month> May </month> <year> 1993. </year> <note> 222 BIBLIOGRAPHY </note>
Reference-contexts: Abramsky and Sykes [1] introduced the Secd-m machine, which shares a similar basis as our intermediate machines, but is non-deterministic and uses fair merge. 2.2 Relating cost models Previous work on formally relating language-based models (languages with cost-augmented semantics) to machine models is sparse. Jones <ref> [60] </ref> related the time-augmented semantics of simple while-loop language to that of an equivalent machine language in order to study the effect of constant factors in time complexity. Seidl and Wilhelm [114] provide complexity bounds for an implementation of graph reduction on the PRAM. <p> While powerful, his framework does not correspond to typical programmers' intuition because of its use of category theory. And while very intriguing, the generalization to complexities is only partially successful. Also, Talcott provided an intensional theory similar to those used for automatic cost analysis [119]. Jones, e.g., <ref> [60] </ref> has been exploring traditional complexity theory from a programming language perspective. This includes re-examining how certain complexity classes arise from different language idioms.
Reference: [61] <author> Mike Joy and Tom Axford. </author> <title> Parallel combinator reduction: Some performance bounds. </title> <type> Technical Report RR210, </type> <institution> University of Warwick, </institution> <year> 1992. </year>
Reference-contexts: Speculative evaluation is most closely related to the futures of Multilisp and its many descendant languages [50, 51, 26, 31, 79, 69, 58, 125], the lenient evaluation of Id and pH [120, 87, 88], and parallel graph reduction, e.g., <ref> [55, 96, 61] </ref>. * In Multilisp, any expression can be designated as a future that spawns a thread which may be executed in parallel. If its value is needed and not yet computed, the thread requesting the future's value blocks until the value is available.
Reference: [62] <author> A. R. Karlin and E. Upfal. </author> <title> Parallel hashing: an efficient implementation of shared memory. </title> <journal> Journal of the ACM, </journal> <volume> 35 </volume> <pages> 876-892, </pages> <year> 1988. </year>
Reference-contexts: It abstractness stems from the unrealistic assumption of constant time communication between arbitrary processors. But the PRAM has been related to other more realistic parallel models, such as those for the butterfly and hypercube <ref> [62, 101] </ref>. These relations depend entirely on simulating the more realistic communication networks, and for the butterfly and hypercube, these work-efficient simulations entail a slowdown logarithmic in the number of processors.
Reference: [63] <author> R. M. Karp and V. Ramachandran. </author> <title> Parallel algorithms for shared memory machines. </title> <editor> In J. Van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science | Volume A: Algorithms and Complexity. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: In parallel models, a program's execution time is dependent on (at least) its work and depth and the number of processors available. Work and depth are frequently used to describe parallelism, especially in teaching parallel algorithms <ref> [63, 59, 17, 15] </ref> and implementing various applications [46, 11, 10]. Let's examine the work and depth of quicksort on m data elements. Figure 1.4 gives pseudo-code for a parallel quicksort. First, recall that a serial quicksort algorithm requires O (m log m) time in the expected case.
Reference: [64] <author> Richard Kennaway. </author> <title> A conflict between call-by-need computation and parallelism (extended abstract). </title> <booktitle> In Proceedings Conditional Term Rewriting Systems-94, </booktitle> <pages> pages 247-261, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: We show that in quicksort, for example, NESL allows more efficient data access and thus more efficient algorithms than the PAL or PSL models. We do not use any call-by-name or call-by-need (lazy) languages, because they inherently do not offer significant parallelism <ref> [64, 121] </ref>. <p> Id and pH evaluate all subexpressions fully because they may contain side-effects, although a compiler might optimize cases when this is not necessary. * Graph reduction is one technique for implementing lazy (call-by-need) functional languages. But since lazy evaluation entails an inherent lack of parallelism <ref> [64, 121] </ref>, parallel versions of these languages have incorporated partial speculation, compromising on the laziness of the language. The PSL model is "speculative" in two senses.
Reference: [65] <author> Amir Kishon. </author> <title> Monitoring Semantics: Theory and Practice of Semantics-directed Execution Monitoring. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <year> 1991. </year>
Reference-contexts: Any profiler requires at least an informal definition of a language's costs, but frequently these definitions are ad hoc or special purpose. Some recent profiling tools have been based on the language semantics and a more formal notion of the costs <ref> [66, 65, 112, 113] </ref>. Since pro-filers generally need to produce highly accurate resource profiles, they require more detailed semantics than the abstract semantics provided here. But our framework allows detailed semantics and provides a formalism which could be used as the basis for these semantics-based profilers. 36 CHAPTER 2. <p> Thus, a logical step would be to use these models as the core of automatic complexity analysis tools such as Metric, ACE, COMPLEXA, and fl [124, 77, 35, 126], Kishon's profiling tool <ref> [66, 65] </ref>, or compiler analyses. 216 CHAPTER 11. CONCLUSIONS
Reference: [66] <author> Amir Kishon, Paul Hudak, and Charles Consel. </author> <title> Monitoring semantics: A formal framework for specifying, implementing, and reasoning about execution monitors. </title> <booktitle> In Proceedings ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 338-352, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Any profiler requires at least an informal definition of a language's costs, but frequently these definitions are ad hoc or special purpose. Some recent profiling tools have been based on the language semantics and a more formal notion of the costs <ref> [66, 65, 112, 113] </ref>. Since pro-filers generally need to produce highly accurate resource profiles, they require more detailed semantics than the abstract semantics provided here. But our framework allows detailed semantics and provides a formalism which could be used as the basis for these semantics-based profilers. 36 CHAPTER 2. <p> Thus, a logical step would be to use these models as the core of automatic complexity analysis tools such as Metric, ACE, COMPLEXA, and fl [124, 77, 35, 126], Kishon's profiling tool <ref> [66, 65] </ref>, or compiler analyses. 216 CHAPTER 11. CONCLUSIONS
Reference: [67] <author> Jurgen Knopp. </author> <title> Improving the performance of parallel lisp by compile time analysis. </title> <editor> In U. Kastnes and P. Pfahler, editors, </editor> <booktitle> Compiler Construction, volume 641 of Lecture Notes in Computer Science, </booktitle> <pages> pages 271-277. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Any such technique should be based on a formal model, such as provided by this work, so that the resulting optimization can be verified and quantified. For example, Knopp <ref> [67, 68] </ref> and Flanagan and Felleisen [37] both used language models somewhat similar to the PAL model in analyses to avoid runtime checks. 2.4.3 Profiling tools A profiling tool (or profiler) instruments source or object code to keep track of run-time costs.
Reference: [68] <author> Jurgen Knopp. </author> <title> Touching analysis: Avoiding runtime checking in future-based parallel languages. </title> <editor> In Hesham El-Rewini, Ted Lewis, and Bruce D. Shriver, editors, </editor> <booktitle> 26th Proceedings Hawaii International Conference on System Sciences, </booktitle> <volume> volume 2, </volume> <pages> pages 407-416. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year>
Reference-contexts: Any such technique should be based on a formal model, such as provided by this work, so that the resulting optimization can be verified and quantified. For example, Knopp <ref> [67, 68] </ref> and Flanagan and Felleisen [37] both used language models somewhat similar to the PAL model in analyses to avoid runtime checks. 2.4.3 Profiling tools A profiling tool (or profiler) instruments source or object code to keep track of run-time costs.
Reference: [69] <author> David A. Kranz, Jr. Robert H. Halstead, and Eric Mohr. Mul-T: </author> <title> A high-performance parallel lisp. </title> <booktitle> In Proceedings ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 81-90, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: The latter offers a wide spectrum of implementations, depending on which computations it aborts and when. Speculative evaluation is most closely related to the futures of Multilisp and its many descendant languages <ref> [50, 51, 26, 31, 79, 69, 58, 125] </ref>, the lenient evaluation of Id and pH [120, 87, 88], and parallel graph reduction, e.g., [55, 96, 61]. * In Multilisp, any expression can be designated as a future that spawns a thread which may be executed in parallel.
Reference: [70] <author> P. J. Landin. </author> <title> The mechanical evaluation of expressions. </title> <journal> The Computer Journal, </journal> <volume> 6 </volume> <pages> 308-320, </pages> <year> 1964. </year>
Reference-contexts: The semantics can be read as a simple interpreter for the language. For better efficiency, many compilation techniques have been developed over the years for the -calculus and related languages. The implementations used here are in the tradition of the SECD state machine of Landin <ref> [70] </ref> and its descendants, as further described in Chapter 6. 5.2 Computation graphs We use directed acyclic graphs (DAGs) to represent the process of computation, generalizing the computation's work and depth. As usual, a directed graph consists of a set of nodes and a set of directed edges. <p> Section 6.3 shows the equivalence of the PAL model and the P-CEK q PAL machine. Later, Chapter 7 relates the P-CEK q less abstract machine models to complete the implementation. 1 The CESK is one of many variants of the original SECD machine for implementing the -calculus <ref> [70] </ref>. The names of these machines are formed by the names of the meta-variables representing the elements of each state: the CESK uses a control string, environment, store, and continuation; the SECD uses a stack, environment, control string, and dump (a form of continuation). 81 82 CHAPTER 6.
Reference: [71] <author> Peter Lee and Uwe Pleban. </author> <title> A realistic compiler generator based on high-level semantics. </title> <booktitle> In Proceedings 14th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 284-299, </pages> <year> 1987. </year>
Reference-contexts: But our framework allows detailed semantics and provides a formalism which could be used as the basis for these semantics-based profilers. 36 CHAPTER 2. RELATED WORK 2.5 Provably correct implementations The general goal in provably correct implementation is obtaining compilers which produce efficient and provably correct code, e.g. <ref> [95, 71, 98, 24, 19] </ref>. At the core, this work provides a relatively abstract source code semantics, a very detailed object code semantics, and a provably correct compiler mapping between the two. These correspond to our language model, machine model, and mapping.
Reference: [72] <author> F. T. Leighton, B. M. Maggs, A. G. Ranade, and S. B. Rao. </author> <title> Randomized routing and sorting on fixed-connection networks. </title> <journal> Journal of Algorithms, </journal> <volume> 17(1) </volume> <pages> 157-205, </pages> <month> July </month> <year> 1994. </year> <note> BIBLIOGRAPHY 223 </note>
Reference-contexts: But the PRAM is commonly used to describe algorithms so that computation issues are not obscured by communication issues|a common problem in more realistic models. These simpler PRAM algorithms can then be mapped to other models using standard implementation techniques <ref> [101, 123, 72] </ref>. We discuss several variants of the PRAM: primarily the concurrent-read concurrent-write (CRCW), but also the exclusive-read exclusive-write (EREW) and concurrent-read exclusive-write (CREW), which differ in what memory accesses are allowed, as their names imply.
Reference: [73] <author> Yossi Matias and Uzi Vishkin. </author> <title> On parallel hashing and integer sorting. </title> <journal> Journal of Algorithms, </journal> <volume> 12(4) </volume> <pages> 573-606, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: of holes in the array, we can replace the scan operations with linear approximate compaction: given an array of m cells, m 0 of which contain an object, place the m 0 objects in distinct cells of an array of size k m 0 for some constant k &gt; 1 <ref> [73] </ref>. Gil, Matias, and Vishkin [42] have shown that the linear approximate compaction problem can be solved on a p processor CRCW PRAM (arbitrary) in O (m=p + log fl p) expected time, using a randomized solution. <p> Our cost bounds are parameterized by the time cost of this operation, T F (p), as Figure 8.1 shows for the same machine models as in Section 7.1. These bounds hold with high probability (w.h.p., for short) <ref> [100, 73, 41, 74] </ref>. 8.1.
Reference: [74] <author> Yossi Matias and Uzi Vishkin. </author> <title> A note on reducing parallel model simulations to integer sorting. </title> <booktitle> In Proceedings 9th International Parallel Processing Symposium, </booktitle> <pages> pages 208-212, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Our cost bounds are parameterized by the time cost of this operation, T F (p), as Figure 8.1 shows for the same machine models as in Section 7.1. These bounds hold with high probability (w.h.p., for short) <ref> [100, 73, 41, 74] </ref>. 8.1.
Reference: [75] <author> Kurt Mehlhorn and Uzi Vishkin. </author> <title> Randomized and deterministic simulations of PRAMs by parallel machines with restricted granularity of parallel memory. </title> <journal> Acta Informatica, </journal> <volume> 21 </volume> <pages> 339-374, </pages> <year> 1984. </year>
Reference-contexts: On such a machine, each of the p processors can access (read or write) n elements in O (n + log p) time, with high probability <ref> [75, 101] </ref>. 1 The O (log p) time is due to latency through the network. We also assume the butterfly network has simple integer adders in the switches, such that scan and reduce operations can execute in O (log p) time.
Reference: [76] <author> Daniel Le Metayer. </author> <title> Mechanical analysis of program complexity. </title> <booktitle> In Proceedings SIG-PLAN Symposium on Language Issues in Programming Environments, </booktitle> <year> 1985. </year>
Reference: [77] <author> Daniel Le Metayer. </author> <title> Ace: An automatic complexity evaluator. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(2) </volume> <pages> 248-266, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: Thus, a logical step would be to use these models as the core of automatic complexity analysis tools such as Metric, ACE, COMPLEXA, and fl <ref> [124, 77, 35, 126] </ref>, Kishon's profiling tool [66, 65], or compiler analyses. 216 CHAPTER 11. CONCLUSIONS
Reference: [78] <author> Daniel Le Metayer. </author> <title> Analysis of functional programs by program transformation. </title> <editor> In J.-P. Ban^atre, S. B. Jones, and D. Le Metayer, editors, </editor> <booktitle> Prospects for Functional Programming in Software Engineering, volume 1 of Research Reports, ESPRIT, Project 302, chapter 5, </booktitle> <pages> pages 87-120. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference: [79] <author> James S. Miller. MultiScheme: </author> <title> A Parallel Processing System Based on MIT Scheme. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> September </month> <year> 1987. </year>
Reference-contexts: The latter offers a wide spectrum of implementations, depending on which computations it aborts and when. Speculative evaluation is most closely related to the futures of Multilisp and its many descendant languages <ref> [50, 51, 26, 31, 79, 69, 58, 125] </ref>, the lenient evaluation of Id and pH [120, 87, 88], and parallel graph reduction, e.g., [55, 96, 61]. * In Multilisp, any expression can be designated as a future that spawns a thread which may be executed in parallel.
Reference: [80] <author> Peter H. Mills, Lars S. Nyland, Jan F. Prins, John H. Reif, and Robert A. Wagner. </author> <title> Prototyping parallel and distributed programs in Proteus. </title> <type> Technical Report UNC-CH TR90-041, </type> <institution> Computer Science Department, University of North Carolina, </institution> <year> 1990. </year>
Reference-contexts: Under these conditions they show how to process n nodes in O (n=p + p log p) time (which is a factor of p worse than our bounds in the second term). Riely, Prins, and Iyer [104] defined a data-parallel language model based on Proteus <ref> [80] </ref> and related it to the VRAM model. The structure of their work is very similar since it is based on earlier versions of this work. Also, their Proteus-based model is similar to the Nesl-based model shown here since these two languages are fundamentally similar.
Reference: [81] <author> Robin Milner, Mads Tofte, and Robert Harper. </author> <title> The Definition of Standard ML. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: These are the simplest of the models we consider and are appropriate for introducing the framework. Chapters 4 and 5 define the language and its profiling semantics, respectively. Syntactically, the language is based on the -calculus and thus most resembles languages such as Scheme, ML, Haskell, and Id <ref> [27, 81, 56, 87] </ref>. Chapters 6 and 7 relate the language model to traditional machine models of computation (hypercube, butterfly, and PRAM), staging this via an intermediate model for convenience (P-CEK). Part III uses this methodology for some other parallel language models.
Reference: [82] <author> John C. Mitchell. </author> <title> On abstraction and the expressive power of programming languages. </title> <booktitle> In Proceedings Theoretical Aspects of Computer Software, </booktitle> <pages> pages 290-310, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: However, there is no single formal notion of what this means. For example, Felleisen [32] and Mitchell <ref> [82] </ref> compared languages based on different criteria|Felleisen observed when language features could equivalently be defined as macros, whereas Mitchell observed when features can be used as abstraction contexts. Both also discussed additional previous work on comparing languages.
Reference: [83] <author> Luc Moreau. </author> <title> The PCKS-machine. an abstract machine for sound evaluation of parallel functional programs with first-class continuations. </title> <booktitle> In European Symposium on Programming, number 788 in Lecture Notes in Computer Science, </booktitle> <pages> pages 424-438. </pages> <publisher> Springer-Verlag, </publisher> <month> April </month> <year> 1994. </year>
Reference-contexts: Although significantly more complicated, they present semantics (or parts thereof) corresponding to the PAL and PSLf models. However, they did not provide implementations or otherwise relate their model to other models of parallelism or describe how it would effect algorithms. Roe [105, 106], Flanagan and Felleisen [37], and Moreau <ref> [83, 84, 85] </ref> provided cost models of speculative evaluation. Roe tracks only the depth of the computation, whereas Flanagan and Felleisen and Moreau track only the work. Roe used his model to analyze algorithms, but did not relate his model to more concrete models. <p> This is consistent with Hudak and Anderson's call-by-speculation [53] terminology, which they contrasted with call-by-value and call-by-need. Second, it is speculative relative to a call-by-need evaluation, as it at least starts the evaluation of an argument even if it is irrelevant. This contrasts with some descriptions of speculativeness <ref> [90, 37, 83] </ref> that are speculative relative to a call-by-value evaluation. By definition of those descriptions, the parallel execution of a program must be extensionally equivalent to the serial execution, even in the presence of control escapes or side-effects. <p> These tags are subsumed here by * tagging values in environments with their computation graphs (recall that the encoding of a data structure is a closure, which contains an environment), together with * the evaluation judgment resulting in a value and its graph. Flanagan and Felleisen [37] and Moreau <ref> [83, 84, 85] </ref> also provided semantics for speculative languages that were augmented with costs. Both used small-step contextual-style operational semantics and included continuations or escapes in the language. Moreau also included side-effects. Each described two measures of the work cost of evaluation: the total and the mandatory, i.e., non-speculative, work.
Reference: [84] <author> Luc Moreau. </author> <title> The semantics of Scheme with future. </title> <type> Technical Report M95/7, </type> <institution> Department of Electronics and Computer Science, University of Southampton, </institution> <year> 1995. </year>
Reference-contexts: Although significantly more complicated, they present semantics (or parts thereof) corresponding to the PAL and PSLf models. However, they did not provide implementations or otherwise relate their model to other models of parallelism or describe how it would effect algorithms. Roe [105, 106], Flanagan and Felleisen [37], and Moreau <ref> [83, 84, 85] </ref> provided cost models of speculative evaluation. Roe tracks only the depth of the computation, whereas Flanagan and Felleisen and Moreau track only the work. Roe used his model to analyze algorithms, but did not relate his model to more concrete models. <p> These tags are subsumed here by * tagging values in environments with their computation graphs (recall that the encoding of a data structure is a closure, which contains an environment), together with * the evaluation judgment resulting in a value and its graph. Flanagan and Felleisen [37] and Moreau <ref> [83, 84, 85] </ref> also provided semantics for speculative languages that were augmented with costs. Both used small-step contextual-style operational semantics and included continuations or escapes in the language. Moreau also included side-effects. Each described two measures of the work cost of evaluation: the total and the mandatory, i.e., non-speculative, work. <p> This results in a q-DFT if the reactivated threads are added to the active states in their 1-DFT order. Thus, the P-CEK q PSLf executes a q-DFT if threads blocking on a given thread block in their 1-DFT order. Comparisons to similar machines. Moreau <ref> [84, 85] </ref> uses two similar, but more abstract, machines for speculative computation. <p> The definition of the PSL model is simpler than Roe's similar model, while more useful for describing parallelism. The PSL defines computation graphs, and thus the amount of parallelism available, unlike those of Roe [105, 106], Flanagan and Felleisen [37], and Moreau <ref> [84, 85] </ref>, which only define the work of a computation. Similarly, the computation graphs in the NESL model are a more appropriate measure of parallelism than the maximum number of useful processors as defined by Zimmermann [128]. And this definition is formal, unlike that specified for NESL [14]. <p> The speculative implementation is asymptotically faster than those used in practice, as it correctly parallelizes the suspension and reawakening of blocked threads. It is a much more detailed implementation that than of Moreau <ref> [84] </ref>. The data-parallel implementation is asymptotically more space-efficient than the current version of NESL. We examine three examples|quicksort, mergesort, and Fast Fourier Transform|of how to program and analyze program costs in these models. <p> First, the speculative model could be adapted in two orthogonal ways: * It could incorporate side-effects and/or continuations, in the style of Moreau <ref> [84, 85] </ref>.
Reference: [85] <author> Luc Moreau. </author> <title> The semantics of Scheme with future. </title> <booktitle> In Proceedings 1st ACM SIGPLAN International Conference on Functional Programming, </booktitle> <pages> pages 146-156, </pages> <month> May </month> <year> 1996. </year> <note> 224 BIBLIOGRAPHY </note>
Reference-contexts: Although significantly more complicated, they present semantics (or parts thereof) corresponding to the PAL and PSLf models. However, they did not provide implementations or otherwise relate their model to other models of parallelism or describe how it would effect algorithms. Roe [105, 106], Flanagan and Felleisen [37], and Moreau <ref> [83, 84, 85] </ref> provided cost models of speculative evaluation. Roe tracks only the depth of the computation, whereas Flanagan and Felleisen and Moreau track only the work. Roe used his model to analyze algorithms, but did not relate his model to more concrete models. <p> These tags are subsumed here by * tagging values in environments with their computation graphs (recall that the encoding of a data structure is a closure, which contains an environment), together with * the evaluation judgment resulting in a value and its graph. Flanagan and Felleisen [37] and Moreau <ref> [83, 84, 85] </ref> also provided semantics for speculative languages that were augmented with costs. Both used small-step contextual-style operational semantics and included continuations or escapes in the language. Moreau also included side-effects. Each described two measures of the work cost of evaluation: the total and the mandatory, i.e., non-speculative, work. <p> This results in a q-DFT if the reactivated threads are added to the active states in their 1-DFT order. Thus, the P-CEK q PSLf executes a q-DFT if threads blocking on a given thread block in their 1-DFT order. Comparisons to similar machines. Moreau <ref> [84, 85] </ref> uses two similar, but more abstract, machines for speculative computation. <p> The definition of the PSL model is simpler than Roe's similar model, while more useful for describing parallelism. The PSL defines computation graphs, and thus the amount of parallelism available, unlike those of Roe [105, 106], Flanagan and Felleisen [37], and Moreau <ref> [84, 85] </ref>, which only define the work of a computation. Similarly, the computation graphs in the NESL model are a more appropriate measure of parallelism than the maximum number of useful processors as defined by Zimmermann [128]. And this definition is formal, unlike that specified for NESL [14]. <p> First, the speculative model could be adapted in two orthogonal ways: * It could incorporate side-effects and/or continuations, in the style of Moreau <ref> [84, 85] </ref>.
Reference: [86] <author> Rishiyur S. Nikhil. </author> <title> The parallel programming language Id and its compilation for parallel machines. </title> <type> Technical Report Computation Structures Group Memo 313, </type> <institution> Mas-sachusetts Institute of Technology, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: This work provides a general framework for such comparisons, although mainly targeted towards the use of more abstract language models of computation. 2.3 Implementations of dynamically parallel languages This section briefly overviews some related work in implementing languages with dynamic parallelism. Parallel implementations of Id and pH, e.g. <ref> [4, 86, 94, 93] </ref>, are generally based on assigning tasks to processors and minimizing the movement of tasks between processors. Each processor 34 CHAPTER 2. RELATED WORK has a queue of tasks waiting for a processor.
Reference: [87] <author> Rishiyur S. Nikhil. </author> <note> Id version 90.1 reference manual. Technical Report Computation Structures Group Memo 284-1, </note> <institution> Laboratory for Computer Science, Massachusetts Institute of Technology, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: This style of parallelism is closely related to the futures of Multilisp [50] (also known as promises [40])|Multilisp applies an application's function to futures which represent the arguments and which eventually receive the arguments' final values. Speculative parallelism also forms the core of languages such as Id and pH <ref> [87, 2, 88] </ref>. The Parallel Speculative -calculus (PSL) allows two threads to be spawned by an application expression, as in the PAL. Synchronization occurs only when looking up a variable's value. <p> These are the simplest of the models we consider and are appropriate for introducing the framework. Chapters 4 and 5 define the language and its profiling semantics, respectively. Syntactically, the language is based on the -calculus and thus most resembles languages such as Scheme, ML, Haskell, and Id <ref> [27, 81, 56, 87] </ref>. Chapters 6 and 7 relate the language model to traditional machine models of computation (hypercube, butterfly, and PRAM), staging this via an intermediate model for convenience (P-CEK). Part III uses this methodology for some other parallel language models. <p> The latter offers a wide spectrum of implementations, depending on which computations it aborts and when. Speculative evaluation is most closely related to the futures of Multilisp and its many descendant languages [50, 51, 26, 31, 79, 69, 58, 125], the lenient evaluation of Id and pH <ref> [120, 87, 88] </ref>, and parallel graph reduction, e.g., [55, 96, 61]. * In Multilisp, any expression can be designated as a future that spawns a thread which may be executed in parallel. <p> When t 2 finishes with a result value, t 1 reactivates and then accesses the value. We implement suspension using a queue per thread of the threads suspended on it. I.e., the above t 1 would be on t 2 's queue. Previous implementations, e.g., <ref> [90, 87, 88] </ref>, serialized the operations on these queues. Thus if other threads also suspend on t 2 at the same step of the machine that t 1 does, it would take steps proportional to the number of these threads to enqueue them.
Reference: [88] <author> Rishiyur S. Nikhil, Arvind, James Hicks, Shail Aditya, Lennart Augustsson, Jan-Willem Maessen, and Yuli Zhou. </author> <title> pH language reference manual, </title> <note> version 1.0|preliminary. Technical Report Computation Structures Group Memo 369, </note> <institution> Laboratory for Computer Science, Massachusetts Institute of Technology, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: This style of parallelism is closely related to the futures of Multilisp [50] (also known as promises [40])|Multilisp applies an application's function to futures which represent the arguments and which eventually receive the arguments' final values. Speculative parallelism also forms the core of languages such as Id and pH <ref> [87, 2, 88] </ref>. The Parallel Speculative -calculus (PSL) allows two threads to be spawned by an application expression, as in the PAL. Synchronization occurs only when looking up a variable's value. <p> The latter offers a wide spectrum of implementations, depending on which computations it aborts and when. Speculative evaluation is most closely related to the futures of Multilisp and its many descendant languages [50, 51, 26, 31, 79, 69, 58, 125], the lenient evaluation of Id and pH <ref> [120, 87, 88] </ref>, and parallel graph reduction, e.g., [55, 96, 61]. * In Multilisp, any expression can be designated as a future that spawns a thread which may be executed in parallel. <p> When t 2 finishes with a result value, t 1 reactivates and then accesses the value. We implement suspension using a queue per thread of the threads suspended on it. I.e., the above t 1 would be on t 2 's queue. Previous implementations, e.g., <ref> [90, 87, 88] </ref>, serialized the operations on these queues. Thus if other threads also suspend on t 2 at the same step of the machine that t 1 does, it would take steps proportional to the number of these threads to enqueue them.
Reference: [89] <author> Chris Okasaki. </author> <title> Simple and efficient purely functional queues and dequeues. </title> <journal> Journal of Functional Programming, </journal> <volume> 5(4) </volume> <pages> 583-592, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: There has been some work on obtaining asymptotically efficient data structures using functional languages, e.g. <ref> [52, 30, 89] </ref>. These each approached traditional algorithmic analysis problems, but from the perspective of modern programming languages, using language 2.7. EXPRESSIVENESS 37 features such as higher-order functions and laziness. Many problems of interest here involve persistent data structures, where updates do not destroy the original data structure.
Reference: [90] <author> Randy B. Osborne. </author> <title> Speculative Computation in Multilisp. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: This is consistent with Hudak and Anderson's call-by-speculation [53] terminology, which they contrasted with call-by-value and call-by-need. Second, it is speculative relative to a call-by-need evaluation, as it at least starts the evaluation of an argument even if it is irrelevant. This contrasts with some descriptions of speculativeness <ref> [90, 37, 83] </ref> that are speculative relative to a call-by-value evaluation. By definition of those descriptions, the parallel execution of a program must be extensionally equivalent to the serial execution, even in the presence of control escapes or side-effects. <p> When t 2 finishes with a result value, t 1 reactivates and then accesses the value. We implement suspension using a queue per thread of the threads suspended on it. I.e., the above t 1 would be on t 2 's queue. Previous implementations, e.g., <ref> [90, 87, 88] </ref>, serialized the operations on these queues. Thus if other threads also suspend on t 2 at the same step of the machine that t 1 does, it would take steps proportional to the number of these threads to enqueue them.
Reference: [91] <author> Robert Paige. </author> <title> Real-time simulation of a set machine on a RAM. </title> <editor> In W. Koczkodaj, editor, </editor> <booktitle> Proceedings International Conference on Computing and Information, </booktitle> <volume> volume 2, </volume> <pages> pages 68-73, </pages> <year> 1989. </year>
Reference-contexts: The models we use follow in this tradition, although we also use arrays in the NESL model. However, our model is based on a high-level language and also incorporate parallelism. But we find the PAL model suffers a corresponding slowdown from the NESL model. Paige <ref> [91] </ref> also compares models similar to those used by Ben-Amram and Gali, although using the set-based language SETL. Pippenger [97] also worked with serial pointer-based models, but compared a call-by-value model without side-effects (i.e., without setcar! and setcdr!) to a model with these.
Reference: [92] <author> Michel Parigot. </author> <title> Programming with proofs: A second order type theory. </title> <editor> In H. Ganzinger, editor, </editor> <booktitle> Proceedings 2nd European Symposium on Programming, volume 300 of Lecture Notes in Computer Science, </booktitle> <pages> pages 145-159. </pages> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: EXTENDED -CALCULUS 49 i 2 Integers c 2 Constants ::= i j add j sub j mul j lt j numeric constants add i j sub i j mul i j div i j lt i operations <ref> [92] </ref>. But since most machines use a fixed-precision arithmetic, they provide constant time operations for multiplication, division, equality, etc. To model this, we include such operations as constants.
Reference: [93] <author> Andrew S. Partridge. </author> <title> Speculative Evaluation in Parallel Implementations of Lazy Functional Languages. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Tasma-nia, </institution> <year> 1991. </year>
Reference-contexts: This work provides a general framework for such comparisons, although mainly targeted towards the use of more abstract language models of computation. 2.3 Implementations of dynamically parallel languages This section briefly overviews some related work in implementing languages with dynamic parallelism. Parallel implementations of Id and pH, e.g. <ref> [4, 86, 94, 93] </ref>, are generally based on assigning tasks to processors and minimizing the movement of tasks between processors. Each processor 34 CHAPTER 2. RELATED WORK has a queue of tasks waiting for a processor. <p> PARTIALLY SPECULATIVE IMPLEMENTATIONS 151 * Threads created by APP are more speculative than the original thread, because the original parent thread must be used to communicate this one's name for it to be used. The prioritization used by Partridge <ref> [94, 93] </ref> is an example of this. If we assume that each speculative child has equal probability to be relevant, then the active states should be kept as a tree, where each node represents active states of equal priority, and each edge represents a speculative child relationship. <p> Several methods of garbage collection of processes has been previously described. Baker and Hewitt [6] and Hudak and Keller [54] used a mark-and-sweep approach, which is not asymptotically efficient since it traverses pointers too many times. Grit and Page [48] and Partridge <ref> [94, 93] </ref> used a reference counting approach which can be efficient if we don't spend too much effort garbage collecting on each step. We discuss this option in more detail. For each thread we maintain a count of the references to this thread from environments.
Reference: [94] <author> Andrew S. Partridge and Anthony H. Dekker. </author> <title> Speculative parallelism in a distributed graph reduction machine. </title> <booktitle> In Proceedings Hawaii International Conference on System Sciences, </booktitle> <volume> volume 2, </volume> <pages> pages 771-779, </pages> <year> 1989. </year>
Reference-contexts: This work provides a general framework for such comparisons, although mainly targeted towards the use of more abstract language models of computation. 2.3 Implementations of dynamically parallel languages This section briefly overviews some related work in implementing languages with dynamic parallelism. Parallel implementations of Id and pH, e.g. <ref> [4, 86, 94, 93] </ref>, are generally based on assigning tasks to processors and minimizing the movement of tasks between processors. Each processor 34 CHAPTER 2. RELATED WORK has a queue of tasks waiting for a processor. <p> PARTIALLY SPECULATIVE IMPLEMENTATIONS 151 * Threads created by APP are more speculative than the original thread, because the original parent thread must be used to communicate this one's name for it to be used. The prioritization used by Partridge <ref> [94, 93] </ref> is an example of this. If we assume that each speculative child has equal probability to be relevant, then the active states should be kept as a tree, where each node represents active states of equal priority, and each edge represents a speculative child relationship. <p> Several methods of garbage collection of processes has been previously described. Baker and Hewitt [6] and Hudak and Keller [54] used a mark-and-sweep approach, which is not asymptotically efficient since it traverses pointers too many times. Grit and Page [48] and Partridge <ref> [94, 93] </ref> used a reference counting approach which can be efficient if we don't spend too much effort garbage collecting on each step. We discuss this option in more detail. For each thread we maintain a count of the references to this thread from environments.
Reference: [95] <author> Lawrence C. Paulson. </author> <title> A semantics-directed compiler generator. </title> <booktitle> In Proceedings 9th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 224-239, </pages> <month> January </month> <year> 1982. </year>
Reference-contexts: But our framework allows detailed semantics and provides a formalism which could be used as the basis for these semantics-based profilers. 36 CHAPTER 2. RELATED WORK 2.5 Provably correct implementations The general goal in provably correct implementation is obtaining compilers which produce efficient and provably correct code, e.g. <ref> [95, 71, 98, 24, 19] </ref>. At the core, this work provides a relatively abstract source code semantics, a very detailed object code semantics, and a provably correct compiler mapping between the two. These correspond to our language model, machine model, and mapping.
Reference: [96] <author> Simon L Peyton Jones. </author> <title> Parallel implementations of functional programming languages. </title> <journal> The Computer Journal, </journal> <volume> 32(2) </volume> <pages> 175-186, </pages> <year> 1989. </year>
Reference-contexts: Speculative evaluation is most closely related to the futures of Multilisp and its many descendant languages [50, 51, 26, 31, 79, 69, 58, 125], the lenient evaluation of Id and pH [120, 87, 88], and parallel graph reduction, e.g., <ref> [55, 96, 61] </ref>. * In Multilisp, any expression can be designated as a future that spawns a thread which may be executed in parallel. If its value is needed and not yet computed, the thread requesting the future's value blocks until the value is available. <p> The implementations aggressively create many threads to maximize parallelism and frequently synchronize all threads to guarantee load-balancing. Since most expressions are relatively simple, and the cost of thread management is high, creating a thread for each 11.2. FUTURE WORK 213 subexpression involves too much overhead (e.g., <ref> [96] </ref>). Furthermore, each step performs little computation between each load-balancing. Thus the ratios of computation to both overhead and communication are relatively low. One way to increase both ratios is grouping multiple sets of substeps between each load-balancing. <p> In terms of the computation graph, this means that on each step the machine visits some set of ready nodes and then immediately visits the ready children of those nodes. This is the same basic idea as work examining heuristics for building large sequential blocks of code, e.g., <ref> [57, 96] </ref>. As long as each step performs constant work per selected state, this does not effect our asymptotic time bounds, but reduces load-balancing costs and other overhead by a constant factor.
Reference: [97] <author> Nicholas Pippenger. </author> <title> Pure versus impure lisp. </title> <booktitle> In Proceedings 23rd ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 104-109, </pages> <month> January </month> <year> 1996. </year> <note> BIBLIOGRAPHY 225 </note>
Reference-contexts: However, our model is based on a high-level language and also incorporate parallelism. But we find the PAL model suffers a corresponding slowdown from the NESL model. Paige [91] also compares models similar to those used by Ben-Amram and Gali, although using the set-based language SETL. Pippenger <ref> [97] </ref> also worked with serial pointer-based models, but compared a call-by-value model without side-effects (i.e., without setcar! and setcdr!) to a model with these. He found that in general the purely functional model suffers a polylogarithmic slowdown relative to the imperative model.
Reference: [98] <author> Uwe F. Pleban and Peter Lee. </author> <title> An automatically generated, realistic compiler for an imperative programming language. </title> <booktitle> In Proceedings ACM SIGPLAN Conference on Programming Language Design and Implementation, volume 23 of SIGPLAN Notices, </booktitle> <pages> pages 222-227, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: But our framework allows detailed semantics and provides a formalism which could be used as the basis for these semantics-based profilers. 36 CHAPTER 2. RELATED WORK 2.5 Provably correct implementations The general goal in provably correct implementation is obtaining compilers which produce efficient and provably correct code, e.g. <ref> [95, 71, 98, 24, 19] </ref>. At the core, this work provides a relatively abstract source code semantics, a very detailed object code semantics, and a provably correct compiler mapping between the two. These correspond to our language model, machine model, and mapping.
Reference: [99] <author> Gordon D. Plotkin. </author> <title> Call-by-name, call-by-value and the -calculus. </title> <journal> Theoretical Computer Science, </journal> <volume> 1, </volume> <month> August </month> <year> 1974. </year>
Reference-contexts: Thus any data structure using pairs is also built speculatively. Since we include no basic serialization construct in the core of PSL, providing a serializing binding construct, for example, is more difficult. But it can be encoded using a continuation passing style (CPS) transformation (e.g., <ref> [99] </ref>): T PSL [[slet x = e 1 in e 2 ]] = CPS [[e 1 ]] (x:e 2 ): Traditionally used for serial computation, CPS makes the standard serial path of evaluation control explicit.
Reference: [100] <author> Abhiram G. Ranade. </author> <title> Fluent Parallel Computation. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <address> New Haven, CT, </address> <year> 1989. </year>
Reference-contexts: Our cost bounds are parameterized by the time cost of this operation, T F (p), as Figure 8.1 shows for the same machine models as in Section 7.1. These bounds hold with high probability (w.h.p., for short) <ref> [100, 73, 41, 74] </ref>. 8.1. <p> The simulation we use gives the same results for the EREW, CREW, and CRCW PRAM as well as for the multiprefix <ref> [100] </ref> and scan models [12]. The simulation is optimal in terms of work for all the PRAM variants. This is because it takes logarithmic work to simulate each random access into memory (this is the same as for pointer machines [7]).
Reference: [101] <author> Abhiram G. Ranade. </author> <title> How to emulate shared memory. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 42(3) </volume> <pages> 307-326, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: It abstractness stems from the unrealistic assumption of constant time communication between arbitrary processors. But the PRAM has been related to other more realistic parallel models, such as those for the butterfly and hypercube <ref> [62, 101] </ref>. These relations depend entirely on simulating the more realistic communication networks, and for the butterfly and hypercube, these work-efficient simulations entail a slowdown logarithmic in the number of processors. <p> But the PRAM is commonly used to describe algorithms so that computation issues are not obscured by communication issues|a common problem in more realistic models. These simpler PRAM algorithms can then be mapped to other models using standard implementation techniques <ref> [101, 123, 72] </ref>. We discuss several variants of the PRAM: primarily the concurrent-read concurrent-write (CRCW), but also the exclusive-read exclusive-write (EREW) and concurrent-read exclusive-write (CREW), which differ in what memory accesses are allowed, as their names imply. <p> On such a machine, each of the p processors can access (read or write) n elements in O (n + log p) time, with high probability <ref> [75, 101] </ref>. 1 The O (log p) time is due to latency through the network. We also assume the butterfly network has simple integer adders in the switches, such that scan and reduce operations can execute in O (log p) time.
Reference: [102] <author> Rudiger Reischuk. </author> <title> Probabilistic parallel algorithms for sorting and selection. </title> <journal> SIAM Journal of Computing, </journal> <volume> 14(2) </volume> <pages> 396-409, </pages> <year> 1985. </year>
Reference-contexts: We note that the worst case recursion depth is O (m) and that fewer than 1 of the m possible inputs will lead to a recursion depth greater than k log m, for some constant k <ref> [102] </ref>. To determine the total computational depth of qsort rec, we need to consider the computational depth along the longest path. We claim that this computational depth is at most O (d) times the recursion depth since each node along the recursion tree will require at most O (d) depth.
Reference: [103] <author> Oege de Moor Richard Bird, Geraint Jones. </author> <title> A lazy pure language versus impure lisp. </title> <booktitle> Post in comp.lang.functional newsgroup, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Pippenger [97] also worked with serial pointer-based models, but compared a call-by-value model without side-effects (i.e., without setcar! and setcdr!) to a model with these. He found that in general the purely functional model suffers a polylogarithmic slowdown relative to the imperative model. Bird, Jones, and de Moor <ref> [103] </ref> showed that Pippenger's results extended to also show that the same call-by-value model suffers the same slowdown relative to a purely functional call-by-need model. The implicit side-effect in implementing call-by-need substitutes for the explicit side-effects used by Pippenger. Other parallel work used the PRAM [38].
Reference: [104] <author> J. W. Riely, J. Prins, and S. P. Iyer. </author> <title> Provably correct vectorization of nested-parallel programs. </title> <booktitle> In Proceedings Programming Models for Massively Parallel Computers, </booktitle> <pages> pages 213-222. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1995. </year>
Reference-contexts: Under these conditions they show how to process n nodes in O (n=p + p log p) time (which is a factor of p worse than our bounds in the second term). Riely, Prins, and Iyer <ref> [104] </ref> defined a data-parallel language model based on Proteus [80] and related it to the VRAM model. The structure of their work is very similar since it is based on earlier versions of this work.
Reference: [105] <author> Paul Roe. </author> <title> Calculating lenient programs' performance. </title> <editor> In Simon L Peyton Jones, Graham Hutton, and Carsten Kehler Holst, editors, </editor> <booktitle> Proceedings Functional Programming, Glasgow 1990, Workshops in computing, </booktitle> <pages> pages 227-236. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1990. </year>
Reference-contexts: Thus, these pomsets correspond closely with computation graphs. Although significantly more complicated, they present semantics (or parts thereof) corresponding to the PAL and PSLf models. However, they did not provide implementations or otherwise relate their model to other models of parallelism or describe how it would effect algorithms. Roe <ref> [105, 106] </ref>, Flanagan and Felleisen [37], and Moreau [83, 84, 85] provided cost models of speculative evaluation. Roe tracks only the depth of the computation, whereas Flanagan and Felleisen and Moreau track only the work. <p> The left spine including the root represents the main thread; the other two nodes are separate threads, only one of which synchronizes. Comparisons to similar semantics By using computation graphs as our costs, we have been able to simplify the semantics as compared to those by Roe <ref> [105, 106] </ref> and by the author and Blelloch [47]. Similar to here, they 128 CHAPTER 8. SPECULATIVE MODELS 8.1. LANGUAGE AND PROFILING SEMANTICS 129 included depths in an environment to describe when values had been computed. <p> The PAL and NESL models also define the amount of maximum reachable space during evaluation. The definition of the PSL model is simpler than Roe's similar model, while more useful for describing parallelism. The PSL defines computation graphs, and thus the amount of parallelism available, unlike those of Roe <ref> [105, 106] </ref>, Flanagan and Felleisen [37], and Moreau [84, 85], which only define the work of a computation. Similarly, the computation graphs in the NESL model are a more appropriate measure of parallelism than the maximum number of useful processors as defined by Zimmermann [128].
Reference: [106] <author> Paul Roe. </author> <title> Parallel Programming using Functional Languages. </title> <type> PhD thesis, </type> <institution> Department of Computing Science, University of Glasgow, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: Thus, these pomsets correspond closely with computation graphs. Although significantly more complicated, they present semantics (or parts thereof) corresponding to the PAL and PSLf models. However, they did not provide implementations or otherwise relate their model to other models of parallelism or describe how it would effect algorithms. Roe <ref> [105, 106] </ref>, Flanagan and Felleisen [37], and Moreau [83, 84, 85] provided cost models of speculative evaluation. Roe tracks only the depth of the computation, whereas Flanagan and Felleisen and Moreau track only the work. <p> The left spine including the root represents the main thread; the other two nodes are separate threads, only one of which synchronizes. Comparisons to similar semantics By using computation graphs as our costs, we have been able to simplify the semantics as compared to those by Roe <ref> [105, 106] </ref> and by the author and Blelloch [47]. Similar to here, they 128 CHAPTER 8. SPECULATIVE MODELS 8.1. LANGUAGE AND PROFILING SEMANTICS 129 included depths in an environment to describe when values had been computed. <p> The PAL and NESL models also define the amount of maximum reachable space during evaluation. The definition of the PSL model is simpler than Roe's similar model, while more useful for describing parallelism. The PSL defines computation graphs, and thus the amount of parallelism available, unlike those of Roe <ref> [105, 106] </ref>, Flanagan and Felleisen [37], and Moreau [84, 85], which only define the work of a computation. Similarly, the computation graphs in the NESL model are a more appropriate measure of parallelism than the maximum number of useful processors as defined by Zimmermann [128].
Reference: [107] <author> John R. Rose and Guy L. Steele Jr. </author> <title> C*: An extended C language for data parallel programming. </title> <booktitle> In Proceedings 2nd International Conference on Supercomputing, </booktitle> <volume> volume 2, </volume> <pages> pages 2-16, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: This forms the core of languages such as Nesl [14], HPF, and C* <ref> [107] </ref>. NESL provides a very flexible model of data-parallelism, where e 00 may be any general expression. In particular we allow nested data-parallelism, i.e., forked threads can fork additional threads (as in Nesl, but not HPF and C*).
Reference: [108] <author> Mads Rosendahl. </author> <title> Automatic complexity analysis. </title> <booktitle> In Proceedings 4th International Conference on Functional Programming Languages and Computer Architecture. </booktitle> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1989. </year>
Reference-contexts: On the other hand, an intensional semantics defines an abstract model of how a computation is performed, such as how long a computation takes or the resources needed during a computation. An intensional semantics that tracks run-time cost information is called a profiling semantics <ref> [108, 110] </ref>. A semantics can also be considered a simple abstract implementation of a language. This is especially true for operational styles of semantics, which are of primary interest here. These 15 16 CHAPTER 1. INTRODUCTION simple implementations do not necessarily embody the intensional properties expected of a realistic implementation. <p> Chapter 5 Profiling semantics Recall that most semantics define only the extensional properties of a language: a program's results and termination properties. A profiling semantics augments such an extensional semantics with definitions of the intensional information, the costs of evaluating the expression <ref> [108, 110] </ref>. In particular we are interested in the time and space costs of evaluation, where parallel time is modeled by work and depth, or more generally, computation graphs. We add these costs to operational semantics in a natural deduction, "big-step", operational style of semantics.
Reference: [109] <author> David Sands. </author> <title> Complexity analysis for a lazy higher-order language. </title> <booktitle> In Proceedings Functional Programming, Glasgow 1989, Workshops in Computing Series. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1989. </year>
Reference: [110] <author> David Sands. </author> <title> Calculi for Time Analysis of Functional Programs. </title> <type> PhD thesis, </type> <institution> University of London, Imperial College, </institution> <month> September </month> <year> 1990. </year> <note> 226 BIBLIOGRAPHY </note>
Reference-contexts: On the other hand, an intensional semantics defines an abstract model of how a computation is performed, such as how long a computation takes or the resources needed during a computation. An intensional semantics that tracks run-time cost information is called a profiling semantics <ref> [108, 110] </ref>. A semantics can also be considered a simple abstract implementation of a language. This is especially true for operational styles of semantics, which are of primary interest here. These 15 16 CHAPTER 1. INTRODUCTION simple implementations do not necessarily embody the intensional properties expected of a realistic implementation. <p> Chapter 5 Profiling semantics Recall that most semantics define only the extensional properties of a language: a program's results and termination properties. A profiling semantics augments such an extensional semantics with definitions of the intensional information, the costs of evaluating the expression <ref> [108, 110] </ref>. In particular we are interested in the time and space costs of evaluation, where parallel time is modeled by work and depth, or more generally, computation graphs. We add these costs to operational semantics in a natural deduction, "big-step", operational style of semantics.
Reference: [111] <author> David Sands. </author> <title> Time analysis, cost equivalence and program refinement. </title> <booktitle> In Proceedings 11th Conference on Foundations of Software Technology and Theoretical Computer Science, Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <month> December </month> <year> 1991. </year>
Reference: [112] <author> Patrick M. Sansom. </author> <title> Execution Profiling for Non-strict Functional Languages. </title> <type> PhD thesis, </type> <institution> Department of Computing Science, University of Glasgow, </institution> <year> 1994. </year>
Reference-contexts: Any profiler requires at least an informal definition of a language's costs, but frequently these definitions are ad hoc or special purpose. Some recent profiling tools have been based on the language semantics and a more formal notion of the costs <ref> [66, 65, 112, 113] </ref>. Since pro-filers generally need to produce highly accurate resource profiles, they require more detailed semantics than the abstract semantics provided here. But our framework allows detailed semantics and provides a formalism which could be used as the basis for these semantics-based profilers. 36 CHAPTER 2.
Reference: [113] <author> Patrick M. Sansom and Simon L Peyton Jones. </author> <title> Time and space profiling for non-strict, higher-order functional languages. </title> <booktitle> In ACM Symposium on Principles of Programming Languages, </booktitle> <year> 1995. </year>
Reference-contexts: Any profiler requires at least an informal definition of a language's costs, but frequently these definitions are ad hoc or special purpose. Some recent profiling tools have been based on the language semantics and a more formal notion of the costs <ref> [66, 65, 112, 113] </ref>. Since pro-filers generally need to produce highly accurate resource profiles, they require more detailed semantics than the abstract semantics provided here. But our framework allows detailed semantics and provides a formalism which could be used as the basis for these semantics-based profilers. 36 CHAPTER 2.
Reference: [114] <author> Helmut Seidl and Reinhard Wilhelm. </author> <title> Probabilistic load balancing for parallel graph reduction. </title> <booktitle> In Proceedings TENCON '89, 4th IEEE Region 10 International Conference, </booktitle> <pages> pages 879-884, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Jones [60] related the time-augmented semantics of simple while-loop language to that of an equivalent machine language in order to study the effect of constant factors in time complexity. Seidl and Wilhelm <ref> [114] </ref> provide complexity bounds for an implementation of graph reduction on the PRAM. However, their implementation only considers a single step and requires that you know which graph nodes to execute in parallel in that step and that the graph has constant in-degree.
Reference: [115] <author> Jon Shultis. </author> <title> On the complexity of higher-order programs. </title> <type> Technical Report CU-CS-288-85, </type> <institution> University of Colorado, Boulder, </institution> <month> January </month> <year> 1985. </year>
Reference: [116] <author> David B. Skillicorn. </author> <title> The Bird-Meertens formalism as a parallel model. </title> <booktitle> In Proceedings Software for Parallel Computation, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: Note that one general technique used with skeletons is shape analysis which is a generalization of the size abstraction in automatic complexity analysis. Also, the work of Skillicorn, et al. overlaps with that of automatic complexity analysis <ref> [25, 116, 117] </ref>. In addition, their work also overlaps greatly with that in algorithm analysis using high-level functional data parallel languages, which also includes work using Nesl, e.g., [11, 10, 46], and other work in the Bird-Meertens formalism.
Reference: [117] <author> David B. Skillicorn and W. Cai. </author> <title> A cost calculus for parallel functional programming. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 28(1) </volume> <pages> 65-83, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: Note that one general technique used with skeletons is shape analysis which is a generalization of the size abstraction in automatic complexity analysis. Also, the work of Skillicorn, et al. overlaps with that of automatic complexity analysis <ref> [25, 116, 117] </ref>. In addition, their work also overlaps greatly with that in algorithm analysis using high-level functional data parallel languages, which also includes work using Nesl, e.g., [11, 10, 46], and other work in the Bird-Meertens formalism.
Reference: [118] <author> Dan Suciu and Val Tannen. </author> <title> Efficient compilation of high-level data parallel algorithms. </title> <booktitle> In Proceedings 6th ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 57-66, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: This perspective results in some novel results, including a theoretical equivalent of the intuition that constant factors in performance really do matter in what can be computed. Skeletons are parameterized complexity functions obtained using traditional algorithm analysis techniques, e.g., for a general-purpose divide-and-conquer algorithm <ref> [118] </ref>. Work in this area also uses functional languages for simplicity. We use the same basic idea to parameterize our bounds with respect to the load-balancing and latency costs in various machine models.
Reference: [119] <author> Carolyn Talcott. Rum: </author> <title> An intensional theory of function and control abstractions. </title> <booktitle> In Proceedings Workshop on Foundations of Logic and Functional Programming. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: While powerful, his framework does not correspond to typical programmers' intuition because of its use of category theory. And while very intriguing, the generalization to complexities is only partially successful. Also, Talcott provided an intensional theory similar to those used for automatic cost analysis <ref> [119] </ref>. Jones, e.g., [60] has been exploring traditional complexity theory from a programming language perspective. This includes re-examining how certain complexity classes arise from different language idioms.
Reference: [120] <author> Kenneth R. Traub. </author> <title> Sequential Implementation of Lenient Programming Languages. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> October </month> <year> 1988. </year>
Reference-contexts: The latter offers a wide spectrum of implementations, depending on which computations it aborts and when. Speculative evaluation is most closely related to the futures of Multilisp and its many descendant languages [50, 51, 26, 31, 79, 69, 58, 125], the lenient evaluation of Id and pH <ref> [120, 87, 88] </ref>, and parallel graph reduction, e.g., [55, 96, 61]. * In Multilisp, any expression can be designated as a future that spawns a thread which may be executed in parallel. <p> SPECULATIVE MODELS * Full speculation and leniency are essentially the same thing, although the term "leniency" originally implied a specific lack of evaluation ordering <ref> [120] </ref>. Id and pH evaluate all subexpressions fully because they may contain side-effects, although a compiler might optimize cases when this is not necessary. * Graph reduction is one technique for implementing lazy (call-by-need) functional languages.
Reference: [121] <author> Guy Tremblay and G. R. Gao. </author> <title> The impact of laziness on parallelism and the limits of strictness analysis. </title> <editor> In A. P. Wim Bohm and John T. Feo, editors, </editor> <booktitle> Proceedings High Performance Functional Computing, </booktitle> <pages> pages 119-133, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: We show that in quicksort, for example, NESL allows more efficient data access and thus more efficient algorithms than the PAL or PSL models. We do not use any call-by-name or call-by-need (lazy) languages, because they inherently do not offer significant parallelism <ref> [64, 121] </ref>. <p> Id and pH evaluate all subexpressions fully because they may contain side-effects, although a compiler might optimize cases when this is not necessary. * Graph reduction is one technique for implementing lazy (call-by-need) functional languages. But since lazy evaluation entails an inherent lack of parallelism <ref> [64, 121] </ref>, parallel versions of these languages have incorporated partial speculation, compromising on the laziness of the language. The PSL model is "speculative" in two senses.
Reference: [122] <author> L. G. Valiant. </author> <title> A scheme for fast parallel communication. </title> <journal> SIAM Journal of Computing, </journal> <volume> 11(2) </volume> <pages> 350-361, </pages> <month> May </month> <year> 1982. </year>
Reference-contexts: Our simulation uses the scan and reduce operations (cf. Appendix A), and our cost bounds are parameterized by their cost. We denote this time overhead T S (p), as shown in congestion <ref> [122] </ref>, and thus those bounds hold with high probability (w.h.p., for short). We show that each step of a P-CEK p log p PAL machine can be implemented in O (T S (p)) amortized time. These bounds hold with high probability (w.h.p.) on the randomized machines.
Reference: [123] <author> L. G. Valiant. </author> <title> General Purpose Parallel Architectures, volume A, </title> <booktitle> chapter 18, </booktitle> <pages> pages 943-972. </pages> <publisher> Elsevier Science Publishers, </publisher> <year> 1990. </year> <note> BIBLIOGRAPHY 227 </note>
Reference-contexts: But the PRAM is commonly used to describe algorithms so that computation issues are not obscured by communication issues|a common problem in more realistic models. These simpler PRAM algorithms can then be mapped to other models using standard implementation techniques <ref> [101, 123, 72] </ref>. We discuss several variants of the PRAM: primarily the concurrent-read concurrent-write (CRCW), but also the exclusive-read exclusive-write (EREW) and concurrent-read exclusive-write (CREW), which differ in what memory accesses are allowed, as their names imply.
Reference: [124] <author> Ben Wegbreit. </author> <title> Mechanical program analysis. </title> <journal> Communications of the ACM, </journal> <volume> 18(9), </volume> <month> September </month> <year> 1975. </year>
Reference-contexts: Thus, a logical step would be to use these models as the core of automatic complexity analysis tools such as Metric, ACE, COMPLEXA, and fl <ref> [124, 77, 35, 126] </ref>, Kishon's profiling tool [66, 65], or compiler analyses. 216 CHAPTER 11. CONCLUSIONS
Reference: [125] <author> C. K. Yuen, M. D. Feng, and J. J. Yee. </author> <title> Speculative parallelism in BaLinda Lisp. </title> <type> Technical Report TR31/92, </type> <institution> Department of Information Systems and Computer Science, National University of Singapore, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: The latter offers a wide spectrum of implementations, depending on which computations it aborts and when. Speculative evaluation is most closely related to the futures of Multilisp and its many descendant languages <ref> [50, 51, 26, 31, 79, 69, 58, 125] </ref>, the lenient evaluation of Id and pH [120, 87, 88], and parallel graph reduction, e.g., [55, 96, 61]. * In Multilisp, any expression can be designated as a future that spawns a thread which may be executed in parallel.
Reference: [126] <author> Paul Zimmermann and Wolf Zimmermann. </author> <title> The automatic complexity analysis of divide-and-conquer algorithms. </title> <type> Technical Report 1149, </type> <institution> Institut National de Recherche en Informatique et en Automatique, Rocquencourt, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: Thus, a logical step would be to use these models as the core of automatic complexity analysis tools such as Metric, ACE, COMPLEXA, and fl <ref> [124, 77, 35, 126] </ref>, Kishon's profiling tool [66, 65], or compiler analyses. 216 CHAPTER 11. CONCLUSIONS
Reference: [127] <author> Paul Zimmermann and Wolf Zimmermann. </author> <title> The automatic complexity analysis of divide-and-conquer algorithms. </title> <journal> Computer and Information Sciences VI, </journal> <volume> 1 </volume> <pages> 395-404, </pages> <month> November </month> <year> 1991. </year>
Reference: [128] <author> Wolf Zimmermann. </author> <title> Automatic worst case complexity analysis of parallel programs. </title> <type> Technical Report TR-90-066, </type> <institution> International Computer Science Institute, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Also he did not give a formal cost mapping 31 32 CHAPTER 2. RELATED WORK for Nesl's implementation, although he did outline the costs of its mapping to the VRAM model. Zimmerman <ref> [128, 130] </ref> introduced a profiling semantics for a data-parallel language for the purpose of automatically analyzing PRAM algorithms. The language therefore almost directly modeled the PRAM by adding a set of PRAM-like primitive operations. Complexity was measured in terms of time and number of processors, as measured for the PRAM. <p> Similarly, the computation graphs in the NESL model are a more appropriate measure of parallelism than the maximum number of useful processors as defined by Zimmermann <ref> [128] </ref>. And this definition is formal, unlike that specified for NESL [14]. For each of these language models, we give a formal implementation onto the hypercube, butterfly, and PRAM machine models, and obtained the induced cost mapping.
Reference: [129] <author> Wolf Zimmermann. </author> <title> The automatic worst case analysis of parallel programs: Simple parallel sorting and algorithms on graphs. </title> <type> Technical Report TR-91-045, </type> <institution> International Computer Science Institute, </institution> <month> August </month> <year> 1991. </year>
Reference: [130] <author> Wolf Zimmermann. </author> <title> Complexity issues in the design of functional languages with explicit parallelism. </title> <booktitle> In Proceedings International Conference on Computer Languages, </booktitle> <pages> pages 34-43, </pages> <month> April </month> <year> 1992. </year> <note> 228 BIBLIOGRAPHY </note>
Reference-contexts: Also he did not give a formal cost mapping 31 32 CHAPTER 2. RELATED WORK for Nesl's implementation, although he did outline the costs of its mapping to the VRAM model. Zimmerman <ref> [128, 130] </ref> introduced a profiling semantics for a data-parallel language for the purpose of automatically analyzing PRAM algorithms. The language therefore almost directly modeled the PRAM by adding a set of PRAM-like primitive operations. Complexity was measured in terms of time and number of processors, as measured for the PRAM. <p> However, there has also been some work on worst-case analysis of for a PRAM-like parallel language <ref> [130] </ref>, defining the depth of the computation and the maximum number of processes that the computation can employ. Also there has been some work on worst-case analysis of imperative languages [28] and average-case analysis of serial functional languages [36].
References-found: 130

