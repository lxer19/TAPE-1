URL: http://www.cs.sunysb.edu/~mitra/text.ps
Refering-URL: http://www.cs.sunysb.edu/~mitra/
Root-URL: http://www.cs.sunysb.edu
Email: mitra,ckyang@cs.sunysb.edu  
Title: File System Extensions for Application Specific Disk Prefetching  
Author: Tulika Mitra and Chuan-Kai Yang 
Address: NY 11794-4400  
Affiliation: Computer Science Department State University of New York at Stony Brook Stony Brook,  
Abstract: In recent year, I/O performance has become a critical bottleneck to overall system performance due to increasing performance gap between CPU speed and disk latency. Even though I/O bandwidth and write performance have improved considerably due to large disk arrays [8], and buffered write back as well as log-structured file systems [15], , read latency for serial applications is still a serious problem. Many important applications consist of computation interleaved with synchronous I/O where the disk access pattern is completely unpredictable from the file system point of view. However, the application programmer can provide important hints regarding future disk accesses. In this work, we implement a file system extension so that the application programmer can provide the disk access pattern in the form of a C function to the system. The function calls used in this function have the same syntax as read/write system calls, except for a different name, for example, future read/future write etc. The function is essentially the application program without the computation part. Our prefetcher developed in Linux operating system kernel along with an user level prefetch library, uses this function to intelligently perform prefetching and caching. Our prototype implementation is fully operational and gives upto 54% performance improvement for real application. We also show that the performance of our implemetation is very close to the optimal perfomance under this framework. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Cao et al. </author> <title> Application Controlled File Caching Policies. </title> <booktitle> In USENIX summer 1994 Technical Conference, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: Patterson et. al. implemented their scheme in Digital's OSF/1 operating system [13], and obtained 20-83% reduction in execution time for different workloads including text search, scientific visualization, database queries etc. 2.3. Application Controlled Caching While Patterson et. al. were performing application controlled prefetching, Cao et. al. <ref> [1, 2] </ref> were investigating the effect of application hints on file system caching. Traditionally, filesystem cache implements LRU replacement policy. Not all applications are amenable to this policy. In that case, the application typically implements its own user level cache. This approach has two problems.
Reference: [2] <author> P. Cao et al. </author> <title> Implementation and Performance of Application-Controlled File Caching. </title> <booktitle> In First USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: Patterson et. al. implemented their scheme in Digital's OSF/1 operating system [13], and obtained 20-83% reduction in execution time for different workloads including text search, scientific visualization, database queries etc. 2.3. Application Controlled Caching While Patterson et. al. were performing application controlled prefetching, Cao et. al. <ref> [1, 2] </ref> were investigating the effect of application hints on file system caching. Traditionally, filesystem cache implements LRU replacement policy. Not all applications are amenable to this policy. In that case, the application typically implements its own user level cache. This approach has two problems.
Reference: [3] <author> P. Cao et al. </author> <title> A Study of Integrated Prefetching and Caching Strategies. </title> <booktitle> In ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: Prefetching and caching are complimentary to each other, and performing aggressive prefetching while using the same old LRU cache replacement policy might result in performance degradation. Similarly, just innovative caching cannot improve the performance of applications. Recently, Cao et. al. <ref> [3] </ref> first proposed an integrated prefetching and caching scheme. They first developed a theoretical framework that proposed four properties of optimal prefetching and caching, and based on those properties implemented two strategies: aggressive and conservative.
Reference: [4] <author> K. Curewitz et al. </author> <title> Practical Prefetching via Data Compression. </title> <booktitle> In ACM Conference on Management of Data, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Kotz, for example [5], have looked at detecting more complex non-sequential access patterns within a file. More recently, Vitter at. al [17] applied a form of data compression philosophy to develop a theoretical optimal prefetcher and later showed <ref> [4] </ref> that it works well in practice as well.The primary advantage of this type of prefetching is that it is completely transparent to the application programmer.
Reference: [5] <author> D. Kotz et al. </author> <title> Practical Prefetching Techniques for Parallel File Systems. </title> <booktitle> In First International Conference on Parallel and Distributed Information Systems, </booktitle> <month> December </month> <year> 1991. </year>
Reference-contexts: Recently however, the interest has shifted to speculative prefetching that tries to predict the access pattern from past history. Kotz, for example <ref> [5] </ref>, have looked at detecting more complex non-sequential access patterns within a file.
Reference: [6] <author> M. K. McKusick et al. </author> <title> A Fast File System for UNIX. </title> <journal> ACM Transaction on Computer Sytems, </journal> <volume> 2(3), </volume> <month> August </month> <year> 1984. </year>
Reference-contexts: We discuss all these methods in detail in the next subsections. 2.1. Predictive Prefetching For a long time, the most common predictive prefetching was sequential readahead. This readahead can vary from very conservative one block prefetch in BSD UNIX <ref> [6] </ref> to quite aggressive strategy of prefetching upto 64, 8 KB blocks in Digital's OSF 1 operating system. Recently however, the interest has shifted to speculative prefetching that tries to predict the access pattern from past history.
Reference: [7] <author> T. C. Mowry et al. </author> <title> Automatic Compiler-Inserted I/O Prefetching for Oout-of-Core Applications. </title> <booktitle> In 2nd USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: They obtained 17-69% reduction in execution time that shows the scheme can be extended for network file system quite easily. 2.5. Compiler directed I/O To reduce the burden on the application programmer, Mowry et al. <ref> [7] </ref>, suggested some compiler optimizations, where the compiler can analysis a program and insert the prefetch requests when the operating system supports this kind of prefetching and caching. This fully automatic scheme further enhances the feasibility of application controlled prefetching and caching. 3.
Reference: [8] <author> D. Patterson et al. </author> <title> A Case for Redundant Array of Inexpensive Disks (RAID). </title> <booktitle> In ACM Conference on Management of Data, </booktitle> <month> June </month> <year> 1988. </year>
Reference: [9] <author> R. H. Patterson. </author> <title> Using Transparent Informed Prefetching to Reduce File System Latency. </title> <booktitle> In Goddard Conference on Mass Storage Systems and Technologies, </booktitle> <month> September </month> <year> 1992. </year>
Reference-contexts: This is the main idea behind Informed prefetching where the programmer discloses its upcoming access pattern through a well-defined interface to the kernel [10]. Patterson et. al. <ref> [9, 11] </ref> modified two UNIX applications "make" and "grep" to provide hints to the filesystem regarding the files the applications are going to touch (implemented by forking off another process that just access all the file going to be accessed by the original process a little ahead of time) and their
Reference: [10] <author> R. H. Patterson. </author> <title> Informed Prefetching and Caching. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> December </month> <year> 1997. </year>
Reference-contexts: This is the main idea behind Informed prefetching where the programmer discloses its upcoming access pattern through a well-defined interface to the kernel <ref> [10] </ref>.
Reference: [11] <author> R. H. Patterson et al. </author> <title> A Status Report on Research in Transparent Informed Prefetching. </title> <journal> Operating Systems Review, </journal> <volume> 27(2), </volume> <month> April </month> <year> 1993. </year>
Reference-contexts: This is the main idea behind Informed prefetching where the programmer discloses its upcoming access pattern through a well-defined interface to the kernel [10]. Patterson et. al. <ref> [9, 11] </ref> modified two UNIX applications "make" and "grep" to provide hints to the filesystem regarding the files the applications are going to touch (implemented by forking off another process that just access all the file going to be accessed by the original process a little ahead of time) and their
Reference: [12] <author> R. H. Patterson et al. </author> <title> Exposing I/O Concurrency with Informed Prefetching. </title> <booktitle> In 3rd International Conference on Parallel and Distributed Information Systems, </booktitle> <month> September </month> <year> 1994. </year>
Reference-contexts: In <ref> [12] </ref>, they modified a volume visualization application to provide hints and obtained reduction in execution time by factor of 1.9 to 3.7. They coined the term "disclosure", which are hints in the same abstraction level as system calls.
Reference: [13] <author> R. H. Patterson et al. </author> <title> Informed Prefetching and Caching. </title> <booktitle> In 15th ACM Symposium on Operating System Principle, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: Can use these hints to perform superior cache management that LRU 3. Since multiple request are waiting in the disk driver queue, better disk scheduling can be performed to reduce seek latency. Patterson et. al. implemented their scheme in Digital's OSF/1 operating system <ref> [13] </ref>, and obtained 20-83% reduction in execution time for different workloads including text search, scientific visualization, database queries etc. 2.3. Application Controlled Caching While Patterson et. al. were performing application controlled prefetching, Cao et. al. [1, 2] were investigating the effect of application hints on file system caching. <p> They showed that the performance of both are within a factor of two of the optimal strategy and their trace driven simulation showed execution time reduction of upto 50%. Patterson et. al. <ref> [13] </ref>, came up with a different strategy for prefetching and managing cache. Their scheme is based on the hints supplied by the user and allocates buffers among three consumers: prefetching hinted blocks, caching hinted blocks for reuse, and caching used blocks for unhinted access using cost-benefit analysis.
Reference: [14] <author> D. Rochberg et al. </author> <title> Perfetching Over a Network: Early Experience with CTIP. SIGMETRIC Performance Evaluation Review, </title> <type> 25(3). </type>
Reference-contexts: Cao et. al. proposed two-level buffer allocation whereas Patterson et. al. has a unified global buffer pool Joint work by both the groups recently found an adaptive approach that incorporates best of both the schemes~citeKimbrel96, Tomkins and performs better. Finally,Rochberg et. al. <ref> [14] </ref> implemented a network filesystem extension of the Informed prefetching that used modified NFS clients to disclose the application's future disk accesses. They obtained 17-69% reduction in execution time that shows the scheme can be extended for network file system quite easily. 2.5.
Reference: [15] <author> M. Rosenblum et al. </author> <title> The Design and Implementation of Log-Structured File System. </title> <booktitle> In 13th ACM Symposium on Operating Systems Principle, </booktitle> <month> October </month> <year> 1991. </year>
Reference: [16] <author> A. Tomkins et al. </author> <title> Informed Multi-Process Prefetching and Caching. </title> <booktitle> In ACM International Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Their scheme is based on the hints supplied by the user and allocates buffers among three consumers: prefetching hinted blocks, caching hinted blocks for reuse, and caching used blocks for unhinted access using cost-benefit analysis. Later, they extended their scheme <ref> [16] </ref> for multiple processes where the kernel allocates resources among multiple competing processes as well as demands of individual process. The main difference between Cao and Patterson's schemes are the following: 1.
Reference: [17] <author> J. S. Vitter et al. </author> <title> Optimal Prefetching via Data Compression. </title> <booktitle> In 32nd Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: Recently however, the interest has shifted to speculative prefetching that tries to predict the access pattern from past history. Kotz, for example [5], have looked at detecting more complex non-sequential access patterns within a file. More recently, Vitter at. al <ref> [17] </ref> applied a form of data compression philosophy to develop a theoretical optimal prefetcher and later showed [4] that it works well in practice as well.The primary advantage of this type of prefetching is that it is completely transparent to the application programmer.
References-found: 17

