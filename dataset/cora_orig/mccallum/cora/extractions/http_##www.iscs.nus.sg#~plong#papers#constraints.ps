URL: http://www.iscs.nus.sg/~plong/papers/constraints.ps
Refering-URL: 
Root-URL: 
Email: nickl@research.nj.nec.com  plong@igi.tu-graz.ac.at  
Title: On-line Learning with Linear Loss Constraints  
Author: Nicholas Littlestone Philip M. Long 
Address: 4 Independence Way Princeton, NJ 08540  Klosterwiesgasse 32/2 A-8010 Graz, Austria  
Affiliation: NEC Research Institute  Institute for Theoretical Computer Science Technische Universitaet Graz  
Abstract: We consider a generalization of the mistake-bound model (for learning f0; 1g-valued functions) in which the learner must satisfy a general constraint on the number M + of incorrect 1 predictions and the number M of incorrect 0 predictions. We describe a general-purpose optimal algorithm for our formulation of this problem. We describe several applications of our general results, involving situations in which the learner wishes to satisfy linear inequalities in M + and M .
Abstract-found: 1
Intro-found: 1
Reference: [Ang88] <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference: [BF72] <author> J. M. Barzdin and R. V. Freivald. </author> <title> On the prediction of general recursive functions. </title> <journal> Soviet Mathematics-Doklady, </journal> <volume> 13 </volume> <pages> 1224-1228, </pages> <year> 1972. </year>
Reference: [GRS89] <author> S.A. Goldman, R.L. Rivest, and R.E. Schapire. </author> <title> Learning binary relations and total orders. </title> <booktitle> Proceedings of the 30th Annual Symposium on the Foundations of Computer Science, </booktitle> <year> 1989. </year>
Reference-contexts: Results of this type are in the same spirit as the fundamental "Halving algorithm," [BF72,Ang88,Lit88] which has served as a useful tool in analyzing target classes in the standard mistake-bound model (c.f., <ref> [GRS89] </ref> [Lit88] [Lit89] [MT89] [MT90]). SCS is roughly a generalization of the Standard Optimal Algorithm (SOA) of [Lit88] and CCS roughly generalizes the Halving algorithm.
Reference: [HLL92] <author> D.P. Helmbold, N. Littlestone, </author> <title> and P.M. Long. Apple tasting and nearly one-sided learning. </title> <booktitle> Proceedings of the 33rd Annual Symposium on the Foundations of Computer Science, </booktitle> <year> 1992. </year>
Reference-contexts: Taking this point of view allows us to unify our treatment of all of these cases. One application of these algorithms is to the learning model discussed in <ref> [HLL92] </ref>, called there the apple-tasting model. The apple-tasting model deals with situations, such as visually identifying tasty apples, where the value of the hidden function f is obtained only when the learner (effectively) predicts 1 (e.g. bites into the apple). As discussed in [HLL92], algorithms which "trade" effectively between false positive <p> is to the learning model discussed in <ref> [HLL92] </ref>, called there the apple-tasting model. The apple-tasting model deals with situations, such as visually identifying tasty apples, where the value of the hidden function f is obtained only when the learner (effectively) predicts 1 (e.g. bites into the apple). As discussed in [HLL92], algorithms which "trade" effectively between false positive and false negative mistakes are useful as subroutines for apple-tasting learning algorithms. Theorems 15 and 16 in the present paper allow us to tighten certain of the upper and lower bounds obtained in [HLL92]. <p> As discussed in <ref> [HLL92] </ref>, algorithms which "trade" effectively between false positive and false negative mistakes are useful as subroutines for apple-tasting learning algorithms. Theorems 15 and 16 in the present paper allow us to tighten certain of the upper and lower bounds obtained in [HLL92]. <p> We also consider cases where there are two linear constraints. The most basic case is one where there are separate constraints on the numbers of false positive and false negative mistakes. Helmbold, Littlestone, and Long have shown <ref> [HLL92] </ref> that the learner can satisfy the pair of constraints M + k and M l for all classes of size n if and only if k+l+2 &gt; n. Using the tools of this paper, we obtain a simpler proof of this result. <p> The following says that this algorithm is in fact optimal. It further establishes the fact that if EG (C; F ) is false then any algorithm can be forced to make a mistake on every trial until that constraint is violated. This proved useful in <ref> [HLL92] </ref>. <p> This has the solution for v 1 ; v 2 1 of LF G (v 1 ; v 2 ) = bv 2 c+1 . This result was previously obtained using an ad hoc argument in <ref> [HLL92] </ref>. 3.4 Bounds on the number of false negative mistakes and the total number of mistakes Here G = h 0 1 . <p> ); F ) =) SAT (SCS (LIN ( 1 v 2 ) (2); F ); RECT (2v 1 ; 2v 2 ); F ): 4 Apple tasting In this section, we discuss applications of the techniques of this paper to problems in another model of learning, called the apple-tasting model <ref> [HLL92] </ref>. In this model, a response is given to the learner only on trials in which the algorithm predicts 1. <p> Algorithms are allowed to be randomized, and bounds on the expected number of mistakes are given in terms of the total number of trials experienced by the learner. 4.1 Apple tasting definitions These definitions are modeled after those of <ref> [HLL92] </ref>. Learning in the apple tasting model proceeds in a man ner similar to the on-line learning process described at the beginning of the present paper. Again we assume there is a target function f chosen from a class of f0; 1g-valued functions over some domain X. <p> The proof, which uses Theorem 16 and Lemma 17, together with results from <ref> [HLL92] </ref>, is omitted. Theorem 20 For n 1, ALC (n; T ) min T =2; (n1); 8 T ln n ) Theorem 20 improves on the previously known bound [HLL92] of ( 2 ln n r ln T : As an example, if T = fi (log 2 n), the old <p> The proof, which uses Theorem 16 and Lemma 17, together with results from <ref> [HLL92] </ref>, is omitted. Theorem 20 For n 1, ALC (n; T ) min T =2; (n1); 8 T ln n ) Theorem 20 improves on the previously known bound [HLL92] of ( 2 ln n r ln T : As an example, if T = fi (log 2 n), the old upper bound is fi ((log 3=2 n)(log log n) 1=2 ), whereas Theorem 20 yields an O ((log 3=2 n)=(log log n) 1=2 ) bound, an improvement of a <p> Theorem 21 ALC (n; T ) min T ; 2 2 1 s ln (1 + T ) This closes a gap between the best previously known upper and lower bounds <ref> [HLL92] </ref>. We may also directly apply Theorem 20 to obtain improved upper bounds on the apple-tasting learning complexity of conjunctions of boolean variables. 5 Acknowledgements We thank David Helmbold for numerous important insightful comments about this work.
Reference: [Lit88] <author> N. Littlestone. </author> <title> Learning quickly when irrelevant attributes abound: a new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: Results of this type are in the same spirit as the fundamental "Halving algorithm," [BF72,Ang88,Lit88] which has served as a useful tool in analyzing target classes in the standard mistake-bound model (c.f., [GRS89] <ref> [Lit88] </ref> [Lit89] [MT89] [MT90]). SCS is roughly a generalization of the Standard Optimal Algorithm (SOA) of [Lit88] and CCS roughly generalizes the Halving algorithm. <p> Results of this type are in the same spirit as the fundamental "Halving algorithm," [BF72,Ang88,Lit88] which has served as a useful tool in analyzing target classes in the standard mistake-bound model (c.f., [GRS89] <ref> [Lit88] </ref> [Lit89] [MT89] [MT90]). SCS is roughly a generalization of the Standard Optimal Algorithm (SOA) of [Lit88] and CCS roughly generalizes the Halving algorithm. <p> We show that log ff n maxfa; bg &lt; v log ff n, where ff is the solution to ff a + ff b = 1. Note that in the special case of the mistake bound model, where a = b = 1, the halving result <ref> [Lit88] </ref> follows as a special case, since in that case, we have bounded v in the range (log 2 n 1; log 2 n], which contains only one integer: blog 2 nc.
Reference: [Lit89] <author> N. Littlestone. </author> <title> Mistake Bounds and Logarithmic Linear-threshold Learning Algorithms. </title> <type> PhD thesis, </type> <institution> UC Santa Cruz, </institution> <year> 1989. </year>
Reference-contexts: Results of this type are in the same spirit as the fundamental "Halving algorithm," [BF72,Ang88,Lit88] which has served as a useful tool in analyzing target classes in the standard mistake-bound model (c.f., [GRS89] [Lit88] <ref> [Lit89] </ref> [MT89] [MT90]). SCS is roughly a generalization of the Standard Optimal Algorithm (SOA) of [Lit88] and CCS roughly generalizes the Halving algorithm.
Reference: [MT89] <author> W. Maass and G. Turan. </author> <title> On the complexity of learning from counterexamples. </title> <booktitle> Proceedings of the 30th Annual Symposium on the Foundations of Computer Science, </booktitle> <year> 1989. </year>
Reference-contexts: Results of this type are in the same spirit as the fundamental "Halving algorithm," [BF72,Ang88,Lit88] which has served as a useful tool in analyzing target classes in the standard mistake-bound model (c.f., [GRS89] [Lit88] [Lit89] <ref> [MT89] </ref> [MT90]). SCS is roughly a generalization of the Standard Optimal Algorithm (SOA) of [Lit88] and CCS roughly generalizes the Halving algorithm.
Reference: [MT90] <author> W. Maass and G. Turan. </author> <title> On the complexity of learning from counterexamples and membership queries. </title> <booktitle> Proceedings of the 31st Annual Symposium on the Foundations of Computer Science, </booktitle> <year> 1990. </year>
Reference-contexts: Results of this type are in the same spirit as the fundamental "Halving algorithm," [BF72,Ang88,Lit88] which has served as a useful tool in analyzing target classes in the standard mistake-bound model (c.f., [GRS89] [Lit88] [Lit89] [MT89] <ref> [MT90] </ref>). SCS is roughly a generalization of the Standard Optimal Algorithm (SOA) of [Lit88] and CCS roughly generalizes the Halving algorithm.
Reference: [Vov90] <author> V. Vovk. </author> <title> Aggregating strategies. </title> <booktitle> In Proceedings of the 3nd Workshop on Computational Learning Theory, </booktitle> <pages> pages 371-383. </pages> <publisher> Mor-gan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: The upper bound of log ff n also follows from a special case of an algorithm of Vovk <ref> [Vov90] </ref>. His algorithm can be used to approximate CCS. However, there are regions where log ff n is not a close approximation to the true bound, and in those regions Vovk's algorithm can perform substantially worse than CCS. <p> Since Vovk and we have looked at this learning question from rather different perspectives and have made different sorts of generalizations, it should be interesting to make further comparisons between his approach and ours. In <ref> [Vov90] </ref>, Vovk does not discuss the behavior of the parameter ff as a function of a and b. We do not know how to solve explicitly for ff. However, we can make a further approximation that gives a better sense of the behavior of the bounds. <p> This result also follows from the work of Vovk <ref> [Vov90] </ref>. His algorithms depend on a parameter that he restricts to (0; 1). If one lets go to 0 then one obtains from Vovk's results an algorithm that we can show satisfies the hypotheses of Lemma 11 with h (v) = ff v .
References-found: 9

