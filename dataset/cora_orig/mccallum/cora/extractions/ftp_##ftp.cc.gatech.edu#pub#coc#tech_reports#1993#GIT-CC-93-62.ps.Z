URL: ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1993/GIT-CC-93-62.ps.Z
Refering-URL: http://www.cs.gatech.edu/tech_reports/index.93.html
Root-URL: 
Email: e-mail: rama@cc.gatech.edu  
Phone: Phone: (404) 894-5136 FAX: (404) 894-9442  
Title: An Approach to Scalability Study of Shared Memory Parallel Systems  
Author: Anand Sivasubramaniam Aman Singla Umakishore Ramachandran H. Venkateswaran 
Keyword: Key words: Scalability, Shared Memory Multiprocessors, Execution-driven Simulation, Application-driven Studies, Latency, Contention  
Note: This work has been funded in part by NSF grants MIPS-9058430 and MIPS-9200005, and an equipment grant from DEC.  
Address: Atlanta, Ga 30332-0280  
Affiliation: College of Computing Georgia Institute of Technology  
Abstract: Technical Report GIT-CC-93/62 October 1993 Abstract The overheads in a parallel system that limit its scalability need to be identified and separated in order to enable parallel algorithm design and the development of parallel machines. Such overheads may be broadly classified into two components. The first one is intrinsic to the algorithm and arises due to factors such as the work-imbalance and the serial fraction. The second one is due to the interaction between the algorithm and the architecture and arises due to latency and contention in the network. A top-down approach to scalability study of shared memory parallel systems is proposed in this research. We define the notion of overhead functions associated with the different algorithmic and architectural characteristics to quantify the scalability of parallel systems; we develop a method for separating the algorithmic overhead into a serial component and a work-imbalance component; we also develop a method for isolating the overheads due to network latency and contention from the overall execution time of an application; we design and implement an execution-driven simulation platform that incorporates these methods for quantifying the overhead functions; and we use this simulator to study the scalability characteristics of five applications on shared memory platforms with different communication topologies. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal. </author> <title> Limits on Interconnection Network Performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 398-412, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: There have been several performance studies that have addressed issues such as latency, contention and synchronization. The scalability of synchronization primitives supported by the hardware [3, 19], the limits on interconnection network performance <ref> [1, 21] </ref>, and the performance of scheduling policies [33, 16] are examples of such studies undertaken over the years. While such issues are extremely important, it is appropriate to put the impact of these factors into perspective by considering them in the context of overall application performance.
Reference: [2] <author> G. M. </author> <title> Amdahl. Validity of the Single Processor Approach to achieving Large Scale Computing Capabilities. </title> <booktitle> In Proceedings of the AFIPS Spring Joint Computer Conference, </booktitle> <pages> pages 483-485, </pages> <month> April </month> <year> 1967. </year>
Reference-contexts: Several performance metrics such as speedup <ref> [2] </ref>, scaled speedup [12], sizeup [30], experimentally determined serial fraction [14], and isoefficiency function [15] have been proposed for quantifying the scalability of parallel systems. <p> Parallel system overheads may be broadly classified into a purely algorithmic component (algorithmic overhead), and a component arising due to the interaction of the algorithm and the architecture (interaction overhead). The algorithmic overhead is due to the inherent serial part <ref> [2] </ref> and the work-imbalance in the algorithm, and is independent of the architectural characteristics. Work imbalance could result with a differential amount of work done by the executing threads in a parallel phase.
Reference: [3] <author> Thomas E. Anderson. </author> <title> The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: There have been several performance studies that have addressed issues such as latency, contention and synchronization. The scalability of synchronization primitives supported by the hardware <ref> [3, 19] </ref>, the limits on interconnection network performance [1, 21], and the performance of scheduling policies [33, 16] are examples of such studies undertaken over the years. <p> An application programmer may further define sub-modes if necessary. * BARRIER: Mode corresponding to a barrier synchronization operation. * MUTEX: Even though the simulated hardware provides only a test&set operation, mutual exclusion lock (implemented using test-test&set <ref> [3] </ref>) is available as a library function in SPASM. A program enters this mode during lock operations. With this mechanism, we can separate the overheads due to the synchronization operations from the rest of the program execution. * PGM SYNC: Parallel programs may use Signal-Wait semantics for pairwise synchronization. <p> Each processor uses this partial sum in calculating the partial sums for the chunk of global buckets allotted to it (phase 5) which is again a local operation. At the completion of this phase, a processor sets a lock (test-test&set lock <ref> [3] </ref>) for each global bucket, subtracts the value found in the corresponding local bucket, updates the local bucket with this new value in the global bucket, and unlocks the bucket (phase 7).
Reference: [4] <author> D. Bailey et al. </author> <title> The NAS Parallel Benchmarks. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 5(3) </volume> <pages> 63-73, </pages> <year> 1991. </year>
Reference-contexts: Three of them (EP, IS and CG) are from the NAS parallel benchmark suite <ref> [4] </ref>; CHOLESKY is from the SPLASH benchmark suite [25]; and FFT is the well-known Fast Fourier Transform algorithm.
Reference: [5] <author> Eric A. Brewer, Chrysanthos N. Dellarocas, Adrian Colbrook, and William E. Weihl. </author> <title> PROTEUS : A high-performance parallel-architecture simulator. </title> <type> Technical Report MIT-LCS-TR-516, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA 02139, </address> <month> September </month> <year> 1991. </year> <month> 21 </month>
Reference-contexts: Overhead functions can be used to study the growth of system overheads for any of these scaling strategies. In our simulation experiments, we limit ourselves to the constant problem size scaling model. 2.2 SPASM SPASM is an execution-driven simulator written in CSIM. As with other recent simulators <ref> [5, 7, 23] </ref>, the bulk of the instructions in the parallel program is executed at the speed of the native processor (SPARC in this study) and only the instructions that may potentially involve a network access are simulated.
Reference: [6] <author> D. Chen, H. Su, and P. Yew. </author> <title> The Impact of Synchronization and Granularity on Parallel Systems. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 239-248, </pages> <year> 1990. </year>
Reference-contexts: There are studies that use real applications to address specific issues like the effect of sharing in parallel programs on the cache and bus performance [11] and the impact of synchronization and task granularity on parallel system performance <ref> [6] </ref>. Cypher et al. [10], identify the architectural requirements such as floating point operations, communications, and input/output for message-passing scientific applications. Rothberg et al. [24] conduct a similar study towards identifying the cache and memory size requirements for several applications.
Reference: [7] <author> R. G. Covington, S. Madala, V. Mehta, J. R. Jump, and J. B. Sinclair. </author> <title> The Rice parallel processing testbed. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1988 Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 4-11, </pages> <address> Santa Fe, NM, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: Overhead functions can be used to study the growth of system overheads for any of these scaling strategies. In our simulation experiments, we limit ourselves to the constant problem size scaling model. 2.2 SPASM SPASM is an execution-driven simulator written in CSIM. As with other recent simulators <ref> [5, 7, 23] </ref>, the bulk of the instructions in the parallel program is executed at the speed of the native processor (SPARC in this study) and only the instructions that may potentially involve a network access are simulated.
Reference: [8] <author> David Culler et al. </author> <title> LogP : Towards a realistic model of parallel computation. </title> <booktitle> In Proceedings of the 4th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 1-12, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: N/A N/A Barrier 5 Local radix-2 butterfly O ( N p log p) N/A N/A Table 3: Characteristics of FFT points. Phase 3 is the only communication phase in which the cyclic layout of data is changed to a blocked layout as described in <ref> [8] </ref>. It involves an all-to-all communication step where each processor distributes its local data equally among the p processors. <p> The latency and contention overheads (Figures 20 and 21) incurred in this mode are insignificant compared to the total execution time, despite the growth of contention overhead with increasing number of processors. The communication in FFT has been optimized as suggested in <ref> [8] </ref> into a single phase where every processor accesses the data of all the other processors in a skewed manner.
Reference: [9] <author> Zarka Cvetanovic. </author> <title> The effects of problem partitioning, allocation, and granularity on the performance of multiple-processor systems. </title> <journal> IEEE Transactions on Computer Systems, </journal> <volume> 36(4) </volume> <pages> 421-432, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: In general, such models tend to make simplistic assumptions about program behavior and architectural characteristics to make the analysis using the model tractable. These assumptions restrict their applicability for capturing complex interactions between algorithms and architectures. For instance, models developed in <ref> [17, 31, 9] </ref> are mainly applicable to algorithms with regular communication structures that can be predetermined before the execution of the algorithm. Madala and Sinclair [17] confine their studies to synchronous algorithms while [31] and [9] develop models for regular iterative algorithms. <p> For instance, models developed in [17, 31, 9] are mainly applicable to algorithms with regular communication structures that can be predetermined before the execution of the algorithm. Madala and Sinclair [17] confine their studies to synchronous algorithms while [31] and <ref> [9] </ref> develop models for regular iterative algorithms. However, there exist several applications [24] with irregular data access, communication, and synchronization characteristics which cannot always be captured by such simple parameters. Further, an application may be structured to hide a particular overhead such as latency by overlapping computation with communication.
Reference: [10] <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina. </author> <title> Architectural requirements of parallel scientific applications with explicit communication. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: There are studies that use real applications to address specific issues like the effect of sharing in parallel programs on the cache and bus performance [11] and the impact of synchronization and task granularity on parallel system performance [6]. Cypher et al. <ref> [10] </ref>, identify the architectural requirements such as floating point operations, communications, and input/output for message-passing scientific applications. Rothberg et al. [24] conduct a similar study towards identifying the cache and memory size requirements for several applications.
Reference: [11] <author> Susan J. Eggers and Randy H. Katz. </author> <title> The Effect of Sharing on the Cache and Bus Performance of Parallel Programs. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 257-270, </pages> <address> Boston, Massachusetts, </address> <month> April </month> <year> 1989. </year>
Reference-contexts: There are studies that use real applications to address specific issues like the effect of sharing in parallel programs on the cache and bus performance <ref> [11] </ref> and the impact of synchronization and task granularity on parallel system performance [6]. Cypher et al. [10], identify the architectural requirements such as floating point operations, communications, and input/output for message-passing scientific applications.
Reference: [12] <author> John L. Gustafson, Gary R. Montry, and Robert E. Benner. </author> <title> Development of Parallel Methods for a 1024-node Hypercube. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9(4) </volume> <pages> 609-638, </pages> <year> 1988. </year>
Reference-contexts: Several performance metrics such as speedup [2], scaled speedup <ref> [12] </ref>, sizeup [30], experimentally determined serial fraction [14], and isoefficiency function [15] have been proposed for quantifying the scalability of parallel systems.
Reference: [13] <institution> Intel Corporation, Oregon. Intel iPSC/2 and iPSC/860 User's Guide, </institution> <year> 1989. </year>
Reference-contexts: Otherwise, the number of columns is twice the number of rows (we restrict the number of processors to a power of 2 in this study). Messages are circuit-switched and use a transmission scheme similar to the one used on the Intel iPSC/860 <ref> [13] </ref>. A circuit is set up between the source and the destination, and the message is then sent in a single packet. Message-sizes can vary upto 32 bytes. We assume that the switching time for setting up a circuit (in a contention free scenario) is negligible.
Reference: [14] <author> Alan H. Karp and Horace P. Flatt. </author> <title> Measuring Parallel processor Performance. </title> <journal> Communications of the ACM, </journal> <volume> 33(5) </volume> <pages> 539-543, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Several performance metrics such as speedup [2], scaled speedup [12], sizeup [30], experimentally determined serial fraction <ref> [14] </ref>, and isoefficiency function [15] have been proposed for quantifying the scalability of parallel systems. While these metrics are extremely useful for tracking performance trends, they do not provide the information needed to understand the reason why an application does not scale well with an architecture.
Reference: [15] <author> Vipin Kumar and V. Nageswara Rao. </author> <title> Parallel Depth-First Search. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16(6) </volume> <pages> 501-519, </pages> <year> 1987. </year>
Reference-contexts: Several performance metrics such as speedup [2], scaled speedup [12], sizeup [30], experimentally determined serial fraction [14], and isoefficiency function <ref> [15] </ref> have been proposed for quantifying the scalability of parallel systems. While these metrics are extremely useful for tracking performance trends, they do not provide the information needed to understand the reason why an application does not scale well with an architecture.
Reference: [16] <author> Scott T. Leutenegger and Mary K. Vernon. </author> <title> The Performance of Multiprogrammed Multiprocessor Scheduling Policies. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1990 Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 226-236, </pages> <year> 1990. </year> <month> 22 </month>
Reference-contexts: There have been several performance studies that have addressed issues such as latency, contention and synchronization. The scalability of synchronization primitives supported by the hardware [3, 19], the limits on interconnection network performance [1, 21], and the performance of scheduling policies <ref> [33, 16] </ref> are examples of such studies undertaken over the years. While such issues are extremely important, it is appropriate to put the impact of these factors into perspective by considering them in the context of overall application performance.
Reference: [17] <author> Sridhar Madala and James B. Sinclair. </author> <title> Performance of Synchronous Parallel Algorithms with Regular Structures. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(1) </volume> <pages> 105-116, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: In general, such models tend to make simplistic assumptions about program behavior and architectural characteristics to make the analysis using the model tractable. These assumptions restrict their applicability for capturing complex interactions between algorithms and architectures. For instance, models developed in <ref> [17, 31, 9] </ref> are mainly applicable to algorithms with regular communication structures that can be predetermined before the execution of the algorithm. Madala and Sinclair [17] confine their studies to synchronous algorithms while [31] and [9] develop models for regular iterative algorithms. <p> These assumptions restrict their applicability for capturing complex interactions between algorithms and architectures. For instance, models developed in [17, 31, 9] are mainly applicable to algorithms with regular communication structures that can be predetermined before the execution of the algorithm. Madala and Sinclair <ref> [17] </ref> confine their studies to synchronous algorithms while [31] and [9] develop models for regular iterative algorithms. However, there exist several applications [24] with irregular data access, communication, and synchronization characteristics which cannot always be captured by such simple parameters.
Reference: [18] <author> F. H. McMahon. </author> <title> The Livermore Fortran Kernels : A Computer Test of the Numerical Performance Range. </title> <type> Technical Report UCRL-53745, </type> <institution> Lawrence Livermore National Laboratory, Livermore, </institution> <address> CA, </address> <month> December </month> <year> 1986. </year>
Reference-contexts: Such abstractions of real applications that capture the main phases of the computation are called kernels. One can go even lower than kernels by abstracting the main loops in the computation (like the Lawrence Livermore loops <ref> [18] </ref>) and evaluating their performance. As one goes lower, the outcome of the evaluation becomes less realistic. Even though an application may be abstracted by the kernels inside it, the sum of the times spent in the underlying kernels may not necessarily yield the time taken by the application.
Reference: [19] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: There have been several performance studies that have addressed issues such as latency, contention and synchronization. The scalability of synchronization primitives supported by the hardware <ref> [3, 19] </ref>, the limits on interconnection network performance [1, 21], and the performance of scheduling policies [33, 16] are examples of such studies undertaken over the years.
Reference: [20] <author> Janak H. Patel. </author> <title> Analysis of multiprocessors with private cache memories. </title> <journal> IEEE Transactions on Computer Systems, </journal> <volume> 31(4) </volume> <pages> 296-304, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: Further, an application may be structured to hide a particular overhead such as latency by overlapping computation with communication. It may be difficult to capture such dynamic program behavior using analytical models. Similarly, several other models make assumptions about architectural characteristics. For instance, the model developed in <ref> [20] </ref> ignores data inconsistency that can arise in a cache-based multiprocessor during the course of execution of an application and thus does not consider the coherence traffic on the network.
Reference: [21] <author> Gregory F. Pfister and V. Alan Norton. </author> <title> Hot Spot Contention and Combining in Multistage Interconnection Networks. </title> <journal> IEEE Transactions on Computer Systems, </journal> <volume> C-34(10):943-948, </volume> <month> October </month> <year> 1985. </year>
Reference-contexts: There have been several performance studies that have addressed issues such as latency, contention and synchronization. The scalability of synchronization primitives supported by the hardware [3, 19], the limits on interconnection network performance <ref> [1, 21] </ref>, and the performance of scheduling policies [33, 16] are examples of such studies undertaken over the years. While such issues are extremely important, it is appropriate to put the impact of these factors into perspective by considering them in the context of overall application performance.
Reference: [22] <author> U. Ramachandran, G. Shah, S. Ravikumar, and J. Muthukumarasamy. </author> <title> Scalability study of the KSR-1. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <pages> pages I-237-240, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: IS IS is the Integer Sort kernel that uses bucket sort to rank a list of integers which is an important operation in particle method codes. A list of 64K integers with 2K buckets is chosen for this study. An implementation of the algorithm is described in <ref> [22] </ref> and Table 2 summarizes its characteristics. The input list is equally partitioned among the processors. Each processor maintains two sets of buckets. One set of buckets (of size 2K) is used to maintain the information for the portion of the list local to it. The 10 Phase Description Comp.
Reference: [23] <author> S. K. Reinhardt et al. </author> <title> The Wisconsin Wind Tunnel : Virtual prototyping of parallel computers. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1993 Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 48-60, </pages> <address> Santa Clara, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Overhead functions can be used to study the growth of system overheads for any of these scaling strategies. In our simulation experiments, we limit ourselves to the constant problem size scaling model. 2.2 SPASM SPASM is an execution-driven simulator written in CSIM. As with other recent simulators <ref> [5, 7, 23] </ref>, the bulk of the instructions in the parallel program is executed at the speed of the native processor (SPARC in this study) and only the instructions that may potentially involve a network access are simulated.
Reference: [24] <author> Edward Rothberg, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> Working sets, cache sizes and node granularity issues for large-scale multiprocessors. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 14-25, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Cypher et al. [10], identify the architectural requirements such as floating point operations, communications, and input/output for message-passing scientific applications. Rothberg et al. <ref> [24] </ref> conduct a similar study towards identifying the cache and memory size requirements for several applications. However, there have been very few attempts at quantifying the effects of algorithmic and architectural interactions in a parallel system. <p> Madala and Sinclair [17] confine their studies to synchronous algorithms while [31] and [9] develop models for regular iterative algorithms. However, there exist several applications <ref> [24] </ref> with irregular data access, communication, and synchronization characteristics which cannot always be captured by such simple parameters. Further, an application may be structured to hide a particular overhead such as latency by overlapping computation with communication. It may be difficult to capture such dynamic program behavior using analytical models.
Reference: [25] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <year> 1991. </year>
Reference-contexts: Three of them (EP, IS and CG) are from the NAS parallel benchmark suite [4]; CHOLESKY is from the SPLASH benchmark suite <ref> [25] </ref>; and FFT is the well-known Fast Fourier Transform algorithm. The characteristics include the data access pattern, the synchronization pattern, the communication pattern, the computation granularity (the amount of work done) and data granularity (the amount of data communicated) for each phase of the program. <p> The sparse nature of the input matrix results in an algorithm with a data dependent dynamic access pattern. The algorithm requires an initial symbolic factorization of the input matrix which is done sequentially because it requires only a small fraction of the total compute time. Only numerical factorization <ref> [25] </ref> is parallelized and analyzed. Sets of columns having similar non-zero structure are combined into supernodes at the end of symbolic factorization. Processors get tasks from a central task queue. Each supernode is a potential task which is used to modify subsequent supernodes.
Reference: [26] <author> Anand Sivasubramaniam, Umakishore Ramachandran, and H. Venkateswaran. </author> <title> Message-Passing: Computational Model, Programming Paradigm, and Experimental Studies. </title> <type> Technical Report GIT-CC-91/11, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> February </month> <year> 1991. </year> <month> 23 </month>
Reference-contexts: Experimentation, simulation, and analytical models are three techniques that have been commonly used in such studies. But each has its own limitations. We adopted the first technique in our earlier work by experimenting with frequently used parallel algorithms on shared memory [27] and message-passing <ref> [26] </ref> 4 platforms.
Reference: [27] <author> Anand Sivasubramaniam, Gautam Shah, Joonwon Lee, Umakishore Ramachandran, and H. Venkateswaran. </author> <title> Experimental Evaluation of Algorithmic Performance on Two Shared Memory Multiprocessors. In Norihisa Suzuki, editor, </title> <booktitle> Shared Memory Multiprocessing, </booktitle> <pages> pages 81-107. </pages> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: Experimentation, simulation, and analytical models are three techniques that have been commonly used in such studies. But each has its own limitations. We adopted the first technique in our earlier work by experimenting with frequently used parallel algorithms on shared memory <ref> [27] </ref> and message-passing [26] 4 platforms.
Reference: [28] <author> Anand Sivasubramaniam, Aman Singla, Umakishore Ramachandran, and H. Venkateswaran. </author> <title> Machine Abstractions and Locality Issues in Studying Parallel Systems. </title> <type> Technical Report GIT-CC-93/63, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: In [29], we illustrated this approach for the scalability study of message-passing systems. In this paper, we conduct a similar study for shared memory systems. A companion paper <ref> [28] </ref> develops a framework using the overhead functions for studying machine abstractions and impact of locality on the performance of parallel systems. The top-down approach and the overhead functions are elaborated in Section 2.
Reference: [29] <author> Anand Sivasubramaniam, Aman Singla, Umakishore Ramachandran, and H. Venkateswaran. </author> <title> A Simulation-based Scalability Study of Parallel Systems. </title> <type> Technical Report GIT-CC-93/27, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: The work we present in this paper is part of a larger project which aims at understanding the significant issues in the design of scalable parallel systems using the above-mentioned top-down approach. In <ref> [29] </ref>, we illustrated this approach for the scalability study of message-passing systems. In this paper, we conduct a similar study for shared memory systems. A companion paper [28] develops a framework using the overhead functions for studying machine abstractions and impact of locality on the performance of parallel systems. <p> As with other recent simulators [5, 7, 23], the bulk of the instructions in the parallel program is executed at the speed of the native processor (SPARC in this study) and only the instructions that may potentially involve a network access are simulated. The reader is referred to <ref> [29] </ref> for a detailed description of the implementation of SPASM.
Reference: [30] <author> Xian-He Sun and John L. Gustafson. </author> <title> Towards a better Parallel Performance Metric. </title> <journal> Parallel Computing, </journal> <volume> 17 </volume> <pages> 1093-1109, </pages> <year> 1991. </year>
Reference-contexts: Several performance metrics such as speedup [2], scaled speedup [12], sizeup <ref> [30] </ref>, experimentally determined serial fraction [14], and isoefficiency function [15] have been proposed for quantifying the scalability of parallel systems.
Reference: [31] <author> D. F. Vrsalovic, D. P. Siewiorek, Z. Z. Segall, and E. Gehringer. </author> <title> Performance Prediction and Calibration for a Class of Multiprocessors. </title> <journal> IEEE Transactions on Computer Systems, </journal> <volume> 37(11) </volume> <pages> 1353-1365, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: In general, such models tend to make simplistic assumptions about program behavior and architectural characteristics to make the analysis using the model tractable. These assumptions restrict their applicability for capturing complex interactions between algorithms and architectures. For instance, models developed in <ref> [17, 31, 9] </ref> are mainly applicable to algorithms with regular communication structures that can be predetermined before the execution of the algorithm. Madala and Sinclair [17] confine their studies to synchronous algorithms while [31] and [9] develop models for regular iterative algorithms. <p> For instance, models developed in [17, 31, 9] are mainly applicable to algorithms with regular communication structures that can be predetermined before the execution of the algorithm. Madala and Sinclair [17] confine their studies to synchronous algorithms while <ref> [31] </ref> and [9] develop models for regular iterative algorithms. However, there exist several applications [24] with irregular data access, communication, and synchronization characteristics which cannot always be captured by such simple parameters.
Reference: [32] <author> J. C. Wyllie. </author> <title> The Complexity of Parallel Computations. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Cornell University, </institution> <year> 1979. </year>
Reference-contexts: Algorithmic overhead is the difference between the linear curve and that which would be obtained (the ideal curve in Figure 1) by executing the algorithm on an ideal machine such as the PRAM <ref> [32] </ref>. Such a machine idealizes the parallel architecture by assuming an infinite number of processors, and unit costs for communication and synchronization. A real execution could deviate significantly from the ideal execution due to the overheads such as latency, contention, synchronization, scheduling and cache effects. <p> The algorithmic overhead is quantified by computing the time taken for execution of a given parallel program on an ideal machine such as the PRAM <ref> [32] </ref> and measuring its deviation from a linear speedup curve. Further, we separate this overhead into that due to the serial part (serial overhead) and that due to work imbalance (work-imbalance overhead). As we mentioned earlier, the interaction overhead should be separated into its component parts.
Reference: [33] <author> John Zahorjan and Cathy McCann. </author> <title> Processor Scheduling in Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1990 Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 214-225, </pages> <year> 1990. </year> <title> 24 Speedup Graphs 25 EP Graphs 26 IS Graphs (Mesh) Figure 14: Mode-wise Latency (Mesh) 27 FFT Graphs (Mesh) Figure 20: Mode-wise Latency (Mesh) 28 CG Graphs (Mesh) Figure 26: Mode-wise Latency (Mesh) 29 CHOLESKY Graphs (Mesh) Figure 32: Mode-wise Latency (Mesh) 30 </title>
Reference-contexts: There have been several performance studies that have addressed issues such as latency, contention and synchronization. The scalability of synchronization primitives supported by the hardware [3, 19], the limits on interconnection network performance [1, 21], and the performance of scheduling policies <ref> [33, 16] </ref> are examples of such studies undertaken over the years. While such issues are extremely important, it is appropriate to put the impact of these factors into perspective by considering them in the context of overall application performance.
References-found: 33

