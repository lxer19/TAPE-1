URL: ftp://ftp.cis.upenn.edu/pub/msingh/mlj96.ps.Z
Refering-URL: http://www.cis.upenn.edu/~msingh/frames/papers_list.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Induction of Selective Bayesian Network Classifiers  
Author: Moninder Singh Gregory M. Provan Pat Langley 
Keyword: Bayesian networks, naive Bayesian classifier, feature selection.  
Address: 200 S 33rd Street Philadelphia, PA 19104-6389  2164 Staunton Court Palo Alto, CA, 94306  
Affiliation: Department of Computer and Information Science University of Pennsylvania  Institute for the Study of Learning and Expertise  
Note: Submitted to the Machine Learning Journal  Current address: Rockwell Science Center, 1049 Camino dos Rios, Thousand Oaks CA 91360.  
Email: &lt; msingh@gradient.cis.upenn.edu &gt;  
Web: &lt; fprovan,langleyg@fcamis,csg.stanford.edu &gt;  
Abstract: We present an algorithm for inducing Bayesian networks using feature selection. The algorithm selects a subset of attributes that maximizes predictive accuracy prior to the network learning phase, thereby incorporating a bias for small networks that retain high predictive accuracy. We compare the behavior of this selective Bayesian network classifier with that of (a) Bayesian network classifiers that incorporate all attributes, (b) selective and non-selective naive Bayesian classifiers, and (c) the decision-tree algorithm C4.5. With respect to (a), we show that our approach generates networks that are computationally simpler to evaluate but display comparable predictive accuracy. With respect to (b), we show that the selective Bayesian network classifier performs significantly better than both versions of the naive Bayesian classifier on almost all databases studied, and hence is an enhancement of the naive method. With respect to (c), we show that the selective Bayesian network classifier displays comparable behavior. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W. & Bankert, R. L. </author> <year> (1994). </year> <title> Feature selection for case-based classification of cloud types. </title> <booktitle> In AAAI Workshop on Case-based Reasoning, </booktitle> <pages> pages 106-112. </pages> <publisher> AAAI Press. </publisher>
Reference: <author> Amuallim, H. & Dietterich, T. </author> <year> (1991). </year> <title> Learning with many irrelevant features. </title> <booktitle> In Proc. Conf. of the AAAI, </booktitle> <pages> pages 547-552, </pages> <address> Menlo Park, CA. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Feature selection has also received considerable attention in the last few years within the computational learning community, using both filter and wrapper methods. Filter model approaches used for the induction of decision trees include the Focus algorithm <ref> (Amuallim & Dietterich, 1991) </ref> and the Relief algorithm (Kira & Rendell, 1992b; Kira & Rendell, 1992a), which Kononenko (1994) has extended. Cardie (1993) used a filtering approach in an extended nearest-neighbor algorithm, while Kubat et al. (1993) filtered features for use with a naive Bayesian classifier.
Reference: <author> Anderson, S., Olesen, K., Jensen, F., & Jensen, F. </author> <year> (1989). </year> <title> HUGIN A Shell for building Bayesian Belief Universes for Expert Systems. </title> <booktitle> In Proceedings of the 11th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1080-1085. </pages>
Reference-contexts: Once again, the (reduced) training data set was split into two parts for the two selective algorithms. We used the HUGIN <ref> (Anderson et al., 1989) </ref> clique-tree inference algorithm for obtaining the classification accuracy of the learned Bayesian networks. The statistical significance of the differences in classification accuracies was measured using a two-tailed, paired t-test at the 95% confidence level. Table 3: Predictive Accuracies for the Different Algorithms.
Reference: <author> Andreassen, S., Woldbye, M., Falck, B., & Andersen, S. </author> <year> (1987). </year> <title> A causal probabilistic network for interpretation of electromyographic findings. </title> <booktitle> In Proc. Tenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 366-372, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cardie, C. </author> <year> (1993). </year> <title> Using decision trees to improve case-based learning. </title> <booktitle> In Proc. Machine Learning, </booktitle> <pages> pages 25-32. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Caruana, R. & Freitag, D. </author> <year> (1994). </year> <title> Greedy attribute selection. </title> <editor> In Cohen, W. & Hirsch, H., editors, </editor> <booktitle> Proc. Machine Learning, </booktitle> <pages> pages 28-36. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Charniak, E. & Goldman, R. </author> <year> (1989). </year> <title> Plan recognition in stories and in life. </title> <booktitle> In Uncertainty in Artificial Intelligence: Proceedings of the Fifth Workshop, </booktitle> <pages> pages 54-60, </pages> <address> Mountain View, California. </address>
Reference-contexts: Bayesian networks are being increasingly used in various real-world applications, including medical diagnosis (Suermondt & Amylon, 1989; Andreassen et al., 1987), telecommunications (Ezawa & Norton, 1995), information retrieval (Fung & Favero, 1995), system troubleshooting (Heckerman et al., 1994a), vision (Levitt et al., 1989) and language understanding <ref> (Charniak & Goldman, 1989) </ref>. This paper presents K2-AS, a novel method for inducing selective Bayesian networks, and compares its classification performance with three other probabilistic induction methods: a Bayesian network induction method that uses all features (non-selective classifier), and selective and nonselective naive Bayesian classifiers.
Reference: <author> Cheeseman, P. & Oldford, W., </author> <title> editors (1994). Selecting Models from Data: AI and Statistics IV. </title> <publisher> Springer-Verlag. </publisher>
Reference: <author> Cooper, G. </author> <year> (1990). </year> <title> The computational complexity of probabilistic inference using Belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 393-405. </pages>
Reference-contexts: Inference is more complicated than in the naive case: the general inference task is NP-hard <ref> (Cooper, 1990) </ref>. In practice, network topology (especially maximum clique size) and the number of variables are two of the most signif icant parameters that govern inference complexity.
Reference: <author> Cooper, G. & Herskovits, E. </author> <year> (1992). </year> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 309-347. </pages>
Reference: <author> Cowell, R., Dawid, P., & Spiegelhalter, D. </author> <year> (1993). </year> <title> Sequential model criticism in probabilistic expert systems. </title> <journal> IEEE Transactions of Pattern Analysis and Machine Intelligence, </journal> <volume> 15(3) </volume> <pages> 209-219. </pages>
Reference-contexts: Note that even though a given model may be "correct" in the sense of generating the data, it need not be the best model when it comes to making predictions <ref> (Cowell et al., 1993) </ref>. As pointed out in Section 3.4, our choice of CB was arbitrary. One could as well use other methods, such as Heckerman et al.'s (1994b) algorithm, but most of the current learning methods suffer from this drawback.
Reference: <author> Dawid, A. </author> <year> (1992). </year> <title> Prequential analysis, stochastic complexity and Bayesian inference. </title> <editor> In Bernardo, J., Berger, J., Dawid, A., & Smith, A., editors, </editor> <booktitle> Bayesian Statistics 4, </booktitle> <pages> pages 109-125. </pages> <publisher> Oxford Science Publications. </publisher>
Reference: <author> Ezawa, K. & Norton, S. </author> <year> (1995). </year> <title> Knowledge discovery in telecommunication services data using Bayesian network models. </title> <booktitle> In Proc. 1st Int. Conf. on Knowledge Discovery and Data Mining. </booktitle>
Reference-contexts: Moreover, in terms of learning, one can use this structure to explicitly encode priors, a feature absent in many learning frameworks. Bayesian networks are being increasingly used in various real-world applications, including medical diagnosis (Suermondt & Amylon, 1989; Andreassen et al., 1987), telecommunications <ref> (Ezawa & Norton, 1995) </ref>, information retrieval (Fung & Favero, 1995), system troubleshooting (Heckerman et al., 1994a), vision (Levitt et al., 1989) and language understanding (Charniak & Goldman, 1989).
Reference: <author> Fung, R. & Favero, B. D. </author> <year> (1995). </year> <title> Applying Bayesian networks to information retrieval. </title> <journal> Communications of the ACM, </journal> <volume> 38(3). </volume>
Reference-contexts: Moreover, in terms of learning, one can use this structure to explicitly encode priors, a feature absent in many learning frameworks. Bayesian networks are being increasingly used in various real-world applications, including medical diagnosis (Suermondt & Amylon, 1989; Andreassen et al., 1987), telecommunications (Ezawa & Norton, 1995), information retrieval <ref> (Fung & Favero, 1995) </ref>, system troubleshooting (Heckerman et al., 1994a), vision (Levitt et al., 1989) and language understanding (Charniak & Goldman, 1989).
Reference: <author> Heckerman, D. </author> <year> (1995). </year> <title> A tutorial on learning Bayesian networks. </title> <type> Technical Report MSR-TR-95-06, </type> <institution> Microsoft Research, </institution> <address> Redmond, WA. </address>
Reference: <author> Heckerman, D., Breese, J. S., & Rommelse, K. </author> <year> (1994a). </year> <title> Troubleshooting under uncertainty. </title> <type> Technical Report MSR-TR-94-07, </type> <institution> Microsoft Research, Redmond, WA. </institution> <note> 29 Heckerman, </note> <author> D., Geiger, D., & Chickering, M. </author> <year> (1994b). </year> <title> Learning Bayesian networks: the combina-tion of knowledge and statistical data. </title> <type> Technical Report MSR-TR-94-09, </type> <institution> Microsoft research, </institution> <address> Redmond, WA. </address>
Reference-contexts: Bayesian networks are being increasingly used in various real-world applications, including medical diagnosis (Suermondt & Amylon, 1989; Andreassen et al., 1987), telecommunications (Ezawa & Norton, 1995), information retrieval (Fung & Favero, 1995), system troubleshooting <ref> (Heckerman et al., 1994a) </ref>, vision (Levitt et al., 1989) and language understanding (Charniak & Goldman, 1989).
Reference: <author> Heckerman, D. & Shachter, R. </author> <year> (1994). </year> <title> A decision-based view of causality. </title> <booktitle> In Proc. Conf. Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 302-310. </pages>
Reference: <author> John, G., Kohavi, R., & Pfleger, K. </author> <year> (1994). </year> <title> Irrelevant features and the subset selection problem. </title> <editor> In Cohen, W. & Hirsch, H., editors, </editor> <booktitle> Proc. Machine Learning, </booktitle> <pages> pages 121-129. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Feature selection can be regarded as a search through the space of possible attribute subsets. The strategies that can be used to evaluate alternative subsets of attributes fall into two broad classes. One type of strategies constitute what are commonly described as wrapper methods <ref> (John et al., 1994) </ref> in that the induction algorithm itself is used during feature selection to evaluate alternative subsets of attributes. In the second type of strategy, known as filter methods, less relevant features are filtered out during feature selection using an algorithm different from the induction algorithm.
Reference: <author> Kira, K. & Rendell, L. </author> <year> (1992a). </year> <title> The feature selection problem: Traditional methods and a new algorithm. </title> <booktitle> In Proc. AAAI, </booktitle> <pages> pages 129-134. </pages> <publisher> AAAI Press. </publisher>
Reference: <author> Kira, K. & Rendell, L. </author> <year> (1992b). </year> <title> A practical approach to feature selection. </title> <booktitle> In Proc. Machine Learning, </booktitle> <pages> pages 249-256. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kirkpatrick, S., Gelatt, C., & Vecchi, M. </author> <year> (1983). </year> <title> Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220 </volume> <pages> 671-680. </pages>
Reference-contexts: This limits the effectiveness of feature selection, especially in the case of K2-AS&lt;, which may add a large number of redundant variables simply because they do not affect the predictive accuracy of the learned Bayesian network (either increase or decrease). Other search strategies like simulated annealing <ref> (Kirkpatrick et al., 1983) </ref> or faster stochastic search methods like GSAT (Selman et al., 1992) may yield better results. 7 In addition, we should consider the usefulness of other strategies to evaluate alternate subsets of features. As described in Section 3.4, K2-AS falls into the class of wrapper models.
Reference: <author> Kononenko, I. </author> <year> (1990). </year> <title> Comparison of inductive and noise Bayesian learning approaches to automatic knowledge acquisition. </title> <editor> In et al., B. W., editor, </editor> <title> Current Trends in Knowledge Acquisition. </title> <publisher> IOS Press, Amsterdam. </publisher>
Reference: <author> Kononenko, I. </author> <year> (1994). </year> <title> Estimating attributes: Analysis and extension of RELIEF. </title> <booktitle> In Proc. European Conf. on Machine Learning, </booktitle> <pages> pages 171-182. </pages> <publisher> Springer Verlag. </publisher>
Reference: <author> Kubat, M., Flotzinger, D., & Pfurtscheller, G. </author> <year> (1993). </year> <title> Discovering patters in eeg signals: Comparative study of a few methods. </title> <booktitle> In Proc. European Conf. on Machine Learning, </booktitle> <pages> pages 367-371. </pages> <publisher> Springer Verlag. </publisher>
Reference: <author> Lam, W. & Bacchus, F. </author> <year> (1994). </year> <title> Learning Bayesian belief networks, an approach based on the MDL principle. </title> <journal> Computational Intelligence, </journal> <volume> 10(4). </volume>
Reference: <author> Langley, P. </author> <year> (1993). </year> <title> Induction of recursive Bayesian classifiers. </title> <booktitle> In Proc. European Conf. on Machine Learning, </booktitle> <pages> pages 153-164. </pages> <publisher> Springer Verlag. </publisher>
Reference-contexts: However, this approach is typically limited to learning classes that can be separated by a single decision boundary <ref> (Langley, 1993) </ref>, and it can suffer in domains in which the features are correlated given the class variable. The selective naive Bayesian classifier (Langley & Sage, 1994a) is an extension to the naive Bayesian classifier, and is designed to perform better in domains with redundant features.
Reference: <author> Langley, P. </author> <year> (1994). </year> <title> Selection of relevant features in machine learning. </title> <editor> In Greiner, R., editor, </editor> <booktitle> Proc. AAAI Fall Symposium on Relevance. </booktitle> <publisher> AAAI Press. </publisher>
Reference: <author> Langley, P., Iba, W., & Thompson, K. </author> <year> (1992). </year> <title> An analysis of Bayesian classifiers. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 223-228. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: We can write this unique joint distribution as P (Z 1 ; Z 2 ; : : : ; Z n ) = i=1 The naive Bayesian classifier <ref> (Langley et al., 1992) </ref> assumes that the attributes are conditionally independent given the class variable. Thus, it is a Bayesian network whose structure is restricted to having arcs only from the class node to the feature nodes, i.e., the only parent node in Equation 1 is C. <p> Moreover, despite its 8 simplicity and the strong assumption that attributes are independent within each class, the naive classifier has been shown to give remarkably high accuracies in many natural domains <ref> (Langley et al., 1992) </ref>. However, this approach is typically limited to learning classes that can be separated by a single decision boundary (Langley, 1993), and it can suffer in domains in which the features are correlated given the class variable.
Reference: <author> Langley, P. & Sage, S. </author> <year> (1994a). </year> <title> Induction of selective Bayesian classifiers. </title> <booktitle> In Proc. Conf. on Uncertainty in AI. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, this approach is typically limited to learning classes that can be separated by a single decision boundary (Langley, 1993), and it can suffer in domains in which the features are correlated given the class variable. The selective naive Bayesian classifier <ref> (Langley & Sage, 1994a) </ref> is an extension to the naive Bayesian classifier, and is designed to perform better in domains with redundant features. The intuition is that, if highly correlated features are not selected, the classifier should perform better given its feature independence assumptions. <p> However, as described in the following section, these show convincingly that selective Bayesian networks offer several advantages over traditional learning methods. 4 Experimental Comparison of Bayesian Induction Algorithms 4.1 Experimental Design We designed our experiments to accomplish three main objectives. First, previous experimental studies <ref> (Langley & Sage, 1994a) </ref> have shown that although the naive classifier performs fairly well in some domains, it performs poorly in others. This is probably due to the restrictive conditional independence assumptions made in such classifiers, which may degrade their performance in domains with several correlated features. <p> Besides studying such methods in the context of decision-tree induction, they also examine notions of relevance and irrelevance in the context of machine learning. Other work within the wrapper framework has dealt with the nearest neighbor method (Langley & Sage, 1994b) and naive Bayesian classifiers <ref> (Langley & Sage, 1994a) </ref>, among others. Tradeoffs between the accuracy and the computational cost of wrapper methods have been studied by Caruana and Freitag (1994). Langley (1994) presents a thorough review of these, as well as other, feature-selection approaches studied within the machine learning literature.
Reference: <author> Langley, P. & Sage, S. </author> <year> (1994b). </year> <title> Oblivious decision trees and abstract cases. </title> <booktitle> In Working notes of the AAAI'94 Workshop on Case-Based Reasoning, </booktitle> <pages> pages 113-117. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: Besides studying such methods in the context of decision-tree induction, they also examine notions of relevance and irrelevance in the context of machine learning. Other work within the wrapper framework has dealt with the nearest neighbor method <ref> (Langley & Sage, 1994b) </ref> and naive Bayesian classifiers (Langley & Sage, 1994a), among others. Tradeoffs between the accuracy and the computational cost of wrapper methods have been studied by Caruana and Freitag (1994).
Reference: <author> Lauritzen, S. L. & Spiegelhalter, D. J. </author> <year> (1988). </year> <title> Local computations with probabilities on graphical structures and their application to expert systems. </title> <journal> Journal of the Royal Statistical Society (Series B), </journal> <volume> 50 </volume> <pages> 157-224. </pages>
Reference: <author> Levitt, T., Mullin, J., & Binford, T. </author> <year> (1989). </year> <title> Model-based influence diagrams for machine vision. </title> <booktitle> In Proc. Fifth Workshop on Uncertainty in AI, </booktitle> <pages> pages 233-244. </pages> <note> 30 Madigan, </note> <author> D., Raftery, A., York, J., Bradshaw, J., & Almond, R. </author> <year> (1993). </year> <title> Strategies for graphical model selection. </title> <booktitle> In Proc. International Workshop on AI and Statistics, </booktitle> <pages> pages 331-336. </pages>
Reference-contexts: Bayesian networks are being increasingly used in various real-world applications, including medical diagnosis (Suermondt & Amylon, 1989; Andreassen et al., 1987), telecommunications (Ezawa & Norton, 1995), information retrieval (Fung & Favero, 1995), system troubleshooting (Heckerman et al., 1994a), vision <ref> (Levitt et al., 1989) </ref> and language understanding (Charniak & Goldman, 1989).
Reference: <author> Marill, T. & Green, D. </author> <year> (1963). </year> <title> On the effectiveness of receptors in recognition systems. </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> 9 </volume> <pages> 11-17. </pages>
Reference-contexts: Techniques developed include sequential backward selection <ref> (Marill & Green, 1963) </ref>, branch and bound (Naren-dra & Fukunaga, 1977), best-first (Xu et al., 1989) and beam search as well as bidirectional search (Siedlecki & Sklansky, 1988).
Reference: <author> Murphy, P. & Aha, D. </author> <year> (1992). </year> <title> UCI repository of machine learning databases. </title> <institution> Machine-readable data repository Department of Information and Computer Science, University of California, Irvine. </institution>
Reference-contexts: With these goals in mind, we carried out a series of experiments using data sets from the University of California, Irvine, repository of machine learning databases <ref> (Murphy & Aha, 1992) </ref>, namely Michalski and Chilausky's soybean data, the gene-splicing data set due to Towell, Noordewier, and Shavlik, 4 Schlimmer's mushroom and voting data sets, and Shapiro's chess endgame data.
Reference: <author> Narendra, M. & Fukunaga, K. </author> <year> (1977). </year> <title> A branch and bound algorithm for feature subset selection. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 26(9) </volume> <pages> 917-922. </pages>
Reference: <author> Pazzani, M. </author> <year> (1995). </year> <title> Searching for attribute dependencies in Bayesian classifiers. </title> <booktitle> In Proc. Fifth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 424-429. </pages>
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: 1 Introduction Bayesian induction methods have proven to be an important class of algorithms that behave competitively with well-known induction techniques for other representations such as decision trees and neural networks. In this paper we investigate experimentally the behavior of several methods for inducing Bayesian networks <ref> (Pearl, 1988) </ref>, one of the most important Bayesian representations. Bayesian networks are an important representation because they can encode joint probability distributions, in addition to precisely defining the conditional distributions that constitute the joint distribution by means of an underlying graphical structure.
Reference: <author> Quinlan, J. R. & Cameron-Jones, R. M. </author> <year> (1995). </year> <title> Oversearching and layered search in empirical learning. </title> <booktitle> In Procs. 14th Intl. Joint Conf. on Artificial Intelligence, </booktitle> <pages> pages 1019-1024. </pages>
Reference-contexts: In order to learn better selective Bayesian network classifiers in terms of classification 7 One alternative hypothesis is that overfitting from such approaches may prevent them from yielding better results in certain situations <ref> (Quinlan & Cameron-Jones, 1995) </ref>. 27 accuracy, it is important to develop methods for learning Bayesian networks with a view towards maximizing their performance on that measure. The global metrics of Cowell et al. (Cowell et al., 1993; Spiegelhalter et al., 1993) are a step in the right direction.
Reference: <author> Selman, B., Levesque, H., & Mitchell, D. </author> <year> (1992). </year> <title> A new method for solving hard satisfiability problems. </title> <booktitle> In Proc. </booktitle> <publisher> AAAI. </publisher>
Reference-contexts: Other search strategies like simulated annealing (Kirkpatrick et al., 1983) or faster stochastic search methods like GSAT <ref> (Selman et al., 1992) </ref> may yield better results. 7 In addition, we should consider the usefulness of other strategies to evaluate alternate subsets of features. As described in Section 3.4, K2-AS falls into the class of wrapper models.
Reference: <author> Siedlecki, W. & Sklansky, J. </author> <year> (1988). </year> <title> On automatic feature selection. </title> <journal> Intl. J. of Pattern Recognition and Artificial Intelligence, </journal> <volume> 2(2) </volume> <pages> 197-220. </pages>
Reference-contexts: Techniques developed include sequential backward selection (Marill & Green, 1963), branch and bound (Naren-dra & Fukunaga, 1977), best-first (Xu et al., 1989) and beam search as well as bidirectional search <ref> (Siedlecki & Sklansky, 1988) </ref>. A recent meeting of the Society of AI and Statistics was dedicated to papers on "Selecting Models from Data"(Cheeseman & Oldford, 1994), and contains numerous papers on feature selection.
Reference: <author> Singh, M. & Provan, G. M. </author> <year> (1996). </year> <title> Efficient learning of selective Bayesian network classifiers. </title> <booktitle> In Proc. 13th Intl. Conference on Machine Learning. To appear. </booktitle>
Reference-contexts: Use of some measure other than classification accuracy (e.g., information content) to select the features may help reduce the complexity of this phase. Preliminary results involving filter models using conditional, information-based metrics <ref> (Singh & Provan, 1996) </ref> show that such metrics not only greatly improve the speed of the subset selection phase, but also generate selective Bayesian networks with even better predictive accuracy than those constructed by K2-AS.
Reference: <author> Singh, M. & Valtorta, M. </author> <year> (1995). </year> <title> Construction of Bayesian network structures from data: a brief survey and an efficient algorithm. </title> <journal> International Journal of Approximate Reasoning, </journal> <volume> 12 </volume> <pages> 111-131. </pages>
Reference-contexts: CB keeps constructing networks for increasing orders of conditional independence tests as long as the predictive accuracy of the resultant network keeps increasing. Since CB uses the K2 algorithm to generate the Bayesian network from a particular ordering, the algorithm is correct in the same sense as K2 <ref> (Singh & Valtorta, 1995) </ref>.
Reference: <author> Spiegelhalter, D., Dawid, P., Lauritzen, S., & Cowell, R. </author> <year> (1993). </year> <title> Bayesian analysis in expert systems. </title> <journal> Statistical Science, </journal> <volume> 8(3) </volume> <pages> 219-283. </pages>
Reference: <author> Suermondt, H. J. & Amylon, M. D. </author> <year> (1989). </year> <title> Probabilistic prediction of the outcome of bone-marrow transplantation. </title> <booktitle> In Proceedings of the Symposium on Computer Applications in Medical Care, </booktitle> <pages> pages 208-212, </pages> <address> Washington, D.C. </address>
Reference: <author> Todd, B. S. & Stamper, R. </author> <year> (1994). </year> <title> The relative accuracy of a variety of medical diagnostic programs. </title> <journal> Methods Inform. Med., </journal> <volume> 33 </volume> <pages> 402-416. </pages>
Reference-contexts: We study the naive Bayesian classifier because it is a valuable benchmark against which to compare K2-AS: the naive Bayesian classifier compares favorably with other classifiers, and indeed is viewed as optimal in some domains <ref> (Todd & Stamper, 1994) </ref>). However, since Bayesian networks generalize this basic approach to account for correlations among features, they should perform better than the naive classifiers in many domains. The naive Bayesian classifier is perhaps the simplest and most widely studied probabilistic learning method.
Reference: <author> Xu, L., Yan, P., & Chang, T. </author> <year> (1989). </year> <title> Best-first strategy for feature selection. </title> <booktitle> In Proc. Ninth International Conf. on Pattern Recognition, </booktitle> <pages> pages 706-708. </pages> <publisher> IEEE Computer Society Press. </publisher> <pages> 31 </pages>
Reference-contexts: Techniques developed include sequential backward selection (Marill & Green, 1963), branch and bound (Naren-dra & Fukunaga, 1977), best-first <ref> (Xu et al., 1989) </ref> and beam search as well as bidirectional search (Siedlecki & Sklansky, 1988). A recent meeting of the Society of AI and Statistics was dedicated to papers on "Selecting Models from Data"(Cheeseman & Oldford, 1994), and contains numerous papers on feature selection.
References-found: 46

