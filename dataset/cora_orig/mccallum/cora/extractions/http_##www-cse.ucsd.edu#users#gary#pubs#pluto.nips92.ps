URL: http://www-cse.ucsd.edu/users/gary/pubs/pluto.nips92.ps
Refering-URL: http://www.cse.ucsd.edu/users/gary/
Root-URL: 
Title: Learning Mackey-Glass from 25 examples, Plus or Minus 2  
Author: Mark Plutowski* Garrison Cottrell* Halbert White** 
Address: La Jolla, CA 92093  
Affiliation: Institute for Neural Computation *Department of Computer Science and Engineering **Department of Economics University of California, San Diego  
Abstract: We apply active exemplar selection (Plutowski & White, 1991; 1993) to predicting a chaotic time series. Given a fixed set of examples, the method chooses a concise subset for training. Fitting these exemplars results in the entire set being fit as well as desired. The algorithm incorporates a method for regulating network complexity, automatically adding exemplars and hidden units as needed. Fitting examples generated from the Mackey-Glass equation with fractal dimension 2.1 to an rmse of 0.01 required about 25 exemplars and 3 to 6 hidden units. The method requires an order of magnitude fewer floating point operations than training on the entire set of examples, is significantly cheaper than two contending exemplar selection techniques, and suggests a simpler active selection technique that performs comparably.
Abstract-found: 1
Intro-found: 1
Reference: <author> Lapedes, Alan, and Robert Farber. </author> <year> 1987. </year> <title> "Nonlinear signal processing using neural networks. Prediction and system modelling." </title> <type> Los Alamos technical report LA-UR-87-2662. </type>
Reference-contexts: We used the standard feed-forward network architecture with [0; 1] sigmoids and one or two hidden layers. Denoting the time series as x (t), the inputs were x (t); x (t 6); x (t 12); x (t 18), and the desired output is x (t + 6) <ref> (Lapedes & Farber, 1987) </ref>. We used conjugate gradient optimization for all of the training runs. The line search routine typically required 5 to 7 passes through the data set for each downhill step, and was restricted to use no more than 10. <p> To ensure that each method is achieving a comparable fit over novel data, we evaluated each network over a test set. The generalization tests also looked at the iterated prediction error (IPE) over the candidate set and test set <ref> (Lapedes & Farber, 1987) </ref>. Here we start the network on the first example from the set, and feed the output back into the network to obtain predictions in multiples of 6 time steps. Finally, for each of these we compare the final network sizes.
Reference: <author> Mackey, M.C., and L. Glass. </author> <year> 1977. </year> <title> "Oscillation and chaos in physiological control systems." </title> <booktitle> Science 197, </booktitle> <pages> 287. </pages>
Reference-contexts: Increment n. Set F n = F N . g While (e n (x n ) &gt; F n ) f Train the network on the current training set x n , restarting and growing as necessary. gg 4 The Problem We generated the data from the Mackey-Glass equation <ref> (Mackey & Glass, 1977) </ref>, with t = 17, a = 0:2, and b = 0:1. We integrated the equation using fourth order Runge-Kutta with step size 0.1, and the history initialized to 0.5. We generated two data sets.
Reference: <author> Plutowski, Mark E., and Halbert White. </author> <year> 1991. </year> <title> "Active selection of training examples for network learning in noiseless environments." </title> <type> Technical Report No. </type> <institution> CS91-180, CSE Dept., UCSD, La Jolla, California. </institution>
Reference-contexts: Note that using this property for x n+1 will not necessarily deliver the globally optimal solution. Nevertheless, this approach permits a computationally feasible and attractive method for sequential selection of training examples. Choosing x n+1 to maximize this decrement directly is expensive. We use the following simple approximation <ref> (see Plutowski & White, 1991) </ref> for justification): Given x n , select x n+1 2 arg max x n+1 ISB (x n+1 jx n ), where ISB (x n+1 jx n ) = w n+1 N X r w f (x i ; w n )(g (x i ) f (x
Reference: <author> Plutowski, Mark E., and Halbert White. </author> <year> 1993. </year> <title> "Selecting concise training sets from clean data." To appear, </title> <journal> IEEE Transactions on Neural Networks. </journal> <volume> 3, </volume> <pages> 1. </pages>
Reference-contexts: In preliminary experiments we evaluated sensitivity of ISB to the calculation of H. We compared two ways of estimating H, in terms of the number of exemplars selected and the total cost of training. The first approach uses the diagonal terms of H <ref> (Plutowski & White, 1993) </ref>. The second approach replaces H with the identity matrix. Evaluated over 10 separate runs, fitting 500 examples to an rmse of 0.01, ISB gave similar results for both approaches, in terms of total computation used and the number of exemplars selected.
References-found: 4

