URL: http://www.cs.umn.edu/Users/dept/users/kumar/pspases.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Title: PSPASES: Building a High Performance Scalable Parallel Direct Solver for Sparse Linear Systems  
Author: Mahesh Joshi George Karypis Vipin Kumar Anshul Gupta Fred Gustavson 
Address: Minneapolis, MN 55455  Yorktown Heights, NY 10598  
Affiliation: Department of Computer Science University of Minnesota  Mathematical Sciences Department IBM T.J.Watson Research Center  
Abstract: Many problems in engineering and scientific domains require solving large sparse systems of linear equations, as a computationally intensive step towards the final solution. It has long been a challenge to develop efficient parallel formulations of sparse direct solvers due to several different complex steps involved in the process. In this paper, we describe PSPASES, one of the first efficient, portable, and robust scalable parallel solvers for sparse symmetric positive definite linear systems that we have developed. We discuss the algorithmic and implementation issues involved in its development; and present performance and scalability results on Cray T3E and SGI Origin 2000. PSPASES could solve the largest sparse system (1 million equations) ever solved by a direct method, with the highest performance (51 GFLOPS for Cholesky factorization) ever reported. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anshul Gupta, George Karypis and Vipin Kumar, </author> <title> Highly Scalable Parallel Algorithms for Sparse Matrix Factorization, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 8, no. 5, </volume> <pages> pp. 502-520, </pages> <year> 1997. </year>
Reference-contexts: In this paper, we describe one of the first scalable and high performance parallel direct solvers for sparse linear systems involving SPD matrices. At the heart of this solver is a highly parallel algorithm based on multifrontal Cholesky factorization we recently developed <ref> [1] </ref>. This algorithm is able to achieve high computational rates (of over 50 GFLOPS on a 256 processor Cray T3E) and it successfully parallelizes the most computationally expensive phase of the sparse solver. <p> We use our highly scalable algorithm <ref> [1] </ref> that is based on the multifrontal algorithm [8]. Given a sparse matrix and the associated elimination tree, the multifrontal algorithm can be recursively formulated as follows. Consider an N fiN matrix A. The algorithm performs a postorder traversal of the elimination tree associated with A. <p> The subcubes of processors working on various subtrees are shown in the form of a logical mesh labeled with P. The frontal matrix of each supernode is distributed among this logical mesh using a bitmask based block-cyclic scheme <ref> [1] </ref>. Figure 2 (b) shows such a distribution for unit blocksize. This distribution ensures that the extend-add operations required by the multifrontal algorithm can be performed in parallel with each processor exchanging roughly half of its data only with its partner from the other subcube.
Reference: [2] <author> Anshul Gupta, </author> <title> Analysis and Design of Scalable Parallel Algorithms for Scientific Computing, </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, </institution> <year> 1995. </year>
Reference-contexts: The number of non-zeros in L is N log N for 2-D problems, and N 4=3 for 3-D problems. Using this information, the parallel algorithms presented earlier in this section have been analyzed for their parallel runtime. For details, refer to <ref> [2, 3] </ref>. Table 1 shows the serial runtime complexity, parallel overhead, and isoefficiency functions [7] for all four phases. It can be seen that all the phases are scalable to varying degrees.
Reference: [3] <author> Mahesh Joshi, Anshul Gupta, George Karypis and Vipin Kumar, </author> <title> A High Performance Two Dimensional Scalable Parallel Algorithm for Solving Sparse Triangular Systems, </title> <booktitle> Proc. of 4th Int. Conf. on High Performance Computing (HiPC'97), </booktitle> <address> Bangalore, India, </address> <year> 1997. </year>
Reference-contexts: Both algorithms are able to effectively parallelize the corresponding phases. In particular, our parallel forward elimination and parallel backward substitution algorithms are scalable and achieve high computational rates <ref> [3] </ref>. We have integrated all these algorithms into a software library, PSPASES ( Parallel SPArse Symmetric dirEct Solver). It uses the MPI library for communication, making it portable to a wide range of parallel computers. <p> This update vector needs to be sent to the processors that store the first column of the L matrix of the parent supernode. Because of the bitmask based block-cyclic distribution, this can be done by using at most two communication steps <ref> [3] </ref>. The details of the two-dimensional pipelined dense forward elimination algorithm are illustrated in Figure 4 (b) for a hypothetical supernode. The solutions are computed by the processors owning diagonal elements of L matrix and flow down along a column. <p> First, the computation proceeds from the top supernode of the tree down to the leaf. Second, the computed solution that gets communicated across the levels of the supernodal tree instead of accumulated updates and this is achieved with at most one communication per processor. Refer to <ref> [3] </ref> for details. 2.5 Parallel Runtime and Scalability Analysis Analyzing a general sparse solver is a difficult problem. We present the analysis for two wide classes of sparse systems arising out of two-dimensional and three-dimensional constant node-degree graphs. We refer to these problems as 2-D and 3-D problems, respectively. <p> The number of non-zeros in L is N log N for 2-D problems, and N 4=3 for 3-D problems. Using this information, the parallel algorithms presented earlier in this section have been analyzed for their parallel runtime. For details, refer to <ref> [2, 3] </ref>. Table 1 shows the serial runtime complexity, parallel overhead, and isoefficiency functions [7] for all four phases. It can be seen that all the phases are scalable to varying degrees.
Reference: [4] <author> Mahesh Joshi, George Karypis, Vipin Kumar, Anshul Gupta, and Fred Gustavson, PSPASES: </author> <title> Scalable Parallel Direct Solver Library for Sparse Symmetric Positive Definite Linear Systems (Version 1.0), </title> <note> User's Manual, Available via http://www.cs.umn.edu/~mjoshi/pspases. </note>
Reference-contexts: IsoEff is the isoefficiency function indicating the rate at which the problem size should increase with the number of processors to achieve same efficiency. 3 Library Implementation and Experimental Results We have implemented all the parallel algorithms presented in previous section, and integrated them into a library called PSPASES <ref> [4] </ref>. This library uses MPI call interface for communication, and BLAS interface for computation within a processor. This makes PSPASES portable to a wide range of parallel computers. Furthermore, using BLAS, it is able to achieve high computational performance especially on the platforms with vendor-tuned BLAS libraries.
Reference: [5] <author> George Karypis and Vipin Kumar, </author> <title> Parallel Algorithm for Multilevel Graph Partitioning and Sparse Matrix Ordering, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 48, </volume> <pages> pp. 71-95, </pages> <year> 1998. </year>
Reference-contexts: This algorithm is able to achieve high computational rates (of over 50 GFLOPS on a 256 processor Cray T3E) and it successfully parallelizes the most computationally expensive phase of the sparse solver. Fill reducing ordering is obtained using a parallel formulation of the multilevel nested dissection algorithm <ref> [5] </ref> that has been found to be effective in producing orderings that are suited for parallel factorization. For symbolic factorization and solution of triangular systems, we have developed parallel algorithms that utilize the same data distribution as used by the numerical factorization algorithm. <p> The matrix A is converted to its equivalent graph, such that each row i is mapped to a vertex i of the graph and each nonzero a ij (i 6= j) is mapped to an edge between vertices i and j. A parallel graph partitioning algorithm <ref> [5] </ref> is used to compute a high quality p-way partition, where p is the number of processors. The graph is then redistributed according to this partition. <p> Finally, a global numbering is imposed on all nodes such that nodes in each separator and each subgraph are numbered consecutively, respecting the MMD ordering. Details of the multilevel paradigm can be found in <ref> [5] </ref>. Briefly, it involves gradual coarsening of a graph to a few hundred vertices, then partitioning this smaller graph, and finally, projecting the partitions back to the original graph by gradually refining them. Since the finer graph has more degrees of freedom, such refinements improve the quality of the partition.
Reference: [6] <author> George Karypis, Kirk Schlogel, and Vipin Kumar ParMetis: </author> <title> Parallel Graph Partitioning and Sparse Matrix Ordering Library, </title> <note> (Version 1.0), User's Manual, Available via http://www.cs.umn.edu/~metis. </note>
Reference-contexts: We used native versions of both the MPI and BLAS libraries. The notations used in presenting the results are described in Table 2. We give numbers for the numerical factorization and triangular solve phases only. The parallel ordering algorithm we use is incorporated into a separate library called ParMetis <ref> [6] </ref>, and PSPASES library functions just call the ParMetis library functions. ParMetis performance numbers are reported elsewhere [6]. The symbolic factorization took relatively very small time, and hence, we do not report those numbers here. <p> We give numbers for the numerical factorization and triangular solve phases only. The parallel ordering algorithm we use is incorporated into a separate library called ParMetis <ref> [6] </ref>, and PSPASES library functions just call the ParMetis library functions. ParMetis performance numbers are reported elsewhere [6]. The symbolic factorization took relatively very small time, and hence, we do not report those numbers here. It should be noted that the blocksize of distributing supernodal matrices and the load balance of the supernodal tree played important roles in determining the performance of the solver.
Reference: [7] <author> Vipin Kumar, Ananth Grama, Anshul Gupta and George Karypis, </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms Benjamin/Cummings, </title> <address> Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: Using this information, the parallel algorithms presented earlier in this section have been analyzed for their parallel runtime. For details, refer to [2, 3]. Table 1 shows the serial runtime complexity, parallel overhead, and isoefficiency functions <ref> [7] </ref> for all four phases. It can be seen that all the phases are scalable to varying degrees. <p> Since the overall time complexity is dominated by the numerical factorization phase, the isoefficiency function of the entire parallel solver is determined by the numerical factorization phase, and it is O (p 1:5 ) for both 2-D and 3-D problems. As discussed in <ref> [7] </ref>, the isoefficiency function of the dense Cholesky factorization algorithm is also O (p 1:5 ).
Reference: [8] <author> Joseph W. H. Liu, </author> <title> The Multifrontal Method for Sparse Matrix Solution: </title> <journal> Theory and Practice, SIAM Review, </journal> <volume> vol. 34, no.1, </volume> <pages> pp. 82-109, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: We use our highly scalable algorithm [1] that is based on the multifrontal algorithm <ref> [8] </ref>. Given a sparse matrix and the associated elimination tree, the multifrontal algorithm can be recursively formulated as follows. Consider an N fiN matrix A. The algorithm performs a postorder traversal of the elimination tree associated with A.

References-found: 8

