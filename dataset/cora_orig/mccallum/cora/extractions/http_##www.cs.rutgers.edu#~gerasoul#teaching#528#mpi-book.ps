URL: http://www.cs.rutgers.edu/~gerasoul/teaching/528/mpi-book.ps
Refering-URL: http://www.cs.rutgers.edu/~gerasoul/teaching/528/index.html
Root-URL: http://www.cs.rutgers.edu
Title: MPI: The Complete Reference  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> V. Bala and S. Kipnis. </author> <title> Process groups: a mechanism for the coordination of and communication among processes in the Venus collective communication library. </title> <type> Technical report, </type> <institution> IBM T. J. Watson Research Center, </institution> <month> October </month> <year> 1992. </year> <type> Preprint. </type>
Reference-contexts: char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; MPI_Comm comm; MPI_Datatype Particletype; MPI_Datatype type [3] = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3]; /* compute displacements */ MPI_Address (particle, &disp [0]); MPI_Address (particle [0].d, &disp <ref> [1] </ref>); MPI_Address (particle [0].b, &disp [2]); for (i=2; i &gt;= 0; i--) disp [i] -= disp [0]; /* build datatype */ 130 Chapter 3 MPI_Type_struct (3, blocklen, disp, type, &Particletype); MPI_Type_commit (&Particletype); ... /* send the entire array */ MPI_Send (particle, 1000, Particletype, dest, tag, comm); ... Advice to implementors. <p> -; struct Partstruct particle [1000]; int i, dest, rank; 132 Chapter 3 MPI_Comm comm; MPI_Datatype Particletype; MPI_Datatype type [4] = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR, MPI_UB-; int blocklen [4] = -1, 6, 7, 1-; MPI_Aint disp [4]; /* compute displacements of structure components */ MPI_Address (particle, &disp [0]); MPI_Address (particle [0].d, &disp <ref> [1] </ref>); MPI_Address (particle [0].b, &disp [2]); MPI_Address (particle [1], &disp [3]); for (i=3; i &gt;= 0; i--) disp [i] -= disp [0]; /* build datatype for structure */ MPI_Type_struct (4, blocklen, disp, type, &Particletype); MPI_Type_commit (&Particletype); /* send the entire array */ MPI_Send (particle, 1000, Particletype, dest, tag, comm); The two <p> rank; 132 Chapter 3 MPI_Comm comm; MPI_Datatype Particletype; MPI_Datatype type [4] = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR, MPI_UB-; int blocklen [4] = -1, 6, 7, 1-; MPI_Aint disp [4]; /* compute displacements of structure components */ MPI_Address (particle, &disp [0]); MPI_Address (particle [0].d, &disp <ref> [1] </ref>); MPI_Address (particle [0].b, &disp [2]); MPI_Address (particle [1], &disp [3]); for (i=3; i &gt;= 0; i--) disp [i] -= disp [0]; /* build datatype for structure */ MPI_Type_struct (4, blocklen, disp, type, &Particletype); MPI_Type_commit (&Particletype); /* send the entire array */ MPI_Send (particle, 1000, Particletype, dest, tag, comm); The two functions below can be used for finding the <p> Example 3.25 The following two programs generate identical messages. Derived datatype is used: int i; char c [100]; int disp [2]; int blocklen [2] = -1, 100-MPI_Datatype type [2] = -MPI_INT, MPI_CHAR-; MPI_Datatype Type; /* create datatype */ MPI_Address (&i, &disp [0]); MPI_Address (c, &disp <ref> [1] </ref>); MPI_Type_struct (2, blocklen, disp, type, &Type); MPI_Type_commit (&Type); /* send */ MPI_Send (MPI_BOTTOM, 1, Type, 1, 0, MPI_COMM_WORLD); Packing is used: int i; char c [100]; char buffer [110]; int position = 0; /* pack */ MPI_Pack (&i, 1, MPI_INT, buffer, 110,&position, MPI_COMM_WORLD); User-Defined Datatypes and Packing 139 MPI_Pack (c, <p> The outcome will be identical. Derived datatype is used: int i; char c [100]; MPI_status status; int disp [2]; int blocklen [2] = -1, 100-MPI_Datatype type [2] = -MPI_INT, MPI_CHAR-; MPI_Datatype Type; /* create datatype */ MPI_Address (&i, &disp [0]); MPI_Address (c, &disp <ref> [1] </ref>); MPI_Type_struct (2, blocklen, disp, type, &Type); MPI_Type_commit (&Type); /* receive */ MPI_Recv (MPI_BOTTOM, 1, Type, 0, 0, MPI_COMM_WORLD, &status); Unpacking is used: int i; char c [100]; 140 Chapter 3 MPI_Status status; char buffer [110]; int position = 0; /* receive */ MPI_Recv (buffer, 110, MPI_PACKED, 1, 0, MPI_COMM_WORLD, &status); <p> (int *)malloc (gsize*stride*sizeof (int)); displs = (int *)malloc (gsize*sizeof (int)); rcounts = (int *)malloc (gsize*sizeof (int)); for (i=0; i&lt;gsize; ++i) - displs [i] = i*stride; 162 Chapter 4 rcounts [i] = 100-i; - /* Create datatype for one int, with extent of entire row */ disp [0] = 0; disp <ref> [1] </ref> = 150*sizeof (int); type [0] = MPI_INT; type [1] = MPI_UB; blocklen [0] = 1; blocklen [1] = 1; MPI_Type_struct ( 2, blocklen, disp, type, &stype ); MPI_Type_commit ( &stype ); sptr = &sendarray [0][myrank]; MPI_Gatherv ( sptr, 100-myrank, stype, rbuf, rcounts, displs, MPI_INT, root, comm); Example 4.9 Same as <p> (int)); rcounts = (int *)malloc (gsize*sizeof (int)); for (i=0; i&lt;gsize; ++i) - displs [i] = i*stride; 162 Chapter 4 rcounts [i] = 100-i; - /* Create datatype for one int, with extent of entire row */ disp [0] = 0; disp <ref> [1] </ref> = 150*sizeof (int); type [0] = MPI_INT; type [1] = MPI_UB; blocklen [0] = 1; blocklen [1] = 1; MPI_Type_struct ( 2, blocklen, disp, type, &stype ); MPI_Type_commit ( &stype ); sptr = &sendarray [0][myrank]; MPI_Gatherv ( sptr, 100-myrank, stype, rbuf, rcounts, displs, MPI_INT, root, comm); Example 4.9 Same as Example 4.7 at sending side, but at receiving side <p> (i=0; i&lt;gsize; ++i) - displs [i] = i*stride; 162 Chapter 4 rcounts [i] = 100-i; - /* Create datatype for one int, with extent of entire row */ disp [0] = 0; disp <ref> [1] </ref> = 150*sizeof (int); type [0] = MPI_INT; type [1] = MPI_UB; blocklen [0] = 1; blocklen [1] = 1; MPI_Type_struct ( 2, blocklen, disp, type, &stype ); MPI_Type_commit ( &stype ); sptr = &sendarray [0][myrank]; MPI_Gatherv ( sptr, 100-myrank, stype, rbuf, rcounts, displs, MPI_INT, root, comm); Example 4.9 Same as Example 4.7 at sending side, but at receiving side we make the stride between received blocks vary <p> (gsize*sizeof (int)); displs [0] = 0; for (i=1; i&lt;gsize; ++i) - displs [i] = displs [i-1]+rcounts [i-1]; - /* And, create receive buffer */ rbuf = (int *)malloc (gsize*(displs [gsize-1]+rcounts [gsize-1]) *sizeof (int)); /* Create datatype for one int, with extent of entire row */ disp [0] = 0; disp <ref> [1] </ref> = 150*sizeof (int); type [0] = MPI_INT; type [1] = MPI_UB; blocklen [0] = 1; blocklen [1] = 1; MPI_Type_struct ( 2, blocklen, disp, type, &stype ); MPI_Type_commit ( &stype ); sptr = &sendarray [0][myrank]; MPI_Gatherv ( sptr, num, stype, rbuf, rcounts, displs, MPI_INT, root, comm); Collective Communications 165 4.7 <p> ++i) - displs [i] = displs [i-1]+rcounts [i-1]; - /* And, create receive buffer */ rbuf = (int *)malloc (gsize*(displs [gsize-1]+rcounts [gsize-1]) *sizeof (int)); /* Create datatype for one int, with extent of entire row */ disp [0] = 0; disp <ref> [1] </ref> = 150*sizeof (int); type [0] = MPI_INT; type [1] = MPI_UB; blocklen [0] = 1; blocklen [1] = 1; MPI_Type_struct ( 2, blocklen, disp, type, &stype ); MPI_Type_commit ( &stype ); sptr = &sendarray [0][myrank]; MPI_Gatherv ( sptr, num, stype, rbuf, rcounts, displs, MPI_INT, root, comm); Collective Communications 165 4.7 Scatter MPI SCATTER ( sendbuf, sendcount, sendtype, recvbuf, recvcount, <p> - /* And, create receive buffer */ rbuf = (int *)malloc (gsize*(displs [gsize-1]+rcounts [gsize-1]) *sizeof (int)); /* Create datatype for one int, with extent of entire row */ disp [0] = 0; disp <ref> [1] </ref> = 150*sizeof (int); type [0] = MPI_INT; type [1] = MPI_UB; blocklen [0] = 1; blocklen [1] = 1; MPI_Type_struct ( 2, blocklen, disp, type, &stype ); MPI_Type_commit ( &stype ); sptr = &sendarray [0][myrank]; MPI_Gatherv ( sptr, num, stype, rbuf, rcounts, displs, MPI_INT, root, comm); Collective Communications 165 4.7 Scatter MPI SCATTER ( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, root, comm) IN sendbuf address of send <p> MPI_TYPE_CONTIGUOUS (2, MPI_REAL, MPI_2REAL) Similar statements apply for MPI 2INTEGER, MPI 2DOUBLE PRECISION, and MPI 2INT. The datatype MPI FLOAT INT is as if defined by the following sequence of instructions. type [0] = MPI_FLOAT type <ref> [1] </ref> = MPI_INT disp [0] = 0 disp [1] = sizeof (float) block [0] = 1 block [1] = 1 MPI_TYPE_STRUCT (2, block, disp, type, MPI_FLOAT_INT) Similar statements apply for the other mixed types in C. Example 4.17 Each process has an array of 30 doubles, in C. <p> MPI_TYPE_CONTIGUOUS (2, MPI_REAL, MPI_2REAL) Similar statements apply for MPI 2INTEGER, MPI 2DOUBLE PRECISION, and MPI 2INT. The datatype MPI FLOAT INT is as if defined by the following sequence of instructions. type [0] = MPI_FLOAT type <ref> [1] </ref> = MPI_INT disp [0] = 0 disp [1] = sizeof (float) block [0] = 1 block [1] = 1 MPI_TYPE_STRUCT (2, block, disp, type, MPI_FLOAT_INT) Similar statements apply for the other mixed types in C. Example 4.17 Each process has an array of 30 doubles, in C. <p> The datatype MPI FLOAT INT is as if defined by the following sequence of instructions. type [0] = MPI_FLOAT type <ref> [1] </ref> = MPI_INT disp [0] = 0 disp [1] = sizeof (float) block [0] = 1 block [1] = 1 MPI_TYPE_STRUCT (2, block, disp, type, MPI_FLOAT_INT) Similar statements apply for the other mixed types in C. Example 4.17 Each process has an array of 30 doubles, in C. <p> Group *newgroup) MPI GROUP EXCL (GROUP, N, RANKS, NEWGROUP, IERROR) INTEGER GROUP, N, RANKS (*), NEWGROUP, IERROR The function MPI GROUP EXCL creates a group of processes newgroup that is obtained by deleting from group those processes with ranks ranks [0],: : : , ranks [n-1] in C or ranks <ref> [1] </ref>,: : : , ranks [n] in Fortran. The ordering of processes in newgroup is identical to the ordering in group. Each of the n elements of ranks must be a valid rank in group and all elements must be distinct; otherwise, the call is erroneous. <p> See Figure 5.10 #include &lt;mpi.h&gt; main (int argc, char **argv) - MPI_Comm myComm; /* intra-communicator of local sub-group */ MPI_Comm myFirstComm; /* inter-communicator */ MPI_Comm mySecondComm; /* second inter-communicator (group 1 only) */ int membershipKey, rank; MPI_Init (&argc, &argv); MPI_Comm_rank (MPI_COMM_WORLD, &rank); /* Generate membershipKey in the range <ref> [0, 1, 2] </ref> */ membershipKey = rank % 3; /* Build intra-communicator for local sub-group */ MPI_Comm_split (MPI_COMM_WORLD, membershipKey, rank, &myComm); /* Build inter-communicators.
Reference: [2] <author> V. Bala, S. Kipnis, L. Rudolph, and Marc Snir. </author> <title> Designing efficient, scalable, and portable collective communication libraries. </title> <booktitle> In SIAM 1993 Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 862-872, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; MPI_Comm comm; MPI_Datatype Particletype; MPI_Datatype type [3] = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3]; /* compute displacements */ MPI_Address (particle, &disp [0]); MPI_Address (particle [0].d, &disp [1]); MPI_Address (particle [0].b, &disp <ref> [2] </ref>); for (i=2; i &gt;= 0; i--) disp [i] -= disp [0]; /* build datatype */ 130 Chapter 3 MPI_Type_struct (3, blocklen, disp, type, &Particletype); MPI_Type_commit (&Particletype); ... /* send the entire array */ MPI_Send (particle, 1000, Particletype, dest, tag, comm); ... Advice to implementors. <p> int i, dest, rank; 132 Chapter 3 MPI_Comm comm; MPI_Datatype Particletype; MPI_Datatype type [4] = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR, MPI_UB-; int blocklen [4] = -1, 6, 7, 1-; MPI_Aint disp [4]; /* compute displacements of structure components */ MPI_Address (particle, &disp [0]); MPI_Address (particle [0].d, &disp [1]); MPI_Address (particle [0].b, &disp <ref> [2] </ref>); MPI_Address (particle [1], &disp [3]); for (i=3; i &gt;= 0; i--) disp [i] -= disp [0]; /* build datatype for structure */ MPI_Type_struct (4, blocklen, disp, type, &Particletype); MPI_Type_commit (&Particletype); /* send the entire array */ MPI_Send (particle, 1000, Particletype, dest, tag, comm); The two functions below can be used <p> The message can be received with any datatype that matches this send datatype. Example 3.25 The following two programs generate identical messages. Derived datatype is used: int i; char c [100]; int disp <ref> [2] </ref>; int blocklen [2] = -1, 100-MPI_Datatype type [2] = -MPI_INT, MPI_CHAR-; MPI_Datatype Type; /* create datatype */ MPI_Address (&i, &disp [0]); MPI_Address (c, &disp [1]); MPI_Type_struct (2, blocklen, disp, type, &Type); MPI_Type_commit (&Type); /* send */ MPI_Send (MPI_BOTTOM, 1, Type, 1, 0, MPI_COMM_WORLD); Packing is used: int i; char c <p> The message can be received with any datatype that matches this send datatype. Example 3.25 The following two programs generate identical messages. Derived datatype is used: int i; char c [100]; int disp <ref> [2] </ref>; int blocklen [2] = -1, 100-MPI_Datatype type [2] = -MPI_INT, MPI_CHAR-; MPI_Datatype Type; /* create datatype */ MPI_Address (&i, &disp [0]); MPI_Address (c, &disp [1]); MPI_Type_struct (2, blocklen, disp, type, &Type); MPI_Type_commit (&Type); /* send */ MPI_Send (MPI_BOTTOM, 1, Type, 1, 0, MPI_COMM_WORLD); Packing is used: int i; char c [100]; char buffer <p> The message can be received with any datatype that matches this send datatype. Example 3.25 The following two programs generate identical messages. Derived datatype is used: int i; char c [100]; int disp <ref> [2] </ref>; int blocklen [2] = -1, 100-MPI_Datatype type [2] = -MPI_INT, MPI_CHAR-; MPI_Datatype Type; /* create datatype */ MPI_Address (&i, &disp [0]); MPI_Address (c, &disp [1]); MPI_Type_struct (2, blocklen, disp, type, &Type); MPI_Type_commit (&Type); /* send */ MPI_Send (MPI_BOTTOM, 1, Type, 1, 0, MPI_COMM_WORLD); Packing is used: int i; char c [100]; char buffer [110]; int position = 0; <p> Example 3.26 Any of the following two programs can be used to receive the message sent in Example 3.25. The outcome will be identical. Derived datatype is used: int i; char c [100]; MPI_status status; int disp <ref> [2] </ref>; int blocklen [2] = -1, 100-MPI_Datatype type [2] = -MPI_INT, MPI_CHAR-; MPI_Datatype Type; /* create datatype */ MPI_Address (&i, &disp [0]); MPI_Address (c, &disp [1]); MPI_Type_struct (2, blocklen, disp, type, &Type); MPI_Type_commit (&Type); /* receive */ MPI_Recv (MPI_BOTTOM, 1, Type, 0, 0, MPI_COMM_WORLD, &status); Unpacking is used: int i; char <p> Example 3.26 Any of the following two programs can be used to receive the message sent in Example 3.25. The outcome will be identical. Derived datatype is used: int i; char c [100]; MPI_status status; int disp <ref> [2] </ref>; int blocklen [2] = -1, 100-MPI_Datatype type [2] = -MPI_INT, MPI_CHAR-; MPI_Datatype Type; /* create datatype */ MPI_Address (&i, &disp [0]); MPI_Address (c, &disp [1]); MPI_Type_struct (2, blocklen, disp, type, &Type); MPI_Type_commit (&Type); /* receive */ MPI_Recv (MPI_BOTTOM, 1, Type, 0, 0, MPI_COMM_WORLD, &status); Unpacking is used: int i; char c [100]; 140 <p> Example 3.26 Any of the following two programs can be used to receive the message sent in Example 3.25. The outcome will be identical. Derived datatype is used: int i; char c [100]; MPI_status status; int disp <ref> [2] </ref>; int blocklen [2] = -1, 100-MPI_Datatype type [2] = -MPI_INT, MPI_CHAR-; MPI_Datatype Type; /* create datatype */ MPI_Address (&i, &disp [0]); MPI_Address (c, &disp [1]); MPI_Type_struct (2, blocklen, disp, type, &Type); MPI_Type_commit (&Type); /* receive */ MPI_Recv (MPI_BOTTOM, 1, Type, 0, 0, MPI_COMM_WORLD, &status); Unpacking is used: int i; char c [100]; 140 Chapter 3 MPI_Status status; char <p> particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, size, myrank; int position = 0; MPI_Status status; char buffer [BUFSIZE]; /* pack-unpack buffer */ /* variables used to create datatype for particle, not including class field */ MPI_Datatype Particletype; MPI_Datatype type <ref> [2] </ref> = -MPI_DOUBLE, MPI_CHAR-; int blocklen [2] = -6, 7-; MPI_Aint disp [2] = -0, 6*sizeof (double)-; /* define datatype */ MPI_Type_struct (2, blocklen, disp, type, &Particletype); MPI_Type_commit (&Particletype); MPI_Comm_rank (MPI_COMM_WORLD, &myrank); if (myrank == 0) - /* send message that consists of class zero particles */ /* pack class zero <p> /* some additional information */ -; struct Partstruct particle [1000]; int i, size, myrank; int position = 0; MPI_Status status; char buffer [BUFSIZE]; /* pack-unpack buffer */ /* variables used to create datatype for particle, not including class field */ MPI_Datatype Particletype; MPI_Datatype type <ref> [2] </ref> = -MPI_DOUBLE, MPI_CHAR-; int blocklen [2] = -6, 7-; MPI_Aint disp [2] = -0, 6*sizeof (double)-; /* define datatype */ MPI_Type_struct (2, blocklen, disp, type, &Particletype); MPI_Type_commit (&Particletype); MPI_Comm_rank (MPI_COMM_WORLD, &myrank); if (myrank == 0) - /* send message that consists of class zero particles */ /* pack class zero particles and their index */ for <p> struct Partstruct particle [1000]; int i, size, myrank; int position = 0; MPI_Status status; char buffer [BUFSIZE]; /* pack-unpack buffer */ /* variables used to create datatype for particle, not including class field */ MPI_Datatype Particletype; MPI_Datatype type <ref> [2] </ref> = -MPI_DOUBLE, MPI_CHAR-; int blocklen [2] = -6, 7-; MPI_Aint disp [2] = -0, 6*sizeof (double)-; /* define datatype */ MPI_Type_struct (2, blocklen, disp, type, &Particletype); MPI_Type_commit (&Particletype); MPI_Comm_rank (MPI_COMM_WORLD, &myrank); if (myrank == 0) - /* send message that consists of class zero particles */ /* pack class zero particles and their index */ for (i=0; i &lt; 1000; i++) if <p> We create a datatype that causes the correct striding at the sending end so that that we read a column of a C array. MPI_Comm comm; int gsize,sendarray [100][150],*sptr; int root, *rbuf, stride, myrank, disp <ref> [2] </ref>, blocklen [2]; MPI_Datatype stype,type [2]; int *displs,i,*rcounts; ... <p> We create a datatype that causes the correct striding at the sending end so that that we read a column of a C array. MPI_Comm comm; int gsize,sendarray [100][150],*sptr; int root, *rbuf, stride, myrank, disp <ref> [2] </ref>, blocklen [2]; MPI_Datatype stype,type [2]; int *displs,i,*rcounts; ... <p> We create a datatype that causes the correct striding at the sending end so that that we read a column of a C array. MPI_Comm comm; int gsize,sendarray [100][150],*sptr; int root, *rbuf, stride, myrank, disp <ref> [2] </ref>, blocklen [2]; MPI_Datatype stype,type [2]; int *displs,i,*rcounts; ... <p> The complicating factor is that the various values of num are not known to root, so a separate gather must first be run to find these out. The data is placed contiguously at the receiving end. MPI_Comm comm; int gsize,sendarray [100][150],*sptr; int root, *rbuf, stride, myrank, disp <ref> [2] </ref>, blocklen [2]; MPI_Datatype stype,types [2]; 164 Chapter 4 int *displs,i,*rcounts,num; ... <p> The complicating factor is that the various values of num are not known to root, so a separate gather must first be run to find these out. The data is placed contiguously at the receiving end. MPI_Comm comm; int gsize,sendarray [100][150],*sptr; int root, *rbuf, stride, myrank, disp <ref> [2] </ref>, blocklen [2]; MPI_Datatype stype,types [2]; 164 Chapter 4 int *displs,i,*rcounts,num; ... <p> The data is placed contiguously at the receiving end. MPI_Comm comm; int gsize,sendarray [100][150],*sptr; int root, *rbuf, stride, myrank, disp <ref> [2] </ref>, blocklen [2]; MPI_Datatype stype,types [2]; 164 Chapter 4 int *displs,i,*rcounts,num; ... <p> When using this operator, we must be careful * to specify that it is non-commutative, as in the following. */ int i,base; SeqScanPair a, answer; MPI_Op myOp; MPI_Datatype type <ref> [2] </ref> = -MPI_DOUBLE, MPI_INT-; MPI_Aint disp [2]; int blocklen [2] = - 1, 1-; MPI_Datatype sspair; /* explain to MPI how type SegScanPair is defined */ MPI_Address ( a, disp); MPI_Address ( a.log, disp+1); base = disp [0]; for (i=0; i&lt;2; ++i) disp [i] -= base; MPI_Type_struct ( 2, blocklen, disp, <p> When using this operator, we must be careful * to specify that it is non-commutative, as in the following. */ int i,base; SeqScanPair a, answer; MPI_Op myOp; MPI_Datatype type <ref> [2] </ref> = -MPI_DOUBLE, MPI_INT-; MPI_Aint disp [2]; int blocklen [2] = - 1, 1-; MPI_Datatype sspair; /* explain to MPI how type SegScanPair is defined */ MPI_Address ( a, disp); MPI_Address ( a.log, disp+1); base = disp [0]; for (i=0; i&lt;2; ++i) disp [i] -= base; MPI_Type_struct ( 2, blocklen, disp, type, &sspair ); MPI_Type_commit ( &sspair <p> When using this operator, we must be careful * to specify that it is non-commutative, as in the following. */ int i,base; SeqScanPair a, answer; MPI_Op myOp; MPI_Datatype type <ref> [2] </ref> = -MPI_DOUBLE, MPI_INT-; MPI_Aint disp [2]; int blocklen [2] = - 1, 1-; MPI_Datatype sspair; /* explain to MPI how type SegScanPair is defined */ MPI_Address ( a, disp); MPI_Address ( a.log, disp+1); base = disp [0]; for (i=0; i&lt;2; ++i) disp [i] -= base; MPI_Type_struct ( 2, blocklen, disp, type, &sspair ); MPI_Type_commit ( &sspair ); /* create <p> See Figure 5.10 #include &lt;mpi.h&gt; main (int argc, char **argv) - MPI_Comm myComm; /* intra-communicator of local sub-group */ MPI_Comm myFirstComm; /* inter-communicator */ MPI_Comm mySecondComm; /* second inter-communicator (group 1 only) */ int membershipKey, rank; MPI_Init (&argc, &argv); MPI_Comm_rank (MPI_COMM_WORLD, &rank); /* Generate membershipKey in the range <ref> [0, 1, 2] </ref> */ membershipKey = rank % 3; /* Build intra-communicator for local sub-group */ MPI_Comm_split (MPI_COMM_WORLD, membershipKey, rank, &myComm); /* Build inter-communicators.
Reference: [3] <author> Luc Bomans and Rolf Hempel. </author> <title> The Argonne/GMD macros in FORTRAN for portable parallel programming and their implementation on the Intel iPSC/2. </title> <journal> Parallel Computing, </journal> <volume> 15 </volume> <pages> 119-132, </pages> <year> 1990. </year>
Reference-contexts: Sending an array of structures. struct Partstruct - char class; /* particle class */ double d [6]; /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; MPI_Comm comm; /* build datatype describing structure */ MPI_Datatype Particletype; MPI_Datatype type <ref> [3] </ref> = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3] = -0, sizeof (double-, 7*sizeof (double)-; MPI_Type_struct (3, blocklen, disp, type, &Particletype); MPI_Type_commit (&Particletype); /* send the array */ MPI_Send (particle, 1000, Particletype, dest, tag, comm); The array disp was initialized assuming that a double is <p> - char class; /* particle class */ double d [6]; /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; MPI_Comm comm; /* build datatype describing structure */ MPI_Datatype Particletype; MPI_Datatype type <ref> [3] </ref> = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3] = -0, sizeof (double-, 7*sizeof (double)-; MPI_Type_struct (3, blocklen, disp, type, &Particletype); MPI_Type_commit (&Particletype); /* send the array */ MPI_Send (particle, 1000, Particletype, dest, tag, comm); The array disp was initialized assuming that a double is double-word aligned. <p> double d [6]; /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; MPI_Comm comm; /* build datatype describing structure */ MPI_Datatype Particletype; MPI_Datatype type <ref> [3] </ref> = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3] = -0, sizeof (double-, 7*sizeof (double)-; MPI_Type_struct (3, blocklen, disp, type, &Particletype); MPI_Type_commit (&Particletype); /* send the array */ MPI_Send (particle, 1000, Particletype, dest, tag, comm); The array disp was initialized assuming that a double is double-word aligned. <p> Process one receives these particles in contiguous locations. struct Partstruct - char class; /* particle class */ double d [6]; /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, j, myrank; MPI_Status status; MPI_Datatype Particletype; MPI_Datatype type <ref> [3] </ref> = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3] = -0, sizeof (double), 7*sizeof (double)-, sizeaint; int base; MPI_Datatype Zparticles; /* datatype describing all particles with class zero (needs to be recomputed if classes change) */ MPI_Aint *zdisp; int *zblocklen; MPI_Type_struct (3, blocklen, disp, type, <p> particles in contiguous locations. struct Partstruct - char class; /* particle class */ double d [6]; /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, j, myrank; MPI_Status status; MPI_Datatype Particletype; MPI_Datatype type <ref> [3] </ref> = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3] = -0, sizeof (double), 7*sizeof (double)-, sizeaint; int base; MPI_Datatype Zparticles; /* datatype describing all particles with class zero (needs to be recomputed if classes change) */ MPI_Aint *zdisp; int *zblocklen; MPI_Type_struct (3, blocklen, disp, type, &Particletype); MPI_Comm_rank (comm, &myrank); if (myrank == <p> char class; /* particle class */ double d [6]; /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, j, myrank; MPI_Status status; MPI_Datatype Particletype; MPI_Datatype type <ref> [3] </ref> = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3] = -0, sizeof (double), 7*sizeof (double)-, sizeaint; int base; MPI_Datatype Zparticles; /* datatype describing all particles with class zero (needs to be recomputed if classes change) */ MPI_Aint *zdisp; int *zblocklen; MPI_Type_struct (3, blocklen, disp, type, &Particletype); MPI_Comm_rank (comm, &myrank); if (myrank == 0) - /* send message consisting of <p> ADDRESS are used to compute the displacements of the structure components. struct Partstruct - char class; /* particle class */ double d [6]; /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; MPI_Comm comm; MPI_Datatype Particletype; MPI_Datatype type <ref> [3] </ref> = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3]; /* compute displacements */ MPI_Address (particle, &disp [0]); MPI_Address (particle [0].d, &disp [1]); MPI_Address (particle [0].b, &disp [2]); for (i=2; i &gt;= 0; i--) disp [i] -= disp [0]; /* build datatype */ 130 Chapter 3 <p> of the structure components. struct Partstruct - char class; /* particle class */ double d [6]; /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; MPI_Comm comm; MPI_Datatype Particletype; MPI_Datatype type <ref> [3] </ref> = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3]; /* compute displacements */ MPI_Address (particle, &disp [0]); MPI_Address (particle [0].d, &disp [1]); MPI_Address (particle [0].b, &disp [2]); for (i=2; i &gt;= 0; i--) disp [i] -= disp [0]; /* build datatype */ 130 Chapter 3 MPI_Type_struct (3, blocklen, disp, type, &Particletype); MPI_Type_commit <p> char class; /* particle class */ double d [6]; /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; MPI_Comm comm; MPI_Datatype Particletype; MPI_Datatype type <ref> [3] </ref> = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3]; /* compute displacements */ MPI_Address (particle, &disp [0]); MPI_Address (particle [0].d, &disp [1]); MPI_Address (particle [0].b, &disp [2]); for (i=2; i &gt;= 0; i--) disp [i] -= disp [0]; /* build datatype */ 130 Chapter 3 MPI_Type_struct (3, blocklen, disp, type, &Particletype); MPI_Type_commit (&Particletype); ... /* send the entire array <p> Chapter 3 MPI_Comm comm; MPI_Datatype Particletype; MPI_Datatype type [4] = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR, MPI_UB-; int blocklen [4] = -1, 6, 7, 1-; MPI_Aint disp [4]; /* compute displacements of structure components */ MPI_Address (particle, &disp [0]); MPI_Address (particle [0].d, &disp [1]); MPI_Address (particle [0].b, &disp [2]); MPI_Address (particle [1], &disp <ref> [3] </ref>); for (i=3; i &gt;= 0; i--) disp [i] -= disp [0]; /* build datatype for structure */ MPI_Type_struct (4, blocklen, disp, type, &Particletype); MPI_Type_commit (&Particletype); /* send the entire array */ MPI_Send (particle, 1000, Particletype, dest, tag, comm); The two functions below can be used for finding the lower bound <p> relative displacements. struct Partstruct - char class; /* particle class */ double d [6]; /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; 134 Chapter 3 MPI_Comm comm; /* build datatype describing structure */ MPI_Datatype Particletype; MPI_Datatype type <ref> [3] </ref> = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3]; /* compute addresses of components in 1st structure*/ MPI_Address (particle, disp); MPI_Address (particle [0].d, disp+1); MPI_Address (particle [0].b, disp+2); /* build datatype for 1st structure */ MPI_Type_struct (3, blocklen, disp, type, &Particletype); MPI_Type_commit (&Particletype); /* send <p> /* particle class */ double d [6]; /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; 134 Chapter 3 MPI_Comm comm; /* build datatype describing structure */ MPI_Datatype Particletype; MPI_Datatype type <ref> [3] </ref> = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3]; /* compute addresses of components in 1st structure*/ MPI_Address (particle, disp); MPI_Address (particle [0].d, disp+1); MPI_Address (particle [0].b, disp+2); /* build datatype for 1st structure */ MPI_Type_struct (3, blocklen, disp, type, &Particletype); MPI_Type_commit (&Particletype); /* send the entire array */ MPI_Send (MPI_BOTTOM, 1000, <p> /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; 134 Chapter 3 MPI_Comm comm; /* build datatype describing structure */ MPI_Datatype Particletype; MPI_Datatype type <ref> [3] </ref> = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3]; /* compute addresses of components in 1st structure*/ MPI_Address (particle, disp); MPI_Address (particle [0].d, disp+1); MPI_Address (particle [0].b, disp+2); /* build datatype for 1st structure */ MPI_Type_struct (3, blocklen, disp, type, &Particletype); MPI_Type_commit (&Particletype); /* send the entire array */ MPI_Send (MPI_BOTTOM, 1000, Particletype, dest, tag, comm); Advice to implementors. <p> coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, size, position, myrank; int count; /* number of class zero particles */ char *buffer; /* pack buffer */ MPI_Status status; /* variables used to create datatype for particle */ MPI_Datatype Particletype; MPI_Datatype type <ref> [3] </ref> = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3] = -0, sizeof (double), 7*sizeof (double)-; /* define datatype for one particle */ MPI_Type_struct ( 3, blocklen, disp, type, &Particletype); MPI_Type_commit ( &Particletype); MPI_Comm_rank (comm, &myrank); 142 Chapter 3 if (myrank == 0) - /* send <p> additional information */ -; struct Partstruct particle [1000]; int i, size, position, myrank; int count; /* number of class zero particles */ char *buffer; /* pack buffer */ MPI_Status status; /* variables used to create datatype for particle */ MPI_Datatype Particletype; MPI_Datatype type <ref> [3] </ref> = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3] = -0, sizeof (double), 7*sizeof (double)-; /* define datatype for one particle */ MPI_Type_struct ( 3, blocklen, disp, type, &Particletype); MPI_Type_commit ( &Particletype); MPI_Comm_rank (comm, &myrank); 142 Chapter 3 if (myrank == 0) - /* send message that consists of class zero particles <p> [1000]; int i, size, position, myrank; int count; /* number of class zero particles */ char *buffer; /* pack buffer */ MPI_Status status; /* variables used to create datatype for particle */ MPI_Datatype Particletype; MPI_Datatype type <ref> [3] </ref> = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3] = -0, sizeof (double), 7*sizeof (double)-; /* define datatype for one particle */ MPI_Type_struct ( 3, blocklen, disp, type, &Particletype); MPI_Type_commit ( &Particletype); MPI_Comm_rank (comm, &myrank); 142 Chapter 3 if (myrank == 0) - /* send message that consists of class zero particles */ /* allocate pack buffer */ MPI_Pack_size <p> This can lead to compromises when a specific topology may over- or under-specify the connectivity that is used at any time in the program. Overall, however, the chosen topology mechanism was seen as a useful compromise between functionality and ease of usage. Experience with similar techniques in PARMACS <ref> [3, 7] </ref> show that this information is usually sufficient for a good mapping. (End of rationale.) Process Topologies 255 Relationship between ranks and Cartesian coordinates for a 3x4 2D topology.
Reference: [4] <author> J. Bruck, R. Cypher, P. Elustond, A. Ho, C-T. Ho, V. Bala, S. Kipnis, , and M. Snir. </author> <title> Ccl: A portable and tunable collective communicationlibrary for scalable parallel computers. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 6(2) </volume> <pages> 154-164, </pages> <year> 1995. </year>
Reference-contexts: rather than trusting MPI to compute fills correctly. struct Partstruct - char class; /* particle class */ double d [6]; /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; 132 Chapter 3 MPI_Comm comm; MPI_Datatype Particletype; MPI_Datatype type <ref> [4] </ref> = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR, MPI_UB-; int blocklen [4] = -1, 6, 7, 1-; MPI_Aint disp [4]; /* compute displacements of structure components */ MPI_Address (particle, &disp [0]); MPI_Address (particle [0].d, &disp [1]); MPI_Address (particle [0].b, &disp [2]); MPI_Address (particle [1], &disp [3]); for (i=3; i &gt;= 0; i--) disp [i] <p> struct Partstruct - char class; /* particle class */ double d [6]; /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; 132 Chapter 3 MPI_Comm comm; MPI_Datatype Particletype; MPI_Datatype type <ref> [4] </ref> = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR, MPI_UB-; int blocklen [4] = -1, 6, 7, 1-; MPI_Aint disp [4]; /* compute displacements of structure components */ MPI_Address (particle, &disp [0]); MPI_Address (particle [0].d, &disp [1]); MPI_Address (particle [0].b, &disp [2]); MPI_Address (particle [1], &disp [3]); for (i=3; i &gt;= 0; i--) disp [i] -= disp [0]; /* build datatype for structure <p> */ double d [6]; /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; 132 Chapter 3 MPI_Comm comm; MPI_Datatype Particletype; MPI_Datatype type <ref> [4] </ref> = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR, MPI_UB-; int blocklen [4] = -1, 6, 7, 1-; MPI_Aint disp [4]; /* compute displacements of structure components */ MPI_Address (particle, &disp [0]); MPI_Address (particle [0].d, &disp [1]); MPI_Address (particle [0].b, &disp [2]); MPI_Address (particle [1], &disp [3]); for (i=3; i &gt;= 0; i--) disp [i] -= disp [0]; /* build datatype for structure */ MPI_Type_struct (4, blocklen, disp, type, &Particletype); MPI_Type_commit <p> An optimal implementation of collective communication will take advantage of the specifics of the underlying communication network (such as support for multicast, which can be used for MPI broadcast), and will use different algorithms, according to the number of participating processes and the amounts of data communicated. See, e.g. <ref> [4] </ref>. (End of advice to implementors.) 4.3 Communicator Argument The key concept of the collective functions is to have a "group" of participating processes. The routines do not have a group identifier as an explicit argument. Instead, there is a communicator argument.
Reference: [5] <author> R. Butler and E. Lusk. </author> <title> User's guide to the p4 programming system. </title> <type> Technical Report TM-ANL-92/17, </type> <institution> Argonne National Laboratory, </institution> <year> 1992. </year>
Reference: [6] <author> Ralph Butler and Ewing Lusk. </author> <title> Monitors, messages, and clusters: the p4 parallel programming system. </title> <journal> Journal of Parallel Computing, </journal> <volume> 20(4) </volume> <pages> 547-564, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The relative displacement between successive triplets of coordinates may not be a multiple of sizeof (double); therefore, the Hvector datatype constructor is used. struct Partstruct - char class; /* particle class */ double d <ref> [6] </ref>; /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; MPI_Comm comm; MPI_Datatype Locationtype; /* datatype for locations */ MPI_Type_hvector (1000, 3, sizeof (Partstruct), MPI_DOUBLE, &Locationtype); MPI_Type_commit (&Locationtype); MPI_Send (particle [0].d, 1, Locationtype, dest, tag, comm); 114 Chapter 3 <p> Example 3.14 Sending an array of structures. struct Partstruct - char class; /* particle class */ double d <ref> [6] </ref>; /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; MPI_Comm comm; /* build datatype describing structure */ MPI_Datatype Particletype; MPI_Datatype type [3] = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3] = -0, <p> Process one receives these particles in contiguous locations. struct Partstruct - char class; /* particle class */ double d <ref> [6] </ref>; /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, j, myrank; MPI_Status status; MPI_Datatype Particletype; MPI_Datatype type [3] = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3] = -0, sizeof (double), 7*sizeof (double)-, sizeaint; int <p> Calls to MPI ADDRESS are used to compute the displacements of the structure components. struct Partstruct - char class; /* particle class */ double d <ref> [6] </ref>; /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; MPI_Comm comm; MPI_Datatype Particletype; MPI_Datatype type [3] = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3]; /* compute displacements */ MPI_Address (particle, &disp [0]); <p> Example 3.23 We modify Example 3.21, so that the code explicitly sets the extent of Particletype to the right value, rather than trusting MPI to compute fills correctly. struct Partstruct - char class; /* particle class */ double d <ref> [6] </ref>; /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; 132 Chapter 3 MPI_Comm comm; MPI_Datatype Particletype; MPI_Datatype type [4] = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR, MPI_UB-; int blocklen [4] = -1, 6, 7, 1-; MPI_Aint disp [4]; /* compute displacements <p> Example 3.24 The code in Example 3.21 on page 129 is modified to use absolute addresses, rather than relative displacements. struct Partstruct - char class; /* particle class */ double d <ref> [6] </ref>; /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; 134 Chapter 3 MPI_Comm comm; /* build datatype describing structure */ MPI_Datatype Particletype; MPI_Datatype type [3] = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp <p> Process one receives and stores these structures in contiguous locations. Process zero uses calls to MPI PACK to gather class zero particles, whereas process one uses a regular receive. struct Partstruct - char class; /* particle class */ double d <ref> [6] </ref>; /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, size, position, myrank; int count; /* number of class zero particles */ char *buffer; /* pack buffer */ MPI_Status status; /* variables used to create datatype for particle */ MPI_Datatype Particletype; <p> One could be rigorous and define an additional derived datatype for the purpose of computing such an estimate. Or User-Defined Datatypes and Packing 143 one can use an approximate estimate.) struct Partstruct - char class; /* particle class */ double d <ref> [6] </ref>; /* particle coordinates */ char b [7]; /* some additional information */ -; struct Partstruct particle [1000]; int i, size, myrank; int position = 0; MPI_Status status; char buffer [BUFSIZE]; /* pack-unpack buffer */ /* variables used to create datatype for particle, not including class field */ MPI_Datatype Particletype; MPI_Datatype
Reference: [7] <author> Robin Calkin, Rolf Hempel, Hans-Christian Hoppe, and Peter Wypior. </author> <title> Portable programming with the parmacs message-passing library. </title> <journal> Parallel Computing, </journal> <volume> 20(4) </volume> <pages> 615-632, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The relative displacement between successive triplets of coordinates may not be a multiple of sizeof (double); therefore, the Hvector datatype constructor is used. struct Partstruct - char class; /* particle class */ double d [6]; /* particle coordinates */ char b <ref> [7] </ref>; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; MPI_Comm comm; MPI_Datatype Locationtype; /* datatype for locations */ MPI_Type_hvector (1000, 3, sizeof (Partstruct), MPI_DOUBLE, &Locationtype); MPI_Type_commit (&Locationtype); MPI_Send (particle [0].d, 1, Locationtype, dest, tag, comm); 114 Chapter 3 3.3.4 Indexed The Indexed constructor allows one <p> Example 3.14 Sending an array of structures. struct Partstruct - char class; /* particle class */ double d [6]; /* particle coordinates */ char b <ref> [7] </ref>; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; MPI_Comm comm; /* build datatype describing structure */ MPI_Datatype Particletype; MPI_Datatype type [3] = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3] = -0, sizeof (double-, 7*sizeof (double)-; MPI_Type_struct (3, blocklen, <p> Process one receives these particles in contiguous locations. struct Partstruct - char class; /* particle class */ double d [6]; /* particle coordinates */ char b <ref> [7] </ref>; /* some additional information */ -; struct Partstruct particle [1000]; int i, j, myrank; MPI_Status status; MPI_Datatype Particletype; MPI_Datatype type [3] = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3] = -0, sizeof (double), 7*sizeof (double)-, sizeaint; int base; MPI_Datatype Zparticles; /* datatype describing all <p> Calls to MPI ADDRESS are used to compute the displacements of the structure components. struct Partstruct - char class; /* particle class */ double d [6]; /* particle coordinates */ char b <ref> [7] </ref>; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; MPI_Comm comm; MPI_Datatype Particletype; MPI_Datatype type [3] = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3]; /* compute displacements */ MPI_Address (particle, &disp [0]); MPI_Address (particle [0].d, &disp [1]); MPI_Address (particle <p> Example 3.23 We modify Example 3.21, so that the code explicitly sets the extent of Particletype to the right value, rather than trusting MPI to compute fills correctly. struct Partstruct - char class; /* particle class */ double d [6]; /* particle coordinates */ char b <ref> [7] </ref>; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; 132 Chapter 3 MPI_Comm comm; MPI_Datatype Particletype; MPI_Datatype type [4] = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR, MPI_UB-; int blocklen [4] = -1, 6, 7, 1-; MPI_Aint disp [4]; /* compute displacements of structure components */ MPI_Address (particle, &disp <p> Example 3.24 The code in Example 3.21 on page 129 is modified to use absolute addresses, rather than relative displacements. struct Partstruct - char class; /* particle class */ double d [6]; /* particle coordinates */ char b <ref> [7] </ref>; /* some additional information */ -; struct Partstruct particle [1000]; int i, dest, rank; 134 Chapter 3 MPI_Comm comm; /* build datatype describing structure */ MPI_Datatype Particletype; MPI_Datatype type [3] = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR-; int blocklen [3] = -1, 6, 7-; MPI_Aint disp [3]; /* compute addresses of components in <p> Process one receives and stores these structures in contiguous locations. Process zero uses calls to MPI PACK to gather class zero particles, whereas process one uses a regular receive. struct Partstruct - char class; /* particle class */ double d [6]; /* particle coordinates */ char b <ref> [7] </ref>; /* some additional information */ -; struct Partstruct particle [1000]; int i, size, position, myrank; int count; /* number of class zero particles */ char *buffer; /* pack buffer */ MPI_Status status; /* variables used to create datatype for particle */ MPI_Datatype Particletype; MPI_Datatype type [3] = -MPI_CHAR, MPI_DOUBLE, MPI_CHAR-; <p> One could be rigorous and define an additional derived datatype for the purpose of computing such an estimate. Or User-Defined Datatypes and Packing 143 one can use an approximate estimate.) struct Partstruct - char class; /* particle class */ double d [6]; /* particle coordinates */ char b <ref> [7] </ref>; /* some additional information */ -; struct Partstruct particle [1000]; int i, size, myrank; int position = 0; MPI_Status status; char buffer [BUFSIZE]; /* pack-unpack buffer */ /* variables used to create datatype for particle, not including class field */ MPI_Datatype Particletype; MPI_Datatype type [2] = -MPI_DOUBLE, MPI_CHAR-; int blocklen <p> This can lead to compromises when a specific topology may over- or under-specify the connectivity that is used at any time in the program. Overall, however, the chosen topology mechanism was seen as a useful compromise between functionality and ease of usage. Experience with similar techniques in PARMACS <ref> [3, 7] </ref> show that this information is usually sufficient for a good mapping. (End of rationale.) Process Topologies 255 Relationship between ranks and Cartesian coordinates for a 3x4 2D topology.
Reference: [8] <author> S. Chittor and R. J. Enbody. </author> <title> Performance evaluation of mesh-connected wormhole-routed networks for interprocessor communication in multicomputers. </title> <booktitle> In Proceedings of the 1990 Supercomputing Conference, </booktitle> <pages> pages 647-656, </pages> <year> 1990. </year>
Reference-contexts: On some machines, this will lead to unnecessary contention in the interconnection network. Some details about predicted and measured performance improvements that result from good process-to-processor mapping on modern wormhole-routing architectures can be found in <ref> [9, 8] </ref>.
Reference: [9] <author> S. Chittor and R. J. Enbody. </author> <title> Predicting the effect of mapping on the communication performance of large multicomputers. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, vol. II (Software), </booktitle> <address> pages II-1 II-4, </address> <year> 1991. </year>
Reference-contexts: On some machines, this will lead to unnecessary contention in the interconnection network. Some details about predicted and measured performance improvements that result from good process-to-processor mapping on modern wormhole-routing architectures can be found in <ref> [9, 8] </ref>.
Reference: [10] <author> R. Cypher and E. Leu. </author> <title> The semantics of blocking and nonblocking send and receive primitives. </title> <booktitle> In 8th International Parallel Processing Symposium, </booktitle> <pages> pages 729-735, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Interested readers can find a more formal treatment of the issues in this section in <ref> [10] </ref>. 2.4.1 Buffering and Safety The receive described in Section 2.2.5 can be started whether or not a matching send has been posted. That version of receive is blocking. It returns only after the receive buffer contains the newly received message.
Reference: [11] <author> J. J. Dongarra, R. Hempel, A. J. G. Hey, and D. W. Walker. </author> <title> A proposal for a user-level, message passing interface in a distributed memory environment. </title> <type> Technical Report TM-12231, </type> <institution> Oak Ridge National Laboratory, </institution> <month> February </month> <year> 1993. </year>
Reference: [12] <author> Nathan Doss, William Gropp, Ewing Lusk, and Anthony Skjellum. </author> <title> A model implementation of MPI. </title> <type> Technical report, </type> <institution> Argonne National Laboratory, </institution> <year> 1993. </year>
Reference-contexts: However, early work by Lusk, Gropp, Skjellum, Doss, Franke and others on early implementations of MPI showed that it could be fully implemented without a prohibitively large effort <ref> [12, 16] </ref>. Thus, the rationale for the MPI subset was lost, and this idea was dropped. 9.1.3 Why does MPI not guarantee buffering? MPI does not guarantee to buffer arbitrary messages because memory is a finite resource on all computers. Thus, all computers will fail under sufficiently adverse communication loads. <p> Conclusions 323 Unintended behavior of program. In this case the message from process 2 to process 0 is never received, and deadlock results. 9.4 MPI Implementations At the time of writing several portable implementations of MPI exist, * the MPICH implementation from Argonne National Laboratory and Mississippi State University <ref> [12] </ref>, available by anonymous ftp at info.mcs.anl.gov/pub/mpi.
Reference: [13] <institution> Edinburgh Parallel Computing Centre, University of Edinburgh. CHIMP Concepts, </institution> <month> June </month> <year> 1991. </year>
Reference: [14] <institution> Edinburgh Parallel Computing Centre, University of Edinburgh. </institution> <note> CHIMP Version 1.0 Interface, </note> <month> May </month> <year> 1992. </year>
Reference: [15] <author> Message Passing Interface Forum. </author> <title> MPI: A message-passing interface standard. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 8(3/4), </volume> <year> 1994. </year> <note> Special issue on MPI. </note>
Reference: [16] <author> H. Franke, H. Wu, C.E. Riviere, P.Pattnaik, and M. Snir. </author> <title> MPI programming environment for IBM SP1/SP2. </title> <booktitle> In 15th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 127-135, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: However, early work by Lusk, Gropp, Skjellum, Doss, Franke and others on early implementations of MPI showed that it could be fully implemented without a prohibitively large effort <ref> [12, 16] </ref>. Thus, the rationale for the MPI subset was lost, and this idea was dropped. 9.1.3 Why does MPI not guarantee buffering? MPI does not guarantee to buffer arbitrary messages because memory is a finite resource on all computers. Thus, all computers will fail under sufficiently adverse communication loads.
Reference: [17] <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam. </author> <title> PVM: A Users' Guide and Tutorial for Networked Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1994. </year> <note> The book is available electronically, the url is ftp://www.netlib.org/pvm3/book/pvm-book.ps. </note>
Reference: [18] <author> G. A. Geist, M. T. Heath, B. W. Peyton, and P. H. Worley. </author> <title> A user's guide to PICL: a portable instrumented communication library. </title> <type> Technical Report TM-11616, </type> <institution> Oak Ridge National Laboratory, </institution> <month> October </month> <year> 1990. </year>
Reference: [19] <author> William D. Gropp and Barry Smith. </author> <title> Chameleon parallel programming tools users manual. </title> <type> Technical Report ANL-93/23, </type> <institution> Argonne National Laboratory, </institution> <month> March </month> <year> 1993. </year>
Reference: [20] <author> V. Karamcheti and A.A. Chien. </author> <title> Software overheads in messaging layers: </title> <booktitle> Where does the time go? In 6th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 51-60, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Datatype matching (between sender and receiver) and data conversion on heterogeneous systems are discussed in more detail in Section 2.3. Example 2.1 C code. Process 0 sends a message to process 1. char msg <ref> [20] </ref>; int myrank, tag = 99; MPI_Status status; ... <p> Also, experience in this arena is quite limited, and underlying technology can be expected to change rapidly: fast, user-space interprocessor communication mechanisms are an active research area <ref> [28, 20] </ref>. Attempts by the MPI Forum to design mechanisms for querying or setting the amount of buffer space available to standard communication led to the conclusion that such mechanisms will either restrict allowed implementations unacceptably, or provide bounds that will be extremely pessimistic on most implementations in most cases.
Reference: [21] <author> O. Kramer and H. Muhlenbein. </author> <title> Mapping strategies in message-based multiprocessor systems. </title> <journal> Parallel Computing, </journal> <volume> 9 </volume> <pages> 213-225, </pages> <year> 1989. </year> <title> [22] nCUBE Corporation. nCUBE 2 Programmers Guide, </title> <address> r2.0, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: There are well-known techniques for mapping grid/torus structures to hardware topologies such as hypercubes or grids. For more complicated graph structures good heuristics often yield nearly optimal results <ref> [21] </ref>. On the other hand, if there is no way for the user to specify the logical process arrangement as a "virtual topology," a random mapping is most likely to result. On some machines, this will lead to unnecessary contention in the interconnection network.
Reference: [23] <institution> Parasoft Corporation, Pasadena, CA. </institution> <note> Express User's Guide, version 3.2.5 edition, </note> <year> 1992. </year>
Reference: [24] <author> Paul Pierce. </author> <title> The NX/2 operating system. </title> <booktitle> In Proceedings of the Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 384-390. </pages> <publisher> ACM Press, </publisher> <year> 1988. </year>
Reference: [25] <author> A. Skjellum and A. Leung. </author> <title> Zipcode: a portable multicomputer communication library atop the reactive kernel. </title> <editor> In D. W. Walker and Q. F. Stout, editors, </editor> <booktitle> Proceedings of the Fifth Distributed Memory Concurrent Computing Conference, </booktitle> <pages> pages 767-776. </pages> <publisher> IEEE Press, </publisher> <year> 1990. </year>
Reference: [26] <author> A. Skjellum, S. Smith, C. Still, A. Leung, and M. Morari. </author> <title> The Zipcode message passing system. </title> <type> Technical report, </type> <institution> Lawrence Livermore National Laboratory, </institution> <month> September </month> <year> 1992. </year>
Reference: [27] <author> V.S. Sunderam, G.A. Geist, J. Dongarra, and R. Manchek. </author> <title> The PVM concurrent computing system: Evolution, experiences, and trends. </title> <journal> Parallel Computing, </journal> <volume> 20(4) </volume> <pages> 531-545, </pages> <month> April </month> <year> 1994. </year>
Reference: [28] <author> T. von Eicken, D.E. Culler, S.C. Goldstein, and K.E. Shauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Also, experience in this arena is quite limited, and underlying technology can be expected to change rapidly: fast, user-space interprocessor communication mechanisms are an active research area <ref> [28, 20] </ref>. Attempts by the MPI Forum to design mechanisms for querying or setting the amount of buffer space available to standard communication led to the conclusion that such mechanisms will either restrict allowed implementations unacceptably, or provide bounds that will be extremely pessimistic on most implementations in most cases.

References-found: 27

