URL: http://www.ai.sri.com/~harabagi/coling-acl98/acl_work/krymolowski.ps.gz
Refering-URL: http://www.ai.sri.com/~harabagi/coling-acl98/acl_work/acl_work.html
Root-URL: 
Email: yuvalk@cs.biu.ac.il  danr@cs.uiuc.edu  
Title: Incorporating Knowledge in Natural Language Learning: A Case Study  
Author: Yuval Krymolowski Dan Roth 
Address: 52900 Ramat Gan, Israel  Urbana, IL 61801  
Affiliation: Dept. of Math and Computer Science Bar-Ilan University  Dept. of Computer Science University of Illinois  
Abstract: Incorporating external information during a learning process is expected to improve its efficiency. We study a method for incorporating noun-class information, in the context of learning to resolve Prepositional Phrase Attachment (PPA) disambiguation. This is done within a recently introduced architecture, SNOW, a sparse network of threshold gates utilizing the Winnow learning algorithm. That architecture has already been demonstrated to perform remarkably well on a number of natural language learning tasks. The knowledge sources used were compiled from the WordNet database for general linguistic purposes, irrespective of the PPA problem, and are being incorporated into the learning algorithm by enriching its feature space. We study two strategies of using enriched features and the effects of using class information at different granularities, as well as randomly-generated knowledge which serves as a control set. Incorporating external knowledge sources within SNOW yields a statistically significant performance improvement. In addition, we find an interesting relation between the granularity of the knowledge sources used and the magnitude of the improvement. The encouraging results with noun-class data provide a motivation for carrying out more work on generating better linguistic knowledge sources. 
Abstract-found: 1
Intro-found: 1
Reference: <author> A. Blum. </author> <year> 1992. </year> <title> Learning boolean functions in an infinite attribute space. </title> <journal> Machine Learning, </journal> <volume> 9(4) </volume> <pages> 373-386, </pages> <month> October. </month>
Reference-contexts: The sparse architecture along with the representation of each example as a list of active features is reminiscent of infinite attribute models of Winnow <ref> (Blum, 1992) </ref>. Theoretical analysis has shown that multiplicative update algorithms, like Winnow, have exceptionally good behavior in the presence of irrelevant attributes, noise, and even a target function changing in time (Littlestone, 1988; Littlestone and War-muth, 1994; Herbster and Warmuth, 1995).
Reference: <author> E. Brill and P. Resnik. </author> <year> 1994. </year> <title> A rule-based approach to prepositional phrase attachment disambiguation. </title> <booktitle> In Proc. of COLING. </booktitle>
Reference: <author> P. Buitelaar. </author> <year> 1998. </year> <title> CoreLex: Systematic Polysemy and Underspecification. </title> <type> Ph.D. thesis, </type> <institution> Computer Science Department, Brandeis University, </institution> <month> Feb. </month>
Reference-contexts: We derived the classes at different granu-larities. At the highest level, nouns are classified according to their synsets. The lower levels are obtained by successively using the hypernym relation defined in WordNet. In addition, we use the Corelex database <ref> (Buitelaar, 1998) </ref>. Consisting of 126 coarse-grained semantic types covering around 40; 000 nouns, Corelex defines a large number of systematic polysemous classes that are derived from an analysis of sense distributions in WordNet. <p> Thus, WN2 is obtained by replacing each WN1 synset with the set of hypernyms to which it points, WN3 by performing a similar process on the WN2 hypernyms, etc. We have used WN1, WN5, WN10, and WN15, Table 1 lists properties of these datasets. CoreLex (CL): The Corelex database <ref> (Buitelaar, 1998) </ref> was derived from WordNet as part of a linguistic research attempting to provide a unified approach to the systematic polysemy and underspecification of nouns. Systematic polysemy is the phenomena of word senses that are systematically related and therefore predictable over classes of lexical items.
Reference: <author> M. Collins and J Brooks. </author> <year> 1995. </year> <title> Prepositional phrase attachment through a backed-off modle. </title> <booktitle> In Proceedings of Third the Workshop on Very Large Corpora. </booktitle>
Reference-contexts: In order to obtain a fair comparison we have tested our system on the complete data set, including the preposition of (cf. Sec. 2). The results are compared with a maximum-entropy method (Ratnaparkhi et al., 1994), transformation-based learning (TBL, Brill and Resnik (1994)), an instantiation of the back-off estimation <ref> (Collins and Brooks, 1995) </ref> and a memory-based method (Zavrel et al., 1997). All these works have used the same train and test data set. Table 5 presents the comparison.
Reference: <author> I. Dagan, Y. Karov, and D. Roth. </author> <year> 1997. </year> <title> Mistake-driven learning in text categorization. </title> <booktitle> In EMNLP-97, The Second Conference on Empirical Methods in Natural Language Processing, </booktitle> <pages> pages 55-63, </pages> <month> August. </month>
Reference: <author> G. DeJong and R. Mooney. </author> <year> 1986. </year> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1(2) </volume> <pages> 145-176. </pages>
Reference: <author> G. DeJong. </author> <year> 1981. </year> <title> Generalization based on explanations. </title> <booktitle> In IJCAI, </booktitle> <pages> pages 67-70. </pages>
Reference: <author> A. R. Golding and D. Roth. </author> <year> 1996. </year> <title> Applying winnow to context-sensitive spelling correction. </title> <booktitle> In Machine Learning, </booktitle> <pages> pages 182-190. </pages>
Reference: <author> M. Herbster and M. Warmuth. </author> <year> 1995. </year> <title> Tracking the best expert. </title> <booktitle> In Proc. 12th International Conference on Machine Learning, </booktitle> <pages> pages 286-294. </pages> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> M. Junker. </author> <year> 1997. </year> <title> Sigir poster: The effectiveness of using thesauri in ir. </title> <booktitle> In Proc. of International Conference on Research and Development in Information Retrieval, SIGIR. </booktitle>
Reference: <author> Y. Karov and S. Edelman. </author> <year> 1996. </year> <title> Learning similarity-based word sense disambiguation from sparse data. </title> <booktitle> In Fourth workshop on very large corpora, </booktitle> <pages> pages 42-55, </pages> <month> August. </month>
Reference: <author> R. Khardon and D. Roth. </author> <year> 1997. </year> <title> Learning to reason. </title> <journal> Journal of the ACM, </journal> <volume> 44(5) </volume> <pages> 697-725, </pages> <month> Sept. </month> <note> Earlier version appeared in AAAI-94. </note>
Reference: <author> J. Kivinen and M. K. Warmuth. </author> <year> 1995. </year> <title> Exponentiated gradient versus gradient descent for linear predictors. </title> <note> In Proc. of STOC. Tech Report UCSC-CRL-94-16. </note>
Reference: <author> Ratnaparkhi Brill and Collins and Zavrel SNOW et al. </author> <title> (1994) Resnik (1994) Brooks (1995) et al. (1997) 81.6 81.9 84.5 84.4 84.8 Table 5: System comparison: Comparison of SNOW results with those of previous works. All the quoted figures are the best results obtained by the authors, with the exception of the Brill and Resnik (1994) result which was obtained by Zavrel et al. </title> <year> (1997). </year>
Reference-contexts: In order to obtain a fair comparison we have tested our system on the complete data set, including the preposition of (cf. Sec. 2). The results are compared with a maximum-entropy method <ref> (Ratnaparkhi et al., 1994) </ref>, transformation-based learning (TBL, Brill and Resnik (1994)), an instantiation of the back-off estimation (Collins and Brooks, 1995) and a memory-based method (Zavrel et al., 1997). All these works have used the same train and test data set. Table 5 presents the comparison.
Reference: <author> A. Kosmynin and I Davidson. </author> <year> 1996. </year> <title> Using background contextual knowledge for documents representation. </title> <booktitle> In PODP Workshop, </booktitle> <address> Palo-Alto. </address>
Reference: <author> R. Krovetz and W. B. Croft. </author> <year> 1992. </year> <title> Lexical ambiguity and information retrieval. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 10(2) </volume> <pages> 115-141. </pages>
Reference: <author> N. Littlestone and M. K. Warmuth. </author> <year> 1994. </year> <title> The weighted majority algorithm. </title> <journal> Information and Computation, </journal> <volume> 108(2) </volume> <pages> 212-261. </pages>
Reference: <author> N. Littlestone. </author> <year> 1988. </year> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318. </pages>
Reference-contexts: In general, a target node in the SNOW architecture is represented by a collection of subnetworks, which we call a cloud, but in the application described here we have used cloud size of 1 so this will not be discussed here. The Winnow local mistake-driven learning algorithm <ref> (Littlestone, 1988) </ref> is used at each target node to learn its dependence on the input nodes. Winnow updates the weight on the links in a multiplicative fashion. <p> In particular, Winnow was shown to learn efficiently any linear threshold function <ref> (Littlestone, 1988) </ref>, with a mistake bound that depends on the margin between positive and negative examples. The key feature of Winnow is that its mistake bound grows linearly with the number of relevant attributes and only logarithmically with the total number of attributes n.
Reference: <author> N. Littlestone. </author> <year> 1991. </year> <title> Redundant noisy attributes, attribute errors, and linear threshold learning using Winnow. </title> <booktitle> In Proc. 4th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 147-156, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> M. P. Marcus, B. Santorini, and M. Marcinkiewicz. </author> <year> 1993. </year> <title> Building a large annotated corpus of English: The Penn Treebank. </title> <journal> Computational Linguistics, </journal> <volume> 19(2) </volume> <pages> 313-330, </pages> <month> June. </month>
Reference-contexts: The first example in the previous paragraph is thus represented by &lt;buy, car, with, wheel&gt;. The experiments reported here were done using data extracted by Ratnaparkhi et al. (1994) from the Penn Treebank <ref> (Marcus et al., 1993) </ref> WSJ corpus. It consists of 20801 training examples and 3097 separate test examples. The preposition of turns out to be a very strong indicator for noun attachment.
Reference: <author> George A. Miller. </author> <year> 1990. </year> <title> Wordnet: An on-line lexical database. </title> <journal> International Journal of Lexicography, </journal> <volume> 3(4) </volume> <pages> 235-312. </pages>
Reference: <author> T.M. Mitchell, R.M. Keller, and S.T. Kedar-Cabelli. </author> <year> 1986. </year> <title> Explanation Based Learning. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 47-80. </pages>
Reference: <author> A. Ratnaparkhi, J. Reynar, and S. Roukos. </author> <year> 1994. </year> <title> A maximum entropy model for prepositional phrase attachment. </title> <booktitle> In ARPA, </booktitle> <address> Plainsboro, NJ, </address> <month> March. </month>
Reference-contexts: In order to obtain a fair comparison we have tested our system on the complete data set, including the preposition of (cf. Sec. 2). The results are compared with a maximum-entropy method <ref> (Ratnaparkhi et al., 1994) </ref>, transformation-based learning (TBL, Brill and Resnik (1994)), an instantiation of the back-off estimation (Collins and Brooks, 1995) and a memory-based method (Zavrel et al., 1997). All these works have used the same train and test data set. Table 5 presents the comparison.
Reference: <author> P. Resnik. </author> <year> 1992. </year> <title> Wordnet and distributional analysis: A class-based approach to lexical discovery. </title> <booktitle> In AAAI Workshop on Statistically-based Natural Language Processing Techniques, </booktitle> <pages> pages 54-64, </pages> <month> July. </month>
Reference: <author> P. Resnik. </author> <year> 1995. </year> <title> Disambiguating noun groupings with respect to wordnet senses. </title> <booktitle> In Proceedings of the Third Annual Workshop on Very Large Corpora. </booktitle>
Reference: <author> D. Roth and D. Zelenko. </author> <year> 1998. </year> <title> Part of speech tagging using a network of linear separators. </title> <booktitle> In COLING-ACL 98, The 17th International Conference on Computational Linguistics. </booktitle>
Reference: <author> L. G. Valiant. </author> <year> 1994. </year> <title> Circuits of the Mind. </title> <publisher> Oxford University Press, </publisher> <month> November. </month>
Reference-contexts: Thus, we use the learning algorithm here in a more complex way than just as a discriminator. One reason is that the SNOW architecture, influenced by the Neuroidal system <ref> (Valiant, 1994) </ref>, is being used in a system developed for the purpose of learning knowledge representations for natural language understanding tasks, and is being evaluated on a variety of tasks for which the node allocation process is of importance.
Reference: <author> L. G. Valiant. </author> <year> 1995. </year> <title> Rationality. </title> <booktitle> In Workshop on Computational Learning Theory, </booktitle> <pages> pages 3-14, </pages> <month> July. </month>
Reference: <author> J. Zavrel, W. Daelemans, and J. Veenstra. </author> <year> 1997. </year> <title> Resolving pp attachment ambiguities with memory based learning. </title> <booktitle> In Computational Natural Language Learning, </booktitle> <address> Madrid, Spain, </address> <month> July. </month>
Reference-contexts: Sec. 2). The results are compared with a maximum-entropy method (Ratnaparkhi et al., 1994), transformation-based learning (TBL, Brill and Resnik (1994)), an instantiation of the back-off estimation (Collins and Brooks, 1995) and a memory-based method <ref> (Zavrel et al., 1997) </ref>. All these works have used the same train and test data set. Table 5 presents the comparison.
References-found: 29

