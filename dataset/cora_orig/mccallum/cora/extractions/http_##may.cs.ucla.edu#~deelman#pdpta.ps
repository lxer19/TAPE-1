URL: http://may.cs.ucla.edu/~deelman/pdpta.ps
Refering-URL: http://may.cs.ucla.edu/~deelman/vitae.html
Root-URL: http://www.cs.ucla.edu
Email: deelmane@cs.rpi.edu  szymansk@cs.rpi.edu  
Title: Continuously Monitored Global Virtual Time  
Author: Ewa Deelman Boleslaw K. Szymanski 
Keyword: distributed simulation, discrete event simulation, virtual time, parallel processing  
Address: Troy, NY 12180  Troy, NY 12180  
Affiliation: Computer Science Department Rensselaer Polytechnic Institute  Computer Science Department Rensselaer Polytechnic Institute  
Abstract: Optimistic protocols designed for Parallel Discrete Event Simulation (PDES) rely heavily on the Global Virtual Time (GVT) calculation. Since the simulation uses large amounts of memory, the GVT is used to synchronize processes and discard obsolete system information. In this paper we present a new algorithm, the Continuously Monitored Global Virtual Time (CMGVT). Unlike others, this algorithm allows processes to calculate the GVT based on the local information constantly available to each process. System information, such as the Local Virtual Time (LVT) of each process and information about messages in transit, is appended to simulation messages. We present experimental data that show the performance of the CMGVT algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C.G. Cassandras. </author> <title> Discrete Event Systems: Modeling and Performance Analysis. </title> <year> 1993. </year>
Reference-contexts: 1 Introduction Many systems are of a discrete nature. They change their state at discrete points in time due to the occurrence of events <ref> [1] </ref>. Discrete Event Simulation (DES) models the behavior of such a system using the following mechanism: Events are placed in an event queue in the order of the time for which they are scheduled to occur. The simulation progresses as events are removed from the queue and processed. <p> It is possible that an LP receives information about an incoming message before it receives the message itself. T F V 1 = 6 4 17 26 <ref> [F (9; 1; 3)] </ref> 7 5 A simple TFV is shown in Eqn.2. This TFV belongs to LP 1 . Forcing F (10; 1; 4) represents a message sent by LP 1 to LP 0 , the message has the serial number 4 and was sent at virtual time 10.
Reference: [2] <author> R. M. Fujimoto. </author> <title> Parallel Discrete Event Simulation. </title> <journal> Communications of the ACM, </journal> <volume> 33(10) </volume> <pages> 31-53, </pages> <year> 1990. </year>
Reference-contexts: The challenge in managing multiple LPs is to preserve causality between events. The two main approaches developed to solve this challenge are based on conservative and optimistic protocols <ref> [2] </ref>. The former prevents causality errors by limiting each LP's progress in time, so although causality is guaranteed [3], tight synchronization between LPs is introduced. The optimistic approach allows causality errors to occur; however, recovery requires rolling back the computation to the time just prior to such error [4].
Reference: [3] <author> K. M. Chandy and J. Misra. </author> <title> Distributed Simulation: A Case Study in Design and Verification of Distributed Programs. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 5 </volume> <pages> 440-452, </pages> <year> 1979. </year>
Reference-contexts: The challenge in managing multiple LPs is to preserve causality between events. The two main approaches developed to solve this challenge are based on conservative and optimistic protocols [2]. The former prevents causality errors by limiting each LP's progress in time, so although causality is guaranteed <ref> [3] </ref>, tight synchronization between LPs is introduced. The optimistic approach allows causality errors to occur; however, recovery requires rolling back the computation to the time just prior to such error [4]. Once the rollback is completed, the computation may safely be restarted.
Reference: [4] <author> D.R. Jefferson. </author> <title> Virtual Time. </title> <journal> Trans. Prog. Lang. and Syst., </journal> <volume> 7 </volume> <pages> 404-425, </pages> <year> 1985. </year>
Reference-contexts: The former prevents causality errors by limiting each LP's progress in time, so although causality is guaranteed [3], tight synchronization between LPs is introduced. The optimistic approach allows causality errors to occur; however, recovery requires rolling back the computation to the time just prior to such error <ref> [4] </ref>. Once the rollback is completed, the computation may safely be restarted. When a process at time t 1 receives an event with time t 2 &lt; t 1 (this event is known as a straggler), it rolls back as follows.
Reference: [5] <author> M. Raynal and M. Singhal. </author> <title> Logical Time: Capturing Causality in Distributed Systems. </title> <booktitle> IEEE Computer, </booktitle> <pages> page 49, </pages> <year> 1996. </year>
Reference-contexts: Our algorithm keeps track of all messages in transit by appending information about them to event messages as well as to antimessages. This algorithm is based on the idea of the vector and matrix clocks <ref> [5] </ref>, used to order events and discard obsolete system information in distributed systems. The challenge, however, is to be able to handle logical time going backward as well as forward, which is not supported by the vector and matrix clocks. <p> The vector clock [11], which consists of a vector with a size equal to the number of processes, describes the logical progress of each process in the system. This clock can be used, for example, for causal ordering of messages in a distributed environment. A matrix clock <ref> [5] </ref>, which is represented by a p fi p matrix, where p is the number of processes, describes the knowledge a particular process has about the knowledge that all the processes in the system have about each other. The matrix clock is mostly used to discard obsolete system information.
Reference: [6] <author> J. S. Steinman, C. A. Lee, L. F. Wilson, and D. M. Nicol. </author> <title> Global Virtual Time and Distributed Synchronization. </title> <booktitle> Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pages 139-148, </pages> <year> 1995. </year>
Reference-contexts: The CMGVT is computed on each process based on the information constantly available to it. Although this algorithm involves some communication overhead, we demonstrate that it works well in our simulations. We also compare the performance of the CMGVT against a well known GVT algorithm used in SPEEDES <ref> [6] </ref>. 2 Global Virtual Time Algo rithms The major difficulty in the GVT calculation involves accounting for messages in transit. <p> Hence, the value obtained during the GVT calculation is often referred to as the EGVT (estimate for the GVT). The SPEEDES system uses a practical approach to the GVT calculation <ref> [6] </ref>. When this calculation is initiated, the processes enter a risk free mode, in which, although they continue to process local events, they do not send any event messages; however, antimessages are sent in order to minimize impending rollbacks.
Reference: [7] <author> G. Tel. </author> <title> Topics in distributed algorithms. </title> <publisher> Cambridge University Press, </publisher> <year> 1991. </year>
Reference-contexts: Upon receipt of the message, the receiving LP will have to roll back to the time just prior to t m . To keep track of messages in transit, some approaches involve acknowledging every message received while keeping track of the messages that were not acknowledged <ref> [7] </ref>. Each process keeps a list of unacknowledged messages. When an LP sends a message, it puts it on the list. Upon receipt of a message, the receiving LP sends an acknowledgment to the sender.
Reference: [8] <author> T. Lai and T. Yang. </author> <title> On distributed snapshots. </title> <journal> Inform. Process. Lett., </journal> <volume> 25 </volume> <pages> 153-158, </pages> <year> 1987. </year>
Reference-contexts: The GVT is calculated by synchronizing the LPs and taking the minimum of all local virtual times and the times-tamp of all unacknowledged messages. Another approach used in GVT calculations is to keep lists of messages sent and received <ref> [8] </ref>. These lists can be globally gathered and their difference determined. Then the minimum timestamp of the messages in transit and minimum of all local virtual times can be computed. Unfortunately, the lists and the messages carrying them can get very long.
Reference: [9] <author> F. Mattern. </author> <title> Efficient Algorithms for Distributed Snapshots and Global Virtual Time Approximation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18 </volume> <pages> 423-434, </pages> <year> 1993. </year>
Reference-contexts: Then the minimum timestamp of the messages in transit and minimum of all local virtual times can be computed. Unfortunately, the lists and the messages carrying them can get very long. If messages carry sequence numbers, they can be acknowledged by sending the highest consecutive message number received <ref> [9] </ref>. For example, assume LP 1 sends messages numbered from 1 to 50 to LP 2 . Assume also that, when LP 2 receives a control message that signals the start of the GVT calculation, it already received messages numbered from 1 to 45.
Reference: [10] <author> H. Bauer and C. Sporrer. </author> <title> Distributed Logic Simulation and an Approach to Asynchronous GVT-Calculation. </title> <booktitle> Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pages 205-208, </pages> <year> 1992. </year>
Reference-contexts: Based on this information, LP 1 can compute the minimum local time as the minimum timestamp of unacknowledged messages. A centralized message tracking algorithm was proposed by Bauer <ref> [10] </ref>. In this algorithm, the processes send information messages to a central process. The messages contain information about what messages were sent and received via static communication channels. The central process combines the available information and redistributes its knowledge back to the processes.
Reference: [11] <author> C. Fidge. </author> <title> Logical time in distributed computing systems. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 28-33, </pages> <year> 1991. </year>
Reference-contexts: Once a process receives a message from another process, it knows at least as much about the system as the sender knew at the time the message was issued. The idea of piggy-backing system information has been used before in distributed systems. The vector clock <ref> [11] </ref>, which consists of a vector with a size equal to the number of processes, describes the logical progress of each process in the system. This clock can be used, for example, for causal ordering of messages in a distributed environment.
Reference: [12] <author> E. Deelman, T. Caraco, and B. K. Szy-manski. </author> <title> Parallel Discrete Event Simulation of Lyme Disease. </title> <booktitle> Pacific Biocomput-ing Conference, </booktitle> <pages> pages 191-202, </pages> <year> 1996. </year>
Reference-contexts: This message contains the object being moved, the "Move Event", and all the future events associated with the object. We have currently implemented in our system a simulation of the spread of Lyme disease <ref> [12] </ref>. The application is programmed in C++ using MPI [13] for message passing. The simulation runs on the IBM SP2, an MIMD distributed memory machine with several processors (our configuration consists of 32). We used a strip decomposition in the dominant direction to divide the lattice among LPs.
Reference: [13] <author> W. Gropp, E. Lusk, and A. Skjellum. </author> <title> Using MPI. </title> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: This message contains the object being moved, the "Move Event", and all the future events associated with the object. We have currently implemented in our system a simulation of the spread of Lyme disease [12]. The application is programmed in C++ using MPI <ref> [13] </ref> for message passing. The simulation runs on the IBM SP2, an MIMD distributed memory machine with several processors (our configuration consists of 32). We used a strip decomposition in the dominant direction to divide the lattice among LPs.
Reference: [14] <author> E. Deelman and B. K. Szymanski. </author> <title> Simulating Lyme Disease Using Parallel Discrete Event Simulation. </title> <booktitle> Winter Simulation Conference, </booktitle> <pages> pages 1191-1198, </pages> <year> 1996. </year>
Reference-contexts: The results presented below were obtained with the division of the space into as many strips as we have processors, giving the assignment of one LP per processor. We have also investigated multiple LP-to-processor mappings <ref> [14] </ref>. To support rollback, we use incremental state saving [15]. Using full state saving would be too expensive in our simulation, because the state of the system would have to be saved after each event or at some time interval.
Reference: [15] <author> J. S. Steinman. </author> <title> Incremental State Saving in SPEEDES using C++. </title> <booktitle> Winter Simulation Conference, </booktitle> <pages> pages 687-696, </pages> <year> 1993. </year>
Reference-contexts: The results presented below were obtained with the division of the space into as many strips as we have processors, giving the assignment of one LP per processor. We have also investigated multiple LP-to-processor mappings [14]. To support rollback, we use incremental state saving <ref> [15] </ref>. Using full state saving would be too expensive in our simulation, because the state of the system would have to be saved after each event or at some time interval. This approach could be efficient if the events affected the majority of the state variables.
References-found: 15

