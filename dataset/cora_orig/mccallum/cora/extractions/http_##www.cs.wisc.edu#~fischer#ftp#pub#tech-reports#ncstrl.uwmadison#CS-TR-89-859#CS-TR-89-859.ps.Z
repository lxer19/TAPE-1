URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-89-859/CS-TR-89-859.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-89-859/
Root-URL: http://www.cs.wisc.edu
Email: shavlik@cs.wisc.edu  
Author: W. Shavlik Geoffrey G. Towell 
Keyword: Combining Explanation-Based and Neural Learning:  Keywords: explanation-based learning, neural networks, connectionism, symbolic systems, imperfect domain theories, neural network topologies, hybrid machine learning systems  
Address: 1210 West Dayton Street Madison, WI 53706  
Affiliation: Computer Sciences Department University of Wisconsin  
Note: Jude  (608) 262-7784  
Abstract: University of Wisconsin Computer Sciences Technical Report 859 (June 1989) An Algorithm and Empirical Results Abstract Machine learning is an area where both symbolic and neural approaches have been heavily investigated. However, there has been little research into the synergies achievable by combining these two learning paradigms. A hybrid approach that combines the symbolically-oriented explanation-based learning paradigm with the neural back-propagation algorithm is described. Most realistic problems can never be formalized exactly. However, there is much to be gained by utilizing the capability to reason nearly correctly. In the presented EBL-ANN algorithm, a roughly-correct explanatory capability leads to the acquisition of a classification rule that is almost correct. The rule is mapped into a neural network, where subsequent refinement improves it. This approach overcomes problems that arise when using imperfect domain theories to build explanations and addresses the problem of choosing a good initial neural network configuration. Empirical results show that the hybrid system more accurately learns concepts than an explanation-based system by itself and that the hybrid also learns them much faster than a neural learning system by itself. 
Abstract-found: 1
Intro-found: 1
Reference: [Ahalt89] <author> S. C. Ahalt, F. D. Garber, I. Jouny and A. K. Krishnamurthy, </author> <title> "Performance of Synthetic Neural Network Classification of Noisy Radar Signals," </title> <booktitle> in Advances in Neural Information Processing Systems, </booktitle> <volume> Volume 1, </volume> <editor> D. S. Touretsky (ed.), </editor> <publisher> Morgan-Kaufmann, </publisher> <address> Los Altos, CA, </address> <year> 1989, </year> <pages> pp. 281-288. </pages>
Reference-contexts: Neural (or connectionist) networks are largely numerically-oriented. Large numbers of processing units, each with minimal capability, operate without global control and together produce intelligent behavior. ANNs have proven successful on low-level tasks, such as perception [McClelland81] and signal processing <ref> [Ahalt89] </ref>, as well as on classification and diagnostic tasks [Mooney89b]. Each approach has its strengths and weaknesses. An approach that combines the two styles, building on each's strengths and overcoming each's weaknesses, is described and a successful implementation reported.
Reference: [Ahmad88] <author> S. Ahmad, </author> <title> "A Study of Scaling and Generalization in Neural Networks," </title> <type> Technical Report CCSR-88-13, </type> <institution> Center for Complex Systems Research, University of Illinois, Urbana, IL, </institution> <month> November </month> <year> 1988. </year>
Reference-contexts: The only difference between the runs was the randomly-chosen initial set of weights. Others have also reported that performance can be highly dependent on the initial set of weights <ref> [Ahmad88] </ref>. One way to overcome this problem is to train several networks, keeping the one that performs best on the training examples. For example, in [Ahmad88] usually 20 runs are performed. <p> Others have also reported that performance can be highly dependent on the initial set of weights <ref> [Ahmad88] </ref>. One way to overcome this problem is to train several networks, keeping the one that performs best on the training examples. For example, in [Ahmad88] usually 20 runs are performed. However, this substantially increases the training time. (3) There is no known problem-independent way to choose a good network topology.
Reference: [Baum89] <author> E. B. Baum and D. Haussler, </author> <title> "What Size Net Gives Valid Generalization?," </title> <booktitle> Neural Computation 1, </booktitle> <year> (1989), </year> <pages> pp. 151-160. </pages>
Reference-contexts: Again, several runs with different topologies can be performed, keeping the best. (4) Many training examples are needed. As is standard in learning systems that inductively acquire concepts from examples, ANNs, unlike EBL systems, need many training examples to acquire a concept <ref> [Baum89] </ref>. (5) There is no way to focus attention on significant features. ANNs have traditionally been trained in environments where the set of features to be considered is carefully selected by the experimenter.
Reference: [Bennett88] <author> S. Bennett, </author> <title> "Real World EBL: Learning Error Tolerant Plans in the Robotics Domain," </title> <booktitle> Proceedings of the AAAI Explanation-Based Learning Symposium, </booktitle> <address> Stanford, CA, </address> <month> March </month> <year> 1988, </year> <pages> pp. 122-126. </pages>
Reference-contexts: Furthermore, the complexity of problem solving prohibits any semblance of completeness and tractability. Finally, some domains are inherently uncertain. The actions of other agents, both competitors and cooperators, cannot be fully determined, nor can the physical world be exactly predicted. In these circumstances a domain theory is necessarily approximate <ref> [Bennett88] </ref>. To be truly useful, EBL systems must be able to work in uncertain, intractable, incompletely-specified, and changing environments, something they currently are incapable of doing. Artificial neural networks have also generated much interest recently. ANNs consist of a collection of simple, neuron-like processing units.
Reference: [DeJong86] <author> G. F. DeJong and R. J. Mooney, </author> <title> "Explanation-Based Learning: An Alternative View," </title> <booktitle> Machine Learning 1, 2 (1986), </booktitle> <pages> pp. 145-176. </pages>
Reference-contexts: The approach is not intended to be plausible at the level of neurophysiology. Rather, it investigates whether symbolic and neural learning techniques can be profitably combined to produce more powerful, general-purpose machine learning algorithms. Explanation-based learning is a recently developed and actively pursued approach to machine learning <ref> [DeJong86, Mitchell86] </ref>. In this type of learning, the solution to a sample problem is generalized into a form that can be later used to solve conceptually similar problems. The generalization process is driven by the explanation of why the solution worked.
Reference: [Fisher89] <author> D. H. Fisher and K. B. McKusick, </author> <title> "An Empirical Comparison of ID3 and Back-propagation," </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, MI, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: Combining Explanation-Based and Neural Learning 3 While the performance of ANNs that adjust their connection weights has been impressive, there are some problems with this form of machine learning: (1) Training times are lengthy. Recent experiments <ref> [Fisher89, Mooney89b, Shavlik89b, Weiss89] </ref> indicate that while ANNs classify new examples slightly better than do symbolic learning algorithms (such as ID3 [Quinlan86]), they require 100 to 1000 times as much training time. (2) The initial weights can greatly effect how well the concept is learned.
Reference: [Hammond88] <author> K. J. Hammond and N. </author> <title> Hurwitz, "Extracting Diagnostic Features from Explanations," </title> <booktitle> Proceedings of the AAAI Explanation-Based Learning Symposium, </booktitle> <address> Stanford, CA, </address> <month> March </month> <year> 1988, </year> <pages> pp. 31-35. </pages>
Reference-contexts: These two styles approach the learning problem from very different viewpoints. EBL falls within the traditional symbolically-oriented approach to artificial intelligence. It has shown promise in high-level tasks such as planning [Minton88], natural language processing [Mooney89a], and diagnosis <ref> [Hammond88] </ref>. Neural (or connectionist) networks are largely numerically-oriented. Large numbers of processing units, each with minimal capability, operate without global control and together produce intelligent behavior. ANNs have proven successful on low-level tasks, such as perception [McClelland81] and signal processing [Ahalt89], as well as on classification and diagnostic tasks [Mooney89b].
Reference: [Hinton86] <author> G. E. Hinton and T. J. Sejnowski, </author> <title> "Learning and Relearning in Boltzmann Machines," </title> <booktitle> in Parallel Distributed Processing, </booktitle> <volume> Vol. I, </volume> <editor> D. E. Rumelhart and J. L. McClelland (eds.), </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986, </year> <pages> pp. 282-317. </pages>
Reference-contexts: Its task is to determine a procedure for classifying future examples. Impressive performance has been achieved on problems such as converting text to speech [Sejnowski87], evaluating moves in backgammon [Tesauro89], and diagnosing diseases [Mooney89b]. Several algorithms have been developed for training an ANN to classify new examples (e.g., <ref> [Hinton86, Rosenblatt62, Rumelhart86b] </ref>). Training is accomplished by analyzing a set of provided, pre-classified examples to determine how to appropriately modify the connection weights so that these training examples are more accurately classified.
Reference: [Hirsh87] <author> H. Hirsh, </author> <title> "Explanation-Based Generalization in a Logic-Programming Environment," </title> <booktitle> Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <address> Milan, Italy, </address> <month> August </month> <year> 1987, </year> <pages> pp. 221-227. </pages>
Reference-contexts: Combining Explanation-Based and Neural Learning 2 The strength of EBL arises largely from it use of prior knowledge to guide its learning. However, EBL's dependence on prior knowledge is the major source of EBL's weaknesses. The basic algorithms (e.g., <ref> [Hirsh87, Kedar-Cabelli87, Mooney86, Shavlik89a] </ref>) assume a complete and correct domain theory [Mitchell86, Rajamoney87]. This domain theory, which is usually a collection of inference rules describing a given domain, is used to build the explanations that are then generalized.
Reference: [Kedar-Cabelli87] <author> S. T. Kedar-Cabelli and L. T. McCarty, </author> <title> "Explanation-Based Generalization as Resolution Theorem Proving," </title> <booktitle> Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <address> Irvine, CA, </address> <month> June </month> <year> 1987, </year> <pages> pp. 383-389. </pages>
Reference-contexts: Combining Explanation-Based and Neural Learning 2 The strength of EBL arises largely from it use of prior knowledge to guide its learning. However, EBL's dependence on prior knowledge is the major source of EBL's weaknesses. The basic algorithms (e.g., <ref> [Hirsh87, Kedar-Cabelli87, Mooney86, Shavlik89a] </ref>) assume a complete and correct domain theory [Mitchell86, Rajamoney87]. This domain theory, which is usually a collection of inference rules describing a given domain, is used to build the explanations that are then generalized.
Reference: [McClelland81] <author> J. L. McClelland and D. E. Rumelhart, </author> <title> "An Interactive Activation Model of the Effect of Context in Perception: Part 1. An Account of Basic Findings," </title> <journal> Psychological Review 88, </journal> <year> (1981), </year> <pages> pp. 375-407. </pages>
Reference-contexts: Neural (or connectionist) networks are largely numerically-oriented. Large numbers of processing units, each with minimal capability, operate without global control and together produce intelligent behavior. ANNs have proven successful on low-level tasks, such as perception <ref> [McClelland81] </ref> and signal processing [Ahalt89], as well as on classification and diagnostic tasks [Mooney89b]. Each approach has its strengths and weaknesses. An approach that combines the two styles, building on each's strengths and overcoming each's weaknesses, is described and a successful implementation reported.
Reference: [Michalski83] <author> R. S. Michalski, </author> <title> "A Theory and Methodology of Inductive Learning," in Machine Learning: An Artificial Intelligence Approach, </title> <editor> R. S. Michalski, J. G. Carbonell and T. M. Mitchell (eds.), </editor> <publisher> Tioga Publishing Company, </publisher> <address> Palo Alto, CA, </address> <year> 1983, </year> <pages> pp. 83-134. </pages>
Reference-contexts: One advantage of EBL systems is that they only require a small number of examples to learn a concept, unlike empirically-based learning methods (e.g., <ref> [Michalski83, Mitchell82, Quinlan86, Rumelhart86a] </ref>) which require large numbers of examples. Only a few examples are needed because EBL systems possess the ability to explain what is relevant in these examples. Other approaches, including connectionist ones, rely on statistics to determine relevance and hence need many examples.
Reference: [Minton88] <author> S. N. Minton, </author> <title> Learning Search Control Knowledge: An Explanation-Based Approach, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Hingham, MA, </address> <year> 1988. </year>
Reference-contexts: 1. INTRODUCTION Explanation-based learning (EBL) and artificial neural networks (ANNs) are two actively pursued approaches to machine learning. These two styles approach the learning problem from very different viewpoints. EBL falls within the traditional symbolically-oriented approach to artificial intelligence. It has shown promise in high-level tasks such as planning <ref> [Minton88] </ref>, natural language processing [Mooney89a], and diagnosis [Hammond88]. Neural (or connectionist) networks are largely numerically-oriented. Large numbers of processing units, each with minimal capability, operate without global control and together produce intelligent behavior.
Reference: [Mitchell82] <author> T. M. Mitchell, </author> <title> "Generalization as Search," </title> <booktitle> Artificial Intelligence 18, 2 (1982), </booktitle> <pages> pp. 203-226. </pages>
Reference-contexts: One advantage of EBL systems is that they only require a small number of examples to learn a concept, unlike empirically-based learning methods (e.g., <ref> [Michalski83, Mitchell82, Quinlan86, Rumelhart86a] </ref>) which require large numbers of examples. Only a few examples are needed because EBL systems possess the ability to explain what is relevant in these examples. Other approaches, including connectionist ones, rely on statistics to determine relevance and hence need many examples.
Reference: [Mitchell86] <author> T. M. Mitchell, R. M. Keller and S. Kedar-Cabelli, </author> <title> "Explanation-Based Generalization: A Unifying View," </title> <booktitle> Machine Learning 1, 1 (1986), </booktitle> <pages> pp. 47-80. </pages>
Reference-contexts: The approach is not intended to be plausible at the level of neurophysiology. Rather, it investigates whether symbolic and neural learning techniques can be profitably combined to produce more powerful, general-purpose machine learning algorithms. Explanation-based learning is a recently developed and actively pursued approach to machine learning <ref> [DeJong86, Mitchell86] </ref>. In this type of learning, the solution to a sample problem is generalized into a form that can be later used to solve conceptually similar problems. The generalization process is driven by the explanation of why the solution worked. <p> Knowledge about the domain allows the explanation to be developed, and then generalized, thereby producing a general concept from the solution to a specific problem. EBL systems can be used to produce general classification rules by analyzing the classification of a specific object (e.g., a rule for recognizing cups <ref> [Mitchell86] </ref>) or to produce general plans by observing the achievement of a goal in a specific situation (e.g., a plan for building towers [Shavlik90]). <p> However, EBL's dependence on prior knowledge is the major source of EBL's weaknesses. The basic algorithms (e.g., [Hirsh87, Kedar-Cabelli87, Mooney86, Shavlik89a]) assume a complete and correct domain theory <ref> [Mitchell86, Rajamoney87] </ref>. This domain theory, which is usually a collection of inference rules describing a given domain, is used to build the explanations that are then generalized. If the domain theory is not complete, some examples cannot be explained and, hence, nothing is learned.
Reference: [Mooney86] <author> R. J. Mooney and S. W. Bennett, </author> <title> "A Domain Independent Explanation-Based Generalizer," </title> <booktitle> Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <address> Philadelphia, PA, </address> <month> August </month> <year> 1986, </year> <pages> pp. 551-555. </pages>
Reference-contexts: Combining Explanation-Based and Neural Learning 2 The strength of EBL arises largely from it use of prior knowledge to guide its learning. However, EBL's dependence on prior knowledge is the major source of EBL's weaknesses. The basic algorithms (e.g., <ref> [Hirsh87, Kedar-Cabelli87, Mooney86, Shavlik89a] </ref>) assume a complete and correct domain theory [Mitchell86, Rajamoney87]. This domain theory, which is usually a collection of inference rules describing a given domain, is used to build the explanations that are then generalized.
Reference: [Mooney89a] <author> R. J. Mooney, </author> <title> A General Explanation-Based Learning Mechanism and its Application to Narrative Understanding, </title> <publisher> Pitman, </publisher> <address> London, </address> <year> 1989. </year>
Reference-contexts: These two styles approach the learning problem from very different viewpoints. EBL falls within the traditional symbolically-oriented approach to artificial intelligence. It has shown promise in high-level tasks such as planning [Minton88], natural language processing <ref> [Mooney89a] </ref>, and diagnosis [Hammond88]. Neural (or connectionist) networks are largely numerically-oriented. Large numbers of processing units, each with minimal capability, operate without global control and together produce intelligent behavior.
Reference: [Mooney89b] <author> R. J. Mooney, J. W. Shavlik, G. Towell and A. Gove, </author> <title> "An Experimental Comparison of Symbolic and Connectionist Learning Algorithms," </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, MI, </address> <month> August </month> <year> 1989, </year> <pages> pp. 775-780. </pages>
Reference-contexts: Neural (or connectionist) networks are largely numerically-oriented. Large numbers of processing units, each with minimal capability, operate without global control and together produce intelligent behavior. ANNs have proven successful on low-level tasks, such as perception [McClelland81] and signal processing [Ahalt89], as well as on classification and diagnostic tasks <ref> [Mooney89b] </ref>. Each approach has its strengths and weaknesses. An approach that combines the two styles, building on each's strengths and overcoming each's weaknesses, is described and a successful implementation reported. The approach is not intended to be plausible at the level of neurophysiology. <p> Its task is to determine a procedure for classifying future examples. Impressive performance has been achieved on problems such as converting text to speech [Sejnowski87], evaluating moves in backgammon [Tesauro89], and diagnosing diseases <ref> [Mooney89b] </ref>. Several algorithms have been developed for training an ANN to classify new examples (e.g., [Hinton86, Rosenblatt62, Rumelhart86b]). Training is accomplished by analyzing a set of provided, pre-classified examples to determine how to appropriately modify the connection weights so that these training examples are more accurately classified. <p> Combining Explanation-Based and Neural Learning 3 While the performance of ANNs that adjust their connection weights has been impressive, there are some problems with this form of machine learning: (1) Training times are lengthy. Recent experiments <ref> [Fisher89, Mooney89b, Shavlik89b, Weiss89] </ref> indicate that while ANNs classify new examples slightly better than do symbolic learning algorithms (such as ID3 [Quinlan86]), they require 100 to 1000 times as much training time. (2) The initial weights can greatly effect how well the concept is learned.
Reference: [Quinlan86] <author> J. R. Quinlan, </author> <title> "Induction of Decision Trees," </title> <booktitle> Machine Learning 1, 1 (1986), </booktitle> <pages> pp. 81-106. </pages>
Reference-contexts: One advantage of EBL systems is that they only require a small number of examples to learn a concept, unlike empirically-based learning methods (e.g., <ref> [Michalski83, Mitchell82, Quinlan86, Rumelhart86a] </ref>) which require large numbers of examples. Only a few examples are needed because EBL systems possess the ability to explain what is relevant in these examples. Other approaches, including connectionist ones, rely on statistics to determine relevance and hence need many examples. <p> Recent experiments [Fisher89, Mooney89b, Shavlik89b, Weiss89] indicate that while ANNs classify new examples slightly better than do symbolic learning algorithms (such as ID3 <ref> [Quinlan86] </ref>), they require 100 to 1000 times as much training time. (2) The initial weights can greatly effect how well the concept is learned. Because training ANNs usually involves a hill-climbing algorithm, there is the problem of local minima.
Reference: [Rajamoney87] <author> S. Rajamoney and G. F. DeJong, </author> <title> "The Classification, Detection and Handling of Imperfect Theory Problems," </title> <booktitle> Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <address> Milan, Italy, </address> <month> August </month> <year> 1987, </year> <pages> pp. 205-207. </pages>
Reference-contexts: However, EBL's dependence on prior knowledge is the major source of EBL's weaknesses. The basic algorithms (e.g., [Hirsh87, Kedar-Cabelli87, Mooney86, Shavlik89a]) assume a complete and correct domain theory <ref> [Mitchell86, Rajamoney87] </ref>. This domain theory, which is usually a collection of inference rules describing a given domain, is used to build the explanations that are then generalized. If the domain theory is not complete, some examples cannot be explained and, hence, nothing is learned.
Reference: [Rosenblatt62] <author> F. Rosenblatt, </author> <title> Principles of Neurodynamics, </title> <publisher> Spartan, </publisher> <address> New York, </address> <year> 1962. </year>
Reference-contexts: Its task is to determine a procedure for classifying future examples. Impressive performance has been achieved on problems such as converting text to speech [Sejnowski87], evaluating moves in backgammon [Tesauro89], and diagnosing diseases [Mooney89b]. Several algorithms have been developed for training an ANN to classify new examples (e.g., <ref> [Hinton86, Rosenblatt62, Rumelhart86b] </ref>). Training is accomplished by analyzing a set of provided, pre-classified examples to determine how to appropriately modify the connection weights so that these training examples are more accurately classified.
Reference: [Rumelhart86a] <author> D. E. Rumelhart, G. E. Hinton and J. L. McClelland, </author> <title> "A General Framework for Parallel Distributed Processing," in Parallel Distributed Processing: Explorations in the Micro-Structure of Cognition, </title> <editor> D. E. Rumelhart and J. L. McClelland (eds.), </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986, </year> <pages> pp. 46-73. </pages>
Reference-contexts: One advantage of EBL systems is that they only require a small number of examples to learn a concept, unlike empirically-based learning methods (e.g., <ref> [Michalski83, Mitchell82, Quinlan86, Rumelhart86a] </ref>) which require large numbers of examples. Only a few examples are needed because EBL systems possess the ability to explain what is relevant in these examples. Other approaches, including connectionist ones, rely on statistics to determine relevance and hence need many examples.
Reference: [Rumelhart86b] <author> D. E. Rumelhart, G. E. Hinton and J. R. Williams, </author> <title> "Learning Internal Representations by Error Propagation," </title> <booktitle> in Parallel Distributed Processing, </booktitle> <volume> Vol. I, </volume> <editor> D. E. Rumelhart and J. L. McClelland (eds.), </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986, </year> <pages> pp. 318-362. </pages> <booktitle> Combining Explanation-Based and Neural Learning 24 </booktitle>
Reference-contexts: Its task is to determine a procedure for classifying future examples. Impressive performance has been achieved on problems such as converting text to speech [Sejnowski87], evaluating moves in backgammon [Tesauro89], and diagnosing diseases [Mooney89b]. Several algorithms have been developed for training an ANN to classify new examples (e.g., <ref> [Hinton86, Rosenblatt62, Rumelhart86b] </ref>). Training is accomplished by analyzing a set of provided, pre-classified examples to determine how to appropriately modify the connection weights so that these training examples are more accurately classified.
Reference: [Sejnowski87] <author> T. J. Sejnowski and C. R. Rosenberg, </author> <title> "Parallel Networks that Learn to Pronounce English Text," </title> <journal> Complex Systems 1, </journal> <year> (1987), </year> <pages> pp. 145-168. </pages>
Reference-contexts: A set of examples, each labelled as belonging to a particular class, is provided to the ANN. Its task is to determine a procedure for classifying future examples. Impressive performance has been achieved on problems such as converting text to speech <ref> [Sejnowski87] </ref>, evaluating moves in backgammon [Tesauro89], and diagnosing diseases [Mooney89b]. Several algorithms have been developed for training an ANN to classify new examples (e.g., [Hinton86, Rosenblatt62, Rumelhart86b]).
Reference: [Shavlik89a] <author> J. W. Shavlik, </author> <title> "Acquiring Recursive Concepts with Explanation-Based Learning," </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, MI, </address> <month> August </month> <year> 1989, </year> <pages> pp. 688-693. </pages>
Reference-contexts: Combining Explanation-Based and Neural Learning 2 The strength of EBL arises largely from it use of prior knowledge to guide its learning. However, EBL's dependence on prior knowledge is the major source of EBL's weaknesses. The basic algorithms (e.g., <ref> [Hirsh87, Kedar-Cabelli87, Mooney86, Shavlik89a] </ref>) assume a complete and correct domain theory [Mitchell86, Rajamoney87]. This domain theory, which is usually a collection of inference rules describing a given domain, is used to build the explanations that are then generalized.
Reference: [Shavlik89b] <author> J. W. Shavlik, R. J. Mooney and G. G. Towell, </author> <title> "Symbolic and Neural Net Learning Algorithms: An Experimental Comparison," </title> <type> Technical Report 857, </type> <institution> Department of Computer Science, University of Wisconsin, Madison, WI, </institution> <year> 1989. </year>
Reference-contexts: Combining Explanation-Based and Neural Learning 3 While the performance of ANNs that adjust their connection weights has been impressive, there are some problems with this form of machine learning: (1) Training times are lengthy. Recent experiments <ref> [Fisher89, Mooney89b, Shavlik89b, Weiss89] </ref> indicate that while ANNs classify new examples slightly better than do symbolic learning algorithms (such as ID3 [Quinlan86]), they require 100 to 1000 times as much training time. (2) The initial weights can greatly effect how well the concept is learned. <p> Occasionally the learning algorithms cannot find any way to adjust weights to locally improve performance. Classification correctness on new examples can depend significantly on the initial set of weights of the ANN. For example, in some experiments <ref> [Shavlik89b] </ref> involving the conversion of text to speech, correctness on a large dictionary of examples ranged from 1% to 31%. The only difference between the runs was the randomly-chosen initial set of weights. Others have also reported that performance can be highly dependent on the initial set of weights [Ahmad88].
Reference: [Shavlik90] <author> J. W. Shavlik, </author> <title> Extending Explanation-Based Learning by Generalizing the Structure of Explanations, </title> <publisher> Pitman, </publisher> <address> London, </address> <year> 1990. </year>
Reference-contexts: EBL systems can be used to produce general classification rules by analyzing the classification of a specific object (e.g., a rule for recognizing cups [Mitchell86]) or to produce general plans by observing the achievement of a goal in a specific situation (e.g., a plan for building towers <ref> [Shavlik90] </ref>). One advantage of EBL systems is that they only require a small number of examples to learn a concept, unlike empirically-based learning methods (e.g., [Michalski83, Mitchell82, Quinlan86, Rumelhart86a]) which require large numbers of examples.
Reference: [Tesauro89] <author> G. Tesauro and T. J. Sejnowski, </author> <title> "A Parallel Network that Learns to Play Backgammon," </title> <booktitle> Artificial Intelligence 39, 3 (1989), </booktitle> <pages> pp. 357-390. </pages>
Reference-contexts: A set of examples, each labelled as belonging to a particular class, is provided to the ANN. Its task is to determine a procedure for classifying future examples. Impressive performance has been achieved on problems such as converting text to speech [Sejnowski87], evaluating moves in backgammon <ref> [Tesauro89] </ref>, and diagnosing diseases [Mooney89b]. Several algorithms have been developed for training an ANN to classify new examples (e.g., [Hinton86, Rosenblatt62, Rumelhart86b]).
Reference: [Weiss89] <author> S. M. Weiss and I. Kapouleas, </author> <title> "An Empirical Comparison of Pattern Recognition, Neural Nets, and Machine Learning Classification Methods," </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, MI, </address> <month> August </month> <year> 1989. </year> <title> Combining Explanation-Based and Neural Learning </title>
Reference-contexts: Combining Explanation-Based and Neural Learning 3 While the performance of ANNs that adjust their connection weights has been impressive, there are some problems with this form of machine learning: (1) Training times are lengthy. Recent experiments <ref> [Fisher89, Mooney89b, Shavlik89b, Weiss89] </ref> indicate that while ANNs classify new examples slightly better than do symbolic learning algorithms (such as ID3 [Quinlan86]), they require 100 to 1000 times as much training time. (2) The initial weights can greatly effect how well the concept is learned.
References-found: 29

