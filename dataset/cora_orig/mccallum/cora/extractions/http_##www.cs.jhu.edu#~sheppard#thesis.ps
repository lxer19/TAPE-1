URL: http://www.cs.jhu.edu/~sheppard/thesis.ps
Refering-URL: http://www.cs.jhu.edu/~sheppard/thesis.html
Root-URL: 
Title: Multi-Agent Reinforcement Learning in Markov Games  
Author: John Wilbur Sheppard John Wilbur Sheppard 
Degree: A dissertation submitted to The  in conformity with the requirement for the degree of Doctor of Philosophy.  All rights reserved.  
Note: Copyright c 1997 by  
Date: 1997  
Affiliation: Johns Hopkins University  Baltimore, Maryland  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> E. Aboaf, S. Drucker, and C. Atkeson. </author> <title> Task-level robot learning: Juggling a tennis ball more accurately. </title> <booktitle> In Proceedings of the IEEE Conference on Robotics and Automation. IEEE, </booktitle> <year> 1989. </year>
Reference-contexts: Control theory focuses on developing optimal procedures for maintaining equilibrium or for achieving some performance objective by determining values of several "control variables" in a dynamic system <ref> [1, 26, 40, 45, 71, 100, 101, 156, 237, 252, 253] </ref>. Machine learning systems attempt to determine optimal values for these control variables from experience rather than explicitly solving a set of differential equations. <p> Sutton goes on to describe a family of temporal difference methods based on the influence past updates have on the current update of the weight vector. These methods are based on a parameter, 2 <ref> [0; 1] </ref>, which specifies a discount factor in the prediction equation (see Section 2.2), and refers to the family as the TD () family. When = 0, past updates have no influence on the current update. When = 1, all past predictions receive equal weight. <p> The optimal move comes at u = 3 2 , and sampling the action space provides a smooth slope on either side of optimal. For E, on the other hand, the optimal move (v = 1), arises at the boundary of legal moves (v 2 <ref> [0; 1] </ref>). Sampling only has benefit on one side of optimal side (because the other side is infeasible). <p> For each test, we played 50 games and averaged the payoff received after each game. For each of the 50 games, we used a uniform probability distribution and randomly generated a new starting position such that x P 2 <ref> [1; 1] </ref>, y P 2 [1; 1], x E 2 [1; 1], and y E 2 [1; 1]. For these and all subsequent experiments, we used a variable learning rate. Specifically, a learning rate was associated with each example stored in the memory base. <p> For each test, we played 50 games and averaged the payoff received after each game. For each of the 50 games, we used a uniform probability distribution and randomly generated a new starting position such that x P 2 <ref> [1; 1] </ref>, y P 2 [1; 1], x E 2 [1; 1], and y E 2 [1; 1]. For these and all subsequent experiments, we used a variable learning rate. Specifically, a learning rate was associated with each example stored in the memory base. <p> For each test, we played 50 games and averaged the payoff received after each game. For each of the 50 games, we used a uniform probability distribution and randomly generated a new starting position such that x P 2 <ref> [1; 1] </ref>, y P 2 [1; 1], x E 2 [1; 1], and y E 2 [1; 1]. For these and all subsequent experiments, we used a variable learning rate. Specifically, a learning rate was associated with each example stored in the memory base. <p> For each of the 50 games, we used a uniform probability distribution and randomly generated a new starting position such that x P 2 <ref> [1; 1] </ref>, y P 2 [1; 1], x E 2 [1; 1], and y E 2 [1; 1]. For these and all subsequent experiments, we used a variable learning rate. Specifically, a learning rate was associated with each example stored in the memory base. <p> Whenever we tested the performance of the algorithm we played 50 games generated at random according to a uniform probability distribution and averaged the payoff received after each game. For each of the 50 games, we generated starting positions such that x P 2 <ref> [1; 1] </ref>, y P 2 [1; 1], x E 2 [1; 1], and y E 2 [1; 1]. <p> Whenever we tested the performance of the algorithm we played 50 games generated at random according to a uniform probability distribution and averaged the payoff received after each game. For each of the 50 games, we generated starting positions such that x P 2 <ref> [1; 1] </ref>, y P 2 [1; 1], x E 2 [1; 1], and y E 2 [1; 1]. Once again, we permitted the learning rate to vary and set fl = 0:95. 195 The results of this experiment are shown in Figure 7.7 with a comparison to optimal performance shown in Figure 7.8. <p> For each of the 50 games, we generated starting positions such that x P 2 <ref> [1; 1] </ref>, y P 2 [1; 1], x E 2 [1; 1], and y E 2 [1; 1]. Once again, we permitted the learning rate to vary and set fl = 0:95. 195 The results of this experiment are shown in Figure 7.7 with a comparison to optimal performance shown in Figure 7.8. <p> For each of the 50 games, we generated starting positions such that x P 2 <ref> [1; 1] </ref>, y P 2 [1; 1], x E 2 [1; 1], and y E 2 [1; 1]. Once again, we permitted the learning rate to vary and set fl = 0:95. 195 The results of this experiment are shown in Figure 7.7 with a comparison to optimal performance shown in Figure 7.8.
Reference: [2] <author> D. Aha. </author> <title> Incremental, instance-based learning of independent and graded concept description. </title> <booktitle> In Proceedings of the Machine Learning Workshop, </booktitle> <year> 1989. </year>
Reference-contexts: Data mining stems from work in statistics and unsupervised learning and has drawn from ideas taken from automatic discovery <ref> [2, 66, 79, 124, 185] </ref>. Many approaches to data mining use techniques such as clustering, rule induction, and classification to analyze the large set of data. From this set of data, data mining systems attempt to induce general "laws" and classification procedures for characterizing the data.
Reference: [3] <author> D. Aha. </author> <title> A study of instance-based algorithms for supervised learning: Mathematical, empirical and psychological evaluations. </title> <type> PhD thesis, </type> <institution> Department of Information and Computer Science, University of California, Irvine, </institution> <year> 1990. </year>
Reference-contexts: Some of the types of problems explored include data classification, data mining, automatic programming, control theory, and planning. Classification systems identify a concept class from a set of available classes to which a particular example belongs <ref> [3, 10, 61, 91, 119, 182] </ref>. Data classification usually proceeds from a set of available attributes and associated values. A training set is used to present examples of concept classes, and the classifier constructed is designed to be consistent with that training set. <p> d 1 and d 2 respectively for some 103 state-action pair in the game, then the stored state-action pair's Q value is updated. 4.3.2 1-NN for Evasive Maneuvers Memory-based learning is a classical approach to machine learning and pattern recognition, most commonly in the form of the 1-nearest neighbor algorithm <ref> [3, 10, 90, 119, 292, 293, 366] </ref>. 1-NN is rarely used for Markov decision problems, so we had to represent the pursuit game in a format amenable to this algorithm. <p> 8i 2 sample do; if class (i) 6= class (k-NN (i)) then delete (i,sample); endif; enddo; memory-base = memory-base + sample; end; algorithm Edit-Correct; 8i 2 memory-base do; if class (i) = class (k-NN (i)) then delete (i); endif; enddo; end; 142 editing) and instance averaging algorithms in instance-based learning <ref> [3, 5, 7, 6, 189, 190] </ref>. Aha's IB2 and IB3 algorithms apply a standard nearest-neighbor rule for classification, but then proceed to edit examples based on Ritter's approach. For Aha et al., the assumption is that misclassification comes from noisy attributes in the data.
Reference: [4] <author> D. Aha. </author> <title> Case-based learning algorithms. </title> <booktitle> In Proceedings of the 1991 DARPA Case Based Reasoning Workshop. </booktitle> <publisher> Morgan Kaufmann, Publishers, </publisher> <year> 1991. </year>
Reference: [5] <author> D. Aha. </author> <title> Tolerating noisy, irrelevant, and novel attributes in instance-based learning algorithms. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 16 </volume> <pages> 267-287, </pages> <year> 1992. </year>
Reference-contexts: This demonstrates that the two-pursuer problem is significantly more difficult for 1-NN. 115 116 One possible reason for 1-NN's poor performance on the two-pursuer task is presence of irrelevant attributes, which is known to cause problems for nearest neighbor algorithms <ref> [5, 293] </ref>. We experimented with a method similar to stepwise forward selection [112] to determine the set of relevant attributes. <p> 8i 2 sample do; if class (i) 6= class (k-NN (i)) then delete (i,sample); endif; enddo; memory-base = memory-base + sample; end; algorithm Edit-Correct; 8i 2 memory-base do; if class (i) = class (k-NN (i)) then delete (i); endif; enddo; end; 142 editing) and instance averaging algorithms in instance-based learning <ref> [3, 5, 7, 6, 189, 190] </ref>. Aha's IB2 and IB3 algorithms apply a standard nearest-neighbor rule for classification, but then proceed to edit examples based on Ritter's approach. For Aha et al., the assumption is that misclassification comes from noisy attributes in the data. <p> Indeed, the additional state information for P is "irrelevant," and irrelevant attributes are known to degrade performance in instance-based and memory-based learning <ref> [5, 293] </ref>. 6.3.4 Pursuit with Limited Mobility The final game we studied with MBCL further extends the pursuit game by limiting the mobility of both players.
Reference: [6] <author> D. Aha and D. Kibler. </author> <title> Noise-tolerant instance-based learning algorithms. </title> <booktitle> In Proceedings of IJCAI-89, </booktitle> <pages> pages 794-799, </pages> <address> Detroit, MI, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 8i 2 sample do; if class (i) 6= class (k-NN (i)) then delete (i,sample); endif; enddo; memory-base = memory-base + sample; end; algorithm Edit-Correct; 8i 2 memory-base do; if class (i) = class (k-NN (i)) then delete (i); endif; enddo; end; 142 editing) and instance averaging algorithms in instance-based learning <ref> [3, 5, 7, 6, 189, 190] </ref>. Aha's IB2 and IB3 algorithms apply a standard nearest-neighbor rule for classification, but then proceed to edit examples based on Ritter's approach. For Aha et al., the assumption is that misclassification comes from noisy attributes in the data.
Reference: [7] <author> D. Aha, D. Kibler, and M. Albert. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6(1), </volume> <year> 1991. </year>
Reference-contexts: 8i 2 sample do; if class (i) 6= class (k-NN (i)) then delete (i,sample); endif; enddo; memory-base = memory-base + sample; end; algorithm Edit-Correct; 8i 2 memory-base do; if class (i) = class (k-NN (i)) then delete (i); endif; enddo; end; 142 editing) and instance averaging algorithms in instance-based learning <ref> [3, 5, 7, 6, 189, 190] </ref>. Aha's IB2 and IB3 algorithms apply a standard nearest-neighbor rule for classification, but then proceed to edit examples based on Ritter's approach. For Aha et al., the assumption is that misclassification comes from noisy attributes in the data.
Reference: [8] <author> D. Aha and S. Salzberg. </author> <title> Learning to catch: Applying nearest neighbor algorithms to dynamic control tasks. </title> <booktitle> In Proceedings of the Fourth International Workshop on AI and Statistics, </booktitle> <pages> pages 363-368, </pages> <address> Ft. Lauderdale, </address> <year> 1993. </year>
Reference: [9] <author> S. Aihara and A. Bagchi. </author> <title> Linear-quadratic stochastic differential games for dis tributed parameter systems. Computers and Mathematics with Applications, </title> <address> 13(1-3):247-259, </address> <year> 1987. </year>
Reference: [10] <author> M. Albert and D. Aha. </author> <title> Analyses of instance-based learning algorithms. </title> <booktitle> In Pro ceedings of the Ninth National Conference on Artificial Intelligence. AAAI, </booktitle> <year> 1991. </year> <month> 228 </month>
Reference-contexts: Some of the types of problems explored include data classification, data mining, automatic programming, control theory, and planning. Classification systems identify a concept class from a set of available classes to which a particular example belongs <ref> [3, 10, 61, 91, 119, 182] </ref>. Data classification usually proceeds from a set of available attributes and associated values. A training set is used to present examples of concept classes, and the classifier constructed is designed to be consistent with that training set. <p> d 1 and d 2 respectively for some 103 state-action pair in the game, then the stored state-action pair's Q value is updated. 4.3.2 1-NN for Evasive Maneuvers Memory-based learning is a classical approach to machine learning and pattern recognition, most commonly in the form of the 1-nearest neighbor algorithm <ref> [3, 10, 90, 119, 292, 293, 366] </ref>. 1-NN is rarely used for Markov decision problems, so we had to represent the pursuit game in a format amenable to this algorithm.
Reference: [11] <author> L. Allis. </author> <title> Searching for Solutions in Games and Artificial Intelligence. </title> <type> PhD thesis, </type> <institution> University of Limburg, </institution> <year> 1994. </year>
Reference: [12] <author> H. Almuallim and T. Dietterich. </author> <title> Learning with many irrelevant features. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 547-552. </pages> <publisher> AAAI, </publisher> <year> 1991. </year>
Reference: [13] <author> H. Almuallim and T. Dietterich. </author> <title> Learning boolean concepts in the presence of many irrelevant features. </title> <journal> Artificial Intelligence, </journal> <volume> 69 </volume> <pages> 279-305, </pages> <year> 1994. </year>
Reference: [14] <author> E. Alpaydin. </author> <title> Voting over multiple condensed nearest neighbors. </title> <type> Technical Report TR-80816, </type> <institution> Bogazici University, </institution> <year> 1995. </year>
Reference: [15] <author> E. Altman and G. Koole. </author> <title> Stochastic scheduling games with markov decision ar rival processes. </title> <journal> Computers and Mathematics with Applications, </journal> <volume> 26(6) </volume> <pages> 141-148, </pages> <year> 1993. </year>
Reference: [16] <author> C. Anderson and S. Crawford-Hines. </author> <title> Multigrid Q-learning. </title> <type> Technical Report CS-94-121, </type> <institution> Department of Computer Science, Colorado State University, </institution> <year> 1994. </year>
Reference-contexts: In addition to generating the examples, expected behaviors must also be learned. Within the memory-based framework, behaviors (or outcomes) can correspond to the estimate of expected payoff. In this case, any of the real-time dynamic programming or temporal difference methods can be applied to update these behaviors <ref> [16, 43, 44, 53, 210, 263, 264, 265] </ref>. <p> Initial Q values did not matter since they would be learned over time. Nevertheless, a uniform random sampling of the space was, apparently, not sufficient to approximate some of the surfaces encountered in these games. Applications of variable resolution techniques <ref> [16, 110, 240, 312] </ref> may be more appropriate for problems such as these. In some ways, the results from MBCL are highly encouraging. They indicate co-learning can occur and suggest it is possible to learn optimal solutions to two-player differential games.
Reference: [17] <author> M. Ardema and N. Rajan. </author> <title> An approach to three-dimensional aircraft pursuit evasion. Computers and Mathematics with Applications, </title> <address> 13(1-3):97-110, </address> <year> 1987. </year>
Reference-contexts: Unfortunately, these cases are frequently insufficient to cover real-world problems which take place in three dimensions, or at least in two and a half dimensions. Ardema and Rajan examine the three-dimensional aircraft pursuit problem and focus on real-time characteristics of any feasible control law <ref> [17] </ref>. Such cases are closer to 2 1 2 dimensions given normal orientations of pilots in gravity. 38 Merz considered a true three-dimensional pursuit game by considering stochastic guidance of satellites in orbit, engaged in a pursuit-evasion conflict [230].
Reference: [18] <author> S. Arya and D. Mount. </author> <title> Asymptotically efficient randomized algorithm for nearest neighbor searching. </title> <type> Technical Report CS-TR-3011 or UMIACS-TR-92-135, </type> <institution> Computer Science, University of Maryland, College Park, MD, </institution> <month> December </month> <year> 1992. </year>
Reference: [19] <author> S. Arya and D. Mount. </author> <title> Efficient heuristic for nearest neighbor searching. </title> <institution> Tech nical Report CS-TR-3012 or UMIACS-TR-92-136, Computer Science, University of Maryland, College Park, MD, </institution> <month> December </month> <year> 1992. </year>
Reference: [20] <author> M. Asada, E. Uchibe, and K. Hosoda. </author> <title> Agents that learn from other competitive agents. In Proceedings of the Workshop on Agents that Learn from Other Agents, </title> <year> 1995. </year>
Reference: [21] <author> H. Atir. </author> <title> Nonlinear effects in a variable speed pursuit-evasion game. </title> <editor> In R. Hamalainen and H. Ehtamo, editors, </editor> <booktitle> Differential Games|Developments in Modeling and Computation, </booktitle> <pages> pages 58-66. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference: [22] <author> C. Atkeson. </author> <title> Model-based robot learning. </title> <type> Technical Report AI Memo No. 1024, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1988. </year>
Reference-contexts: In traditional reinforcement learning, b would 58 correspond to the expected payoff of taking action a in state s and then performing optimally from that point forward. In Atkeson's approach, an "inverse" control model is used to determine the desired action <ref> [22, 23, 24] </ref>: a = f 1 (s; b) where, typically, the control model is represented as b = f (s; a) This latter form is referred to as the "forward" model. Learning consists of using experience to develop an approximation to f , ^ f .
Reference: [23] <author> C. Atkeson. </author> <title> Using local models to control movement. </title> <editor> In David S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 316-323, </pages> <address> San Mateo, CA, November 1990. </address> <publisher> Morgan Kaufman. </publisher> <pages> 229 </pages>
Reference-contexts: In traditional reinforcement learning, b would 58 correspond to the expected payoff of taking action a in state s and then performing optimally from that point forward. In Atkeson's approach, an "inverse" control model is used to determine the desired action <ref> [22, 23, 24] </ref>: a = f 1 (s; b) where, typically, the control model is represented as b = f (s; a) This latter form is referred to as the "forward" model. Learning consists of using experience to develop an approximation to f , ^ f .
Reference: [24] <author> C. Atkeson. </author> <title> Using locally weighted regression for robot learning. </title> <booktitle> In Proceedings of the IEEE International Conferences on Robotics and Automation, </booktitle> <pages> pages 958-963. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: In traditional reinforcement learning, b would 58 correspond to the expected payoff of taking action a in state s and then performing optimally from that point forward. In Atkeson's approach, an "inverse" control model is used to determine the desired action <ref> [22, 23, 24] </ref>: a = f 1 (s; b) where, typically, the control model is represented as b = f (s; a) This latter form is referred to as the "forward" model. Learning consists of using experience to develop an approximation to f , ^ f .
Reference: [25] <author> C. Atkeson. </author> <title> Memory-based approaches to approximating continuous functions. </title> <editor> In M. Casdagli and S. Eubanks, editors, </editor> <booktitle> Nonlinear Modeling and Forecasting, </booktitle> <pages> pages 503-521. </pages> <publisher> Addison Wesley, </publisher> <year> 1992. </year>
Reference-contexts: Then it becomes important to be able to interpolate between actions within a region. One approach to combating these problems has been proposed by Atkeson, Moore, and Schaal <ref> [26, 27, 25, 110, 237] </ref>. In their approach, they apply local weighted regression among the examples to determine the proper action. <p> The advantages to a lazy approach are three-fold. First, minimal computational time is required during training, because training consists primarily of storing examples (in the most traditional lazy approach, k-nearest neighbor). Second, lazy methods have been shown to be good function-approximators in continuous state and action spaces <ref> [25] </ref>. This capability is important for our task of learning to play 92 differential games. Third, traditional eager approaches to reinforcement learning assume the tasks are Markov decision problems.
Reference: [26] <author> C. Atkeson, A. Moore, and S. Schaal. </author> <title> Locally weighted learning for control. </title> <journal> Artificial Intelligence Review, </journal> <note> 1995. To appear. </note>
Reference-contexts: Control theory focuses on developing optimal procedures for maintaining equilibrium or for achieving some performance objective by determining values of several "control variables" in a dynamic system <ref> [1, 26, 40, 45, 71, 100, 101, 156, 237, 252, 253] </ref>. Machine learning systems attempt to determine optimal values for these control variables from experience rather than explicitly solving a set of differential equations. <p> Then it becomes important to be able to interpolate between actions within a region. One approach to combating these problems has been proposed by Atkeson, Moore, and Schaal <ref> [26, 27, 25, 110, 237] </ref>. In their approach, they apply local weighted regression among the examples to determine the proper action. <p> Moore suggests that inverse models, although somewhat natural, can lead to problems if there is not a one-to-one mapping between actions and behaviors or if there are noisy examples in the memory base <ref> [26, 237] </ref>. As a result, he proposes working directly with the forward model. Using the memory base then consists of searching through available actions (in a given state) until the desired outcome is found.
Reference: [27] <author> C. Atkeson, A. Moore, and S. Schaal. </author> <title> Locally weighted learning. </title> <journal> Artificial Intelligence Review, </journal> <note> 1996. To appear. </note>
Reference-contexts: Then it becomes important to be able to interpolate between actions within a region. One approach to combating these problems has been proposed by Atkeson, Moore, and Schaal <ref> [26, 27, 25, 110, 237] </ref>. In their approach, they apply local weighted regression among the examples to determine the proper action.
Reference: [28] <author> R. Axelrod. </author> <title> The Evolution of Cooperation. </title> <publisher> Basic Books, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: Some work has been done in evolutionary computation, artificial life, and iterated games. The most common game considered in this context is the iterated prisoner's dilemma <ref> [28, 223, 234, 299, 325] </ref>. The Prisoner's Dilemma is a two-player non-zero-sum game in which players must decide to cooperate or defect based on their expected payoffs. <p> Axelrod held a tournament among players of various strategies and found that the "Tit-for-Tat" strategy (where each player plays the other player's previous strategy) was the best <ref> [28] </ref>. The work by Stanley et al. focused on several agents with different strategies evolving chocie and refusal mechanisms to determine when a game was played [325]. The goal was not on learning strategies but on characterising choice and refusal in a competitive environment.
Reference: [29] <author> L. Baird. </author> <title> Advantage updating. </title> <type> Technical Report WL-TR-93-1146, </type> <institution> Wright Labora tory, </institution> <year> 1993. </year>
Reference-contexts: performed better against the hand-built player than minimax-Q-learning did against the hand-built player following training with other learning players. 77 3.3.5 Advantage Updating and Differential Games In independent research, Harmon, Baird, and Klopf investigated applying reinforcement learning in function approximators (namely, artificial neural networks) to learning solutions to differential games <ref> [29, 30, 31, 161, 162, 163] </ref>. For their research, they focused on a single linear-quadratic differential game of pursuit in which a single missile (designated P ) pursues a single airplane (designated E), which is similar to the problem studied by Grefenstette et al. [149, 153, 163, 273].
Reference: [30] <author> L. Baird. </author> <title> Residual algorithms: Reinforcement learning with function approximation. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: performed better against the hand-built player than minimax-Q-learning did against the hand-built player following training with other learning players. 77 3.3.5 Advantage Updating and Differential Games In independent research, Harmon, Baird, and Klopf investigated applying reinforcement learning in function approximators (namely, artificial neural networks) to learning solutions to differential games <ref> [29, 30, 31, 161, 162, 163] </ref>. For their research, they focused on a single linear-quadratic differential game of pursuit in which a single missile (designated P ) pursues a single airplane (designated E), which is similar to the problem studied by Grefenstette et al. [149, 153, 163, 273].
Reference: [31] <author> L. Baird and A. Klopf. </author> <title> Reinforcement learning with high-dimensional, continuous actions. </title> <type> Technical Report WL-TR-93-1147, </type> <institution> Wright Laboratory, </institution> <year> 1993. </year>
Reference-contexts: performed better against the hand-built player than minimax-Q-learning did against the hand-built player following training with other learning players. 77 3.3.5 Advantage Updating and Differential Games In independent research, Harmon, Baird, and Klopf investigated applying reinforcement learning in function approximators (namely, artificial neural networks) to learning solutions to differential games <ref> [29, 30, 31, 161, 162, 163] </ref>. For their research, they focused on a single linear-quadratic differential game of pursuit in which a single missile (designated P ) pursues a single airplane (designated E), which is similar to the problem studied by Grefenstette et al. [149, 153, 163, 273].
Reference: [32] <author> M. Bardi and P. Soravia. </author> <title> Approximation of differential games of pursuit evasion by discrete-time games. </title> <editor> In R. Hamalainen and H. Ehtamo, editors, </editor> <booktitle> Differential Games|Developments in Modeling and Computation, </booktitle> <pages> pages 131-143. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference: [33] <author> E. Bareiss, B. Porter, and C. Wier. </author> <title> PROTOS- an exemplar based learning ap prentice. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <pages> pages 12-23, </pages> <address> Irvine, CA, 1987. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [34] <author> A. Barto, S. Bradtke, and S. Singh. </author> <title> Real-time learning and control using asynchronous dynamic programming. </title> <type> Technical Report 91-57, COINS, </type> <institution> University of Massachusetts, </institution> <year> 1991. </year>
Reference-contexts: Barto et al. <ref> [35, 37, 34] </ref> describe two alternative approaches to solving an MDP that appear to be less computationally expensive and are both forms of asynchronous dy 53 namic programming.
Reference: [35] <author> A. Barto, S. Bradtke, and S. Singh. </author> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <year> 1993. </year>
Reference-contexts: Then, f (s i ) = E t=0 # where E [] is the expectation given policy . Note we can estimate f (s i ) for some (s i ) = a as follows <ref> [35] </ref>: f (s i ) Q f (s i ; a) = c (s i ; a) + fl s j 2S From this, we are able to establish a policy, based on the current estimate Q f ; namely, select (s i ) = a such that, Q f i <p> Two approaches to solving MDPs are value iteration and policy iteration <ref> [35, 211, 23 269] </ref>. <p> Barto et al. <ref> [35, 37, 34] </ref> describe two alternative approaches to solving an MDP that appear to be less computationally expensive and are both forms of asynchronous dy 53 namic programming. <p> to ensure this is to require that all states be selected infinitely often as initial states. 54 One of the problems with methods such as asynchronous dynamic programming and RTDP is that these methods require a complete understanding of the transition probabilities, p (s 0 js; a), underlying the MDP <ref> [35] </ref>. They also require knowledge of the immediate costs, c (s; a). The requirement to know the cost holds, in particular, in the off-line case but can be relaxed when learning on line as in RTDP. <p> For convergence, the schedule for changing ff t (s; a) must conform to the following requirements [363, 364]. For all s 2 S and a 2 A, t=1 1 X ff t (s; a) 2 &lt; 1 One schedule that satisfies these requirements, proposed by Barto et al. <ref> [35] </ref>, is ff t (s; a) = t + n t (s; a) where ff 0 is the initial learning rate (applied to all state-action pairs), t is a user-defined parameter, and n t (s; a) is the number of times Q (s; a) has been updated at time t. <p> It has been shown that Q-learning is a special form of temporal difference learning where the "look-ahead" is cut off. Specifically, Q-learning is shown to be equivalent to TD (0) when there exists only one admissible action in each state <ref> [35, 103] </ref>. The temporal difference method is intended to be applied in "multi-step prediction problems" where payoff is not awarded until several steps after a prediction for payoff is made. This is exactly the problem that arises with delayed reinforcement.
Reference: [36] <author> A. Barto, R. Sutton, and C. Anderson. </author> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13 </volume> <pages> 835-846, </pages> <year> 1983. </year> <month> 230 </month>
Reference-contexts: Both lazy and eager approaches to reinforcement learning can be found in the literature. The most common eager approach is the use of temporal-difference learning on neural networks <ref> [36, 37, 81, 342] </ref>. The advantages to a lazy approach are three-fold. First, minimal computational time is required during training, because training consists primarily of storing examples (in the most traditional lazy approach, k-nearest neighbor). <p> One of the most successful approaches incorporated two separate neuro-controllers coupled together. Barto, Sutton, and Anderson described an adaptive critic element that provides a predicted reinforcement signal to a separate associative search element responsible for determining appropriate actions in a control problem <ref> [36] </ref>. The associative search element (ase) mapped state information into an appropriate control signal to maximize expected reinforcement. The reinforcement signal is used with information from previous actions to modify weights in a linear threshold unit.
Reference: [37] <author> A. Barto, R. Sutton, and C. Watkins. </author> <title> Learning and sequential decision making. </title> <editor> In Gabriel and Moore, editors, </editor> <booktitle> Learning and Computational Neuroscience, </booktitle> <pages> pages 539-602, </pages> <address> Cambridge, 1990. </address> <publisher> MIT Press. </publisher>
Reference-contexts: A key distinction between control systems and planning systems is the focus on achieving a specific goal; although, frequently the distinction between them becomes blurred [252]. 1.3 Learning and Intelligent Agents Recently, the machine learning community has paid increasing attention to problems of delayed reinforcement learning <ref> [37, 121, 179, 225, 228, 318, 368] </ref>. These problems usually involve an agent that has to make a sequence of decisions, or actions, in an environment that provides feedback about those decisions. The feedback about those decisions might be considerably delayed, and this delay makes learning much more difficult. <p> Barto et al. <ref> [35, 37, 34] </ref> describe two alternative approaches to solving an MDP that appear to be less computationally expensive and are both forms of asynchronous dy 53 namic programming. <p> Both lazy and eager approaches to reinforcement learning can be found in the literature. The most common eager approach is the use of temporal-difference learning on neural networks <ref> [36, 37, 81, 342] </ref>. The advantages to a lazy approach are three-fold. First, minimal computational time is required during training, because training consists primarily of storing examples (in the most traditional lazy approach, k-nearest neighbor).
Reference: [38] <author> T. Basar and P. Kumar. </author> <title> On worst case design strategies. Computers and Mathe matics with Applications, </title> <address> 13(1-3):239-245, </address> <year> 1987. </year>
Reference: [39] <author> T. Basar and G. Olsder. </author> <title> Dynamic Noncooperative Game Theory. </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1982. </year>
Reference-contexts: Generally, there are additional assumptions that both the car and the pedestrian are traveling at a fixed speed, the car has a fixed minimum radius of curvature, and the pedestrian is able to make arbitrarily sharp turns <ref> [39] </ref>. In the following, we present a summary of an analysis of this game by Isaacs [177] and Basar and Olsder [39]. <p> car and the pedestrian are traveling at a fixed speed, the car has a fixed minimum radius of curvature, and the pedestrian is able to make arbitrarily sharp turns <ref> [39] </ref>. In the following, we present a summary of an analysis of this game by Isaacs [177] and Basar and Olsder [39]. <p> What this means is if the ratio of the lethal range to the radius of curvature exceeds the maneuverability of the pedestrian at the designated speeds, then the pedestrian will be hit no matter what. From this analysis, Basar and Olsder <ref> [39] </ref> determine optimal strategies for the players to ensure the state trajectories follow the BUP. <p> In other words, P can make instantaneous turns between 45 ffi and +45 ffi while E can make instantaneous turns 174 between 90 ffi and +90 ffi . This game is a generalization of the Homicidal Chauffeur game <ref> [39, 177, 205] </ref>. In the Homicidal Chauffeur, only the mobility of P is limited. Given the added complexity of the game, no optimal solution was available; however, we were able to define a heuristic based on the optimal solution for the Homicidal Chauffeur. <p> Once again, we believe the reason for this unpredicted behavior is associated to the algorithm for selecting a node to split. As shown in Section 2.5, evasion depends on E getting inside P 's radius of curvature. According to Basar and Olsder <ref> [39] </ref>, this requirement is further complicated by a fact the BUP (boundary of the usable part) of the Homicidal Chauffeur game has a "leaky corner." A leaky corner is a characteristic in the surface between terminal conditions of the game in which performance cannot be forced by either player.
Reference: [40] <author> K. Basye, T. Dean, and L. Kaelbling. </author> <title> Learning dynamics: System identification for perceptually challenged agents. </title> <journal> Artificial Intelligence, </journal> <volume> 72, </volume> <year> 1995. </year>
Reference-contexts: Control theory focuses on developing optimal procedures for maintaining equilibrium or for achieving some performance objective by determining values of several "control variables" in a dynamic system <ref> [1, 26, 40, 45, 71, 100, 101, 156, 237, 252, 253] </ref>. Machine learning systems attempt to determine optimal values for these control variables from experience rather than explicitly solving a set of differential equations.
Reference: [41] <author> R. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1957. </year>
Reference-contexts: ) = a such that, Q f i (s i ; (s i )) = min Q f (s i ; a) This equation is in the form of the Bellman optimality equation which can be solved for all f (s j ) using several techniques such as dynamic programming <ref> [41] </ref>. Two approaches to solving MDPs are value iteration and policy iteration [35, 211, 23 269].
Reference: [42] <author> D. Belsley. </author> <title> Regression Diagnostics: Identifying Influential Data and Sources of Collinearity. </title> <publisher> Wiley & Sons, </publisher> <address> New York, </address> <year> 1980. </year>
Reference: [43] <author> S. Benson. </author> <title> Action model learning and action execution in a reactive agent. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: In addition to generating the examples, expected behaviors must also be learned. Within the memory-based framework, behaviors (or outcomes) can correspond to the estimate of expected payoff. In this case, any of the real-time dynamic programming or temporal difference methods can be applied to update these behaviors <ref> [16, 43, 44, 53, 210, 263, 264, 265] </ref>.
Reference: [44] <author> S. Benson. </author> <title> Inductive learning of reactive action models. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <year> 1995. </year>
Reference-contexts: In addition to generating the examples, expected behaviors must also be learned. Within the memory-based framework, behaviors (or outcomes) can correspond to the estimate of expected payoff. In this case, any of the real-time dynamic programming or temporal difference methods can be applied to update these behaviors <ref> [16, 43, 44, 53, 210, 263, 264, 265] </ref>.
Reference: [45] <author> S. Benson and N. Nilsson. </author> <title> Reacting, planning, and learning in an autonomous agent. </title> <booktitle> Machine Intelligence 14, </booktitle> <year> 1995. </year>
Reference-contexts: Control theory focuses on developing optimal procedures for maintaining equilibrium or for achieving some performance objective by determining values of several "control variables" in a dynamic system <ref> [1, 26, 40, 45, 71, 100, 101, 156, 237, 252, 253] </ref>. Machine learning systems attempt to determine optimal values for these control variables from experience rather than explicitly solving a set of differential equations.
Reference: [46] <author> J. Bentley. </author> <title> Multidimensional divide and conquer. </title> <journal> Communications of the ACM, </journal> <volume> 23(4) </volume> <pages> 214-229, </pages> <year> 1980. </year>
Reference-contexts: In this chapter, we consider an alternative algorithm that, although not memory-based, is inspired by the results of applying kd-trees in memory-based learning <ref> [46, 110, 128, 129, 237] </ref>. A kd-tree is a data structure used to store a set of examples in a memory base such that nearest neighbors can be found in logarithmic expected time.
Reference: [47] <author> P. Bernhard, A. Colomb, and G. Papavassilopoulos. Rabbit and hunter game: </author> <title> Two discrete stochastic formulations. Computers and Mathematics with Applications, </title> <address> 13(1-3):205-225, </address> <year> 1987. </year>
Reference-contexts: As an interesting discrete problem, similar to the Lady in the Lake [205], Bernhard et al. consider the Rabbit and Hunter game <ref> [47] </ref> under conditions of stationary and nonstationary dynamics. In both the Lady in the Lake and the Rabbit and Hunter games, one player is constrained to follow a boundary while the other player has free movement.
Reference: [48] <author> D. Bertsekas. </author> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1987. </year>
Reference-contexts: Barto et al. [35, 37, 34] describe two alternative approaches to solving an MDP that appear to be less computationally expensive and are both forms of asynchronous dy 53 namic programming. The first approach, which they refer to as "asynchronous dynamic programming," is a derivative of Gauss-Seidel dynamic programming <ref> [48] </ref> in which, for each state s and each time step t = 0; 1; : : :, update V t (s) as follows: V t (s) = min 2 X p (s 0 js; a)V (s 0 ) 5 a2A t (s; (s)) where V (s 0 ) = V
Reference: [49] <author> D. Blackwell and M. Girshick. </author> <title> Theory of Games and Statistical Decisions. </title> <publisher> Dover Publications, Inc., </publisher> <year> 1954. </year>
Reference-contexts: This, of course, is the simplest form of multi-agent learning. 3.3.3 Reinforcement Learning in Cognitive Game Theory In the economics community, one of the communities responsible for considerable research in game theory <ref> [49, 123, 130, 196, 286, 358, 360] </ref>, limited work is being done in learning game strategies. This work, however, is largely restricted to single-player games.
Reference: [50] <author> L. Booker, D. Goldberg, and J. Holland. </author> <title> Classifier systems and genetic algo rithms. </title> <journal> Artificial Intelligence, </journal> <volume> 40 </volume> <pages> 235-282, </pages> <year> 1989. </year>
Reference-contexts: As with other methods of learning to solve MDPs, GAs must associate sets of admissible actions to states in the problem and provide a means to select the most appropriate action to maximize expected payoff. The first application of GAs to MDPs arose from work in classifier systems <ref> [50, 149, 167, 168] </ref>. In a classifier system, rules are binary condition/action pairs that work within a message-passing architecture (Figure 3.1). State information is converted into a binary message and placed on the message list. <p> Typically, GAs use rules called classifiers, which are simple structures in which terms in the antecedent and the consequent are represented as binary attributes <ref> [50, 167] </ref>. The knowledge for the evasive maneuvers problem requires rules in which the terms have numeric values; we therefore modified the standard GA representation and operators for this problem, using a formulation similar to [153]. We call a set of rules a plan.
Reference: [51] <author> D. Borrajo and M. Veloso. </author> <title> Lazy incremental learning of control knowledge for efficiently obtaining quality plans. </title> <journal> Artificial Intelligence Review, </journal> <note> 1996. To appear. 231 </note>
Reference: [52] <author> J. Boyan. </author> <title> Modular neural networks for learning context-dependent game strategies. </title> <type> Master's thesis, </type> <institution> University of Cambridge, </institution> <year> 1992. </year>
Reference: [53] <author> J. Boyan and A. Moore. </author> <title> Generalization in reinforcement learning: Safely approx imating the value function. </title> <booktitle> In Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: In addition to generating the examples, expected behaviors must also be learned. Within the memory-based framework, behaviors (or outcomes) can correspond to the estimate of expected payoff. In this case, any of the real-time dynamic programming or temporal difference methods can be applied to update these behaviors <ref> [16, 43, 44, 53, 210, 263, 264, 265] </ref>. <p> For Q-learning, the traditional lookup table was not capable of modeling the state and action space; therefore, a function approximator (i.e., a memory-based method) was required to cover the space. In spite of problems anticipated in using memory based approaches to approximate value functions with Q-learning <ref> [53] </ref>, strong performance was still demonstrated. In the next chapter, we discuss an additional modification in which we couple a second learning algorithm|the genetic algorithm|to 1-NN as a teacher.
Reference: [54] <author> S. Bradtke. </author> <title> Reinforcement learning applied to linear quadratic regulation. </title> <booktitle> In Neural Information Processing Systems 5, </booktitle> <pages> pages 295-302, </pages> <year> 1993. </year>
Reference: [55] <author> S. Bradtke and S Duff. </author> <title> Reinforcement learning methods for continuous time markov decision problems. </title> <booktitle> In Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 393-400. </pages> <publisher> The MIT Press, </publisher> <year> 1995. </year>
Reference: [56] <author> R. Brafman and M. Tennenholtz. </author> <title> On partially controlled multi-agent systems. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 3 </volume> <pages> 477-507, </pages> <year> 1996. </year>
Reference-contexts: Also, migration patterns of artificial birds have been evolved. In none of these cases has behavior of individual agents been the focus of the research. Recently, work has begun to appear that focuses on learning in multi-agent systems <ref> [56, 65, 150, 303, 328, 327, 329, 341] </ref>. Stone and Veloso provide a taxonomy of multi-agent systems by focusing on attributes such as agent homogeneity, communication, deliberative versus reactive control, and number of agents [329].
Reference: [57] <author> S. Brams. </author> <title> Theory of Moves. </title> <publisher> Cambridge University Press, </publisher> <year> 1994. </year>
Reference: [58] <author> J. Breese and D. Heckerman. </author> <title> Decision-theoretic case-based reasoning. </title> <type> Technical Report MSR-TR-95-03, </type> <institution> Microsoft Research, </institution> <year> 1995. </year>
Reference: [59] <author> L. Breiman. </author> <title> The method for estimating multivariate functions from noisy data. </title> <journal> Technometrics, </journal> <volume> 33(2) </volume> <pages> 125-143, </pages> <year> 1991. </year>
Reference: [60] <author> L. Breiman. </author> <title> Stacked regressions. </title> <type> Technical Report TR-367, </type> <institution> Department of Statis tics, University of California at Berkeley, </institution> <year> 1992. </year>
Reference: [61] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regres sion Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <year> 1984. </year>
Reference-contexts: Some of the types of problems explored include data classification, data mining, automatic programming, control theory, and planning. Classification systems identify a concept class from a set of available classes to which a particular example belongs <ref> [3, 10, 61, 91, 119, 182] </ref>. Data classification usually proceeds from a set of available attributes and associated values. A training set is used to present examples of concept classes, and the classifier constructed is designed to be consistent with that training set.
Reference: [62] <author> H. Bremermann and J. Pickering. </author> <title> A game-theoretical model of parasite viru lence. </title> <journal> Journal of Theoretical Biology, </journal> <volume> 100 </volume> <pages> 411-426, </pages> <year> 1983. </year>
Reference-contexts: Robert Collins [83] explored issues of sexual selection and female choice in a predatory environment, co-evolution of hosts and parasites, and foraging behavior among artificial ants. Other investigations of host/parasite co-existence and co-evolution have taken a game-theoretic view, but work by Bremermann and Pickering focuses on natural evolution <ref> [62] </ref>. In another biological study, Hamilton considered the effects of competition between hosts and parasites in the evolutionary process [160].
Reference: [63] <author> M. Breton. </author> <title> Approximate solutions to continuous stochastic games. </title> <editor> In R. Hamalainen and H. Ehtamo, editors, </editor> <booktitle> Differential Games|Developments in Modeling and Computation, </booktitle> <pages> pages 257-264. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference: [64] <author> M. Buro. </author> <title> Statistical feature combination for the evaluation of game positions. </title> <journal> Jour nal of Artificial Intelligence Research, </journal> <volume> 3 </volume> <pages> 373-382, </pages> <year> 1995. </year>
Reference: [65] <author> C. Byrne and P. Edwards. </author> <title> Collaborating to refine knowledge. </title> <booktitle> In Proceedings of the ML95 Workshop on Agents that Learn from Other Agents, </booktitle> <year> 1995. </year>
Reference-contexts: Also, migration patterns of artificial birds have been evolved. In none of these cases has behavior of individual agents been the focus of the research. Recently, work has begun to appear that focuses on learning in multi-agent systems <ref> [56, 65, 150, 303, 328, 327, 329, 341] </ref>. Stone and Veloso provide a taxonomy of multi-agent systems by focusing on attributes such as agent homogeneity, communication, deliberative versus reactive control, and number of agents [329].
Reference: [66] <author> C. Cardie. </author> <title> Using decision trees to improve case-based learning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 25-32, </pages> <institution> University of Massachusetts, Amherst, </institution> <year> 1993. </year> <month> 232 </month>
Reference-contexts: Data mining stems from work in statistics and unsupervised learning and has drawn from ideas taken from automatic discovery <ref> [2, 66, 79, 124, 185] </ref>. Many approaches to data mining use techniques such as clustering, rule induction, and classification to analyze the large set of data. From this set of data, data mining systems attempt to induce general "laws" and classification procedures for characterizing the data.
Reference: [67] <author> D. Carmel and S. Markovich. </author> <title> The M * algorithm: Incorporating opponent mod els into adversary search. </title> <type> Technical Report CIS-9402, </type> <institution> Technion-Israel Institute of Technology, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: For example, a pursuer may have a long range and be able to pursue the evader for a long period of time, but the evader may assume (given the nature of the encounter) that the range is more limited. This problem is addressed through techniques such as opponent modeling <ref> [68, 67, 69, 243] </ref>. * The game environment is not fully described. This differs from games of imperfect information in that the environment can be learned from experience. <p> Recent work by Carmel and Markovitch suggests that a more realistic approach is for a player to adapt to the current opponent. This led them to develop their M * algorithm in which a player learns its opponent's strategy and adapts accordingly 84 <ref> [68, 67, 69] </ref>. Others have provided similar strategies to modeling opponents, such as the agent tracking methods of Tambe [336, 337, 338, 339, 340], and the equilibrium search methods of Goldman and Rosenschein [144, 243] and of Koller [194, 195, 196, 197, 359].
Reference: [68] <author> D. Carmel and S. Markovitch. </author> <title> Learning models of opponent's strategy in game playing. </title> <type> Technical Report CIS-9318, </type> <institution> Technion-Israel Institute of Technology, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: For example, a pursuer may have a long range and be able to pursue the evader for a long period of time, but the evader may assume (given the nature of the encounter) that the range is more limited. This problem is addressed through techniques such as opponent modeling <ref> [68, 67, 69, 243] </ref>. * The game environment is not fully described. This differs from games of imperfect information in that the environment can be learned from experience. <p> Recent work by Carmel and Markovitch suggests that a more realistic approach is for a player to adapt to the current opponent. This led them to develop their M * algorithm in which a player learns its opponent's strategy and adapts accordingly 84 <ref> [68, 67, 69] </ref>. Others have provided similar strategies to modeling opponents, such as the agent tracking methods of Tambe [336, 337, 338, 339, 340], and the equilibrium search methods of Goldman and Rosenschein [144, 243] and of Koller [194, 195, 196, 197, 359].
Reference: [69] <author> D. Carmel and S. Markovitch. </author> <title> Opponent modeling in an multi-agent system. </title> <booktitle> In Proceedings of the IJCAI '95 Workshop on Adaptation and Learning in Multiagent Systems, </booktitle> <year> 1995. </year>
Reference-contexts: For example, a pursuer may have a long range and be able to pursue the evader for a long period of time, but the evader may assume (given the nature of the encounter) that the range is more limited. This problem is addressed through techniques such as opponent modeling <ref> [68, 67, 69, 243] </ref>. * The game environment is not fully described. This differs from games of imperfect information in that the environment can be learned from experience. <p> Recent work by Carmel and Markovitch suggests that a more realistic approach is for a player to adapt to the current opponent. This led them to develop their M * algorithm in which a player learns its opponent's strategy and adapts accordingly 84 <ref> [68, 67, 69] </ref>. Others have provided similar strategies to modeling opponents, such as the agent tracking methods of Tambe [336, 337, 338, 339, 340], and the equilibrium search methods of Goldman and Rosenschein [144, 243] and of Koller [194, 195, 196, 197, 359].
Reference: [70] <author> A. Cassandra. </author> <title> Optimal policies for partially observable markov decision processes. </title> <type> Technical Report CS-94-14, </type> <institution> Department of Computer Science, Brown, University, </institution> <year> 1994. </year>
Reference-contexts: Imado and Ishihara also consider the case where two missiles attempt to shoot down an airplane [175]. Lai and Tanaka consider general n-person games where each player attempts to force opponents into a terminal location in the playing field [200]. Related to work in partially observable Markov decision processes <ref> [70, 71, 180] </ref>, Galperin and Skowronski are studying games in which noise is introduced into game dynamics [134]. Corless et al. consider the case where state information is uncertain [88].
Reference: [71] <author> A. Cassandra, L. Kaelbling, and M. Littman. </author> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1023-1028. </pages> <publisher> AAAI, </publisher> <year> 1994. </year>
Reference-contexts: Control theory focuses on developing optimal procedures for maintaining equilibrium or for achieving some performance objective by determining values of several "control variables" in a dynamic system <ref> [1, 26, 40, 45, 71, 100, 101, 156, 237, 252, 253] </ref>. Machine learning systems attempt to determine optimal values for these control variables from experience rather than explicitly solving a set of differential equations. <p> Imado and Ishihara also consider the case where two missiles attempt to shoot down an airplane [175]. Lai and Tanaka consider general n-person games where each player attempts to force opponents into a terminal location in the playing field [200]. Related to work in partially observable Markov decision processes <ref> [70, 71, 180] </ref>, Galperin and Skowronski are studying games in which noise is introduced into game dynamics [134]. Corless et al. consider the case where state information is uncertain [88].
Reference: [72] <author> W. Chan and S. Ng. </author> <title> Partially observable linear-quadratic stochastic pursuit-evasion games. Computers and Mathematics with Applications, </title> <address> 13(1-3):181-189, </address> <year> 1987. </year>
Reference-contexts: Related to work in partially observable Markov decision processes [70, 71, 180], Galperin and Skowronski are studying games in which noise is introduced into game dynamics [134]. Corless et al. consider the case where state information is uncertain [88]. Chan and Ng consider partial observability in linear-quadratic games <ref> [72] </ref>, and Yavin considers 39 the case where the process of observing the state is deceptive or is somehow interrupted [375]. 2.7 A Simple Differential Game To illustrate the problems associated with solving (and ultimately learning) differential games, we provide a simple example of players attempting to move an object in
Reference: [73] <author> C. Chang. </author> <title> Finding prototypes for nearest neighbor classifiers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 23(11) </volume> <pages> 1179-1184, </pages> <month> November </month> <year> 1974. </year>
Reference: [74] <author> D. Chapman. </author> <title> Planning for conjunctive goals. </title> <journal> Artificial Intelligence, </journal> <volume> 32 </volume> <pages> 333-377, </pages> <year> 1987. </year>
Reference-contexts: Finally, planning attempts to determine optimal strategies for an agent to apply in performing some task <ref> [74, 106, 107, 131, 288] </ref>. These strategies usually consist of sequences of steps to perform, and generally, the objective is to reach some terminal state (as opposed to maintaining an equilibrium as in control problems). Classic planning problems in artificial intelligence have included stacking blocks and navigating mazes.
Reference: [75] <author> D. Chapman and P. Agre. Pengi: </author> <title> An implementation of a theory of activity. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <year> 1987. </year>
Reference-contexts: Thus, as advice is received, that advice is encoded as a kbann network and attached to the current network. Weights are modified from there using the Q-update procedure. 133 The kbann advice-taker was tested on the Pengo task as studied by Chapman and Agre (called Pengi in their work) <ref> [75] </ref>. Pengo involves an agent moving obstacles in a field with food and enemies. The object is to collect the most food. Obstacles hitting food destroys food, and enemies can capture food as well. Typically, the enemies follow a fixed strategy.
Reference: [76] <author> P. Cichosz. </author> <title> Reinforcement learning algorithms based on the methods of temporal differences. </title> <type> Master's thesis, </type> <institution> Warsaw University of Technology Institute of Computer Science, </institution> <year> 1994. </year>
Reference: [77] <author> P. Cichosz. </author> <title> Truncating temporal differences: On the efficient implementation of TD() for reinforcement learning. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 287-318, </pages> <year> 1995. </year>
Reference: [78] <author> P. Cichosz and J. Mulawka. </author> <title> Fast and efficient reinforcement learning with trun cated temporal differences. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <year> 1995. </year>
Reference: [79] <author> P. Clark and T. Niblett. </author> <title> Induction in noisy domains. </title> <booktitle> In Proceedings of the second European workshop on Machine Learning, </booktitle> <year> 1987. </year> <month> 233 </month>
Reference-contexts: Data mining stems from work in statistics and unsupervised learning and has drawn from ideas taken from automatic discovery <ref> [2, 66, 79, 124, 185] </ref>. Many approaches to data mining use techniques such as clustering, rule induction, and classification to analyze the large set of data. From this set of data, data mining systems attempt to induce general "laws" and classification procedures for characterizing the data.
Reference: [80] <author> W. Cleveland, S. Devlin, and E. Grosse. </author> <title> Regression by local fitting: Methods, properties, and computational algorithms. </title> <journal> Journal of Econometrics, </journal> <volume> 37 </volume> <pages> 87-114, </pages> <year> 1987. </year>
Reference: [81] <author> J. Clouse and P. Utgoff. </author> <title> A teaching method for reinforcement learning. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pages 92-101, </pages> <address> Aberdeen, Scotland, 1992. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Both lazy and eager approaches to reinforcement learning can be found in the literature. The most common eager approach is the use of temporal-difference learning on neural networks <ref> [36, 37, 81, 342] </ref>. The advantages to a lazy approach are three-fold. First, minimal computational time is required during training, because training consists primarily of storing examples (in the most traditional lazy approach, k-nearest neighbor). <p> We describe such an approach in Section 5.3. 5.2.2 Adding a Teacher in Reinforcement Learning Extending the work of Barto et al. on ace/ase, Clouse and Utgoff provided a method for incorporating a separate teacher to guide the learning process <ref> [81] </ref>. They also explored a modification to Q-learning using a similar teacher. In both cases, eligibility traces were used to smooth the learning process. <p> Such a Helpful Teacher is similar to the oracle used by Clouse and Utgoff <ref> [81] </ref> except that it provides the theoretically minimal number of examples required for learning. 149 Chapter 6 Memory-Based Co-Learning 6.1 Learning "Solutions" to Differential Games In static games, the strategies of all players are generally known. Further, in solving static games, payoffs assigned to selected strategy combinations are also known.
Reference: [82] <author> D. Cohn, Z. Ghahramani, and M. Jordan. </author> <title> Active learning with statistical mod els. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 129-145, </pages> <year> 1996. </year>
Reference: [83] <author> R. Collins. </author> <title> Studies in Artificial Evolution. </title> <type> PhD thesis, </type> <institution> University of California at Los Angeles, </institution> <address> Los Angeles, California, </address> <year> 1992. </year>
Reference-contexts: To date, research in multiple agent planning and control has been limited largely to the area of distributed artificial intelligence [138, 283, 329, 330, 331, 341, 356] and artificial 5 life <ref> [83, 116, 117, 172, 299, 325] </ref>. In distributed AI (DAI), several agents cooperate to achieve some goal or accomplish some task. The task is usually one of sufficient complexity that no single agent can accomplish the task alone. <p> Research in this area focuses on population and evolution dynamics; therefore, we should expect them to be concerned with competition among multiple populations. This work, however, does not consider competition on an individual basis. Robert Collins <ref> [83] </ref> explored issues of sexual selection and female choice in a predatory environment, co-evolution of hosts and parasites, and foraging behavior among artificial ants. Other investigations of host/parasite co-existence and co-evolution have taken a game-theoretic view, but work by Bremermann and Pickering focuses on natural evolution [62].
Reference: [84] <author> M. Colombetti and M. Dorigo. </author> <title> Training agents to perform sequential behavior. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 2(3) </volume> <pages> 247-275, </pages> <year> 1994. </year>
Reference-contexts: In both cases, significant speedup over using reinforcement learning without a teacher was demonstrated. 5.2.3 Phases of Learning Research by Dorigo and Colombetti focused on what they call the "three stages of development" consisting of a baby phase, a young phase, and an adult phase <ref> [84, 85, 115] </ref>. During the baby phase, immediate reinforcement is provided by a trainer after every action. This continues until the learner reaches some performance threshold. At that point, the learner passes into the young phase. In this phase, reinforcement is provided only by the environment and is generally delayed.
Reference: [85] <author> M. Colombetti and M. Dorigo. </author> <title> Behavior analysis and training: A methodology for behavior engineering. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 26(6), </volume> <year> 1996. </year>
Reference-contexts: In both cases, significant speedup over using reinforcement learning without a teacher was demonstrated. 5.2.3 Phases of Learning Research by Dorigo and Colombetti focused on what they call the "three stages of development" consisting of a baby phase, a young phase, and an adult phase <ref> [84, 85, 115] </ref>. During the baby phase, immediate reinforcement is provided by a trainer after every action. This continues until the learner reaches some performance threshold. At that point, the learner passes into the young phase. In this phase, reinforcement is provided only by the environment and is generally delayed.
Reference: [86] <author> D. Comer and R. Sethi. </author> <title> The complexity of trie index construction. </title> <journal> Journal of the ACM, </journal> <volume> 24(3) </volume> <pages> 428-440, </pages> <month> July </month> <year> 1977. </year>
Reference: [87] <author> S. Conry, R. Meyer, and V. Lesser. </author> <title> Multistage negotiation in distributed plan ning. </title> <type> Technical Report COINS Technical Report 86-67, </type> <institution> University of Massachusetts, Amherst, Massachusetts, </institution> <year> 1986. </year>
Reference-contexts: Although not focusing on competitive situations, Conry et al. considered planning in distributed systems when the task requires multiple, intermediate milestones to be reached, and resources are traded between the agents <ref> [87] </ref>. 3.5 Summary Research in multi-agent learning is just recently receiving extensive attention in the machine learning community.
Reference: [88] <author> M. Corless, G. Leitmann, and J. Skowronski. </author> <title> Adaptive control for avoidance or evasion in an uncertain environment. Computers and Mathematics with Applications, </title> <address> 13(1-3):1-11, </address> <year> 1987. </year>
Reference-contexts: Related to work in partially observable Markov decision processes [70, 71, 180], Galperin and Skowronski are studying games in which noise is introduced into game dynamics [134]. Corless et al. consider the case where state information is uncertain <ref> [88] </ref>.
Reference: [89] <author> T. Cormen, C. Leiserson, and R. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press and McGraw-Hill Book Company, </publisher> <year> 1990. </year>
Reference-contexts: Initially, parti-game defines a partitioning of the space and attempts to determine the shortest path from each partition to the partition containing the goal. This can be done using an all-pairs shortest path algorithm such as Dijkstra's algorithm <ref> [89] </ref>. Because this strategy alone can lead to situations where the controller gets hopelessly stuck, parti-game approximates the number of steps to the goal based on the worst-case scenario.
Reference: [90] <author> S. Cost and S. Salzberg. </author> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10(1) </volume> <pages> 57-78, </pages> <year> 1993. </year>
Reference-contexts: d 1 and d 2 respectively for some 103 state-action pair in the game, then the stored state-action pair's Q value is updated. 4.3.2 1-NN for Evasive Maneuvers Memory-based learning is a classical approach to machine learning and pattern recognition, most commonly in the form of the 1-nearest neighbor algorithm <ref> [3, 10, 90, 119, 292, 293, 366] </ref>. 1-NN is rarely used for Markov decision problems, so we had to represent the pursuit game in a format amenable to this algorithm.
Reference: [91] <author> T. Cover and P. Hart. </author> <title> Nearest neighbor pattern classification. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 13 </volume> <pages> 21-27, </pages> <year> 1967. </year>
Reference-contexts: Some of the types of problems explored include data classification, data mining, automatic programming, control theory, and planning. Classification systems identify a concept class from a set of available classes to which a particular example belongs <ref> [3, 10, 61, 91, 119, 182] </ref>. Data classification usually proceeds from a set of available attributes and associated values. A training set is used to present examples of concept classes, and the classifier constructed is designed to be consistent with that training set.
Reference: [92] <author> T. Cover and J. van Campenhout. </author> <title> On the possible orderings in the measurement selection problems. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> SMC-7(9), </volume> <year> 1977. </year>
Reference: [93] <author> M. Cox and A. Ram. </author> <title> Using introspective reasoning to select learning strategies. </title> <booktitle> In Proceedings of the First International Workshop on Multistrategy Learning, </booktitle> <year> 1991. </year> <month> 234 </month>
Reference: [94] <author> M. Cox and A. Ram. </author> <title> Multistrategy learning with introspective meta-explanations. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pages 123-128, </pages> <year> 1992. </year>
Reference: [95] <author> M. Cox and A. Ram. </author> <title> Choosing learning strategies to achieve learning goals. </title> <booktitle> In Proceedings of the AAAI Spring Symposium on Goal-Driven Learning, </booktitle> <year> 1994. </year>
Reference: [96] <author> R. Crites and A. Barto. </author> <title> An actor/critic algorithm that is equivalent to Q-learning. </title> <booktitle> In Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference: [97] <author> R. Crites and A. Barto. </author> <title> Improving elevator performance using reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference: [98] <author> B. Dasarathy, </author> <title> editor. Nearest neighbor (NN) norms: NN pattern classification tech niques. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1991. </year>
Reference-contexts: Consequently, we decided to take this study one step further, and attempted to reduce the size of the memory store during the memory-based learning phase of GAMB [321, 377]. In the pattern recognition literature, e.g., in <ref> [98] </ref>, algorithms for reducing memory size are known as editing methods. However, because memory-based learning is not usually applied to control tasks, we were not able to find any editing methods specifically tied to our type of problem.
Reference: [99] <author> B. Dasarathy. </author> <title> Minimal consistent set (MCS) identification for optimal near est neighbor systems design. </title> <journal> IEEE transactions on systems, man and cybernetics, </journal> <volume> 24(3) </volume> <pages> 511-517, </pages> <year> 1994. </year>
Reference: [100] <author> D Dasgupta and D. McGregor. </author> <title> Evolving neurocontrollers for pole balancing. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1993. </year>
Reference-contexts: Control theory focuses on developing optimal procedures for maintaining equilibrium or for achieving some performance objective by determining values of several "control variables" in a dynamic system <ref> [1, 26, 40, 45, 71, 100, 101, 156, 237, 252, 253] </ref>. Machine learning systems attempt to determine optimal values for these control variables from experience rather than explicitly solving a set of differential equations.
Reference: [101] <author> D. Dasgupta and D. McGregor. </author> <title> Genetically designing neuro-controllers for a dynamic system. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, Nagoya (Japan), </booktitle> <pages> pages 2869-2872. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: Control theory focuses on developing optimal procedures for maintaining equilibrium or for achieving some performance objective by determining values of several "control variables" in a dynamic system <ref> [1, 26, 40, 45, 71, 100, 101, 156, 237, 252, 253] </ref>. Machine learning systems attempt to determine optimal values for these control variables from experience rather than explicitly solving a set of differential equations.
Reference: [102] <author> Y Davidor. </author> <title> A naturally occurring niche and species phenomenon: The model and first results. </title> <booktitle> In Proceedings of the 1991 International Conference on Genetic Algorithms, </booktitle> <pages> pages 257-263, </pages> <year> 1991. </year>
Reference-contexts: In another biological study, Hamilton considered the effects of competition between hosts and parasites in the evolutionary process [160]. To promote distribution of skills among populations, Davidor <ref> [102] </ref> studied the effects of niching and speciation in 88 genetic algorithms, and Werner and Dyer focused on developing communication mechanisms between organisms through artificial evolution [365]. Some work has been done in evolutionary computation, artificial life, and iterated games.
Reference: [103] <author> P. Dayan. </author> <title> The convergence of TD() for general . Machine Learning, </title> <booktitle> 8 </booktitle> <pages> 341-362, </pages> <year> 1992. </year>
Reference-contexts: It has been shown that Q-learning is a special form of temporal difference learning where the "look-ahead" is cut off. Specifically, Q-learning is shown to be equivalent to TD (0) when there exists only one admissible action in each state <ref> [35, 103] </ref>. The temporal difference method is intended to be applied in "multi-step prediction problems" where payoff is not awarded until several steps after a prediction for payoff is made. This is exactly the problem that arises with delayed reinforcement.
Reference: [104] <author> P. Dayan and G. Hinton. </author> <title> Feudal reinforcement learning. </title> <booktitle> In Neural Information Processing Systems 5, </booktitle> <pages> pages 271-278, </pages> <year> 1993. </year>
Reference-contexts: Several researchers have investigated approaches to varying the resolution of the memory base according to the requirements of the problem <ref> [104, 110, 240, 312] </ref>. For example, Moore and Atkeson's parti-game algorithm uses the concept of an adversary attempting to thwart search to determine how to explore the search space [240].
Reference: [105] <author> P. Dayan and T. Sejnowski. </author> <title> TD() converges with probability 1. </title> <journal> Machine Learn ing, </journal> <volume> 14 </volume> <pages> 295-301, </pages> <year> 1994. </year>
Reference: [106] <author> T. Dean, K. Basye, and J. Shewchuk. </author> <title> Reinforcement learning for planning and control. </title> <editor> In S. Minton, editor, </editor> <title> Machine Learning Methods for Planning and Scheduling. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Finally, planning attempts to determine optimal strategies for an agent to apply in performing some task <ref> [74, 106, 107, 131, 288] </ref>. These strategies usually consist of sequences of steps to perform, and generally, the objective is to reach some terminal state (as opposed to maintaining an equilibrium as in control problems). Classic planning problems in artificial intelligence have included stacking blocks and navigating mazes.
Reference: [107] <author> T. Dean, L. Kaelbling, J. Kirman, and A. Nicholson. </author> <title> Planning under time constraints in stochastic domains. </title> <journal> Artificial Intelligence, </journal> <volume> 76, </volume> <year> 1995. </year> <month> 235 </month>
Reference-contexts: Finally, planning attempts to determine optimal strategies for an agent to apply in performing some task <ref> [74, 106, 107, 131, 288] </ref>. These strategies usually consist of sequences of steps to perform, and generally, the objective is to reach some terminal state (as opposed to maintaining an equilibrium as in control problems). Classic planning problems in artificial intelligence have included stacking blocks and navigating mazes.
Reference: [108] <author> K. DeJong. </author> <title> Genetic algorithm based learning. </title> <editor> In Y. Kodratoff and R. Michalski, editors, </editor> <booktitle> Machine learning, and Artificial Intelligence approach, </booktitle> <volume> volume 3, </volume> <pages> pages 611-638. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference: [109] <author> K. DeJong and A. Schultz. </author> <title> Using experience-based learning in game playing. </title> <booktitle> In Proceedings of the Fifth International Machine Learning Conference, </booktitle> <pages> pages 284-290. </pages> <publisher> Morgan Kaufmann, Publishers, </publisher> <year> 1988. </year>
Reference-contexts: The feedback about those decisions might be considerably delayed, and this delay makes learning much more difficult. In this case, the problem is called a delayed reinforcement problem <ref> [109, 167, 168, 220, 221, 317, 343, 363] </ref>. The basic loop followed in sequential decision making tasks such as these includes evaluating the current state, taking an action, and computing the new state.
Reference: [110] <author> K. Deng and A. Moore. </author> <title> Multiresolution instance-based learning. </title> <booktitle> In Proceedings of the 1995 International Joint Conference on Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: Then it becomes important to be able to interpolate between actions within a region. One approach to combating these problems has been proposed by Atkeson, Moore, and Schaal <ref> [26, 27, 25, 110, 237] </ref>. In their approach, they apply local weighted regression among the examples to determine the proper action. <p> Several researchers have investigated approaches to varying the resolution of the memory base according to the requirements of the problem <ref> [104, 110, 240, 312] </ref>. For example, Moore and Atkeson's parti-game algorithm uses the concept of an adversary attempting to thwart search to determine how to explore the search space [240]. <p> Initial Q values did not matter since they would be learned over time. Nevertheless, a uniform random sampling of the space was, apparently, not sufficient to approximate some of the surfaces encountered in these games. Applications of variable resolution techniques <ref> [16, 110, 240, 312] </ref> may be more appropriate for problems such as these. In some ways, the results from MBCL are highly encouraging. They indicate co-learning can occur and suggest it is possible to learn optimal solutions to two-player differential games. <p> In this chapter, we consider an alternative algorithm that, although not memory-based, is inspired by the results of applying kd-trees in memory-based learning <ref> [46, 110, 128, 129, 237] </ref>. A kd-tree is a data structure used to store a set of examples in a memory base such that nearest neighbors can be found in logarithmic expected time.
Reference: [111] <author> P. Devijver. </author> <title> On the editing rate of the multiedit algorithm. </title> <journal> In Pattern Recognition Letters, </journal> <volume> volume 4, </volume> <pages> pages 9-12, </pages> <year> 1986. </year>
Reference-contexts: The weight for t is determined based on its distance from nn and frequently decreases exponentially as distance increases. 5.4.2 An Editing Algorithm for Evasive Maneuvers The editing approach we took combined the editing procedure of Ritter et al. and a variation on the sampling idea of Tomek <ref> [111] </ref>. We began by generating ten example sets with = 90 where each set consisted of a single set of examples from the GA. We then selected the set with the best performance on 10,000 test games, which in this case obtained nearly perfect 143 accuracy with 1,700 examples.
Reference: [112] <author> P. Devijver and J. Kittler. </author> <title> Pattern Recognition: A Statistical Approach. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1982. </year>
Reference-contexts: We experimented with a method similar to stepwise forward selection <ref> [112] </ref> to determine the set of relevant attributes. However, determining relevant attributes in a dynamic environment is difficult for the same reason that determining good examples is difficult: we do not know which attributes to use until many successful examples have been generated.
Reference: [113] <author> B. Dike and R. Smith. </author> <title> Application of genetic algorithms to air combat maneuver ing. </title> <type> Technical Report TCGA Report No. 93002, </type> <institution> University of Alabama, Tuscaloosa, Alabama, </institution> <year> 1993. </year>
Reference: [114] <author> D. Dobkin and S. Reiss. </author> <title> The complexity of linear programming. </title> <journal> Theoretical Computer Science, </journal> <volume> 11 </volume> <pages> 1-18, </pages> <year> 1980. </year>
Reference: [115] <author> M. Dorigo and M. Colombetti. </author> <title> Robot shaping: Developing autonomous agents through learning. </title> <journal> Artificial Intelligence, </journal> <volume> 71(2) </volume> <pages> 321-370, </pages> <year> 1994. </year>
Reference-contexts: In both cases, significant speedup over using reinforcement learning without a teacher was demonstrated. 5.2.3 Phases of Learning Research by Dorigo and Colombetti focused on what they call the "three stages of development" consisting of a baby phase, a young phase, and an adult phase <ref> [84, 85, 115] </ref>. During the baby phase, immediate reinforcement is provided by a trainer after every action. This continues until the learner reaches some performance threshold. At that point, the learner passes into the young phase. In this phase, reinforcement is provided only by the environment and is generally delayed.
Reference: [116] <author> M. Dorigo, V. Maniezzo, and A. Colorni. </author> <title> The ant system: An autocatalytic optimizing process. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <year> 1994. </year>
Reference-contexts: To date, research in multiple agent planning and control has been limited largely to the area of distributed artificial intelligence [138, 283, 329, 330, 331, 341, 356] and artificial 5 life <ref> [83, 116, 117, 172, 299, 325] </ref>. In distributed AI (DAI), several agents cooperate to achieve some goal or accomplish some task. The task is usually one of sufficient complexity that no single agent can accomplish the task alone. <p> So far, most work in learning and multi-agent systems has focused on multiple agents' learning 6 complementary behaviors in a coordinated environment to accomplish some task, such as team game playing [330, 337, 338, 341], combinatorial optimization <ref> [116, 117] </ref>, and obstacle avoidance [150]. The research discussed in this dissertation combines work in control and planning in the context of competitive multi-agent systems. In particular, we focus on exploring methods for the on-line learning of optimal strategies for playing differential games.
Reference: [117] <author> M. Dorigo, V. Maniezzo, and A. Colorni. </author> <title> The ant system: Optimization by a colony of cooperating agents. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 26(1) </volume> <pages> 1-13, </pages> <year> 1996. </year>
Reference-contexts: To date, research in multiple agent planning and control has been limited largely to the area of distributed artificial intelligence [138, 283, 329, 330, 331, 341, 356] and artificial 5 life <ref> [83, 116, 117, 172, 299, 325] </ref>. In distributed AI (DAI), several agents cooperate to achieve some goal or accomplish some task. The task is usually one of sufficient complexity that no single agent can accomplish the task alone. <p> So far, most work in learning and multi-agent systems has focused on multiple agents' learning 6 complementary behaviors in a coordinated environment to accomplish some task, such as team game playing [330, 337, 338, 341], combinatorial optimization <ref> [116, 117] </ref>, and obstacle avoidance [150]. The research discussed in this dissertation combines work in control and planning in the context of competitive multi-agent systems. In particular, we focus on exploring methods for the on-line learning of optimal strategies for playing differential games.
Reference: [118] <author> M. Dorigo and U. Schnepf. </author> <title> Genetics-based machine learning and behavior based robotics: A new synthesis. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 23(1) </volume> <pages> 141-154, </pages> <year> 1993. </year>
Reference: [119] <author> R. Duda and P. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Some of the types of problems explored include data classification, data mining, automatic programming, control theory, and planning. Classification systems identify a concept class from a set of available classes to which a particular example belongs <ref> [3, 10, 61, 91, 119, 182] </ref>. Data classification usually proceeds from a set of available attributes and associated values. A training set is used to present examples of concept classes, and the classifier constructed is designed to be consistent with that training set. <p> d 1 and d 2 respectively for some 103 state-action pair in the game, then the stored state-action pair's Q value is updated. 4.3.2 1-NN for Evasive Maneuvers Memory-based learning is a classical approach to machine learning and pattern recognition, most commonly in the form of the 1-nearest neighbor algorithm <ref> [3, 10, 90, 119, 292, 293, 366] </ref>. 1-NN is rarely used for Markov decision problems, so we had to represent the pursuit game in a format amenable to this algorithm.
Reference: [120] <author> G. Dueck and T. Scheuer. </author> <title> Threshold accepting: A general purpose optimiza tion algorithm appearing superior to simulated annealing. </title> <journal> Journal of computational physics, </journal> <volume> 90 </volume> <pages> 161-175, </pages> <year> 1990. </year>
Reference: [121] <author> M. Duff. </author> <title> Q-learning for bandit problems. </title> <type> Technical Report 95-26, </type> <institution> Department of Computer Science, University of Massachusetts at Amherst, </institution> <year> 1995. </year> <month> 236 </month>
Reference-contexts: A key distinction between control systems and planning systems is the focus on achieving a specific goal; although, frequently the distinction between them becomes blurred [252]. 1.3 Learning and Intelligent Agents Recently, the machine learning community has paid increasing attention to problems of delayed reinforcement learning <ref> [37, 121, 179, 225, 228, 318, 368] </ref>. These problems usually involve an agent that has to make a sequence of decisions, or actions, in an environment that provides feedback about those decisions. The feedback about those decisions might be considerably delayed, and this delay makes learning much more difficult.
Reference: [122] <author> E. Durfee and V. Lesser. </author> <title> Using partial global plans to coordinate distributed problem solvers. </title> <booktitle> In Proceedings of the 1987 International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 875-883, </pages> <year> 1987. </year>
Reference-contexts: The agents must then be constructed (or must learn) such that they cooperate with one another to perform the task <ref> [122, 139] </ref>. Work by Genesereth and Rosenschein considered cooperation among agents in which there are competing objectives and where there is no communication between the agents [138, 283].
Reference: [123] <author> I. Erev and A. Roth. </author> <title> On the need for low rationality, cognitive game theory: Reinforcement learning in experimental games with unique mixed strategy equilibria. </title> <type> Unpublished manuscript, </type> <month> August </month> <year> 1995. </year>
Reference-contexts: In a normal form game, this is equivalent to filling out the entries in the payoff matrix. In an extensive form game, this could involve learning the heuristic evaluation function to be applied at the interior nodes of the game tree <ref> [123, 140, 155, 156, 157, 204, 258, 259, 260, 261, 286] </ref>. * Opponent capabilities are not known. In most studies in game theory, each player knows the permissible actions for itself and its opponent. In problems such as differential games, this may not be true. <p> This, of course, is the simplest form of multi-agent learning. 3.3.3 Reinforcement Learning in Cognitive Game Theory In the economics community, one of the communities responsible for considerable research in game theory <ref> [49, 123, 130, 196, 286, 358, 360] </ref>, limited work is being done in learning game strategies. This work, however, is largely restricted to single-player games. <p> This work, however, is largely restricted to single-player games. One notable exception is the work by Roth and Erev <ref> [123, 286] </ref> in what they call "cognitive game theory." Specifically, they distinguish between "low" game theory and "high" game theory, where low game theory assumes players have limited rationality but adapt to experience playing the game to derive rational strategies. <p> Further, the players may not consider all strategies available to them, and they may not be "subjective expected utility maximizers <ref> [123] </ref>." High game theory assumes the players already have "full" rationality and can deduce the rational strategy from the rules of the game. Roth and Erev focus their research on low rationality game theory to facilitate modeling the learning process. <p> "propensities," q ij , the probability of selecting strategy j (thus defining a mixed strategy) is p ij (s) = P m Roth and Erev studied the performance of this approach on three simple economic games 74 with pure strategy equilibria [286] and eleven simple economic games with mixed-strategy equilibria <ref> [123] </ref>. They found their simulated results tracked well with several experiments involving human subjects learning to play these same games. The intent of their experiments did not include developing an algorithm for learning the equilibria in the games.
Reference: [124] <author> B. Everitt. </author> <title> Cluster Analysis 3rd Edition. </title> <editor> E. </editor> <publisher> Arnold Press, </publisher> <address> London., </address> <year> 1993. </year>
Reference-contexts: Data mining stems from work in statistics and unsupervised learning and has drawn from ideas taken from automatic discovery <ref> [2, 66, 79, 124, 185] </ref>. Many approaches to data mining use techniques such as clustering, rule induction, and classification to analyze the large set of data. From this set of data, data mining systems attempt to induce general "laws" and classification procedures for characterizing the data.
Reference: [125] <author> S. Fortune and J. Hopcroft. </author> <title> A note on rabin's nearest neighbor algorithm. </title> <journal> Information processing Letters, </journal> <volume> 8(1) </volume> <pages> 20-23, </pages> <month> January </month> <year> 1979. </year>
Reference: [126] <author> A. Friedman. </author> <title> Differential Games. </title> <publisher> Wiley Interscience, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: We can also interpret differential games to be an extension of optimal control theory in which players' positions develop continuously in time, and we wish to optimize competing control laws for the players <ref> [126] </ref>. We restrict a differential game to be a two-person zero-sum game in which both players are required to make a lengthy sequence of decisions to maximize payoff throughout the game. Further, we formulate the game with a set of state variables and a set of control variables.
Reference: [127] <author> J. Friedman. </author> <title> Flexible metric nearest neighbor classification. </title> <type> Technical report, </type> <institution> De partment of Statistics, Stanford University, </institution> <year> 1994. </year>
Reference: [128] <author> J. Friedman, F. Baskett, and L. Shustek. </author> <title> An algorithm for finding nearest neighbors. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 1000-1006, </pages> <month> October </month> <year> 1975. </year>
Reference-contexts: In this chapter, we consider an alternative algorithm that, although not memory-based, is inspired by the results of applying kd-trees in memory-based learning <ref> [46, 110, 128, 129, 237] </ref>. A kd-tree is a data structure used to store a set of examples in a memory base such that nearest neighbors can be found in logarithmic expected time.
Reference: [129] <author> J. Friedman, J. Bentley, and R. Finkel. </author> <title> An algorithm for finding best matches in logarithmic expected time. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 3(3) </volume> <pages> 209-226, </pages> <month> September </month> <year> 1977. </year>
Reference-contexts: In this chapter, we consider an alternative algorithm that, although not memory-based, is inspired by the results of applying kd-trees in memory-based learning <ref> [46, 110, 128, 129, 237] </ref>. A kd-tree is a data structure used to store a set of examples in a memory base such that nearest neighbors can be found in logarithmic expected time.
Reference: [130] <author> D. Fudenberg and J. Tirole. </author> <title> Game Theory. </title> <publisher> The MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: This, of course, is the simplest form of multi-agent learning. 3.3.3 Reinforcement Learning in Cognitive Game Theory In the economics community, one of the communities responsible for considerable research in game theory <ref> [49, 123, 130, 196, 286, 358, 360] </ref>, limited work is being done in learning game strategies. This work, however, is largely restricted to single-player games.
Reference: [131] <author> K. Fujimura. </author> <title> A model of reactive planning for multiple mobile robots. </title> <booktitle> In Pro ceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 1503-1509. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: Finally, planning attempts to determine optimal strategies for an agent to apply in performing some task <ref> [74, 106, 107, 131, 288] </ref>. These strategies usually consist of sequences of steps to perform, and generally, the objective is to reach some terminal state (as opposed to maintaining an equilibrium as in control problems). Classic planning problems in artificial intelligence have included stacking blocks and navigating mazes.
Reference: [132] <author> K. Fukanaga. </author> <title> Introduction to Statistical Pattern Recognition. </title> <publisher> Academic Press, </publisher> <year> 1990. </year>
Reference: [133] <author> J. Gallagher and R. Beer. </author> <title> A qualitative dynamical analysis of evolved locomo tion control. </title> <editor> In H. Roitblat, J-A. Meyer, and S. Wilson, editors, </editor> <booktitle> From Animals to Animats, Proceedings of the Second International Conference on Simulation of Adaptive Behavior (SAB 92). </booktitle> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference: [134] <author> E. Galperin and J. Skowronski. </author> <title> Pursuit-evasion differential games with uncer tainties in dynamics. Computers and Mathematics with Applications, </title> <address> 13(1-3):13-35, </address> <year> 1987. </year>
Reference-contexts: Lai and Tanaka consider general n-person games where each player attempts to force opponents into a terminal location in the playing field [200]. Related to work in partially observable Markov decision processes [70, 71, 180], Galperin and Skowronski are studying games in which noise is introduced into game dynamics <ref> [134] </ref>. Corless et al. consider the case where state information is uncertain [88]. <p> Thus, the in 51 formation sets associated with various states in the game can vary over time as the players learn the environment. This is analogous to exploring the environment in navigation problems [208, 209, 219, 238, 239]. * The game dynamics are uncertain <ref> [134] </ref>. This is analogous to the problem of learning an opponent's strategy except that the dynamics, usually, are fixed. Learning game dynamics when the dynamics are not fixed is also of interest [152, 165]. In this chapter, we review recent work in learning game strategies.
Reference: [135] <author> G. Game and C. James. </author> <title> The application of genetic algorithms to the optimal selection of parameter values in neural networks for attitude control systems. </title> <booktitle> In IEE Colloquium on 'High Accuracy Platform Control in Space', </booktitle> <volume> volume Digest No. 1993/148, </volume> <pages> pages 3/1-3/3. </pages> <publisher> IEE, </publisher> <address> London, </address> <year> 1993. </year> <month> 237 </month>
Reference: [136] <author> M. Garey and D. Johnson. </author> <title> Computers and Intractability: a Guide to the theory of NP-Completeness. </title> <publisher> Freeman and Co., </publisher> <address> San Francisco, CA, </address> <year> 1979. </year>
Reference: [137] <author> G. Gates. </author> <title> The reduced nearest neighbor rule. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 18 </volume> <pages> 431-433, </pages> <month> May </month> <year> 1972. </year>
Reference: [138] <author> M. Genesereth, M. Ginsberg, and J. Rosenschein. </author> <title> Cooperation without com munication. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 51-57, </pages> <address> San Jose CA, 1986. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: This loop is repeated until the system either reaches a goal state or recognizes that it will never terminate. To date, research in multiple agent planning and control has been limited largely to the area of distributed artificial intelligence <ref> [138, 283, 329, 330, 331, 341, 356] </ref> and artificial 5 life [83, 116, 117, 172, 299, 325]. In distributed AI (DAI), several agents cooperate to achieve some goal or accomplish some task. The task is usually one of sufficient complexity that no single agent can accomplish the task alone. <p> The agents must then be constructed (or must learn) such that they cooperate with one another to perform the task [122, 139]. Work by Genesereth and Rosenschein considered cooperation among agents in which there are competing objectives and where there is no communication between the agents <ref> [138, 283] </ref>.
Reference: [139] <author> M. Georgeff. </author> <title> A theory of action for multi-agent planning. </title> <booktitle> In Proceedings of Na tional Conference on Artificial Intelligence, </booktitle> <pages> pages 121-125, </pages> <address> San Jose CA, 1984. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: The agents must then be constructed (or must learn) such that they cooperate with one another to perform the task <ref> [122, 139] </ref>. Work by Genesereth and Rosenschein considered cooperation among agents in which there are competing objectives and where there is no communication between the agents [138, 283].
Reference: [140] <author> M. Gherrity. </author> <title> A Game-Learning Machine. </title> <type> PhD thesis, </type> <institution> University of California, </institution> <address> San Diego, </address> <year> 1993. </year>
Reference-contexts: In a normal form game, this is equivalent to filling out the entries in the payoff matrix. In an extensive form game, this could involve learning the heuristic evaluation function to be applied at the interior nodes of the game tree <ref> [123, 140, 155, 156, 157, 204, 258, 259, 260, 261, 286] </ref>. * Opponent capabilities are not known. In most studies in game theory, each player knows the permissible actions for itself and its opponent. In problems such as differential games, this may not be true.
Reference: [141] <author> D. Ghose and U. Prasad. </author> <title> Determination of rational strategies for players in two target games. </title> <journal> Computers and Mathematics with Applications, </journal> <volume> 26(6) </volume> <pages> 1-11, </pages> <year> 1993. </year>
Reference-contexts: Yavin describes a three-player game in which two agile players attempt to evade a single pursuer [376]. At the same time, the evaders want to shoot down the pursuer. This is also an example of the two-target game where a player has two competing objectives <ref> [141, 322] </ref>. Imado and Ishihara also consider the case where two missiles attempt to shoot down an airplane [175]. Lai and Tanaka consider general n-person games where each player attempts to force opponents into a terminal location in the playing field [200].
Reference: [142] <author> D. Goldberg. </author> <title> Computer-Aided Gas Pipeline Operation Using Genetic Algorithms and Machine Learning. </title> <type> PhD thesis, </type> <institution> University of Michigan, Ann Arbor, Michigan, </institution> <year> 1983. </year>
Reference: [143] <author> D. Goldberg. </author> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1989. </year>
Reference-contexts: During testing, the plan with the highest fitness is used to control E. The heart of the learning algorithm lies in the application of two genetic operators: mutation and crossover. Rules within a plan are selected for mutation using fitness proportional selection <ref> [143] </ref>. Namely, probability of selection is determined as Pr (r) = strength r (t) X 8s2rules strength s (t) where rules is the set of rules in a plan and r is the rule of interest.
Reference: [144] <author> C. Goldman and J. Rosenschein. </author> <title> Mutually supervised learning in multiagent systems. </title> <booktitle> In Proceedings of the ICJAI Workshop on Adaptation and Learning in Mul-tiagent Systems, </booktitle> <year> 1996. </year>
Reference-contexts: Others have provided similar strategies to modeling opponents, such as the agent tracking methods of Tambe [336, 337, 338, 339, 340], and the equilibrium search methods of Goldman and Rosenschein <ref> [144, 243] </ref> and of Koller [194, 195, 196, 197, 359]. For their approach, Carmel and Markovitch assume a two-player game where each player has an evaluation function to be used in a minimax search of the game tree.
Reference: [145] <author> S. Goldman and M. Kearns. </author> <title> On the complexity of teaching. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 303-314, </pages> <address> Santa Cruz, CA, August 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [146] <author> D. Gordon and D. Subramanian. </author> <title> A multistrategy learning scheme for agent knowledge acquisition. </title> <journal> Informatica, </journal> <volume> 17 </volume> <pages> 331-346, </pages> <year> 1993. </year>
Reference-contexts: One approach studied by Gordon and Subramanian uses explanation based learning to devise a set of operational rules for obstacle avoidance in a noisy environment <ref> [146, 147] </ref>. In this approach, high-level domain-specific and spatial knowledge is coded into rules and then operationalized into low-level reactive rules based on the stated goals of the problem. These rules generally are 132 not sufficient to solve the problem but provides useful rules in various situations.
Reference: [147] <author> D. Gordon and D. Subramanian. </author> <title> A multistrategy learning scheme for assimilating advice in embedded agents. </title> <booktitle> In Proceedings of the Second International Workshop on Multistrategy Learning, </booktitle> <pages> pages 218-233. </pages> <institution> George Mason University, </institution> <year> 1993. </year>
Reference-contexts: One approach studied by Gordon and Subramanian uses explanation based learning to devise a set of operational rules for obstacle avoidance in a noisy environment <ref> [146, 147] </ref>. In this approach, high-level domain-specific and spatial knowledge is coded into rules and then operationalized into low-level reactive rules based on the stated goals of the problem. These rules generally are 132 not sufficient to solve the problem but provides useful rules in various situations.
Reference: [148] <author> G. Gordon. </author> <title> Stable function approximation in dynamic programming. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, Publishers, </publisher> <year> 1995. </year> <month> 238 </month>
Reference: [149] <author> J. Grefenstette. </author> <title> Credit assignment in rule discovery systems based on genetic algorithms. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 225-245, </pages> <year> 1988. </year>
Reference-contexts: As with other methods of learning to solve MDPs, GAs must associate sets of admissible actions to states in the problem and provide a means to select the most appropriate action to maximize expected payoff. The first application of GAs to MDPs arose from work in classifier systems <ref> [50, 149, 167, 168] </ref>. In a classifier system, rules are binary condition/action pairs that work within a message-passing architecture (Figure 3.1). State information is converted into a binary message and placed on the message list. <p> For their research, they focused on a single linear-quadratic differential game of pursuit in which a single missile (designated P ) pursues a single airplane (designated E), which is similar to the problem studied by Grefenstette et al. <ref> [149, 153, 163, 273] </ref>. As a linear-quadratic game, the kinematic equations are linear functions of the current state and action, and the payoff function is a quadratic function of acceleration and the distance between the players. <p> Following a game the strengths of the rules that fired are updated based on the payoff received from the game (the same payoff used in Q-learning). Given the payoff function, the strength for each rule that fired in a game is updated using the profit sharing plan <ref> [149] </ref> as follows: (t) = (1 c)(t 1) + c strength (t) = (t) (t) where c is the profit sharing rate (c = 0:01 for our experiments), is the payoff received, is an estimate of the mean strength of a rule, and is an estimate of the variance of rule
Reference: [150] <editor> J. Grefenstette. </editor> <booktitle> Lamarkian learning in multi-agent environments. In Proceedings of the Fourth International Conference of Genetic Algorithms, </booktitle> <pages> pages 303-310. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: Also, migration patterns of artificial birds have been evolved. In none of these cases has behavior of individual agents been the focus of the research. Recently, work has begun to appear that focuses on learning in multi-agent systems <ref> [56, 65, 150, 303, 328, 327, 329, 341] </ref>. Stone and Veloso provide a taxonomy of multi-agent systems by focusing on attributes such as agent homogeneity, communication, deliberative versus reactive control, and number of agents [329]. <p> So far, most work in learning and multi-agent systems has focused on multiple agents' learning 6 complementary behaviors in a coordinated environment to accomplish some task, such as team game playing [330, 337, 338, 341], combinatorial optimization [116, 117], and obstacle avoidance <ref> [150] </ref>. The research discussed in this dissertation combines work in control and planning in the context of competitive multi-agent systems. In particular, we focus on exploring methods for the on-line learning of optimal strategies for playing differential games.
Reference: [151] <author> J. Grefenstette. </author> <title> The evolution of strategies for multi-agent environments. </title> <journal> Adap tive Behavior, </journal> <volume> 1(1) </volume> <pages> 65-89, </pages> <year> 1992. </year>
Reference-contexts: It is unclear how well this procedure would perform, given their simplifications, had mixed strategies been required. 3.3.6 Coevolution Methods Recently, work in co-evolutionary algorithms has begun to suggest approaches to multi-agent co-learning with some encouraging initial results <ref> [151, 152, 266, 323] </ref>. Extending his work on samuel, Grefenstette defines a uniform sensor architecture for multi-agent environments [151]. He claims that modeling information about all agents in the environment would be too complex, requiring significant computational resources. <p> Extending his work on samuel, Grefenstette defines a uniform sensor architecture for multi-agent environments <ref> [151] </ref>. He claims that modeling information about all agents in the environment would be too complex, requiring significant computational resources. His approach models the learning agent in three stages|sensors, conflict resolution, and actions.
Reference: [152] <author> J. Grefenstette and R. Daley. </author> <title> Methods for competitive and cooperative co evolution. In Adaptation, Coevolution, </title> <booktitle> and Learning in Multiagent Systems (ICMAS '95), </booktitle> <pages> pages 276-282. </pages> <publisher> AAAI Press, </publisher> <year> 1995. </year>
Reference-contexts: This is analogous to the problem of learning an opponent's strategy except that the dynamics, usually, are fixed. Learning game dynamics when the dynamics are not fixed is also of interest <ref> [152, 165] </ref>. In this chapter, we review recent work in learning game strategies. Although each of the above issues are touched upon, the focus of this chapter is on reviewing learning algorithms designed to learn strategies in two person games. <p> Although not traditionally considered an implementation of a multi-agent learning strategy, the self-play learning methodology, where Alpha plays Beta and Beta has the best learned polynomial so far, is suggestive of a strategy proposed by Grefenstette and Daley <ref> [152] </ref>. In their approach, competing agents "co-evolve" rules for engagement where each agent alternates learning (see Section 3.3.6). Because the two checker players are essentially the same, transferring a learned polynomial to Beta is analogous to alternating learning between Alpha and Beta. <p> It is unclear how well this procedure would perform, given their simplifications, had mixed strategies been required. 3.3.6 Coevolution Methods Recently, work in co-evolutionary algorithms has begun to suggest approaches to multi-agent co-learning with some encouraging initial results <ref> [151, 152, 266, 323] </ref>. Extending his work on samuel, Grefenstette defines a uniform sensor architecture for multi-agent environments [151]. He claims that modeling information about all agents in the environment would be too complex, requiring significant computational resources. <p> The 82 multi-agent (or composite) plan then consists of the concatenation of the best plans learned by the subagents. Finally, Grefenstette and Daley consider a coevolutionary strategy for cooperative and competitive multi-agent plans <ref> [152] </ref>. These are the first experiments by Grefenstette et al. in which multiple competitive agents learn simultaneously. In these experiments, Grefenstette and Daley consider four multi-agent scenarios: * A single genetic algorithm learns a plan that is used by both agents.
Reference: [153] <author> J. Grefenstette, C. Ramsey, and A. Schultz. </author> <title> Learning sequential decision rules using simulation models and competition. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 355-381, </pages> <year> 1990. </year>
Reference-contexts: During crossover, two classifiers are selected and random substrings of the two classifiers are swapped. Through mutation and crossover, new classifiers are introduced to permit exploration of the space of possible classifiers. Grefenstette et al. <ref> [153] </ref> propose an alternative approach to learning sequential decision rules using a more general production system architecture which they call CPS (Competitive Production System). <p> For their research, they focused on a single linear-quadratic differential game of pursuit in which a single missile (designated P ) pursues a single airplane (designated E), which is similar to the problem studied by Grefenstette et al. <ref> [149, 153, 163, 273] </ref>. As a linear-quadratic game, the kinematic equations are linear functions of the current state and action, and the payoff function is a quadratic function of acceleration and the distance between the players. <p> The specific games used for the experiments in this chapter are two variations of the evasive maneuvers game <ref> [153] </ref>. 4.2 The Evasive Maneuvers Game The evasive maneuvers task as a differential game is a variation on the Homicidal Chauffeur game. Even though the solution to the Homicidal Chauffeur game is intuitive, the actual surface characterizing the solution is highly nonlinear. <p> Even though the solution to the Homicidal Chauffeur game is intuitive, the actual surface characterizing the solution is highly nonlinear. Thus we should reasonably expect the surface for extensions to the problem (such as those discussed in this chapter) to be more difficult to characterize. Grefenstette et al. <ref> [153] </ref> studied the evasive maneuvers task to demonstrate the ability of genetic algorithms to solve complex sequential decision making problems. In their two-dimensional simulation, a single aircraft attempts to evade a single missile. <p> We demonstrate one engagement with two missiles in which the airplane is destroyed in Figure 4.2. When flying against one missile, the capabilities of the aircraft are identical to the aircraft used by Grefenstette et al. <ref> [153] </ref>. As noted earlier, in the two missile task, the aircraft has 13 sensors. The nine state variables measured by these sensors are: * speed: Indicates the previous speed of the aircraft. <p> State s 0 is the state that follows when action a is applied to state s. Reward is determined using the payoff function in <ref> [153] </ref>, namely = 1000; if E evades the pursuers 10t; if E is captured at time t. Each of the pairs in the game are then compared with all of the pairs in the database. <p> The knowledge for the evasive maneuvers problem requires rules in which the terms have numeric values; we therefore modified the standard GA representation and operators for this problem, using a formulation similar to <ref> [153] </ref>. We call a set of rules a plan. <p> When we found that 1-NN did not work well, we considered an eager learning algorithm, the genetic algorithm. This choice was motivated by the previous work by Grefenstette et al. which indicated the GA was capable of solving this type of task <ref> [153] </ref>.
Reference: [154] <author> N. Grigorenko. </author> <title> The problem of pursuit by several objects. </title> <editor> In R. Hamalainen and H. Ehtamo, editors, </editor> <booktitle> Differential Games|Developments in Modeling and Computation, </booktitle> <pages> pages 71-80. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference: [155] <author> V Gullapalli. </author> <title> A stochastic reinforcement learning algorithm for learning real-valued functions. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 671-692, </pages> <year> 1990. </year>
Reference-contexts: In a normal form game, this is equivalent to filling out the entries in the payoff matrix. In an extensive form game, this could involve learning the heuristic evaluation function to be applied at the interior nodes of the game tree <ref> [123, 140, 155, 156, 157, 204, 258, 259, 260, 261, 286] </ref>. * Opponent capabilities are not known. In most studies in game theory, each player knows the permissible actions for itself and its opponent. In problems such as differential games, this may not be true.
Reference: [156] <author> V. Gullapalli. </author> <title> Reinforcement Learning and its Application to Control. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <year> 1992. </year>
Reference-contexts: Control theory focuses on developing optimal procedures for maintaining equilibrium or for achieving some performance objective by determining values of several "control variables" in a dynamic system <ref> [1, 26, 40, 45, 71, 100, 101, 156, 237, 252, 253] </ref>. Machine learning systems attempt to determine optimal values for these control variables from experience rather than explicitly solving a set of differential equations. <p> In a normal form game, this is equivalent to filling out the entries in the payoff matrix. In an extensive form game, this could involve learning the heuristic evaluation function to be applied at the interior nodes of the game tree <ref> [123, 140, 155, 156, 157, 204, 258, 259, 260, 261, 286] </ref>. * Opponent capabilities are not known. In most studies in game theory, each player knows the permissible actions for itself and its opponent. In problems such as differential games, this may not be true.
Reference: [157] <author> V Gullapalli. </author> <title> Learning control under extreme uncertainty. </title> <booktitle> In Neural Information Processing Systems 5, </booktitle> <pages> pages 271-278, </pages> <year> 1993. </year>
Reference-contexts: In a normal form game, this is equivalent to filling out the entries in the payoff matrix. In an extensive form game, this could involve learning the heuristic evaluation function to be applied at the interior nodes of the game tree <ref> [123, 140, 155, 156, 157, 204, 258, 259, 260, 261, 286] </ref>. * Opponent capabilities are not known. In most studies in game theory, each player knows the permissible actions for itself and its opponent. In problems such as differential games, this may not be true.
Reference: [158] <author> S. Gutman, M. Esh, and M. Gefen. </author> <title> Simple linear pursuit-evasion games. Com puters and Mathematics with Applications, </title> <address> 13(1-3):83-95, </address> <year> 1987. </year>
Reference: [159] <editor> R. Hamalainen and H. Ehtamo, editors. </editor> <title> Differential Games|Developments in Mod eling and Computation. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1991. </year>
Reference: [160] <author> W. Hamilton. </author> <title> Instability and cycling of two competing hosts with two parasites. </title> <editor> In S. Karlin and E. Nevo, editors, </editor> <title> Evolutionary Process Theory. </title> <publisher> Academic Press, </publisher> <year> 1986. </year>
Reference-contexts: Other investigations of host/parasite co-existence and co-evolution have taken a game-theoretic view, but work by Bremermann and Pickering focuses on natural evolution [62]. In another biological study, Hamilton considered the effects of competition between hosts and parasites in the evolutionary process <ref> [160] </ref>. To promote distribution of skills among populations, Davidor [102] studied the effects of niching and speciation in 88 genetic algorithms, and Werner and Dyer focused on developing communication mechanisms between organisms through artificial evolution [365]. Some work has been done in evolutionary computation, artificial life, and iterated games.
Reference: [161] <author> M. Harmon and L. Baird. </author> <title> Residual advantage learning applied to a differential game. </title> <booktitle> In Neural Information Processing Systems 7, </booktitle> <year> 1995. </year>
Reference-contexts: performed better against the hand-built player than minimax-Q-learning did against the hand-built player following training with other learning players. 77 3.3.5 Advantage Updating and Differential Games In independent research, Harmon, Baird, and Klopf investigated applying reinforcement learning in function approximators (namely, artificial neural networks) to learning solutions to differential games <ref> [29, 30, 31, 161, 162, 163] </ref>. For their research, they focused on a single linear-quadratic differential game of pursuit in which a single missile (designated P ) pursues a single airplane (designated E), which is similar to the problem studied by Grefenstette et al. [149, 153, 163, 273]. <p> Of particular interest would be work integrating connectionist and symbolic systems into a cohesive multi-agent learner. For example, an interesting architecture might include an artificial neural network to provide a fitness function for filling out a game matrix. Harmon and Baird's approach <ref> [161] </ref> to using a neural network could easily be extended to include a full evaluation of the linear program and its dual. Several variations of MBCL should be considered.
Reference: [162] <author> M. Harmon, L. Baird, and A. Klopf. </author> <title> Advantage updating applied to a differen tial game. </title> <booktitle> In Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 353-360, </pages> <year> 1994. </year> <month> 239 </month>
Reference-contexts: performed better against the hand-built player than minimax-Q-learning did against the hand-built player following training with other learning players. 77 3.3.5 Advantage Updating and Differential Games In independent research, Harmon, Baird, and Klopf investigated applying reinforcement learning in function approximators (namely, artificial neural networks) to learning solutions to differential games <ref> [29, 30, 31, 161, 162, 163] </ref>. For their research, they focused on a single linear-quadratic differential game of pursuit in which a single missile (designated P ) pursues a single airplane (designated E), which is similar to the problem studied by Grefenstette et al. [149, 153, 163, 273].
Reference: [163] <author> M. Harmon, L. Baird, and A. Klopf. </author> <title> Reinforcement learning applied to a dif ferential game. Adaptive Behavior, </title> <note> 1995. To appear. </note>
Reference-contexts: performed better against the hand-built player than minimax-Q-learning did against the hand-built player following training with other learning players. 77 3.3.5 Advantage Updating and Differential Games In independent research, Harmon, Baird, and Klopf investigated applying reinforcement learning in function approximators (namely, artificial neural networks) to learning solutions to differential games <ref> [29, 30, 31, 161, 162, 163] </ref>. For their research, they focused on a single linear-quadratic differential game of pursuit in which a single missile (designated P ) pursues a single airplane (designated E), which is similar to the problem studied by Grefenstette et al. [149, 153, 163, 273]. <p> For their research, they focused on a single linear-quadratic differential game of pursuit in which a single missile (designated P ) pursues a single airplane (designated E), which is similar to the problem studied by Grefenstette et al. <ref> [149, 153, 163, 273] </ref>. As a linear-quadratic game, the kinematic equations are linear functions of the current state and action, and the payoff function is a quadratic function of acceleration and the distance between the players.
Reference: [164] <author> P. Hart. </author> <title> The condensed nearest neighbor rule. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 14(3) </volume> <pages> 515-516, </pages> <month> May </month> <year> 1968. </year>
Reference-contexts: Editing then proceeds using Wilson's approach. Specifically, Wilson's approach is modified as in Figure 5.4. Ritter et al. [281] described another editing method, which differs from Wilson in that points that are correctly classified are discarded. The Ritter method, which is similar to Hart's <ref> [164] </ref>, basically keeps only points near the boundaries between classes, and eliminates examples that are in the midst of a homogeneous region. Procedurally, this method is shown in Figure 5.5.
Reference: [165] <author> T. Haynes and S. Sen. </author> <title> Evolving behavioral strategies in predators and prey. </title> <editor> In S. Weiss and S. Sen, editors, </editor> <booktitle> Adaptation and Learning in Multiagent Systems, </booktitle> <pages> pages 113-126. </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year>
Reference-contexts: This is analogous to the problem of learning an opponent's strategy except that the dynamics, usually, are fixed. Learning game dynamics when the dynamics are not fixed is also of interest <ref> [152, 165] </ref>. In this chapter, we review recent work in learning game strategies. Although each of the above issues are touched upon, the focus of this chapter is on reviewing learning algorithms designed to learn strategies in two person games.
Reference: [166] <author> D. Heath. </author> <title> A Geometric Framework for Machine Learning. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, The Johns Hopkins University, </institution> <year> 1992. </year>
Reference-contexts: Much work in the area of computational learning theory has focused on the problem of assessing the complexity of a learning task in terms of the samples required to learn that task (called the sample complexity). Salzberg et al. and Heath <ref> [296, 166] </ref> considered an alternative to the Probably Approximately Correct (PAC) learning model [353, 354] in which a Helpful Teacher is providing good examples to the learning. Their research focused on answering four questions: 1. What is the minimum number of examples needed to learn a concept? 2. <p> The Helpful Teacher model, on the other hand, provides a best-case analysis and helps bound the complexity of the learning task. In fact, Salzberg and Heath have demonstrated that 135 many experimental results tend to more closely follow Helpful Teacher complexity bounds than PAC complexity bounds <ref> [166] </ref>. 5.3 Bootstrapping Nearest Neighbor Given the potential benefit of using a teacher in learning tasks, we sought to provide a teacher to 1-NN to improve 1-NN's performance. <p> It might be possible with careful editing to reduce the size of memory even further. This question is related to the theoretical work by Salzberg and Heath <ref> [166, 296] </ref> that studies the question of how to find a minimal-size training set through the use of a Helpful Teacher, which explicitly provides good examples (see Section 5.2.5). <p> Finally, it may be appropriate to consider non-axis-parallel trees in growing trees for TBCL. Work by Heath and Murthy has pointed out several issues and offered several suggestions for constructing oblique decision trees and addressing concerns such as look-ahead and splitting criteria <ref> [166, 247] </ref>. In Section 7.4 we point out that one of the significant differences between MBCL and TBCL is that Q-updates in MBCL occur over a region in the instance space where Q-updates in TBCL apply only to individual cells in the game matrix.
Reference: [167] <author> J. Holland. </author> <title> Adaptation in Natural and Artificial Systems. </title> <institution> University of Michigan Press, Ann Arbor, Michigan, </institution> <year> 1975. </year>
Reference-contexts: The feedback about those decisions might be considerably delayed, and this delay makes learning much more difficult. In this case, the problem is called a delayed reinforcement problem <ref> [109, 167, 168, 220, 221, 317, 343, 363] </ref>. The basic loop followed in sequential decision making tasks such as these includes evaluating the current state, taking an action, and computing the new state. <p> As with other methods of learning to solve MDPs, GAs must associate sets of admissible actions to states in the problem and provide a means to select the most appropriate action to maximize expected payoff. The first application of GAs to MDPs arose from work in classifier systems <ref> [50, 149, 167, 168] </ref>. In a classifier system, rules are binary condition/action pairs that work within a message-passing architecture (Figure 3.1). State information is converted into a binary message and placed on the message list. <p> Typically, GAs use rules called classifiers, which are simple structures in which terms in the antecedent and the consequent are represented as binary attributes <ref> [50, 167] </ref>. The knowledge for the evasive maneuvers problem requires rules in which the terms have numeric values; we therefore modified the standard GA representation and operators for this problem, using a formulation similar to [153]. We call a set of rules a plan.
Reference: [168] <author> J. Holland. </author> <title> Escaping brittleness: The possibilities of general purpose learning algorithms applied to parallel rule-based systems. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 593-623, </pages> <year> 1986. </year>
Reference-contexts: The feedback about those decisions might be considerably delayed, and this delay makes learning much more difficult. In this case, the problem is called a delayed reinforcement problem <ref> [109, 167, 168, 220, 221, 317, 343, 363] </ref>. The basic loop followed in sequential decision making tasks such as these includes evaluating the current state, taking an action, and computing the new state. <p> As with other methods of learning to solve MDPs, GAs must associate sets of admissible actions to states in the problem and provide a means to select the most appropriate action to maximize expected payoff. The first application of GAs to MDPs arose from work in classifier systems <ref> [50, 149, 167, 168] </ref>. In a classifier system, rules are binary condition/action pairs that work within a message-passing architecture (Figure 3.1). State information is converted into a binary message and placed on the message list.
Reference: [169] <author> R. Holte. </author> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11(1) </volume> <pages> 63-90, </pages> <year> 1993. </year>
Reference: [170] <author> E. Horowitz and S. Sahni. </author> <title> Fundamentals of Computer Algorithms. </title> <publisher> Computer Science Press, </publisher> <address> Rockville, MD, </address> <year> 1984. </year>
Reference: [171] <author> R. Huang. </author> <title> Systems control with the genetic algorithm and the nearest neighbor classification. </title> <address> CC-AI, 9((2-3)):225-236, </address> <year> 1992. </year>
Reference: [172] <author> B. Huberman and N. Glance. </author> <title> Evolutionary games and computer simulations. </title> <booktitle> Proceedings of the National Academy of Sciences, </booktitle> <year> 1995. </year> <note> in press. </note>
Reference-contexts: To date, research in multiple agent planning and control has been limited largely to the area of distributed artificial intelligence [138, 283, 329, 330, 331, 341, 356] and artificial 5 life <ref> [83, 116, 117, 172, 299, 325] </ref>. In distributed AI (DAI), several agents cooperate to achieve some goal or accomplish some task. The task is usually one of sufficient complexity that no single agent can accomplish the task alone.
Reference: [173] <author> L. Hyafil and R. Rivest. </author> <title> Constructing optimal binary decision trees is NP complete. </title> <journal> Information Processing Letters, </journal> <volume> 5(1) </volume> <pages> 15-17, </pages> <year> 1976. </year>
Reference: [174] <author> Y. Ichikawa. </author> <title> Evolution of neural networks and application to motion control. </title> <booktitle> In Proceedings of the IEEE International Conference on Intelligent Motion Control, </booktitle> <pages> pages 239-245. </pages> <publisher> IEEE, </publisher> <year> 1990. </year>
Reference: [175] <author> F. Imado and T. Ishihara. </author> <title> Pursuit-evasion geometry analysis between two missiles and an aircraft. </title> <journal> Computers and Mathematics with Applications, </journal> <volume> 26(3) </volume> <pages> 125-139, </pages> <year> 1993. </year>
Reference-contexts: At the same time, the evaders want to shoot down the pursuer. This is also an example of the two-target game where a player has two competing objectives [141, 322]. Imado and Ishihara also consider the case where two missiles attempt to shoot down an airplane <ref> [175] </ref>. Lai and Tanaka consider general n-person games where each player attempts to force opponents into a terminal location in the playing field [200]. <p> Our extended version includes a second pursuer, which makes the problem much harder. Unlike the single-pursuer problems, the two-pursuer problem has no known optimal strategy <ref> [175] </ref>, and for some initial states, there is no possibility of escape. Second, we gave the evader additional capabilities: in the one-pursuer game, E only controls its turn angle at each time step.
Reference: [176] <author> R. Isaacs. </author> <title> Differential games: A mathematical theory with applications to warfare and other topics. </title> <type> Technical Report Research Contribution No. 1, </type> <institution> Center for Naval Analysis, </institution> <address> Washington, D.C., </address> <year> 1963. </year>
Reference-contexts: Differential game theory is an extension of traditional game theory where the game follows a sequence of actions through a continuous state space to achieve some payoff <ref> [176, 177] </ref>. This sequence can be modeled with a set of differential equations (called kinematic equations) which are analyzed to determine optimal play by the players. <p> More recently, systems for intelligent highways, air traffic control, railroad monitoring, and ship routing are benefiting from differential game theory in determining how to optimize vehicle control in the presence of competing goals. Differential game theory originated in the early 1960s <ref> [176] </ref> in response to the need for a more formal analysis of war games.
Reference: [177] <author> R. Isaacs. </author> <title> Differential Games. </title> <editor> Robert E. </editor> <publisher> Krieger, </publisher> <address> New York, </address> <year> 1975. </year> <month> 240 </month>
Reference-contexts: Differential game theory is an extension of traditional game theory where the game follows a sequence of actions through a continuous state space to achieve some payoff <ref> [176, 177] </ref>. This sequence can be modeled with a set of differential equations (called kinematic equations) which are analyzed to determine optimal play by the players. <p> In the following, we present a summary of an analysis of this game by Isaacs <ref> [177] </ref> and Basar and Olsder [39]. <p> For E, the angle is fixed and E must determine the appropriate magnitude of force to apply. This simple game of force is derived from the dolichobrachistochrone game|one of 156 the classic games studied in differential game theory <ref> [177, 205] </ref>. 5 In this game, play takes place in the quadrant of a plane where x 0 and y 0. The objective is for P to drive an object into the y axis. E attempts to keep P from succeeding. <p> In other words, P can make instantaneous turns between 45 ffi and +45 ffi while E can make instantaneous turns 174 between 90 ffi and +90 ffi . This game is a generalization of the Homicidal Chauffeur game <ref> [39, 177, 205] </ref>. In the Homicidal Chauffeur, only the mobility of P is limited. Given the added complexity of the game, no optimal solution was available; however, we were able to define a heuristic based on the optimal solution for the Homicidal Chauffeur.
Reference: [178] <author> C. Isbell. </author> <title> Explorations of the practical issues of learning prediction-control tasks using temporal difference learning methods. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1992. </year>
Reference: [179] <author> T. Jaakkola, M. Jordan, and S. Singh. </author> <title> On the convergence of stochastic itera tive dynamic programming algorithms. </title> <booktitle> Neural Computation, </booktitle> <year> 1994. </year>
Reference-contexts: A key distinction between control systems and planning systems is the focus on achieving a specific goal; although, frequently the distinction between them becomes blurred [252]. 1.3 Learning and Intelligent Agents Recently, the machine learning community has paid increasing attention to problems of delayed reinforcement learning <ref> [37, 121, 179, 225, 228, 318, 368] </ref>. These problems usually involve an agent that has to make a sequence of decisions, or actions, in an environment that provides feedback about those decisions. The feedback about those decisions might be considerably delayed, and this delay makes learning much more difficult.
Reference: [180] <author> T. Jaakkola, S. Singh, and M Jordan. </author> <title> Reinforcement learning algorithm for partially observable markovian decision problems. </title> <booktitle> In Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 345-352. </pages> <publisher> The MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Imado and Ishihara also consider the case where two missiles attempt to shoot down an airplane [175]. Lai and Tanaka consider general n-person games where each player attempts to force opponents into a terminal location in the playing field [200]. Related to work in partially observable Markov decision processes <ref> [70, 71, 180] </ref>, Galperin and Skowronski are studying games in which noise is introduced into game dynamics [134]. Corless et al. consider the case where state information is uncertain [88].
Reference: [181] <author> R. Jacobs, M. Jordan, S. Nowlan, and G. Hinton. </author> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 79-87, </pages> <year> 1991. </year>
Reference: [182] <author> M. James. </author> <title> Classification Algorithms. </title> <publisher> Wiley-Interscience Publications, </publisher> <year> 1985. </year>
Reference-contexts: Some of the types of problems explored include data classification, data mining, automatic programming, control theory, and planning. Classification systems identify a concept class from a set of available classes to which a particular example belongs <ref> [3, 10, 61, 91, 119, 182] </ref>. Data classification usually proceeds from a set of available attributes and associated values. A training set is used to present examples of concept classes, and the classifier constructed is designed to be consistent with that training set.
Reference: [183] <author> B. J armark. </author> <title> On closed-loop controls in pursuit-evasion. Computers and Mathemat ics with Applications, </title> <address> 13(1-3):157-166, </address> <year> 1987. </year>
Reference: [184] <author> T. Jervis and F. Fallside. </author> <title> Pole balancing on a real rig using a reinforcement learn ing controller. </title> <type> Technical Report CUED/F-INFENG/TR 115, </type> <institution> Cambridge University Engineering Department, </institution> <year> 1992. </year>
Reference: [185] <author> S. Johnson. </author> <title> Hierarchical clustering schemes. </title> <journal> Psychometrika, </journal> <volume> 32(3), </volume> <month> September </month> <year> 1967. </year>
Reference-contexts: Data mining stems from work in statistics and unsupervised learning and has drawn from ideas taken from automatic discovery <ref> [2, 66, 79, 124, 185] </ref>. Many approaches to data mining use techniques such as clustering, rule induction, and classification to analyze the large set of data. From this set of data, data mining systems attempt to induce general "laws" and classification procedures for characterizing the data.
Reference: [186] <author> M. Jordan and R. Jacobs. </author> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <type> Technical Report AI Memo 1440, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1993. </year>
Reference: [187] <author> M. Jordan and L. Xu. </author> <title> Convergence results for the EM approach to mixtures of experts architectures. </title> <type> Technical Report AI Memo 1458, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1993. </year>
Reference: [188] <author> L. Kaelbling. </author> <title> Associative reinforcement learning: A generate and test algorithm. </title> <journal> Machine Learning, </journal> <volume> 15(3), </volume> <year> 1994. </year>
Reference: [189] <author> D. Kibler and D. Aha. </author> <title> Comparing instance-averaging with instance-filtering learn ing algorithms. </title> <booktitle> In Proceedings of the Third European Working Session on Learning, </booktitle> <pages> pages 68-80, </pages> <year> 1988. </year>
Reference-contexts: 8i 2 sample do; if class (i) 6= class (k-NN (i)) then delete (i,sample); endif; enddo; memory-base = memory-base + sample; end; algorithm Edit-Correct; 8i 2 memory-base do; if class (i) = class (k-NN (i)) then delete (i); endif; enddo; end; 142 editing) and instance averaging algorithms in instance-based learning <ref> [3, 5, 7, 6, 189, 190] </ref>. Aha's IB2 and IB3 algorithms apply a standard nearest-neighbor rule for classification, but then proceed to edit examples based on Ritter's approach. For Aha et al., the assumption is that misclassification comes from noisy attributes in the data.
Reference: [190] <author> D. Kibler and D. Aha. </author> <title> Comparing instance-saving with instance-averaging learn ing algorithms. In D.P.Benjamin, editor, Change of Representation and Inductive Bias. </title> <publisher> Kluwer Academic Publisher, Norwell, </publisher> <address> MA, </address> <year> 1989. </year>
Reference-contexts: 8i 2 sample do; if class (i) 6= class (k-NN (i)) then delete (i,sample); endif; enddo; memory-base = memory-base + sample; end; algorithm Edit-Correct; 8i 2 memory-base do; if class (i) = class (k-NN (i)) then delete (i); endif; enddo; end; 142 editing) and instance averaging algorithms in instance-based learning <ref> [3, 5, 7, 6, 189, 190] </ref>. Aha's IB2 and IB3 algorithms apply a standard nearest-neighbor rule for classification, but then proceed to edit examples based on Ritter's approach. For Aha et al., the assumption is that misclassification comes from noisy attributes in the data.
Reference: [191] <author> H. Kimura, M. Yamamura, and S. Kobayashi. </author> <title> Reinforcement learning by stochastic hill climbing on discounted reward. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 295-303, </pages> <year> 1995. </year> <month> 241 </month>
Reference: [192] <author> K. Kira and L. Rendell. </author> <title> A practical approach to feature selection. </title> <booktitle> In Pro ceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pages 249-256, </pages> <address> Aberdeen, Scotland, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [193] <author> S. Koenig and R. Simmons. </author> <title> Complexity analysis of real-time reinforcement learning applied to finding shortest paths in deterministic domains. </title> <type> Technical Report CMU-CS-93-106, </type> <institution> Carnegie Mellon University, </institution> <year> 1992. </year>
Reference: [194] <author> D. Koller and N. Megiddo. </author> <title> Finding mixed strategies with small supports in extensive form games. </title> <journal> International Journal of Game Theory, </journal> <volume> 25 </volume> <pages> 73-92, </pages> <year> 1996. </year>
Reference-contexts: Others have provided similar strategies to modeling opponents, such as the agent tracking methods of Tambe [336, 337, 338, 339, 340], and the equilibrium search methods of Goldman and Rosenschein [144, 243] and of Koller <ref> [194, 195, 196, 197, 359] </ref>. For their approach, Carmel and Markovitch assume a two-player game where each player has an evaluation function to be used in a minimax search of the game tree.
Reference: [195] <author> D. Koller, N. Megiddo, and B. von Stengel. </author> <title> Fast algorithms for finding randomized strategies in game trees. </title> <booktitle> In Proceedings of the 26th ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 750-759, </pages> <year> 1994. </year>
Reference-contexts: Others have provided similar strategies to modeling opponents, such as the agent tracking methods of Tambe [336, 337, 338, 339, 340], and the equilibrium search methods of Goldman and Rosenschein [144, 243] and of Koller <ref> [194, 195, 196, 197, 359] </ref>. For their approach, Carmel and Markovitch assume a two-player game where each player has an evaluation function to be used in a minimax search of the game tree.
Reference: [196] <author> D. Koller, N. Megiddo, and B. von Stengel. </author> <title> Efficient computation of equilibria for extensive two-person games. Games and Economic Behavior, </title> <note> 1995. To appear. </note>
Reference-contexts: This, of course, is the simplest form of multi-agent learning. 3.3.3 Reinforcement Learning in Cognitive Game Theory In the economics community, one of the communities responsible for considerable research in game theory <ref> [49, 123, 130, 196, 286, 358, 360] </ref>, limited work is being done in learning game strategies. This work, however, is largely restricted to single-player games. <p> Others have provided similar strategies to modeling opponents, such as the agent tracking methods of Tambe [336, 337, 338, 339, 340], and the equilibrium search methods of Goldman and Rosenschein [144, 243] and of Koller <ref> [194, 195, 196, 197, 359] </ref>. For their approach, Carmel and Markovitch assume a two-player game where each player has an evaluation function to be used in a minimax search of the game tree.
Reference: [197] <author> D. Koller and A. Pfeffer. </author> <title> Generating and solving imperfect information games. </title> <booktitle> In Proceedings of the 1995 International Joint Conference on Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: Others have provided similar strategies to modeling opponents, such as the agent tracking methods of Tambe [336, 337, 338, 339, 340], and the equilibrium search methods of Goldman and Rosenschein [144, 243] and of Koller <ref> [194, 195, 196, 197, 359] </ref>. For their approach, Carmel and Markovitch assume a two-player game where each player has an evaluation function to be used in a minimax search of the game tree.
Reference: [198] <author> J. Koza. </author> <title> Genetic Programming: On the Programming of Computers by Means of Natural Selection. </title> <publisher> The MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: Automatic programming had lost popularity until recently with the advent of genetic programming, evolutionary programming, and relational learning. The task of automatic programming is to derive a procedure for a program using a set of examples or formal specifications of that behavior <ref> [198, 206, 208, 219] </ref>. Genetic programming approaches the problem by representing programs as parse trees of functions and variables and applying a genetic algorithm to populations of parse trees to evolve programs satisfying requirements and maximizing performance on a set of test cases.
Reference: [199] <author> H. Kuhn. </author> <title> Extensive games and the problem of information. </title> <editor> In H. W. Kuhn and A. W. Tucker, editors, </editor> <title> Contributions to the Theory of Games II, </title> <journal> volume 28 of Annals of Mathematical Studies, </journal> <pages> pages 193-216. </pages> <publisher> Princeton University Press, </publisher> <year> 1953. </year>
Reference-contexts: Proof: See <ref> [199] </ref>. Given the concept of a behavioral strategy, we can modify (slightly) our solution concept by applying the theorem by Nash and the theorem by Kuhn. Specifically, we 21 determine a payoff matrix at each stage of play and solve it as if it is a normal-form game.
Reference: [200] <author> H. Lai and K. Tanaka. </author> <title> An n-person noncooperative discounted vector valued dynamic game with a stopped set. Computers and Mathematics with Applications, </title> <address> 13(1-3):227-237, </address> <year> 1987. </year>
Reference-contexts: Imado and Ishihara also consider the case where two missiles attempt to shoot down an airplane [175]. Lai and Tanaka consider general n-person games where each player attempts to force opponents into a terminal location in the playing field <ref> [200] </ref>. Related to work in partially observable Markov decision processes [70, 71, 180], Galperin and Skowronski are studying games in which noise is introduced into game dynamics [134]. Corless et al. consider the case where state information is uncertain [88].
Reference: [201] <author> P. Langley and S. Sage. </author> <title> Scaling to domains with many irrelevant features. </title> <institution> Learn ing Systems Department, Siemens Corporate Research, Princeton, NJ, </institution> <year> 1993. </year>
Reference: [202] <author> V. Laporte, J. Nicolas, and P. Bernhard. </author> <title> About the resolution of discrete pursuit games and its application to naval warfare. </title> <editor> In R. Hamalainen and H. Ehtamo, editors, </editor> <booktitle> Differential Games|Developments in Modeling and Computation, </booktitle> <pages> pages 151-163. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference: [203] <author> A. Levchenkov, A. Pashkov, and S. Terekhov. </author> <title> A construction of the value function in some differential games of approach with two pursuers and one evader. </title> <editor> In R. Hamalainen and H. Ehtamo, editors, </editor> <booktitle> Differential Games|Developments in Modeling and Computation, </booktitle> <pages> pages 38-47. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year> <month> 242 </month>
Reference: [204] <author> R. Levinson. </author> <title> General game-playing and reinforcement learning. </title> <type> Technical Report UCSC-CRL-95-06, </type> <institution> Department of Computer Science, University of California, Santa Cruz, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: In a normal form game, this is equivalent to filling out the entries in the payoff matrix. In an extensive form game, this could involve learning the heuristic evaluation function to be applied at the interior nodes of the game tree <ref> [123, 140, 155, 156, 157, 204, 258, 259, 260, 261, 286] </ref>. * Opponent capabilities are not known. In most studies in game theory, each player knows the permissible actions for itself and its opponent. In problems such as differential games, this may not be true.
Reference: [205] <author> J. Lewin. </author> <title> Differential Games. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: As an interesting discrete problem, similar to the Lady in the Lake <ref> [205] </ref>, Bernhard et al. consider the Rabbit and Hunter game [47] under conditions of stationary and nonstationary dynamics. In both the Lady in the Lake and the Rabbit and Hunter games, one player is constrained to follow a boundary while the other player has free movement. <p> This game is described in detail in <ref> [205] </ref>. The playing field is shown in Figure 2.3. In this game, there are two players, P and E. An object is placed in the plane somewhere above the x-axis (i.e., y &gt; 0) at a random location and begins to drop toward the x-axis. <p> For E, the angle is fixed and E must determine the appropriate magnitude of force to apply. This simple game of force is derived from the dolichobrachistochrone game|one of 156 the classic games studied in differential game theory <ref> [177, 205] </ref>. 5 In this game, play takes place in the quadrant of a plane where x 0 and y 0. The objective is for P to drive an object into the y axis. E attempts to keep P from succeeding. <p> In other words, P can make instantaneous turns between 45 ffi and +45 ffi while E can make instantaneous turns 174 between 90 ffi and +90 ffi . This game is a generalization of the Homicidal Chauffeur game <ref> [39, 177, 205] </ref>. In the Homicidal Chauffeur, only the mobility of P is limited. Given the added complexity of the game, no optimal solution was available; however, we were able to define a heuristic based on the optimal solution for the Homicidal Chauffeur.
Reference: [206] <author> M. Lewis, A. Fagg, and A. Solidum. </author> <title> Genetic programming approach to the construction of a neural network for control of a walking robot. </title> <booktitle> In Proceedings of the 1992 IEEE International Conference on Robotics and Automation, </booktitle> <volume> volume 3, </volume> <pages> pages 2618-2623. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1992. </year>
Reference-contexts: Automatic programming had lost popularity until recently with the advent of genetic programming, evolutionary programming, and relational learning. The task of automatic programming is to derive a procedure for a program using a set of examples or formal specifications of that behavior <ref> [198, 206, 208, 219] </ref>. Genetic programming approaches the problem by representing programs as parse trees of functions and variables and applying a genetic algorithm to populations of parse trees to evolve programs satisfying requirements and maximizing performance on a set of test cases.
Reference: [207] <author> J. Lin and J. Vitter. </author> <title> A theory for memory-based learning. </title> <type> Technical Report CS-92-53, </type> <institution> Brown University, </institution> <year> 1992. </year>
Reference: [208] <author> L. Lin. </author> <title> Programming robots using reinforcement learning and teaching. </title> <booktitle> In Pro ceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 781-786, </pages> <year> 1991. </year>
Reference-contexts: Automatic programming had lost popularity until recently with the advent of genetic programming, evolutionary programming, and relational learning. The task of automatic programming is to derive a procedure for a program using a set of examples or formal specifications of that behavior <ref> [198, 206, 208, 219] </ref>. Genetic programming approaches the problem by representing programs as parse trees of functions and variables and applying a genetic algorithm to populations of parse trees to evolve programs satisfying requirements and maximizing performance on a set of test cases. <p> This differs from games of imperfect information in that the environment can be learned from experience. Thus, the in 51 formation sets associated with various states in the game can vary over time as the players learn the environment. This is analogous to exploring the environment in navigation problems <ref> [208, 209, 219, 238, 239] </ref>. * The game dynamics are uncertain [134]. This is analogous to the problem of learning an opponent's strategy except that the dynamics, usually, are fixed. Learning game dynamics when the dynamics are not fixed is also of interest [152, 165].
Reference: [209] <author> L. Lin. </author> <title> Self-improving reactive agents based on reinforcement learning, planning and teaching. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 293-321, </pages> <year> 1992. </year>
Reference-contexts: This differs from games of imperfect information in that the environment can be learned from experience. Thus, the in 51 formation sets associated with various states in the game can vary over time as the players learn the environment. This is analogous to exploring the environment in navigation problems <ref> [208, 209, 219, 238, 239] </ref>. * The game dynamics are uncertain [134]. This is analogous to the problem of learning an opponent's strategy except that the dynamics, usually, are fixed. Learning game dynamics when the dynamics are not fixed is also of interest [152, 165].
Reference: [210] <author> L. Lin and T. Mitchell. </author> <title> Memory approaches to reinforcement learning in non markovian domains. </title> <type> Technical Report CMU-CS-92-138, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <year> 1992. </year>
Reference-contexts: In addition to generating the examples, expected behaviors must also be learned. Within the memory-based framework, behaviors (or outcomes) can correspond to the estimate of expected payoff. In this case, any of the real-time dynamic programming or temporal difference methods can be applied to update these behaviors <ref> [16, 43, 44, 53, 210, 263, 264, 265] </ref>.
Reference: [211] <author> M. Littman. </author> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Proceedings of the Eleventh International Machine Learning Conference, </booktitle> <pages> pages 157-163, </pages> <address> New Brunswick, NJ, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Two approaches to solving MDPs are value iteration and policy iteration <ref> [35, 211, 23 269] </ref>. <p> We consider (s) to be the combined policy of 1 (s) and 2 (s). Then, f (s i ) = E t=0 # as before. We can estimate f (s i ) for some (s i ) = a as follows <ref> [211] </ref>: f (s i ) Q f (s i ; a 1 ; a 2 ) = c (s i ; a 1 ; a 2 ) + fl s j 2S From this, we are able to establish a combined policy, based on the current estimate Q f ; namely, <p> Michael Littman explored the possibility of using Q-learning for co-learning among homogeneous players in the context of Markov games <ref> [211, 213] </ref>. It appears his approach also applies to heterogeneous players, but no such experiments were reported.
Reference: [212] <author> M. Littman. </author> <title> Memoryless policies: Theoretical limitations and practical results. </title> <booktitle> In Proceedings of the Third International Conference on the Simulation of Adaptive Behavior, </booktitle> <year> 1994. </year>
Reference: [213] <author> M. Littman. </author> <title> Algorithms for Sequential Decision Making. </title> <type> PhD thesis, </type> <institution> Brown Uni versity, Department of Computer Science, </institution> <year> 1996. </year>
Reference-contexts: Michael Littman explored the possibility of using Q-learning for co-learning among homogeneous players in the context of Markov games <ref> [211, 213] </ref>. It appears his approach also applies to heterogeneous players, but no such experiments were reported. <p> This policy is given by returning an action according to mixed strategies derived for one player which is then fixed to permit selecting the other player's action through simple minimization <ref> [213] </ref>. The mixed strategy for the first player is determined by solving the linear program described in Section 2.3. Obviously, this approach is not guaranteed to be optimal because both the linear program and its dual must be solved to determine appropriate strategies satisfying the constraints of the linear program. <p> These results (and the results of the next chapter) can be extended to alternating Markov games (in which players take turns) <ref> [213] </ref>, team games (in which teams of players cooperate to devise mutual strategies) [337, 338], and community games (in which players choose opponents to maximize their personal payoff) [325].
Reference: [214] <author> M. Littman and J. Boyan. </author> <title> A distributed reinforcement learning scheme for network routing. </title> <type> Technical Report CMU-CS-93-165, </type> <institution> School of Computer Science, Carnegie-Mellon University, </institution> <year> 1993. </year>
Reference: [215] <author> M. Littman, A. Cassandra, and L. Kaelbling. </author> <title> Learning policies for partially observable environments: Scaling up. </title> <booktitle> In Proceedings of the Twelfth International Machine Learning Conference. </booktitle> <publisher> Morgan Kaufmann, Publishers, </publisher> <year> 1995. </year>
Reference: [216] <author> M. Littman, T. Dean, and L. Kaelbling. </author> <title> On the complexity of solving markov decision problems. </title> <booktitle> In Proceedings of the Eleventh International Machine Learning Conference. </booktitle> <publisher> Morgan Kaufmann, Publishers, </publisher> <year> 1994. </year> <month> 243 </month>
Reference: [217] <author> R. Luce and H. Raiffa. </author> <title> Games and Decisions: Introduction and Critical Survey. </title> <publisher> John Wiley and Sons, </publisher> <year> 1957. </year>
Reference-contexts: Similar to defining the set of mixed strategies to each player in a game in normal form, if we define the set of "mixed strategies" for each player for each information set in a game of imperfect information, then we have defined behavioral strategies for each player <ref> [217] </ref>.
Reference: [218] <author> R. Maclin and J. Shavlik. </author> <title> Creating advice-taking reinforcement learners. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 251-282, </pages> <year> 1996. </year>
Reference-contexts: Note this approach differs from previous teaching approaches in that only high-level advice is provided rather than specific instructions to be carried out during the task. Maclin and Shavlik also provide an advice-taking reinforcement learner <ref> [218] </ref>. In their system, they use a connectionist Q-learner that interacts with an external advisor in five steps: 1. The agent requests and receives advice from the advisor. 2. The advice is converted into an internal representation. 3. The advice is converted into a usable form. 4.
Reference: [219] <author> S. Mahadevan. </author> <title> Automatic programming of behavior-based robots using reinforce ment learning. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 768-773, </pages> <year> 1991. </year>
Reference-contexts: Automatic programming had lost popularity until recently with the advent of genetic programming, evolutionary programming, and relational learning. The task of automatic programming is to derive a procedure for a program using a set of examples or formal specifications of that behavior <ref> [198, 206, 208, 219] </ref>. Genetic programming approaches the problem by representing programs as parse trees of functions and variables and applying a genetic algorithm to populations of parse trees to evolve programs satisfying requirements and maximizing performance on a set of test cases. <p> This differs from games of imperfect information in that the environment can be learned from experience. Thus, the in 51 formation sets associated with various states in the game can vary over time as the players learn the environment. This is analogous to exploring the environment in navigation problems <ref> [208, 209, 219, 238, 239] </ref>. * The game dynamics are uncertain [134]. This is analogous to the problem of learning an opponent's strategy except that the dynamics, usually, are fixed. Learning game dynamics when the dynamics are not fixed is also of interest [152, 165].
Reference: [220] <author> S. Mahadevan. </author> <title> To discount or not to discount in reinforcement learning: A case study comparing R learning and Q learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, Publishers, </publisher> <year> 1994. </year>
Reference-contexts: The feedback about those decisions might be considerably delayed, and this delay makes learning much more difficult. In this case, the problem is called a delayed reinforcement problem <ref> [109, 167, 168, 220, 221, 317, 343, 363] </ref>. The basic loop followed in sequential decision making tasks such as these includes evaluating the current state, taking an action, and computing the new state.
Reference: [221] <author> S. Mahadevan. </author> <title> Average reward reinforcement learning: Foundations, algorithms, and empirical results. </title> <booktitle> Machine Learning, </booktitle> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: The feedback about those decisions might be considerably delayed, and this delay makes learning much more difficult. In this case, the problem is called a delayed reinforcement problem <ref> [109, 167, 168, 220, 221, 317, 343, 363] </ref>. The basic loop followed in sequential decision making tasks such as these includes evaluating the current state, taking an action, and computing the new state.
Reference: [222] <author> K. Markey. </author> <title> Efficient learning of multiple degree-of-freedom control problems with quasi-independent Q-agents. </title> <booktitle> In Proceedings of 1993 Connectionist Models Summer School, </booktitle> <year> 1993. </year>
Reference: [223] <author> J. Maynard Smith. </author> <title> Evolution and the Theory of Games. </title> <publisher> Cambridge University Press, </publisher> <year> 1982. </year>
Reference-contexts: Some work has been done in evolutionary computation, artificial life, and iterated games. The most common game considered in this context is the iterated prisoner's dilemma <ref> [28, 223, 234, 299, 325] </ref>. The Prisoner's Dilemma is a two-player non-zero-sum game in which players must decide to cooperate or defect based on their expected payoffs.
Reference: [224] <author> R. McCallum. </author> <title> Overcoming incomplete perception with utile distinction memory. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <year> 1993. </year>
Reference: [225] <author> R. McCallum. </author> <title> First results with instance-based state identification for reinforce ment learning. </title> <type> Technical Report TR 502, </type> <institution> University of Rochester, </institution> <year> 1994. </year>
Reference-contexts: A key distinction between control systems and planning systems is the focus on achieving a specific goal; although, frequently the distinction between them becomes blurred [252]. 1.3 Learning and Intelligent Agents Recently, the machine learning community has paid increasing attention to problems of delayed reinforcement learning <ref> [37, 121, 179, 225, 228, 318, 368] </ref>. These problems usually involve an agent that has to make a sequence of decisions, or actions, in an environment that provides feedback about those decisions. The feedback about those decisions might be considerably delayed, and this delay makes learning much more difficult.
Reference: [226] <author> R. McCallum. </author> <title> Reduced training time for reinforcement learning with hidden state. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <year> 1994. </year>
Reference: [227] <author> R. McCallum. </author> <title> Instance-based state identification for reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 377-384, </pages> <year> 1995. </year>
Reference: [228] <author> R. McCallum. </author> <title> Instance-based utile distinction for reinforcement learning with hid den state. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <year> 1995. </year>
Reference-contexts: A key distinction between control systems and planning systems is the focus on achieving a specific goal; although, frequently the distinction between them becomes blurred [252]. 1.3 Learning and Intelligent Agents Recently, the machine learning community has paid increasing attention to problems of delayed reinforcement learning <ref> [37, 121, 179, 225, 228, 318, 368] </ref>. These problems usually involve an agent that has to make a sequence of decisions, or actions, in an environment that provides feedback about those decisions. The feedback about those decisions might be considerably delayed, and this delay makes learning much more difficult.
Reference: [229] <author> D. McGregor, M. Odetayo, and D Dasgupta. </author> <title> Adaptive-control of a dynamic system using genetic-based methods. </title> <booktitle> In Proceedings of the 1992 IEEE International Symposium on Intelligent Control, </booktitle> <pages> pages 521-525. </pages> <publisher> IEEE, </publisher> <year> 1992. </year>
Reference: [230] <author> A. Merz. </author> <title> Stochastic guidance laws in satellite pursuit-evasion. Computers and Mathematics with Applications, </title> <address> 13(1-3):151-156, </address> <year> 1987. </year> <month> 244 </month>
Reference-contexts: Such cases are closer to 2 1 2 dimensions given normal orientations of pilots in gravity. 38 Merz considered a true three-dimensional pursuit game by considering stochastic guidance of satellites in orbit, engaged in a pursuit-evasion conflict <ref> [230] </ref>. As an interesting discrete problem, similar to the Lady in the Lake [205], Bernhard et al. consider the Rabbit and Hunter game [47] under conditions of stationary and nonstationary dynamics.
Reference: [231] <author> S. Mikami, H. Tano, and Y. Kakazu. </author> <title> An autonomous legged robot that learns to walk through simulated evolution. In Self-organization and life, from simple rules to global complexity, </title> <booktitle> Proceedings of the Second European Conference on Artificial Life, </booktitle> <pages> pages 758-767. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1993. </year>
Reference: [232] <author> J. Millan and C. Torras. </author> <title> A reinforcement connectionist approach to robot path finding in non-maze-like environments. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 363-395, </pages> <year> 1992. </year>
Reference: [233] <author> G. Miller and D. Cliff. </author> <title> Co-evolution of pursuit and evasion I: Biological and game-theoretic foundations. </title> <type> Technical Report CSRP311, </type> <institution> School of Cognitive and Computing Sciences, University of Sussex, </institution> <month> August </month> <year> 1994. </year>
Reference: [234] <author> J. Miller. </author> <title> The coevolution of automata in the repeated prisoner's dilemma. </title> <type> Tech nical Report No. 8903, </type> <institution> Santa Fe Institute, </institution> <year> 1989. </year>
Reference-contexts: Some work has been done in evolutionary computation, artificial life, and iterated games. The most common game considered in this context is the iterated prisoner's dilemma <ref> [28, 223, 234, 299, 325] </ref>. The Prisoner's Dilemma is a two-player non-zero-sum game in which players must decide to cooperate or defect based on their expected payoffs.
Reference: [235] <author> J. Mingers. </author> <title> An empirical comparison of pruning methods for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 4(2) </volume> <pages> 227-243, </pages> <year> 1989. </year>
Reference-contexts: resulting system GAME (GA plus memory plus editing). 5.4.1 Editing Methods for Nearest Neighbor Early work by Wilson [373] showed that examples could be removed from a set used for classification, and suggested that simply editing would frequently improve classification accuracy (in the same way that pruning improves decision trees <ref> [235] </ref>). Wilson's algorithm classifies each example in a data set with its own k nearest neighbors.
Reference: [236] <author> J. Mingers. </author> <title> An empirical comparison of selection measures for decision tree induc tion. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 319-342, </pages> <year> 1989. </year>
Reference: [237] <author> A Moore. </author> <title> Efficient Memory-Based Learning for Robot Control. </title> <type> PhD thesis, </type> <institution> Com puter Laboratory, Cambridge University, </institution> <year> 1990. </year>
Reference-contexts: Control theory focuses on developing optimal procedures for maintaining equilibrium or for achieving some performance objective by determining values of several "control variables" in a dynamic system <ref> [1, 26, 40, 45, 71, 100, 101, 156, 237, 252, 253] </ref>. Machine learning systems attempt to determine optimal values for these control variables from experience rather than explicitly solving a set of differential equations. <p> Then it becomes important to be able to interpolate between actions within a region. One approach to combating these problems has been proposed by Atkeson, Moore, and Schaal <ref> [26, 27, 25, 110, 237] </ref>. In their approach, they apply local weighted regression among the examples to determine the proper action. <p> Moore suggests that inverse models, although somewhat natural, can lead to problems if there is not a one-to-one mapping between actions and behaviors or if there are noisy examples in the memory base <ref> [26, 237] </ref>. As a result, he proposes working directly with the forward model. Using the memory base then consists of searching through available actions (in a given state) until the desired outcome is found. <p> In this chapter, we consider an alternative algorithm that, although not memory-based, is inspired by the results of applying kd-trees in memory-based learning <ref> [46, 110, 128, 129, 237] </ref>. A kd-tree is a data structure used to store a set of examples in a memory base such that nearest neighbors can be found in logarithmic expected time. <p> Specifically, a kd-tree is a binary tree where each interior node of the tree tests the values of one of the attributes in the k-dimensional attribute space. In addition, each node corresponds to a single instance in the memory base <ref> [237] </ref>. Nodes are selected for splitting until no further splits are required (i.e., until all points are represented in the tree). In memory-based learning, the kd-tree can provide significant speed-up in search 182 ing for nearest neighbors; however, the size of the memory based does not change.
Reference: [238] <author> A. Moore. </author> <title> The parti-game algorithm for variable resolution reinforcement learning in multidimensional state-spaces. </title> <booktitle> In Neural Information Processing Systems 6, </booktitle> <year> 1994. </year>
Reference-contexts: This differs from games of imperfect information in that the environment can be learned from experience. Thus, the in 51 formation sets associated with various states in the game can vary over time as the players learn the environment. This is analogous to exploring the environment in navigation problems <ref> [208, 209, 219, 238, 239] </ref>. * The game dynamics are uncertain [134]. This is analogous to the problem of learning an opponent's strategy except that the dynamics, usually, are fixed. Learning game dynamics when the dynamics are not fixed is also of interest [152, 165].
Reference: [239] <author> A. Moore and C. Atkeson. </author> <title> Prioritized sweeping: Reinforcement learning with less data and less time. </title> <journal> Machine Learning, </journal> <volume> 13 </volume> <pages> 103-130, </pages> <year> 1993. </year>
Reference-contexts: This differs from games of imperfect information in that the environment can be learned from experience. Thus, the in 51 formation sets associated with various states in the game can vary over time as the players learn the environment. This is analogous to exploring the environment in navigation problems <ref> [208, 209, 219, 238, 239] </ref>. * The game dynamics are uncertain [134]. This is analogous to the problem of learning an opponent's strategy except that the dynamics, usually, are fixed. Learning game dynamics when the dynamics are not fixed is also of interest [152, 165].
Reference: [240] <author> A. Moore and C. Atkeson. </author> <title> The parti-game algorithm for variable resolution reinforcement learning in multidimensional state-spaces. </title> <booktitle> Machine Learning, </booktitle> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: Several researchers have investigated approaches to varying the resolution of the memory base according to the requirements of the problem <ref> [104, 110, 240, 312] </ref>. For example, Moore and Atkeson's parti-game algorithm uses the concept of an adversary attempting to thwart search to determine how to explore the search space [240]. <p> Several researchers have investigated approaches to varying the resolution of the memory base according to the requirements of the problem [104, 110, 240, 312]. For example, Moore and Atkeson's parti-game algorithm uses the concept of an adversary attempting to thwart search to determine how to explore the search space <ref> [240] </ref>. In the current version of parti-game, the problem to be solved is limited to the case where there exists a known goal region rather than a reward function. <p> Initial Q values did not matter since they would be learned over time. Nevertheless, a uniform random sampling of the space was, apparently, not sufficient to approximate some of the surfaces encountered in these games. Applications of variable resolution techniques <ref> [16, 110, 240, 312] </ref> may be more appropriate for problems such as these. In some ways, the results from MBCL are highly encouraging. They indicate co-learning can occur and suggest it is possible to learn optimal solutions to two-player differential games.
Reference: [241] <author> A. Moore, C. Atkeson, and S. Schaal. </author> <title> Memory-based learning for control. </title> <type> Technical Report CMU-RI-TR-95-18, </type> <institution> Carnegie-Mellon University, </institution> <month> April </month> <year> 1995. </year>
Reference: [242] <author> A. Moore and J. Schneider. </author> <title> Memory-based stochastic optimization. </title> <booktitle> In Neural Information Processing Systems 7, </booktitle> <year> 1995. </year>
Reference: [243] <author> Y. Mor, C. Goldman, and J. Rosenschein. </author> <title> Learn your opponent's strategy (in polynomial time)! In Proceedings of the IJCAI Workshop on Adaptation and Learning in Multagent Systems, </title> <year> 1995. </year>
Reference-contexts: For example, a pursuer may have a long range and be able to pursue the evader for a long period of time, but the evader may assume (given the nature of the encounter) that the range is more limited. This problem is addressed through techniques such as opponent modeling <ref> [68, 67, 69, 243] </ref>. * The game environment is not fully described. This differs from games of imperfect information in that the environment can be learned from experience. <p> Others have provided similar strategies to modeling opponents, such as the agent tracking methods of Tambe [336, 337, 338, 339, 340], and the equilibrium search methods of Goldman and Rosenschein <ref> [144, 243] </ref> and of Koller [194, 195, 196, 197, 359]. For their approach, Carmel and Markovitch assume a two-player game where each player has an evaluation function to be used in a minimax search of the game tree.
Reference: [244] <author> D. Moriarty and R. Miikkulainen. </author> <title> Learning sequential decision tasks. </title> <institution> Techni cal Report AI95-229, Department of Computer Science, The University of Texas at Austin, </institution> <month> January </month> <year> 1995. </year> <month> 245 </month>
Reference: [245] <author> K. Moritz, R. Polis, and K. </author> <title> Well. Pursuit-evasion in medium-range air-combat scenarios. Computers and Mathematics with Applications, </title> <address> 13(1-3):167-180, </address> <year> 1987. </year>
Reference: [246] <author> P. Morris. </author> <title> Introduction to Game Theory. </title> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference: [247] <author> S. Murthy. </author> <title> On Growing Better Decision Trees from Data. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, The Johns Hopkins University, </institution> <year> 1995. </year>
Reference-contexts: Finally, it may be appropriate to consider non-axis-parallel trees in growing trees for TBCL. Work by Heath and Murthy has pointed out several issues and offered several suggestions for constructing oblique decision trees and addressing concerns such as look-ahead and splitting criteria <ref> [166, 247] </ref>. In Section 7.4 we point out that one of the significant differences between MBCL and TBCL is that Q-updates in MBCL occur over a region in the instance space where Q-updates in TBCL apply only to individual cells in the game matrix.
Reference: [248] <author> D. Mutchler. </author> <title> The multi-player version of minimax displays game pathology. </title> <journal> Arti ficial Intelligence, </journal> <volume> 64(2) </volume> <pages> 323-336, </pages> <month> December </month> <year> 1993. </year>
Reference: [249] <author> T. Nagao, T. Agui, and H. Nagahashi. </author> <title> A genetic method for optimization of asynchronous random neural networks and its application to action control. </title> <booktitle> In Proceedings of the 1993 International Joint Conference on Neural Networks, Nagoya (Japan), </booktitle> <pages> pages 2869-2872. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference: [250] <author> J. Nash. </author> <title> Non-cooperative games. </title> <journal> Annals of Mathematics, </journal> <volume> 54(2) </volume> <pages> 286-295, </pages> <year> 1951. </year>
Reference-contexts: From this, we can determine a Nash equilibrium point for any normal-form game. Theorem 2.1 (Nash) Given any n-player non-cooperative game, there exists at least one Nash equilibrium point consisting of mixed strategies. Proof: See <ref> [250, 254, 360] </ref>. If we reconsider the concept of a normal form game and limit our discussion to two-person zero-sum games, it has been shown that Nash equilibrium points with mixed strategies can be found by solving the following linear program.
Reference: [251] <author> D. Nau. </author> <title> Decision quality as a function of search depth on game trees. </title> <journal> Journal of the Association of Computing Machinery, </journal> <volume> 30(4) </volume> <pages> 687-708, </pages> <month> October </month> <year> 1983. </year>
Reference: [252] <author> D. Nguyen and B. Widrow. </author> <title> The truck backer-upper: An example of self learning in neural networks. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 357-363, </pages> <year> 1989. </year>
Reference-contexts: Control theory focuses on developing optimal procedures for maintaining equilibrium or for achieving some performance objective by determining values of several "control variables" in a dynamic system <ref> [1, 26, 40, 45, 71, 100, 101, 156, 237, 252, 253] </ref>. Machine learning systems attempt to determine optimal values for these control variables from experience rather than explicitly solving a set of differential equations. <p> Classic planning problems in artificial intelligence have included stacking blocks and navigating mazes. A key distinction between control systems and planning systems is the focus on achieving a specific goal; although, frequently the distinction between them becomes blurred <ref> [252] </ref>. 1.3 Learning and Intelligent Agents Recently, the machine learning community has paid increasing attention to problems of delayed reinforcement learning [37, 121, 179, 225, 228, 318, 368].
Reference: [253] <author> N. Nilsson. </author> <title> Teloe-reactive programs for agent control. </title> <journal> Journal of Artificial Intelli gence Research, </journal> <volume> 1 </volume> <pages> 139-158, </pages> <year> 1994. </year>
Reference-contexts: Control theory focuses on developing optimal procedures for maintaining equilibrium or for achieving some performance objective by determining values of several "control variables" in a dynamic system <ref> [1, 26, 40, 45, 71, 100, 101, 156, 237, 252, 253] </ref>. Machine learning systems attempt to determine optimal values for these control variables from experience rather than explicitly solving a set of differential equations.
Reference: [254] <author> G. Owen. </author> <title> Game Theory. </title> <publisher> Academic Press, </publisher> <year> 1982. </year>
Reference-contexts: From this, we can determine a Nash equilibrium point for any normal-form game. Theorem 2.1 (Nash) Given any n-player non-cooperative game, there exists at least one Nash equilibrium point consisting of mixed strategies. Proof: See <ref> [250, 254, 360] </ref>. If we reconsider the concept of a normal form game and limit our discussion to two-person zero-sum games, it has been shown that Nash equilibrium points with mixed strategies can be found by solving the following linear program.
Reference: [255] <author> M. Pachter. </author> <title> Simple-motion pursuit-evasion in the half plane. </title> <journal> Computers and Math ematics with Applications, </journal> <volume> 13(1-3):69-82, </volume> <year> 1987. </year>
Reference: [256] <author> M. Pachter and T. Miloh. </author> <title> The geometric approach to the construction of the barrier surface in differential games. Computers and Mathematics with Applications, </title> <address> 13(1-3):47-67, </address> <year> 1987. </year>
Reference: [257] <author> L. Paletta. </author> <title> Temporal difference learning in RISK-like environments. </title> <type> Master's the sis, </type> <institution> Technical University Graz and The Johns Hopkins University, </institution> <year> 1996. </year>
Reference: [258] <author> B. Pell. </author> <title> Exploratory learning in the game of GO. </title> <type> Technical Report TR 275, </type> <institution> Uni versity of Cambridge, Computer Laboratory, </institution> <year> 1992. </year>
Reference-contexts: In a normal form game, this is equivalent to filling out the entries in the payoff matrix. In an extensive form game, this could involve learning the heuristic evaluation function to be applied at the interior nodes of the game tree <ref> [123, 140, 155, 156, 157, 204, 258, 259, 260, 261, 286] </ref>. * Opponent capabilities are not known. In most studies in game theory, each player knows the permissible actions for itself and its opponent. In problems such as differential games, this may not be true. <p> Carmel and Markovitch applied a basic gradient-descent search strategy for several depths to learn, and combine the results with the learned depth. 85 3.4.2 Learning Chess-Like Games Barney Pell developed an approach to deriving strategies from declarative specifications of games is a system he calls metagamer <ref> [258, 259, 260, 261] </ref>. Metagamer processes the rules and constraints of "symmetric chess-like" games and derives the rules and evaluation functions for these games. Pell's approach has been applied to several symmetric chess-like games including chess, checkers, Tic-Tac-Toe, Othello, and Go. <p> At the end of the game, moves associated with the winner have their w scores incremented, and moves associated with the loser have their l scores incremented <ref> [258, 259] </ref>. The learning problem is one of determining a function that takes the set of legal moves, the w and l statistics, and the features of the current state and returns an evaluation of those moves.
Reference: [259] <author> B. Pell. Metagame: </author> <title> A new challenge for games and learning. </title> <type> Technical Report TR 276, </type> <institution> University of Cambridge, Computer Laboratory, </institution> <year> 1992. </year>
Reference-contexts: In a normal form game, this is equivalent to filling out the entries in the payoff matrix. In an extensive form game, this could involve learning the heuristic evaluation function to be applied at the interior nodes of the game tree <ref> [123, 140, 155, 156, 157, 204, 258, 259, 260, 261, 286] </ref>. * Opponent capabilities are not known. In most studies in game theory, each player knows the permissible actions for itself and its opponent. In problems such as differential games, this may not be true. <p> Carmel and Markovitch applied a basic gradient-descent search strategy for several depths to learn, and combine the results with the learned depth. 85 3.4.2 Learning Chess-Like Games Barney Pell developed an approach to deriving strategies from declarative specifications of games is a system he calls metagamer <ref> [258, 259, 260, 261] </ref>. Metagamer processes the rules and constraints of "symmetric chess-like" games and derives the rules and evaluation functions for these games. Pell's approach has been applied to several symmetric chess-like games including chess, checkers, Tic-Tac-Toe, Othello, and Go. <p> At the end of the game, moves associated with the winner have their w scores incremented, and moves associated with the loser have their l scores incremented <ref> [258, 259] </ref>. The learning problem is one of determining a function that takes the set of legal moves, the w and l statistics, and the features of the current state and returns an evaluation of those moves.
Reference: [260] <author> B. Pell. </author> <title> Strategy Generation and Evaluation for Meta-Game Playing. </title> <type> PhD thesis, </type> <institution> University of Cambridge, </institution> <address> Cambridge, England, </address> <year> 1993. </year> <month> 246 </month>
Reference-contexts: In a normal form game, this is equivalent to filling out the entries in the payoff matrix. In an extensive form game, this could involve learning the heuristic evaluation function to be applied at the interior nodes of the game tree <ref> [123, 140, 155, 156, 157, 204, 258, 259, 260, 261, 286] </ref>. * Opponent capabilities are not known. In most studies in game theory, each player knows the permissible actions for itself and its opponent. In problems such as differential games, this may not be true. <p> Carmel and Markovitch applied a basic gradient-descent search strategy for several depths to learn, and combine the results with the learned depth. 85 3.4.2 Learning Chess-Like Games Barney Pell developed an approach to deriving strategies from declarative specifications of games is a system he calls metagamer <ref> [258, 259, 260, 261] </ref>. Metagamer processes the rules and constraints of "symmetric chess-like" games and derives the rules and evaluation functions for these games. Pell's approach has been applied to several symmetric chess-like games including chess, checkers, Tic-Tac-Toe, Othello, and Go.
Reference: [261] <author> B. Pell. </author> <title> A strategic metagame player for general chess-like games. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <year> 1994. </year>
Reference-contexts: In a normal form game, this is equivalent to filling out the entries in the payoff matrix. In an extensive form game, this could involve learning the heuristic evaluation function to be applied at the interior nodes of the game tree <ref> [123, 140, 155, 156, 157, 204, 258, 259, 260, 261, 286] </ref>. * Opponent capabilities are not known. In most studies in game theory, each player knows the permissible actions for itself and its opponent. In problems such as differential games, this may not be true. <p> Carmel and Markovitch applied a basic gradient-descent search strategy for several depths to learn, and combine the results with the learned depth. 85 3.4.2 Learning Chess-Like Games Barney Pell developed an approach to deriving strategies from declarative specifications of games is a system he calls metagamer <ref> [258, 259, 260, 261] </ref>. Metagamer processes the rules and constraints of "symmetric chess-like" games and derives the rules and evaluation functions for these games. Pell's approach has been applied to several symmetric chess-like games including chess, checkers, Tic-Tac-Toe, Othello, and Go.
Reference: [262] <author> M. Pendrith. </author> <title> On reinforcement learning of control actions in noise and non markovian domains. </title> <type> Technical Report UNSW-CSE-TR-9410, </type> <institution> The University of New South Wales, </institution> <year> 1994. </year>
Reference: [263] <author> J. Peng. </author> <title> Efficient Dynamic Programming-Based Learning for Control. </title> <type> PhD thesis, </type> <institution> Northeastern University, </institution> <year> 1993. </year>
Reference-contexts: In addition to generating the examples, expected behaviors must also be learned. Within the memory-based framework, behaviors (or outcomes) can correspond to the estimate of expected payoff. In this case, any of the real-time dynamic programming or temporal difference methods can be applied to update these behaviors <ref> [16, 43, 44, 53, 210, 263, 264, 265] </ref>.
Reference: [264] <author> J. Peng. </author> <title> Efficient memory-based dynamic programming. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <year> 1995. </year>
Reference-contexts: In addition to generating the examples, expected behaviors must also be learned. Within the memory-based framework, behaviors (or outcomes) can correspond to the estimate of expected payoff. In this case, any of the real-time dynamic programming or temporal difference methods can be applied to update these behaviors <ref> [16, 43, 44, 53, 210, 263, 264, 265] </ref>.
Reference: [265] <author> J. Peng and R. Williams. </author> <title> Incremental multi-step Q-learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <year> 1994. </year>
Reference-contexts: In addition to generating the examples, expected behaviors must also be learned. Within the memory-based framework, behaviors (or outcomes) can correspond to the estimate of expected payoff. In this case, any of the real-time dynamic programming or temporal difference methods can be applied to update these behaviors <ref> [16, 43, 44, 53, 210, 263, 264, 265] </ref>.
Reference: [266] <author> M. Potter, K. De Jong, and J. Grefenstette. </author> <title> A coevolutionary approach to learning sequential decision rules. </title> <booktitle> In Proceedings of the International Conference on Genetic Algorithms, </booktitle> <pages> pages 366-372, </pages> <year> 1995. </year>
Reference-contexts: It is unclear how well this procedure would perform, given their simplifications, had mixed strategies been required. 3.3.6 Coevolution Methods Recently, work in co-evolutionary algorithms has begun to suggest approaches to multi-agent co-learning with some encouraging initial results <ref> [151, 152, 266, 323] </ref>. Extending his work on samuel, Grefenstette defines a uniform sensor architecture for multi-agent environments [151]. He claims that modeling information about all agents in the environment would be too complex, requiring significant computational resources. <p> This seems to track well with our experiments (reported in Section 4.3.3). Further extending samuel, Potter, DeJong, and Grefenstette developed an approach to coevolution in which an agent is decomposed into "subagents" each responsible for learning an activity to be combined to solve a complex task <ref> [266] </ref>. They called this extension Cooperative Coevolutionary Genetic Algorithms. In this approach, multiple agents operate on a single task in parallel. The agents are initialized with rules to bias their activity toward some subset of the total problem.
Reference: [267] <author> U. Prasad and N. Rajan. </author> <title> Aircraft pursuit-evasion problems with variable speeds. Computers and Mathematics with Applications, </title> <address> 13(1-3):111-121, </address> <year> 1987. </year>
Reference-contexts: Many pursuit games focus on fixed control parameters in the dynamics, such as speed, and only permit changes in turn angle or angular velocity. Prasad and Rajan considered the case where, in addition to controlling turn angle, the players had control over speed as well <ref> [267] </ref>. This complicates play by permitting multi-dimensional action spaces. Many researchers limit studies to two dimensions, arguing that the complexities in two dimensions are, in themselves, worthy of study.
Reference: [268] <author> T. Prescott and J. </author> <title> Mayhew. Obstacle avoidance through reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 523-530, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [269] <author> M. Puterman. </author> <title> Markov Decision Processes|Discrete Stochastic Dynamic Program ming. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1994. </year>
Reference: [270] <author> J. Quinlan. </author> <title> Learning efficient classification procedures and their application to chess end games. </title> <editor> In R. Michalski, J. Carbonell, and T. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1983. </year>
Reference-contexts: The mixed strategies stored at each of the tree's leaves is shown in Table 7.1. Note the splits occur along a single attribute, thus the corresponding tree is analogous to the axis-parallel decision trees generated by approaches such as ID3 and C4.5 <ref> [270, 271, 272] </ref>. When limited to two dimensions, axis-parallel trees can be shown as a partitioning of the attribute space.
Reference: [271] <author> J. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: The mixed strategies stored at each of the tree's leaves is shown in Table 7.1. Note the splits occur along a single attribute, thus the corresponding tree is analogous to the axis-parallel decision trees generated by approaches such as ID3 and C4.5 <ref> [270, 271, 272] </ref>. When limited to two dimensions, axis-parallel trees can be shown as a partitioning of the attribute space.
Reference: [272] <author> J. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, Publishers, </publisher> <address> San Mateo, California, </address> <year> 1993. </year>
Reference-contexts: The mixed strategies stored at each of the tree's leaves is shown in Table 7.1. Note the splits occur along a single attribute, thus the corresponding tree is analogous to the axis-parallel decision trees generated by approaches such as ID3 and C4.5 <ref> [270, 271, 272] </ref>. When limited to two dimensions, axis-parallel trees can be shown as a partitioning of the attribute space.
Reference: [273] <author> N. Rajan, U. Prasad, and N. Rao. </author> <title> Pursuit-evasion of two aircraft in a horizontal plane. </title> <journal> Journal of Guidance and Control, </journal> <volume> 3(3) </volume> <pages> 261-267, </pages> <year> 1980. </year>
Reference-contexts: For their research, they focused on a single linear-quadratic differential game of pursuit in which a single missile (designated P ) pursues a single airplane (designated E), which is similar to the problem studied by Grefenstette et al. <ref> [149, 153, 163, 273] </ref>. As a linear-quadratic game, the kinematic equations are linear functions of the current state and action, and the payoff function is a quadratic function of acceleration and the distance between the players.
Reference: [274] <author> A. Ram, R. Arkin, G. Boone, and M. Pearce. </author> <title> Using genetic algorithms to learn reactive control parameters for autonomous robot navigation. Adaptive Behavior, </title> <type> 2(3), </type> <year> 1994. </year> <month> 247 </month>
Reference: [275] <author> A. Ram and J. Santamaria. </author> <title> Continuous case-based reasoning. </title> <booktitle> In Proceedings of the AAAI Workshop on Case-Based Reasoning, </booktitle> <year> 1993. </year>
Reference: [276] <author> A. Ram and J. Santamaria. </author> <title> Multistrategy learning in reactive control systems for autonomous robot navigation. </title> <journal> Informatica, </journal> <volume> 17(4) </volume> <pages> 347-369, </pages> <year> 1993. </year>
Reference: [277] <author> A. Ram and J. Santamaria. </author> <title> Introspective reasoning using meta-explanations for multistrategy learning. Machine Learning: A Multistrategy Approach, </title> <type> 4, </type> <year> 1994. </year>
Reference: [278] <author> C. Ramsey and J. Grefenstette. </author> <title> Case-based anytime learning. </title> <editor> In D. Aha, editor, </editor> <title> Case Based Reasoning: </title> <booktitle> Papers from the 1994 Workshop, </booktitle> <pages> pages 91-95, </pages> <address> Menlo Park, California, 1994. </address> <publisher> AAAI Press. </publisher>
Reference: [279] <author> C. Reynolds. </author> <title> Competition, coevolution, and the game of tag. </title> <editor> In R. Brooks and P. Maes, editors, </editor> <booktitle> Artificial Life IV, </booktitle> <pages> pages 59-69, </pages> <address> Cambridge, Massachusetts, 1994. </address> <publisher> MIT Press. </publisher>
Reference: [280] <author> M. </author> <title> Ring. Continual Learning in Reinforcement Environments. </title> <type> PhD thesis, </type> <institution> Depart ment of Computer Science, The University of Texas at Austin, </institution> <year> 1994. </year>
Reference: [281] <author> G. Ritter, H. Woodruff, S. Lowry, and T. Isenhour. </author> <title> An algorithm for a selective nearest neighbor decision rule. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 21(6) </volume> <pages> 665-669, </pages> <year> 1975. </year>
Reference-contexts: This is shown procedurally in Figure 5.3. Tomek [349] modified this approach by taking a sample (&gt; 1) of the data and classifying the sample with the remaining examples. Editing then proceeds using Wilson's approach. Specifically, Wilson's approach is modified as in Figure 5.4. Ritter et al. <ref> [281] </ref> described another editing method, which differs from Wilson in that points that are correctly classified are discarded. The Ritter method, which is similar to Hart's [164], basically keeps only points near the boundaries between classes, and eliminates examples that are in the midst of a homogeneous region.
Reference: [282] <author> E. Rodin, Y. Lirov, S. Mittnik, B. McElhaney, and L. Wilbur. </author> <title> Artificial intelligence in air combat games. Computers and Mathematics with Applications, </title> <address> 13(1-3):261-274, </address> <year> 1987. </year>
Reference: [283] <author> J. Rosenschein and M. Genesereth. </author> <title> Deals among rational agents. </title> <booktitle> In Proceedings of the 1985 International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 91-99, </pages> <year> 1985. </year>
Reference-contexts: This loop is repeated until the system either reaches a goal state or recognizes that it will never terminate. To date, research in multiple agent planning and control has been limited largely to the area of distributed artificial intelligence <ref> [138, 283, 329, 330, 331, 341, 356] </ref> and artificial 5 life [83, 116, 117, 172, 299, 325]. In distributed AI (DAI), several agents cooperate to achieve some goal or accomplish some task. The task is usually one of sufficient complexity that no single agent can accomplish the task alone. <p> The agents must then be constructed (or must learn) such that they cooperate with one another to perform the task [122, 139]. Work by Genesereth and Rosenschein considered cooperation among agents in which there are competing objectives and where there is no communication between the agents <ref> [138, 283] </ref>.
Reference: [284] <author> S. Rosenschein and L. Kaelbling. </author> <title> A situated view of representation and control. </title> <journal> Artificial Intelligence, </journal> <volume> 73, </volume> <year> 1995. </year>
Reference: [285] <author> C. Rosin and R. Belew. </author> <title> A competitive approach to game learning. </title> <booktitle> In Proceedings of the Ninth Annual ACM Conference on Computational Learning Theory, </booktitle> <year> 1996. </year>
Reference-contexts: Tesauro's work in which learning occurred during self-play. 86 with a 9 fi 9 board, the learning player was ability to match but not surpass a hand-crafted player. 3.4.3 Competitive Learning In an interesting alternative view to game learning, Rosin and Belew provide a general framework for metalearning in games <ref> [285] </ref>. Their approach assumes a player is learning to beat a fixed set of opponents and that each player is applying a different learning algorithm. In the simplest case, there are only two players, each with its own strategy-learning algorithm.
Reference: [286] <author> A. Roth and I. Erev. </author> <title> Learning in extensive-form games: Experimental data and simple dynamic models in the intermediate term. </title> <journal> Games and Economic Behavior, </journal> <volume> 8 </volume> <pages> 164-212, </pages> <year> 1995. </year>
Reference-contexts: In a normal form game, this is equivalent to filling out the entries in the payoff matrix. In an extensive form game, this could involve learning the heuristic evaluation function to be applied at the interior nodes of the game tree <ref> [123, 140, 155, 156, 157, 204, 258, 259, 260, 261, 286] </ref>. * Opponent capabilities are not known. In most studies in game theory, each player knows the permissible actions for itself and its opponent. In problems such as differential games, this may not be true. <p> This, of course, is the simplest form of multi-agent learning. 3.3.3 Reinforcement Learning in Cognitive Game Theory In the economics community, one of the communities responsible for considerable research in game theory <ref> [49, 123, 130, 196, 286, 358, 360] </ref>, limited work is being done in learning game strategies. This work, however, is largely restricted to single-player games. <p> This work, however, is largely restricted to single-player games. One notable exception is the work by Roth and Erev <ref> [123, 286] </ref> in what they call "cognitive game theory." Specifically, they distinguish between "low" game theory and "high" game theory, where low game theory assumes players have limited rationality but adapt to experience playing the game to derive rational strategies. <p> Given the "propensities," q ij , the probability of selecting strategy j (thus defining a mixed strategy) is p ij (s) = P m Roth and Erev studied the performance of this approach on three simple economic games 74 with pure strategy equilibria <ref> [286] </ref> and eleven simple economic games with mixed-strategy equilibria [123]. They found their simulated results tracked well with several experiments involving human subjects learning to play these same games. The intent of their experiments did not include developing an algorithm for learning the equilibria in the games.
Reference: [287] <author> S. Russel and D. Subramanian. </author> <title> Probably bounded-optimal agents. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 575-609, </pages> <year> 1995. </year>
Reference: [288] <author> S. Safra and M. Tennenholtz. </author> <title> On planning while learning. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 111-129, </pages> <year> 1994. </year> <month> 248 </month>
Reference-contexts: Finally, planning attempts to determine optimal strategies for an agent to apply in performing some task <ref> [74, 106, 107, 131, 288] </ref>. These strategies usually consist of sequences of steps to perform, and generally, the objective is to reach some terminal state (as opposed to maintaining an equilibrium as in control problems). Classic planning problems in artificial intelligence have included stacking blocks and navigating mazes.
Reference: [289] <author> M. Salganicoff. </author> <title> Density-adaptive learning and forgetting. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 276-283. </pages> <publisher> Morgan Kauf-mann, </publisher> <year> 1993. </year>
Reference: [290] <author> M. Salganicoff. </author> <title> Improved learning of time-varying mappings with performance error based forgetting. </title> <type> Technical Report MS-CIS-93-80, </type> <institution> University of Pennsylvania, </institution> <year> 1993. </year>
Reference: [291] <author> M. Salganicoff and L. Ungar. </author> <title> Active exploration and learning in real-valued spaces using multi-armed bandit allocation indices. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <year> 1995. </year>
Reference: [292] <author> S. Salzberg. </author> <title> Learning with Nested Generalized Exemplars. </title> <publisher> Kluwer Academic Pub lishers, Norwell, </publisher> <address> MA, </address> <year> 1990. </year>
Reference-contexts: d 1 and d 2 respectively for some 103 state-action pair in the game, then the stored state-action pair's Q value is updated. 4.3.2 1-NN for Evasive Maneuvers Memory-based learning is a classical approach to machine learning and pattern recognition, most commonly in the form of the 1-nearest neighbor algorithm <ref> [3, 10, 90, 119, 292, 293, 366] </ref>. 1-NN is rarely used for Markov decision problems, so we had to represent the pursuit game in a format amenable to this algorithm.
Reference: [293] <author> S. Salzberg. </author> <title> Distance metrics for instance-based learning. </title> <booktitle> In Methodologies for Intelligent Systems: 6th International Symposium, </booktitle> <pages> pages 399-408, </pages> <year> 1991. </year>
Reference-contexts: d 1 and d 2 respectively for some 103 state-action pair in the game, then the stored state-action pair's Q value is updated. 4.3.2 1-NN for Evasive Maneuvers Memory-based learning is a classical approach to machine learning and pattern recognition, most commonly in the form of the 1-nearest neighbor algorithm <ref> [3, 10, 90, 119, 292, 293, 366] </ref>. 1-NN is rarely used for Markov decision problems, so we had to represent the pursuit game in a format amenable to this algorithm. <p> This demonstrates that the two-pursuer problem is significantly more difficult for 1-NN. 115 116 One possible reason for 1-NN's poor performance on the two-pursuer task is presence of irrelevant attributes, which is known to cause problems for nearest neighbor algorithms <ref> [5, 293] </ref>. We experimented with a method similar to stepwise forward selection [112] to determine the set of relevant attributes. <p> Indeed, the additional state information for P is "irrelevant," and irrelevant attributes are known to degrade performance in instance-based and memory-based learning <ref> [5, 293] </ref>. 6.3.4 Pursuit with Limited Mobility The final game we studied with MBCL further extends the pursuit game by limiting the mobility of both players.
Reference: [294] <author> S. Salzberg. </author> <title> A nearest hyperrectangle learning method. </title> <journal> Machine Learning, </journal> <volume> 6:251 276, </volume> <year> 1991. </year>
Reference: [295] <author> S. Salzberg. </author> <title> Combining learning and search to create good classifiers. </title> <type> Technical Report JHU-92/12, </type> <institution> Johns Hopkins University, Baltimore MD, </institution> <year> 1992. </year>
Reference: [296] <author> S. Salzberg, A. Delcher, D. Heath, and S. Kasif. </author> <title> Learning with a helpful teacher. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 705-711, </pages> <address> Sydney, Australia, August 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Much work in the area of computational learning theory has focused on the problem of assessing the complexity of a learning task in terms of the samples required to learn that task (called the sample complexity). Salzberg et al. and Heath <ref> [296, 166] </ref> considered an alternative to the Probably Approximately Correct (PAC) learning model [353, 354] in which a Helpful Teacher is providing good examples to the learning. Their research focused on answering four questions: 1. What is the minimum number of examples needed to learn a concept? 2. <p> It might be possible with careful editing to reduce the size of memory even further. This question is related to the theoretical work by Salzberg and Heath <ref> [166, 296] </ref> that studies the question of how to find a minimal-size training set through the use of a Helpful Teacher, which explicitly provides good examples (see Section 5.2.5).
Reference: [297] <author> A. Samuel. </author> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 3(3) </volume> <pages> 211-229, </pages> <year> 1959. </year>
Reference-contexts: How many times did the computer win? How long did the game last? Was the computer, at least, an 2 interesting opponent to play? Until Arthur Samuel developed his checkers player <ref> [297, 298] </ref>, the thought of constructing a machine that could "learn" to play a game capable of competing with a human was just a dream. <p> In 1959 and 1967, Arthur Samuel reported on experiments he performed with a machine learning an evalua 69 tion function for board positions in the game of checkers <ref> [297, 298] </ref>. Samuel also reported on experiments in rote learning, but we limit the discussion to his novel reinforcement learning procedure. Early research in computer game playing focused on developing evaluation functions to be used in searching game trees. <p> They can also be applied to more traditional games with homogenous agents such as backgammon [342], checkers <ref> [297, 298] </ref>, and othello [323]. The strengths of the approach include the relative simplicity in storing examples and updating value estimates for game play. Unfortunately, the approach is both memory and computation intensive.
Reference: [298] <author> A. Samuel. </author> <title> Some studies in machine learning using the game of checkers II-recent progress. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 11(6) </volume> <pages> 601-617, </pages> <year> 1967. </year>
Reference-contexts: How many times did the computer win? How long did the game last? Was the computer, at least, an 2 interesting opponent to play? Until Arthur Samuel developed his checkers player <ref> [297, 298] </ref>, the thought of constructing a machine that could "learn" to play a game capable of competing with a human was just a dream. <p> In 1959 and 1967, Arthur Samuel reported on experiments he performed with a machine learning an evalua 69 tion function for board positions in the game of checkers <ref> [297, 298] </ref>. Samuel also reported on experiments in rote learning, but we limit the discussion to his novel reinforcement learning procedure. Early research in computer game playing focused on developing evaluation functions to be used in searching game trees. <p> They can also be applied to more traditional games with homogenous agents such as backgammon [342], checkers <ref> [297, 298] </ref>, and othello [323]. The strengths of the approach include the relative simplicity in storing examples and updating value estimates for game play. Unfortunately, the approach is both memory and computation intensive.
Reference: [299] <author> T. Sandholm and R. Crites. </author> <title> Multiagent reinforcement learning in the iterated prisoner's dilemma. </title> <journal> Biosystems, </journal> <volume> 37 </volume> <pages> 147-166, </pages> <year> 1995. </year>
Reference-contexts: To date, research in multiple agent planning and control has been limited largely to the area of distributed artificial intelligence [138, 283, 329, 330, 331, 341, 356] and artificial 5 life <ref> [83, 116, 117, 172, 299, 325] </ref>. In distributed AI (DAI), several agents cooperate to achieve some goal or accomplish some task. The task is usually one of sufficient complexity that no single agent can accomplish the task alone. <p> Some work has been done in evolutionary computation, artificial life, and iterated games. The most common game considered in this context is the iterated prisoner's dilemma <ref> [28, 223, 234, 299, 325] </ref>. The Prisoner's Dilemma is a two-player non-zero-sum game in which players must decide to cooperate or defect based on their expected payoffs. <p> The goal was not on learning strategies but on characterising choice and refusal in a competitive environment. Sandholm and Crites applied Q-learning in the iterated prisoner's dilemma in which multiple learners faced each other and a fixed player using "Tit-for-Tat" <ref> [299] </ref>. They explored methods involving lookup tables and recurrent networks. They found all learners fared well against the fixed player but had difficulty when playing against other learners (presumably because of the non-stationarity of the problem).
Reference: [300] <author> L. Saul and S. Singh. </author> <title> Markov decision processes in large state spaces. </title> <booktitle> In Proceed ings of COLT '95. </booktitle> <publisher> Morgan Kaufmann, Publishers, </publisher> <year> 1995. </year>
Reference: [301] <author> S. Schaal. </author> <title> Nonparametric regression for learning. </title> <booktitle> In Proceedings of the Conference on Prerational Intelligence-Adaptive and Learning Behavior, </booktitle> <year> 1994. </year>
Reference: [302] <author> A. Schaerf, Y. Shoham, and M. Tennenholtz. </author> <title> Adaptive load balancing: A study in multi-agent learning. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 475-500, </pages> <year> 1995. </year> <month> 249 </month>
Reference: [303] <author> J. Schmidhuber. </author> <title> A general method for incremental self-improvement and multi agent learning in unrestricted environments. Evolutionary Computation: </title> <journal> Theory and Applications, </journal> <note> 1996. To appear. </note>
Reference-contexts: Also, migration patterns of artificial birds have been evolved. In none of these cases has behavior of individual agents been the focus of the research. Recently, work has begun to appear that focuses on learning in multi-agent systems <ref> [56, 65, 150, 303, 328, 327, 329, 341] </ref>. Stone and Veloso provide a taxonomy of multi-agent systems by focusing on attributes such as agent homogeneity, communication, deliberative versus reactive control, and number of agents [329].
Reference: [304] <author> J. Sheppard and S. Salzberg. </author> <title> Memory-based learning of pursuit games. </title> <institution> Techni cal Report JHU-94-02, Department of Computer Science, Johns Hopkins University, Baltimore, Maryland, </institution> <month> January </month> <year> 1993. </year> <note> revised May, </note> <year> 1995. </year>
Reference-contexts: Results in Chapter 4 will appear in "A Teaching Method for Memory-Based Control" by J. Sheppard and S. Salzberg in the journal, Artificial Intelligence Review [307] and have appeared in "Memory-Based Learning of Pursuit Games" by J. Sheppard and S. Salzberg as Johns Hopkins technical report, JHU-94-02 <ref> [304] </ref>. The results from Chapter 5 appeared in "Bootstrapping Memory-Based Learning with Genetic 14 Algorithms" by J. Sheppard and S. Salzberg presented at the 1994 AAAI Workshop on Case-Based Reasoning [305], "Combining Memory Based Reasoning with Genetic Algorithms" by J. Sheppard and S.
Reference: [305] <author> J. Sheppard and S. Salzberg. </author> <title> Bootstrapping memory-based learning with genetic algorithms. In 1994 Workshop on Case Based Reasoning. </title> <publisher> AAAI, </publisher> <month> August </month> <year> 1994. </year>
Reference-contexts: Sheppard and S. Salzberg as Johns Hopkins technical report, JHU-94-02 [304]. The results from Chapter 5 appeared in "Bootstrapping Memory-Based Learning with Genetic 14 Algorithms" by J. Sheppard and S. Salzberg presented at the 1994 AAAI Workshop on Case-Based Reasoning <ref> [305] </ref>, "Combining Memory Based Reasoning with Genetic Algorithms" by J. Sheppard and S. Salzberg in Proceedings of the Sixth International Conference on Genetic Algorithms [306], and will appear in the previously referenced article in Artificial Intelligence Review.
Reference: [306] <author> J. Sheppard and S. Salzberg. </author> <title> Combining memory based reasoning with genetic algorithms. </title> <editor> In Larry J. Eshelman, editor, </editor> <booktitle> Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <pages> pages 452-459. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Sheppard and S. Salzberg presented at the 1994 AAAI Workshop on Case-Based Reasoning [305], "Combining Memory Based Reasoning with Genetic Algorithms" by J. Sheppard and S. Salzberg in Proceedings of the Sixth International Conference on Genetic Algorithms <ref> [306] </ref>, and will appear in the previously referenced article in Artificial Intelligence Review. This research has been supported, in part, by the National Science Foundation under Grant Nos.
Reference: [307] <author> J. Sheppard and S. Salzberg. </author> <title> A teaching method for memory-based control. </title> <journal> Artificial Intelligence Review, </journal> <note> 1997. To appear. </note>
Reference-contexts: Some of the work reported in this dissertation has been reported in preliminary form in papers that are cited here. Results in Chapter 4 will appear in "A Teaching Method for Memory-Based Control" by J. Sheppard and S. Salzberg in the journal, Artificial Intelligence Review <ref> [307] </ref> and have appeared in "Memory-Based Learning of Pursuit Games" by J. Sheppard and S. Salzberg as Johns Hopkins technical report, JHU-94-02 [304]. The results from Chapter 5 appeared in "Bootstrapping Memory-Based Learning with Genetic 14 Algorithms" by J. Sheppard and S.
Reference: [308] <author> T. Shibata, T. Fukada, and K. Tanie. </author> <title> Fuzzy critic for robotic motion planning by genetic algorithm in hierarchical intelligent control. </title> <booktitle> In Proceedings of the 1993 International Joint Conference on Neural Networks, Nagoya (Japan), </booktitle> <pages> pages 770-773. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference: [309] <author> T. Shibata, T. Fukada, and K. Tanie. </author> <title> Nonlinear backlash compensation using recurrent neural network unsupervised learning by genetic algorithm. </title> <booktitle> In Proceedings of the 1993 International Joint Conference on Neural Networks, Nagoya (Japan), </booktitle> <pages> pages 742-745. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference: [310] <author> T. Shibata, T. Fukada, and K. Tanie. </author> <title> Synthesis of fuzzy artificial intelligence, neural networks, and genetic algorithms for hierarchical intelligent control. </title> <booktitle> In Proceedings of the 1993 International Joint Conference on Neural Networks, Nagoya (Japan), </booktitle> <pages> pages 2869-2872. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference: [311] <author> J. Shinar and A. Davidovitz. </author> <title> A two-target game analysis in line-of-sight coordi nates. Computers and Mathematics with Applications, </title> <address> 13(1-3):123-140, </address> <year> 1987. </year>
Reference: [312] <author> J. Simons, H. van Brussel, J. DeSchutter, and J. Verhaert. </author> <title> A self-learning automaton with variable resolution for high precision assembly by industrial robots. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 27(5) </volume> <pages> 1109-1113, </pages> <month> October </month> <year> 1982. </year>
Reference-contexts: Several researchers have investigated approaches to varying the resolution of the memory base according to the requirements of the problem <ref> [104, 110, 240, 312] </ref>. For example, Moore and Atkeson's parti-game algorithm uses the concept of an adversary attempting to thwart search to determine how to explore the search space [240]. <p> Initial Q values did not matter since they would be learned over time. Nevertheless, a uniform random sampling of the space was, apparently, not sufficient to approximate some of the surfaces encountered in these games. Applications of variable resolution techniques <ref> [16, 110, 240, 312] </ref> may be more appropriate for problems such as these. In some ways, the results from MBCL are highly encouraging. They indicate co-learning can occur and suggest it is possible to learn optimal solutions to two-player differential games.
Reference: [313] <author> W. Simpson. </author> <title> The sensitivity of air combat maneuvering engagements to the initial conditions in the near-neighborhood of a neutral start. </title> <type> Technical Report 78-0276, </type> <institution> Center for Naval Analyses, Arlington, Virginia, </institution> <year> 1978. </year>
Reference: [314] <author> S. Singh. </author> <title> The efficient learning of multiple task sequences. </title> <booktitle> In Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 251-258, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 250 </pages>
Reference: [315] <author> S. Singh. </author> <title> Transfer of learning by composing solutions of elemental sequential tasks. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 323-340, </pages> <year> 1992. </year>
Reference: [316] <author> S. Singh. </author> <title> Learning to Solve Markovian Decision Processes. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <year> 1994. </year>
Reference-contexts: The approach falls in the class of iterative relaxation algorithms <ref> [316] </ref>.
Reference: [317] <author> S. Singh. </author> <title> Reinforcement learning algorithms for average-payoff markovian decision processes. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 700-706. </pages> <publisher> AAAI, </publisher> <year> 1994. </year>
Reference-contexts: The feedback about those decisions might be considerably delayed, and this delay makes learning much more difficult. In this case, the problem is called a delayed reinforcement problem <ref> [109, 167, 168, 220, 221, 317, 343, 363] </ref>. The basic loop followed in sequential decision making tasks such as these includes evaluating the current state, taking an action, and computing the new state.
Reference: [318] <author> S. Singh, T. Jaakola, and M. Jordan. </author> <title> Learning without state-estimation in partially observable markovian decision processes. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, Publishers, </publisher> <year> 1994. </year>
Reference-contexts: A key distinction between control systems and planning systems is the focus on achieving a specific goal; although, frequently the distinction between them becomes blurred [252]. 1.3 Learning and Intelligent Agents Recently, the machine learning community has paid increasing attention to problems of delayed reinforcement learning <ref> [37, 121, 179, 225, 228, 318, 368] </ref>. These problems usually involve an agent that has to make a sequence of decisions, or actions, in an environment that provides feedback about those decisions. The feedback about those decisions might be considerably delayed, and this delay makes learning much more difficult.
Reference: [319] <author> S. Singh, T. Jaakola, and M. Jordan. </author> <title> Reinforcement learning with soft state aggregation. </title> <booktitle> In Neural Information Processing Systems 7, </booktitle> <year> 1995. </year>
Reference: [320] <author> S. Singh and R. Sutton. </author> <title> Reinforcement learning with replacing eligibility traces. </title> <booktitle> Machine Learning, </booktitle> <year> 1996. </year> <note> To appear. </note>
Reference: [321] <author> D. Skalak. </author> <title> Prototype and feature selection by sampling and random mutation hill climbing algorithms. </title> <booktitle> In Proceedings of the Eleventh International Machine Learning Conference, </booktitle> <pages> pages 293-301, </pages> <address> New Brunswick, NJ, 1994. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Consequently, we decided to take this study one step further, and attempted to reduce the size of the memory store during the memory-based learning phase of GAMB <ref> [321, 377] </ref>. In the pattern recognition literature, e.g., in [98], algorithms for reducing memory size are known as editing methods. However, because memory-based learning is not usually applied to control tasks, we were not able to find any editing methods specifically tied to our type of problem.
Reference: [322] <author> J. Skowronski and R. </author> <title> Stonier. The barrier in a pursuit-evasion game with two targets. Computers and Mathematics with Applications, </title> <address> 13(1-3):37-45, </address> <year> 1987. </year>
Reference-contexts: Yavin describes a three-player game in which two agile players attempt to evade a single pursuer [376]. At the same time, the evaders want to shoot down the pursuer. This is also an example of the two-target game where a player has two competing objectives <ref> [141, 322] </ref>. Imado and Ishihara also consider the case where two missiles attempt to shoot down an airplane [175]. Lai and Tanaka consider general n-person games where each player attempts to force opponents into a terminal location in the playing field [200].
Reference: [323] <author> R. Smith and B. Gray. </author> <title> Co-adaptive genetic algorithms: An example in oth ello strategy. </title> <type> Technical Report TCGA Report No. 94002, </type> <institution> University of Alabama, Tuscaloosa, Alabama, </institution> <year> 1993. </year>
Reference-contexts: It is unclear how well this procedure would perform, given their simplifications, had mixed strategies been required. 3.3.6 Coevolution Methods Recently, work in co-evolutionary algorithms has begun to suggest approaches to multi-agent co-learning with some encouraging initial results <ref> [151, 152, 266, 323] </ref>. Extending his work on samuel, Grefenstette defines a uniform sensor architecture for multi-agent environments [151]. He claims that modeling information about all agents in the environment would be too complex, requiring significant computational resources. <p> Further, because they did not compare the results of coevolution to any static strategy, it was difficult to assess whether any improvement occurred at all. 83 Smith and Gray describe an alternative approach to coevolution, which they call a co-adaptive genetic algorithm, applied to the game of Othello <ref> [323] </ref>. Their approach focuses on developing a fitness function that is derived from the ability of a member of the population to compete against other members of the population. <p> They can also be applied to more traditional games with homogenous agents such as backgammon [342], checkers [297, 298], and othello <ref> [323] </ref>. The strengths of the approach include the relative simplicity in storing examples and updating value estimates for game play. Unfortunately, the approach is both memory and computation intensive.
Reference: [324] <author> C. Stanfill and D. Waltz. </author> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1213-1228, </pages> <year> 1986. </year>
Reference: [325] <author> E. Stanley, D. Ashlock, and L. Tesfatsion. </author> <title> Iterated prisoner's dilemma with choice and refusal of partners. </title> <booktitle> In Proceedings of ALife III. </booktitle> <address> Sante Fe Institute, </address> <year> 1993. </year>
Reference-contexts: To date, research in multiple agent planning and control has been limited largely to the area of distributed artificial intelligence [138, 283, 329, 330, 331, 341, 356] and artificial 5 life <ref> [83, 116, 117, 172, 299, 325] </ref>. In distributed AI (DAI), several agents cooperate to achieve some goal or accomplish some task. The task is usually one of sufficient complexity that no single agent can accomplish the task alone. <p> Some work has been done in evolutionary computation, artificial life, and iterated games. The most common game considered in this context is the iterated prisoner's dilemma <ref> [28, 223, 234, 299, 325] </ref>. The Prisoner's Dilemma is a two-player non-zero-sum game in which players must decide to cooperate or defect based on their expected payoffs. <p> The work by Stanley et al. focused on several agents with different strategies evolving chocie and refusal mechanisms to determine when a game was played <ref> [325] </ref>. The goal was not on learning strategies but on characterising choice and refusal in a competitive environment. Sandholm and Crites applied Q-learning in the iterated prisoner's dilemma in which multiple learners faced each other and a fixed player using "Tit-for-Tat" [299]. <p> These results (and the results of the next chapter) can be extended to alternating Markov games (in which players take turns) [213], team games (in which teams of players cooperate to devise mutual strategies) [337, 338], and community games (in which players choose opponents to maximize their personal payoff) <ref> [325] </ref>. They can also be applied to more traditional games with homogenous agents such as backgammon [342], checkers [297, 298], and othello [323]. The strengths of the approach include the relative simplicity in storing examples and updating value estimates for game play.
Reference: [326] <author> B. Steer and M. Larcombe. </author> <title> A goal seeking and obstacle avoiding algorithm for autonomous mobile robots. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 1518-1528. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference: [327] <author> P. Stone and M. Veloso. </author> <title> Beating a defender in robotic soccer: Memory-based learning of a continuous function. </title> <booktitle> In Proceedings of Neural Information Processing Systems, </booktitle> <year> 1995. </year> <month> 251 </month>
Reference-contexts: Also, migration patterns of artificial birds have been evolved. In none of these cases has behavior of individual agents been the focus of the research. Recently, work has begun to appear that focuses on learning in multi-agent systems <ref> [56, 65, 150, 303, 328, 327, 329, 341] </ref>. Stone and Veloso provide a taxonomy of multi-agent systems by focusing on attributes such as agent homogeneity, communication, deliberative versus reactive control, and number of agents [329].
Reference: [328] <author> P. Stone and M. Veloso. </author> <title> Broad learning from narrow training: A case study in robotic soccer. </title> <type> Technical Report CMU-CS-95-207, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <year> 1995. </year>
Reference-contexts: Also, migration patterns of artificial birds have been evolved. In none of these cases has behavior of individual agents been the focus of the research. Recently, work has begun to appear that focuses on learning in multi-agent systems <ref> [56, 65, 150, 303, 328, 327, 329, 341] </ref>. Stone and Veloso provide a taxonomy of multi-agent systems by focusing on attributes such as agent homogeneity, communication, deliberative versus reactive control, and number of agents [329].
Reference: [329] <author> P. Stone and M. Veloso. </author> <title> Multiagent systems: A survey from a machine learning perspective. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <note> 1996. submitted. </note>
Reference-contexts: This loop is repeated until the system either reaches a goal state or recognizes that it will never terminate. To date, research in multiple agent planning and control has been limited largely to the area of distributed artificial intelligence <ref> [138, 283, 329, 330, 331, 341, 356] </ref> and artificial 5 life [83, 116, 117, 172, 299, 325]. In distributed AI (DAI), several agents cooperate to achieve some goal or accomplish some task. The task is usually one of sufficient complexity that no single agent can accomplish the task alone. <p> Also, migration patterns of artificial birds have been evolved. In none of these cases has behavior of individual agents been the focus of the research. Recently, work has begun to appear that focuses on learning in multi-agent systems <ref> [56, 65, 150, 303, 328, 327, 329, 341] </ref>. Stone and Veloso provide a taxonomy of multi-agent systems by focusing on attributes such as agent homogeneity, communication, deliberative versus reactive control, and number of agents [329]. <p> Recently, work has begun to appear that focuses on learning in multi-agent systems [56, 65, 150, 303, 328, 327, 329, 341]. Stone and Veloso provide a taxonomy of multi-agent systems by focusing on attributes such as agent homogeneity, communication, deliberative versus reactive control, and number of agents <ref> [329] </ref>. Problems in multi-agent systems are distinct from problems in DAI and distributed computing, from which the field was derived, in that DAI and distributed computing focus on information processing and multi-agent systems focus on behavior development and behavior management. <p> In other words, each player has different objectives and capabilities and must learn appropriate strategies on their own. Thus they are heterogeneous agents <ref> [329] </ref>. Further, this game is more difficult than the game of force in that a separate action must be taken depending on the position of the opponent|no single fixed action applies.
Reference: [330] <author> P. Stone and M. Veloso. </author> <title> Towards collaborative and adversarial learning: A case study in robotic soccer. </title> <booktitle> In 1996 AAAI Spring Symposium on Adaptation, CoEvolution, and Learning in Multiagent Systems, </booktitle> <year> 1996. </year> <note> submitted. </note>
Reference-contexts: This loop is repeated until the system either reaches a goal state or recognizes that it will never terminate. To date, research in multiple agent planning and control has been limited largely to the area of distributed artificial intelligence <ref> [138, 283, 329, 330, 331, 341, 356] </ref> and artificial 5 life [83, 116, 117, 172, 299, 325]. In distributed AI (DAI), several agents cooperate to achieve some goal or accomplish some task. The task is usually one of sufficient complexity that no single agent can accomplish the task alone. <p> So far, most work in learning and multi-agent systems has focused on multiple agents' learning 6 complementary behaviors in a coordinated environment to accomplish some task, such as team game playing <ref> [330, 337, 338, 341] </ref>, combinatorial optimization [116, 117], and obstacle avoidance [150]. The research discussed in this dissertation combines work in control and planning in the context of competitive multi-agent systems. In particular, we focus on exploring methods for the on-line learning of optimal strategies for playing differential games.
Reference: [331] <author> T. Suguwara and V. Lesser. </author> <title> On-line learning of coordination plans. </title> <type> Technical Report COINS TR 93-27, </type> <institution> University of Massachusetts, </institution> <year> 1993. </year>
Reference-contexts: This loop is repeated until the system either reaches a goal state or recognizes that it will never terminate. To date, research in multiple agent planning and control has been limited largely to the area of distributed artificial intelligence <ref> [138, 283, 329, 330, 331, 341, 356] </ref> and artificial 5 life [83, 116, 117, 172, 299, 325]. In distributed AI (DAI), several agents cooperate to achieve some goal or accomplish some task. The task is usually one of sufficient complexity that no single agent can accomplish the task alone.
Reference: [332] <author> R. Sutton. </author> <title> Learning to predict by methods of temporal differences. </title> <journal> Machine Learn ing, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: At each step, an agent predicts what its future payoff will be, based on several available actions, and chooses its action based on the prediction. However, the ramifications for taking the sequence of actions are not revealed until (typically) the end of the process. According to Sutton <ref> [332] </ref>, the temporal difference method can be considered as an extension of the prototypical supervised learning rule based on gradient descent. <p> Sutton's temporal difference method permits incremental update and is based on the observations that z P t = k=t where P m+1 = z. In this case, the supervised learning rule becomes, w t = ff (P t+1 P t ) k=1 (see <ref> [332] </ref> for derivation). This update can be computed incrementally because it depends only on a pair of successive predictions (P t and P t+1 ), and on the sum of past values for r w P t . <p> Note that this approach assumes the backed-up score is more accurate than the current score, presumably because the backed-up scores are computed from states closer to the terminal board positions in the game. Note further that this is exactly the assumption made in Sutton's TD (0) algorithm <ref> [332] </ref>. In addition to updating coefficients on the terms, Samuel observed that the performance of the scoring polynomial depended on the current 16 terms selected from the 38 available terms. Samuel hand picked the 38 terms and (somewhat arbitrarily) decided the scoring polynomial would only use 16 of these terms. <p> TD methods usually assume that both the feature space and the variables being predicted are discrete <ref> [332, 342] </ref>. Q-learning typically represents a problem using a lookup table that contains all states, which naturally causes problems with large, continuous state spaces such as those encountered in differential games. We therefore had to develop a method for predicting the rewards for some state-action pairs without explicitly generating them.
Reference: [333] <author> R. Sutton. </author> <title> TD models: Modeling the world at a mixture of time scales. </title> <booktitle> In Pro ceedings of the Twelfth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kauf-mann, Publishers, </publisher> <year> 1995. </year>
Reference: [334] <author> R. Sutton. </author> <title> Generalization in reinforcement learning: Successful examples using sparse coarse coding. </title> <booktitle> In Advances in Neural Information Processing 8. </booktitle> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference: [335] <author> C. Swonger. </author> <title> Sample set condensation for a condensed nearest neighbor decision rule for pattern recognition. </title> <editor> In S. Watanabe, editor, </editor> <booktitle> Frontiers of Pattern Recognition, </booktitle> <pages> pages 511-519. </pages> <publisher> Academic Press, </publisher> <year> 1972. </year>
Reference: [336] <author> M. Tambe. </author> <title> Recursive agent and agent-group tracking in a real-time, dynamic envi ronment. In Adaptation, Coevolution, and learning in Multiagent Systems (ICMAS '95). </title> <publisher> AAAI Press, </publisher> <year> 1995. </year>
Reference-contexts: This led them to develop their M * algorithm in which a player learns its opponent's strategy and adapts accordingly 84 [68, 67, 69]. Others have provided similar strategies to modeling opponents, such as the agent tracking methods of Tambe <ref> [336, 337, 338, 339, 340] </ref>, and the equilibrium search methods of Goldman and Rosenschein [144, 243] and of Koller [194, 195, 196, 197, 359].
Reference: [337] <author> M. Tambe. </author> <title> Teamwork in real-world, dynamic environments. </title> <booktitle> In 1996 International Conference on Multiagent Systems. </booktitle> <publisher> AAAI Press, </publisher> <year> 1996. </year>
Reference-contexts: So far, most work in learning and multi-agent systems has focused on multiple agents' learning 6 complementary behaviors in a coordinated environment to accomplish some task, such as team game playing <ref> [330, 337, 338, 341] </ref>, combinatorial optimization [116, 117], and obstacle avoidance [150]. The research discussed in this dissertation combines work in control and planning in the context of competitive multi-agent systems. In particular, we focus on exploring methods for the on-line learning of optimal strategies for playing differential games. <p> This led them to develop their M * algorithm in which a player learns its opponent's strategy and adapts accordingly 84 [68, 67, 69]. Others have provided similar strategies to modeling opponents, such as the agent tracking methods of Tambe <ref> [336, 337, 338, 339, 340] </ref>, and the equilibrium search methods of Goldman and Rosenschein [144, 243] and of Koller [194, 195, 196, 197, 359]. <p> These results (and the results of the next chapter) can be extended to alternating Markov games (in which players take turns) [213], team games (in which teams of players cooperate to devise mutual strategies) <ref> [337, 338] </ref>, and community games (in which players choose opponents to maximize their personal payoff) [325]. They can also be applied to more traditional games with homogenous agents such as backgammon [342], checkers [297, 298], and othello [323].
Reference: [338] <author> M. Tambe. </author> <title> Tracking dynamic team activity. </title> <booktitle> In Proceedings of the 13th National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press, </publisher> <year> 1996. </year>
Reference-contexts: So far, most work in learning and multi-agent systems has focused on multiple agents' learning 6 complementary behaviors in a coordinated environment to accomplish some task, such as team game playing <ref> [330, 337, 338, 341] </ref>, combinatorial optimization [116, 117], and obstacle avoidance [150]. The research discussed in this dissertation combines work in control and planning in the context of competitive multi-agent systems. In particular, we focus on exploring methods for the on-line learning of optimal strategies for playing differential games. <p> This led them to develop their M * algorithm in which a player learns its opponent's strategy and adapts accordingly 84 [68, 67, 69]. Others have provided similar strategies to modeling opponents, such as the agent tracking methods of Tambe <ref> [336, 337, 338, 339, 340] </ref>, and the equilibrium search methods of Goldman and Rosenschein [144, 243] and of Koller [194, 195, 196, 197, 359]. <p> These results (and the results of the next chapter) can be extended to alternating Markov games (in which players take turns) [213], team games (in which teams of players cooperate to devise mutual strategies) <ref> [337, 338] </ref>, and community games (in which players choose opponents to maximize their personal payoff) [325]. They can also be applied to more traditional games with homogenous agents such as backgammon [342], checkers [297, 298], and othello [323].
Reference: [339] <author> M. Tambe, L. Johnson, and W. Shen. </author> <title> Adaptive agent tracking in real-world multi-agent domains: A preliminary report. </title> <journal> International Journal of Human Computer Studies, </journal> <note> 1996. To appear. </note>
Reference-contexts: This led them to develop their M * algorithm in which a player learns its opponent's strategy and adapts accordingly 84 [68, 67, 69]. Others have provided similar strategies to modeling opponents, such as the agent tracking methods of Tambe <ref> [336, 337, 338, 339, 340] </ref>, and the equilibrium search methods of Goldman and Rosenschein [144, 243] and of Koller [194, 195, 196, 197, 359].
Reference: [340] <author> M. Tambe and P. Rosenbloom. </author> <title> Architectures for agents that track other agents in multi-agent worlds. </title> <booktitle> Intelligent Agents: Lecture Notes in Artificial Intelligence, </booktitle> <address> II(1037), </address> <year> 1996. </year> <month> 252 </month>
Reference-contexts: This led them to develop their M * algorithm in which a player learns its opponent's strategy and adapts accordingly 84 [68, 67, 69]. Others have provided similar strategies to modeling opponents, such as the agent tracking methods of Tambe <ref> [336, 337, 338, 339, 340] </ref>, and the equilibrium search methods of Goldman and Rosenschein [144, 243] and of Koller [194, 195, 196, 197, 359].
Reference: [341] <author> M. Tan. </author> <title> Multi-agent reinforcement learning: independent vs. </title> <booktitle> cooperative agents. In Machine Learning: Proceedings of the Tenth International Conference, </booktitle> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This loop is repeated until the system either reaches a goal state or recognizes that it will never terminate. To date, research in multiple agent planning and control has been limited largely to the area of distributed artificial intelligence <ref> [138, 283, 329, 330, 331, 341, 356] </ref> and artificial 5 life [83, 116, 117, 172, 299, 325]. In distributed AI (DAI), several agents cooperate to achieve some goal or accomplish some task. The task is usually one of sufficient complexity that no single agent can accomplish the task alone. <p> Also, migration patterns of artificial birds have been evolved. In none of these cases has behavior of individual agents been the focus of the research. Recently, work has begun to appear that focuses on learning in multi-agent systems <ref> [56, 65, 150, 303, 328, 327, 329, 341] </ref>. Stone and Veloso provide a taxonomy of multi-agent systems by focusing on attributes such as agent homogeneity, communication, deliberative versus reactive control, and number of agents [329]. <p> So far, most work in learning and multi-agent systems has focused on multiple agents' learning 6 complementary behaviors in a coordinated environment to accomplish some task, such as team game playing <ref> [330, 337, 338, 341] </ref>, combinatorial optimization [116, 117], and obstacle avoidance [150]. The research discussed in this dissertation combines work in control and planning in the context of competitive multi-agent systems. In particular, we focus on exploring methods for the on-line learning of optimal strategies for playing differential games.
Reference: [342] <author> G. Tesauro. </author> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 257-277, </pages> <year> 1992. </year>
Reference-contexts: The primary difference is that Alpha and Beta do not 71 follow two independent paths as they learn|Beta's strategy comes from Alpha's experience. 3.3.2 Temporal Difference Methods and TD-Gammon More recently, Gerald Tesauro applied temporal difference learning in self-play in the game of backgammon <ref> [342, 343, 344] </ref>. As with Samuel's checkers player, Tesauro's program (named TD-Gammon) has the two players playing each other using an evaluation function resulting from learning. The approaches differ in that, for TD-Gammon, both players use the current evaluation function that has been learned. <p> Both lazy and eager approaches to reinforcement learning can be found in the literature. The most common eager approach is the use of temporal-difference learning on neural networks <ref> [36, 37, 81, 342] </ref>. The advantages to a lazy approach are three-fold. First, minimal computational time is required during training, because training consists primarily of storing examples (in the most traditional lazy approach, k-nearest neighbor). <p> TD methods usually assume that both the feature space and the variables being predicted are discrete <ref> [332, 342] </ref>. Q-learning typically represents a problem using a lookup table that contains all states, which naturally causes problems with large, continuous state spaces such as those encountered in differential games. We therefore had to develop a method for predicting the rewards for some state-action pairs without explicitly generating them. <p> They can also be applied to more traditional games with homogenous agents such as backgammon <ref> [342] </ref>, checkers [297, 298], and othello [323]. The strengths of the approach include the relative simplicity in storing examples and updating value estimates for game play. Unfortunately, the approach is both memory and computation intensive.
Reference: [343] <author> G. Tesauro. </author> <title> Temporal difference learning and TD-gammon. </title> <journal> Communications of the ACM, </journal> <pages> pages 58-67, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: The feedback about those decisions might be considerably delayed, and this delay makes learning much more difficult. In this case, the problem is called a delayed reinforcement problem <ref> [109, 167, 168, 220, 221, 317, 343, 363] </ref>. The basic loop followed in sequential decision making tasks such as these includes evaluating the current state, taking an action, and computing the new state. <p> The primary difference is that Alpha and Beta do not 71 follow two independent paths as they learn|Beta's strategy comes from Alpha's experience. 3.3.2 Temporal Difference Methods and TD-Gammon More recently, Gerald Tesauro applied temporal difference learning in self-play in the game of backgammon <ref> [342, 343, 344] </ref>. As with Samuel's checkers player, Tesauro's program (named TD-Gammon) has the two players playing each other using an evaluation function resulting from learning. The approaches differ in that, for TD-Gammon, both players use the current evaluation function that has been learned.
Reference: [344] <author> G. Tesauro and T. Sejnowski. </author> <title> A parallel network that learns to play backgam mon. </title> <journal> Artificial Intelligence, </journal> <volume> 39 </volume> <pages> 357-390, </pages> <year> 1989. </year>
Reference-contexts: The primary difference is that Alpha and Beta do not 71 follow two independent paths as they learn|Beta's strategy comes from Alpha's experience. 3.3.2 Temporal Difference Methods and TD-Gammon More recently, Gerald Tesauro applied temporal difference learning in self-play in the game of backgammon <ref> [342, 343, 344] </ref>. As with Samuel's checkers player, Tesauro's program (named TD-Gammon) has the two players playing each other using an evaluation function resulting from learning. The approaches differ in that, for TD-Gammon, both players use the current evaluation function that has been learned.
Reference: [345] <author> S. Thrun. </author> <title> Efficient exploration in reinforcement learning. </title> <type> Technical Report CMU CS-92-102, </type> <institution> Carnegie Mellon University, </institution> <year> 1992. </year>
Reference: [346] <author> S. Thrun and A. Schwartz. </author> <title> Issues in using function approximation in reinforce ment learning. </title> <booktitle> In Proceedings of the Fourth Connectionist Models Summer School. </booktitle> <publisher> Lawrence Erlbaum Publisher, </publisher> <year> 1993. </year>
Reference: [347] <author> S. Thrun and S. Schwartz. </author> <title> Finding structure in reinforcement learning. </title> <booktitle> In Ad vanced in Neural Information Processing Systems 7, </booktitle> <pages> pages 385-392. </pages> <publisher> The MIT Press, </publisher> <year> 1995. </year>
Reference: [348] <author> B. Tolwinski. </author> <title> Solving dynamic games via markov game approximations. </title> <editor> In R. Hamalainen and H. Ehtamo, editors, </editor> <booktitle> Differential Games|Developments in Modeling and Computation, </booktitle> <pages> pages 265-274. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference: [349] <author> I. Tomek. </author> <title> An experiment with the edited nearest-neighbor rule. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> SMC-6(6):448-452, </volume> <month> June </month> <year> 1976. </year>
Reference-contexts: Let class (i) return the class associated with instance i. Let class (k-NN (i)) return the class associated with the k nearest neighbors of instance i. If class (i) 6= class (k-NN (i)), then delete instance i from the database. This is shown procedurally in Figure 5.3. Tomek <ref> [349] </ref> modified this approach by taking a sample (&gt; 1) of the data and classifying the sample with the remaining examples. Editing then proceeds using Wilson's approach. Specifically, Wilson's approach is modified as in Figure 5.4.
Reference: [350] <author> J. Tsitsiklis and B. van Roy. </author> <title> An analysis of temporal difference learning with function approximation. </title> <type> Technical Report LIDS-P-2322, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1994. </year>
Reference: [351] <author> P. Turney. </author> <title> Theoretical analysis of cross-validation error and voting in instance based learning. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <note> 1993. submitted. </note>
Reference: [352] <author> P. Turney. </author> <title> A theory of cross-validation error. </title> <journal> Journal of Experimental and Theo retical Artificial Intelligence, </journal> <note> 1993. submitted. </note>
Reference: [353] <author> L. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142, </pages> <year> 1984. </year> <month> 253 </month>
Reference-contexts: Salzberg et al. and Heath [296, 166] considered an alternative to the Probably Approximately Correct (PAC) learning model <ref> [353, 354] </ref> in which a Helpful Teacher is providing good examples to the learning. Their research focused on answering four questions: 1. What is the minimum number of examples needed to learn a concept? 2. What is the minimal representation for a given concept? 3.
Reference: [354] <author> L. Valiant. </author> <title> Learning disjunctions of conjunctions. </title> <booktitle> In Proceedings of the Interna tional Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 560-566, </pages> <address> Los Altos, California, 1985. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Salzberg et al. and Heath [296, 166] considered an alternative to the Probably Approximately Correct (PAC) learning model <ref> [353, 354] </ref> in which a Helpful Teacher is providing good examples to the learning. Their research focused on answering four questions: 1. What is the minimum number of examples needed to learn a concept? 2. What is the minimal representation for a given concept? 3.
Reference: [355] <author> J. van der Wal. </author> <title> Stochastic Dynamic Programming. </title> <publisher> Morgan Kaufmann, </publisher> <address> Amster dam, </address> <year> 1981. </year>
Reference-contexts: The policy iteration is halted when there is no change in the value estimate, V t (s) for all s. 2.3 Markov Games The game theory literature refers to MDPs applied to games as Markov games <ref> [355] </ref>. A Markov game is an extension of the MDP in which decisions by multiple players must be considered, and these decisions generally conflict.
Reference: [356] <author> M. Veloso and P. Stone. FLECS: </author> <title> Planning with a flexible commitment strategy. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 3 </volume> <pages> 25-52, </pages> <year> 1995. </year>
Reference-contexts: This loop is repeated until the system either reaches a goal state or recognizes that it will never terminate. To date, research in multiple agent planning and control has been limited largely to the area of distributed artificial intelligence <ref> [138, 283, 329, 330, 331, 341, 356] </ref> and artificial 5 life [83, 116, 117, 172, 299, 325]. In distributed AI (DAI), several agents cooperate to achieve some goal or accomplish some task. The task is usually one of sufficient complexity that no single agent can accomplish the task alone.
Reference: [357] <author> D. Volper and S. Hampson. </author> <title> Learning and using specific instances. </title> <journal> Biological Cybernetics, </journal> <volume> 57 </volume> <pages> 57-71, </pages> <year> 1987. </year>
Reference: [358] <author> J. von Neumann and O. Morgenstern. </author> <title> Theory of Games and Economic Behav ior. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1947. </year>
Reference-contexts: This, of course, is the simplest form of multi-agent learning. 3.3.3 Reinforcement Learning in Cognitive Game Theory In the economics community, one of the communities responsible for considerable research in game theory <ref> [49, 123, 130, 196, 286, 358, 360] </ref>, limited work is being done in learning game strategies. This work, however, is largely restricted to single-player games.
Reference: [359] <author> B. von Stengel and D. Koller. </author> <title> Minmax equilibria in team games. Games and Economic Behavior, </title> <note> 1996. To appear. </note>
Reference-contexts: Others have provided similar strategies to modeling opponents, such as the agent tracking methods of Tambe [336, 337, 338, 339, 340], and the equilibrium search methods of Goldman and Rosenschein [144, 243] and of Koller <ref> [194, 195, 196, 197, 359] </ref>. For their approach, Carmel and Markovitch assume a two-player game where each player has an evaluation function to be used in a minimax search of the game tree.
Reference: [360] <author> N. Vorb'ev. </author> <title> Game Theory, Lectures for Economists and Systems Scientists. </title> <publisher> Springer-Verlag, </publisher> <year> 1977. </year>
Reference-contexts: From this, we can determine a Nash equilibrium point for any normal-form game. Theorem 2.1 (Nash) Given any n-player non-cooperative game, there exists at least one Nash equilibrium point consisting of mixed strategies. Proof: See <ref> [250, 254, 360] </ref>. If we reconsider the concept of a normal form game and limit our discussion to two-person zero-sum games, it has been shown that Nash equilibrium points with mixed strategies can be found by solving the following linear program. <p> This, of course, is the simplest form of multi-agent learning. 3.3.3 Reinforcement Learning in Cognitive Game Theory In the economics community, one of the communities responsible for considerable research in game theory <ref> [49, 123, 130, 196, 286, 358, 360] </ref>, limited work is being done in learning game strategies. This work, however, is largely restricted to single-player games.
Reference: [361] <author> T. Wagner. </author> <title> Convergence of the edited nearest neighbor. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 19 </volume> <pages> 696-697, </pages> <month> September </month> <year> 1973. </year> <title> Correspondence. </title>
Reference: [362] <author> J Wang. </author> <title> The Theory of Games. </title> <publisher> Oxford Science Publications, </publisher> <year> 1988. </year>
Reference: [363] <author> C. Watkins. </author> <title> Learning with Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University, Department of Computer Science, </institution> <address> Cambridge, England, </address> <year> 1989. </year>
Reference-contexts: The feedback about those decisions might be considerably delayed, and this delay makes learning much more difficult. In this case, the problem is called a delayed reinforcement problem <ref> [109, 167, 168, 220, 221, 317, 343, 363] </ref>. The basic loop followed in sequential decision making tasks such as these includes evaluating the current state, taking an action, and computing the new state. <p> Value iteration is a form of dynamic programming and can be shown to be a special case of iterative relaxation, in particular, when = 1 and ^ V (s) = Q V (s; (s)). A more generally used form of value iteration in machine learning is Q-learning <ref> [363] </ref> which has the form Q t (s; a) = (1 )Q t1 (s; a) + [c (s; a) + flQ t1 (s 0 ; (s 0 ))] where s 0 is the state transitioned to when applying action a in state s and c (s; a) is the cost of <p> In many control tasks, such as the differential games studies in this dissertation, such knowledge may not be available. A conceptually simple approach to solving MDPs with incomplete knowledge was proposed by Watkins in 1989 <ref> [363, 364] </ref> called Q-learning. As with traditional value iteration methods, Q-learning can be performed both off line and on line. In Q-learning, the controller maintains estimates of the optimal Q values for each admissible state-action pair. <p> Typically, 0 ff t (s; a) &lt; 1 and ff t (s; a) decreases over time. Specifically, ff t (s; a) is changed only when action a is applied in state s. For convergence, the schedule for changing ff t (s; a) must conform to the following requirements <ref> [363, 364] </ref>. <p> We then tried a traditional memory-based learning approach, nearest neighbor. Finally, we experimented with an eager learning method, genetic algorithms, to compare with the two memory-based methods. 4.3.1 Q-Learning for Evasive Maneuvers Q-learning solves delayed reinforcement learning problems by using a temporal difference (TD) learning rule <ref> [363] </ref>. TD methods usually assume that both the feature space and the variables being predicted are discrete [332, 342]. Q-learning typically represents a problem using a lookup table that contains all states, which naturally causes problems with large, continuous state spaces such as those encountered in differential games.
Reference: [364] <author> C. Watkins and P. </author> <title> Dayan. </title> <journal> Q-learning. Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: In many control tasks, such as the differential games studies in this dissertation, such knowledge may not be available. A conceptually simple approach to solving MDPs with incomplete knowledge was proposed by Watkins in 1989 <ref> [363, 364] </ref> called Q-learning. As with traditional value iteration methods, Q-learning can be performed both off line and on line. In Q-learning, the controller maintains estimates of the optimal Q values for each admissible state-action pair. <p> Typically, 0 ff t (s; a) &lt; 1 and ff t (s; a) decreases over time. Specifically, ff t (s; a) is changed only when action a is applied in state s. For convergence, the schedule for changing ff t (s; a) must conform to the following requirements <ref> [363, 364] </ref>.
Reference: [365] <author> G. Werner and M. Dyer. </author> <title> Evolution of communication in artificial organisms. </title> <booktitle> In Artificial life II, </booktitle> <pages> pages 659-687. </pages> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: To promote distribution of skills among populations, Davidor [102] studied the effects of niching and speciation in 88 genetic algorithms, and Werner and Dyer focused on developing communication mechanisms between organisms through artificial evolution <ref> [365] </ref>. Some work has been done in evolutionary computation, artificial life, and iterated games. The most common game considered in this context is the iterated prisoner's dilemma [28, 223, 234, 299, 325].
Reference: [366] <author> D. Wettschereck. </author> <title> A Study of Distance-Based Machine Learning Algorithms. </title> <type> PhD thesis, </type> <institution> Oregon State University, </institution> <year> 1994. </year>
Reference-contexts: d 1 and d 2 respectively for some 103 state-action pair in the game, then the stored state-action pair's Q value is updated. 4.3.2 1-NN for Evasive Maneuvers Memory-based learning is a classical approach to machine learning and pattern recognition, most commonly in the form of the 1-nearest neighbor algorithm <ref> [3, 10, 90, 119, 292, 293, 366] </ref>. 1-NN is rarely used for Markov decision problems, so we had to represent the pursuit game in a format amenable to this algorithm.
Reference: [367] <author> D. Wettschereck, D. Aha, and T. Mohri. </author> <title> A review and comparative evalua tion of feature weighting methods for lazy learning algorithms. </title> <journal> Artificial Intelligence Review, </journal> <note> 1996. To appear. </note>
Reference-contexts: Rather than focusing on the attributes themselves (for these studies), they attempted to retain examples with noise and discard examples without noise. In later studies, they provided several approaches to weighting the features as well <ref> [367] </ref>. Instance averaging algorithms provide techniques for replacing actual instances with "prototypical" instances derived from the data. In this approach, an initial instance for a concept receives a weight of 1.0.
Reference: [368] <author> S. Whitehead. </author> <title> Reinforcement Learning for the Adaptive Control of Perception and Action. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Rochester, </institution> <year> 1992. </year> <month> 254 </month>
Reference-contexts: A key distinction between control systems and planning systems is the focus on achieving a specific goal; although, frequently the distinction between them becomes blurred [252]. 1.3 Learning and Intelligent Agents Recently, the machine learning community has paid increasing attention to problems of delayed reinforcement learning <ref> [37, 121, 179, 225, 228, 318, 368] </ref>. These problems usually involve an agent that has to make a sequence of decisions, or actions, in an environment that provides feedback about those decisions. The feedback about those decisions might be considerably delayed, and this delay makes learning much more difficult.
Reference: [369] <author> D. Whitley, S. Dominic, and R. Das. </author> <title> Genetic reinforcement learning with multi layer neural networks. </title> <editor> In R. Belew and L. Booker, editors, </editor> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference: [370] <author> B. Widrow. </author> <title> The original adaptive neural net broom-balancer. </title> <booktitle> In International Symposium on Circuits and Systems, </booktitle> <pages> pages 351-357, </pages> <year> 1987. </year>
Reference: [371] <author> R. Williams and L. Baird. </author> <title> Analysis of some incremental variants of policy it eration: First steps toward understanding actor-critic learning systems. </title> <type> Technical Report NU-CCS-93-11, </type> <institution> Northeastern University, </institution> <year> 1993. </year>
Reference: [372] <author> R. Williams and L. Baird. </author> <title> Tight performance bounds on greedy policies based on imperfect value functions. </title> <type> Technical Report NU-CCS-93-14, </type> <institution> Northeastern University, </institution> <year> 1993. </year>
Reference: [373] <author> D. Wilson. </author> <title> Asymptotic properties of nearest neighbor rules using edited data. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 2(3) </volume> <pages> 408-421, </pages> <month> July </month> <year> 1972. </year>
Reference-contexts: We therefore modified a known editing algorithm for our problem, and call the resulting system GAME (GA plus memory plus editing). 5.4.1 Editing Methods for Nearest Neighbor Early work by Wilson <ref> [373] </ref> showed that examples could be removed from a set used for classification, and suggested that simply editing would frequently improve classification accuracy (in the same way that pruning improves decision trees [235]). Wilson's algorithm classifies each example in a data set with its own k nearest neighbors.
Reference: [374] <author> M. Wooldridge and N. Jennings. </author> <title> Intelligent agents: </title> <journal> Theory and practice. Knowledge Engineering Review, </journal> <note> 1996. submitted. </note>
Reference: [375] <author> Y. Yavin. </author> <title> Pursuit-evasion differential games with deception or interrupted observa tion. Computers and Mathematics with Applications, </title> <address> 13(1-3):191-203, </address> <year> 1987. </year>
Reference-contexts: Corless et al. consider the case where state information is uncertain [88]. Chan and Ng consider partial observability in linear-quadratic games [72], and Yavin considers 39 the case where the process of observing the state is deceptive or is somehow interrupted <ref> [375] </ref>. 2.7 A Simple Differential Game To illustrate the problems associated with solving (and ultimately learning) differential games, we provide a simple example of players attempting to move an object in the plane. This game is described in detail in [205]. The playing field is shown in Figure 2.3.
Reference: [376] <author> Y. Yavin. </author> <title> A stochastic two-target pursuit-evasion differential game with three players moving in a plane. Computers and Mathematics with Applications, </title> <address> 13(1-3):141-149, </address> <year> 1987. </year>
Reference-contexts: Other work in differential game theory is extending the number of players. Yavin describes a three-player game in which two agile players attempt to evade a single pursuer <ref> [376] </ref>. At the same time, the evaders want to shoot down the pursuer. This is also an example of the two-target game where a player has two competing objectives [141, 322]. Imado and Ishihara also consider the case where two missiles attempt to shoot down an airplane [175].
Reference: [377] <author> J. Zhang. </author> <title> Selecting typical instances in instance-based learning. </title> <booktitle> In Proceedings of the Ninth International Machine Learning Conferences, </booktitle> <pages> pages 470-479, </pages> <address> Aberdeen, Scotland, 1992. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Consequently, we decided to take this study one step further, and attempted to reduce the size of the memory store during the memory-based learning phase of GAMB <ref> [321, 377] </ref>. In the pattern recognition literature, e.g., in [98], algorithms for reducing memory size are known as editing methods. However, because memory-based learning is not usually applied to control tasks, we were not able to find any editing methods specifically tied to our type of problem.
Reference: [378] <author> J. Zhao and J. Schmidhuber. </author> <title> Incremental self-improvement for life-time multi agent reinforcement learning. </title> <booktitle> In Proceedings of SAB '96, </booktitle> <year> 1996. </year> <note> To appear. 255 </note>
References-found: 378

