URL: http://www.cs.orst.edu:80/~tadepall/research/papers/h-learning-tr.ps
Refering-URL: http://www.cs.orst.edu:80/~tadepall/research/publications.html
Root-URL: 
Email: tadepall@research.cs.orst.edu okd@research.cs.orst.edu  
Title: H-learning: A Reinforcement Learning Method to Optimize Undiscounted Average Reward  
Author: Prasad Tadepalli and DoKyeong Ok 
Date: 12 May 1994  
Address: Corvallis, Oregon 97331-3202  
Affiliation: Computer Science Department Oregon State University  
Abstract: In this paper, we introduce a model-based reinforcement learning method called H-learning, which optimizes undiscounted average reward. We compare it with three other reinforcement learning methods in the domain of scheduling Automatic Guided Vehicles, transportation robots used in modern manufacturing plants and facilities. The four methods differ along two dimensions. They are either model-based or model-free, and optimize discounted total reward or undiscounted average reward. Our experimental results indicate that H-learning is more robust with respect to changes in the domain parameters, and in many cases, converges in fewer steps to better average reward per time step than all the other methods. An added advantage is that unlike the other methods it does not have any parameters to tune.
Abstract-found: 1
Intro-found: 1
Reference: [Barto et al. 1993] <author> Barto, A. G., Bradtke, S. J., and Singh, S. P. </author> <title> Learning to Act using Real-Time Dynamic Programming, </title> <note> submitted to AI Journal special issue on Computational Theories of Interaction and Agency, </note> <year> 1993. </year>
Reference-contexts: Since this is essentially a high-level task, we study it using a simulator. Most approaches to reinforcement learning, including Q-learning [Watkins and Dayan 92] and ARTDP <ref> [Barto et al. 1993] </ref>, optimize the total discounted reward the learner receives. In other words, a reward which is received after one time step is considered equivalent to a fraction of the same reward received immediately. <p> RL methods are divided into "model-free methods" that do not explicitly model the effects of actions, and "model-based methods" that learn and use action models simultaneously while learning optimal control <ref> [Barto et al. 1993] </ref>. Q-learning [Watkins and Dayan 92] and R-learning [Schwartz 93] are examples of model-free methods, while Adaptive Real-Time Dynamic Programming (ARTDP) is an example of a model-based method [Barto et al. 1993]. <p> model the effects of actions, and "model-based methods" that learn and use action models simultaneously while learning optimal control <ref> [Barto et al. 1993] </ref>. Q-learning [Watkins and Dayan 92] and R-learning [Schwartz 93] are examples of model-free methods, while Adaptive Real-Time Dynamic Programming (ARTDP) is an example of a model-based method [Barto et al. 1993]. Previously it has been found that while model-free methods have simpler and more efficient update procedures, model-based methods converge in fewer steps [Barto et al. 1993]. <p> Dayan 92] and R-learning [Schwartz 93] are examples of model-free methods, while Adaptive Real-Time Dynamic Programming (ARTDP) is an example of a model-based method <ref> [Barto et al. 1993] </ref>. Previously it has been found that while model-free methods have simpler and more efficient update procedures, model-based methods converge in fewer steps [Barto et al. 1993]. Unlike Q-learning and R-learning, and like ARTDP, H-learning is a model-based method, and is similar to the "Algorithm B" of Jalali and Ferguson [Jalali and Ferguson 89]. In this paper, we derive H-learning from the classical successive 2 approximation algorithm for optimizing undiscounted rewards [Bertsekas 87]. <p> A number of RL algorithms including Q-learning [Watkins and Dayan 92] and Adaptive RTDP <ref> [Barto et al. 1993] </ref> are designed to learn policies that maximize discounted total rewards. While discounting solves the problem of infinite totals, it is not clear that discounted totals is what we want to optimize. <p> state i is stored explicitly as an array U best (i). [Ok 94] describes a variant of H-learning where it is not explicitly represented, which has slightly better performance. 2.3 A summary of other Reinforcement Learning methods H-learning can be seen as a cross between R-learning [Schwartz 93] and ARTDP <ref> [Barto et al. 1993] </ref>. Like R-learning and unlike ARTDP, H-learning optimizes undiscounted average reward per step. Like ARTDP and unlike R-learning, H-learning is model-based. <p> R-learning updates the incrementally using + ff (r 0 U R (i) + U R (j) ) (3) ARTDP is similar to H-learning in that they both use models, but different in that it optimizes the discounted total reward f (i) for each state i <ref> [Barto et al. 1993] </ref>. Its update equation is given by u2U (i) n X p ij (u)f (j)g (4) Watkins's Q-learning differs from H-learning in both the above dimensions. Unlike H-learning, it is both discounted and model-free. <p> Since ARTDP uses action models to propagate more information in each step than Q-learning, it is shown to converge in fewer steps <ref> [Barto et al. 1993] </ref>. We will see that the same relationship holds between H-learning and R-learning. All the above RL methods, except H-learning, have one or more parameters. ARTDP has fl, Q-learning has fl and fi, and R-learning has ff and fi. <p> Our results are also consistent with the results of Barto et al. that showed that model-based methods converge in fewer steps than model-free methods <ref> [Barto et al. 1993] </ref>. However, unlike in the experiments of Barto et al., the updating cost for each step in H-learning was only 4.3 times higher than the corresponding model-free method, while the number of steps needed for convergence is at least an order of magnitude smaller.
Reference: [Bertsekas 87] <author> Bertsekas, D. P. </author> <title> Dynamic Programming: Deterministic and Stochastic Models, </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1987. </year>
Reference-contexts: Although optimizing undiscounted average rewards is a well-studied problem in dynamic programming literature <ref> [Bertsekas 87] </ref>, its adaptation to RL is fairly recent, Schwartz's R-learning being one of the best known examples [Schwartz 93]. <p> Unlike Q-learning and R-learning, and like ARTDP, H-learning is a model-based method, and is similar to the "Algorithm B" of Jalali and Ferguson [Jalali and Ferguson 89]. In this paper, we derive H-learning from the classical successive 2 approximation algorithm for optimizing undiscounted rewards <ref> [Bertsekas 87] </ref>. We evaluated H-learning in a simple version of AGV domain and compared its performance to ARTDP, Q-learning, and R-learning. <p> As t ! 1, * t (s) converges to a finite limit h (s): 1 Hence h (s) can be interpreted as the expected long-term differential reward (or eld-reward) for being in state s. The following theorem is proved in <ref> [Bertsekas 87] </ref>. Theorem 1 If a scalar and an n-dimensional vector h satisfy the recurrence relation h (i) = max fr (i; u) + j=1 then is the optimal average reward ( fl ), and fl attains the above maximum for each state i. <p> Setting h (1) = 0, h (2) = -101 satisfies the recurrence relations in (1). Bertsekas shows that the above recurrence relation can be solved by synchronous successive approximation of the h vector <ref> [Bertsekas 87] </ref>. H-learning is an on-line asynchronous version of this algorithm that also learns action models simultaneously as it learns the h values and controls the system using them (see Figure 2). It is also similar to the "Algorithm B" of Jalali and Ferguson [Jalali and Ferguson 89]. <p> It estimates the probabilities p ij (a) and rewards r (i; a) by straightforward counting, and makes the so called "certainty equivalence assumption" that the current estimates are the true values while updating the h values <ref> [Bertsekas 87] </ref>. It updates the h value of the current state i using equation (1). Unlike the Algorithm B, which estimates the parameters of its action models before estimating the optimal policy, H-learning estimates them simultaneously. <p> Unlike the Algorithm B, which estimates the parameters of its action models before estimating the optimal policy, H-learning estimates them simultaneously. Since it is 1 Strictly speaking, this is not always the case; but it does not seriously affect the argument. See <ref> [Bertsekas 87] </ref> for details. 5 not important to converge to a unique set of h values as long as the optimal policy is found, it does not use any reference state, also unlike the Algorithm B. 1.
Reference: [Jalali and Ferguson 89] <author> Jalali, A. and Ferguson, M. </author> <title> Computationally Efficient Adaptive Control Algorithms for Markov Chains. </title> <booktitle> In IEEE Proceedings of the 28'th Conference on Decision and Control, </booktitle> <address> Tampa, FL, </address> <year> 1989. </year>
Reference-contexts: Previously it has been found that while model-free methods have simpler and more efficient update procedures, model-based methods converge in fewer steps [Barto et al. 1993]. Unlike Q-learning and R-learning, and like ARTDP, H-learning is a model-based method, and is similar to the "Algorithm B" of Jalali and Ferguson <ref> [Jalali and Ferguson 89] </ref>. In this paper, we derive H-learning from the classical successive 2 approximation algorithm for optimizing undiscounted rewards [Bertsekas 87]. We evaluated H-learning in a simple version of AGV domain and compared its performance to ARTDP, Q-learning, and R-learning. <p> H-learning is an on-line asynchronous version of this algorithm that also learns action models simultaneously as it learns the h values and controls the system using them (see Figure 2). It is also similar to the "Algorithm B" of Jalali and Ferguson <ref> [Jalali and Ferguson 89] </ref>. It estimates the probabilities p ij (a) and rewards r (i; a) by straightforward counting, and makes the so called "certainty equivalence assumption" that the current estimates are the true values while updating the h values [Bertsekas 87].
Reference: [Kaelbling 90] <author> Kaelbling, L. P. </author> <title> Learning in Embedded Systems, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: In spite of this, many researchers used techniques developed for optimizing discounted rewards for such problems, and evaluated them with respect to cumulative or average rewards without discounting <ref> [Kaelbling 90, Lin 92, Mahadevan and Connell 91] </ref>.
Reference: [Lin 92] <author> Lin, L. J. </author> <title> Self-improving Reactive Agents based on Reinforcement Learning, Planning, and Teaching. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: In spite of this, many researchers used techniques developed for optimizing discounted rewards for such problems, and evaluated them with respect to cumulative or average rewards without discounting <ref> [Kaelbling 90, Lin 92, Mahadevan and Connell 91] </ref>.
Reference: [Mahadevan and Connell 91] <author> Mahadevan, S. and Connell, J. </author> <title> Automatic Programming of Behavior-based Robots Using Reinforcement Learning. </title> <booktitle> In Proceedings of AAAI-91, </booktitle> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1991. </year>
Reference-contexts: In spite of this, many researchers used techniques developed for optimizing discounted rewards for such problems, and evaluated them with respect to cumulative or average rewards without discounting <ref> [Kaelbling 90, Lin 92, Mahadevan and Connell 91] </ref>.
Reference: [Mahadevan 94] <author> Mahadevan, S. </author> <title> To Discount or Not To Discount in Reinforcement Learning: A Case Study Comparing R-learning and Q-Learning. </title> <booktitle> In Proceedings of International Machine Learning Conference, </booktitle> <address> New Brunswick, NJ, </address> <year> 1994. </year>
Reference-contexts: While testing a policy, the AGV was made to follow the learned policy without any random explorations, and the average reward per action was recorded. Since the trials consisted of a small number of outliers that distorted the means, following <ref> [Mahadevan 94] </ref>, we plotted the median rewards of 30 trials against the number of training actions. ARTDP was run with fl=0.9, 0.99, and 0.999. We denote these three versions with ARTDP 0:9 , ARTDP 0:99 , and ARTDP 0:999 . <p> This, however, could change in other cases and other domains. Recently, Mahadvan has shown that Q-learning works better than R-learning in his robot simulator domain, when compared with respect to R-learning's evaluation function <ref> [Mahadevan 94] </ref>. However, it appears that there is no conflict between the two evaluation functions in his domain. Our results also suggest that Q-learning converges faster than R-learning under such conflict-free conditions. It would be interesting to see the result of conflict cases in his robot simulator domain.
Reference: [Minoura et al. 1993] <author> Minoura, T., Choi, S., and Robinson, R. </author> <title> Structural Active Object Systems for Manufacturing Control. </title> <journal> Integrated Computer-Aided Engineering, </journal> <volume> 1(2), </volume> <pages> 121-136, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Automatic Guided Vehicles are robots which are used for routine transportation tasks in modern manufacturing plants, hospitals and office buildings <ref> [Minoura et al. 1993] </ref>. These robots, or AGVs as we shall call them henceforth, are more sophisticated than typical "industrial robots" used on the assembly lines, and yet, side-step many difficult AI problems such as general-purpose vision or autonomous navigation in unexplored territories.
Reference: [Schwartz 93] <author> Schwartz, A. </author> <title> A Reinforcement Learning Method for Maximizing Undiscounted Rewards. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: As pointed out by Schwartz, many real world domains which we would like to apply RL to do not have a natural interpretation or need for discounting <ref> [Schwartz 93] </ref>. In spite of this, many researchers used techniques developed for optimizing discounted rewards for such problems, and evaluated them with respect to cumulative or average rewards without discounting [Kaelbling 90, Lin 92, Mahadevan and Connell 91]. <p> Although optimizing undiscounted average rewards is a well-studied problem in dynamic programming literature [Bertsekas 87], its adaptation to RL is fairly recent, Schwartz's R-learning being one of the best known examples <ref> [Schwartz 93] </ref>. In this paper, we present a new RL method, called H-learning, that optimizes undiscounted average reward per step and compare it with three other previously published RL methods in the domain of AGV scheduling. <p> RL methods are divided into "model-free methods" that do not explicitly model the effects of actions, and "model-based methods" that learn and use action models simultaneously while learning optimal control [Barto et al. 1993]. Q-learning [Watkins and Dayan 92] and R-learning <ref> [Schwartz 93] </ref> are examples of model-free methods, while Adaptive Real-Time Dynamic Programming (ARTDP) is an example of a model-based method [Barto et al. 1993]. Previously it has been found that while model-free methods have simpler and more efficient update procedures, model-based methods converge in fewer steps [Barto et al. 1993]. <p> While discounting solves the problem of infinite totals, it is not clear that discounted totals is what we want to optimize. As Schwartz pointed out, even researchers who use learning methods that optimize discounted totals evaluate their systems using a different, but more natural, measure average reward per step <ref> [Schwartz 93] </ref>. <p> After convergence, the difference between these two must equal the difference between the eld-reward in state i and the expected value of the eld-reward of the state after executing u. This is also the recurrence relation used by Schwartz to derive R-learning <ref> [Schwartz 93] </ref>, except he used to denote the h values. Note that if there is one solution to equation (1), infinitely many solutions can be generated by increasing all the h values by a fixed amount. <p> Instead, we borrow the idea that Schwartz used to estimate the average reward <ref> [Schwartz 93] </ref>. From the recurrence relation (1), after convergence, for any best move u in any state i, = r (i; u) h (i) + P n j=1 p ij (u)h (j). <p> optimal actions in each state i is stored explicitly as an array U best (i). [Ok 94] describes a variant of H-learning where it is not explicitly represented, which has slightly better performance. 2.3 A summary of other Reinforcement Learning methods H-learning can be seen as a cross between R-learning <ref> [Schwartz 93] </ref> and ARTDP [Barto et al. 1993]. Like R-learning and unlike ARTDP, H-learning optimizes undiscounted average reward per step. Like ARTDP and unlike R-learning, H-learning is model-based.
Reference: [Singh 94] <author> Singh, S. P. </author> <title> Reinforcement Learning Algorithms for Average-Payoff Markovian Decision Processes. </title> <booktitle> In Proceedings of AAAI-94, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: Our results also suggest that Q-learning converges faster than R-learning under such conflict-free conditions. It would be interesting to see the result of conflict cases in his robot simulator domain. Singh derived some average-reward RL algorithms for policy evaluation and optimal control questions from the recurrence relation (1) <ref> [Singh 94] </ref>. All these algorithms are model-free, and hence different from H-learning. 15 Much remains to be done to scale H-learning to more realistic problem sizes. Our table-based approach to store the probability transition matrix and the H-values can be generalized by parameterization and function approximation.
Reference: [Ok 94] <author> Ok, D. </author> <title> A Comparative Study of Undiscounted and Discounted Reinforcement Learning Methods, </title> <type> Technical Report, </type> <institution> 94-30-3, Dept. of Computer Science, Oregon State University. </institution>
Reference-contexts: In this algorithm the set of optimal actions in each state i is stored explicitly as an array U best (i). <ref> [Ok 94] </ref> describes a variant of H-learning where it is not explicitly represented, which has slightly better performance. 2.3 A summary of other Reinforcement Learning methods H-learning can be seen as a cross between R-learning [Schwartz 93] and ARTDP [Barto et al. 1993]. <p> H-learning and ARTDP with high exploration rate find the optimal policy even though there is a conflict. But, they can not find the optimal policy using a lower exploration rate. The sensitivity of the performance of H-learning and ARTDP to exploration is also demonstrated in <ref> [Ok 94] </ref>. Q-learning can not find the optimal policy using any exploration rate. However, R-learning finds the optimal policy with any exploration rate. Less exploration does not effect R-learning as it does 14 the other learning methods. <p> This opens up many interesting issues for future research such as dealing with actions that span more than one unit of time (e.g., going to the end of the corridor), and approximation methods for functions over real-valued attributes. <ref> [Ok 94] </ref> contains some specific proposals to do generalization of the value function using nearest neighbor approach.
Reference: [Watkins and Dayan 92] <author> Watkins, C. J. C. H. and Dayan, P. </author> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: Since this is essentially a high-level task, we study it using a simulator. Most approaches to reinforcement learning, including Q-learning <ref> [Watkins and Dayan 92] </ref> and ARTDP [Barto et al. 1993], optimize the total discounted reward the learner receives. In other words, a reward which is received after one time step is considered equivalent to a fraction of the same reward received immediately. <p> RL methods are divided into "model-free methods" that do not explicitly model the effects of actions, and "model-based methods" that learn and use action models simultaneously while learning optimal control [Barto et al. 1993]. Q-learning <ref> [Watkins and Dayan 92] </ref> and R-learning [Schwartz 93] are examples of model-free methods, while Adaptive Real-Time Dynamic Programming (ARTDP) is an example of a model-based method [Barto et al. 1993]. <p> A number of RL algorithms including Q-learning <ref> [Watkins and Dayan 92] </ref> and Adaptive RTDP [Barto et al. 1993] are designed to learn policies that maximize discounted total rewards. While discounting solves the problem of infinite totals, it is not clear that discounted totals is what we want to optimize.
References-found: 12

