URL: ftp://cns.brown.edu/nin/papers/horn7.ps.Z
Refering-URL: http://www.math.tau.ac.il/~nin/research.html
Root-URL: 
Title: Optimal Ensemble Averaging of Neural Networks  
Author: Ury Naftaly Nathan Intrator David Horn Raymond and Beverly Sackler 
Note: Submitted to Network  
Date: September 1996  
Address: 69978, Israel  
Affiliation: Faculty of Exact Sciences Tel Aviv University, Tel Aviv  
Abstract: Based on an observation about the different effect of ensemble averaging on the bias and variance portion of the prediction error, we discuss training methodologies for ensembles of networks. We demonstrate the effect of variance reduction and present a method of extrapolation to the limit of an infinite ensemble. A significant reduction of variance is obtained by averaging just over initial conditions of the neural networks, without varying architectures or training sets. The minimum of the ensemble prediction error is reached later than that of a single network. In the vicinity of the minimum, the ensemble prediction error appears to be flatter than that of the single network, thus simplifying optimal stopping decision. The results are demonstrated on the sunspots data, where the predictions are among the best obtained, and on the 1993 energy prediction competition data-set B.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Weigend, A. S. and Gershenfeld, N. A. (editors), </author> <year> 1994. </year> <title> Time Series Prediction. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: 1 Introduction In recent years, the use of artificial neural networks (NN) for time series prediction has gained popularity and nowadays, NN can compete with the best time series methods <ref> [1] </ref>. In this paper we reexamine one of the major techniques for NN performance improvement ensemble averaging [2, 3]. We argue that it requires a special training methodology, and can be more effective when not combined with popular training constraints such as weight decay and early stopping 1 .
Reference: [2] <author> Lincoln, W.P. and Skrzypek, J. </author> <year> 1990. </year> <title> Synergy of clustering multiple back propagation networks. </title> <editor> In Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 650-657, </pages> <address> SanMateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1 Introduction In recent years, the use of artificial neural networks (NN) for time series prediction has gained popularity and nowadays, NN can compete with the best time series methods [1]. In this paper we reexamine one of the major techniques for NN performance improvement ensemble averaging <ref> [2, 3] </ref>. We argue that it requires a special training methodology, and can be more effective when not combined with popular training constraints such as weight decay and early stopping 1 . The theoretical setting of the method is provided by the bias/variance decomposition.
Reference: [3] <author> Wolpert, D. H. </author> <year> 1992. </year> <title> Stacked generalization. </title> <booktitle> Neural Networks 5 </booktitle> <pages> 241-259. </pages>
Reference-contexts: 1 Introduction In recent years, the use of artificial neural networks (NN) for time series prediction has gained popularity and nowadays, NN can compete with the best time series methods [1]. In this paper we reexamine one of the major techniques for NN performance improvement ensemble averaging <ref> [2, 3] </ref>. We argue that it requires a special training methodology, and can be more effective when not combined with popular training constraints such as weight decay and early stopping 1 . The theoretical setting of the method is provided by the bias/variance decomposition.
Reference: [4] <author> Nowlan, S. J. & Hinton, G. E. </author> <year> 1992. </year> <title> Simplifying neural networks by soft weight-sharing. </title> <journal> Neural Comp. </journal> <volume> 4, </volume> <pages> 473-493. </pages>
Reference-contexts: Results for the sunspots problem are presented in Section 5, where we also evaluate the combination of ensemble averaging with other popular techniques. Our results outperform the best published results <ref> [4] </ref>. Our method is further evaluated with a data set from the 1993 energy competition (Section 6). The extrapolation method works somewhat differently in this case, signifying the existence of correlations among networks with different initial conditions. <p> The data are plotted in Figure 1. These data have been extensively studied and have served as a benchmark in the statistical literature [7, 8, 9]. Following previous publications <ref> [8, 4, 10] </ref> we choose the training set to contain the period between 1701 and 1920, and the test-set to contain the years 1921 to 1955. <p> Possible overfitting was addressed by the use of weight decay [15]. Their best result was an ARV of 0.082 on the training set and 0.086 on the prediction set. Nowlan and Hinton <ref> [4] </ref> imposed a mixture of Gaussians prior on the weights which they called "Soft Weight Sharing" to get an ARV of 0.072 on the test set. Pi and Peterson [10] introduced the ffi-test which establishes the dependence of a sequence of numbers on previous element (s) of the sequence. <p> It is well known [18, 8] that a large network cannot generalize well. Therefore, it is advisable to use the smallest network that is able to fit the training data. Many algorithms were used to achieve this goal <ref> [18, 8, 4] </ref>. Here we use weight-decay [8] which lets the network itself decrease non-useful connections during training. The cost function is MSE + 2 X w 2 1 + w 2 (6) where the sum is over the network weights, and is the smoothing parameter to be adjusted.
Reference: [5] <author> Geman, S., Bienenstock, E., and Doursat, R. </author> <year> 1992. </year> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4(1) </volume> <pages> 1-58. </pages>
Reference-contexts: squared error of f is given by E D [(f D (x) E [yjx]) 2 ]; where E D represents expectation with respect to all possible training sets D of fixed size. 2 To further see the performance under MSE we decompose the error to bias and variance com- ponents <ref> [5] </ref> to get E D [(f D (x) E [yjx]) 2 ] = (E D [f D (x)] E [yjx]) 2 + E D [(f D (x) E D [f D (x)]) 2 ]: (1) The first RHS term is called the bias of the estimator and the second term is
Reference: [6] <author> Raviv, Y. and Intrator, N. </author> <year> 1996. </year> <title> Bootstraping with Noise: An Effective Regularization Technique. </title> <journal> Connection Science, </journal> <note> Special issue on combining estimators (To appear). 13 </note>
Reference-contexts: a particularly useful subset of the general set of all sources fl School of Physics and Astronomy, ury@tarazan.tau.ac.il y School of Mathematical Sciences, nin@math.tau.ac.il z School of Physics and Astronomy, horn@vm.tau.ac.il 1 Under a different setup (training with input noise) weight decay was found useful in conjunction with ensemble averaging <ref> [6] </ref> 1 of variance. We show that while the bias of the ensemble of networks with different initial con-ditions remains unchanged, the variance error decreases considerably. The theoretical background is presented in the next section. We then describe the sunspots data, on which our technique is demonstrated.
Reference: [7] <author> Priestley, M. B. </author> <year> 1981. </year> <title> Spectral Analysis and Time Series. </title> <publisher> Academic Press. </publisher>
Reference-contexts: The data are plotted in Figure 1. These data have been extensively studied and have served as a benchmark in the statistical literature <ref> [7, 8, 9] </ref>. Following previous publications [8, 4, 10] we choose the training set to contain the period between 1701 and 1920, and the test-set to contain the years 1921 to 1955.
Reference: [8] <author> Weigend, A. S., Huberman, B. A., and Rumelhart, D. </author> <year> 1990. </year> <title> Predicting the future: A connectionist approach. </title> <journal> Int. J. Neural Syst. </journal> <volume> 1, </volume> <pages> 193-209. </pages>
Reference-contexts: The data are plotted in Figure 1. These data have been extensively studied and have served as a benchmark in the statistical literature <ref> [7, 8, 9] </ref>. Following previous publications [8, 4, 10] we choose the training set to contain the period between 1701 and 1920, and the test-set to contain the years 1921 to 1955. <p> The data are plotted in Figure 1. These data have been extensively studied and have served as a benchmark in the statistical literature [7, 8, 9]. Following previous publications <ref> [8, 4, 10] </ref> we choose the training set to contain the period between 1701 and 1920, and the test-set to contain the years 1921 to 1955. <p> These data have been extensively studied and have served as a benchmark in the statistical literature [7, 8, 9]. Following previous publications [8, 4, 10] we choose the training set to contain the period between 1701 and 1920, and the test-set to contain the years 1921 to 1955. Following <ref> [8] </ref>, we calculate the prediction error according to the average relative variance (ARV) ARV = k2S (y k f (~x k )) P 2 (5) which is the MSE divided by the variance of the data set S. The denominator is 2 = 1535 for the training set. <p> The TAR estimator gave a training ARV of 0.097 and the same result for the prediction set. Weigend et al. <ref> [8] </ref> used a standard multilayer perceptron architecture with 12 input units, 8 sigmoidal hidden units and a linear output unit. Possible overfitting was addressed by the use of weight decay [15]. Their best result was an ARV of 0.082 on the training set and 0.086 on the prediction set. <p> The ARV obtained on the test-set was 0.073. 4 Ensemble Averaging over Initial Conditions In this section we will demonstrate the method we use for ensemble averaging over initial conditions by applying it to the sunspots problem. We use neural networks with 12 inputs (as in <ref> [8] </ref>). All nets have one sigmoidal hidden layer consisting of 4 units and a linear output. They are then enlarged to form recurrent networks (SRN) [16] in which the input layer is increased by adding to it the hidden layer of the previous point in the time series. <p> Learning rate: 0.001 SRN FF Ensemble 0.0706 0.0713 Single 0.0739 0.0764 Table 2: The prediction ARV of recurrent and non-recurrent networks trained with the same learning and prediction conditions. 5.3 Weight Decay. It is well known <ref> [18, 8] </ref> that a large network cannot generalize well. Therefore, it is advisable to use the smallest network that is able to fit the training data. Many algorithms were used to achieve this goal [18, 8, 4]. <p> It is well known [18, 8] that a large network cannot generalize well. Therefore, it is advisable to use the smallest network that is able to fit the training data. Many algorithms were used to achieve this goal <ref> [18, 8, 4] </ref>. Here we use weight-decay [8] which lets the network itself decrease non-useful connections during training. The cost function is MSE + 2 X w 2 1 + w 2 (6) where the sum is over the network weights, and is the smoothing parameter to be adjusted. <p> It is well known [18, 8] that a large network cannot generalize well. Therefore, it is advisable to use the smallest network that is able to fit the training data. Many algorithms were used to achieve this goal [18, 8, 4]. Here we use weight-decay <ref> [8] </ref> which lets the network itself decrease non-useful connections during training. The cost function is MSE + 2 X w 2 1 + w 2 (6) where the sum is over the network weights, and is the smoothing parameter to be adjusted. In [8] the authors use this cost function to <p> Here we use weight-decay <ref> [8] </ref> which lets the network itself decrease non-useful connections during training. The cost function is MSE + 2 X w 2 1 + w 2 (6) where the sum is over the network weights, and is the smoothing parameter to be adjusted. In [8] the authors use this cost function to predict the sunspots series. We repeat their experiment using a Q = 20 ensemble of (12,8,1) networks. We get the results presented in Table 4.
Reference: [9] <author> Morris, J. </author> <year> (1977). </year> <title> Forecasting the sunspot cycle. </title> <journal> J. of the Roy. Statist. Soc. Ser. A, </journal> <volume> 140, </volume> <pages> 437-447. </pages>
Reference-contexts: The data are plotted in Figure 1. These data have been extensively studied and have served as a benchmark in the statistical literature <ref> [7, 8, 9] </ref>. Following previous publications [8, 4, 10] we choose the training set to contain the period between 1701 and 1920, and the test-set to contain the years 1921 to 1955.
Reference: [10] <author> Pi, H., and Peterson, C., </author> <year> 1994. </year> <title> Finding the Embedding Dimension and Variable Dependencies in Time Series. </title> <booktitle> Neural Computation 6, </booktitle> <pages> 509-520. </pages>
Reference-contexts: The data are plotted in Figure 1. These data have been extensively studied and have served as a benchmark in the statistical literature [7, 8, 9]. Following previous publications <ref> [8, 4, 10] </ref> we choose the training set to contain the period between 1701 and 1920, and the test-set to contain the years 1921 to 1955. <p> Nowlan and Hinton [4] imposed a mixture of Gaussians prior on the weights which they called "Soft Weight Sharing" to get an ARV of 0.072 on the test set. Pi and Peterson <ref> [10] </ref> introduced the ffi-test which establishes the dependence of a sequence of numbers on previous element (s) of the sequence. They found that x t1 , x t2 , x t3 , x t4 , x t9 and x t10 are the most important variables in the sunspots series. <p> This sounds 9 Q = 1 and for Q ! 1. plausible because of the extra dependence of an SRN on the previous data point. 5.2 Selected Inputs Pi & Peterson <ref> [10] </ref> report very good results using a (6,8,1) network where the input vectors consist of the variables x t1 , x t2 , x t4 , x t9 and x t10 . Applying the same inputs to our paradigm, indeed improved the prediction results of the ensemble (Table 3). <p> Some decorrelation methods could turn out to be useful here. In the sunspots case, the Q dependence was as expected from statistical independence. Using the ffi-test of Pi and Peterson <ref> [10] </ref> together with our method, we obtained the best predictions yet for this problem.
Reference: [11] <author> Priestley, M. B. </author> <year> 1988. </year> <title> Non-linear and Non-stationary Time Series Analysis. </title> <publisher> Academic Press. </publisher>
Reference-contexts: The denominator is 2 = 1535 for the training set. The same value is used for the test set. 4 3.1 Previous Results In a survey of sunspots prediction models <ref> [11] </ref>, the Threshold Autoregressive model (TAR) of Tong and Lim [12, 13] was favored. The TAR model is a combination of two linear autoregressive models with an activity threshold above which one autoregression model is used and below which the other is used [14].
Reference: [12] <author> Tong, H. and Lim, K. S. </author> <year> 1980. </year> <title> Threshold autoregression, limit cycles and cyclical data. </title> <journal> Journal of the Royal Statistical Society, 42:245. </journal>
Reference-contexts: The denominator is 2 = 1535 for the training set. The same value is used for the test set. 4 3.1 Previous Results In a survey of sunspots prediction models [11], the Threshold Autoregressive model (TAR) of Tong and Lim <ref> [12, 13] </ref> was favored. The TAR model is a combination of two linear autoregressive models with an activity threshold above which one autoregression model is used and below which the other is used [14].
Reference: [13] <author> Tong, H. </author> <year> 1983. </year> <title> Threshold Models in Non-linear Time Series Analysis, </title> <booktitle> volume 21 of Lecture Notes in Statistics. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference-contexts: The denominator is 2 = 1535 for the training set. The same value is used for the test set. 4 3.1 Previous Results In a survey of sunspots prediction models [11], the Threshold Autoregressive model (TAR) of Tong and Lim <ref> [12, 13] </ref> was favored. The TAR model is a combination of two linear autoregressive models with an activity threshold above which one autoregression model is used and below which the other is used [14].
Reference: [14] <author> Tong, H. </author> <year> 1990. </year> <title> Non-linear Time Series: A Dynamical Systems Approach. </title> <publisher> Oxford University Press. </publisher>
Reference-contexts: The TAR model is a combination of two linear autoregressive models with an activity threshold above which one autoregression model is used and below which the other is used <ref> [14] </ref>. The TAR estimator gave a training ARV of 0.097 and the same result for the prediction set. Weigend et al. [8] used a standard multilayer perceptron architecture with 12 input units, 8 sigmoidal hidden units and a linear output unit.
Reference: [15] <author> Hinton, G. E. </author> <year> 1986. </year> <booktitle> Learning distributed representations of concepts.In Proceedings of the 8th Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp 1-12, </pages> <address> Hillsdale: </address> <publisher> Erlbaum </publisher>
Reference-contexts: Weigend et al. [8] used a standard multilayer perceptron architecture with 12 input units, 8 sigmoidal hidden units and a linear output unit. Possible overfitting was addressed by the use of weight decay <ref> [15] </ref>. Their best result was an ARV of 0.082 on the training set and 0.086 on the prediction set. Nowlan and Hinton [4] imposed a mixture of Gaussians prior on the weights which they called "Soft Weight Sharing" to get an ARV of 0.072 on the test set.
Reference: [16] <author> Elman J. L. and Zipser D. </author> <year> 1988. </year> <title> Learning the Hidden Structure of Speech. </title> <journal> Journal of Acoustical Society of America 83, </journal> <pages> pp 1615-1626. </pages>
Reference-contexts: We use neural networks with 12 inputs (as in [8]). All nets have one sigmoidal hidden layer consisting of 4 units and a linear output. They are then enlarged to form recurrent networks (SRN) <ref> [16] </ref> in which the input layer is increased by adding to it the hidden layer of the previous point in the time series. This favors ordered temporal application. The learning algorithm consists of back propagation applied to an error function which is the MSE of the training set. <p> Simple recurrent networks (SRN) <ref> [16] </ref> are a practical compromise between fully recurrent dynamics and computational overload, and are being widely used [18]. In such networks, the connections are mainly feed-forward, but include some feedback connections. The recurrency lets the network remember cues from the recent past, but does not appreciably complicate the training.
Reference: [17] <author> Elman, J. L. </author> <year> 1990. </year> <title> Finding Structure in Time. </title> <booktitle> Cognitive Science 14, </booktitle> <pages> pp. 179-211. </pages>
Reference-contexts: In such networks, the connections are mainly feed-forward, but include some feedback connections. The recurrency lets the network remember cues from the recent past, but does not appreciably complicate the training. We have already presented in the previous section the results of a simple recurrent network <ref> [17] </ref>. Here we compare it to a simple feed-forward network (Table 2). We see that recurrency helps lowering the ARV on both ensemble and single nets, but its influence on single nets is about four times stronger than on the ensemble.
Reference: [18] <author> Hertz, J., Krogh, A., and Palmer, R. G. </author> <year> 1991. </year> <title> Introduction to The Theory of Neural Computation. </title> <booktitle> Lecture Notes Volume I, </booktitle> <address> Santa Fe Institute. </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Simple recurrent networks (SRN) [16] are a practical compromise between fully recurrent dynamics and computational overload, and are being widely used <ref> [18] </ref>. In such networks, the connections are mainly feed-forward, but include some feedback connections. The recurrency lets the network remember cues from the recent past, but does not appreciably complicate the training. We have already presented in the previous section the results of a simple recurrent network [17]. <p> Learning rate: 0.001 SRN FF Ensemble 0.0706 0.0713 Single 0.0739 0.0764 Table 2: The prediction ARV of recurrent and non-recurrent networks trained with the same learning and prediction conditions. 5.3 Weight Decay. It is well known <ref> [18, 8] </ref> that a large network cannot generalize well. Therefore, it is advisable to use the smallest network that is able to fit the training data. Many algorithms were used to achieve this goal [18, 8, 4]. <p> It is well known [18, 8] that a large network cannot generalize well. Therefore, it is advisable to use the smallest network that is able to fit the training data. Many algorithms were used to achieve this goal <ref> [18, 8, 4] </ref>. Here we use weight-decay [8] which lets the network itself decrease non-useful connections during training. The cost function is MSE + 2 X w 2 1 + w 2 (6) where the sum is over the network weights, and is the smoothing parameter to be adjusted.

References-found: 18

