URL: ftp://ftp.cs.yale.edu/pub/mcdermott/papers/aips94.ps.gz
Refering-URL: http://www.cs.yale.edu/HTML/YALE/CS/HyPlans/mcdermott.html
Root-URL: http://www.cs.yale.edu
Email: beetz@cs.yale.edu, mcdermott@cs.yale.edu  
Title: Improving Robot Plans During Their Execution  
Author: Michael Beetz and Drew McDermott 
Address: P.O. Box 2158, Yale Station New Haven, CT 06520  
Affiliation: Yale University, Department of Computer Science  
Date: June 1994  
Note: To appear in: Proc. of 2nd Int. Conf. on AI Planning Systems Chicago, IL,  AAAI Press, Boston, MA  
Abstract: We describe how our planner, xfrm, carries out the process of anticipating and forestalling execution failures. xfrm is a planning system that is embedded in a simulated robot performing a varying set of complex tasks in a changing and partially unknown environment. xfrm revises plans controlling the robot while they are executed. Thus whenever the robot detects a contingency, xfrm projects the effects of the contingency on its plan and|if necessary|revises its plan in order to make it more robust. Using xfrm, the robot can perform its tasks almost as efficiently as it could using efficient default plans, but much more robustly. Revising default plans requires xfrm to reason about full-fledged robot plans and diagnose various kinds of plan failures that might be caused by imperfect sensing and effecting, incomplete and faulty world models, and exogenous events. To this end, xfrm reasons about the structure, function, and behavior of plans, and diagnoses projected plan failures by classifying them in a taxonomy of predefined failure models. Declarative commands for goals, perceptions, and beliefs make the structure of robot plans and the functions of subplans explicit and thereby provide xfrm with a (partial) model of its plan that is used to perform hierarchical model-based diagnosis. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Beetz, M., and McDermott, D. </author> <year> 1992. </year> <title> Declarative goals in reactive plans. </title> <editor> In Hendler, J., ed., </editor> <booktitle> AIPS-92, </booktitle> <pages> 3-12. </pages>
Reference-contexts: Besides constructs for reactive robot control, rpl offers declarative statements, which allow for the representation of goals, perceptual descriptions of objects, and beliefs as logical expressions <ref> (Beetz & McDermott 1992) </ref>: For instance, (ACHIEVE (LOC DES h0,10i)) specifies the task to get the object denoted by the designator DES to the location h0,10i. The perception statement (PERCEIVE D) gets a perceptual description D and returns designators for objects satisfying D.
Reference: <author> Dean, T.; Kaelbling, L.; Kirman, J.; and Nicholson, A. </author> <year> 1993. </year> <title> Planning with deadlines in stochastic domains. </title> <booktitle> In Proc. of AAAI-93, </booktitle> <pages> 574-579. </pages>
Reference: <author> Drummond, M., and Bresina, J. </author> <year> 1990. </year> <title> Anytime synthetic projection: Maximizing the probability of goal satisfaction. </title> <booktitle> In Proc. of AAAI-90, </booktitle> <pages> 138-144. </pages>
Reference-contexts: Other planning architectures that make plans more robust during their execution mainly address plan failures caused by nondeterministic outcomes of plan actions <ref> (Drummond & Bresina 1990, Dean et al. 1993) </ref>, or they plan for single, highly repetitive tasks (Lyons & Hendriks 1992). Acknowledgments. We would like to thank Sean Engelson, Sam Steel, Wenhong Zhu, and the reviewers for valuable comments on earlier versions of this paper.
Reference: <author> Hammond, K. J. </author> <year> 1989. </year> <title> Case-Based Planning. </title> <publisher> Academic Press, Inc. </publisher>
Reference-contexts: Secondly, xfrm has a taxonomy of failure models, defined as logical expressions, for which xfrm can check whether or not they are satisfied by the behavior of a projected task. xfrm, like Hacker (Sussman 1977), Gordius (Simmons 1992), and Chef <ref> (Hammond 1989) </ref>, is a 3 We haven't studied how xfrm might learn such a model from experience, or what the penalties are for not having learned it perfectly. transformational planner that diagnoses "bugs" or, in our case, plan failures, in order to revise plans appropriately. xfrm differs from other transformational planners
Reference: <author> Hamscher, W. C. </author> <year> 1991. </year> <title> Modeling digital circuits for troubleshooting. </title> <booktitle> Artificial Intelligence 51 </booktitle> <pages> 223-271. </pages>
Reference-contexts: Also, xfrm reasons about full-fledged robot plans and is able to diagnose a larger variety of failures. Gordius also applies model-based diagnosis methods by propagating causal constraints through a partially ordered set of plan steps. We use hierarchical model-based diagnosis with explicit failure models <ref> (Hamscher 1991) </ref> since the rpl plan steps and their effects are typically modified by parallel monitoring processes and the control structures the step occurs in.
Reference: <author> Hanks, S.; Pollack, M.; and Cohen, P. </author> <year> 1993. </year> <title> Benchmarks, test beds, controlled experimentation, and the design of agent architectures. </title> <journal> AI Magazine 14(4) </journal> <pages> 17-42. </pages>
Reference-contexts: These routines are stored as procedure definitions in xfrm's plan library. The entries in xfrm's plan library have been carefully designed to work for most situations encountered in our environment, and to be able to recover from common plan failures. A key constraint on 1 See <ref> (Hanks, Pollack, & Cohen 1993) </ref> for a discussion of the use and limitations of simulators for agent research. the design of plans is that they are restartable. As explained earlier, the agent will swap a new plan in whenever it thinks it has a better version.
Reference: <author> Lyons, D., and Hendriks, A. </author> <year> 1992. </year> <title> A practical approach to integrating reaction and deliberation. </title> <editor> In Hendler, J., ed., </editor> <booktitle> AIPS-92, </booktitle> <pages> 153-162. </pages>
Reference-contexts: Other planning architectures that make plans more robust during their execution mainly address plan failures caused by nondeterministic outcomes of plan actions (Drummond & Bresina 1990, Dean et al. 1993), or they plan for single, highly repetitive tasks <ref> (Lyons & Hendriks 1992) </ref>. Acknowledgments. We would like to thank Sean Engelson, Sam Steel, Wenhong Zhu, and the reviewers for valuable comments on earlier versions of this paper.
Reference: <author> McDermott, D. </author> <year> 1991. </year> <title> A reactive plan language. </title> <institution> Research Report YALEU/DCS/RR-864, Yale University, Department of Computer Science. </institution>
Reference-contexts: Thus control routines that are robust for our environment require parallelism, complex flow control, and the capability to store information. Rather than hiding this complexity from xfrm we encode the whole plan in rpl (Reactive Plan Language) <ref> (McDermott 1991) </ref>, a Lisp-like robot control language with conditionals, loops, program variables, processes, and subroutines. rpl also provides high-level constructs (interrupts, monitors) to synchronize parallel actions and make plans reactive and robust.
Reference: <author> McDermott, D. </author> <year> 1992. </year> <title> Transformational planning of reactive behavior. </title> <institution> Research Report YALEU/DCS/RR-941, Yale University, Department of Computer Science. </institution>
Reference-contexts: There is a broader class of "global transformations," which can affect the entire plan. An example would be dealing with the cleaning robot by constraining plan steps to precede or follow its projected actions. See <ref> (McDermott 1992) </ref> for examples of global transformations. Plan Representation in XFRM Currently, xfrm does errand planning in a simulated world of discrete locations. <p> Besides constructs for reactive robot control, rpl offers declarative statements, which allow for the representation of goals, perceptual descriptions of objects, and beliefs as logical expressions <ref> (Beetz & McDermott 1992) </ref>: For instance, (ACHIEVE (LOC DES h0,10i)) specifies the task to get the object denoted by the designator DES to the location h0,10i. The perception statement (PERCEIVE D) gets a perceptual description D and returns designators for objects satisfying D. <p> As explained earlier, the agent will swap a new plan in whenever it thinks it has a better version. Hence no plan's correctness should depend on saved state; starting over again should always work. For details, see <ref> (McDermott 1992) </ref>. Projected Execution Scenarios While the robot performs its tasks as specified by the default plan that is glued together from the entries in the plan library, xfrm projects this plan in order to generate and analyze possible execution scenarios for it. <p> A detailed description of the algorithms for projecting parallel rpl plans, the language for describing causal models of actions and events, and the mechanisms for reconstructing a rpl program's and robot's state after a projection can be found in <ref> (McDermott 1992) </ref>. <p> Therefore, xfrm diagnoses a failure by classifying it, using a taxonomy of failure models, and produces a detailed failure description, which contains pointers to appropriate plan revision methods and the subplans that have to be revised. (This idea is due to (Sussman 1977).) 2 See <ref> (McDermott 1992) </ref> for more on plan-generated failures that occur during projection. We have not yet devoted any special attention to the case of failures that occur during execution.
Reference: <author> McDermott, D. </author> <year> 1994. </year> <title> An algorithm for probabilistic, totally-ordered temporal projection. </title> <institution> Research Report YALEU/DCS/RR-1014, Yale University, Department of Computer Science. </institution>
Reference-contexts: It records how the world evolved in the projected execution scenario. Projection is probabilistic, so it is necessary to generate multiple scenarios to be sure that a moderately probable bug will manifest itself. (For details, see <ref> (McDermott 1994) </ref>.) xfrm diagnoses plan failures by analyzing projected execution scenarios. Diagnostic rules are written in xfrm-ml, a prolog-like language that comprises prolog primitives, a lisp interface, and a set of built-in predicates for temporal reasoning, as well as predicates on task networks and rpl plans.
Reference: <author> Simmons, R. G. </author> <year> 1992. </year> <title> The roles of associational and causal reasoning in problem solving. </title> <booktitle> Artificial Intelligence 53 </booktitle> <pages> 159-207. </pages>
Reference-contexts: Secondly, xfrm has a taxonomy of failure models, defined as logical expressions, for which xfrm can check whether or not they are satisfied by the behavior of a projected task. xfrm, like Hacker (Sussman 1977), Gordius <ref> (Simmons 1992) </ref>, and Chef (Hammond 1989), is a 3 We haven't studied how xfrm might learn such a model from experience, or what the penalties are for not having learned it perfectly. transformational planner that diagnoses "bugs" or, in our case, plan failures, in order to revise plans appropriately. xfrm differs
Reference: <author> Sussman, G. J. </author> <year> 1977. </year> <title> A Computer Model of Skill Acquisition, </title> <booktitle> volume 1 of Aritficial Intelligence Series. </booktitle> <address> New York, NY: </address> <publisher> American Elsevier. </publisher>
Reference-contexts: Therefore, xfrm diagnoses a failure by classifying it, using a taxonomy of failure models, and produces a detailed failure description, which contains pointers to appropriate plan revision methods and the subplans that have to be revised. (This idea is due to <ref> (Sussman 1977) </ref>.) 2 See (McDermott 1992) for more on plan-generated failures that occur during projection. We have not yet devoted any special attention to the case of failures that occur during execution. <p> Secondly, xfrm has a taxonomy of failure models, defined as logical expressions, for which xfrm can check whether or not they are satisfied by the behavior of a projected task. xfrm, like Hacker <ref> (Sussman 1977) </ref>, Gordius (Simmons 1992), and Chef (Hammond 1989), is a 3 We haven't studied how xfrm might learn such a model from experience, or what the penalties are for not having learned it perfectly. transformational planner that diagnoses "bugs" or, in our case, plan failures, in order to revise plans
References-found: 12

