URL: file://ftp.cs.utexas.edu/pub/mooney/papers/wolfie-submitted-97.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/cthomp/pubs.html
Root-URL: 
Email: cthomp@cs.utexas.edu, mooney@cs.utexas.edu  
Title: Semantic Lexicon Acquisition for Learning Parsers  
Author: Cynthia A. Thompson and Raymond J. Mooney 
Address: Austin, TX 78712  
Affiliation: Department of Computer Sciences University of Texas  
Abstract: This paper describes a system, Wolfie (WOrd Learning From Interpreted Examples), that learns a semantic lexicon from a corpus of sentences paired with representations of their meaning. The lexicon learned consists of words paired with representations of their meaning, and allows for both synonymy and polysemy. Wolfie is part of an integrated system that learns to parse novel sentences into their meaning representations. Experimental results are presented that demonstrate Wolfie's ability to learn useful lexicons for a realistic domain. The lexicons learned by Wolfie are also compared to those learned by another lexical acquisition system, that of [ Siskind, 1996 ] . 
Abstract-found: 1
Intro-found: 1
Reference: [ Beckwith et al., 1991 ] <author> Beckwith, R.; Fellbaum, C.; Gross, D.; and Miller, G. </author> <year> 1991. </year> <title> Wordnet: A lexical database organized on psycholinguistic principles. </title> <editor> In Zernik, U., editor 1991, </editor> <title> Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon. </title> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ. </address> <pages> 211-232. </pages>
Reference-contexts: Enhancing the search heuristic and expanding the search may improve the learned lexicons. Active learning, where the system chooses which examples can be most usefully annotated, will be examined. We also plan to investigate the use of background knowledge, such as WordNet <ref> [ Beckwith et al., 1991 ] </ref> , by adding to the heuristic a preference for matching a word to terms in the representation that are semantically related. Methods for handling noisy data are also needed. Additional comparisons using the current corpus are needed to establish statistical significance.
Reference: [ Borland International, 1988 ] <author> Borland International, </author> <year> 1988. </year> <title> Turbo Prolog 2.0 Reference Guide. </title> <booktitle> Borland International, </booktitle> <address> Scotts Valley, CA. </address>
Reference-contexts: The accuracy is the percentage of test sentences for which the correct answer to the query was produced. The horizontal line is the accuracy for a hand-built application, Geobase, supplied with a commercial Prolog system, Turbo Prolog 2:0 <ref> [ Borland International, 1988 ] </ref> . The results show that a lexicon learned by Wolfie with cross-validation considering all heuristic measures led to learned parsers that were slightly worse than parsers learned from the hand-built lexicon.
Reference: [ Brent, 1991 ] <author> Brent, M. </author> <year> 1991. </year> <title> Automatic acquisition of subcategorization frames from untagged text. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics. </booktitle> <pages> 209-214. </pages>
Reference-contexts: This paper describes a system, Wolfie (WOrd Learning From Interpreted Examples), that learns a semantic lexicon from input consisting of sentences paired with representations of their meanings. Although a few others <ref> [ Siskind, 1996; Hastings and Lytinen, 1994; Brent, 1991 ] </ref> have presented systems for lexical acquisition, this work is unique in combining several features. First, arbitrary amounts of both pol-ysemy and synonymy can be handled. <p> They assume access to an initial concept hierarchy, and show no experimental results. Many systems [ Fukumoto and Tsujii, 1995; Haruno, 1995; Johnston et al., 1995 ] focus only on the acquisition of verbs or nouns, rather than both. <ref> [ Manning, 1993; Brent, 1991 ] </ref> acquire subcategorization information for verbs. The most closely related work is that of Siskind. His system handles some situations that ours cannot. For example, it handles noise and referential uncertainty (multiple possible meanings for a sentence).
Reference: [ Copestake, 1995 ] <editor> Copestake, et. al. A. </editor> <year> 1995. </year> <title> Acquisition of lexical translation relations from MRDS. </title> <booktitle> Machine Translation 9. </booktitle>
Reference-contexts: The semantic lexicon, or the mapping from words to meanings, is one component that is typically difficult to construct and update, and changes from one domain to the next. Constructing a lexicon by hand is difficult and time consuming, as noted by <ref> [ Copestake, 1995 ] </ref> and [ Walker and Amsler, 1986 ] . Also, [ Johnston et al., 1995 ] discuss the need for systems that can learn the meanings of novel words.
Reference: [ Fukumoto and Tsujii, 1995 ] <author> Fukumoto, Fumiyo and Tsu-jii, </author> <title> Jun'ichi 1995. Representation and acquisition of verbal polysemy. </title> <booktitle> In Papers from the 1995 AAAI Symposium on the Representation and Acquisition of Lexical Knowledge: Polysemy, Ambiguity, and Generativity, </booktitle> <address> Stanford, CA. </address> <pages> 39-44. </pages>
Reference: [ Haruno, 1995 ] <author> Haruno, </author> <title> Masahiko 1995. A case frame learning method for Japanese polysemous verbs. </title> <booktitle> In Papers from the 1995 AAAI Symposium on the Representation and Acquisition of Lexical Knowledge: Polysemy, Ambiguity, and Generativity, </booktitle> <address> Stanford, CA. </address> <pages> 45-50. </pages>
Reference: [ Hastings and Lytinen, 1994 ] <author> Hastings, P. and Lytinen, </author> <title> Steven 1994. The ups and downs of lexical acquisition. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence. </booktitle> <pages> 754-759. </pages>
Reference-contexts: This paper describes a system, Wolfie (WOrd Learning From Interpreted Examples), that learns a semantic lexicon from input consisting of sentences paired with representations of their meanings. Although a few others <ref> [ Siskind, 1996; Hastings and Lytinen, 1994; Brent, 1991 ] </ref> have presented systems for lexical acquisition, this work is unique in combining several features. First, arbitrary amounts of both pol-ysemy and synonymy can be handled.
Reference: [ Johnston et al., 1995 ] <author> Johnston, M.; Boguraev, B.; and Pustejovsky, J. </author> <year> 1995. </year> <title> The acquisition and interpretation of complex nominals. </title> <booktitle> In Papers from the 1995 AAAI Symposium on the Representation and Acquisition of Lexical Knowledge: Polysemy, Ambiguity, and Generativity, </booktitle> <address> Stanford, CA. </address> <pages> 69-74. </pages>
Reference-contexts: Constructing a lexicon by hand is difficult and time consuming, as noted by [ Copestake, 1995 ] and [ Walker and Amsler, 1986 ] . Also, <ref> [ Johnston et al., 1995 ] </ref> discuss the need for systems that can learn the meanings of novel words. Therefore, automating the acquisition of the semantic lexicon is an important task in automating the development of NLP systems.
Reference: [ Kohavi and John, 1995 ] <author> Kohavi, R. and John, G. </author> <year> 1995. </year> <title> Automatic parameter selection by minimizing estimated error. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Lea rning, </booktitle> <address> Tahoe City, CA. </address> <pages> 304-312. </pages>
Reference-contexts: We used a weight of one for the generality measure of the heuristic function. To intelligently choose the weights of the first four parameters for the heuristic, we used 10-fold cross-validation <ref> [ Kohavi and John, 1995 ] </ref> . 15 different predefined weight sets were evaluated, in addition to four random weight sets 2 . To choose the best weight set, we measured the coverage of the learned lexicon for the held out training sentences, and the ambiguity, A, of the lexicon. <p> The weights of the other heuristics were again chosen by cross-validation. 2 We are currently working on the implementation of a version which uses a best first search for the weight sets, as in <ref> [ Kohavi and John, 1995 ] </ref> Lexicons System 125 225 Wolfie 91.7% 97.3% Wolfie-NoOrtho 83.2% 91.2% Siskind 82.6% 89.5% Table 1: Coverage Results learned by the two systems as background knowledge for Chill.
Reference: [ Manning, 1993 ] <author> Manning, Christopher D. </author> <year> 1993. </year> <title> Automatic acquisition of a large subcategorization dictionary from corpora. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Colum-bus, </address> <publisher> Ohio. </publisher> <pages> 235-242. </pages>
Reference-contexts: They assume access to an initial concept hierarchy, and show no experimental results. Many systems [ Fukumoto and Tsujii, 1995; Haruno, 1995; Johnston et al., 1995 ] focus only on the acquisition of verbs or nouns, rather than both. <ref> [ Manning, 1993; Brent, 1991 ] </ref> acquire subcategorization information for verbs. The most closely related work is that of Siskind. His system handles some situations that ours cannot. For example, it handles noise and referential uncertainty (multiple possible meanings for a sentence).
Reference: [ Pedersen and Chen, 1995 ] <author> Pedersen, Ted and Chen, </author> <month> Wei-dong </month> <year> 1995. </year> <title> Lexical acquisition via constraint solving. </title> <booktitle> In Papers from the 1995 AAAI Symposium on the Representation and Acquisition of Lexical Knowledge: Polysemy, Ambiguity, and Generativity, </booktitle> <address> Stanford, CA. </address> <pages> 118-122. </pages>
Reference-contexts: Related Work A method for acquiring syntactic and semantic features of an unknown word is described by <ref> [ Pedersen and Chen, 1995 ] </ref> . They assume access to an initial concept hierarchy, and show no experimental results.
Reference: [ Plotkin, 1970 ] <author> Plotkin, G. D. </author> <year> 1970. </year> <title> A note on inductive generalization. </title> <editor> In Meltzer, B. and Michie, D., editors 1970, </editor> <booktitle> Machine Intelligence (Vol. </booktitle> <volume> 5). </volume> <publisher> Elsevier North-Holland, </publisher> <address> New York. </address>
Reference-contexts: To find the common substructure between pairs of query representations, we use a method which is similar to that of finding Least General Generalizations (LGGs) of clauses <ref> [ Plotkin, 1970 ] </ref> . To summarize, the LGG of two clauses is the least general clause that subsumes both clauses. For example, given the queries from the previous page, the common substructure is state ( ). We now describe the algorithm in more detail.
Reference: [ Schank, 1975 ] <author> Schank, R. C. </author> <year> 1975. </year> <title> Conceptual Information Processing. </title> <publisher> North-Holland, Oxford. </publisher>
Reference-contexts: The Wolfie algorithm, outlined in Figure 1, has been implemented to handle sentences paired with two kinds of representations. First, it can handle sentences with a case-role semantic representation, such as conceptual dependency <ref> [ Schank, 1975 ] </ref> . For example, the sentence "The man ate the cheese" is represented by [ingest, agent: [person, sex:male, age:adult], patient: [food, type:cheese]]. The second representation handled is the logical query representation used in the geoquery domain, the one focussed on in the remainder of this paper.
Reference: [ Siskind, 1992 ] <author> Siskind, Jeffrey M. </author> <year> 1992. </year> <title> Naive Physics, Event Perception, Lexical Semantics and Language Acquisition. </title> <type> Ph.D. Dissertation, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <address> Cambridge, MA. </address>
Reference-contexts: An element of R can be fractured into all of its subcomponents, denoted p j , and the method for doing so depends upon R. <ref> [ Siskind, 1992 ] </ref> was the first to utilize this notion of fracturing within a lexical learning procedure. For each valid set of these subcomponents, we can build them back into a valid sentence meaning using a relation we will call compose.
Reference: [ Siskind, 1994 ] <author> Siskind, Jeffrey M. </author> <year> 1994. </year> <title> Lexical acquisition in the presence of noise and homonymy. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence. </booktitle> <pages> 760-766. </pages>
Reference-contexts: Results demonstrate that the final application system performs only slightly worse at accurately answering questions when using a learned lexicon compared to a correct hand-built one. The system is also compared to an alternative lexical acquisition system developed by <ref> [ Siskind, 1994; Siskind, 1996 ] </ref> , demonstrating superior performance on this task. Chill and the Geoquery Domain The output produced by Wolfie can be used to assist a larger language acquisition system; in particular, it is currently used as part of the input to Chill, a parser acquisition system.
Reference: [ Siskind, 1996 ] <author> Siskind, Jeffrey Mark 1996. </author> <title> A computational study of cross-situational techniques for learning word-to-meaning mappings. </title> <journal> Cognition 61(1) </journal> <pages> 39-91. </pages>
Reference-contexts: This paper describes a system, Wolfie (WOrd Learning From Interpreted Examples), that learns a semantic lexicon from input consisting of sentences paired with representations of their meanings. Although a few others <ref> [ Siskind, 1996; Hastings and Lytinen, 1994; Brent, 1991 ] </ref> have presented systems for lexical acquisition, this work is unique in combining several features. First, arbitrary amounts of both pol-ysemy and synonymy can be handled. <p> Results demonstrate that the final application system performs only slightly worse at accurately answering questions when using a learned lexicon compared to a correct hand-built one. The system is also compared to an alternative lexical acquisition system developed by <ref> [ Siskind, 1994; Siskind, 1996 ] </ref> , demonstrating superior performance on this task. Chill and the Geoquery Domain The output produced by Wolfie can be used to assist a larger language acquisition system; in particular, it is currently used as part of the input to Chill, a parser acquisition system. <p> In addition, the ability of the learned lexicons to cover the testing corpus was measured as described below. The corpus consists of 250 sentences. We compared our system to that of <ref> [ Siskind, 1996 ] </ref> . To use his system, the input had to be slightly modified. The representation largest (P,(capital (C),population (C,P))) was changed to ((largest2 population (and (capital1 capital) (population2 capital population)))). The numbers on predicate names are used to distinguish them from the variables.
Reference: [ Walker and Amsler, 1986 ] <author> Walker, D. and Amsler, R. </author> <year> 1986. </year> <title> The use of machine-readable dictionaries in sublanguage analysis. </title> <editor> In Grishman, R. and Kittredge, R., editors 1986, </editor> <title> Analyzing Language in Restricted Domains. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address> <pages> 69-83. </pages>
Reference-contexts: The semantic lexicon, or the mapping from words to meanings, is one component that is typically difficult to construct and update, and changes from one domain to the next. Constructing a lexicon by hand is difficult and time consuming, as noted by [ Copestake, 1995 ] and <ref> [ Walker and Amsler, 1986 ] </ref> . Also, [ Johnston et al., 1995 ] discuss the need for systems that can learn the meanings of novel words. Therefore, automating the acquisition of the semantic lexicon is an important task in automating the development of NLP systems.
Reference: [ Zelle and Mooney, 1996 ] <author> Zelle, J. M. and Mooney, R. J. </author> <year> 1996. </year> <title> Learning to parse database queries using inductive logic programming. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <address> Portland, OR. </address>
Reference-contexts: In this paper, we will limit our discussion of Chill to its ability to learn parsers that can map natural language database queries about geography directly into an executable Prolog query that answers the question <ref> [ Zelle and Mooney, 1996 ] </ref> . Following are two examples of sentences for this domain, the geoquery domain, paired with their corresponding Prolog query: What is the capital of the state with the biggest population? answer (C, (capital (S,C), largest (P, (state (S), population (S,P))))).
Reference: [ Zelle, 1995 ] <author> Zelle, J. M. </author> <year> 1995. </year> <title> Using Inductive Logic Programming to Automate the Construction of Natural Language Parsers. </title> <type> Ph.D. Dissertation, </type> <institution> University of Texas, Austin, TX. </institution> <note> Also appears as Artificial Intelligence Labo--ratory Technical Report AI 96-249. </note>
Reference-contexts: Although a few others [ Siskind, 1996; Hastings and Lytinen, 1994; Brent, 1991 ] have presented systems for lexical acquisition, this work is unique in combining several features. First, arbitrary amounts of both pol-ysemy and synonymy can be handled. Second, interaction with a system, Chill <ref> [ Zelle, 1995 ] </ref> , that learns to parse database queries directly into logical form is demonstrated. Third, it uses a fairly simple batch, greedy algorithm that is quite fast and accurate.
References-found: 19

