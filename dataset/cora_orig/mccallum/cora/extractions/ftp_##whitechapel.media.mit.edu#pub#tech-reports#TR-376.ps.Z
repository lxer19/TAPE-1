URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-376.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: pinhanez bobick@media.mit.edu  
Title: Approximate World Models: Incorporating Qualitative and Linguistic Information into Vision Systems  
Author: Claudio S. Pinhanez and Aaron F. Bobick 
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 376 To appear in AAAI'96 - Portland, Oregon, 1996 Abstract Approximate world models are coarse descriptions of the elements of a scene, and are intended to be used in the selection and control of vision routines in a vision system. In this paper we present a control architecture in which the approximate models represent the complex relationships among the objects in the world, allowing the vision routines to be situation or context specific. Moreover, because of their reduced accuracy requirements, approximate world models can employ qualitative information such as those provided by linguistic descriptions of the scene. The concept is demonstrated in the development of automatic cameras for a TV studio Smart-Cams. Results are shown where SmartCams use vision processing of real imagery and information written in the script of a TV show to achieve TV quality framing.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> James F. Allen. </author> <title> Towards a general theory of action an time. </title> <journal> Artificial Intelligence, </journal> <volume> 23 </volume> <pages> 123-154, </pages> <year> 1984. </year>
Reference-contexts: We are still debating the convenience of using Schank's primitives to describe every action. Also, action frames need to be augmented by incorporating at least visual elements, as in [5], and time references, possibly using Allen's interval algebra, <ref> [1] </ref>. ;; "chef wraps chicken with a plastic bag" (do (actor chef) (result (change (object chicken) (to (and (contained plastic-bag) (physical-contact plastic-bag)))))) ;; "chef pounds the chicken with a meat-mallet" (propel (actor chef) (object meat-mallet) (from (location ?not-in-contact)) (to (location ?-in-contact)) (result (change (object chicken) (from (flatness ?X)) (to (flatness (greater
Reference: [2] <author> Aaron Bobick and Claudio Pinhanez. </author> <title> Using approximate models as source of contextual information for vision processing. </title> <booktitle> In Proc. of the ICCV'95 Workshop on Context-Based Vision, </booktitle> <pages> pages 13-21, </pages> <address> Cambridge, Massachusetts, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Although we use the same keywords (IF, THEN), the implied control structure has no resemblance to a traditional rule-based system: there is no inference or chaining of results. More examples of applicability rules can be found in <ref> [2] </ref>. 4 A Working Example: SmartCams Our approach of using approximate world models is being developed in a system we are constructing for the control of TV cameras. <p> Other examples of applicability rules and results can be found in <ref> [2] </ref>. 5 Building and Maintaining Approximate World Models Having shown how the information contained in approximate world models can be exploited by a vision system performing tasks in a dynamic environment, a fundamental issue remains: how does one construct an initial model and then maintain such a model as time progresses <p> The two-dimensional motions of an object detected by each of the wide-angle cameras are integrated to determine the movement of the object in the 3-D world. More details can be found in <ref> [2] </ref>. Note that the use of an approximate world model may require additional sensing and computation which might not be performed to directly address current perceptual tasks: e.g. the position of the body of the chef is maintained even though the current task may only involve framing the hands.
Reference: [3] <author> Aaron F. Bobick and Robert C. Bolles. </author> <title> The representation space paradigm of concurrent evolving object descriptions. </title> <journal> IEEE PAMI, </journal> <volume> 14(2) </volume> <pages> 146-156, </pages> <month> Jan-uary </month> <year> 1992. </year>
Reference-contexts: And third, as we will demonstrate, reducing the accuracy requirements enables the use of qualitative information which might be available to the vision system. Coarse and/or hierarchical descriptions have been used before in computer vision <ref> [3, 6] </ref>. Particularly, Bobick and Bolles employed a multi-level representational system where different queries were answered by different representations of the same object.
Reference: [4] <author> B. A. Draper, R. T. Collins, J. Brolio, J. Griffith, A. R. Hanson, and E. M. Riseman. </author> <title> Tools and experiments in the knowledge-directed interpretation of road scenes. </title> <booktitle> In Proc. of the DARPA Image Understanding Work 7 shop, </booktitle> <pages> pages 178-193, </pages> <address> Los Angeles, California, </address> <month> Febru--ary </month> <year> 1987. </year>
Reference-contexts: For instance, in the case of "extract-narrowest-moving-blob", often the lack of actual object movement makes the routine return tiny, incorrectly positioned regions which are filtered out by the post-conditions. It is important to differentiate the concept of applicability rules from rule-based or expert-system approaches to computer vision <ref> [4, 15] </ref>. Although we use the same keywords (IF, THEN), the implied control structure has no resemblance to a traditional rule-based system: there is no inference or chaining of results.
Reference: [5] <author> Jugal Kumar Kalita. </author> <title> Natural Language Control of Animation of Task Performance in a Physical Domain. </title> <type> PhD thesis, </type> <institution> University of Pennsylvania, </institution> <address> Philadelphia, Pennsylvania, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: It is not the objective of this paper to argue about the different possible meanings of the term action. Here, actions refer to major segments of time which people usually describe by single action verbs as discussed by <ref> [8, 14, 5] </ref>. A change in the action normally alters substantially the relationships among subjects and objects. For example, we have a situation in the cooking show domain where the chef first talks to the camera, and then he picks up a bowl and starts mixing ingredients. <p> In the SmartCam domain the final objective is to employ TV scripts in the format they are normally written (see figure 4). 6.1 Representing Actions Many formalisms have been developed to represent action, some targeting linguistic concerns [10], computer graphics synthesis <ref> [5] </ref>, or computer vision recognition [11]. Currently we employ a simple representation based on Schank's conceptualizations as described in [10]. <p> We are still debating the convenience of using Schank's primitives to describe every action. Also, action frames need to be augmented by incorporating at least visual elements, as in <ref> [5] </ref>, and time references, possibly using Allen's interval algebra, [1]. ;; "chef wraps chicken with a plastic bag" (do (actor chef) (result (change (object chicken) (to (and (contained plastic-bag) (physical-contact plastic-bag)))))) ;; "chef pounds the chicken with a meat-mallet" (propel (actor chef) (object meat-mallet) (from (location ?not-in-contact)) (to (location ?-in-contact)) (result
Reference: [6] <author> David Marr and H. K. Nishihara. </author> <title> Representation and recognition of the spatial organization of three-dimensional shapes. </title> <journal> In Proc. R. Soc. Lond. B, </journal> <volume> volume 200, </volume> <pages> pages 269-294, </pages> <year> 1978. </year>
Reference-contexts: And third, as we will demonstrate, reducing the accuracy requirements enables the use of qualitative information which might be available to the vision system. Coarse and/or hierarchical descriptions have been used before in computer vision <ref> [3, 6] </ref>. Particularly, Bobick and Bolles employed a multi-level representational system where different queries were answered by different representations of the same object.
Reference: [7] <author> Hans-Hellmut Nagel. </author> <title> A vision of `vision and language' comprises action: An example from road traffic. </title> <journal> Artificial Intelligence Review, </journal> <volume> 8 </volume> <pages> 189-214, 1994-5. </pages>
Reference-contexts: However, in many situations the set of actions is severely restricted by the environment or by the task, as, for example, in the case of recognizing vehicle maneuvers in a gas-station as described in <ref> [7] </ref>. The SmartCam domain exemplifies another type of situation, one in which there is available a linguistic description of the sequence of actions to occur.
Reference: [8] <author> Darren Newtson, Gretchen Engquist, and Joyce Bois. </author> <title> The objective basis of behavior units. </title> <journal> Journal of Personality and Social Psychology, </journal> <volume> 35(12) </volume> <pages> 847-862, </pages> <month> De-cember </month> <year> 1977. </year>
Reference-contexts: It is not the objective of this paper to argue about the different possible meanings of the term action. Here, actions refer to major segments of time which people usually describe by single action verbs as discussed by <ref> [8, 14, 5] </ref>. A change in the action normally alters substantially the relationships among subjects and objects. For example, we have a situation in the cooking show domain where the chef first talks to the camera, and then he picks up a bowl and starts mixing ingredients.
Reference: [9] <author> Charles J. Rieger III. </author> <title> Conceptual memory and inference. </title> <booktitle> In Conceptual Information Processing, chapter 5, </booktitle> <pages> pages 157-288. </pages> <publisher> North-Holland, </publisher> <year> 1975. </year>
Reference-contexts: The system was designed to infer position and movement information about human beings' hands, and physical contact and proximity among objects. The inference system is based on Rieger's inference system for Schank's conceptualizations, <ref> [9] </ref>. The inferred action frames are sub-actions, or instrument actions of the actions from which they are produced. To guarantee termination in a fast time, the inference rules are applied in a pre-determined sequence, in a 1-pass algorithm.
Reference: [10] <author> Roger C. Schank. </author> <title> Conceptual dependency theory. </title> <booktitle> In Conceptual Information Processing, chapter 3, </booktitle> <pages> pages 22-82. </pages> <publisher> North-Holland, </publisher> <year> 1975. </year>
Reference-contexts: In the SmartCam domain the final objective is to employ TV scripts in the format they are normally written (see figure 4). 6.1 Representing Actions Many formalisms have been developed to represent action, some targeting linguistic concerns <ref> [10] </ref>, computer graphics synthesis [5], or computer vision recognition [11]. Currently we employ a simple representation based on Schank's conceptualizations as described in [10]. <p> TV scripts in the format they are normally written (see figure 4). 6.1 Representing Actions Many formalisms have been developed to represent action, some targeting linguistic concerns <ref> [10] </ref>, computer graphics synthesis [5], or computer vision recognition [11]. Currently we employ a simple representation based on Schank's conceptualizations as described in [10]. In spite of its weaknesses | see [16] | Schank's representation scheme is interesting for us because the reduced number of primitive actions helps the design of both the translation and the inference procedures.
Reference: [11] <author> Jeffrey Mark Siskind. </author> <title> Grounding language in perception. </title> <journal> Artificial Intelligence Review, </journal> <volume> 8 </volume> <pages> 371-391, 1994-5. </pages>
Reference-contexts: In the SmartCam domain the final objective is to employ TV scripts in the format they are normally written (see figure 4). 6.1 Representing Actions Many formalisms have been developed to represent action, some targeting linguistic concerns [10], computer graphics synthesis [5], or computer vision recognition <ref> [11] </ref>. Currently we employ a simple representation based on Schank's conceptualizations as described in [10]. In spite of its weaknesses | see [16] | Schank's representation scheme is interesting for us because the reduced number of primitive actions helps the design of both the translation and the inference procedures.
Reference: [12] <author> Thomas M. Strat and Martin A. Fischler. </author> <title> Context-based vision: Recognizing objects using information from both 2-d and 3-d imagery. </title> <journal> IEEE PAMI, </journal> <volume> 13(10) </volume> <pages> 1050-1065, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: 1 Introduction It has been argued | e.g. in <ref> [12] </ref> | that in any given situation most visual tasks can be performed by a relatively simple visual routine. For example: finding the ground reduces to finding a large (body-relative) horizontal plane if the observer is vertical and there are no other large horizontal planes. <p> Part of the novelty of our work is related to the use of the models in the dynamic selection of appropriate vision methods according to the world situation. And compared to other architectures for context-based vision systems, like Strat and Fischler's Condor system, <ref> [12] </ref>, approximate world models provide a much more clear distinction between the vision component and the 3-D world component. 1 model is used. It is interesting to situate our scheme in the ongoing debate about reconstructionist vs. purposive vision discussed in [13] and in the replies in the same issue.
Reference: [13] <author> Michael J. Tarr and Michael J. Black. </author> <title> A computational and evolutionary perspective of the role of representation in vision. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 60(1) </volume> <pages> 65-73, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: It is interesting to situate our scheme in the ongoing debate about reconstructionist vs. purposive vision discussed in <ref> [13] </ref> and in the replies in the same issue. Our proposal falls between the strictly reconstructionist and purely purposive strategies.
Reference: [14] <author> Robert Thibadeau. </author> <title> Artificial perception of actions. </title> <journal> Cognitive Science, </journal> <volume> 10 </volume> <pages> 117-149, </pages> <year> 1986. </year>
Reference-contexts: It is not the objective of this paper to argue about the different possible meanings of the term action. Here, actions refer to major segments of time which people usually describe by single action verbs as discussed by <ref> [8, 14, 5] </ref>. A change in the action normally alters substantially the relationships among subjects and objects. For example, we have a situation in the cooking show domain where the chef first talks to the camera, and then he picks up a bowl and starts mixing ingredients.
Reference: [15] <author> J. K. Tsotsos. </author> <title> Knowledge organization and its role in representation and interpretation of time-varying data: The ALVEN system. </title> <journal> Computational Intelligence, </journal> <volume> 1 </volume> <pages> 16-32, </pages> <year> 1985. </year>
Reference-contexts: For instance, in the case of "extract-narrowest-moving-blob", often the lack of actual object movement makes the routine return tiny, incorrectly positioned regions which are filtered out by the post-conditions. It is important to differentiate the concept of applicability rules from rule-based or expert-system approaches to computer vision <ref> [4, 15] </ref>. Although we use the same keywords (IF, THEN), the implied control structure has no resemblance to a traditional rule-based system: there is no inference or chaining of results.
Reference: [16] <author> W. Wilks. </author> <title> A preferential, pattern-seeking semantics for natural language inference. </title> <journal> Artificial Intelligence, </journal> <volume> 6(1) </volume> <pages> 53-74, </pages> <year> 1975. </year>
Reference-contexts: Currently we employ a simple representation based on Schank's conceptualizations as described in [10]. In spite of its weaknesses | see <ref> [16] </ref> | Schank's representation scheme is interesting for us because the reduced number of primitive actions helps the design of both the translation and the inference procedures.
References-found: 16

