URL: http://www.cs.berkeley.edu/~alanm/CP/bilas.tr-512-96.96.ps
Refering-URL: http://www.cs.berkeley.edu/~alanm/CP/bib.html
Root-URL: 
Email: fbilas,felteng@cs.princeton.edu  
Title: Fast RPC on the SHRIMP Virtual Memory Mapped Network Interface  
Author: Bilas and Edward W. Felten 
Address: Princeton NJ 08544 USA  
Affiliation: Department of Computer Science, Princeton University,  
Note: Angelos  
Abstract: Princeton University Technical Report TR-512-96 A shorter version of this report is submitted for publication. Abstract The emergence of new network interface technology is enabling new approaches to the development of communications software. This paper evaluates the SHRIMP virtual memory mapped network interface by using it to build two fast implementations of remote procedure call (RPC). Our first implementation, called vRPC, is fully compatible with the SunRPC standard. We change the RPC runtime library; the operating system kernel is unchanged, and only a minimal change was needed in the stub generator to create a new protocol identifier. Despite these restrictions, our vRPC implementation is several times faster than existing SunRPC implementations. A round-trip null RPC with no arguments and results under vRPC takes about 33 microseconds. Our second implementation, called ShrimpRPC, is not compatible with SunRPC but offers much better performance. ShrimpRPC specializes the stub generator and runtime library to take full advantage of SHRIMP's features. The result is a round-trip null RPC latency of 9.5 microseconds, which is about one microsecond above the hardware minimum. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. Bershad, T. Anderson, E. Lazowska, and H. Levy. </author> <title> User-level interprocess communication for shared memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems 9, </journal> <month> 2 (May </month> <year> 1991), </year> <pages> 175-198. </pages>
Reference-contexts: The stub generator and runtime library were designed with SHRIMP in mind, so we believe they come close to the best possible RPC performance on the SHRIMP hardware. Buffer Management The design of ShrimpRPC is similar to Bershad's URPC <ref> [1] </ref>; the main difference is that URPC runs on shared-memory machines while ShrimpRPC runs on the distributed-memory SHRIMP system. Each RPC binding consists of one receive buffer on each side (client and server) with bidirectional import-export mappings between them. <p> update (CAU) Write back (CAUWB) 12 Write through (CAUWT) Deliberate update (CDU) Write back (CDUWB) Write through (CDUWT) NoCopy (NC) Automatic update (NCAU) Write back (NCAUWB) Write through (NCAUWT) Deliberate update (NCDU) Write back (NCDUWB) Write through (NCDUWT) 9 Related Work Our approach is similar in some ways to URPC <ref> [1] </ref>, since both exploit user-level communication. URPC is built on top of a shared memory architecture while we use the distributed-memory SHRIMP architecture. Bershad's LRPC [2] tries to optimize the kernel path for same-machine RPC calls. Since we have eliminated the kernel entirely, LRPC does not apply to our situation.
Reference: [2] <author> B. Bershad, T. Anderson, E. Lazowska, H. Levy. </author> <title> Lightweight remote procedure call. </title> <journal> ACM Transactions on Computer Systems 8, </journal> <month> 1 (Feb. </month> <year> 1990), </year> <pages> 37-55. </pages>
Reference-contexts: 1 Introduction Much is known about how to optimize remote procedure call (RPC) mechanisms on traditional workstation networks <ref> [2, 14, 17, 21] </ref>. The main effort in previous work was to reduce or avoid copying, to make traps and context switches fast, and to take advantage of common-case behavior. The emergence of new multiprocessor network interfaces opens new possibilities for constructing network software. <p> URPC is built on top of a shared memory architecture while we use the distributed-memory SHRIMP architecture. Bershad's LRPC <ref> [2] </ref> tries to optimize the kernel path for same-machine RPC calls. Since we have eliminated the kernel entirely, LRPC does not apply to our situation. Thekkath and Levy [7] investigated the impact of recent improvements in network technology on communication software.
Reference: [3] <author> A. Birrell, and B. Nelson. </author> <title> Implementing remote procedure calls. </title> <journal> ACM Transactions on Computer Systems 2, </journal> <month> 1 (Feb. </month> <year> 1984), </year> <pages> 39-59. </pages>
Reference: [4] <author> M.A. Blumrich, C. Dubnicki, E.W. Felten, and Kai Li. </author> <title> Protected, User-level DMA for the SHRIMP Network Interface. </title> <booktitle> Proceedings of 2nd International Symposium on High-Performance Computer Architecture, </booktitle> <month> February, </month> <year> 1996, </year> <pages> pages 154-165. </pages>
Reference-contexts: These accesses specify the source address, destination address, and size of a transfer. The ordinary virtual memory protection mechanisms (MMU and page tables) are used to maintain protection <ref> [4] </ref>. VMMC guarantees the in-order, reliable delivery of all data transfers, provided that the ordinary, blocking version of the deliberate-update send operation is used. The ordering guarantees are a bit more complicated when the non-blocking deliberate-update send operation is used.
Reference: [5] <author> M.A. Blumrich, K. Li, R.D. Alpert, C. Dubnicki, E.W. Felten, and J. Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Mul-ticomputer. </title> <booktitle> Proceedings of 17th Annual Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1990, </year> <pages> pages 142-153. </pages>
Reference-contexts: The emergence of new multiprocessor network interfaces opens new possibilities for constructing network software. It is not always clear, however, which interface is most appropriate for which task. The SHRIMP project <ref> [5, 6, 9] </ref> at Princeton University supports user level communication between processes by mapping memory pages between virtual address spaces. This Virtual Memory Mapped network interface seems to have many advantages including flexible user-level communication, and very low overhead to initiate data transfer. <p> The key hardware component is the network interface board, which supports the virtual memory mapped communication (VMMC) model, to provide low-overhead, protected, user-level communication. For more details on the SHRIMP architecture the reader can consult <ref> [5, 6, 9] </ref>. VMMC is discussed in the next section. 3 Virtual Memory-Mapped Communication Virtual memory-mapped communication (VMMC) [10] was developed in response to the need for a basic multicomputer communication mechanism with extremely low latency and high bandwidth.
Reference: [6] <author> M.A. Blumrich, C. Dubnicki, E.W. Felten, K. Li, and M. Mesarina. </author> <title> Virtual memory mapped network interfaces. </title> <booktitle> IEEE Micro 15(1) </booktitle> <pages> 21-28, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: The emergence of new multiprocessor network interfaces opens new possibilities for constructing network software. It is not always clear, however, which interface is most appropriate for which task. The SHRIMP project <ref> [5, 6, 9] </ref> at Princeton University supports user level communication between processes by mapping memory pages between virtual address spaces. This Virtual Memory Mapped network interface seems to have many advantages including flexible user-level communication, and very low overhead to initiate data transfer. <p> The key hardware component is the network interface board, which supports the virtual memory mapped communication (VMMC) model, to provide low-overhead, protected, user-level communication. For more details on the SHRIMP architecture the reader can consult <ref> [5, 6, 9] </ref>. VMMC is discussed in the next section. 3 Virtual Memory-Mapped Communication Virtual memory-mapped communication (VMMC) [10] was developed in response to the need for a basic multicomputer communication mechanism with extremely low latency and high bandwidth.
Reference: [7] <author> C. A. Thekkath, and H.M. Levy. </author> <title> Limits to low-latency communication on high-speed networks. </title> <journal> ACM Transactions on Computer Systems 11, </journal> <month> 2 (May </month> <year> 1993), </year> <pages> 179-203. </pages>
Reference-contexts: with vRPC show that even without changing the stub generator or the kernel, RPC can be made several times faster on the new network interface than it is on conventional networks. vRPC outperforms by more than a factor of 4 the best, to our knowledge, reported implementation for fast networks <ref> [7] </ref>, with a round trip time of about 33s for a null RPC. Even greater gains could be achieved by applying well-known techniques that rely on changes to the stub generator. The second library, ShrimpRPC, is a full-functionality RPC system but is not compatible with any standard. <p> URPC is built on top of a shared memory architecture while we use the distributed-memory SHRIMP architecture. Bershad's LRPC [2] tries to optimize the kernel path for same-machine RPC calls. Since we have eliminated the kernel entirely, LRPC does not apply to our situation. Thekkath and Levy <ref> [7] </ref> investigated the impact of recent improvements in network technology on communication software. They point out that both high throughput and low latency are required by modern distributed systems and that newer networks strive only for high throughput.
Reference: [8] <author> D. Cheriton. </author> <title> The V kernel: A software base for distributed systems. </title> <booktitle> IEEE Software 1(2) </booktitle> <pages> 19-42, </pages> <month> April </month> <year> 1984. </year>
Reference: [9] <author> C. Dubnicki, K. Li, and M. Mesarina. </author> <title> Network interface support for user-level buffer management. </title> <booktitle> Workshop on Parallel Computer Routing and Communications, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: The emergence of new multiprocessor network interfaces opens new possibilities for constructing network software. It is not always clear, however, which interface is most appropriate for which task. The SHRIMP project <ref> [5, 6, 9] </ref> at Princeton University supports user level communication between processes by mapping memory pages between virtual address spaces. This Virtual Memory Mapped network interface seems to have many advantages including flexible user-level communication, and very low overhead to initiate data transfer. <p> The key hardware component is the network interface board, which supports the virtual memory mapped communication (VMMC) model, to provide low-overhead, protected, user-level communication. For more details on the SHRIMP architecture the reader can consult <ref> [5, 6, 9] </ref>. VMMC is discussed in the next section. 3 Virtual Memory-Mapped Communication Virtual memory-mapped communication (VMMC) [10] was developed in response to the need for a basic multicomputer communication mechanism with extremely low latency and high bandwidth.
Reference: [10] <author> C. Dubnicki, L. Iftode, E.W. Felten, and K. Li. </author> <title> Software support for virtual memory-mapped communication. </title> <booktitle> Proceedings of 10th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: For more details on the SHRIMP architecture the reader can consult [5, 6, 9]. VMMC is discussed in the next section. 3 Virtual Memory-Mapped Communication Virtual memory-mapped communication (VMMC) <ref> [10] </ref> was developed in response to the need for a basic multicomputer communication mechanism with extremely low latency and high bandwidth. These performance goals are achieved by allowing applications to transfer data directly between two virtual memory address spaces over the network.
Reference: [11] <author> C. Dubnicki. </author> <title> SHRIMP Basic Library and Its Simulator. </title> <institution> Dept. of Computer Science, Princeton Univ., </institution> <year> 1995. </year>
Reference-contexts: This effort shows that new architectures can open new horizons to distributed programming by providing high performance at low implementation cost. Acknowledgments We are grateful to Cezary Dubnicki for his expert advice on the SHRIMP simulator <ref> [11] </ref>, and for his implementation of the SHRIMP system software. We would also like to thank Liviu Iftode for his implementation of an early mini version of the system software. We thank the members of the SHRIMP team for many useful discussions during the course of this work.
Reference: [12] <author> T. von Eicken, D.E. Culler, S.C. Goldstein, and K.E. </author> <title> Schauser Active Messages: A Mechanism for Integrated Communication and Computation. </title> <booktitle> Proceedings of 19th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992, </year> <pages> pages 256-266. </pages>
Reference-contexts: Unlike signals, however, notifications are queued when blocked. Currently we have an implementation of notifications on top of UNIX signals, which works correctly but is slow. Soon, we expect to have an implementation similar to active messages <ref> [12] </ref>, and thus to have much better performance than signals in the common case. VMMC provides a call that allows a process to block until a notification arrives for it. <p> They develop techniques to achieve low latency in communication software, using RPC as a case study. Their goal along with demonstrating the new techniques is to provide design guidelines for network controllers that will facilitate writing low latency communication software in traditional architectures. Active messages <ref> [12] </ref> are a restricted form of RPC, in which the server-side procedure may not perform any actions that might block. Active messages achieve performance similar to ShrimpRPC on high-performance hardware, but without allowing general handlers to be invoked.
Reference: [13] <author> E.W. Felten, R.D. Alpert, A. Bilas, M.A. Blum-rich, D.W. Clark, S.N. Damianakis, C. Dubnicki, L. Iftode, and K. Li. </author> <title> Early Experience with Message-Passing on the SHRIMP Multicomputer. </title> <booktitle> Proceedings of 23rd International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996, </year> <pages> pages 296-307. </pages>
Reference-contexts: The basic mechanism is designed to efficiently support applications and common communication models such as message passing, shared memory, and client-server. The VMMC mechanism consists of several calls to support user-level buffer management, various data transfer strategies, and transfer of control. by the SHRIMP VMMC layer <ref> [13] </ref>. 3.1 Import-Export Mappings In the VMMC model, an import-export mapping must be established before communication begins. A receiving process can export a region of its address space as a receive buffer together with a set of permissions to define access rights for the buffer.
Reference: [14] <author> D. Johnson, and W. Zwaenepoel, </author> <title> The Pere-grine high performance RPC system. </title> <type> Tech. Rep. </type> <institution> COMP TR91-152, Dept. of Computer Science, Rice Univ., </institution> <year> 1991. </year>
Reference-contexts: 1 Introduction Much is known about how to optimize remote procedure call (RPC) mechanisms on traditional workstation networks <ref> [2, 14, 17, 21] </ref>. The main effort in previous work was to reduce or avoid copying, to make traps and context switches fast, and to take advantage of common-case behavior. The emergence of new multiprocessor network interfaces opens new possibilities for constructing network software. <p> The Optimistic Active Messages [22] approach allows an arbitrary handler to be invoked, using a fast-path implementation but switching to a slower path if the handler blocks. Neither of these systems provides full RPC services, such as automatic stub generation or binding between untrusting parties. Several papers (e.g. <ref> [14, 17] </ref>) describe optimizations that dramatically improve the performance of RPC in traditional systems. This is generally done by avoiding copying, and reducing context switching overhead and network and RPC protocol overhead. 10 Discussion and conclusions Network interfaces can have a great impact on communication performance and ease of programming.
Reference: [15] <author> A. Karlin, K. Li, M. Menasse, and S. Owicki. </author> <title> Empirical Studies of Competitive Spinning for a Shared-Memory Multiprocessor. </title> <booktitle> In Proceedings of 13th Symposium on Operating Systems Principles, </booktitle> <month> Oct. </month> <year> 1991, </year> <pages> pages 41-55. </pages>
Reference-contexts: Waiting in the above protocol is implemented by spinning. Timeouts are used to avoid indefinite postponement. More sophisticated techniques for waiting are given in <ref> [15] </ref>. To summarize, the first version of vRPC, Direct vRPC, replaces the stream layer with a simple stream communication abstraction implemented directly on top of the VMMC interface. The are still six copies per RPC.
Reference: [16] <author> D. Ritchie and K. Thompson. </author> <title> The Unix timesharing system. </title> <journal> Communications of the ACM, </journal> <volume> 17(7) </volume> <pages> 365-375, </pages> <month> July </month> <year> 1974. </year>
Reference: [17] <author> M. Schroeder, and M. Burrows. </author> <title> Performance of Firefly RPC. </title> <journal> ACM Transactions on Computer Systems 8(1) </journal> <pages> 1-17, </pages> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Much is known about how to optimize remote procedure call (RPC) mechanisms on traditional workstation networks <ref> [2, 14, 17, 21] </ref>. The main effort in previous work was to reduce or avoid copying, to make traps and context switches fast, and to take advantage of common-case behavior. The emergence of new multiprocessor network interfaces opens new possibilities for constructing network software. <p> STRUCT thingy - int a; float b <ref> [17] </ref>; - PROCEDURE add (IN int x, IN int y, OUT int sum) PROCEDURE myproc (IN thingy q) The stub generator creates an include-file that contains the data structure definition shown in Figure 9. The client-side stub is shown in Figure 10. <p> As shown here, the main server stub uses polling to detect remote calls. The ShrimpRPC library switches from polling to blocking if a sufficient period of time has elapsed without any RPC requests arriving. This struct thingy - int a; float b <ref> [17] </ref>; - enum _foo_rpc_procedure - _foo_rpc_add_proc = 1, _foo_rpc_myproc_proc = 2; - struct _add_args - char _padding [60]; int sum; int y; volatile int flag; -; struct _myproc_args - struct thingy q; volatile int flag; - union - _add_args add_args; _myproc_args myproc_args; - _foo_rpc_args ShrimpRPC stub generator for the example interface. <p> The Optimistic Active Messages [22] approach allows an arbitrary handler to be invoked, using a fast-path implementation but switching to a slower path if the handler blocks. Neither of these systems provides full RPC services, such as automatic stub generation or binding between untrusting parties. Several papers (e.g. <ref> [14, 17] </ref>) describe optimizations that dramatically improve the performance of RPC in traditional systems. This is generally done by avoiding copying, and reducing context switching overhead and network and RPC protocol overhead. 10 Discussion and conclusions Network interfaces can have a great impact on communication performance and ease of programming.
Reference: [18] <author> Sun Microsystems, Inc. XDR: </author> <title> external data representation standard. Internet Request For Comments RFC 1014, </title> <institution> Internet Network Working Group, </institution> <month> June </month> <year> 1987. </year>
Reference-contexts: SunRPC consists of a set of library functions and a stub generator that follows the XDR <ref> [18] </ref> standard for data representation. SunRPC is a single thread implementation. The general structure of SunRPC is shown in providing services to the layers above. * The network layer implements the read and write system calls that transfer data across the network. * The stream layer does buffer management. <p> The stream layer hides the details of buffer management and network packet size from the higher layers. Its existence is very important to the performance of standard SunRPC implementations, since it reduces accesses to the expensive lower layers. * The XDR <ref> [18] </ref> layer implements the XDR data representation specification, which insulates the higher layers from issues of machine-specific data representation. Data transferred between nodes in a network are translated to XDR format before sending, and translated back from XDR when received.
Reference: [19] <author> Sun Microsystems, Inc. </author> <title> RPC: Remote Procedure Call Protocol Specification Version 2. Internet Request For Comments RFC 1057, </title> <institution> Internet Network Working Group, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: VMMC provides a call that allows a process to block until a notification arrives for it. When a process is waiting for a VMMC message to arrive, this call can be used to switch easily between spinning and blocking as appropriate. 4 SunRPC SunRPC <ref> [19] </ref> is a widely used remote procedure call interface and specification. SunRPC consists of a set of library functions and a stub generator that follows the XDR [18] standard for data representation. SunRPC is a single thread implementation.
Reference: [20] <author> C. Thacker, L. Stewart, and E. Satterthwaite, JR. Firefly: </author> <title> A multiprocessor workstation. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(8) </volume> <pages> 909-920, </pages> <month> Aug. </month> <year> 1988. </year>
Reference: [21] <author> R. Van Renesse, H. Van Staveren, and A. Tanen-baum. </author> <title> The performance of the Amoeba distributed operating system. </title> <journal> Software Practice and Experience, </journal> <volume> 19(3) </volume> <month> 223-234 (Mar. </month> <year> 1989). </year>
Reference-contexts: 1 Introduction Much is known about how to optimize remote procedure call (RPC) mechanisms on traditional workstation networks <ref> [2, 14, 17, 21] </ref>. The main effort in previous work was to reduce or avoid copying, to make traps and context switches fast, and to take advantage of common-case behavior. The emergence of new multiprocessor network interfaces opens new possibilities for constructing network software.
Reference: [22] <author> D.A. Wallach, W.C. Hsieh, K. Johnson, M..F. Kaashoek, and W.E. Weihl. </author> <title> Optimistic Active Messages: A Mechanism for Scheduling Communication with Computation. </title> <booktitle> Proceedings of 5th ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Active messages [12] are a restricted form of RPC, in which the server-side procedure may not perform any actions that might block. Active messages achieve performance similar to ShrimpRPC on high-performance hardware, but without allowing general handlers to be invoked. The Optimistic Active Messages <ref> [22] </ref> approach allows an arbitrary handler to be invoked, using a fast-path implementation but switching to a slower path if the handler blocks. Neither of these systems provides full RPC services, such as automatic stub generation or binding between untrusting parties.

References-found: 22

