URL: http://www.cs.ust.hk/faculty/dlee/Papers/www/tkde.ps
Refering-URL: http://www.public.iastate.edu/~CYBERSTACKS/Aristotle.htm
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: WISE: A World Wide Web Resource Database System  
Author: Budi Yuwono Dik L. Lee 
Keyword: resource discovery, information retrieval, World Wide Web, meta-information indexing  
Address: Columbus, Ohio, U.S.A. Clear Water Bay, Hong Kong.  
Affiliation: Department of Computer and Computer Science Department Information Science Hong Kong University of Science The Ohio State University and Technology  
Abstract: noindent This paper describes the World Wide Web Index and Search Engine (WISE) for resource discovery. The system is designed around a resource database containing meta-information about WWW resources and is automatically built using an indexer robot, a special WWW client agent. The resource database allows users to search for resources based on keywords, and to learn about potentially relevant resources without having to directly access them. Such capabilities can significantly reduce the amount of time that a user needs to spend in order to find the information of his/her interest. The main strength of WISE is its use of advanced information retrieval techniques. The paper discusses WISE's main components: the resource database, the indexer robot, the search engine, and the user interface, and discusses the issues involved in the design, implementation and evaluation of the system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Berners-Lee, T., </author> <title> "Uniform Resource Locators," Internet Working Draft, </title> <month> 1 January </month> <year> 1994. </year>
Reference-contexts: The meta-information is a compact representation of the resources and therefore takes less time to browse through. WISE supports this by providing users with a list of potentially relevant WWW addresses or Uniform Resource Locators (URL's <ref> [1] </ref>) and their descriptions in a hierarchical format. The robot-based approach is by far the only workable interim solution before a more suitable communication protocol, such as the one used by Archie FTP directory service [6], for meta-information exchange between Web servers and information brokers is available. <p> Fortunately, all this meta-information is readily available from the resource's URL <ref> [1] </ref>. Another potentially useful meta-information is the size of a data object. This information allows the user to approximate the time and space it will take to retrieve the object.
Reference: [2] <author> Berners-Lee, T., </author> <title> "Hypertext Transfer Protocol," Internet Working Draft, </title> <month> 5 November </month> <year> 1993. </year>
Reference-contexts: system; its index database is built by an autonomous client agent, known as WWW robot, which accesses resources from various WWW sites or servers to collect the meta-information. 3.1 Indexer Robot Our indexer robot is basically a WWW client which communicates with WWW servers using the Hypertext Transfer Protocol (HTTP <ref> [2] </ref>). It only retrieves objects of text format (including non-HTML texts) from remote servers. In other words, it avoids interactive and dynamically generated resources, as well as non-textual and application-specific objects on which it is not capable of processing.
Reference: [3] <author> Berners-Lee, T., and Connolly, D., </author> <title> "Hypertext Markup Language," Internet Working Draft, </title> <month> 13 July </month> <year> 1993. </year>
Reference-contexts: These words are explicitly marked as HTML tokens <ref> [3] </ref>. It is reasonable to assume that the author of a WWW page will use these token markups only for words or sentences he/she consider important. Therefore, we can assume that these words make good representations of the page's content.
Reference: [4] <author> Berners-Lee, T., Cailliau, R., Groff, J., and Pollermann, B., </author> <title> "World Wide Web: The Information Universe," </title> <journal> Electronic Networking: Research, Applications and Policy, </journal> <volume> 1(2), </volume> <year> 1992. </year>
Reference-contexts: This research leads to the development of systems which allow users to locate resources by specifying keywords, i.e., searching on databases or catalogs which index the online resources. In this paper, we discuss the WWW Index and Search Engine 1 (WISE), which is an integrated World Wide Web (WWW <ref> [4] </ref>) resource discovery system developed at the Department of Computer Science, the Hong Kong University of Science and Technology. 2 As the name implies, WISE is based entirely on WWW and is mainly designed for locating information on WWW, as well as other online resources advertised on WWW.
Reference: [5] <author> Eichmann, D., </author> <title> "The RBSE Spider - Balanching Effective Search against Web Load," </title> <booktitle> in Proceedings of the First International Conference on the World Wide Web, </booktitle> <address> p113-120, Geneva, Switzerland, </address> <year> 1994. </year>
Reference-contexts: view [14] or zooming capabilities, possible. 6 Conclusions In the last two years, the population of robot-based WWW index servers has been growing rapidly. 12 Among the well known robots before WISE, only a few employ full-text indexing, e.g., WebCrawler [11], the Repository Based Software Engineering Project Spider 13 (RBSE <ref> [5] </ref>), and Lycos, while others index only page titles and anchor hypertexts. Today, all major robot-based index servers such as Alta Vista 14 employ full-text indexing. WISE's scheme lies between full-text and title-only schemes by taking advantage of HTML-based meta-information as much as possible.
Reference: [6] <author> Emtage, A., and Deutsch, P., "Archie: </author> <title> An Electronic Directory Service for the Internet," </title> <booktitle> in Proceedings of the USENIX Winter Conference, </booktitle> <address> p93-110, Berkeley CA, </address> <year> 1992. </year>
Reference-contexts: The robot-based approach is by far the only workable interim solution before a more suitable communication protocol, such as the one used by Archie FTP directory service <ref> [6] </ref>, for meta-information exchange between Web servers and information brokers is available. So far, ALIWEB [7] is the only known system to employ a server-to-server meta-information exchange protocol.
Reference: [7] <author> Koster, M., "ALIWEB: </author> <title> Archie-like Indexing in the Web," Computer Networks and ISDN Systems, </title> <type> 27(2), </type> <institution> p175-182, </institution> <year> 1994. </year>
Reference-contexts: The robot-based approach is by far the only workable interim solution before a more suitable communication protocol, such as the one used by Archie FTP directory service [6], for meta-information exchange between Web servers and information brokers is available. So far, ALIWEB <ref> [7] </ref> is the only known system to employ a server-to-server meta-information exchange protocol. However, this scheme demands a highly cooperative environment, which is hard to come by on the Internet (at least for now) where servers are largely distributed and autonomous.
Reference: [8] <author> Lee, D., and Chuang, A., </author> <title> "Performance of Document Ranking and Relevance Feedback," </title> <note> submitted for publication. </note>
Reference-contexts: The keywords and their occurrence frequencies in the feedback URL's are obtained from the forward index described in section 2. The relevance feedback algorithm is based on the algorithms proposed in <ref> [8] </ref> which are designed to refine a query by capturing the context in which the original query words appear in the feedback URL's. Finally, the reformulated query is submitted to the system for processing when the user clicks on the Feedback button, and another result page is then shown.
Reference: [9] <author> McBryan, O., </author> <title> "GENVL and WWWW: Tools for Taming the Web," </title> <booktitle> in Proceedings of the First International Conference on the World Wide Web, </booktitle> <address> p79-90, Geneva, Switzerland, </address> <year> 1994. </year>
Reference: [10] <author> Nelson, P., </author> <title> "GDBM The GNU Database Manager," online manual pages, </title> <note> Version 1.7.3, </note> <year> 1990. </year>
Reference-contexts: Two additional tables are used for mapping between URL-ID and URL, and between term-ID and term. We implemented the above tables using the public-domain GNU Database Manager (GDBM <ref> [10] </ref>) library package. 3 Meta-data Collection WISE is a robot-based WWW indexing system; its index database is built by an autonomous client agent, known as WWW robot, which accesses resources from various WWW sites or servers to collect the meta-information. 3.1 Indexer Robot Our indexer robot is basically a WWW client
Reference: [11] <author> Pinkerton, B., </author> <title> "Finding What People Want: Experiences with the WebCrawler," </title> <booktitle> in Proceedings of the First International Conference on the World Wide Web, </booktitle> <address> Geneva, Switzerland, </address> <year> 1994. </year>
Reference-contexts: better display techniques, such as graphical hypertext maps with fish-eye view [14] or zooming capabilities, possible. 6 Conclusions In the last two years, the population of robot-based WWW index servers has been growing rapidly. 12 Among the well known robots before WISE, only a few employ full-text indexing, e.g., WebCrawler <ref> [11] </ref>, the Repository Based Software Engineering Project Spider 13 (RBSE [5]), and Lycos, while others index only page titles and anchor hypertexts. Today, all major robot-based index servers such as Alta Vista 14 employ full-text indexing.
Reference: [12] <author> Salton, G., and Buckley, C., </author> <title> "Term-Weighting Approaches in Automatic Text Retrieval," </title> <booktitle> Information Processing & Management, 24(5), </booktitle> <address> p513-523, </address> <year> 1988. </year>
Reference-contexts: However, as we have shown empirically in [17], the vector-length normalization causes a drop in the average retrieval precision when applied to the WWW document environment. A similar phenomenon is reported in <ref> [12] </ref> for collections of short documents. Therefore, the vector-length normalization factor is not used in our TFxIDF algorithm. <p> F i;max where: 7 T F i;j : the term frequency of Q j in D i T F i;max : the maximum term frequency of a keyword in D i IDF j : log (N= i=1 C i;j ) This term weighting formula is known as the nfx formula <ref> [12] </ref>. As shown in the equation, the TF component is normalized by the maximum TF in the vector, and further normalized to lie between 0.5 to 1.0. 4.2.2 Other Algorithms We have considered a number of alternative ranking algorithms.
Reference: [13] <author> Salton, G., and McGill, M., </author> <title> Introduction to Modern Information Retrieval, </title> <publisher> McGraw-Hill, </publisher> <address> New York NY, </address> <year> 1983. </year>
Reference-contexts: The pool is then handed over to the ranking algorithm. 6 4.2 Ranking Algorithm The current version of WISE employs a ranking algorithm called TFxIDF which is based on the well-known vector space model <ref> [13] </ref>. The decision to use TFxIDF was made based on the result of an experiment which compares the algorithm with a number of alternative algorithms. This point is discussed later in this section after the descriptions of the algorithms. <p> C i;j : occurrence of Q j in D i , where C i;j = 1 if D i contains Q j , or 0 otherwise. 4.2.1 TFxIDF TFxIDF algorithm is based on the vector space model <ref> [13] </ref>. The similarity between a document and a query is measured by the cosine of the angle between their vector representations in a multi-dimensional space. The similarity value is taken as the relevance score of the document with respect to the query. <p> We manually examined the entire collection to identify the relevant URL's for each query. The maximum number of hits for this experiment was set to 500 URL's. We used the standard evaluation procedure to compute the recall/precision of the algorithms <ref> [13] </ref>. in this experiment is attributed to the query construction procedure which guarantees that each query has at least one highly relevant document. Although the vector spread-activation shows the best average precision, the difference between it and TFxIDF is practically negligible.
Reference: [14] <author> Sarkar, M., and Brown, M., </author> <title> "Graphical Fish-Eye Views of Graphs," </title> <booktitle> in Proceeding of CHI '92: Human Factors in Computing Systems, </booktitle> <publisher> ACM Press, </publisher> <address> New York NY, p83-92, </address> <year> 1992. </year>
Reference-contexts: Ideally, functionalities such as hypertext mapping and relevance feedback mechanism should be implemented on the client's side, where the client down loads the necessary data from the server and performs the computation locally. This scheme will also make better display techniques, such as graphical hypertext maps with fish-eye view <ref> [14] </ref> or zooming capabilities, possible. 6 Conclusions In the last two years, the population of robot-based WWW index servers has been growing rapidly. 12 Among the well known robots before WISE, only a few employ full-text indexing, e.g., WebCrawler [11], the Repository Based Software Engineering Project Spider 13 (RBSE [5]), and
Reference: [15] <author> Schwartz, M., Emtage, A., Kahle, B., and Neumann, B., </author> <title> "A comparison of Internet Resource Discovery Approaches," </title> <journal> Computer Systems, </journal> <volume> 5(4), p461-493, </volume> <year> 1992. </year>
Reference-contexts: 1 Introduction One of the most pressing issues with today's explosive growth of the Internet is the so-called resource discovery problem <ref> [15] </ref>. That is, how to find information of interest among the vast and ever growing amount of information available online. Research in this area has been gaining popularity among online information providers and researchers in information retrieval, database, artificial intelligence, and distributed computing areas.
Reference: [16] <author> Yuwono, B., Lam, S., Ying, J., and Lee, D., </author> <title> "A World Wide Web Resource Discovery System," </title> <booktitle> to appear in Proceedings of the Fourth International World Wide Web Conference, </booktitle> <address> Boston MA, </address> <year> 1995. </year>
Reference-contexts: The user interface mechanism is implemented using the standard Common Gateway Interface (CGI) and is run by an NCSA HTTPD WWW Server program. A more detailed description of the user interface can be found in <ref> [16] </ref>. After typing in a query, the user sends it to the search engine by clicking on the Search the Web submit button. Upon receiving the result from the search engine, the user interface displays a list of URL's (represented by their respective titles) ordered in descending relevance scores.
Reference: [17] <author> Yuwono, B., and Lee, D., </author> <title> "Search and Ranking Algorithms for Locating Resources on the World Wide Web," </title> <booktitle> to appear in Proceedings of the Twelfth International Conference on Data Engineering, </booktitle> <address> New Orleans LA, </address> <year> 1996. </year> <month> 15 </month>
Reference-contexts: This weighting function gives higher weights to terms which occur frequently in a small set of the documents. However, as we have shown empirically in <ref> [17] </ref>, the vector-length normalization causes a drop in the average retrieval precision when applied to the WWW document environment. A similar phenomenon is reported in [12] for collections of short documents. Therefore, the vector-length normalization factor is not used in our TFxIDF algorithm. <p> For document D i , the algorithm assigns a relevance score with respect to query q as follows: 9 The algorithm is formerly called Boolean spread activation <ref> [17] </ref> which can be misleading. 8 R i;q = j=1 where I i;j is defined as: I i;j = &gt; &lt; c 1 if C i;j = 1 c 2 if there exists k such that C k;j = 1 and Li i;k + Lo i;k &gt; 0 0 otherwise c <p> In our implementation, we use c 1 = 10 and c 2 = 1. It has been shown in <ref> [17] </ref> that the algorithm is not sensitive to the values of these two constants. 3. Weight-vector spread activation This algorithm combines the vector space model and the spread activation model. <p> Through an experiment we found that 0:2 is the optimal value for ff <ref> [17] </ref>. Among the retrieved documents returned by the ranking algorithm, only the top H documents are selected. H is called the maximum number of hits. WISE's default value of H is set to 40.
References-found: 17

