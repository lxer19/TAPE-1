URL: http://www.cs.umd.edu/~tseng/papers/crpc93307.ps
Refering-URL: http://www.cs.umd.edu/~tseng/papers.html
Root-URL: 
Title: Preliminary Experiences with the Fortran D Compiler  
Author: Seema Hiranandani Ken Kennedy Chau-Wen Tseng 
Note: Center for Research on Parallel Computation  To appear in Supercomputing '93, Portland, OR, November 1993.  
Date: April 1993  Revised September 1993.  
Address: 93307  P.O. Box 1892 Houston, TX 77251-1892  
Affiliation: CRPC-TR  Rice University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. Amarasinghe and M. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Program Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Their studies validate the effectiveness of selected compiler optimizations for complete programs. Ruhl performed studies on a variety of parallel architectures, demonstrating excellent speedups for the Oxygen compiler [26]. Amarasinghe & Lam use precise data-flow information for arrays from last-write-trees in Suif to avoid over-communication <ref> [1] </ref>. They report speedups for Gaussian elimination without pivoting. 7 Conclusions An efficient, portable, data-parallel programming model is required to make large-scale parallel machines useful for scientific programmers. We believe that Fortran D provides such a model for distributed-memory machines.
Reference: [2] <institution> Applied Parallel Research, Placerville, CA. </institution> <note> Forge 90 Distributed Memory Parallelizer: User's Guide, version 8.0 edition, </note> <year> 1992. </year>
Reference-contexts: Compared with other contemporary systems <ref> [2, 3, 7, 8, 18, 24] </ref>, The Fortran D compiler is less flexible but performs deeper compile-time analysis, many more advanced optimizations, requires fewer language extensions, and relies on less run-time support. Few researchers have published experimental results for large programs.
Reference: [3] <author> T. Brandes. </author> <title> Efficient data parallel programming without explicit message passing for distributed memory multiprocessors. </title> <type> Internal Report AHR-92-4, </type> <institution> High Performance Computing Center, GMD, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: Compared with other contemporary systems <ref> [2, 3, 7, 8, 18, 24] </ref>, The Fortran D compiler is less flexible but performs deeper compile-time analysis, many more advanced optimizations, requires fewer language extensions, and relies on less run-time support. Few researchers have published experimental results for large programs.
Reference: [4] <author> M. Bromley, S. Heller, T. McNerney, and G. Steele, Jr. </author> <title> Fortran at ten gigaflops: The Connection Machine convolution compiler. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Olander & Schnabel show that Dino programs can be significantly improved through iteration reordering and pipelining [23]. Bromley et al. develop optimizations in CM Fortran compiler for stencils on the CM-2 <ref> [4] </ref>. By inserting calls to hand-coded microcode routines that apply unroll-and-jam, they avoid unnecessary intra-processor data motion, insert communication only for nonlocal data, and improve register usage. The resulting compiler achieves significant improvements in execution speed for a finite-difference seismic model.
Reference: [5] <author> C. Burns, R. Kuhn, and E. Werme. </author> <title> Low copy message passing on the Alliant CAMPUS/800. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <address> Minneapolis, MN, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: For parallel kernels, the output of the Fortran D compiler was within 50% of the best hand-optimized codes. The deficit was mainly caused by the Fortran D compiler not exploiting unbuffered messages in order to eliminate buffering and overlap communication overhead with local computation <ref> [5, 17] </ref>. The compiler-generated code actually outperformed the hand-optimized pipelined codes, even though the two message-passing Fortran 77 versions of the program were nearly identical. We thus assume the differences to be due to complications with the scalar i860 node compiler in the parameterized hand-optimized version. <p> Hatcher et al. demonstrate that Dataparal-lel C can achieve speedups for large scientific applications on MIMD architectures [14]. Burns et al. developed techniques for guiding the use of unbuffered messages on the Alliant CAMPUS/800 using data dependence information <ref> [5] </ref>. They show that unbuffered messages improve overall performance for a collection of hand-parallelized scientific programs. Their studies validate the effectiveness of selected compiler optimizations for complete programs. Ruhl performed studies on a variety of parallel architectures, demonstrating excellent speedups for the Oxygen compiler [26].
Reference: [6] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: The two major steps in compiling for MIMD distributed-memory machines are partitioning the data and computation across processors, then introducing communication for nonlocal accesses where needed. The compiler partitions computation across processors using the "owner computes" rule|where each processor only computes values of data it owns <ref> [6, 12, 25] </ref>. It performs a large number of communication and parallelism optimizations based on data dependence. <p> However, as shown by the performance of the CM Fortran compiler, Fortran 90 syntax does not eliminate the need for advanced compile-time analysis and optimization. 6 Related Work The Fortran D compiler is a second-generation distributed-memory compiler that incorporates and extends features from previous compilation systems <ref> [6, 12, 19, 21, 25] </ref>. Compared with other contemporary systems [2, 3, 7, 8, 18, 24], The Fortran D compiler is less flexible but performs deeper compile-time analysis, many more advanced optimizations, requires fewer language extensions, and relies on less run-time support.
Reference: [7] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 31-50, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: Compared with other contemporary systems <ref> [2, 3, 7, 8, 18, 24] </ref>, The Fortran D compiler is less flexible but performs deeper compile-time analysis, many more advanced optimizations, requires fewer language extensions, and relies on less run-time support. Few researchers have published experimental results for large programs.
Reference: [8] <author> C. Chase, A. Cheung, A. Reeves, and M. Smith. </author> <title> Paragon: A parallel programming environment for scientific applications using communication structures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16(2) </volume> <pages> 79-91, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Compared with other contemporary systems <ref> [2, 3, 7, 8, 18, 24] </ref>, The Fortran D compiler is less flexible but performs deeper compile-time analysis, many more advanced optimizations, requires fewer language extensions, and relies on less run-time support. Few researchers have published experimental results for large programs.
Reference: [9] <author> K. Cooper, M. W. Hall, R. T. Hood, K. Kennedy, K. S. McKinley, J. M. Mellor-Crummey, L. Torczon, and S. K. Warren. </author> <title> The ParaScope parallel programming environment. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 244-263, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Details of the compilation process are presented elsewhere [13, 16, 17, 27]. 2.2 Prototype Compiler The prototype Fortran D compiler is implemented as a source-to-source Fortran translator in the context of the ParaScope parallel programming environment <ref> [9] </ref>. It utilizes existing tools for performing dependence analysis, program transformations, and interprocedural analysis.
Reference: [10] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kre-mer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Each dimension is distributed in a block, cyclic, or block-cyclic manner; the symbol ":" marks dimensions that are not distributed. Alignment and distribution statements can be executed, permitting dynamic data decomposition. Details of the language are presented elsewhere <ref> [10] </ref>. Given a data decomposition, the Fortran D compiler automatically translates sequential programs into efficient parallel programs. The two major steps in compiling for MIMD distributed-memory machines are partitioning the data and computation across processors, then introducing communication for nonlocal accesses where needed.
Reference: [11] <author> G. Geist and C. Romine. </author> <title> LU factorization algorithms on distributed-memory multiprocessor architectures. </title> <journal> SIAM Journal of Scientific Stat. Computing, </journal> <volume> 9 </volume> <pages> 639-649, </pages> <year> 1988. </year>
Reference-contexts: 96 fi 96 4 1.517 3.49 1.151 4.60 0.76 16 1.481 3.58 0.813 6.52 0.55 1 estimated sequential time = 12.6 128 fi 128 8 2.738 4.60 1.905 6.61 0.70 32 2.533 4.97 1.347 9.35 0.53 Table 4: iPSC/860 Timings for Erlebacher (in seconds) on optimizations described in the literature <ref> [11, 22] </ref>. First, we combined the two messages broadcast on each iteration of the outermost k loop. Instead of broadcasting the pivot value immediately, we wait until multipliers are also computed. The values can then be combined in one broadcast.
Reference: [12] <author> M. Gerndt. </author> <title> Updating distributed variables in local computations. </title> <journal> Concurrency: Practice & Experience, </journal> <volume> 2(3) </volume> <pages> 171-193, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: The two major steps in compiling for MIMD distributed-memory machines are partitioning the data and computation across processors, then introducing communication for nonlocal accesses where needed. The compiler partitions computation across processors using the "owner computes" rule|where each processor only computes values of data it owns <ref> [6, 12, 25] </ref>. It performs a large number of communication and parallelism optimizations based on data dependence. <p> However, as shown by the performance of the CM Fortran compiler, Fortran 90 syntax does not eliminate the need for advanced compile-time analysis and optimization. 6 Related Work The Fortran D compiler is a second-generation distributed-memory compiler that incorporates and extends features from previous compilation systems <ref> [6, 12, 19, 21, 25] </ref>. Compared with other contemporary systems [2, 3, 7, 8, 18, 24], The Fortran D compiler is less flexible but performs deeper compile-time analysis, many more advanced optimizations, requires fewer language extensions, and relies on less run-time support.
Reference: [13] <author> M. W. Hall, S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Interprocedural compilation of Fortran D for MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <address> Minneapolis, MN, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: The goal of our research is to identify important compilation issues and explore possible solutions. Previous work has described the design and implementation of a prototype Fortran D compiler for regular dense-matrix computations <ref> [13, 16, 17] </ref>. This paper describes our preliminary experiences with that compiler. Its major contributions include 1) advanced compilation techniques needed for complex loop nests, 2) empirical evaluation of the prototype Fortran D compiler, and 3) identifying necessary improvements for the compiler. <p> The compiler partitions computation across processors using the "owner computes" rule|where each processor only computes values of data it owns [6, 12, 25]. It performs a large number of communication and parallelism optimizations based on data dependence. Details of the compilation process are presented elsewhere <ref> [13, 16, 17, 27] </ref>. 2.2 Prototype Compiler The prototype Fortran D compiler is implemented as a source-to-source Fortran translator in the context of the ParaScope parallel programming environment [9]. It utilizes existing tools for performing dependence analysis, program transformations, and interprocedural analysis.
Reference: [14] <author> P. Hatcher, M. Quinn, R. Anderson, A. Lapadula, B. Seev-ers, and A. Bennett. </author> <title> Architecture-independent scientific programming in Dataparallel C: Three case studies. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> Novem-ber </month> <year> 1991. </year>
Reference-contexts: The resulting compiler achieves significant improvements in execution speed for a finite-difference seismic model. Hatcher et al. demonstrate that Dataparal-lel C can achieve speedups for large scientific applications on MIMD architectures <ref> [14] </ref>. Burns et al. developed techniques for guiding the use of unbuffered messages on the Alliant CAMPUS/800 using data dependence information [5]. They show that unbuffered messages improve overall performance for a collection of hand-parallelized scientific programs. Their studies validate the effectiveness of selected compiler optimizations for complete programs.
Reference: [15] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification, version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Center for Research on Parallel Computation, Rice University, Houston, TX, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Fortran D is designed to provide a simple yet efficient machine-independent data-parallel programming model, shifting the burden of optimizations to the compiler. It has contributed to the development of High Performance Fortran (HPF), an informal Fortran standard adopted by researchers and vendors for programming massively-parallel processors <ref> [15] </ref>. The success of HPF hinges on the development of compilers that can provide performance satisfactory to users. The goal of our research is to identify important compilation issues and explore possible solutions.
Reference: [16] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: The goal of our research is to identify important compilation issues and explore possible solutions. Previous work has described the design and implementation of a prototype Fortran D compiler for regular dense-matrix computations <ref> [13, 16, 17] </ref>. This paper describes our preliminary experiences with that compiler. Its major contributions include 1) advanced compilation techniques needed for complex loop nests, 2) empirical evaluation of the prototype Fortran D compiler, and 3) identifying necessary improvements for the compiler. <p> The compiler partitions computation across processors using the "owner computes" rule|where each processor only computes values of data it owns [6, 12, 25]. It performs a large number of communication and parallelism optimizations based on data dependence. Details of the compilation process are presented elsewhere <ref> [13, 16, 17, 27] </ref>. 2.2 Prototype Compiler The prototype Fortran D compiler is implemented as a source-to-source Fortran translator in the context of the ParaScope parallel programming environment [9]. It utilizes existing tools for performing dependence analysis, program transformations, and interprocedural analysis. <p> However, the Fortran D compiler determines that t is a private variable with respect to the k loop. With additional analysis, the compiler discovers that it does not need to replicate the assignment on all processors <ref> [16] </ref>. 3.3.2 MINLOC/MAXLOC Reductions Putting statements S 1 through S 4 in the same statement group requires detecting it as a reduction. The Fortran D compiler recognizes reductions through simple pattern matching. <p> This information can be employed to eliminate the unnecessary global concatenation. Array kill analysis has not yet been implemented in the prototype compiler. 3.4.4 Exploiting Pipeline Parallelism Because the computational wavefront traverses across processors in the Z dimension, the Fortran D compiler must efficiently exploit pipeline parallelism <ref> [16] </ref>.
Reference: [17] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Evaluation of compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Wash-ington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: The goal of our research is to identify important compilation issues and explore possible solutions. Previous work has described the design and implementation of a prototype Fortran D compiler for regular dense-matrix computations <ref> [13, 16, 17] </ref>. This paper describes our preliminary experiences with that compiler. Its major contributions include 1) advanced compilation techniques needed for complex loop nests, 2) empirical evaluation of the prototype Fortran D compiler, and 3) identifying necessary improvements for the compiler. <p> The compiler partitions computation across processors using the "owner computes" rule|where each processor only computes values of data it owns [6, 12, 25]. It performs a large number of communication and parallelism optimizations based on data dependence. Details of the compilation process are presented elsewhere <ref> [13, 16, 17, 27] </ref>. 2.2 Prototype Compiler The prototype Fortran D compiler is implemented as a source-to-source Fortran translator in the context of the ParaScope parallel programming environment [9]. It utilizes existing tools for performing dependence analysis, program transformations, and interprocedural analysis. <p> that arose during compilation of Erlebacher to a machine with four processors, P 0 : : : P 3 . 3.4.1 Overlapping Communication In Erlebacher, we discovered unexpected benefits for vector message pipelining, an optimization that separates matching send and recv statements to create opportunities for overlapping communication with computation <ref> [17] </ref>. Consider the computation in the Z dimension, shown in loops enclosing statements S 1 -S 4 because they belong to two distinct statement groups. Message vectorization then extracts all communication outside of each loop nest. Finally, the Fortran D compiler applies vector message pipelining. <p> Each line represents the speedup for a given problem size. 4.1.1 Results for Stencil Kernels The hand-optimized stencil kernels are taken from a previous study evaluating the effect of different communication & parallelism optimizations on overall performance <ref> [17] </ref>. We selected a sum reduction (Livermore 3), two parallel kernels (Livermore 18, Jacobi), and two pipelined kernels (Livermore 23, SOR). As before, all arrays are double precision and distributed block-wise in one dimension. <p> For parallel kernels, the output of the Fortran D compiler was within 50% of the best hand-optimized codes. The deficit was mainly caused by the Fortran D compiler not exploiting unbuffered messages in order to eliminate buffering and overlap communication overhead with local computation <ref> [5, 17] </ref>. The compiler-generated code actually outperformed the hand-optimized pipelined codes, even though the two message-passing Fortran 77 versions of the program were nearly identical. We thus assume the differences to be due to complications with the scalar i860 node compiler in the parameterized hand-optimized version. <p> The effect of these optimizations on overall execution time increases in importance as the problem size and number of processors increases. In particular, the Fortran D compiler will need to use information from training sets and static performance estimation to select an efficient granularity for coarse-grain pipelining <ref> [17] </ref>. To summarize, the Fortran D compiler performs extensive analysis for stencil computations and is able to achieve good speedups.
Reference: [18] <author> K. Ikudome, G. Fox, A. Kolawa, and J. Flower. </author> <title> An automatic and symbolic parallelization system for distributed memory parallel computers. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Compared with other contemporary systems <ref> [2, 3, 7, 8, 18, 24] </ref>, The Fortran D compiler is less flexible but performs deeper compile-time analysis, many more advanced optimizations, requires fewer language extensions, and relies on less run-time support. Few researchers have published experimental results for large programs.
Reference: [19] <author> C. Koelbel and P. Mehrotra. </author> <title> Programming data parallel algorithms on distributed memory machines using Kali. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: However, as shown by the performance of the CM Fortran compiler, Fortran 90 syntax does not eliminate the need for advanced compile-time analysis and optimization. 6 Related Work The Fortran D compiler is a second-generation distributed-memory compiler that incorporates and extends features from previous compilation systems <ref> [6, 12, 19, 21, 25] </ref>. Compared with other contemporary systems [2, 3, 7, 8, 18, 24], The Fortran D compiler is less flexible but performs deeper compile-time analysis, many more advanced optimizations, requires fewer language extensions, and relies on less run-time support. <p> Few researchers have published experimental results for large programs. Pingali & Rogers apply message vectoriza-tion, message pipelining, and reduction recognition in Id Nouveau to parallelize Simple [25]. Koelbel & Mehrotra are able to parallelize ADI integration in Kali by implicitly applying dynamic data decomposition between computation phases <ref> [19] </ref>. Olander & Schnabel show that Dino programs can be significantly improved through iteration reordering and pipelining [23]. Bromley et al. develop optimizations in CM Fortran compiler for stencils on the CM-2 [4].
Reference: [20] <author> U. Kremer and Marcelo Rame. </author> <title> Compositional oil reservoir simulation in Fortran D: A feasibility study on Intel iPSC/860. </title> <type> Technical Report TR93-209, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: This style of programming, while efficient for vector machines, does not lend itself to massively-parallel processors. To explore whether Utcomp can be written in a machine-independent programming style using Fortran D or HPF, researchers at Rice rewrote Disper to have regular accesses and simple subscripts in multidimensional arrays <ref> [20] </ref>. Figure 3 shows a fragment of the rewritten form of Disper. Its main arrays have differing sizes and dimensionality, but have the same size in the first dimension. Arrays were aligned along the first dimension and distributed block-wise.
Reference: [21] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: However, as shown by the performance of the CM Fortran compiler, Fortran 90 syntax does not eliminate the need for advanced compile-time analysis and optimization. 6 Related Work The Fortran D compiler is a second-generation distributed-memory compiler that incorporates and extends features from previous compilation systems <ref> [6, 12, 19, 21, 25] </ref>. Compared with other contemporary systems [2, 3, 7, 8, 18, 24], The Fortran D compiler is less flexible but performs deeper compile-time analysis, many more advanced optimizations, requires fewer language extensions, and relies on less run-time support.
Reference: [22] <author> M. Mu and J. Rice. </author> <title> Row oriented Gauss elimination on distributed memory multiprocessors. </title> <journal> International Journal of High Speed Computing, </journal> <volume> 4(2) </volume> <pages> 143-168, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: 96 fi 96 4 1.517 3.49 1.151 4.60 0.76 16 1.481 3.58 0.813 6.52 0.55 1 estimated sequential time = 12.6 128 fi 128 8 2.738 4.60 1.905 6.61 0.70 32 2.533 4.97 1.347 9.35 0.53 Table 4: iPSC/860 Timings for Erlebacher (in seconds) on optimizations described in the literature <ref> [11, 22] </ref>. First, we combined the two messages broadcast on each iteration of the outermost k loop. Instead of broadcasting the pivot value immediately, we wait until multipliers are also computed. The values can then be combined in one broadcast.
Reference: [23] <author> D. Olander and R. Schnabel. </author> <title> Preliminary experience in developing a parallel thin-layer Navier Stokes code and implications for parallel language design. </title> <booktitle> In Proceedings of the 1992 Scalable High Performance Computing Conference, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Koelbel & Mehrotra are able to parallelize ADI integration in Kali by implicitly applying dynamic data decomposition between computation phases [19]. Olander & Schnabel show that Dino programs can be significantly improved through iteration reordering and pipelining <ref> [23] </ref>. Bromley et al. develop optimizations in CM Fortran compiler for stencils on the CM-2 [4]. By inserting calls to hand-coded microcode routines that apply unroll-and-jam, they avoid unnecessary intra-processor data motion, insert communication only for nonlocal data, and improve register usage.
Reference: [24] <author> M. Philippsen and W. Tichy. </author> <title> Compiling for massively parallel machines. </title> <booktitle> In First International Conference of the Austrian Center for Parallel Computation, </booktitle> <address> Salzburg, Aus-tria, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: Compared with other contemporary systems <ref> [2, 3, 7, 8, 18, 24] </ref>, The Fortran D compiler is less flexible but performs deeper compile-time analysis, many more advanced optimizations, requires fewer language extensions, and relies on less run-time support. Few researchers have published experimental results for large programs.
Reference: [25] <author> K. Pingali and A. Rogers. </author> <title> Compiling for locality. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: The two major steps in compiling for MIMD distributed-memory machines are partitioning the data and computation across processors, then introducing communication for nonlocal accesses where needed. The compiler partitions computation across processors using the "owner computes" rule|where each processor only computes values of data it owns <ref> [6, 12, 25] </ref>. It performs a large number of communication and parallelism optimizations based on data dependence. <p> However, as shown by the performance of the CM Fortran compiler, Fortran 90 syntax does not eliminate the need for advanced compile-time analysis and optimization. 6 Related Work The Fortran D compiler is a second-generation distributed-memory compiler that incorporates and extends features from previous compilation systems <ref> [6, 12, 19, 21, 25] </ref>. Compared with other contemporary systems [2, 3, 7, 8, 18, 24], The Fortran D compiler is less flexible but performs deeper compile-time analysis, many more advanced optimizations, requires fewer language extensions, and relies on less run-time support. <p> Few researchers have published experimental results for large programs. Pingali & Rogers apply message vectoriza-tion, message pipelining, and reduction recognition in Id Nouveau to parallelize Simple <ref> [25] </ref>. Koelbel & Mehrotra are able to parallelize ADI integration in Kali by implicitly applying dynamic data decomposition between computation phases [19]. Olander & Schnabel show that Dino programs can be significantly improved through iteration reordering and pipelining [23].
Reference: [26] <author> R. Ruhl. </author> <title> Evaluation of compiler-generated parallel programs on three multicomputers. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Wash-ington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: They show that unbuffered messages improve overall performance for a collection of hand-parallelized scientific programs. Their studies validate the effectiveness of selected compiler optimizations for complete programs. Ruhl performed studies on a variety of parallel architectures, demonstrating excellent speedups for the Oxygen compiler <ref> [26] </ref>. Amarasinghe & Lam use precise data-flow information for arrays from last-write-trees in Suif to avoid over-communication [1]. They report speedups for Gaussian elimination without pivoting. 7 Conclusions An efficient, portable, data-parallel programming model is required to make large-scale parallel machines useful for scientific programmers.
Reference: [27] <author> C. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: The compiler partitions computation across processors using the "owner computes" rule|where each processor only computes values of data it owns [6, 12, 25]. It performs a large number of communication and parallelism optimizations based on data dependence. Details of the compilation process are presented elsewhere <ref> [13, 16, 17, 27] </ref>. 2.2 Prototype Compiler The prototype Fortran D compiler is implemented as a source-to-source Fortran translator in the context of the ParaScope parallel programming environment [9]. It utilizes existing tools for performing dependence analysis, program transformations, and interprocedural analysis. <p> For good load balance we choose a column-cyclic distribution, scattering array columns round-robin across processors. The Fortran D compiler then uses this data decomposition to derive the computation partition. Two important steps are generating proper loop bounds & indices and indexing accesses into temporary messages buffers <ref> [27] </ref>. In addition, we found using statement groups to guide guard generation and identifying max/maxloc reductions to be necessary. 3.3.1 Guard Generation Dgefa also demonstrates how statement groups may be used to guide guard generation. <p> We converted program kernels into CM Fortran by hand for the CM Fortran compiler, inserting the appropriate layout directives to achieve the same data decomposition <ref> [27] </ref>. The inner product in Livermore 3 was replaced by dotproduct, a CM Fortran intrinsic. Jacobi, Livermore 18, and Shallow can be transformed directly into CM Fortran. Loop skew and interchange were applied to SOR and Livermore 23 to expose parallelism in the form of forall loops.
References-found: 27

