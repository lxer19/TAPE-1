URL: http://www.cscs.ch/~wylie/doc/CUG93s.ps.gz
Refering-URL: 
Root-URL: 
Title: METEOROLOGICAL DATA ASSIMILATION MIGRATION TO MASSIVELY PARALLEL SYSTEMS  
Author: Brian J N Wylie and Patrick D Surry 
Address: Scotland  
Affiliation: Edinburgh Parallel Computing Centre, University of Edinburgh,  
Abstract: The observation data assimilation portion of the UK Meteorological Office unified weather forecasting and climate mod-elling code was studied by EPCC with a view to parallelisation. In the first instance, the original Fortran-77 source code (containing vector optimisations for the Cray Y-MP) was transformed via the array-language subset of Fortran-90 to CM-Fortran for the CM-200. Detailed profiling and analyses suggested performance benefits were possible from conceptually simple yet otherwise sophisticated algorithmic modifications, which were subsequently implemented. Significantly improved performance and scalability were thereby achieved from efficient implementation of effective massively parallel schemes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. S. Bell and A. Dickinson, </author> <title> The Meteorological Office Unified Model for Data Assimilation, Climate Modelling and NWP and its Implementation on a Cray Y-MP, </title> <booktitle> in Fourth Workshop on the Use of Parallel Processors in Meteorology (ECMWF), </booktitle> <year> 1990. </year>
Reference-contexts: operation on a Cray Y-MP/8 y Electronic contact: bjnw@epcc.ed.ac.uk, pds@epcc.ed.ac.uk z Prepared for the proceedings of `Scalable Computing' the Cray User Group Conference (Spring '93, Montreux) takes roughly equal time for this analysis correction as for the rest of the code, with a faster than real time daily forecast requirement. <ref> [1] </ref> The model consists of a number of rectangular grids, covering the whole globe or selective parts of it, with a number of atmospheric levels, and numerous options to tailor a simulation as required.
Reference: [2] <author> A. C. Lorenc, R. S. Bell, and B. Macpherson, </author> <title> The Meteorological Office Analysis Correction Data Assimilation Scheme, </title> <editor> Q. J. R. </editor> <title> Meteorol. </title> <journal> Soc., </journal> <volume> vol. 117, </volume> <pages> pp. 59-89, </pages> <year> 1991. </year>
Reference-contexts: These footprint shapes are distorted in a nontrivial manner by the mapping of the grid onto the globe, as well as having different radii of influence governed by the characteristic `quality' of the observation. Full details of the UKMO analysis correction scheme can be found in <ref> [2] </ref>. 3 Experiment design and characteristics The Unified Model code is continually being refined and developed, and represents a significant software investment over a number of years.
Reference: [3] <author> H. P. F. Forum, </author> <title> High Performance Fortran Language Specification, </title> <note> 1992. Draft Version 1.0 released January 1993. Available as technical report CRPC-TR 92225 from the Center for Research in Parallel Computation at Rice University, Texas. </note>
Reference-contexts: Further language developments throughout 1992 culminated in a specification for High Performance Fortran 1 (HPF), which particularly addresses the need for language extensions to ensure both performance and portability can be achieved across a range of high performance computer architectures. <ref> [3] </ref> Migration of the Unified Model to HPF via the language subset likely to be most widely and earliest supported, HPF implemented on a high-performance parallel computer, offered an attractive route to pursue.
Reference: [4] <author> Thinking Machines Corporation, </author> <title> Connection Machine Series CM-200 Technical Summary, </title> <year> 1991. </year>
Reference-contexts: The distributed memory of such systems, however, requires careful attention to data locality, since this has a pronounced impact on performance. The Connection Machine CM-200 at EPCC 1 is an example of a massively parallel system, providing a powerful computing resource and a convenient programming environment for data-parallel computing <ref> [4] </ref>. The dialect of Fortran developed with the Connection Machine, CM-Fortran [5], supports the array language of Fortran-90 and includes static array distribution and alignment directives similar to those included in HPF (and HPF).
Reference: [5] <institution> Thinking Machines Corporation, </institution> <note> CM Fortran Reference Manual, 1991. Version 1.1. </note>
Reference-contexts: The Connection Machine CM-200 at EPCC 1 is an example of a massively parallel system, providing a powerful computing resource and a convenient programming environment for data-parallel computing [4]. The dialect of Fortran developed with the Connection Machine, CM-Fortran <ref> [5] </ref>, supports the array language of Fortran-90 and includes static array distribution and alignment directives similar to those included in HPF (and HPF).
Reference: [6] <author> P. D. Surry and B. J. N. Wylie, </author> <title> High Performance Fortran Migration (HPF & HPF) via CM-Fortran, </title> <type> Technical Report EPCC-TN93-01, </type> <institution> Edinburgh Parallel Computing Centre, </institution> <year> 1993. </year>
Reference-contexts: CM-Fortran is also shown in 1 The networked CMNS CM-2 was also made available for this work, which is an older, slower model, but having a larger machine configuration which was relevant for scaling studies. 2 for application migration <ref> [6] </ref>. This path was therefore chosen as an appropriate basis for the study of the Unified Model.
Reference: [7] <author> B. J. N. Wylie and P. D. Surry, </author> <title> ACexpt Report: Profiling, Porting, Performance & Parallelisation, </title> <type> EPCC Project Report, </type> <institution> Ed-inburgh Parallel Computing Centre, </institution> <year> 1992. </year> <month> 6 </month>
References-found: 7

