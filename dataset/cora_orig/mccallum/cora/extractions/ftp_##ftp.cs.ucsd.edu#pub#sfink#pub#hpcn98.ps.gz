URL: ftp://ftp.cs.ucsd.edu/pub/sfink/pub/hpcn98.ps.gz
Refering-URL: http://www.cs.ucsd.edu/~sfink/pub.html
Root-URL: http://www.cs.ucsd.edu
Email: fjhm, barbarag@vcpc.univie.ac.at  fbaden, sfinkg@cs.ucsd.edu  
Phone: 2  
Title: Multiple data parallelism with HPF and KeLP  
Author: John H. Merlin Scott B. Baden Stephen J. Fink and Barbara M. Chapman 
Address: Vienna, Liechtensteinstr. 22, A-1090 Vienna, Austria.  San Diego, La Jolla, CA 92093-0114, USA.  
Affiliation: 1 VCPC, Institute for Software Technology and Parallel Systems, University of  Department of Computer Science and Engineering, University of California,  
Abstract: High Performance Fortran (HPF) is an effective language for implementing regular data parallel applications on distributed memory architectures, but it is not well suited to irregular, block-structured applications such as multiblock and adaptive mesh methods. A solution to this problem is to use a non-HPF SPMD program to coordinate multiple concurrent HPF tasks, each operating on a regular subgrid of an irregular data domain. To this end we have developed an interface between the C++ class library KeLP, which supports irregular, dynamic block-structured applications on distributed systems, and an HPF compiler, SHPF. This allows KeLP to handle the data layout and inter-block communications, and to invoke HPF concurrently on each block. There are a number of advantages to this approach: it combines the strengths of both KeLP and HPF; it is relatively easy to implement; and it involves no extensions to HPF or HPF compilers. This paper describes the KeLP-HPF implementation and programming model, and shows an example KeLP-HPF multiblock solver.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> High Performance Fortran Forum: </author> <title> High Performance Fortran Language Specification version 2.0. </title> <address> URL: http://www.crpc.rice.edu/HPFF/hpf2/ or http://www.vcpc.univie.ac.at/information/mirror/HPFF/hpf2/. </address>
Reference-contexts: 1 Introduction High Performance Fortran (HPF <ref> [1, 2] </ref>) provides a simple, high level programming model for implementing regular data parallel applications on distributed memory architectures. For such applications it can greatly ease program development and maintenance and provide high performance. However, HPF by itself is not well suited to irregular, block-structured data parallel applications. <p> Irregular patterns of communication must also be managed between the grids. Such a model is not currently supported by commercial HPF compilers, which offer only static uniform data decompositions. Indeed, this limitation has motivated the development of a set of `HPF 2.0 Approved Extensions' <ref> [1] </ref>. These extensions distribute the grids at runtime onto subsets of processors and specify how computations are to be mapped onto the processors [5]. This paper describes an alternative approach that does not rely on language extensions.
Reference: 2. <author> Merlin, J. H., Chapman, B. M.: </author> <title> High Performance Fortran 2.0, </title> <booktitle> in Proc. </booktitle> <institution> Sommer-schule uber Moderne Programmiersprachen und Programmiermodelle, Technical University of Hamburg-Harburg, </institution> <month> Sept 15-19, </month> <year> 1997. </year> <note> To appear. Also at URL: ftp://ftp.vcpc.univie.ac.at/vcpc/jhm/proc/hpf2.ps.gz </note>
Reference-contexts: 1 Introduction High Performance Fortran (HPF <ref> [1, 2] </ref>) provides a simple, high level programming model for implementing regular data parallel applications on distributed memory architectures. For such applications it can greatly ease program development and maintenance and provide high performance. However, HPF by itself is not well suited to irregular, block-structured data parallel applications.
Reference: 3. <author> Berger, M. J., Oliger J.: </author> <title> Adaptive Mesh Refinement for Hyperbolic Partial Differential Equations, </title> <journal> Jnl. of Computational Physics, </journal> <volume> 53, </volume> <month> 3 (March </month> <year> 1984), </year> <pages> 484-512. </pages>
Reference-contexts: However, HPF by itself is not well suited to irregular, block-structured data parallel applications. Such applications operate on domains consisting of a set of regular grids of various sizes, which are arranged irregularly and may be created dynamically. Examples include multiblock and adaptive mesh methods <ref> [3, 4] </ref>. Such applications can exploit data parallelism at two levels: within grids and between them. To exploit both types of parallelism on distributed memory systems one needs to distribute individual grids over subsets of processors, allocating different grids to different (possibly overlapping) processor subsets.
Reference: 4. <author> Berger, M. J., Colella P.: </author> <title> Local Adaptive Mesh Refinement for Shock Hydrodynamics, </title> <journal> Jnl. of Computational Physics, </journal> <volume> 82, </volume> <month> 1 (May </month> <year> 1989), </year> <pages> 64-84. </pages>
Reference-contexts: However, HPF by itself is not well suited to irregular, block-structured data parallel applications. Such applications operate on domains consisting of a set of regular grids of various sizes, which are arranged irregularly and may be created dynamically. Examples include multiblock and adaptive mesh methods <ref> [3, 4] </ref>. Such applications can exploit data parallelism at two levels: within grids and between them. To exploit both types of parallelism on distributed memory systems one needs to distribute individual grids over subsets of processors, allocating different grids to different (possibly overlapping) processor subsets.
Reference: 5. <author> Mehrotra, P., Van Rosendale, J., Zima, H.: </author> <title> High Performance Fortran: History, Status and Future, </title> <booktitle> Parallel Computing, Special Issue on Languages and Compilers for Parallel Computers, </booktitle> <year> 1997. </year>
Reference-contexts: Indeed, this limitation has motivated the development of a set of `HPF 2.0 Approved Extensions' [1]. These extensions distribute the grids at runtime onto subsets of processors and specify how computations are to be mapped onto the processors <ref> [5] </ref>. This paper describes an alternative approach that does not rely on language extensions. Instead we use an external SPMD program to coordinate multiple concurrent HPF tasks and to perform the required data movements between them. We employ the C++ class library KeLP [6, 7] as the SPMD coordination layer.
Reference: 6. <author> Fink, S. J., Baden, S. B., Kohn, S. R.: </author> <title> Flexible Communication Mechanisms for Dynamic Structured Applications, </title> <booktitle> in Proc. 3rd Int'l Workshop IRREGULAR '96 , 203-215, </booktitle> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: This paper describes an alternative approach that does not rely on language extensions. Instead we use an external SPMD program to coordinate multiple concurrent HPF tasks and to perform the required data movements between them. We employ the C++ class library KeLP <ref> [6, 7] </ref> as the SPMD coordination layer. KeLP can dynamically create and distribute multiblock domains, and it allows inter-block communication patterns to be constructed at runtime and preprocessed for efficiency. <p> Both are available in the public domain and have been installed on a wide range of parallel and distributed architectures. The reader is referred to the references provided for more details about these systems. 2.1 KeLP KeLP <ref> [6, 7] </ref> is a C++ class library built on the standard Message Passing Interface, MPI [9].
Reference: 7. <author> Fink, S. J., Kohn, S. R., Baden, S. B.: </author> <title> Efficient Run-time Support for Irregular Block-Structured Applications, </title> <booktitle> Jnl. of Parallel and Distrib. Computing, </booktitle> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: This paper describes an alternative approach that does not rely on language extensions. Instead we use an external SPMD program to coordinate multiple concurrent HPF tasks and to perform the required data movements between them. We employ the C++ class library KeLP <ref> [6, 7] </ref> as the SPMD coordination layer. KeLP can dynamically create and distribute multiblock domains, and it allows inter-block communication patterns to be constructed at runtime and preprocessed for efficiency. <p> Both are available in the public domain and have been installed on a wide range of parallel and distributed architectures. The reader is referred to the references provided for more details about these systems. 2.1 KeLP KeLP <ref> [6, 7] </ref> is a C++ class library built on the standard Message Passing Interface, MPI [9]. <p> This class allows the representation of arbitrary irregular block-structured data decompositions. The basic unit of communication in KeLP is the collective block copy operation, which copies regular sections between two (not necessarily distinct) XArrays. KeLP provides communication orchestration abstractions to express and optimize such communications, which are described in <ref> [7] </ref>. Finally, KeLP defines a parallel looping construct 1 , for all-end for all, which iterates concurrently over the Grids of an XArray. <p> This function exchanges the boundary data between all pairs of overlapping grids. Its code is `standard KeLP', that is, it is unaffected by using HPF invocations in lieu of serial Fortran 77, and a detailed explanation of it can be found in <ref> [7] </ref>. 5 Related work The work most closely related to ours|in the sense that it uses a communication library to coordinate concurrent data parallel tasks|is HPF/MPI [12], in which HPF tasks communicate by explicit message-passing in MPI.
Reference: 8. <author> Merlin, J. H., Carpenter, D. B., Hey, A. J. G.: SHPF: </author> <title> a Subset High Performance Fortran Compilation System, Fortran Journal (March / April 1996), </title> <type> 2-6. </type>
Reference-contexts: Here we have built a simple interface from KeLP to HPF. This allows KeLP to invoke multiple data parallel HPF tasks concurrently, each operating on an individual distributed grid. Our implementation of the interface works in conjunction with the public domain HPF compiler SHPF <ref> [8] </ref>. There are good reasons for using the KeLP-HPF approach for implementing `multiple data parallelism', quite apart from the current difficulties of using pure HPF for this purpose. <p> Data layout and communication operations must be performed in the single-threaded code. 1 Actually for all and end for all are macros. 4 John Merlin et al 2.2 SHPF SHPF <ref> [8] </ref> is a public domain HPF 2.0 compilation system. It comprises a translator that translates HPF into Fortran 90 SPMD code that runs on each node of the target computer, and a runtime library, ADLIB, that is called by the Fortran 90 code to perform communications.
Reference: 9. <author> Message Passing Interface Forum: </author> <title> MPI: A Message-Passing Interface Standard, </title> <type> Technical report, </type> <institution> University of Tennessee, Knoxville, TN, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: The reader is referred to the references provided for more details about these systems. 2.1 KeLP KeLP [6, 7] is a C++ class library built on the standard Message Passing Interface, MPI <ref> [9] </ref>. It supports general block decompositions via the following classes which are used to manage data decomposition at run time (where i is an integer in the range 1-4): Region i: which represents an i-dimensional index space, that is, a rectangular subset of Z i .
Reference: 10. <author> Zhang, G., Carpenter, D. B., Fox, G., Li, X., Li, X., Wen, Y.: </author> <title> PCRC-based HPF Compilation, </title> <booktitle> in 10th International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1997, </year> <note> To appear in Lecture Notes in Computer Science. </note>
Reference-contexts: It comprises a translator that translates HPF into Fortran 90 SPMD code that runs on each node of the target computer, and a runtime library, ADLIB, that is called by the Fortran 90 code to perform communications. ADLIB <ref> [10] </ref>, like KeLP, is a C++ class library that calls MPI to perform message-passing. 3 Implementation of the KeLP-SHPF interface Our programming model assumes that a KeLP program executes in SPMD mode on all processors and invokes concurrent HPF procedures 2 on subsets of processors.
Reference: 11. <author> Baden, S. B., Koelbel, C.: </author> <title> Proposed Extension for SPMD calling HPF, HPF Forum proposal, </title> <year> 1996. </year> <month> ftp://ftp.cs.ucsd.edu/pub/baden/tr/MDP/spmd-hpf.ps.gz. </month>
Reference-contexts: Many of the issues governing the coordination of multiple concurrent HPF tasks by a SPMD program are described in a proposal to the HPF Forum <ref> [11] </ref>. Multiple data parallelism with HPF and KeLP 9 4.1 An example KeLP-HPF multiblock solver The Appendix shows the KeLP source code (excluding include files, preprocessor directives and function prototypes) for a 2-dimensional multiblock solver written in KeLP-HPF.
Reference: 12. <author> Foster, I., Kohr, D., Krishnaiyer, R., Choudhary, A.: </author> <title> Double standards: Bringing task parallelism to HPF via the Message Passing Interface, </title> <booktitle> Proc. Supercomputing '96 , Pittsburg, </booktitle> <address> PA, </address> <month> November </month> <year> 1996. </year>
Reference-contexts: is, it is unaffected by using HPF invocations in lieu of serial Fortran 77, and a detailed explanation of it can be found in [7]. 5 Related work The work most closely related to ours|in the sense that it uses a communication library to coordinate concurrent data parallel tasks|is HPF/MPI <ref> [12] </ref>, in which HPF tasks communicate by explicit message-passing in MPI. If one ignores the HPF directives in an HPF/MPI program it can be read as a normal Fortran-MPI program, so this approach has both the advantages|flexibility and control|and the disadvantage|complexity|of message-passing programming.
Reference: 13. <author> Fink., S. J., Baden, S. B.: </author> <title> Runtime Support for Multi-Tier Programming of Block-Structured Applications on SMP Clusters, </title> <booktitle> in Proc. Int'l Scientific Computing in Object-Oriented Parallel Environments Conf., </booktitle> <month> Dec </month> <year> 1997, </year> <institution> Marina del Rey, </institution> <address> CA. </address>
Reference-contexts: In contrast with HPF/MPI, both of the language components of KeLP-HPF are high-level and have no explicit message-passing, and KeLP-HPF requires no changes to be made to HPF code. On a related front, KeLP has been extended to support `multi-tier' programming on SMP clusters <ref> [13] </ref>. In effect this model exposes the second level of parallelism to the programmer, with the advantage that it is more flexible than HPF, but the disadvantage of increased code complexity.
Reference: 14. <author> Chapman, B., Haines, M., Mehrotra, P., Van Rosendale, J., Zima, H.: </author> <title> Opus: A Coordination Language for Multidisciplinary Applications, </title> <institution> Sci. Prog., </institution> <year> 1997. </year>
Reference-contexts: In effect this model exposes the second level of parallelism to the programmer, with the advantage that it is more flexible than HPF, but the disadvantage of increased code complexity. Other approaches to coordinating multiple data parallel tasks are based on new languages, e.g. <ref> [14] </ref>, or compiler analysis guided by directives to extract implicit task parallelism, e.g. [15].
Reference: 15. <author> Gross, T., O'Halloran, D., Subhlok, J.: </author> <title> Task Parallelism in a High Performance Fortran Framework, </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 2 (1994), </volume> <pages> 16-26. </pages> <note> 12 John Merlin et al </note>
Reference-contexts: Other approaches to coordinating multiple data parallel tasks are based on new languages, e.g. [14], or compiler analysis guided by directives to extract implicit task parallelism, e.g. <ref> [15] </ref>.
References-found: 15

