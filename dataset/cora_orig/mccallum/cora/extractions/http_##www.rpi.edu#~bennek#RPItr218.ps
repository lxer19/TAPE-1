URL: http://www.rpi.edu/~bennek/RPItr218.ps
Refering-URL: http://www.rpi.edu/~bennek/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Feature Minimization within Decision Trees  
Author: Erin J. Bredensteiner Kristin P. Bennett 
Keyword: Key Words: Data mining, machine learning, feature minimization, decision trees, bilinear programming.  
Date: November 1995  
Affiliation: R.P.I. Math  
Pubnum: Report No. 218  
Abstract: Decision trees for classification can be constructed using mathematical programming. Within decision tree algorithms, the feature minimization problem is to construct accurate decisions using as few features or attributes within each decision as possible. Feature minimization is an important aspect of data mining since it helps identify what attributes are important and helps produce accurate and interpretable decision trees. In feature minimization with bounded accuracy, we minimize the number of features using a given misclassification error tolerance. This problem can be formulated as a parametric bilinear program and is shown to be NP-complete. A parametric Frank-Wolfe method is used to solve the bilinear subproblems. The resulting minimization algorithm produces more compact, accurate, and interpretable trees. Computational results compare favorably with a popular greedy feature elimination method as well as with a linear programming method of tree construction. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. P. Bennett. </author> <title> Decision tree construction via linear programming. </title> <editor> In M. Evans, editor, </editor> <booktitle> Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society Conference, </booktitle> <pages> pages 97-101, </pages> <address> Utica, Illinois, </address> <year> 1992. </year>
Reference-contexts: For a vector x in R n , x + will denote the vector in R n with components (x + ) i := maxfx i ; 0g; i = 1; : : :; n. The step function x fl will denote the vector in <ref> [0; 1] </ref> n with components (x fl ) i := 0 if x i 0 and (x fl ) i := 1 if x i &gt; 0; i = 1; : : : ; n. 2 Feature Minimization At each decision we are interested in finding a linear function that separates <p> In this paper, we use an error function that minimizes the average magnitude of misclassified points within each class. The underlying problem without feature minimization is a linear program. This robust linear program (RLP) [3] has been used for decision tree construction <ref> [1] </ref>. RLP combined with the greedy sequential backward elimination method for feature minimization forms the basis of a breast cancer diagnosis system [17, 16]. <p> accuracy. 4 Find the positive integer such that - = min f-jf (-) = 0g (7) where f (-) = min (w + + w )(e r) subject to 1 m eu + 1 u + A (w + w ) efl e 0 0 r e er - 2 <ref> [1; n] </ref> (8) For each fixed value of -, problem (8) finds a linear separator within a specific error rate. If for any given -, f (-) 6= 0, then no linear discriminant exists with the error tolerance that uses at most features. <p> Is there a 5 vector y 2 R n+1 such that at most - (0 &lt; - n) entries y i ; i = 1; : : :; n are nonzero and such that xy &gt; 0 for at least K vectors x? Specifically, X contains vectors of the form <ref> [A i ; 1] </ref> and [B i ; 1]. Also, y i = (w + w ) i for i = 1; : : : ; n and y n+1 = fl. Theorem 3.1 The Bounded Accuracy with Limited Features Problem is NP-complete. Proof. <p> y 2 R n+1 such that at most - (0 &lt; - n) entries y i ; i = 1; : : :; n are nonzero and such that xy &gt; 0 for at least K vectors x? Specifically, X contains vectors of the form [A i ; 1] and <ref> [B i ; 1] </ref>. Also, y i = (w + w ) i for i = 1; : : : ; n and y n+1 = fl. Theorem 3.1 The Bounded Accuracy with Limited Features Problem is NP-complete. Proof. It is easy to show that this problem is in NP.
Reference: [2] <author> K. P. Bennett and E. J. Bredensteiner. </author> <title> A parametric optimization method for machine learning. </title> <type> Math Report 217, </type> <institution> Rensselaer Polytechnic Institute, </institution> <address> Troy, New York, </address> <year> 1995. </year> <note> Submitted to ORSA Journal on Computing. </note>
Reference-contexts: RLP combined with the greedy sequential backward elimination method for feature minimization forms the basis of a breast cancer diagnosis system [17, 16]. Our feature minimization method could also be applied to algorithms that minimize the number of points misclassified such as <ref> [2, 11] </ref> or to other successful linear programming approaches [10, 15], but we leave these extensions for future work. 3 The following robust linear programming problem, RLP [3], minimizes a weighted average of the sum of the distances from the misclassified points to the separating plane. min 1 k ev subject <p> Some possibilities are to apply branch and bound techniques, cutting plane methods, or the Frank-Wolfe method. The approach implemented in this paper uses a Frank-Wolfe type algorithm used successfully to solve bilinear programs in <ref> [4, 2] </ref>. This algorithm reduces the original bilinear program into two linear programs. One of these linear programs has a closed form solution as shown in [2]. <p> The approach implemented in this paper uses a Frank-Wolfe type algorithm used successfully to solve bilinear programs in [4, 2]. This algorithm reduces the original bilinear program into two linear programs. One of these linear programs has a closed form solution as shown in <ref> [2] </ref>. A complete description of our algorithm is given in the following two subsections. 4.1 Bilinear Subproblems The parametric bilinear programming formulation (8) is an uncoupled bilinear program. <p> For each a series of linear programs 7 must be solved, thus it is computationally valuable to solve for as few values of as possible. We have used a modification of the secant method, similar to that used in <ref> [2] </ref>, in the following algorithm: Algorithm 4.2 (Feature Minimization with Bounded Accuracy) Let max denote the smallest number of features such that the error tolerance is satisfied thus far. Let min denote the largest number of features attempted so far in Algorithm 4.1 such that the error tolerance is violated.
Reference: [3] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Neural network training via linear programming. </title> <editor> In P. M. Pardalos, editor, </editor> <booktitle> Advances in Optimization and Parallel Computing, </booktitle> <pages> pages 56-67, </pages> <address> Amsterdam, 1992. </address> <publisher> North Holland. </publisher> <pages> 14 </pages>
Reference-contexts: Our formulations of the feature minimization problem can be applied to many different error functions. In this paper, we use an error function that minimizes the average magnitude of misclassified points within each class. The underlying problem without feature minimization is a linear program. This robust linear program (RLP) <ref> [3] </ref> has been used for decision tree construction [1]. RLP combined with the greedy sequential backward elimination method for feature minimization forms the basis of a breast cancer diagnosis system [17, 16]. <p> Our feature minimization method could also be applied to algorithms that minimize the number of points misclassified such as [2, 11] or to other successful linear programming approaches [10, 15], but we leave these extensions for future work. 3 The following robust linear programming problem, RLP <ref> [3] </ref>, minimizes a weighted average of the sum of the distances from the misclassified points to the separating plane. min 1 k ev subject to u + Aw efl e 0 v Bw + efl e 0 (4) We are interested in minimizing the number of features at each decision in
Reference: [4] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Bilinear separation of two sets in n-space. </title> <journal> Computational Optimization and Applications, </journal> <volume> 2 </volume> <pages> 207-227, </pages> <year> 1993. </year>
Reference-contexts: We then prove in Section 3 that our feature minimization problem is NP-complete. In Section 4, we propose an algorithm based on the Frank-Wolfe method discussed in <ref> [4] </ref> for solving the parametric bilinear programming problem. Section 5 contains a computational comparison of our feature minimization method to GSBE and a linear programming approach to decision tree construction. Results on a number of practical problems are given. The following notation is used. <p> Some possibilities are to apply branch and bound techniques, cutting plane methods, or the Frank-Wolfe method. The approach implemented in this paper uses a Frank-Wolfe type algorithm used successfully to solve bilinear programs in <ref> [4, 2] </ref>. This algorithm reduces the original bilinear program into two linear programs. One of these linear programs has a closed form solution as shown in [2]. <p> It has been shown that a Frank-Wolfe algorithm [8] applied to an uncoupled bilinear program will converge to a global solution or a stationary point <ref> [4] </ref>.
Reference: [5] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International, </booktitle> <address> California, </address> <year> 1984. </year>
Reference-contexts: This process can be used to construct a decision tree to recognize any two disjoint sets with 100% accuracy. However the resulting tree frequently overfits the data causing poor generalization. Thus we must simplify or prune the tree. The pruning method chosen is Error Complexity pruning <ref> [5] </ref> in which 10% of the training set is reserved specifically for pruning. Decision tree growth is terminated when each leaf contains either a single class or both classes where one class has less than 1% of the total points represented.
Reference: [6] <author> C. E. Brodley and P. E. Utgoff. </author> <title> Multivariate decision trees. </title> <journal> Machine Learning, </journal> <volume> 19(1) </volume> <pages> 45-77, </pages> <year> 1995. </year>
Reference-contexts: The goal of feature minimization is to construct good decisions using as few features as possible. By minimizing the number of features used at each decision, understandability of the resulting tree is increased and the number of data evaluations is decreased <ref> [6] </ref>. Feature minimization is not necessary in univariate decision tree algorithms in which each decision in the tree is based on a single feature or attribute. Note that in this paper we use the term feature and attribute interchangeably. <p> We will be using mathematical programming methods to construct the decisions. In contrast, other common approaches to feature minimization are based on heuristics. Sequential Backward Elimination (SBE) and Sequential Forward Elimination (SFE) <ref> [6] </ref> involve searching the feature space for features that do not contribute (SBE) or contribute (SFE) to the quality of the decision. In SBE an initial discriminant function is constructed using all of the features and then features are removed sequentially from the problem until some stopping criterion is satisfied. <p> In our effort to achieve this goal, we propose removing from the problem by bounding the error function in a constraint. Problem (7) removes features while maintaining accuracy within some tolerance, ffi. A similar concept was used by <ref> [6] </ref> and [16] in their feature elimination methods. In [6], feature elimination was allowed to continue as long as a specific error tolerance was maintained. Street [16] computed planes for all feature counts and then used a tuning set to determine the best plane. <p> In our effort to achieve this goal, we propose removing from the problem by bounding the error function in a constraint. Problem (7) removes features while maintaining accuracy within some tolerance, ffi. A similar concept was used by <ref> [6] </ref> and [16] in their feature elimination methods. In [6], feature elimination was allowed to continue as long as a specific error tolerance was maintained. Street [16] computed planes for all feature counts and then used a tuning set to determine the best plane.
Reference: [7] <institution> CPLEX Optimization Incorporated, </institution> <address> Incline Village, Nevada. Using the CPLEX Callable Library, </address> <year> 1994. </year>
Reference-contexts: Computational results on single linear separators and decision trees are contained in section 5.2. 8 5.1 Experimental Method Feature Minimization results are compared to the LP (4) and our implementation of GSBE as described below. Each method utilizes the CPLEX 3.0 <ref> [7] </ref> solver to optimize the linear subproblems. To estimate generalization or accuracy on future data, 10-fold cross validation was used to evaluate the testing set accuracies. The original data set is split into ten equal parts.
Reference: [8] <author> M. Frank and P. Wolfe. </author> <title> An algorithm for quadratic programming. </title> <journal> Naval Research Logistics Quarterly, </journal> <volume> 3 </volume> <pages> 95-110, </pages> <year> 1956. </year>
Reference-contexts: One of these linear programs has a closed form solution as shown in [2]. A complete description of our algorithm is given in the following two subsections. 4.1 Bilinear Subproblems The parametric bilinear programming formulation (8) is an uncoupled bilinear program. It has been shown that a Frank-Wolfe algorithm <ref> [8] </ref> applied to an uncoupled bilinear program will converge to a global solution or a stationary point [4].
Reference: [9] <author> M.R. Garey and D.S. Johnson. </author> <title> Computers and Intractability, A Guide to the Theory of NP-Completeness. W.H. </title> <publisher> Freeman and Company, </publisher> <address> San Francisco, </address> <year> 1979. </year>
Reference-contexts: To show that the above problem is NP-complete the Open Hemisphere problem of <ref> [9] </ref> can be easily transformed into a single instance of the bounded accuracy with limited features problem. The Open Hemisphere problem is the problem of determining if there is a vector y such that xy &gt; 0 for at least K vectors x 2 X .
Reference: [10] <author> F. Glover. </author> <title> Improved linear programming models for discriminant analysis. </title> <journal> Decision Sciences, </journal> <volume> 21 </volume> <pages> 771-785, </pages> <year> 1990. </year>
Reference-contexts: Our feature minimization method could also be applied to algorithms that minimize the number of points misclassified such as [2, 11] or to other successful linear programming approaches <ref> [10, 15] </ref>, but we leave these extensions for future work. 3 The following robust linear programming problem, RLP [3], minimizes a weighted average of the sum of the distances from the misclassified points to the separating plane. min 1 k ev subject to u + Aw efl e 0 v Bw
Reference: [11] <author> O. L. Mangasarian. </author> <title> Misclassification minimization. </title> <journal> Journal of Global Optimization, </journal> <volume> 5 </volume> <pages> 309-332, </pages> <year> 1994. </year>
Reference-contexts: RLP combined with the greedy sequential backward elimination method for feature minimization forms the basis of a breast cancer diagnosis system [17, 16]. Our feature minimization method could also be applied to algorithms that minimize the number of points misclassified such as <ref> [2, 11] </ref> or to other successful linear programming approaches [10, 15], but we leave these extensions for future work. 3 The following robust linear programming problem, RLP [3], minimizes a weighted average of the sum of the distances from the misclassified points to the separating plane. min 1 k ev subject <p> The step function is removed from problem (5) using properties found in <ref> [11] </ref> and [12]. The details are contained in the appendix. <p> A Removal of the Step Function The following equivalence relation will be used to transform the step function from program (5): 13 Proposition A.1 (Characterization of the Step Function) <ref> [11] </ref>, [12] r = (a) fl u = (a) + () (r; u) 2 arg min er subject to: u = r u + a ! In addition, the elementary relationship in Proposition A.2 will be useful in the removal of the plus function: Proposition A.2 [11] c = d + <p> of the Step Function) <ref> [11] </ref>, [12] r = (a) fl u = (a) + () (r; u) 2 arg min er subject to: u = r u + a ! In addition, the elementary relationship in Proposition A.2 will be useful in the removal of the plus function: Proposition A.2 [11] c = d + () c 0 c d 0 c (c d) = 0 In order to apply these properties to problem (5) we let r and u be as follows: r = (w + + w ) fl (10) Thus from property A.1: r = (r u +
Reference: [12] <author> O. L. Mangasarian. </author> <title> Mathematical programming in machine learning. </title> <type> Technical Report 95-06, </type> <institution> University of Wisconsin, Madison, Wisconsin, </institution> <year> 1995. </year> <title> Submitted to Proceedings of Nonlinear Optimization and Applications Workshop, </title> <address> June 1995, </address> <publisher> Plenum Press. </publisher>
Reference-contexts: The step function is removed from problem (5) using properties found in [11] and <ref> [12] </ref>. The details are contained in the appendix. <p> A Removal of the Step Function The following equivalence relation will be used to transform the step function from program (5): 13 Proposition A.1 (Characterization of the Step Function) [11], <ref> [12] </ref> r = (a) fl u = (a) + () (r; u) 2 arg min er subject to: u = r u + a ! In addition, the elementary relationship in Proposition A.2 will be useful in the removal of the plus function: Proposition A.2 [11] c = d + ()
Reference: [13] <author> P.M. Murphy and D.W. Aha. </author> <title> UCI repository of machine learning databases. </title> <institution> Department of Information and Computer Science, University of California, Irvine, California, </institution> <year> 1992. </year>
Reference-contexts: The data sets used in the computational experiments are listed below. All of these data sets except the Star/Galaxy Database and the Plastics data set are available via anonymous file transfer protocol (ftp) from the University of California Irvine UCI Repository of Machine Learning Databases <ref> [13] </ref>. Cleveland Heart Disease Database The Cleveland Heart Disease Database has 297 patients listed with 13 numeric attributes. Each patient is classified as to 9 whether there is presence or absence of heart disease. There are 137 patients who have a presence of heart disease.
Reference: [14] <author> S. Odewahn, E. Stockwell, R. Pennington, R Humphreys, and W Zumach. </author> <title> Automated star/galaxy discrimination with neural networks. </title> <journal> Astronomical Journal, </journal> <volume> 103(1) </volume> <pages> 318-331, </pages> <year> 1992. </year>
Reference-contexts: Each example represents a star or a galaxy and is described by 14 numeric attributes. The bright data set is nearly linearly separable. These two data sets are generated from a large set of star and galaxy images collected by Odewahn <ref> [14] </ref> at the University of Minnesota. BUPA liver disorders The BUPA data set contains 345 single male patients with 6 numeric attributes. Five of these attributes are blood tests which are thought to be relevant to liver disorders.
Reference: [15] <author> A. Roy, L. S. Kim, and S. Mukhopadhyay. </author> <title> A polynomial time algorithm for the construction and training of a class of multilayer perceptrons. </title> <booktitle> Neural Networks, </booktitle> <volume> 6 </volume> <pages> 535-545, </pages> <year> 1993. </year>
Reference-contexts: Our feature minimization method could also be applied to algorithms that minimize the number of points misclassified such as [2, 11] or to other successful linear programming approaches <ref> [10, 15] </ref>, but we leave these extensions for future work. 3 The following robust linear programming problem, RLP [3], minimizes a weighted average of the sum of the distances from the misclassified points to the separating plane. min 1 k ev subject to u + Aw efl e 0 v Bw
Reference: [16] <author> W.N. </author> <title> Street. Cancer diagnosis and prognosis via linear-programming-based machine learning. </title> <type> Technical Report 94-14, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <month> August </month> <year> 1994. </year> <type> Ph.D. thesis. </type>
Reference-contexts: The underlying problem without feature minimization is a linear program. This robust linear program (RLP) [3] has been used for decision tree construction [1]. RLP combined with the greedy sequential backward elimination method for feature minimization forms the basis of a breast cancer diagnosis system <ref> [17, 16] </ref>. <p> In our effort to achieve this goal, we propose removing from the problem by bounding the error function in a constraint. Problem (7) removes features while maintaining accuracy within some tolerance, ffi. A similar concept was used by [6] and <ref> [16] </ref> in their feature elimination methods. In [6], feature elimination was allowed to continue as long as a specific error tolerance was maintained. Street [16] computed planes for all feature counts and then used a tuning set to determine the best plane. <p> Problem (7) removes features while maintaining accuracy within some tolerance, ffi. A similar concept was used by [6] and <ref> [16] </ref> in their feature elimination methods. In [6], feature elimination was allowed to continue as long as a specific error tolerance was maintained. Street [16] computed planes for all feature counts and then used a tuning set to determine the best plane.
Reference: [17] <author> W.H. Wolberg, W. N. </author> <title> Street, and O.L. Mangasarian. Image analysis and machine learning applied to breast cancer diagnosis and prognosis. </title> <journal> Quantitative Cytology and Histology, </journal> <volume> 17(2) </volume> <pages> 77-87, </pages> <year> 1995. </year> <month> 15 </month>
Reference-contexts: The underlying problem without feature minimization is a linear program. This robust linear program (RLP) [3] has been used for decision tree construction [1]. RLP combined with the greedy sequential backward elimination method for feature minimization forms the basis of a breast cancer diagnosis system <ref> [17, 16] </ref>.
References-found: 17

