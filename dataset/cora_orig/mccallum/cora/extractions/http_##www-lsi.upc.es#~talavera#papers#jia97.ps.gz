URL: http://www-lsi.upc.es/~talavera/papers/jia97.ps.gz
Refering-URL: http://www-lsi.upc.es/~talavera/papers.html
Root-URL: 
Email: talavera@lsi.upc.es  
Title: From unsupervised learning to data mining: linking cognition and data analysis  
Author: Luis Talavera 
Keyword: Unsupervised learning, Knowledge Discovery, Data Mining, Clustering, Cognitive Psychology.  
Address: Campus Nord, Modul C6, Jordi Girona 3 08034 Barcelona, Catalonia, Spain  
Affiliation: Departament de Llenguatges i Sistemes Informatics Universitat Politecnica de Catalunya  
Abstract: Recently, Knowledge Discovery on Databases (KDD) has emerged as a promising research area encompassing methods from several disciplines. Particularly, the data mining step of KDD shares most of its goals with unsupervised learning. But data mining methods are biased towards statistical techniques arguing that Machine Learning (ML) methods are not suitable to deal with real-world databases. We claim that (a) the problems of ML systems in dealing with databases may come from their traditional symbolic nature, (b) some ML challenges may be interpreted from a data mining standpoint, and, as a consequence, (c) ML methods which make use of statistical concepts may be good candidates to solve data mining problems, still incorporating characteristic symbolic biases. Conversely, we also suggest the use of ML biases to constraint existing data mining techniques, thus bridging purely statistical techniques and more heuristic -and cognitive-inspired-ML ones. The unsupervised learning system ISAAC is described to show an application of the proposed ideas. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ahn, W. & Medin, D. L. </author> <year> (1992). </year> <title> A two-stage model of category construction. </title> <journal> Cognitive Science, </journal> <volume> (16), </volume> <pages> 81-121. </pages>
Reference-contexts: Selective attention. Psychologists have observed a tendency in humans to focus their cognitive effort on some properties when creating categories. This behavior is often referred to as selective attention in Cognitive Psychology and sometimes results in a very strong bias since humans tend to form even unidimensional categories <ref> (Ahn & Medin, 1992) </ref>. This fact has inspired some research in order to incorporate similar constraints to unsupervised learning systems. Gennari's work in focused concept formation (Gennari, 1989) moves along these lines incorporating an attentional mechanism in COBWEB in order to improve the efficiency of the system.
Reference: <author> Bejar, J. </author> <year> (1995). </year> <institution> Adquisicion automatica de conocimiento en dominios poco estructurados. </institution> <type> PhD thesis, </type> <institution> Facultat d'Informatica de Barcelona, UPC. </institution>
Reference: <author> Brodley, C. & Smyth, P. </author> <year> (1995). </year> <title> The process of applying machine learning algorithms. </title> <editor> In Aha, D. & Riddle, P. (Eds.), </editor> <booktitle> Working Notes for Applying Machine Learning in Practice: A Workshop at the Twelfth International Machine Learning Conference, </booktitle> <address> Washing-ton, DC. </address>
Reference-contexts: Actually, the need of improving an existing model to solve a problem is not an specific drawback in the application of ML to data mining problems, but a general requirement in the process of applying ML algorithms <ref> (Brodley & Smyth, 1995) </ref>. Secondly, we cannot assume that every data mining problem has a cognitive-inspired counterpart in symbolic learning. Rather, some problems must be approached by directly using statistical principles, although still combined with heuristics and control strategies characteristic of symbolic AI approaches.
Reference: <author> Cheeseman, P., Kelly, J., Self, M., Stutz, J., Taylor, W., & Freeman, D. </author> <year> (1988). </year> <title> AutoClass: A bayesian classification system. </title> <booktitle> In Proceedings of the Fifth International Workshop on Machine Learning, </booktitle> <pages> (pp. 54-64). </pages> <publisher> Morgan Kauffmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: But it is not necessary to restrict the view of concepts to logical descriptions. Rather, as shown in several unsupervised learning systems, other formalisms fit well into the basic framework of symbolic learning. There are several examples of this tendency such as COBWEB (Fisher, 1987), AUTOCLASS <ref> (Cheeseman et al., 1988) </ref> or WITT (Hanson, 1990), which make use of typical statistical topics such as conditional probabilities, Bayesian theory or statistical correlations, but are not by themselves straight statistical methods. In fact, many times there is a cognitive rationale behind the use of statistical principles.
Reference: <author> Devaney, M. & Ram, A. </author> <year> (1997). </year> <title> Efficient feature selection in conceptual clustering. </title> <booktitle> In Machine Learning: Proceedings of the Fourteenth International Conference, </booktitle> <address> Nashville, TN. </address> <note> (To appear). </note>
Reference-contexts: Data mining Unsupervised learning imperfect information probabilistic concepts database size hill-climbing and incremental learning high dimensionality selective attention user interaction bias selection Table 1: Correspondence between data mining problems and unsupervised learning topics in COBWEB <ref> (Devaney & Ram, 1997) </ref>. Additional work outside the COBWEB framework includes the ISAAC system (Talavera & Cortes, 1996) which computes attribute relevances in order to guide induction.
Reference: <author> Fayyad, U. M., Piatetsky-Shapiro, G., & Smyth, P. </author> <year> (1996). </year> <title> From data mining to knowledge discovery: An overview. </title> <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, & R. Uthurusamy (Eds.), </editor> <booktitle> Advances in Knowledge Discovery and Data Mining (pp. </booktitle> <pages> 1-34). </pages> <address> Cambridge, Massachusetts: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: A related commonly used term is data mining, which some authors use to denote the application of algorithms for extracting patterns from 1 data, thus considering these methods as a part of the KDD process. Typical data mining methods are classification, regression, clustering, summarization, dependency modeling and change detection <ref> (Fayyad, Piatetsky-Shapiro & Smyth, 1996) </ref>. Both, inductive learning and data mining share the common goal of extracting information (or knowledge) from data. Thus, inductive learning systems appear as good candidates for data mining tasks.
Reference: <author> Fisher, D. </author> <year> (1995). </year> <title> Optimization and simplification of hierarchical clusterings. </title> <booktitle> In Proceedings of the First International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> (pp. 118-123)., </pages> <address> Montreal, Que-bec, Canada. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: The ideas presented here are not completely new in the ML community. Fisher's work in iterative optimization of hierarchical clusterings <ref> (Fisher, 1995) </ref> is an example of how a learning system designed from some psychological findings (COBWEB) can be profitably extended to serve as a data analysis tool.
Reference: <author> Fisher, D. & Chan, P. </author> <year> (1990). </year> <title> Statistical guidance in symbolic learning. </title> <journal> Annals of Mathematics and Artificial Intelligence, </journal> <volume> (2), </volume> <pages> 135-148. </pages>
Reference-contexts: Rather, some problems must be approached by directly using statistical principles, although still combined with heuristics and control strategies characteristic of symbolic AI approaches. As an example, consider the application of statistical tests in concept tree pruning strategies within the COBWEB framework <ref> (Fisher & Chan, 1990) </ref> aimed to reduce the noise sensitivity of the system. In the light of this brief account of the relationship between the unsupervised learning topics described and data ming problems, two main and related conclusions can be drawn.
Reference: <author> Fisher, D. & Pazzani, M. </author> <year> (1991). </year> <title> Concept formation in context. </title> <editor> In D. Fisher, M. Pazzani, & P. Langley (Eds.), </editor> <title> Concept Formation: </title> <journal> Knowledge and Experience in unsupervised learning (pp. </journal> <pages> 307-322). </pages> <address> San Ma-teo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Application areas of unsupervised learning include discovery, problem solving, planning, engineering, natural language and information retrieval <ref> (Fisher & Pazzani, 1991) </ref>. On the other hand, the increasing automation of industrial and business activities produces a rapidly growing stream of data which has become too huge to be analyzed by manual methods.
Reference: <author> Fisher, D. H. </author> <year> (1987). </year> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> (2), </volume> <pages> 139-172. </pages>
Reference-contexts: But it is not necessary to restrict the view of concepts to logical descriptions. Rather, as shown in several unsupervised learning systems, other formalisms fit well into the basic framework of symbolic learning. There are several examples of this tendency such as COBWEB <ref> (Fisher, 1987) </ref>, AUTOCLASS (Cheeseman et al., 1988) or WITT (Hanson, 1990), which make use of typical statistical topics such as conditional probabilities, Bayesian theory or statistical correlations, but are not by themselves straight statistical methods. In fact, many times there is a cognitive rationale behind the use of statistical principles.
Reference: <author> Frawley, W. J., Piatetsky-Shapiro, G., & Matheus, C. J. </author> <year> (1991). </year> <title> Knowledge discovery in databases. </title> <editor> In G. Piatetsky-Shapiro & W. J. Frawley (Eds.), </editor> <title> Knowledge Discovery in databases: an overview (pp. </title> <address> 1-27). Cambridge, Massachusetts: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Particularly, they claim that many ML algorithms assume that a small, well structured, error-free dataset from which learning takes place exists. Using a database as a training set for ML systems may cause several problems as databases contain data generated for purposes other than learning <ref> (Frawley et al., 1991, Holsheimer & Siebes, 1994) </ref>. The first problem is the size of databases. A data mining system is expected to deal with large databases, containing large data volumes.
Reference: <author> Gennari, J. </author> <year> (1989). </year> <title> Focused concept formation. </title> <booktitle> In Proceedings of the Fifth International Workshop on Machine Learning, </booktitle> <pages> (pp. 379-382). </pages> <publisher> Morgan Kauff-mann. </publisher>
Reference-contexts: This fact has inspired some research in order to incorporate similar constraints to unsupervised learning systems. Gennari's work in focused concept formation <ref> (Gennari, 1989) </ref> moves along these lines incorporating an attentional mechanism in COBWEB in order to improve the efficiency of the system. More recent research has studied other methods for feature selection 1 However, it has been suggested that low typicality could be an indicator of noisy instances (Talavera, 1996).
Reference: <author> Gennari, J. H., Langley, P., & Fisher, D. </author> <year> (1989). </year> <title> Models of incremental concept formation. </title> <journal> Artificial Intelligence, </journal> <volume> (40), </volume> <pages> 11-61. </pages>
Reference-contexts: This fact has inspired some research in order to incorporate similar constraints to unsupervised learning systems. Gennari's work in focused concept formation <ref> (Gennari, 1989) </ref> moves along these lines incorporating an attentional mechanism in COBWEB in order to improve the efficiency of the system. More recent research has studied other methods for feature selection 1 However, it has been suggested that low typicality could be an indicator of noisy instances (Talavera, 1996).
Reference: <editor> Gordon, D. F. & Desjardins, M. </editor> <year> (1995). </year> <title> Evaluation and selection of biases in machine learning. </title> <booktitle> Machine Learning, </booktitle> <pages> 20 (1-2), 5-22. </pages>
Reference-contexts: Inductive bias. There is a potentially infinite number of inductive hypothesis that can be formulated, but only a small subset make sense for humans. These preferences are implemented in ML algorithms as a bias, considering a bias as any factor that influences the formulation and selection of inductive hypothesis <ref> (Gordon & Desjardins, 1995) </ref>. Some research has been oriented towards the design of autonomous systems thus incorporating only internal biases (e.g. COBWEB). Other, allow the user to specify certain preferences about the concepts to be created.
Reference: <author> Hanson, S. J. </author> <year> (1990). </year> <title> Conceptual clustering and categorization: Bridging the gap between induction and causal models. </title> <editor> In R. S. Michalski & Y. Kodratoff (Eds.), </editor> <booktitle> Machine Learning: An Artificial Ingelligence Appproach (Volume III) chapter 9, </booktitle> <pages> (pp. 235-268). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kauffmann. </publisher>
Reference-contexts: Rather, as shown in several unsupervised learning systems, other formalisms fit well into the basic framework of symbolic learning. There are several examples of this tendency such as COBWEB (Fisher, 1987), AUTOCLASS (Cheeseman et al., 1988) or WITT <ref> (Hanson, 1990) </ref>, which make use of typical statistical topics such as conditional probabilities, Bayesian theory or statistical correlations, but are not by themselves straight statistical methods. In fact, many times there is a cognitive rationale behind the use of statistical principles.
Reference: <author> Holsheimer, M. & Siebes, A. </author> <year> (1994). </year> <title> Data mining: the search for knowledge in databases. </title> <type> Technical Report CS-R9406, </type> <institution> Computer Science/Department of Al-gorithmics and Architecture, CWI. </institution>
Reference: <author> Lopez de Mantaras, R. </author> <year> (1991). </year> <title> A distance based attribute selection measure for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> (6), </volume> <pages> 81-92. </pages>
Reference-contexts: New abstract concept descriptions are generated so that they contain only the subset of attributes considered useful. This is done in the Reflection stage, which uses a relevance measure formerly used in attribute selection in decision trees, called the distance measure <ref> (Lopez de Mantaras, 1991) </ref>, and the current set of hypothesis, to compute a relevance for each individual attribute. The next generalization step is performed using only the useful attributes subset and weighting calculations with the computed attribute relevances.
Reference: <author> McEwen, M. </author> <year> (1996). </year> <title> ML experiments with an accuracy-efficiency trade off in unsupervised attribute prediction. </title> <type> Master's thesis, </type> <institution> University of Aberdeen, Computer Science Department. </institution>
Reference-contexts: Also, some experiments have been made for automating the selection of the N G parameter for measurable performance goals (e.g. predictive accuracy), using cross-validation techniques to estimate performance <ref> (McEwen, 1996, Talavera & Cortes, 1997b) </ref>.
Reference: <author> Medin, D. L. </author> <year> (1989). </year> <title> Concepts and conceptual structure. </title> <journal> American Psychologist, </journal> <volume> 44 (12), </volume> <pages> 1469-1481. </pages>
Reference-contexts: The three stages explained above try to model the assumption that humans often make use of contextual information in categorization processes. This information comes from background knowledge which reflects previous experiences and beliefs <ref> (Medin, 1989) </ref>. But in some situations there is no previous experience in a given domain and background knowledge may not be available. In 6 such a case, an inductive system may either per-form a completely uninformed clustering or try to obtain some information during the process to guide induction. <p> In addition, the attribute selection procedure implements an attentional mechanism in the system which, as mentioned earlier, is cognitively well-justified. Also, there is some cognitive basis in using some simple initial classification procedure assuming that the results could indicate more interesting relationships <ref> (Medin & Ortony, 1989) </ref>. To summarize, ISAAC exemplifies a system which incorporates some traditional unsupervised symbolic learning biases, and, at the same time, solve important data mining problems as database size, dimensionality or user interaction. Furthermore, ISAAC posses other features that are well suited to the data mining process.
Reference: <author> Medin, D. L. & Ortony, A. </author> <year> (1989). </year> <editor> Psychological essentialism. In S. Vosniadou & A. Ortony (Eds.), </editor> <title> Similarity and analogical reasoning. </title> <address> New York: </address> <publisher> Cambridge University Press. </publisher>
Reference-contexts: The three stages explained above try to model the assumption that humans often make use of contextual information in categorization processes. This information comes from background knowledge which reflects previous experiences and beliefs <ref> (Medin, 1989) </ref>. But in some situations there is no previous experience in a given domain and background knowledge may not be available. In 6 such a case, an inductive system may either per-form a completely uninformed clustering or try to obtain some information during the process to guide induction. <p> In addition, the attribute selection procedure implements an attentional mechanism in the system which, as mentioned earlier, is cognitively well-justified. Also, there is some cognitive basis in using some simple initial classification procedure assuming that the results could indicate more interesting relationships <ref> (Medin & Ortony, 1989) </ref>. To summarize, ISAAC exemplifies a system which incorporates some traditional unsupervised symbolic learning biases, and, at the same time, solve important data mining problems as database size, dimensionality or user interaction. Furthermore, ISAAC posses other features that are well suited to the data mining process.
Reference: <author> Michalski, R. S. </author> <year> (1983). </year> <title> A theory and methodology of inductive learning. </title> <editor> In R. S. Michalski, J. G. Car-bonell, & T. M. Mitchell (Eds.), </editor> <booktitle> Machine Learning: An Artificial intelligence approach (pp. </booktitle> <pages> 83-134). </pages> <address> Los Altos, CA: </address> <publisher> Morgan Kauffmann. </publisher>
Reference-contexts: 1 Introduction Inductive concept learning <ref> (Michalski, 1983) </ref> is one of the most widely studied areas in Machine Learning (ML). From a cognitive standpoint, it is very important for an intelligent agent to have the ability of grouping and characterizing observations from a given environment. <p> Logic also provides well-defined inference methods that represent an intuitive manner of dealing with symbolic information. This tendency was followed, for example, in one of the first approaches to unsupervised learning, the CLUSTER/2 system <ref> (Michalski & Stepp, 1983) </ref>. This system is the original implementation of the conceptual clustering ideas. This approach advocates for a tight coupling of the interpretation task (carried out by an external analyst in statistical approaches) and the clustering processes.
Reference: <author> Michalski, R. S. & Stepp, R. E. </author> <year> (1983). </year> <title> Learning from observation: Conceptual clustering. </title> <editor> In R. </editor> <publisher> S. </publisher>
Reference-contexts: 1 Introduction Inductive concept learning <ref> (Michalski, 1983) </ref> is one of the most widely studied areas in Machine Learning (ML). From a cognitive standpoint, it is very important for an intelligent agent to have the ability of grouping and characterizing observations from a given environment. <p> Logic also provides well-defined inference methods that represent an intuitive manner of dealing with symbolic information. This tendency was followed, for example, in one of the first approaches to unsupervised learning, the CLUSTER/2 system <ref> (Michalski & Stepp, 1983) </ref>. This system is the original implementation of the conceptual clustering ideas. This approach advocates for a tight coupling of the interpretation task (carried out by an external analyst in statistical approaches) and the clustering processes.
Reference: <editor> Michalski, J. G. Carbonell, & T. M. Mitchell (Eds.), </editor> <booktitle> 8 Machine Learning: An Artificial intelligence approach (pp. </booktitle> <pages> 331-363). </pages> <address> Los Altos, CA: </address> <publisher> Morgan Kauffmann. </publisher>
Reference: <author> Smith, E. E. & Medin, D. L. </author> <year> (1981). </year> <title> Categories and concepts. </title> <publisher> Cambridge,MA: Harvard University Press. </publisher>
Reference: <author> Talavera, L. </author> <year> (1996). </year> <institution> Reflexion y refinamiento del conocimiento en la formacion de conceptos. </institution> <type> Master's thesis, </type> <institution> Facultat d'Informatica de Barcelona, UPC. </institution>
Reference-contexts: More recent research has studied other methods for feature selection 1 However, it has been suggested that low typicality could be an indicator of noisy instances <ref> (Talavera, 1996) </ref>. Data mining Unsupervised learning imperfect information probabilistic concepts database size hill-climbing and incremental learning high dimensionality selective attention user interaction bias selection Table 1: Correspondence between data mining problems and unsupervised learning topics in COBWEB (Devaney & Ram, 1997). <p> Data mining Unsupervised learning imperfect information probabilistic concepts database size hill-climbing and incremental learning high dimensionality selective attention user interaction bias selection Table 1: Correspondence between data mining problems and unsupervised learning topics in COBWEB (Devaney & Ram, 1997). Additional work outside the COBWEB framework includes the ISAAC system <ref> (Talavera & Cortes, 1996) </ref> which computes attribute relevances in order to guide induction. Although data mining and symbolic learning methods may view the problem of reducing the number of terms used in the induction process under a very different light, essentially, both approaches try to solve a similar question. <p> Also, some experiments have been made for automating the selection of the N G parameter for measurable performance goals (e.g. predictive accuracy), using cross-validation techniques to estimate performance <ref> (McEwen, 1996, Talavera & Cortes, 1997b) </ref>.
Reference: <author> Talavera, L. </author> <year> (1997). </year> <title> Using background knowledge to guide conceptual clustering: an attribute relevance based approach. </title> <type> Unpublished manuscript. </type>
Reference-contexts: more inclined to use statistical methods, they are perfectly coupled with other cognitive related features, thus effectively showing the plausibility of coupling symbolic learning and statistical principles to solve complex problems. 2 Actually, some initial work on using prior knowledge in the form of attribute relevances has being carried out <ref> (Talavera, 1997) </ref>. 5 Concluding remarks Data mining is an emerging field which encloses topics of several disciplines and faces the task of extracting useful information from large, noisy and uncertain data.
Reference: <author> Talavera, L. & Cortes, U. </author> <year> (1996). </year> <editor> Generalizacion y atencion selectiva para la formacion de conceptos. </editor> <booktitle> In V Congreso Iberoamericano de Inteligencia Artificial, IBERAMIA96, </booktitle> <pages> (pp. 320-330)., </pages> <address> Cholula, Puebla, Mex-ico. </address> <publisher> Limusa, Mexico. </publisher>
Reference-contexts: More recent research has studied other methods for feature selection 1 However, it has been suggested that low typicality could be an indicator of noisy instances <ref> (Talavera, 1996) </ref>. Data mining Unsupervised learning imperfect information probabilistic concepts database size hill-climbing and incremental learning high dimensionality selective attention user interaction bias selection Table 1: Correspondence between data mining problems and unsupervised learning topics in COBWEB (Devaney & Ram, 1997). <p> Data mining Unsupervised learning imperfect information probabilistic concepts database size hill-climbing and incremental learning high dimensionality selective attention user interaction bias selection Table 1: Correspondence between data mining problems and unsupervised learning topics in COBWEB (Devaney & Ram, 1997). Additional work outside the COBWEB framework includes the ISAAC system <ref> (Talavera & Cortes, 1996) </ref> which computes attribute relevances in order to guide induction. Although data mining and symbolic learning methods may view the problem of reducing the number of terms used in the induction process under a very different light, essentially, both approaches try to solve a similar question. <p> Also, some experiments have been made for automating the selection of the N G parameter for measurable performance goals (e.g. predictive accuracy), using cross-validation techniques to estimate performance <ref> (McEwen, 1996, Talavera & Cortes, 1997b) </ref>.
Reference: <author> Talavera, L. & Cortes, U. </author> <year> (1997a). </year> <title> Exploiting bias shift in knowledge acquisition. </title> <booktitle> In 10th European Workshop on Knowledge Acquisition, Modeling, and Management, </booktitle> <address> Sant Feliu de Guixols, Barcelona, Spain. </address> <note> To appear. </note>
Reference-contexts: Therefore, interaction with user in learning algorithms can be more easily modeled in systems capable of performing some sort of bias selection via parameter setting <ref> (Talavera & Cortes, 1997a) </ref>. 4 But note that some parameters may not be consid-ered adequate for this task as they may not have such a clear meaning for the user as biases have.
Reference: <author> Talavera, L. & Cortes, U. </author> <year> (1997b). </year> <title> Inductive hypothesis validation and bias selection in unsupervised learning. </title> <editor> In Vanthienen, J. & van Harmelen, F. (Eds.), </editor> <booktitle> Proceedings of the 4th European Symposium on the Validation and Verification of Knowledge Based Systems, EUROVAV-97, </booktitle> <pages> (pp. 169-179)., </pages> <address> Leuven, Belgium. </address> <month> 9 </month>
References-found: 29

