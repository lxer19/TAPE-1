URL: http://mt.www.media.mit.edu/people/mt/agar/Agar-SM-Thesis.ps.gz
Refering-URL: http://mt.www.media.mit.edu/people/mt/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Note: Contents  
Abstract-found: 0
Intro-found: 1
Reference: [Abelson85] <author> Harold Abelson and Gerald Jay Sussman. </author> <title> Structure and Interpretation of Computer Programs. </title> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1985. </year>
Reference-contexts: In the absence of state, a statement of a programming language can be replaced with an equivalent value, regardless of context. This property is known as referential transparency <ref> [Abelson85] </ref>. The presence of state requires a more complicated semantics. State appears in ethology as the concept of motivation.
Reference: [Agre87a] <author> Philip E. Agre and David Chapman. Pengi: </author> <title> an implementation of a theory of situated action. </title> <booktitle> In Proceedings of AAAI-87, </booktitle> <year> 1987. </year>
Reference-contexts: Agre and Chapman describe an artificial creature that operates inside a grid-like videogame world using only combinatorial logic to connect a set of sensory and motor primitives <ref> [Agre87a] </ref>. They also suggest how abstract reasoning might arise as an emergent function of situated activity [Chapman86]. Their system, called Pengi, is a player of the videogame Pengo. Pengo requires the player to manipulate a penguin in a world consisting of hostile bees and obstacles CHAPTER 2.
Reference: [Agre87b] <author> Philip E. Agre and David Chapman. </author> <title> What are plans for? 1987. Prepared for the Panel on Representing Plans and Goals, </title> <booktitle> DARPA Planning Workshop, </booktitle> <month> October 21-23, </month> <year> 1987, </year> <institution> Santa Cruz, California. </institution>
Reference-contexts: Thrownness refers to the fact that an intelligent agent is always in a situation and cannot avoid acting within and on that situation. Thrownness is to be contrasted with the standard AI view of action as planning, a detached reasoning process that operates independently of the world <ref> [Agre87b] </ref>. The background is the implicit beliefs and assumptions that inform action. Heidegger concluded that these beliefs are not amenable to representation and cannot be made explicit [Winograd87, p. 32]. <p> This notion of planning has come under attack as unrepresentative of the way in which intelligent beings actually act in the world <ref> [Suchman87, Agre87b] </ref>. We do not compute a plan ahead of time and then carry it out; instead we are in an interactive relationship with the world, continuously exploring possibilities for actions. This is what gives us the flexibility to deal with a world that cannot be fully represented in memory.
Reference: [Alcock84] <author> John Alcock. </author> <title> Animal Behavior: An Evolutionary Approach. </title> <publisher> Sinauer Associates, </publisher> <year> 1984. </year>
Reference-contexts: R-trees as described above are ad hoc learning mechanisms, attached to a specific agent and hence fulfilling a specific and fixed purpose. This one-shot learning mechanism corresponds to the ethological phenomenon of imprinting <ref> [Alcock84] </ref>. A more general learning mechanism might employ agents that could detect situations where better recognition was necessary and then could create an R-tree to fill this need. On the implementation level, both K-lines and R-trees can be built out of Hebb synapses [Hebb49].
Reference: [Axelrod84] <author> Robert Axelrod. </author> <title> The Evolution of Cooperation. </title> <publisher> Basic Books, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: These include worlds with standard euclidean geometry and simplified physics, more realistic worlds that can simulate locomotion (such as balance and limb placement) in greater detail [Sims87], more abstract worlds such as cellular arrays [Turner87, Rizki86], or completely non-physical game-theory worlds for simulating theoretical problems of behavioral strategy <ref> [Axelrod84] </ref>. The world, sensors, and motors together constitute an implementation of the animal's Umwelt or subjective world. Umwelt is a term introduced by von Uexkull to denote the unique private world defined by an animal's sensorimotor capabilities.
Reference: [Baerends76] <author> G. P. Baerends. </author> <title> The functional organization of behavior. Animal Behaviour, </title> <booktitle> 24 </booktitle> <pages> 726-738, </pages> <year> 1976. </year>
Reference-contexts: Lateral inhibition links ensure that only one unit on any given level is active at a time. This hierarchical scheme was later generalized to be a heterarchy and to include feedback mechanisms <ref> [Baerends76] </ref>. A heterarchy allows a low-level unit to be activated by more than one high-level unit. These structural models of behavioral systems formed an organizing paradigm for ethology until the late 1950s, when the concept of drive energy fell into disrepute.
Reference: [Braitenberg84] <author> Valentino Braitenberg. </author> <title> Vehicles: Experiments in Synthetic Psychology. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1984. </year>
Reference-contexts: Early cyberneticians built simple, reflex-based artificial animals to demonstrate the possibility of mechanical behavior and learning. Grey Wal-ter's turtles [Walter50] responded to light and could learn by a mechanism that implemented classical conditioning. Braitenberg continued the tradition of building simple autonomous animals <ref> [Braitenberg84] </ref> by designing a series of imaginary robotic animals to illustrate concepts of "synthetic psychology". More recent animal simulation systems emphasize ecological interaction by supporting more than one species of artificial life. RAM [Turner87] and EVOLVEIII [Rizki86] fall into this category. <p> When the turtle is placed into its world, it responds according to its wiring. BrainWorks was largely inspired by Valentino Braitenberg's Vehicles: Experiments in Synthetic Psychology <ref> [Braitenberg84] </ref>. In addition to allowing users to program animals, Brain-Works can be used to simulate evolutionary adaptation of animals to their environment. This work is discussed in section 7.3. A turtle's usual task is to catch food while avoiding being blocked by a wall or obstacle (see figure 5.2).
Reference: [Brooks86] <author> Rodney A. Brooks. </author> <title> Achieving Artificial Intelligence through Building Robots. </title> <type> AI Memo 899, </type> <institution> Massachusetts Institute of Technology Artificial Intelligence Laboratory, </institution> <year> 1986. </year>
Reference-contexts: The creature's actions are limited to moving in one of four directions and kicking. Pengi also has a global view of its world rather than strictly local sensing capabilities, although most sensing computations are locally centered around the player. 2.4.3 Subsumption architectures for robotics Rod Brooks <ref> [Brooks86] </ref> and Peter Cudhea [Cudhea88] have developed architectures for robots along similar lines. As in Agar, their behavioral control system is divided into task-specific modules with fixed channels of communication between them. They base their architecture on the notion of subsumption. <p> The Umwelt forms the basis for situated action in animals. There is also the possibility of bypassing the simulation problem entirely by means of real-world robotics. This approach is strongly advocated by Brooks <ref> [Brooks86] </ref> (see section 2.4.3) and brought to pedagogical reality in the Lego/Logo system [Ocko88] (see section 2.2.1). Since robot technology has not yet advanced to the point where one can take the mechanical details for granted, it is difficult to do behavioral research with robots.
Reference: [Brooks87] <author> Rodney A. Brooks. </author> <title> Intelligence without representation. </title> <note> 1987. Unpublished paper. </note>
Reference-contexts: Instead, it depends only on a simple associative mechanism for recording temporal relationships between events as they occur, and some simple special-purpose control mechanisms. Its use of abstract representations is limited, since scripts are firmly grounded in the recording process rather than in a semantic theory. Brooks has argued <ref> [Brooks87] </ref> that representation can be dispensed with altogether for intelligent situated creatures. We take a similar but less radical position that representation should be dethroned from its central position, emphasizing instead the mechanisms that underlie representation and are responsible for its generation and use.
Reference: [Carey85] <author> Susan Carey. </author> <title> Conceptual Change in Childhood. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mas-sachusetts, </address> <year> 1985. </year>
Reference-contexts: Young children tend to think in animistic terms. They tend to attribute action to intentions of objects rather than mechanistic forces. Thus a rock rolling down a hill is alive to a child, while a stationary rock is not <ref> [Carey85] </ref>. Computer processes share some of the attributes of living things. Processes are inherently dynamic, following a path of action determined by their program and the computational environment. A Process can have goals, which may be explicitly represented within the process or may be implicit in its design.
Reference: [Chapman86] <author> David Chapman and Philip E. Agre. </author> <title> Abstract reasoning as emergent from concrete activity. </title> <booktitle> In Proceedings of the 1986 Workshop on Reasoning About Actions & Plans, </booktitle> <pages> pages 411-424, </pages> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, California, </address> <year> 1986. </year> <note> BIBLIOGRAPHY 82 </note>
Reference-contexts: Agre and Chapman describe an artificial creature that operates inside a grid-like videogame world using only combinatorial logic to connect a set of sensory and motor primitives [Agre87a]. They also suggest how abstract reasoning might arise as an emergent function of situated activity <ref> [Chapman86] </ref>. Their system, called Pengi, is a player of the videogame Pengo. Pengo requires the player to manipulate a penguin in a world consisting of hostile bees and obstacles CHAPTER 2. BACKGROUND, BASIC CONCEPTS, AND RELATED WORK 26 in the form of ice cubes.
Reference: [Chapman87] <author> David Chapman. </author> <title> Planning for conjunctive goals. </title> <journal> Artificial Intelligence, </journal> <volume> 32(3) </volume> <pages> 333-378, </pages> <year> 1987. </year>
Reference-contexts: The task of maintaining a world model over changes is called the frame problem [Shoham86], and severely challenges standard logical deduction methods. The planning problem itself has been shown to be computationally intractable <ref> [Chapman87] </ref>. In our view these unsurmounted problems are the consequence of the logicist framework used by planning rather than inherent obstacles to programming intelligent action. The situated action approach advocates a more interactive relationship with the world.
Reference: [Coderre88] <author> Willam Coderre. </author> <title> Modelling behavior in petworld. </title> <editor> In C. Langton, editor, </editor> <booktitle> Artifical Life: SFI Series in the Sciences of Complexity, </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1988. </year>
Reference-contexts: This is reminiscent of Society of Mind theories (see section 2.4.1)) and suggests that a method for learning within individual agents. CHAPTER 2. BACKGROUND, BASIC CONCEPTS, AND RELATED WORK 16 Petworld is another animal simulation system developed under the auspices of the Vivarium <ref> [Coderre88] </ref>. Its features include a system of ranking alternative behaviors through a hierarchical structure of modules, and performance of the relatively complex task of nest-building. 2.1.3 Computational evolution The idea of simulating individual animals can be extended to include simulating multiple generations of evolving animals. <p> The worlds used in the examples are all two-dimensional euclidean worlds with a continuous coordinate system. This is in contrast to most other animal simulation systems (ie, [Turner87] <ref> [Coderre88] </ref> [Wilson86]) that use discrete grids. I found that grids failed to capture certain CHAPTER 3. ANIMAL PARTS 31 important properties.
Reference: [Cudhea88] <author> Peter W. Cudhea. </author> <title> Describing the Control Systems of Simple Robot Creatures. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1988. </year>
Reference-contexts: Pengi also has a global view of its world rather than strictly local sensing capabilities, although most sensing computations are locally centered around the player. 2.4.3 Subsumption architectures for robotics Rod Brooks [Brooks86] and Peter Cudhea <ref> [Cudhea88] </ref> have developed architectures for robots along similar lines. As in Agar, their behavioral control system is divided into task-specific modules with fixed channels of communication between them. They base their architecture on the notion of subsumption.
Reference: [Dawkins76] <author> Richard Dawkins. </author> <title> Hierarchical organization: a candidate principle for ethology. </title> <editor> In P.P.G. Bateson and R.A. Hinde, editors, </editor> <booktitle> Growing Points in Ethology, </booktitle> <pages> pages 7-54, </pages> <publisher> Cambridge University Press, </publisher> <year> 1976. </year>
Reference-contexts: A heterarchy allows a low-level unit to be activated by more than one high-level unit. These structural models of behavioral systems formed an organizing paradigm for ethology until the late 1950s, when the concept of drive energy fell into disrepute. It was later revived by Dawkins <ref> [Dawkins76] </ref> who pointed out that the idea of hierarchical behavior structure could be salvaged by changing the interpretation of interlevel connections from energy transfer relations to control relations.
Reference: [Dawkins87] <author> Richard Dawkins. </author> <title> The Blind Watchmaker. </title> <editor> W. W. </editor> <publisher> Norton, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: To do so requires modeling the basic mechanisms of evolutionary adaptation: selective survival and reproduction with survival. Richard Dawkins has recently developed a software package called The Blind Watchmaker (after his book of the same name <ref> [Dawkins87] </ref>) that demonstrates the evolution of form. This system does an excellent job of creating a diversity of forms from a few simple parameters. However, the resulting form (called a "biomorph") does not behave, and its only criterion for survival is its appeal to the eye of the user.
Reference: [Dennett83] <author> Daniel C. Dennett. </author> <title> Intentional systems in cognitive ethology: the "panglossian paradigm" defended. </title> <journal> The Behavioral and Brain Sciences, </journal> <volume> 6 </volume> <pages> 343-390, </pages> <year> 1983. </year>
Reference-contexts: It would seem that Dandy possesses a thorough knowledge about the world and about the actions and intentions of the other chimps, and that he has the ability to reason with this knowledge and form plans based on manipulating the knowledge of the others. In Dennett's terms <ref> [Dennett83] </ref>, he has a high intentionality of a high order, that is, the ability to have beliefs about the beliefs of others, and to reason about them. CHAPTER 7.
Reference: [deWaal82] <author> Frans B. M. de Waal. </author> <title> Chimpanzee Politics: Power and Sex Among Apes. </title> <publisher> Harper & Row, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: This necessitates searching for a branching point that leads to the undesirable outcome and selecting an alternate pathway. As an example, we will take an anecdotal account of an instance of deception in a chimpanzee colony (from <ref> [deWaal82, p. 73] </ref>): Dandy has to offset his lack of strength by guile. I witnessed an amazing instance of this...We had hidden some grapefruit in the chimpanzees' enclosure. The fruit had been half buried in sand, with small yellow patches left uncovered.
Reference: [Dreyfus79] <author> Hubert Dreyfus. </author> <title> What Computers Can't Do: </title> <booktitle> The Limits of Artificial Intelligence. </booktitle> <publisher> Harper and Row, </publisher> <year> 1979. </year>
Reference-contexts: CHAPTER 2. BACKGROUND, BASIC CONCEPTS, AND RELATED WORK 24 The phenomenological critique of AI The idea that abstract thought is independent of its embedding in a body has been attacked by Dreyfus and others <ref> [Dreyfus79, Winograd87] </ref>. The starting point for this attack is the philosophy of Heidegger, which emphasizes the "thrownness" of everyday activity and the dependence of action on "background". Thrownness refers to the fact that an intelligent agent is always in a situation and cannot avoid acting within and on that situation.
Reference: [Ewert87] <author> Jorg-Peter Ewert. </author> <title> Neuroethology of releasing mechanisms: </title> <journal> prey-catching in toads. Behavioral and Brain Sciences, </journal> <volume> 10 </volume> <pages> 337-405, </pages> <year> 1987. </year>
Reference-contexts: While there are many problems with Gibson's analysis, most stemming from his failure to apprehend the computational mechanisms required to implement his ideas [Marr82, pp. 29-31], his fundamental insight was sound, and has been supported by later neuroethological research <ref> [Ewert87] </ref> that demonstrates specialized neural pathways for behaviorally relevant information. Sometimes the rule about giving sensors only local access to the world must be violated. <p> In fact, this pattern of control is often found within nature. Ewert's neuroethological studies of fly-catching in toads has demonstrated the existence of a chain of behavioral responses and uncovered some of the neural mechanisms involved <ref> [Ewert87] </ref>. The most striking cases often involve two animals that are involved in a dynamic pattern of such responses, particularly in mating rituals such as the zig-zag dance of the stickleback [Tinbergen51].
Reference: [Fentress76] <author> J. C. Fentress. </author> <title> Dynamic boundaries of patterned behavior: </title> <booktitle> interaction and self-organization, </booktitle> <pages> pages 135-169. </pages> <publisher> Cambridge University Press, </publisher> <year> 1976. </year>
Reference-contexts: Fentress, in particular, objects to these models as erecting artificial and overly rigid boundaries between behaviors that are actually plastic, ill-defined, and mutable CHAPTER 2. BACKGROUND, BASIC CONCEPTS, AND RELATED WORK 15 <ref> [Fentress76] </ref>. His critique bears much the same relationship to orthodox ethological modeling as the connectionist critique of symbolic processing [Smolensky88] does to mainstream artificial intelligence. Fentress has studied the development of hierarchically-structured grooming patterns in rats [Fentress83].
Reference: [Fentress83] <editor> J. C. Fentress. </editor> <booktitle> The analysis of behavioral networks, </booktitle> <pages> pages 939-968. </pages> <publisher> Plenum Press, </publisher> <year> 1983. </year>
Reference-contexts: BACKGROUND, BASIC CONCEPTS, AND RELATED WORK 15 [Fentress76]. His critique bears much the same relationship to orthodox ethological modeling as the connectionist critique of symbolic processing [Smolensky88] does to mainstream artificial intelligence. Fentress has studied the development of hierarchically-structured grooming patterns in rats <ref> [Fentress83] </ref>. Apparently juvenile rats have a less differentiated repertoire of movements than do mature rats, resulting in phenomena such as "motor traps". A motor trap is a situation in which movements that are part of more than one higher-level activity cause arbitrary switching between one activity and the other.
Reference: [Finzer84] <author> William Finzer and Laura Gould. </author> <title> Programming by rehearsal. </title> <journal> Byte, </journal> <month> June </month> <year> 1984. </year>
Reference-contexts: In such a system, the user performs a series of concrete operations on some computational objects, then generalizes these operations into routines by various methods, such as replacing constants with variables <ref> [Lieberman84, Finzer84] </ref>. Programming by example is a natural technique to use with situated creatures. Creatures in a world generate their own tasks in the form of situations that require new behaviors in the form of new agents.
Reference: [Foley87] <author> J. D. Foley. </author> <title> Interfaces for advanced computing. </title> <journal> Scientific American, </journal> <volume> 257(4) </volume> <pages> 126-135, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: The specification of action can be done using a menu, or more advanced systems can combine menu and gestural input using devices such as the VPL Dataglove <ref> [Foley87] </ref>. For example, imagine we've just created a new animal and placed it in a featureless part of the world, where it is free of all but the default sensory stimuli. Since its mind is blank, without any agents, it just sits there.
Reference: [Freud95] <author> Sigmund Freud. </author> <title> Project for a scientific psychology. </title> <type> 1895. </type>
Reference-contexts: The concept of drive or activation energy has a long history that can be traced back through Freud's concept of cathexis <ref> [Freud95] </ref> to his early 19th century sources. It has lately re-emerged as the basis of connectionist modeling [McClelland86]. <p> We now consider what sort of computational elements should be in a kit. Neurons have a long history as hypothetical components of psychological and computational systems (going back at least to Freud <ref> [Freud95] </ref>). These neurons are simplified versions of the neurons actually found in brains, envisioned as analog, linear, sum-and-threshold devices. Connections between neurons are weighted, and neural systems can learn by changing these weights. Rules operate on symbolic data. They cause an action to be performed whenever some condition is met.
Reference: [Gibson79] <author> James J. Gibson. </author> <title> The Ecological Approach to Visual Perception. </title> <publisher> Houghton Mi*in, </publisher> <year> 1979. </year>
Reference-contexts: In the latter case they may be weighted by closeness. For a detailed description of a general sensing function, see Section 4.4. The notion of perception as detecting specifically useful classes of object was anticipated by Gibson in his notion of ecological optics <ref> [Gibson79] </ref>. Gibson believed that the function of perception was to extract relevant information from an environment, and that animals sensory systems were specialized to extract the information they required.
Reference: [Goldberg76] <author> Adele Goldberg and Alan Kay. </author> <title> Smalltalk-72 Instruction Manual. </title> <type> Technical Report SSL-76-6, </type> <institution> Xerox Palo Alto Research Center, </institution> <year> 1976. </year> <note> BIBLIOGRAPHY 83 </note>
Reference-contexts: Novices and CHAPTER 2. BACKGROUND, BASIC CONCEPTS, AND RELATED WORK 19 children are not experienced in dealing with such objects. Good user interfaces allow the novice to draw upon existing knowledge, by presenting the computational objects using metaphors with tangible objects in the real world <ref> [Papert80, Goldberg76] </ref>. The properties of the objects on the screen are presented metaphorically, via an analogy to familiar objects.
Reference: [Gould82] <author> James L. Gould. Ethology: </author> <title> The Mechanisms and Evolution of Behavior. </title> <editor> W. W. Norton, </editor> <year> 1982. </year>
Reference-contexts: Umwelt is a term introduced by von Uexkull to denote the unique private world defined by an animal's sensorimotor capabilities. Describing the limited world of a tick, he says "the very poverty of this world guarantees the unfailing certainty of her actions" <ref> [Gould82, p. 27] </ref>. The Umwelt forms the basis for situated action in animals. There is also the possibility of bypassing the simulation problem entirely by means of real-world robotics.
Reference: [Hammond86] <author> Kristian John Hammond. </author> <title> Case-baed Planning: An Integrated Theory of Planning, Learning and Memory. </title> <type> PhD thesis, </type> <institution> Yale, </institution> <year> 1986. </year>
Reference-contexts: Case-based planners <ref> [Hammond86] </ref> generate plans by recombining pieces of earlier plans, which are then stored for future use in similar situations. The case memory indexes plans by the situations they were useful in, and by situations in which they fail.
Reference: [Hebb49] <author> D. O. Hebb. </author> <title> Organization of Behavior. </title> <publisher> Wiley and Son, </publisher> <year> 1949. </year>
Reference-contexts: A more general learning mechanism might employ agents that could detect situations where better recognition was necessary and then could create an R-tree to fill this need. On the implementation level, both K-lines and R-trees can be built out of Hebb synapses <ref> [Hebb49] </ref>. These are synaptic connections that increase their weight when both sides of the CHAPTER 7. LEARNING AND EVOLUTION 72 connection are active simultaneously.
Reference: [Hinde59] <author> R. A. Hinde. </author> <title> Unitary drives. Animal Behavior, </title> <address> 7:130, </address> <year> 1959. </year>
Reference-contexts: The concept of drive or activation energy has a long history that can be traced back through Freud's concept of cathexis [Freud95] to his early 19th century sources. It has lately re-emerged as the basis of connectionist modeling [McClelland86]. Within ethology it was a powerful organizing concept until Hinde <ref> [Hinde59] </ref>, who criticized drive energy theory as being a non-explanation (in that a special drive could be postulated to form the basis of any behavior) and for overextending the analogy to physical energy (i.e., explaining displacement behavior by "sparking over" of drive energy from one center to another).
Reference: [Holland86] <author> John H. Holland. </author> <title> Escaping brittleness: the possibilities of general-purpose learning algorithms applied to parallel rule-based systems. </title> <editor> In Tom M. Mitchell Ryszard S. Michalski, Jaime G. Carbonell, editor, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, chapter 20, </booktitle> <pages> pages 593-623, </pages> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <year> 1986. </year>
Reference-contexts: Creating an artificial world that has enough combinatorial flexibility to permit innovative behavior is still an unmet challenge. The point mutations used in the evolution simulation generate incremental exploratory moves in creature-space. More powerful techniques such as the crossover rules used in genetic algorithms <ref> [Holland86] </ref> can improve the ability to adapt to novel circumstances.
Reference: [Kahn79] <author> Kenneth Kahn. </author> <title> Creation of Computer Animations from Story Descriptions. </title> <type> AI Lab Technical Report 540, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1979. </year>
Reference-contexts: It does not attempt to simulate sensorimotor constraints (birds have direct, global knowledge of the locations of other birds and obstacles) and the behavioral model is very simple. Kahn developed a system called ANI that creates animations from story descriptions <ref> [Kahn79] </ref>.
Reference: [Kolodner85] <author> Janet L. Kolodner. </author> <title> Experiential Processes in Natural Problem Solving. </title> <type> Technical Report GIT-ICS-85/23, </type> <institution> School of Information and Computer Science, Georgia Institute of Technology, </institution> <year> 1985. </year>
Reference-contexts: Essentially the subsumption architecture uses a slightly higher level of description, although the range of mechanisms that can be constructed is the same, since Agar can implement finite-state machines (see 4.6.1). 2.4.4 Case-based reasoning Case-based reasoning <ref> [Kolodner85] </ref> is a method of organizing intelligence based on the use of past experience stored as cases. Case-based planners [Hammond86] generate plans by recombining pieces of earlier plans, which are then stored for future use in similar situations. <p> This script must also be connected to various other agents in the mind. CHAPTER 7. LEARNING AND EVOLUTION 73 Janet Kolodner <ref> [Kolodner85] </ref> suggests a variety of mechanisms by which episodic, experiential memory structures may be generalized to and integrated with generic, factual structures. To make these scripts useful we will have to postulate some additional machinery.
Reference: [Lieberman84] <author> Henry Lieberman. </author> <title> Seeing what your programs are doing. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 21 </volume> <pages> 311-331, </pages> <year> 1984. </year>
Reference-contexts: In such a system, the user performs a series of concrete operations on some computational objects, then generalizes these operations into routines by various methods, such as replacing constants with variables <ref> [Lieberman84, Finzer84] </ref>. Programming by example is a natural technique to use with situated creatures. Creatures in a world generate their own tasks in the form of situations that require new behaviors in the form of new agents.
Reference: [Marr82] <author> David Marr. </author> <title> Vision. </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <address> San Francisco, </address> <year> 1982. </year>
Reference-contexts: While there are many problems with Gibson's analysis, most stemming from his failure to apprehend the computational mechanisms required to implement his ideas <ref> [Marr82, pp. 29-31] </ref>, his fundamental insight was sound, and has been supported by later neuroethological research [Ewert87] that demonstrates specialized neural pathways for behaviorally relevant information. Sometimes the rule about giving sensors only local access to the world must be violated.
Reference: [McClelland86] <author> James L. McClelland, Devid E. Rumelhard, </author> <title> and the PDP Research Group. Parallel Distributed Processing: Explorations in the Microstructure of Cognition. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: The concept of drive or activation energy has a long history that can be traced back through Freud's concept of cathexis [Freud95] to his early 19th century sources. It has lately re-emerged as the basis of connectionist modeling <ref> [McClelland86] </ref>.
Reference: [McCulloch43] <author> Warren S. McCulloch and Walter H. Pitts. </author> <title> A logical calculus of the ideas immanent in nervous activity. </title> <journal> Bulletin of Mathematical Biophysics, </journal> <volume> 5 </volume> <pages> 115-133, </pages> <year> 1943. </year>
Reference-contexts: The properties of the world and the turtle were the result of compromises between the goals of keeping computation time low (to preserve the real-time feel of the system) and making a rich, realistic, and interesting world. CHAPTER 5. EARLY DESIGNS 55 5.1.1 Neurons BrainWorks uses modified McCulloch-Pitts neurons <ref> [McCulloch43] </ref> as its computational units. They implement a mostly boolean logic, in that neurons can be in one of only two states (on or off), connections similarly transmit a boolean value, and there are no registers. Connections can be inhibitory or excitatory.
Reference: [Minsky87] <author> Marvin Minsky. </author> <title> Society of Mind. </title> <publisher> Simon & Schuster, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Although suspect, it appears to be a perennial component of theories of the mind. Minsky <ref> [Minsky87, p. 284] </ref> suggests a new way of looking at mental energy as an emergent property rather than a fundamental mechanism. <p> These efforts tend to emphasize the role of experience and the environment in contributing to intelligence. An emphasis on mechanism over formalism also characterizes much of this work. 2.4.1 Society of Mind The Society of Mind <ref> [Minsky87] </ref> refers to a broad class of theories that model mental functioning in terms of a collection of communicating agents. An agent is a part of a mind that is responsible for some particular aspect of mental functioning. <p> The great problem for distributed models is coordination|once we have split the functioning of mind up into isolated agents, how do we get them to cooperate in carrying out the business of generating intelligent behavior? A-brains and B-brains One Society of Mind subtheory <ref> [Minsky87, p. 59] </ref> suggests that a mind can be organized into separate layers of control. These layers are an A-brain that interacts with the world, and a B-brain that interacts solely with the A-brain. The B-brain monitors the A-brain's activity and exerts control over it when necessary. <p> It is easy to construct universal computation devices using rules together with an appropriate environment. An agent is a simple computational process that performs a particular function, often by turning on other agents. This notion of agent derives from Minsky's Society of Mind theory <ref> [Minsky87] </ref>. Considered as computational objects, agents can: * Execute concurrently, * Maintain a small amount of local state, * Access the state of other agents through connections, and * Take actions automatically when environmental conditions are met. CHAPTER 3. <p> The most important justification for this is to allow rules that are triggered by the presence of active goals in a goal memory. This permits a GPS-like programming style whereby the agent compares the current state to a goal state and generates a difference-reducing action <ref> [Minsky87, p. 78] </ref>. Another motive for re-introducing state is to allow the rules themselves to be stored in memory and thus be mutable by learning algorithms that can themselves be expressed as rules. In 1-Agar, agents are sets of rules. <p> A script agent can play back this record to recreate a pattern of events. It is rarely desirable to recreate a past event sequence exactly, since circumstances will differ. Thus a script will only record agents at a fixed level of detail (this could be implemented by level-bands; see <ref> [Minsky87, p. 86] </ref>). For instance, we could record agents that correspond to general patterns of action, but not specific muscle firings. We don't want to store exact recordings of every sequence in an animal's life. Instead, we want to form scripts that are generalizations of sequences. <p> This may be another function of level-bands, in that the lower (motor) levels can be shut off in order to perform look-ahead computations. This mechanism allows the animal to "hallucinate" the consequences of possible actions. (see <ref> [Minsky87, p. 169] </ref> for a similar example). We will also need agents that can look at the branching paths of a script and influence action based on them. If a pathway leads to an undesirable condition, we would like to avoid taking that pathway.
Reference: [Niklas86] <author> K. J. Niklas. </author> <title> Computer-simulated plant evolution. </title> <journal> Scientific American, </journal> <volume> 254(3) </volume> <pages> 78-86, </pages> <year> 1986. </year>
Reference-contexts: This system does an excellent job of creating a diversity of forms from a few simple parameters. However, the resulting form (called a "biomorph") does not behave, and its only criterion for survival is its appeal to the eye of the user. Niklas <ref> [Niklas86] </ref> also simulates evolution parametrically, in the realm of plants. Three factors affecting branching pattern were allowed to change by mutation: probability of branching, branching angle, and rotation angle. The plants compete with each other for available sunlight.
Reference: [Ocko88] <author> Steve Ocko, Seymour Papert, and Mitchel Resnick. </author> <title> Lego, logo, </title> <journal> and science. Technology and Learning, </journal> <volume> 2(1), </volume> <year> 1988. </year>
Reference-contexts: Logo's accessibility to children is derived in part from its use of an animistic actor (the turtle). More recently the Lego/Logo project <ref> [Ocko88] </ref> has extended the Logo turtle to include sensors as well as motors, and provided a reconfigurable mechanical medium (the Lego construction kit) for creating different physical animals. The Lego/Logo system both gains and loses from the use of real-world physical creatures. <p> The Umwelt forms the basis for situated action in animals. There is also the possibility of bypassing the simulation problem entirely by means of real-world robotics. This approach is strongly advocated by Brooks [Brooks86] (see section 2.4.3) and brought to pedagogical reality in the Lego/Logo system <ref> [Ocko88] </ref> (see section 2.2.1). Since robot technology has not yet advanced to the point where one can take the mechanical details for granted, it is difficult to do behavioral research with robots. Simply dealing with the robot's mechanical details can consume all of one's research energies.
Reference: [Papert80] <author> Seymour Papert. Mindstorms: </author> <title> Children, Computers, and Powerful Ideas. </title> <publisher> Basic Books, </publisher> <address> New York, </address> <year> 1980. </year> <note> BIBLIOGRAPHY 84 </note>
Reference-contexts: Animals and processes share characteristics of being dynamic, rule-driven, and goal-directed. Thus animals can serve as metaphorical representations CHAPTER 2. BACKGROUND, BASIC CONCEPTS, AND RELATED WORK 17 of processes. 2.2.1 Lego/Logo The Logo project <ref> [Papert80] </ref> is a long-standing effort to put computation within the grasp of young children. Logo's accessibility to children is derived in part from its use of an animistic actor (the turtle). <p> Novices and CHAPTER 2. BACKGROUND, BASIC CONCEPTS, AND RELATED WORK 19 children are not experienced in dealing with such objects. Good user interfaces allow the novice to draw upon existing knowledge, by presenting the computational objects using metaphors with tangible objects in the real world <ref> [Papert80, Goldberg76] </ref>. The properties of the objects on the screen are presented metaphorically, via an analogy to familiar objects. <p> Motors and sensors act through the creature object. Creatures are also responsible for interfacing to the display system. They may contain other state specific to their species (such as an individual ant's caste). The basic design of creatures in the two-dimensional world is based on the Logo turtle <ref> [Papert80] </ref> and its autonomous antecedent [Walter50]. They implement a form of local computational geometry. The locality is critical: all the turtle's operations are expressed as relative to its present position and orientation, rather than to a global coordinate system.
Reference: [Pearson76] <author> Keir Pearson. </author> <title> The control of walking. </title> <journal> Scientific American, </journal> <volume> 235(6) </volume> <pages> 72-86, </pages> <year> 1976. </year>
Reference-contexts: is capable of embodying some common behavioral techniques, in particular oscillators, which appear in walking control systems; and finite-state machines, which form the modules of Brooks' subsumption architecture (see 2.4.3). 4.6.1 Oscillators Oscillators or rhythm generators play an important role in the control of periodic animal motion such as walking <ref> [Pearson76] </ref>. The following two agents define a creature named wiggler that has an oscillating movement. The period and duty-cycle are controlled by the :self-inhibit-after parameters. Each agent inhibits the other with a0. <p> We can also use body configurational state for determining behavior. For example, the walking control system of the cockroach is driven in part by stress-receptors in the leg that indicate that it is supporting weight <ref> [Pearson76] </ref>. Another example is found in the Agar model of ant trail laying, in which the laying of the trail is determined by the ant's having a piece of food in its pincers (see section 4.5.3).
Reference: [Reynolds82] <author> Craig W. Reynolds. </author> <title> Computer animation with scripts and actors. </title> <booktitle> In Proceedings of SIGGRAPH '82, </booktitle> <year> 1982. </year>
Reference-contexts: Current work in the field focuses on self-scripting animations, in which each object to be animated is responsible for determining its own motion by the application of a set of rules to its particular conditions in a simulated world. Reynolds <ref> [Reynolds82] </ref> developed an animation system called ASAS that used a distributed control model. It introduced the idea of generating animation by having active objects that are controlled by their own individual programs, rather than a single global script.
Reference: [Reynolds87] <author> Craig W. Reynolds. </author> <title> Flocks, herds, and schools: a distributed behavioral model. </title> <booktitle> In Proceedings of SIGGRAPH '87, </booktitle> <year> 1987. </year>
Reference-contexts: It introduced the idea of generating animation by having active objects that are controlled by their own individual programs, rather than a single global script. This idea was later extended to allow objects to alter their behavior based on external conditions <ref> [Reynolds87] </ref>. The resulting system was used to produce animations of animal group motion such as bird flocking. It uses a true distributed behavioral model (each bird computes its own motion).
Reference: [Rizki86] <author> Mateen M. Rizki and Michael Conrad. </author> <title> Computing the theory of evolution. </title> <journal> Physica, </journal> <volume> 22D:83-99, </volume> <year> 1986. </year>
Reference-contexts: Braitenberg continued the tradition of building simple autonomous animals [Braitenberg84] by designing a series of imaginary robotic animals to illustrate concepts of "synthetic psychology". More recent animal simulation systems emphasize ecological interaction by supporting more than one species of artificial life. RAM [Turner87] and EVOLVEIII <ref> [Rizki86] </ref> fall into this category. The latter is particularly interesting because it simulates several levels of biological organization (genetic, behavioral, and ecological) and the relationships among them. Stewart Wilson has studied learning in artificial animals [Wilson87]. <p> These include worlds with standard euclidean geometry and simplified physics, more realistic worlds that can simulate locomotion (such as balance and limb placement) in greater detail [Sims87], more abstract worlds such as cellular arrays <ref> [Turner87, Rizki86] </ref>, or completely non-physical game-theory worlds for simulating theoretical problems of behavioral strategy [Axelrod84]. The world, sensors, and motors together constitute an implementation of the animal's Umwelt or subjective world.
Reference: [Schank82] <author> Roger C. Schank. </author> <title> Dynamic Memory: A theory of reminding and learning in computers and people. </title> <publisher> Cambridge University Press, </publisher> <year> 1982. </year>
Reference-contexts: A script-agent works like a K-line, in that it can record and recall activation states of an agent population. Unlike a K-line, however, it records and recall a temporal sequence of such activations. The standard meaning of "script" in artificial intelligence <ref> [Schank82] </ref> is rather different. A Schankian script encodes general knowledge of how scenarios unfold, in the form of descriptive symbolic relations.
Reference: [Shoham86] <author> Yoav Shoham. </author> <booktitle> What is the frame problem? In Proceedings of the 1986 Workshop on Reasoning About Actions & Plans, </booktitle> <pages> pages 83-98, </pages> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, California, </address> <year> 1986. </year>
Reference-contexts: This perfect model is not only an accurate representation of the current state of the world, but can be manipulated to compute the possible changes induced in the world by the agent's hypothetical operations. The task of maintaining a world model over changes is called the frame problem <ref> [Shoham86] </ref>, and severely challenges standard logical deduction methods. The planning problem itself has been shown to be computationally intractable [Chapman87]. In our view these unsurmounted problems are the consequence of the logicist framework used by planning rather than inherent obstacles to programming intelligent action.
Reference: [Sims87] <author> Karl Sims. </author> <title> Locomotion of Jointed Figures over Complex Terrain. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1987. </year>
Reference-contexts: As a result, we need different kinds of worlds depending on the properties that we are interested in. These include worlds with standard euclidean geometry and simplified physics, more realistic worlds that can simulate locomotion (such as balance and limb placement) in greater detail <ref> [Sims87] </ref>, more abstract worlds such as cellular arrays [Turner87, Rizki86], or completely non-physical game-theory worlds for simulating theoretical problems of behavioral strategy [Axelrod84]. The world, sensors, and motors together constitute an implementation of the animal's Umwelt or subjective world. <p> While visual realism was not a high priority of the present work, it would be easy, in principle, to connect Agar to a three-dimensional animation and world simulation system. Other work on animals within the computer animation field <ref> [Sims87] </ref> has concentrated on simulating the mechanics of walking in various animals.
Reference: [Sloane86] <author> Burt Sloane, David Levitt, Michael Travers, Bosco So, and Ivan Cavero. Hookup: </author> <title> a software kit. </title> <note> 1986. Unpublished software. </note>
Reference-contexts: The animal's program must be coded as a single process that consists of a loop that checks all conditions, and has no provisions for interrupts or other constructs that would allow behavioral flexibility. These limitations are the focus of current research. 2.2.2 Graphic programming Graphic programming <ref> [Sutherland63, Smith77, Sloane86] </ref> refers to techniques that allow programs to be specified by manipulating diagrams rather than text. Graphic representations of programs can exploit common-sense notions such as connectivity, enclosure, and flow. <p> As the diagrams grow, they tend to become even less readable than textual programs; furthermore, not all programming constructs are easily translated to graphics, for example, iteration and state are hard to represent in graphic languages based on dataflow models <ref> [Sloane86] </ref>. BrainWorks (see section 5.1) uses graphic programming technology successfully because the complexity of programs, and hence diagrams, is inherently limited by the simplicity of the world model. CHAPTER 2.
Reference: [Smith77] <author> David Canfield Smith. Pygmalion: </author> <title> A computer Program to Model and Stimulate Creative Thought. </title> <publisher> Birkhauser, Basil und Stuttgart, </publisher> <year> 1977. </year>
Reference-contexts: The animal's program must be coded as a single process that consists of a loop that checks all conditions, and has no provisions for interrupts or other constructs that would allow behavioral flexibility. These limitations are the focus of current research. 2.2.2 Graphic programming Graphic programming <ref> [Sutherland63, Smith77, Sloane86] </ref> refers to techniques that allow programs to be specified by manipulating diagrams rather than text. Graphic representations of programs can exploit common-sense notions such as connectivity, enclosure, and flow.
Reference: [Smolensky88] <author> Paul Smolensky. </author> <title> On the proper treatment of connectionism. </title> <journal> Behavioral and Brain Sciences, </journal> <year> 1988. </year> <month> forthcoming. </month>
Reference-contexts: BACKGROUND, BASIC CONCEPTS, AND RELATED WORK 15 [Fentress76]. His critique bears much the same relationship to orthodox ethological modeling as the connectionist critique of symbolic processing <ref> [Smolensky88] </ref> does to mainstream artificial intelligence. Fentress has studied the development of hierarchically-structured grooming patterns in rats [Fentress83]. Apparently juvenile rats have a less differentiated repertoire of movements than do mature rats, resulting in phenomena such as "motor traps".
Reference: [Suchman87] <author> Lucy A. Suchman. </author> <title> Plans and situated actions: The Problem of human-machine communication. </title> <publisher> Cambridge University Press, </publisher> <year> 1987. </year>
Reference-contexts: This notion of planning has come under attack as unrepresentative of the way in which intelligent beings actually act in the world <ref> [Suchman87, Agre87b] </ref>. We do not compute a plan ahead of time and then carry it out; instead we are in an interactive relationship with the world, continuously exploring possibilities for actions. This is what gives us the flexibility to deal with a world that cannot be fully represented in memory.
Reference: [Sutherland63] <author> Ivan Sutherland. </author> <title> Sketchpad: A Man-machine Graphical Communications System. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1963. </year>
Reference-contexts: The animal's program must be coded as a single process that consists of a loop that checks all conditions, and has no provisions for interrupts or other constructs that would allow behavioral flexibility. These limitations are the focus of current research. 2.2.2 Graphic programming Graphic programming <ref> [Sutherland63, Smith77, Sloane86] </ref> refers to techniques that allow programs to be specified by manipulating diagrams rather than text. Graphic representations of programs can exploit common-sense notions such as connectivity, enclosure, and flow.
Reference: [Symbolics88] <institution> Symbolics Common Lisp| Language Concepts. Symbolics, Inc., </institution> <year> 1988. </year>
Reference-contexts: The process of building a simulation consists of choosing a world, defining creatures (including their sensorimotor capabilities and appearance) and defining agents for the creatures. 36 CHAPTER 4. 3-AGAR: AGENTS AS GATED RULES 37 3-Agar uses Symbolics New Flavors <ref> [Symbolics88] </ref>, an object-oriented programming package. For the most part the implementation details are invisible to the user, although there are mechanisms for accessing the underlying flavor mechanisms if necessary. 4.1.1 Creature definition New classes of creatures are defined by a defcreature form.
Reference: [Tinbergen51] <author> Niko Tinbergen. </author> <title> The Study of Instinct. </title> <publisher> Oxford University Press, Oxford, </publisher> <year> 1951. </year>
Reference-contexts: Tinbergen developed such a theory based on hierarchically-structured drive centers <ref> [Tinbergen51] </ref>. The three-spined stickleback is a small fish that Tinbergen studied extensively. The behavioral hierarchy for the stickleback reproductive cycle is illustrated in figure 2.1. In this model, "drive energy" moves downward from the top unit to lower units under the control of innate releasing mechanisms (IRMs). <p> The most striking cases often involve two animals that are involved in a dynamic pattern of such responses, particularly in mating rituals such as the zig-zag dance of the stickleback <ref> [Tinbergen51] </ref>. We can eliminate some more state as such by considering the physiological part of the animal as external to its behavioral control system.
Reference: [Turner87] <author> Scott R. Turner and Seth Goldman. </author> <title> Ram Design Notes. 1987. </title> <journal> UCLA Artificial Intelligence Laboratory. </journal>
Reference-contexts: Braitenberg continued the tradition of building simple autonomous animals [Braitenberg84] by designing a series of imaginary robotic animals to illustrate concepts of "synthetic psychology". More recent animal simulation systems emphasize ecological interaction by supporting more than one species of artificial life. RAM <ref> [Turner87] </ref> and EVOLVEIII [Rizki86] fall into this category. The latter is particularly interesting because it simulates several levels of biological organization (genetic, behavioral, and ecological) and the relationships among them. Stewart Wilson has studied learning in artificial animals [Wilson87]. <p> These include worlds with standard euclidean geometry and simplified physics, more realistic worlds that can simulate locomotion (such as balance and limb placement) in greater detail [Sims87], more abstract worlds such as cellular arrays <ref> [Turner87, Rizki86] </ref>, or completely non-physical game-theory worlds for simulating theoretical problems of behavioral strategy [Axelrod84]. The world, sensors, and motors together constitute an implementation of the animal's Umwelt or subjective world. <p> The worlds used in the examples are all two-dimensional euclidean worlds with a continuous coordinate system. This is in contrast to most other animal simulation systems (ie, <ref> [Turner87] </ref> [Coderre88] [Wilson86]) that use discrete grids. I found that grids failed to capture certain CHAPTER 3. ANIMAL PARTS 31 important properties.
Reference: [Walter50] <author> W. G. Walter. </author> <title> An imitation of life. </title> <publisher> Scientific American, </publisher> <pages> 42-45, </pages> <month> May </month> <year> 1950. </year> <note> BIBLIOGRAPHY 85 </note>
Reference-contexts: Early cyberneticians built simple, reflex-based artificial animals to demonstrate the possibility of mechanical behavior and learning. Grey Wal-ter's turtles <ref> [Walter50] </ref> responded to light and could learn by a mechanism that implemented classical conditioning. Braitenberg continued the tradition of building simple autonomous animals [Braitenberg84] by designing a series of imaginary robotic animals to illustrate concepts of "synthetic psychology". <p> Creatures are also responsible for interfacing to the display system. They may contain other state specific to their species (such as an individual ant's caste). The basic design of creatures in the two-dimensional world is based on the Logo turtle [Papert80] and its autonomous antecedent <ref> [Walter50] </ref>. They implement a form of local computational geometry. The locality is critical: all the turtle's operations are expressed as relative to its present position and orientation, rather than to a global coordinate system. This is appropriate for simulating animals, all of whose information and action must necessarily be local.
Reference: [Wilson71] <author> Edward O. Wilson. </author> <title> The Insect Societies. </title> <publisher> The Belknap Press of Harvard University Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1971. </year>
Reference-contexts: In this case, the oracular knowledge is justified as a stand-in for simulating the details of the ant's true navigational abilities, which depend upon a combination of sun-compass navigation, landmark learning, and integration of the outward trip <ref> [Wilson71, p. 216] </ref>. The oracle is an abstraction for the ant's true abilities rather than a cheat. The goal of the sensorimotor interface language is to force the user to explicitly declare such abstractions, and so justify them. CHAPTER 3. <p> These are simulating sensors at the ends of the ant's antennae, and so have a sensitive range that is a small area, displaced from the creature's position by a short vector. 4.5 Example: ant foraging and recruitment Ant colonies can coordinate the activity of their inhabitants in many ways <ref> [Wilson71] </ref>. One of the most striking methods of coordination is food recruitment|the process by which a foraging ant who has located a large source of food will lead other ants to the source to help carry it back to the nest. Highly-developed species of ants do this with odor trails.
Reference: [Wilson86] <author> Stewart W. Wilson. </author> <title> Knowledge growth in an artificial animal. </title> <editor> In Kumpati S. Narendra, editor, </editor> <title> Adaptive and Learning Systems, </title> <publisher> Plenum Publishing Corporation, </publisher> <year> 1986. </year>
Reference-contexts: The worlds used in the examples are all two-dimensional euclidean worlds with a continuous coordinate system. This is in contrast to most other animal simulation systems (ie, [Turner87] [Coderre88] <ref> [Wilson86] </ref>) that use discrete grids. I found that grids failed to capture certain CHAPTER 3. ANIMAL PARTS 31 important properties.
Reference: [Wilson87] <author> Stewart W. Wilson. </author> <title> Classifier systems and the animat problem. </title> <journal> Machine Learning, </journal> <volume> 2(3), </volume> <year> 1987. </year>
Reference-contexts: RAM [Turner87] and EVOLVEIII [Rizki86] fall into this category. The latter is particularly interesting because it simulates several levels of biological organization (genetic, behavioral, and ecological) and the relationships among them. Stewart Wilson has studied learning in artificial animals <ref> [Wilson87] </ref>. He used classifier systems, which learn by mutating rules in a manner derived from biological genetics. This system had some success in learning simple regularities in a gridlike world. Wilson suggests that in practice, classifiers must be arranged in hierarchical modules that learn to control each other.
Reference: [Winograd87] <author> Terry Winograd and Fernando Flores. </author> <title> Understanding Computers and Cognition. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1987. </year>
Reference-contexts: CHAPTER 2. BACKGROUND, BASIC CONCEPTS, AND RELATED WORK 24 The phenomenological critique of AI The idea that abstract thought is independent of its embedding in a body has been attacked by Dreyfus and others <ref> [Dreyfus79, Winograd87] </ref>. The starting point for this attack is the philosophy of Heidegger, which emphasizes the "thrownness" of everyday activity and the dependence of action on "background". Thrownness refers to the fact that an intelligent agent is always in a situation and cannot avoid acting within and on that situation. <p> The background is the implicit beliefs and assumptions that inform action. Heidegger concluded that these beliefs are not amenable to representation and cannot be made explicit <ref> [Winograd87, p. 32] </ref>. In contrast, one of the long-standing goals of AI has been to explicitly represent what it calls "common-sense knowledge", a concept that seems to correspond to the background. <p> People and computers can demonstrate their intelligence by manipulating symbols, but animals have no such ability, or only very limited forms of it. Instead, they display their intelligence by action in their world. The appropriateness of an animal's actions to its situation constitutes its intelligence. Winograd <ref> [Winograd87] </ref> refers to this close relationship between an intelligent agent and its environment as structural coupling, and suggests that it is a fundamental to the understanding of human intelligence. This topic is discussed further in section 2.4.2. We want our systems to run in near-real-time on present-day hardware.
Reference: [Zeltzer87] <author> David Zeltzer. </author> <title> Motor problem solving for three dimensional computer animation. </title> <booktitle> In Proc. L'Imaginaire Numerique, </booktitle> <year> 1987. </year>
Reference-contexts: In addition to affecting the actions of particular characters, agents can manipulate the film medium itself in order to aid the story-telling process, for instance, by introducing a close shot of two characters to suggest intimacy between them. Zeltzer outlines a scheme for intelligent motor control <ref> [Zeltzer87] </ref>. It distinguishes between two domains of problem solving, cognitive and motor. Motor problem solving does not use symbolic representations. Instead it uses a lattice of behavioral skills that is quite similar to Tinbergen's hierarchy of drive centers (see section 2.1.1).
References-found: 63

