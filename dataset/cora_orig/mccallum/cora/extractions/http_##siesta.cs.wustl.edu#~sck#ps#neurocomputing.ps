URL: http://siesta.cs.wustl.edu/~sck/ps/neurocomputing.ps
Refering-URL: http://siesta.cs.wustl.edu/~sck/
Root-URL: 
Email: barry@cs.wustl.edu sck@cs.wustl.edu  
Phone: (314) 935-7539  
Title: High Performance Training of Feedforward Simple Recurrent Networks  
Author: Barry L. Kalman and Stan C. Kwasny 
Note: This material is based upon work supported by the National Science Foundation under Grant No. IRI-9201987.  
Address: St. Louis, MO 63130  
Affiliation: Department of Computer Science Washington University  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> E. Barnard(1992), </author> <title> Optimization for Training Neural Nets, </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> 3(2), </volume> <pages> 232-241. </pages>
Reference-contexts: This creates difficulties which skip connections overcome by permitting the linear (or perceptron) part of the mapping to be associated with the direct connections from input to output layers. See Figure 1 and Section 2.2. 5 FIGURE 1 ABOUT HERE The <ref> [0, 1] </ref> interval. The bipolar ([-1, 1]) interval is better for Boolean functions and many other problems. While a simple affine transformation can map between the two inter vals, avoiding zero as an extreme value can have an important effect on training. Sec tion 2.3 contains more discussion. <p> Therefore, the four criteria and the finite interval force the following differential equation to hold: (EQ 8) For the interval <ref> [-1, 1] </ref>, the only functional form which satisfies EQ 8 is: (EQ 9) This relation leads to the scaling function: (EQ 10) We have shown in [11] that selecting the value: g pk pk -= s pk ki pi b += pk pk 1 s 2 s' l 1 s = <p> For layer n (by counting back from the output layer) and from EQ 8 we get By integrating over the interval <ref> [-1, 1] </ref> with respect to we get . In [10] we reported on the performance of our error function on the problem of training a feedforward network to be a deterministic parser (see Marcus [16, 272-276]) for a medium sized subset of English grammar. <p> FIGURE 2 ABOUT HERE 2.3 Singular Value Decomposition for Preprocessing Input We previously reported [12] the use of singular value decomposition to preprocess input. We discussed the use of singular value decomposition along with an affine transformation to place the inputs in the interval <ref> [-1, 1] </ref>. <p> If singular value decomposition causes h columns to be removed then let be the number of remaining columns. U is , G is and V is . Let Q = UG and construct an affine transformation that maps the transformed patterns of Q into <ref> [-1, 1] </ref> by computing: (EQ 14) (EQ 16) A is the transformed input matrix of p patterns over I input units and: (EQ 18) The training takes place using A . <p> Others (see, for example <ref> [1] </ref>) have independently found it very useful. Because of quadratic convergence it uses fewer epochs (often many fewer) than backprop especially if a Powell update [22] is used. The conjugate gradient algorithm requires linear storage (in weights) while Newton-style quadratically convergent methods require quadratic storage.
Reference: [2] <author> J. L. Elman(1988), </author> <title> Finding Structure in Time, </title> <type> CRL Technical Report 8801, </type> <institution> Center for Research in Language, University of California, </institution> <address> San Diego. </address>
Reference-contexts: The middle number for each network 12 size is the number of hidden units. The percentage of the variables taken up with skip connections reects the degree of linearity present in the problem. For our recurrent networks we use only feedback from the hidden layer ala Elman <ref> [2] </ref>. More general recurrent networks are discussed by Williams and Zipser [32]. The nodes to which the outputs of the hidden units are fed back we call the pseudo input layer. We connect the pseudo input layer just as we do the ordinary input layer.
Reference: [3] <author> Laurene Fausett(1994), </author> <title> Fundamentals of Neural Networks: Architectures, Algorithms and Applications, </title> <publisher> Prentice Hall. </publisher> <pages> 30 </pages>
Reference-contexts: Singular value decomposition may also reduce the number of input units needed to represent the input without a significant effort in feature extraction. (Fausett <ref> [3, 309-312] </ref> also has reported uses for the [-1,1] interval.) We also reported a transformation back from the parameters of the transformed input into the parameters of TABLE 2.
Reference: [4] <author> Shelly D. D. Goggin, Karl E. Gustafson and Kristina M. Johnson, </author> <title> An Asymptotic Singular Value Decomposition Analysis of Nonlinear Multilayer Neural Networks(1991), </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> 1, </volume> <pages> 785-790. </pages>
Reference-contexts: We summarize those results here. Details of singular value decomposition are presented in [5, 427-435]. Uses of singular value decomposition to analyze the hidden layer are presented in <ref> [4] </ref> and [33]. Let A be the original input matrix which is , where p is the number of patterns and I is the number of input units. Then the singular value decomposition of A is: (EQ 13) where U and V are orthonormal and G is diagonal.
Reference: [5] <author> Gene H. Golub and Charles F. Van Loan(1989), </author> <title> Matrix Computations 2d Edition, </title> <publisher> The Johns Hopkins University Press. </publisher>
Reference-contexts: The self-scaling error function has been a key element in the success of this effort. See Section 2.1. Use of singular value decomposition to preprocess input. We have found it productive to use singular value decomposition <ref> [5] </ref> on the collection of input patterns. The benefits are two-fold: (i) re-orientation of the input space so that inputs are orthogonally aligned; (ii) analysis of input units for the purpose of eliminating useless ones. See Section 2.3. The bottleneck of recurrent network training. <p> We summarize those results here. Details of singular value decomposition are presented in <ref> [5, 427-435] </ref>. Uses of singular value decomposition to analyze the hidden layer are presented in [4] and [33]. Let A be the original input matrix which is , where p is the number of patterns and I is the number of input units. <p> There are terms of the type on the left side of the equation and each one involves the sum over H terms. If the sum part of EQ35 is evaluated as a scalar product the code to implement EQ 35 can be optimized for a scalar pipelined processor <ref> [5, 36-37] </ref>.
Reference: [6] <author> D. Gorse, A. Shepherd and J. G. Taylor(1994), </author> <title> A Classical Algorithm for Avoiding Local Minima, </title> <booktitle> Proceedings of the World Congress on Neural Networks, </booktitle> <address> San Diego, CA, </address> <booktitle> III, </booktitle> <pages> 364-369. </pages>
Reference-contexts: As an upper bound, we find the minimum hidden layer that can explain all the data. 3.5 Adjusting the Targets to Avoid Local Minima We have found that adjusting the targets for the output units in the fashion of Gorse, Shepherd and Taylor <ref> [6] </ref> avoids local minima that stop the conjugate gradient algorithm if we hold the targets at . We have modified the method slightly. We start with targets of which avoids many of the local minima found with and also avoids extremum found with all outputs = 0. <p> This is found to be sufficient to obtain what appear to be global extrema. We then iteratively increase the targets to . We have found that occasionally the solution gets better and never gets worse during the target increase. This phenomena was not reported in <ref> [6] </ref>. We attribute it to our choice of error function. Since this error function approches infinity at the wrong end of the interval big errors encountered as the targets are increased may be dramatically reduced, which yields the observed improvements.
Reference: [7] <author> Barry L. Kalman(1990), </author> <title> Superlinear Learning in Back-Propagation Neural Networks, </title> <type> Technical Report WUCS-90-21, </type> <institution> Washington University. </institution>
Reference-contexts: We also discuss an adaptive step size control technique which further enhances derivative free line search. 17 3.1 Conjugate Gradient Algorithm We use the conjugate gradient algorithm <ref> [7] </ref>, [20], [22], [23, 420-425]because it is quadrat-ically convergent, it behaves as well as backprop in tough regions and it is possible to use less expensive derivative free line search for most of the training. Others (see, for example [1]) have independently found it very useful.
Reference: [8] <author> Gary M. Kuhn and Norman P. Herzberg(1991), </author> <title> Some Variations on Training of Recurrent Networks, </title> <publisher> Academic Press. </publisher>
Reference-contexts: The nodes to which the outputs of the hidden units are fed back we call the pseudo input layer. We connect the pseudo input layer just as we do the ordinary input layer. This is illustrated in Figure 2. Others <ref> [8] </ref> use only self feedback in which feedback units are only connected to themselves. In Section 4 we show how the pseudo input can be treated as ordinary input.
Reference: [9] <author> Stan C. Kwasny, Sahnny Johnson and Barry L. Kalman(1994), </author> <title> Recurrent Natural Language Parsing, </title> <booktitle> Proceedings of the Sixteenth Annual Conference of the Cognitive Science Society, </booktitle> <address> Atlanta, GA, 1,525-530. </address>
Reference-contexts: In our NETTalk [27] experiment, there were 5,523 patterns, but only 5,208 of those led to unique states. Therefore, no more than 13 feedback units should be required, which we have verified empirically. In training a recurrent neural network to be a deterministic parser for English <ref> [9] </ref>, we found that the 7,659 patterns resulted in 5,433 unique states which suggested an initial choice of 13 feedback units. This too has been verified experimentally.
Reference: [10] <author> Barry L. Kalman and Stan C. Kwasny(1991), </author> <title> A Superior Error Function for Training Neural Networks, </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> 2, </volume> <pages> 49-52. </pages>
Reference-contexts: Given our assumption of epoch-based training, we can analyze the characteristics of the error function over an entire collection of patterns. From our studies on the convergence of the conjugate gradient method <ref> [10] </ref> we have identified four criteria for the behavior of an error function on a finite interval. <p> For layer n (by counting back from the output layer) and from EQ 8 we get By integrating over the interval [-1, 1] with respect to we get . In <ref> [10] </ref> we reported on the performance of our error function on the problem of training a feedforward network to be a deterministic parser (see Marcus [16, 272-276]) for a medium sized subset of English grammar. The network had 53 inputs, 22 outputs, 30 hidden units and no skip connections. <p> The network had 53 inputs, 22 outputs, 30 hidden units and no skip connections. The data set consisted of 113 training patterns. Table 1 contains a summary of results from <ref> [10] </ref>. These results are strong evidence for using the self-scaling error function. The entries of in Table 1 mean that the experiment always terminated at an exterior saddle point with incorrect predictions. This did not occur for the other experiments. TABLE 1.
Reference: [11] <author> Barry L. Kalman and Stan C. Kwasny(1992), </author> <title> Why Tanh: Choosing a Sigmoidal Function, </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, IV, </booktitle> <pages> 578-581. 31 </pages>
Reference-contexts: Therefore, the four criteria and the finite interval force the following differential equation to hold: (EQ 8) For the interval [-1, 1], the only functional form which satisfies EQ 8 is: (EQ 9) This relation leads to the scaling function: (EQ 10) We have shown in <ref> [11] </ref> that selecting the value: g pk pk -= s pk ki pi b += pk pk 1 s 2 s' l 1 s = s x ( ) lx ( )tanh= s 1 a 2 10 maintains an equitable scaling of weight layers for training purposes.
Reference: [12] <author> Barry L. Kalman, Stan C. Kwasny and Aurorita Abella(1993), </author> <title> Decomposing Input Patterns to Facilitate Training, </title> <booktitle> Proceedings of the World Congress on Neural Networks, </booktitle> <address> Portland, OR, </address> <booktitle> III, </booktitle> <pages> 503-506. </pages>
Reference-contexts: Of course, TRAINREC permits disconnection of inputs and outputs by user choice. This is an essential choice, of course, for recursive auto associative memory networks. In <ref> [12] </ref> we reported on several problems for which we used TRAINREC. We employed skip connections in all of these cases. <p> Others [8] use only self feedback in which feedback units are only connected to themselves. In Section 4 we show how the pseudo input can be treated as ordinary input. FIGURE 2 ABOUT HERE 2.3 Singular Value Decomposition for Preprocessing Input We previously reported <ref> [12] </ref> the use of singular value decomposition to preprocess input. We discussed the use of singular value decomposition along with an affine transformation to place the inputs in the interval [-1, 1]. <p> After training it is useful to transform the parameters of the network to a network which uses the original input. We show in <ref> [12] </ref> that for weights, , on connections which involve the input units, weights that perform equivalently on the original patterns can be obtained as follows: p I T I' I h= r min Q = i 1 k p ki c 2 i i -= i i - s r +(
Reference: [13] <author> Stan C. Kwasny, Barry L. Kalman, A. Maynard Engebretson, and Weilan Wu, </author> <title> Real-time Identification of Language from Raw Speech Waveforms(1993), </title> <booktitle> Proceedings of the International Workshop on Applications of Neural Networks to Telecommunications, </booktitle> <publisher> Lawrence Erlbaum Associates, </publisher> <pages> 161-167. </pages>
Reference-contexts: FIGURE 3 ABOUT HERE 4.3 Settling and Voting We used both settling and voting to aid in the training of a recurrent network that successfully identifies which of two languages is being spoken <ref> [13] </ref>. The language identification problem uses raw speech signals and therefore is an example of a class of problems in which a possibly noisy sequence of inputs all predict the same output while the lengths of the sequences may vary.
Reference: [14] <author> Stan C. Kwasny, Barry L. </author> <title> Kalman, Tail-recursive Distributed Representations and Simple Recurrent Networks, </title> <journal> Connection Science, </journal> <volume> 7(1), </volume> <year> 1995, </year> <pages> pp. 61-80. </pages>
Reference-contexts: More general architectures are often desired as discussed in Sec tion 4.1. Static targets for Recursive Auto-Associative Memories. Recursive auto associative memories cleverly use auto-associativity to permit representation of complex structures as distributed patterns (see [21] and <ref> [14] </ref>). In our experience, the effect of the variability of the target is usually not properly considered under standard implementations of backprop. See Section 4.2. 6 1.2 Primary Discoveries In building TRAINREC we made several discoveries which are helping us improve the efficiency of training. <p> We now analyze recursive auto associative memories where O = H + I. The simple recurrent neural network version of recursive auto associative memories that we use is described in <ref> [14] </ref>. By definition, we are not able to use skip connections with our recursive auto associative memories. <p> Since RAAM Tk depends on the network parameters, we add a correction term to EQ 34 to get (EQ 37) Where SRN is the derivative of the term which corresponds to EQ 34. We reported in <ref> [14] </ref> that without this correction term, training for recursive auto associative memories usually does not converge and the error function often oscillates. A sample problem for which EQ 37 is important is a five level stack with a set of 3 symbols.
Reference: [15] <author> Samuel E. Lee and Bradley R. Holt(1992), </author> <title> Regression Analysis of Spectroscopic Process Data Using a Combined Architecture of Linear and Nonlinear Artificial Neural Networks, </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, IV, </booktitle> <pages> 549-554. </pages>
Reference-contexts: Some require only a linear component and are therefore solvable by a perceptron. Connection of input units to output units places the linear component of the solution in those connections while the non-linear component is isolated in the hidden layer weights <ref> [15] </ref>. For a simple example, consider the simplest architecture capable of learning XOR. Connecting inputs to outputs is required to give a 2-1-1 network. Other evidence comes from work on genetic algorithms used to prune neural nets [29] which shows that connections from input to output are important.
Reference: [16] <author> Mitchell Marcus(1980), </author> <title> A Theory of Syntactic Recognition for Natural Language, </title> <publisher> MIT Press. </publisher>
Reference-contexts: In [10] we reported on the performance of our error function on the problem of training a feedforward network to be a deterministic parser (see Marcus <ref> [16, 272-276] </ref>) for a medium sized subset of English grammar. The network had 53 inputs, 22 outputs, 30 hidden units and no skip connections. The data set consisted of 113 training patterns. Table 1 contains a summary of results from [10].
Reference: [17] <author> J. L. McClelland and D. E. Rumelhart(1988), </author> <title> Explorations in Parallel Distributed Processing: A Handbook of Models, Programs and Exercises, </title> <publisher> MIT Press. </publisher>
Reference-contexts: We have established the effectiveness of TRAINREC by applying it to dozens of problems. Our approach in this paper is to examine backward error propagation (backprop) under popular assumptions (e.g., those contained within the software package by McClelland & Rumelhart <ref> [17] </ref>) and point out where they can be improved or altered slightly to achieve a 4 more positive result. We also point out areas of popular misconceptions where appropriate.
Reference: [18] <author> Alexander M. Mood, Franklin A. Graybill and Duane C. Boes(1974), </author> <title> Introduction to the Theory of Statistics, </title> <publisher> 3rd ed., McGraw-Hill. </publisher>
Reference-contexts: This measure tends to be high when performance is distributed over both frequent and infrequent categories. Unfortunately it can be high under other conditions. The confusion matrix is the same as a two-way contingency table discussed in <ref> [18, 452-461] </ref>. Construction of the confusion matrix is discussed in Section 2.4. The third measure concerns maximizing performance of the most difficult case.
Reference: [19] <author> Peter J. McCann and Barry L. Kalman(1994), </author> <title> Parallel Training of Simple Recurrent Neural Networks, </title> <booktitle> Proceedings of the IEEE World Congress on Computational Intelligence(WCCI 94), </booktitle> <address> Orlando, FL, </address> <publisher> I, </publisher> <pages> 167-170. 32 </pages>
Reference-contexts: Even more of the potential concurrency in EQ 35 can be implemented on a shared memory multi-processor computer as demonstrated in McCann and Kalman <ref> [19] </ref>. 4.2 Feedback Derivatives for Recursive Auto Associative Memories w D a p a pi ji a d a ji a w s d li H 25 Recursive auto associative memories are designed to use auto-associative training to evolve a collection of representations for structures.
Reference: [20] <author> E. Polak(1971), </author> <title> Computational Methods in Optimization: A Unified Approach, </title> <publisher> Academic Press. </publisher>
Reference-contexts: We also discuss an adaptive step size control technique which further enhances derivative free line search. 17 3.1 Conjugate Gradient Algorithm We use the conjugate gradient algorithm [7], <ref> [20] </ref>, [22], [23, 420-425]because it is quadrat-ically convergent, it behaves as well as backprop in tough regions and it is possible to use less expensive derivative free line search for most of the training. Others (see, for example [1]) have independently found it very useful.
Reference: [21] <author> Jordan Pollack, </author> <title> Recursive Distributed Representations(1990), </title> <journal> Artificial Intelligence, </journal> <volume> 46, </volume> <pages> 77-105. </pages>
Reference-contexts: Thus, efficient training methods are essential for the field to progress. In this article we present a unique collection of methods and features which we have combined into an efficient system called TRAINREC, for training feedforward neural networks, simple recurrent neural networks and recursive auto associative memories <ref> [21] </ref>. The emphasis in TRAINREC is on training networks with recurrent architecture and hence the name. We have established the effectiveness of TRAINREC by applying it to dozens of problems. <p> More general architectures are often desired as discussed in Sec tion 4.1. Static targets for Recursive Auto-Associative Memories. Recursive auto associative memories cleverly use auto-associativity to permit representation of complex structures as distributed patterns (see <ref> [21] </ref> and [14]). In our experience, the effect of the variability of the target is usually not properly considered under standard implementations of backprop. See Section 4.2. 6 1.2 Primary Discoveries In building TRAINREC we made several discoveries which are helping us improve the efficiency of training.
Reference: [22] <author> M. J. D. Powell(1977), </author> <title> Restart Procedures for the Conjugate Gradient Method, </title> <journal> Mathematical Programming, </journal> <volume> 12(2), </volume> <pages> 241-254. </pages>
Reference-contexts: We also discuss an adaptive step size control technique which further enhances derivative free line search. 17 3.1 Conjugate Gradient Algorithm We use the conjugate gradient algorithm [7], [20], <ref> [22] </ref>, [23, 420-425]because it is quadrat-ically convergent, it behaves as well as backprop in tough regions and it is possible to use less expensive derivative free line search for most of the training. Others (see, for example [1]) have independently found it very useful. <p> Others (see, for example [1]) have independently found it very useful. Because of quadratic convergence it uses fewer epochs (often many fewer) than backprop especially if a Powell update <ref> [22] </ref> is used. The conjugate gradient algorithm requires linear storage (in weights) while Newton-style quadratically convergent methods require quadratic storage. The conjugate gradient algorithms generalization properties are very close to those of backprop.
Reference: [23] <author> William H. Press, Brian P. Flannery, Saul A. Teukolsky and William T. Vetter-ing(1988), </author> <title> Numerical Recipes in C, </title> <publisher> Cambridge University Press. </publisher>
Reference-contexts: We also discuss an adaptive step size control technique which further enhances derivative free line search. 17 3.1 Conjugate Gradient Algorithm We use the conjugate gradient algorithm [7], [20], [22], <ref> [23, 420-425] </ref>because it is quadrat-ically convergent, it behaves as well as backprop in tough regions and it is possible to use less expensive derivative free line search for most of the training. Others (see, for example [1]) have independently found it very useful. <p> The results in Table 1 give strong evidence for the use of conjugate gradient algorithm over ordinary backprop. 3.2 Derivative-Free Line Search The conjugate gradient algorithm requires a line search algorithm to find a local minimum in a particular direction for each iteration. Two line search algorithms are Brent <ref> [23, 404-405] </ref> and Dbrent [23, 406-408]. Dbrent requires the magnitude of the gradient in the search direction for each inner iteration. Brent only requires the value of the error function for each inner iteration. <p> Two line search algorithms are Brent [23, 404-405] and Dbrent <ref> [23, 406-408] </ref>. Dbrent requires the magnitude of the gradient in the search direction for each inner iteration. Brent only requires the value of the error function for each inner iteration. First we analyze training for ordinary feedforward networks to see the effect of using derivative free line search. <p> We get: (EQ 28) which leads to , the same as for simple recurrent neural networks. 3.3 Adaptive Step Size Control Line search requires a region known to contain a local minimum before it can start. The algorithm mnbrak <ref> [23, 400-401] </ref> locates such a region. It requires three points along the search direction to initialize it. At the beginning of conjugate gradient algorithm we use the ratio of the error function to the magnitude of the gradient as a start for mnbrak. <p> If making the correct prediction for the smaller category is important such a network will be a failure. The second measure is a computation (as specified in <ref> [23, 628-632] </ref>) on the confusion matrix. This measure tends to be high when performance is distributed over both frequent and infrequent categories. Unfortunately it can be high under other conditions. The confusion matrix is the same as a two-way contingency table discussed in [18, 452-461].
Reference: [24] <author> A. K. Rigler, J. M. </author> <title> Irvine and T.P. Vogl(1991), Rescaling of Variables in Back Propagation Learning, </title> <booktitle> Neural Networks, </booktitle> <volume> 4, </volume> <pages> 225-229. </pages>
Reference: [25] <author> David E. Rumelhart, James L. </author> <title> McClelland and the PDP Research Group(1986), Parallel Distributed Processing, 1, </title> <publisher> The MIT Press. </publisher>
Reference-contexts: Target, , and activation, , are specific to an input pattern presentation and an output unit. The derivative equations are given by the generalized delta rule <ref> [25] </ref> with an additional term due to the feedback units.
Reference: [26] <author> David Servan-Schreiber, Axel Cleeremans and James L. McClelland(1988), </author> <title> Encoding Sequential Structure in Simple Recurrent Networks, </title> <type> Technical Report CMU-CS-88-183, </type> <institution> Carnegie Mellon University. </institution>
Reference: [27] <author> Terrence J. Sejnowski and Charles R. Rosenberg(1987), </author> <title> Parallel Networks that Learn to Pronounce English Text, </title> <journal> Complex Systems, </journal> <volume> 1, </volume> <pages> 145-168. </pages>
Reference-contexts: This is because every vector is orthogonal to the zero vector. Vector best match is used quite effectively, for example, in NetTalk <ref> [27] </ref>. c 2 16 In our work on recursive auto associative memories, we have both distributed patterns and category vectors as outputs. To test which symbol is represented by the output vector that is extracted we use vector best match. <p> Presumably the feedback units would be utilized in a more distributed manner, but selecting more feedback units than this would certainly over-specify the network leading to poor generalization. In our NETTalk <ref> [27] </ref> experiment, there were 5,523 patterns, but only 5,208 of those led to unique states. Therefore, no more than 13 feedback units should be required, which we have verified empirically.
Reference: [28] <author> Jacques de Villiers and Etienne Barnard(1993), </author> <title> Backpropagation Neural Nets with One and Two Hidden Layers, </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(1), </volume> <pages> 136-141. </pages>
Reference-contexts: De Vil-liers and Barnard found that with nearly equal numbers of weights feedforward networks with one and two hidden layers performed equally well and networks with two hidden layers had more frequent occurrences of local extrema during training <ref> [28] </ref>. If one is testing for the optimal number of hidden units for a network then the case of zero hidden units is a more natural limiting case than for a network without skip connections. Of course, TRAINREC permits disconnection of inputs and outputs by user choice.
Reference: [29] <author> Darrell Whitley and Christopher Bogart(1990), </author> <title> The Evolution of Connectivity: Pruning Neural Networks Using Genetic Algorithms, </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> 1, </volume> <pages> 134-137. 33 </pages>
Reference-contexts: For a simple example, consider the simplest architecture capable of learning XOR. Connecting inputs to outputs is required to give a 2-1-1 network. Other evidence comes from work on genetic algorithms used to prune neural nets <ref> [29] </ref> which shows that connections from input to output are important. Without skip connections more nodes are needed in the hidden layer to account for the linear component of the solution. We use one hidden layer because it is the smallest number for which non-linear separation can be effected.
Reference: [30] <author> Sholom M. Weiss and Casimir A. Kulikowski(1990), </author> <title> Computer Systems That Learn, </title> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: We observe that less than O H 4 ( ) De Fe 2pH 2 H I+( ) 2H I+( )+= Tf 20 50% of the derivative free line search epochs are needed when compared with that required by a fixed initial search range. 3.4 Tests of Generalization Weiss and Kulikowski <ref> [30, 31-33] </ref> suggest that the technique of cross validation be used when only a modest amount of data is available, which is the case in most neural network investigations. Cross validation involves randomly dividing the data into several roughly equally sized sets.
Reference: [31] <author> Weilan Wu, Stan C. Kwasny, Barry L. Kalman and A. Maynard Engebretson(1993), </author> <title> Identifying Language from Raw Speech: An Application of Recurrent Neural Networks, </title> <booktitle> Proceedings of the Midwest Artificial Intelligence and Cognitive Science Conference, </booktitle> <volume> 1, </volume> <pages> 53-57. </pages>
Reference-contexts: But for a sequence of events in which each event is supposed to predict the same output, we can think of the N column as being an upper bound of the length of sequence needed for virtually always correct prediction. In <ref> [31] </ref> we reported on a recurrent network which was trained by TRAINREC to identify which of two languages a speaker was using. The overall performance on patterns in the test set was 78.9%. The worst case was 100/190 where 96/190 would have been acceptable.
Reference: [32] <author> Ronald. J. Williams and David Zipser(1989), </author> <title> A Learning Algorithm for Continually Running Fully Recurrent Neural Networks, </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 270-280. </pages>
Reference-contexts: The percentage of the variables taken up with skip connections reects the degree of linearity present in the problem. For our recurrent networks we use only feedback from the hidden layer ala Elman [2]. More general recurrent networks are discussed by Williams and Zipser <ref> [32] </ref>. The nodes to which the outputs of the hidden units are fed back we call the pseudo input layer. We connect the pseudo input layer just as we do the ordinary input layer. This is illustrated in Figure 2.

References-found: 32

