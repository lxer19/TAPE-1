URL: http://www.research.att.com/~singer/papers/pstw.ps.gz
Refering-URL: http://www.research.att.com/~singer/pub.html
Root-URL: 
Title: BEYOND WORD N-GRAMS  
Author: FERNANDO C. PEREIRA YORAM SINGER NAFTALI TISHBY 
Address: Jerusalem, Israel  
Affiliation: AT&T Research  AT&T Research  The Hebrew University of  
Abstract: We describe, analyze, and evaluate experimentally a new probabilistic model for word-sequence prediction in natural language based on prediction suffix trees (PSTs). By using efficient data structures, we extend the notion of PST to unbounded vocabularies. We also show how to use a Bayesian approach based on recursive priors over all possible PSTs to efficiently maintain tree mixtures. These mixtures have provably and practically better performance than almost any single model. We evaluate the model on several corpora. The low perplexity achieved by relatively small PST mixture models suggests that they may be an advantageous alternative, both theoretically and practically, to the widely used n-gram models. 
Abstract-found: 1
Intro-found: 1
Reference: <author> N. Cesa-Bianchi, Y. Freund, D. Haussler, D. P. Helmbold, R. E. Schapire, M. K. War-muth. </author> <year> 1993. </year> <title> How to use expert advice. </title> <booktitle> Proceedings of the 24th Annual ACM Symposium on Theory of Computing (STOC). </booktitle>
Reference: <author> K. W. Church and W. A. Gale. </author> <year> 1991. </year> <title> A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 5 </volume> <pages> 19-54. </pages>
Reference: <author> K. W. Church and W. A. Gale. </author> <year> 1996. </year> <title> Inverse Document Frequency (IDF): A Measure of Deviations from Poisson. This volume. </title>
Reference-contexts: Natural language is often bursty <ref> (Church and Gale, 1996) </ref>, that is, rare or new words may appear and be used relatively frequently for some stretch of text only to drop to a much lower frequency of use for the rest of the corpus.
Reference: <author> A. DeSantis, G. Markowski, M. N. Wegman. </author> <year> 1988. </year> <title> Learning Probabilistic Prediction Functions. </title> <booktitle> Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pp. 312-328. </pages>
Reference: <author> R. A. Fisher, A. S. Corbet, C. B. Williams. </author> <year> 1943. </year> <title> The relation between the number of species and the number of individuals in a random sample of an animal population. </title>
Reference-contexts: The GT method sets fl s () = c s . This method has been given several justifications, such as a Poisson assumption on the appearance of new words <ref> (Fisher et al., 1943) </ref>. It is, however, difficult to analyze and requires keeping track of the rank of each word. Our online learning method and data structures favor instead any method that is based only on word counts.
Reference: <editor> J. Animal Ecology, </editor> <volume> Vol. 12, </volume> <pages> pp. 42-58. </pages>
Reference: <author> I. J. Good. </author> <year> 1953. </year> <title> The population frequencies of species and the estimation of population parameters. </title> <journal> Biometrika, </journal> <volume> 40(3) </volume> <pages> 237-264. </pages>
Reference: <author> I. J. Good. </author> <year> 1969. </year> <title> Statistics of Language: Introduction. Encyclopedia of Linguistics, Information and Control. </title> <editor> A. R. Meetham and R. A. Hudson, editors. </editor> <address> pages 567-581. </address> <publisher> Pergamon Press, Oxford, </publisher> <address> England. </address>
Reference-contexts: 1. Introduction Finite-state methods for statistical prediction of word sequences in natural language have had an important role in language processing research since the pioneering investigations of Markov and Shannon (1951). It is clear that natural texts are not Markov processes of any finite order <ref> (Good, 1969) </ref>, because of very long range correlations between words in a text such as arise from subject matter.
Reference: <author> S. M. Katz. </author> <year> 1987. </year> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Trans. on ASSP 35(3) </journal> <pages> 400-401. </pages>
Reference-contexts: The results are summarized in Table 4, and compared with a backoff trigram model <ref> (Katz, 1987) </ref>. Model Test Set (1) Test Set (2) Backoff Trigram 247.7 158.7 PST Mixture, D = 2 223 168 PST Mixture, D = 3, pruned 214.1 157.6 PST Mixture, D = 3 210.6 149.3 TABLE 4. Comparison of different PSTs and a backoff trigram. 6.
Reference: <author> F. Pereira, N. Tishby, and L. Lee. </author> <year> 1993. </year> <title> Distributional clustering of English words. </title> <booktitle> In 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 183-190, </pages> <institution> Columbus, Ohio. Ohio State University, Association for Computational Linguistics, Morristown, </institution> <address> New Jersey. </address>
Reference-contexts: Of course, these techniques still fail to represent subtler aspects of syntactic and semantic information. We plan to investigate how the present work may be refined by taking advantage of distributional models of semantic relations <ref> (Pereira et al., 1993) </ref>. In the next sections we present PSTs and the data structure for the word prediction problem. We then describe and shortly analyze the learning algorithm. We also discuss several implementation issues.
Reference: <author> D. Ron, Y. Singer, N. Tishby. </author> <year> 1996. </year> <title> The power of amnesia: learning probabilistic automata with variable memory length. </title> <note> Machine Learning journal (to appear in COLT94 special issue). </note>
Reference: <author> C. E. </author> <title> Shannon 1951. Prediction and Entropy of Printed English. </title> <journal> Bell Sys. Tech. J., </journal> <volume> Vol. 30, No. 1, </volume> <pages> pp. 50-64. </pages>
Reference: <author> D. D. Sleator and R. E. Tarjan. </author> <year> 1985. </year> <title> Self-Adjusting Binary Search Trees. </title> <journal> Journal of the ACM, </journal> <volume> Vol. 32, No. 3, </volume> <pages> pp. 653-686. </pages>
Reference-contexts: When we know in advance a (large) bound on vocabulary size, we represent the root node by arrays of word counts and possible sons subscripted by word indices. At other nodes, we use splay trees <ref> (Sleator and Tarjan, 1985) </ref> to store the branches to longer contexts. Splay trees support search, insertion and deletion in amortized O (log (n)) time per operation.
Reference: <author> F. M. J. Willems, Y. M. Shtarkov, T. J. Tjalkens. </author> <year> 1995. </year> <title> The context tree weighting method: basic properties. </title> <journal> IEEE Trans. on Inform. Theory, </journal> <volume> 41(3) </volume> <pages> 653-664. </pages>
Reference-contexts: Although there might be exponentially many different PSTs in the ensemble, it has been recently shown <ref> (Willems et al., 1995) </ref> that a mixture of PSTs can be efficiently represented for small alphabets. We will use here Bayesian formalism to derive an online learning procedure for mixtures of PSTs of words.
Reference: <author> I. H. Witten and T. C. Bell. </author> <year> 1991. </year> <title> The zero-frequency problem: estimating the probabilities of novel events in adaptive text compression. </title> <journal> IEEE Trans. on Inform. Theory, </journal> <volume> 37(4) </volume> <pages> 1085-1094. </pages>
References-found: 15

