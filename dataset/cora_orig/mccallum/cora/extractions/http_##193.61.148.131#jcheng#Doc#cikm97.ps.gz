URL: http://193.61.148.131/jcheng/Doc/cikm97.ps.gz
Refering-URL: http://193.61.148.131/jcheng/bninordabs.htm
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: -j.cheng, da.bell, w.liu-@ulst.ac.uk  
Phone: Tel: 44 1232 366500 Fax: 44 1232 366068  
Title: Learning Belief Networks from Data: An Information Theory Based Approach  
Author: Jie Cheng, David A. Bell, Weiru Liu 
Note: computational complexity are also presented.  
Address: United Kingdom, BT37 0QB  
Affiliation: School of Information and Software Engineering University of Ulster at Jordanstown  
Abstract: This paper presents an efficient algorithm for learning Bayesian belief networks from databases. The algorithm takes a database as input and constructs the belief network structure as output. The construction process is based on the computation of mutual information of attribute pairs. Given a data set that is large enough, this algorithm can generate a belief network very close to the underlying model, and at the same time, enjoys the time When the data set has a normal DAG-Faithful (see Section 3.2) probability distribution, the algorithm guarantees that the structure of a perfect map [Pearl, 1988] of the underlying dependency model is generated. To evaluate this algorithm, we present the experimental results on three versions of the well-known ALARM network database, which has 37 attributes and 10,000 records. The results show that this algorithm is accurate and efficient. The proof of correctness and the analysis of complexity of O N( ) 4 on conditional independence (CI) tests.
Abstract-found: 1
Intro-found: 1
Reference: [Beinlich et al ., 1989] <author> Beinlich, I.A., Suermondt, H.J., Chavez, R.M. and Cooper, </author> <title> G.F., The ALARM monitoring system: A case study with two probabilistic inference techniques for belief networks. </title> <booktitle> Proceedings of the Second European Conference on Artificial Intelligence in Medicine (pp.247-256), </booktitle> <address> London, England, </address> <year> 1989. </year>
Reference-contexts: Results on dataset1, dataset2 and dataset3. (M.O. and W.O. in the table stand for missing orientation and wrongly oriented.) 4 Results on ALARM Network ALARM network <ref> [Beinlich et al ., 1989] </ref> is a medical diagnostic alarm message system for patient monitoring, it contains 37 nodes and 46 arcs (see Figure 2). This belief network has become the de facto benchmark for evaluating algorithms on belief network construction.
Reference: [Buntine, 1996] <author> Buntine, W., </author> <title> A guide to the literature on learning probabilistic networks from data. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 8(2), </volume> <pages> 195-210, </pages> <year> 1996. </year>
Reference: [Cheng et al., 1997] <author> Cheng, J., Bell, D.A. and Liu, W., </author> <title> An algorithm for Bayesian belief network construction from data, </title> <booktitle> Proceedings of AI & STAT97 (pp.83-90), </booktitle> <address> Ft. Lauderdale, Florida, </address> <year> 1997. </year>
Reference-contexts: Other algorithms [Cooper and Herskovits, 1992; Herskovits, 1991; Suzuki, 1996; Wermuth and Lauritzen, 1983; Srinivas et al ., 1990] deal with a rather special case where node ordering is known. In this special case, our simplified algorithm <ref> [Cheng et al., 1997] </ref> require O N ( ) 2 times of CI tests and is correct when the underlying model is DAG-faithful.
Reference: [Chow and Liu, 1968] <author> Chow, C.K. and Liu, C.N., </author> <title> Approximating discrete probability distributions with dependence trees. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 14, </volume> <pages> 462-467, </pages> <year> 1968. </year>
Reference-contexts: Generally, this algorithm has two major advantages over other CI test based algorithms. First of all, it constructs Bayesian networks through three phases, drafting, thickening and thinning. This is a natural extension of the well-known Chow and Lius algorithm <ref> [Chow and Liu, 1968] </ref> to multi-connected network. In the case where the node ordering is known, this algorithm even preserves the merit of Chow and Lius algorithm that only O N ( ) 2 times of CI tests are needed. <p> Using mutual information in probabilistic model construction can be traced back to Chow and Lius tree construction algorithm <ref> [Chow and Liu, 1968] </ref>. In 1987, Rebane and Pearl extend Chow and Lius algorithm to causal polytree construction [Pearl, 1988]. Our algorithm extends those algorithms further to Bayesian belief network construction. <p> This is also the reason why Phase I can construct a graph close to the original graph to some extent. In fact, if the underlying graph is a singly connected graph, Phase I of this algorithm is essentially the algorithm of <ref> [Chow and Liu, 1968] </ref>, and it guarantees the constructed network structure is the same as the original one. In this example, (B,E) is wrongly added and (D,E) and (B,C) are missing because of the existing adjacency paths (D-B-E) and (B-E-C). <p> In our experiments, we found that most running time is consumed by database engine on query preparation and data retrieving. The actual computations consume only about 5% of runtime. 5 Discussion Mutual information has been used in probabilistic model construction since 1968 <ref> [Chow and Liu, 1968; Pearl, 1988] </ref>. However, their works are limited to tree and polytree construction. We apply the idea to Bayesian network construction. Like our Bayesian network construction algorithm, algorithms presented in [Spirtes et al. , 1991] and [Singh and Valtorta, 1995] do not require node ordering.
Reference: [Cooper and Herskovits, 1992] <author> Cooper, G.F., Herskovits, E., </author> <title> A Bayesian Method for the induction of probabilistic networks from data, </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <pages> 309-347, </pages> <year> 1992. </year>
Reference-contexts: This process continues until the score of the new model is not significantly better than the previous one. Different scoring criteria have been applied in these algorithms, such as, Bayesian scoring method <ref> [Cooper and Herskovits, 1992; Heckerman et al ., 1994] </ref>, entropy based method [Herskovits, 1991], and minimum description length method [Suzuki, 1996]. The other category of algorithms constructs Bayesian networks by analyzing dependency relationships among nodes. The dependency relationships are measured by using some kind of conditional independence (CI) test. <p> in the worst case (when the underlying DAG is densely connected), but it may not find the best solution due to its heuristic nature; The second category of algorithms is usually asymptotically correct when the probability distribution of data satisfies certain assumption, but as Cooper et al. pointed out in <ref> [Cooper and Herskovits, 1992] </ref>, CI tests with large conditionsets may be unreliable unless the volume of data is enormous. On developing this algorithm, we take the following two facts into consideration.
Reference: [Heckerman et al ., 1994] <author> Heckerman, D., Geiger, D. and Chickering, </author> <title> D.M., Learning Bayesian networks: the combination of knowledge and statistical data, </title> <type> Technical Report MSR-TR-94-09, </type> <institution> Microsoft Research, </institution> <year> 1994.[Pearl, </year> <note> 1988] Pearl, </note> <author> J. </author> , <title> Probabilistic reasoning in intelligent systems: networks of plausible inference, </title> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: This process continues until the score of the new model is not significantly better than the previous one. Different scoring criteria have been applied in these algorithms, such as, Bayesian scoring method <ref> [Cooper and Herskovits, 1992; Heckerman et al ., 1994] </ref>, entropy based method [Herskovits, 1991], and minimum description length method [Suzuki, 1996]. The other category of algorithms constructs Bayesian networks by analyzing dependency relationships among nodes. The dependency relationships are measured by using some kind of conditional independence (CI) test.
Reference: [Herskovits, 1991] <author> Herskovits, </author> <title> E.H., Computer-based probabilistic network construction, </title> <institution> Doctoral dissertation , Medical information sciences, Stanford University, Stanford, </institution> <address> CA, </address> <year> 1991. </year>
Reference-contexts: This process continues until the score of the new model is not significantly better than the previous one. Different scoring criteria have been applied in these algorithms, such as, Bayesian scoring method [Cooper and Herskovits, 1992; Heckerman et al ., 1994], entropy based method <ref> [Herskovits, 1991] </ref>, and minimum description length method [Suzuki, 1996]. The other category of algorithms constructs Bayesian networks by analyzing dependency relationships among nodes. The dependency relationships are measured by using some kind of conditional independence (CI) test.
Reference: [Madigan et al ., 1994] <author> Madigan, D., Mosurski, K. and Almond R.G., </author> <title> Explanation in belief networks. </title> <institution> Technical Report , Department of Statistics, University of Washington, </institution> <year> 1994. </year>
Reference: [Neapolitan, 1990] <author> Neapolitan, R.E. </author> , <title> Probabilistic reasoning in expert systems: </title> <publisher> theory and algorithms , John Wiley & Sons, </publisher> <year> 1990. </year>
Reference: [Singh and Valtorta, 1995] <author> Singh, M. and Valtorta, M. </author> <title> Construction of Bayesian network structures from data: a brief survey and an efficient algorithm, </title> <journal> International Journal of Approximate Reasoning, </journal> <volume> 12, </volume> <pages> 111-131, </pages> <year> 1995. </year>
Reference-contexts: However, their works are limited to tree and polytree construction. We apply the idea to Bayesian network construction. Like our Bayesian network construction algorithm, algorithms presented in [Spirtes et al. , 1991] and <ref> [Singh and Valtorta, 1995] </ref> do not require node ordering. Other algorithms [Cooper and Herskovits, 1992; Herskovits, 1991; Suzuki, 1996; Wermuth and Lauritzen, 1983; Srinivas et al ., 1990] deal with a rather special case where node ordering is known.
Reference: [Spirtes et al ., 1996] <author> Spirtes, P., Glymour, C. and Scheines, R., </author> <title> Causation, Prediction, and Search (Book), </title> <address> http://hss.cmu.edu/html/departments/philosophy/TETRAD.BOO K/book.html,1996. </address>
Reference-contexts: An adjacency path is a path between two nodes without considering the directionality of the arcs. A path between X and Y is active given Z if: (1) every collider <ref> [Spirtes et al ., 1996] </ref> in the path is in Z or has a descendant in Z; (2) every other node in the path is outside Z. A collider of a path is a node where two arcs in the path meet at their endpoints. <p> the next pair of nodes and go back to step 4 unless p is pointing to the end of L or G contains n-1 edges. (n is the number of nodes in G.) In order to illustrate this algorithms working mechanism, we use a simple multi-connected network example borrowed from <ref> [Spirtes et al ., 1996] </ref>. Suppose we have a database that has underlying Bayesian network as Figure 1.a, our task is to rediscover the underlying network structure from data. After step 2, we can get the mutual information of all 10 pair of nodes. <p> The correctness proof of this procedure is in section 3.2. The collider based edge orientation methods have also been studied in [Pearl, 1988; Spirtes et al., 1996]. 3.2 Correctness Before we present the correctness proof, we give the definition of normal DAG-faithful. Definition 1 In a DAG-faithful <ref> [Spirtes et al ., 1996] </ref> probability model, for any two nodes that are connected by at least two adjacency paths, under an arbitrary situation where some paths between the two nodes are closed by a conditionset, if we can only increase the mutual information by opening any previously closed paths between
Reference: [Spirtes et al. , 1991] <author> Spirtes, P., Glymour, C. and Scheines, R., </author> <title> An algorithm for fast recovery of sparse causal graphs, </title> <journal> Social Science Computer Review, </journal> <volume> 9, </volume> <pages> 62-72, </pages> <year> 1991. </year>
Reference-contexts: However, their works are limited to tree and polytree construction. We apply the idea to Bayesian network construction. Like our Bayesian network construction algorithm, algorithms presented in <ref> [Spirtes et al. , 1991] </ref> and [Singh and Valtorta, 1995] do not require node ordering. Other algorithms [Cooper and Herskovits, 1992; Herskovits, 1991; Suzuki, 1996; Wermuth and Lauritzen, 1983; Srinivas et al ., 1990] deal with a rather special case where node ordering is known.
Reference: [Srinivas et al ., 1990] <author> Srinivas, S. Russell, S. and Agogino, A., </author> <title> Automated construction of sparse Bayesian networks from unstructured probabilistic models and domain information, </title> <editor> In Henrion, M., Shachter, R.D., Kanal, L.N. and Lemmer, J.F. (Eds.), </editor> <booktitle> Uncertainty in artificial intelligence 5 , Amsterdam: </booktitle> <publisher> North-Holland, </publisher> <year> 1990. </year>
Reference: [Suzuki, 1996] <author> Suzuki, J., </author> <title> Learning Bayesian belief networks based on the MDL principle: An efficient algorithm using the branch and bound technique , Proceedings of the international conference on machine learning, </title> <address> Bally, Italy, </address> <year> 1996. </year>
Reference-contexts: Different scoring criteria have been applied in these algorithms, such as, Bayesian scoring method [Cooper and Herskovits, 1992; Heckerman et al ., 1994], entropy based method [Herskovits, 1991], and minimum description length method <ref> [Suzuki, 1996] </ref>. The other category of algorithms constructs Bayesian networks by analyzing dependency relationships among nodes. The dependency relationships are measured by using some kind of conditional independence (CI) test.
Reference: [Wermuth and Lauritzen, 1983] <author> Wermuth, N. and Lauritzen, S., </author> <title> Graphical and recursive models for contingency tables. </title> <journal> Biometrika, </journal> <volume> 72, </volume> <pages> 537-552, </pages> <year> 1983. </year> <title> i The CI tests are grouped by the cardinalities of conditionsets. ii Dataset1 has the underlying belief network described in the web page of Norsys Software Corp. Dataset2 has the underlying belief network used by David Heckerman. Dataset3 is generated by Gregory F. Cooper and Edward Herskovits. We generated dataset1 and dataset2 using a Monte Carlo technique. </title>
Reference-contexts: In the case where the node ordering is known, this algorithm even preserves the merit of Chow and Lius algorithm that only O N ( ) 2 times of CI tests are needed. Secondly, unlike other CI methods which use c 2 tests <ref> [Spirtes et al., 1991; Wermuth and Lauritzen, 1983] </ref> to check if two variables are dependent, we use mutual information as CI test which can tell us not only if two variables are dependent but also how close their relationship is.
References-found: 15

