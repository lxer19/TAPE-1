URL: http://www.cs.gatech.edu/computing/Database/faculty/edwardo/papers/VLDB95.ps.gz
Refering-URL: http://www.cs.gatech.edu/computing/Database/faculty/edwardo/edwardo.html
Root-URL: 
Email: e-mail: fashok,edwardo,shamg@cc.gatech.edu  
Title: An Efficient Algorithm for Mining Association Rules in Large Databases  
Author: Ashok Savasere Edward Omiecinski Shamkant Navathe 
Address: Atlanta, GA 30332  
Affiliation: College of Computing Georgia Institute of Technology  
Pubnum: Technical Report No. GIT-CC-95-04  
Abstract: Mining for association rules between items in a large database of sales transactions has been described as an important database mining problem. In this paper we present an efficient algorithm for mining association rules that is fundamentally different from known algorithms. Compared to the previous algorithms, our algorithm reduces both CPU and I/O overheads. In our experimental study it was found that for large databases, the CPU overhead was reduced by as much as a factor of seven and I/O was reduced by almost an order of magnitude. Hence this algorithm is especially suitable for very large size databases. The algorithm is also ideally suited for parallelization. We have performed extensive experiments and compared the performance of the algorithm with one of the best existing algorithms.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 207-216, </pages> <address> Washington, DC, </address> <month> May 26-28 </month> <year> 1993. </year>
Reference-contexts: Given the large database sizes, one of the main challenges in database mining is developing fast and efficient algorithms that can handle large volumes of data. Discovering association rules between items over basket data was introduced in <ref> [1] </ref>. Basket data typically consists of items bought by a customer along with the date of transaction, quantity, price, etc. Such data may be collected, for example, at supermarket checkout counters. Association rules identify the set of items that are most often purchased with another set of items. <p> Many algorithms have been discussed in the literature for discovering association rules <ref> [1, 8, 2] </ref>. One of the key features of all the previous algorithms is that they require multiple passes over the database. For disk resident databases, this requires reading the database completely for each pass resulting in a large number of disk reads. <p> In section 3, we describe our algorithm. Performance results are described in section 4. An approach to parallelizing our algorithm is described in Section 5. Section 6 contains conclusion and future work. 2 2 Problem Description This section is largely based on the description of the problem in <ref> [1] </ref> and [2]. Formally, the problem can be stated as follows: Let I = fi 1 ; i 2 ; : : : ; i m g be a set of m distinct literals called items 1 . D is a set of variable length transactions over I. <p> To reduce the combinatorial search space, all algorithms exploit the following property: any subset of a large itemset must also be large. For example, if a transaction contains itemset ABCD, then it also contains A, AB, BC, ABC, etc. 1 In this paper we use the terminology introduced by <ref> [1] </ref> 3 Conversely, all extensions of a small itemset are also small. Therefore, if at some stage it is found that itemset ADE is small, then none of the itemsets which are extensions of ADE, i.e., ADEF, ADEFG, etc., need be tested for minimum support. <p> This process is stopped when in some iteration n, no large itemsets are generated. The algorithm, in this case, makes n database scans. 2.1 Previous Work The problem of generating association rules was first introduced in <ref> [1] </ref> and an algorithm called AIS was proposed for mining all association rules. In [8], an algorithm called SETM was proposed to solve this problem using relational operations in a relational database environment. In [2], two new algorithms called Apriori and AprioriTid were proposed. <p> The algorithms vary mainly in (a) how the candidate itemsets are generated; and (b) how the supports for the candidate itemsets are counted. In <ref> [1] </ref>, the candidate itemsets are generated on the fly during the pass over the database. For every transaction, candidate itemsets are generated by extending the large itemsets from previous pass with the items in the transaction such that the new itemsets are contained in that transaction. <p> This technique produces a much smaller candidate set than the former technique. To count the supports for the candidate itemsets, for each transaction the set of all candidate itemsets that are contained in that transaction are identified. The counts for these itemsets are then incremented by one. In <ref> [1] </ref> the authors do not describe the data structures used for this subset operation. Apriori and AprioriTid differ based on the data structures used for generating the supports for candidate itemsets. 4 In Apriori, bitmaps are generated for transactions as well as the candidate itemsets. <p> When there is no ambiguity we omit the partition number when referring to a local itemset. We use the notation c <ref> [1] </ref>c [2] c [k] to represent a k-itemset c consisting of items c [1], c [2], : : :, c [k]. Algorithm The Partition algorithm is shown in Figure 1. Initially the database D is logically partitioned into n partitions. Phase I of the algorithm takes n iterations. During iteration i only partition p i is considered. <p> partition) 1) L 1 = flarge 1-itemsets along with their tidlistsg 2) for ( k = 2; L k 6= ;; k++) do begin 3) forall itemsets l 1 2 L p k1 do begin 4) forall itemsets l 2 2 L p k1 do begin 5) if l 1 <ref> [1] </ref> = l 2 [1] ^ l 1 [2] = l 2 [2] ^ : : : ^ l 1 [k 1] &lt; l 2 [k 1] then 6) c = l 1 [1] l 1 [2] l 1 [k 1] l 2 [k 1] 7) if c cannot be pruned <p> = flarge 1-itemsets along with their tidlistsg 2) for ( k = 2; L k 6= ;; k++) do begin 3) forall itemsets l 1 2 L p k1 do begin 4) forall itemsets l 2 2 L p k1 do begin 5) if l 1 <ref> [1] </ref> = l 2 [1] ^ l 1 [2] = l 2 [2] ^ : : : ^ l 1 [k 1] &lt; l 2 [k 1] then 6) c = l 1 [1] l 1 [2] l 1 [k 1] l 2 [k 1] 7) if c cannot be pruned then 8) c.tidlist = <p> do begin 4) forall itemsets l 2 2 L p k1 do begin 5) if l 1 <ref> [1] </ref> = l 2 [1] ^ l 1 [2] = l 2 [2] ^ : : : ^ l 1 [k 1] &lt; l 2 [k 1] then 6) c = l 1 [1] l 1 [2] l 1 [k 1] l 2 [k 1] 7) if c cannot be pruned then 8) c.tidlist = l 1 .tidlist"l 2 .tidlist 9) if jc.tidlist j / jpj minSup then 10) L k = L k [ fcg 12) end 14) return [ k L p
Reference: [2] <author> R. Agrawal and R. Srikant. </author> <title> Fast algorithms for mining association rules in large databases. </title> <booktitle> In Proceedings of the 20th International Conference on Very Large Data Bases, </booktitle> <address> Santiago, Chile, </address> <month> August 29-September 1 </month> <year> 1994. </year>
Reference-contexts: Many algorithms have been discussed in the literature for discovering association rules <ref> [1, 8, 2] </ref>. One of the key features of all the previous algorithms is that they require multiple passes over the database. For disk resident databases, this requires reading the database completely for each pass resulting in a large number of disk reads. <p> Performance results are described in section 4. An approach to parallelizing our algorithm is described in Section 5. Section 6 contains conclusion and future work. 2 2 Problem Description This section is largely based on the description of the problem in [1] and <ref> [2] </ref>. Formally, the problem can be stated as follows: Let I = fi 1 ; i 2 ; : : : ; i m g be a set of m distinct literals called items 1 . D is a set of variable length transactions over I. <p> In [8], an algorithm called SETM was proposed to solve this problem using relational operations in a relational database environment. In <ref> [2] </ref>, two new algorithms called Apriori and AprioriTid were proposed. These algorithms achieved significant improvements over the previous algorithms and were specifically applicable to large databases. In [2], the rule generation process was extended to include multiple items in the consequent and an efficient algorithm for generating the rules was also <p> In [8], an algorithm called SETM was proposed to solve this problem using relational operations in a relational database environment. In <ref> [2] </ref>, two new algorithms called Apriori and AprioriTid were proposed. These algorithms achieved significant improvements over the previous algorithms and were specifically applicable to large databases. In [2], the rule generation process was extended to include multiple items in the consequent and an efficient algorithm for generating the rules was also presented. The algorithms vary mainly in (a) how the candidate itemsets are generated; and (b) how the supports for the candidate itemsets are counted. <p> In [1], the candidate itemsets are generated on the fly during the pass over the database. For every transaction, candidate itemsets are generated by extending the large itemsets from previous pass with the items in the transaction such that the new itemsets are contained in that transaction. In <ref> [2] </ref> candidate itemsets are generated using only the large itemsets from the previous pass. It is performed by joining the large itemset with itself. The resulting set is further pruned to exclude any itemset whose subset is not contained in the previous large itemsets. <p> We assume the transactions are in the form hT ID; i j ; i k ; : : :; i n i. The items in a transaction are assumed to be kept sorted in the lexicographic order. Similar assumption is also made in <ref> [2] </ref>. It is straight-forward to adapt the algorithm to the case where the transactions are kept normalized in hT ID; itemi form. We also assume that the T IDs are monotonically increasing. This is justified considering the nature of the application. <p> We use the notation shown in Table 1 in this paper. Individual itemsets are represented by small letters and sets of itemsets are represented by capital letters. When there is no ambiguity we omit the partition number when referring to a local itemset. We use the notation c [1]c <ref> [2] </ref> c [k] to represent a k-itemset c consisting of items c [1], c [2], : : :, c [k]. Algorithm The Partition algorithm is shown in Figure 1. Initially the database D is logically partitioned into n partitions. Phase I of the algorithm takes n iterations. <p> When there is no ambiguity we omit the partition number when referring to a local itemset. We use the notation c [1]c <ref> [2] </ref> c [k] to represent a k-itemset c consisting of items c [1], c [2], : : :, c [k]. Algorithm The Partition algorithm is shown in Figure 1. Initially the database D is logically partitioned into n partitions. Phase I of the algorithm takes n iterations. During iteration i only partition p i is considered. <p> with their tidlistsg 2) for ( k = 2; L k 6= ;; k++) do begin 3) forall itemsets l 1 2 L p k1 do begin 4) forall itemsets l 2 2 L p k1 do begin 5) if l 1 [1] = l 2 [1] ^ l 1 <ref> [2] </ref> = l 2 [2] ^ : : : ^ l 1 [k 1] &lt; l 2 [k 1] then 6) c = l 1 [1] l 1 [2] l 1 [k 1] l 2 [k 1] 7) if c cannot be pruned then 8) c.tidlist = l 1 .tidlist"l 2 <p> for ( k = 2; L k 6= ;; k++) do begin 3) forall itemsets l 1 2 L p k1 do begin 4) forall itemsets l 2 2 L p k1 do begin 5) if l 1 [1] = l 2 [1] ^ l 1 <ref> [2] </ref> = l 2 [2] ^ : : : ^ l 1 [k 1] &lt; l 2 [k 1] then 6) c = l 1 [1] l 1 [2] l 1 [k 1] l 2 [k 1] 7) if c cannot be pruned then 8) c.tidlist = l 1 .tidlist"l 2 .tidlist 9) if jc.tidlist <p> forall itemsets l 2 2 L p k1 do begin 5) if l 1 [1] = l 2 [1] ^ l 1 <ref> [2] </ref> = l 2 [2] ^ : : : ^ l 1 [k 1] &lt; l 2 [k 1] then 6) c = l 1 [1] l 1 [2] l 1 [k 1] l 2 [k 1] 7) if c cannot be pruned then 8) c.tidlist = l 1 .tidlist"l 2 .tidlist 9) if jc.tidlist j / jpj minSup then 10) L k = L k [ fcg 12) end 14) return [ k L p 3.1 Generation of <p> However, itemset f1 3 4 5g is pruned since f1 4 5g is not in L p 3 . This technique is same as the one described in <ref> [2] </ref> except in our case, as each candidate itemset is generated, its count is determined immediately. The counts for the candidate itemsets are generated as follows. Associated with every itemset, we define a structure called as tidlist. <p> = 2; C G k 6= ;; k++) do begin 4) forall k-itemset c 2 C G k do begin 5) templist = c [1].tidlist "c <ref> [2] </ref>.tidlist " : : : " c [k].tidlist 6) c.count = c.count + j templist j 7) end Correctness It has been shown in [2] that the candidate generation process correctly produces all potential large candidate itemsets. Therefore, it is sufficient to show that steps 8-9 correctly generate the support for an itemset in a partition. This can be shown by induction. <p> If the ratio is at least equal to the user specified minimum confidence, them the rule a =) (l a) is output. A more efficient algorithm is described in <ref> [2] </ref>. As mentioned earlier, generating rules given the large itemsets and their supports is much simpler compared to generating the large itemsets. <p> However, the number of large itemsets that will be generated cannot be estimated accurately. In some situations it may be necessary to write the temporary data to disk. The buffer management technique in phase I is similar to the one described in <ref> [2] </ref>. However, in Partition algorithm there is no separate step for counting the supports. As each local candidate k-itemset is generated, its count is also immediately generated. <p> The data resided on a 1 GB SCSI disk. All the experiments were run on synthetic data. For the performance comparison experiments, we used the same synthetic data sets as in <ref> [2] </ref>. Both Apriori and AprioriTid algorithms were implemented as described in [2]. Our initial experiments showed that the performance of Apriori is superior to that of AprioriTid confirming the results reported in [2]. Hence, in the following experiments we have limited the comparison to the Apriori algorithm. <p> The data resided on a 1 GB SCSI disk. All the experiments were run on synthetic data. For the performance comparison experiments, we used the same synthetic data sets as in <ref> [2] </ref>. Both Apriori and AprioriTid algorithms were implemented as described in [2]. Our initial experiments showed that the performance of Apriori is superior to that of AprioriTid confirming the results reported in [2]. Hence, in the following experiments we have limited the comparison to the Apriori algorithm. The synthetic data generation procedure is described in detail in [2]. <p> For the performance comparison experiments, we used the same synthetic data sets as in <ref> [2] </ref>. Both Apriori and AprioriTid algorithms were implemented as described in [2]. Our initial experiments showed that the performance of Apriori is superior to that of AprioriTid confirming the results reported in [2]. Hence, in the following experiments we have limited the comparison to the Apriori algorithm. The synthetic data generation procedure is described in detail in [2]. In the following section, we describe the Apriori algorithm and the synthetic data generation procedure for the sake of completeness. <p> implemented as described in <ref> [2] </ref>. Our initial experiments showed that the performance of Apriori is superior to that of AprioriTid confirming the results reported in [2]. Hence, in the following experiments we have limited the comparison to the Apriori algorithm. The synthetic data generation procedure is described in detail in [2]. In the following section, we describe the Apriori algorithm and the synthetic data generation procedure for the sake of completeness. The Apriori algorithm is shown in Figure 4. The procedure apriori-gen is similar to the candidate generation step described earlier. <p> The Apriori algorithm is shown in Figure 4. The procedure apriori-gen is similar to the candidate generation step described earlier. The subset operation is performed using bit fields and hashtree structure as described in <ref> [2] </ref>. 4.1 Synthetic Data The synthetic data is said to simulate a customer buying pattern in a retail environment. The length of a transaction is determined by poisson distribution with mean equal to jT j. <p> Table 5 shows the names and parameter settings for each data set. For all data sets N was set to 1,000 and jLj was set to 2,000. These datasets are same as those used in <ref> [2] </ref> for the experiments. only about 8.4 MB, we could run the Partition algorithm setting the number of partitions to 1. However, for comparison, we also ran the experiments setting the number of partitions to 10. These results are indicated as Partition-1 and Partition-10 in the figure. <p> An important (and surprising) contribution of our approach is that we achieve both CPU and I/O improvements over Apriori, one of the best previous algorithms. We have also presented experimental results for the same synthetic data as used in <ref> [2] </ref>. The Partition algorithm outperformed Apriori for large problem sizes by upto a factor of seven at the same time reduced the number of disk reads by a factor of eight.
Reference: [3] <author> T. M. Anwar, S. B. Navathe, and H. W. Beck. </author> <title> Knowledge mining in databases: A unified approach through conceptual clustering. </title> <type> Technical report, </type> <institution> Georgia Institute of Technology, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: This may be useful when such results are sufficient. 2. The algorithm is inherently parallel in nature and can be parallelized with minimal commu nication and synchronization between the processing nodes. Other related, but not directly applicable work in database mining are reported in <ref> [7, 10, 6, 9, 3, 13, 14] </ref>. The paper is organized as follows: in the next section, we give a formal description of the problem. In Section 2, we describe the problem and give an overview of the previous algorithms. In section 3, we describe our algorithm.
Reference: [4] <author> A. Bhide, F. Bancilhon, and D. J. Dewitt. </author> <title> An analysis of three transaction processing architectures. </title> <booktitle> In Proceedings of the 14th International Conference on Very Large Data Bases, </booktitle> <pages> pages 339-350, </pages> <address> Los Angeles, CA, </address> <month> August 29 - September 1 </month> <year> 1988. </year>
Reference-contexts: Eventually such databases may need to augment their functionality with database mining capabilities. The algorithm we have proposed in this paper is ideally suited for paralleliza-tion. Shared nothing architectures have been shown to be highly suitable for parallel database systems <ref> [4] </ref>. It contains a number of processing nodes each with its own primary memory and a set of local disks. As a realistic implementation of a OLTP system for collecting data, we assume that the transactions are directed to the node with the least load.
Reference: [5] <author> D. DeWitt and J. Gray. </author> <title> Parallel database systems: the future of high performance database systems. </title> <journal> Communications of the ACM, </journal> <volume> 35(6) </volume> <pages> 85-98, </pages> <month> June </month> <year> 1992. </year> <month> 20 </month>
Reference-contexts: This also confirms the trend exhibited in earlier performance studies. As the problem difficulty increases, the Partition algorithm performs much better than Apriori. 5 Parallelization Parallel database systems have been shown to be viable means of delivering the performance required in supporting very large databases <ref> [5] </ref>. Many commercial parallel databases are available to the users today. These are beginning to replace mainframe computers for very large database and OLTP tasks. Eventually such databases may need to augment their functionality with database mining capabilities.
Reference: [6] <author> J. Han, Y. Cai, and N. Cercone. </author> <title> Knowledge discovery in databases: an attribute-oriented approach. </title> <booktitle> In Proceedings of the 18th International Conference on Very Large Data Bases, </booktitle> <pages> pages 547-559, </pages> <address> Vancouver, Canada, 23-27, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: This may be useful when such results are sufficient. 2. The algorithm is inherently parallel in nature and can be parallelized with minimal commu nication and synchronization between the processing nodes. Other related, but not directly applicable work in database mining are reported in <ref> [7, 10, 6, 9, 3, 13, 14] </ref>. The paper is organized as follows: in the next section, we give a formal description of the problem. In Section 2, we describe the problem and give an overview of the previous algorithms. In section 3, we describe our algorithm.
Reference: [7] <author> M. Holsheimer and A. Siebes. </author> <title> Data mining: The search for knowledge in databases. </title> <type> Technical Report CS-R9406, </type> <institution> CWI, </institution> <address> Amsterdam, The Netherlands, </address> <year> 1993. </year>
Reference-contexts: This may be useful when such results are sufficient. 2. The algorithm is inherently parallel in nature and can be parallelized with minimal commu nication and synchronization between the processing nodes. Other related, but not directly applicable work in database mining are reported in <ref> [7, 10, 6, 9, 3, 13, 14] </ref>. The paper is organized as follows: in the next section, we give a formal description of the problem. In Section 2, we describe the problem and give an overview of the previous algorithms. In section 3, we describe our algorithm.
Reference: [8] <author> M. Houtsma and A. Swami. </author> <title> Set-oriented mining of association rules. </title> <type> Technical Report RJ 9567, </type> <institution> IBM, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: Many algorithms have been discussed in the literature for discovering association rules <ref> [1, 8, 2] </ref>. One of the key features of all the previous algorithms is that they require multiple passes over the database. For disk resident databases, this requires reading the database completely for each pass resulting in a large number of disk reads. <p> The algorithm, in this case, makes n database scans. 2.1 Previous Work The problem of generating association rules was first introduced in [1] and an algorithm called AIS was proposed for mining all association rules. In <ref> [8] </ref>, an algorithm called SETM was proposed to solve this problem using relational operations in a relational database environment. In [2], two new algorithms called Apriori and AprioriTid were proposed. These algorithms achieved significant improvements over the previous algorithms and were specifically applicable to large databases.
Reference: [9] <author> R. Krishnamurthy and T. Imielinski. </author> <title> Practitioner problems in need of database research. </title> <journal> ACM SIGMOD Record, </journal> <volume> 20(3) </volume> <pages> 76-78, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: This may be useful when such results are sufficient. 2. The algorithm is inherently parallel in nature and can be parallelized with minimal commu nication and synchronization between the processing nodes. Other related, but not directly applicable work in database mining are reported in <ref> [7, 10, 6, 9, 3, 13, 14] </ref>. The paper is organized as follows: in the next section, we give a formal description of the problem. In Section 2, we describe the problem and give an overview of the previous algorithms. In section 3, we describe our algorithm.
Reference: [10] <editor> G. Piatetsky-Shapiro and W. J. Frawley, editors. </editor> <title> Knowledge Discovery in Databases. </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: This may be useful when such results are sufficient. 2. The algorithm is inherently parallel in nature and can be parallelized with minimal commu nication and synchronization between the processing nodes. Other related, but not directly applicable work in database mining are reported in <ref> [7, 10, 6, 9, 3, 13, 14] </ref>. The paper is organized as follows: in the next section, we give a formal description of the problem. In Section 2, we describe the problem and give an overview of the previous algorithms. In section 3, we describe our algorithm.
Reference: [11] <author> A. Silberschatz, M. Stonebraker, and J. Ullman. </author> <title> Database systems: achievements and opportunities. </title> <journal> Communications of the ACM, </journal> <volume> 34(10) </volume> <pages> 110-120, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: This information may be used for retaining market leadership by tuning to the needs of customers. Database mining is motivated by such decision support problems and is described as an important area of research <ref> [11, 12] </ref>. One of the most difficult problems in database mining is the large volume of data that needs to be handled. In a medium sized business, it is not uncommon to collect hundreds of megabytes to a few gigabytes of data.
Reference: [12] <author> M. Stonebraker, R. Agrawal, U. Dayal, E. Nuehold, and A. Reuter. </author> <title> Database research at a crossroads: The vienna update. </title> <booktitle> In Proceedings of the 19th International Conference on Very Large Data Bases, </booktitle> <pages> pages 688-192, </pages> <address> Dublin, Ireland, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: This information may be used for retaining market leadership by tuning to the needs of customers. Database mining is motivated by such decision support problems and is described as an important area of research <ref> [11, 12] </ref>. One of the most difficult problems in database mining is the large volume of data that needs to be handled. In a medium sized business, it is not uncommon to collect hundreds of megabytes to a few gigabytes of data.
Reference: [13] <author> S. Tsur. </author> <title> Data dedging. </title> <journal> IEEE Data Engineering Bulletin, </journal> <volume> 13(4) </volume> <pages> 58-63, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: This may be useful when such results are sufficient. 2. The algorithm is inherently parallel in nature and can be parallelized with minimal commu nication and synchronization between the processing nodes. Other related, but not directly applicable work in database mining are reported in <ref> [7, 10, 6, 9, 3, 13, 14] </ref>. The paper is organized as follows: in the next section, we give a formal description of the problem. In Section 2, we describe the problem and give an overview of the previous algorithms. In section 3, we describe our algorithm.
Reference: [14] <author> J. T-L. Wang, G-W. Chirn, T. G. Marr, B. Shapiro, D. Shasha, and K. Zhang. </author> <title> Cobinatorial pattern discovery for scientific data: some preliminary results. </title> <booktitle> In Proceedings of the 1994 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 115-125, </pages> <address> Minneapolis, MN, </address> <month> May 24-27 </month> <year> 1994. </year> <pages> 21 22 23 24 </pages>
Reference-contexts: This may be useful when such results are sufficient. 2. The algorithm is inherently parallel in nature and can be parallelized with minimal commu nication and synchronization between the processing nodes. Other related, but not directly applicable work in database mining are reported in <ref> [7, 10, 6, 9, 3, 13, 14] </ref>. The paper is organized as follows: in the next section, we give a formal description of the problem. In Section 2, we describe the problem and give an overview of the previous algorithms. In section 3, we describe our algorithm.
References-found: 14

