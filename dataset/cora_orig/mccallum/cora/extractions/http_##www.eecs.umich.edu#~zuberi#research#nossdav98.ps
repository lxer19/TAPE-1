URL: http://www.eecs.umich.edu/~zuberi/research/nossdav98.ps
Refering-URL: http://www.eecs.umich.edu/~zuberi/research/
Root-URL: http://www.cs.umich.edu
Email: fzuberi,kgshing@eecs.umich.edu  
Title: An Efficient End-Host Protocol Processing Architecture for Real-Time Audio and Video Traffic  
Author: Khawar M. Zuberi and Kang G. Shin 
Address: Michigan  
Affiliation: Real-Time Computing Laboratory, The University of  
Abstract: Information appliances (IAs) are mass-produced, low-cost devices for specialized computation/communication tasks. Examples include webTV, smart cellular phones, and web phones. Since low cost and low power-consumption are important requirements in the design of these devices, IAs use relatively slow/cheap CPUs (usually 20-40 MHz). Real-time audio and video communication over the Internet is an integral part of many IAs which means that despite slow hardware, the communication subsystem within the OS must be able to efficiently handle heavy network traffic. This paper presents a protocol architecture which reduces I-cache miss overheads (benefiting short audio messages) and also enables use of single-copy without any hardware support or restrictions on network APIs (benefiting long video messages). We implemented UDP/IP to evaluate our architecture and measurements show that overheads for short messages are reduced about 20% while overheads for long messages are reduced 15-22%. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Lewis, </author> <title> "Information appliances: </title> <journal> Gadget netopia," IEEE Computer, </journal> <volume> vol. 31, no. 1, </volume> <pages> pp. 59-70, </pages> <month> January </month> <year> 1998. </year>
Reference-contexts: 1 Introduction Information appliances (IAs) <ref> [1] </ref> are single-user devices with Internet connectivity, used for specialized communication and information retrieval purposes. IAs include devices such as webTVs, smart cellular phones with e-mail, PDAs, and web video phones. <p> Since IAs are mass produced, keeping per-unit costs low is a primary design objective. As a result, IAs use simple, low-cost hardware. For example, the current generation of personal information managers (PIMs) use processors running at 16-44 MHz <ref> [1] </ref>. Since audio/video communication is a primary function performed by IAs, the communication subsystem within the OS must be highly efficient to work well with the low-cost, slow hardware of IAs for both short audio and long video messages.
Reference: [2] <editor> Appliance war could make web less open. </editor> <title> News Briefs, </title> <journal> IEEE Computer, </journal> <volume> vol. 30, no. 10, </volume> <pages> pp. 20-25, </pages> <month> October </month> <year> 1997. </year>
Reference-contexts: IAs include devices such as webTVs, smart cellular phones with e-mail, PDAs, and web video phones. With annual production volume of IAs expected to reach 48 million units by year 2001 <ref> [2] </ref>, IAs are becoming an important class of computation devices. Since IAs are mass produced, keeping per-unit costs low is a primary design objective. As a result, IAs use simple, low-cost hardware. For example, the current generation of personal information managers (PIMs) use processors running at 16-44 MHz [1].
Reference: [3] <author> H. Schulzrinne, </author> <title> "RTP profile for audio and video conferences with minimal control," </title> <type> RFC 1890, </type> <month> January </month> <year> 1996. </year>
Reference-contexts: Different overheads come into play depending on whether short or long messages are being processed. Data-touching overheads (which include data copying and checksum overheads) tend to dominate when dealing with long messages. For short (audio) messages, copying overheads are not important, but messages are sent once every 10-30ms <ref> [3] </ref>. With messages arriving with such high frequency, non-data-touching overheads (context switching, interrupt handling, I-cache miss overheads, etc.) become an important part of protocol processing. <p> This is in contrast to non-real-time applications where no such bound exists on how long an application may take to retrieve its packets from the NA. Real-time audio and video applications run with some period T . T for audio is quite short, usually 10-30ms <ref> [3] </ref>, but video applications can run as slow as 30 or even 10 frames/s, giving a T as large as 0.1s. This is the maximum time messages for a video application have to stay in NA buffers.
Reference: [4] <author> J. Kay and J. Pasquale, </author> <title> "Measurement, analysis, and improvement of UDP/IP throughput for the DECsta-tion 5000," </title> <booktitle> in Proc. Winter USENIX, </booktitle> <pages> pp. 249-258, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: With messages arriving with such high frequency, non-data-touching overheads (context switching, interrupt handling, I-cache miss overheads, etc.) become an important part of protocol processing. Studies have shown that receive-side protocol processing is more complicated and has higher overhead than the send-side <ref> [4, 5] </ref> and this is what limits throughput; so, here we focus on improving receive-side overhead. For reducing non-data-touching overheads, we present layer bypass which uses application-specific knowledge to safely bypass select layers within the protocol stack, completely avoiding all I-cache misses associated with those layers. <p> In all cases, the UDP checksum is turned off. From the figure, we see the benefit of bypassing IP and UDP layers. Performance is improved 20% (beyond that of LRP). Note that the sharper variations in the plots are a result of BSD's mbuf allocation scheme <ref> [4] </ref> and are not related to the protocol architecture. Long Messages: Figure 4 plots the receive overhead for messages ranging from 20 to 6000 bytes. It shows that single-copy incurs 15-22% less overhead than the two-copy scheme.
Reference: [5] <author> D. Kandlur, D. Saha, and M. Willebeek-LeMair, </author> <title> "Protocol architecture for multimedia applications over ATM networks," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 14, no. 7, </volume> <pages> pp. 1349-1359, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: With messages arriving with such high frequency, non-data-touching overheads (context switching, interrupt handling, I-cache miss overheads, etc.) become an important part of protocol processing. Studies have shown that receive-side protocol processing is more complicated and has higher overhead than the send-side <ref> [4, 5] </ref> and this is what limits throughput; so, here we focus on improving receive-side overhead. For reducing non-data-touching overheads, we present layer bypass which uses application-specific knowledge to safely bypass select layers within the protocol stack, completely avoiding all I-cache misses associated with those layers.
Reference: [6] <author> C. Dalton, G. Watson, D. Banks, C. Calamvokis, A. Edwards, and J. Lumley, </author> <title> "Afterburner," </title> <journal> IEEE Network, </journal> <volume> vol. 7, no. 4, </volume> <pages> pp. 36-43, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Regarding data-touching overheads, we exploit the periodic nature of video applications. We show that the single-copy scheme presented for non-real-time systems in <ref> [6] </ref> | which requires specialized network adapter hardware to be feasible for non-real-time systems | works well without any hardware support for multimedia applications because of their periodic nature. For evaluation, we implemented UDP/IP using our protocol architecture within the EMERALDS real-time operating system [7]. <p> This indicates a need to develop an easier-to-use scheme to reduce I-cache misses. 2.2 Single-Copy Architectures The single-copy network architecture was proposed by network adapter (NA) designers <ref> [6] </ref> to reduce data-touching overheads. <p> To work well for general-purpose systems, single-copy schemes require the NA to have "flexible" buffers. The Afterburner NA <ref> [6] </ref> uses linked lists to manage NA buffers, so it can continue to receive packets as long as some free buffers are available. This is a more complicated and expensive NA design than common NAs such as LANCE which uses circular queues of buffers.
Reference: [7] <author> K. M. Zuberi and K. G. Shin, "EMERALDS: </author> <title> A mi-crokernel for embedded real-time systems," </title> <booktitle> in Proc. Real-Time Technology and Applications Symposium, </booktitle> <pages> pp. 241-249, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: For evaluation, we implemented UDP/IP using our protocol architecture within the EMERALDS real-time operating system <ref> [7] </ref>. EMERALDS is designed for use in small embedded systems such as digital cellular phones. We chose UDP as the protocol to implement since it is commonly used for audio and video applications. The next section gives an overview of protocol processing overheads.
Reference: [8] <author> D. Mosberger, L. L. Peterson, P. G. Bridges, and S. O'Malley, </author> <title> "Analysis of techniques to improve protocol processing latency," </title> <booktitle> in Proc. SIGCOMM, </booktitle> <pages> pp. 73-84, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: However, code is still fetched into the I-cache, causing replacement misses. Moreover, repeated branches can cause CPU pipeline stalls. For relatively slow CPUs such as those used in IAs, this results in significant non-data-touching overhead. Researchers have proposed techniques such as outlining/cloning <ref> [8] </ref> and incremental specialization [9] to reduce I-cache misses, but these schemes involve low-level optimization of frequently-executed code in the if (check1) - ... - if (check2) - ... - if (check3) - ... - protocol stack, and this entails considerable effort on part of the programmer.
Reference: [9] <author> C. Pu, T. Autrey, A. Black, C. Consel, C. Cowan, J. Inouye, L. Kethana, J. Walpole, and K. Zhang, </author> <title> "Optimistic incremental specialization: Streamlining a commercial operating system," </title> <booktitle> in Proc. Symposium on Operating Systems Principles, </booktitle> <pages> pp. 314-324, </pages> <month> De-cember </month> <year> 1995. </year>
Reference-contexts: However, code is still fetched into the I-cache, causing replacement misses. Moreover, repeated branches can cause CPU pipeline stalls. For relatively slow CPUs such as those used in IAs, this results in significant non-data-touching overhead. Researchers have proposed techniques such as outlining/cloning [8] and incremental specialization <ref> [9] </ref> to reduce I-cache misses, but these schemes involve low-level optimization of frequently-executed code in the if (check1) - ... - if (check2) - ... - if (check3) - ... - protocol stack, and this entails considerable effort on part of the programmer.
Reference: [10] <author> P. Druschel and G. Banga, </author> <title> "Lazy receiver processing (LRP): A network subsystem architecture for server systems," </title> <booktitle> in Proc. Operating Systems Design and Implementation, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: even with cheap NAs such as LANCE (which is essential to keep costs low in IAs). 3 Protocol Architecture The next subsection describes the basic structure we chose for our protocol architecture, followed by a description of our protocol processing optimizations. 3.1 Basic Structure We chose lazy receiver processing (LRP) <ref> [10] </ref> (Figure 2) as our basic architecture. Under LRP, the packet filter [11] tries to forward real-time packets directly to queues associated with the destination thread where packets stay unprocessed until the application makes a receive system call.
Reference: [11] <author> D. Engler and M. F. Kaashoek, "DPF: </author> <title> Fast, flexible message demultiplexing using dynamic code generation," </title> <booktitle> in Proc. SIGCOMM, </booktitle> <pages> pp. 53-59, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: Under LRP, the packet filter <ref> [11] </ref> tries to forward real-time packets directly to queues associated with the destination thread where packets stay unprocessed until the application makes a receive system call. This is possible for real-time messages since the application threads are usually periodic so that packets are always processed within a known time interval. <p> For evaluation, we implemented UDP/IP using our architecture. The protocol and LANCE device driver code was taken from FreeBSD 4.4 and minor modifications were made to make it work with EMERALDS. For simplicity, we used a UDP/IP-specific packet filter. Interested readers are referred to <ref> [11] </ref> for more generalized high-performance packet filters. 4.1 Performance Improvements We sent datagram messages from one processor to another and measured the total overhead of receive-side protocol processing including interrupt handling and all relevant context switches using a 5MHz on-chip timer.
Reference: [12] <author> C. Mercer and H. Tokuda, </author> <title> "An evaluation of priority consistency in protocol architectures," </title> <booktitle> in Proc. IEEE Conf. Local Computer Networks, </booktitle> <pages> pp. 386-398, </pages> <month> Octo-ber </month> <year> 1991. </year>
Reference-contexts: Non-real-time packets are forwarded to a special network thread which performs protocol processing and keeps the message until the final destination thread makes a receive call. LRP minimizes priority inversion <ref> [12] </ref> and also saves one context switch for real-time messages compared to architectures which always use intermediate network threads for protocol processing, which is why we use LRP. Next, we describe our optimizations for protocol processing.
Reference: [13] <author> K. M. Zuberi, </author> <title> Real-Time Operating System Services for Networked Embedded Systems, </title> <type> PhD thesis, </type> <institution> University of Michigan, EECS Dept., </institution> <year> 1998. </year>
Reference-contexts: So, for live audio messages, the packet filter bypasses both IP and UDP layers and forwards packets straight to the socket layer. Layer bypass can also be used for handling other types of short messages such as web server requests as discussed in <ref> [13] </ref>. 3.3 Improving Data-Touching Overheads We now show that the single-copy scheme | which, without hardware support, has limited value for non-real-time systems | can be used effectively for video communication with no special hardware support. <p> We implemented our protocol architecture within EMERALDS on a 25MHz Motorola 68040 processor. The 68040 is typical of CPUs used in many IAs today. (Refer to <ref> [13] </ref> for evaluation results on a faster processor.) We use two 68040s in our experiments, connected by a 10Mb/s private Ethernet using the LANCE network adapter. For evaluation, we implemented UDP/IP using our architecture.
Reference: [14] <author> V. Paxson and S. Floyd, </author> <title> "Wide area traffic: The failure of poisson modeling," </title> <journal> IEEE/ACM Trans. Networking, </journal> <volume> vol. 3, no. 3, </volume> <pages> pp. 226-244, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Estimating Non-Real-Time Packet Arrivals: Poisson processes have previously been used to model packet arrivals. However, studies have shown that wide-area network traffic is too bursty to be correctly modeled by a Poisson process <ref> [14] </ref>. In fact, only empirical models exist for web browsing [15] and other mass data transfer applications. This precludes any closed-form derivation of non-real-time packet arrival distributions. Instead, we present an engineering approximation of packet arrival rates to see if the single-copy scheme can be used successfully in IAs.
Reference: [15] <author> C. Cunha, A. Bestavros, and M. Crovella, </author> <title> "Characteristics of WWW client-based traces," </title> <type> Technical Report BU-CS-95-010, </type> <institution> Boston University, Computer Science Department, </institution> <year> 1995. </year>
Reference-contexts: Estimating Non-Real-Time Packet Arrivals: Poisson processes have previously been used to model packet arrivals. However, studies have shown that wide-area network traffic is too bursty to be correctly modeled by a Poisson process [14]. In fact, only empirical models exist for web browsing <ref> [15] </ref> and other mass data transfer applications. This precludes any closed-form derivation of non-real-time packet arrival distributions. Instead, we present an engineering approximation of packet arrival rates to see if the single-copy scheme can be used successfully in IAs. We use web browsing as a representative non-real-time networking application. <p> We use web browsing as a representative non-real-time networking application. Measurements of web traffic have shown that retrieval of even small web pages take more than 2 seconds <ref> [15] </ref>. This is the time needed to look up the remote host's DNS entry and establish TCP connections. After this initial phase data transfer begins at the rate of 1 byte per 90-100s [15]. Most web pages are relatively small-sized. <p> of web traffic have shown that retrieval of even small web pages take more than 2 seconds <ref> [15] </ref>. This is the time needed to look up the remote host's DNS entry and establish TCP connections. After this initial phase data transfer begins at the rate of 1 byte per 90-100s [15]. Most web pages are relatively small-sized. Considering the small display screens that IAs have, we assume a 10kB page size and 20 packets to carry 10kB.
References-found: 15

