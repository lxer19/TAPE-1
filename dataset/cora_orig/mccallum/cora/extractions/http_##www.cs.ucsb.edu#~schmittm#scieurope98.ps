URL: http://www.cs.ucsb.edu/~schmittm/scieurope98.ps
Refering-URL: http://www.cs.ucsb.edu/~schmittm/res.html
Root-URL: http://www.cs.ucsb.edu
Email: -ibel,schmittm,schauser,acha-@cs.ucsb.edu  
Title: An Efficient Global Address Space Model with SCI  
Author: Maximilian Ibel, Michael Schmitt, Klaus Schauser, Anurag Acharya 
Keyword: Cluster Computing, Global Address Space, Message Passing, SCI, Shared Memory, Split-C.  
Address: Santa Barbara  
Affiliation: Department of Computer Science University of California,  
Abstract: One of the most powerful features of I/O-bus-based SCI is its ability for non-coherent remote memory access with a simple load/store interface. SCI bridges the gap between clusters with expensive, proprietary solutions implementing true coherent shared memory and low-performance software implementations of distributed shared memory (DSM). This allows for a true global address space scheme where shared data of parallel applications maps directly to shared memory segments (Direct Remote Memory Access - DRMA). Additionally, SCI can be used for efficient user-level message passing (e.g., Active Messages). As part of the SCINTILLA cluster computing project, we evaluate the trade-offs between these two paradigms using the explicitely parallel language Split-C which is based on a global address space abstraction. A direct implementation of Split-C on SCI can significantly outperform an Active Message-based implementation due to reduced copying and less synchronization. However, SCI poses a few severe restrictions that make an efficient DRMA implementation very challenging. For example, the current SCI network interface cards and drivers limit the amount of shared memory and slow down local accesses to shared memory segments. Despite these difficulties, we present a hybrid DRMA-based implementation that can improve performance over an implementation based only on message passing. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Arpaci, D. Culler, A. Krishnamurthy, S. Steinberg, and K. Yelick. </author> <title> Empirical Evaluation of the CRAY T3D: A compiler perspective. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference: [2] <author> R. Butenuth. </author> <title> Project Arminius - SCI coupled linux PCs. </title> <booktitle> In SISCI Workshop, </booktitle> <address> Oslo, </address> <year> 1997. </year>
Reference: [3] <author> D. E. Culler, A. Dusseau, S. C. Golstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proc. of Supercomputing, </booktitle> <month> November </month> <year> 1993. </year>
Reference: [4] <author> D. E. Culler, R. M. Karp, D. A. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramonian, and T. von Eicken. </author> <title> LogP: Towards a Realistic Model of Parallel Computation. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Raw Bandwidth 8 byte 64 byte Store Bandwidth 28.0 MB/s 66.6 MB/s Load Bandwidth 1.2 MB/s 9.4 MB/s TABLE III BANDWIDTH FOR THE RAW COMMUNICATION PRIMITIVES . B.3.c Overhead/Gap. In addition to latency and bandwidth, two important parameters which define the LogP model <ref> [4] </ref> of a parallel machine are overhead and gap. The overhead is the time the CPU is spending on a network request; the gap is the time the network adapters have to wait between successive network requests.
Reference: [5] <author> J. Ja'Ja' D. Bader. </author> <title> SIMPLE: A methodology for programming high performance algorithms on clusters of symmetric multiprocessors (SMPs). </title> <note> In Maryland CS Technical Reports by David A. Bader(CS-TR-3798), </note> <month> 5 </month> <year> 1997. </year> <title> 10 We believe that an architectural improvement might resolve this problem. </title>
Reference: [6] <author> Dolphin. </author> <title> PCI-SCI Adapter Programming Specification. </title> <publisher> Dolphin Interconnect Solutions Inc., </publisher> <year> 1997. </year>
Reference-contexts: The Dolphin PCI-SCI adapter cards also allow using remote interrupts for signaling, as well as using DMA for large data transfers amongst other features which are not addressed here. Refer to [11], <ref> [6] </ref>, [16] for more information. B.3 SCI Performance Measurements In this subsection, we measure the raw performance of our SCI platform. For this study, we use four Ultra-30 workstations with 250 MHz CPU's, 128 MB RAM, and PCI-based Dolphin network adapters using the LC-2 link controller.
Reference: [7] <author> C. Dubnicki, L. Iftode, E. Felten, and K. Li. </author> <title> Software support for virtual memory mapped communication. </title> <booktitle> In Proceedings of IPPS, </booktitle> <year> 1996. </year>
Reference: [8] <author> M. Eberl, H. Hellwagner, and B. Herland. </author> <title> A common messaging layer for MPI and PVM over SCI. </title> <booktitle> In Proceedings of HPCN Europe, LNCS 1401, </booktitle> <month> April </month> <year> 1998. </year>
Reference: [9] <author> D. B. Gustavson and Q. Li. </author> <title> Local-Area MultiProcessor: the Scalable Coherent Interface. </title> <type> Technical report, </type> <institution> SCIzzl, Santa Clara University, Department of Computer Engineering, </institution> <address> Santa Clara, California, </address> <year> 1995. </year>
Reference: [10] <author> Y.-S. Hwang, B. Moon, S. Sharma, R. Ponnusamy, and J. Saltz R. Das. </author> <title> Runtime and language support for compiling adaptive irregular programs on distributed memory machines. </title> <journal> Software Practice and Experience, </journal> <volume> 25(6), </volume> <month> June </month> <year> 1995. </year>
Reference: [11] <author> M. Ibel, K. E. Schauser, C. J. Scheiman, and M. Weis. </author> <title> High-Performance Cluster Computing Using SCI. </title> <booktitle> In Proc. of Hot Interconnects V, </booktitle> <month> August </month> <year> 1997. </year>
Reference-contexts: The Dolphin PCI-SCI adapter cards also allow using remote interrupts for signaling, as well as using DMA for large data transfers amongst other features which are not addressed here. Refer to <ref> [11] </ref>, [6], [16] for more information. B.3 SCI Performance Measurements In this subsection, we measure the raw performance of our SCI platform. For this study, we use four Ultra-30 workstations with 250 MHz CPU's, 128 MB RAM, and PCI-based Dolphin network adapters using the LC-2 link controller. <p> Now we describe our different implementations of a runtime system to interface the global address space programming model of Split-C with the underlying SCI architecture. We describe our message passing run-time system based on Active Messages (see <ref> [11] </ref> for more details) and the shared address space runtime system based on the direct mapping of Split-C onto SCI. Finally, we compare the two approaches by experimental evaluation. A.
Reference: [12] <editor> IEEE, </editor> <address> 345 East 47th Street, New York. </address> <institution> IEEE Standard for Scalable Coherent Interface (SCI), </institution> <year> 1993. </year>
Reference: [13] <author> David Kranz, Kirk Johnson, Anant Agarwal, John Kubiatowicz, and Beng-Hong Lim. </author> <title> Integrating message-passing and shared-memory: Early experience. </title> <booktitle> In Proceedings of the Fourth Symposium on Principles and Practices of Parallel Programming, </booktitle> <year> 1993. </year>
Reference: [14] <author> Krishnamurthy, A., K. E. Schauser, C. J. Scheiman, R. Y. Wang, D. E. Culler, and K. Yelick. </author> <title> Evaluation of Architectural Support for Global Address-Based Communication in Large-Scale Parallel Machines. </title> <booktitle> In 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1996. </year>
Reference: [15] <author> S. Lumetta and D. Culler. </author> <title> Managing concurrent access for shared memory active messages. </title> <booktitle> In Proceedings of IPPS/SPDP, </booktitle> <address> Orlando, FL, </address> <month> March </month> <year> 1998. </year>
Reference: [16] <author> K. Omang. </author> <title> SCI Clustering through the I/O bus: A Performance and Functionality Analysis. </title> <type> PhD thesis, </type> <institution> University of Oslo, </institution> <month> April </month> <year> 1998. </year>
Reference-contexts: The Dolphin PCI-SCI adapter cards also allow using remote interrupts for signaling, as well as using DMA for large data transfers amongst other features which are not addressed here. Refer to [11], [6], <ref> [16] </ref> for more information. B.3 SCI Performance Measurements In this subsection, we measure the raw performance of our SCI platform. For this study, we use four Ultra-30 workstations with 250 MHz CPU's, 128 MB RAM, and PCI-based Dolphin network adapters using the LC-2 link controller.
Reference: [17] <author> S. Paas, M. Dormanns, T. Bemmerl, K. Scholtyssik, and S. Lankes. </author> <title> Computing on a cluster of PCs: Project overview and early experiences. </title> <type> Technical Report CSR-97-05, </type> <institution> TU Chemnitz, </institution> <year> 1997. </year> <booktitle> First Workshop on Cluster-Computing. </booktitle>
Reference: [18] <author> D. Culler R. Martin, A. Vahdat and T. Anderson. </author> <title> Effects of communication latency, overhead, and bandwidth in a cluster architecture. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <month> 6 </month> <year> 1997. </year>
Reference-contexts: The gap can be larger than the overhead if the network adapters can accept traffic from the CPU at a higher rate than they can inject it into the network. As has been pointed out in <ref> [18] </ref>, optimizing a system for a low overhead is equally or even more important than minimizing the latency. We measure the gap and overhead for the different SCI transaction sizes and show the results in Table IV.
Reference: [19] <author> A. Reinefeld and J. Simon. </author> <title> A high performance compute cluster with SCI. </title> <booktitle> In SISCI Workshop, </booktitle> <address> Oslo, </address> <year> 1997. </year>
Reference: [20] <author> D. Culler S. Lumetta, A. Mainwaring. </author> <title> Multi-protocol active messages on a cluster of SMPs. </title> <booktitle> In Supercomputing '97, </booktitle> <month> 6 </month> <year> 1997. </year>
Reference: [21] <author> R. Saavedra. </author> <title> CPU Performance Analysis and Execution Time Prediction using Narrow Spectrum Benchmarking. </title> <type> PhD thesis, </type> <institution> UC Berkeley, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: However, it turns out that even local memory accesses to the pinned down exported memory segments perform worse than local memory accesses to not exported memory segments. Figure 4 shows the result of a simple cache benchmark adapted from [22], <ref> [21] </ref>, which yields the memory access times for strided memory references in arrays of varying sizes. The two figures are for standard memory references (left) and exported memory (right).
Reference: [22] <author> R. Saavedra, S. Gaines, and M. Carlton. </author> <title> Micro benchmark analysis of the KSR1. </title> <booktitle> In Proceedings of Supercomputing, </booktitle> <year> 1993. </year>
Reference-contexts: However, it turns out that even local memory accesses to the pinned down exported memory segments perform worse than local memory accesses to not exported memory segments. Figure 4 shows the result of a simple cache benchmark adapted from <ref> [22] </ref>, [21], which yields the memory access times for strided memory references in arrays of varying sizes. The two figures are for standard memory references (left) and exported memory (right).
Reference: [23] <author> M. Schmitt, M. Ibel, A. Acharya, and K. Schauser. </author> <title> Adaptive receiver notification for non-dedicated workstation clusters. </title> <booktitle> In Proceedings of PACT, </booktitle> <year> 1998. </year>
Reference-contexts: This allows for a much higher performance of applications on non-dedicated clusters. We plan to compare our two basic Split-C implementations on clusters running interactive workloads and multiple parallel applications and study the tradeoffs, similar to <ref> [23] </ref>. We also plan to investigate several replacement strategies and monitoring techniques to optimally select memory regions which work best with message passing or direct remote memory accesses.
Reference: [24] <author> T. von Eicken. </author> <title> Active Messages: an Efficient Communications Architecture for Multiprocessors. </title> <type> PhD thesis, </type> <institution> Computer Science Division EECS, U.C. Berkeley, </institution> <month> December </month> <year> 1993. </year>
References-found: 24

