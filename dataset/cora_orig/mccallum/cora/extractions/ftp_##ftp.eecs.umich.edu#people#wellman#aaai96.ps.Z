URL: ftp://ftp.eecs.umich.edu/people/wellman/aaai96.ps.Z
Refering-URL: http://ai.eecs.umich.edu/people/wellman/Publications.html
Root-URL: http://www.cs.umich.edu
Email: fpynadath,wellmang@umich.edu  
Title: Generalized Queries on Probabilistic Context-Free Grammars  
Author: David V. Pynadath and Michael P. Wellman 
Address: 1101 Beal Avenue Ann Arbor, MI 48109 USA  
Affiliation: Artificial Intelligence Laboratory University of Michigan  
Date: August 1996  
Note: In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96), Portland, OR, USA,  
Abstract: Probabilistic context-free grammars (PCFGs) provide a simple way to represent a particular class of distributions over sentences in a context-free language. Efficient parsing algorithms for answering particular queries about a PCFG (i.e., calculating the probability of a given sentence, or finding the most likely parse) have been applied to a variety of pattern-recognition problems. We extend the class of queries that can be answered in several ways: (1) allowing missing tokens in a sentence or sentence fragment, (2) supporting queries about intermediate structure, such as the presence of particular nonterminals, and (3) flexible conditioning on a variety of types of evidence. Our method works by constructing a Bayesian network to represent the distribution of parse trees induced by a given PCFG. The network structure mirrors that of the chart in a standard parser, and is generated using a similar dynamic-programming approach. We present an algorithm for constructing Bayesian networks from PCFGs, and show how queries or patterns of queries on the network correspond to interesting queries on PCFGs. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Briscoe, T., and Carroll, J. </author> <year> 1993. </year> <title> Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars. </title> <booktitle> Computational Linguistics 19(1) </booktitle> <pages> 25-59. </pages>
Reference-contexts: The Bayesian network also provides a flexible structure for future extensions to context-sensitive probabilities, similar to the probabilistic parse tables of <ref> (Briscoe & Carroll 1993) </ref>.
Reference: <author> Charniak, E., and Carroll, G. </author> <year> 1994. </year> <title> Context-sensitive statistics for improved grammatical language models. </title> <booktitle> In Proceedings of the National Conference on AI, </booktitle> <pages> 728-733. </pages>
Reference-contexts: For instance, we may make the conditional probabilities a function of the (i; j; k) index values. Alternatively, we may introduce additional dependencies on other nodes in the network, or perhaps on features beyond the parse tree itself. The context-sensitivity of <ref> (Charniak & Carroll 1994) </ref>, which conditions the production probabilities on the parent of the left-hand side symbol, would require only an additional link from N nodes to their potential children P nodes.
Reference: <author> Charniak, E. </author> <year> 1993. </year> <title> Statistical Language Learning. </title> <address> Cam-bridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Therefore, the probability of a given derivation is simply the product of the probabilities of the individual productions involved. The probability of a string in the language is the sum taken over all possible derivations. In the grammar (from <ref> (Charniak 1993) </ref>) shown in Figure 1, the start symbol is s. s ! np vp (0.8) pp ! p np (1.0) s ! vp (0.2) p ! like (1.0) np ! n (0.4) v ! swat (0.2) np ! n pp (0.4) v ! flies (0.4) np ! n np (0.2) <p> This algorithm is capable of computing any inside probability, the probability of a particular terminal string appearing inside the subtree rooted by a particular nonterminal. We can work top-down in an analogous manner to compute any outside probability <ref> (Charniak 1993) </ref>, the probability of a subtree rooted by a particular nonterminal appearing amid a particular terminal string. Given these probabilities we can compute the probability of any particular nonterminal symbol appearing in the parse tree as the root of a subtree covering some substring.
Reference: <author> Chou, P. </author> <year> 1989. </year> <title> Recognition of equations using a two-dimensional stochastic context-free grammar. </title> <booktitle> In Proceedings SPIE, Visual Communications and Image Processing IV, </booktitle> <pages> 852-863. </pages>
Reference-contexts: Introduction Most pattern-recognition problems start from observations generated by some structured stochastic process. Probabilistic context-free grammars (PCFGs) (Gonzalez & Thomason 1978; Charniak 1993) have provided a useful method for modeling uncertainty in a wide range of structures, including programming languages (Wetherell 1980), images <ref> (Chou 1989) </ref>, speech signals (Ney 1992), and RNA sequences (Sakakibara et al. 1995). Domains like plan recognition, where non-probabilistic grammars have provided useful models (Vilain 1990), may also benefit from an explicit stochastic model.
Reference: <author> Earley, J. </author> <year> 1970. </year> <title> An efficient context-free parsing algorithm. </title> <journal> Communications of the Association for Computing Machinery 13(2) </journal> <pages> 94-102. </pages>
Reference: <author> Gonzalez, R. C., and Thomason, M. S. </author> <year> 1978. </year> <title> Syntactic pattern recognition: An introduction. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley Publishing Company. </publisher> <pages> 177-215. </pages>
Reference: <author> Jelinek, F.; Lafferty, J. D.; and Mercer, R. </author> <year> 1992. </year> <title> Basic methods of probabilistic context free grammars. </title> <editor> In Laface, P., and DeMori, R., eds., </editor> <title> Speech Recognition and Understanding. </title> <publisher> Berlin: Springer. </publisher> <pages> 345-360. </pages>
Reference-contexts: Fortunately, parse trees often share common subtrees, a fact exploited by the standard dynamic programming approach for both probabilistic and non-probabilistic CFGs <ref> (Jelinek, Lafferty, & Mercer 1992) </ref>. The central structure is a table, or chart, storing previous results for each substring in the input sentence. Each entry in the chart corresponds to a substring x i x i+j1 (ignoring abstraction level, k) of the observation string x 1 x L . <p> For example, in the sentence Swat flies like ants, we can compute the probability that like ants is a prepositional phrase, using a combination of inside and outside probabilities. The Left-to-Right Inside (LRI) algorithm <ref> (Jelinek, Lafferty, & Mercer 1992) </ref> specifies how we can manipulate certain probability matrices and combine the results with the inside probabilities to obtain the probability of a given initial substring, such as the probability of a sentence (of any length) beginning with the words Swat flies. <p> The network represents a distribution over strings of bounded length, so we cannot obtain the same probability of an initial substring x 1 x 2 x L as <ref> (Jelinek, Lafferty, & Mer cer 1992) </ref>, which considered all completion lengths. However, we can find initial substring probabilities over completions of length bounded by nL.
Reference: <author> Ney, H. </author> <year> 1992. </year> <title> Stochastic grammars and pattern recognition. </title> <editor> In Laface, P., and DeMori, R., eds., </editor> <title> Speech Recognition and Understanding. </title> <publisher> Berlin: Springer. </publisher> <pages> 319-344. </pages>
Reference-contexts: Introduction Most pattern-recognition problems start from observations generated by some structured stochastic process. Probabilistic context-free grammars (PCFGs) (Gonzalez & Thomason 1978; Charniak 1993) have provided a useful method for modeling uncertainty in a wide range of structures, including programming languages (Wetherell 1980), images (Chou 1989), speech signals <ref> (Ney 1992) </ref>, and RNA sequences (Sakakibara et al. 1995). Domains like plan recognition, where non-probabilistic grammars have provided useful models (Vilain 1990), may also benefit from an explicit stochastic model.
Reference: <author> Pearl, J. </author> <year> 1987. </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We seek, therefore, a mechanism that would admit observational evidence of any form as part of a query about a PCFG, without requiring us to enumerate all consistent parse trees. Bayesian Networks for PCFGs Bayesian networks <ref> (Pearl 1987) </ref> provide an expressive and efficient representation for probability distributions. They are expressive in that they can represent any joint distribution over a finite set of discrete-valued random variables. They are efficient in that they exploit an important class of conditional independence relationships among the random variables.
Reference: <author> Pynadath, D. V., and Wellman, M. P. </author> <year> 1995. </year> <title> Accounting for context in plan recognition, with application to traffic monitoring. </title> <booktitle> In Proceedings of the Conference on Uncertainty in AI, </booktitle> <pages> 472-481. </pages>
Reference: <author> Sakakibara, Y.; Brown, M.; Underwood, R. C.; Mian, I. S.; and Haussler, D. </author> <year> 1995. </year> <title> Stochastic context-free grammars for modeling RNA. </title> <booktitle> In Proceedings of the 27th Hawaii International Conference on System Sciences, </booktitle> <pages> 284-293. </pages>
Reference-contexts: Probabilistic context-free grammars (PCFGs) (Gonzalez & Thomason 1978; Charniak 1993) have provided a useful method for modeling uncertainty in a wide range of structures, including programming languages (Wetherell 1980), images (Chou 1989), speech signals (Ney 1992), and RNA sequences <ref> (Sakakibara et al. 1995) </ref>. Domains like plan recognition, where non-probabilistic grammars have provided useful models (Vilain 1990), may also benefit from an explicit stochastic model. Once we have created a PCFG model of a process, we can apply existing PCFG parsing algorithms to answer a variety of queries.
Reference: <author> Vilain, M. </author> <year> 1990. </year> <title> Getting serious about parsing plans: A grammatical analysis of plan recognition. </title> <booktitle> In Proceedings of the National Conference on AI, </booktitle> <pages> 190-197. </pages>
Reference-contexts: Domains like plan recognition, where non-probabilistic grammars have provided useful models <ref> (Vilain 1990) </ref>, may also benefit from an explicit stochastic model. Once we have created a PCFG model of a process, we can apply existing PCFG parsing algorithms to answer a variety of queries.
Reference: <author> Wetherell, C. S. </author> <year> 1980. </year> <title> Probabilistic languages: a review and some open questions. </title> <journal> Comp. </journal> <volume> Surveys 12(4) </volume> <pages> 361-379. </pages>
Reference-contexts: Introduction Most pattern-recognition problems start from observations generated by some structured stochastic process. Probabilistic context-free grammars (PCFGs) (Gonzalez & Thomason 1978; Charniak 1993) have provided a useful method for modeling uncertainty in a wide range of structures, including programming languages <ref> (Wetherell 1980) </ref>, images (Chou 1989), speech signals (Ney 1992), and RNA sequences (Sakakibara et al. 1995). Domains like plan recognition, where non-probabilistic grammars have provided useful models (Vilain 1990), may also benefit from an explicit stochastic model.
Reference: <author> Younger, D. </author> <year> 1967. </year> <title> Recognition and parsing of context-free languages in time n 3 . Info. </title> <booktitle> and Control 10(2) </booktitle> <pages> 189-208. </pages>
References-found: 14

