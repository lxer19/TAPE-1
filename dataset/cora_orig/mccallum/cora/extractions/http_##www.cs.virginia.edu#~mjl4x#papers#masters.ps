URL: http://www.cs.virginia.edu/~mjl4x/papers/masters.ps
Refering-URL: http://www.cs.virginia.edu/~mjl4x/vitae.html
Root-URL: http://www.cs.virginia.edu
Title: DPS: A Distributed Program Simulator and Performance Measurement Tool  
Author: Michael J. Lewis C. Mic Bowman 
Affiliation: The Pennsylvania State University The Graduate School The Department of Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Al Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Robert Manchek, and Vaidy Sun-deram. </author> <title> PVM 3 User's Guide and Reference Manual. </title> <type> Technical Report ORNL/TM-12187, </type> <institution> Oak Ridge National Labs, Oak Ridge, TN, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Both MatrixMultiply1 and MatrixMultiply2 contain a single Master process, named Master.p [0], and four Workers, named Worker.p [0], Worker.p <ref> [1] </ref>, Worker.p [2], and Worker.p [3]. Each process in both paradigms is simple enough to be specified within a single functional unit. Multiple functional units are only necessary when the same message can have different meanings depending upon the state of the receiving process when that message arrives. <p> The four distinct htags cause DPS to map the five processes onto four different machines. Master.p [0] will be run on an SGI whose SPEED variable is greater than or equal to 1000. Worker.p [0] will be executed on the machine named htsun2. Processes Worker.p <ref> [1] </ref> and Worker.p [3] will be run on the same host, an SGI named htsgi3. (Providing the machine name and architecture type may seem redundant, but this could be done to ensure that a particular machine is of a specified type.) Finally, DPS will map Worker.p [2] to a machine that
Reference: [2] <author> Brian K. Grant and Anthony Skjellum. </author> <title> The PVM System: An In-Depth Analysis and Documenting Study Concise Edition. </title> <type> Technical Report TR UCRL-JC-112016, </type> <institution> Lawrence Livermore National Laboratories, Livermore, </institution> <address> CA, </address> <year> 1992. </year>
Reference-contexts: Both MatrixMultiply1 and MatrixMultiply2 contain a single Master process, named Master.p [0], and four Workers, named Worker.p [0], Worker.p [1], Worker.p <ref> [2] </ref>, and Worker.p [3]. Each process in both paradigms is simple enough to be specified within a single functional unit. Multiple functional units are only necessary when the same message can have different meanings depending upon the state of the receiving process when that message arrives. <p> Processes Worker.p [1] and Worker.p [3] will be run on the same host, an SGI named htsgi3. (Providing the machine name and architecture type may seem redundant, but this could be done to ensure that a particular machine is of a specified type.) Finally, DPS will map Worker.p <ref> [2] </ref> to a machine that is neither an SGI nor an IBM. The Terrain Specification Section of MatrixMultiply2's blueprint (Figure 20) presents a more realistic and useful example of mapping processes to machines. It ensures that all processes run on separate processors by assigning distinct htags to each process.
Reference: [3] <author> Michael J. Lewis and Raymond E. Cline Jr. </author> <title> PVM Communication Performance in Switched FDDI Heterogeneous Distributed Computing Environments. </title> <booktitle> Proceedings of the IEEE Workshop on Advances in Parallel and Distributed Systems, </booktitle> <pages> pages 13-19, </pages> <month> October </month> <year> 1993. </year> <note> 23 Master(A,B) for i = 1 to numworkers Send(Worker[i],A,B); for i = 1 to A.size for j = 1 to B.size Receive(&Worker,&value,&row,&col); if (row&gt;=0) C[row,col] = value; Send(Worker,i,j); for i = 1 to numworkers Receive(&Worker,&value,&row,&col); if (row&gt;=0) C[row,col] = value; for i = 1 to numworkers Send(Worker[i],SHUTDOWN); WriteMatrix(C); Worker(id) Receive(Master,&A,&B); Send(Master,0,-1,-1); while (continue) Receive(Master,&row,&col); value = Multiplyvector(A[row,*],B[*,col]); Send(Master,value,row,col); 24 25 26 </note>
Reference-contexts: Both MatrixMultiply1 and MatrixMultiply2 contain a single Master process, named Master.p [0], and four Workers, named Worker.p [0], Worker.p [1], Worker.p [2], and Worker.p <ref> [3] </ref>. Each process in both paradigms is simple enough to be specified within a single functional unit. Multiple functional units are only necessary when the same message can have different meanings depending upon the state of the receiving process when that message arrives. <p> The four distinct htags cause DPS to map the five processes onto four different machines. Master.p [0] will be run on an SGI whose SPEED variable is greater than or equal to 1000. Worker.p [0] will be executed on the machine named htsun2. Processes Worker.p [1] and Worker.p <ref> [3] </ref> will be run on the same host, an SGI named htsgi3. (Providing the machine name and architecture type may seem redundant, but this could be done to ensure that a particular machine is of a specified type.) Finally, DPS will map Worker.p [2] to a machine that is neither an
References-found: 3

