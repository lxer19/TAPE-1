URL: http://www.cs.yale.edu/HTML/YALE/CS/HyPlans/douglas-craig/Preprints/pub37.ps.gz
Refering-URL: http://www.cs.yale.edu/HTML/YALE/CS/HyPlans/douglas-craig/ccd-preprints.html
Root-URL: http://www.cs.yale.edu
Title: GEMMW: A PORTABLE LEVEL 3 BLAS WINOGRAD VARIANT OF STRASSEN'S MATRIX-MATRIX MULTIPLY ALGORITHM  
Author: CRAIG C. DOUGLAS MICHAEL HEROUX GORDON SLISHMAN AND ROGER M. SMITH 
Keyword: Key Words. Level 3 BLAS, matrix multiplication, Winograd's variant of Strassen's algorithm, multilevel algorithms AMS(MOS) subject classification. Numerical Analysis: Numerical Linear Algebra  
Abstract: Matrix-matrix multiplication is normally computed using one of the BLAS or a reinvention of part of the BLAS. Unfortunately, the BLAS were designed with small matrices in mind. When huge, well conditioned matrices are multiplied together, the BLAS perform like the blahs, even on vector machines. For matrices where the coefficients are well conditioned, Winograd's variant of Strassen's algorithm offers some relief, but is rarely available in a quality form on most computers. We reconsider this method and offer a highly portable solution based on the Level 3 BLAS interface. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aho, J. Hopcroft, and J. Ullman, </author> <title> The Design and Analysis of Computer Algorithms, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1974. </year>
Reference-contexts: 1. Preliminaries. Matrix-matrix multiplication is a very basic computer oper ation. A very clear description of how to do it can be found in many textbooks, e.g., <ref> [1] </ref>. Suppose we want to multiply two matrices A : M fi K and B : K fi N; where the elements of A and B are real or complex numbers and M , K, and N are natural numbers. <p> In other words, caveat emptor for either class of matrix-matrix multiplication in real codes. For a more complete discussion of this issue, see [3] and [8] along with their references. An interesting application to solving linear systems of equations is contained in <ref> [1] </ref> and [4]. <p> Complex Strassen-Winograd. Provided with gemmw is a specialized version of the classical matrix multiplication algorithm for complex matrices. Let P , Q, R, and S be real matrices. A well known trick <ref> [1] </ref> calculates (P + Qi) (R + Si) using the formula [P (R S) + (P Q) S)] + [(P + Q) R P (R S)]i: Note that there are only 3 matrix multiplies instead of the usual 4. When applying this trick to Strassen-Winograd there are two options: 1.
Reference: [2] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. J. Dongarra, and J. Du Croz, </author> <title> LAPACK Users' Guide, </title> <publisher> SIAM Books, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: In this manner, it is trivial to make the code work with the BLAS, Cray Scientific Library [9], ESSL [10], NAG [11], or any other library the user chooses. We used the BLAS distributed with LAPACK <ref> [2] </ref>. A list of the macros and the routines that are actually called is contained in Table 2. Some of the operations required by Strassen-Winograd (e.g., op (A) + op (B)) are not part of the Level 3 BLAS.
Reference: [3] <author> D. H. Bailey, </author> <title> Extra high speed matrix multiplication on the Cray-2, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 9 (1988), </volume> <pages> pp. 603-607. </pages>
Reference-contexts: Hence, for matrices A and B, we assume the coefficients are "well conditioned" enough so that both methods get acceptable answers. In other words, caveat emptor for either class of matrix-matrix multiplication in real codes. For a more complete discussion of this issue, see <ref> [3] </ref> and [8] along with their references. An interesting application to solving linear systems of equations is contained in [1] and [4].
Reference: [4] <author> D. H. Bailey, K. Lee, and H. Simon, </author> <title> Using Strassen's algorithm to accelerate the solution of linear systems, </title> <journal> J. Supercomp., </journal> <volume> 4 (1990), </volume> <pages> pp. 357-371. </pages>
Reference-contexts: In other words, caveat emptor for either class of matrix-matrix multiplication in real codes. For a more complete discussion of this issue, see [3] and [8] along with their references. An interesting application to solving linear systems of equations is contained in [1] and <ref> [4] </ref>.
Reference: [5] <author> N. Carriero and D. Gelernter, </author> <title> Linda in context, </title> <journal> Comm. ACM, </journal> <volume> 32 (1989), </volume> <pages> pp. 444-458. </pages>
Reference-contexts: Hence, we took a tuple (M ,K,N ) of (3,4,5) and produced a "partitioning" (M,K,N ) of (2,3,2) of blocks of A, B, and C. 8 The user stores submatrices of A and B into the parallel data base. In our example code, the Linda system <ref> [5] </ref> was used. The computation continues with a call to the parallel matrix multiplication routine, matmulp, with seven parameters. These include the original sizes (M ,K,N ), the partitioning (M,K,N ), and the number of processors (p) to use.
Reference: [6] <author> J. J. Dongarra, J. Du Croz, S. Hammarling, and I. Duff, </author> <title> A set of level 3 basic linear algebra subprograms, </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 16 (1990), </volume> <pages> pp. 1-17. </pages>
Reference-contexts: For a more complete discussion of this issue, see [3] and [8] along with their references. An interesting application to solving linear systems of equations is contained in [1] and [4]. This paper is actually interested in a highly portable Level 3 BLAS <ref> [6] </ref> interface for computing C ff op (A)op (B) + fi C;(2) where op (X) = &gt; &gt; &lt; X; X transpose; X conjugate transpose; X conjugate; 2 and Most of the discussion will ignore the conjugate and transpose cases, but the implementation is that of (2).
Reference: [7] <author> C. C. Douglas and J. Douglas, </author> <title> A unified convergence theory for abstract multigrid or multilevel algorithms, serial and parallel, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 30 (1993), </volume> <pages> pp. 136-158. </pages>
Reference-contexts: Strassen's method recursively works with sets of 2 fi 2 submatrices to form the product using 7 matrix multiplications instead of the obvious 8. This is not very different from standard multilevel methods <ref> [7] </ref> used routinely to solve partial differential equations. Strictly speaking, we compute A 21 A 22 B 11 B 12 C 21 C 22 fl Yale University Department of Computer Science Report YALEU/DCS/TR-904, New Haven, CT, 1992.
Reference: [8] <author> N. J. Higham, </author> <title> Exploiting fast matrix multiplication within the Level 3 BLAS, </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 16 (1990), </volume> <pages> pp. 352-368. </pages>
Reference-contexts: Hence, for matrices A and B, we assume the coefficients are "well conditioned" enough so that both methods get acceptable answers. In other words, caveat emptor for either class of matrix-matrix multiplication in real codes. For a more complete discussion of this issue, see [3] and <ref> [8] </ref> along with their references. An interesting application to solving linear systems of equations is contained in [1] and [4].
Reference: [9] <author> J. Madsen, </author> <title> Volume 3: UNICOS Math and Scientific Library Reference Manual (SR-2081), </title> <institution> Cray Research, Inc., </institution> <note> version 6.0 ed., </note> <year> 1990. </year>
Reference-contexts: The code is flexible enough that by modifying the macro definitions in one header file, essentially any library can be substituted for the default ones. In this manner, it is trivial to make the code work with the BLAS, Cray Scientific Library <ref> [9] </ref>, ESSL [10], NAG [11], or any other library the user chooses. We used the BLAS distributed with LAPACK [2]. A list of the macros and the routines that are actually called is contained in Table 2.
Reference: [10] <author> L. Mason and M. E. Sliva, </author> <title> Engineering and Scientific Subroutine Library: Guide and Reference, </title> <type> Version 2, </type> <institution> IBM Corporation, Kingston, NY, 1.0 ed., </institution> <year> 1992. </year>
Reference-contexts: The code is flexible enough that by modifying the macro definitions in one header file, essentially any library can be substituted for the default ones. In this manner, it is trivial to make the code work with the BLAS, Cray Scientific Library [9], ESSL <ref> [10] </ref>, NAG [11], or any other library the user chooses. We used the BLAS distributed with LAPACK [2]. A list of the macros and the routines that are actually called is contained in Table 2.
Reference: [11] <author> NAG, </author> <title> NAG Fortran Library Manual, Numerical Algorithms Group, </title> <publisher> Ltd., Oxford, </publisher> <address> UK, Mark 14 ed., </address> <year> 1990. </year>
Reference-contexts: The code is flexible enough that by modifying the macro definitions in one header file, essentially any library can be substituted for the default ones. In this manner, it is trivial to make the code work with the BLAS, Cray Scientific Library [9], ESSL [10], NAG <ref> [11] </ref>, or any other library the user chooses. We used the BLAS distributed with LAPACK [2]. A list of the macros and the routines that are actually called is contained in Table 2.
Reference: [12] <author> V. Strassen, </author> <title> Gaussian elimination is not optimal, </title> <journal> Numer. Math., </journal> <volume> 13 (1969), </volume> <pages> pp. 354-356. </pages>
Reference-contexts: = A 12 B 21 S 5 = B 12 B 11 M 5 = S 1 S 5 C 11 = M 2 + M 3 S 7 = B 22 B 12 M 7 = A 22 S 8 C 21 = T 2 M 7 (1) (see <ref> [12] </ref> and [13]). This is not a very convenient way to define this algorithm, but it is the standard textbook definition. Unlike textbook exercises, we do not require square matrices nor restrict the dimensions to 2 k for some natural number k.

References-found: 12

