URL: http://www.cs.duke.edu/~jsv/Papers/HoV92.actech.ps.gz
Refering-URL: http://www.cs.duke.edu/~jsv/Papers/catalog/node30.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Practical Implementations of Arithmetic Coding  Image and Text Compression,  
Author: Paul G. Howard and Jeffrey Scott Vitter James A. Storer, 
Date: October 16-18, 1991.  
Address: Canada,  
Affiliation: Victoria, British Columbia,  
Note: Appears in  ed., Kluwer Academic Publishers, Norwell, MA, 1992, pages 85-112. A shortened version appears in the proceedings of the International Conference on Advances in Communication and Control (COMCON 3),  
Abstract: Brown University Department of Computer Science Technical Report No. 92-18 Revised version, April 1992 (Formerly Technical Report No. CS-91-45) 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Abramson, </author> <title> Information Theory and Coding, </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1963. </year> <month> 27 </month>
Reference-contexts: In practice we have to output 7 bits. 2 The idea of arithmetic coding originated with Shannon in his seminal 1948 paper on information theory [54]. It was rediscovered by Elias about 15 years later, as briefly mentioned in <ref> [1] </ref>. Implementation details. The basic implementation of arithmetic coding described above has two major difficulties: the shrinking current interval requires the use of high precision arithmetic, and no output is produced until the entire file has been read.
Reference: [2] <author> R. B. Arps, T. K. Truong, D. J. Lu, R. C. Pasco & T. D. Friedman, </author> <title> "A MultiPurpose VLSI Chip for Adaptive Data Compression of Bilevel Images," </title> <journal> IBM J. Res. Develop. </journal> <volume> 32 (Nov. </volume> <year> 1988), </year> <pages> 775-795. </pages>
Reference: [3] <author> T. Bell, </author> <title> "A Unifying Theory and Improvements for Existing Approaches to Text Compression," </title> <institution> Univ. of Canterbury, </institution> <type> Ph.D. Thesis, </type> <year> 1986. </year>
Reference: [4] <author> T. Bell & A. M. Moffat, </author> <title> "A Note on the DMC Data Compression Scheme," </title> <journal> Computer Journal 32 (1989), </journal> <pages> 16-20. </pages>
Reference: [5] <author> T. C. Bell, J. G. Cleary & I. H. Witten, </author> <title> Text Compression, </title> <publisher> Prentice-Hall, </publisher> <address> Engle-wood Cliffs, NJ, </address> <year> 1990. </year>
Reference-contexts: A minor disadvantage is the need to indicate the end of the file. One final minor problem is that arithmetic codes have poor error resistance, especially when used with adaptive models <ref> [5] </ref>. A single bit error in the encoded file causes the decoder's internal state to be in error, making the remainder of the decoded file wrong. In fact this is a drawback of all adaptive codes, including Ziv-Lempel codes and adaptive Huffman codes [12,15,18,26,55,56]. <p> state, given by H = i=1 The excess code length is E = L H; if we let d i = q i p i and expand asymptotically in d, we obtain E = i=1 1 d 2 p i i i : (1) (This corrects a similar derivation in <ref> [5] </ref>, in which the factor of 1= ln 2 is omitted.) The vanishing of the linear terms means that small errors in the probabilities used by the coder lead to very small increases in code length. <p> PPMP and PPMX appear in [57]; they are based on the assumption that the appearance of symbols for the first time in a 12 3 FAST ARITHMETIC CODING file is approximately a Poisson process. See Table 1 for formulas for the probabilities used by the different methods, and see <ref> [5] </ref> or [6] for a detailed description of the PPM method. <p> We have compared PPMC and PPMD on the Bell-Cleary-Witten corpus <ref> [5] </ref> (including the four papers not described in the book). Table 2 shows that for text files PPMD compresses consistently about 0.02 bit per character better than PPMC. The compression results for PPMC differ from those reported in [5] because of implementation differences; we used versions of PPMC and PPMD that <p> We have compared PPMC and PPMD on the Bell-Cleary-Witten corpus <ref> [5] </ref> (including the four papers not described in the book). Table 2 shows that for text files PPMD compresses consistently about 0.02 bit per character better than PPMC. The compression results for PPMC differ from those reported in [5] because of implementation differences; we used versions of PPMC and PPMD that were identical except for the escape probability calculations. PPMD has the added advantage of making analysis more tractable by making the code length independent of the appearance order of symbols in the context. Indirect probability estimation. <p> We expect that using indirect probability estimation in conjunction with the full multi-order PPM mechanism will yield substantially improved compression. 3.6 Hashed high-order Markov models. For finding contexts in the PPM method, Moffat [37] and Bell et al. <ref> [5] </ref> give complicated data structures called backward trees and vine pointers. For fast access and minimal memory usage we propose single hashing without collision resolution.
Reference: [6] <author> T. C. Bell, I. H. Witten & J. G. Cleary, </author> <title> "Modeling for Text Compression," </title> <journal> Comput. </journal> <note> Surveys 21 (Dec. </note> <year> 1989), </year> <pages> 557-591. </pages>
Reference-contexts: See Table 1 for formulas for the probabilities used by the different methods, and see [5] or <ref> [6] </ref> for a detailed description of the PPM method.
Reference: [7] <author> J. L. Bentley, D. D. Sleator, R. E. Tarjan & V. K. Wei, </author> <title> "A Locally Adaptive Data Compression Scheme," </title> <journal> Comm. ACM 29 (Apr. </journal> <year> 1986), </year> <pages> 320-330. </pages>
Reference: [8] <author> A. C. Blumer & R. J. </author> <title> McEliece, "The Renyi Redundancy of Generalized Huffman Codes," </title> <journal> IEEE Trans. Inform. </journal> <note> Theory IT-34 (Sept. </note> <year> 1988), </year> <pages> 1242-1249. </pages>
Reference: [9] <author> R. M. Capocelli, R. Giancarlo & I. J. Taneja, </author> <title> "Bounds on the Redundancy of Huffman Codes," </title> <journal> IEEE Trans. Inform. Theory IT-32 (Nov. </journal> <year> 1986), </year> <pages> 854-857. </pages>
Reference: [10] <author> D. Chevion, E. D. Karnin & E. Walach, </author> <title> "High Efficiency, Multiplication Free Approximation of Arithmetic Coding," </title> <booktitle> in Proc. Data Compression Conference, </booktitle> <editor> J. A. Storer & J. H. Reif, eds., </editor> <address> Snowbird, Utah, </address> <month> Apr. </month> <pages> 8-11, </pages> <year> 1991, </year> <pages> 43-52. </pages>
Reference-contexts: Historically, much of the arithmetic coding research by Rissanen, Langdon, and others at IBM has focused on bilevel images [29]. The Q-Coder [2,27,33,41,42,43] is a binary arithmetic coder; work by Rissanen and Mohiuddin [50] and Chevion et al. <ref> [10] </ref> extends some of the Q-Coder ideas to multi-symbol alphabets.
Reference: [11] <author> G. V. Cormack & R. N. Horspool, </author> <title> "Data Compression Using Dynamic Markov Modelling," </title> <note> Computer Journal 30 (Dec. </note> <year> 1987), </year> <pages> 541-550. </pages>
Reference-contexts: In practice there are several ways to do this: * Periodically restarting the model. This often discards too much information to be effective, although Cormack and Horspool find that it gives good results when growing large dynamic Markov models <ref> [11] </ref>. * Using a sliding window on the text [26]. This requires excessive computational resources. * Recency rank coding [7,13,53]. This is simple but corresponds to a rather coarse model of recency. * Exponential aging (giving exponentially increasing weights to successive symbols) [12,38]. <p> text files, the increased sophistication invariably takes the form of conditioning the symbol probabilities on contexts consisting of one or more symbols of preceding text. (Langdon [28] and Bell, Witten, Cleary, and Moffat [3,4,5] have proven that both Ziv-Lempel coding and the dynamic Markov coding method of Cormack and Horspool <ref> [11] </ref> can be reduced to finite context models, despite superficial indications to the contrary.) One significant difficulty with using high-order models is that many contexts do not occur often enough to provide reliable symbol probability estimates.
Reference: [12] <author> G. V. Cormack & R. N. Horspool, </author> <title> "Algorithms for Adaptive Huffman Codes," </title> <journal> Inform. Process. Lett. </journal> <month> 18 (Mar. </month> <year> 1984), </year> <pages> 159-165. </pages>
Reference: [13] <author> P. Elias, </author> <title> "Interval and Recency Rank Source Coding: Two On-line Adaptive Variable Length Schemes," </title> <journal> IEEE Trans. Inform. Theory IT-33 (Jan. </journal> <year> 1987), </year> <pages> 3-10. </pages>
Reference: [14] <author> P. Elias, </author> <title> "Universal Codeword Sets and Representations of Integers," </title> <journal> IEEE Trans. Inform. Theory IT-21 (Mar. </journal> <year> 1975), </year> <pages> 194-203. </pages>
Reference-contexts: Example 9 : We can derive the Elias code for the positive integers <ref> [14] </ref> by using the maximally unbalanced subdivision technique of Example 8 and by doubling the full integer range whenever we see enough 1s to output a bit and expand the current interval so that it coincides with the full range.
Reference: [15] <author> N. Faller, </author> <title> "An Adaptive System for Data Compression," </title> <booktitle> Record of the 7th Asilo-mar Conference on Circuits, Systems, and Computers, </booktitle> <year> 1973. </year>
Reference: [16] <author> Ph. Flajolet, </author> <title> "Approximate Counting: a Detailed Analysis," </title> <note> BIT 25 (1985), 113. </note>
Reference: [17] <author> Ph. Flajolet & G. N. N. Martin, </author> <title> "Probabilistic Counting Algorithms for Data Base Applications," </title> <institution> INRIA, Rapport de Recherche No. </institution> <month> 313, June </month> <year> 1984. </year>
Reference: [18] <author> R. G. Gallager, </author> <title> "Variations on a Theme by Huffman," </title> <journal> IEEE Trans. Inform. Theory IT-24 (Nov. </journal> <year> 1978), </year> <pages> 668-674. </pages>
Reference: [19] <author> M. Guazzo, </author> <title> "A General Minimum-Redundancy Source-Coding Algorithm," </title> <journal> IEEE Trans. Inform. Theory IT-26 (Jan. </journal> <year> 1980), </year> <pages> 15-25. </pages> <address> 28 4 CONCLUSION </address>
Reference-contexts: This follow-on procedure may be repeated any number of times, so the current interval size is always longer than 1/4. Mechanisms for incremental transmission and fixed precision arithmetic have been developed through the years by Pasco [40], Rissanen [48], Rubin [52], Rissanen and Langdon [49], Guazzo <ref> [19] </ref>, and Witten, Neal, and Cleary [58]. The bit-stuffing idea of Langdon and others at IBM that limits the propagation of carries in the additions is roughly equivalent to the follow-on procedure described above. We now describe in detail how the coding and interval expansion work.
Reference: [20] <author> M. E. Hellman, </author> <title> "Joint Source and Channel Encoding," </title> <booktitle> Proc. Seventh Hawaii International Conf. System Sci., </booktitle> <year> 1974. </year>
Reference: [21] <author> R. N. Horspool, </author> <title> "Improving LZW," </title> <booktitle> in Proc. Data Compression Conference, </booktitle> <editor> J. A. Storer & J. H. Reif, eds., </editor> <address> Snowbird, Utah, </address> <month> Apr. </month> <pages> 8-11, </pages> <year> 1991, </year> <pages> 332-341. </pages>
Reference: [22] <author> P. G. Howard & J. S. Vitter, </author> <title> "Analysis of Arithmetic Coding for Data Compression," </title> <booktitle> Information Processing and Management 28 (1992), </booktitle> <pages> 749-763. </pages>
Reference-contexts: Overview of this paper. In Section 2 we give a tutorial on arithmetic coding. We include an introduction to modeling for text compression. We also restate several important theorems from <ref> [22] </ref> relating to the optimality of arithmetic coding in theory and in practice. In Section 3 we present some of our current research into practical ways of improving the speed of arithmetic coding without sacrificing much compression efficiency. <p> It is possible to maintain higher precision, truncating (and adjusting to avoid overlapping subintervals) only when the expansion process is complete; this makes it possible to prove a tight analytical bound on the lost compression caused by the use of integer arithmetic, as we do in <ref> [22] </ref>, restated as Theorem 1 below. In practice this refinement makes the coding more difficult without improving compression. 2 Analysis. In [22] we prove a number of theorems about the code lengths of files coded with arithmetic coding. <p> expansion process is complete; this makes it possible to prove a tight analytical bound on the lost compression caused by the use of integer arithmetic, as we do in <ref> [22] </ref>, restated as Theorem 1 below. In practice this refinement makes the coding more difficult without improving compression. 2 Analysis. In [22] we prove a number of theorems about the code lengths of files coded with arithmetic coding. Most of the results involve the use of arithmetic coding in conjunction with various models of the input; these will be discussed in Section 2.3. <p> The adaptive approach has advantages in practice: there is no coding delay and no need to encode the model, since the decoder can maintain the same model as the encoder in a synchronized fashion. In the following theorem from <ref> [22] </ref> we compare context-free coding using a two-pass method and a one-pass adaptive method. <p> This is simple to implement, fast and effective in operation, and amenable to analysis. It also has the computationally desirable property of keeping the symbol weights small. In effect, scaling is a practical version of exponential aging. Analysis of scaling. In <ref> [22] </ref> we give a precise characterization of the effect of scaling on code length, in terms of an elegant notion we introduce called weighted entropy. <p> When scaling is done, we must ensure that no symbol's count becomes 0; an easy way to do this is to round fractional counts to the next higher integer. We show in the following theorem from <ref> [22] </ref> that this roundup effect is negligible. Theorem 5 Rounding counts up to the next higher integer increases the code length for the file by no more than n=2B bits per input symbol.
Reference: [23] <author> P. G. Howard & J. S. Vitter, </author> <title> "New Methods for Lossless Image Compression Using Arithmetic Coding," </title> <booktitle> Information Processing and Management 28 (1992), </booktitle> <pages> 765-779. </pages>
Reference-contexts: Compression is indeed improved, but at the cost of slowing down the algorithm and increasing its complexity. Lossless image compression is often performed using predictive coding, and it is often found that the prediction errors follow a Laplace distribution. In <ref> [23] </ref> we present methods that use tables of the Laplace distribution precomputed for arithmetic coding to obtain excellent compression ratios of grayscale images.
Reference: [24] <author> P. G. Howard & J. S. Vitter, </author> <title> "Design and Analysis of Fast Text Compression Based on Quasi-Arithmetic Coding," </title> <booktitle> in Proc. Data Compression Conference, </booktitle> <editor> J. A. Storer & M. Cohn, eds., </editor> <address> Snowbird, Utah, </address> <month> Mar. </month> <journal> 30-Apr. </journal> <volume> 1, </volume> <year> 1993, </year> <pages> 98-107. </pages>
Reference-contexts: Here we present a fast, reduced-precision binary arithmetic coder (which we refer to as quasi-arithmetic coding in a companion paper <ref> [24] </ref>) and develop it through a series of examples. It should be noted that the compression is still completely reversible; using reduced precision merely affects the average code length. <p> We have developed a fast coder, based on reduced-precision arithmetic coding, which gives only minimal loss of compression efficiency; we can use the concept of *-partitions to find the probabilities to include in the coder to keep the compression loss small. In a companion paper <ref> [24] </ref>, in which we refer to this fast coding method as quasi-arithmetic coding, we give implementation details and performance analysis for both binary and multi-symbol alphabets. We prove analytically that the loss in compression efficiency compared with exact arithmetic coding is negligible.
Reference: [25] <author> D. A. Huffman, </author> <title> "A Method for the Construction of Minimum Redundancy Codes," </title> <booktitle> Proceedings of the Institute of Radio Engineers 40 (1952), </booktitle> <pages> 1098-1101. </pages>
Reference-contexts: The other important advantage of arithmetic coding is its optimality. Arithmetic coding is optimal in theory and very nearly optimal in practice, in the sense of encoding using minimal average code length. This optimality is often less important than it might seem, since Huffman coding <ref> [25] </ref> is also very nearly optimal in most cases [8,9, 18,39]. When the probability of some single symbol is close to 1, however, arithmetic coding does give considerably better compression than other methods.
Reference: [26] <author> D. E. Knuth, </author> <title> "Dynamic Huffman Coding," </title> <journal> J. </journal> <note> Algorithms 6 (June 1985), 163-180. </note>
Reference-contexts: In practice there are several ways to do this: * Periodically restarting the model. This often discards too much information to be effective, although Cormack and Horspool find that it gives good results when growing large dynamic Markov models [11]. * Using a sliding window on the text <ref> [26] </ref>. This requires excessive computational resources. * Recency rank coding [7,13,53]. This is simple but corresponds to a rather coarse model of recency. * Exponential aging (giving exponentially increasing weights to successive symbols) [12,38].
Reference: [27] <author> G. G. Langdon, </author> <title> "Probabilistic and Q-Coder Algorithms for Binary Source Adaptation," </title> <booktitle> in Proc. Data Compression Conference, </booktitle> <editor> J. A. Storer & J. H. Reif, eds., </editor> <address> Snowbird, Utah, </address> <month> Apr. </month> <pages> 8-11, </pages> <year> 1991, </year> <pages> 13-22. </pages>
Reference: [28] <author> G. G. Langdon, </author> <title> "A Note on the Ziv-Lempel Model for Compressing Individual Sequences," </title> <journal> IEEE Trans. Inform. Theory IT-29 (Mar. </journal> <year> 1983), </year> <pages> 284-287. </pages>
Reference-contexts: High order models. The only way to obtain substantial improvements in compression is to use more sophisticated models. For text files, the increased sophistication invariably takes the form of conditioning the symbol probabilities on contexts consisting of one or more symbols of preceding text. (Langdon <ref> [28] </ref> and Bell, Witten, Cleary, and Moffat [3,4,5] have proven that both Ziv-Lempel coding and the dynamic Markov coding method of Cormack and Horspool [11] can be reduced to finite context models, despite superficial indications to the contrary.) One significant difficulty with using high-order models is that many contexts do not
Reference: [29] <author> G. G. Langdon & J. Rissanen, </author> <title> "Compression of Black-White Images with Arithmetic Coding," </title> <journal> IEEE Trans. Comm. COM-29 (1981), </journal> <pages> 858-867. </pages>
Reference-contexts: The coding of bilevel images, an important problem with a natural two-symbol alphabet, often produces probabilities close to 1, indicating the use of arithmetic coding to obtain good compression. Historically, much of the arithmetic coding research by Rissanen, Langdon, and others at IBM has focused on bilevel images <ref> [29] </ref>. The Q-Coder [2,27,33,41,42,43] is a binary arithmetic coder; work by Rissanen and Mohiuddin [50] and Chevion et al. [10] extends some of the Q-Coder ideas to multi-symbol alphabets. <p> 0 &lt; p &lt; 1 ff 00 [0; 4) - [1; 4) ff &lt; p &lt; 1 - [0; 3) 11 [0; 4) 1 = 2 p &lt; 1 0 [0; 4) 10 [0; 4) 1 = 2 p &lt; 1 1 [0; 4) 01 [0; 4) Langdon and Rissanen <ref> [29] </ref> suggest identifying the symbols as the more probable symbol (MPS) and less probable symbol (LPS) rather than as 1 and 0. By doing this we can often combine transitions and eliminate states. Example 6 : We modify Example 5 to use the MPS/LPS idea.
Reference: [30] <author> F. T. Leighton & R. L. Rivest, </author> <title> "Estimating a Probability Using Finite Memory," </title> <journal> IEEE Trans. Inform. Theory IT-32 (Nov. </journal> <year> 1986), </year> <pages> 733-742. </pages>
Reference-contexts: Each state indicates a probability, and some of the states also indicate the size of the sample used to estimate the probability. We need a method for estimating the probability at each node of the binary tree. Leighton and Rivest <ref> [30] </ref> and Pennebaker and Mitchell [41] describe probabilistic methods. Their estimators are also finite state automata, with each state corresponding to a probability. When a new symbol occurs, a transition to another state may occur, the probability of the transition depending on the current state and the new symbol. <p> When a new symbol occurs, a transition to another state may occur, the probability of the transition depending on the current state and the new symbol. Generally, the transition probability is higher when the LPS occurs. In <ref> [30] </ref> transitions occur only between adjacent states. In [41] the LPS always causes a transition, possibly to a non-adjacent state; a transition after the MPS, when one occurs, is always to an adjacent state. We give a deterministic estimator based on the same idea.
Reference: [31] <author> D. A. Lelewer & D. S. Hirschberg, </author> <title> "Streamlining Context Models for Data Compression," </title> <booktitle> in Proc. Data Compression Conference, </booktitle> <editor> J. A. Storer & J. H. Reif, eds., </editor> <address> Snowbird, Utah, </address> <month> Apr. </month> <pages> 8-11, </pages> <year> 1991, </year> <pages> 313-322. </pages>
Reference-contexts: In our experiments we use a third-order model; when a symbol has not occurred previously in its context of length 3, we simply use 8 bits to indicate the ASCII value of the symbol. (The idea of skipping some shorter contexts for speed, space, and simplicity appears also in <ref> [31] </ref>.) Even with this simplistic way of dropping to shorter contexts, the improved estimation of p esc gives slightly better overall compression than PPMC for book1, the longest file in the Bell-Cleary-Witten corpus. <p> Lelewer and Hirschberg <ref> [31] </ref> apply hashing with collision resolution in a similar high-order scheme. 4 Conclusion We have shown the details of an implementation of arithmetic coding and have pointed out its advantages (flexibility and near-optimality) and its main disadvantage (slowness).
Reference: [32] <author> V. S. Miller & M. N. Wegman, </author> <title> "Variations on a Theme by Ziv and Lempel," in Combinatorial Algorithms on Words, </title> <editor> A. Apostolico & Z. Galil, eds., </editor> <booktitle> NATO ASI Series #F12, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1984, </year> <pages> 131-140. </pages>
Reference: [33] <author> J. L. Mitchell & W. B. Pennebaker, </author> <title> "Optimal Hardware and Software Arithmetic Coding Procedures for the Q-Coder," </title> <journal> IBM J. Res. Develop. </journal> <volume> 32 (Nov. </volume> <year> 1988), </year> <pages> 727-736. </pages>
Reference: [34] <author> A. M. Moffat, </author> <title> "Predictive Text Compression Based upon the Future Rather than the Past," </title> <booktitle> Australian Computer Science Communications 9 (1987), </booktitle> <pages> 254-261. </pages>
Reference-contexts: In the two-pass method, the exact symbol counts are encoded after the first pass; during the second pass each symbol's count is decremented whenever it occurs, so at each point the relative counts reflect the correct symbol probabilities for the remainder of the file (as in <ref> [34] </ref>). In the one-pass adaptive method, all symbols are given initial counts of 1; we add 1 to a symbol's count whenever it occurs.
Reference: [35] <author> A. M. Moffat, </author> <title> "Word-Based Text Compression," </title> <journal> Software-Practice and Experience 19 (Feb. </journal> <year> 1989), </year> <pages> 185-198. </pages>
Reference-contexts: The new subinterval is [L + P C (H L); L + P N (H L)). The need to maintain and supply cumulative probabilities requires the model to have a complicated data structure; Moffat <ref> [35] </ref> investigates this problem, and concludes for a multi-symbol alphabet that binary search trees are about twice as fast as move-to front lists.
Reference: [36] <author> A. M. Moffat, </author> <title> "Linear Time Adaptive Arithmetic Coding," </title> <journal> IEEE Trans. Inform. Theory IT-36 (Mar. </journal> <year> 1990), </year> <pages> 401-406. 29 </pages>
Reference-contexts: In implementations [58] it is more convenient to subtract 1 from the right endpoints and use closed intervals. Moffat <ref> [36] </ref> considers the calculation of cumulative frequency counts for large alphabets.) Example 3 : Suppose that at a certain point in the encoding we have symbol counts c a = 4, c b = 5, and c EOF = 1 and current interval [25; 89) from the full interval [0; 128).
Reference: [37] <author> A. M. Moffat, </author> <title> "Implementing the PPM Data Compression Scheme," </title> <journal> IEEE Trans. Comm. </journal> <month> COM-38 (Nov. </month> <year> 1990), </year> <pages> 1917-1921. </pages>
Reference-contexts: Cleary and Witten specify two ad hoc methods, called PPMA and PPMB, for computing the probability of the escape symbol. Moffat <ref> [37] </ref> implements the algorithm and proposes a third method, PPMC, for computing the escape probability: he treats the escape event as a separate symbol; when a symbol occurs for the first time he adds 1 to both the escape count and the new symbol's count. <p> In this section we present yet another ad hoc method, which we call PPMD, and also a more complicated but more principled approach to the problem. PPMD. Moffat's PPMC method <ref> [37] </ref> is widely considered to be the best method of estimating escape probabilities. In PPMC, each symbol's weight in a context is taken to be number of times it has occurred so far in the context. <p> We expect that using indirect probability estimation in conjunction with the full multi-order PPM mechanism will yield substantially improved compression. 3.6 Hashed high-order Markov models. For finding contexts in the PPM method, Moffat <ref> [37] </ref> and Bell et al. [5] give complicated data structures called backward trees and vine pointers. For fast access and minimal memory usage we propose single hashing without collision resolution.
Reference: [38] <author> K. Mohiuddin, J. J. Rissanen & M. Wax, </author> <title> "Adaptive Model for Nonstationary Sources," </title> <journal> IBM Technical Disclosure Bulletin 28 (Apr. </journal> <year> 1986), </year> <pages> 4798-4800. </pages>
Reference: [39] <author> D. S. Parker, </author> <title> "Conditions for the Optimality of the Huffman Algorithm," </title> <journal> SIAM J. Comput. </journal> <month> 9 (Aug. </month> <year> 1980), </year> <pages> 470-489. </pages>
Reference: [40] <author> R. </author> <title> Pasco, "Source Coding Algorithms for Fast Data Compression," </title> <institution> Stanford Univ., </institution> <type> Ph.D. Thesis, </type> <year> 1976. </year>
Reference-contexts: This follow-on procedure may be repeated any number of times, so the current interval size is always longer than 1/4. Mechanisms for incremental transmission and fixed precision arithmetic have been developed through the years by Pasco <ref> [40] </ref>, Rissanen [48], Rubin [52], Rissanen and Langdon [49], Guazzo [19], and Witten, Neal, and Cleary [58]. The bit-stuffing idea of Langdon and others at IBM that limits the propagation of carries in the additions is roughly equivalent to the follow-on procedure described above.
Reference: [41] <author> W. B. Pennebaker & J. L. Mitchell, </author> <title> "Probability Estimation for the Q-Coder," </title> <journal> IBM J. Res. Develop. </journal> <volume> 32 (Nov. </volume> <year> 1988), </year> <pages> 737-752. </pages>
Reference-contexts: Each state indicates a probability, and some of the states also indicate the size of the sample used to estimate the probability. We need a method for estimating the probability at each node of the binary tree. Leighton and Rivest [30] and Pennebaker and Mitchell <ref> [41] </ref> describe probabilistic methods. Their estimators are also finite state automata, with each state corresponding to a probability. When a new symbol occurs, a transition to another state may occur, the probability of the transition depending on the current state and the new symbol. <p> When a new symbol occurs, a transition to another state may occur, the probability of the transition depending on the current state and the new symbol. Generally, the transition probability is higher when the LPS occurs. In [30] transitions occur only between adjacent states. In <ref> [41] </ref> the LPS always causes a transition, possibly to a non-adjacent state; a transition after the MPS, when one occurs, is always to an adjacent state. We give a deterministic estimator based on the same idea.
Reference: [42] <author> W. B. Pennebaker & J. L. Mitchell, </author> <title> "Software Implementations of the Q-Coder," </title> <journal> IBM J. Res. Develop. </journal> <volume> 32 (Nov. </volume> <year> 1988), </year> <pages> 753-774. </pages>
Reference: [43] <author> W. B. Pennebaker, J. L. Mitchell, G. G. Langdon & R. B. </author> <title> Arps, "An Overview of the Basic Principles of the Q-Coder Adaptive Binary Arithmetic Coder," </title> <journal> IBM J. Res. Develop. </journal> <volume> 32 (Nov. </volume> <year> 1988), </year> <pages> 717-726. </pages>
Reference: [44] <author> J. Rissanen, </author> <title> "Modeling by Shortest Data Description," </title> <booktitle> Automatica 14 (1978), </booktitle> <pages> 465-571. </pages>
Reference: [45] <author> J. Rissanen, </author> <title> "A Universal Prior for Integers and Estimation by Minimum Description Length," </title> <journal> Ann. Statist. </journal> <volume> 11 (1983), </volume> <pages> 416-432. </pages>
Reference: [46] <author> J. Rissanen, </author> <title> "Universal Coding, Information, Prediction, and Estimation," </title> <journal> IEEE Trans. Inform. Theory IT-30 (July 1984), </journal> <pages> 629-636. </pages>
Reference: [47] <author> J. Rissanen & G. G. Langdon, </author> <title> "Universal Modeling and Coding," </title> <journal> IEEE Trans. Inform. Theory IT-27 (Jan. </journal> <year> 1981), </year> <pages> 12-23. </pages>
Reference-contexts: Models used for arithmetic coding may be adaptive, and in fact a number of independent models may be used in succession in coding a single file. This great flexibility results from the sharp separation of the coder from the modeling process <ref> [47] </ref>. There is a cost associated with this flexibility: the interface between the model and the coder, while simple, places considerable time and space demands on the model's data structures, especially in the case of a multi-symbol input alphabet. The other important advantage of arithmetic coding is its optimality.
Reference: [48] <author> J. J. Rissanen, </author> <title> "Generalized Kraft Inequality and Arithmetic Coding," </title> <institution> IBM J. Res. Develop. </institution> <month> 20 (May </month> <year> 1976), </year> <pages> 198-203. </pages>
Reference-contexts: This follow-on procedure may be repeated any number of times, so the current interval size is always longer than 1/4. Mechanisms for incremental transmission and fixed precision arithmetic have been developed through the years by Pasco [40], Rissanen <ref> [48] </ref>, Rubin [52], Rissanen and Langdon [49], Guazzo [19], and Witten, Neal, and Cleary [58]. The bit-stuffing idea of Langdon and others at IBM that limits the propagation of carries in the additions is roughly equivalent to the follow-on procedure described above.
Reference: [49] <author> J. J. Rissanen & G. G. Langdon, </author> <title> "Arithmetic Coding," </title> <institution> IBM J. Res. Develop. </institution> <month> 23 (Mar. </month> <year> 1979), </year> <pages> 146-162. </pages>
Reference-contexts: This follow-on procedure may be repeated any number of times, so the current interval size is always longer than 1/4. Mechanisms for incremental transmission and fixed precision arithmetic have been developed through the years by Pasco [40], Rissanen [48], Rubin [52], Rissanen and Langdon <ref> [49] </ref>, Guazzo [19], and Witten, Neal, and Cleary [58]. The bit-stuffing idea of Langdon and others at IBM that limits the propagation of carries in the additions is roughly equivalent to the follow-on procedure described above. We now describe in detail how the coding and interval expansion work.
Reference: [50] <author> J. J. Rissanen & K. M. Mohiuddin, </author> <title> "A Multiplication-Free Multialphabet Arithmetic Code," </title> <journal> IEEE Trans. Comm. </journal> <month> 37 (Feb. </month> <year> 1989), </year> <pages> 93-98. </pages>
Reference-contexts: Historically, much of the arithmetic coding research by Rissanen, Langdon, and others at IBM has focused on bilevel images [29]. The Q-Coder [2,27,33,41,42,43] is a binary arithmetic coder; work by Rissanen and Mohiuddin <ref> [50] </ref> and Chevion et al. [10] extends some of the Q-Coder ideas to multi-symbol alphabets.
Reference: [51] <author> C. Rogers & C. D. Thomborson, </author> <title> "Enhancements to Ziv-Lempel Data Compression," </title> <institution> Dept. of Computer Science, Univ. of Minnesota, </institution> <type> Technical Report TR 89-2, </type> <institution> Duluth, Minnesota, </institution> <month> Jan. </month> <year> 1989. </year>
Reference: [52] <author> F. Rubin, </author> <title> "Arithmetic Stream Coding Using Fixed Precision Registers," </title> <journal> IEEE Trans. Inform. Theory IT-25 (Nov. </journal> <year> 1979), </year> <pages> 672-675. </pages>
Reference-contexts: This follow-on procedure may be repeated any number of times, so the current interval size is always longer than 1/4. Mechanisms for incremental transmission and fixed precision arithmetic have been developed through the years by Pasco [40], Rissanen [48], Rubin <ref> [52] </ref>, Rissanen and Langdon [49], Guazzo [19], and Witten, Neal, and Cleary [58]. The bit-stuffing idea of Langdon and others at IBM that limits the propagation of carries in the additions is roughly equivalent to the follow-on procedure described above.
Reference: [53] <author> B. Y. Ryabko, </author> <title> "Data Compression by Means of a Book Stack," </title> <note> Problemy Peredachi Informatsii 16 (1980). </note>
Reference: [54] <author> C. E. Shannon, </author> <title> "A Mathematical Theory of Communication," </title> <institution> Bell Syst. Tech. J. </institution> <month> 27 (July </month> <year> 1948), </year> <pages> 398-403. </pages> <address> 30 4 CONCLUSION </address>
Reference-contexts: Additional support was provided by a Universities Space Research Association/CESDIS associate membership. 1 1 Data Compression and Arithmetic Coding Data can be compressed whenever some data symbols are more likely than others. Shannon <ref> [54] </ref> showed that for the best possible compression code (in the sense of minimum average code length), the output length contains a contribution of lg p bits from the encoding of each symbol whose probability of occurrence is p. <p> In practice we have to output 7 bits. 2 The idea of arithmetic coding originated with Shannon in his seminal 1948 paper on information theory <ref> [54] </ref>. It was rediscovered by Elias about 15 years later, as briefly mentioned in [1]. Implementation details.
Reference: [55] <author> J. S. Vitter, </author> <title> "Dynamic Huffman Coding," </title> <journal> ACM Trans. Math. </journal> <note> Software 15 (June 1989), 158-167, also appears as Algorithm 673, Collected Algorithms of ACM, </note> <year> 1989. </year>
Reference: [56] <author> J. S. Vitter, </author> <title> "Design and Analysis of Dynamic Huffman Codes," </title> <journal> Journal of the ACM 34 (Oct. </journal> <year> 1987), </year> <pages> 825-845. </pages>
Reference: [57] <author> I. H. Witten & T. C. Bell, </author> <title> "The Zero Frequency Problem: Estimating the Probabilities of Novel Events in Adaptive Text Compression," </title> <journal> IEEE Trans. Inform. Theory IT-37 (July 1991), </journal> <pages> 1085-1094. </pages>
Reference-contexts: In practice, PPMC compresses better than PPMA and PPMB. PPMP and PPMX appear in <ref> [57] </ref>; they are based on the assumption that the appearance of symbols for the first time in a 12 3 FAST ARITHMETIC CODING file is approximately a Poisson process.
Reference: [58] <author> I. H. Witten, R. M. Neal & J. G. Cleary, </author> <title> "Arithmetic Coding for Data Compression," </title> <journal> Comm. </journal> <note> ACM 30 (June 1987), 520-540. </note>
Reference-contexts: The center of this research is a reduced-precision arithmetic coder, supported by efficient data structures for text modeling. 2 Tutorial on Arithmetic Coding In this section we explain how arithmetic coding works and give implementation details; our treatment is based on that of Witten, Neal, and Cleary <ref> [58] </ref>. We point out the usefulness of binary arithmetic coding (that is, coding with a 2-symbol alphabet), and discuss the modeling issue, particularly high-order Markov modeling for text compression. Our focus is on encoding, but the decoding process is similar. 2.1 Arithmetic coding and its implementation Basic algorithm. <p> The most straightforward solution to both of these problems is to output each leading bit as soon as it is known, and then to double the length of the current interval so that it reflects only the unknown part of the final interval. Witten, Neal, and Cleary <ref> [58] </ref> add a clever mechanism for preventing the current interval from shrinking too much when the endpoints are close to 1=2 but straddle 1=2. <p> Mechanisms for incremental transmission and fixed precision arithmetic have been developed through the years by Pasco [40], Rissanen [48], Rubin [52], Rissanen and Langdon [49], Guazzo [19], and Witten, Neal, and Cleary <ref> [58] </ref>. The bit-stuffing idea of Langdon and others at IBM that limits the propagation of carries in the additions is roughly equivalent to the follow-on procedure described above. We now describe in detail how the coding and interval expansion work. <p> In implementations <ref> [58] </ref> it is more convenient to subtract 1 from the right endpoints and use closed intervals. <p> The simplest adaptive models do not rely on contexts for conditioning probabilities; a symbol's probability is just its relative frequency in the part of the file already coded. (We need a mechanism for encoding a symbol for the first time, when its frequency is 0; the easiest way <ref> [58] </ref> is to start all symbol counts at 1 instead of 0.) The average code length per input symbol of a file encoded using such a 0-order adaptive model is very close to the 0-order entropy of the file. <p> This is moderately difficult to implement because of the changing weight increments, although our probability estimation method in Section 3.4 uses an approximate form of this technique. * Periodic scaling <ref> [58] </ref>. This is simple to implement, fast and effective in operation, and amenable to analysis. It also has the computationally desirable property of keeping the symbol weights small. In effect, scaling is a practical version of exponential aging. Analysis of scaling. <p> The distributions are chosen to guarantee that, for a given variance estimate, the resulting code length exceeds the ideal for the estimate by only a small fixed amount. Especially when encoding model parameters, it is often necessary to encode arbitrarily large non-negative integers. Witten et al. <ref> [58] </ref> note that arithmetic coding can encode integers according to any given distribution. In the examples in Section 3.1 we show how some encodings of integers found in the literature can be derived as low-precision arithmetic codes. <p> We make a more pragmatic decision. We know that periodic scaling is an approximation to exponential aging and we can show that a scaling factor of f corresponds to a scaling block size B of approximately f ln 2=(1 f ). Since B = 16 works well for scaling <ref> [58] </ref>, we choose f = 0:96. 3.5 Improved modeling for text compression To obtain good, fast text compression, we wish to use the multi-symbol extension of the reduced-precision arithmetic coder in conjunction with a good model.
Reference: [59] <author> J. Ziv & A. Lempel, </author> <title> "A Universal Algorithm for Sequential Data Compression," </title> <journal> IEEE Trans. Inform. Theory IT-23 (May 1977), </journal> <pages> 337-343. </pages>
Reference-contexts: In addition, the model lookup and update operations are slow because of the input requirements of the coder. Both Huffman coding and Ziv-Lempel <ref> [59, 60] </ref> coding are faster because the model is represented directly in the data structures 2 2 TUTORIAL ON ARITHMETIC CODING used for coding. (This reduces the coding efficiency of those methods by narrowing the range of possible models.) Much of the current research in arithmetic coding concerns finding approximations that
Reference: [60] <author> J. Ziv & A. Lempel, </author> <title> "Compression of Individual Sequences via Variable Rate Coding," </title> <journal> IEEE Trans. Inform. </journal> <note> Theory IT-24 (Sept. </note> <year> 1978), </year> <pages> 530-536. </pages>
Reference-contexts: In addition, the model lookup and update operations are slow because of the input requirements of the coder. Both Huffman coding and Ziv-Lempel <ref> [59, 60] </ref> coding are faster because the model is represented directly in the data structures 2 2 TUTORIAL ON ARITHMETIC CODING used for coding. (This reduces the coding efficiency of those methods by narrowing the range of possible models.) Much of the current research in arithmetic coding concerns finding approximations that
References-found: 60

