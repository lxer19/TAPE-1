URL: http://www.cs.utexas.edu/users/plaxton/ps/1998/tocs.ps
Refering-URL: http://www.cs.utexas.edu/users/plaxton/html/abc.html
Root-URL: 
Title: An Experimental Analysis of Parallel Sorting Algorithms  
Author: Guy E. Blelloch C. Greg Plaxton Charles E. Leiserson Stephen J. Smith Bruce M. Maggs Marco Zagha 
Address: Pittsburgh, PA 15213  Austin, TX 78712  Cambridge, MA 02139  Cambridge, MA 02141  Pittsburgh, PA 15213  Mountain View, CA 94043  
Affiliation: Carnegie Mellon University  University of Texas  MIT  Pilot Software  Carnegie Mellon University  Silicon Graphics  
Abstract: We have developed a methodology for predicting the performance of parallel algorithms on real parallel machines. The methodology consists of two steps. First, we characterize a machine by enumerating the primitive operations that it is capable of performing along with the cost of each operation. Next, we analyze an algorithm by making a precise count of the number of times the algorithm performs each type of operation. We have used this methodology to evaluate many of the parallel sorting algorithms proposed in the literature. Of these, we selected the three most promising, Batcher's bitonic sort, a parallel radix sort, and a sample sort similar to Reif and Valiant's flashsort, and implemented them on the Connection Machine model CM-2. This paper analyzes the three algorithms in detail and discusses the issues that led us to our particular implementations. On the CM-2 the predicted performance of the algorithms closely matches the observed performance, and hence our methodology can be used to tune the algorithms for optimal performance. Although our programs were designed for the CM-2, our conclusions about the merits of the three algorithms apply to other parallel machines as well. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Ajtai, J. Komlos, and E. Szemeredi. </author> <title> Sorting in c log n parallel steps. </title> <journal> Combinatorica, </journal> <volume> 3 </volume> <pages> 1-19, </pages> <year> 1983. </year>
Reference-contexts: The question of the existence of a o (lg 2 n)-depth sorting network remained open until 1983, when Ajtai, Komlos, and Szemeredi <ref> [1] </ref> provided an optimal fi (lg n)-depth sorting network, but unfortunately, their construction leads to larger networks than those given by bitonic sort for all "practical" values of n. <p> The fastest comparison-based sort is Cole's parallel merge sort [6]. This algorithm requires optimal O (lg n) time to sort n items on an n-node exclusive-read exclusive-write (EREW) PRAM. Another way to sort in O (lg n) time is to emulate the AKS sorting circuit <ref> [1] </ref>. In this case, however, the constants hidden by the O-notation are large. If one is interested in emulating a PRAM algorithm on a fixed-connection network such as the hypercube or butterfly, the cost of the emulation must be taken into account. Most emulation schemes are based on routing.
Reference: [2] <author> S. G. Akl. </author> <title> Parallel Sorting Algorithms. </title> <publisher> Academic Press, </publisher> <address> Toronto, </address> <year> 1985. </year>
Reference-contexts: We shall not prove the correctness of this well-known algorithm; the interested reader is referred to <ref> [2, 7] </ref>. To this point, we have assumed that the number n of input keys is equal to the number p of processors. In practice, it is important for a sorting algorithm to be able to cope with unequal values of n and p.
Reference: [3] <author> K. Batcher. </author> <title> Sorting networks and their applications. </title> <booktitle> In Proceedings of the AFIPS Spring Joint Computing Conference, </booktitle> <volume> volume 32, </volume> <pages> pages 307-314, </pages> <year> 1968. </year>
Reference-contexts: Parallel algorithms for sorting have been studied since at least the 1960's. An early advance in parallel sorting came in 1968 when Batcher discovered the elegant fi (lg 2 n)-depth bitonic sorting network <ref> [3] </ref>. (Throughout this paper lg n denotes log 2 n.) For certain families of fixed interconnection networks, such as the hypercube and shu*e-exchange, Batcher's bitonic fl This research was supported in part by Thinking Machines Corporation, and in part by the Defense Advanced Research Projects Agency under Contracts N00014-87-0825 and F33615-90-C-1465. <p> In the case of multiple independent scans (each on one element per processor), however, the fixed overhead S must be taken into consideration. The other operations (Cube Swap and Send) have fixed overheads as well, but they are negligible by comparison. 3 Batcher's Bitonic Sort Batcher's bitonic sort <ref> [3] </ref> is a parallel merge sort that is based upon an efficient technique for merging so-called "bitonic" sequences. A bitonic sequence is one that increases monotonically and then decreases monotonically, or can be circularly shifted to become so.
Reference: [4] <author> G. Baudet and D. Stevenson. </author> <title> Optimal sorting algorithms for parallel computers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-27:84-87, </volume> <year> 1978. </year>
Reference-contexts: Multiport bitonic sort can be further improved by using a linear-time serial merge instead of a bitonic merge in order to execute the merges that occur entirely within a processor <ref> [4] </ref>. We estimated that the time for a processor to merge two sorted sequences of length (n=2p) to form a single sorted sequence of length (n=p) is approximately (n=p) 10A. The constant is large because of the indirect addressing that would be required by the implementation.
Reference: [5] <author> G. E. Blelloch. </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: In 1983, Reif and Valiant proposed a more practical O (lg n)-time randomized algorithm for sorting [19], called flashsort. Many other parallel sorting algorithms have been proposed in the literature, including parallel versions of radix sort and quicksort <ref> [5] </ref>, a variant of quicksort called hyperquicksort [23], smoothsort [18], column sort [15], Nassimi and Sahni's sort [17], and parallel merge sort [6]. This paper reports the findings of a project undertaken at Thinking Machines Corporation to develop a fast sorting algorithm for the Connection Machine Supercomputer model CM-2. <p> The most interesting part of radix sort is the subroutine for computing ranks called in line 2. We first consider the simple algorithm underlying the original Connection Machine library sort <ref> [5] </ref>, which was programmed by one of us several years ago. In the following implementation of Counting-Rank, the vector Block holds the r-bit values on which we are sorting.
Reference: [6] <author> R. Cole. </author> <title> Parallel merge sort. </title> <journal> SIAM Journal on Computing, </journal> <pages> pages 770-785, </pages> <year> 1988. </year>
Reference-contexts: Many other parallel sorting algorithms have been proposed in the literature, including parallel versions of radix sort and quicksort [5], a variant of quicksort called hyperquicksort [23], smoothsort [18], column sort [15], Nassimi and Sahni's sort [17], and parallel merge sort <ref> [6] </ref>. This paper reports the findings of a project undertaken at Thinking Machines Corporation to develop a fast sorting algorithm for the Connection Machine Supercomputer model CM-2. <p> In certain instances, a significant additional penalty must be paid in order to "port" the algorithm to the particular architecture provided by the CM-2. Many algorithms have been developed for sorting on Parallel Random Access Machines (PRAMs). The fastest comparison-based sort is Cole's parallel merge sort <ref> [6] </ref>. This algorithm requires optimal O (lg n) time to sort n items on an n-node exclusive-read exclusive-write (EREW) PRAM. Another way to sort in O (lg n) time is to emulate the AKS sorting circuit [1]. In this case, however, the constants hidden by the O-notation are large.
Reference: [7] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press and McGraw-Hill, </publisher> <year> 1990. </year>
Reference-contexts: We shall not prove the correctness of this well-known algorithm; the interested reader is referred to <ref> [2, 7] </ref>. To this point, we have assumed that the number n of input keys is equal to the number p of processors. In practice, it is important for a sorting algorithm to be able to cope with unequal values of n and p. <p> the local merges could not be executed in place, so that the algorithm would lose one of its major advantages: it would no longer only require a fixed amount of additional memory. 4 Radix Sort The second algorithm that we implemented is a parallel version of a counting-based radix sort <ref> [7, Section 9.3] </ref>. In contrast with bitonic sort, radix sort is not a comparison sort: it does not use comparisons alone to determine the relative ordering of keys. Instead, it relies on the representation of keys as b-bit integers. (Floating-point numbers can also be sorted using radix sort. <p> If the expected bucket expansion is fi (s; n), the expected size of the largest bucket is (n=p)fi (s; n). We use a standard serial radix sort in which each pass is implemented using several passes of a counting sort (see, for example, <ref> [7, Section 9.3] </ref>). Radix sort was used because it is significantly faster than comparison sorts such as quicksort.
Reference: [8] <author> R. E. Cypher and C. G. Plaxton. </author> <title> Deterministic sorting in nearly logarithmic time on the hypercube and related computers. </title> <booktitle> In Proceedings of the 22nd Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 193-203, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: For the hypercube and related networks such as the butterfly, cube-connected cycles, and shu*e-exchange, there have been recent asymptotic improvements in both the deterministic and randomized settings. A deterministic, O (lg n (lg lg n) 2 )-time algorithm for the case n = p is described in <ref> [8] </ref>. An O (lg n)-time algorithm that admits an efficient bit-serial implementation and also improves upon the asymptotic failure probability of the Reif-Valiant flashsort algorithm is presented in [16]. Unfortunately, both of these algorithms are quite impractical.
Reference: [9] <author> R. E. Cypher and J. L. C. Sanz. Cubesort: </author> <title> A parallel algorithm for sorting N data items with S-sorters. </title> <journal> Journal of Algorithms, </journal> <volume> 13 </volume> <pages> 211-234, </pages> <year> 1992. </year>
Reference-contexts: Column sort might well be a useful component of a hybrid sorting scheme that automatically selects an appropriate algorithm depending upon the values of n and p. Cubesort. Like column sort, the cubesort algorithm of Cypher and Sanz <ref> [9] </ref> gives a scheme for sorting n items in a number of "rounds", where in each round the data is partitioned into 27 n=s sets of size s (for some s, 2 s n), and each set is sorted. (Successive partitions of the data are determined by simple fixed permutations that
Reference: [10] <author> W. D. Frazer and A. C. McKellar. Samplesort: </author> <title> A sampling approach to minimal storage tree sorting. </title> <journal> Journal of the ACM, </journal> <volume> 17(3) </volume> <pages> 496-507, </pages> <year> 1970. </year>
Reference-contexts: As r is increased, the scan time increases and the send time decreases. (The arithmetic time is negligible.) For the parameters chosen, the optimal value of r is 11. 5 Sample Sort The third sort that we implemented is a sample sort <ref> [10, 13, 19, 20, 24] </ref>. This sorting algorithm was the fastest for large sets of input keys, beating radix sort by more than a factor of 2. It also was the most complicated to implement. The sort is a randomized sort: it uses a random number generator. <p> Predicted performance was calculated using equation (8). called the oversampling ratio. This sample is sorted, and then the p 1 splitters are selected by taking those keys in the sample that have ranks s; 2s; 3s; : : :; (p 1)s. Some sample sort algorithms <ref> [10, 20, 24] </ref> choose an oversampling ratio of s = 1, but this choice results in a relatively large deviation in the bucket sizes.
Reference: [11] <author> T. Hagerup and C. Rub. </author> <title> A guided tour of Chernoff bounds. </title> <journal> Information Processing Letters, </journal> <volume> 33(6) </volume> <pages> 305-308, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: equal to 1 with probability q and to 0 with probability 1 q. (Note that E [W ] = E [Z] = qn.) If k qn 1 is an integer, then Pr [W k] Pr [Z k] : Our second lemma is a "Chernoff" bound due to Angluin and Valiant <ref> [11] </ref>. Lemma B.2 Consider a sequence of r Bernoulli trials, where success occurs in each trial with probability q. Let Y be the random variable denoting the total number of successes.
Reference: [12] <author> William L. Hightower, Jan F. Prins, and John. H. Reif. </author> <title> Implementations of randomized sorting on large parallel machines. </title> <booktitle> In Proceedings of the Symposium on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: In both cases, when appropriate values for the machine parameters are used, our equations accurately predicted the running times. Similar equations are used by Stricker [21] to analyze the running time of bitonic sort on the iWarp, and by Hightower, Prins and Reif <ref> [12] </ref> to analyze the running time of flashsort on the Maspar MP-1. The remainder of this paper studies the implementations of bitonic sort, radix sort and sample sort. <p> Lacking that on the CM-2, the scheme that we chose to implement was faster and much simpler to code. Since our original work, Hightower, Prins and Reif have implemented a version of splitter-directed routing <ref> [12] </ref> on a toroidal mesh and found that when the number of keys per processor is not large, splitter directed routing can outperform samplesort on the Maspar MP-1. Smaller sets of keys.
Reference: [13] <author> J. S. Huang and Y. C. Chow. </author> <title> Parallel sorting and data partitioning by sampling. </title> <booktitle> In Proceedings of the IEEE Computer Society's Seventh International Computer Software and Applications Conference, </booktitle> <pages> pages 627-631, </pages> <month> November </month> <year> 1983. </year>
Reference-contexts: As r is increased, the scan time increases and the send time decreases. (The arithmetic time is negligible.) For the parameters chosen, the optimal value of r is 11. 5 Sample Sort The third sort that we implemented is a sample sort <ref> [10, 13, 19, 20, 24] </ref>. This sorting algorithm was the fastest for large sets of input keys, beating radix sort by more than a factor of 2. It also was the most complicated to implement. The sort is a randomized sort: it uses a random number generator. <p> Some sample sort algorithms [10, 20, 24] choose an oversampling ratio of s = 1, but this choice results in a relatively large deviation in the bucket sizes. By choosing a larger value, as suggested by Reif and Valiant [19] and by Huang and Chow <ref> [13] </ref>, we can guarantee with high probability that no bucket contains many more keys than the average. (The Reif-Valiant flashsort algorithm differs in that it uses buckets corresponding to O (lg 7 p)-processor subcubes of the hypercube.) The time for Phase 3 of the algorithm depends on the maximum number, call <p> This selection process differs from that where each processor selects s tagged keys randomly from the entire set, as is done in both the Reif-Valiant [19] and Huang-Chow <ref> [13] </ref> algorithms. All of these methods yield small bucket expansions. Since the CM-2 is a distributed-memory machine, however, the local-choice method has an advantage in performance over global-choice methods: no global communication is required to select the candidates.
Reference: [14] <author> Lennart Johnsson. </author> <title> Combining parallel and sequential sorting on a boolean n-cube. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 444-448, </pages> <year> 1984. </year> <month> 32 </month>
Reference-contexts: We now consider an improved version of parallel radix sort. The idea behind this algorithm was used by Johnsson <ref> [14] </ref>. We shall describe the new algorithm for counting ranks in terms of the physical processors, rather than in terms of the keys themselves. Thus, we view the length-n input vector Block as a length-p vector, each element of which is a length-(n=p) array stored in a single processor.
Reference: [15] <author> F. T. Leighton. </author> <title> Tight bounds on the complexity of parallel sorting. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34(4):344-354, </volume> <month> April </month> <year> 1985. </year>
Reference-contexts: Leighton <ref> [15] </ref> has shown that any fi (lg n)-depth family of sorting networks can be used to sort n numbers in fi (lg n) time in the n-node bounded-degree fixed-connection network domain. Not surprisingly, the optimal fi (lg n)-time n-node fixed-connection sorting networks implied by the AKS construction are also impractical. <p> Many other parallel sorting algorithms have been proposed in the literature, including parallel versions of radix sort and quicksort [5], a variant of quicksort called hyperquicksort [23], smoothsort [18], column sort <ref> [15] </ref>, Nassimi and Sahni's sort [17], and parallel merge sort [6]. This paper reports the findings of a project undertaken at Thinking Machines Corporation to develop a fast sorting algorithm for the Connection Machine Supercomputer model CM-2. <p> It is unclear, however, that one would need a parallel computer to solve such small problems and one might get better times by solving the problem on a single processor, or by reducing p. Column sort. Leighton's column sort <ref> [15] </ref> is an elegant parallel sorting technique that has found many theoretical applications. Column sort sorts n keys using two primitive operations. The first primitive operation is to sort n 1=3 separate sets (called columns) of n 2=3 keys each.
Reference: [16] <author> T. Leighton and G. Plaxton. </author> <title> A (fairly) simple circuit that (usually) sorts. </title> <booktitle> In Proceedings of the 31st Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 264-274, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: A deterministic, O (lg n (lg lg n) 2 )-time algorithm for the case n = p is described in [8]. An O (lg n)-time algorithm that admits an efficient bit-serial implementation and also improves upon the asymptotic failure probability of the Reif-Valiant flashsort algorithm is presented in <ref> [16] </ref>. Unfortunately, both of these algorithms are quite impractical. The reader interested in theoretical bounds should consult the aforementioned papers for further references to previous work. B Probabilistic Analysis of Sample Sort This appendix analyzes the sizes of the buckets created by the sample sort algorithm from Section 5.
Reference: [17] <author> D. Nassimi and S. Sahni. </author> <title> Parallel permutation and sorting algorithms and a new generalized connection network. </title> <journal> Journal of the ACM, </journal> <volume> 29(3) </volume> <pages> 642-667, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: Many other parallel sorting algorithms have been proposed in the literature, including parallel versions of radix sort and quicksort [5], a variant of quicksort called hyperquicksort [23], smoothsort [18], column sort [15], Nassimi and Sahni's sort <ref> [17] </ref>, and parallel merge sort [6]. This paper reports the findings of a project undertaken at Thinking Machines Corporation to develop a fast sorting algorithm for the Connection Machine Supercomputer model CM-2. <p> Although hyperquicksort may perform less arithmetic than bitonic sort in the best case, it uses indirect addressing, which is relatively expensive on the CM-2. Sparse enumeration sort. The Nassimi-Sahni sorting algorithm <ref> [17] </ref>, which will be referred to as sparse enumeration sort, is used when the number n of items to be sorted is smaller than the number p of processors. In the special case n = p p, sparse enumeration sort is a very simple algorithm indeed.
Reference: [18] <author> C. G. Plaxton. </author> <title> Efficient computation on sparse interconnection networks. </title> <type> Technical Report STAN-CS-89-1283, </type> <institution> Stanford University, Department of Computer Science, </institution> <month> Septem-ber </month> <year> 1989. </year>
Reference-contexts: In 1983, Reif and Valiant proposed a more practical O (lg n)-time randomized algorithm for sorting [19], called flashsort. Many other parallel sorting algorithms have been proposed in the literature, including parallel versions of radix sort and quicksort [5], a variant of quicksort called hyperquicksort [23], smoothsort <ref> [18] </ref>, column sort [15], Nassimi and Sahni's sort [17], and parallel merge sort [6]. This paper reports the findings of a project undertaken at Thinking Machines Corporation to develop a fast sorting algorithm for the Connection Machine Supercomputer model CM-2. <p> Nonadaptive smoothsort. There are several variants of the smoothsort algorithm, all of which are described in <ref> [18] </ref>. The most practical variant, and the one of interest to us here, is the nonadaptive version of the smoothsort algorithm. The structure of this algorithm, hereinafter referred to simply as "smoothsort," is similar to that of column sort. <p> Smoothsort outperforms column sort for smaller values of n=p, however. For a detailed analysis of the running time of smoothsort, the reader is referred to <ref> [18] </ref>. Theoretical results. This subsection summarizes several "theoretical" sorting results| algorithms with optimal or near-optimal asymptotic performance but which remain impractical due to large constant factors and/or nonconstant costs that are not accounted for by the model of computation.
Reference: [19] <author> J. H. Reif and L. G. Valiant. </author> <title> A logarithmic time sort for linear size networks. </title> <journal> Journal of the ACM, </journal> <volume> 34(1) </volume> <pages> 60-76, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: Not surprisingly, the optimal fi (lg n)-time n-node fixed-connection sorting networks implied by the AKS construction are also impractical. In 1983, Reif and Valiant proposed a more practical O (lg n)-time randomized algorithm for sorting <ref> [19] </ref>, called flashsort. Many other parallel sorting algorithms have been proposed in the literature, including parallel versions of radix sort and quicksort [5], a variant of quicksort called hyperquicksort [23], smoothsort [18], column sort [15], Nassimi and Sahni's sort [17], and parallel merge sort [6]. <p> As r is increased, the scan time increases and the send time decreases. (The arithmetic time is negligible.) For the parameters chosen, the optimal value of r is 11. 5 Sample Sort The third sort that we implemented is a sample sort <ref> [10, 13, 19, 20, 24] </ref>. This sorting algorithm was the fastest for large sets of input keys, beating radix sort by more than a factor of 2. It also was the most complicated to implement. The sort is a randomized sort: it uses a random number generator. <p> Some sample sort algorithms [10, 20, 24] choose an oversampling ratio of s = 1, but this choice results in a relatively large deviation in the bucket sizes. By choosing a larger value, as suggested by Reif and Valiant <ref> [19] </ref> and by Huang and Chow [13], we can guarantee with high probability that no bucket contains many more keys than the average. (The Reif-Valiant flashsort algorithm differs in that it uses buckets corresponding to O (lg 7 p)-processor subcubes of the hypercube.) The time for Phase 3 of the algorithm <p> This selection process differs from that where each processor selects s tagged keys randomly from the entire set, as is done in both the Reif-Valiant <ref> [19] </ref> and Huang-Chow [13] algorithms. All of these methods yield small bucket expansions. Since the CM-2 is a distributed-memory machine, however, the local-choice method has an advantage in performance over global-choice methods: no global communication is required to select the candidates.
Reference: [20] <author> S. R. Seidel and W. L. George. </author> <title> Binsorting on hypercubes with d-port communication. </title> <booktitle> In Proceedings of the Third Conference on Hypercube Concurrent Computers, </booktitle> <pages> pages 1455-1461, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: As r is increased, the scan time increases and the send time decreases. (The arithmetic time is negligible.) For the parameters chosen, the optimal value of r is 11. 5 Sample Sort The third sort that we implemented is a sample sort <ref> [10, 13, 19, 20, 24] </ref>. This sorting algorithm was the fastest for large sets of input keys, beating radix sort by more than a factor of 2. It also was the most complicated to implement. The sort is a randomized sort: it uses a random number generator. <p> Predicted performance was calculated using equation (8). called the oversampling ratio. This sample is sorted, and then the p 1 splitters are selected by taking those keys in the sample that have ranks s; 2s; 3s; : : :; (p 1)s. Some sample sort algorithms <ref> [10, 20, 24] </ref> choose an oversampling ratio of s = 1, but this choice results in a relatively large deviation in the bucket sizes.
Reference: [21] <author> Thomas M. Stricker. </author> <title> Supporting the hypercube programming model on mesh architectures (a fast sorter for iWarp tori). </title> <booktitle> In Proceedings of the Symposium on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: In both cases, when appropriate values for the machine parameters are used, our equations accurately predicted the running times. Similar equations are used by Stricker <ref> [21] </ref> to analyze the running time of bitonic sort on the iWarp, and by Hightower, Prins and Reif [12] to analyze the running time of flashsort on the Maspar MP-1. The remainder of this paper studies the implementations of bitonic sort, radix sort and sample sort.
Reference: [22] <author> K. Thearling and S. Smith. </author> <title> An improved supercomputer sorting benchmark. </title> <booktitle> In Proceedings Supercomputing '92, </booktitle> <pages> pages 14-19, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Finally, the equations allow anyone to make reasonable estimates of the running times of the algorithms on other machines. For example, the radix sort has been implemented and analyzed on the Cray Y-MP [25], and the CM-5 <ref> [22] </ref>, which differ significantly from the CM-2. In both cases, when appropriate values for the machine parameters are used, our equations accurately predicted the running times.
Reference: [23] <author> B. A. Wagar. Hyperquicksort: </author> <title> A fast sorting algorithm for hypercubes. </title> <editor> In M. T. Heath, editor, </editor> <booktitle> Hypercube Multiprocessors 1987 (Proceedings of the Second Conference on Hypercube Multiprocessors), </booktitle> <pages> pages 292-299, </pages> <address> Philadelphia, PA, </address> <year> 1987. </year> <note> SIAM. </note>
Reference-contexts: In 1983, Reif and Valiant proposed a more practical O (lg n)-time randomized algorithm for sorting [19], called flashsort. Many other parallel sorting algorithms have been proposed in the literature, including parallel versions of radix sort and quicksort [5], a variant of quicksort called hyperquicksort <ref> [23] </ref>, smoothsort [18], column sort [15], Nassimi and Sahni's sort [17], and parallel merge sort [6]. This paper reports the findings of a project undertaken at Thinking Machines Corporation to develop a fast sorting algorithm for the Connection Machine Supercomputer model CM-2. <p> This algorithm has been implemented in a high level language (*Lisp) and runs about 2 times slower than the original system sort. We believed that we could not speed it up significantly, since the scan and route operations are already performed in hardware. 25 Hyperquicksort. The hyperquicksort algorithm <ref> [23] </ref> can be outlined as follows. First, each hypercube node sorts its n=p keys locally. Then, one of the hypercube nodes broadcasts its median key, m, to all of the other nodes. This key is used as a pivot.
Reference: [24] <author> Y. Won and S. Sahni. </author> <title> A balanced bin sort for hypercube multicomputers. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 435-448, </pages> <year> 1988. </year>
Reference-contexts: As r is increased, the scan time increases and the send time decreases. (The arithmetic time is negligible.) For the parameters chosen, the optimal value of r is 11. 5 Sample Sort The third sort that we implemented is a sample sort <ref> [10, 13, 19, 20, 24] </ref>. This sorting algorithm was the fastest for large sets of input keys, beating radix sort by more than a factor of 2. It also was the most complicated to implement. The sort is a randomized sort: it uses a random number generator. <p> Predicted performance was calculated using equation (8). called the oversampling ratio. This sample is sorted, and then the p 1 splitters are selected by taking those keys in the sample that have ranks s; 2s; 3s; : : :; (p 1)s. Some sample sort algorithms <ref> [10, 20, 24] </ref> choose an oversampling ratio of s = 1, but this choice results in a relatively large deviation in the bucket sizes.
Reference: [25] <author> Marco Zagha and Guy E. Blelloch. </author> <title> Radix sort for vector multiprocessors. </title> <booktitle> In Proceedings Supercomputing '91, </booktitle> <pages> pages 712-721, </pages> <month> November </month> <year> 1991. </year> <month> 33 </month>
Reference-contexts: Finally, the equations allow anyone to make reasonable estimates of the running times of the algorithms on other machines. For example, the radix sort has been implemented and analyzed on the Cray Y-MP <ref> [25] </ref>, and the CM-5 [22], which differ significantly from the CM-2. In both cases, when appropriate values for the machine parameters are used, our equations accurately predicted the running times.
References-found: 25

