URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/jmorrow/www/pubs/thesis.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/jmorrow/www/home.html
Root-URL: 
Title: Sensorimotor Primitives for Programming Robotic Assembly Skills  
Author: James Daniel Morrow 
Degree: Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in  
Note: Copyright 1997 by James Daniel Morrow. All rights reserved.  
Date: May 1997  
Address: Pittsburgh, Pennsylvania 15213-3890  
Affiliation: Robotics The Robotics Institute Carnegie Mellon University  
Abstract: This Research was supported in part by the Department of Energy Computational Science Fellowship Program, by the Department of Energy Integrated Manufacturing Predoctoral Fellowship, by Sandia National Laboratories, and by The Robotics Institute at Carnegie Mellon University. The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either expressed or implied, of the funding agencies. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Ahlbehrendt, H.-J. Horch, and A. Tentschew, </author> <title> Techniques of Teaching Skills to Sensor Guided Robots, </title> <booktitle> IFAC Skill Based Automated Production, </booktitle> <pages> pp. 203-208, </pages> <address> Vienna, Austria, </address> <year> 1989. </year>
Reference: [2] <author> D.S. Ahn, H.S. Cho, K. Ide, F. Miyazaki, and S. Arimoto, </author> <title> Strategy Generation and Skill Acquisition for Automated Robotic Assembly Task, </title> <journal> pp. </journal> <pages> 128-133, </pages> <booktitle> Proceedings of the IEEE International Symposium on Intelligent Control, </booktitle> <address> Arlington, VA, </address> <month> 13-15 August, </month> <year> 1991. </year>
Reference: [3] <author> P. Anandan, </author> <title> Measuring visual motion from image sequences, </title> <type> Tech. </type> <address> Rept. COINS-TR-87-21, Amherst, Mass., </address> <institution> University of Massachusetts COINS Department. </institution>
Reference-contexts: Two types of features have been used in this thesis: SSD windows and corners. SSD tracking involves selecting a rectangular window as the feature and then performing a search in the next image to find its location, which minimizes the sum-of-square differences (SSD) between the template and the image <ref> [3] </ref>. This rectangular-window feature is considered a point feature for control purposes. The feature selected should have strong gradients in both directions to provide strong discrimination information in both directions.
Reference: [4] <author> H. Asada, </author> <title> Representation and Learning of Nonlinear Compliance Using Neural Nets, </title> <journal> IEEE Transactions on Robotics and Automation, pp. </journal> <volume> 863-867, Vol. 9, No. 6, </volume> <month> Decem-ber </month> <year> 1993. </year>
Reference: [5] <author> H. Asada and S. Hirai, </author> <title> Towards a Symbolic-level Force Feedback: Recognition of Assembly Process States, </title> <booktitle> in Robotics Research, Fifth International Symposium, </booktitle> <address> Tokyo, Japan, 28-31 Aug, </address> <year> 1989. </year>
Reference-contexts: This thesis does not consider the analysis of these contact states for selection and parameterization of force-based primitives. Some relevant approaches might be the polyhedral convex cone contacts analysis of Asada and Hirai <ref> [5] </ref> in which instantaneous motion constraints can be determined from a geometric model (assuming rigid objects). The automatic recognition of force-based primitives may be better performed with a richer set of input data.
Reference: [6] <author> D. Baraff, </author> <title> Interactive Simulation of Solid Rigid Bodies, </title> <journal> in IEEE Computer Graphics and Applications, v. </journal> <volume> 15, no. 3, </volume> <pages> pp. 63-75, </pages> <year> 1995. </year>
Reference-contexts: The sensor signals are transformed to the task frame and force control is performed about it. Any velocity commands (e.g. vision) apply to the task frame location and in its coordinate system. After creating the part geometry in Telegrip, the geometry is exported to Coriolis. Coriolis <ref> [6] </ref> is a C++ library for simulating rigid-body dynamics including collisions and impact under Newtonian mechanics assumptions. Coriolis allows us to simulate tasks involving contact and provide simulated force feedback. Besides modelling the task mechanics, Coriolis also replaces the robot and the force sensor during simulation.
Reference: [7] <author> A. Blake and A.L. Yuille, </author> <title> Active Vision, </title> <publisher> MIT Press, </publisher> <year> 1992. </year> <note> CHAPTER 8: REFERENCES 154 </note>
Reference-contexts: This points out the need for additional feature trackers -- e.g. for ellipses. Extending the basic image processing to create an ellipse tracker by using the 1D edgefinder windows should not be difficult. Indeed, more general snake-trackers have been created to track non-rigid bodies <ref> [7] </ref>. An ellipse feature would have the x,y location, the lengths of the principal axes, and the orientation of the ellipse in the image.
Reference: [8] <author> R.W. Brockett, </author> <title> On the computer control of movement, </title> <booktitle> in the Proceedings of IEEE Internationall Conference on Robotics and Automation, </booktitle> <address> Philadelphia, PA, </address> <pages> pp. 534-540, </pages> <month> 24-29 April, </month> <year> 1988. </year>
Reference: [9] <author> B. Brunner, K. Arbter, G. Hirzinger, and R. Koeppe, </author> <title> Programming robots via learning by showing in a virtual environment, Virtual Reality World 95, </title> <publisher> Stuttgart, </publisher> <month> Feb 21-23, </month> <year> 1995. </year>
Reference: [10] <author> H. Bruyninckx and J. De Schutter, </author> <title> Specification of Force-Controlled Actions in the Task Frame Formalism -- A Synthesis, </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> vol. 12, no. 4, </volume> <month> August </month> <year> 1996, </year> <pages> pp. 581-589. </pages>
Reference: [11] <author> M. Buss and H. Hashimoto, </author> <title> Skill Acquisition System for the Intelligent Assisting System (IAS), </title> <journal> pp. 157-171, Advanced Robotics, </journal> <volume> Vol. 8, No. 2, </volume> <year> 1994. </year>
Reference: [12] <author> J. Canny and K. Goldberg, </author> <title> A RISC approach to robotics, </title> <journal> IEEE Robotics and Automation Magazine, </journal> <volume> vol. 1, no. 1, </volume> <month> March </month> <year> 1994, </year> <pages> pp. 26-28. </pages>
Reference: [13] <author> W.F. Carriker, </author> <title> A Rapid Prototyping System for Flexible Assembly, </title> <type> Ph.D. Thesis, </type> <institution> Electrical and Computer Engineering Dept., Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1995. </year>
Reference: [14] <author> A. Castano and S. </author> <title> Hutchinson Visual Compliance: </title> <journal> Task-Directed Visual Servo Control, IEEE Transactions on Robotics and Automation, </journal> <volume> vol. 10, no. 3, </volume> <pages> pp. 334-342, </pages> <month> June </month> <year> 1994. </year>
Reference: [15] <author> D.C. Deno, R.M. Murray, K.S.J. Pister, and S.S. Sastry, </author> <title> Control Primitives for Robot Systems, </title> <booktitle> in Proceedings of IEEE International Conference on Robotics and Automation, </booktitle> <pages> pp. 1866-1871, </pages> <address> Cincinnati, OH, </address> <year> 1990. </year>
Reference: [16] <author> B. Donald, </author> <title> Planning multi-step error detection and recovery strategies, </title> <journal> International Journal of Robotics Research, </journal> <volume> vol. 9, no. 1, </volume> <pages> pp. 3-60, </pages> <month> February </month> <year> 1990. </year>
Reference: [17] <author> S.D. Eppinger and W.P. Seering, </author> <title> Understanding bandwidth limitations in robot force control, </title> <booktitle> Proceedings of the 1987 IEEE Intl Conf. on Robotics and Automation, </booktitle> <address> Raleigh, NC, 31 March - 3 April, </address> <year> 1987. </year>
Reference: [18] <author> M. Erdmann, </author> <title> Understanding Action and Sensing by Designing Action-Based Sensors, </title> <journal> International Journal of Robotics Research, </journal> <volume> 14(5), </volume> 1995 483-509. 
Reference: [19] <author> B. Espiau, F. Chaumette, and P. Rives, </author> <title> A New Approach to Visual Servoing in Robotics, </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> Vol. 8, No. 3, </volume> <pages> pp. 313-326, </pages> <month> June, </month> <year> 1992. </year>
Reference-contexts: Keying on natural task features is more chal 5.4 SUMMARY 119 lenging but recurring features like edges, vertices, and surfaces can be used. One advantage of fiducials is that both geometric projection and photometric effects can be addressed. The photometric effects are extremely significant and often ignored <ref> [19] </ref>. Specularity, shadows, and other real-world imaging phenomena caused significant problems with simple feature tracking algorithms. However, fiducials are usually internal targets which necessarily introduce offsets -- these offsets introduce additional sensitivity to camera placement uncertainty. <p> The tools exist for simulating a complete skill, but their integration must be completed. Visual feature tracking needs to be improved in a number of ways. Clearly a larger set of visual features must be considered for a real system including edges, regions, and ellipses. Espiau et al <ref> [19] </ref> present different features in their task-function approach to visual servoing which focuses on eye-in-hand formulation. Improved robustness is needed to apply vision-based primitives to real tasks. The simple corner tracker allowed tracking of occluding boundaries but additional constraints are necessary for improved robustness.
Reference: [20] <author> J. Feddema, </author> <title> Visual Servoing: A Technology in Search of An Application, </title> <booktitle> Proc. of Workshop M-5: Visual Servoing: Achievements, Applications, and Open Problems, at IEEE International Conference on Robotics and Automation, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1994. </year>
Reference: [21] <author> M.W. Gertz, </author> <title> A Visual Programming Environment for Real-time Control Systems, </title> <type> Ph.D. Thesis, </type> <institution> Electrical and Computer Engineering Dept., Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1994. </year> <month> 155 </month>
Reference: [22] <author> M.W. Gertz, D.B. Stewart and P.K. Khosla, </author> <title> A Software Architecture-Based Human-Machine Interface for Reconfigurable Sensor-Based Control Systems, </title> <booktitle> in Proceedings of the 8th IEEE Symposium on Intelligent Control, </booktitle> <address> Chicago, IL, </address> <month> August </month> <year> 1993. </year>
Reference: [23] <author> V. Gullapalli, R. Grupen, and A. Barto, </author> <title> Learning Reactive Admittance Control, </title> <booktitle> in Proceedings of IEEE International l Conference. on Robotics and Automation, </booktitle> <address> Nice, France, </address> <year> 1992. </year>
Reference-contexts: This allows the skills to be used wherever a higher-level program can initialize them appropriately relative to the task. This differential nature of skill definition is contrasted with some other skill-building efforts which use absolute position data as input and output -- for example Gullapallis peg insertion skills <ref> [23] </ref>. Philosophically the idea is to get away from specifying tasks in terms of absolute robot movement and toward specifying tasks in terms of the robots view of them -- their projection onto the robots sensors.
Reference: [24] <author> T. Hasegawa, T. Suchiro, and K. Takase, </author> <title> A Model-Based Manipulation System with Skill-Based Manipulation, </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> Vol 8, No. 5, </volume> <pages> pp. 535-544, </pages> <month> October </month> <year> 1992. </year>
Reference: [25] <author> L.S. Haynes and G.H. Morris, </author> <title> A Formal Approach to Specifying Assembly Operations, </title> <journal> Int. J. Mach. Tools Manufact., </journal> <volume> Vol. 28, No. 3, </volume> <pages> pp. 281-298, </pages> <year> 1988. </year>
Reference: [26] <author> S.H. Hopkins , C.J. Bland, and M.H. Wu, </author> <title> Force sensing as an aid to assembly, </title> <journal> Int. J. Prod. Res., </journal> <volume> vol. 29, no. 2, </volume> <pages> pp 293-301, </pages> <year> 1991. </year>
Reference-contexts: Other robotics researchers have realized the need for such reactive, sensor-based primitives. Smithers and Malcolm [64] combine task-achieving behaviors with a planner for a SOMA-cube world. Hopkins et al <ref> [26] </ref> suggest the development a force primitive library for assembly but do not provide a methodology for creating it.
Reference: [27] <author> S. Hutchinson, G.D. Hager, </author> <title> and P.I. Corke, A Tutorial on Visual Servo Control, </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> vol. 12, no. 5, </volume> <month> October </month> <year> 1996, </year> <pages> pp. 651-670. </pages>
Reference-contexts: A recent survey can be found in <ref> [27] </ref>. Image-based visual servoing is used to incorporate vision into the feedback loop because it supports closed-loop control around task errors and because it is robust to calibration errors between the camera and manipulator.
Reference: [28] <author> S.B. Kang, </author> <title> Automatic Robot Instruction from Human Observation, </title> <type> Ph.D. Thesis, </type> <institution> The Robotics Institute, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> December, </month> <year> 1994. </year>
Reference: [29] <author> J.C. Latombe, </author> <title> Robot Motion Planning, </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1991. </year>
Reference: [30] <author> S. Lee and H. Asada, </author> <title> Assembly of Parts with Irregular Surfaces Using Active Force Sensing, </title> <journal> pp. </journal> <pages> 2639-2644, </pages> <booktitle> Proceedings of IEEE International Conference on Robotics and Automation, </booktitle> <address> San Diego, CA, </address> <year> 1994. </year>
Reference-contexts: Lee and Asada <ref> [30] </ref> use a similar approach to adjust to the minimum stiffness location during an insertion operation. The defining event is the loss of freedom along the surface motion direction.
Reference: [31] <author> S. Lee and M. H. Kim, </author> <title> Learning Expert Systems for Robot Fine Motion Control, </title> <journal> pp. </journal> <pages> 534-544, </pages> <booktitle> Proceedings of the IEEE International Symposium on Intelligent Control, </booktitle> <address> Arlington, VA, </address> <year> 1988. </year>
Reference: [32] <author> S. Liu and H. Asada, </author> <title> Transferring Manipulative Skills to Robots: Representation and Acquisition of Tool Manipulative Skills Using a Process Dynamics Model, </title> <journal> ASME Journal of Dynamic Systems, Measurement, and Control, </journal> <volume> Vol. 114, No. 2, </volume> <pages> pp. 200-228, </pages> <month> June </month> <year> 1992. </year>
Reference: [33] <author> T. Lozano-Perez, </author> <title> Task Planning, Ch. 6, Robot Motion: Planning and Control, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1982. </year>
Reference: [34] <author> T. Lozano-Perez, M.T. Mason, and R.H. Taylor, </author> <title> Automatic Synthesis of Fine-Motion Strategies for Robots, </title> <journal> Int. J. of Rob. Res., </journal> <volume> Vol 3, No 1, </volume> <pages> pp. 3-23, </pages> <month> Spring, </month> <year> 1984. </year>
Reference: [35] <author> D. Lyons and A.J. Hendriks, </author> <title> Planning by Adaptation: Experimental Results, </title> <booktitle> Proceedings of IEEE International Conference on Robotics and Automation, </booktitle> <pages> pp. 855-860, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1994. </year> <note> CHAPTER 8: REFERENCES 156 </note>
Reference: [36] <author> D. Lyons, </author> <title> RS: A Formal Model of Distributed Computation For Sensory-Based Robot Control, </title> <type> COINS Tech. </type> <institution> Rept. 86-43, University of Massachusetts at Amherst, </institution> <month> Sept. </month> <year> 1986. </year>
Reference: [37] <author> M.T. Mason, </author> <title> Compliance and Force Control for Computer Controlled Manipulators, </title> <journal> IEEE Trans. on Sys., Man, and Cybernetics, </journal> <volume> SMC-11, </volume> <month> 6 (June, </month> <year> 1981), </year> <pages> pp. 418-432. </pages>
Reference-contexts: small force set-points are necessary, force feedback which is closer to the task (e.g. fingertips) is probably F T R S F S = T T S 3.3 FORCE SENSING AND CONTROL 53 a better solution than wrist force sensing. 3.3.2 Control Our basic force controller is damping force control <ref> [37] </ref> which translates force errors into velocity perturbations (Figure 3-6). The controller accepts both force and velocity set-points in the task frame for input (V d and/or F d ). Selection of force and velocity controlled axes is done via 6x6 diagonal selection matrices (S V and S F ). <p> A surface against surface is one contact interpretation of bbc. The hybrid control specification is straight-forward to comply to this constraint set (see, for example, Mason <ref> [37] </ref>). Assuming Z is the surface normal direction, force is commanded in Z, and torques about X, Y are zero. Class bbc can also be interpreted as a non-contact constraint set implemented with visual feedback.
Reference: [38] <author> R.S. Mattikalli, </author> <title> Mechanics-based Assembly Planning, </title> <type> Ph.D. Thesis, </type> <institution> Department of Mechanical Engineering, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1994. </year>
Reference: [39] <author> B.J. McCarragher and H. Asada, </author> <title> Qualitative Template Matching using Dynamic Process Models for State Transition Recognition in Robotic Assembly, </title> <journal> Trans. ASME J. of Dynamic Systems, Measurement, and Control, </journal> <volume> vol. 115, no. 2A, </volume> <month> June </month> <year> 1993, </year> <pages> pp. 261-269. </pages>
Reference-contexts: In general event detection requires predicting the event projection onto the sensor signal. Predicting this projection is currently more of an art than a science. Complex contact state effects on force signals are notoriously difficult to predict algorithmically. McCarragher and Asada <ref> [39] </ref> have used qualitative interpretation of force signals along with a dynamic task process model to recognize discrete state changes in a robotic assembly task. This thesis focuses on simpler event detectors which can be re-applied to different tasks.
Reference: [40] <author> B.J. McCarragher, </author> <title> Force Sensing from Human Demonstration Using a Hybrid Dynamical Model and Qualitative Reasoning, </title> <booktitle> Proceedings of IEEE International Conference on Robotics and Automation, </booktitle> <pages> pp. 557-563, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1994. </year>
Reference: [41] <author> P. Michelman and P. Allen, </author> <title> Forming Complex Dextrous Manipulations from Task Primitives, </title> <journal> pp. </journal> <pages> 3383-3388, </pages> <booktitle> Proceedings of IEEE International Conference on Robotics and Automation, </booktitle> <address> San Diego, CA, </address> <year> 1994. </year>
Reference-contexts: Instead, a process model is needed to relate the goal (burr removal) to the relative motion which the robot can implement. A manipulation task primitive (MTP) can be classified by a particular relative motion between two parts. This is a more task-centered definition than Michelman and Allen <ref> [41] </ref> or Speeter [65], who use the term to refer to primitive multi-fingered hand motions useful for manipulation tasks. Speeter generates a collection of coordinated hand joint motions which implement useful finger motions (e.g. grasping or pinching).
Reference: [42] <author> G.H. Morris and L.S. Haynes, </author> <title> Robotic Assembly by Constraints, </title> <journal> pp. </journal> <pages> 1507-1515, </pages> <booktitle> Proceedings of IEEE International Conference on Robotics and Automation, </booktitle> <address> Raleigh, NC, </address> <year> 1987. </year>
Reference-contexts: There are 64 (2 6 ) different possible definitions of relative motion between two parts (each of 6 DOF has two possible values: 1=artificial or 0=natural) and 20 of these are unique. Morris and Haynes <ref> [42] </ref> identify 17 as reasonable for describing assembly constraints. We use a different method of representing the relative motions than Morris and Haynes [42]. <p> Morris and Haynes <ref> [42] </ref> identify 17 as reasonable for describing assembly constraints. We use a different method of representing the relative motions than Morris and Haynes [42]. Rather than list each DOF with a 1 or 0, the translation and rotation DOF for an axis are combined into one symbol which encodes translation/rotation classification (Table 1). The possible DOF (T/R) for each axis can be expressed as a letter from a 4-letter alphabet.
Reference: [43] <author> J.D. Morrow and P.K. Khosla, </author> <title> Sensorimotor Primitives for Robotic Assembly Skills, </title> <booktitle> in Proceedings of the 1995 IEEE International Conference on Robotics and Automation, </booktitle> <address> Nagoya, Japan, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: Using all the information about the task model and the control strategy can help in designing event detectors. A normalized correlation measure between the reference and force-perturbed velocity signals detects the presence of a planar motion constraint <ref> [43] </ref>. In this section the basic structure and components defining a sensorimotor primitive were introduced. The rest of this chapter is organized around the different MTP classifications of relative motion. Geometric interpretation (s) which actually define the constraints are used to derive or implement algorithms to execute the primitive motion.
Reference: [44] <author> J.D. Morrow, B.J. Nelson, and P.K. Khosla, </author> <title> Vision and Force Driven Sensorimotor Primitives for Robotic Assembly Skills, </title> <booktitle> to appear in Proceedings of the 1995 IEEE/ RSJ International Conference on Intelligent Robots and Systems (IROS), </booktitle> <address> Pittsburgh, PA, </address> <month> August, </month> <year> 1995. </year>
Reference: [45] <author> B.J. Nelson and P.K. Khosla, </author> <title> Integrating Sensor Placement and Visual Tracking Strategies, </title> <booktitle> Proceedings of the 1994 IEEE Conf. on Robotics and Automation, </booktitle> <pages> pp. 1351-1356, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1994. </year>
Reference: [46] <author> B.J. Nelson and P.K. Khosla, </author> <title> The Resolvability Ellipsoid for Visual Servoing, </title> <booktitle> in Proceedings of the 1994 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR94), </booktitle> <pages> pp. 829-832, </pages> <year> 1994. </year>
Reference-contexts: The difference between the features forms the projected task error on the visual sensor space. Image-based control laws rather than pose-based control laws are used to reduce sensitivity to camera/manipulation calibration and sensor modelling errors. Resolvability analysis developed by Nelson <ref> [46] </ref> drives simple control law decomposition. For a single camera tracking a single corner, the x,y motion on the image plane and rotation about the optical axis are the most resolvable errors (i.e. generate the largest sensor signals).
Reference: [47] <author> B.J. Nelson, J.D. Morrow, and P.K. Khosla, </author> <title> Robotic Manipulation Using High Bandwidth Force and Vision Feedback, </title> <note> to appear in Mathematical and Computer Model-ling Journal, </note> <year> 1995. </year>
Reference: [48] <author> B.J. Nelson, J.D. Morrow, and P.K. Khosla, </author> <title> Improved Force Control Through Visual Servoing, </title> <booktitle> in Proceedings of the 1995 American Control Conference, </booktitle> <address> Seattle, WA, </address> <month> June, </month> <year> 1995. </year> <month> 157 </month>
Reference: [49] <author> B.J. Nelson, N.P. Papanikolopolous, and P.K. Khosla, </author> <title> Visual servoing for robotic assembly. Visual Servoing - Real-Time Control of Robot Manipulators Based on Visual Sensory Feedback. </title> <editor> ed. K. Hashimoto. </editor> <address> River Edge, NJ: </address> <publisher> World Scientific Publishing Co. Pte. Ltd. </publisher> <pages> pp. 139-164, </pages> <year> 1993. </year> <note> [50] , J. </note> <author> OSullivan, T. Mitchell, and S. Thrun, </author> <title> Explanation-Based Learning for Mobile Robot Perception, in Symbolic Visual Learning, </title> <editor> Ikeuchi and Veloso(eds.), </editor> <year> 1995. </year>
Reference-contexts: A pin-hole camera model, is used to compute the projection of a task point onto the image plane. Differentiating this equation leads to the familiar optical-ow equation which relates task point velocities to image plane velocities. From Nelson et al <ref> [49] </ref>, the result is: (3-7) where the image jacobian relates a point feature velocity on the image plane to the task frame velocities for the task frame aligned with the camera frame. 3.4.4 Control In this thesis simple proportional control laws are used based on the errors between a moving (and
Reference: [51] <author> J. K. Ousterhout, </author> <title> Tcl and the Tk Toolkit, </title> <type> draft, </type> <institution> Addison-Welsey, </institution> <year> 1993. </year>
Reference: [52] <author> W. Paetsch, and G. von Wichert, </author> <title> Solving Insertion Tasks with a Multifingered Gripper by Fumbling, </title> <journal> pp. </journal> <pages> 173-179, </pages> <booktitle> Proceedings of IEEE International Conference on Robotics and Automation, </booktitle> <address> Atlanta, GA, </address> <year> 1993. </year>
Reference: [53] <author> R. Palm and H. Fuchs, </author> <title> Skill Based Robot Control for Flexible Manufacturing Systems, </title> <booktitle> IFAC Skill Based Automated Production, </booktitle> <pages> pp. 189-195, </pages> <address> Vienna, Austria, </address> <year> 1989. </year>
Reference: [54] <author> N.P. Papanikolopoulos, B.J. Nelson, and P.K. </author> <title> Khosla , Full 3-d tracking using the controlled active vision paradigm, </title> <booktitle> Proc. 1992 IEEE Int. Symp. on Intelligent Control (ISIC-92). </booktitle> <address> New York:IEEE, </address> <pages> pp. 267-274, </pages> <year> 1992. </year>
Reference: [55] <author> C.J.J. Paredis, </author> <title> An Agent-based Approach to the Design of Rapidly Deployable Fault Tolerant Manipulators, </title> <type> Ph.D. Thesis, </type> <institution> Electrical and Computer Engineering Dept., Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> August, </month> <year> 1996. </year>
Reference: [56] <author> D. Pomerleau, </author> <title> Neural Network Perception for Mobile Robot Guidance, </title> <type> Ph.D. Thesis, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1992. </year>
Reference: [57] <author> J.M. Schimmels and M.A. </author> <title> Peshkin, Admittance Matrix Design for Force-Guided Assembly, </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> Vol. 8, No. 2, </volume> <pages> pp. 213-227, </pages> <month> April </month> <year> 1992. </year>
Reference: [58] <author> K.K.W. Selke, </author> <title> A framework for intelligent robotic assembly, </title> <journal> Mechatronic Systems Engineering, </journal> <volume> vol. 1, no. 1, </volume> <pages> pp. 41-51, </pages> <year> 1990. </year>
Reference: [59] <author> J.J. Shah and M.T. Rogers, </author> <title> Assembly Modeling as an Extension of Feature-Based Design, </title> <booktitle> in Research in Engineering Design, </booktitle> <volume> vol 5., </volume> <year> 1993, </year> <pages> pp. 218-237. </pages>
Reference-contexts: In fact, the idea of defining assembly features which relate geometric primitives on different parts has been proposed <ref> [59] </ref> as a method of facilitating downstream processes like assembly sequence planning. This thesis focuses on the generation of an executable program from the CAD environment instead of an abstract assembly sequence.
Reference: [60] <author> K. Shimokura and S. Liu, </author> <title> Programming Deburring Robots Based on Human Demonstration with Direct Burr Size Measurement, </title> <journal> pp. </journal> <pages> 572-577, </pages> <booktitle> Proceedings of IEEE International Conference on Robotics and Automation, </booktitle> <address> San Diego, CA, </address> <year> 1994. </year>
Reference: [61] <author> D.A. Simon, L.E. Weiss, and A.C. Sanderson, </author> <title> Self-Tuning of Robot Program Primitives, </title> <booktitle> in the Proceedings of the 1990 International Conference on Robotics and Automation, Cincinnati, OH, </booktitle> <volume> vol. 1, </volume> <pages> pp. 708-713, </pages> <month> May </month> <year> 1990. </year>
Reference: [62] <author> J. Simons, H. Van Brussel, J. De Schutter, and J. Verhaert, </author> <title> A Self-Learning Automaton with Variable Resolution for High Precision Assembly by Industrial Robots, </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> Vol AC-27, No. 5, </volume> <month> October, </month> <year> 1982. </year>
Reference: [63] <author> R. N. Singer, </author> <title> Motor Learning and Human Performance: An Application to Motor Skills and Movement Behaviors, </title> <journal> pp. </journal> <pages> 102-103, </pages> <publisher> MacMillan Publishing Co., </publisher> <address> New York, </address> <year> 1980. </year> <note> CHAPTER 8: REFERENCES 158 </note>
Reference: [64] <author> T. Smithers and C. Malcolm, </author> <title> Programming Robotic Assembly in terms of Task Achieving Behavioural Modules, DAI Research Paper No. </title> <type> 417, </type> <institution> University of Edin-burgh, Edinburgh, </institution> <address> Scotland, </address> <month> December </month> <year> 1988. </year>
Reference-contexts: The goal is the same: to discover patterns in the higher-dimensional space which are useful for tasks and capture them in parameterized primitives. Other robotics researchers have realized the need for such reactive, sensor-based primitives. Smithers and Malcolm <ref> [64] </ref> combine task-achieving behaviors with a planner for a SOMA-cube world. Hopkins et al [26] suggest the development a force primitive library for assembly but do not provide a methodology for creating it.
Reference: [65] <author> T.H. Speeter, </author> <title> Primitive-Based Control of the Utah/MIT Dextrous Hand, </title> <booktitle> Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <pages> pp. 866-877, </pages> <address> Sac-ramento, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: A manipulation task primitive (MTP) can be classified by a particular relative motion between two parts. This is a more task-centered definition than Michelman and Allen [41] or Speeter <ref> [65] </ref>, who use the term to refer to primitive multi-fingered hand motions useful for manipulation tasks. Speeter generates a collection of coordinated hand joint motions which implement useful finger motions (e.g. grasping or pinching).
Reference: [66] <author> D.B. Stewart, </author> <title> Real-time Software Design and Analysis of Reconfigurable Multisensor based Systems, </title> <type> Ph.D. Thesis, </type> <institution> Electrical and Computer Engineering Department, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1994. </year>
Reference-contexts: The Chimera SBS level provides an excellent framework for creating modular real-time software for implementing periodic task modules <ref> [66] </ref> and is an enabling technology for this research. Without the ability to construct modular, reusable real-time software, skills could not be efficiently composed. The periodic modules can be naturally combined into controllers which have a continuous nature. The higher-level, meta-control of these modules was missing.
Reference: [67] <author> D.B. Stewart, R.A. Volpe, and P.K. Khosla, </author> <title> Integration of Real-Time Software Modules for Reconfigurable, </title> <booktitle> Sensor-Based Control Systems, Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems, </booktitle> <address> Raleigh, NC, </address> <pages> pp. 325-333, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: The lack of kinematic redundancy in the robot requires the Cartesian moves to be relatively small to avoid robot singularities. Since we are focusing on fine-motion or gross/fine motion transition, these small Cartesian motions (~10 cm) are adequate. The resolved-rate cartesian controller is implemented with Chimera <ref> [67] </ref> reconfigurable modules (Figure 3-3). These modules are port-based objects with well-defined input/output and data processing functions. The advantage of the Chimera architecture is that modular real-time software components can be quickly developed and tested. The velocity setpoint may be generated by many different modules. <p> These primitive requirements are passed onto the skill as its requirements (at different states). To implement these finite-state machines required extending the Chimera reconfigurable software framework developed by Stewart et al <ref> [67] </ref>. This extension is described in the next section and then example skills are described with experimental results presented. 5.2 Chimera Agent Level Implementing these event-driven skills required extensions to the Chimera real-time reconfigurable software environment (Figure 5-1).
Reference: [68] <author> D. </author> <title> Strip, Technology for Robotics Mechanical Assembly: </title> <journal> Force-Directed Insertions, AT&T Technical Journal, </journal> <volume> Vol. 67, Issue 2, </volume> <pages> pp. 23-34, </pages> <month> March/April </month> <year> 1988. </year>
Reference-contexts: MTP classification is one tool to try to ensure a primitive set with broad coverage of basic manipulation motions. The same task with different parameters might be considered both primitive and com CHAPTER 2: MANIPULATION TASK PRIMITIVES 44 plex. For example, primitives exist for solving close tolerance insertions <ref> [68] </ref> but typically these primitives have fairly small applicability regions (i.e. deal with relatively small uncertainty regions). So the same insertion task with a larger initial uncertainty region would be considered complex if no primitive existed to solve it. One decomposition of it could include a more limited insertion primitive.
Reference: [69] <author> B. Stroustrup, </author> <title> The C++ Programming Language, </title> <editor> p. </editor> <volume> 8, </volume> <publisher> Addison-Wesley Publishing Co., </publisher> <address> Reading, MA, </address> <year> 1991. </year>
Reference: [70] <author> K. Toyama and G.D. Hager, </author> <title> Keeping Your Eye on the Ball: Tracking Occluding Contours of Unfamiliar Objects without Distraction, </title> <journal> pp. </journal> <volume> 354-359, Vol. 1, </volume> <booktitle> Proceedings of 1995 IEEE/RSJ International Conference on Intelligent Robots and Systems, </booktitle> <address> Pitts-burgh, PA, </address> <year> 1995. </year>
Reference-contexts: In response to these problems, we have developed a different feature tracker based on a technique introduced in <ref> [70] </ref>. The basic assumption is that foreground color or intensity of the tracked object has enough spatial and temporal stability to be recognized in successive images (i.e. it is relatively constant). This assumption precludes tracking objects which are textured or otherwise have a mix of intensities.
Reference: [71] <author> W.O. Troxell, </author> <title> A robotic assembly description language derived from task-achieving behaviors, </title> <booktitle> Proceedings of Manufacturing International 90, </booktitle> <address> Atlanta, GA, </address> <month> 25-28 March </month> <year> 1990. </year>
Reference: [72] <author> E.G. Vaaler and W.P. Seering, </author> <title> A Machine Learning Algorithm for Automated Assembly, </title> <booktitle> Proceedings of IEEE International Conference on Robotics and Automation, </booktitle> <year> 1991. </year>
Reference: [73] <author> R. Volpe and P. Khosla, </author> <title> A Theoretical and Experimental Investigation of Explicit Force Control Strategies for Manipulators, </title> <journal> IEEE Trans. of Automatic Control, </journal> <volume> vol. 38, no. 11, </volume> <month> Nov. </month> <year> 1993, </year> <pages> pp. 1634-50. </pages>
Reference: [74] <author> R.M. Voyles, J.D. Morrow, and P.K. Khosla, </author> <title> Gesture-Based Programming, Part 2: Primordial Learning, </title> <booktitle> in Intelligent Engineering Systems through Artificial Neural Networks, Volume 6; Smart Engineering Systems: Neural Networks, Fuzzy Logic and Evolutionary Programming, </booktitle> <publisher> ASME Press 1996. </publisher>
Reference-contexts: Obviously, since sensor frame tool frame task frame gripper part mg contact forces CHAPTER 3: SENSOR AND MOTOR RESOURCES 52 the mass is not changing the measured weight should be constant. Using a more advanced calibration technique <ref> [74] </ref> can reduce this variation to ~5%, but completely cancelling the gripper weight with a feedforward term is not easy. When parts are in contact (or very near) low velocities are required for (stable) low-gain force feedback, and inertial loads due to acceleration are ignored. <p> The automatic recognition of force-based primitives may be better performed with a richer set of input data. For example, Voyles et al <ref> [74] </ref> apply principal components analysis technique to recognize guarded moves and align moves from teleoperated demonstration. This is not applicable to our system since the demonstration phase involves only indicating a task state change and does not involve a continuous task motion. <p> Formalizing the interaction between the human, primitives, and task model is important. The design component of the primitives must be able to recognize when the human is trying to perform that primitive by observing the humans actions as well as the task state change. Voyles et al <ref> [74] </ref> present some preliminary results from applying principal components analysis to primitive recognition for simple primitives. As the number of primitives grows, the user interface will have to evolve to efficiently manage them.
Reference: [75] <author> R.M. Voyles, Jr., G.F. Fedder, and P.K. Khosla, </author> <title> A Modular Tactile Sensor and Actuator based on an Electrorheological Gel, </title> <booktitle> in the Proceedings of the 1996 IEEE International Conference on Robotics and Automation, </booktitle> <address> Minneapolis, MN, </address> <month> April, </month> <year> 1996. </year> <month> 159 </month>
Reference: [76] <author> L. Weiss, </author> <title> Dynamic Visual Servo Control of Robots: An Adaptive Image-based Approach, </title> <type> Ph.D. Thesis, </type> <institution> Electrical and Computer Engineering Dept., Carnegie Mellon University, </institution> <address> Piitsburgh, PA, </address> <year> 1984. </year>
Reference-contexts: Y task (control) frame true task frame F Y F c q ( )cos K F V X V c q ( )cos F c q ( )sin K F += CHAPTER 3: SENSOR AND MOTOR RESOURCES 56 3.4 Visual Servoing Weiss <ref> [76] </ref> first studied the incorporation of vision directly into the robot servo-control loop and today the technique is known as visual servoing. A recent survey can be found in [27].
Reference: [77] <author> D. Whitney, </author> <title> A Survey of Manipulation and Assembly: </title> <booktitle> Development of the Field and Open Research Issues, in Robotics Science, </booktitle> <editor> ed. M. Brady, </editor> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: The term sensorimotor indicates the fusing of sensing and action into one command with a task-relevant definition. This thesis focuses on primitives which involve relatively small motions at the gross-fine motion boundary and fine-motion <ref> [77] </ref>. Because of this, the primitives do not consider gross constraints like avoiding joint singularities or avoiding obstacles. Task-driven primitives are defined via sensor information. The goal is to control the task, and the robot is merely a tool to effect it.
Reference: [78] <author> D. Whitney, </author> <title> Quasi-Static Assembly of Compliantly Supported Parts, </title> <journal> ASME Journal of Dynamic Systems, Measurement, and Control, </journal> <volume> Vol 104, </volume> <pages> pp. 65-77, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: Although the single translation DOF does not require two cameras, the general specification of only 2 rotation DOF does. 4.7 Constraining Four DOF 4.7.1 add The add class has a very common contact interpretation: round peg-in-hole. Whitney <ref> [78] </ref> showed the appropriate task frame location is at the tip of the peg leading motion (or beyond it) -- this location positions the center of compliance to provide proper rotation in response to insertion forces and torques. <p> A very common contact interpretation is non-round peg in b c interpretation 4.9 TRANSITION PRIMITIVES 87 hole. The task frame can be defined by either part and we will use the controlled part -- following Whitney <ref> [78] </ref>, the task frame should be located at or beyond the tip of the peg which leads motion into the hole. Hybrid control is straight-forward to specify -- force control about all axes DOF except the insertion translation direction. Again, a simple non-contact implementation can be implemented under visual servoing.
Reference: [79] <author> J. Yang, Y. Xu, and C.S. Chen, </author> <title> Hidden Markov Model Approach to Skill Learning and Its Application to Telerobotics, </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> Vol. 10, No. 5, </volume> <pages> pp. 621-631, </pages> <month> October </month> <year> 1994. </year> <note> CHAPTER 8: REFERENCES 160 Chapter 9 </note>
References-found: 78

