URL: ftp://ftp.cs.Helsinki.FI/pub/Reports/by_Title/Mapping_Bayesian_Networks_to_Stochastic_Neural_Networks.ps.gz
Refering-URL: 
Root-URL: 
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Aarts, E., and Korst, J. </author> <title> Simulated Annealing and Boltzmann Machines: A Stochastic Approach to Combinatorial Optimization and Neural Computing. </title> <publisher> John Wiley & Sons, </publisher> <address> Chichester, </address> <year> 1989. </year>
Reference-contexts: This iterative process can be seen as a kind of a stochastic local search, where the probability of finding the globally optimal solution approaches one as the number of iterative steps used approaches infinity <ref> [40, 1] </ref>. Consequently, simulated annealing can be used for solving the MAP problem stochastically. In Section 3.3 we present the general theoretical framework for MCMC algorithms and the simulated annealing technique. <p> After being introduced to the optimization theory community in [71], SA has been applied to many different (NP-hard) optimization problems, such as TSP [71], graph partitioning [65], graph coloring [66], number set partitioning [66] and clustering [105, 14] (for an extensive survey of applications, see [73] or <ref> [1, pp. 89-90] </ref>). One of the main difficulties with the SA algorithm is that the SA theory requires that the objective function to be optimized has to be representable in a specific Gibbs distribution form. <p> In the Bayesian reasoning framework we can distinguish two different approaches to this problem: the maximum a posteriori probability assignment approach (MAP), and the expected value estimation (EVE) approach. As in <ref> [1] </ref>, the former can be defined formally as follows: Definition 2.1 (The MAP problem) Let P max denote the maximal probability in the set E = f ~ E = ~eg consisting of all the configurations consistent with the given value assignment, P max = max Pf~ug: Furthermore, let opt denote <p> However, very little is known about the quality of 22 Solving the MAP problem by Bayesian networks the solutions produced by the Markov chain models, if a limited amount of time is available, but some analyses of the convergence speed can be found in <ref> [32, 1, 113, 101, 98, 22] </ref>. In particular, for Bayesian networks restricted by their dependence value (see above), there exists a stochastic polynomial-time algorithm which solves the EVE problem in the PAC sense [24]. <p> Proof: See <ref> [1, p.42] </ref>. fl For the acceptance probability matrix A, the original choice suggested by Metropolis et al. in [80] was the following: A ij = &lt; 1 ; if Pf~u i g 1, Pf~u i g ; if Pf~u i g &lt; 1. <p> Then the unique limiting distribution of is P. Proof: As Pf~u i g &gt; 0 for all i (and assuming that condition (G1) is met), the resulting Markov chain is irreducible and aperiodic (see <ref> [1, p.39] </ref>). <p> Gibbs sampler converges to zero, the probability of finding the annealing process in state ~u i converges to the uniform distribution in the set opt : lim lim P T (S (t) = ~u i ) = 1=j opt j ; if ~u i 2 opt 0 otherwise, Proof: See <ref> [1, p.18] </ref>. fl The sampling/annealing procedure presented above applies naturally also outside the actual statistical physics environment, and hence it is usually referred to as simulated annealing (SA). As SA in principle finds the global minimum of any energy function, the algorithm is applicable to combinatorial optimization problems in general. <p> This possibility was brought to general 3.3 Simulated annealing 31 attention of the optimization theory community by Kirkpatrick et al. in [71], and SA has since been widely applied for many different optimization problems (see the references in <ref> [1] </ref>). Simulated annealing is also widely used in image processing, in particular due to the pioneering theoretical work done by Geman & Geman (see [40]). <p> In contrast to the negative theoretical results, it has been empirically observed that in many cases good quality solutions can be found quite reliably with polynomial time cooling schedules <ref> [1] </ref>. Nevertheless, the task of finding a suitable cooling schedule seems to be a very difficult problem. <p> The condition concerning the constant t in Theorem 3.10 is used only to ensure that the Gibbs sampling process is fair, i.e. no variables are going to be ignored during the sequential variable updating process. Actually, as noted later by several researches (see the list of references in <ref> [1, Ch. 3.4.] </ref>), the assumption of changing only one variable at a time is not necessary at all. <p> Now it follows that lim lim P T (S (t) 2 C opt ) = 1: Proof: As Theorem 4.1, but instead of Theorem 3.7, apply Theorem 8.2 in <ref> [1] </ref>. fl Obviously, as all the nodes in a cluster can be updated simultaneously, the degree of parallelism depends on the number of clusters in the network. <p> The annealing factor F determines how much the temperature is decreased after processing L iterations at a constant temperature. Although simple, this type of cooling schedule is very common, and has proven successful in many applications <ref> [1] </ref>. It is also empirically observed that more sophisticated annealing methods do not necessarily produce any better results than this simple method [65]. The cooling schedule used in our simulations is discussed in more detail in the next section. <p> In the stochastic simulation community, it is generally assumed that one long simulation run produces better results than several short runs, since the shorter runs never quite reach the equilibrium state of the stochastic process [118], <ref> [1, p. 94] </ref>. <p> For the same reason, we did not experiment with the more complex cooling schedules listed in <ref> [73, 1] </ref>, or with any of the elaborate techniques for speeding up the convergence speed of the simulated annealing process [97, 12, 3, 7, 100, 63], but assumed that the proportional speedup gained from using any of these methods would be roughly equal with both SSA and BMSA. 76 Empirical results
Reference: [2] <author> Abbott, K. </author> <title> Robust operative diagnosis as problem solving in a hypothesis space. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence (Saint Paul, </booktitle> <address> Minnesota, August 1988), </address> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <pages> pp. 369-374. </pages>
Reference-contexts: This means that traditional rule-based system relying on pure logic suffer from the brittleness of the resulting software: as the programs are sensitive even to the slightest inaccuracy or incompleteness in their input data <ref> [2] </ref>, the systems tend to grow to have rule bases consisting of tens of thousands of rules, when trying to provide a specific rule for every possible situation [28].
Reference: [3] <author> Alspector, J., Zeppenfeld, T., and Luna, S. </author> <title> A volatility measure for annealing in feedback neural networks. </title> <booktitle> Neural Computation 4 (1992), </booktitle> <pages> 191-195. </pages>
Reference-contexts: For the same reason, we did not experiment with the more complex cooling schedules listed in [73, 1], or with any of the elaborate techniques for speeding up the convergence speed of the simulated annealing process <ref> [97, 12, 3, 7, 100, 63] </ref>, but assumed that the proportional speedup gained from using any of these methods would be roughly equal with both SSA and BMSA. 76 Empirical results 6.3 Results As noted earlier, the primary objective of the experiments was to study the behavior of the SSA and
Reference: [4] <author> Andersen, S., Olesen, K., Jensen, F., and Jensen, F. </author> <title> Hugin a shell for building belief universes for expert systems. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence (Detroit, </booktitle> <address> Michigan, August 1989), </address> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <pages> pp. 1080-1085. </pages>
Reference-contexts: Consequently, most existing systems for Bayesian reasoning first transform a given multi-connected BN structure to a singly-connected network, and then use the existing polynomial-time algorithms (see e.g. <ref> [4, 13] </ref>). This transformation can be done either explicitly by clustering several nodes together as in [76, 106] or [96, Ch.4.4.1], or implicitly by blocking multiply connected paths by conditioning a set of variables as in [96, Ch.4.4.2] (for a discussion of different transformation techniques, see [107]).
Reference: [5] <editor> Anderson, J., and Rosenfeld, E., Eds. Neurocomputing: </editor> <booktitle> Foundations of Research. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference-contexts: In Chapter 4 we consider another family of models for building MAP solvers, neural networks (NN). Neural networks are massively parallel computational models consisting of a large number of very simple processing units (for a survey of neural models, see e.g. the collections <ref> [5, 6, 103, 78] </ref>). These models can perform certain computational tasks extremely fast when run on customized parallel hardware, and hence they have been suggested as a computationally efficient tool for solving NP-hard optimization problems ap-proximatively [59, 10]. <p> For a good introduction to neural 41 0.5 -10 -5 0 5 10 x fi = 1 fi = 10 fi = 0:1 computing, see e.g. the collections <ref> [5, 6, 103, 78] </ref>, or the books [46, 54, 47].
Reference: [6] <editor> Anderson, J., and Rosenfeld, E., Eds. </editor> <title> Neurocomputing 2: Directions for Research. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: In Chapter 4 we consider another family of models for building MAP solvers, neural networks (NN). Neural networks are massively parallel computational models consisting of a large number of very simple processing units (for a survey of neural models, see e.g. the collections <ref> [5, 6, 103, 78] </ref>). These models can perform certain computational tasks extremely fast when run on customized parallel hardware, and hence they have been suggested as a computationally efficient tool for solving NP-hard optimization problems ap-proximatively [59, 10]. <p> For a good introduction to neural 41 0.5 -10 -5 0 5 10 x fi = 1 fi = 10 fi = 0:1 computing, see e.g. the collections <ref> [5, 6, 103, 78] </ref>, or the books [46, 54, 47].
Reference: [7] <author> Ansari, N., S., R., and Wang, G. </author> <title> An efficient annealing algorithm for global optimization in Boltzmann machines. </title> <journal> Journal of Applied Intelligence 3, </journal> <volume> 3 (1993), </volume> <pages> 177-192. </pages>
Reference-contexts: For the same reason, we did not experiment with the more complex cooling schedules listed in [73, 1], or with any of the elaborate techniques for speeding up the convergence speed of the simulated annealing process <ref> [97, 12, 3, 7, 100, 63] </ref>, but assumed that the proportional speedup gained from using any of these methods would be roughly equal with both SSA and BMSA. 76 Empirical results 6.3 Results As noted earlier, the primary objective of the experiments was to study the behavior of the SSA and
Reference: [8] <author> Barker, A. </author> <title> Monte Carlo calculations of the radial distribution functions for a proton-electron plasma. Aust. </title> <journal> J. Phys. </journal> <volume> 18 (1965), </volume> <pages> 119-133. </pages>
Reference-contexts: This is still perhaps the most commonly used method in MCMC applications. The Metropolis method clearly fulfills the conditions (A1) and (A2) in Theorem 3.7. In an alternative model, Barker's method <ref> [8] </ref>, these require ments are not met: A ij = Pf~u j g + Pf~u i g 1 Pf~u i g : (3.3) Nevertheless, existence of a unique limiting distribution can still be proved: Theorem 3.8 Let = fS (t); t = 0; 1; : : :g be a stochastic simulation
Reference: [9] <author> Barnden, J., and Pollack, J., Eds. </author> <title> Advances in Connectionist and Neural Computation Theory, Vol. I: High Level Connectionist Models. </title> <publisher> Ablex Publishing Company, </publisher> <address> Norwood, NJ, </address> <year> 1991. </year> <note> 84 BIBLIOGRAPHY </note>
Reference-contexts: Naturally, the resulting neural network could also be regarded as a cleverly chosen starting point to some learning algorithm, but this interesting idea is not studied here further. Compared to other neural-symbolic hybrid systems (see e.g. <ref> [9, 56, 41, 116] </ref>), the Bayesian-neural hybrid system suggested here has two clear advantages.
Reference: [10] <author> Baum, E. </author> <title> Towards practical 'neural' computation for combinatorial optimization problems. </title> <booktitle> In Proceedings of the AIP Conference 151: Neural Networks for Computing (Snowbird, </booktitle> <address> UT, </address> <year> 1986), </year> <editor> Denker, J., Ed., </editor> <booktitle> American Institute of Physics, </booktitle> <address> New York, NY, </address> <pages> pp. 53-58. </pages>
Reference-contexts: These models can perform certain computational tasks extremely fast when run on customized parallel hardware, and hence they have been suggested as a computationally efficient tool for solving NP-hard optimization problems ap-proximatively <ref> [59, 10] </ref>. Especially suitable for these tasks are stochastic neural network architectures, as these models are based on a stochastic updating process very similar to simulated annealing.
Reference: [11] <author> Besag, J. </author> <title> Spatial interaction and the statistical analysis of lattice systems (with discussion). </title> <journal> J. Royal Statist. Soc., series B 34 (1972), </journal> <pages> 75-83. </pages>
Reference-contexts: Usually, a suitable Gibbs distribution has to be constructed manually for each separate optimization problem instance using very low-level concepts of the problem domain. However, there exists a graphical model called a Markov random field (MRF) <ref> [27, 11, 70, 40] </ref>, which can be used as a formal tool for constructing Gibbs distributions. Similarly to Bayesian networks, an MRF representation con 5 sists of a graphical representation of the dependencies between the variables, together with a set of parameters, clique potentials. <p> For each variable U i , we define a set of neighbors G i : G i = fU j j (U i ; U j ) 2 Gg: Using the concept of neighborhood we can now give the definition for a Markov Random Field (MRF) <ref> [27, 11, 70, 40] </ref>: Definition 3.8 (Markov Random Field) A family of random variables U is a Markov Random Field with respect to a relation G and a probability distribution P, if 1.
Reference: [12] <author> Bilbro, G., Mann, R., and Miller, T. </author> <title> Optimization by mean field annealing. </title> <booktitle> In Advances in Neural Information Processing Systems I, </booktitle> <editor> D. Touretzky, Ed. </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1989, </year> <pages> pp. 91-98. </pages>
Reference-contexts: For the same reason, we did not experiment with the more complex cooling schedules listed in [73, 1], or with any of the elaborate techniques for speeding up the convergence speed of the simulated annealing process <ref> [97, 12, 3, 7, 100, 63] </ref>, but assumed that the proportional speedup gained from using any of these methods would be roughly equal with both SSA and BMSA. 76 Empirical results 6.3 Results As noted earlier, the primary objective of the experiments was to study the behavior of the SSA and
Reference: [13] <author> Booker, L., Hota, N., and Ramsey, C. Bart: </author> <title> A Bayesian reasoning tool for knowledge based systems. </title> <editor> In Henrion et al. </editor> <volume> [53], </volume> <pages> pp. 271-282. </pages>
Reference-contexts: Consequently, most existing systems for Bayesian reasoning first transform a given multi-connected BN structure to a singly-connected network, and then use the existing polynomial-time algorithms (see e.g. <ref> [4, 13] </ref>). This transformation can be done either explicitly by clustering several nodes together as in [76, 106] or [96, Ch.4.4.1], or implicitly by blocking multiply connected paths by conditioning a set of variables as in [96, Ch.4.4.2] (for a discussion of different transformation techniques, see [107]).
Reference: [14] <author> Brown, D., and Huntley, C. </author> <title> A practical application of simulated annealing to clustering. </title> <journal> Pattern Recognition 25, </journal> <volume> 4 (1992), </volume> <pages> 401-412. </pages>
Reference-contexts: After being introduced to the optimization theory community in [71], SA has been applied to many different (NP-hard) optimization problems, such as TSP [71], graph partitioning [65], graph coloring [66], number set partitioning [66] and clustering <ref> [105, 14] </ref> (for an extensive survey of applications, see [73] or [1, pp. 89-90]). One of the main difficulties with the SA algorithm is that the SA theory requires that the objective function to be optimized has to be representable in a specific Gibbs distribution form.
Reference: [15] <author> Buchanan, B., and Shortliffe, E. </author> <title> Rule-Based Expert Systems. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1984. </year>
Reference-contexts: Several different frameworks for handling noisy data have been developed during the last two decades. In this work, we concentrate on numerical approaches for uncertain reasoning. The first systems of this kind, the most famous example being the MYCIN <ref> [15] </ref> system for diagnosing bacterial infections, used uncertainty factors and heuristic rules for manipulating these factors.
Reference: [16] <author> Cheeseman, P. </author> <title> Probabilistic versus fuzzy reasoning. </title> <booktitle> In Kanal and Lemmer [68], </booktitle> <pages> pp. 85-102. </pages>
Reference-contexts: However, it has been shown that any consistent computational 2 Introduction framework representing degree of uncertainty as numbers has to be based on axioms of probability theory [20, 77]. Consequently, Bayesian probability theory seems to offer a solid, unifying framework for uncertain reasoning in general <ref> [16, 37, 64] </ref>. In this work, we study uncertain reasoning in the probabilistic framework.
Reference: [17] <author> Cheeseman, P., Kelly, J., Self, M., Stutz, J., Taylor, W., and Freeman, D. </author> <title> Autoclass: A Bayesian classification system. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning (Ann Arbor, </booktitle> <month> June </month> <year> 1988), </year> <pages> pp. 54-64. </pages>
Reference-contexts: Alternatively, clustering can also be done by introducing new latent variables which represent separate clusters of original variables <ref> [17, 88] </ref>. The latent variable model offers also an interesting opportunity for constructing Bayesian networks from data using unsupervised learning algorithms.
Reference: [18] <author> Chin, H., and Cooper, G. </author> <title> Bayesian belief network inference using simulation. </title> <booktitle> In Uncertainty in Artificial Intelligence 3, </booktitle> <editor> L. Kanal and J. Lemmer, Eds. </editor> <publisher> Elsevier Science Publishers B.V. (North-Holland), </publisher> <address> Amsterdam, </address> <year> 1989, </year> <pages> pp. 129-147. </pages>
Reference-contexts: As * approaches zero, the binary-variable Bayesian network approximates the probability distribution of the original network more and more accurately. However, it has been suggested <ref> [18] </ref> that Bayesian networks containing this kind of extreme probabilities are the most difficult ones to approach by stochastic simulation methods. <p> The size of the configuration space was increased by allowing more variables in the Bayesian networks, and allowing the variables to have more values. It has been noted <ref> [18] </ref> that solving the EVE problem can become very difficult if the Bayesian network contains a lot of extreme probabilities (probabilities with values near zero or one). However, as already noted in [81], in our MAP problem framework this does not seem to be true.
Reference: [19] <author> Cooper, G. </author> <title> The computational complexity of probabilistic inference using Bayesian belief networks. </title> <booktitle> Artificial Intelligence 42, </booktitle> <month> 2-3 (March </month> <year> 1990), </year> <pages> 393-405. </pages>
Reference-contexts: In Section 3.2, we discuss the complexity of the MAP problem within the Bayesian network framework, and review briefly some attempts to develop algorithms for solving MAP problems in the Bayesian network framework. Unfortunately, for a general network structure, the MAP problem can be shown to be NP-hard <ref> [19, 112] </ref>, which means that very probably it is not possible for any algorithm to solve this task (in the worst case) 4 Introduction in polynomial time with respect to the size of the network. <p> It has been shown that if the probability distribution is represented as a Bayesian network, both the EVE task <ref> [19] </ref> and the MAP problem [112] belong to the class of NP-hard problems with respect to the size of the corresponding network. These results mean that it is very unlikely that a polynomial-time algorithm for solving the MAP problem for Bayesian networks could exist. 1 .
Reference: [20] <author> Cox, R. </author> <title> Of inference and inquiry | an essay in inductive logic. In Maximum Entropy Formalism, </title> <editor> Levine and Tribus, Eds. </editor> <publisher> MIT Press, </publisher> <year> 1979. </year> <note> BIBLIOGRAPHY 85 </note>
Reference-contexts: Recently, uncertain reasoning systems based on fuzzy logic [119] have gained popularity, especially in Japan [72, 94]. However, it has been shown that any consistent computational 2 Introduction framework representing degree of uncertainty as numbers has to be based on axioms of probability theory <ref> [20, 77] </ref>. Consequently, Bayesian probability theory seems to offer a solid, unifying framework for uncertain reasoning in general [16, 37, 64]. In this work, we study uncertain reasoning in the probabilistic framework.
Reference: [21] <author> Csiszar, I., and Tusnady, G. </author> <title> Information geometry and alternating minimization procedures. </title> <journal> Statistics & Decisions Supplement Issue No. </journal> <volume> 1 (1984), </volume> <pages> 205-237. </pages>
Reference-contexts: In this respect, Gibbs sampling can be regarded as a stochastic version of a local search algorithm belonging to the very general family of expectation maximization (EM) [26] (or, as they are also called, alternating minimization <ref> [21] </ref>) algorithms. A Gibbs sampling process can be made much faster by parallelizing the state generation-acceptance process.
Reference: [22] <author> Dagum, P., and Horvitz, E. </author> <title> A Bayesian analysis of simulation algorithms for inference in belief networks. </title> <booktitle> Networks 23 (1993), </booktitle> <pages> 499-516. </pages>
Reference-contexts: However, very little is known about the quality of 22 Solving the MAP problem by Bayesian networks the solutions produced by the Markov chain models, if a limited amount of time is available, but some analyses of the convergence speed can be found in <ref> [32, 1, 113, 101, 98, 22] </ref>. In particular, for Bayesian networks restricted by their dependence value (see above), there exists a stochastic polynomial-time algorithm which solves the EVE problem in the PAC sense [24].
Reference: [23] <author> Dagum, P., and Luby, M. </author> <title> Approximating probabilistic inference in Bayesian belief networks is NP-hard. </title> <booktitle> Artificial Intelligence 60 (1993), </booktitle> <pages> 141-153. </pages>
Reference-contexts: These results mean that it is very unlikely that a polynomial-time algorithm for solving the MAP problem for Bayesian networks could exist. 1 . Even finding an approximation of the EVE solution (in any constant degree of accuracy) is an NP-hard problem <ref> [23, 102] </ref>. As the answer to a MAP problem is a configuration vector, and not a probability, it is not obvious what an approximative solution in this case means.
Reference: [24] <author> Dagum, P., and Shavez, M. </author> <title> Approximating probabilistic inference in Bayesian networks. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 15, </journal> <month> 3 (March </month> <year> 1993), </year> <pages> 246-255. </pages> <note> [25] de Gloria, </note> <author> A., Faraboschi, P., and Olivieri, M. </author> <title> Clustered Boltzmann machines: Massively parallel architectures for constrained optimization problems. </title> <booktitle> Parallel Computing (1993), </booktitle> <pages> 163-175. </pages>
Reference-contexts: For networks with less restricted conditional probabilities | networks with probabilities bounded by their dependence value, which is, intuitively speaking, a measure of how much the probabilities differ from uniform probabilities | the Bayesian reasoning tasks stay NP-hard <ref> [24] </ref>. <p> In particular, for Bayesian networks restricted by their dependence value (see above), there exists a stochastic polynomial-time algorithm which solves the EVE problem in the PAC sense <ref> [24] </ref>. Unfortunately, as the above mentioned convergence results deal mainly with the EVE problem, the analysis of the probabilistic solution for the MAP task is still very much an open problem.
Reference: [26] <author> Dempster, A., Laird, N., and Rubin, D. </author> <title> Maximum likelihood from incomplete data via the em algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B 39, </journal> <volume> 1 (1977), </volume> <pages> 1-38. </pages>
Reference-contexts: In this respect, Gibbs sampling can be regarded as a stochastic version of a local search algorithm belonging to the very general family of expectation maximization (EM) <ref> [26] </ref> (or, as they are also called, alternating minimization [21]) algorithms. A Gibbs sampling process can be made much faster by parallelizing the state generation-acceptance process.
Reference: [27] <author> Dobrushin, R. </author> <title> The description of a random field by means of conditional probabilities and conditions of its regularity. </title> <journal> Theory Prob. Appl. </journal> <volume> 13 (1968), </volume> <pages> 197-224. </pages>
Reference-contexts: Usually, a suitable Gibbs distribution has to be constructed manually for each separate optimization problem instance using very low-level concepts of the problem domain. However, there exists a graphical model called a Markov random field (MRF) <ref> [27, 11, 70, 40] </ref>, which can be used as a formal tool for constructing Gibbs distributions. Similarly to Bayesian networks, an MRF representation con 5 sists of a graphical representation of the dependencies between the variables, together with a set of parameters, clique potentials. <p> For each variable U i , we define a set of neighbors G i : G i = fU j j (U i ; U j ) 2 Gg: Using the concept of neighborhood we can now give the definition for a Markov Random Field (MRF) <ref> [27, 11, 70, 40] </ref>: Definition 3.8 (Markov Random Field) A family of random variables U is a Markov Random Field with respect to a relation G and a probability distribution P, if 1.
Reference: [28] <author> Doorenbos, R. </author> <title> Matching 100,000 learned rules. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence (Washington, </booktitle> <address> DC, July 1993), </address> <publisher> AAAI Press/MIT, </publisher> <address> Menlo Park, CA, </address> <pages> pp. 290-296. </pages>
Reference-contexts: the brittleness of the resulting software: as the programs are sensitive even to the slightest inaccuracy or incompleteness in their input data [2], the systems tend to grow to have rule bases consisting of tens of thousands of rules, when trying to provide a specific rule for every possible situation <ref> [28] </ref>. Collecting these kinds of large rule bases can be a very expensive and time-consuming task in practice, and moreover, maintaining and updating large rule bases (while preserving consistency) appears to be extremely difficult [93].
Reference: [29] <author> Druzdzel, M., and Henrion, M. </author> <title> Efficient reasoning in qualitative probabilistic networks. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence (Washington, </booktitle> <address> DC, July 1993), </address> <publisher> AAAI Press/MIT, </publisher> <address> Menlo Park, CA, </address> <pages> pp. 548-553. </pages>
Reference-contexts: In another approach to Bayesian reasoning the structure of the network is not changed, but the quantitative conditional probabilities are replaced by a finite set of qualitative measures of uncertainty [117]. It seems that Bayesian reasoning can in this case be done in polynomial time <ref> [29] </ref>, but it is not yet clear what is the expressive power of this kind of qualitative Bayesian networks.
Reference: [30] <author> Duda, R., Hart, P., and Nilsson, N. </author> <title> Subjective Bayesian methods for rule-based inference systems. </title> <type> Tech. Rep. TR 124, </type> <institution> Stanford Research Institute, </institution> <address> Menlo Park, CA, </address> <year> 1976. </year>
Reference-contexts: It has been later discovered [48] that MYCIN's original heuristic computational scheme can actually be interpreted as (Bayesian) probabilistic reasoning with certain independence assumptions, similar to those used for constructing the first simple Bayesian schemes for uncertain reasoning, such as the PROSPECTOR <ref> [30] </ref> system. Unfortunately, in many problem domains these independence assumptions are not valid [74]. Recently, uncertain reasoning systems based on fuzzy logic [119] have gained popularity, especially in Japan [72, 94].
Reference: [31] <author> Eizirik, L., Barbosa, V., and Mendes, S. </author> <title> A Bayesian-network approach to lexical disambiguation. </title> <booktitle> Cognitive Science 17, 2 (April-June 1993), </booktitle> <pages> 257-283. 86 BIBLIOGRAPHY </pages>
Reference-contexts: The values of the binary variables C i are denoted by c i1 and c i2 , i = 1; 2; 3. 5.3 Coping with multi-valued variables 63 0, since in this case the resulting probability distribution may not approximate the original distribution very well <ref> [31] </ref>. Nevertheless, in [81] we experimented with harmony networks using the transformation scheme suggested above, and got relatively good results even with extreme probabilities. Another problem with the Bayesian network level transformation is the large number of resulting conditional probabilities, as can be noted in Figure 5.2.
Reference: [32] <author> Faigle, U., and Schrader, R. </author> <title> On the convergence of stationary distributions in simulated annealing algorithms. </title> <booktitle> Information Processing Letters 27 (1988), </booktitle> <pages> 189-194. </pages>
Reference-contexts: However, very little is known about the quality of 22 Solving the MAP problem by Bayesian networks the solutions produced by the Markov chain models, if a limited amount of time is available, but some analyses of the convergence speed can be found in <ref> [32, 1, 113, 101, 98, 22] </ref>. In particular, for Bayesian networks restricted by their dependence value (see above), there exists a stochastic polynomial-time algorithm which solves the EVE problem in the PAC sense [24].
Reference: [33] <author> Flor een, P., Myllym aki, P., Orponen, P., and Tirri, H. </author> <title> Neural representation of concepts for robust inference. </title> <booktitle> In Proceedings of the International Symposium Computational Intelligence II (Milano, </booktitle> <address> Italy, </address> <month> September </month> <year> 1989), </year> <editor> F. Gardin and G. Mauri, Eds., </editor> <publisher> Elsevier Science Publishers B.V. (North-Holland), </publisher> <pages> pp. 89-98. </pages>
Reference-contexts: First of all, the mathematical model behind the system is the theoretically sound framework of Bayesian reasoning, compared to the more or less heuristic models of most other hybrid systems (for our earlier, heuristic attempts towards a hybrid system, see <ref> [33, 89, 34] </ref>). Secondly, although some hybrid models provide theoretical justifications for the computations (see e.g.
Reference: [34] <author> Flor een, P., Myllym aki, P., Orponen, P., and Tirri, H. </author> <title> Compiling object declarations into connectionist networks. </title> <journal> AI Communications 3, </journal> <month> 4 (December </month> <year> 1990), </year> <pages> 172-183. </pages>
Reference-contexts: First of all, the mathematical model behind the system is the theoretically sound framework of Bayesian reasoning, compared to the more or less heuristic models of most other hybrid systems (for our earlier, heuristic attempts towards a hybrid system, see <ref> [33, 89, 34] </ref>). Secondly, although some hybrid models provide theoretical justifications for the computations (see e.g.
Reference: [35] <author> Freund, Y., and Haussler, D. </author> <title> Unsupervised learning of distributions on binary vectors using two layer networks. </title> <booktitle> In Neural Information Processing Systems 4, </booktitle> <editor> J. Moody, S. Hanson, and R. Lippmann, Eds. </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1992, </year> <pages> pp. 912-919. </pages>
Reference-contexts: the connec 7 Sample data BM learning algorithm Boltzmann machine Consensus function BM updating process Partial value assignment Complete MAP value assignment tions, and then using some gradient-based greedy algorithm for changing the weights until the behavior of the network seems to be consistent with a sample of training data <ref> [58, 35, 36] </ref>. <p> The resulting neural network could also be used as a (cleverly chosen) initial starting point to some of the existing learning algorithms <ref> [58, 35, 36, 55] </ref> for Boltzmann machines, in which case the learning problem should become much easier than with a randomly chosen initial state.
Reference: [36] <author> Galland, C. </author> <title> Learning in Deterministic Boltzmann Machine Networks. </title> <type> PhD thesis, </type> <institution> Department of Physics, University of Toronto, </institution> <year> 1992. </year>
Reference-contexts: the connec 7 Sample data BM learning algorithm Boltzmann machine Consensus function BM updating process Partial value assignment Complete MAP value assignment tions, and then using some gradient-based greedy algorithm for changing the weights until the behavior of the network seems to be consistent with a sample of training data <ref> [58, 35, 36] </ref>. <p> The resulting neural network could also be used as a (cleverly chosen) initial starting point to some of the existing learning algorithms <ref> [58, 35, 36, 55] </ref> for Boltzmann machines, in which case the learning problem should become much easier than with a randomly chosen initial state.
Reference: [37] <editor> G ardenfors, P., and Sahlin, N.-E., Eds. </editor> <title> Decision, Probability, and Utility. </title> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: However, it has been shown that any consistent computational 2 Introduction framework representing degree of uncertainty as numbers has to be based on axioms of probability theory [20, 77]. Consequently, Bayesian probability theory seems to offer a solid, unifying framework for uncertain reasoning in general <ref> [16, 37, 64] </ref>. In this work, we study uncertain reasoning in the probabilistic framework.
Reference: [38] <author> Garey, M., and Johnson, D. </author> <title> Computers and Intractability: A Guide to the Theory of NP-completeness. </title> <address> W.H.Freeman, New York, NY, </address> <year> 1979. </year>
Reference-contexts: As finding an MAP configuration gives directly the answer to this decision problem, the MAP problem is said to be NP-hard. (For a good introduction to the theory of complexity classes, see <ref> [38] </ref>.) 3.2 Complexity of the MAP problem 21 oning (meaning here both the EVE and MAP tasks) can be done in polynomial time [96, 92].
Reference: [39] <author> Geffner, H., and Pearl, J. </author> <title> On the probabilistic semantics of connectionist networks. </title> <type> Tech. Rep. </type> <institution> R-84, UCLA Computer Science Department, </institution> <address> Los Angeles, CA, </address> <year> 1987. </year>
Reference-contexts: Although in some restricted domains this kind of a transformation is fairly straightforward to construct <ref> [57, 39, 75, 62, 90] </ref>, the methods presented do not apply to general Bayesian network structures. <p> model is defined by using a sum of parameters depending on only two variables, a BM network with a consensus function equal to the potential function of a given MRF can be easily constructed by assigning one node in the BM network to each random variable 53 in the MRF <ref> [57, 39] </ref>. However, in this case the parallelism of the resulting BM is lost, as no two adjacent nodes (variables) can be updated at the same time. <p> As a matter of fact, this kind of a generalization of the basic BM model, containing also higher-order hyper-arcs, has been suggested earlier <ref> [39] </ref>. However, as in this study we do not allow such extensions of the basic neural network models, this approach is not examined here.
Reference: [40] <author> Geman, S., and Geman, D. </author> <title> Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 6 (1984), </journal> <pages> 721-741. </pages>
Reference-contexts: Markov Chain Monte Carlo (MCMC) algorithms [80, 45] are stochastic algorithms that can be used in exponentially large search spaces for finding global maxima in feasible time with high probability. In this study, we are mainly concerned with the most common of the MCMC methods, Gibbs sampling <ref> [40] </ref> (for a short survey of other stochastic simulation methods, see [52]). In Gibbs sampling, a large collection of representative configurations of the problem domain model is generated by iterative local sampling, and the actual variable-value combination occurrence probabilities can be estimated by sample frequencies. <p> This iterative process can be seen as a kind of a stochastic local search, where the probability of finding the globally optimal solution approaches one as the number of iterative steps used approaches infinity <ref> [40, 1] </ref>. Consequently, simulated annealing can be used for solving the MAP problem stochastically. In Section 3.3 we present the general theoretical framework for MCMC algorithms and the simulated annealing technique. <p> Usually, a suitable Gibbs distribution has to be constructed manually for each separate optimization problem instance using very low-level concepts of the problem domain. However, there exists a graphical model called a Markov random field (MRF) <ref> [27, 11, 70, 40] </ref>, which can be used as a formal tool for constructing Gibbs distributions. Similarly to Bayesian networks, an MRF representation con 5 sists of a graphical representation of the dependencies between the variables, together with a set of parameters, clique potentials. <p> Simulated annealing is also widely used in image processing, in particular due to the pioneering theoretical work done by Geman & Geman (see <ref> [40] </ref>). In Section 3.4 we show how to represent a Bayesian network probability distribution in the Gibbs distribution form, which allows us to use simulated annealing for finding MAP configurations on Bayesian networks. A generic simulated annealing algorithm can be defined as follows: Algorithm 3.2 Simulated annealing (SA) . <p> T (t) ! 0 as t ! 1. Now the convergence result of Theorem 3.9 applies. Proof: See <ref> [40] </ref>. From the implementational point of view, Theorem 3.10 has two major drawbacks. First of all, although the number of steps required in the theorem is not infinite, it is still exponential with respect to time t and hence impractical for most applications. <p> For each variable U i , we define a set of neighbors G i : G i = fU j j (U i ; U j ) 2 Gg: Using the concept of neighborhood we can now give the definition for a Markov Random Field (MRF) <ref> [27, 11, 70, 40] </ref>: Definition 3.8 (Markov Random Field) A family of random variables U is a Markov Random Field with respect to a relation G and a probability distribution P, if 1. <p> Proof: See <ref> [40] </ref>. fl Consequently, any Gibbs distribution can be defined by using a graphical Markov random field representation, and correspondingly, for each MRF there exists a corresponding Gibbs distribution (and a corresponding Gibbs sampler).
Reference: [41] <editor> Goonatilake, S., and Khebbal, S., Eds. </editor> <title> Intelligent Hybrid Systems. </title> <publisher> John Wiley & Sons, </publisher> <address> Chichester, </address> <year> 1995. </year>
Reference-contexts: Naturally, the resulting neural network could also be regarded as a cleverly chosen starting point to some learning algorithm, but this interesting idea is not studied here further. Compared to other neural-symbolic hybrid systems (see e.g. <ref> [9, 56, 41, 116] </ref>), the Bayesian-neural hybrid system suggested here has two clear advantages.
Reference: [42] <author> Greening, D. </author> <title> Parallel simulated annealing techniques. </title> <journal> Physica D 42 (1990), </journal> <pages> 293-306. </pages>
Reference-contexts: It should also be noted that the methods developed here apply only for massively parallel neural network implementations | parallelization of simulated annealing using more conventional computing architectures is discussed in <ref> [42] </ref>. In Chapter 4 we saw how the two stochastic neural network models, BM and HBM, can be regarded as massively parallel implementations of simulated annealing, being capable of finding the optimum of the given objective function.
Reference: [43] <author> Haario, H., and Saksman, E. </author> <title> Simulated annealing process in general state space. </title> <journal> Adv. Appl. Prob. </journal> <volume> 23 (1991), </volume> <pages> 866-893. BIBLIOGRAPHY 87 </pages>
Reference-contexts: variables Y i 2 U X , we can write f ~ X = ~xg = f X i 2U X ^ X i = x i g; 1 We shall henceforth restrict ourselves to discrete variables, although this restriction is not necessary for the simulated annealing method in general <ref> [43] </ref>. 12 The MAP problem or more briefly as f ~ U X ; ~xg. Let P denote a probability measure defined on . The triple (; F; P) now defines a joint probability distribution on our variable base U .
Reference: [44] <author> Hajek, B. </author> <title> Cooling schedules for optimal annealing. </title> <booktitle> Mathematics of Operations Research 13 (1988), </booktitle> <pages> 311-329. </pages>
Reference-contexts: However, with certain additional constraints to the problem it can be shown that exponential time is not only sufficient, but also necessary <ref> [44] </ref>. In the light of the complexity of the MAP problem (see Section 3.2), it seems 3.3 Simulated annealing 33 probable that a similar kind of result could be proven for the general case also, although this has not been done.
Reference: [45] <author> Hastings, W. </author> <title> Monte Carlo sampling methods using Markov chains and their applications. </title> <booktitle> Biometrika 57 (1970), </booktitle> <pages> 97-109. </pages>
Reference-contexts: Markov Chain Monte Carlo (MCMC) algorithms <ref> [80, 45] </ref> are stochastic algorithms that can be used in exponentially large search spaces for finding global maxima in feasible time with high probability.
Reference: [46] <author> Haykin, S. </author> <title> Neural Networks: A Comprehensive Foundation. </title> <publisher> IEEE Press/Macmillan College Publishing Company, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: For a good introduction to neural 41 0.5 -10 -5 0 5 10 x fi = 1 fi = 10 fi = 0:1 computing, see e.g. the collections [5, 6, 103, 78], or the books <ref> [46, 54, 47] </ref>.
Reference: [47] <author> Hecht-Nielsen, R. </author> <title> Neurocomputing. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1990. </year>
Reference-contexts: each processing element can be defined arbitrarily with the restriction that it must be completely local; that is, it must depend only on the current values of the input signals arriving at the processing element via impinging connections and on values stored in the processing element's local memory." Robert Hecht-Nielsen <ref> [47] </ref>. The processing elements (units, nodes or "neurons") of a neural network are usually based on a simple artificial neuron model, inspired originally [79] by the structure of a real biological neuron. <p> One of the main reasons for using the sigmoid function is the fact that it has a very simple derivative, f 0 (x) = f (x)(1 f (x)), and besides, efficient implementations of sigmoid function elements have been developed for analogical and digital circuits <ref> [47, Ch. 8] </ref>,[51, Ch. 3]. The interconnected nodes form a network structure, a neural network. Neural network architectures can be divided into two main categories: feed-forward models and feedback models. <p> For a good introduction to neural 41 0.5 -10 -5 0 5 10 x fi = 1 fi = 10 fi = 0:1 computing, see e.g. the collections [5, 6, 103, 78], or the books <ref> [46, 54, 47] </ref>.
Reference: [48] <author> Heckerman, D. </author> <title> Probabilistic interpretation for MYCIN's certainty factors. </title> <booktitle> In Kanal and Lemmer [68], </booktitle> <pages> pp. 167-196. </pages>
Reference-contexts: In this work, we concentrate on numerical approaches for uncertain reasoning. The first systems of this kind, the most famous example being the MYCIN [15] system for diagnosing bacterial infections, used uncertainty factors and heuristic rules for manipulating these factors. It has been later discovered <ref> [48] </ref> that MYCIN's original heuristic computational scheme can actually be interpreted as (Bayesian) probabilistic reasoning with certain independence assumptions, similar to those used for constructing the first simple Bayesian schemes for uncertain reasoning, such as the PROSPECTOR [30] system.
Reference: [49] <author> Heckerman, D., Geiger, D., and Chickering, D. </author> <title> Learning Bayesian networks: The combination of knowledge and statistical data. </title> <booktitle> Machine Learning 20, </booktitle> <month> 3 (September </month> <year> 1995), </year> <pages> 197-243. </pages>
Reference-contexts: There exists also several interesting approaches for constructing Bayesian networks from sample data, and moreover, theoretically solid techniques for combining domain expert knowledge with the machine learning approach <ref> [49] </ref>. The Bayesian network theory offers a framework for constructing algorithms for different probabilistic reasoning tasks (for a survey of probabilistic reasoning algorithms, see [52]).
Reference: [50] <author> Heckerman, D., and Shwe, M. </author> <title> Diagnosis of multiple faults: A sensitivity analysis. </title> <booktitle> In Uncertainty in Artificial Intelligence 9, </booktitle> <editor> D. Hecker-man and A. Mamdani, Eds. </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1993, </year> <pages> pp. 80-87. </pages>
Reference-contexts: As a result, the noisy-OR model is usually not accurate enough for practical applications <ref> [50] </ref>. For an accurate representation for general BN structures, it seems at a first glance that binary connections are not sufficient, but higher-order connections (arcs connecting three or more nodes to each other) are needed.
Reference: [51] <author> Heem, J. </author> <title> Neurocomputers for Brain-Style Processing. Design, Implementation and Application. </title> <type> PhD thesis, </type> <institution> Leiden University, Unit of Experimental and Theoretical Psychology, </institution> <year> 1995. </year> <note> Updated version of Chapter 3 can be found in "ftp.mrc-apu.cam.ac.uk/pub/nn/neurhard.ps". </note>
Reference-contexts: Nevertheless, several massively parallel platforms for neural computing already exists: a recent review <ref> [51, Ch. 3] </ref> lists 9 neural network accelerator boards, 13 neurocomputers with general purpose processors, and 20 neurocomputers built from neuro-chips. Although not all of these are commercially available at the moment, it is already possible to buy hardware with quite an impressive performance for NN applications.
Reference: [52] <author> Henrion, M. </author> <title> An introduction to algorithms for inference in belief nets. </title> <editor> In Henrion et al. </editor> <volume> [53], </volume> <pages> pp. 129-138. </pages>
Reference-contexts: The Bayesian network theory offers a framework for constructing algorithms for different probabilistic reasoning tasks (for a survey of probabilistic reasoning algorithms, see <ref> [52] </ref>). In Section 3.2, we discuss the complexity of the MAP problem within the Bayesian network framework, and review briefly some attempts to develop algorithms for solving MAP problems in the Bayesian network framework. <p> In this study, we are mainly concerned with the most common of the MCMC methods, Gibbs sampling [40] (for a short survey of other stochastic simulation methods, see <ref> [52] </ref>). In Gibbs sampling, a large collection of representative configurations of the problem domain model is generated by iterative local sampling, and the actual variable-value combination occurrence probabilities can be estimated by sample frequencies.
Reference: [53] <editor> Henrion, M., Shachter, R., Kanal, L., and Lemmer, J., Eds. </editor> <booktitle> Uncertainty in Artificial Intelligence 5. </booktitle> <publisher> Elsevier Science Publishers B.V. (North-Holland), </publisher> <address> Amsterdam, </address> <year> 1990. </year>
Reference: [54] <author> Hertz, J., Krogh, A., and Palmer, R. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Redwood City, CA, </address> <year> 1991. </year>
Reference-contexts: For a good introduction to neural 41 0.5 -10 -5 0 5 10 x fi = 1 fi = 10 fi = 0:1 computing, see e.g. the collections [5, 6, 103, 78], or the books <ref> [46, 54, 47] </ref>.
Reference: [55] <author> Hinton, G. </author> <title> Connectionist learning procedures. </title> <booktitle> Artificial Intelligence 40, </booktitle> <month> 1-3 (September </month> <year> 1989). </year> <note> 88 BIBLIOGRAPHY </note>
Reference-contexts: The resulting neural network could also be used as a (cleverly chosen) initial starting point to some of the existing learning algorithms <ref> [58, 35, 36, 55] </ref> for Boltzmann machines, in which case the learning problem should become much easier than with a randomly chosen initial state.
Reference: [56] <author> Hinton, G. </author> <title> Special issue on connectionist symbol processing. </title> <booktitle> Artificial Intelligence 46, </booktitle> <month> 1-2 </month> <year> (1990). </year>
Reference-contexts: Naturally, the resulting neural network could also be regarded as a cleverly chosen starting point to some learning algorithm, but this interesting idea is not studied here further. Compared to other neural-symbolic hybrid systems (see e.g. <ref> [9, 56, 41, 116] </ref>), the Bayesian-neural hybrid system suggested here has two clear advantages.
Reference: [57] <author> Hinton, G., and Sejnowski, T. </author> <title> Optimal perceptual inference. </title> <booktitle> In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Washington DC, </booktitle> <address> June 1983), </address> <publisher> IEEE, </publisher> <address> New York, NY, </address> <pages> pp. 448-453. </pages>
Reference-contexts: Especially suitable for these tasks are stochastic neural network architectures, as these models are based on a stochastic updating process very similar to simulated annealing. In Section 4.1 we describe such a network architecture, called the Boltzmann machine (BM) <ref> [57, 58] </ref>, and show how the updating process of a BM provably converges to a state which maximizes the consensus function, which is an objective function determined by 6 Introduction Sample data Expert knowledge Bayesian network theory Bayesian network MRF representation Gibbs distribution Simulated annealing process Partial value assignment Complete MAP <p> Although in some restricted domains this kind of a transformation is fairly straightforward to construct <ref> [57, 39, 75, 62, 90] </ref>, the methods presented do not apply to general Bayesian network structures. <p> In the following, we are mainly concerned with one of the most common of the feedback models, the Boltzmann machine <ref> [57, 58] </ref> neural network architecture, and its variants. 42 Solving the MAP problem by stochastic neural networks 4.1 Boltzmann machines A Boltzmann machine (BM) [57, 58] is a neural network consisting of a set of binary nodes fS 1 ; : : : ; S n g, where state s i <p> In the following, we are mainly concerned with one of the most common of the feedback models, the Boltzmann machine <ref> [57, 58] </ref> neural network architecture, and its variants. 42 Solving the MAP problem by stochastic neural networks 4.1 Boltzmann machines A Boltzmann machine (BM) [57, 58] is a neural network consisting of a set of binary nodes fS 1 ; : : : ; S n g, where state s i of node S i is either 1 ("on"), or 0 ("off"). <p> model is defined by using a sum of parameters depending on only two variables, a BM network with a consensus function equal to the potential function of a given MRF can be easily constructed by assigning one node in the BM network to each random variable 53 in the MRF <ref> [57, 39] </ref>. However, in this case the parallelism of the resulting BM is lost, as no two adjacent nodes (variables) can be updated at the same time.
Reference: [58] <author> Hinton, G., and Sejnowski, T. </author> <title> Learning and relearning in Boltzmann machines. </title> <booktitle> In Rumelhart and McClelland [103], </booktitle> <pages> pp. 282-317. </pages>
Reference-contexts: Especially suitable for these tasks are stochastic neural network architectures, as these models are based on a stochastic updating process very similar to simulated annealing. In Section 4.1 we describe such a network architecture, called the Boltzmann machine (BM) <ref> [57, 58] </ref>, and show how the updating process of a BM provably converges to a state which maximizes the consensus function, which is an objective function determined by 6 Introduction Sample data Expert knowledge Bayesian network theory Bayesian network MRF representation Gibbs distribution Simulated annealing process Partial value assignment Complete MAP <p> the connec 7 Sample data BM learning algorithm Boltzmann machine Consensus function BM updating process Partial value assignment Complete MAP value assignment tions, and then using some gradient-based greedy algorithm for changing the weights until the behavior of the network seems to be consistent with a sample of training data <ref> [58, 35, 36] </ref>. <p> In the following, we are mainly concerned with one of the most common of the feedback models, the Boltzmann machine <ref> [57, 58] </ref> neural network architecture, and its variants. 42 Solving the MAP problem by stochastic neural networks 4.1 Boltzmann machines A Boltzmann machine (BM) [57, 58] is a neural network consisting of a set of binary nodes fS 1 ; : : : ; S n g, where state s i <p> In the following, we are mainly concerned with one of the most common of the feedback models, the Boltzmann machine <ref> [57, 58] </ref> neural network architecture, and its variants. 42 Solving the MAP problem by stochastic neural networks 4.1 Boltzmann machines A Boltzmann machine (BM) [57, 58] is a neural network consisting of a set of binary nodes fS 1 ; : : : ; S n g, where state s i of node S i is either 1 ("on"), or 0 ("off"). <p> The resulting neural network could also be used as a (cleverly chosen) initial starting point to some of the existing learning algorithms <ref> [58, 35, 36, 55] </ref> for Boltzmann machines, in which case the learning problem should become much easier than with a randomly chosen initial state.
Reference: [59] <author> Hopfield, J., and Tank, D. </author> <title> Neural computation of decisions in optimization problems. </title> <booktitle> Biological Cybernetics 52 (1985), </booktitle> <pages> 141-152. </pages>
Reference-contexts: These models can perform certain computational tasks extremely fast when run on customized parallel hardware, and hence they have been suggested as a computationally efficient tool for solving NP-hard optimization problems ap-proximatively <ref> [59, 10] </ref>. Especially suitable for these tasks are stochastic neural network architectures, as these models are based on a stochastic updating process very similar to simulated annealing.
Reference: [60] <author> Howard, R., and Matheson, J. </author> <title> Influence diagrams. In Readings in Decision Analysis, </title> <editor> R.A.Howard and J.E.Matheson, Eds. </editor> <title> Strategic Decisions Group, </title> <address> Menlo Park, CA, </address> <year> 1984, </year> <pages> pp. 763-771. </pages>
Reference-contexts: In the field of decision theory, a model similar to Bayesian networks is known as influence diagrams <ref> [60] </ref>. Thorough introductions to the Bayesian network theory can be found in [96, 92]. The importance of a Bayesian network structure lies in the way the network facilitates computing conditional probabilities.
Reference: [61] <author> Hrycej, T. </author> <title> Gibbs sampling in Bayesian networks. </title> <booktitle> Artificial Intelligence 46 (1990), </booktitle> <pages> 351-363. </pages>
Reference-contexts: As noted earlier by several authors <ref> [61, 75, 76, 115] </ref>, we can construct for any given Bayesian network a corresponding MRF having the same probability distribution, by using a transformation called moralization of a Bayesian network.
Reference: [62] <author> Hrycej, T. </author> <title> Common features of neural-network models of high and low level human information processing. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks (ICANN-91) (Es-poo, </booktitle> <address> Finland, </address> <month> June </month> <year> 1991), </year> <editor> T. Kohonen, K. Makisara, O. Simula, , and J. Kangas, Eds., </editor> <publisher> Elsevier Science Publishers B.V. (North-Holland), </publisher> <pages> pp. 861-866. </pages>
Reference-contexts: Although in some restricted domains this kind of a transformation is fairly straightforward to construct <ref> [57, 39, 75, 62, 90] </ref>, the methods presented do not apply to general Bayesian network structures.
Reference: [63] <author> Ingber, L. </author> <title> Very fast simulated re-annealing. </title> <booktitle> Mathematical and Computer Modelling 8, 12 (1989), </booktitle> <pages> 967-973. </pages>
Reference-contexts: For the same reason, we did not experiment with the more complex cooling schedules listed in [73, 1], or with any of the elaborate techniques for speeding up the convergence speed of the simulated annealing process <ref> [97, 12, 3, 7, 100, 63] </ref>, but assumed that the proportional speedup gained from using any of these methods would be roughly equal with both SSA and BMSA. 76 Empirical results 6.3 Results As noted earlier, the primary objective of the experiments was to study the behavior of the SSA and
Reference: [64] <author> Jeffreys, R. </author> <title> Probability and the Art of Judgement. </title> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: However, it has been shown that any consistent computational 2 Introduction framework representing degree of uncertainty as numbers has to be based on axioms of probability theory [20, 77]. Consequently, Bayesian probability theory seems to offer a solid, unifying framework for uncertain reasoning in general <ref> [16, 37, 64] </ref>. In this work, we study uncertain reasoning in the probabilistic framework.
Reference: [65] <author> Johnson, D., Aragon, C., McGeoch, L., and Schevon, C. </author> <title> Optimization by simulated annealing: an experimental evaluation; Part I, graph partitioning. </title> <booktitle> Operations Research 37, 6 (November-December 1989), </booktitle> <pages> 865-892. </pages>
Reference-contexts: In Section 3.3 we present the general theoretical framework for MCMC algorithms and the simulated annealing technique. After being introduced to the optimization theory community in [71], SA has been applied to many different (NP-hard) optimization problems, such as TSP [71], graph partitioning <ref> [65] </ref>, graph coloring [66], number set partitioning [66] and clustering [105, 14] (for an extensive survey of applications, see [73] or [1, pp. 89-90]). <p> Function RandomFreeVarIdx () returns the index of a randomly chosen variable, which was not instantiated in E. Function Converged () determines whether the simulated annealing process is converged or not. As in <ref> [71, 65] </ref>, we consider a SA process converged when the last c generated states have the same probability. In our tests, we used c = L, where L is the sweepsize parameter, i.e. the number of iterations performed at each temperature. <p> Although simple, this type of cooling schedule is very common, and has proven successful in many applications [1]. It is also empirically observed that more sophisticated annealing methods do not necessarily produce any better results than this simple method <ref> [65] </ref>. The cooling schedule used in our simulations is discussed in more detail in the next section. <p> The final temperature of this "simulated heating" process is then used as the initial temperature for a simulated annealing process. Following the suggestions in <ref> [65] </ref>, we typically set * = 0:1. The cooling schedule of our version of SA can be made slower by increasing the cooling factor F or by increasing the sweepsize L.
Reference: [66] <author> Johnson, D., Aragon, C., McGeoch, L., and Schevon, C. </author> <title> Optimization by simulated annealing: an experimental evaluation; Part II, graph coloring and number partitioning. </title> <note> Operations Research 39, 3 (May-June 1991), 378-406. BIBLIOGRAPHY 89 </note>
Reference-contexts: In Section 3.3 we present the general theoretical framework for MCMC algorithms and the simulated annealing technique. After being introduced to the optimization theory community in [71], SA has been applied to many different (NP-hard) optimization problems, such as TSP [71], graph partitioning [65], graph coloring <ref> [66] </ref>, number set partitioning [66] and clustering [105, 14] (for an extensive survey of applications, see [73] or [1, pp. 89-90]). <p> After being introduced to the optimization theory community in [71], SA has been applied to many different (NP-hard) optimization problems, such as TSP [71], graph partitioning [65], graph coloring <ref> [66] </ref>, number set partitioning [66] and clustering [105, 14] (for an extensive survey of applications, see [73] or [1, pp. 89-90]). One of the main difficulties with the SA algorithm is that the SA theory requires that the objective function to be optimized has to be representable in a specific Gibbs distribution form.
Reference: [67] <author> Kahaner, D. </author> <title> Japan moves towards information society. </title> <journal> IEEE Expert (April 1992), </journal> <pages> 54-58. </pages>
Reference-contexts: What is more, in the next few years the availability of neural hardware is likely to improve drastically, as Japan started in 1992 a new 10-year programme under the name "Real World Computing (RWC)", which aims at "developing computational bases incorporating massively parallel, neural and optical techniques" <ref> [67, 99] </ref>.
Reference: [68] <editor> Kanal, L., and Lemmer, J., Eds. </editor> <booktitle> Uncertainty in Artificial Intelligence 1. </booktitle> <publisher> Elsevier Science Publishers B.V. (North-Holland), </publisher> <address> Amsterdam, </address> <year> 1986. </year>
Reference: [69] <author> Karlin, S., and Taylor, H. </author> <title> A First Course in Stochastic Processes. </title> <publisher> Academic Press, </publisher> <address> San Diego, CA, </address> <year> 1975. </year>
Reference-contexts: matrix operation: 24 Solving the MAP problem by Bayesian networks Theorem 3.5 The probabilities of moving from state i to state j in n steps, denoted by P (n) ij , are the elements of the matrix P n : P ij = (P n ) ij : Proof: See <ref> [69, p. 58] </ref>. fl Definition 3.4 A Markov chain with transition matrix P is irreducible, if 8i; j 9n 1 : P ij &gt; 0: Definition 3.5 The period of a state i is the greatest common divisor of all integers n 1 for which P (n) ii &gt; 0. <p> Then there exists a stationary distri bution which is uniquely determined by the set of equations j = i under the constraints i 0; P Proof: See <ref> [69, p. 85] </ref>. fl 3.3 Simulated annealing 25 3.3.2 Markov chain Monte Carlo methods Markov chain Monte Carlo (MCMC) methods are stochastic algorithms that are based on Markov chain processes (for a survey of using MCMC methods in the Bayesian reasoning framework, see [91]).
Reference: [70] <author> Kinderman, R., and Snell, J. </author> <title> Markov Random Fields and their Applications. </title> <publisher> American Mathematical Society, </publisher> <address> Providence, RI, </address> <year> 1980. </year>
Reference-contexts: Usually, a suitable Gibbs distribution has to be constructed manually for each separate optimization problem instance using very low-level concepts of the problem domain. However, there exists a graphical model called a Markov random field (MRF) <ref> [27, 11, 70, 40] </ref>, which can be used as a formal tool for constructing Gibbs distributions. Similarly to Bayesian networks, an MRF representation con 5 sists of a graphical representation of the dependencies between the variables, together with a set of parameters, clique potentials. <p> For each variable U i , we define a set of neighbors G i : G i = fU j j (U i ; U j ) 2 Gg: Using the concept of neighborhood we can now give the definition for a Markov Random Field (MRF) <ref> [27, 11, 70, 40] </ref>: Definition 3.8 (Markov Random Field) A family of random variables U is a Markov Random Field with respect to a relation G and a probability distribution P, if 1.
Reference: [71] <author> Kirkpatrick, S., Gelatt, D., and Vecchi, M. </author> <title> Optimization by simulated annealing. </title> <booktitle> Science 220, </booktitle> <month> 4598 (May </month> <year> 1983), </year> <pages> 671-680. </pages>
Reference-contexts: In Gibbs sampling, a large collection of representative configurations of the problem domain model is generated by iterative local sampling, and the actual variable-value combination occurrence probabilities can be estimated by sample frequencies. On the other hand, combined with a stochastic technique called simulated annealing (SA) <ref> [80, 71] </ref>, the Gibbs sampling process will, with high probability, converge to the global maximum of a given function consistent with the initial constraints. <p> Consequently, simulated annealing can be used for solving the MAP problem stochastically. In Section 3.3 we present the general theoretical framework for MCMC algorithms and the simulated annealing technique. After being introduced to the optimization theory community in <ref> [71] </ref>, SA has been applied to many different (NP-hard) optimization problems, such as TSP [71], graph partitioning [65], graph coloring [66], number set partitioning [66] and clustering [105, 14] (for an extensive survey of applications, see [73] or [1, pp. 89-90]). <p> In Section 3.3 we present the general theoretical framework for MCMC algorithms and the simulated annealing technique. After being introduced to the optimization theory community in <ref> [71] </ref>, SA has been applied to many different (NP-hard) optimization problems, such as TSP [71], graph partitioning [65], graph coloring [66], number set partitioning [66] and clustering [105, 14] (for an extensive survey of applications, see [73] or [1, pp. 89-90]). <p> As SA in principle finds the global minimum of any energy function, the algorithm is applicable to combinatorial optimization problems in general. This possibility was brought to general 3.3 Simulated annealing 31 attention of the optimization theory community by Kirkpatrick et al. in <ref> [71] </ref>, and SA has since been widely applied for many different optimization problems (see the references in [1]). Simulated annealing is also widely used in image processing, in particular due to the pioneering theoretical work done by Geman & Geman (see [40]). <p> Function RandomFreeVarIdx () returns the index of a randomly chosen variable, which was not instantiated in E. Function Converged () determines whether the simulated annealing process is converged or not. As in <ref> [71, 65] </ref>, we consider a SA process converged when the last c generated states have the same probability. In our tests, we used c = L, where L is the sweepsize parameter, i.e. the number of iterations performed at each temperature. <p> On the other hand, if the annealing is done too quickly, the results are unreliable. In statistical mechanics it has been observed that during the annealing process there is a certain temperature at which the rate of change of the energy is exceptionally high <ref> [71] </ref>. This critical temperature can be seen as a phase transition point which starts the freezing process. It appears that for successful annealing, the cooling should be done very slowly around the critical temperature. <p> Some attempts towards analyzing the critical temperature of harmony networks can be found in [104]. In our tests, we adopted the approach proposed in <ref> [71] </ref> and selected the initial temperature by an iterative search process: we start with temperature 1, compute the acceptance probabilities, and double the temperature until all the acceptance probabilities are within the range [0:5 *; 0:5 + *].
Reference: [72] <author> Kosko, B., and Isaka, S. </author> <title> Fuzzy logic. </title> <publisher> Scientific American 269, </publisher> <month> 1 (July </month> <year> 1993), </year> <pages> 62-69. </pages>
Reference-contexts: Unfortunately, in many problem domains these independence assumptions are not valid [74]. Recently, uncertain reasoning systems based on fuzzy logic [119] have gained popularity, especially in Japan <ref> [72, 94] </ref>. However, it has been shown that any consistent computational 2 Introduction framework representing degree of uncertainty as numbers has to be based on axioms of probability theory [20, 77]. Consequently, Bayesian probability theory seems to offer a solid, unifying framework for uncertain reasoning in general [16, 37, 64].
Reference: [73] <author> Laarhoven, P., and Aarts, E. </author> <title> Simulated Annealing: Theory and Applications. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Dordrecht, The Nether-lands, </address> <year> 1987. </year>
Reference-contexts: After being introduced to the optimization theory community in [71], SA has been applied to many different (NP-hard) optimization problems, such as TSP [71], graph partitioning [65], graph coloring [66], number set partitioning [66] and clustering [105, 14] (for an extensive survey of applications, see <ref> [73] </ref> or [1, pp. 89-90]). One of the main difficulties with the SA algorithm is that the SA theory requires that the objective function to be optimized has to be representable in a specific Gibbs distribution form. <p> For the same reason, we did not experiment with the more complex cooling schedules listed in <ref> [73, 1] </ref>, or with any of the elaborate techniques for speeding up the convergence speed of the simulated annealing process [97, 12, 3, 7, 100, 63], but assumed that the proportional speedup gained from using any of these methods would be roughly equal with both SSA and BMSA. 76 Empirical results
Reference: [74] <author> Lam, F., and Yeap, W. </author> <title> Bayesian updating: on the interpretation of exhaustive and mutually exclusive assumptions. </title> <booktitle> Artificial Intelligence 53, </booktitle> <month> 2-3 (February </month> <year> 1992), </year> <pages> 245-254. </pages>
Reference-contexts: Unfortunately, in many problem domains these independence assumptions are not valid <ref> [74] </ref>. Recently, uncertain reasoning systems based on fuzzy logic [119] have gained popularity, especially in Japan [72, 94]. However, it has been shown that any consistent computational 2 Introduction framework representing degree of uncertainty as numbers has to be based on axioms of probability theory [20, 77].
Reference: [75] <author> Laskey, K. </author> <title> Adapting connectionist learning to Bayesian networks. </title> <journal> International Journal of Approximate Reasoning 4 (1990), </journal> <pages> 261-282. </pages>
Reference-contexts: Although in some restricted domains this kind of a transformation is fairly straightforward to construct <ref> [57, 39, 75, 62, 90] </ref>, the methods presented do not apply to general Bayesian network structures. <p> As noted earlier by several authors <ref> [61, 75, 76, 115] </ref>, we can construct for any given Bayesian network a corresponding MRF having the same probability distribution, by using a transformation called moralization of a Bayesian network.
Reference: [76] <author> Lauritzen, S., and Spiegelhalter, D. </author> <title> Local computations with probabilities on graphical structures and their application to expert systems. </title> <journal> J. Royal Stat. Soc., Ser. </journal> <volume> B 50, 2 (1988), </volume> <pages> 157-224. </pages> <note> Reprinted as pp. 415-448 in [108]. </note>
Reference-contexts: Consequently, most existing systems for Bayesian reasoning first transform a given multi-connected BN structure to a singly-connected network, and then use the existing polynomial-time algorithms (see e.g. [4, 13]). This transformation can be done either explicitly by clustering several nodes together as in <ref> [76, 106] </ref> or [96, Ch.4.4.1], or implicitly by blocking multiply connected paths by conditioning a set of variables as in [96, Ch.4.4.2] (for a discussion of different transformation techniques, see [107]). <p> As noted earlier by several authors <ref> [61, 75, 76, 115] </ref>, we can construct for any given Bayesian network a corresponding MRF having the same probability distribution, by using a transformation called moralization of a Bayesian network.
Reference: [77] <author> Lindley, D. </author> <title> Making Decisions, second ed. </title> <publisher> John Wiley & Sons, </publisher> <address> Lon-don, </address> <year> 1992. </year>
Reference-contexts: Recently, uncertain reasoning systems based on fuzzy logic [119] have gained popularity, especially in Japan [72, 94]. However, it has been shown that any consistent computational 2 Introduction framework representing degree of uncertainty as numbers has to be based on axioms of probability theory <ref> [20, 77] </ref>. Consequently, Bayesian probability theory seems to offer a solid, unifying framework for uncertain reasoning in general [16, 37, 64]. In this work, we study uncertain reasoning in the probabilistic framework.
Reference: [78] <editor> McClelland, J., and Rumelhart, D., Eds. </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> vol. 2. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: In Chapter 4 we consider another family of models for building MAP solvers, neural networks (NN). Neural networks are massively parallel computational models consisting of a large number of very simple processing units (for a survey of neural models, see e.g. the collections <ref> [5, 6, 103, 78] </ref>). These models can perform certain computational tasks extremely fast when run on customized parallel hardware, and hence they have been suggested as a computationally efficient tool for solving NP-hard optimization problems ap-proximatively [59, 10]. <p> For a good introduction to neural 41 0.5 -10 -5 0 5 10 x fi = 1 fi = 10 fi = 0:1 computing, see e.g. the collections <ref> [5, 6, 103, 78] </ref>, or the books [46, 54, 47].
Reference: [79] <author> McCulloch, W., and Pitts, W. </author> <title> A logical calculus of the ideas immanent in nervous activity. </title> <journal> Bulletin of Math. Bio. </journal> <volume> 5 (1943), </volume> <pages> 115-133. </pages> <note> Reprinted as pp. 18-27 in [5]. 90 BIBLIOGRAPHY </note>
Reference-contexts: The processing elements (units, nodes or "neurons") of a neural network are usually based on a simple artificial neuron model, inspired originally <ref> [79] </ref> by the structure of a real biological neuron. In this model, the local memory of a node S i consists of two real-valued parameters: a constant parameter i called the threshold or bias, and a variable parameter s i , the activity value or the state of the node.
Reference: [80] <author> Metropolis, N., Rosenbluth, A., Rosenbluth, M., Teller, M., and Teller, E. </author> <title> Equations of state calculations by fast computing machines. </title> <journal> Journal of Chem. Phys. </journal> <volume> 21 (1953), </volume> <pages> 1087-1092. </pages>
Reference-contexts: Markov Chain Monte Carlo (MCMC) algorithms <ref> [80, 45] </ref> are stochastic algorithms that can be used in exponentially large search spaces for finding global maxima in feasible time with high probability. <p> In Gibbs sampling, a large collection of representative configurations of the problem domain model is generated by iterative local sampling, and the actual variable-value combination occurrence probabilities can be estimated by sample frequencies. On the other hand, combined with a stochastic technique called simulated annealing (SA) <ref> [80, 71] </ref>, the Gibbs sampling process will, with high probability, converge to the global maximum of a given function consistent with the initial constraints. <p> Proof: See [1, p.42]. fl For the acceptance probability matrix A, the original choice suggested by Metropolis et al. in <ref> [80] </ref> was the following: A ij = &lt; 1 ; if Pf~u i g 1, Pf~u i g ; if Pf~u i g &lt; 1. This is still perhaps the most commonly used method in MCMC applications. The Metropolis method clearly fulfills the conditions (A1) and (A2) in Theorem 3.7. <p> In the next section, we present a technique for this purpose. 3.3.4 Gibbs distributions and simulated annealing The stochastic simulation algorithm presented in the previous section was first introduced by Metropolis et al. <ref> [80] </ref> in the context of condensed matter physics.
Reference: [81] <author> Myllym aki, P. </author> <title> Bayesian Reasoning by Stochastic Neural Networks. Ph.Lic. </title> <type> Thesis, Tech. Rep. </type> <institution> C-1993-67, Department of Computer Science, University of Helsinki, </institution> <year> 1993. </year>
Reference-contexts: In particular, we show a simple extension of the binary-variable case mapping which makes no assumptions about the number of values of the variables. The three constructions presented in these sections are published earlier in reports <ref> [84, 81] </ref>, [82], and [83], respectively. From the neural network point of view, the mapping from a Bayesian network to a Boltzmann machine can also be seen as a method for incorporating high-level, probabilistic a priori information directly into neural networks, without recourse to the time-consuming and unreliable learning process. <p> The values of the binary variables C i are denoted by c i1 and c i2 , i = 1; 2; 3. 5.3 Coping with multi-valued variables 63 0, since in this case the resulting probability distribution may not approximate the original distribution very well [31]. Nevertheless, in <ref> [81] </ref> we experimented with harmony networks using the transformation scheme suggested above, and got relatively good results even with extreme probabilities. Another problem with the Bayesian network level transformation is the large number of resulting conditional probabilities, as can be noted in Figure 5.2. <p> However, it is easy to see that there is a large number of conditional probabilities that are actually irrelevant for the functionality of the network. This fact was exploited in <ref> [81] </ref>, where the size of the neural network structure was decreased by pruning the irrelevant pattern nodes. In the neural network level approach for coping with multivalued attributes, we might consider modifying our neural network models in such a way that mappings from multivalued Bayesian networks become straightforward. <p> In these experiments, the parameters j were determined by using the formula (5.7). As with the harmony network experiments in <ref> [81] </ref>, we noted that the constant k is not very relevant for the results, as long as it stays relatively close to 1. In the sequel, the results are obtained with models constructed by using k = 1 10 12 . <p> It has been noted [18] that solving the EVE problem can become very difficult if the Bayesian network contains a lot of extreme probabilities (probabilities with values near zero or one). However, as already noted in <ref> [81] </ref>, in our MAP problem framework this does not seem to be true.
Reference: [82] <author> Myllym aki, P. </author> <title> Using Bayesian networks for incorporating probabilistic a priori knowledge into Boltzmann machines. </title> <booktitle> In Proceedings of SOUTHCON'94 (Orlando, </booktitle> <address> March 1994), </address> <publisher> IEEE, </publisher> <address> Piscataway, NJ, </address> <pages> pp. 97-102. </pages>
Reference-contexts: In particular, we show a simple extension of the binary-variable case mapping which makes no assumptions about the number of values of the variables. The three constructions presented in these sections are published earlier in reports [84, 81], <ref> [82] </ref>, and [83], respectively. From the neural network point of view, the mapping from a Bayesian network to a Boltzmann machine can also be seen as a method for incorporating high-level, probabilistic a priori information directly into neural networks, without recourse to the time-consuming and unreliable learning process. <p> This ensures that the updating process converges to a state where only one pattern node for each clique is on, and thus the consensus of such a final state is equal to potential V . In <ref> [82] </ref> we suggested one possible way of choosing the weights; here we present an alternative solution, which seems to work better in practice.
Reference: [83] <author> Myllym aki, P. </author> <title> Mapping Bayesian networks to Boltzmann machines. </title> <booktitle> In Proceedings of Applied Decision Technologies 1995 (London, </booktitle> <month> April </month> <year> 1995), </year> <editor> A. Gammerman, Ed., </editor> <publisher> Unicom Seminars, London, </publisher> <pages> pp. 269-280. </pages>
Reference-contexts: In particular, we show a simple extension of the binary-variable case mapping which makes no assumptions about the number of values of the variables. The three constructions presented in these sections are published earlier in reports [84, 81], [82], and <ref> [83] </ref>, respectively. From the neural network point of view, the mapping from a Bayesian network to a Boltzmann machine can also be seen as a method for incorporating high-level, probabilistic a priori information directly into neural networks, without recourse to the time-consuming and unreliable learning process. <p> However, as we restrict ourselves in this study to standard neural network processing elements and structures, we do not consider such modifications. Instead, in the following we describe a relatively straightforward extension to the mapping presented earlier, introduced in <ref> [83] </ref>, which uses the same simple neural network processing elements as the BM 2 network in the previous section. Let B=(B S ; B P ) be a Bayesian network with N (not necessarily binary) variables, and let V be the corresponding potential function of the form (5.3).
Reference: [84] <author> Myllym aki, P., and Orponen, P. </author> <booktitle> Programming the Harmonium. In Proceedings of the International Joint Conference on Neural Networks (Singapore, November 1991), </booktitle> <volume> vol. 1, </volume> <publisher> IEEE, </publisher> <address> New York, NY, </address> <pages> pp. 671-677. </pages>
Reference-contexts: In particular, we show a simple extension of the binary-variable case mapping which makes no assumptions about the number of values of the variables. The three constructions presented in these sections are published earlier in reports <ref> [84, 81] </ref>, [82], and [83], respectively. From the neural network point of view, the mapping from a Bayesian network to a Boltzmann machine can also be seen as a method for incorporating high-level, probabilistic a priori information directly into neural networks, without recourse to the time-consuming and unreliable learning process.
Reference: [85] <author> Myllym aki, P., Orponen, P., and Silander, T. </author> <title> Integrating symbolic reasoning with neurally represented background knowledge. </title> <booktitle> In Proceedings of STeP-92, the Finnish Artificial Intelligence Conference (Otaniemi, </booktitle> <address> Finland, </address> <month> June </month> <year> 1992), </year> <editor> E.Hyvonen, J.Seppanen, and M.Syrjanen, Eds., </editor> <volume> vol. 2, </volume> <booktitle> Finnish Artificial Intelligence Society, Hel-sinki, </booktitle> <pages> pp. 231-240. Also pp. </pages> <booktitle> 168-172 in Workshop Notes of the AAAI-92 Workshop on Integrating Neural and Symbolic Processes. </booktitle>
Reference-contexts: An attempt towards this kind of a hybrid system, consisting of a symbolic Prolog interpreter and a neural network, is described in <ref> [85] </ref>. In a sense, the updating processes of the NN component of our Bayesian-neural system correspond to simulated annealing processes on the Bayesian network, where all the variables can be updated at the same time.
Reference: [86] <author> Myllym aki, P., and Tirri, H. </author> <title> Bayesian case-based reasoning with neural networks. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks (San Francisco, March 1993), </booktitle> <volume> vol. 1, </volume> <publisher> IEEE, </publisher> <address> Pis-cataway, NJ, </address> <pages> pp. 422-427. </pages>
Reference-contexts: Nevertheless, it should be also noted that Bayesian reasoning process on a singly connected network can be realized on a massively parallel neural network structure <ref> [86, 87] </ref>, which allows efficient implementations of Bayesian reasoning even for large networks, provided that suitable hardware is available. In another approach to Bayesian reasoning the structure of the network is not changed, but the quantitative conditional probabilities are replaced by a finite set of qualitative measures of uncertainty [117]. <p> Chapter 5 Mapping Bayesian networks to stochastic neural networks As noted in <ref> [86, 87] </ref>, singly connected Bayesian networks offer a possibility for implementing Bayesian reasoning on massively parallel architectures. In this study, however, we are concerned with general BN structures, and we wish to apply the stochastic simulated annealing method for solving the MAP task.
Reference: [87] <author> Myllym aki, P., and Tirri, H. </author> <title> Massively parallel case-based reasoning with probabilistic similarity metrics. In Topics in Case-Based Reasoning, </title> <editor> S. Wess, K.-D. Althoff, and M. Richter, Eds., </editor> <volume> vol. </volume> <booktitle> 837 of Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1994, </year> <pages> pp. 144-154. </pages>
Reference-contexts: Nevertheless, it should be also noted that Bayesian reasoning process on a singly connected network can be realized on a massively parallel neural network structure <ref> [86, 87] </ref>, which allows efficient implementations of Bayesian reasoning even for large networks, provided that suitable hardware is available. In another approach to Bayesian reasoning the structure of the network is not changed, but the quantitative conditional probabilities are replaced by a finite set of qualitative measures of uncertainty [117]. <p> Chapter 5 Mapping Bayesian networks to stochastic neural networks As noted in <ref> [86, 87] </ref>, singly connected Bayesian networks offer a possibility for implementing Bayesian reasoning on massively parallel architectures. In this study, however, we are concerned with general BN structures, and we wish to apply the stochastic simulated annealing method for solving the MAP task.
Reference: [88] <author> Myllym aki, P., and Tirri, H. </author> <title> Constructing computationally efficient Bayesian models via unsupervised clustering. In Probabilistic BIBLIOGRAPHY 91 Reasoning and Bayesian Belief Networks, </title> <editor> A.Gammerman, Ed. </editor> <publisher> Alfred Waller Publishers, </publisher> <address> Suffolk, </address> <year> 1995, </year> <pages> pp. 237-248. </pages>
Reference-contexts: Alternatively, clustering can also be done by introducing new latent variables which represent separate clusters of original variables <ref> [17, 88] </ref>. The latent variable model offers also an interesting opportunity for constructing Bayesian networks from data using unsupervised learning algorithms.
Reference: [89] <author> Myllym aki, P., Tirri, H., Flor een, P., and Orponen, P. </author> <title> Compiling high-level specifications into neural networks. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks (Washington D.C., January 1990), </booktitle> <volume> vol. 2, </volume> <publisher> IEEE, </publisher> <address> New York, NY, </address> <pages> pp. 475-478. </pages>
Reference-contexts: First of all, the mathematical model behind the system is the theoretically sound framework of Bayesian reasoning, compared to the more or less heuristic models of most other hybrid systems (for our earlier, heuristic attempts towards a hybrid system, see <ref> [33, 89, 34] </ref>). Secondly, although some hybrid models provide theoretical justifications for the computations (see e.g.
Reference: [90] <author> Neal, R. </author> <title> Connectionist learning of belief networks. </title> <booktitle> Artificial Intelligence 56 (1992), </booktitle> <pages> 71-113. </pages>
Reference-contexts: Although in some restricted domains this kind of a transformation is fairly straightforward to construct <ref> [57, 39, 75, 62, 90] </ref>, the methods presented do not apply to general Bayesian network structures. <p> In this case, a BM structure corresponding to a given MRF can be constructed in a similar way as in the 2-variable clique MRF case <ref> [90] </ref>. However, using only basic two-member conditional probabilities, it is generally not possible to find any function capable of approximating the missing probabilities accurately, or even to obtain informative upper or lower bounds for the missing values [92, p.138].
Reference: [91] <author> Neal, R. </author> <title> Probabilistic inference using Markov chain Monte Carlo methods. </title> <type> Tech. Rep. </type> <institution> CRG-TR-93-1, University of Toronto, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: the constraints i 0; P Proof: See [69, p. 85]. fl 3.3 Simulated annealing 25 3.3.2 Markov chain Monte Carlo methods Markov chain Monte Carlo (MCMC) methods are stochastic algorithms that are based on Markov chain processes (for a survey of using MCMC methods in the Bayesian reasoning framework, see <ref> [91] </ref>). As pointed out in the previous section, under certain conditions, Markov chains will converge to a unique stationary distribution.
Reference: [92] <author> Neapolitan, R. </author> <title> Probabilistic Reasoning in Expert Systems. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, NY, </address> <year> 1990. </year>
Reference-contexts: As in diagnostic applications this kind of a system would provide the user the best explanation for a given set of symptoms, the solution is sometimes called the most probable explanation [96], and the process of obtaining the MAP solution is referred to as abductive inference <ref> [92] </ref>. In the other, expected value estimation (EVE) approach, the goal is to compute (estimates of) the probabilities of the form PfX = x j ~ E = ~eg for variables X =2 E. <p> In the field of decision theory, a model similar to Bayesian networks is known as influence diagrams [60]. Thorough introductions to the Bayesian network theory can be found in <ref> [96, 92] </ref>. The importance of a Bayesian network structure lies in the way the network facilitates computing conditional probabilities. <p> gives directly the answer to this decision problem, the MAP problem is said to be NP-hard. (For a good introduction to the theory of complexity classes, see [38].) 3.2 Complexity of the MAP problem 21 oning (meaning here both the EVE and MAP tasks) can be done in polynomial time <ref> [96, 92] </ref>. Consequently, most existing systems for Bayesian reasoning first transform a given multi-connected BN structure to a singly-connected network, and then use the existing polynomial-time algorithms (see e.g. [4, 13]). <p> Moreover, the Bayesian network structure corresponding to this kind of a simple MRF is a tree, which means that there is no need for approximate stochastic methods since polynomial time exact algorithms for solving the MAP problem exist in the first place <ref> [96, 92] </ref>. Let us then consider Markov random fields with an unrestricted clique size, corresponding to general BN structures. <p> In the noisy-OR approximation model, only a linear number of simple two-member conditional probabilities (one for each incoming arc) are stored for each variable in the BN structure, and the missing parameters are approximated as a function of the combination of the stored parameters (see e.g. <ref> [96, 92] </ref>). In this case, a BM structure corresponding to a given MRF can be constructed in a similar way as in the 2-variable clique MRF case [90]. <p> However, using only basic two-member conditional probabilities, it is generally not possible to find any function capable of approximating the missing probabilities accurately, or even to obtain informative upper or lower bounds for the missing values <ref> [92, p.138] </ref>. As a result, the noisy-OR model is usually not accurate enough for practical applications [50].
Reference: [93] <author> Newquist, H. </author> <title> Struggling to maintain. </title> <journal> AI Expert 8, </journal> <volume> 3 (1988), </volume> <pages> 69-71. </pages>
Reference-contexts: Collecting these kinds of large rule bases can be a very expensive and time-consuming task in practice, and moreover, maintaining and updating large rule bases (while preserving consistency) appears to be extremely difficult <ref> [93] </ref>. Consequently, it is clear that some kind of a computational mechanism capable of handling uncertain data is necessary for providing expert systems with the robustness required for practical applications. Several different frameworks for handling noisy data have been developed during the last two decades.
Reference: [94] <author> Nishikawa, T. </author> <booktitle> Fuzzy theory the science of human intuition. Japan Computer Quarterly 79 (1989). </booktitle>
Reference-contexts: Unfortunately, in many problem domains these independence assumptions are not valid [74]. Recently, uncertain reasoning systems based on fuzzy logic [119] have gained popularity, especially in Japan <ref> [72, 94] </ref>. However, it has been shown that any consistent computational 2 Introduction framework representing degree of uncertainty as numbers has to be based on axioms of probability theory [20, 77]. Consequently, Bayesian probability theory seems to offer a solid, unifying framework for uncertain reasoning in general [16, 37, 64].
Reference: [95] <author> Orponen, P., Flor een, P., Myllym aki, P., and Tirri, H. </author> <title> A neural implementation of conceptual hierarchies with Bayesian reasoning. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks (San Diego, </booktitle> <address> CA, </address> <month> June </month> <year> 1990), </year> <title> vol. </title> <booktitle> 1, IEEE, </booktitle> <address> New York, NY, </address> <pages> pp. 297-303. </pages>
Reference-contexts: Proof: See [96, p. 218]. fl In <ref> [95] </ref>, this formula was used for finding an approximate solution to the EVE task, and in principle the same method could be used for the MAP task as well.
Reference: [96] <author> Pearl, J. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: In Chapter 5, we show how to build a hybrid Bayesian-neural MAP solver, which offers an interesting possibility to avoid the disadvantages that occur when using either of these two families of models alone. A Bayesian (belief) network <ref> [96, 106] </ref> is a graphical high-level representation of a probability distribution over a set of discrete variables. <p> As in diagnostic applications this kind of a system would provide the user the best explanation for a given set of symptoms, the solution is sometimes called the most probable explanation <ref> [96] </ref>, and the process of obtaining the MAP solution is referred to as abductive inference [92]. In the other, expected value estimation (EVE) approach, the goal is to compute (estimates of) the probabilities of the form PfX = x j ~ E = ~eg for variables X =2 E. <p> In the field of decision theory, a model similar to Bayesian networks is known as influence diagrams [60]. Thorough introductions to the Bayesian network theory can be found in <ref> [96, 92] </ref>. The importance of a Bayesian network structure lies in the way the network facilitates computing conditional probabilities. <p> Proof: See <ref> [96, p. 121] </ref>. fl 18 Solving the MAP problem by Bayesian networks X F 2 S 1 S 3 Y 2 B X Thus, to determine the distribution of a variable X, given the values of all the other variables, it suffices to consider only the values of the variables in <p> A set of variables covering X in this sense is called a Markov blanket of X <ref> [96, p. 97] </ref>. Even an explicit formula for computing the distribution of X from its Markov blanket can be given as follows: Theorem 3.3 Let B be a Bayesian network over a variable set fU 1 ; : : : ; U N g. <p> Proof: See <ref> [96, p. 218] </ref>. fl In [95], this formula was used for finding an approximate solution to the EVE task, and in principle the same method could be used for the MAP task as well. <p> gives directly the answer to this decision problem, the MAP problem is said to be NP-hard. (For a good introduction to the theory of complexity classes, see [38].) 3.2 Complexity of the MAP problem 21 oning (meaning here both the EVE and MAP tasks) can be done in polynomial time <ref> [96, 92] </ref>. Consequently, most existing systems for Bayesian reasoning first transform a given multi-connected BN structure to a singly-connected network, and then use the existing polynomial-time algorithms (see e.g. [4, 13]). <p> Consequently, most existing systems for Bayesian reasoning first transform a given multi-connected BN structure to a singly-connected network, and then use the existing polynomial-time algorithms (see e.g. [4, 13]). This transformation can be done either explicitly by clustering several nodes together as in [76, 106] or <ref> [96, Ch.4.4.1] </ref>, or implicitly by blocking multiply connected paths by conditioning a set of variables as in [96, Ch.4.4.2] (for a discussion of different transformation techniques, see [107]). Alternatively, clustering can also be done by introducing new latent variables which represent separate clusters of original variables [17, 88]. <p> This transformation can be done either explicitly by clustering several nodes together as in [76, 106] or [96, Ch.4.4.1], or implicitly by blocking multiply connected paths by conditioning a set of variables as in <ref> [96, Ch.4.4.2] </ref> (for a discussion of different transformation techniques, see [107]). Alternatively, clustering can also be done by introducing new latent variables which represent separate clusters of original variables [17, 88]. The latent variable model offers also an interesting opportunity for constructing Bayesian networks from data using unsupervised learning algorithms. <p> Moreover, the Bayesian network structure corresponding to this kind of a simple MRF is a tree, which means that there is no need for approximate stochastic methods since polynomial time exact algorithms for solving the MAP problem exist in the first place <ref> [96, 92] </ref>. Let us then consider Markov random fields with an unrestricted clique size, corresponding to general BN structures. <p> In the noisy-OR approximation model, only a linear number of simple two-member conditional probabilities (one for each incoming arc) are stored for each variable in the BN structure, and the missing parameters are approximated as a function of the combination of the stored parameters (see e.g. <ref> [96, 92] </ref>). In this case, a BM structure corresponding to a given MRF can be constructed in a similar way as in the 2-variable clique MRF case [90].
Reference: [97] <author> Peterson, C., and Anderson, J. </author> <title> A mean field theory learning algorithm for neural networks. </title> <booktitle> Complex Systems 1 (1987), </booktitle> <pages> 995-1019. </pages>
Reference-contexts: For the same reason, we did not experiment with the more complex cooling schedules listed in [73, 1], or with any of the elaborate techniques for speeding up the convergence speed of the simulated annealing process <ref> [97, 12, 3, 7, 100, 63] </ref>, but assumed that the proportional speedup gained from using any of these methods would be roughly equal with both SSA and BMSA. 76 Empirical results 6.3 Results As noted earlier, the primary objective of the experiments was to study the behavior of the SSA and
Reference: [98] <author> Pilarski, S., and Kameda, T. </author> <title> Simple bounds on the convergence rate of an ergodic Markov chain. </title> <booktitle> Information Processing Letters 45 (1993), </booktitle> <pages> 81-87. </pages>
Reference-contexts: However, very little is known about the quality of 22 Solving the MAP problem by Bayesian networks the solutions produced by the Markov chain models, if a limited amount of time is available, but some analyses of the convergence speed can be found in <ref> [32, 1, 113, 101, 98, 22] </ref>. In particular, for Bayesian networks restricted by their dependence value (see above), there exists a stochastic polynomial-time algorithm which solves the EVE problem in the PAC sense [24].
Reference: [99] <author> Pollitzer, E. </author> <booktitle> Engineering cognitive systems: Japan's real-world computing programme. AI Communications 5, </booktitle> <month> 2 (June </month> <year> 1992), </year> <pages> 56-61. 92 BIBLIOGRAPHY </pages>
Reference-contexts: What is more, in the next few years the availability of neural hardware is likely to improve drastically, as Japan started in 1992 a new 10-year programme under the name "Real World Computing (RWC)", which aims at "developing computational bases incorporating massively parallel, neural and optical techniques" <ref> [67, 99] </ref>.
Reference: [100] <author> Rajasekaran, S., and Reif, J. H. </author> <title> Nested annealing: A provable improvement to simulated annealing. </title> <booktitle> Theoretical Computer Science 99 (1992), </booktitle> <pages> 157-176. </pages>
Reference-contexts: For the same reason, we did not experiment with the more complex cooling schedules listed in [73, 1], or with any of the elaborate techniques for speeding up the convergence speed of the simulated annealing process <ref> [97, 12, 3, 7, 100, 63] </ref>, but assumed that the proportional speedup gained from using any of these methods would be roughly equal with both SSA and BMSA. 76 Empirical results 6.3 Results As noted earlier, the primary objective of the experiments was to study the behavior of the SSA and
Reference: [101] <author> Rego, V. </author> <title> Naive asymptotics for hitting time bounds in Markov chains. </title> <journal> Acta Informatica 29 (1992), </journal> <pages> 579-594. </pages>
Reference-contexts: However, very little is known about the quality of 22 Solving the MAP problem by Bayesian networks the solutions produced by the Markov chain models, if a limited amount of time is available, but some analyses of the convergence speed can be found in <ref> [32, 1, 113, 101, 98, 22] </ref>. In particular, for Bayesian networks restricted by their dependence value (see above), there exists a stochastic polynomial-time algorithm which solves the EVE problem in the PAC sense [24].
Reference: [102] <author> Roth, D. </author> <title> On the hardness of approximate reasoning. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence (1993), </booktitle> <volume> vol. 1, </volume> <pages> pp. 613-618. </pages>
Reference-contexts: These results mean that it is very unlikely that a polynomial-time algorithm for solving the MAP problem for Bayesian networks could exist. 1 . Even finding an approximation of the EVE solution (in any constant degree of accuracy) is an NP-hard problem <ref> [23, 102] </ref>. As the answer to a MAP problem is a configuration vector, and not a probability, it is not obvious what an approximative solution in this case means.
Reference: [103] <editor> Rumelhart, D., and McClelland, J., Eds. </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> vol. 1. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: In Chapter 4 we consider another family of models for building MAP solvers, neural networks (NN). Neural networks are massively parallel computational models consisting of a large number of very simple processing units (for a survey of neural models, see e.g. the collections <ref> [5, 6, 103, 78] </ref>). These models can perform certain computational tasks extremely fast when run on customized parallel hardware, and hence they have been suggested as a computationally efficient tool for solving NP-hard optimization problems ap-proximatively [59, 10]. <p> For a good introduction to neural 41 0.5 -10 -5 0 5 10 x fi = 1 fi = 10 fi = 0:1 computing, see e.g. the collections <ref> [5, 6, 103, 78] </ref>, or the books [46, 54, 47].
Reference: [104] <author> Santos, S. </author> <title> Phase transitions in sparsely connected Boltzmann machines. </title> <type> Tech. Rep. </type> <institution> C-1994-15, University of Helsinki, Department of Computer Science, </institution> <year> 1994. </year>
Reference-contexts: Some attempts towards analyzing the critical temperature of harmony networks can be found in <ref> [104] </ref>. In our tests, we adopted the approach proposed in [71] and selected the initial temperature by an iterative search process: we start with temperature 1, compute the acceptance probabilities, and double the temperature until all the acceptance probabilities are within the range [0:5 *; 0:5 + *].
Reference: [105] <author> Selim, S., and Alsultan, K. </author> <title> A simulated annealing algorithm for the clustering problem. </title> <journal> Pattern Recognition 24, </journal> <volume> 10 (1991), </volume> <pages> 1003-1008. </pages>
Reference-contexts: After being introduced to the optimization theory community in [71], SA has been applied to many different (NP-hard) optimization problems, such as TSP [71], graph partitioning [65], graph coloring [66], number set partitioning [66] and clustering <ref> [105, 14] </ref> (for an extensive survey of applications, see [73] or [1, pp. 89-90]). One of the main difficulties with the SA algorithm is that the SA theory requires that the objective function to be optimized has to be representable in a specific Gibbs distribution form.
Reference: [106] <author> Shachter, R. </author> <title> Probabilistic inference and influence diagrams. </title> <booktitle> Operations Research 36, 4 (July-August 1988), </booktitle> <pages> 589-604. </pages>
Reference-contexts: In Chapter 5, we show how to build a hybrid Bayesian-neural MAP solver, which offers an interesting possibility to avoid the disadvantages that occur when using either of these two families of models alone. A Bayesian (belief) network <ref> [96, 106] </ref> is a graphical high-level representation of a probability distribution over a set of discrete variables. <p> Consequently, most existing systems for Bayesian reasoning first transform a given multi-connected BN structure to a singly-connected network, and then use the existing polynomial-time algorithms (see e.g. [4, 13]). This transformation can be done either explicitly by clustering several nodes together as in <ref> [76, 106] </ref> or [96, Ch.4.4.1], or implicitly by blocking multiply connected paths by conditioning a set of variables as in [96, Ch.4.4.2] (for a discussion of different transformation techniques, see [107]).
Reference: [107] <author> Shachter, R. </author> <title> Evidence absorption and propagation through evidence reversals. </title> <editor> In Henrion et al. </editor> <volume> [53], </volume> <pages> pp. 173-190. </pages>
Reference-contexts: This transformation can be done either explicitly by clustering several nodes together as in [76, 106] or [96, Ch.4.4.1], or implicitly by blocking multiply connected paths by conditioning a set of variables as in [96, Ch.4.4.2] (for a discussion of different transformation techniques, see <ref> [107] </ref>). Alternatively, clustering can also be done by introducing new latent variables which represent separate clusters of original variables [17, 88]. The latent variable model offers also an interesting opportunity for constructing Bayesian networks from data using unsupervised learning algorithms.
Reference: [108] <author> Shafer, G., and Pearl, J., Eds. </author> <title> Readings in Uncertain Reasoning. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference: [109] <author> Shastri, L. </author> <title> Semantic Networks: An Evidential Formalization and Its Connectionist Realization. </title> <publisher> Pitman, </publisher> <address> London, </address> <year> 1988. </year>
Reference-contexts: Secondly, although some hybrid models provide theoretical justifications for the computations (see e.g. Shastri's system for evidential reasoning <ref> [109] </ref>), they may require fairly complicated and heterogeneous computing elements and control regimes, whereas the neural network model behind our Bayesian-neural system is structurally very simple and uniform, and confirms to an already existing family of neural architectures, the Boltzmann machines.
Reference: [110] <author> Shawe-taylor, J., and Zerovnik, J. </author> <title> Generalized Boltzmann machines. </title> <type> Tech. Rep. </type> <institution> CSD-TR-92-29, Department of Computer Science, Royal Holloway, University of London, </institution> <year> 1992. </year>
Reference-contexts: Such modifications could include, for example, using winner-take-all subnet-works as modules of the network, or more complex, multi-state processing elements, such as the nodes in the generalized Boltzmann machine model in <ref> [110] </ref>. However, as we restrict ourselves in this study to standard neural network processing elements and structures, we do not consider such modifications.
Reference: [111] <author> Shimony, S. </author> <title> Cost-based abduction and MAP explanation. </title> <booktitle> Artificial Intelligence 66 (1994), </booktitle> <pages> 345-374. </pages>
Reference-contexts: Obviously, a MAP solver can be used for solving various configuration and design problems, where the goal is to find the best combination of discrete attributes. In addition, many problems in e.g. medical diagnosing, pattern recognition and natural language processing can be formulated in the MAP framework <ref> [111] </ref>. The structure of a generic MAP solver is shown in Figure 1.1. The system consists of two modules: a model construction module and a query processing module.
Reference: [112] <author> Shimony, S. </author> <title> Finding MAPs for belief networks is NP-hard. </title> <booktitle> Artificial Intelligence 68 (1994), </booktitle> <pages> 399-410. </pages>
Reference-contexts: In Section 3.2, we discuss the complexity of the MAP problem within the Bayesian network framework, and review briefly some attempts to develop algorithms for solving MAP problems in the Bayesian network framework. Unfortunately, for a general network structure, the MAP problem can be shown to be NP-hard <ref> [19, 112] </ref>, which means that very probably it is not possible for any algorithm to solve this task (in the worst case) 4 Introduction in polynomial time with respect to the size of the network. <p> It has been shown that if the probability distribution is represented as a Bayesian network, both the EVE task [19] and the MAP problem <ref> [112] </ref> belong to the class of NP-hard problems with respect to the size of the corresponding network. These results mean that it is very unlikely that a polynomial-time algorithm for solving the MAP problem for Bayesian networks could exist. 1 .
Reference: [113] <author> Sinclair, A., and Jerrum, M. </author> <title> Approximate counting, uniform generation and rapidly mixing Markov chains. </title> <booktitle> Information and Computation 82 (1989), </booktitle> <pages> 93-133. BIBLIOGRAPHY 93 </pages>
Reference-contexts: However, very little is known about the quality of 22 Solving the MAP problem by Bayesian networks the solutions produced by the Markov chain models, if a limited amount of time is available, but some analyses of the convergence speed can be found in <ref> [32, 1, 113, 101, 98, 22] </ref>. In particular, for Bayesian networks restricted by their dependence value (see above), there exists a stochastic polynomial-time algorithm which solves the EVE problem in the PAC sense [24].
Reference: [114] <author> Smolensky, P. </author> <booktitle> Information processing in dynamical systems: Foundations of harmony theory. In Rumelhart and McClelland [103], </booktitle> <pages> pp. 194-281. </pages>
Reference-contexts: In Section 4.2 we present a special case of the BM structure, the harmony network <ref> [114] </ref>, which consists of two separate layers of nodes. The two-layer structure of the harmony network model allows us to update all the nodes in one layer simultaneously, making massively parallel implementations possible. <p> However, in the sequel we deal with a special class of two-layer BM architectures which have by definition only two clusters, being in this sense optimal BM architectures. 4.2 Harmony networks 45 4.2 Harmony networks A basic difficulty with understanding Smolensky's theory of harmony networks <ref> [114] </ref> lies in the fact that Smolensky uses two disturbingly similar, structurally identical models on different levels of abstraction. <p> According to Theorem 4.2, the resulting HBM updating process will find a maximum of the Gibbs distribution (4.11), which has the same maximum points as the harmony function H. fl In <ref> [114] </ref>, Smolensky gives no formal method for determining the number of harmony network pattern nodes, or for choosing the network structure. <p> if ~! i ~x=j~! i j = 1; 0; otherwise. (5.6) Proposition 5.1 The HBM 2 network updating process maximizes the po tential function (5.3), provided that all the parameters i are nonnegative. 56 Mapping Bayesian networks to stochastic neural networks Proof: Using a clever trick presented by Smolensky in <ref> [114] </ref>, we can express the function (5.6) as (~x; ~! i ) = max [y i j~! i j ] = &lt; 1; if ~! i ~x 0; if ~! i ~x Now the potential function (5.5) can be expressed as V (~x) = i y i 2f0;1g ~! i ~x
Reference: [115] <author> Spiegelhalter, D. </author> <title> Probabilistic reasoning in predictive expert systems. </title> <booktitle> In Kanal and Lemmer [68], </booktitle> <pages> pp. 47-67. </pages>
Reference-contexts: As noted earlier by several authors <ref> [61, 75, 76, 115] </ref>, we can construct for any given Bayesian network a corresponding MRF having the same probability distribution, by using a transformation called moralization of a Bayesian network.
Reference: [116] <author> Sun, R. </author> <title> Integrating Rules and Connectionism for Robust Common-sense Reasoning. </title> <publisher> John Wiley & Sons, </publisher> <address> Chichester, </address> <year> 1994. </year>
Reference-contexts: Naturally, the resulting neural network could also be regarded as a cleverly chosen starting point to some learning algorithm, but this interesting idea is not studied here further. Compared to other neural-symbolic hybrid systems (see e.g. <ref> [9, 56, 41, 116] </ref>), the Bayesian-neural hybrid system suggested here has two clear advantages.
Reference: [117] <author> Wellman, M. </author> <title> Fundamental concepts of qualitative probabilistic networks. </title> <booktitle> Artificial Intelligence 44, </booktitle> <month> 3 (August </month> <year> 1990), </year> <pages> 257-304. </pages>
Reference-contexts: In another approach to Bayesian reasoning the structure of the network is not changed, but the quantitative conditional probabilities are replaced by a finite set of qualitative measures of uncertainty <ref> [117] </ref>. It seems that Bayesian reasoning can in this case be done in polynomial time [29], but it is not yet clear what is the expressive power of this kind of qualitative Bayesian networks.
Reference: [118] <author> York, J. </author> <title> Use of Gibbs sampler in expert systems. </title> <booktitle> Artificial Intelligence 56 (1992), </booktitle> <pages> 115-130. </pages> <note> Addendum in pp. 397-398 of the same volume. </note>
Reference-contexts: In the stochastic simulation community, it is generally assumed that one long simulation run produces better results than several short runs, since the shorter runs never quite reach the equilibrium state of the stochastic process <ref> [118] </ref>, [1, p. 94].
Reference: [119] <author> Zadeh, L. </author> <title> Fuzzy logic and approximate reasoning. </title> <booktitle> Synthese 30 (1975), </booktitle> <pages> 407-425. </pages> <address> ISSN 1238-8645 ISBN 951-45-7211-4 Helsinki 1995 Yliopistopaino </address>
Reference-contexts: Unfortunately, in many problem domains these independence assumptions are not valid [74]. Recently, uncertain reasoning systems based on fuzzy logic <ref> [119] </ref> have gained popularity, especially in Japan [72, 94]. However, it has been shown that any consistent computational 2 Introduction framework representing degree of uncertainty as numbers has to be based on axioms of probability theory [20, 77].
References-found: 118

