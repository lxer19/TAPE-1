URL: ftp://ftp.cs.wisc.edu/tech-reports/reports/91/tr1030.ps
Refering-URL: http://www.cs.wisc.edu/math-prog/tech-reports/
Root-URL: 
Title: Globally Convergent Methods for Nonlinear Equations  
Author: M. Ferris S. Lucidi 
Date: July 1991  
Abstract: We are concerned with enlarging the domain of convergence for solution methods of nonlinear equations. To this end, we produce a general framework in which to prove global convergence. Our framework relies on several notions: the use of a merit function, a generalization of a forcing function and conditions on the choice of direction. We also incorporate the idea of a nonmonotone stabilization procedure as a means of producing very good practical rates of convergence. The general theory is specialized to yield several well known results from the literature and is also used to generate three new algorithms for the solution of nonlinear equations. Numerical results for these algorithms applied to the nonlinear equations arising from nonlinear complementarity problems are given. 
Abstract-found: 1
Intro-found: 1
Reference: [Ber82] <author> D.P. Bertsekas. </author> <title> Constrained Optimization and Lagrange Multiplier Methods. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: By (27) and (28) we have kd 2 kr (x and by repeating, with minor modifications, the same reasonings of the proof of Proposition 1.12 in <ref> [Ber82] </ref> and by recalling that Theorem 5 implies that (x k ) ! 0, we have that lim k!1 x k = x. <p> k ) T rH (x k ) + k I, with k = c max 0iM (N+1) fl fl fl, satisfies the Dennis-More condition which ensures both that the unit stepsize is eventually accepted by the linesearch technique and that the sequence fx k g converges superlinearly (see, for example, <ref> [Ber82, Proposition 1.15] </ref>). Finally, the last statement of the proposition follows from the superlinear convergence rate of fx k g. 12 In the second case we take as our direction a solution of a modified system of equations, where the Jacobian of H has been modified by a diagonal matrix.
Reference: [BS91] <author> R.S. Bain and W.E. Stewart. </author> <title> Application of robust nonmonotonic descent criteria to the solution of nonlinear algebraic equation systems. </title> <journal> Computers in Chemical Engineering, </journal> <volume> 15 </volume> <pages> 203-208, </pages> <year> 1991. </year>
Reference-contexts: Computationally, this has proven very effective, see for example, [GLL86] where the poor performance on the Rosenbrock problem has been overcome by nonmonotone linesearch (also <ref> [BS91] </ref>). For different problems, several types of merit function have been proposed in the literature. By far the most popular is to use the Euclidean norm of the residual as the merit function. Other forms which can be considered are p-norm merit functions and 1-norm merit functions.
Reference: [Bur80] <author> O.P. Burdakov. </author> <title> Some globally convergent modifications of Newtons's method for solving systems of nonlinear equations. </title> <journal> Soviet Math Doklady, </journal> <volume> 22(2) </volume> <pages> 376-379, </pages> <year> 1980. </year>
Reference-contexts: There are many known results on the global convergence of Newton type methods for nonlinear equations. Most of these methods are based on the use of a merit function. Several different types of merit function have been proposed in the literature (see for example <ref> [Bur80, SB80, Pol76, HPR89, HX90] </ref>). One of the main differences between many of the approaches is whether or not the underlying equations are smooth or nonsmooth. In the smooth case, global convergence is proven in many instances: we note in particular the results of Stoer [SB80]. <p> The work of Burdakov <ref> [Bur80] </ref> can also be seen to be a special case of the method given in Section 2. In this case, the merit function is given by (x): = kH (x)k r for some r &gt; 0.
Reference: [Cla83] <author> F.H. Clarke. </author> <title> Optimization and Nonsmooth Analysis. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: This assumption is a strengthening of (A2). In fact, we note that if is subdifferentially regular <ref> [Cla83] </ref> then both (A2) and (A4) are equivalent to 0 k (x k ; d k ) 0 (x k ; d k ) We are now able to prove our convergence result.
Reference: [DS83] <author> J.E. Dennis and R.B. Schnabel. </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations. </title> <publisher> Prentice-Hall, </publisher> <address> New Jersey, </address> <year> 1983. </year>
Reference-contexts: In particular, the method of Subramanian [Sub85] gives a direction which is consistent. The results in this paper show that our method can be used to establish global convergence in the case where the Jacobian is singular at the solution point (Dennis and Schnabel <ref> [DS83] </ref> give other results using the trust region approach). The paper is organized as follows. The main theoretical results of the paper are given in Section 2.
Reference: [Fra88] <author> C. Fraley. </author> <title> Algorithms for nonlinear least squares. </title> <type> Technical Report SOL 88.16, </type> <institution> Stanford University, </institution> <month> July </month> <year> 1988. </year>
Reference-contexts: In this paper, we propose several modifications of the search direction when the Newton direction cannot be found. Most of these find their basis in solving the equations in a least squares sense; see the survey paper of <ref> [Fra88] </ref> for several search direction modifications in least squares problems. Much of this work was motivated by a desire to solve nonlinear complementarity problems. We are interested in using the smooth equations determined by Mangasarian [Man76] which are equivalent to the nonlinear complementarity problem.
Reference: [GLL86] <author> L. Grippo, F. Lampariello, and S. Lucidi. </author> <title> A nonmonotone line search technique for Newton's method. </title> <journal> SIAM Journal of Numerical Analysis, </journal> <volume> 23 </volume> <pages> 707-716, </pages> <year> 1986. </year>
Reference-contexts: For instance, a nonmonotone stabilization procedure is used to overcome some of the difficulties associated with ill-conditioning and to enable a steps of length one to be taken much more frequently in Newton-type methods. Computationally, this has proven very effective, see for example, <ref> [GLL86] </ref> where the poor performance on the Rosenbrock problem has been overcome by nonmonotone linesearch (also [BS91]). For different problems, several types of merit function have been proposed in the literature. By far the most popular is to use the Euclidean norm of the residual as the merit function. <p> Furthermore, we believe that the trade-off for this easier subproblem manifests itself in the form of ill-conditioning for which the nonmonotone line search technique has proven effective <ref> [GLL86] </ref>. The condition (26) appears to be a mild assumption on the problem in this case. Certainly it is not implied by the conditions which are assumed to guarantee convergence for the nonsmooth algorithms mentioned above.
Reference: [GLL90] <author> L. Grippo, F. Lampariello, and S. Lucidi. </author> <title> A class of nonmonotone stabilization methods in unconstrained optimization. </title> <note> Technical Report IASI-CNR R.290, 1990 (to appear in Numerische Mathematik). </note>
Reference-contexts: In order to to obtain a method for the solution of (1) we define a general stabilization scheme that includes different strategies for enforcing global convergence without requiring a monotonic reduction of the objective function. The proposed algorithm is similar to the one proposed by <ref> [GLL90] </ref>. NonMonotone Stabilization Algorithm (NMS) Data: x o , o &gt; 0, fi 2 (0; 1), fl 2 (0; 1) and N 1. Step 1: Set k = 0; ` = 0; = o . Compute (x o ) and set W = (x o ). <p> then, for any integer k, there exist indices h k and j k such that: 0 &lt; h k k N (M + 1); h k = s (j k ); Proof The proof of lemma follows, with minor modification from the proofs of Lemma 1 and Lemma 2 of <ref> [GLL90] </ref>. The following result is central to our development. We show that the merit function converges to a limit and also the product of the step size and the auxiliary function tends to zero.
Reference: [Har86] <author> P.T. Harker. </author> <title> Alternative models of spatial competition. </title> <journal> Operations Research, </journal> <volume> 34 </volume> <pages> 410-425, </pages> <year> 1986. </year>
Reference-contexts: 0; 0; 10; 0; 0; 1; 1; 0; 0; 0; 0; 0) Table 7: Powell's nonlinear programming problem [Pow69] Starting Point Algorithm 1 Algorithm 2 Algorithm 3 Gradient (Function) Gradient (Function) Gradient (Function) Evaluations Evaluations Evaluations (0,. . . ,0) F 33 (24) 35 (34) Table 8: Spatial competition example <ref> [Har86] </ref> 21 were considered too close to an optimal point. Furthermore, the functions and gradients were coded without any problem specific knowledge. In particular, the use of fixing a numeraire in the economic equilibrium problems was not used to force the Jacobian to be nonsingular at the solution.
Reference: [Har88] <author> P.T. Harker. </author> <title> Accelerating the convergence of the diagonalization and projection algorithms for finite-dimensional variational inequalities. </title> <journal> Mathematical Programming, </journal> <volume> 41 </volume> <pages> 29-50, </pages> <year> 1988. </year> <month> 22 </month>
Reference-contexts: 0:1; 0:2; 0:5; 0:0; 4:0; 0:0; 0:0; 0:0; 0:4; 0:0) Table 4: Scarf's economic equilibrium model [Sca73] Starting Point Algorithm 1 Algorithm 2 Algorithm 3 Gradient (Function) Gradient (Function) Gradient (Function) Evaluations Evaluations Evaluations (1,. . . ,1) 15 (3) 15 (3) 13 (18) Table 5: Nash noncooperative game example <ref> [Har88] </ref> Starting Point Algorithm 1 Algorithm 2 Algorithm 3 Gradient (Function) Gradient (Function) Gradient (Function) Evaluations Evaluations Evaluations (0,. . . ,0) 28 (23) 28 (40) 42 (101) x 1 34 (20) 25 (40) 24 (62) Table 6: Tobin's spatial price equilibrium model [Tob88] 20 Starting Point Algorithm 1 Algorithm 2
Reference: [HPR89] <author> S.-P. Han, J.-S. Pang, and N. Rangaraj. </author> <title> Globally convergent Newton methods for nonsmooth equations. </title> <institution> Department of Mathematical Sciences, The Johns Hopkins University, </institution> <address> Baltimore, MD 21218, </address> <year> 1989. </year>
Reference-contexts: There are many known results on the global convergence of Newton type methods for nonlinear equations. Most of these methods are based on the use of a merit function. Several different types of merit function have been proposed in the literature (see for example <ref> [Bur80, SB80, Pol76, HPR89, HX90] </ref>). One of the main differences between many of the approaches is whether or not the underlying equations are smooth or nonsmooth. In the smooth case, global convergence is proven in many instances: we note in particular the results of Stoer [SB80]. <p> However, many of the approaches for solving the nonlinear complementarity problem consider reformulations of the problem as a system of nonsmooth equations. Global convergence can also be established here, for example, some pertinent results are given in <ref> [HPR89, Ral91] </ref>. fl This material is partially based on research supported by the Air Force Office of Scientific Research Grant AFOSR-89-0410, the National Science Foundation Grant CCR 9157632 and Istituto di Analisi dei Sistemi ed Informatica del CNR y Computer Sciences Department, University of Wisconsin, Madison, Wisconsin 53706 z Istituto di <p> In fact, some well known results from the literature can be cast in our framework. In <ref> [HPR89] </ref>, the following method is described. Let (x) = 1 2 kH (x)k and choose the search direction to satisfy H (x k ) + G (x k ; d k ) = 0 at each iteration. <p> Here, G (x k ; d) is an appropriate approximation of the directional derivative of H in the direction d at x k . The assumptions made in <ref> [HPR89] </ref> are essentially equivalent to the ones we make in Section 2. This can be seen by defining k (x k ; d k ): = 2 (x k ). In the same paper, a Gauss-Newton method is also proposed. The same merit function is used. <p> Particular instances of functions k which are considered are * k (d) = d T B k d where B k is a symmetric positive definite n fi n matrix * k (d) = fl fl fl + * k kdk where * k is a nonnegative scalar. In <ref> [HPR89] </ref> the conditions on k require in the first case that the sequence fB k g should have eigenvalues which are bounded away from zero and in the second case that f* k g should be bounded away from zero. <p> Other algorithmic approaches for solving NCP (F) have been tried. Recently, much emphasis has been placed on approaches using nonsmooth equations <ref> [Rob88, Pan89, HPR89, HX90] </ref>. We mention that there are many ways of formulating this problem as a system of nonsmooth equations. For example, the following ways are well-known. 17 1. <p> Certainly it is not implied by the conditions which are assumed to guarantee convergence for the nonsmooth algorithms mentioned above. In particular, the following example satisfies (26) but is not regular in the sense of <ref> [HPR89] </ref>. Example 10 Let F (x) = " x 1 . The solution set of this linear complementarity prob lem is f (0; ffi) j 0 ffi 1 g. <p> It can be shown that r (x) = 0 implies that x solves the linear complementarity problem and thus (26) is satisfied. However, all the solution points are not regular in the sense of <ref> [HPR89, page 15] </ref>. In order to justify performing a standard Newton method, we need the Jacobian of H to be nonsingular. Mangasarian [Man76] gives the following sufficient conditions to guarantee this nonsingularity. Proposition 11 Let x solve NCP (F) and satisfy x + F (x) &gt; 0.
Reference: [HX90] <author> P.T. Harker and B. Xiao. </author> <title> Newton's method for the nonlinear complementarity problem: A B-differentiable equation approach. </title> <journal> Mathematical Programming, </journal> <volume> 48 </volume> <pages> 339-358, </pages> <year> 1990. </year>
Reference-contexts: There are many known results on the global convergence of Newton type methods for nonlinear equations. Most of these methods are based on the use of a merit function. Several different types of merit function have been proposed in the literature (see for example <ref> [Bur80, SB80, Pol76, HPR89, HX90] </ref>). One of the main differences between many of the approaches is whether or not the underlying equations are smooth or nonsmooth. In the smooth case, global convergence is proven in many instances: we note in particular the results of Stoer [SB80]. <p> Other algorithmic approaches for solving NCP (F) have been tried. Recently, much emphasis has been placed on approaches using nonsmooth equations <ref> [Rob88, Pan89, HPR89, HX90] </ref>. We mention that there are many ways of formulating this problem as a system of nonsmooth equations. For example, the following ways are well-known. 17 1. <p> If rF (x) has nonsingular principal minors, then rH (x) is nonsingular. We present some results on standard nonlinear complementarity problems found in the literature. A fuller description of the problems can be found in <ref> [HX90] </ref>. Note that an entry of F in the table signifies that the algorithm failed to converge. In all the above examples we used the starting points suggested in [HX90]. <p> A fuller description of the problems can be found in <ref> [HX90] </ref>. Note that an entry of F in the table signifies that the algorithm failed to converge. In all the above examples we used the starting points suggested in [HX90].
Reference: [Jos79] <author> N.H. Josephy. </author> <title> Newton's method for generalized equations. </title> <type> Technical Summary Report 1965, </type> <institution> Mathematics Research Center, University of Wisconsin, Madison, Wisconsin, </institution> <year> 1979. </year>
Reference-contexts: aforementioned paper 18 Starting Point Algorithm 1 Algorithm 2 Algorithm 3 Gradient (Function) Gradient (Function) Gradient (Function) Evaluations Evaluations Evaluations (0,. . . ,0) F 30 (30) 35 (14) (100,. . . ,100) 19 (8) 19 (8) 18 (34) (1,0,0,0) 8 (3) 9 (3) 8 (3) Table 1: Josephy's example <ref> [Jos79] </ref> LCP Dimension Algorithm 1 Algorithm 2 Algorithm 3 (Started at Gradient (Function) Gradient (Function) Gradient (Function) Origin) Evaluations Evaluations Evaluations 8 23 (4) 18 (30) 8 (8) 32 17 (3) 15 (4) 8 (11) 128 21 (3) 18 (4) 9 (14) Table 2: Murty's exponential linear complementarity problem [Mur88] Starting
Reference: [Kiw85] <author> K.C. Kiwiel. </author> <title> Methods of Descent for Nondifferentiable Optimization. </title> <publisher> Springer-Verlag, </publisher> <year> 1985. </year>
Reference-contexts: Assumption (A2) follows from [PD78, Theorem 6.1] since it is shown that there exists an ff &gt; 0 such that for all ff 2 (0; ff k ) for any * &lt; 1. Assumption (A3) follows immediately from 2. For assumption (A4), it is proven in <ref> [Kiw85] </ref> that is subdifferentially regular and hence (A4) is equivalent to (A2). 5 Application to Nonlinear Complementarity Prob lems We consider applying the algorithms described in Section 3 to solve the nonlinear complementarity problem, NCP (F), namely to find x 2 IR n such that z 0; F (z) 0; hz;
Reference: [Man76] <author> O.L. Mangasarian. </author> <title> Equivalence of the complementarity problem to a system of nonlinear equations. </title> <journal> SIAM Journal of Applied Mathematics, </journal> <volume> 31 </volume> <pages> 89-92, </pages> <year> 1976. </year>
Reference-contexts: Much of this work was motivated by a desire to solve nonlinear complementarity problems. We are interested in using the smooth equations determined by Mangasarian <ref> [Man76] </ref> which are equivalent to the nonlinear complementarity problem. The essential difference is that while in least squares problems, the optimal value will not be zero, in this case it is, and so various new methods have been proposed. <p> In particular, Mangasarian <ref> [Man76] </ref> proved the following theorem. Theorem 9 x solves NCP (F) if and only if H (x) = 0, where H is defined in (42). In particular, note that H is differentiable. <p> However, all the solution points are not regular in the sense of [HPR89, page 15]. In order to justify performing a standard Newton method, we need the Jacobian of H to be nonsingular. Mangasarian <ref> [Man76] </ref> gives the following sufficient conditions to guarantee this nonsingularity. Proposition 11 Let x solve NCP (F) and satisfy x + F (x) &gt; 0. If rF (x) has nonsingular principal minors, then rH (x) is nonsingular. We present some results on standard nonlinear complementarity problems found in the literature.
Reference: [Mat87] <author> L. Mathiesen. </author> <title> An algorithm based on a sequence of linear complementarity problems applied to a Walrasian equilibrium model: An example. </title> <journal> Mathematical Programming, </journal> <volume> 37 </volume> <pages> 1-18, </pages> <year> 1987. </year>
Reference-contexts: 2: Murty's exponential linear complementarity problem [Mur88] Starting Point Algorithm 1 Algorithm 2 Algorithm 3 Gradient (Function) Gradient (Function) Gradient (Function) Evaluations Evaluations Evaluations (2.5,5.5,1.5,4.5) 6 (2) 21 (4) 19 (11) (2.5,1.5,1.5,3.5) 12 (3) 21 (4) 21 (16) (1.0,1.0,1.0,1.0) F 20 (4) 20 (17) Table 3: Mathiesen's Walrasian equilibrium model <ref> [Mat87] </ref> 19 Starting Point Algorithm 1 Algorithm 2 Algorithm 3 Gradient (Function) Gradient (Function) Gradient (Function) Evaluations Evaluations Evaluations x 1 19 (3) 15 (3) 18 (22) x 1 = (0:2; 0:2; 0:2; 0:1; 0:1; 0:2; 0:5; 0:0; 4:0; 0:0; 0:0; 0:0; 0:4; 0:0) Table 4: Scarf's economic equilibrium model [Sca73]
Reference: [Mur88] <author> K.G. Murty. </author> <title> Linear Complementarity, Linear and Nonlinear Programming. </title> <address> Helderman-Verlag, Berlin, </address> <year> 1988. </year>
Reference-contexts: example [Jos79] LCP Dimension Algorithm 1 Algorithm 2 Algorithm 3 (Started at Gradient (Function) Gradient (Function) Gradient (Function) Origin) Evaluations Evaluations Evaluations 8 23 (4) 18 (30) 8 (8) 32 17 (3) 15 (4) 8 (11) 128 21 (3) 18 (4) 9 (14) Table 2: Murty's exponential linear complementarity problem <ref> [Mur88] </ref> Starting Point Algorithm 1 Algorithm 2 Algorithm 3 Gradient (Function) Gradient (Function) Gradient (Function) Evaluations Evaluations Evaluations (2.5,5.5,1.5,4.5) 6 (2) 21 (4) 19 (11) (2.5,1.5,1.5,3.5) 12 (3) 21 (4) 21 (16) (1.0,1.0,1.0,1.0) F 20 (4) 20 (17) Table 3: Mathiesen's Walrasian equilibrium model [Mat87] 19 Starting Point Algorithm 1 Algorithm
Reference: [OR70] <author> J.M. Ortega and W.C. Rheinboldt. </author> <title> Iterative Solution of Nonlinear Equations in Several Variables. </title> <publisher> Academic Press, </publisher> <year> 1970. </year>
Reference-contexts: Our formulation also relies on an auxiliary function, : IR n+n+1 ! IR, which is a generalization of the familiar notion of a forcing function <ref> [OR70] </ref>. The relationship between these constructs will be described in the sequel. In order to to obtain a method for the solution of (1) we define a general stabilization scheme that includes different strategies for enforcing global convergence without requiring a monotonic reduction of the objective function.
Reference: [Pan89] <author> J.-S. Pang. </author> <title> A B-differentiable equation based, globally and locally quadratically convergent algorithm for nonlinear programs, complementarity and variational inequality problems. </title> <institution> Department of Mathematical Sciences, The Johns Hopkins University, </institution> <address> Baltimore, MD 21218, </address> <year> 1989. </year>
Reference-contexts: Other algorithmic approaches for solving NCP (F) have been tried. Recently, much emphasis has been placed on approaches using nonsmooth equations <ref> [Rob88, Pan89, HPR89, HX90] </ref>. We mention that there are many ways of formulating this problem as a system of nonsmooth equations. For example, the following ways are well-known. 17 1.
Reference: [PD78] <author> B.N. Pshenichny and Yu. M. Danilin. </author> <title> Numerical Methods in Extremal Problems. </title> <publisher> MIR Publishers, </publisher> <address> Moscow, </address> <year> 1978. </year>
Reference-contexts: In this case, the merit function is given by (x): = kH (x)k r for some r &gt; 0. An interesting extension to the work of Pshenichny and Danilin <ref> [PD78] </ref> on minimax problems can be seen by using our formulation. In this case the merit function is given by (x) = max jH i (x)j where H i are assumed to be continuously differentiable functions whose gradients satisfy a Lipschitz condition. <p> It can be shown that these assumptions imply the assumptions of the previous section. Let k (x k ; d k ) = *(x k ), for some * 2 (0; 1). Assumption (A1) is then immediate. Assumption (A2) follows from <ref> [PD78, Theorem 6.1] </ref> since it is shown that there exists an ff &gt; 0 such that for all ff 2 (0; ff k ) for any * &lt; 1. Assumption (A3) follows immediately from 2.
Reference: [Pol76] <author> E. Polak. </author> <title> On the global stabilization of locally convergent algorithms. </title> <journal> Automatica, </journal> <volume> 12 </volume> <pages> 337-349, </pages> <year> 1976. </year>
Reference-contexts: There are many known results on the global convergence of Newton type methods for nonlinear equations. Most of these methods are based on the use of a merit function. Several different types of merit function have been proposed in the literature (see for example <ref> [Bur80, SB80, Pol76, HPR89, HX90] </ref>). One of the main differences between many of the approaches is whether or not the underlying equations are smooth or nonsmooth. In the smooth case, global convergence is proven in many instances: we note in particular the results of Stoer [SB80].
Reference: [Pow69] <author> M.J.D. Powell. </author> <title> A method for nonlinear constraints in minimization problems. </title> <editor> In R. Fletcher, editor, </editor> <booktitle> Optimization, </booktitle> <pages> pages 283-298. </pages> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1969. </year>
Reference-contexts: x 3 24 (4) 28 (40) 54 (79) x 1 = (0; 2; 2; 0; 0; 2; 0; 0; 1; 1; 0; 0; 0; 0; 0) x 3 = (0; 10; 10; 0; 0; 10; 0; 0; 1; 1; 0; 0; 0; 0; 0) Table 7: Powell's nonlinear programming problem <ref> [Pow69] </ref> Starting Point Algorithm 1 Algorithm 2 Algorithm 3 Gradient (Function) Gradient (Function) Gradient (Function) Evaluations Evaluations Evaluations (0,. . . ,0) F 33 (24) 35 (34) Table 8: Spatial competition example [Har86] 21 were considered too close to an optimal point.
Reference: [Ral91] <author> D. Ralph. </author> <title> Global convergence of damped Newton's method for nonsmooth equations, via the path search. </title> <type> Technical Report 90-1181, </type> <institution> Computer Science Department, Cornell University, </institution> <address> Ithaca, NY 14853, </address> <year> 1991. </year> <month> 23 </month>
Reference-contexts: However, many of the approaches for solving the nonlinear complementarity problem consider reformulations of the problem as a system of nonsmooth equations. Global convergence can also be established here, for example, some pertinent results are given in <ref> [HPR89, Ral91] </ref>. fl This material is partially based on research supported by the Air Force Office of Scientific Research Grant AFOSR-89-0410, the National Science Foundation Grant CCR 9157632 and Istituto di Analisi dei Sistemi ed Informatica del CNR y Computer Sciences Department, University of Wisconsin, Madison, Wisconsin 53706 z Istituto di
Reference: [Rob88] <author> S.M. Robinson. </author> <title> Newton's method for a class of nonsmooth functions. </title> <type> Working paper, </type> <institution> Department of Industrial Engineering, University of Wisconsin, Madison, Wisconsin, </institution> <year> 1988. </year>
Reference-contexts: Other algorithmic approaches for solving NCP (F) have been tried. Recently, much emphasis has been placed on approaches using nonsmooth equations <ref> [Rob88, Pan89, HPR89, HX90] </ref>. We mention that there are many ways of formulating this problem as a system of nonsmooth equations. For example, the following ways are well-known. 17 1.
Reference: [SB80] <author> J. Stoer and R. </author> <title> Bulirsch. Introduction to Numerical Analysis. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: There are many known results on the global convergence of Newton type methods for nonlinear equations. Most of these methods are based on the use of a merit function. Several different types of merit function have been proposed in the literature (see for example <ref> [Bur80, SB80, Pol76, HPR89, HX90] </ref>). One of the main differences between many of the approaches is whether or not the underlying equations are smooth or nonsmooth. In the smooth case, global convergence is proven in many instances: we note in particular the results of Stoer [SB80]. <p> One of the main differences between many of the approaches is whether or not the underlying equations are smooth or nonsmooth. In the smooth case, global convergence is proven in many instances: we note in particular the results of Stoer <ref> [SB80] </ref>. However, many of the approaches for solving the nonlinear complementarity problem consider reformulations of the problem as a system of nonsmooth equations.
Reference: [Sca73] <author> H.E. Scarf. </author> <title> The Computation of Economic Equilibria. </title> <publisher> Yale University Press, </publisher> <address> New Haven, Conneticut, </address> <year> 1973. </year>
Reference-contexts: [Mat87] 19 Starting Point Algorithm 1 Algorithm 2 Algorithm 3 Gradient (Function) Gradient (Function) Gradient (Function) Evaluations Evaluations Evaluations x 1 19 (3) 15 (3) 18 (22) x 1 = (0:2; 0:2; 0:2; 0:1; 0:1; 0:2; 0:5; 0:0; 4:0; 0:0; 0:0; 0:0; 0:4; 0:0) Table 4: Scarf's economic equilibrium model <ref> [Sca73] </ref> Starting Point Algorithm 1 Algorithm 2 Algorithm 3 Gradient (Function) Gradient (Function) Gradient (Function) Evaluations Evaluations Evaluations (1,. . . ,1) 15 (3) 15 (3) 13 (18) Table 5: Nash noncooperative game example [Har88] Starting Point Algorithm 1 Algorithm 2 Algorithm 3 Gradient (Function) Gradient (Function) Gradient (Function) Evaluations Evaluations
Reference: [Sub85] <author> P.K. Subramanian. </author> <title> Iterative Methods of Solution for Complementarity Problems. </title> <type> PhD thesis, </type> <institution> University of Wisconsin, Madison, Wisconsin, </institution> <year> 1985. </year>
Reference-contexts: The essential difference is that while in least squares problems, the optimal value will not be zero, in this case it is, and so various new methods have been proposed. In particular, the method of Subramanian <ref> [Sub85] </ref> gives a direction which is consistent. The results in this paper show that our method can be used to establish global convergence in the case where the Jacobian is singular at the solution point (Dennis and Schnabel [DS83] give other results using the trust region approach). <p> In the remainder of this section we propose several techniques for dealing with the case when rH (x) is singular or badly conditioned. We consider three techniques for modifying the Newton direction. The first possibility is to consider a modified Gauss-Newton direction, along the lines of the work of <ref> [Sub85] </ref>. <p> (d) = d T B k d + c 1 F k kdk where B k is a symmetric positive semidefinite n fi n matrix * k (d) = fl fl fl + c 1 F k kdk The motivation for this type of approach comes from the work of <ref> [Sub85] </ref> and can be thought of a a generalization of Algorithm 1 from the previous section. The work of Burdakov [Bur80] can also be seen to be a special case of the method given in Section 2.
Reference: [Tob88] <author> R.L. Tobin. </author> <title> A variable dimension solution approach for the general spatial equilibrium problem. </title> <journal> Mathematical Programming, </journal> <volume> 40 </volume> <pages> 33-51, </pages> <year> 1988. </year> <month> 24 </month>
Reference-contexts: (18) Table 5: Nash noncooperative game example [Har88] Starting Point Algorithm 1 Algorithm 2 Algorithm 3 Gradient (Function) Gradient (Function) Gradient (Function) Evaluations Evaluations Evaluations (0,. . . ,0) 28 (23) 28 (40) 42 (101) x 1 34 (20) 25 (40) 24 (62) Table 6: Tobin's spatial price equilibrium model <ref> [Tob88] </ref> 20 Starting Point Algorithm 1 Algorithm 2 Algorithm 3 Gradient (Function) Gradient (Function) Gradient (Function) Evaluations Evaluations Evaluations x 1 53 (7) 28 (40) F x 3 24 (4) 28 (40) 54 (79) x 1 = (0; 2; 2; 0; 0; 2; 0; 0; 1; 1; 0; 0; 0; 0;
References-found: 28

