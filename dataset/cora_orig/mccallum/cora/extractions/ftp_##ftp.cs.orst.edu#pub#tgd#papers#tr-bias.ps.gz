URL: ftp://ftp.cs.orst.edu/pub/tgd/papers/tr-bias.ps.gz
Refering-URL: http://www.cs.orst.edu/~tgd/cv/pubs.html
Root-URL: 
Email: tgd@cs.orst.edu  ebkong@cs.orst.edu  
Title: Machine Learning Bias, Statistical Bias, and Statistical Variance of Decision Tree Algorithms  
Author: Thomas G. Dietterich Eun Bae Kong 
Address: 303 Dearborn Hall  Corvallis, OR 97331-3202  
Affiliation: Department of Computer Science  Oregon State University  
Abstract: The term "bias" is widely used|and with different meanings|in the fields of machine learning and statistics. This paper clarifies the uses of this term and shows how to measure and visualize the statistical bias and variance of learning algorithms. Statistical bias and variance can be applied to diagnose problems with machine learning bias, and the paper shows four examples of this. Finally, the paper discusses methods of reducing bias and variance. Methods based on voting can reduce variance, and the paper compares Breiman's bagging method and our own tree randomization method for voting decision trees. Both methods uniformly improve performance on data sets from the Irvine repository. Tree randomization yields perfect performance on the Letter Recognition task. A weighted nearest neighbor algorithm based on the infinite bootstrap is also introduced. In general, decision tree algorithms have moderate-to-high variance, so an important implication of this work is that variance|rather than appropriate or inappropriate machine learning bias|is an important cause of poor performance for decision tree algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L. </author> <year> (1994). </year> <title> Bagging predictors. </title> <type> Tech. rep. 421, </type> <institution> Department of Statistics, University of California, Berkeley, </institution> <address> CA. </address>
Reference: <author> Buntine, W. L. </author> <year> (1990). </year> <title> A Theory of Learning Classification Rules. </title> <type> Ph.D. thesis, </type> <institution> University of Technology, Sydney. </institution>
Reference-contexts: There are many different ways of producing and voting the hypotheses, and this is a very active topic of research, particularly in the neural network community (Perrone, 1993; Perrone & Cooper, 1993; Perrone, 1994). There are strong Bayesian justifications for voting as well <ref> (Buntine, 1990) </ref>. We explored two methods for generating multiple hypotheses. The first is bootstrapping (Efron & Tibshirani, 1993; Breiman, 1994). Many equally-plausible decision trees can be 8 constructed by the following procedure. Let S be the available training set of size m.
Reference: <author> Clearwater, S., & Provost, F. </author> <year> (1990). </year> <title> RL4: A tool for knowledge-based induction. </title> <booktitle> In Proceedings of the Second International IEEE Conference on Tools for Artificial Intelligence, </booktitle> <pages> pp. 24-30. </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Further improvements can probably be made here. We predict that the Markov Tree models of Jordan et al (1991, 1994), which are soft, stochastic versions of decision trees, would have much lower variance than C4.5. We also predict that other learning systems, such as RL <ref> (Clearwater & Provost, 1990) </ref>, which learn collections of independent rules, will have lower variance also, because they reduce the cascading of decisions that results from the top-down construction of decision trees.
Reference: <author> Dietterich, T. G., & Bakiri, G. </author> <year> (1991). </year> <title> Error-correcting output codes: A general method for improving multiclass inductive learning programs. </title> <booktitle> In Proc. of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pp. 572-577. </pages> <publisher> AAAI Press/MIT Press. </publisher>
Reference: <author> Dietterich, T. G., & Bakiri, G. </author> <year> (1995). </year> <title> Solving multiclass learning problems via error-correcting output codes. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <pages> 263-286. </pages>
Reference-contexts: From this and many other test problems, we conclude that pruning does not necessarily work the way its advocates claim. In our experience, it rarely produces improvement in classification accuracy <ref> (Dietterich & Bakiri, 1995) </ref>. A very general technique for reducing variance is to construct a set of hypotheses and then have them vote on the classification of test cases. <p> To classify a new example, each of the "bit-position hypotheses" is evaluated to produce a binary string of 2-class decisions. This string is then mapped to the nearest legal codeword (in Hamming distance) to classify the example. See Dietterich and Bakiri (1991, 1995) for more details. A companion paper <ref> (Kong & Dietterich, 1995) </ref> shows that the bias errors made in each of the bit-position hypotheses can be substantially uncorrelated, so that the error-correction procedure can correct for bias errors in the algorithm. We applied this procedure to the 6-class learning problem from Figure 4 with the following results.
Reference: <author> Efron, B., & Tibshirani, R. J. </author> <year> (1993). </year> <title> An Introduction to the Bootstrap. </title> <publisher> Chapman and Hall, </publisher> <address> New York, NY. </address>
Reference: <author> Geman, S., Bienenstock, E., & Doursat, R. </author> <year> (1992). </year> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 (1), </volume> <pages> 1-58. </pages>
Reference-contexts: This variation can result from variation in the training sample, from random noise (*) in the training data, or from random behavior in the learning algorithm itself, such as the random initial weights often used in backpropagation. It can be shown <ref> (e.g., Geman, Bienenstock, & Doursat, 1992) </ref> that the average error of learning algorithm A at point x is equal to the squared bias plus the variance: Error (A; m; x) = Bias (A; m; x) 2 + V ariance (A; m; x): Hence, an important goal in algorithm design is to
Reference: <author> Jacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton, G. E. </author> <year> (1991). </year> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3 (1), </volume> <pages> 79-87. </pages>
Reference: <author> Jordan, M. I. </author> <year> (1994). </year> <title> A statistical approach to decision tree modeling. </title> <booktitle> In Proc. 7th Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <pages> pp. 13-20. </pages> <publisher> ACM Press, </publisher> <address> New York, NY. </address> <note> 12 Kong, </note> <author> E. B., & Dietterich, T. G. </author> <year> (1995). </year> <title> Error-correcting output coding works by correcting bias and variance. </title> <booktitle> In Submitted to the International Conference on Machine Learning. </booktitle>
Reference: <author> Kwok, S. W., & Carter, C. </author> <year> (1990). </year> <title> Multiple decision trees. </title> <editor> In Schachter, R. D., Levitt, T. S., Kannal, L. N., & Lemmer, J. F. (Eds.), </editor> <booktitle> Uncertainty in Artificial Intelligence 4, </booktitle> <pages> pp. 327-335. </pages> <publisher> Elsevier Science, Amsterdam. </publisher>
Reference: <author> Littlestone, N., & Warmuth, M. K. </author> <year> (1989). </year> <title> The weighted majority algorithm. </title> <booktitle> In Proc. 30th Annu. IEEE Sympos. Found. Comput. Sci., </booktitle> <pages> pp. 256-261. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA. </address>
Reference-contexts: For example, virtually none of the work in computational learning theory has addressed the bias/variance tradeoff and the nature of bias and variance in predicting the effectiveness of algorithms. For example, the boosting (Schapire, 1990) algorithm appears to be a variance-reduction algorithm, while the weighted majority algorithm <ref> (Littlestone & Warmuth, 1989) </ref> could provide both variance and bias reduction.
Reference: <author> Mitchell, T. M. </author> <year> (1980). </year> <title> The need for biases in learning generalizations. </title> <type> Tech. rep. </type> <institution> CBM-TR-117, Rutgers University, </institution> <address> New Brunswick, NJ. </address>
Reference: <author> Perrone, M. P. </author> <year> (1993). </year> <title> Improving regression estimation: Averaging methods for variance reduction with extensions to general convex measure optimization. </title> <type> Ph.D. thesis, </type> <institution> Brown University, Institute for Brain and Neural Systems. </institution>
Reference: <author> Perrone, M. P. </author> <year> (1994). </year> <title> Putting it all together: Methods for combining neural networks. </title> <editor> In Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 6, </volume> <pages> pp. 1188-1189. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Perrone, M. P., & Cooper, L. N. </author> <year> (1993). </year> <title> When networks disagree: Ensemble methods for hybrid neural networks. </title> <editor> In Mammone, R. J. (Ed.), </editor> <title> Neural networks for speech and image processing. </title> <publisher> Chapman and Hall. </publisher>
Reference: <author> Schapire, R. E. </author> <year> (1990). </year> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5 (2), </volume> <pages> 197-227. </pages>
Reference-contexts: There are many additional implications of the bias/variance perspective on learning algorithms. For example, virtually none of the work in computational learning theory has addressed the bias/variance tradeoff and the nature of bias and variance in predicting the effectiveness of algorithms. For example, the boosting <ref> (Schapire, 1990) </ref> algorithm appears to be a variance-reduction algorithm, while the weighted majority algorithm (Littlestone & Warmuth, 1989) could provide both variance and bias reduction.
Reference: <author> Shavlik, J. and Dietterich, T.G. </author> <year> (1990). </year> <title> Readings in Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address> <month> 13 </month>
Reference-contexts: For example, the decision tree algorithms (e.g., C4.5, CART) consider small trees before they consider larger ones. If these algorithms find a small tree that can correctly classify the training data, then a larger one is not considered. The field of supervised learning has been described <ref> (Shavlik, J. and Dietterich, T.G., 1990) </ref> as the study of biases| their expressive power, their computational complexity, and their sample complexity (i.e., the number of examples required to produce accurate generalization).
References-found: 17

