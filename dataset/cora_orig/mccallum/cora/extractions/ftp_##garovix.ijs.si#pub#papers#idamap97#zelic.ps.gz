URL: ftp://garovix.ijs.si/pub/papers/idamap97/zelic.ps.gz
Refering-URL: http://www-ai.ijs.si/ailab/activities/idamap97.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Induction of decision trees and Bayesian classification applied to diagnosis of sport injuries  
Author: Igor Zelic INFONET Planina Igor Kononenko Nada Lavrac J. Stefan Vanja Vuga 
Address: Slovenia  Trzaska 25 1001 Ljubljana Slovenia  Jamova 39 1001 Ljubljana Slovenia  Celovska 25 1001 Ljubljana Slovenia  
Affiliation: 4000 Kranj  Faculty of Computer and Information Science  Institute  University Medical Centre Center for Sport Medicine  
Abstract: Machine learning techniques can be used to extract knowledge from data stored in medical databases. In our application, various machine learning algorithms were used to extract diagnostic knowledge to support the diagnosis of sport injuries. The applied methods include variants of the Assistant algorithm for top-down induction of decision trees, and variants of the Bayesian classifier. The available dataset was insufficent for reliable diagnosis of all sport injuries considered by the system. Consequently, expert-defined diagnostic rules were added and used as pre-classifiers or as generators of additional training instances for injuries with few training examples. Experimental results show that the classification accuracy and the explanation capability of the naive Bayesian classifier with the fuzzy discretization of numerical attributes was superior to other methods and was estimated as the most appro priate for practical use.
Abstract-found: 1
Intro-found: 1
Reference: [ 1 ] <author> Aha, D., Kibler, D., and Albert, M. </author> <title> (1991) Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6: </volume> <pages> 37-66. </pages>
Reference-contexts: be classified into three major groups [ 13 ] : inductive learning of symbolic rules (such as induction of rules [ 17; 6 ] , decision trees [ 20 ] and induction of logic programs [ 15 ] ), statistical or pattern-recognition methods (such as k-nearest neighbors or instance-based learning <ref> [ 7; 1 ] </ref> , discriminate analysis and Bayesian classifiers), and artificial neural networks (such as networks with backpropagation learning, Kohonen's self-organizing network and Hopfield's associative memory [ 2 ] ).
Reference: [ 2 ] <author> Anderson, J.A. and Rosenfeld, E. </author> <year> (1988). </year> <title> Neu-rocomputing: </title> <booktitle> Foundations of Research. </booktitle> <publisher> The MIT Press. </publisher>
Reference-contexts: [ 20 ] and induction of logic programs [ 15 ] ), statistical or pattern-recognition methods (such as k-nearest neighbors or instance-based learning [ 7; 1 ] , discriminate analysis and Bayesian classifiers), and artificial neural networks (such as networks with backpropagation learning, Kohonen's self-organizing network and Hopfield's associative memory <ref> [ 2 ] </ref> ).
Reference: [ 3 ] <author> Cestnik B. </author> <year> (1990). </year> <title> Estimating probabilities: A crucial task in machine learning, </title> <booktitle> Proc. European Conf. on Artificial Intelligence, </booktitle> <address> Stockholm, </address> <month> August, </month> <year> 1990, </year> <pages> pp. 147-149. </pages>
Reference-contexts: For example, RELIEF can efficiently estimate the quality of attributes in parity problems. In addition, wherever appropriate, instead of the relative frequency, Assistant-R uses the m-estimate of probabilities, which was shown to often improve the performance of machine learning algorithms <ref> [ 3 ] </ref> . Assistant-I is a variant of Assistant-R that, instead of ReliefF, uses the information gain as the selection criterion, the same as the original Assistant algorithm. However, other differences to Assistant remain (such as the use of the m-estimate of probabilities). <p> We use the m-estimate <ref> [ 3 ] </ref> for comput ing the estimate of conditional probabilities: P (CjV i ) = N (V i ) + m N (C&V i ) + N (V i ) + m where N (Cond) stands for the number of examples for which Cond is fulfilled, and m is a <p> The parameter m trades-off the contribution of the relative frequency and the prior probability. In our experiments, the parameter m was set to 2.0 (this setting is usually used as a default and, empirically, gives satisfactory results <ref> [ 3 ] </ref> ). For computing the prior probability, the Laplace law of succession is used [ 19 ] : P (C) = N ex + N cl where N ex stands for the number of examples and N cl for the number of classes.
Reference: [ 4 ] <author> Cestnik, B., Kononenko, I., and Bratko, I. </author> <year> (1987). </year> <title> ASSISTANT 86: A knowledge elicitation tool for sophisticated users. </title> <editor> In I. Bratko and N. Lavrac, editors, </editor> <booktitle> Progress in Machine Learning, </booktitle> <pages> pages 31-45. </pages> <publisher> Sigma Press, Wilmslow. </publisher>
Reference-contexts: Assistant-R is a reimplementation of the Assistant learning system for top down induction of decision trees <ref> [ 4 ] </ref> . The basic algorithm goes back to CLS (Concept Learning System) developed by Hunt et al. [ 8 ] and reim-plemented by several authors (see [ 20 ] for an overview). <p> Positions of attributes in the tree, especially the top (most informative) ones, often directly correspond to domain expert's knowledge. However, in order to produce general rules, these methods use pruning <ref> [ 20; 4 ] </ref> which drastically reduces tree sizes. Consequently, the paths from the root to the leaves are shorter, containing only few most informative attributes.
Reference: [ 5 ] <author> Clark, P. and Boswell, R. </author> <year> (1991). </year> <title> Rule induction with CN2: Some recent improvements. </title> <booktitle> In Proc. Fifth European Working Session on Learning, </booktitle> <pages> pages 151-163. </pages> <publisher> Springer, </publisher> <address> Berlin. </address>
Reference: [ 6 ] <author> Clark, P. and Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3(4) </volume> <pages> 261-283. </pages>
Reference-contexts: In recent years, many different machine learning systems were developed. Machine learning methods [ 16; 18 ] can be classified into three major groups [ 13 ] : inductive learning of symbolic rules (such as induction of rules <ref> [ 17; 6 ] </ref> , decision trees [ 20 ] and induction of logic programs [ 15 ] ), statistical or pattern-recognition methods (such as k-nearest neighbors or instance-based learning [ 7; 1 ] , discriminate analysis and Bayesian classifiers), and artificial neural networks (such as networks with backpropagation learning, Kohonen's
Reference: [ 7 ] <editor> Dasarathy, B.V., editor. </editor> <year> (1990). </year> <title> Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques. </title> <publisher> IEEE Computer Society Press, </publisher> <address> Los Alami-tos, CA. </address>
Reference-contexts: be classified into three major groups [ 13 ] : inductive learning of symbolic rules (such as induction of rules [ 17; 6 ] , decision trees [ 20 ] and induction of logic programs [ 15 ] ), statistical or pattern-recognition methods (such as k-nearest neighbors or instance-based learning <ref> [ 7; 1 ] </ref> , discriminate analysis and Bayesian classifiers), and artificial neural networks (such as networks with backpropagation learning, Kohonen's self-organizing network and Hopfield's associative memory [ 2 ] ).
Reference: [ 8 ] <author> Hunt, E., Martin, J., and Stone, P. </author> <year> (1966). </year> <title> Experiments in Induction. </title> <address> New York, </address> <publisher> Academic Press. </publisher>
Reference-contexts: Assistant-R is a reimplementation of the Assistant learning system for top down induction of decision trees [ 4 ] . The basic algorithm goes back to CLS (Concept Learning System) developed by Hunt et al. <ref> [ 8 ] </ref> and reim-plemented by several authors (see [ 20 ] for an overview).
Reference: [ 9 ] <author> Kira K. and Rendell L. </author> <year> (1992). </year> <title> A practical approach to feature selection. </title> <editor> In: D.Sleeman and P.Edwards (eds.), </editor> <booktitle> Proc. Int. Conf. on Machine Learning ICML-92 (Aberdeen, </booktitle> <address> July 1992). </address> <publisher> Morgan Kaufmann, pp.249-256. </publisher>
Reference-contexts: The main difference between Assistant and its reimplementation Assistant-R is that ReliefF is used as a heuristic for attribute selection [ 11 ] . ReliefF is an extended version of RELIEF, developed by Kira and Ren-dell <ref> [ 9 ] </ref> , which is a non-myopic heuristic measure that is able to estimate the quality of attributes even if there are strong conditional dependencies between attributes. For example, RELIEF can efficiently estimate the quality of attributes in parity problems.
Reference: [ 10 ] <author> Kononenko, I. </author> <year> (1993). </year> <title> Inductive and Bayesian learning in medical diagnosis. </title> <journal> Applied Artificial Intelligence, </journal> <volume> 7 </volume> <pages> 317-337. </pages>
Reference-contexts: the application of the `black-box' neural networks was considered inappropriate; we have also limited the selection of systems to several variants of top-down decision tree learners and to several variants of the Bayesian classifier that have proved to be well suited for supporting diagnostic decision making in numerous medical domains <ref> [ 10 ] </ref> . The paper is organized as follows. Section 2 briefly describes the algorithms used in our study. Section 3 gives the description of the problem of sport injury diagnosis. <p> The relative performance of the naive Bayesian classifier can serve as an estimate of the conditional independence of attributes. Semi-naive Bayesian classifier is an extension of the naive Bayesian classifier that explicitly searches for dependencies between the values of different attributes <ref> [ 10 ] </ref> . <p> Fuzzy discretization overcomes this problem by considering the values of the continuous attribute (or, equivalently, the boundaries of intervals) as fuzzy values instead of point values <ref> [ 10 ] </ref> . <p> Therefore, it is considered as the most promising machine learning classifier that can support physicians' decisions. Decision trees are considered to be inapropriate due to the low number of attributes that they take into account. These conclusions are in agreement with previous studies in medicine <ref> [ 10 ] </ref> . Assistant's performance was less successful, which is mostly due to the large number of diagnostic classes. In this domain, ReliefF used by Assistant-R is a better heuristic for estimating the quality of attributes than the standard information gain heuristic used by Assistant-I.
Reference: [ 11 ] <author> Kononenko, I. </author> ( <year> 1994). </year> <title> Estimating attributes: Analysis and extensions of RELIEF. </title> <editor> In: F. Bergadano and L. de Readt (eds.), </editor> <booktitle> Proc. European Conf. on Machine Learning ECML-94, (Catania, </booktitle> <address> Sicily, April 1994) , Springer Verlag, pp.171-182. </address>
Reference-contexts: The main difference between Assistant and its reimplementation Assistant-R is that ReliefF is used as a heuristic for attribute selection <ref> [ 11 ] </ref> . ReliefF is an extended version of RELIEF, developed by Kira and Ren-dell [ 9 ] , which is a non-myopic heuristic measure that is able to estimate the quality of attributes even if there are strong conditional dependencies between attributes.
Reference: [ 12 ] <author> Kononenko, I. and Bratko, I. </author> <year> (1991). </year> <title> Information-based evaluation criterion for classifier's performance. </title> <journal> Machine Learning, </journal> <volume> 6(1): </volume> <pages> 67-80. </pages>
Reference: [ 13 ] <author> Kononenko, I., and Kukar, M. </author> <year> (1995). </year> <title> Machine learning for medical diagnosis. </title> <editor> In: N. Lavrac (ed.) </editor> <booktitle> Proc. Workshop on Computer-Aided Data Analysis in Medicine, </booktitle> <address> CADAM-95, (Bled, </address> <month> November </month> <year> 1995), </year> <title> IJS Scientific Publishing, </title> <address> Ljubljana. </address>
Reference-contexts: Moreover, to support diagnostic decisions, reasonably high diagnostic accuracy has to be achieved, as well as the transparency of proposed solutions. In recent years, many different machine learning systems were developed. Machine learning methods [ 16; 18 ] can be classified into three major groups <ref> [ 13 ] </ref> : inductive learning of symbolic rules (such as induction of rules [ 17; 6 ] , decision trees [ 20 ] and induction of logic programs [ 15 ] ), statistical or pattern-recognition methods (such as k-nearest neighbors or instance-based learning [ 7; 1 ] , discriminate analysis
Reference: [ 14 ] <author> Kononenko, I. and Simec, E. </author> <year> (1995). </year> <title> Induction of decision trees using RELIEFF. </title> <editor> In: G. Della Ric-cia, R. Kruse and R. Viertl (eds.), </editor> <booktitle> Proc. of ISSEK Workshop on Mathematical and Statistical Methods in Artificial Intelligence, </booktitle> <address> (Udine, September 1994), </address> <publisher> Springer Verlag, pp.199-220. </publisher>
Reference-contexts: We used several decision tree learners and several variants of the Bayesian classifier. 2.1 Decision tree learners Three variants of the Assistant algorithm were used in our experiments: Assistant-R, Assistant-I, and Assistant-R2 <ref> [ 14 ] </ref> . Assistant-R is a reimplementation of the Assistant learning system for top down induction of decision trees [ 4 ] .
Reference: [ 15 ] <author> Lavrac, N. and Dzeroski, S. </author> <year> (1994). </year> <title> Inductive Logic Programming: Techniques and Applications. </title> <publisher> Ellis Horwood, </publisher> <address> Chichester. </address>
Reference-contexts: Machine learning methods [ 16; 18 ] can be classified into three major groups [ 13 ] : inductive learning of symbolic rules (such as induction of rules [ 17; 6 ] , decision trees [ 20 ] and induction of logic programs <ref> [ 15 ] </ref> ), statistical or pattern-recognition methods (such as k-nearest neighbors or instance-based learning [ 7; 1 ] , discriminate analysis and Bayesian classifiers), and artificial neural networks (such as networks with backpropagation learning, Kohonen's self-organizing network and Hopfield's associative memory [ 2 ] ).
Reference: [ 16 ] <author> Michalski, R.S., Carbonell, J.G., and Mitchell, </author> <title> T.M., editors (1983). Machine Learning: An Artificial Intelligence Approach, Volume I. </title> <publisher> Tioga, </publisher> <address> Palo Alto, CA. </address>
Reference-contexts: Moreover, to support diagnostic decisions, reasonably high diagnostic accuracy has to be achieved, as well as the transparency of proposed solutions. In recent years, many different machine learning systems were developed. Machine learning methods <ref> [ 16; 18 ] </ref> can be classified into three major groups [ 13 ] : inductive learning of symbolic rules (such as induction of rules [ 17; 6 ] , decision trees [ 20 ] and induction of logic programs [ 15 ] ), statistical or pattern-recognition methods (such as k-nearest
Reference: [ 17 ] <author> Michalski, R.S., Mozetic, I., Hong, J., and Lavrac, N. </author> <year> (1986). </year> <title> The multi-purpose incremental learning system AQ15 and its testing application on three medical domains. </title> <booktitle> In Proc. Fifth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1041-1045. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: In recent years, many different machine learning systems were developed. Machine learning methods [ 16; 18 ] can be classified into three major groups [ 13 ] : inductive learning of symbolic rules (such as induction of rules <ref> [ 17; 6 ] </ref> , decision trees [ 20 ] and induction of logic programs [ 15 ] ), statistical or pattern-recognition methods (such as k-nearest neighbors or instance-based learning [ 7; 1 ] , discriminate analysis and Bayesian classifiers), and artificial neural networks (such as networks with backpropagation learning, Kohonen's
Reference: [ 18 ] <editor> Michie, D., Spiegelhalter, D.J., and Taylor, C.C., editors (1994). </editor> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Ellis Horwood, </publisher> <address> Chichester. </address>
Reference-contexts: Moreover, to support diagnostic decisions, reasonably high diagnostic accuracy has to be achieved, as well as the transparency of proposed solutions. In recent years, many different machine learning systems were developed. Machine learning methods <ref> [ 16; 18 ] </ref> can be classified into three major groups [ 13 ] : inductive learning of symbolic rules (such as induction of rules [ 17; 6 ] , decision trees [ 20 ] and induction of logic programs [ 15 ] ), statistical or pattern-recognition methods (such as k-nearest
Reference: [ 19 ] <author> Niblett, T. and Bratko, I. </author> <title> (1986) Learning decision rules in noisy domains. </title> <editor> In Bramer, M. (ed.) </editor> <booktitle> Research and Development in Expert Systems III, </booktitle> <publisher> Cambridge University Press, </publisher> <pages> pp. 24-25. </pages>
Reference-contexts: In our experiments, the parameter m was set to 2.0 (this setting is usually used as a default and, empirically, gives satisfactory results [ 3 ] ). For computing the prior probability, the Laplace law of succession is used <ref> [ 19 ] </ref> : P (C) = N ex + N cl where N ex stands for the number of examples and N cl for the number of classes. The relative performance of the naive Bayesian classifier can serve as an estimate of the conditional independence of attributes.
Reference: [ 20 ] <author> Quinlan, J.R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning 1(1): </booktitle> <pages> 81-106. </pages>
Reference-contexts: In recent years, many different machine learning systems were developed. Machine learning methods [ 16; 18 ] can be classified into three major groups [ 13 ] : inductive learning of symbolic rules (such as induction of rules [ 17; 6 ] , decision trees <ref> [ 20 ] </ref> and induction of logic programs [ 15 ] ), statistical or pattern-recognition methods (such as k-nearest neighbors or instance-based learning [ 7; 1 ] , discriminate analysis and Bayesian classifiers), and artificial neural networks (such as networks with backpropagation learning, Kohonen's self-organizing network and Hopfield's associative memory [ <p> Assistant-R is a reimplementation of the Assistant learning system for top down induction of decision trees [ 4 ] . The basic algorithm goes back to CLS (Concept Learning System) developed by Hunt et al. [ 8 ] and reim-plemented by several authors (see <ref> [ 20 ] </ref> for an overview). <p> Positions of attributes in the tree, especially the top (most informative) ones, often directly correspond to domain expert's knowledge. However, in order to produce general rules, these methods use pruning <ref> [ 20; 4 ] </ref> which drastically reduces tree sizes. Consequently, the paths from the root to the leaves are shorter, containing only few most informative attributes.
Reference: [ 21 ] <author> Richeldi M. and Rossotto M. </author> <year> (1995). </year> <title> Class-driven statistical discretization of continuous attributes. </title> <editor> In Lavrac N., Wrobel S.(eds.), </editor> <booktitle> Machine Learning: Proc. </booktitle> <address> ECML-95, </address> <publisher> Springer Verlag, </publisher> <pages> pp. 335-342. </pages>
Reference-contexts: Discretiza-tion can be done manually by a domain expert or by applying a discretization algorithm <ref> [ 21 ] </ref> . The problem of (strict) discretization is that minor changes in the values of continuous attributes (or, equivalently, minor changes in boundaries) may have a drastic effect on the probability distribution and therefore on the classification.
References-found: 21

