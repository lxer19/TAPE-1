URL: ftp://ftp.cs.virginia.edu/pub/techreports/CS-98-09.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.cs.virginia.edu
Email: fchapin,dk3x,karp,grimshawg@virginia.edu  chapin@cs.virginia.edu  
Title: Resource Management in Legion  
Author: Steve J. Chapin Dimitrios Katramatos John Karpovich Andrew Grimshaw Steve Chapin, 
Keyword: parallel and distributed systems, task scheduling, resource management, autonomy Topic Area: OS and Resource Management  
Note: Corresponding Author:  This work was sponsored in part by Northrop-Grumman contract 9729373-00.  
Address: Charlottesville, VA 22903-2442  
Affiliation: Dept. of Computer Science School of Engineering Applied Science University of Virginia  
Abstract: The recent development of gigabit networking technology, combined with the proliferation of low-cost, high-performance microprocessors, has given rise to metacomputing environments. These environments can combine many thousands of hosts, from hundreds of administrative domains, connected by transnational and world-wide networks. Managing the resources in such a system is a complex task, but is necessary to efficiently and economically execute user programs. In this paper, we describe the resource management portions of the Legion meta-computing system, including the basic model and its implementation. These mechanisms are flexible both in their support for system-level resource management but also in their adaptability for user-level scheduling policies. We show this by implementing a simple scheduling policy and demonstrating how it can be adapted to more complex algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. Berman and R. Wolski. </author> <title> Scheduling from the perspective of the application. </title> <booktitle> In Proceedings of the 5th International Symposium on High-Performance Distributed Computing (HPDC-5). IEEE, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: This approach has been validated by both our own past history [4, 8] and the more recent work of groups such as the AppLeS project at UCSD <ref> [1] </ref>. These negotiating agents can either be the principals themselves (objects or programs), or Schedulers and intermediaries acting on their behalfs. Scheduling in Legion is never of a dictatorial nature; requests are made of resource guardians, who have final authority over what requests are honored.
Reference: [2] <author> S. Chapin and J. Karpovich. </author> <title> Resource Management in Legion. </title> <booktitle> Legion Winter Workshop. </booktitle> <address> http://www.cs.virginia.edu/~legion/WinterWorkshop/slides/Resource Management/, </address> <month> January, </month> <year> 1997. </year>
Reference-contexts: At a high level, their scheduling model closely resembles that of Legion, as first presented at the 1997 Legion Winter Workshop <ref> [2] </ref>. There is a rough correspondence between Globus Resource Brokers and Legion Schedulers; Globus Information Services and Legion Collections; Globus Co-allocators and Legion Enactors; and Globus GRAMs and Legion Host Objects.
Reference: [3] <author> S. Chapin and E. Spafford. </author> <title> Support for Implementing Scheduling Algorithms Using MESSIAHS. </title> <journal> Scientific Programming, </journal> <volume> 3 </volume> <pages> 325-340, </pages> <year> 1994. </year> <title> special issue on Operating System Support for Massively Parallel Computer Architectures. </title>
Reference-contexts: As noted earlier, Collections may also pull data from resources. Users, or their agents, obtain information about resources by issuing queries to a Collection. A Collection query is string conforming to the following grammar, which is largely the same as that used in our earlier work <ref> [3] </ref>. This grammar allows typical operations (field matching, semantic comparisons, and boolean combinations of terms).
Reference: [4] <author> S. J. Chapin. </author> <title> Distributed Scheduling Support in the Presence of Autonomy. </title> <booktitle> In Proceedings of the 4th Heterogeneous Computing Workshop, IPPS, </booktitle> <pages> pages 22-29, </pages> <month> April </month> <year> 1995. </year> <institution> Santa Barbara, </institution> <address> CA. </address>
Reference-contexts: This approach has been validated by both our own past history <ref> [4, 8] </ref> and the more recent work of groups such as the AppLeS project at UCSD [1]. These negotiating agents can either be the principals themselves (objects or programs), or Schedulers and intermediaries acting on their behalfs. <p> Therefore, Collections may receive data from, and send data to, other Collections. This allows us to have a Collection for each administrative domain, and to combine Collections in other Collections. This is analogous to the hierarchical structuring of scheduling modules in <ref> [4] </ref>, and we expect to see the same benefits realized there. 13 3.3 The Scheduler and Schedules The Scheduler computes the mapping of objects to resources. At a minimum, the Scheduler knows how many instances of each class must be started.
Reference: [5] <author> I. Foster and C. Kesselman. Globus: </author> <title> A metacomputing infrastructure toolkit. </title> <journal> International Journal of Supercomputer Applications, </journal> <note> to appear. </note>
Reference-contexts: For example, we are working with the DoD MSRC in Stennis, Mississippi to develop a Scheduler for an MPI-based ocean simulation which uses nearest-neighbor communication within a 2-D grid. 5 Related Work The Globus project <ref> [5] </ref> is also building metacomputing infrastructure. At a high level, their scheduling model closely resembles that of Legion, as first presented at the 1997 Legion Winter Workshop [2].
Reference: [6] <author> A. S. Grimshaw, Wm. A. Wulf, </author> <title> and the Legion Team. The legion vision of a worldwide virtual computer. </title> <journal> Communications of the ACM, </journal> <volume> 40(1), </volume> <month> January </month> <year> 1997. </year>
Reference-contexts: The Legion project is developing metacomputing software, and in this paper, we will describe the resource management subsystem of Legion. In particular, we will describe the Legion scheduling model, our implementation of the model, and the use of these mechanisms to support user-level scheduling. Legion <ref> [6] </ref> is an object-oriented metacomputing environment, intended to connect many thousands, perhaps millions, of hosts ranging from PCs to massively parallel supercomputers. Such a system will manage millions to billions of objects. <p> These objectives are de-scribed in greater depth in Grimshaw et al. <ref> [6] </ref>. Resource Management is concerned primarily with autonomy and heterogeneity, although other issues certainly play a role. Supporting heterogeneity requires Legion to accommodate vastly differing computing capabilities among constituent machines, including differences in architectures, operating systems, and installed software.
Reference: [7] <author> D. Hensgen, L. Moore, T. Kidd, R. Freund, E. Keith, M. Kussow, J. Lima, and M. Campbell. </author> <title> Adding rescheduling to and integrating condor with smartnet. </title> <booktitle> In Proceedings of the 4th Heterogeneous Computing Workshop, </booktitle> <pages> pages 4-11. </pages> <publisher> IEEE, </publisher> <year> 1995. </year> <month> 24 </month>
Reference-contexts: Indeed, these types of systems are complementary to a metasystem, and we will incorporate them into Legion by developing specialized Host Objects to act as mediators between the queuing systems and Legion at large. SmartNet <ref> [7] </ref> provides scheduling frameworks for heterogeneous resources. It is intended for use in dedicated environments, such as the suite of resources available at a supercomputer center. Unlike Legion, SmartNet is not intended for large-scale systems spanning administrative domains.
Reference: [8] <author> J. Karpovich. </author> <title> Support for object placement in wide area heterogeneous distributed systems. </title> <type> Technical Report CS-96-03, </type> <institution> Dept. of Computer Science, University of Virginia, </institution> <month> January </month> <year> 1996. </year>
Reference-contexts: This approach has been validated by both our own past history <ref> [4, 8] </ref> and the more recent work of groups such as the AppLeS project at UCSD [1]. These negotiating agents can either be the principals themselves (objects or programs), or Schedulers and intermediaries acting on their behalfs.
Reference: [9] <institution> Legion main web page. </institution> <note> http://legion.virginia.edu. </note>
Reference-contexts: We then discuss our plans for building more sophisticated Schedulers with application and domain-specific knowledge. For the sake of brevity and presentation, we have omitted the full source code in favor of pseudocode. The source code will be available in an upcoming release of the Legion software, available from <ref> [9] </ref>. 4.1 Random Scheduling The Random Scheduling Policy, as the name implies, randomly selects from the available resources that appear to be able to run the task. There is no consideration of load, speed, memory contention, communication patterns, or other factors that might affect the completion time of the task.
Reference: [10] <author> M. J. Lewis and A. S. Grimshaw. </author> <title> The core legion object model. </title> <booktitle> In Proceedings of the 5th International Symposium on High-Performance Distributed Computing (HPDC-5). IEEE, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: Examples of service objects include caches for object implementations, file objects, and the resource management infrastructure. In the remainder of this section, we will examine the core objects and their role in resource management. For a complete discussion of the Legion Core Objects, see <ref> [10] </ref>. We will defer discussion of the service objects until section 3. 4 2.1 Legion Core Objects Class objects (e.g. HostClass, LegionClass) in Legion serve two functions. As in other object-oriented systems, Classes define the types of their instances.
Reference: [11] <author> M. Litzkow, M. Livny, and M. W. </author> <title> Mutka. Condor|A Hunter of Idle Workstations. </title> <booktitle> In Proceedings of the International Conference on Distributed Computing Systems, </booktitle> <pages> pages 104-111, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Legion achieves its goals with a "whole-cloth" design, while Globus presents a "sum-of-services" architecture layered over pre-existing components. Globus has the advantage of a faster path to maturity, while Legion encompasses functionality not present in Globus. 22 There are many software systems for managing a locally-distributed multicomputer, in-cluding Condor <ref> [11] </ref> and LoadLeveler [13]. These systems are typically Queue Management Systems intended for use with homogeneous resource pools. While extremely well-suited to what they do, they do not map well onto wide-area environments, where heterogeneity, multiple administrative domains, and communications irregularities dramatically complicate the job of resource management.
Reference: [12] <author> J. E. Moreira and V. K. Naik. </author> <title> Dynamic resource management on distributed systems using reconfigurable applications. </title> <journal> IBM Journal of Research & Development, </journal> <volume> 41(3), </volume> <year> 1997. </year>
Reference-contexts: Unlike Legion, SmartNet is not intended for large-scale systems spanning administrative domains. Thus, SmartNet could be used within a Legion system by developing a specialized Host Object, similar to the Condor and LoadLeveler Host Objects mentioned earlier. IBM's DRMS <ref> [12] </ref> also provides scheduling frameworks, in this case targeted towards reconfigurable applications.
Reference: [13] <author> A. Prenneis, Jr. Loadleveler: </author> <title> Workload management for parallel and distributed computing environments. </title> <booktitle> In Proceedings of Supercomputing Europe (SUPEUR), </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Globus has the advantage of a faster path to maturity, while Legion encompasses functionality not present in Globus. 22 There are many software systems for managing a locally-distributed multicomputer, in-cluding Condor [11] and LoadLeveler <ref> [13] </ref>. These systems are typically Queue Management Systems intended for use with homogeneous resource pools. While extremely well-suited to what they do, they do not map well onto wide-area environments, where heterogeneity, multiple administrative domains, and communications irregularities dramatically complicate the job of resource management.
Reference: [14] <author> C. L. Viles, M. J. Lewis, A. J. Ferrari, A. Nguyen-Tuong, and A. S. Grimshaw. </author> <title> Enabling flexiblity in the legion run-time library. </title> <booktitle> In Proceedings of the International Conference on Parallel and Distributed Processing Techniques and Applications (PDPTA'97), </booktitle> <pages> pages 265-274, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Conceptually, triggers are guarded statements which raise events if the guard evaluates to a boolean true. These events are handled by the Reflective Graph and Event (RGE) mechanisms in all Legion objects. RGE is described in detail in <ref> [14] </ref>; for our purposes, it is sufficient to note that this capability exists. Vaults are the generic storage abstraction in Legion. To be executed, a Legion object must have a location to hold its Object Persistent Representation (OPR). <p> The Class object is still responsible for checking the placement for validity and conformance to local policy, but the Class does not have to go through the standard placement steps. 3.5 Application Monitoring As noted earlier, Legion provides an event-based notification mechanism through its RGE model <ref> [14] </ref>. Using this mechanism, the Enactor can register an outcall to the host objects; this outcall will be performed when a trigger's guard evaluates to true. There is no explicitly-defined interface for this functionality, as it is implicit in the use of RGE facilities.
Reference: [15] <author> J. Weissman and X. Zhao. </author> <title> Scheduling parallel applications in distributed networks. </title> <journal> Journal of Cluster Computing, </journal> <note> to appear. </note>
Reference-contexts: Application writers can take advantage of the resource management infrastructure, described below, to write per-application or application-type-specific user-level Schedulers. We are working with Weissman's group at UTSA <ref> [15] </ref> to develop Schedulers for broad classes of applications with similar structures (e.g. 5-point stencils). Our resource management model, shown in figure 3, supports our scheduling philosophy by allowing user-defined Schedulers to interact with the infrastructure.
Reference: [16] <author> R. Wolski. </author> <title> Dynamically forecasting network performance to support dynamic scheduling using the network weather service. </title> <booktitle> In Proceedings of the 6th International Symposium on High-Performance Distributed Computing (HPDC-6), </booktitle> <month> August </month> <year> 1997. </year> <month> 25 </month>
Reference-contexts: We plan to extend Collections to support function injection|the ability for users to install code to dynamically compute new description information and integrate it with the already existing description information for a resource. This capability is especially important to users of the Network Weather Service <ref> [16] </ref>, which predicts future resource availability based on statistical analysis of past behavior. An important use of Collections is to structure resources within the Legion system. Having a few, global, Collections will prohibit the scalability we wish to achieve.
References-found: 16

