URL: ftp://ftp.cs.rochester.edu/pub/papers/ai/94.tr510rev.Supervised_learning_without_output_labels.ps.Z
Refering-URL: http://www.cs.rochester.edu/trs/ai-trs.html
Root-URL: 
Title: Supervised Learning Without Output Labels  
Note: This material is based upon work supported by the National Science Foundation under Grant number IRI-8903582. The Government has certain rights in this material.  
Abstract: Ramesh R. Sarukkai e-mail: sarukkai@cs.rochester.edu The University of Rochester Computer Science Department Rochester, New York 14627 Technical Report 510 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> "The use of multiple measurements in taxonomic problems", R. A. Fisher, Ann. Eugen., </editor> <volume> vol. 7, </volume> <pages> pp: 178-188, </pages> <year> 1936. </year>
Reference-contexts: The proposed supervised self-organization scheme has been successfully implemented on three real data sets: multi-speaker speech, iris, and multi-modal data. 2 Separability Enhancement by Nonlinear Mappings The idea of viewing supervised learning as a process of class separability enhancing, dimensionality reduction was first presented in Fisher's <ref> [1, 2] </ref> classic work in 1936, which was the origin of discriminant analysis. Supervised learning is viewed as a linear dimensionality reduction, and the criterion function, to be maximized, was defined as the ratio of the between-class scatter matrix, and the within-class scatter matrix. <p> Sammon's stress Self-organizing Back-prop 98.96% 0.510 Table 2: Summary of results obtained for the Iris data set 4 Experiment I: Iris Data 4.1 Experimental Setup The first data set used is the well-known iris data set, which was created by R. A. Fisher <ref> [1, 2] </ref> in 1936. The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other two, but the latter are not linearly separable from each other.
Reference: [2] <author> Pattern Classification and Scene Analysis, Richard O. Duda, and Peter E. Hart, </author> <title> A Wiley-Interscience Publication, </title> <publisher> John Wiley and sons, </publisher> <year> 1973. </year>
Reference-contexts: The proposed supervised self-organization scheme has been successfully implemented on three real data sets: multi-speaker speech, iris, and multi-modal data. 2 Separability Enhancement by Nonlinear Mappings The idea of viewing supervised learning as a process of class separability enhancing, dimensionality reduction was first presented in Fisher's <ref> [1, 2] </ref> classic work in 1936, which was the origin of discriminant analysis. Supervised learning is viewed as a linear dimensionality reduction, and the criterion function, to be maximized, was defined as the ratio of the between-class scatter matrix, and the within-class scatter matrix. <p> Sammon's stress Self-organizing Back-prop 98.96% 0.510 Table 2: Summary of results obtained for the Iris data set 4 Experiment I: Iris Data 4.1 Experimental Setup The first data set used is the well-known iris data set, which was created by R. A. Fisher <ref> [1, 2] </ref> in 1936. The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other two, but the latter are not linearly separable from each other.
Reference: [3] <author> Classification, </author> <title> Estimation, and Pattern Recognition, </title> <editor> Tzay Y. Young, and Thomas W. Calvert, </editor> <publisher> American Elsevier Publishing Company Inc., </publisher> <year> 1974. </year>
Reference-contexts: The goodness of fit between Y fl , and X fl is based on the inter-sample distances d fl ij (= kY i Y j k) and d ij (= kX i X j k) in the Y and X spaces, respectively. Various measures <ref> [3, 7] </ref> have been proposed : * Monotonicity [Shepard, 1962] N X N X ij ) R (d ij )gd fl2 2 where f (~) = 0; ~ = 0 and R (:) is the rank of the distance [3, 7]. * Stress [Kruskal, 1964] N X N X ij d <p> Various measures <ref> [3, 7] </ref> have been proposed : * Monotonicity [Shepard, 1962] N X N X ij ) R (d ij )gd fl2 2 where f (~) = 0; ~ = 0 and R (:) is the rank of the distance [3, 7]. * Stress [Kruskal, 1964] N X N X ij d ij g 2 (2) * Continuity [Shepard, 1966] N X N X ij =d ij g 2 (3) Typically, iterative techniques proceed by starting off with random values of Y , and search for the "good" projections based on <p> The algorithm terminates when no further improvement can be obtained in the projections, and the mapping function is then usually found by a least-squares technique. Modifications include the random-perturbation method <ref> [3] </ref> developed to minimize the continuity criterion as a means of maintaining structure. Non-iterative methods have also been proposed to enhance class separability: For instance, Koontz's method involves deriving a scalar distance function which relates distances in the original space to distances in the mapped space.
Reference: [4] <editor> Classification and Regression Trees, Breiman, L., Friedman, J.H., Olshen, R.A., & Stone, C.J., </editor> <address> Monterey, CA: </address> <publisher> Wadsworth and Brooks, </publisher> <year> 1984. </year> <title> [5] "Learning internal representations by error propagation", </title> <editor> Rumelhart, D.E., Hinton, G.E., and Williams, R. J., in Rumelhart, & McClelland (eds.), </editor> <booktitle> Parallel Distributed Processing,, </booktitle> <volume> Vol. 1, </volume> <pages> 318-362, </pages> <year> 1987. </year> <title> [6] "Parallel networks that learn to pronounce English text", Sejnowski, T.J., and Rosenberg, </title> <journal> C.R., Complex Systems, </journal> <volume> 1, </volume> <pages> 145-168, </pages> <year> 1987. </year>
Reference-contexts: Usually the function F () takes on values from a discrete set of "classes": fC O ; :::C K g. Many algorithms have been proposed for such a learning task: decision tree methods such as CART <ref> [4] </ref>, and multi-layered feedforward networks [5] trained by the "error-backpropagation" algorithm, to name a few. In practice, however, there is an intermediate mapping that is needed, namely the "label assignments".

Reference: [20] <institution> Parallel Distributed Processing: </institution> <note> Explorations in the Microstructure of Cognition. Vol. 1: Foundations, </note> <editor> David E. Rumelhart, James L. McClelland, </editor> <booktitle> and the PDP Research Group, </booktitle> <publisher> The MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: A neural implementation of Sammon's algorithm has been proposed by Jain and Mao [9]. The advantages of neural network implementations of projection algorithms include the ability to easily project unseen data after training. In the neural network literature <ref> [20, 21] </ref>, supervised learning is usually implicitly tied with the output labeling function. Perhaps this was due to the simplicity of formulating gradient weight adaptation equations (i.e. error is related to (teaching output actual output)). Independently of our work, discriminant analysis neural networks have been studied [16, 17].

References-found: 5

