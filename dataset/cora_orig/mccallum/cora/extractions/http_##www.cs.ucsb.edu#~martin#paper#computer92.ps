URL: http://www.cs.ucsb.edu/~martin/paper/computer92.ps
Refering-URL: http://www.cs.ucsb.edu/~martin/paper/index.html
Root-URL: http://www.cs.ucsb.edu
Title: Jade: A High-Level, Machine-Independent Language for Parallel Programming  
Author: Martin C. Rinard, Daniel J. Scales and Monica S. Lam 
Address: CA 94305  
Affiliation: Computer Systems Laboratory Stanford University,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> D.W. Anderson, F.J. Sparacio, </author> <title> and F.M. Tomasulo. The IBM System/360 Model 91: </title> <journal> Machine Philosophy and Instruction-Handling. IBM Journal of Research and Development, </journal> <volume> 11(1) </volume> <pages> 8-24, </pages> <month> January </month> <year> 1967. </year>
Reference-contexts: The IBM 360/91 <ref> [1] </ref> applied this dynamic approach at the instruction level. It fetched and decoded several instructions at a time, and dynamically determined which memory locations and registers each instruction read and wrote. The execution unit detected and concurrently executed independent instructions.
Reference: [2] <author> Henri E. Bal, M. Frans Kaashoek, and Andrew S. Tanenbaum. Orca: </author> <title> A Language for Parallel Programming of Distributed Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(3), </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: The comparatively large size of the pages also increases the probability of an application suffering from excessive communication caused by false sharing (when multiple processors repeatedly access disjoint regions of a single page in conflicting ways). Object-oriented parallel languages such as Amber [5] and Orca <ref> [2] </ref> provide shared memory at an object level. These systems support the shared memory abstraction by allowing methods to be invoked on any existing object from any processor.
Reference: [3] <author> N. Carriero and D. Gelernter. </author> <title> Applications Experience with Linda. </title> <booktitle> In Proceedings of the ACM Symposium on Parallel Programming, </booktitle> <pages> pages 173-187, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: While this set of tools provides a low-level abstraction, it has the greatest degree of generality and is useful as a foundation for high-level tools. Linda <ref> [3] </ref> provides a higher-level abstraction in the form of a shared tuple space. Rather than communicating via messages, tasks communicate via data that are added to and removed from the shared tuple space. Linda's tuple space operations port across a variety of parallel hardware environments.
Reference: [4] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The next layer of software support implements the abstraction of shared memory on a set of message-passing machines. Shared Virtual Memory systems such as Ivy [8] and Munin <ref> [4] </ref> use the virtual memory hardware to implement a page-based coherence protocol. The page-fault hardware is used to detect accesses to remote or invalid pages, and the page-fault handler can be used to move or copy pages from remote processors.
Reference: [5] <author> Jeffrey S. Chase, Franz G. Amador, Edward D. Lazowska, Henry M. Levy, and Richard J. Littlefield. </author> <title> The Amber System: Parallel Programming on a Network of Multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems, </booktitle> <pages> pages 147-158, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: The comparatively large size of the pages also increases the probability of an application suffering from excessive communication caused by false sharing (when multiple processors repeatedly access disjoint regions of a single page in conflicting ways). Object-oriented parallel languages such as Amber <ref> [5] </ref> and Orca [2] provide shared memory at an object level. These systems support the shared memory abstraction by allowing methods to be invoked on any existing object from any processor.
Reference: [6] <author> Jyh-Herng Chow and William Ludwell Harrison III. </author> <title> Compile-Time Analysis of Parallel Programs that Share Memory. </title> <booktitle> In Record of the Nineteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 130-141, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Jade therefore promotes an integrated model of parallel computation in which a high-level programming language allows programmers to exploit coarse-grain, data-dependent concurrency, while the compiler exploits fine-grain, concurrency statically available within tasks. The difficulty of applying compiler optimizations to explicitly parallel code <ref> [6] </ref> limits the amount of concurrency that compilers can extract from programs written in explicitly parallel languages. 3 2 Language Overview Because of Jade's sequential semantics, it is possible to implement Jade as an extension to a sequential programming language.
Reference: [7] <author> D. Lenoski, K. Gharachorloo, J. Laudon, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: There are no source code modifications required to port Jade applications between these platforms. Our shared memory implementation of Jade runs on the Silicon Graphics 4D/240S multiprocessor, and on the Stanford DASH multiprocessor <ref> [7] </ref>. The workstation implementation of Jade uses PVM as a reliable, typed transport protocol. Currently, this implementation of Jade runs on the SPARC-based SUN Microsystems workstations, and MIPS-based systems including the DECStation 3100 and 5000 machines, the Silicon Graphics workstations and the Stanford DASH multiprocessor.
Reference: [8] <author> Kai Li. IVY: </author> <title> A Shared Virtual Memory System for Parallel Computing. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <pages> pages II 94-101, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: The next layer of software support implements the abstraction of shared memory on a set of message-passing machines. Shared Virtual Memory systems such as Ivy <ref> [8] </ref> and Munin [4] use the virtual memory hardware to implement a page-based coherence protocol. The page-fault hardware is used to detect accesses to remote or invalid pages, and the page-fault handler can be used to move or copy pages from remote processors.
Reference: [9] <author> J. Duane Northcutt, Gerard A. Wall, James G. Hanko, and Eugene M. Kuerner. </author> <title> A High Resolution Video Workstation. Signal Processing: </title> <journal> Image Communication, </journal> <volume> 4(4 </volume> & 5):445-455, 1992. 
Reference-contexts: In this section we show how to write a simple digital video imaging program that runs on the High-Resolution Video (HRV) machine at Sun Laboratories <ref> [9] </ref>. and five i860 processors as graphics accelerators that drive a high-definition television monitor. The SPARCs interface to several input devices; the imaging application uses a video camera. The HRV machine has both a control bus and a high-bandwidth internal bus over which the SPARC and i860 processors communicate.
Reference: [10] <author> V.S. Sunderam. </author> <title> PVM: a framework for parallel distributed computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Pointers to shared objects are identified in a Jade program using the shared type qualifier. For example: double shared A <ref> [10] </ref>; double shared *B; The first declaration defines a statically allocated shared vector of doubles, while the second declaration defines a reference (pointer) to a dynamically allocated shared vector of doubles. <p> At the lowest level of abstraction are tools that provide only basic message-passing capabilities for sending data between processors, but hide the heterogeneity from the programmer. Software packages such as PVM <ref> [10] </ref> manage the translation issues associated with heterogeneity. This allows the same message-passing code to run on machines with different internal data formats forexample, a heterogeneous set of workstations connected by a network.
Reference: [11] <author> M. E. Wolf and M. S. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: Like the data parallel approach, this works well for a restricted kind of parallelism: the loop-level parallelism present in scientific programs that manipulate dense matrices. The complexity of the highly tuned, machine-specific code that parallelizing compilers generate <ref> [11] </ref> illustrates the need for high-level abstractions that shield programmers from the low-level details of their computations. Both the parallelizing compiler and the data parallel approaches are designed to exploit regular concurrency available within a single operation on aggregate data structures.
Reference: [12] <author> Songnian Zhou, Michael Stumm, Kai Li, and David Wortman. </author> <title> Heterogeneous Distributed Shared Memory. </title> <type> Technical Report CSRI-244, </type> <institution> Computer Systems Research Institute, University of Toronto, </institution> <month> September </month> <year> 1990. </year> <month> 21 </month>
Reference-contexts: Each parallel program must therefore have the same address space on all the 13 different machines on which it executes. This restriction has so far limited these systems to homogeneous collections of machines (except for a prototype described in <ref> [12] </ref>). The comparatively large size of the pages also increases the probability of an application suffering from excessive communication caused by false sharing (when multiple processors repeatedly access disjoint regions of a single page in conflicting ways).
References-found: 12

