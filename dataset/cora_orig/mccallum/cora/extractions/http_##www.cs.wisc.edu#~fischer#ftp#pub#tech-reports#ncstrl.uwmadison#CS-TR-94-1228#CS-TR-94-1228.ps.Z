URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-94-1228/CS-TR-94-1228.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-94-1228/
Root-URL: http://www.cs.wisc.edu
Email: wwt@cs.wisc.edu  
Title: Cost/Performance of a Parallel Computer Simulator  
Author: Babak Falsafi and David A. Wood 
Address: 1210 West Dayton Street Madison, WI 53706 USA  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract: This paper examines the cost/performance of simulating a hypothetical target parallel computer using a commercial host parallel computer. We address the question of whether parallel simulation is simply faster than sequential simulation, or if it is also more cost-effective. To answer this, we develop a performance model of the Wisconsin Wind Tunnel (WWT), a system that simulates cache-coherent shared-memory machines on a message-passing Thinking Machines CM-5. The performance model uses Kruskal and Weiss's fork-join model to account for the effect of event processing time variability on WWT's conservative fixed-window simulation algorithm. A generalization of Thiebaut and Stone's footprint model accurately predicts the effect of cache interference on the CM-5. The model is calibrated using parameters extracted from a fully-parallel simulation (p = N ), and validated by measuring the speedup as the number of processors (p) ranges from one to the number of target nodes (N ). Together with simple cost models, the performance model indicates that for target system sizes of 32 nodes and larger, parallel simulation is more cost-effective than sequential simulation. The key intuition behind this result is that large simulations require large memories, which dominate the cost of a uniprocessor; parallel computers allow multiple processors to simultaneously access this large memory. fl This work is supported in part by NSF PYI Award CCR-9157366, NSF Grant MIP-9225097, and donations from Thinking Machines Corp., Xerox Corp., and Digital Equipment Corp. Our Thinking Machines CM-5 was purchased through NSF Grant No. CDA-9024618 with matching funding from the Univ. of Wisconsin Graduate School. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Rassul Ayani. </author> <title> A Parallel Simulation Scheme Based on the Distance Between Objects. </title> <booktitle> In Proceedings of the SCS Multiconfer-ence on Distributed Simulation, </booktitle> <pages> pages 113-118, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: WWT differs from earlier simulators in two ways. First, it directly executes all load and store instructions that hit in the target system's cache. Second, it integrates direct-execution with a conservative fixed-window parallel discrete-event simulation algorithm to not only parallelize event generation, but also the memory system simulation <ref> [16, 1, 18, 8, 17] </ref>. Parallel simulators like WWT are much faster than comparable uniprocessor simulators, providing the quick turn-around-time that can be so important to the design cycle. However, parallel simulation is not necessarily cost-effective for evaluating alternative parallel machines.
Reference: [2] <author> David Bailey, John Barton, Thomas Lasinski, and Horst Simon. </author> <title> The NAS Parallel Benchmarks. </title> <type> Technical Report RNR-91-002 Revision 2, </type> <institution> Ames Research Center, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: Appbt is a computational fluid dynamics program that solves systems of tridiagonal equations <ref> [2] </ref>. Sparse solves AX = B in parallel for a sparse matrix A. T omcatv is a parallel version of the SPEC benchmark [23]. <p> Real data sets are much larger; for example, the official NAS input to appbt is 125 times larger than the data set presented here <ref> [2] </ref>. Our uniprocessor cost model is based on the Silicon Graphics CHALLENGE M, a rack-mounted uniprocessor workstation server. We use a server configuration because desktop and deskside units do not provide the necessary memory expansion capability [19].
Reference: [3] <author> Bob Boothe. </author> <title> Fast Accurate Simulation of Large Shared Memory Multiprocessors. </title> <type> Technical Report CSD 92/682, </type> <institution> Computer Science Division (EECS), University of California at Berkeley, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: Real applications on parallel machines run for billions, or even trillions of cycles; even register-transfer-level simulators are much too slow. Over the last several years, direct execution has become widely used to accelerate architectural simulations <ref> [6, 4, 3, 7, 15] </ref>. Direct execution exploits the commonality between the instruction set of the simulated target machine and the underlying host system. For example, a floating-point multiply on the target is "simulated" by executing a floating-point multiply on the host. <p> Such a system need only simulate the differences between the target system and the host, achieving impressive performance when the two systems are very similar. Simulations of parallel computers have exploited direct execution in several ways <ref> [3, 7, 5] </ref>. Most commonly, a parallel target system is simulated on a uniprocessor host. For example, the Tango system spawns an event generation process for each processor in a target shared-memory system.
Reference: [4] <author> Eric A. Brewer, Chrysanthos N Dellarocas, Adrian Colbrook, and William Weihl. PROTEUS: </author> <title> A High-Performance Parallel-Architecture Simulator. </title> <type> Technical Report MIT/LCS/TR-516, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Real applications on parallel machines run for billions, or even trillions of cycles; even register-transfer-level simulators are much too slow. Over the last several years, direct execution has become widely used to accelerate architectural simulations <ref> [6, 4, 3, 7, 15] </ref>. Direct execution exploits the commonality between the instruction set of the simulated target machine and the underlying host system. For example, a floating-point multiply on the target is "simulated" by executing a floating-point multiply on the host.
Reference: [5] <author> Robert F. Cmelik and David Keppel. Shade: </author> <title> A Fast Instruction-Set Simulator for Execution Profiling. </title> <type> Technical Report UWCSE 93-06-06, </type> <institution> Department of Computer Science, University of Wash-ington, </institution> <year> 1993. </year>
Reference-contexts: Such a system need only simulate the differences between the target system and the host, achieving impressive performance when the two systems are very similar. Simulations of parallel computers have exploited direct execution in several ways <ref> [3, 7, 5] </ref>. Most commonly, a parallel target system is simulated on a uniprocessor host. For example, the Tango system spawns an event generation process for each processor in a target shared-memory system.
Reference: [6] <author> R.C. Covington, S. Madala, V. Mehta, J.R. Jump, and J.B. Sin-clair. </author> <title> The Rice Parallel Processing Testbed. </title> <booktitle> In Proceedings of the 1988 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 4-11, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Real applications on parallel machines run for billions, or even trillions of cycles; even register-transfer-level simulators are much too slow. Over the last several years, direct execution has become widely used to accelerate architectural simulations <ref> [6, 4, 3, 7, 15] </ref>. Direct execution exploits the commonality between the instruction set of the simulated target machine and the underlying host system. For example, a floating-point multiply on the target is "simulated" by executing a floating-point multiply on the host.
Reference: [7] <author> Helen Davis, Stephen R. Goldschmidt, and John Hennessy. </author> <title> Multiprocessor Simulation and Tracing Using Tango. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing (Vol. II Software), </booktitle> <pages> pages II99-107, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Real applications on parallel machines run for billions, or even trillions of cycles; even register-transfer-level simulators are much too slow. Over the last several years, direct execution has become widely used to accelerate architectural simulations <ref> [6, 4, 3, 7, 15] </ref>. Direct execution exploits the commonality between the instruction set of the simulated target machine and the underlying host system. For example, a floating-point multiply on the target is "simulated" by executing a floating-point multiply on the host. <p> Such a system need only simulate the differences between the target system and the host, achieving impressive performance when the two systems are very similar. Simulations of parallel computers have exploited direct execution in several ways <ref> [3, 7, 5] </ref>. Most commonly, a parallel target system is simulated on a uniprocessor host. For example, the Tango system spawns an event generation process for each processor in a target shared-memory system.
Reference: [8] <author> Richard M. Fujimoto. </author> <title> Parallel Discrete Event Simulation. </title> <journal> Communications of the ACM, </journal> <volume> 33(10) </volume> <pages> 30-53, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: WWT differs from earlier simulators in two ways. First, it directly executes all load and store instructions that hit in the target system's cache. Second, it integrates direct-execution with a conservative fixed-window parallel discrete-event simulation algorithm to not only parallelize event generation, but also the memory system simulation <ref> [16, 1, 18, 8, 17] </ref>. Parallel simulators like WWT are much faster than comparable uniprocessor simulators, providing the quick turn-around-time that can be so important to the design cycle. However, parallel simulation is not necessarily cost-effective for evaluating alternative parallel machines. <p> WWT uses logical clocks to correctly calculate the logical execution time of a target system, modeling latencies, dependencies, and queuing. WWT manages interprocessor interactions by dividing program execution into lock-step quanta (also called fixed windows <ref> [8] </ref>, bounded lag [16] or time buckets [24]) to ensure all events originating on a remote node that affect a node 2 in the current quantum are known at the quantum's be-ginning.
Reference: [9] <author> Mark D. Hill, James R. Larus, Steven K. Reinhardt, and David A. Wood. </author> <title> Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4) </volume> <pages> 300-318, </pages> <month> November </month> <year> 1993. </year> <note> Ealier version appeared in ASPLOS V, </note> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: is the number of target nodes per host node. 1 Thiebaut and Stone called this the "footprint in a finite cache." 4 Validating the Model We validate the model by simulating a 32-node cache-coherent shared-memory multiprocessor with a 4-way set-associative 32-Kbyte cache kept coherent using the Dir 1 SW coherence <ref> [9, 26] </ref>. The network latency (and hence quantum length) is 100 cycles. The target system executes in one of two phases.
Reference: [10] <author> Raj Jain. </author> <title> The Art of Computer Systems Performance Analysis: Techniques for Experimental Design, Measurement, Simulation, and Modeling. </title> <publisher> John Wiley & Sons, </publisher> <year> 1991. </year>
Reference-contexts: Kruskal and Weiss's model uses two parameters to characterize the workload: the mean and variance 2 of the processing times. We modify their model slightly, by using standard analysis-of-variance techniques to separate the variance within a quantum, 2 inter , from the variance of the entire population, 2 <ref> [10] </ref>. This modification approximates the more technically correct, but computationally expensive, alternative of computing and 2 separately for each quantum.
Reference: [11] <author> John L. Hennessy Jaswinder P. Singh and Anoop Gupta. </author> <title> Scaling Parallel Programs for Multiprocessors: Methodology and Examples. </title> <journal> IEEE Computer, </journal> <volume> 26(7) </volume> <pages> 42-50, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: The model, however, says nothing about the simulation performance of larger or smaller target systems. To extend the model, we must make several assumptions about how the target and simulation systems scale. We assume memory-constrained scaling <ref> [11] </ref> when we vary the size of the target system. In memory-constrained scaling, the data set size grows linearly with respect to the number of (target) nodes. This scaling model has two key properties.
Reference: [12] <author> C. P. Kruskal and A. Weiss. </author> <title> Allocating Independent Subtasks on Parallel Processors. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> 11(10) </volume> <pages> 1001-1016, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: This paper also develops an analytic model of WWT's performance which incorporates three major factors: event processing time, context switch overhead, and host cache and TLB interference. We show that the variability in event processing times can be accurately modeled using Kruskal and Weiss's model for fork-join parallel programs <ref> [12] </ref>. The frequency of context switches, incurred when switching between target nodes, is accurately modeled by the maximum of binomial random variables. We extend Thiebaut and Stone's footprint model to predict the interference of multiple targets in the host cache and TLB [25]. <p> We have modeled this variability using a model that Kruskal and Weiss proposed for estimating the completion time of fork-join programs on MIMD parallel processors <ref> [12] </ref>. The model is asymptotically exact (as p and K go to infinity, with K growing faster than log p) if the processing times are independent and identically distributed (i.i.d.) and the distribution function is increasing failure rate.
Reference: [13] <author> Charles E. Leiserson, Zahi S. Abuhamdeh, David C. Dou-glas, Carl R. Feynman, Mahesh N. Ganmukhi, Jeffrey V. Hill, W. Daniel Hillis, Bradley C. Kuszmaul, Margaret A. St. Pierre, David S. Wells, Monica C. Wong, Shaw-Wen Yang, and Robert Zak. </author> <title> The Network Architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of the Fifth ACM Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <month> July </month> <year> 1992. </year>
Reference-contexts: WWT implements this using the CM-5's "network done" barrier, which guarantees that all messages are received before the quantum completes <ref> [13] </ref>.
Reference: [14] <author> Kai Li and Paul Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: WWT uses direct execution to exploit similarities between the target system (under study) and the host system (on which it executes). Because WWT executes on a message-passing machine (a Thinking Machines CM-5), it must simulate the shared memory abstraction using a fine-grain extension of Li's shared virtual memory <ref> [14] </ref>. Shared virtual memory uses the standard address translation hardware to control memory access on each node. When a node first accesses a shared data page, it allocates a local copy and maps it into the shared address space on that node; subsequent accesses reference the copy.
Reference: [15] <author> Jia-Jen Lin. </author> <title> Efficient Parallel Simulation for Designing Multiprocessor System. </title> <type> PhD thesis, </type> <institution> University of Michigan, </institution> <address> Ann Arbor, </address> <year> 1992. </year>
Reference-contexts: Real applications on parallel machines run for billions, or even trillions of cycles; even register-transfer-level simulators are much too slow. Over the last several years, direct execution has become widely used to accelerate architectural simulations <ref> [6, 4, 3, 7, 15] </ref>. Direct execution exploits the commonality between the instruction set of the simulated target machine and the underlying host system. For example, a floating-point multiply on the target is "simulated" by executing a floating-point multiply on the host.
Reference: [16] <author> Boris D. Lubachevsky. </author> <title> Efficient Distributed Event-Driven Simulations of Multiple-Loop Networks. </title> <journal> Communications of the ACM, </journal> <volume> 32(2) </volume> <pages> 111-123, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: WWT differs from earlier simulators in two ways. First, it directly executes all load and store instructions that hit in the target system's cache. Second, it integrates direct-execution with a conservative fixed-window parallel discrete-event simulation algorithm to not only parallelize event generation, but also the memory system simulation <ref> [16, 1, 18, 8, 17] </ref>. Parallel simulators like WWT are much faster than comparable uniprocessor simulators, providing the quick turn-around-time that can be so important to the design cycle. However, parallel simulation is not necessarily cost-effective for evaluating alternative parallel machines. <p> WWT uses logical clocks to correctly calculate the logical execution time of a target system, modeling latencies, dependencies, and queuing. WWT manages interprocessor interactions by dividing program execution into lock-step quanta (also called fixed windows [8], bounded lag <ref> [16] </ref> or time buckets [24]) to ensure all events originating on a remote node that affect a node 2 in the current quantum are known at the quantum's be-ginning. WWT implements this using the CM-5's "network done" barrier, which guarantees that all messages are received before the quantum completes [13].
Reference: [17] <author> Jayadev Misra. </author> <title> Distributed-Discrete Event Simulation. </title> <journal> ACM Computing Surveys, </journal> <volume> 18(1) </volume> <pages> 39-65, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: WWT differs from earlier simulators in two ways. First, it directly executes all load and store instructions that hit in the target system's cache. Second, it integrates direct-execution with a conservative fixed-window parallel discrete-event simulation algorithm to not only parallelize event generation, but also the memory system simulation <ref> [16, 1, 18, 8, 17] </ref>. Parallel simulators like WWT are much faster than comparable uniprocessor simulators, providing the quick turn-around-time that can be so important to the design cycle. However, parallel simulation is not necessarily cost-effective for evaluating alternative parallel machines.
Reference: [18] <author> David Nicol. </author> <title> Conservative Parallel Simulation of Priority Class Queueing Networks. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(3) </volume> <pages> 398-412, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: WWT differs from earlier simulators in two ways. First, it directly executes all load and store instructions that hit in the target system's cache. Second, it integrates direct-execution with a conservative fixed-window parallel discrete-event simulation algorithm to not only parallelize event generation, but also the memory system simulation <ref> [16, 1, 18, 8, 17] </ref>. Parallel simulators like WWT are much faster than comparable uniprocessor simulators, providing the quick turn-around-time that can be so important to the design cycle. However, parallel simulation is not necessarily cost-effective for evaluating alternative parallel machines.
Reference: [19] <author> Ed Reidenbach. </author> <title> CHALLENGE Server Perdiodic Table. </title> <institution> Silicon Graphics Computer Systems, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: Our uniprocessor cost model is based on the Silicon Graphics CHALLENGE M, a rack-mounted uniprocessor workstation server. We use a server configuration because desktop and deskside units do not provide the necessary memory expansion capability <ref> [19] </ref>. <p> (1 + X network )C processor + KpC memory (6) For the purposes of this study, we use current Silicon Graphics list prices for our uniprocessor and shared-memory multiprocessor cost estimates: C processor = $20000, C memory = $3200 (32 megabytes), BaseC Uni = $3200, and BaseC Bus = $76800 <ref> [19] </ref>. We assume X network = 2, which is a reasonable estimate of network cost for current generation MPP systems. Ultimately, we expect competition to reduce X network to values of 0:1 ~ 0:5. number of host nodes for a 32-node target system. <p> The minimum cost of a parallel host is approximately two times the cost of a uniprocessor host system. The figure also depicts the prices of Silicon Graphics CHALLENGE XL bus-based multiprocessor servers <ref> [19] </ref>.
Reference: [20] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, and David A. Wood. </author> <title> The Wiscon-sin Wind Tunnel: Virtual Prototyping of Parallel Computers. </title> <booktitle> In Proceedings of the 1993 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 48-60, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: A recent simulator|the Wisconsin Wind Tunnel 1 (WWT)|extends direct execution to simulate a par-allel target machine on top of parallel host (a Thinking Machines CM-5) <ref> [20] </ref>. WWT differs from earlier simulators in two ways. First, it directly executes all load and store instructions that hit in the target system's cache. <p> Section 6 presents and discusses the results from this cost/performance model, and Section 7 summarizes our contributions. 2 Simulation Methodology 2.1 The Wisconsin Wind Tunnel The Wisconsin Wind Tunnel (WWT) is a simulator for evaluating parallel computer systems|specifically cache-coherent shared-memory computers <ref> [20] </ref>. WWT uses the execution of shared-memory applications to drive a distributed discrete-event simulation of proposed hardware. Events generated by the simulation, such as cache misses and coherence messages, are used to schedule the application, permitting accurate calculation of the target system execution time.
Reference: [21] <author> Jaswinder Pal Singh, Truman Joe, Anoop Gupta, and John L. Hennessy. </author> <title> An Empirical Comparison of the Kendall Square Research KSR-1 and Stanford DASH Multiprocessor. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 214-225, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: The interference that results has a first-order effect on simulation performance. Other researchers have seen similar effects for more general parallel programs; for example, Singh, et al. recently presented significant superlinear speedups that result from cache and TLB performance improvements as the number of processors increases <ref> [21] </ref>. We use a generalization of Thiebaut and Stone's footprint model [25] to predict cache and TLB interference (T cache&T LB ), as the number of target nodes per host node increases.
Reference: [22] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Appbt is a computational fluid dynamics program that solves systems of tridiagonal equations [2]. Sparse solves AX = B in parallel for a sparse matrix A. T omcatv is a parallel version of the SPEC benchmark [23]. Barnes and W ater are from the SPLASH benchmarks <ref> [22] </ref>. tion of a process to be the set of blocks a process leaves in a finite cache that it may reference again 1 .
Reference: [23] <author> SPEC. </author> <title> SPEC Benchmark Suite Release 1.0, </title> <month> Winter </month> <year> 1990. </year>
Reference-contexts: Appbt is a computational fluid dynamics program that solves systems of tridiagonal equations [2]. Sparse solves AX = B in parallel for a sparse matrix A. T omcatv is a parallel version of the SPEC benchmark <ref> [23] </ref>. Barnes and W ater are from the SPLASH benchmarks [22]. tion of a process to be the set of blocks a process leaves in a finite cache that it may reference again 1 .
Reference: [24] <author> Jeff. S. Steinman. SPEEDES: </author> <title> A Multiple-Synchronization Environment for Parallel Discrete-Event Simulation. </title> <journal> International Journal in Computer Simulation, </journal> <volume> 2 </volume> <pages> 251-286, </pages> <year> 1992. </year>
Reference-contexts: WWT uses logical clocks to correctly calculate the logical execution time of a target system, modeling latencies, dependencies, and queuing. WWT manages interprocessor interactions by dividing program execution into lock-step quanta (also called fixed windows [8], bounded lag [16] or time buckets <ref> [24] </ref>) to ensure all events originating on a remote node that affect a node 2 in the current quantum are known at the quantum's be-ginning. WWT implements this using the CM-5's "network done" barrier, which guarantees that all messages are received before the quantum completes [13].
Reference: [25] <author> D. Thiebaut and H.S. Stone. </author> <title> Footprints in the cache. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(4) </volume> <pages> 305-329, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: The frequency of context switches, incurred when switching between target nodes, is accurately modeled by the maximum of binomial random variables. We extend Thiebaut and Stone's footprint model to predict the interference of multiple targets in the host cache and TLB <ref> [25] </ref>. Finally, we show that the model accurately estimates the measured speedup of WWT, with maximum error of 8% in three applications and 16% for all five applications. The next section reviews the design of the Wiscon-sin Wind Tunnel. <p> Other researchers have seen similar effects for more general parallel programs; for example, Singh, et al. recently presented significant superlinear speedups that result from cache and TLB performance improvements as the number of processors increases [21]. We use a generalization of Thiebaut and Stone's footprint model <ref> [25] </ref> to predict cache and TLB interference (T cache&T LB ), as the number of target nodes per host node increases. The footprint of a process is defined to be the set of blocks that a process leaves in an infinite cache.
Reference: [26] <author> David A. Wood, Satish Chandra, Babak Falsafi, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, Shubhendu S. Mukherjee, Subbarao Palacharla, and Steven K. Reinhardt. </author> <title> Mechanisms for Cooperative Shared Memory. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 156-168, </pages> <month> May </month> <year> 1993. </year> <month> 10 </month>
Reference-contexts: is the number of target nodes per host node. 1 Thiebaut and Stone called this the "footprint in a finite cache." 4 Validating the Model We validate the model by simulating a 32-node cache-coherent shared-memory multiprocessor with a 4-way set-associative 32-Kbyte cache kept coherent using the Dir 1 SW coherence <ref> [9, 26] </ref>. The network latency (and hence quantum length) is 100 cycles. The target system executes in one of two phases.
References-found: 26

