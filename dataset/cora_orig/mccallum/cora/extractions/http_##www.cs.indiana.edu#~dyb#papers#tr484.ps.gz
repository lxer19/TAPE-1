URL: http://www.cs.indiana.edu/~dyb/papers/tr484.ps.gz
Refering-URL: http://www.cs.indiana.edu/hyplan/owaddell.html
Root-URL: http://www.cs.indiana.edu
Title: Fast and Effective Procedure Inlining  
Author: Oscar Waddell and R. Kent Dybvig 
Keyword: procedure inlining, procedure integration, compiler optimization  
Address: Lindley Hall 215 Bloomington, Indiana 47405  
Affiliation: Indiana University Computer Science Department  Indiana University Computer Science Department  
Pubnum: Technical Report No. 484  
Email: fowaddell,dybg@cs.indiana.edu, 812-855-3608  
Date: June 11, 1997  
Abstract: Inlining is an important optimization for programs that use procedural abstraction. Because inlining trades code size for execution speed, the effectiveness of an inlining algorithm is determined not only by its ability to recognize inlining opportunities but also by its discretion in exercising those opportunities. This paper presents a new inlining algorithm for higher-order languages that combines simple analysis techniques with demand-driven online transformation to achieve consistent and often dramatic performance gains in fast linear time. Benchmark results reported here demonstrate that this inlining algorithm is as effective as and significantly faster than o*ine, analysis-intensive algorithms recently described in the literature. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Andrew W. Appel. </author> <title> Compiling with Continuations. </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: When creating a fresh variable, for example, residual-variable flags are copied to source-variable flags by a single logical shift operation. We do not CPS-convert input programs; doing so would not materially simplify the algorithm and could exaggerate the benefits obtained by inlining <ref> [1] </ref>. Table 1 describes the benchmarks used to evaluate the inlining algorithm. The benchmarks in the first part of the table were provided by Jagannathan and Wright [14] and have already been processed by their local simplification pass, which performs many of the same optimizations as our inliner. <p> This means that flow analysis must be repeated after inlining. Our one-pass algorithm differs significantly from the iterated multi-pass algorithm used by Appel in the SML/NJ compiler <ref> [1] </ref>. Appel uses an o*ine method in which a data gathering pass is run alternately with a transformation pass that performs several optimizations including constant folding, fi-contraction, uncurrying, and inlining of functions called only once. This transformation phase is iterated until the number of contractions found is below some threshold.
Reference: [2] <author> Andrew W. Appel and Trevor Jim. </author> <title> Shrinking lambda expressions in linear time. </title> <note> To appear in Journal of Functional Programming. </note>
Reference-contexts: Although our algorithm can be iterated, we have found that few programs actually benefit from additional passes. Appel and Jim recently described a linear-time algorithm that performs constant folding, dead-variable elimination, and inlining of functions called only once <ref> [2] </ref>. They have implemented an O (n 2 ) variant of the algorithm to replace the contraction phase of the SML/NJ compiler described above. The new algorithm is online in that usage counts for variables are updated as the optimizations are performed.
Reference: [3] <author> J. Michael Ashley. </author> <title> The effectiveness of flow analysis for inlining. </title> <booktitle> To appear in Proceedings of the 1997 ACM SIGPLAN International Conference on Functional Programming. </booktitle>
Reference-contexts: Recent research on procedure inlining for higher-order languages has focused on the use of o*ine poly-variant flow analyses to identify appropriate inlining opportunities <ref> [14, 3] </ref>. Although reported performance gains are impressive, the cost and volatility of such analyses renders the method impractical for use in production compilers. <p> Our algorithm combines a less accurate analysis (effectively sub-0CFA [4]) with online transformation and polyvariant specialization. The time required by their flow analysis varies widely and can be excessive|110 seconds to analyze dynamic|rendering their method impractical for use in a production compiler. Ashley <ref> [3] </ref> evaluated the effectiveness of four different flow analyses for inlining. The analyses range from a fast analysis less accurate than 0CFA to a polyvariant analysis similar to 1CFA. His inlining algorithm is based on that of Jagannathan and Wright but exposes more opportunities for inlining.
Reference: [4] <author> J. Michael Ashley. </author> <title> A practical and flexible flow analysis for higher-order languages. </title> <booktitle> In Proceedings of the ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 184-194, </pages> <year> 1996. </year>
Reference-contexts: Their approach is o*ine: all inlining decisions are made prior to any transformation. Our algorithm combines a less accurate analysis (effectively sub-0CFA <ref> [4] </ref>) with online transformation and polyvariant specialization. The time required by their flow analysis varies widely and can be excessive|110 seconds to analyze dynamic|rendering their method impractical for use in a production compiler. Ashley [3] evaluated the effectiveness of four different flow analyses for inlining. <p> We also want to determine what additional benefit can be obtained by extending our algorithm to exploit the results of flow analysis. We plan to study these effects using the polyvariant flow analysis of Ashley <ref> [4] </ref>. Our implementation inlines procedures with free variables only when those variables can be eliminated during optimization or when the scope of the variables includes the call site. In the second case, inlining may add new free variables to closures.
Reference: [5] <author> J. Eugene Ball. </author> <title> Predicting the effects of optimization on a procedure body. </title> <journal> SIGPLAN Notices, </journal> <volume> 14(8) </volume> <pages> 214-220, </pages> <month> August </month> <year> 1979. </year> <booktitle> Proceedings of the ACM SIGPLAN '79 Symposium on Compiler Construction. </booktitle>
Reference-contexts: Their algorithm is not online, polyvariant, or context-sensitive in the sense we have described. Ball describes an analysis that determines which parameters contribute to the value of the expressions in a procedure body <ref> [5] </ref>. When constant parameters are available at a call site he uses the parameter dependency information to guide inlining decisions with an estimate of code savings and performance gain that is based on predictions about the impact of subsequent optimizations.
Reference: [6] <author> Sandip K. Biswas. </author> <title> A demand-driven set-based analysis. </title> <booktitle> In Proceedings of the ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 372-385, </pages> <year> 1997. </year> <month> 17 </month>
Reference-contexts: We achieve a similar effect in direct style via our contexts, which abstract the relevant information about the continuation in which expression values are used. Biswas <ref> [6] </ref> describes a demand-driven set-based analysis and uses this to obtain a polynomial-time (O (n 3 )) algorithm for dead-code elimination in a purely functional higher-order language. No empirical evidence is given to support the effectiveness of the optimization.
Reference: [7] <author> Anders Bondorf. </author> <title> Improving binding times without explicit CPS-conversion. </title> <booktitle> In Proceedings of the 1992 ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 1-10, </pages> <year> 1992. </year>
Reference-contexts: They do not consider the more difficult problem of inlining functions called more than once. Bondorf uses an abstract interpreter written in continuation-passing style (CPS) to improve the accuracy of binding-time analysis in an off-line partial evaluator <ref> [7] </ref>. The CPS structure of the interpreter enables him to move static contexts into binding forms, thereby reducing the need to CPS-convert source programs as part of binding-time improvement.
Reference: [8] <author> Carl Bruggeman, Oscar Waddell, and R. Kent Dybvig. </author> <title> Representing control in the presence of one-shot continuations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 99-107, </pages> <year> 1996. </year>
Reference-contexts: When a size or effort counter exceeds its bound, the inlining attempt is abandoned via non-local exit to the point at which the counter was set, and a call is residualized instead. Because the continuations which provide these non-local exits are never reinstated, simple one-shot continuations suffice <ref> [8] </ref>. Operations on sets of flags are implemented efficiently as operations on bit-vectors. When creating a fresh variable, for example, residual-variable flags are copied to source-variable flags by a single logical shift operation.
Reference: [9] <author> Robert G. Burger. </author> <title> Efficient Compilation and Profile-Driven Recompilation in Scheme. </title> <type> PhD thesis, </type> <institution> Indiana University, </institution> <year> 1997. </year>
Reference-contexts: Finally, we have presented evidence that online techniques are faster than o*ine, analysis-intensive techniques, yet are equally effective for inlining. There are several directions we intend to pursue as future work. Inlining decisions might be improved by investing more effort in frequently executed parts of the program. Burger <ref> [9] </ref> has developed a framework in which profiling data can be collected and used to dynamically recompile programs as they run. When recompiling, his implementation currently performs only low-level optimizations such as block reordering. We plan to extend the scope of recompilation to include front-end optimizations such as inlining.
Reference: [10] <author> R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <type> Technical Report RC 14756, </type> <institution> IBM, </institution> <year> 1991. </year>
Reference-contexts: Wegman and Zadeck [16] present a fast global constant propagation algorithm and show how it can be extended to perform many of the specializations needed when procedures are inlined. Their algorithm propagates constants through the static single-assignment (SSA) graph <ref> [10] </ref> using a demand-driven traversal of the control-flow graph that facilitates elimination of unreachable code which, in turn, improves the accuracy of the information collected.
Reference: [11] <author> Jeffrey Dean and Craig Chambers. </author> <title> Towards better inlining decisions using inlining trials. </title> <booktitle> In Proceedings of the 1994 ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 273-282, </pages> <year> 1994. </year>
Reference-contexts: Biswas [6] describes a demand-driven set-based analysis and uses this to obtain a polynomial-time (O (n 3 )) algorithm for dead-code elimination in a purely functional higher-order language. No empirical evidence is given to support the effectiveness of the optimization. Dean and Chambers <ref> [11] </ref> describe an online approach in which the cost and benefit of inlining at a call site are estimated by examining the code produced when the routine is tentatively inlined and optimized for the call site.
Reference: [12] <author> L. Greengard. </author> <title> The Rapid Evaluation of Potential Fields in Particle Systems. </title> <publisher> ACM Press, </publisher> <year> 1987. </year>
Reference-contexts: k vertices, each having out-degree at most 2 lattice 214 Enumerates the lattice of maps between two lattices matrix 585 Tests whether matrix is maximal among all matrices obtained by reordering rows and columns maze 787 Hexagonal maze generator by Olin Shivers nbody 1,534 Implementation [17] of Greengard multipole algorithm <ref> [12] </ref> splay 971 Implementation of splay trees conform 395 Type checker by Jim Miller earley 455 Earley's parser by Marc Feeley em-fun 416 Jeffrey Siskind's functional implementation of an EM clustering algorithm em-imp 404 Siskind's imperative implementation of an EM clustering algorithm interpret 842 Marc Feeley's Scheme interpreter evaluating takl peval
Reference: [13] <author> Fritz Henglein. </author> <title> Global tagging optimization by type inference. </title> <booktitle> In Proceedings of the 1992 ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 205-215, </pages> <year> 1992. </year>
Reference-contexts: Operands for as-yet unreferenced variables are visited as described in Section 4.2. Bindings for unused formals are then eliminated from the specialized 11 Benchmark Lines Description boyer 528 Logic programming benchmark originally by Bob Boyer dynamic 1,503 Henglein's dynamic type inferencer <ref> [13] </ref> graphs 558 Counts directed graphs with distinguished root and k vertices, each having out-degree at most 2 lattice 214 Enumerates the lattice of maps between two lattices matrix 585 Tests whether matrix is maximal among all matrices obtained by reordering rows and columns maze 787 Hexagonal maze generator by Olin
Reference: [14] <author> Suresh Jagannathan and Andrew Wright. </author> <title> Flow-directed inlining. </title> <booktitle> In Proceedings of the ACM SIGPLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 193-205, </pages> <year> 1996. </year>
Reference-contexts: Recent research on procedure inlining for higher-order languages has focused on the use of o*ine poly-variant flow analyses to identify appropriate inlining opportunities <ref> [14, 3] </ref>. Although reported performance gains are impressive, the cost and volatility of such analyses renders the method impractical for use in production compilers. <p> We do not CPS-convert input programs; doing so would not materially simplify the algorithm and could exaggerate the benefits obtained by inlining [1]. Table 1 describes the benchmarks used to evaluate the inlining algorithm. The benchmarks in the first part of the table were provided by Jagannathan and Wright <ref> [14] </ref> and have already been processed by their local simplification pass, which performs many of the same optimizations as our inliner. To facilitate comparison with their results, performance data was collected on a 150 MHz SGI MIPS R4400 workstation. <p> Increasing the effort limit by an order of magnitude typically only doubles the run time of the algorithm. When the size limit is varied, the run time of the algorithm increases roughly in proportion to the increase in code size. 6 Related work Jagannathan and Wright <ref> [14] </ref> use a polyvariant flow analysis to identify inlining opportunities and to estimate the size of a procedure when specialized for a particular call site. Their approach is o*ine: all inlining decisions are made prior to any transformation.
Reference: [15] <author> Owen Kaser, C. R. Ramakrishnan, and Shaunak Pawagi. </author> <title> On the conversion of indirect to direct recursion. </title> <journal> ACM Letters on Programming Languages and Systems, </journal> <volume> 2(4) </volume> <pages> 151-164, </pages> <month> mar </month> <year> 1993. </year>
Reference-contexts: Specializing (fold * x 1 zero? (lambda (x) x) (lambda (x) (- x 1))) in this manner yields the same code as the earlier definition of factorial, modulo renaming. Although we do not explicitly perform specialization for mutually recursive functions, inlining often converts mutual recursion into direct recursion <ref> [15] </ref>. The resulting direct recursion can then be specialized as described above. 5 Performance We have implemented the inlining algorithm as an additional pass within Chez Scheme, a commercial optimizing compiler for Scheme.
Reference: [16] <author> M. N. Wegman and F. K. Zadeck. </author> <title> Constant propagation with conditional branches. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 3(2) </volume> <pages> 181-210, </pages> <year> 1991. </year>
Reference-contexts: By using inlining trials instead of o*ine scoring estimates, they were able to reduce overall compile time and code size with minimal loss of run-time performance. Our algorithm can be viewed as incorporating a lightweight form of inlining trials. Wegman and Zadeck <ref> [16] </ref> present a fast global constant propagation algorithm and show how it can be extended to perform many of the specializations needed when procedures are inlined.
Reference: [17] <author> F. Zhao. </author> <title> An O(n) algorithm for three-dimensional n-body simulations. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1987. </year> <month> 18 </month>
Reference-contexts: graphs with distinguished root and k vertices, each having out-degree at most 2 lattice 214 Enumerates the lattice of maps between two lattices matrix 585 Tests whether matrix is maximal among all matrices obtained by reordering rows and columns maze 787 Hexagonal maze generator by Olin Shivers nbody 1,534 Implementation <ref> [17] </ref> of Greengard multipole algorithm [12] splay 971 Implementation of splay trees conform 395 Type checker by Jim Miller earley 455 Earley's parser by Marc Feeley em-fun 416 Jeffrey Siskind's functional implementation of an EM clustering algorithm em-imp 404 Siskind's imperative implementation of an EM clustering algorithm interpret 842 Marc Feeley's
References-found: 17

