URL: http://www.ai.mit.edu/people/sayan/webPub/jpr.ps
Refering-URL: http://www.ai.mit.edu/people/sayan/pub.html
Root-URL: 
Title: Automatic Generation of RBF Networks Using Wavelets  
Author: Shayan Mukherjee and Shree K. Nayar 
Keyword: Multivariate Approximation, Radial Basis Function Networks, Wavelets, Supervised Learning, 3D Object Recognition and Pose Estimation  
Address: New York, NY 10027  
Affiliation: Department of Computer Science Columbia University  
Abstract: Learning can be viewed as mapping from an input space to an output space. Examples of these mappings are used to construct a continuous function that approximates the given data and generalizes for intermediate instances. Radial basis function (RBF) networks are used to formulate this approximating function. A novel method is introduced that automatically constructs a Generalized radial basis function (GRBF) network for a given mapping and error bound. This network is shown to be the smallest network within the error bound for the given mapping. The integral wavelet transform is used to determine the parameters of the network. Simple one-dimensional examples are used to demonstrate how the network constructed using the transform is superior to one constructed using standard ad hoc optimization techniques. The paper concludes with the automatic generation of GRBF networks for a multidimensional problem, namely, real-time 3D object recognition and pose estimation. The results of this application are favorable. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Poggio and F. Girosi, </author> <title> "Networks for Approximation and Learning," </title> <journal> Proc. IEEE, </journal> <volume> Vol. 78, </volume> <pages> pp. 1481-1497, </pages> <year> 1990. </year>
Reference-contexts: A rigorous formulation of this approximating function results in a weighted sum of Radial Basis Functions (RBF). This type of approximating function can be cast as a class of neural networks called RBF networks <ref> [1] </ref>. These networks are universal approximators, theoretically capable of approximating any function to a reasonable degree of precision [2] with only one layer of basis functions. (Neural networks with sigmoidal bases require two layers of basis functions to be universal approximators). <p> In the case of RBF networks, this representation is formulated using approximation theory and regularization techniques [7]. The first part of this section is a brief overview of RBF and GRBF networks (see <ref> [1] </ref> for details). Learning a mapping between an input and output space is often viewed as determining a function that performs the mapping. <p> If G is a positive definite matrix then it is invertible. Two theorems by Micchelli [8] exploit this property to impose sufficient conditions for the basis functions. The following are a few basis functions that satisfy Micchelli's conditions <ref> [1] </ref> : G (r) = e r 2 = 2 (c 2 +r 2 ) ff ff &gt; 0 G (r) = r where r = jjx x i jj. Once the type of basis function has been selected, casting the approximating function, equation (6), as a network is straightforward.
Reference: [2] <author> P. Baldi, </author> <title> "Computing with Arrays of Bell-Shaped and Sigmoidal Functions," </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <editor> R.P. Lippman, J.E. Moody, and D.S. Touret-zky (ed.), </editor> <publisher> Morgan Kaufman, </publisher> <pages> pp. 735-742, </pages> <year> 1991. </year>
Reference-contexts: This type of approximating function can be cast as a class of neural networks called RBF networks [1]. These networks are universal approximators, theoretically capable of approximating any function to a reasonable degree of precision <ref> [2] </ref> with only one layer of basis functions. (Neural networks with sigmoidal bases require two layers of basis functions to be universal approximators). RBF networks have been used for a variety of practical applications ranging from the recognition of stick figures [3] to pricing derivative securities [4].
Reference: [3] <author> T. Poggio and S. Edelman, </author> <title> "A Network That Learns to Recognize Three-dimensional Objects," </title> <journal> Nature, </journal> <volume> Vol. 343, </volume> <pages> pp. 263-266, </pages> <year> 1990. </year>
Reference-contexts: RBF networks have been used for a variety of practical applications ranging from the recognition of stick figures <ref> [3] </ref> to pricing derivative securities [4]. Although RBF networks have a rigorous formulation, this advantage is lost in most practical implementations. The reason is the assumption that the network consists of as many basis functions as training examples.
Reference: [4] <author> J. M. Hutchinson, A. W. Lo, and T. Poggio, </author> <title> "A Nonparametric Approach to Pricing and Hedging Derivative Securities Via Learning Networks" The Journal of Finance, </title> <journal> Vol XLIX, </journal> <volume> No. 3, </volume> <month> July </month> <year> 1994, </year> <pages> pages 851-889. </pages>
Reference-contexts: RBF networks have been used for a variety of practical applications ranging from the recognition of stick figures [3] to pricing derivative securities <ref> [4] </ref>. Although RBF networks have a rigorous formulation, this advantage is lost in most practical implementations. The reason is the assumption that the network consists of as many basis functions as training examples.
Reference: [5] <author> C. K. Chui, </author> <title> An Introduction to Wavelets, </title> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1992. </year>
Reference-contexts: The GRBF networks constructed using this method can be shown to be the smallest network for a given mapping and error bound. This result is achieved through a novel construction and application of the integral wavelet transform <ref> [5] </ref>. Wavelet bases are constructed from approximations of radial basis functions. The magnitude of the coefficient corresponding to each basis determines the importance to the mapping of the radial basis functions comprising that wavelet. <p> The IWT allows us to construct orthonormal or biorthonor-mal basis functions that are localized in space and decompose a function in terms of these bases. For the case of a 1-D function, f (x) (R 1 7! R 1 ), the IWT has the basic form <ref> [5] </ref>: (T f)(b; a) = 1 where : b;a (x) = jaj 1=2 ( a are the wavelet bases and T f)(b; a) are the transform coefficients. <p> Alternatively, the bases can be made biorthonormal. The result is a set of orthonormal bases : &lt; j;k ; l;m &gt;= ffi j;l ffi k;m : An approximation to a function f (x) exists at different resolution levels. At the resolution level, j, the approximating function has the form <ref> [5] </ref>: f j (x) = k X d j+1 (k) (2 j x k): (13) From the above, it can be shown that the exact representation of the function f (x) has the form [5]: f (x) = j k where : d j (k) =&lt; f (k); (2 j x <p> At the resolution level, j, the approximating function has the form <ref> [5] </ref>: f j (x) = k X d j+1 (k) (2 j x k): (13) From the above, it can be shown that the exact representation of the function f (x) has the form [5]: f (x) = j k where : d j (k) =&lt; f (k); (2 j x k) &gt; : The decomposition in equation (14) is equivalent to a weighted sum of scaling functions at the finest resolution level: f 0 (x) = k=1 If we use a scaling function that
Reference: [6] <author> S. G. Mallat, </author> <title> "A Theory for Multiresolution Signal Decomposition: The Wavelet Representation," </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> Vol. 11, No. 7, </volume> <pages> pp. 674-693, </pages> <year> 1989. </year>
Reference-contexts: The wavelet coefficients along with Parseval's identity are used to determine the number of bases required and their parameters. In order to compute the wavelet coefficients, we extended the fast wavelet algorithm <ref> [6] </ref> for multidimensional sparse data. The paper is organized as follows. The formulation of an input-output mapping as a RBF network is described in section 2. The relation between the integral wavelet transform and RBF networks is presented in section 3. <p> Since the bins are small, we assume that any error introduced from the rounding off is negligible. The next step involves calculating the transform coefficients, d j;k , for the wavelet bases, j;k . The Fast Wavelet Algorithm <ref> [6] </ref> is extended to unevenly sampled data to calculate these coefficients. This algorithm allows one to compute the coefficients recursively from finer levels to coarser levels by convolving the signal at a finer level with two filters. <p> Thus, one can conclude that all spline wavelets can be represented as sums of functions that are almost radial basis functions. 9 Appendix B There exist several ways to construct wavelet bases using B-splines as scaling functions. The wavelet basis used in our work was the Battle-Lamarie orthonormal basis <ref> [6] </ref>. The scaling functions are cubic B-splines : fi 3 (x) = j=0 3! j ) (x + 2 j) 3 (x + 2 j) where is the unit step function.
Reference: [7] <author> A. N. Tikhonov and V. Y. Arsenin, </author> <title> Solutions of Ill Posed Problems W.H. </title> <publisher> Winston, </publisher> <address> Washington D.C., </address> <year> 1977. </year>
Reference-contexts: In the case of RBF networks, this representation is formulated using approximation theory and regularization techniques <ref> [7] </ref>. The first part of this section is a brief overview of RBF and GRBF networks (see [1] for details). Learning a mapping between an input and output space is often viewed as determining a function that performs the mapping.
Reference: [8] <author> C. A. Micchelli, </author> <title> "Interpolation of Scattered Data: Distance Matrices and Conditionally Positive Definite Functions," Constructive Approximation, </title> <journal> Vol. </journal> <volume> 2, </volume> <pages> pp. 11-22, </pages> <year> 1986. </year>
Reference-contexts: If G is a positive definite matrix then it is invertible. Two theorems by Micchelli <ref> [8] </ref> exploit this property to impose sufficient conditions for the basis functions.
Reference: [9] <author> N. R. Lomb, </author> <title> "Least-Squares Frequency Analysis of Unequally Spaced Data," </title> <booktitle> Astrophysics and Space Science, </booktitle> <volume> Vol. 39, </volume> <pages> pp. 447-462, </pages> <year> 1976. </year>
Reference-contexts: Spectral analysis of unevenly sampled data has been explored by astronomers. A popular approach is the Lomb periodogram <ref> [9] </ref>. The periodogram performs a least-mean-square fit of the data to sines and cosines under the assumption that the error in the fit decreases with the addition of frequency terms. We use it to help define the operation in equation (18).
Reference: [10] <author> M. D. Buhmann and C. A. Micchelli, </author> <title> "Spline Prewavelets for Non-uniform Knots," </title> <journal> Numerische Mathematik, </journal> <volume> Vol. 61, </volume> <pages> pp. 455-474, </pages> <year> 1992. </year>
Reference-contexts: A discussion of what approximations are introduced due to this oversampling assumption is beyond the scope of this paper. Alternative approaches to performing wavelet transforms on non-uniform data have been developed by Buhmann and Micchelli <ref> [10] </ref> [11] and Sweldens [12]. We did not use the approach of Buhmann and Micchelli for two reasons. First, extending their approach to multidimensional problems results in an enormous number of computations. Secondly, they introduce an extra spline space of radial basis functions called prewavelets to perform the transform.
Reference: [11] <author> M. D. Buhmann, </author> <title> "Multiquadratic Prewavelets on Non-Equally Spaced Centres," </title> <type> Tech--nical Report 94-06, </type> <institution> Seminar fur Angewandte Mathematik Eidgenossische Technische Hochshule, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: A discussion of what approximations are introduced due to this oversampling assumption is beyond the scope of this paper. Alternative approaches to performing wavelet transforms on non-uniform data have been developed by Buhmann and Micchelli [10] <ref> [11] </ref> and Sweldens [12]. We did not use the approach of Buhmann and Micchelli for two reasons. First, extending their approach to multidimensional problems results in an enormous number of computations. Secondly, they introduce an extra spline space of radial basis functions called prewavelets to perform the transform.
Reference: [12] <author> W. Sweldens, </author> <title> "Lifting Scheme : A Construction of Second-Generation Wavelets," </title> <booktitle> SPIE Proceedings on Wavelet Applications in Signal and Image Processing III ,to appear, </booktitle> <year> 1995. </year>
Reference-contexts: A discussion of what approximations are introduced due to this oversampling assumption is beyond the scope of this paper. Alternative approaches to performing wavelet transforms on non-uniform data have been developed by Buhmann and Micchelli [10] [11] and Sweldens <ref> [12] </ref>. We did not use the approach of Buhmann and Micchelli for two reasons. First, extending their approach to multidimensional problems results in an enormous number of computations. Secondly, they introduce an extra spline space of radial basis functions called prewavelets to perform the transform. <p> The assumptions made in using the periodogram and the errors introduced in going between continuous and discrete representations need to be explored. Second generation wavelets <ref> [12] </ref> offer an efficient and elegantalternative to the Lomb periodogram and future work must explore this possibility. 8 Appendix A Orthogonal spline wavelets can be written as : (x=2) = k2Z where q (k) = ([1 k b 2n+1 fl b 2n+1 ] #2 fl b 2n+1 ) 1=2 (33) and
Reference: [13] <author> H. Murase and S. K. Nayar, </author> <title> "Learning and Recognition of 3D Objects from Appearance," </title> <journal> International Journal of Computer Vision, </journal> <pages> pp. 1-24, </pages> <month> Jan </month> <year> 1995. </year>
Reference-contexts: This application involves a network that recognizes an object and estimates its pose in a scene. These networks take as input a compact representation that uses principal component analysis to parameterize object appearance by pose, introduced by Murase and Nayar <ref> [13] </ref>. A brief overview of this representation is in order. For each object, a large image set is acquired by varying pose. The eigenvectors of the correlation matrix of this image set, corresponding to the largest eigenvalues, make up the dimensions of a subspace (typically 10-20 dimensions) called the eigenspace. <p> This results in a single point in eigenspace. The projections of all images of an object results in a set of points (corresponding to the different discrete poses), that is referred to as a discrete manifold. In <ref> [13] </ref>, the discrete manifold is interpolated using biquadratic splines to obtain a continuous manifold that is parameterized by object pose. The manifold is then densely resampled to obtain a large number of manifold points. This large point set represents that object's appearance model. <p> Given a novel object image, the object region is segmented, normalized in scale and projected to universal eigenspace. The closest manifold determines the identity of the object in the image and the exact position of the new projection on the manifold yields the pose of the object. In <ref> [13] </ref>, the closest manifold point is determined either by exhaustive search (which is inefficient in time and memory) or by binary search (which is inefficient in memory). <p> The learning time for wavelet-based networks seems to increase as a polynomial function of the input dimensionality, however further investigation is needed to state anything more definitive. Murase and Nayar <ref> [13] </ref>. The image set for each object includes 72 images corresponding to uniformly sampled discrete poses between 0 o and 360 o .
Reference: [14] <author> E. Horowitz and S. Sahni, </author> <title> Fundamentals of Data Structures, </title> <publisher> Computer Science Press Int. Inc., </publisher> <year> 1993. </year>
Reference-contexts: The input space, a 15-dimensional eigenspace, was discretized into 1024 boxes in each of its dimensions. Clearly, it is impossible to store and process 1024 15 entries. Instead, a sparse tensor (an extension of the concept of a sparse matrix <ref> [14] </ref>) was constructed with only the entries for which f (x i ) were known. The networks ability to learn and generalize examples presented to it was tested using two data sets.
Reference: [15] <author> R. A. Devore, B. Jawerth, and V. A. Popov, </author> <title> "Compression of Wavelet Decompositions" American Journal of Math, </title> <year> 1992. </year>
Reference-contexts: One needs a quantative method of judging which representation is better. Approximation spaces such as the Besov space offer possibilities <ref> [15] </ref> to quantitaively measure representations. For the representation ~ f (x) an error function : ~a N ( ~ f ) = jj ~ f (x) ~ f 0 (x)jj 2 is defined where ~ f 0 (x) is a decomposition of ~ f (x) into N orthonormal basis.
Reference: [16] <author> R. A. Devore, B. Jawerth, and B. J. Lucier, </author> <title> "Image Compression Through Wavelet Transform Coding," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 38, No. 2, </volume> <pages> pp. 719-746, </pages> <year> 1992. </year>
Reference-contexts: It has been shown that the ff-class and Besov spaces are closely related <ref> [16] </ref> : O (N ff=2 ) , f 2 B ff when the error function is L 2 . Here, B ff 2 is a Besov space.
Reference: [17] <author> M. Unser, A. Aldroubi, M. Eden, </author> <title> "On the Asymptotic Convergence of fi-spline Wavelets to Gabor Functions," </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> Vol. 38, </volume> <pages> pp. 864-872, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: This shows that wavelets can be constructed from B-splines. In addition, it has been shown that B-splines asymptotically approach Gaussians for a sufficiently large n <ref> [17] </ref>: fi n (x) ~ = 6 e 6x 2 The basis functions in higher dimensions are separable B-splines which are very similar to a product of Gaussians, and therefore radial basis functions. (Note that the Gaussian is the only function that is both separable and circular).
Reference: [18] <author> M. Unser, A. Aldroubi, and M. Eden, </author> <title> "A Family of Polynomial Spline Wavelet Transforms," </title> <booktitle> Signal Processing, </booktitle> <volume> Vol. 30, </volume> <pages> pp. 141-162, </pages> <year> 1993. </year>
Reference-contexts: The filter formulas, ffi ffi w (k), that were used in section 3.2 to calculate the wavelet coefficients, d j;k , are given in Tables 4 and 3. These formulas were derived by Unser, Aldroubi, and Eden <ref> [18] </ref> [19]. function frequency response B 7 5040 (2416 + 1191 [e 2f + e 2f ] + 120 [e 4f + e 4f ] + e 6f + e 6f ) 2 (f ) 1 Table 3: The components of the transfer functions ffi ffi filter frequency response ffi 2

References-found: 18

