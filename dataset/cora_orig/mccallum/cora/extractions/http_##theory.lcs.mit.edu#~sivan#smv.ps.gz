URL: http://theory.lcs.mit.edu/~sivan/smv.ps.gz
Refering-URL: http://theory.lcs.mit.edu/~sivan/papers.html
Root-URL: 
Title: Improving Memory-System Performance of Sparse Matrix-Vector Multiplication  
Author: Sivan Toledo 
Date: November 25, 1996  
Address: 3333 Coyote Hill Road Palo Alto, CA 94304  
Affiliation: Xerox Palo Alto Research Center  
Abstract: Sparse Matrix-Vector Multiplication is an important kernel that often runs inefficiently on superscalar RISC processors. This paper describe techniques that increase instruction-level parallelism and improve performance. The techniques include reordering to reduce cache misses originally due to Das et al., blocking to reduce load instructions, and prefetching to prevent multiple load-store units from stalling simulteneously. The techniques improve performnance from about 40 Mflops (on a well-ordered matrix) to over 100 Mflops on a 266 Mflops machine. The techniques are applicable to other superscalar RISC processors as well and have improved performance on a Sun UltraSparc I workstation, for example.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. C. Agarwal, F. G. Gustavson, and M. Zubair. </author> <title> A high performance algorithm using preprocessing for sparse matrix-vector multiplication. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 32-41, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: The floating-point units are therefore underutilized. We present below two techniques that address this issue, including blocking to reduce the number of load instructions. Blocking in sparse matrix-vector multiplication was used in somewhat different forms in <ref> [1, 4] </ref>. Although the techniques that we propose can be applied separately, they are most effective when they are combined. In particular, reordering the matrix can enhance or degrade the effect of blocking. <p> Our approach to blocking is different from previous algorithms, mainly in that our algorithm attempts to find many small completely dense blocks. Other researchers have proposed algorithms that attempt to find larger (and hence fewer) dense blocks and/or blocks that are not completely dense. Agarwal, Gustavson and Zubair <ref> [1] </ref> describe an algorithm designed for vector processors. Their algorithm tried to find few large relatively dense blocks. They divide the rows of the matrix into blocks, and attempt to find one fairly dense rectangular block in every block of rows. <p> We have explored it further and found that the Cuthill-McKee yields excellent results on a variety of matrices. Two other techniques, representing nonzeros in small dense blocks and prefetching to allow cache hits-under-miss processing are novel (others have proposed to represent nonzeros in larger blocks <ref> [1, 4] </ref>). They, too, improve performance significantly on many matrices. On an IBM RS/6000 workstations, the combined effect of the four techniques can improve performance from about 40 Mflops to over 100 Mflops, depending on the size and sparseness of the matrix.
Reference: [2] <author> R. C. Agarwal, F. G. Gustavson, and M. Zubair. </author> <title> Improving performance of linear algebra algorithms for dense matrices using algorithmic prefetch. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 38(3) </volume> <pages> 265-275, </pages> <year> 1994. </year>
Reference-contexts: cause more runtime overhead than fixed-size blocks. 2.2 Prefetching in Irregular Loops Traditionally, prefetching was considered to be a technique for hiding latency, in the sense that prefetching can prevent memory access latency from degrading performance, as long as memory bandwidth is sufficient to keep the processing units busy (See <ref> [2] </ref>, [15], or [14], for example). In many codes, for example dense matrix multiplication, the ratio of floating-point to load instructions is high. This high ratio allows the algorithm to hide the latency of cache misses by prefetching cache lines early.
Reference: [3] <author> J.-L. Baer and T.-F. Chen. </author> <title> An effective on-chip preloading scheme to reduce data access penalty. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <year> 1991. </year>
Reference-contexts: Mowry [14] compared compiler-based sofware-directed prefetching to prefetching by hardware with no software intervention, as proposed by Lee [13], Porterfield [15], and Baer and Chen <ref> [3] </ref>. Our proposal lies somewhere in-between the two extremes. Our proposal enjoys very little instuction overhead, since the prefetching instructions are outside, rather than inside, the inner loop. Our proposal does not suffer from excessive memory contention overheads, because prefetching is 17 enables and disabled by software.
Reference: [4] <author> Satish Balay, William Gropp, Lois Curman McInnes, and Barry Smith. </author> <title> PETSc 2.0 users manual. </title> <type> Technical Report ANL-95/11, </type> <institution> Revision 2.0.15, Argonne National Laboratory, </institution> <year> 1996. </year>
Reference-contexts: The floating-point units are therefore underutilized. We present below two techniques that address this issue, including blocking to reduce the number of load instructions. Blocking in sparse matrix-vector multiplication was used in somewhat different forms in <ref> [1, 4] </ref>. Although the techniques that we propose can be applied separately, they are most effective when they are combined. In particular, reordering the matrix can enhance or degrade the effect of blocking. <p> to load these structural zeros into cache and into registers is likely to mask any performance benefit that larger dense blocks might afford. (A simple analysis shows, however, that allowing one zero in a 2-by-2 block might improve performance slightly on the POWER2 processor.) PETSc, a toolkit for scientific computations <ref> [4] </ref>, uses a blocked sparse matrix-vector multiplication subroutine that attemps to find blocks of rows with the same nonzero structure. Our approach is therefore similar to PETSc's in that both approaches require the blocks to be completely dense, but there is an important difference between the two approaches. <p> We have explored it further and found that the Cuthill-McKee yields excellent results on a variety of matrices. Two other techniques, representing nonzeros in small dense blocks and prefetching to allow cache hits-under-miss processing are novel (others have proposed to represent nonzeros in larger blocks <ref> [1, 4] </ref>). They, too, improve performance significantly on many matrices. On an IBM RS/6000 workstations, the combined effect of the four techniques can improve performance from about 40 Mflops to over 100 Mflops, depending on the size and sparseness of the matrix.
Reference: [5] <author> R. Barret, M. Berry, T. Chan, J. Demmel, J. Donato, J. Dongarra, V. Eijkhout, R. Pozo, C. Romine, and H. van der Vorst. </author> <title> Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods. </title> <publisher> SIAM, </publisher> <address> Philadeplphia, PA, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction Sparse matrix-vector multiplication is an important computational kernel in many iterative linear solvers (see <ref> [5] </ref>, for example). Unfortunately, on many computers this kernel runs slowly relative to other numerical codes, such as dense matrix computations. This paper proposes new techniques for improving the performance of sparse matrix-vector multiplication on superscalar RISC processors.
Reference: [6] <author> D. A. Burgess and M. B. Giles. </author> <title> Renumbering unstructured grids to improve the performance of codes on hierarchical memory machines. </title> <type> Technical Report 95/06, </type> <institution> Numerical Analysis Group, Oxford University Computing Laboratory, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: One technique that can reduce the number of cache misses is to reorder the matrix to reduce the number of cache misses on x. This technique was proposed by Das et al. [7], analyzed in certain cases by Temam and Jalby [16] and further investigated by Burgess and Giles <ref> [6] </ref>. We study this technique further in this paper, and we also show that the effectiveness of the new techniques that we propose depends on it. fl Part of this work was done while the author was a postdoctoral fellow at the IBM T. J. <p> Temam and Jalby [16] analyzed the number of cache misses as a function of the bandwidth for certain cache configurations. The technique was investigated further by Burgess and Giles <ref> [6] </ref>, who extended it to other unstructured-grid computations. Burgess and Giles studied experimentally several reordering strategies, including reverse Cuthill-McKee and a greedy blocking method. The found that reordering improved performance relative to a random ordering, but they did not find a particular sensitivity to the particular ordering method used. <p> One of the techniques, precomputing addresses for indirect addressing, is trivial but important. The technique of reordering the matrix to reduce its bandwidth and hence reduce cache misses has been proposed by Das et al. [7] and investigated further in two papers <ref> [6, 16] </ref>. We have explored it further and found that the Cuthill-McKee yields excellent results on a variety of matrices. Two other techniques, representing nonzeros in small dense blocks and prefetching to allow cache hits-under-miss processing are novel (others have proposed to represent nonzeros in larger blocks [1, 4]). <p> Our techniques, with the exception of prefetching, are portable. The ordering and blocking techniques should improve performance on most RISC processors, as shown by our experiments and the experiments of Burgess and Giles <ref> [6] </ref>. Replacing integers by pointers should improve performance on RISC machines that use the same functional units for address calculations and for shifts. The code that implements these three techniques is portable. The prefetching technique should improve performance on superscalar RISC processors that have more than one load/store unit.
Reference: [7] <author> R. Das, D. J. Mavriplis, J. Saltz, S. Gupta, and R. Ponnusamy. </author> <title> The design and implementation of a parallel unstructured Euler solver using software primitives. </title> <journal> AIAA Journal, </journal> <volume> 32(3) </volume> <pages> 489-496, </pages> <year> 1994. </year>
Reference-contexts: One technique that can reduce the number of cache misses is to reorder the matrix to reduce the number of cache misses on x. This technique was proposed by Das et al. <ref> [7] </ref>, analyzed in certain cases by Temam and Jalby [16] and further investigated by Burgess and Giles [6]. <p> Let us see how to cope with these problems. Reducing Cache Misses through Bandwidth Reduction Das et al. <ref> [7] </ref> proposed to reorder sparse matrices using a bandwidth-reducing technique in order to reduce the number of cache misses that accesses to x generate. Temam and Jalby [16] analyzed the number of cache misses as a function of the bandwidth for certain cache configurations. <p> One of the techniques, precomputing addresses for indirect addressing, is trivial but important. The technique of reordering the matrix to reduce its bandwidth and hence reduce cache misses has been proposed by Das et al. <ref> [7] </ref> and investigated further in two papers [6, 16]. We have explored it further and found that the Cuthill-McKee yields excellent results on a variety of matrices.
Reference: [8] <author> Iain S. Duff and Gerard Meurant. </author> <title> The effect of ordering on preconditioned conjugate gradient. </title> <journal> BIT, </journal> <volume> 29(4) </volume> <pages> 635-657, </pages> <year> 1989. </year>
Reference-contexts: Reordering sparse matrices using the Cuthill-McKee ordering has another benefit in sparse iterative solvers. When a conjugate gradient solver uses an incomplete Cholesky preconditioner, the ordering of the matrix effects the convergence rate. Duff and Meurant <ref> [8] </ref> compared the convergence rate of incomplete-Cholesky-preconditioned conjugate gradient with 17 different orderings on 4 model problems. In their tests the Cuthill-McKee and the reverse Cuthill-McKee resulted convergence rates that were best or close to best.
Reference: [9] <author> John H. Edmondson, Paul Rubinfeld, Ronald Preston, and Vidya Rajagopalan. </author> <title> Superscalar instruction execution in the 21164 Alpha microprocessor. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 33-43, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: The POWER2 processor can perform four flops (by issuing multiply-add instructions) and load four 64-bit words (by issuing two quad-load instructions) in every cycle. Digital's Alpha 21164 <ref> [9] </ref> and Sun's UltraSparc [17] both have floating-point units that can issue one add and one multiply instruction every cycle.
Reference: [10] <author> Anshul Gupta. </author> <title> Fast and effective algorithms for graph partitioning and sparse matrix ordering. </title> <type> Technical Report RC20496, </type> <institution> IBM T.J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <month> July </month> <year> 1996. </year>
Reference-contexts: The WGPP ordering code was written by Anshul Gupta <ref> [10, 11] </ref>. In each experiment we measured the running time of the matrix-vector multiplication code, the number of load instructions of various kinds it performed, and the number of cache and TLB misses. Each measurement is an average over 10 multiplications.
Reference: [11] <author> Anshul Gupta. WGPP: </author> <title> Watson graph partitioning (and sparse matrix ordering) package. </title> <type> Technical Report RC20453, </type> <institution> IBM T.J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: The WGPP ordering code was written by Anshul Gupta <ref> [10, 11] </ref>. In each experiment we measured the running time of the matrix-vector multiplication code, the number of load instructions of various kinds it performed, and the number of cache and TLB misses. Each measurement is an average over 10 multiplications.
Reference: [12] <author> C. L. Lawson, R. J. Hanson, D. R. Kincaid, and F. T. Krogh. </author> <title> Basic linear algebra subprogram for Fortran usage. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 5(3) </volume> <pages> 308-323, </pages> <year> 1979. </year>
Reference-contexts: For example, the double-precision dot product subroutine in the Fortran BLAS <ref> [12] </ref>, as well as the same subroutine in Sun's Performance Library, run at less than 20 Mflops on this machine.
Reference: [13] <author> R. L. Lee. </author> <title> The Effectiveness of Caches and Data Prefetch Buffers in Large-Scale Shared Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1987. </year>
Reference-contexts: Our proposed hardware-assisted prefetching can eliminate the somewhat complicated coding technique that we used to implement prefetching in the irregular loop over the sparse rows of the matrix. Mowry [14] compared compiler-based sofware-directed prefetching to prefetching by hardware with no software intervention, as proposed by Lee <ref> [13] </ref>, Porterfield [15], and Baer and Chen [3]. Our proposal lies somewhere in-between the two extremes. Our proposal enjoys very little instuction overhead, since the prefetching instructions are outside, rather than inside, the inner loop.
Reference: [14] <author> Todd C. Mowry. </author> <title> Tolerating Latency Through Software-Controlled Data Prefetching. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: overhead than fixed-size blocks. 2.2 Prefetching in Irregular Loops Traditionally, prefetching was considered to be a technique for hiding latency, in the sense that prefetching can prevent memory access latency from degrading performance, as long as memory bandwidth is sufficient to keep the processing units busy (See [2], [15], or <ref> [14] </ref>, for example). In many codes, for example dense matrix multiplication, the ratio of floating-point to load instructions is high. This high ratio allows the algorithm to hide the latency of cache misses by prefetching cache lines early. <p> Our proposed hardware-assisted prefetching can eliminate the somewhat complicated coding technique that we used to implement prefetching in the irregular loop over the sparse rows of the matrix. Mowry <ref> [14] </ref> compared compiler-based sofware-directed prefetching to prefetching by hardware with no software intervention, as proposed by Lee [13], Porterfield [15], and Baer and Chen [3]. Our proposal lies somewhere in-between the two extremes.
Reference: [15] <author> A. K. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: more runtime overhead than fixed-size blocks. 2.2 Prefetching in Irregular Loops Traditionally, prefetching was considered to be a technique for hiding latency, in the sense that prefetching can prevent memory access latency from degrading performance, as long as memory bandwidth is sufficient to keep the processing units busy (See [2], <ref> [15] </ref>, or [14], for example). In many codes, for example dense matrix multiplication, the ratio of floating-point to load instructions is high. This high ratio allows the algorithm to hide the latency of cache misses by prefetching cache lines early. <p> Our proposed hardware-assisted prefetching can eliminate the somewhat complicated coding technique that we used to implement prefetching in the irregular loop over the sparse rows of the matrix. Mowry [14] compared compiler-based sofware-directed prefetching to prefetching by hardware with no software intervention, as proposed by Lee [13], Porterfield <ref> [15] </ref>, and Baer and Chen [3]. Our proposal lies somewhere in-between the two extremes. Our proposal enjoys very little instuction overhead, since the prefetching instructions are outside, rather than inside, the inner loop.
Reference: [16] <author> O. Temam and W. Jalby. </author> <title> Characterizing the behavior of sparse algorithms on caches. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <month> November </month> <year> 1992. </year>
Reference-contexts: One technique that can reduce the number of cache misses is to reorder the matrix to reduce the number of cache misses on x. This technique was proposed by Das et al. [7], analyzed in certain cases by Temam and Jalby <ref> [16] </ref> and further investigated by Burgess and Giles [6]. <p> Let us see how to cope with these problems. Reducing Cache Misses through Bandwidth Reduction Das et al. [7] proposed to reorder sparse matrices using a bandwidth-reducing technique in order to reduce the number of cache misses that accesses to x generate. Temam and Jalby <ref> [16] </ref> analyzed the number of cache misses as a function of the bandwidth for certain cache configurations. The technique was investigated further by Burgess and Giles [6], who extended it to other unstructured-grid computations. Burgess and Giles studied experimentally several reordering strategies, including reverse Cuthill-McKee and a greedy blocking method. <p> One of the techniques, precomputing addresses for indirect addressing, is trivial but important. The technique of reordering the matrix to reduce its bandwidth and hence reduce cache misses has been proposed by Das et al. [7] and investigated further in two papers <ref> [6, 16] </ref>. We have explored it further and found that the Cuthill-McKee yields excellent results on a variety of matrices. Two other techniques, representing nonzeros in small dense blocks and prefetching to allow cache hits-under-miss processing are novel (others have proposed to represent nonzeros in larger blocks [1, 4]).
Reference: [17] <author> Marc Tremblay and J. Michael O'Connor. UltraSparc I: </author> <title> A four-issue processor supporting multimedia. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 42-49, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: The POWER2 processor can perform four flops (by issuing multiply-add instructions) and load four 64-bit words (by issuing two quad-load instructions) in every cycle. Digital's Alpha 21164 [9] and Sun's UltraSparc <ref> [17] </ref> both have floating-point units that can issue one add and one multiply instruction every cycle. The Alpha can issue two load instructions and load two words, giving the same balance as the POWER2, but the UltraSparc can issue only one load instruction per cycle, giving a worse balance.
Reference: [18] <author> E. H. Welbon, C. C. Chan-Nui, D. J. Shippy, and D. A. Hicks. </author> <title> The POWER2 performance monitor. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 38(5), </volume> <year> 1994. </year>
Reference-contexts: The performance of the algorithms was assessed using measurements of both running time and cache misses. Time was measured using the machines real-time clock, which has a resolution of one cycle. The number of cache misses was measured using the POWER2 performance monitor <ref> [18] </ref>. The performance monitor is a hardware subsystem in the processor capable of counting cache misses and other processor events. Both the real-time clock and the performance monitor are oblivious to time sharing.
Reference: [19] <author> S. W. White and S. Dhawan. POWER2: </author> <title> Next generation of the RISC System/6000 family. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 38(5), </volume> <year> 1994. </year> <month> 19 </month>
Reference-contexts: We believe that most of our findings would also apply to other RISC processors. 5.1 Experimental Platform The experiments were carried out on an RS/6000 workstation with a 66.5 Mhz POWER2 processor <ref> [19] </ref>, a 256 Kbytes 4-way set-associative data cache, a 256-bit wide memory bus and 512 Mbytes of main memory. The POWER2 processor has 32 architected and 54 physical floating-point registers, two floating-point units, two integer units that also serve as load-store units, and a branch unit.
References-found: 19

