URL: http://www.cs.unc.edu/~azuma/orlando.ps
Refering-URL: http://www.cs.unc.edu/~azuma/azuma_publications.html
Root-URL: http://www.cs.unc.edu
Title: Tracking a head-mounted display in a room-sized environment with head-mounted cameras  
Author: Jih-fang Wang, Ronald Azuma, Gary Bishop Vernon Chi, John Eyles, Henry Fuchs 
Address: Chapel Hill, NC 27599-3175  
Affiliation: Department of Computer Science University of North Carolina  
Abstract: This paper presents our efforts to accurately track a Head-Mounted Display (HMD) in a large environment. We review our current benchtop prototype (introduced in [WCF90]), then describe our plans for building the full-scale system. Both systems use an inside-out optical tracking scheme, where lateral-effect photodiodes mounted on the user's helmet view flashing infrared beacons placed in the environment. Church's method uses the measured 2D image positions and the known 3D beacon locations to recover the 3D position and orientation of the helmet in real-time. We discuss the implementation and performance of the benchtop prototype. The full-scale system design includes ceiling panels that hold the infrared beacons and a new sensor arrangement of two photodiodes with holographic lenses. In the full-scale system, the user can walk almost anywhere under the grid of ceiling panels, making the working volume nearly as large as the room. 
Abstract-found: 1
Intro-found: 1
Reference: [Bis84] <author> T. G. Bishop. Self-Tracker: </author> <title> A Smart Optical Sensor on Silicon. </title> <type> PhD thesis, </type> <institution> U. of North Carolina, Chapel Hill, NC, </institution> <year> 1984. </year>
Reference-contexts: Such a hybrid scheme would increase the robustness of the system and reduce the required density of beacons in our environment, reducing the cost and making it more portable. Ideally, "smart" optical sensors, such as the type Bishop investigated <ref> [Bis84] </ref>, combined with inertial trackers may almost eliminate the need for optical beacons. 10 ACKNOWLEDGEMENTS Thanks are due to Brad Bennett and John Thomas of the Microelectronics Systems Laboratory of UNC for their help with the system programming and hardware. Dr. John H.
Reference: [CHB + 89] <author> J. C. Chung, M. R. Harris, F. P. Brooks Jr., H. Fuchs, M. T. Kelley, J. W. Hughes, M. Ouh-Young, C. Cheung, R. L. Holloway, and M. Pique. </author> <title> Exploring virtual worlds with head-mounted displays. </title> <booktitle> In Proceedings SPIE Conference, Nonholographic True Three-Dimensional Display Technologies, </booktitle> <address> Los Angeles, CA, </address> <month> Jan. </month> <year> 1989. </year>
Reference-contexts: Any conducting materials or radiating sources present in the environment degrades the Polhemus' accuracy. In our Graphics Laboratory, the metal floor can curve the Polhemus' responses by as much as 30 ffi near its spatial range, according to <ref> [CHB + 89] </ref>.
Reference: [Chu45] <author> E. Church. </author> <note> Revised geometry of the aerial photograph. In Bulletin of Aerial Photogrammetry, volume 15. </note> <institution> Syracuse University, </institution> <year> 1945. </year>
Reference-contexts: Our system does this by trying to match the 3D positions of the observed beacons with their 2D images on the photodiode surfaces. Here we describe a method proposed by Earl Church <ref> [Chu45] </ref>, first used in aerial photogrammetry: the process of finding where a camera was when it took an aerial photograph by locating three known landmarks in the photograph.
Reference: [FGH + 85] <author> H. J. Fuchs, J. Goldfeather, J. P. Hultquist, S. Spach, J. Austin, F. P. Brooks Jr., J. Eyles, and J. Poulton. </author> <title> Fast spheres, textures, transparencies, and image enhancements in pixel-planes. </title> <journal> Computer Graphics, </journal> <volume> 19(3), </volume> <year> 1985. </year>
Reference: [Hol87] <author> R. Holloway. </author> <type> Head-mounted display technical report. Technical Report 87-015, </type> <institution> U. of North Carolina, </institution> <year> 1987. </year>
Reference: [Kil76] <author> P. J. Kilpatrick. </author> <title> The Use of a Kinesthetic Supplement in an Interactive Graphics System. </title> <type> PhD thesis, </type> <institution> U. of North Carolina, Chapel Hill, NC, </institution> <year> 1976. </year>
Reference: [Nol71] <author> M. A. Noll. </author> <title> Man-machine Tactile Communication. </title> <type> PhD thesis, </type> <institution> Polytechnic Institute of Brooklyn, Brooklyn, </institution> <address> NY, </address> <year> 1971. </year>
Reference: [Nor88] <author> Northern Digital. </author> <title> Trade literature on Optotrak Northern Digital's Three Dimensional Optical Motion Tracking and Analysis System. </title> <institution> Northern Digital Inc., Waterloo, </institution> <address> Ontario, Canada, </address> <year> 1988. </year>
Reference-contexts: A pair of these cameras can be used to measure the 3D location of the light source using stereopsis. The SELSPOT system is expensive ($40,000) and does not report the 3D position in real-time. The OP-EYE system has poor resolution and a very limited working volume [Wan90]. OPTOTRAK <ref> [Nor88] </ref> uses one camera with two dual-axis CCD infrared position sensors. Each position sensor has a dedicated processor board to calculate the image position of the light source. Again, the triangulation principle is applied to recover the position of the light source in space.
Reference: [Pol80] <institution> Polhemus Navigation Sciences Division. </institution> <note> 3Space Isotrak User's Manual. </note> <editor> McDonnell Douglas Electronics Company, </editor> <year> 1980. </year>
Reference-contexts: For example, one visualization program lets users walk around a large virtual model of a molecule and dock a different drug molecule to it. However, our current tracking device, the Polhemus <ref> [Pol80] </ref>, does not offer satisfactory performance for these applications. The Polhemus suffers from a limited working volume, slow update rate, and interference from magnetic perturbations in the environment. <p> Mechanical tracking greatly restricts the range of motion of the user, yielding highly constrained working volumes. The third category, magnetic tracking, includes a common tracking system in HMD systems: the Polhemus 3D position tracker <ref> [Pol80] </ref>. The Polhemus consists of two devices: a source and a sensor. The source generates a low-frequency magnetic field, and the sensor detects the polarization and orientation of the field.
Reference: [Rob66] <author> L. G. Roberts. </author> <title> The lincoln wand. </title> <booktitle> In FJCC, </booktitle> <address> Washington, D.C., 1966. </address> <publisher> Spartan Books. </publisher>
Reference-contexts: Finally, sections 8 and 9 offer conclusions and future directions to explore. 1 2 BACKGROUND In this section, we survey existing approaches to 3D tracking. Commercial and experimental 3D position tracking devices fall into four categories: acoustic, mechanical, magnetic, and optical tracking. The first two methods, acoustic <ref> [Rob66] </ref> and mechanical [Kil76][Nol71][Vic74], are not suitable for our purposes. Because the speed of sound varies as the ambient air density changes, acoustic systems yield poor accuracy over long ranges. Acoustic systems also cannot sense orientation directly and are limited in bandwidth by the speed of sound.
Reference: [Uni81] <author> United Detector Technology. </author> <title> Trade literature on OP-EYE optical position indicator. </title> <institution> United Detector Technology Inc., </institution> <address> Santa Monica, CA, </address> <year> 1981. </year>
Reference-contexts: It requires an unobstructed line-of-sight between multiple sources and sensors, but it works at long ranges, can be made fast and accurate, and is relatively immune to environmental distortions. We now survey several commercial and experimental systems that use optical tracking. SELSPOT [Wol74] and OP-EYE <ref> [Uni81] </ref> are two commercial systems that use camera-like units with lateral-effect photodiodes as the detecting surfaces. These systems detect a single light source that shines on the surface of the photodiode. The 2D location where the light beam strikes the photodiode surface can be measured in real-time.
Reference: [Vic74] <author> D. L. Vickers. </author> <title> Sorcerer's Apprentice: Head-mounted Display and Wand. </title> <type> PhD thesis, </type> <institution> U. of Utah, </institution> <address> Salt Lake City, UT, </address> <year> 1974. </year>
Reference: [Wan90] <author> J. F. Wang. </author> <title> A Real-time 6D Optical Tracker for Head-mounted Display Systems. </title> <type> PhD thesis, </type> <institution> U. of North Carolina, Chapel Hill, NC, </institution> <year> 1990. </year>
Reference-contexts: In our Graphics Laboratory, the metal floor can curve the Polhemus' responses by as much as 30 ffi near its spatial range, according to [CHB + 89]. The Polhemus is not accurate enough (6 mm in position and 0:7 ffi in rotation <ref> [Wan90] </ref>) to prevent "jittering." That is, when the user stands completely still, the virtual world appears to "swim" in front of the user because of the noise in the position and orientation readings. <p> A pair of these cameras can be used to measure the 3D location of the light source using stereopsis. The SELSPOT system is expensive ($40,000) and does not report the 3D position in real-time. The OP-EYE system has poor resolution and a very limited working volume <ref> [Wan90] </ref>. OPTOTRAK [Nor88] uses one camera with two dual-axis CCD infrared position sensors. Each position sensor has a dedicated processor board to calculate the image position of the light source. Again, the triangulation principle is applied to recover the position of the light source in space.
Reference: [WCF90] <author> J. F. Wang, V. Chi, and H. Fuchs. </author> <title> A real-time 6D optical tracker for head-mounted display systems. </title> <booktitle> In 1990 Symposium on Interactive 3D Graphics. </booktitle> <address> Snowbird, Utah, </address> <year> 1990. </year>
Reference-contexts: In this paper, we review a benchtop prototype of this system, which was described in more detail in <ref> [WCF90] </ref>, then introduce our design of the full-scale system. The remainder of this paper is organized as follows: Section 2 categorizes and surveys examples of current tracking schemes. Section 3 discusses the fundamental working principles of our tracker. <p> The error of the face angles measured in camera coordinates is mainly due to the camera's resolution limits, and can be expressed by <ref> [WCF90] </ref> " = ( a 2 + f 2 + b 2 + f 2 ) 2r where D is the width of the square photosensitive surface, r is the resolution of the photodiode, and a, f, and b are shown in Figure 3. <p> Finally, the host uses Church's method to recover the 3D position and orientation of the unit. An overview of the system is shown in Figure 4. We quantitatively measured the accuracy, range, and speed of this prototype <ref> [WCF90] </ref>. From the experiments conducted, we estimate that this design offers excellent accuracy in tracking head motion (0:1 ffi in rotation and 2 mm in translation) at long range (3 meters between source and sensor). Because of this accuracy, we have observed almost no jittering in our prototype.
Reference: [Wol74] <author> H. J. Woltring. </author> <title> New possibilities for human motion studies by real-time light spot position measurement. </title> <journal> Bioelemetry, </journal> <volume> 1, </volume> <year> 1974. </year> <month> 11 </month>
Reference-contexts: It requires an unobstructed line-of-sight between multiple sources and sensors, but it works at long ranges, can be made fast and accurate, and is relatively immune to environmental distortions. We now survey several commercial and experimental systems that use optical tracking. SELSPOT <ref> [Wol74] </ref> and OP-EYE [Uni81] are two commercial systems that use camera-like units with lateral-effect photodiodes as the detecting surfaces. These systems detect a single light source that shines on the surface of the photodiode. The 2D location where the light beam strikes the photodiode surface can be measured in real-time.
References-found: 15

