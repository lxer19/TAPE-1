URL: ftp://ftp.cs.utexas.edu/pub/mooney/papers/foidl-bkchapter-95.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ml/abstracts.html
Root-URL: 
Title: Appears in: Symbolic, Connectionist, and Statistical Approaches to Learning for Natural Language Learning the Past
Author: Raymond J. Mooney and Mary Elaine Califf 
Address: Austin,TX 78712-1188  
Affiliation: Department of Computer Sciences, University of Texas  
Date: 1996  
Note: Processing, Springer Verlag,  
Abstract: This paper presents results on using a new inductive logic programming method called Foidl to learn the past tense of English verbs. The past tense task has been widely studied in the context of the symbolic/connectionist debate. Previous papers have presented results using various neural-network and decision-tree learning methods. We have developed a technique for learning a special type of Prolog program called a first-order decision list, defined as an ordered list of clauses each ending in a cut. Foidl is based on Foil [19] but employs intensional background knowledge and avoids the need for explicit negative examples. It is particularly useful for problems that involve rules with specific exceptions, such as the past-tense task. We present results showing that Foidl learns a more accurate past-tense generator from significantly fewer examples than all previous methods.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> F. Bergadano and D. Gunetti. </author> <title> An interactive system to learn functional logic programs. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1044-1049, </pages> <address> Chambery, France, </address> <year> 1993. </year>
Reference-contexts: However, output completeness permits more flexibility by allowing some arguments to be specified as inputs and only counting as negative examples those extra outputs generated for specific inputs in the training set. Flip <ref> [1] </ref> provides a method for learning functional programs without negative examples by making an assumption equivalent to output completeness for the functional case only. The notion of a first-order decision list is unique to Foidl.
Reference: 2. <author> F. Bergadano, D. Gunetti, and U. Trinchero. </author> <title> The difficulties of learning logic programs with cut. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1 </volume> <pages> 91-107, </pages> <year> 1993. </year>
Reference-contexts: A third assumption is that the target program is expressed in "pure" Prolog where clause-order is irrelevant and procedural operators such as cut (!) are disallowed. However, a concise representation of many concepts requires the use of clause-ordering and/or cuts <ref> [2] </ref>. The currently most well-known and successful ILP systems, Golem [14] and Foil [19], both make all three of these assumptions. Due to these limitations, we were unable to get reasonable results on learning past tense from either Foil or Golem. <p> Focl [16], mFoil [[8]], Grendel [6], Forte [20], and Chillin [25] all use intensional background to some degree in the context of a Foil-like algorithm. The use of implicit negatives is significantly more novel. Bergadano et al. <ref> [2] </ref> allows the user to supply an intensional definition of negative examples that covers a large set of ground instances; however, to be equivalent to output completeness, the user would have to explicitly provide a separate intensional negative definition for each positive example. <p> The notion of a first-order decision list is unique to Foidl. The only other ILP system that attempts to learn programs that exploit clause-order and cuts is that of Bergadano et al. <ref> [2] </ref>. Their paper discusses learning arbitrary programs with cuts, and the brute-force search used in their approach is intractable for most realistic problems.
Reference: 3. <author> M. E. Califf. </author> <title> Learning the past tense of English verbs: An inductive logic programming approach. </title> <type> Unpublished project report, </type> <year> 1994. </year>
Reference-contexts: Each output unit or separate decision tree is used to predict a character in the fixed-length output pattern from all of the input characters. Although ILP methods seem more appropriate for this problem, our initial attempts to apply Foil and Golem to past-tense learning gave very disappointing results <ref> [3] </ref>. Below, we discuss how the three problems listed in the introduction contribute to the difficulty of applying current ILP methods to this problem.
Reference: 4. <author> R. Mike Cameron-Jones and J. Ross Quinlan. </author> <title> Efficient top-down induction of logic programs. </title> <journal> SIGART Bulletin, </journal> <volume> 5(1) </volume> <pages> 33-42, </pages> <month> Jan </month> <year> 1994. </year>
Reference-contexts: Section 7 summarizes and presents our conclusions. 2 Background 2.1 FOIL Since Foidl is based on Foil, this section presents a brief review of this important ILP system; see articles on Foil for a more complete description <ref> [19, 18, 4] </ref>. Foil learns a function-free, first-order, Horn-clause definition of a target predicate in terms of itself and other background predicates. The input consists of extensional definitions of these predicates as tuples of constants of specified types.
Reference: 5. <author> P. Clark and T. Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 261-284, </pages> <year> 1989. </year>
Reference-contexts: In the original algorithm of Rivest [21] and in CN2 <ref> [5] </ref>, rules are learned in the order they appear in the final decision list (i.e., new rules are appended to the end of the list as they are learned).
Reference: 6. <author> W.W. Cohen. </author> <title> Compiling prior knowledge into an explicit bias. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pages 102-110, </pages> <address> Ab-erdeen, Scotland, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: The use of intensional background knowledge is the least distinguishing feature, since a number of other ILP systems also incorporate this aspect. Focl [16], mFoil [[8]], Grendel <ref> [6] </ref>, Forte [20], and Chillin [25] all use intensional background to some degree in the context of a Foil-like algorithm. The use of implicit negatives is significantly more novel.
Reference: 7. <author> L. De Raedt and M. Bruynooghe. </author> <title> A theory of clausal discovery. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1058-1063, </pages> <address> Chambery, France, </address> <year> 1993. </year>
Reference-contexts: The non-monotonic semantics used to eliminate the need for negative examples in Claudien <ref> [7] </ref> has the same effect as an output completeness assumption in the case where all arguments of the target relation are outputs.
Reference: 8. <author> N. Lavrac and S. Dzeroski, </author> <title> editors. Inductive Logic Programming: Techniques and Applications. </title> <publisher> Ellis Horwood, </publisher> <year> 1994. </year>
Reference-contexts: Inductive logic programming (ILP) is a growing subtopic of machine learning that studies the induction of Prolog programs from examples in the presence of background knowledge <ref> [15, 8] </ref>. Due to the expressiveness of first-order logic, ILP methods can learn relational and recursive concepts that cannot be represented in the attribute/value representations assumed by most machine-learning algorithms. However, current ILP techniques make important assumptions that restrict their application.
Reference: 9. <author> C. X. Ling. </author> <title> Learning the past tense of English verbs: The symbolic pattern associa-tor vs. connectionist models. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1 </volume> <pages> 209-229, </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction The problem of learning the past tense of English verbs has been widely studied as an interesting subproblem in language acquisition. Previous research has applied both connectionist and symbolic method to this problem <ref> [22, 12, 9] </ref>; however, previous efforts used specially-designed feature-based encodings that impose a fixed limit on the length of words and fail to capture the generativity and position-independence of the underlying transformation. <p> The papers referenced above provide details and information on additional features. 2.2 Learning the Past Tense of English Verbs The problem of learning the English past tense has been attempted by both connectionist systems [22, 12] and systems based on decision tree induction <ref> [11, 9] </ref>. The task to be learned in these experiments is: given a phonetic encoding of the base form of an English verb, generate the phonetic encoding of the past tense form of that verb. <p> An implementation of Foidl in Quintus Prolog is available by anonymous FTP from ftp.cs.utexas.edu. 4 Experimental Results To test Foidl's performance on the English past tense task, we ran experiments using data from Ling <ref> [9] </ref> which consist of 1390 pairs of base and past tense verb forms in alphabetic and UNIBET phonemic form. We ran three different experiments. In one we used the phonetic forms of all verbs.
Reference: 10. <author> C. X. Ling, </author> <year> 1995. </year> <type> Personal communication. </type>
Reference-contexts: However, humans can obviously produce the correct past tense of arbitrarily-long novel words, which Foidl can easily model while fixed-length feature-based representations clearly cannot. Ling also developed a version of SPA that eliminates position dependence and fixed word-length <ref> [10] </ref> by using a sliding window. A large window is used which includes 15 letters on either side of the current position (padded with blanks if necessary) in order to always include the en-tire word for all the examples in the corpus.
Reference: 11. <author> C. X. Ling and M. Marinov. </author> <title> Answering the connectionist challenge: A symbolic model of learning the past tense of English verbs. </title> <journal> Cognition, </journal> <volume> 49(3) </volume> <pages> 235-290, </pages> <year> 1993. </year>
Reference-contexts: The papers referenced above provide details and information on additional features. 2.2 Learning the Past Tense of English Verbs The problem of learning the English past tense has been attempted by both connectionist systems [22, 12] and systems based on decision tree induction <ref> [11, 9] </ref>. The task to be learned in these experiments is: given a phonetic encoding of the base form of an English verb, generate the phonetic encoding of the past tense form of that verb. <p> Incorporating over-fitting avoidance methods may allow the system to model the U-shaped learning curve in a manner analogous to that demonstrated by Ling and Marinov <ref> [11] </ref>. Its ability to model human results on generating the past tense of novel psuedo-verbs (e.g., spling ! splang) could also be examined and compared to SPA and connectionist methods.
Reference: 12. <author> B. MacWhinney and J. Leinbach. </author> <title> Implementations are not conceptualizations: Revising the verb model. </title> <journal> Cognition, </journal> <volume> 40 </volume> <pages> 291-296, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction The problem of learning the past tense of English verbs has been widely studied as an interesting subproblem in language acquisition. Previous research has applied both connectionist and symbolic method to this problem <ref> [22, 12, 9] </ref>; however, previous efforts used specially-designed feature-based encodings that impose a fixed limit on the length of words and fail to capture the generativity and position-independence of the underlying transformation. <p> The papers referenced above provide details and information on additional features. 2.2 Learning the Past Tense of English Verbs The problem of learning the English past tense has been attempted by both connectionist systems <ref> [22, 12] </ref> and systems based on decision tree induction [11, 9]. The task to be learned in these experiments is: given a phonetic encoding of the base form of an English verb, generate the phonetic encoding of the past tense form of that verb.
Reference: 13. <author> R. J. Mooney and M. E. Califf. </author> <title> Induction of first-order decision lists: Results on learning the past tense of English verbs. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 3 </volume> <pages> 1-24, </pages> <year> 1995. </year>
Reference-contexts: Partial support was also provided by grant IRI-9310819 from the National Science Foundation and an MCD fellowship from the University of Texas awarded to the second author. A fuller discussion of this research appears in <ref> [13] </ref>.
Reference: 14. <author> S. Muggleton and C. Feng. </author> <title> Efficient induction of logic programs. </title> <booktitle> In Proceedings of the First Conference on Algorithmic Learning Theory, </booktitle> <address> Tokyo, Japan, 1990. </address> <publisher> Ohmsha. </publisher>
Reference-contexts: However, a concise representation of many concepts requires the use of clause-ordering and/or cuts [2]. The currently most well-known and successful ILP systems, Golem <ref> [14] </ref> and Foil [19], both make all three of these assumptions. Due to these limitations, we were unable to get reasonable results on learning past tense from either Foil or Golem. This paper presents a new ILP method called Foidl (First-Order Induction of Decision Lists) which helps overcome these limitations.
Reference: 15. <editor> S. H. Muggleton, editor. </editor> <booktitle> Inductive Logic Programming. </booktitle> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: Inductive logic programming (ILP) is a growing subtopic of machine learning that studies the induction of Prolog programs from examples in the presence of background knowledge <ref> [15, 8] </ref>. Due to the expressiveness of first-order logic, ILP methods can learn relational and recursive concepts that cannot be represented in the attribute/value representations assumed by most machine-learning algorithms. However, current ILP techniques make important assumptions that restrict their application.
Reference: 16. <author> M. Pazzani and D. Kibler. </author> <title> The utility of background knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 57-94, </pages> <year> 1992. </year>
Reference-contexts: The use of intensional background knowledge is the least distinguishing feature, since a number of other ILP systems also incorporate this aspect. Focl <ref> [16] </ref>, mFoil [[8]], Grendel [6], Forte [20], and Chillin [25] all use intensional background to some degree in the context of a Foil-like algorithm. The use of implicit negatives is significantly more novel.
Reference: 17. <author> J. R. Quinlan. </author> <title> Past tenses of verbs and first-order learning. </title> <editor> In C. Zhang, J. Debenham, and D. Lukose, editors, </editor> <booktitle> Proceedings of the Seventh Australian Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 13-20, </pages> <address> Singapore, 1994. </address> <publisher> World Scientific. </publisher>
Reference-contexts: However, supplying all possible strings of 15 characters or less as negative examples of the past tense of each word is clearly intractable. When Quinlan applied Foil to the past tense problem <ref> [17] </ref>, he used a three-place predicate past (X,Y,Z) which is true iff the input word X is transformed into past-tense form by removing its current ending Y and substituting the ending Z; for example: past ([a,c,t],[],[e,d]), past ([r,i,s,e],[i,s,e],[o,s,e]). <p> This method allows the generation of useful negatives under the closed world assumption, but relies on an understanding of the desired transformation. Although he solves the problem of providing negatives, Quinlan notes that his results are still hampered by Foil's inability to exploit clause order <ref> [17] </ref>. For example, when using normal alphabetic encoding, Foil quickly learns a clause sufficient for regular verbs: past (A,B,C) :- B=[], C=[e,d]. However, since this clause still covers a fair number of negative examples due to many irregular verbs, it continues to add literals. <p> We ran our own experiments with Foil, Foidl, and IFoil and compared those with the results from Ling. The Foil experiments were run using Quin-lan's representation described above. As in Quinlan <ref> [17] </ref>, negative examples were provided by using a randomly-selected 25% of those which could be generated using the closed world assumption. 2 All experiments with Foidl and IFoil used the standard default values for the various numeric parameters.
Reference: 18. <author> J. R. Quinlan and R. M. Cameron-Jones. </author> <title> FOIL: A midterm report. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <pages> pages 3-20, </pages> <address> Vienna, </address> <year> 1993. </year>
Reference-contexts: Section 7 summarizes and presents our conclusions. 2 Background 2.1 FOIL Since Foidl is based on Foil, this section presents a brief review of this important ILP system; see articles on Foil for a more complete description <ref> [19, 18, 4] </ref>. Foil learns a function-free, first-order, Horn-clause definition of a target predicate in terms of itself and other background predicates. The input consists of extensional definitions of these predicates as tuples of constants of specified types.
Reference: 19. <author> J.R. Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5(3) </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: However, a concise representation of many concepts requires the use of clause-ordering and/or cuts [2]. The currently most well-known and successful ILP systems, Golem [14] and Foil <ref> [19] </ref>, both make all three of these assumptions. Due to these limitations, we were unable to get reasonable results on learning past tense from either Foil or Golem. This paper presents a new ILP method called Foidl (First-Order Induction of Decision Lists) which helps overcome these limitations. <p> Section 7 summarizes and presents our conclusions. 2 Background 2.1 FOIL Since Foidl is based on Foil, this section presents a brief review of this important ILP system; see articles on Foil for a more complete description <ref> [19, 18, 4] </ref>. Foil learns a function-free, first-order, Horn-clause definition of a target predicate in terms of itself and other background predicates. The input consists of extensional definitions of these predicates as tuples of constants of specified types.
Reference: 20. <author> B. L. Richards and R. J. Mooney. </author> <title> Automated refinement of first-order Horn-clause domain theories. </title> <journal> Machine Learning, </journal> <volume> 19(2) </volume> <pages> 95-131, </pages> <year> 1995. </year>
Reference-contexts: The use of intensional background knowledge is the least distinguishing feature, since a number of other ILP systems also incorporate this aspect. Focl [16], mFoil [[8]], Grendel [6], Forte <ref> [20] </ref>, and Chillin [25] all use intensional background to some degree in the context of a Foil-like algorithm. The use of implicit negatives is significantly more novel.
Reference: 21. <author> R. L . Rivest. </author> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2(3) </volume> <pages> 229-246, </pages> <year> 1987. </year>
Reference-contexts: Foidl can directly learn programs of this form, i.e., ordered sets of clauses each ending in a cut. We call such programs first-order decision lists due to the similarity to the propositional decision lists introduced by Rivest <ref> [21] </ref>. Foidl uses the normal binary target predicate and requires no explicit negative examples. <p> As described above, first-order decision lists are ordered sets of clauses each ending in a cut. When answering an output query, the cuts simply eliminate all but the first answer produced when trying the clauses in order. Therefore, this representation is similar to propositional decision lists <ref> [21] </ref>, which are ordered lists of pairs (rules) of the form (t i ; c i ) where the test t i is a conjunction of features and c i is a category label and an example is assigned to the category of the first pair whose test it satisfies. <p> In the original algorithm of Rivest <ref> [21] </ref> and in CN2 [5], rules are learned in the order they appear in the final decision list (i.e., new rules are appended to the end of the list as they are learned).
Reference: 22. <author> D. E. Rumelhart and J. McClelland. </author> <title> On learning the past tense of English verbs. </title> <editor> In D. E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. II, </volume> <pages> pages 216-271. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: 1 Introduction The problem of learning the past tense of English verbs has been widely studied as an interesting subproblem in language acquisition. Previous research has applied both connectionist and symbolic method to this problem <ref> [22, 12, 9] </ref>; however, previous efforts used specially-designed feature-based encodings that impose a fixed limit on the length of words and fail to capture the generativity and position-independence of the underlying transformation. <p> The papers referenced above provide details and information on additional features. 2.2 Learning the Past Tense of English Verbs The problem of learning the English past tense has been attempted by both connectionist systems <ref> [22, 12] </ref> and systems based on decision tree induction [11, 9]. The task to be learned in these experiments is: given a phonetic encoding of the base form of an English verb, generate the phonetic encoding of the past tense form of that verb.
Reference: 23. <author> G. I. Webb and N. Brkic. </author> <title> Learning decision lists by prepending inferred rules. </title> <booktitle> In Proceedings of the Australian Workshop on Machine Learning and Hybrid Systems, </booktitle> <pages> pages 6-10, </pages> <address> Melbourne, Australia, </address> <year> 1993. </year>
Reference-contexts: In the original algorithm of Rivest [21] and in CN2 [5], rules are learned in the order they appear in the final decision list (i.e., new rules are appended to the end of the list as they are learned). However, Webb and Brkic <ref> [23] </ref> argue for learning decision lists in the reverse order since most preference functions tend to learn more general rules first, and these are best positioned as default cases towards the end.
Reference: 24. <author> David Yarowsky. </author> <title> Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French. </title> <booktitle> In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 88-95, </pages> <address> Las Cruces, NM, </address> <year> 1994. </year>
Reference-contexts: The decision list mechanism in general should be applicable to other language problems (as evidenced by the use of propositional decision lists for problems such as lexical disambiguation <ref> [24] </ref>.
Reference: 25. <author> J. M. Zelle and R. J. Mooney. </author> <title> Combining top-down and bottom-up methods in inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 343-351, </pages> <address> New Brunswick, NJ, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: The use of intensional background knowledge is the least distinguishing feature, since a number of other ILP systems also incorporate this aspect. Focl [16], mFoil [[8]], Grendel [6], Forte [20], and Chillin <ref> [25] </ref> all use intensional background to some degree in the context of a Foil-like algorithm. The use of implicit negatives is significantly more novel.
References-found: 25

