URL: http://www.cs.ucsd.edu/users/pasquale/Papers/dtj95.ps
Refering-URL: http://www.cs.ucsd.edu/users/pasquale/Pub.html
Root-URL: http://www.cs.ucsd.edu
Email: jkay-@cs.ucsd.edu  
Title: Page 1 High-Performance I/O and Networking Software in Sequoia 2000  
Author: Joseph Pasquale, Eric Anderson, Kevin Fall, Jonathan Kay -pasquale, ewa, kfall, 
Keyword: bytes  
Note: 1.0 Introduction  ond, since the workstation of choice in the Sequoia 2000 project was of the DEC 5000 series or DEC 3000 series (Alpha), both of which use the Turbochannel as the system bus.  
Address: San Diego, CA 92093-0114  
Affiliation: Computer Systems Laboratory Department of Computer Science and Engineering University of California, San Diego  
Abstract: We describe our experiences producing high-speed network and I/O software in the Sequoia 2000 Project. The efficient movement of very large objects, from tens to hundreds of megabytes in size, is a key requirement for Sequoia distributed applications. We present new designs and implementations of operating system I/O software. Our methods provide significant performance improvements for transfers among devices, processes, and between the two. We describe techniques that reduce or eliminate costly memory accesses, avoid unnecessary processing, and bypass system overheads to improve throughput while reduc ing latency. In the Sequoia 2000 project, we addressed the problem of designing a distributed computer system that can efficiently retrieve, store, and transfer very large data objects. By very large, we mean data objects in excess of tens or even hundreds of megabytes (MB). Sequoia 2000 chose Earth science as an application area because of its massive computational requirements, in large part due to the large data objects often found in these applications. There are many examples: an AVHRR (Advanced Very High Resolution Radiometer) image cube requires 300 MB, an AVIRIS (Advanced Visible and Infrared Imaging Spectrometer) image requires 140 MB, and the common LANDSAT (Land Satellite) image requires 278 MB. Any throughput bottleneck in a distributed computer system becomes greatly magnified when dealing with such large objects. In addition, Sequoia 2000 was an experiment in distributed collaboration; thus, collaboration tools such as video conferencing were also important applications to support. Our efforts in the project focused on operating system I/O and the network. We designed the Sequoia 2000 wide-area network testbed (see Section 2), and we explored new designs in operating system I/O and network software. The contributions of this paper are twofold: (1) this paper surveys the main results of this work, and puts them in perspective by relating them to the general data transfer problem (see Section 3), and (2) it presents a new design for Container Shipping, originally described in [PAM94]. Since the latter constitutes new material, we devote more space to it relative to the other surveyed work (whose more detailed descriptions may be found in [FP93, FP94, F94, KP93, KP93a, KP93b, K95, KP95]). In addition to this work, other network research was also carried out as part of the Sequoia 2000 project. This includes research on protocols providing performance guarantees [FBZ92, ZVF92, BKTZ94] To support a high performance distributed computing environment where applications can effectively manipulate large data objects, we were concerned with achieving high throughput when transferring these objects from where they happen to reside to where they are needed. The processes or devices representing the data sources and sinks may all reside on the same workstation (single node case), or may be distributed over many workstations connected by the network (multiple node case). In either case, we wanted applications, be they Earth Science distributed computations or collaboration tools involving multipoint video, to make full use of the raw bandwidth provided by the underlying communication system. In the multiple node case involving the network, this raw bandwidth is from 45 to 100 megabits per second (Mbps), since the Sequoia 2000 network used T3 links for long distance communication and FDDI for local area communication. In the single node case, this raw bandwidth is approximately 100 mega Our work focused only on software improvements, in particular how to achieve maximum system software performance given the hardware we selected. In fact, we found that the throughput bottlenecks in the Sequoia distributed computing environment were indeed in the workstations operating system software, and not in the underlying communication system hardware (e.g. network links or the system bus). This problem is not limited to the Sequoia environment: given modern high-speed workstations (100+ MIPS) and fast networks (100+ Mbps), performance bottlenecks are often caused by software, especially operating system software. This is because system software throughput has not kept up with the throughputs of I/O devices, especially network adapters, which have improved tremendously over recent years. These technology improvements are being driven by a new generation of applications, such as interactive multimedia involving digital video and high resolution graphics, that have high I/O throughput requirements. Supporting these applications and controlling these devices have taxed operating system technology, much of which was designed during This work was supported in part by grants from Digital Equip ment Corporation and the National Science Foundation. A version of this paper will appear in the Digital Technical Journal, 1995 This document was created with FrameMaker 4.0.4 and multicasting [KPP93, KPP93a, PPAK94, PPAK95, PPK93].
Abstract-found: 1
Intro-found: 0
Reference: [A95] <author> E. Anderson, </author> <title> Container Shipping: A Uniform Interface for Fast, Efficient, High-Bandwidth I/O, </title> <type> Ph.D. Dissertation, </type> <institution> University of California, </institution> <address> San Diego, </address> <year> 1995. </year>
Reference-contexts: With our new OSF implementation on Alpha workstations, we compared the I/O performance of conventional Unix I/O to that of container shipping for a variety of I/O devices as well as IPC. These experiments are described in detail in <ref> [A95] </ref>. Large improvements in throughput were observed, from 40% for FDDI network I/O (despite large non-data-touching protocol and device-driver overheads) to 700% for socket-based IPC. We devised an experiment that exercises both the IPC and I/O capabilities of container shipping. <p> The differences between these two systems and container shipping are examined in detail in <ref> [A95] </ref>. 5.0 Peer-to-Peer I/O In addition to container shipping, we have investigated an alternative I/O system software model called peer-to-peer I/O (PPIO).
Reference: [BKTZ94] <author> A. Banerjea, E. W. Knightly, F. L. Templin, and H. Zhang, </author> <title> Experiments with the Tenet Real-Time Protocol Suite on the Sequoia 2000 Wide Area Network, </title> <booktitle> Proc. ACM Multimedia , San Francisco, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: In addition to this work, other network research was also carried out as part of the Sequoia 2000 project. This includes research on protocols providing performance guarantees <ref> [FBZ92, ZVF92, BKTZ94] </ref> and multicasting [KPP93, KPP93a, PPAK94, PPAK95, PPK93]. To support a high performance distributed computing environment where applications can effectively manipulate large data objects, we were concerned with achieving high throughput when transferring these objects from where they happen to reside to where they are needed.
Reference: [BALM90] <author> B. Bershad, T. Anderson, E. Lazowska, H. Levy, </author> <title> Lightweight Remote Procedure Call, </title> <journal> ACM Trans. on Computer Systems , Vol. </journal> <volume> 8, No. 1, </volume> <month> February </month> <year> 1990, </year> <pages> pp. 37-55. </pages>
Reference-contexts: Thus, context-switch operations for data transfer are eliminated. This is important: context switches consume CPU resources, degrade cache performance by reducing locality of reference [MB91], and effect the performance of virtual memory by requiring TLB invalidations <ref> [BALM90] </ref>. For applications making no direct manipulation of I/O data (or for those allowing the kernel to make such manipulations), splice relegates the issues of managing the data ow (e.g. buffering and ow control) to the kernel.
Reference: [BBMT72] <author> D. G. Bobrow, J. D. Burchfiel, D. L. Murphy, and R. S. Tomlinson, TENEX, </author> <title> a paged time sharing system for Page 9 Comm. </title> <journal> ACM , Vol. </journal> <volume> 15 (3), </volume> <month> (March </month> <year> 1972), </year> <pages> pp. 135-143. </pages>
Reference: [B89] <author> R. Braden, </author> <title> Requirements for Internet Hosts - Communication Layers, Internet Request for Comments 1122, </title> <institution> Network Information Center, </institution> <month> October </month> <year> 1989. </year>
Reference-contexts: But, the Internet checksum exists for a good reason; packets are occasionally corrupted during transmission, and the checksum is needed to detect corrupted data. In fact, the IETF recommends that systems not be shipped with checksumming disabled by default <ref> [B89] </ref>. However, Ethernet and FDDI networks implement their own cyclic redundancy checksum (CRC). Thus, packets sent directly over an Ethernet or FDDI network are already protected from data corruption, at least at the level provided by the CRC.
Reference: [CHKM88] <author> L-F. Cabrera, E. Hunter, M. Karels, D. </author> <title> Mosher, User-Process Communication Performance in Networks of Computers, </title> <journal> IEEE Trans. on Software Engineering Vol. </journal> <volume> 14, No. 1, </volume> <month> January </month> <year> 1988, </year> <pages> pp. 38-53. </pages>
Reference: [CJRS89] <author> D. Clark, V. Jacobson, J. Romkey, H. Salwen, </author> <title> An Analysis of TCP Processing Overhead, </title> <journal> IEEE Commu nications , June 1989, </journal> <pages> pp. 23-29. </pages>
Reference: [DAPP92] <author> P. Druschel, M. Abbott, M. Pagels, and L. Peterson, </author> <title> Analysis of I/O Subsystem Design for Multimedia Workstations, </title> <booktitle> Proc. 3rd International Workshop on Network and Operating System Support for Digital Audio and Video (NOSSDAV) , November 1992. </booktitle>
Reference-contexts: A kernel-level implementation provides great exibility in choosing which control abstraction is most appropriate. One criticism of streaming-based data transfer mechanisms is that it inhibits innovation in application development by disallowing applications direct access to I/O data (for a good discussion of this issue, see <ref> [DAPP92] </ref>). However, there are many applications that do not require direct manipulation of I/O data that can benefit from streaming (e.g. data-retrieving servers that do not need to inspect the data they have been requested to deliver to a client).
Reference: [DP93] <author> P. Druschel and L. Peterson, Fbufs: </author> <title> a high-bandwidth cross-domain transfer facility, </title> <booktitle> Proc. 14th ACM Symp. Operating System Principles (SOSP) , Asheville, </booktitle> <address> NC, </address> <month> (December </month> <year> 1993), </year> <pages> pp. 189-202. </pages>
Reference: [F94] <author> K. </author> <month> Fall, </month> <title> A Peer-to-Peer I/O System in Support of I/O Intensive Workloads, </title> <type> Ph.D. Dissertation, </type> <institution> University of California, </institution> <address> San Diego, </address> <year> 1994. </year>
Reference-contexts: Since the latter constitutes new material, we devote more space to it relative to the other surveyed work (whose more detailed descriptions may be found in <ref> [FP93, FP94, F94, KP93, KP93a, KP93b, K95, KP95] </ref>). In addition to this work, other network research was also carried out as part of the Sequoia 2000 project. This includes research on protocols providing performance guarantees [FBZ92, ZVF92, BKTZ94] and multicasting [KPP93, KPP93a, PPAK94, PPAK95, PPK93]. <p> Furthermore, for applications requiring well-known data manipulations, kernel-resident processing modules (e.g. like Ritchies Streams [R84, PR85]) or outboard dedicated processors are more easily exploited within the kernel operating environment than in user processes. In fact, PPIO supports processing modules; these are described in <ref> [F94] </ref>. source dev sink dev kernel space user space application layer process FIGURE 5. An splice connecting a source to a sink device. The solid line shows the data path, while the dotted line indicates the control path. <p> Three performance evaluation studies of PPIO have been carried out, and are described in <ref> [FP93, FP94, F94] </ref>. They indicate CPU availability improves by 30% or more and throughput and latency improve by a factor of two to three, depending on the speed of I/O devices.
Reference: [FP93] <author> K. Fall and J. Pasquale, </author> <title> Exploiting In-kernel Data Paths to Improve I/O Throughput and CPU Availabil ity, </title> <booktitle> Proc. USENIX Winter Tech. Conf. </booktitle> , <address> San Diego, </address> <month> Jan uary 93, </month> <pages> pp. 327-333. </pages>
Reference-contexts: Since the latter constitutes new material, we devote more space to it relative to the other surveyed work (whose more detailed descriptions may be found in <ref> [FP93, FP94, F94, KP93, KP93a, KP93b, K95, KP95] </ref>). In addition to this work, other network research was also carried out as part of the Sequoia 2000 project. This includes research on protocols providing performance guarantees [FBZ92, ZVF92, BKTZ94] and multicasting [KPP93, KPP93a, PPAK94, PPAK95, PPK93]. <p> In PPIO, processes use the splice system call to create kernel-maintained associations between producer and consumer. Splice represents an addition to the conventional operating system I/O interfaces, and is not a replacement for the read write system functions. 5.1 The Splice Mechanism splice mechanism <ref> [FP93, FP94] </ref> is a system function used to establish a kernel-managed data path directly between I/O device peers. It is the primary mechanism processes invoke to use PPIO. <p> Three performance evaluation studies of PPIO have been carried out, and are described in <ref> [FP93, FP94, F94] </ref>. They indicate CPU availability improves by 30% or more and throughput and latency improve by a factor of two to three, depending on the speed of I/O devices.
Reference: [FP94] <author> K. Fall and J. Pasquale, </author> <title> Improving Continuous-media Playback Performance with In-kernel Data Paths, </title> <booktitle> Proc. IEEE Intl. Conf. on Multimedia Computing and Systems (ICMCS) , Boston, </booktitle> <address> MA, </address> <month> June 94, </month> <pages> pp. 100-109. </pages>
Reference-contexts: Since the latter constitutes new material, we devote more space to it relative to the other surveyed work (whose more detailed descriptions may be found in <ref> [FP93, FP94, F94, KP93, KP93a, KP93b, K95, KP95] </ref>). In addition to this work, other network research was also carried out as part of the Sequoia 2000 project. This includes research on protocols providing performance guarantees [FBZ92, ZVF92, BKTZ94] and multicasting [KPP93, KPP93a, PPAK94, PPAK95, PPK93]. <p> In PPIO, processes use the splice system call to create kernel-maintained associations between producer and consumer. Splice represents an addition to the conventional operating system I/O interfaces, and is not a replacement for the read write system functions. 5.1 The Splice Mechanism splice mechanism <ref> [FP93, FP94] </ref> is a system function used to establish a kernel-managed data path directly between I/O device peers. It is the primary mechanism processes invoke to use PPIO. <p> Three performance evaluation studies of PPIO have been carried out, and are described in <ref> [FP93, FP94, F94] </ref>. They indicate CPU availability improves by 30% or more and throughput and latency improve by a factor of two to three, depending on the speed of I/O devices.
Reference: [FBZ92] <author> D. Ferrari, A. Banerjea, and H. Zhang, </author> <title> Network Support for Multimedia: A Discussion of the Tenet Approach, </title> <type> Technical Report TR-92-072, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> CA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: In addition to this work, other network research was also carried out as part of the Sequoia 2000 project. This includes research on protocols providing performance guarantees <ref> [FBZ92, ZVF92, BKTZ94] </ref> and multicasting [KPP93, KPP93a, PPAK94, PPAK95, PPK93]. To support a high performance distributed computing environment where applications can effectively manipulate large data objects, we were concerned with achieving high throughput when transferring these objects from where they happen to reside to where they are needed.
Reference: [K95] <author> J. Kay, PathIDs: </author> <title> Reducing Latency in Network Soft ware, </title> <type> Ph.D. Dissertation, </type> <institution> University of California, </institution> <address> San Diego, </address> <year> 1995. </year>
Reference-contexts: Since the latter constitutes new material, we devote more space to it relative to the other surveyed work (whose more detailed descriptions may be found in <ref> [FP93, FP94, F94, KP93, KP93a, KP93b, K95, KP95] </ref>). In addition to this work, other network research was also carried out as part of the Sequoia 2000 project. This includes research on protocols providing performance guarantees [FBZ92, ZVF92, BKTZ94] and multicasting [KPP93, KPP93a, PPAK94, PPAK95, PPK93].
Reference: [KP93] <author> J. Kay and J. Pasquale, </author> <title> The Importance of Non-Data Touching Processing Overheads in TCP/IP, </title> <booktitle> Proc. ACM Communications Architectures and Protocols Conf. </booktitle> <address> (SIGCOMM) , San Francisco, CA, </address> <month> September 93, </month> <pages> pp. 259-269. </pages>
Reference-contexts: Since the latter constitutes new material, we devote more space to it relative to the other surveyed work (whose more detailed descriptions may be found in <ref> [FP93, FP94, F94, KP93, KP93a, KP93b, K95, KP95] </ref>). In addition to this work, other network research was also carried out as part of the Sequoia 2000 project. This includes research on protocols providing performance guarantees [FBZ92, ZVF92, BKTZ94] and multicasting [KPP93, KPP93a, PPAK94, PPAK95, PPK93].
Reference: [KP93a] <author> J. Kay and J. Pasquale, </author> <title> Measurement, Analysis, and Improvement of UDP/IP Throughput for the DECsta tion 5000, </title> <booktitle> Proc. USENIX Winter Tech. Conf. </booktitle> <address> Diego, </address> <month> January 93, </month> <pages> pp. 249-258. </pages>
Reference-contexts: Since the latter constitutes new material, we devote more space to it relative to the other surveyed work (whose more detailed descriptions may be found in <ref> [FP93, FP94, F94, KP93, KP93a, KP93b, K95, KP95] </ref>). In addition to this work, other network research was also carried out as part of the Sequoia 2000 project. This includes research on protocols providing performance guarantees [FBZ92, ZVF92, BKTZ94] and multicasting [KPP93, KPP93a, PPAK94, PPAK95, PPK93]. <p> e c e i v e P r o c e s s i n g T e ( u s e c ) Checksum Message Size (bytes) UDP Processing Overhead Times DataMove DataStruct ErrorChk Mbuf OpSys ProtSpec Other Checksum DataMove Mbuf, ProtSpec, Other OpSys, DataStruct, ErrorChk Page 8 See <ref> [KP93a] </ref> for detailed results. A very easy way of significantly raising TCP and UDP throughput is to simply avoid computing checksums; in fact, many systems provide options to do just this.
Reference: [KP93b] <author> J. Kay and J. Pasquale, </author> <title> A Summary of TCP/IP Networking Software Performance for the DECstation 5000, </title> <booktitle> Proc. ACM Conf. on Measurement and Model ing of Computer Systems (SIGMETRICS) , Santa Clara, </booktitle> <address> CA, </address> <month> May 93, </month> <pages> pp. 266-267. </pages>
Reference-contexts: Since the latter constitutes new material, we devote more space to it relative to the other surveyed work (whose more detailed descriptions may be found in <ref> [FP93, FP94, F94, KP93, KP93a, KP93b, K95, KP95] </ref>). In addition to this work, other network research was also carried out as part of the Sequoia 2000 project. This includes research on protocols providing performance guarantees [FBZ92, ZVF92, BKTZ94] and multicasting [KPP93, KPP93a, PPAK94, PPAK95, PPK93].
Reference: [KP95] <author> J. Kay and J. Pasquale, </author> <title> Profiling and Reducing Pro cessing Overheads in TCP/IP, </title> <note> IEEE/ACM Transac tions on Networking , accepted for publication. </note>
Reference-contexts: Since the latter constitutes new material, we devote more space to it relative to the other surveyed work (whose more detailed descriptions may be found in <ref> [FP93, FP94, F94, KP93, KP93a, KP93b, K95, KP95] </ref>). In addition to this work, other network research was also carried out as part of the Sequoia 2000 project. This includes research on protocols providing performance guarantees [FBZ92, ZVF92, BKTZ94] and multicasting [KPP93, KPP93a, PPAK94, PPAK95, PPK93]. <p> The full study is described in <ref> [KP95] </ref>. First, we categorized various generic functions commonly executed by TCP/IP (and UDP/IP) protocol stacks: Checksum: the checksum computation for UDP and TCP. DataMove: any operations that involve moving data from one memory location to another. Mbuf: the message buffering scheme used by Berkeley Unix-based network subsystems [43]. <p> This is because generally, data-touching overhead times scale linearly with packet size, whereas non-data-touching overhead times are comparatively constant. Thus, data-touching overheads present the major limitations to achieving maximum throughput. Processing times for TCP packets (not shown here, see <ref> [KP95] </ref>) are similar for large packets since data-touching operations, which do identical work in the TCP and UDP software, dom inate. Even TCP protocol-specific processing is only slightly more expensive than UDP protocol-specific processing.
Reference: [KPP93] <author> V. Kompella, J. Pasquale, and G. Polyzos, </author> <title> Multicast Routing for Multimedia Applications, </title> <journal> IEEE/ACM Trans. Networking , Vol. </journal> <volume> 1, No. 3, </volume> <month> June 93, </month> <pages> pp. 286-2 92. </pages>
Reference-contexts: In addition to this work, other network research was also carried out as part of the Sequoia 2000 project. This includes research on protocols providing performance guarantees [FBZ92, ZVF92, BKTZ94] and multicasting <ref> [KPP93, KPP93a, PPAK94, PPAK95, PPK93] </ref>. To support a high performance distributed computing environment where applications can effectively manipulate large data objects, we were concerned with achieving high throughput when transferring these objects from where they happen to reside to where they are needed.
Reference: [KPP93a] <author> V. Kompella, J. Pasquale, and G. Polyzos, </author> <title> Two Dis tributed Algorithms for Multicasting Multimedia Infor mation, </title> <booktitle> Proc. 2nd Intl. Conf. on Computer Communications and Networks (ICCCN) , San Diego, </booktitle> <address> CA, </address> <month> June 93, </month> <pages> pp. 343-349. </pages>
Reference-contexts: In addition to this work, other network research was also carried out as part of the Sequoia 2000 project. This includes research on protocols providing performance guarantees [FBZ92, ZVF92, BKTZ94] and multicasting <ref> [KPP93, KPP93a, PPAK94, PPAK95, PPK93] </ref>. To support a high performance distributed computing environment where applications can effectively manipulate large data objects, we were concerned with achieving high throughput when transferring these objects from where they happen to reside to where they are needed.
Reference: [MB91] <author> J. Mogul and A. Borg, </author> <title> The Effect of Context Switches on Cache Performance, </title> <booktitle> Proc. </booktitle> <month> ASPLOS-IV , April </month> <year> 1991, </year> <pages> pp. 75-84. </pages>
Reference-contexts: Thus, context-switch operations for data transfer are eliminated. This is important: context switches consume CPU resources, degrade cache performance by reducing locality of reference <ref> [MB91] </ref>, and effect the performance of virtual memory by requiring TLB invalidations [BALM90]. For applications making no direct manipulation of I/O data (or for those allowing the kernel to make such manipulations), splice relegates the issues of managing the data ow (e.g. buffering and ow control) to the kernel.
Reference: [MP91] <author> K. Muller and J. Pasquale, </author> <title> A High-Performance Multi-Structured File System Design, </title> <booktitle> Proc. 13th ACM Symp. on Operating System Principles (SOSP) , Asilo mar, </booktitle> <address> CA, </address> <month> October 91, </month> <pages> pp. 56-67. </pages>
Reference-contexts: We describe two solutions to the data transfer problem that avoid all physical copying and are based on the principle of providing separate mechanisms for I/O control and data transfer <ref> [MP91, P92, P93, TLL94] </ref>. The reader will see that while these two solutions are based on different approaches (indeed, they can even be viewed as competing), they fill different niches based on differing assumptions of how I/O is structured.
Reference: [P92] <author> J. Pasquale, </author> <title> I/O system design for intensive multime dia I/O, P roc. </title> <booktitle> 3rd IEEE Workshop Workstation Opera tion Systems (WWOS) , Key Biscayne, </booktitle> <address> FL, </address> <month> April 92, </month> <pages> pp. 29-33. </pages>
Reference-contexts: We describe two solutions to the data transfer problem that avoid all physical copying and are based on the principle of providing separate mechanisms for I/O control and data transfer <ref> [MP91, P92, P93, TLL94] </ref>. The reader will see that while these two solutions are based on different approaches (indeed, they can even be viewed as competing), they fill different niches based on differing assumptions of how I/O is structured.
Reference: [P93] <author> J. Pasquale, </author> <title> System Software and Hardware Support Considerations for Digital Video and Audio Comput ing, </title> <booktitle> Proc. 26th Hawaii Intl. Conf. on System Sciences (HICSS) , Maui, Hawaii, January 93, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 15-20. </pages>
Reference-contexts: We describe two solutions to the data transfer problem that avoid all physical copying and are based on the principle of providing separate mechanisms for I/O control and data transfer <ref> [MP91, P92, P93, TLL94] </ref>. The reader will see that while these two solutions are based on different approaches (indeed, they can even be viewed as competing), they fill different niches based on differing assumptions of how I/O is structured.
Reference: [PAM94] <author> J. Pasquale, E. Anderson, and K. Muller, </author> <title> Container Shipping: Operating System Support for Intensive I/O Applications, </title> <journal> IEEE Computer , Vol. </journal> <volume> 27, No. 3, March 94, </volume> <pages> pp. 84-93. </pages>
Reference-contexts: The contributions of this paper are twofold: (1) this paper surveys the main results of this work, and puts them in perspective by relating them to the general data transfer problem (see Section 3), and (2) it presents a new design for Container Shipping, originally described in <ref> [PAM94] </ref>. Since the latter constitutes new material, we devote more space to it relative to the other surveyed work (whose more detailed descriptions may be found in [FP93, FP94, F94, KP93, KP93a, KP93b, K95, KP95]). <p> All six system calls are supported, and container I/O can be measured in a variety of situations. Conventional Unix I/O remains, so a system can boot and run normally, using container I/O only for specific experiments. In <ref> [PAM94] </ref>, we showed significant throughput improvements for container-based IPC within Ultrix 4.2a on a DECstation 5000/ 200. With our new OSF implementation on Alpha workstations, we compared the I/O performance of conventional Unix I/O to that of container shipping for a variety of I/O devices as well as IPC.
Reference: [PPAK94] <author> J. Pasquale, G. Polyzos, E. Anderson, and V. Kompella, </author> <title> Filter Propagation in Dissemination Trees: Trading Off Bandwidth and Processing in Continuous Media Networks, </title> <booktitle> Proc. 4th International Workshop on Network and Operating System Support for Digital Audio and Video (NOSSDAV) , D. </booktitle> <editor> Shepherd, G. Blair, G. Coul son, N. Davies and F. Garcia (eds), </editor> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> Vol. 846, </volume> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: In addition to this work, other network research was also carried out as part of the Sequoia 2000 project. This includes research on protocols providing performance guarantees [FBZ92, ZVF92, BKTZ94] and multicasting <ref> [KPP93, KPP93a, PPAK94, PPAK95, PPK93] </ref>. To support a high performance distributed computing environment where applications can effectively manipulate large data objects, we were concerned with achieving high throughput when transferring these objects from where they happen to reside to where they are needed.
Reference: [PPAK95] <author> J. Pasquale, G. Polyzos, E. Anderson, and V. Kompella, </author> <title> The Multimedia Multicast Channel, </title> <journal> Journal of Inter networking: </journal> <note> Research and Experience , (in press). </note>
Reference-contexts: In addition to this work, other network research was also carried out as part of the Sequoia 2000 project. This includes research on protocols providing performance guarantees [FBZ92, ZVF92, BKTZ94] and multicasting <ref> [KPP93, KPP93a, PPAK94, PPAK95, PPK93] </ref>. To support a high performance distributed computing environment where applications can effectively manipulate large data objects, we were concerned with achieving high throughput when transferring these objects from where they happen to reside to where they are needed.
Reference: [PPK93] <author> J. Pasquale, G. Polyzos, and V. Kompella, </author> <title> Real-time Dissemination of Continuous media in Packet-switched Networks, </title> <booktitle> Proc. 38th IEEE Computer Society Intl. Conf. </booktitle> <address> (COMPCON) , San Francisco, </address> <month> February 93, </month> <pages> pp. 47-48. </pages>
Reference-contexts: In addition to this work, other network research was also carried out as part of the Sequoia 2000 project. This includes research on protocols providing performance guarantees [FBZ92, ZVF92, BKTZ94] and multicasting <ref> [KPP93, KPP93a, PPAK94, PPAK95, PPK93] </ref>. To support a high performance distributed computing environment where applications can effectively manipulate large data objects, we were concerned with achieving high throughput when transferring these objects from where they happen to reside to where they are needed.
Reference: [PR85] <author> D. Presotto and D. Ritchie, </author> <title> Interprocess Communica tion in the Eighth Edition UNIX System, </title> <booktitle> Proc. USENIX Winter Conference , January 1985, </booktitle> <pages> pp. </pages> <note> 309 316. </note> <author> [RTYG+88]R. Rashid, A. Tevanian, M. Young, D. Golub, R. Baron, D. Black, W. Bolosky, J. Chew, </author> <title> Machine-Independent Virtual Memory Management for Paged Uniprocessor Page 10 and Multiprocessor Architectures, </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37, 8, </volume> <pages> pp. 896-908, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Furthermore, for applications requiring well-known data manipulations, kernel-resident processing modules (e.g. like Ritchies Streams <ref> [R84, PR85] </ref>) or outboard dedicated processors are more easily exploited within the kernel operating environment than in user processes. In fact, PPIO supports processing modules; these are described in [F94]. source dev sink dev kernel space user space application layer process FIGURE 5.
Reference: [R84] <author> D. Ritchie, </author> <title> A Stream Input-Output System, </title> <journal> AT&T Bell Laboratories Technical Journal, </journal> <volume> Vol. 63, No. 8, </volume> <month> October </month> <year> 1984, </year> <pages> pp. 1897-1910. </pages>
Reference-contexts: Furthermore, for applications requiring well-known data manipulations, kernel-resident processing modules (e.g. like Ritchies Streams <ref> [R84, PR85] </ref>) or outboard dedicated processors are more easily exploited within the kernel operating environment than in user processes. In fact, PPIO supports processing modules; these are described in [F94]. source dev sink dev kernel space user space application layer process FIGURE 5.
Reference: [TLL94] <author> C. A. Thekkath, H. M. Levy, and E. D. Lazowska, </author> <title> Separating Data and Control Transfer in Distributed Operating Systems, </title> <booktitle> Proc. Sixth International Conference on Architectural Support for Programming Lan guages and Operating Systems (ASPLOS) , San Jose, </booktitle> <address> CA, </address> <month> October </month> <year> 1994, </year> <pages> pp. 2-11. </pages>
Reference-contexts: We describe two solutions to the data transfer problem that avoid all physical copying and are based on the principle of providing separate mechanisms for I/O control and data transfer <ref> [MP91, P92, P93, TLL94] </ref>. The reader will see that while these two solutions are based on different approaches (indeed, they can even be viewed as competing), they fill different niches based on differing assumptions of how I/O is structured.
Reference: [WM87] <author> R. W. Watson, S. A. Mamrak, </author> <title> Gaining Efficiency in Transport Services by Appropriate Design and Imple mentation Choices, </title> <journal> ACM Transactions on Computer Systems </journal>

References-found: 32

