URL: ftp://robotics.stanford.edu/pub/gjohn/papers/wrap-final.ps
Refering-URL: http://www.robotics.stanford.edu/~gjohn/pubs.html
Root-URL: http://www.robotics.stanford.edu
Email: ronnyk@sgi.com gjohn@cs.stanford.edu robotics.stanford.edu/~ronnyk robotics.stanford.edu/~gjohn  
Title: 1 THE WRAPPER APPROACH  
Author: Ron Kohavi George H. John 
Note: 1.1 INTRODUCTION Significant parts of this chapter are reprinted from Artificial Intelligence Journal, Vol. 97, Nos. 1-2, pp. 273-324, 1997 with kind permission from Elsevier Science NL, Sara Burger-hartstraat 25, 1055 KV Amsterdam, The Netherlands.  
Address: 2011 N. Shoreline Boulevard 2141 Landings Drive Mountain View, CA 94043 Mountain View, CA 94043  
Affiliation: Data Mining and Visualization Data Mining Silicon Graphics, Inc. Epiphany Marketing Software  
Abstract: In the feature subset selection problem, a learning algorithm is faced with the problem of selecting a relevant subset of features upon which to focus its attention, while ignoring the rest. To achieve the best possible performance with a particular learning algorithm on a particular training set, a feature subset selection method should consider how the algorithm and the training set interact. We explore the relation between optimal feature subset selection and relevance. The wrapper method searches for an optimal feature subset tailored to a particular algorithm and a domain. We compare the wrapper approach to induction without feature subset selection and to Relief, a filter approach to feature subset selection. Improvement in accuracy is achieved for some datasets for the two families of induction algorithms used: decision trees and Naive-Bayes. In addition, the feature subsets selected by the wrapper are significantly smaller than the original subsets used by the learning algorithms, thus producing more In supervised machine learning, an induction algorithm is typically presented with a set of training instances, where each instance is described by a vector of feature (or attribute) values and a class label. For example, in medical diagnosis problems the features might include the age, weight, and blood pressure of a patient, and the class label might indicate whether or not a physician de comprehensible models.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W., Kibler, D., and Albert, M. K. </author> <year> (1991). </year> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6(1) </volume> <pages> 37-66. </pages>
Reference-contexts: Practical machine learning algorithms|decision tree algorithms such as C4.5 (Quinlan, 1993) and CART (Breiman et al., 1984), and instance-based algorithms such as IBL <ref> (Aha et al., 1991) </ref>|are known to degrade in prediction accuracy when trained on data containing superfluous features. Algorithms such as Naive-Bayes (Duda and Hart, 1973; Good, 1965; Domingos and Paz-zani, 1997) are robust with respect to irrelevant features, but their performance degrades when correlated (even if relevant) features are added.
Reference: <author> Almuallim, H. and Dietterich, T. G. </author> <year> (1991). </year> <title> Learning with many irrelevant features. </title> <booktitle> In Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 547-552. </pages> <publisher> MIT Press. </publisher>
Reference: <author> Almuallim, H. and Dietterich, T. G. </author> <year> (1994). </year> <title> Learning boolean concepts in the presence of many irrelevant features. </title> <journal> Artificial Intelligence, 69(1-2):279-306. </journal>
Reference: <author> Atkeson, C. G. </author> <year> (1991). </year> <title> Using locally weighted regression for robot learning. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 958-963. </pages>
Reference: <author> Blum, A. L. and Rivest, R. L. </author> <year> (1992). </year> <title> Training a 3-node neural network is NP-complete. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 117-127. </pages>
Reference: <author> Boddy, M. and Dean, T. </author> <year> (1989). </year> <title> Solving time-dependent planning problems. </title> <editor> In Sridharan, N. S., editor, </editor> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <volume> volume 2, </volume> <pages> pages 979-984. </pages> <publisher> Morgan Kauf-mann Publishers, Inc. </publisher>
Reference-contexts: Best-first search usually terminates upon reaching the goal. Our problem is an optimization problem, so the search can be stopped at any point and the best solution found so far can be returned (theoretically improving over time), thus making it an anytime algorithm <ref> (Boddy and Dean, 1989) </ref>. In practice, we must WRAPPERS 7 to nodes that have one feature deleted or added.
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: Since the typical goal of supervised learning algorithms is to maximize classification accuracy on an unseen test set, we use accuracy as our metric in guiding the feature subset selection. Practical machine learning algorithms|decision tree algorithms such as C4.5 (Quinlan, 1993) and CART <ref> (Breiman et al., 1984) </ref>, and instance-based algorithms such as IBL (Aha et al., 1991)|are known to degrade in prediction accuracy when trained on data containing superfluous features.
Reference: <author> Brunk, C., Kelly, J., and Kohavi, R. </author> <year> (1997). </year> <title> MineSet: an integrated system for data mining. </title> <editor> In Heckerman, D., Mannila, H., Pregibon, D., and Uthurusamy, R., editors, </editor> <booktitle> Proceedings of the third international conference on Knowledge Discovery and Data Mining, </booktitle> <pages> pages 135-138. </pages> <publisher> AAAI Press. </publisher> <address> http://www.sgi.com/Products/software/MineSet. </address>
Reference: <author> Cardie, C. </author> <year> (1993). </year> <title> Using decision trees to improve case-based learning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 25-32. </pages> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: Kohavi and John (1997) discuss our early experiments with Relief and WRAPPERS 5 the variant we used, Relieved-F. Relief searches for all the relevant features (both weak and strong). Tree filters <ref> (Cardie, 1993) </ref> use a decision tree algorithm to select a subset of features, typically for a nearest-neighbor algorithm. Although they work well for some datasets, they may select bad feature subsets because features that are good for decision trees are not necessarily useful for nearest-neighbor.
Reference: <author> Devijver, P. A. and Kittler, J. </author> <year> (1982). </year> <title> Pattern Recognition: A Statistical Approach. </title> <booktitle> Prentice-Hall International. </booktitle>
Reference: <author> Domingos, P. and Pazzani, M. </author> <year> (1997). </year> <title> Beyond independence: Conditions for the optimality of the simple bayesian classifier. </title> <journal> Machine Learning, </journal> 29(2/3):103-130. 
Reference: <author> Duda, R. and Hart, P. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. Wiley. WRAPPERS 17 Geman, </title> <editor> S., Bienenstock, E., and Doursat, R. </editor> <year> (1992). </year> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-48. </pages>
Reference: <author> Ginsberg, M. L. </author> <year> (1993). </year> <booktitle> Essentials of Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Good, I. J. </author> <year> (1965). </year> <title> The Estimation of Probabilities: An Essay on Modern Bayesian Methods. </title> <publisher> M.I.T. Press. </publisher>
Reference: <author> Hyafil, L. and Rivest, R. L. </author> <year> (1976). </year> <title> Constructing optimal binary decision trees is NP-complete. </title> <journal> Information Processing Letters, </journal> <volume> 5(1) </volume> <pages> 15-17. </pages>
Reference: <author> John, G., Kohavi, R., and Pfleger, K. </author> <year> (1994). </year> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 121-129. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Using a clever trick, instead of fully evaluating each node in their search space, they perform partial evaluations of all frontier nodes in parallel in a "race," until one node becomes a clear winner. Since the introduction of the wrapper approach <ref> (John et al., 1994) </ref>, several authors have experimented with it in various contexts. Langley and Sage (1994) used the wrapper approach to select features for Naive-Bayes.
Reference: <author> John, G. H. </author> <year> (1997). </year> <title> Enhancements to the Data Mining Process. </title> <type> PhD thesis, </type> <institution> Stanford University, Computer Science department. </institution>
Reference-contexts: Relevance of a feature does not imply that it is in the optimal feature subset for a particular induction algorithm and, somewhat surprisingly, irrelevance does not imply that it should not be in the optimal feature subset <ref> (Kohavi and John, 1997) </ref>. Example 1 (Optimality does not imply relevance) A feature that always takes the value one is irrelevant by our definitions. But consider a limited Perceptron classifier (Rosenblatt, 1958).
Reference: <author> Kira, K. and Rendell, L. A. </author> <year> (1992). </year> <title> A practical approach to feature selection. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kohavi, R. </author> <year> (1995a). </year> <title> The power of decision tables. </title> <editor> In Lavrac, N. and Wrobel, S., editors, </editor> <booktitle> Proceedings of the European Conference on Machine Learning, Lecture Notes in Artificial Intelligence 914, </booktitle> <pages> pages 174-189, </pages> <address> Berlin, Heidelberg, New York. </address> <publisher> Springer Verlag. </publisher>
Reference: <author> Kohavi, R. </author> <year> (1995b). </year> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection. </title> <editor> In Mellish, C. S., editor, </editor> <booktitle> Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1137-1143. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The feature subset selection algorithm conducts a search for a good subset using the induction algorithm itself as part of the evaluation function. The accuracy of the induced classifiers is estimated using accuracy estimation techniques <ref> (Kohavi, 1995b) </ref>. The problem we are investigating is that of state space search, and different search engines will be investigated in the next sections. The wrapper approach conducts a search in the space of possible parameters.
Reference: <author> Kohavi, R. and John, G. </author> <year> (1995). </year> <title> Automatic parameter selection by minimizing estimated error. </title> <editor> In Prieditis, A. and Russell, S., editors, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 304-312. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For the small sample of 100 instances, the estimated accuracy rose to 76% (26% optimistic) after about 300 node evaluations, indicative of overfitting. Although the theoretical problem exists, our experiments with the wrapper approach indicate that overfitting is mainly a problem when the number of instances is small <ref> (Kohavi and Sommerfield, 1995) </ref>.
Reference: <author> Kohavi, R. and John, G. H. </author> <year> (1997). </year> <title> Wrappers for feature subset selection. </title> <journal> Artificial Intelligence, 97(1-2):273-324. </journal>
Reference-contexts: Relevance of a feature does not imply that it is in the optimal feature subset for a particular induction algorithm and, somewhat surprisingly, irrelevance does not imply that it should not be in the optimal feature subset <ref> (Kohavi and John, 1997) </ref>. Example 1 (Optimality does not imply relevance) A feature that always takes the value one is irrelevant by our definitions. But consider a limited Perceptron classifier (Rosenblatt, 1958).
Reference: <author> Kohavi, R. and Sommerfield, D. </author> <year> (1995). </year> <title> Feature subset selection using the wrapper model: Overfitting and dynamic search space topology. </title> <booktitle> In The First International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> pages 192-197. </pages>
Reference-contexts: For the small sample of 100 instances, the estimated accuracy rose to 76% (26% optimistic) after about 300 node evaluations, indicative of overfitting. Although the theoretical problem exists, our experiments with the wrapper approach indicate that overfitting is mainly a problem when the number of instances is small <ref> (Kohavi and Sommerfield, 1995) </ref>.
Reference: <author> Kohavi, R., Sommerfield, D., and Dougherty, J. </author> <year> (1996). </year> <title> Data mining using MLC ++ : A machine learning library in C ++ . In Tools with Artificial Intelligence, </title> <type> pages 234-245. </type> <institution> IEEE Computer Society Press. </institution> <note> Received the best paper award. http://www.sgi.com/Technology/mlc. </note>
Reference-contexts: This chapter incorporates a number of corrections and improvements suggested by readers of our earlier publications on the wrapper method, especially Pat Langley, Nick Littlestone, Nils Nilsson, and Peter Turney. Dan Sommerfield implemented large parts of the wrapper in MLC ++ <ref> (Kohavi et al., 1996) </ref>, which was used for all of the experiments. George John's work was supported under a National Science Foundation Graduate Research Fellowship. Most of the research for this paper was completed while the authors were at Stanford University.
Reference: <author> Kohavi, R. and Wolpert, D. H. </author> <year> (1996). </year> <title> Bias plus variance decomposition for zero-one loss functions. </title> <editor> In Saitta, L., editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 275-283. </pages> <note> Morgan Kauf-mann. Available at http://robotics.stanford.edu/users/ronnyk. </note>
Reference-contexts: This chapter incorporates a number of corrections and improvements suggested by readers of our earlier publications on the wrapper method, especially Pat Langley, Nick Littlestone, Nils Nilsson, and Peter Turney. Dan Sommerfield implemented large parts of the wrapper in MLC ++ <ref> (Kohavi et al., 1996) </ref>, which was used for all of the experiments. George John's work was supported under a National Science Foundation Graduate Research Fellowship. Most of the research for this paper was completed while the authors were at Stanford University.
Reference: <author> Koller, D. and Sahami, M. </author> <year> (1996). </year> <title> Toward optimal feature selection. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pages 284-292. </pages> <publisher> Morgan Kaufmann Publishers, Inc. </publisher> <address> 18 Kononenko, I. </address> <year> (1994). </year> <title> Estimating attributes: Analysis and extensions of Relief. </title> <editor> In Bergadano, F. and Raedt, L. D., editors, </editor> <booktitle> Proceedings of the European Conference on Machine Learning. </booktitle>
Reference: <author> Langley, P. </author> <year> (1994). </year> <title> Selection of relevant features in machine learning. </title> <booktitle> In AAAI Fall Symposium on Relevance, </booktitle> <pages> pages 140-144. </pages>
Reference: <author> Langley, P. and Sage, S. </author> <year> (1994). </year> <title> Induction of selective Bayesian classifiers. </title> <booktitle> In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 399-406, </pages> <address> Seattle, WA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Mallows, C. L. </author> <year> (1973). </year> <title> Some comments on c p . Technometrics, </title> <booktitle> 15 </booktitle> <pages> 661-675. </pages>
Reference-contexts: In some cases, measures can be devised that are algorithm specific, and these may be computed efficiently. For example, measures such as Mallow's C p <ref> (Mallows, 1973) </ref> and Prediction Sum of Squares (Neter et al., 1990) have been devised specifically for linear regression. However, these tailored measures would not work well with other algorithms such as Naive-Bayes. Filter approaches can fail|the Corral artificial dataset from John et al., (1994) is one example.
Reference: <author> Marill, T. and Green, D. M. </author> <year> (1963). </year> <title> On the effectiveness of receptors in recognition systems. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 9 </volume> <pages> 11-17. </pages>
Reference: <author> Moore, A. W., Hill, D. J., and Johnson, M. P. </author> <year> (1992). </year> <title> An empirical investigation of brute force to choose features, smoothers and function approximators. </title> <booktitle> In Computational Learning Theory and Natural Learning Systems Conference. </booktitle>
Reference-contexts: Atkeson (1991) used leave-one-out cross-validation to search a multidimensional real-valued space which includes feature weights in addition to other parameters for local learning, similar to the Generalized memory-based learning approach <ref> (Moore et al., 1992) </ref>. Skalak (1994) uses the wrapper approach for selecting a prototype subset to use with nearest-neighbor, in addition to feature selection|an interesting example of choosing training instances as opposed to features.
Reference: <author> Moore, A. W. and Lee, M. S. </author> <year> (1994). </year> <title> Efficient algorithms for minimizing cross validation error. </title> <editor> In Cohen, W. W. and Hirsh, H., editors, </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference: <author> Neter, J., Wasserman, W., and Kutner, M. H. </author> <year> (1990). </year> <title> Applied Linear Statistical Models. </title> <type> Irwin: Homewood, </type> <institution> IL, </institution> <note> 3rd edition. </note>
Reference-contexts: In some cases, measures can be devised that are algorithm specific, and these may be computed efficiently. For example, measures such as Mallow's C p (Mallows, 1973) and Prediction Sum of Squares <ref> (Neter et al., 1990) </ref> have been devised specifically for linear regression. However, these tailored measures would not work well with other algorithms such as Naive-Bayes. Filter approaches can fail|the Corral artificial dataset from John et al., (1994) is one example.
Reference: <author> Pazzani, M. J. </author> <year> (1995). </year> <title> Searching for dependencies in bayesian classifiers. </title> <editor> In Fisher, D. and Lenz, H., editors, </editor> <booktitle> Proceedings of the fifth International Workshop on Artificial Intelligence and Statistics, </booktitle> <address> Ft. Lauderdale, FL. </address>
Reference: <author> Provost, F. J. and Buchanan, B. G. </author> <year> (1995). </year> <title> Inductive policy: The pragmatics of bias selection. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 35-61. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, California. </address>
Reference-contexts: Since the typical goal of supervised learning algorithms is to maximize classification accuracy on an unseen test set, we use accuracy as our metric in guiding the feature subset selection. Practical machine learning algorithms|decision tree algorithms such as C4.5 <ref> (Quinlan, 1993) </ref> and CART (Breiman et al., 1984), and instance-based algorithms such as IBL (Aha et al., 1991)|are known to degrade in prediction accuracy when trained on data containing superfluous features.
Reference: <author> Rosenblatt, F. </author> <year> (1958). </year> <title> The perceptron: A probabilistic model for information storage and organization in the brain. </title> <journal> Psychological Review, </journal> <volume> 65 </volume> <pages> 386-408. </pages>
Reference-contexts: Example 1 (Optimality does not imply relevance) A feature that always takes the value one is irrelevant by our definitions. But consider a limited Perceptron classifier <ref> (Rosenblatt, 1958) </ref>. It has a weight associated with each feature, and it classifies instances based upon whether the linear combination of weights and feature values is greater than zero.
Reference: <author> Russell, S. J. and Norvig, P. </author> <year> (1995). </year> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey 07632. </address>
Reference: <author> Singh, M. and Provan, G. M. </author> <year> (1995). </year> <title> A comparison of induction algorithms for selective and non-selective Bayesian classifiers. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 497-505. </pages>
Reference: <author> Skalak, D. B. </author> <year> (1994). </year> <title> Prototype and feature selection by sampling and random mutation hill climbing algorithms. </title> <editor> In Cohen, W. W. and Hirsh, H., editors, </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference: <author> Taylor, C., Michie, D., and Spiegalhalter, D. </author> <year> (1994). </year> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Paramount Publishing International. </publisher>
Reference-contexts: For the DNA dataset, the wrapper approach using Naive-Bayes reduced the error rate from 6.1% to 3.9% (a relative error reduction of 36%), making it the best induction algorithm for this problem out of all the methods used in the StatLog experiments <ref> (Taylor et al., 1994) </ref>. One of the more surprising results was how well Naive-Bayes performed overall: Naive-Bayes outperforms C4.5 (with and without feature selection) on the real datasets. On average, the performance using feature subset selection improved both algorithms.

References-found: 41

