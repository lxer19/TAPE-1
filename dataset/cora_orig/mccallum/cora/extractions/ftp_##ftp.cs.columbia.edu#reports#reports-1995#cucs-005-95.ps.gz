URL: ftp://ftp.cs.columbia.edu/reports/reports-1995/cucs-005-95.ps.gz
Refering-URL: http://www.cs.columbia.edu/~library/1995.html
Root-URL: http://www.cs.columbia.edu
Title: A Generalization of Band Joins and The Merge/Purge Problem  
Author: Mauricio A. Hernandez 
Degree: THESIS PROPOSAL  
Date: February 1995  
Address: New York, NY 10027  
Affiliation: (CUCS-005-95) Department of Computer Science Columbia University  
Abstract-found: 0
Intro-found: 1
Reference: [ ACM, 1991 ] <editor> ACM. </editor> <booktitle> SIGMOD record, </booktitle> <month> December </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Merging large databases acquired from different sources with heterogenous representations of information has become an increasingly difficult problem for many organizations. Instances of this problem appearing in the literature have been called the semantic integration problem <ref> [ ACM, 1991 ] </ref> or the instance identification problem [ Wang and Madnick, 1989 ] . In this thesis we consider the problem over very large databases of information that need to be processed as quickly, efficiently, and accurately as possible. <p> A more loosely-coupled collection of databases for which no DBMS provides consistent schemas among them are called Multidatabase Systems (MDBS) [ Elmagarmid and Pu, 1990 ] . Semantic Heterogeneity has been recognized as a difficult problem in MDBS. Recently, ACM SIGMOD dedicated a special issue to this problem <ref> [ ACM, 1991 ] </ref> . In that issue, [ Kent, 1991 ] explains how many assumptions in centralized database systems cannot be taken for granted when using a multidatabase system. Some of his examples are: 1.
Reference: [ Agrawal and Jagadish, 1988 ] <author> R. Agrawal and H. V. Jagadish. </author> <title> Multiprocessor Transitive Closure Algorithms. </title> <booktitle> In Proc. Int'l Symp. on Databases in Parallel and Distributed Systems, </booktitle> <pages> pages 56-66, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: The more corrupted the data, more runs might be needed to capture the matching records. The transitive closure, however, is executed on pairs of tuple id's, each at most 30 bits, and fast solutions to compute transitive closure exist <ref> [ Agrawal and Jagadish, 1988 ] </ref> . From observing real world scenarios, the size of the data set over which the closure is computed is at least one order of magnitude smaller than the corresponding database of records, and thus does not contribute a large cost.
Reference: [ Anderson and Matessa, 1990 ] <author> J. Anderson and M. Matessa. </author> <title> A Rational Analysis of Categorization. </title> <booktitle> In Machine Learning: Proceedings of the Seventh Int'l Conference, </booktitle> <pages> pages 76-84, </pages> <year> 1990. </year>
Reference-contexts: similarity to form clusters, but also take into consideration background knowledge, such as knowledge about likely cluster shapes. 3 These processes are sometimes referred to as unsupervised and supervised learning. 17 Some recent examples of this approach include the two Bayesian classifiers in [ Cheeseman et al., 1988 ] and <ref> [ Anderson and Matessa, 1990 ] </ref> . The output of these classifications algorithms can be thought as a set of selection predicates that divide the data into disjoint clusters. We can generalize some of these clustering approaches using the following model.
Reference: [ Batini et al., 1986 ] <author> C. Batini, M. Lenzerini, and S. Navathe. </author> <title> A Comparative Analysis of Methodologies for Database Schema Integration. </title> <journal> ACM Computing Surverys, </journal> <volume> 18(4) </volume> <pages> 323-364, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: The first issue, where relations have different schema, has been addressed extensively in the literature and is known as the schema integration problem <ref> [ Batini et al., 1986 ] </ref> . This problem is outside the scope of this thesis and is not discussed further. We are primarily interested in the second problem: heterogeneous representations of data and its implication when merging or joining relations.
Reference: [ Bickel, 1987 ] <author> M. A. Bickel. </author> <title> Automatic Correction to Misspelled Names: a Fourth-generation Language Approach. </title> <journal> Communications of the ACM, </journal> <volume> 30(3) </volume> <pages> 224-228, </pages> <year> 1987. </year>
Reference-contexts: Since we only have a corpus for the names of the cities in the U.S.A. (18670 different names), we only attempted correcting the spelling of the city field. We chose the algorithm described by Bickel in <ref> [ Bickel, 1987 ] </ref> for its simplicity and speed. Although not shown in the results presented in this proposal, the use of spell corrector over the city field improved the percent of correctly found duplicated records by only 1.5% - 2.0%.
Reference: [ Bitton and DeWitt, 1983 ] <author> D. Bitton and D. J. DeWitt. </author> <title> Duplicate Record Elimination in Large Data Files. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 8(2) </volume> <pages> 255-265, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: We are however only interested in sorting algorithms in which duplicates are removed from the final sorted database. To date, the most relevant work in this area is <ref> [ Bitton and DeWitt, 1983 ] </ref> . Finally, the proposed solution to the merge/purge problem resembles a sort-merge join [ Gotlieb, 1975 ] in which the join condition is a user-defined equivalence function. <p> Lower bounds for sorting multisets where studied by [ Munro and Spira, 1976 ] . They showed that the multiplicities (i.e., duplicates) of a set can only be obtained by comparisons if the total order is discovered in the process. Later <ref> [ Bitton and DeWitt, 1983 ] </ref> studied the problem of duplicate elimination in the context of large data files. They first present the "traditional" algorithm for duplicate elimination consisting of a complete sort of the file followed by a scan to remove duplicates. <p> Our initial approach for solving merge/purge will resemble this "traditional" approach. We will sort the data first, and then search "duplicates" in one scan over the resulting sorted data. The main differences between <ref> [ Bitton and DeWitt, 1983 ] </ref> 's approach and the approach we propose here are the use of a "window of records" to limit the number of possible duplicates we will consider, and the use of a user-specified, knowledge-based, equality theory to determine if tuples are indeed "duplicates". [ Bitton and <p> differences between <ref> [ Bitton and DeWitt, 1983 ] </ref> 's approach and the approach we propose here are the use of a "window of records" to limit the number of possible duplicates we will consider, and the use of a user-specified, knowledge-based, equality theory to determine if tuples are indeed "duplicates". [ Bitton and DeWitt, 1983 ] then modify the traditional approach to allow duplicate elimination during different stages of the sorting procedure. Through some cost-analysis and models, they show the modified duplicate elimination algorithm to be superior than the "traditional" approach. <p> Our experimental results, presented in section 4, demonstrate this to be the case. The sorted-neighborhood method resembles a merge-sort in which we are interested in removing duplicates. Two approaches for duplicate elimination using merge-sort were described in <ref> [ Bitton and DeWitt, 1983 ] </ref> . The first approach, called the "traditional" approach (which we will call the "naive" approach), is the following: first, the file is sorted using an external merge-sort algorithm. Then duplicate records are removed in one sequential scan of the sorted database.
Reference: [ Bratbergsengen, 1984 ] <author> K. Bratbergsengen. </author> <title> Hashing methods and relational algebra operators. </title> <booktitle> In Proceedings of the 1984 VLDB Conference, </booktitle> <month> August </month> <year> 1984. </year>
Reference-contexts: As a result of this effort, a number of algorithms to perform the costly Join operation have been proposed (see [ Mishra and Eich, 1992 ] ). Currently, three basic algorithms dominate commercial database implementations: nested-loop-joins (the "naive" algorithm), sort-merge-joins, and hash-joins <ref> [ Bratbergsengen, 1984 ] </ref> , with many variants of each. More recently, attention has shifted towards the efficient parallel execution of S-P-J queries (e.g., [ Schneider and DeWitt, 1989 ] ).
Reference: [ Brinkhoff et al., 1994 ] <author> T. Brinkhoff, H. Kriegel, R. Schneider, and S. Bernhard. </author> <title> Multi-Step Processing of Spatial Joins. </title> <booktitle> In Proceedings of the 1994 ACM-SIGMOD Conference, </booktitle> <pages> pages 197-208, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: The function F could be a simple arithmetic predicate or a complex inference procedure defined over a mixture of domains of the chosen attributes. There are several application problems in which the generalization of band-joins is used. One example is intersection spatial-joins <ref> [ Brinkhoff et al., 1994 ] </ref> , where a possible set of spatial objects are first identified and then a more complex geometric filter is applied to determine which objects satisfy the spatial join predicate. <p> This procedure is repeated until either there is a pass in which no new allocation is produced, or a time deadline is reached. 6.4.2 A Spatial Join Application Spatial Joins is one of the most frequently used operations in a spatial database system <ref> [ Brinkhoff et al., 1994 ] </ref> . <p> Consider a database containing geometric information taken from a map of New York City. A spatial join would be used to answer a query like "Is Central Park inside Manhattan?". This particular type of spatial join is called an intersection join. In <ref> [ Brinkhoff et al., 1994 ] </ref> , a procedure to process intersection joins over relations with spatial attributes is described. We note analogies between their solution, which they call the multi-step procedure of spatial joins, and our sorted-neighborhood method.
Reference: [ Cheeseman et al., 1988 ] <author> P. Cheeseman, J. Kelly, M. Self, J. Sutz, W. Taylor, and D. Freeman. </author> <title> AUTOCLASS: A Bayesian Classification System. </title> <booktitle> In Proceedings of the Fifth Int'l Conference on Machine Learning, </booktitle> <pages> pages 54-64, </pages> <year> 1988. </year>
Reference-contexts: methods which, as the former, use attribute similarity to form clusters, but also take into consideration background knowledge, such as knowledge about likely cluster shapes. 3 These processes are sometimes referred to as unsupervised and supervised learning. 17 Some recent examples of this approach include the two Bayesian classifiers in <ref> [ Cheeseman et al., 1988 ] </ref> and [ Anderson and Matessa, 1990 ] . The output of these classifications algorithms can be thought as a set of selection predicates that divide the data into disjoint clusters. We can generalize some of these clustering approaches using the following model.
Reference: [ Church and Gale, 1991 ] <author> K. W. Church and W. A. Gale. </author> <title> Probability scoring for spelling correction. </title> <journal> Statistics and Computing, </journal> <volume> 1 </volume> <pages> 93-103, </pages> <year> 1991. </year>
Reference: [ Codd, 1970 ] <author> E. Codd. </author> <title> A relational model for large shared data banks. </title> <journal> Communications of the ACM, </journal> <volume> 13(6), </volume> <month> June </month> <year> 1970. </year> [ <title> comp.graphics FAQ, 1994 ] comp.graphics FAQ. Computer Graphics Resource Listing. In http://www.cis.ohio-state.edu/hypertext/faq/usenet/graphics/resources-list/, 1994. Part 6 of 6, Subject 20: User Interface Builders. </title>
Reference-contexts: Take for example the standard definition of a relation under Relational Databases <ref> [ Codd, 1970 ] </ref> . A relation is defined as a set of tuples.
Reference: [ Copeland et al., 1988 ] <author> G. Copeland, W. Alexander, E. Boughter, and T. Keller. </author> <title> Data placement in bubba. </title> <booktitle> In Proceedings of the 1988 ACM-SIGMOD Conference, </booktitle> <pages> pages 99-108, </pages> <year> 1988. </year>
Reference: [ Dewan et al., 1994 ] <author> H. M. Dewan, M. A. Hernandez, K. Mok, and S. Stolfo. </author> <title> Predictive Load Balancing of Parallel Hash-Joins over Heterogeneous Processors in the Presence of Data Skew. </title> <booktitle> In Proc. 3rd Int'l Conf. on Parallel and Distributed Information Systems, </booktitle> <pages> pages 40-49, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: All previous mentioned works assume a homogeneous set of processing sites. Recent work by ourselves <ref> [ Dewan et al., 1994 ] </ref> and by others [ Lu and Tan, 1994 ] have addressed these problems in the context of load balancing protocols to deal with possible heterogeneity among processing sites. <p> It then redistributes the clusters among processors using a longest processing time first [ Graham, 1969 ] strategy. That is, move the largest job in an overloaded processor to the most underloaded processor, and repeat until a "well" balanced load is obtained. In <ref> [ Dewan et al., 1994 ] </ref> we detailed the load balancing algorithm in the context of parallel database joins and how it can deal with skewed data distributions in a heterogenous processing environment. The time results for the clustering method are depicted in figure 15.
Reference: [ Dewan, 1994 ] <author> H. Dewan. </author> <title> Runtime Reorganization of Parallel and Distributed Expert Database Systems. </title> <type> PhD thesis, </type> <institution> Columbia University, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: This allocation is a combinatorial optimization problem whose complexity is known to be NP-complete. ALEXSYS, described elsewhere [ Stolfo et al., 1991 ] , has become a benchmark in the rule-base processing community [ Neiman, 1994 ] and has also recently been used as a test case in <ref> [ Dewan, 1994 ] </ref> . The ALEXSYS rule program works by combining sets of pools into sell contracts whose sizes are controlled by Public Securities Administration (PSA) regulations. Ideally, we would like to apply the PSA rules over the entire set of pools and extract the best allocations.
Reference: [ DeWitt et al., 1991 ] <author> D. J. DeWitt, J. F. Naughton, and D. A. Schneider. </author> <title> An Evaluation of Non-Equijoin Algorithms. </title> <booktitle> In Proc. 17th Int'l. Conf. on Very Large Databases, </booktitle> <pages> pages 443-452, </pages> <address> Barcelona, Spain, </address> <year> 1991. </year>
Reference-contexts: When these "errors" in the data are not severe, we might ideally expect to find the matching instance of a tuple in R within a "band" of tuples in S. This type of non-equijoin joins are called band-joins and have been studied by <ref> [ DeWitt et al., 1991 ] </ref> . <p> Finally, the proposed solution to the merge/purge problem resembles a sort-merge join [ Gotlieb, 1975 ] in which the join condition is a user-defined equivalence function. Of particular relevance to the merge/purge solution proposed here is the work on "band-joins" by <ref> [ DeWitt et al., 1991 ] </ref> . We briefly describe each one of these lines of work in the following sections. 2.1 Heterogenous Multi-Databases A database system consists of a software component called a database management system and a set of databases it manages. <p> Thus, we can define as (S:temp5 R:temp and R:temp S:temp+5). Algorithms for executing this kind of non-equijoin predicate have been presented by <ref> [ DeWitt et al., 1991 ] </ref> . [ DeWitt et al., 1991 ] call joins in which the join-predicate has the form R:A c 1 S:B R:A + c 2 , band-joins. Their paper presents a new algorithm termed a partitioned band join to evaluate these special type of joins. <p> Thus, we can define as (S:temp5 R:temp and R:temp S:temp+5). Algorithms for executing this kind of non-equijoin predicate have been presented by <ref> [ DeWitt et al., 1991 ] </ref> . [ DeWitt et al., 1991 ] call joins in which the join-predicate has the form R:A c 1 S:B R:A + c 2 , band-joins. Their paper presents a new algorithm termed a partitioned band join to evaluate these special type of joins.
Reference: [ DeWitt et al., 1992 ] <author> D. J. DeWitt, J. F. Naughton, D. A. Schneider, and S. Seshadri. </author> <title> Practical skew handling in parallel joins. </title> <booktitle> In Proceedings of the 18th VLDB Conference, </booktitle> <pages> pages 27-39, </pages> <year> 1992. </year>
Reference: [ Dubes and Jain, 1976 ] <author> R. Dubes and A. Jain. </author> <title> Clustering Techniques: The User's Dilema. </title> <journal> Pattern Recognition, </journal> <volume> 8 </volume> <pages> 247-260, </pages> <year> 1976. </year>
Reference-contexts: Discovery algorithms can start their identification process by classifying records into classes (or clusters) that reflect patterns inherent in the data 3 . The creation of clusters can involve traditional cluster analysis methods <ref> [ Dubes and Jain, 1976 ] </ref> or more recent conceptual clus tering methods which, as the former, use attribute similarity to form clusters, but also take into consideration background knowledge, such as knowledge about likely cluster shapes. 3 These processes are sometimes referred to as unsupervised and supervised learning. 17 Some
Reference: [ Elmagarmid and Pu, 1990 ] <author> A. Elmagarmid and C. Pu. </author> <title> Guest Editors' Introduction to the Special Issue on Heterogeneous Databases. </title> <journal> Computing Surveys, </journal> <volume> 22(3) </volume> <pages> 175-178, </pages> <month> September </month> <year> 1990. </year> <month> 45 </month>
Reference-contexts: A more loosely-coupled collection of databases for which no DBMS provides consistent schemas among them are called Multidatabase Systems (MDBS) <ref> [ Elmagarmid and Pu, 1990 ] </ref> . Semantic Heterogeneity has been recognized as a difficult problem in MDBS. Recently, ACM SIGMOD dedicated a special issue to this problem [ ACM, 1991 ] .
Reference: [ Forgy, 1981 ] <author> C. L. Forgy. </author> <title> OPS5 User's Manual. </title> <type> Technical Report CMU-CS-81-135, </type> <institution> Carnegie Mellon University, </institution> <month> July </month> <year> 1981. </year>
Reference-contexts: The results displayed in section 4 are based upon edit distance computation since the outcome of the program did not vary much among the different distance functions for the particular databases used in our study. For the purpose of experimental study, we wrote an OPS5 <ref> [ Forgy, 1981 ] </ref> rule program consisting of 26 rules for this particular domain of employee records and was tested repeatedly over relatively small databases of records.
Reference: [ Frawley et al., 1992 ] <author> W. Frawley, G. Piatetsky, and C. Matheus. </author> <title> Knowledge Discovery in Databases: An Overview. </title> <journal> AI Magazine, </journal> <pages> pages 57-70, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: Hash partitioning uses a hash function to divide the input into P partitions or buckets. This partitioning scheme could be useful in cases where the input relation has a key attribute which is known not to be noisy. Discovery algorithms have been described by <ref> [ Frawley et al., 1992 ] </ref> as procedures to extract knowledge from data. Two processes are involved in these procedures: interesting patterns must be identified, and a meaningful description of each pattern must be provided.
Reference: [ Ghandeharizadeh, 1990 ] <author> S. Ghandeharizadeh. </author> <title> Physical Database Design in Multiprocessor Database Systems. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Wisconsin - Madison, </institution> <year> 1990. </year>
Reference-contexts: Clustering data as described above raises the issue of how well partitioned the data is after clustering. We use an approach that closely resembles the multidimensional partitioning strategy of <ref> [ Ghandeharizadeh, 1990 ] </ref> . If the data from which the n-attribute key is extracted is distributed uniformly over its domain, then we can expect all clusters to have approximately the same number of records in them. <p> In general, this technique can be applied to several attributes of a relation and is known as multidimensional partition strategy (e.g., <ref> [ Ghandeharizadeh, 1990 ] </ref> ). There are, however, other partitioning strategies we might want to provide in a generic merge/purge facility we plan to implement, namely, constant partitioning, uniform partitioning, hash partitioning, and classification.
Reference: [ Gotlieb, 1975 ] <author> L. Gotlieb. </author> <title> Computing joins of relations. </title> <booktitle> In Proceedings of the 1975 ACM SIGMOD Conference, </booktitle> <year> 1975. </year>
Reference-contexts: We are however only interested in sorting algorithms in which duplicates are removed from the final sorted database. To date, the most relevant work in this area is [ Bitton and DeWitt, 1983 ] . Finally, the proposed solution to the merge/purge problem resembles a sort-merge join <ref> [ Gotlieb, 1975 ] </ref> in which the join condition is a user-defined equivalence function. Of particular relevance to the merge/purge solution proposed here is the work on "band-joins" by [ DeWitt et al., 1991 ] .
Reference: [ Graham, 1969 ] <author> R. Graham. </author> <title> Bounds on multiprocessing timing anomalies. </title> <journal> SIAM Journal of Computing, </journal> <volume> 17 </volume> <pages> 416-429, </pages> <year> 1969. </year>
Reference-contexts: The coordinator processor keeps track of how many records it sent to each processor (and cluster) and therefore it knows, at the end of the clustering stage, how balanced the partition is. It then redistributes the clusters among processors using a longest processing time first <ref> [ Graham, 1969 ] </ref> strategy. That is, move the largest job in an overloaded processor to the most underloaded processor, and repeat until a "well" balanced load is obtained.
Reference: [ Gunther, 1993 ] <author> O. Gunther. </author> <title> Efficient Computations of Spatial Joins. </title> <booktitle> In Proceedings of the 9th Int'l Conf. on Data Engineering, </booktitle> <pages> pages 50-69, </pages> <year> 1993. </year>
Reference-contexts: Given a relation A whose i-th column is a spatial attribute and a relation B whose j-th column is also a spatial attribute, then A 1 B is a spatial join if is a spatial predicate involving the spatial attributes of A and B <ref> [ Gunther, 1993 ] </ref> . Consider a database containing geometric information taken from a map of New York City. A spatial join would be used to answer a query like "Is Central Park inside Manhattan?". This particular type of spatial join is called an intersection join.
Reference: [ Harrison and Rubin, 1978 ] <author> M. C. Harrison and N. Rubin. </author> <title> Another generalization of resolution. </title> <journal> Journal of the ACM, </journal> <volume> 25(3), </volume> <month> July </month> <year> 1978. </year>
Reference-contexts: The use of equational theories or knowledge-intensive matching procedures is not a new idea. The first reference to this idea comes from the context of theorem proving by resolution in <ref> [ Harrison and Rubin, 1978 ] </ref> . In that paper, Harrison and Rubin generalize the usual unification procedure allowing the specification of "equality predicates". In a more recent 5 example, [ Tsur, 1991 ] introduced this idea in the context of deductive database.
Reference: [ Hayes et al., 1983 ] <author> F. Hayes, D. Waterman, and D. Lenat. </author> <title> Building Expert Systems. </title> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <year> 1983. </year>
Reference-contexts: This is of particular importance when the system is run in a parallel environment. The use of tools to elicit knowledge from the user resembles the procedure of eliciting knowledge from an expert in the process of building an expert system <ref> [ Hayes et al., 1983 ] </ref> .
Reference: [ Hernandez and Stolfo, 1995 ] <author> M. Hernandez and S. Stolfo. </author> <title> The Merge/Purge Problem for Large Databases. </title> <booktitle> To appear in the Proceedings of the 1995 ACM-SIGMOD Conference, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: The moral is simply that several distinct "cheap" passes over the data produces more accurate results than one "expensive" pass over the data. (Our preliminary results detailing this approach has been accepted for publication at the 1995 ACM-SIGMOD Conference <ref> [ Hernandez and Stolfo, 1995 ] </ref> .) This thesis work is far from complete. The experimental results provided in this proposal verifies our core ideas for the solution method we propose. Additional work is needed in the purge phase of the merge/purge procedure.
Reference: [ Hua and Lee, 1990 ] <author> K. A. Hua and C. Lee. </author> <title> An adaptive data placement scheme for parallel database computer systems. </title> <booktitle> In Proceedings of the 16th VLDB Conference, </booktitle> <pages> pages 493-506, </pages> <year> 1990. </year>
Reference-contexts: have addressed various problems related to the parallel execution of S-P-J queries like, data partition among participating processors [ Copeland et al., 1988; Kitsuregawa and Ogawa, 1990; Ghandeharizadeh, 1990 ] , skew handling [ Schneider and DeWitt, 1990; Wolf et al., 1991; DeWitt et al., 1992 ] , and load-balancing <ref> [ Hua and Lee, 1990 ] </ref> . All previous mentioned works assume a homogeneous set of processing sites.
Reference: [ Kent, 1991 ] <author> W. Kent. </author> <title> The Breakdown of the Information Model in Multi-Database Systems. </title> <booktitle> SIGMOD Record, </booktitle> <volume> 20(4) </volume> <pages> 10-15, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: Finally, we detail in section 6 the work that remains to conclude our thesis. 3 2 Previous Work Several lines of work have baring on efficient solutions for the merge/purge problem. The semantic integration problem <ref> [ Kent, 1991 ] </ref> seeks to identifying a multiplicity of database objects that represent the same or related real-world entity, even though their database representations are different. This problem has been studied by the heterogenous multi-database community. <p> Semantic Heterogeneity has been recognized as a difficult problem in MDBS. Recently, ACM SIGMOD dedicated a special issue to this problem [ ACM, 1991 ] . In that issue, <ref> [ Kent, 1991 ] </ref> explains how many assumptions in centralized database systems cannot be taken for granted when using a multidatabase system. Some of his examples are: 1. <p> Of particular interest for us in this thesis proposal is the identity and naming problem. This problem has been also called the inter-database identification problem by [ Wang and Madnick, 1989 ] . <ref> [ Kent, 1991 ] </ref> proposes the use of spheres of knowledge to address this problem. Spheres of knowledge create views of the underlying multidatabases to integrate data from diverse sources and attempt to provide a consistent view of that data to the end-user.
Reference: [ Kitsuregawa and Ogawa, 1990 ] <author> M. Kitsuregawa and Y. Ogawa. </author> <title> Bucket spreading parallel hash: A new, robust, parallel hash join method for data skew in the super database computer (sdc). </title> <booktitle> In Proceedings of the 16th VLDB Conference, </booktitle> <pages> pages 210-221, </pages> <year> 1990. </year>
Reference: [ Knuth, 1973 ] <author> D. Knuth. </author> <title> The Art of Computer Programming: Sorting and Searching (Volume 3). </title> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: Sorting data is probably the most studied problem in Computer Science and many different algorithms have been presented over the years <ref> [ Knuth, 1973 ] </ref> . We are however only interested in sorting algorithms in which duplicates are removed from the final sorted database. To date, the most relevant work in this area is [ Bitton and DeWitt, 1983 ] .
Reference: [ Kukich, 1992 ] <author> K. Kukich. </author> <title> Techniques for Automatically Correcting Words in Text. </title> <journal> ACM Computing Surveys, </journal> <volume> 24(4) </volume> <pages> 377-439, </pages> <year> 1992. </year>
Reference-contexts: Spelling correction algorithms have received a large amount of attention for decades <ref> [ Kukich, 1992 ] </ref> . Most of the spelling correction algorithms we considered use a corpus of correctly spelled words from which the correct spelling is selected.
Reference: [ Lu and Tan, 1994 ] <author> H. Lu and K. Tan. </author> <title> Load-balanced join processing in shared-nothing systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 23(3) </volume> <pages> 382-398, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: All previous mentioned works assume a homogeneous set of processing sites. Recent work by ourselves [ Dewan et al., 1994 ] and by others <ref> [ Lu and Tan, 1994 ] </ref> have addressed these problems in the context of load balancing protocols to deal with possible heterogeneity among processing sites. However, almost all work in the parallel execution of S-P-J queries has concentrated on hash-join algorithms, and only one type of join queries, namely, equijoins.
Reference: [ Miranker et al., 1990 ] <author> D. P. Miranker, B. Lofaso, G. Farmer, A. Chandra, and D. </author> <title> Brant. On a TREAT-based Production System Compiler. </title> <booktitle> In Proc. 10th Int'l Conf. on Expert Systems, </booktitle> <pages> pages 617-630, </pages> <year> 1990. </year>
Reference-contexts: Another compiler, the OPS5C compiler <ref> [ Miranker et al., 1990 ] </ref> was not available to us in time for these studies. The OPS5C compiler produces code that is reportedly many times faster than previous compilers.
Reference: [ Mishra and Eich, 1992 ] <author> P. Mishra and M. Eich. </author> <title> Join processing in relational databases. </title> <journal> ACM Computing Surveys, </journal> <volume> 24(1) </volume> <pages> 63-113, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The fast execution of complex select-project-join (S-P-J) queries has received considerable attention from many researchers over the past 20 years. As a result of this effort, a number of algorithms to perform the costly Join operation have been proposed (see <ref> [ Mishra and Eich, 1992 ] </ref> ). Currently, three basic algorithms dominate commercial database implementations: nested-loop-joins (the "naive" algorithm), sort-merge-joins, and hash-joins [ Bratbergsengen, 1984 ] , with many variants of each.
Reference: [ Munro and Spira, 1976 ] <author> I. Munro and P. Spira. </author> <title> Sorting and Searching in Multisets. </title> <journal> SIAM Journal of Computing, </journal> <volume> 5(1) </volume> <pages> 1-8, </pages> <month> March </month> <year> 1976. </year>
Reference-contexts: Moreover, in the classic definition of the project relational operator, duplicates in the projected relation are expected to be removed from the resulting relation to be valid. Lower bounds for sorting multisets where studied by <ref> [ Munro and Spira, 1976 ] </ref> . They showed that the multiplicities (i.e., duplicates) of a set can only be obtained by comparisons if the total order is discovered in the process.
Reference: [ Neiman, 1994 ] <author> D. Neiman. </author> <title> Issues in the Design and Control of Parallel Rule-firing Production Systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 23(3), </volume> <month> December </month> <year> 1994. </year>
Reference-contexts: This allocation is a combinatorial optimization problem whose complexity is known to be NP-complete. ALEXSYS, described elsewhere [ Stolfo et al., 1991 ] , has become a benchmark in the rule-base processing community <ref> [ Neiman, 1994 ] </ref> and has also recently been used as a test case in [ Dewan, 1994 ] . The ALEXSYS rule program works by combining sets of pools into sell contracts whose sizes are controlled by Public Securities Administration (PSA) regulations.
Reference: [ Nyberg et al., 1994 ] <author> C. Nyberg, T. Barclay, Z. Cvetanovic, J. Gray, and D. Lomet. </author> <month> AlphaSort: </month>
Reference-contexts: In this case, at least three passes would be needed, one pass for conditioning the data and preparing keys, at least a second pass, likely more, for a high speed sort like, for example, the AlphaSort <ref> [ Nyberg et al., 1994 ] </ref> , and a final pass for window processing and application of the rule program for each record entering the sliding window. Depending upon the complexity of the rule program, the last pass may indeed be the dominant cost. <p> Thus, in this proposal we consider alternative metrics for the purposes of merge/purge to include how accurately can you merge/purge for a fixed dollar and given time constraint, rather than the specific cost- and time-based metrics proposed in <ref> [ Nyberg et al., 1994 ] </ref> . 3.1.2 The Duplicate Elimination Sorted-Neighborhood Method Similar to the "naive" algorithm, given a collection of two or more databases, we first concatenate them into one sequential list of N records. <p> Of course, doubling the speed of the workstations and utilizing the various RAID-based striping optimizations to double disk I/O speeds discussed in <ref> [ Nyberg et al., 1994 ] </ref> and elsewhere (which is certainly possible today since the HP processors and disks used here are slow compared to, for example, Alpha workstations with modern RAID-disk technology) would produce a total time that is at least half the estimated time, i.e. within 3-4 days. 6
References-found: 38

