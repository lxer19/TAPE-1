URL: http://www.cs.princeton.edu/prism/papers-ps/bilas-sc97.ps
Refering-URL: http://www.cs.princeton.edu/prism/html/all-papers.html
Root-URL: http://www.cs.princeton.edu
Email: fbilas, jpsg@cs.princeton.edu  
Title: The Effects of Communication Parameters on End Performance of Shared Virtual Memory Clusters  
Author: Angelos Bilas and Jaswinder Pal Singh 
Address: 35 Olden Street  Princeton, NJ 08544  
Affiliation: Department of Computer Science  Princeton University  
Abstract: Recently there has been a lot of effort in providing cost-effective Shared Memory systems by employing software only solutions on clusters of high-end workstations coupled with highbandwidth, lowlatency commodity networks. Much of the work so far has focused on improving protocols, and there has been some work on restructuring applications to perform better on SVM systems. The result of this progress has been the promise for good performance on a range of applications at least in the 1632 processor range. New system area networks and network interfaces provide significantly lower overhead, lower latency and higher bandwidth communication in clusters, inexpensive SMPs have become common as the nodes of these clusters, and SVM protocols are now quite mature. With this progress, it is now useful to examine what are the important system bottlenecks that stand in the way of effective parallel performance; in particular, which parameters of the communication architecture are most important to improve further relative to processor speed, which ones are already adequate on modern systems for most applications, and how will this change with technology in the future. Such information can assist system designers in determining where to focus their energies in improving performance, and users in determining what system characteristics are appropriate for their applications. We find that the most important system cost to improve is the overhead of generating and delivering interrupts. Improving network interface (and I/O bus) bandwidth relative to processor speed helps some bandwidth-bound applications, but currently available ratios of bandwidth to processor speed are already adequate for many others. Surprisingly, neither the processor overhead for handling messages 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Bilas, L. Iftode, and J. P. Singh. </author> <title> Comparison of shared virtual memory across uniprocessor and SMP nodes. </title> <booktitle> In IMA Workshop on Parallel Algorithms and Parallel Systems, </booktitle> <month> Nov. </month> <year> 1996. </year>
Reference-contexts: The protocol for SMP nodes attempts to utilize the hardware sharing and synchronization within an SMP as much as possible, reducing software involvement <ref> [1] </ref>. The optimizations used include the use of hierarchical barriers and the avoidance of interrupts as much as possible. Interrupts are used only when remote requests for pages and locks arrive at a node. Requests are synchronous (RPC like), to avoid interrupts when replies arrive at the requesting node. <p> Erlichson et al. [6] find that clustering helps shared memory applications. Yeung et al. in [23] find this to be true for SVM systems in which each node is a hardware coherent DSM machine. In <ref> [1] </ref>, they find that the same is true in general for all software SVM systems, and for SVM systems with support for automatic write propagation. 9 Discussion and Future Work This work shows that there is room for improving SVM cluster performance in various directions: * Interrupts.
Reference: [2] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg. </author> <title> A virtual memory mapped network interface for the shrimp multicomputer. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 142153, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: The techniques for the communication architecture range from using less customized and integrated controllers <ref> [17, 2] </ref> to supporting shared virtual memory (SVM) at page level through the operating system [12, 8]. While these techniques reduce cost, unfortunately they usually lower performance as well. A great deal of research effort has been made to improve these systems for large classes of applications. <p> After transmission, each packet enters an incoming network queue at the receiver, where it is processed by the network interface and then deposited directly in host memory without causing an interrupt <ref> [2, 4] </ref>. Thus, the interrupt cost is an overhead related not so much to data transfer but to processing requests. While we examine a range of values for each parameter, in varying a parameter we usually keep the others fixed at the set of achievable values.
Reference: [3] <author> N. J. Boden, D. Cohen, R. E. Felderman, A. E. Kulawik, C. L. Seitz, J. N. Seizovic, and W.-K. Su. Myrinet: </author> <title> A gigabit-per-second local area network. </title> <journal> IEEE Micro, </journal> <volume> 15(1):2936, </volume> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: In this section we present the architectural parameters that we do not vary. The simulated architecture (Figure 2) assumes a cluster of cprocessor SMPs connected with a commodity interconnect like Myrinet <ref> [3] </ref>. Contention is modeled at all levels except in the network links and switches themselves. The processor has a P6-like instruction set, and is assumed to be a 1 IPC processor.
Reference: [4] <author> C. Dubnicki, A. Bilas, K. Li, and J. Philbin. </author> <title> Design and implementation of virtual memory-mapped communication on myrinet. </title> <booktitle> In Proceedings of the 1997 International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1997. </year>
Reference-contexts: If the network queues fill, the NI interrupts the main processor and delays it to allow queues to drain. Network links operate at processor speed and are 16 bits wide. We assume a fast messaging system <ref> [5, 16, 4] </ref> as the basic communication library. The memory bus is split-transaction, 64 bits wide, with a clock cycle four times slower than the processor clock. <p> After transmission, each packet enters an incoming network queue at the receiver, where it is processed by the network interface and then deposited directly in host memory without causing an interrupt <ref> [2, 4] </ref>. Thus, the interrupt cost is an overhead related not so much to data transfer but to processing requests. While we examine a range of values for each parameter, in varying a parameter we usually keep the others fixed at the set of achievable values.
Reference: [5] <author> T. Eicken, D. Culler, S. Goldstein, and K. Schauser. </author> <title> Active messages: A mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 256266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: If the network queues fill, the NI interrupts the main processor and delays it to allow queues to drain. Network links operate at processor speed and are 16 bits wide. We assume a fast messaging system <ref> [5, 16, 4] </ref> as the basic communication library. The memory bus is split-transaction, 64 bits wide, with a clock cycle four times slower than the processor clock.
Reference: [6] <author> A. Erlichson, B. Nayfeh, J. Singh, and K. Olukotun. </author> <title> The benefits of clustering in shared address space multiprocessors: An applications-driven investigation. </title> <booktitle> In Supercomputing '95, </booktitle> <pages> pages 176186, </pages> <year> 1995. </year>
Reference-contexts: In a very different context, they find that applications are more sensitive to the bandwidth than the latency component of communication. Several studies have also examined the performance of different SVM systems across multiprocessor nodes and compared it with the performance of configurations with uniprocessor nodes. Erlichson et al. <ref> [6] </ref> find that clustering helps shared memory applications. Yeung et al. in [23] find this to be true for SVM systems in which each node is a hardware coherent DSM machine.
Reference: [7] <author> C. Holt, M. Heinrich, J. Singh, E. Rothberg, and J. Hennessy. </author> <title> The effects of latency, occupancy and bandwidth in distributed shared memory multiprocessors. </title> <type> Technical Report CSL-TR-95-660, </type> <institution> Stanford, </institution> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: Best and Achievable Speedups for each application 8 Related Work Our work is similar in spirit to some earlier studies, conducted in <ref> [15, 7] </ref>, but in different context. In [15], the authors examine the impact of communication parameters on end performance of a network of workstations with the applications being written in Split-C on top of Generic Active Messages. <p> For SVM, we find these parameters to not be so important since their cost is usually amortized over page granularity. Applications were found to be quite tolerant to latency and bulk transfer bandwidth in the split-C study as well. In <ref> [7] </ref>, Holt et al. find that the occupancy of the communication controller is critical to good performance in DSM machines that provide communication and coherence at cache line granularity. Overhead is not so significant there (unlike in [15]) since it is very small.
Reference: [8] <author> L. Iftode, C. Dubnicki, E. W. Felten, and K. Li. </author> <title> Improving release-consistent shared virtual memory using automatic update. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: The techniques for the communication architecture range from using less customized and integrated controllers [17, 2] to supporting shared virtual memory (SVM) at page level through the operating system <ref> [12, 8] </ref>. While these techniques reduce cost, unfortunately they usually lower performance as well. A great deal of research effort has been made to improve these systems for large classes of applications. Our focus in this paper is on SVM systems. <p> Our focus in this paper is on SVM systems. In the last few years there has been much improvement of SVM protocols and systems, and several applications have been restructured to improve performance <ref> [12, 8, 25, 14] </ref>. <p> The cost of creating and applying a diff is 10 cycles for every word that needs to be compared and 10 additional cycles for each word actually included in the diff. The protocols we use are two versions of a homebased protocol, HLRC and AURC <ref> [8, 25] </ref>. These protocols either use hardware support for automatic write propagation (AURC) or traditional software diffs (HLRC) to propagate updates to the home node of each page at a release point. The necessary pages are invalidated only at acquire points according to lazy release consistency (LRC). <p> The necessary pages are invalidated only at acquire points according to lazy release consistency (LRC). At a subsequent page fault, the whole page is fetched from the home, where it is guaranteed to be up to date according to the lazy release consistency <ref> [8] </ref>. The protocol for SMP nodes attempts to utilize the hardware sharing and synchronization within an SMP as much as possible, reducing software involvement [1]. The optimizations used include the use of hierarchical barriers and the avoidance of interrupts as much as possible.
Reference: [9] <author> L. Iftode, J. P. Singh, and K. Li. </author> <title> Understanding application performance on shared virtual memory. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: This section briefly describes the basic characteristics of each application relevant to this study. A more detailed classification and description of the application behavior for SVM systems with uniprocessor nodes is provided in the context of AURC and LRC in <ref> [9] </ref>. The applications can be divided in two groups, regular and irregular. 4.1 Regular Applications The applications in this category are FFT, LU and Ocean. <p> In HLRC we do not need to compute diffs, and in AURC we do not need to use a write through cache policy. Protocol action is required only to fetch pages. The applications have different inherent and induced communication patterns <ref> [22, 9] </ref>, which affect their performance and the impact on SMP nodes. 7 Application Page Faults Page Fetches Local Lock Acquires Remote Lock Acquires Barriers 1 4 8 1 4 8 1 4 8 1 4 8 1,4,8 LU (contiguous) (512) 81.36 56.61 48.07 71.78 34.94 11.86 0.02 0.22 0.25 0.27
Reference: [10] <author> D. Jiang, H. Shan, and J. P. Singh. </author> <title> Application restructuring and performance portability on shared virtual memory and hardware-coherent multiprocessors. </title> <booktitle> In Sixth ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: These are the page size, which is the granularity of coherence, and the number of processors per node. Assuming a realistic system that can be quite easily implemented today, and a range of applications that are well optimized for SVM systems <ref> [10] </ref>, we see (Figure 1) that, for most applications, protocol and communication overheads are substantial. The speedups obtained in the realistic implementation are much lower than in the ideal case, where all communication costs are zero. This motivates the current research, whose goal is twofold. <p> In the first version (Barnes-rebuild, which is the one in SPLASH-2) processors load the particles that were assigned to them for force calculation directly into the shared tree, locking (frequently) as necessary. The second version, Barnes-space <ref> [10] </ref>, is optimized for SVM, and it avoids locking as much as possible. It uses a different tree-building algorithm, in which disjoint subspaces that match tree cells are assigned to different processors. <p> Raytrace: Raytrace renders complex scenes in computer graphics. The version we use is modified from the SPLASH-2 version to run more efficiently on SVM systems. A global lock that was not necessary was removed, and task queues are implemented better for SVM and SMP <ref> [10] </ref>. Inherent communication is small. Volrend: The version we use is slightly modified from the SPLASH-2 version, to provide a better initial assignment of tasks to processes before stealing [10]. This improves SVM performance greatly. Inherent communication volume is small. <p> A global lock that was not necessary was removed, and task queues are implemented better for SVM and SMP <ref> [10] </ref>. Inherent communication is small. Volrend: The version we use is slightly modified from the SPLASH-2 version, to provide a better initial assignment of tasks to processes before stealing [10]. This improves SVM performance greatly. Inherent communication volume is small. Water: We use both versions of Water from SPLASH-2, Water-nsquared and Water-spatial. Water-nsquared can be categorized as a regular application, but we put it here to ease the comparison with Water-spatial. <p> The speedup becomes 14.64 in the best and 10.62 in the achievable cases respectively. The gap between the best and the achievable speedups is again due to host and NI overheads. Barnes-space: The second version of Barnes we run is an improved version with minimal locking <ref> [10] </ref>. The best speedup is 14.5, close to the ideal. The achievable speedup is 12.5. The difference between these two is mainly because of the lower available I/O bandwidth in the achievable case. This increases the data wait time in an imbalanced way. <p> In doing this work we found that restructuring applications is an area that can make a big difference. Understanding how an application behaves and restructuring it properly can dramatically improve performance far beyond the improvement in system parameters or protocols <ref> [10] </ref>. This however, is not always easy, and, unfortunately, not many tools are available in parallel systems to help easily discover the cause of bottlenecks and obtain insight about application restructuring needs, especially when contention is a major problem as it often is in commodity-based communication architectures.
Reference: [11] <author> M. Karlsson and P. Stenstrom. </author> <title> Performance evaluation of cluster-based multiprocessor built from atm switches and bus-based multiprocessor servers. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: In [7], Holt et al. find that the occupancy of the communication controller is critical to good performance in DSM machines that provide communication and coherence at cache line granularity. Overhead is not so significant there (unlike in [15]) since it is very small. In <ref> [11] </ref>, Karlsson et al. find that the latency and bandwidth of an ATM switch is acceptable in a clustered SVM architecture. In [13] a Lazy Release Consistency protocol for hardware cache-coherence is presented.
Reference: [12] <author> P. Keleher, A. Cox, S. Dwarkadas, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the Winter USENIX Conference, </booktitle> <pages> pages 115132, </pages> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: The techniques for the communication architecture range from using less customized and integrated controllers [17, 2] to supporting shared virtual memory (SVM) at page level through the operating system <ref> [12, 8] </ref>. While these techniques reduce cost, unfortunately they usually lower performance as well. A great deal of research effort has been made to improve these systems for large classes of applications. Our focus in this paper is on SVM systems. <p> Our focus in this paper is on SVM systems. In the last few years there has been much improvement of SVM protocols and systems, and several applications have been restructured to improve performance <ref> [12, 8, 25, 14] </ref>.
Reference: [13] <author> L. I. Kontothanasis, M. L. Scott, and R. Bianchini. </author> <title> Lazy release consistency for hardware-coherent multiprocessors. </title> <booktitle> In Supercomputing '95, </booktitle> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: Overhead is not so significant there (unlike in [15]) since it is very small. In [11], Karlsson et al. find that the latency and bandwidth of an ATM switch is acceptable in a clustered SVM architecture. In <ref> [13] </ref> a Lazy Release Consistency protocol for hardware cache-coherence is presented. In a very different context, they find that applications are more sensitive to the bandwidth than the latency component of communication.
Reference: [14] <author> L. Kontothanassis, G. Hunt, R. Stets, N. Hardavellas, M. Cierniak, S. Parthasarathy, W. Meira, S. Dwarkadas, and M. Scott. </author> <title> VM-based shared memory on low-latency, remote-memory-access networks. </title> <booktitle> Proc., 24th Annual Int'l. Symp. on Computer Architecture, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Our focus in this paper is on SVM systems. In the last few years there has been much improvement of SVM protocols and systems, and several applications have been restructured to improve performance <ref> [12, 8, 25, 14] </ref>.
Reference: [15] <author> R. P. Martin, A. M. Vahdat, D. E. Culler, and T. E. Anderson. </author> <title> Effect of communication latency, overhead, and bandwidth on a cluster architecture. </title> <type> Technical Report CSD-96-925, </type> <institution> Berkeley, </institution> <month> Nov. </month> <year> 1996. </year>
Reference-contexts: Best and Achievable Speedups for each application 8 Related Work Our work is similar in spirit to some earlier studies, conducted in <ref> [15, 7] </ref>, but in different context. In [15], the authors examine the impact of communication parameters on end performance of a network of workstations with the applications being written in Split-C on top of Generic Active Messages. <p> Best and Achievable Speedups for each application 8 Related Work Our work is similar in spirit to some earlier studies, conducted in [15, 7], but in different context. In <ref> [15] </ref>, the authors examine the impact of communication parameters on end performance of a network of workstations with the applications being written in Split-C on top of Generic Active Messages. <p> In [7], Holt et al. find that the occupancy of the communication controller is critical to good performance in DSM machines that provide communication and coherence at cache line granularity. Overhead is not so significant there (unlike in <ref> [15] </ref>) since it is very small. In [11], Karlsson et al. find that the latency and bandwidth of an ATM switch is acceptable in a clustered SVM architecture. In [13] a Lazy Release Consistency protocol for hardware cache-coherence is presented.
Reference: [16] <author> S. Pakin, M. Buchanan, M. Lauria, and A. Chien. </author> <title> The Fast Messages (FM) 2.0 streaming interface. </title> <note> Submitted to Usenix'97, </note> <year> 1996. </year>
Reference-contexts: If the network queues fill, the NI interrupts the main processor and delays it to allow queues to drain. Network links operate at processor speed and are 16 bits wide. We assume a fast messaging system <ref> [5, 16, 4] </ref> as the basic communication library. The memory bus is split-transaction, 64 bits wide, with a clock cycle four times slower than the processor clock.
Reference: [17] <author> S. Reinhardt, J. Larus, and D. Wood. Tempest and typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 325336, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: The techniques for the communication architecture range from using less customized and integrated controllers <ref> [17, 2] </ref> to supporting shared virtual memory (SVM) at page level through the operating system [12, 8]. While these techniques reduce cost, unfortunately they usually lower performance as well. A great deal of research effort has been made to improve these systems for large classes of applications.
Reference: [18] <author> A. Sharma, A. T. Nguyen, J. Torellas, M. Michael, and J. Carbajal. Augmint: </author> <title> a multiprocessor simulation environment for intel x86 architectures. </title> <type> Technical report, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> March </month> <year> 1996. </year>
Reference-contexts: We discuss related work in Section 8. Finally we discuss future work directions and conclusions in Sections 9 and 10 respectively. 2 Simulation Environment The simulation environment we use is built on top of augmint <ref> [18] </ref>, an execution driven simulator using the x86 instruction set, and runs on x86 systems. In this section we present the architectural parameters that we do not vary. The simulated architecture (Figure 2) assumes a cluster of cprocessor SMPs connected with a commodity interconnect like Myrinet [3].
Reference: [19] <author> K. Skadron and D. W. Clark. </author> <title> Design issues and tradeoffs for write buffers. </title> <booktitle> In The 3nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> Feb </month> <year> 1997. </year>
Reference-contexts: The data cache hierarchy consists of a 8 KBytes first-level direct mapped write-through cache and a 512 KBytes second-level two-way set associative cache, each with a line size of 32 Bytes. The write buffer <ref> [19] </ref> has 26 entries, 1 cache line wide each, and a retire-at-4 policy. Write buffer stalls are 4 simulated. The read hit cost is one cycle if satisfied in the write buffer and first level cache, and 10 cycles if satisfied in the second-level cache.
Reference: [20] <author> R. Stets, S. Dwarkadas, N. Hardavellas, G. Hunt, L. Kontothanassis, S. Parthasarathy, and M. Scott. Cashmere-2L: </author> <title> Software Coherent Shared Memory on a Clustered Remote-Write Network. </title> <booktitle> In Proc. of the 16th ACM Symp. on Operating Systems Principles (SOSP-16), </booktitle> <month> Oct. </month> <year> 1997. </year>
Reference-contexts: Recent results for interrupts versus polling in SVM systems vary. One study finds that polling may add a significant overhead, leading to inferior performance than interrupts for page grain SVM systems [24]. On the other hand, Stets et al. find that polling gives generally better results than interrupts <ref> [20] </ref>. We believe more research is needed on modern systems to understand the role of polling.
Reference: [21] <author> D. Stodolsky, J. B. Chen, and B. Bershad. </author> <title> Fast interrupt priority management in operating system kernels. </title> <booktitle> In USENIX Association, editor, Proceedings of the USENIX Symposium on Microkernels and Other Kernel Architectures: </booktitle> <month> September </month> <year> 2021, </year> <title> 1993, </title> <address> San Diego, California, USA, </address> <pages> pages 105110, </pages> <address> Berkeley, CA, USA, Sept. 1993. </address> <publisher> USENIX. </publisher>
Reference-contexts: The achievable value we use is 500 processor cycles, which results in a cost of 1000 cycles for a null interrupt. This choice is significantly 6 more aggressive than what current operating systems provide. However it is achievable with fast interrupt technology <ref> [21] </ref>. We use it as the achievable value when varying other parameters to ensure that interrupt cost does not swamp out the effects of varying those parameters. To capture the effects of each parameter separately, we keep the other parameters fixed at their achievable values.
Reference: [22] <author> S. Woo, M. Ohara, E. Torrie, J. Singh, and A. Gupta. </author> <title> Methodological considerations and characterization of the SPLASH-2 parallel application suite. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: Parameter Range Achievable Best Host Overhead (cycles) 0-10000 600 ~0 I/O Bus Bandwidth (Mbytes/MHz) 0.25-2 0.5 2 NI Occupancy (cycles) 0-10000 1000 200 Interrupt Cost (cycles) 0-50000 500 ~0 Table 1. Ranges and achievable and best values of the communication parameters under consideration. 4 Applications We use the SPLASH-2 <ref> [22] </ref> application suite. This section briefly describes the basic characteristics of each application relevant to this study. A more detailed classification and description of the application behavior for SVM systems with uniprocessor nodes is provided in the context of AURC and LRC in [9]. <p> In HLRC we do not need to compute diffs, and in AURC we do not need to use a write through cache policy. Protocol action is required only to fetch pages. The applications have different inherent and induced communication patterns <ref> [22, 9] </ref>, which affect their performance and the impact on SMP nodes. 7 Application Page Faults Page Fetches Local Lock Acquires Remote Lock Acquires Barriers 1 4 8 1 4 8 1 4 8 1 4 8 1,4,8 LU (contiguous) (512) 81.36 56.61 48.07 71.78 34.94 11.86 0.02 0.22 0.25 0.27
Reference: [23] <author> D. Yeung, J. Kubiatowicz, and A. Agarwal. MGS: </author> <title> a multigrain shared memory system. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Several studies have also examined the performance of different SVM systems across multiprocessor nodes and compared it with the performance of configurations with uniprocessor nodes. Erlichson et al. [6] find that clustering helps shared memory applications. Yeung et al. in <ref> [23] </ref> find this to be true for SVM systems in which each node is a hardware coherent DSM machine.
Reference: [24] <author> M. D. H. Y. Zhou, I. S. L. Iftode, B. R. T. K. Li, J. P. Singh, and D. A. Wood. </author> <title> Relaxed consistency and coherence granularity in DSM systems: A performance evaluation. </title> <type> Technical Report TR-535-96, </type> <institution> Department of Computer Science, Princeton University, </institution> <month> December </month> <year> 1996, </year> <pages> 10 Pages. </pages>
Reference-contexts: Recent results for interrupts versus polling in SVM systems vary. One study finds that polling may add a significant overhead, leading to inferior performance than interrupts for page grain SVM systems <ref> [24] </ref>. On the other hand, Stets et al. find that polling gives generally better results than interrupts [20]. We believe more research is needed on modern systems to understand the role of polling.

References-found: 24

