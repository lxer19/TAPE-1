URL: ftp://ftp.ai.mit.edu/pub/users/isbell/ppserver.ps.gz
Refering-URL: http://www.ai.mit.edu/people/isbell/charles/vita.html
Root-URL: 
Email: parry@supertech.lcs.mit.edu  isbell@ai.mit.edu  
Phone: 2  
Title: The Parallel Problems Server: A Client-Server Model for Interactive Large Scale Scientific Computation  
Author: Parry Husbands and Charles Isbell 
Address: Cambridge MA 02139 USA  Cambridge MA 02139 USA  
Affiliation: 1 Laboratory for Computer Science, MIT,  Artificial Intelligence Laboratory, MIT,  
Abstract: Applying fast scientific computing algorithms to large problems presents a difficult engineering problem. We describe a novel architecture for addressing this problem that uses a robust client-server model for interactive large-scale linear algebra computation. We discuss competing approaches and demonstrate the relative strengths of our approach. By way of example, we describe MITMatlab, a powerful transparent client interface to the linear algebra server. With MIT-Matlab, it is now straightforward to implement full-blown algorithms intended to work on very large problems while still using the powerful interactive and visualization tools that Matlab provides. We also examine the efficiency of our model by timing selected operations and comparing them to commonly used approaches.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Criz, A. Green-baum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <title> LAPACK Users' Guide. </title> <publisher> Siam Publications, </publisher> <address> Philadelphia, </address> <year> 1995. </year>
Reference-contexts: Finally, we conclude, discussing further extensions to the system. 2 Standard Approaches 2.1 Linear Algebra Libraries For many compute-intensive tasks, the best way to maximize performance is to use a library. For example, optimized versions of LAPACK <ref> [1] </ref> exist that outperform similar code written in a high-level programming language (thanks primarily to native implementations of the BLAS). For distributed memory architectures, vendor-optimized libraries (e.g. Sun's S3L and IBM's ESSL) coexist with public domain offerings such as ScaLAPACK [5], PARPACK [11] and Petsc [4][9].
Reference: 2. <author> P. Arbenz, W. Gander, and M. Oettli. </author> <title> The Remote Computation System. </title> <type> Technical Report 245, </type> <institution> ETH Zurich, </institution> <year> 1996. </year>
Reference-contexts: We are currently working on a port to Pentium-driven Linux systems. 3.3 Other Client-Server Models There have been previous library systems that implement a similar model. Both RCS <ref> [2] </ref> and Netsolve [6] act as fast back-ends for slower clients. In their model, Fig. 2. Extending the PPServer. A client communicates with the PPServer using a simple command-argument protocol. The Server itself uses a "package" mechanism to implement all but its most basic functions.
Reference: 3. <institution> Falcon Group at the University of Illinois at Urbana-Champaign. The Falcon Project. </institution> <note> http://www.csrd.uiuc.edu/falcon/falcon.html. </note>
Reference: 4. <author> S. Balay, W. D. Gropp, L. C. McInnes, and B. F. Smith. </author> <title> Efficient Management of Parallelism in Object-Oriented Numerical Software Libraries. </title> <publisher> Birkhauser Press, </publisher> <year> 1997. </year>
Reference: 5. <author> L. S. Blackford, J. Choi, A. Cleary, E. D'Azevedo, J. Demmel, I. Dhilon, J. Dongarra, S. Hammarling, G. Henry, A. Petitet, K. Stanley, D. Walker, </author> <title> and R.C. Whaley. Scalapack Users' Guide. </title> <address> http://www.netlib.org/scalapack/slug/scalapack slug.html, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: For example, optimized versions of LAPACK [1] exist that outperform similar code written in a high-level programming language (thanks primarily to native implementations of the BLAS). For distributed memory architectures, vendor-optimized libraries (e.g. Sun's S3L and IBM's ESSL) coexist with public domain offerings such as ScaLAPACK <ref> [5] </ref>, PARPACK [11] and Petsc [4][9]. Each of these libraries has its own idiosyncratic interface and assumptions about the types and distributions of data allowed.
Reference: 6. <author> Henri Casanova and Jack Dongarra. Netsolve: </author> <title> A Network Server for Solving Computational Science Problems. </title> <booktitle> In Proceedings of SuperComputing 1996, </booktitle> <year> 1996. </year>
Reference-contexts: We are currently working on a port to Pentium-driven Linux systems. 3.3 Other Client-Server Models There have been previous library systems that implement a similar model. Both RCS [2] and Netsolve <ref> [6] </ref> act as fast back-ends for slower clients. In their model, Fig. 2. Extending the PPServer. A client communicates with the PPServer using a simple command-argument protocol. The Server itself uses a "package" mechanism to implement all but its most basic functions.
Reference: 7. <author> Peter Drakenberg, Peter Jacobson, and Bo Kagstrom. </author> <title> A CONLAB Compiler for a Distributed Memory Multicomputer. </title> <booktitle> In Proceedings of the Sixth SIAM Conference on Parallel Processing from Scientific Computing, </booktitle> <volume> volume 2, </volume> <pages> pages 814-821. </pages> <institution> Society for Industrial and Applied Mathematics, </institution> <year> 1993. </year>
Reference-contexts: Matlab is extended to include send, receive and collective operations so that separate Matlab processes can communicate. In short, these approaches implement traditional message passing with Matlab as the implementation language. Compilers for Matlab are also an active area. Both the CONLAB system from the University of Ume-a <ref> [7] </ref> and the FALCON environment from the University of Illinois at Urbana-Champaign [3][12] translate Matlab-like languages into intermediate languages for which high performance compilers exist. For example, FALCON compiles Matlab to Fortran 90 and pC++. Sophisticated analyses of the Matlab source are performed so that efficient target code is generated.
Reference: 8. <author> William Gropp, Ewing Lusk, and Anthong Skjellum. </author> <title> Using MPI: Portable Parallel Programming with the Message-Passing Interface. </title> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Data are created in a distributed fashion and managed among worker processes, which may live on different ma-chines. Currently we support row and column distributed dense arrays, column distributed sparse arrays, and replicated arrays in single precision. Communication and synchronization among the workers is accomplished using the MPI <ref> [8] </ref> message passing library. This is a standard library available on a wide range of platforms; it is currently the most portable way to develop applications on distributed memory computers. Fig. 1. The General Organization of the Parallel Problems Server.
Reference: 9. <author> PETSc Group. </author> <title> PETSc the Portable, Extensible Toolkit for Scientific Computation. </title> <address> http://www.mcs.anl.gov/home/gropp/petsc.html. </address>
Reference: 10. <author> J. Hollingsworth, K. Liu, and P. Pauca. </author> <title> Parallel Toolbox for MATLAB PT v. 1.00: Manual and Reference Pages. </title> <institution> Wake Forest University, </institution> <year> 1996. </year>
Reference-contexts: Here, we focus on systems that add parallel features to Matlab, a widely-used scientific computing tool. Both MultiMatlab from Cornell University [13] and the Parallel Toolbox for Matlab from Wake Forest University <ref> [10] </ref>, make it possible to manage Matlab processes on different machines. Matlab is extended to include send, receive and collective operations so that separate Matlab processes can communicate. In short, these approaches implement traditional message passing with Matlab as the implementation language. Compilers for Matlab are also an active area.
Reference: 11. <author> K. J. Maschhoff and D. C. Sorensen. </author> <title> A Portable Implementation of ARPACK for Distributed Memory Parallel Computers. </title> <booktitle> In Preliminary Proceedings of the Copper Mountain Conference on Iterative Methods, </booktitle> <year> 1996. </year>
Reference-contexts: For example, optimized versions of LAPACK [1] exist that outperform similar code written in a high-level programming language (thanks primarily to native implementations of the BLAS). For distributed memory architectures, vendor-optimized libraries (e.g. Sun's S3L and IBM's ESSL) coexist with public domain offerings such as ScaLAPACK [5], PARPACK <ref> [11] </ref> and Petsc [4][9]. Each of these libraries has its own idiosyncratic interface and assumptions about the types and distributions of data allowed.
Reference: 12. <author> L. De Rose, K. Gallivan, E. Gallopoulos, B. Marsolf, and D. Padua. </author> <title> Falcon: An Environment for the Development of Scientific Libraries and Applications. </title> <booktitle> In Proceedings of KBUP'95 First International Workshop on Knowledge-Based Systems for the (re)Use of Program Libraries, </booktitle> <month> November </month> <year> 1995. </year>
Reference: 13. <author> Anne E. Trefethen, Vijay S. Menon, Chi-Chao Chang, Gregorz J. Czajkowski, Chris Myers, and Lloyd N. Trefethen. </author> <title> MultiMATLAB: MATLAB on Multiple Processors. </title> <note> http://www.cs.cornell.edu/Info/People/lnt/multimatlab.html, 1996. </note>
Reference-contexts: There have been many attempts to extend prototyping tools in order to make them work in parallel with large data sets. Here, we focus on systems that add parallel features to Matlab, a widely-used scientific computing tool. Both MultiMatlab from Cornell University <ref> [13] </ref> and the Parallel Toolbox for Matlab from Wake Forest University [10], make it possible to manage Matlab processes on different machines. Matlab is extended to include send, receive and collective operations so that separate Matlab processes can communicate.
References-found: 13

