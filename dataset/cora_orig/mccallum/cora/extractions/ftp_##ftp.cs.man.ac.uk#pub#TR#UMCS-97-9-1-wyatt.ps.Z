URL: ftp://ftp.cs.man.ac.uk/pub/TR/UMCS-97-9-1-wyatt.ps.Z
Refering-URL: ftp://ftp.cs.man.ac.uk/pub/TR/UMCS-97-9-1.html
Root-URL: http://www.cs.man.ac.uk
Email: J.L.Wyatt@cs.bham.ac.uk  johnho@dai.ed.ac.uk  gmh@dai.ed.ac.uk  
Title: Experimental methods for robot learning  
Author: Jeremy Wyatt, John Hoar Gillian Hayes 
Address: U.K.  Edinburgh, U.K.  Edinburgh, U.K.  
Affiliation: School of Computer Science, University of Birmingham,  Department of Artificial Intelligence, University of  Department of Artificial Intelligence, University of  
Abstract: In this paper we outline some ideas as to how robot learning experiments might best be designed, based on practical experience. The principal finding arises from the fact that an observed robot behaviour will typically have several possible causes. In order to isolate the source of an error or result it is therefore necessary to employ several evaluation methods together. In particular we argue for a division of the evaluation of robot learning along several lines. First, both external and internal measures of a robot's performance are required. Second, both quantitative and qualitative methods of description should ideally be employed. Four methods for agent analysis are then presented. These are drawn from a variety of fields: psychology, ethology, machine learning, statistics and engineering. We describe the application of these techniques to the construction and analysis of a robot that learns to push boxes from reinforcement. The paper concludes with general remarks about the efficacy of each of these techniques. We also emphasize the importance of careful experimental design in order to integrate the various forms of evaluation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Yassine Faihe and Jean-Pierre Muller. </author> <title> Analysis and design of a robot's behaviour: Towards a methodology. </title> <editor> In John Demiris Andreas Birk, editor, </editor> <booktitle> Proceedings of Sixth European Workshop on Learning Robots, </booktitle> <year> 1997. </year>
Reference-contexts: Suprisingly, until recently little work has been done on the problems of evaluation in intelligent robotics <ref> [2, 1] </ref>. Indeed some authors in the learning literature have eschewed the use of real robots because of the difficulties in quantifying robot performance [4]. In this paper we argue that the correct approach for experiments in robot learning is to employ multiple forms of evaluation.
Reference: [2] <author> Henry Hexmoor. </author> <title> Robolearn 97: An international workshop on evaluating robot learning. </title> <type> Technical report TR 97-03, </type> <institution> Department of Computer Science, State University of New York at Buffalo, </institution> <month> April </month> <year> 1997. </year>
Reference-contexts: Suprisingly, until recently little work has been done on the problems of evaluation in intelligent robotics <ref> [2, 1] </ref>. Indeed some authors in the learning literature have eschewed the use of real robots because of the difficulties in quantifying robot performance [4]. In this paper we argue that the correct approach for experiments in robot learning is to employ multiple forms of evaluation.
Reference: [3] <author> J. Hoar. </author> <title> Reinforcement learning applied to a real robot task. </title> <type> Unpublished masters thesis, </type> <institution> University of Edinburgh, Department of Artificial Intelligence, </institution> <month> September </month> <year> 1996. </year>
Reference-contexts: Mahadevan and Connell's hypothesis was that use of a behavioural decomposition simplified the learning task. The agent learned from reinforcement functions specified for each sub-task. They used Q-learning, in combination with two generalisation techniques. We reimplemented their work <ref> [3] </ref>, using the statistical clustering method described in [6]; as well as the parameter values suggested. <p> For the results obtained see <ref> [3] </ref>. 4 We carried out no such experiments here, though a good example is given in [5]. seep into the design of the robot learner in various ways, e.g. in the choice of sensor modalities; the design of the sensory pre-processing; any decisions about the amount of state that should be
Reference: [4] <author> Leslie Pack Kaelbling. </author> <title> Learning in Embedded Systems. </title> <type> PhD thesis, </type> <institution> Dept of Computer Science, Stanford, </institution> <year> 1990. </year>
Reference-contexts: Suprisingly, until recently little work has been done on the problems of evaluation in intelligent robotics [2, 1]. Indeed some authors in the learning literature have eschewed the use of real robots because of the difficulties in quantifying robot performance <ref> [4] </ref>. In this paper we argue that the correct approach for experiments in robot learning is to employ multiple forms of evaluation. This is the only way to disambiguate the source of an error or behaviour when there are many separate causes of any given behaviour.
Reference: [5] <author> H.H. Lund and O. Miglino. </author> <title> From simulated to real robots. </title> <booktitle> In Proceedings of IEEE 3rd International Conference on Evolutionary Computation, </booktitle> <publisher> IEEE Press, </publisher> <year> 1996. </year>
Reference-contexts: For the results obtained see [3]. 4 We carried out no such experiments here, though a good example is given in <ref> [5] </ref>. seep into the design of the robot learner in various ways, e.g. in the choice of sensor modalities; the design of the sensory pre-processing; any decisions about the amount of state that should be available to the learning controller; or the design of the internal performance measure.
Reference: [6] <author> S. Mahadevan and J.H. Connell. </author> <title> Automatic programming of behaviour-based robots using reinforcement learning. </title> <type> Research Report RC 16359 (72625), </type> <institution> IBM Research Division, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: These are formally analysed in Section 4. The task was box pushing. The agent is placed within an arena (2m 2 ) with white walls (height 20cm). The box pushing task is a reimplementation of an experimental setup used by Mahadevan and Connell <ref> [6] </ref>. It is worth noting that reimplementation in robotics has a particular purpose. Rather than than reproducing the same results using the same hardware, roboticists try to show how a technique generalises across slightly different environments and sensory modalities. <p> Mahadevan and Connell's hypothesis was that use of a behavioural decomposition simplified the learning task. The agent learned from reinforcement functions specified for each sub-task. They used Q-learning, in combination with two generalisation techniques. We reimplemented their work [3], using the statistical clustering method described in <ref> [6] </ref>; as well as the parameter values suggested. The only differences were that the learning rate for the Q-learner was initially set to be :95, declining according to an exponential law, until it reached a minimum value of :2; and that one-step rather than five-step Q-learning was employed. <p> The applicability functions and the arbitration network were retained as reported in <ref> [6] </ref>. The reinforcement functions initially employed were identical to those used in [6]. One of these, the reinforcement function for the box finding behaviour was altered, however, because the existing reinforcement function was found to be inadequate. <p> The applicability functions and the arbitration network were retained as reported in <ref> [6] </ref>. The reinforcement functions initially employed were identical to those used in [6]. One of these, the reinforcement function for the box finding behaviour was altered, however, because the existing reinforcement function was found to be inadequate. <p> The experimental setup used was slightly different from that reported in <ref> [6] </ref>. They used a robot with sonar as its primary sense; an infra-red sensor tuned to 4" as a bump sensor; and a motor current sensor to detect if the robot was stuck. In all, these sensors generated 2 18 distinguishable states. <p> Secondly, the Hall effect sensor provides slightly unreliable and delayed information about whether the robot is in motion. These changes in sensor modality can be shown to have a significant effect on the ability of the robot to learn, and on the reliability of its reward function. As with <ref> [6] </ref> we carried out the learning computations using an off-board computer. We employed a physical tether, incorporating both a power feed and a data-line (see Figure 3 (right)). <p> Finally in our setup we randomly placed both boxes and Asterix in the environment. Each 100 steps in a learning run we redistributed the boxes randomly. This contrasts with the approach taken in <ref> [6] </ref> where boxes and robot were given a fixed position, and the only random element was the orientation of the robot. <p> By comparing the robot's internal state to this behaviour, we found that the box pushing module became active and was rewarded when the robot bumped against a wall. This is because the Hall effect sensor | unlike the motor current sensor used in <ref> [6] </ref> | has a delay in reporting the cessation of motion. In addition it is unreliable. Because of this the agent is rewarded for pushing walls for a short period prior to the unwedging behaviour being activated.
References-found: 6

