URL: ftp://publications.ai.mit.edu/ai-publications/1500-1999/AIM-1633.ps
Refering-URL: http://www-swiss.ai.mit.edu/~yip/pubs.html
Root-URL: 
Title: Sparse Representations for Fast, One-Shot Learning  
Author: Kenneth Yip Gerald Jay Sussman 
Keyword: morphophonology, sparse representation, fast learning, rule induction, language learning  
Note: Contact  
Date: November 26, 1997  
Address: Cambridge, MA 02139  yip@ai.mit.edu, 617-253-8581, NE43-433, 545 Technology Square, MIT, Cam-bridge, MA 02139.  
Affiliation: Artificial Intelligence Laboratory Massachusetts Institute of Technology  Information:  
Abstract: Humans rapidly and reliably learn many kinds of regularities and generalizations. We propose a novel model of fast learning that exploits the properties of sparse representations and the constraints imposed by a plausible hardware mechanism. To demonstrate our approach we describe a computational model of acquisition in the domain of morphophonology. We encapsulate phonological information as bidirectional boolean constraint relations operating on the classical linguistic representations of speech sounds in term of distinctive features. The performance model is described as a hardware mechanism that incrementally enforces the constraints. Phonological behavior arises from the action of this mechanism. Constraints are induced from a corpus of common English nouns and verbs. The induction algorithm compiles the corpus into increasingly sophisticated constraints. The algorithm yields one-shot learning from a few examples. Our model has been implemented as a computer program. The program exhibits phonological behavior similar to that of young children. As a bonus the constraints that are acquired can be interpreted as classical linguistic rules. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Adrian Akmajian, Richard Demers, and Robert Harnish. </author> <title> Linguistics: An Intro duction to Language and Communication. </title> <publisher> MIT Press, 3rd edition, </publisher> <year> 1990. </year>
Reference-contexts: However, this time the generalizations are consistent: there are not any new exceptions or near misses. Note that after seeing only 4 positive examples, the learner is able to acquire constraints on the plural formation that closely resemble those found in linguistics texts <ref> [1] </ref>. These rule-classifiers are now available for constraint propagation, and are subject to further refinement when new examples appear. 12 Winston [17] emphasized the usefulness of near misses in his ARCH learning program. In our program, the near misses are not supplied by a teacher or given in the input. <p> There are, however, two differences. First, the standard English pluralization rules are typically ordered (see, for example, <ref> [1] </ref>): a. If the noun ends in a phoneme containing the features [+strident, +coronal] (i.e., one of the sounds [s], [z], [sh], [zh], [ch], [j]), the plural affix is [I z]. Otherwise, b. If the noun ends in a [+voice] phoneme, the affix is [z]. c. <p> The two new higher-order rule-classifier enforce the constraint that the voicing bits of the ending phoneme of the stem and the affix must match: [dc.dc.[-voice].[-voice]] [dc.dc.[+voice].[+voice]] These rule-classifiers can be interpreted as the voicing assimilation rule described in linguistics texts (such as <ref> [1] </ref>). Voicing assimilation captures cross-categorical generalizations governing the formation of not only plural nouns and past-tense verbs, but also third-person singular verbs, possessive nouns, and several other morphological categories. Linguists explain complicated phonological processes in terms of the interactions of nearly independent and widely applicable rules.
Reference: [2] <author> J. Berko. </author> <title> The child's learning of English morphology. Word, </title> <type> 14, </type> <year> 1958. </year> <month> 28 </month>
Reference-contexts: Thus learning language must be easy, but we do not have effective theories that explain the phenomenon. The mystery deepens when we notice that children learn many new words without ever hearing them. In a classic experiment by Berko <ref> [2] </ref>, a number of English-speaking children were shown representations of a fanciful being called a "wug." When asked to say something about a situation with more than one of these beings, the children correctly pluralized the novel word to make "wugz" (not "wugs"). <p> Rule-classifier 4 covers nouns ending in the strident [f] or the non-coronal 17 stops [k] and [p]. Similarly, the voiced-plural rule b is split into rule-classifiers 1 and 2. The learner also exhibits intermediate behaviors similar to those of young children <ref> [2] </ref>. After rule-classifier 1 and rule-classifier 3 are acquired, the performance program produces plurals like *foot [s] and *man [z]. Upon presentation of the nonce word "wug," it gives wug [z]. For nonce words ending in a strident like "tass" or "gutch," it gives the unaltered singular forms as plurals.
Reference: [3] <author> Noam Chomsky and Morris Halle. </author> <title> The Sound Pattern of English. </title> <publisher> Harper and Row, </publisher> <year> 1968. </year>
Reference-contexts: We do not attack the problem of how an acoustic waveform is processed. We start with an abstraction from linguistics (as developed by Roman Jakobson, Nikolai Tru-betzkoy, Morris Halle, and Noam Chomsky) <ref> [3] </ref>: Speech sounds (phonemes) are not atomic but are encoded as combinations of more primitive structures, the distinctive features. <p> We understand that current techniques cannot reliably extract such information from noisy continuous speech. Our only concern here is that the resulting phonemes are represented in terms of some set of distinctive features similar to the SPE set (developed in The Sound Pattern of English <ref> [3] </ref>). 5 Shift Register, each vertical stripe represents a time slot. There are slots for future phonemes (positive time labels) as well as past phonemes (negative time labels). Each horizontal stripe represents a distinctive feature bit.
Reference: [4] <author> Harold Clahsen, Monika Rothweiler, and Andreas Woest. </author> <title> Regular and irregular inflection of German noun plurals. </title> <journal> Cognition, </journal> <volume> 45(3), </volume> <year> 1992. </year>
Reference-contexts: representation is extremely sparse: English uses only 40 or so phonemes out of the thousands possible feature combinations, and no human language uses many 2 Clahsen et. al. showed that the German -s plural acts like a regular plural even though it applies to a tiny fraction of the nouns <ref> [4] </ref>. 3 For example, the voicing feature refers to the state of the vocal cords. If a phoneme (e.g., [z]) is pronounced with vibration of the vocal cords, the phoneme is said to be [+voice]. On the contrary, an unvoiced phoneme (e.g., [s]) is said to be [voice]. <p> But there is evidence that the statistical property may not be essential to the acquisition of regular rules. For example, Marcus et. al. [9] and Clahsen <ref> [4] </ref> showed that the German -s plural behaves like a regular rule despite the fact that the rule applies to fewer than 30 common nouns. This observation raises the question of how a child can acquire regular rules from very few examples.
Reference: [5] <author> Michael Kenstowicz. </author> <title> Phonology in Generative Grammar. </title> <publisher> Blackwell Publishers, </publisher> <year> 1994. </year>
Reference-contexts: The constraint elements implement boolean relations among the values of the 4 Modern phonology postulates more elaborate representation devices such as multiple tiers and metrical grids. See <ref> [5] </ref>. These devices describe phonological phenomena that we do not address. 5 For example, the phonemes may be extracted from the acoustic waveform using statistical techniques on other features, such as cepstrum coefficients [15]. We understand that current techniques cannot reliably extract such information from noisy continuous speech.
Reference: [6] <author> Charles Ling and Marin Marinov. </author> <title> Answering the connectionist challenge: A symbolic model of learning the past tenses of English verbs. </title> <journal> Cognition, </journal> <volume> 49(3), </volume> <year> 1993. </year>
Reference-contexts: supporters of symbolic AI and connectionist AI. 1 Second, learning phonological regularities is an example of a class of induction problems which presents special challenges to a learner who must form valid generalizations on the basis of a few positive examples and no explicit corrections for wrong behavior. 1 See <ref> [16, 13, 12, 14, 6] </ref>. Sparseness of the representation is partially a consequence of the fact that phonemes are not atomic but are encoded as combinations of elementary distinctive features. These features can be thought of as abstract muscular controls of the speech organs. <p> The output of the performance model and learner is a set of bit vectors that typically have a straightforward symbolic interpretation. To test the performance of the program on past-tense inflection, we use the same dataset that MacWhinney [8], Ling <ref> [6] </ref>, and Mooney and Califf [11] used. The dataset contains approximately 1400 stem/past-tense pairs. We randomly choose 500 verb pairs as the test set. The test set is disjoint from the training set. The program is trained on progressively larger samples from the training set. <p> Although the task is simplistic, it does allow quantitative comparisons with previous work. Figure 8 presents the learning curves from four programs: K&G (our program), FOIDL (a program based on inductive logic programming) [11], SPA (a program based on decision trees) <ref> [6] </ref>, and M&L (a program based on artificial neural network) [8]. The graph shows that K&G gives the best accuracy result. FOIDL is the next best performer. K&G has the steepest learning curve. It reaches 90+% performance with 20 examples. deviation of K&G's learning curve is also plotted.
Reference: [7] <author> Brian MacWhinney. </author> <title> Connections and symbols: closing the gap. </title> <journal> Cognition, </journal> <volume> 49, </volume> <year> 1993. </year>
Reference-contexts: It is incremental, greedy, and fast. It has almost no parameters to adjust. Our theory makes falsifiable claims about the learning of phonological constraints: (1) that learning requires very few examples|tens of examples in a few steps as opposed to thousands of examples trained in thousands of epochs <ref> [7] </ref>, (2) that the same target constraints are learned independent of the presentation order of the corpus, (3) that effective learning is nearly insensitive to the token frequency, 2 and (4) that learning is more effective as more constraints are acquired.
Reference: [8] <author> Brian MacWhinney and Jared Leinbach. </author> <title> Implementations are not conceptual izations: Revising the verb learning model. </title> <journal> Cognition, </journal> <volume> 40, </volume> <year> 1991. </year>
Reference-contexts: Our theory of acquisition differs significantly from those based on statistics (such as <ref> [16, 8] </ref>). It is incremental, greedy, and fast. It has almost no parameters to adjust. <p> The output of the performance model and learner is a set of bit vectors that typically have a straightforward symbolic interpretation. To test the performance of the program on past-tense inflection, we use the same dataset that MacWhinney <ref> [8] </ref>, Ling [6], and Mooney and Califf [11] used. The dataset contains approximately 1400 stem/past-tense pairs. We randomly choose 500 verb pairs as the test set. The test set is disjoint from the training set. The program is trained on progressively larger samples from the training set. <p> Figure 8 presents the learning curves from four programs: K&G (our program), FOIDL (a program based on inductive logic programming) [11], SPA (a program based on decision trees) [6], and M&L (a program based on artificial neural network) <ref> [8] </ref>. The graph shows that K&G gives the best accuracy result. FOIDL is the next best performer. K&G has the steepest learning curve. It reaches 90+% performance with 20 examples. deviation of K&G's learning curve is also plotted.
Reference: [9] <author> Gary Marcus, Steven Pinker, Michael Ullman, Michelle Hollander, T. John Rosen, and Fei Xu. </author> <title> Overregularization in Language Acquisition, </title> <booktitle> volume 57. Monographs of the Society for research in child development, </booktitle> <year> 1992. </year>
Reference-contexts: In another experiment <ref> [9] </ref>, Marcus et. al. showed that young children who first use an irregular verb properly (such as "came") would later err on the same verb (by supplementing "came" with "comed") before they use the verb correctly again. <p> We select 250 words (50 common nouns and 200 verbs) that first-graders might know. The corpus includes most of the regular and irregular verbs used in the psycholinguistic experiments of Marcus et. al. <ref> [9] </ref> on English tenses. <p> But there is evidence that the statistical property may not be essential to the acquisition of regular rules. For example, Marcus et. al. <ref> [9] </ref> and Clahsen [4] showed that the German -s plural behaves like a regular rule despite the fact that the rule applies to fewer than 30 common nouns. This observation raises the question of how a child can acquire regular rules from very few examples.
Reference: [10] <author> Tom Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence Journal, </journal> <volume> 18, </volume> <year> 1982. </year>
Reference-contexts: Here the positive and negative labels are arbitrary distinctions. 9 Our generalization algorithm differs from the version space algorithm <ref> [10] </ref> in two respects. First, our algorithm does not maintain all the most general and most specific generalizations consistent with the current set of examples. Second, our algorithm handles disjunctive generalizations and noise. 12 (shown in an abbreviated form in the left column).
Reference: [11] <author> Raymond Mooney and Mary Elaine Califf. </author> <title> Induction of first-order decision lists: results on learning the past tense of English verbs. </title> <journal> JAIR, </journal> <volume> 3, </volume> <year> 1995. </year>
Reference-contexts: The output of the performance model and learner is a set of bit vectors that typically have a straightforward symbolic interpretation. To test the performance of the program on past-tense inflection, we use the same dataset that MacWhinney [8], Ling [6], and Mooney and Califf <ref> [11] </ref> used. The dataset contains approximately 1400 stem/past-tense pairs. We randomly choose 500 verb pairs as the test set. The test set is disjoint from the training set. The program is trained on progressively larger samples from the training set. <p> Although the task is simplistic, it does allow quantitative comparisons with previous work. Figure 8 presents the learning curves from four programs: K&G (our program), FOIDL (a program based on inductive logic programming) <ref> [11] </ref>, SPA (a program based on decision trees) [6], and M&L (a program based on artificial neural network) [8]. The graph shows that K&G gives the best accuracy result. FOIDL is the next best performer. K&G has the steepest learning curve.
Reference: [12] <author> Steven Pinker. </author> <title> Rules of language. </title> <journal> Science, </journal> <volume> 253, </volume> <year> 1991. </year>
Reference-contexts: supporters of symbolic AI and connectionist AI. 1 Second, learning phonological regularities is an example of a class of induction problems which presents special challenges to a learner who must form valid generalizations on the basis of a few positive examples and no explicit corrections for wrong behavior. 1 See <ref> [16, 13, 12, 14, 6] </ref>. Sparseness of the representation is partially a consequence of the fact that phonemes are not atomic but are encoded as combinations of elementary distinctive features. These features can be thought of as abstract muscular controls of the speech organs.
Reference: [13] <author> Steven Pinker and Alan Prince. </author> <title> On language and connectionism: Analysis of a parallel distributed processing model of language acquisition. </title> <journal> Cognition, </journal> <volume> 28, </volume> <year> 1988. </year>
Reference-contexts: supporters of symbolic AI and connectionist AI. 1 Second, learning phonological regularities is an example of a class of induction problems which presents special challenges to a learner who must form valid generalizations on the basis of a few positive examples and no explicit corrections for wrong behavior. 1 See <ref> [16, 13, 12, 14, 6] </ref>. Sparseness of the representation is partially a consequence of the fact that phonemes are not atomic but are encoded as combinations of elementary distinctive features. These features can be thought of as abstract muscular controls of the speech organs. <p> Of course, we cannot at this stage rule out Berko's interpretation. It might be the case that the plural formation involves mechanisms more complicated than the addition of the [s] or [z] or [I.z] ending. For instance, Pinker and Prince <ref> [13] </ref> suggests that, instead of the three rules for adding plural endings, one might have a combination of different morphological and phonological rules that produce the same pronunciation. In their account, there is a morphological rule that adds [z] to a stem.
Reference: [14] <author> Sandeep Prasada and Steven Pinker. </author> <title> Generalization of regular and irregular morphological patterns. </title> <journal> Cognition, </journal> <volume> 45(3), </volume> <year> 1992. </year>
Reference-contexts: supporters of symbolic AI and connectionist AI. 1 Second, learning phonological regularities is an example of a class of induction problems which presents special challenges to a learner who must form valid generalizations on the basis of a few positive examples and no explicit corrections for wrong behavior. 1 See <ref> [16, 13, 12, 14, 6] </ref>. Sparseness of the representation is partially a consequence of the fact that phonemes are not atomic but are encoded as combinations of elementary distinctive features. These features can be thought of as abstract muscular controls of the speech organs.
Reference: [15] <author> Lawrence Rabiner and Biing-Hwang Juang. </author> <title> Fundamentals of Speech Recognition. </title> <publisher> Prentice Hall, </publisher> <year> 1993. </year>
Reference-contexts: See [5]. These devices describe phonological phenomena that we do not address. 5 For example, the phonemes may be extracted from the acoustic waveform using statistical techniques on other features, such as cepstrum coefficients <ref> [15] </ref>. We understand that current techniques cannot reliably extract such information from noisy continuous speech.
Reference: [16] <author> D Rumelhart and J.L. McClelland. </author> <title> On learning the past tenses of English verbs. In Parallel Distributed Processing: Exploration in the microstructure of cognition. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: supporters of symbolic AI and connectionist AI. 1 Second, learning phonological regularities is an example of a class of induction problems which presents special challenges to a learner who must form valid generalizations on the basis of a few positive examples and no explicit corrections for wrong behavior. 1 See <ref> [16, 13, 12, 14, 6] </ref>. Sparseness of the representation is partially a consequence of the fact that phonemes are not atomic but are encoded as combinations of elementary distinctive features. These features can be thought of as abstract muscular controls of the speech organs. <p> Our theory of acquisition differs significantly from those based on statistics (such as <ref> [16, 8] </ref>). It is incremental, greedy, and fast. It has almost no parameters to adjust.
Reference: [17] <author> Patrick Winston. </author> <title> Learning structural descriptions from examples. </title> <booktitle> In The Psy chology of Computer Vision. </booktitle> <publisher> McGraw-Hill, </publisher> <year> 1975. </year> <month> 29 </month>
Reference-contexts: Note that after seeing only 4 positive examples, the learner is able to acquire constraints on the plural formation that closely resemble those found in linguistics texts [1]. These rule-classifiers are now available for constraint propagation, and are subject to further refinement when new examples appear. 12 Winston <ref> [17] </ref> emphasized the usefulness of near misses in his ARCH learning program. In our program, the near misses are not supplied by a teacher or given in the input. They are generated internally. 13 The strident feature refers to noisy fricatives and affricates.
References-found: 17

