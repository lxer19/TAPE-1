URL: http://www.cs.cmu.edu/~quake-papers/spark98.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/project/quake/public/www/papers.html
Root-URL: 
Title: Spark98: Sparse Matrix Kernels for Shared Memory and Message Passing Systems  
Author: David R. O'Hallaron 
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Date: October 8, 1997  
Pubnum: CMU-CS-97-178  
Abstract: Spark98 is a collection of sparse matrix kernels for shared memory and message passing systems. Our aim is to provide system builders with a set of example sparse matrix codes that are simple, realistic, and portable. Each kernel performs a sequence of sparse matrix vector product operations using matrices that are derived from a family of three-dimensional finite element earthquake applications. We describe the computational structure of the kernels, summarize their performance on a parallel system, and discuss some of the insights that such kernels can provide. In particular we notice that efficient parallel programming of sparse codes requires careful partitioning of data references, regardless of the underlying memory system. So on one hand, efficient shared memory programs can be just as difficult to write as efficient message passing programs. On the other hand, shared memory programs are not necessarily less efficient than message passing programs. Effort sponsored in part by the Advanced Research Projects Agency and Rome Laboratory, Air Force Materiel Command, USAF, under agreement number F30602-96-1-0287, in part by the National Science Foundation under Grant CMS-9318163, and in part by a grant from the Intel Corporation. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Advanced Research Projects Agency, Rome Laboratory, or the U.S. Government. Author's email address: droh@cs.cmu.edu 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> AMZA, C., COX, A., DWARKADAS, S., HYAMS, C., LI, Z., AND ZWAENEPOEL, W. Treadmarks: </author> <title> Shared memory computing on networks of workstations. </title> <booktitle> IEEE Computer 29, </booktitle> <month> 2 (Feb </month> <year> 1996), </year> <month> 1828. </month>
Reference-contexts: Towards that end, we introduce the Spark98 kernels, a set of 10 SMVP kernels for shared memory and message passing systems. 1 We expect that builders of cache coherent shared memory multiprocessors [15, 6], distributed memory systems [22], message passing libraries [18], and distributed shared memory libraries <ref> [1, 25] </ref> will find that the Spark98 kernels are useful tools for understanding the performance of irregular codes on their systems. In our own experience with the Spark98 kernels we notice that efficient parallel programming of sparse codes requires careful partitioning of data references, regardless of the underlying memory system.
Reference: [2] <author> ATLAS, S., BANERJEE, S., CUMMINGS, J., HINKER, P., SRIKANT, M., REYNDERS, J., AND THOLBURN, M. POOMA: </author> <title> A high performance distributed simulation environment for scientific applications. </title> <booktitle> In Proceedings of Supercomputing '95 (Washington, </booktitle> <address> D.C., </address> <month> Nov. </month> <year> 1995). </year>
Reference-contexts: Existing sparse matrix collections like the Harwell-Boeing suite [8] are important tools for designers of numerical algorithms and partitioning algorithms. And existing software libraries that support sparse matrix computations are useful tools for application programmers <ref> [2, 7, 11] </ref>. However, to date the designers of parallel systems have not had access to a set of public domain sparse matrix kernels that are simple, realistic, and portable.
Reference: [3] <author> BAO, H., BIELAK, J., GHATTAS, O., O'HALLARON, D., KALLIVOKAS, L., SHEWCHUK, J., AND XU, J. </author> <title> Earthquake ground motion modeling on parallel computers. </title> <booktitle> In Proceedings of Supercomputing '96 (Pittsburgh, </booktitle> <address> PA, </address> <month> Nov. </month> <year> 1996). </year> <note> See also www.cs.cmu.edu/quake/. </note>
Reference-contexts: For example, the Quake project at Carnegie Mellon University uses a sequence of more than 16,000 sparse matrix-vector product (SMVP) operations to simulate the motion of the ground during the first 40 seconds of an aftershock of the 1994 Northridge earthquake in the San Fernando Valley <ref> [4, 3] </ref>. The sparse matrix consists of over 13 million rows and 180 million nonzero entries, where each nonzero entry is a dense 3 fi 3 submatrix of double precision floating point numbers. Repeated executions of a single SMVP routine account for over 70% of the simulation's computation time.
Reference: [4] <author> BAO, H., BIELAK, J., GHATTAS, O., O'HALLARON, D., KALLIVOKAS, L., SHEWCHUK, J., AND XU, J. </author> <title> Large-scale simulation of elastic wave propagation in heterogeneous media on parallel computers. </title> <note> Computer Methods in Applied Mechanics and Engineering (1997). To appear. </note>
Reference-contexts: For example, the Quake project at Carnegie Mellon University uses a sequence of more than 16,000 sparse matrix-vector product (SMVP) operations to simulate the motion of the ground during the first 40 seconds of an aftershock of the 1994 Northridge earthquake in the San Fernando Valley <ref> [4, 3] </ref>. The sparse matrix consists of over 13 million rows and 180 million nonzero entries, where each nonzero entry is a dense 3 fi 3 submatrix of double precision floating point numbers. Repeated executions of a single SMVP routine account for over 70% of the simulation's computation time.
Reference: [5] <author> BARNARD, S., AND SIMON, H. </author> <title> A fast multilevel implementation of recursive spectral bisection for partitioning unstructured problems. </title> <type> Tech. Rep. </type> <institution> RNR-92-033, NASA Ames Research Center, </institution> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: The algorithm enjoys provable upper bounds on the volume of communication, and in practice [10] generates partitions that are comparable to those produced by other modern partitioners <ref> [5, 9, 12, 20, 23] </ref>. To compute w = Kv on a set of PEs, we must consider the data distribution by which vectors and matrices are stored.
Reference: [6] <author> BLUMRICH, M., LI, K., ALPERT, K., DUBNICKI, C., FELTEN, E., AND SANDBERG, J. </author> <title> Virtual memory mapped network interface for the SHRIMP multicomputer. </title> <booktitle> In Proc. 21th Intl. Symp. on Computer Architecture (May 1994), ACM, </booktitle> <pages> pp. 142153. </pages>
Reference-contexts: Towards that end, we introduce the Spark98 kernels, a set of 10 SMVP kernels for shared memory and message passing systems. 1 We expect that builders of cache coherent shared memory multiprocessors <ref> [15, 6] </ref>, distributed memory systems [22], message passing libraries [18], and distributed shared memory libraries [1, 25] will find that the Spark98 kernels are useful tools for understanding the performance of irregular codes on their systems.
Reference: [7] <author> DONGARRA, J., LUMSDAINE, A., POZO, R., AND REMINGTON, K. </author> <title> A sparse matrix library in C++ for high performance architectures. </title> <booktitle> In Proceedings of the Second Object Oriented Numerics Conference (1994), </booktitle> <pages> pp. 214218. </pages>
Reference-contexts: Existing sparse matrix collections like the Harwell-Boeing suite [8] are important tools for designers of numerical algorithms and partitioning algorithms. And existing software libraries that support sparse matrix computations are useful tools for application programmers <ref> [2, 7, 11] </ref>. However, to date the designers of parallel systems have not had access to a set of public domain sparse matrix kernels that are simple, realistic, and portable.
Reference: [8] <author> DUFF, I. S., GRIMES, R. G., AND LEWIS, J. G. </author> <title> Sparse matrix test problems. </title> <journal> ACM Transactions on Mathematical Software 15, </journal> <month> 1 (Mar. </month> <year> 1989), </year> <month> 114. </month>
Reference-contexts: A kernel must also be realistic in that it represents a larger class of applications. Finally, a kernel must also be portable so that it can be run without modification on a large number of platforms. Existing sparse matrix collections like the Harwell-Boeing suite <ref> [8] </ref> are important tools for designers of numerical algorithms and partitioning algorithms. And existing software libraries that support sparse matrix computations are useful tools for application programmers [2, 7, 11].
Reference: [9] <author> FARHAT, C. </author> <title> A simple and efficient automatic FEM domain decomposer. </title> <journal> Comp. & Struct. </journal> <volume> 28, </volume> <month> 5 </month> <year> (1988), </year> <month> 579602. </month>
Reference-contexts: The algorithm enjoys provable upper bounds on the volume of communication, and in practice [10] generates partitions that are comparable to those produced by other modern partitioners <ref> [5, 9, 12, 20, 23] </ref>. To compute w = Kv on a set of PEs, we must consider the data distribution by which vectors and matrices are stored.
Reference: [10] <author> GILBERT, J., MILLER, G., AND TENG, S.-H. </author> <title> Geometric mesh partitioning: Implementation and experiments. </title> <booktitle> In 9th International Parallel Processing Symposium (Santa Barbara, </booktitle> <month> April </month> <year> 1995), </year> <journal> IEEE, </journal> <pages> pp. 418427. </pages>
Reference-contexts: The algorithm enjoys provable upper bounds on the volume of communication, and in practice <ref> [10] </ref> generates partitions that are comparable to those produced by other modern partitioners [5, 9, 12, 20, 23]. To compute w = Kv on a set of PEs, we must consider the data distribution by which vectors and matrices are stored.
Reference: [11] <author> GROPP, W., AND SMITH, B. </author> <title> The design of data-structure-neutral libraries for the iterative solution of sparse linear systems. </title> <booktitle> Scientific Programming 5 (1996), </booktitle> <pages> 329336. </pages>
Reference-contexts: Existing sparse matrix collections like the Harwell-Boeing suite [8] are important tools for designers of numerical algorithms and partitioning algorithms. And existing software libraries that support sparse matrix computations are useful tools for application programmers <ref> [2, 7, 11] </ref>. However, to date the designers of parallel systems have not had access to a set of public domain sparse matrix kernels that are simple, realistic, and portable.
Reference: [12] <author> HENDRICKSON, B., AND LELAND, R. </author> <title> An improved spectral graph partitioning algorithm for mapping parallel computations. </title> <journal> SIAM J. Sci. Comput. </journal> <volume> 16, </volume> <month> 2 </month> <year> (1995), </year> <month> 452469. </month>
Reference-contexts: The algorithm enjoys provable upper bounds on the volume of communication, and in practice [10] generates partitions that are comparable to those produced by other modern partitioners <ref> [5, 9, 12, 20, 23] </ref>. To compute w = Kv on a set of PEs, we must consider the data distribution by which vectors and matrices are stored.
Reference: [13] <author> HINRICHS, S., KOSAK, C., O'HALLARON, D., STRICKER, T., AND TAKE, R. </author> <title> An architecture for optimal all-to-all personalized communication. </title> <booktitle> In Proc. </booktitle> <address> SPAA '94 (Cape May, NJ, </address> <month> June </month> <year> 1994), </year> <booktitle> ACM, </booktitle> <pages> pp. 310 319. </pages>
Reference-contexts: The partitioned meshes also provide a challenging communication pattern for the SMVP that lies somewhere between a simple nearest neighbor pattern and a complete exchange <ref> [13] </ref>.
Reference: [14] <author> HOCKNEY, R. </author> <title> The Science of Computer Benchmarking. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1996. </year> <note> 16 David R. O'Hallaron </note>
Reference-contexts: Although MMV and HMV enjoy the best performance, Figure 12 shows that the performance curve is flattening as the number of PEs increases to 8 PEs, a very small number of PEs indeed. This type of 2 We use Mflop/s rather than MFLOPS, as suggested by <ref> [14] </ref>. Concluding remarks 13 32 Kbyte Icache, 32 Kbyte L1 Dcache, 2 Mbyte L2 Dcache, cc -O2). Sample points are at 1, 2, 4, and 8 PEs. LMV/sf5 was measured using 1, 2, 4, 8, 16, 32, 64, 128, 256, and 512 locks.
Reference: [15] <author> LENOSKI, D., LAUDON, J., GHARACHORLOO, K., WEBER, W., GUPTA, A., HENNESSY, J., HOROWITZ, M., AND LAM, M. </author> <title> The Stanford DASH multiprocessor. </title> <booktitle> IEEE Computer 25, </booktitle> <month> 3 (Mar. </month> <year> 1992), </year> <month> 6379. </month>
Reference-contexts: Towards that end, we introduce the Spark98 kernels, a set of 10 SMVP kernels for shared memory and message passing systems. 1 We expect that builders of cache coherent shared memory multiprocessors <ref> [15, 6] </ref>, distributed memory systems [22], message passing libraries [18], and distributed shared memory libraries [1, 25] will find that the Spark98 kernels are useful tools for understanding the performance of irregular codes on their systems.
Reference: [16] <author> MAGISTRALE, H., MCLAUGHLIN, K., AND DAY, S. </author> <title> A geology-based 3-D velocity model of the Los Angeles basin sediments. </title> <journal> Bulletin of the Seismological Society of America 86 (1996), </journal> <volume> 11611166. </volume>
Reference-contexts: the insights from using the kernels. 2 The Spark98 meshes The Spark98 kernels are based on a pair of three-dimensional unstructured finite element meshes, called sf10 and sf5, that model a volume of earth under the San Fernando Valley roughly 50 km x 50 km x 10 km in size <ref> [16] </ref>. They are unstructured in the sense that the neighbors of a node cannot be determined implicitly and must be determined explicitly from adjacency information stored in memory.
Reference: [17] <author> MILLER, G. L., TENG, S.-H., THURSTON, W., AND VAVASIS, S. A. </author> <title> Automatic Mesh Partitioning. In Graph Theory and Sparse Matrix Computation, </title> <editor> A. George, J. R. Gilbert, and J. W. H. Liu, Eds. </editor> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: Because of the simple partitioning algorithm, both LMV and RMV can run on an arbitrary number of processing elements (PEs). MMV is a parallel message passing program that uses a partition precomputed by a sophisticated geometric recursive bisection mesh partitioning algorithm <ref> [17] </ref>. HMV is a hybrid shared memory program that employs the same mesh partition as MMV, but uses shared memory copy operations rather than sends and receives to transfer data. Precomputed partitions are supplied for these kernels for 1; 2; 4; : : : ; 128 PEs. <p> The partitioner is based on a recursive geometric bisection algorithm that divides the elements equally among the subdomains while attempting to minimize the total number of nodes that are shared by subdomains, and hence minimize the total communication volume <ref> [17, 21] </ref>. The algorithm enjoys provable upper bounds on the volume of communication, and in practice [10] generates partitions that are comparable to those produced by other modern partitioners [5, 9, 12, 20, 23].
Reference: [18] <author> MPI FORUM. </author> <title> MPI: A Message Passing Interface. </title> <booktitle> In Proc. Supercomputing '93 (Portland, </booktitle> <address> OR, </address> <month> November </month> <year> 1993), </year> <month> ACM/IEEE, </month> <pages> pp. 878883. </pages>
Reference-contexts: Towards that end, we introduce the Spark98 kernels, a set of 10 SMVP kernels for shared memory and message passing systems. 1 We expect that builders of cache coherent shared memory multiprocessors [15, 6], distributed memory systems [22], message passing libraries <ref> [18] </ref>, and distributed shared memory libraries [1, 25] will find that the Spark98 kernels are useful tools for understanding the performance of irregular codes on their systems. <p> Precomputed partitions are supplied for these kernels for 1; 2; 4; : : : ; 128 PEs. The shared memory programs can be compiled to use either the Posix or SGI thread interfaces. The message passing program is based on the industry standard MPI interface <ref> [18] </ref>. In sum, Spark98 is a set of sparse matrix kernels that are designed for system builders who want to evaluate the performance of sparse codes on their systems. <p> This suggests that RMV will not scale well as we increase the number of threads. 3.4 MMV: message passing program The MMV program is based on a message passing model where PEs have private memories and use standard MPI message passing primitives to transfer data between the memories <ref> [18] </ref>. Unlike LMV and RMV, which are parallelized at runtime using a trivial partitioning algorithm, MMV is parallelized at compile time by a sophisticated partitioning algorithm that partitions the mesh into p disjoint sets of tetrahedral elements.
Reference: [19] <author> O'HALLARON, D., AND SHEWCHUK, J. </author> <title> Properties of a family of parallel finite element simulations. </title> <type> Tech. Rep. </type> <institution> CMU-CS-96-141, School of Computer Science, Carnegie Mellon University, </institution> <month> Dec. </month> <year> 1996. </year>
Reference-contexts: These partitioned meshes provide a wide range of computation to communication ratios (i.e., the number of floating point operations per PE divided by the number of communicated words per PE), ranging from 500:1 for small numbers of PEs down to 50:1 for large numbers of PEs <ref> [19] </ref>. The partitioned meshes also provide a challenging communication pattern for the SMVP that lies somewhere between a simple nearest neighbor pattern and a complete exchange [13]. See [19] for a complete characterization of the computation and communication properties of the sf10 and sf5 partitioned meshes, along with some even larger <p> number of communicated words per PE), ranging from 500:1 for small numbers of PEs down to 50:1 for large numbers of PEs <ref> [19] </ref>. The partitioned meshes also provide a challenging communication pattern for the SMVP that lies somewhere between a simple nearest neighbor pattern and a complete exchange [13]. See [19] for a complete characterization of the computation and communication properties of the sf10 and sf5 partitioned meshes, along with some even larger meshes that could appear in future versions of the Spark kernels. 3 The Spark98 programs Each of the five Spark98 programs executes a sequence of SMVP pairs of
Reference: [20] <author> POTHEN, A., SIMON, H., AND LIOU, K. </author> <title> Partitioning sparse matrices with eigenvectors of graphs. </title> <note> SIAM Journal on Matrix Analysis and Applications 11 (1990), 430452. </note>
Reference-contexts: The algorithm enjoys provable upper bounds on the volume of communication, and in practice [10] generates partitions that are comparable to those produced by other modern partitioners <ref> [5, 9, 12, 20, 23] </ref>. To compute w = Kv on a set of PEs, we must consider the data distribution by which vectors and matrices are stored.
Reference: [21] <author> SCHWABE, E., BLELLOCH, G., FELDMANN, A., GHATTAS, O., GILBERT, J., MILLER, G., O'HALLARON, D., SHEWCHUK, J., AND TENG, S. </author> <title> A separator-based framework for automated partitioning and mapping of parallel algorithms for numerical solution of PDEs. </title> <booktitle> In Proc.1992 DAGS/PC Symposium (June 1992), </booktitle> <pages> pp. 4862. </pages>
Reference-contexts: The partitioner is based on a recursive geometric bisection algorithm that divides the elements equally among the subdomains while attempting to minimize the total number of nodes that are shared by subdomains, and hence minimize the total communication volume <ref> [17, 21] </ref>. The algorithm enjoys provable upper bounds on the volume of communication, and in practice [10] generates partitions that are comparable to those produced by other modern partitioners [5, 9, 12, 20, 23].
Reference: [22] <author> SCOTT, S. </author> <title> Synchronization and communication in the T3E multiprocessor. </title> <booktitle> In Proc. 7th. International Conference on Architectural Support for Programming Languages and Operating Systems (Boston, </booktitle> <address> MA, </address> <month> Oct. </month> <year> 1996), </year> <booktitle> ACM, </booktitle> <pages> pp. 2636. </pages>
Reference-contexts: Towards that end, we introduce the Spark98 kernels, a set of 10 SMVP kernels for shared memory and message passing systems. 1 We expect that builders of cache coherent shared memory multiprocessors [15, 6], distributed memory systems <ref> [22] </ref>, message passing libraries [18], and distributed shared memory libraries [1, 25] will find that the Spark98 kernels are useful tools for understanding the performance of irregular codes on their systems.
Reference: [23] <author> SIMON, H. </author> <title> Partitioning of unstructured problems for parallel processing. </title> <journal> Comput. Sys. Engr. </journal> <volume> 2, </volume> <month> 3 </month> <year> (1991), </year> <month> 135148. </month>
Reference-contexts: The algorithm enjoys provable upper bounds on the volume of communication, and in practice [10] generates partitions that are comparable to those produced by other modern partitioners <ref> [5, 9, 12, 20, 23] </ref>. To compute w = Kv on a set of PEs, we must consider the data distribution by which vectors and matrices are stored.
Reference: [24] <author> STRICKER, T., STICHNOTH, J., O'HALLARON, D., HINRICHS, S., AND GROSS, T. </author> <title> Decoupling synchronization and data transfer in message passing systems of parallel computers. </title> <booktitle> In Proc. of 9th Intl. Conference on Supercomputing (Barcelona, </booktitle> <address> Spain, </address> <month> July </month> <year> 1995), </year> <booktitle> ACM, </booktitle> <pages> pp. 110. </pages>
Reference-contexts: After the messages are sent, each PE waits for all of the expected messages to arrive, and then sums the values in the messages into the appropriate nodal values. This two-step communication pattern, which is an example of deposit model communication <ref> [24] </ref>, ensures that user-level memory exists for every message and thus avoids unnecessary copying by the message passing system. 10 David R. O'Hallaron 3.5 HMV: hybrid message passing/shared memory program MMV is distinguished by the aggressive way that it partitions its data references.
Reference: [25] <author> ZHOU, Y., IFTODE, L., SINGH, J., LI, K., TOONEN, B., SCHOINAS, I., HILL, M., AND WOOD, D. </author> <title> Relaxed consistency and coherence granularity in DSM systems: A performance evaluation. </title> <booktitle> In Proceedings of the SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP) (Las Vegas, </booktitle> <address> NV, </address> <month> June </month> <year> 1997), </year> <booktitle> ACM, </booktitle> <pages> pp. </pages> <year> 193205. </year>
Reference-contexts: Towards that end, we introduce the Spark98 kernels, a set of 10 SMVP kernels for shared memory and message passing systems. 1 We expect that builders of cache coherent shared memory multiprocessors [15, 6], distributed memory systems [22], message passing libraries [18], and distributed shared memory libraries <ref> [1, 25] </ref> will find that the Spark98 kernels are useful tools for understanding the performance of irregular codes on their systems. In our own experience with the Spark98 kernels we notice that efficient parallel programming of sparse codes requires careful partitioning of data references, regardless of the underlying memory system.
References-found: 25

