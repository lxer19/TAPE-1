URL: http://www.cs.helsinki.fi/~tirri/tr9.ps.gz
Refering-URL: http://www.cs.helsinki.fi/~tirri/publications.html
Root-URL: 
Phone: 26,  
Title: Constructing Bayesian finite mixture models by the EM algorithm  
Author: Petri Kontkanen Petri Myllymaki Henry Tirri 
Keyword: Finite mixture models, EM algorithm, Multinomial-Dirichlet model  
Web: URL: http://www.cs.Helsinki.FI/research/cosco/  
Address: P.O.Box  FIN-00014 University of Helsinki, Finland  
Affiliation: Complex Systems Computation Group (CoSCo)  Department of Computer Science  
Abstract: Email: Firstname.Lastname@cs.Helsinki.FI Report C-1996-9, University of Helsinki, Department of Computer Science. Abstract In this paper we explore the use of finite mixture models for building decision support systems capable of sound probabilistic inference. Finite mixture models have many appealing properties: they are computationally efficient in the prediction (reasoning) phase, they are universal in the sense that they can approximate any problem domain distribution, and they can handle multimod-ality well. We present a formulation of the model construction problem in the Bayesian framework for finite mixture models, and describe how Bayesian inference is performed given such a model. The model construction problem can be seen as missing data estimation and we describe a realization of the Expectation-Maximization (EM) algorithm for finding good models. To prove the feasibility of our approach, we report crossvalidated empirical results on several publicly available classification problem datasets, and compare our results to corresponding results obtained by alternative techniques, such as neural networks and decision trees. The comparison is based on the best results reported in the literature on the datasets in question. It appears that using the theoretically sound Bayesian framework suggested here the other reported results can be outperformed with a relatively small effort. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Aha, D. Kibler, and M. Albert. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference-contexts: Bayes in [15] 75.1 Backprop in [22] 75.2 ITrule in [22] 75.5 RBF in [22] 75.7 SMART in [22] 76.8 CABINET 77.3 Linear discr. in [22] 77.5 Logistic discr. in [22] 77.7 DIPOL92 in [22] 77.6 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in <ref> [1] </ref> 37.8 M (3) in [3] 41.4 Human expert in [19] 42.0 Assistant in [19] 44.0 CABINET 50.4 Naive Bayes in [18] 51.0 Successive Bayes in [18] 51.7 75 80 85 90 Success rate (% correct) HEPATITIS C4 in [14] 81.2 Assistant in [14] 83.0 Naive Bayes in [14] 84.0 1Rw <p> Flexible Bayes in [15] 95.3 CART in [15] 95.3 C4.5 in [15] 95.3 K-NN in [14] 96.0 Naive Bayes in [15] 96.0 Backprop. in [14] 96.7 Quadratic discr. in [14] 97.3 Linear discr. in [14] 98.0 CABINET 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE C4 in <ref> [1] </ref> 75.5 IB3 in [1] 78.0 Flexible Bayes in [15] 80.0 Backprop. in [14] 80.6 MC1 in [31] 80.7 RMHC-P in [31] 82.3 Naive Bayes in [15] 83.3 CABINET 84.8 databases 15 dataset size #attrs #classes #clusters test method default Australian 690 15 2 20 10-fold CV 56.0 Breast cancer 286 <p> 95.3 CART in [15] 95.3 C4.5 in [15] 95.3 K-NN in [14] 96.0 Naive Bayes in [15] 96.0 Backprop. in [14] 96.7 Quadratic discr. in [14] 97.3 Linear discr. in [14] 98.0 CABINET 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE C4 in <ref> [1] </ref> 75.5 IB3 in [1] 78.0 Flexible Bayes in [15] 80.0 Backprop. in [14] 80.6 MC1 in [31] 80.7 RMHC-P in [31] 82.3 Naive Bayes in [15] 83.3 CABINET 84.8 databases 15 dataset size #attrs #classes #clusters test method default Australian 690 15 2 20 10-fold CV 56.0 Breast cancer 286 10 2 21 11-fold
Reference: [2] <author> J.M. Bernardo and A.F.M Smith. </author> <title> Bayesian theory. </title> <publisher> John Wiley, </publisher> <year> 1994. </year>
Reference-contexts: The Bayesian predictive inference (see e.g. <ref> [2] </ref>) aims at predicting future (yet unobserved) quantities by means of already observed quantities.
Reference: [3] <author> B. Cestnik and I. Bratko. </author> <title> On estimating probabilities in tree pruning. </title> <editor> In Y. Kodratoff, editor, </editor> <booktitle> Machine Learning EWSL-91, </booktitle> <pages> pages 138-150. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: in [22] 75.2 ITrule in [22] 75.5 RBF in [22] 75.7 SMART in [22] 76.8 CABINET 77.3 Linear discr. in [22] 77.5 Logistic discr. in [22] 77.7 DIPOL92 in [22] 77.6 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M (3) in <ref> [3] </ref> 41.4 Human expert in [19] 42.0 Assistant in [19] 44.0 CABINET 50.4 Naive Bayes in [18] 51.0 Successive Bayes in [18] 51.7 75 80 85 90 Success rate (% correct) HEPATITIS C4 in [14] 81.2 Assistant in [14] 83.0 Naive Bayes in [14] 84.0 1Rw in [14] 84.5 CABINET 88.0 <p> [14] 65.6 Backprop in [14] 71.5 Linear discr. in [14] 71.6 C4 in [14] 72.0 RMHC-PF1 in [31] 72.3 1Rw in [14] 72.7 CN2 in [14] 73.0 CABINET 76.6 CART in [14] 77.1 Assistant in [14] 78.0 Successive Bayes in [18] 78.4 Naive Bayes in [18] 79.2 M (12) in <ref> [3] </ref> 80.0 Success rate (% correct) IRIS MC1 in [31] 93.5 RMHC-PF1 in [31] 94.7 Flexible Bayes in [15] 95.3 CART in [15] 95.3 C4.5 in [15] 95.3 K-NN in [14] 96.0 Naive Bayes in [15] 96.0 Backprop. in [14] 96.7 Quadratic discr. in [14] 97.3 Linear discr. in [14] 98.0
Reference: [4] <author> P. Cheeseman, J. Kelly, M. Self, J. Stutz, W. Taylor, and D. Freeman. Auto-class: </author> <title> A Bayesian classification system. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> pages 54-64, </pages> <address> Ann Arbor, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: As our main motivation is building tools for decision making, we are more interested in the predictive capability of our models than their semantic interpretation. Consequently, although our approach akin to the AutoClass system <ref> [4] </ref>, our application areas are somewhat different: the industrial applications we are currently working on are mainly focused on nonlinear regression, classification and configuration problems, whereas the AutoClass system has been successfully used for data mining problems, such as LandSat data clustering [5].
Reference: [5] <author> P. Cheeseman and J. Stutz. </author> <title> Bayesian classification (AutoClass): Theory and results. </title> <editor> In U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, chapter 6. </booktitle> <publisher> AAAI Press, </publisher> <address> Menlo Park, </address> <year> 1995. </year>
Reference-contexts: although our approach akin to the AutoClass system [4], our application areas are somewhat different: the industrial applications we are currently working on are mainly focused on nonlinear regression, classification and configuration problems, whereas the AutoClass system has been successfully used for data mining problems, such as LandSat data clustering <ref> [5] </ref>. However, it should also be noted that for acceptable predictive performance, it seems reasonable to assume that the systems have to be based on good models of the problem domain, and hence very probably have also a structure which reflects reality well.
Reference: [6] <author> G.F. Cooper. </author> <title> The computational complexity of probabilistic inference using Bayesian belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 42(2-3):393-405, </volume> <month> March </month> <year> 1990. </year>
Reference-contexts: Secondly, even if we succeed in finding a good Bayesian network model, it is well known that probabilistic reasoning used for prediction in multi-connected Bayesian networks is an NP-hard problem <ref> [6, 7, 29] </ref>, and hence very probably not computationally feasible in general.
Reference: [7] <author> P. Dagum and M. Luby. </author> <title> Approximating probabilistic inference in Bayesian belief networks is NP-hard. </title> <journal> Artificial Intelligence, </journal> <volume> 60 </volume> <pages> 141-153, </pages> <year> 1993. </year>
Reference-contexts: Secondly, even if we succeed in finding a good Bayesian network model, it is well known that probabilistic reasoning used for prediction in multi-connected Bayesian networks is an NP-hard problem <ref> [6, 7, 29] </ref>, and hence very probably not computationally feasible in general.
Reference: [8] <author> M.H. </author> <title> DeGroot. Optimal statistical decisions. </title> <publisher> McGraw-Hill, </publisher> <year> 1970. </year>
Reference-contexts: of all the parameters is Di ( 1 ; : : : ; K ) k=1 i=1 7 with density P (fi) = k=1 ! K Y k 1 ( k ) k=1 i=1 ( l=1 n i Y kil ! The family of Dirichlet densities is conjugate (see e.g. <ref> [8] </ref>) to the family of multi nomials, i.e. the functional form of parameter distribution remains invariant in the prior-to-posterior transformation, thus the choice of Dirichlet priors is justified.
Reference: [9] <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 39(1) </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: In Section 4 we formulate the model construction problem in our Bayesian framework, and in Section 5 show how to apply the Expectation-Maximation (EM) algorithm <ref> [9] </ref> in the learning process. As our main motivation is building tools for decision making, we are more interested in the predictive capability of our models than their semantic interpretation. <p> The usual approach is to regard Z as missing data and estimate it iteratively [21]. In the next section, we show how to use the Expectation-Maximization (EM) algorithm <ref> [9] </ref> for solving this problem. 5 Estimating the MAP parameters by the EM al gorithm In the sequel, we refer to D as incomplete (or observed) data, and to (D; Z) as complete data, where Z denotes the values of the cluster indicators as in section 4.2. <p> Iterate until convergence. The main disadvantage of the EM algorithm is its slow theoretical convergence rate, which is shown to be linear <ref> [9] </ref>. The convergence is especially slow near the mode (s) of the function being maximized. Nevertheless, the convergence of EM is monotonic and the algorithm is assured to converge to a local optimum.
Reference: [10] <author> B.S. </author> <title> Everitt and D.J. Hand. Finite Mixture Distributions. </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1981. </year>
Reference-contexts: Therefore we adopt here an alternative approach, and restrict ourselves to a simpler, computationally efficient class of probabilistic models, the family of finite mixture models <ref> [10, 33] </ref>. By restricting ourselves to finite mixture models we gain the benefit of efficient probabilistic reasoning phase, and just have to concern ourselves with the complexity of the model construction problem.
Reference: [11] <author> A. Gelman, J. Carlin, H. Stern, and D. Rubin. </author> <title> Bayesian Data Analysis. </title> <publisher> Chap-man & Hall, </publisher> <year> 1995. </year>
Reference-contexts: In order to build systems capable of making good predictions, we adopt here the Bayesian approach <ref> [11] </ref>, which offers a solid theoretical framework for combining both sources of information in the model construction process. An example of such an approach is given in [13] in the case of the Bayesian network model family.
Reference: [12] <author> M. Gyllenberg, H.G. Gyllenberg, T. Koski, and J Schindler. </author> <title> Non-uniqueness of numerical taxonomic structures. </title> <journal> Binary, </journal> <volume> 5 </volume> <pages> 138-144, </pages> <year> 1993. </year>
Reference-contexts: When constructing finite mixture models from data with data mining applications in mind, it should be noticed that in the Bayesian framework there does not exist a single "true" finite mixture model which can be identified from data, but several 3 equally good models <ref> [12] </ref>. As our main focus is in prediction and not in data mining, this nonidentifiability is not a major concern for us: from a set of equally well performing models, we just pick randomly one that we use for making predictions.
Reference: [13] <author> D. Heckerman, D. Geiger, </author> <title> and D.M. Chickering. Learning Bayesian networks: The combination of knowledge and statistical data. </title> <journal> Machine Learning, </journal> <volume> 20(3) </volume> <pages> 197-243, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: In order to build systems capable of making good predictions, we adopt here the Bayesian approach [11], which offers a solid theoretical framework for combining both sources of information in the model construction process. An example of such an approach is given in <ref> [13] </ref> in the case of the Bayesian network model family. Although Bayesian networks are an intuitively appealing, a theoretically sound family of probabilistic models, in practice there are some serious computational problems involved when working with general Bayesian network structures.
Reference: [14] <author> R.C. Holte. </author> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11 </volume> <pages> 63-91, </pages> <year> 1993. </year> <month> 18 </month>
Reference-contexts: 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M (3) in [3] 41.4 Human expert in [19] 42.0 Assistant in [19] 44.0 CABINET 50.4 Naive Bayes in [18] 51.0 Successive Bayes in [18] 51.7 75 80 85 90 Success rate (% correct) HEPATITIS C4 in <ref> [14] </ref> 81.2 Assistant in [14] 83.0 Naive Bayes in [14] 84.0 1Rw in [14] 84.5 CABINET 88.0 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 Success rate (% correct) GLASS Naive Bayes in [15] 42.9 1Rw in [14] 62.2 Flexible Bayes in [15] <p> rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M (3) in [3] 41.4 Human expert in [19] 42.0 Assistant in [19] 44.0 CABINET 50.4 Naive Bayes in [18] 51.0 Successive Bayes in [18] 51.7 75 80 85 90 Success rate (% correct) HEPATITIS C4 in <ref> [14] </ref> 81.2 Assistant in [14] 83.0 Naive Bayes in [14] 84.0 1Rw in [14] 84.5 CABINET 88.0 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 Success rate (% correct) GLASS Naive Bayes in [15] 42.9 1Rw in [14] 62.2 Flexible Bayes in [15] 66.2 CABINET 84.0 itis <p> C4 in [1] 37.8 M (3) in [3] 41.4 Human expert in [19] 42.0 Assistant in [19] 44.0 CABINET 50.4 Naive Bayes in [18] 51.0 Successive Bayes in [18] 51.7 75 80 85 90 Success rate (% correct) HEPATITIS C4 in <ref> [14] </ref> 81.2 Assistant in [14] 83.0 Naive Bayes in [14] 84.0 1Rw in [14] 84.5 CABINET 88.0 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 Success rate (% correct) GLASS Naive Bayes in [15] 42.9 1Rw in [14] 62.2 Flexible Bayes in [15] 66.2 CABINET 84.0 itis and Glass datasets 14 50 <p> M (3) in [3] 41.4 Human expert in [19] 42.0 Assistant in [19] 44.0 CABINET 50.4 Naive Bayes in [18] 51.0 Successive Bayes in [18] 51.7 75 80 85 90 Success rate (% correct) HEPATITIS C4 in <ref> [14] </ref> 81.2 Assistant in [14] 83.0 Naive Bayes in [14] 84.0 1Rw in [14] 84.5 CABINET 88.0 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 Success rate (% correct) GLASS Naive Bayes in [15] 42.9 1Rw in [14] 62.2 Flexible Bayes in [15] 66.2 CABINET 84.0 itis and Glass datasets 14 50 55 60 65 70 <p> (% correct) HEPATITIS C4 in <ref> [14] </ref> 81.2 Assistant in [14] 83.0 Naive Bayes in [14] 84.0 1Rw in [14] 84.5 CABINET 88.0 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 Success rate (% correct) GLASS Naive Bayes in [15] 42.9 1Rw in [14] 62.2 Flexible Bayes in [15] 66.2 CABINET 84.0 itis and Glass datasets 14 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in [22] 85.4 ITrule in [22] 86.5 Cal5 in [22] 86.9 SMART in [22] 88.5 NewID in [22] 90.0 AC2 in <p> 93.2 Logistic discr. in [22] 93.9 Quadratic discr. in [22] 94.1 Linear discr. in [22] 94.1 ALLOC80 in [22] 94.3 DIPOL92 in [22] 95.2 RBF in [22] 95.9 CABINET 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [19] 64.0 K-NN in <ref> [14] </ref> 65.3 Quadratic discr. in [14] 65.6 Backprop in [14] 71.5 Linear discr. in [14] 71.6 C4 in [14] 72.0 RMHC-PF1 in [31] 72.3 1Rw in [14] 72.7 CN2 in [14] 73.0 CABINET 76.6 CART in [14] 77.1 Assistant in [14] 78.0 Successive Bayes in [18] 78.4 Naive Bayes in [18] <p> 93.9 Quadratic discr. in [22] 94.1 Linear discr. in [22] 94.1 ALLOC80 in [22] 94.3 DIPOL92 in [22] 95.2 RBF in [22] 95.9 CABINET 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [19] 64.0 K-NN in <ref> [14] </ref> 65.3 Quadratic discr. in [14] 65.6 Backprop in [14] 71.5 Linear discr. in [14] 71.6 C4 in [14] 72.0 RMHC-PF1 in [31] 72.3 1Rw in [14] 72.7 CN2 in [14] 73.0 CABINET 76.6 CART in [14] 77.1 Assistant in [14] 78.0 Successive Bayes in [18] 78.4 Naive Bayes in [18] 79.2 M (12) in [3] <p> [22] 94.1 Linear discr. in [22] 94.1 ALLOC80 in [22] 94.3 DIPOL92 in [22] 95.2 RBF in [22] 95.9 CABINET 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [19] 64.0 K-NN in <ref> [14] </ref> 65.3 Quadratic discr. in [14] 65.6 Backprop in [14] 71.5 Linear discr. in [14] 71.6 C4 in [14] 72.0 RMHC-PF1 in [31] 72.3 1Rw in [14] 72.7 CN2 in [14] 73.0 CABINET 76.6 CART in [14] 77.1 Assistant in [14] 78.0 Successive Bayes in [18] 78.4 Naive Bayes in [18] 79.2 M (12) in [3] 80.0 Success rate (% <p> [22] 94.1 ALLOC80 in [22] 94.3 DIPOL92 in [22] 95.2 RBF in [22] 95.9 CABINET 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [19] 64.0 K-NN in <ref> [14] </ref> 65.3 Quadratic discr. in [14] 65.6 Backprop in [14] 71.5 Linear discr. in [14] 71.6 C4 in [14] 72.0 RMHC-PF1 in [31] 72.3 1Rw in [14] 72.7 CN2 in [14] 73.0 CABINET 76.6 CART in [14] 77.1 Assistant in [14] 78.0 Successive Bayes in [18] 78.4 Naive Bayes in [18] 79.2 M (12) in [3] 80.0 Success rate (% correct) IRIS MC1 in [31] <p> [22] 94.3 DIPOL92 in [22] 95.2 RBF in [22] 95.9 CABINET 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [19] 64.0 K-NN in <ref> [14] </ref> 65.3 Quadratic discr. in [14] 65.6 Backprop in [14] 71.5 Linear discr. in [14] 71.6 C4 in [14] 72.0 RMHC-PF1 in [31] 72.3 1Rw in [14] 72.7 CN2 in [14] 73.0 CABINET 76.6 CART in [14] 77.1 Assistant in [14] 78.0 Successive Bayes in [18] 78.4 Naive Bayes in [18] 79.2 M (12) in [3] 80.0 Success rate (% correct) IRIS MC1 in [31] 93.5 RMHC-PF1 in [31] <p> [22] 95.9 CABINET 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [19] 64.0 K-NN in <ref> [14] </ref> 65.3 Quadratic discr. in [14] 65.6 Backprop in [14] 71.5 Linear discr. in [14] 71.6 C4 in [14] 72.0 RMHC-PF1 in [31] 72.3 1Rw in [14] 72.7 CN2 in [14] 73.0 CABINET 76.6 CART in [14] 77.1 Assistant in [14] 78.0 Successive Bayes in [18] 78.4 Naive Bayes in [18] 79.2 M (12) in [3] 80.0 Success rate (% correct) IRIS MC1 in [31] 93.5 RMHC-PF1 in [31] 94.7 Flexible Bayes in [15] 95.3 CART in <p> 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [19] 64.0 K-NN in <ref> [14] </ref> 65.3 Quadratic discr. in [14] 65.6 Backprop in [14] 71.5 Linear discr. in [14] 71.6 C4 in [14] 72.0 RMHC-PF1 in [31] 72.3 1Rw in [14] 72.7 CN2 in [14] 73.0 CABINET 76.6 CART in [14] 77.1 Assistant in [14] 78.0 Successive Bayes in [18] 78.4 Naive Bayes in [18] 79.2 M (12) in [3] 80.0 Success rate (% correct) IRIS MC1 in [31] 93.5 RMHC-PF1 in [31] 94.7 Flexible Bayes in [15] 95.3 CART in [15] 95.3 C4.5 in <p> 80 Success rate (% correct) BREAST CANCER Human expert in [19] 64.0 K-NN in <ref> [14] </ref> 65.3 Quadratic discr. in [14] 65.6 Backprop in [14] 71.5 Linear discr. in [14] 71.6 C4 in [14] 72.0 RMHC-PF1 in [31] 72.3 1Rw in [14] 72.7 CN2 in [14] 73.0 CABINET 76.6 CART in [14] 77.1 Assistant in [14] 78.0 Successive Bayes in [18] 78.4 Naive Bayes in [18] 79.2 M (12) in [3] 80.0 Success rate (% correct) IRIS MC1 in [31] 93.5 RMHC-PF1 in [31] 94.7 Flexible Bayes in [15] 95.3 CART in [15] 95.3 C4.5 in [15] 95.3 K-NN in [14] 96.0 <p> correct) BREAST CANCER Human expert in [19] 64.0 K-NN in <ref> [14] </ref> 65.3 Quadratic discr. in [14] 65.6 Backprop in [14] 71.5 Linear discr. in [14] 71.6 C4 in [14] 72.0 RMHC-PF1 in [31] 72.3 1Rw in [14] 72.7 CN2 in [14] 73.0 CABINET 76.6 CART in [14] 77.1 Assistant in [14] 78.0 Successive Bayes in [18] 78.4 Naive Bayes in [18] 79.2 M (12) in [3] 80.0 Success rate (% correct) IRIS MC1 in [31] 93.5 RMHC-PF1 in [31] 94.7 Flexible Bayes in [15] 95.3 CART in [15] 95.3 C4.5 in [15] 95.3 K-NN in [14] 96.0 Naive Bayes in [15] <p> in <ref> [14] </ref> 77.1 Assistant in [14] 78.0 Successive Bayes in [18] 78.4 Naive Bayes in [18] 79.2 M (12) in [3] 80.0 Success rate (% correct) IRIS MC1 in [31] 93.5 RMHC-PF1 in [31] 94.7 Flexible Bayes in [15] 95.3 CART in [15] 95.3 C4.5 in [15] 95.3 K-NN in [14] 96.0 Naive Bayes in [15] 96.0 Backprop. in [14] 96.7 Quadratic discr. in [14] 97.3 Linear discr. in [14] 98.0 CABINET 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE C4 in [1] 75.5 IB3 in [1] 78.0 Flexible Bayes in [15] 80.0 Backprop. in [14] 80.6 MC1 <p> in [18] 78.4 Naive Bayes in [18] 79.2 M (12) in [3] 80.0 Success rate (% correct) IRIS MC1 in [31] 93.5 RMHC-PF1 in [31] 94.7 Flexible Bayes in [15] 95.3 CART in [15] 95.3 C4.5 in [15] 95.3 K-NN in <ref> [14] </ref> 96.0 Naive Bayes in [15] 96.0 Backprop. in [14] 96.7 Quadratic discr. in [14] 97.3 Linear discr. in [14] 98.0 CABINET 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE C4 in [1] 75.5 IB3 in [1] 78.0 Flexible Bayes in [15] 80.0 Backprop. in [14] 80.6 MC1 in [31] 80.7 RMHC-P in [31] 82.3 Naive Bayes <p> in [18] 79.2 M (12) in [3] 80.0 Success rate (% correct) IRIS MC1 in [31] 93.5 RMHC-PF1 in [31] 94.7 Flexible Bayes in [15] 95.3 CART in [15] 95.3 C4.5 in [15] 95.3 K-NN in <ref> [14] </ref> 96.0 Naive Bayes in [15] 96.0 Backprop. in [14] 96.7 Quadratic discr. in [14] 97.3 Linear discr. in [14] 98.0 CABINET 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE C4 in [1] 75.5 IB3 in [1] 78.0 Flexible Bayes in [15] 80.0 Backprop. in [14] 80.6 MC1 in [31] 80.7 RMHC-P in [31] 82.3 Naive Bayes in [15] 83.3 CABINET 84.8 <p> in [3] 80.0 Success rate (% correct) IRIS MC1 in [31] 93.5 RMHC-PF1 in [31] 94.7 Flexible Bayes in [15] 95.3 CART in [15] 95.3 C4.5 in [15] 95.3 K-NN in <ref> [14] </ref> 96.0 Naive Bayes in [15] 96.0 Backprop. in [14] 96.7 Quadratic discr. in [14] 97.3 Linear discr. in [14] 98.0 CABINET 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE C4 in [1] 75.5 IB3 in [1] 78.0 Flexible Bayes in [15] 80.0 Backprop. in [14] 80.6 MC1 in [31] 80.7 RMHC-P in [31] 82.3 Naive Bayes in [15] 83.3 CABINET 84.8 databases 15 dataset size #attrs <p> K-NN in <ref> [14] </ref> 96.0 Naive Bayes in [15] 96.0 Backprop. in [14] 96.7 Quadratic discr. in [14] 97.3 Linear discr. in [14] 98.0 CABINET 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE C4 in [1] 75.5 IB3 in [1] 78.0 Flexible Bayes in [15] 80.0 Backprop. in [14] 80.6 MC1 in [31] 80.7 RMHC-P in [31] 82.3 Naive Bayes in [15] 83.3 CABINET 84.8 databases 15 dataset size #attrs #classes #clusters test method default Australian 690 15 2 20 10-fold CV 56.0 Breast cancer 286 10 2 21 11-fold CV 70.3 Diabetes 768 9 2 20 12-fold CV
Reference: [15] <author> G.H. John and P. Langley. </author> <title> Estimating continuous distributions in Bayesian clas-sifiers. </title> <editor> In P. Besnard and S. Hanks, editors, </editor> <booktitle> Proceedings of the 11th Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 338-345. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1995. </year>
Reference-contexts: the individual Dirichlet's and is obtained by setting ^ff k = N + k=1 k K f kil + kil 1 P n i : (10) Assuming that the values of the cluster indicators are part of the training data D results in the simple naive Bayes model (see e.g. <ref> [15] </ref>). If, however, we do not consider the values of cluster indicators to be known, the learning problem becomes much more complex. <p> The goodness of the results with respect to the memory based methods are not especially surprising as they can be seen as approximations to the 13 70 75 80 85 Success rate (% correct) AUSTRALIAN Flexible Bayes in <ref> [15] </ref> 78.3 Quadratic discr. in [22] 79.3 CN2 in [22] 79.6 ALLOC80 in [22] 79.9 LVQ in [22] 80.3 NewID in [22] 81.9 AC2 in [22] 81.9 Bayes tree in [22] 82.9 SMART in [22] 84.2 C4.5 in [22] 84.5 Backprop in [22] 84.6 IndCART in [22] 84.8 Naive Bayes in <p> Success rate (% correct) DIABETES K-NN in [22] 67.6 ALLOC80 in [22] 69.9 CN2 in [22] 71.1 NewID in [22] 71.1 AC2 in [22] 72.4 LVQ in [22] 72.8 Bayes tree in [22] 72.9 IndCART in [22] 72.9 C4.5 in [22] 73.0 Quadratic discr. in [22] 73.8 Flexible Bayes in <ref> [15] </ref> 73.9 CASTLE in [22] 74.2 CART in [22] 74.5 Cal5 in [22] 75.0 Naive Bayes in [15] 75.1 Backprop in [22] 75.2 ITrule in [22] 75.5 RBF in [22] 75.7 SMART in [22] 76.8 CABINET 77.3 Linear discr. in [22] 77.5 Logistic discr. in [22] 77.7 DIPOL92 in [22] 77.6 <p> NewID in [22] 71.1 AC2 in [22] 72.4 LVQ in [22] 72.8 Bayes tree in [22] 72.9 IndCART in [22] 72.9 C4.5 in [22] 73.0 Quadratic discr. in [22] 73.8 Flexible Bayes in <ref> [15] </ref> 73.9 CASTLE in [22] 74.2 CART in [22] 74.5 Cal5 in [22] 75.0 Naive Bayes in [15] 75.1 Backprop in [22] 75.2 ITrule in [22] 75.5 RBF in [22] 75.7 SMART in [22] 76.8 CABINET 77.3 Linear discr. in [22] 77.5 Logistic discr. in [22] 77.7 DIPOL92 in [22] 77.6 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M <p> 85 90 Success rate (% correct) HEPATITIS C4 in [14] 81.2 Assistant in [14] 83.0 Naive Bayes in [14] 84.0 1Rw in [14] 84.5 CABINET 88.0 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 Success rate (% correct) GLASS Naive Bayes in <ref> [15] </ref> 42.9 1Rw in [14] 62.2 Flexible Bayes in [15] 66.2 CABINET 84.0 itis and Glass datasets 14 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in [22] 85.4 ITrule in [22] 86.5 Cal5 in [22] 86.9 SMART in [22] 88.5 NewID in <p> [14] 81.2 Assistant in [14] 83.0 Naive Bayes in [14] 84.0 1Rw in [14] 84.5 CABINET 88.0 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 Success rate (% correct) GLASS Naive Bayes in <ref> [15] </ref> 42.9 1Rw in [14] 62.2 Flexible Bayes in [15] 66.2 CABINET 84.0 itis and Glass datasets 14 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in [22] 85.4 ITrule in [22] 86.5 Cal5 in [22] 86.9 SMART in [22] 88.5 NewID in [22] 90.0 AC2 in [22] 90.0 Bayes tree in <p> 72.3 1Rw in [14] 72.7 CN2 in [14] 73.0 CABINET 76.6 CART in [14] 77.1 Assistant in [14] 78.0 Successive Bayes in [18] 78.4 Naive Bayes in [18] 79.2 M (12) in [3] 80.0 Success rate (% correct) IRIS MC1 in [31] 93.5 RMHC-PF1 in [31] 94.7 Flexible Bayes in <ref> [15] </ref> 95.3 CART in [15] 95.3 C4.5 in [15] 95.3 K-NN in [14] 96.0 Naive Bayes in [15] 96.0 Backprop. in [14] 96.7 Quadratic discr. in [14] 97.3 Linear discr. in [14] 98.0 CABINET 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE C4 in [1] 75.5 IB3 in <p> 72.7 CN2 in [14] 73.0 CABINET 76.6 CART in [14] 77.1 Assistant in [14] 78.0 Successive Bayes in [18] 78.4 Naive Bayes in [18] 79.2 M (12) in [3] 80.0 Success rate (% correct) IRIS MC1 in [31] 93.5 RMHC-PF1 in [31] 94.7 Flexible Bayes in <ref> [15] </ref> 95.3 CART in [15] 95.3 C4.5 in [15] 95.3 K-NN in [14] 96.0 Naive Bayes in [15] 96.0 Backprop. in [14] 96.7 Quadratic discr. in [14] 97.3 Linear discr. in [14] 98.0 CABINET 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE C4 in [1] 75.5 IB3 in [1] 78.0 Flexible Bayes <p> 73.0 CABINET 76.6 CART in [14] 77.1 Assistant in [14] 78.0 Successive Bayes in [18] 78.4 Naive Bayes in [18] 79.2 M (12) in [3] 80.0 Success rate (% correct) IRIS MC1 in [31] 93.5 RMHC-PF1 in [31] 94.7 Flexible Bayes in <ref> [15] </ref> 95.3 CART in [15] 95.3 C4.5 in [15] 95.3 K-NN in [14] 96.0 Naive Bayes in [15] 96.0 Backprop. in [14] 96.7 Quadratic discr. in [14] 97.3 Linear discr. in [14] 98.0 CABINET 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE C4 in [1] 75.5 IB3 in [1] 78.0 Flexible Bayes in [15] 80.0 Backprop. <p> [14] 78.0 Successive Bayes in [18] 78.4 Naive Bayes in [18] 79.2 M (12) in [3] 80.0 Success rate (% correct) IRIS MC1 in [31] 93.5 RMHC-PF1 in [31] 94.7 Flexible Bayes in <ref> [15] </ref> 95.3 CART in [15] 95.3 C4.5 in [15] 95.3 K-NN in [14] 96.0 Naive Bayes in [15] 96.0 Backprop. in [14] 96.7 Quadratic discr. in [14] 97.3 Linear discr. in [14] 98.0 CABINET 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE C4 in [1] 75.5 IB3 in [1] 78.0 Flexible Bayes in [15] 80.0 Backprop. in [14] 80.6 MC1 in [31] 80.7 RMHC-P in <p> C4.5 in <ref> [15] </ref> 95.3 K-NN in [14] 96.0 Naive Bayes in [15] 96.0 Backprop. in [14] 96.7 Quadratic discr. in [14] 97.3 Linear discr. in [14] 98.0 CABINET 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE C4 in [1] 75.5 IB3 in [1] 78.0 Flexible Bayes in [15] 80.0 Backprop. in [14] 80.6 MC1 in [31] 80.7 RMHC-P in [31] 82.3 Naive Bayes in [15] 83.3 CABINET 84.8 databases 15 dataset size #attrs #classes #clusters test method default Australian 690 15 2 20 10-fold CV 56.0 Breast cancer 286 10 2 21 11-fold CV 70.3 Diabetes 768 9 <p> Quadratic discr. in [14] 97.3 Linear discr. in [14] 98.0 CABINET 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE C4 in [1] 75.5 IB3 in [1] 78.0 Flexible Bayes in <ref> [15] </ref> 80.0 Backprop. in [14] 80.6 MC1 in [31] 80.7 RMHC-P in [31] 82.3 Naive Bayes in [15] 83.3 CABINET 84.8 databases 15 dataset size #attrs #classes #clusters test method default Australian 690 15 2 20 10-fold CV 56.0 Breast cancer 286 10 2 21 11-fold CV 70.3 Diabetes 768 9 2 20 12-fold CV 65.0 DNA 3186 181 3 13 train&test 50.8 Glass 214 10 6 48
Reference: [16] <author> R.E. Kass and A.E. Raftery. </author> <title> Bayes factors. </title> <type> Technical Report 254, </type> <institution> Department of Statistics, University of Washington, </institution> <year> 1994. </year>
Reference-contexts: Laplace's method <ref> [16] </ref>. Many information-theoretic measures, such as stochastic complexity [28], can also be seen as approximations of this integral.
Reference: [17] <author> H. Kitano. </author> <title> Challenges of massive parallelism. </title> <booktitle> In Proc. of IJCAI-93, the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 813-834, </pages> <address> Chambery, France, August 1993. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: The model also conforms interestingly to the intuitively appealing memory-based reasoning paradigm (see e.g., <ref> [17] </ref>), and it can be seen to offer a Bayesian solution to the case matching and case adaptation problems in such domains [20]. When compared to single parametric distribution methods and nonparametric methods (such as kernel density estimators [30]), finite mixture models have many 2 appealing properties.
Reference: [18] <author> I. Kononenko. </author> <title> Successive naive Bayesian classifier. </title> <journal> Informatica, </journal> <volume> 17 </volume> <pages> 167-174, </pages> <year> 1993. </year>
Reference-contexts: 77.3 Linear discr. in [22] 77.5 Logistic discr. in [22] 77.7 DIPOL92 in [22] 77.6 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M (3) in [3] 41.4 Human expert in [19] 42.0 Assistant in [19] 44.0 CABINET 50.4 Naive Bayes in <ref> [18] </ref> 51.0 Successive Bayes in [18] 51.7 75 80 85 90 Success rate (% correct) HEPATITIS C4 in [14] 81.2 Assistant in [14] 83.0 Naive Bayes in [14] 84.0 1Rw in [14] 84.5 CABINET 88.0 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 <p> 77.5 Logistic discr. in [22] 77.7 DIPOL92 in [22] 77.6 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M (3) in [3] 41.4 Human expert in [19] 42.0 Assistant in [19] 44.0 CABINET 50.4 Naive Bayes in <ref> [18] </ref> 51.0 Successive Bayes in [18] 51.7 75 80 85 90 Success rate (% correct) HEPATITIS C4 in [14] 81.2 Assistant in [14] 83.0 Naive Bayes in [14] 84.0 1Rw in [14] 84.5 CABINET 88.0 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 Success rate (% correct) GLASS <p> in [19] 64.0 K-NN in [14] 65.3 Quadratic discr. in [14] 65.6 Backprop in [14] 71.5 Linear discr. in [14] 71.6 C4 in [14] 72.0 RMHC-PF1 in [31] 72.3 1Rw in [14] 72.7 CN2 in [14] 73.0 CABINET 76.6 CART in [14] 77.1 Assistant in [14] 78.0 Successive Bayes in <ref> [18] </ref> 78.4 Naive Bayes in [18] 79.2 M (12) in [3] 80.0 Success rate (% correct) IRIS MC1 in [31] 93.5 RMHC-PF1 in [31] 94.7 Flexible Bayes in [15] 95.3 CART in [15] 95.3 C4.5 in [15] 95.3 K-NN in [14] 96.0 Naive Bayes in [15] 96.0 Backprop. in [14] 96.7 <p> [14] 65.3 Quadratic discr. in [14] 65.6 Backprop in [14] 71.5 Linear discr. in [14] 71.6 C4 in [14] 72.0 RMHC-PF1 in [31] 72.3 1Rw in [14] 72.7 CN2 in [14] 73.0 CABINET 76.6 CART in [14] 77.1 Assistant in [14] 78.0 Successive Bayes in <ref> [18] </ref> 78.4 Naive Bayes in [18] 79.2 M (12) in [3] 80.0 Success rate (% correct) IRIS MC1 in [31] 93.5 RMHC-PF1 in [31] 94.7 Flexible Bayes in [15] 95.3 CART in [15] 95.3 C4.5 in [15] 95.3 K-NN in [14] 96.0 Naive Bayes in [15] 96.0 Backprop. in [14] 96.7 Quadratic discr. in [14] 97.3
Reference: [19] <author> I. Kononenko and I. Bratko. </author> <title> Information-based evaluation criterion for classifier's performance. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 67-80, </pages> <year> 1991. </year>
Reference-contexts: [22] 75.5 RBF in [22] 75.7 SMART in [22] 76.8 CABINET 77.3 Linear discr. in [22] 77.5 Logistic discr. in [22] 77.7 DIPOL92 in [22] 77.6 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M (3) in [3] 41.4 Human expert in <ref> [19] </ref> 42.0 Assistant in [19] 44.0 CABINET 50.4 Naive Bayes in [18] 51.0 Successive Bayes in [18] 51.7 75 80 85 90 Success rate (% correct) HEPATITIS C4 in [14] 81.2 Assistant in [14] 83.0 Naive Bayes in [14] 84.0 1Rw in [14] 84.5 CABINET 88.0 10 15 20 25 30 <p> [22] 75.7 SMART in [22] 76.8 CABINET 77.3 Linear discr. in [22] 77.5 Logistic discr. in [22] 77.7 DIPOL92 in [22] 77.6 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M (3) in [3] 41.4 Human expert in <ref> [19] </ref> 42.0 Assistant in [19] 44.0 CABINET 50.4 Naive Bayes in [18] 51.0 Successive Bayes in [18] 51.7 75 80 85 90 Success rate (% correct) HEPATITIS C4 in [14] 81.2 Assistant in [14] 83.0 Naive Bayes in [14] 84.0 1Rw in [14] 84.5 CABINET 88.0 10 15 20 25 30 35 40 45 50 <p> Naive Bayes in [22] 93.2 Logistic discr. in [22] 93.9 Quadratic discr. in [22] 94.1 Linear discr. in [22] 94.1 ALLOC80 in [22] 94.3 DIPOL92 in [22] 95.2 RBF in [22] 95.9 CABINET 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in <ref> [19] </ref> 64.0 K-NN in [14] 65.3 Quadratic discr. in [14] 65.6 Backprop in [14] 71.5 Linear discr. in [14] 71.6 C4 in [14] 72.0 RMHC-PF1 in [31] 72.3 1Rw in [14] 72.7 CN2 in [14] 73.0 CABINET 76.6 CART in [14] 77.1 Assistant in [14] 78.0 Successive Bayes in [18] 78.4
Reference: [20] <author> P. Kontkanen, P. Myllymaki, and H. Tirri. </author> <title> Probabilistic instance-based learning. </title> <note> Submitted to ICML'96, </note> <month> January </month> <year> 1996. </year>
Reference-contexts: The model also conforms interestingly to the intuitively appealing memory-based reasoning paradigm (see e.g., [17]), and it can be seen to offer a Bayesian solution to the case matching and case adaptation problems in such domains <ref> [20] </ref>. When compared to single parametric distribution methods and nonparametric methods (such as kernel density estimators [30]), finite mixture models have many 2 appealing properties. <p> 270 14 2 9 9-fold CV 79.4 Hepatitis 150 20 2 6 5-fold CV 55.6 Iris 150 5 3 5 5-fold CV 33.3 Primary tumor 339 18 21 21 10-fold CV 24.8 Table 1: Description of the datasets and testing methods used in our experiments. finite mixture approach presented here <ref> [20] </ref>. A more interesting observation is that the EM finds MAP estimates that outperform also all other Bayesian approaches present in the StatLog comparison including CASTLE, Naive Bayes, and Bayes tree algorithms.
Reference: [21] <author> R.J.A. Little and D.B. Rubin. </author> <title> Statistical analysis with missing data. </title> <publisher> Wiley, </publisher> <year> 1987. </year>
Reference-contexts: These clusterings are, however, exponential (K N ) in number, so exact MAP estimate cannot be found, and we are forced to use numerical approximation methods. The usual approach is to regard Z as missing data and estimate it iteratively <ref> [21] </ref>.
Reference: [22] <editor> D. Michie, D.J. Spiegelhalter, and C.C. Taylor, editors. </editor> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Ellis Horwood, </publisher> <address> London, </address> <year> 1994. </year>
Reference-contexts: The selection of datasets was done on the basis of their reported use, i.e., we have preferred datasets that have been used for testing many different methods over datasets with only isolated results. Many of the results are from the StatLog project <ref> [22] </ref>, but we have also included more recent results. The descriptions of the datasets, the testing procedures used, and the best model classes (the number of mixtures) found for each data set are given in Table 1. <p> It should be observed that with the exception of the DNA dataset, all our current results are crossvalidated, and when possible (for the StatLog datasets) we have used the same crossvalidation scheme as described in <ref> [22] </ref>. The same does not hold for many of the results for the other methods. In many cases the testing procedure either was not reported, or the best result with a single test set was given. <p> The goodness of the results with respect to the memory based methods are not especially surprising as they can be seen as approximations to the 13 70 75 80 85 Success rate (% correct) AUSTRALIAN Flexible Bayes in [15] 78.3 Quadratic discr. in <ref> [22] </ref> 79.3 CN2 in [22] 79.6 ALLOC80 in [22] 79.9 LVQ in [22] 80.3 NewID in [22] 81.9 AC2 in [22] 81.9 Bayes tree in [22] 82.9 SMART in [22] 84.2 C4.5 in [22] 84.5 Backprop in [22] 84.6 IndCART in [22] 84.8 Naive Bayes in [22] 84.9 CASTLE in [22] <p> The goodness of the results with respect to the memory based methods are not especially surprising as they can be seen as approximations to the 13 70 75 80 85 Success rate (% correct) AUSTRALIAN Flexible Bayes in [15] 78.3 Quadratic discr. in <ref> [22] </ref> 79.3 CN2 in [22] 79.6 ALLOC80 in [22] 79.9 LVQ in [22] 80.3 NewID in [22] 81.9 AC2 in [22] 81.9 Bayes tree in [22] 82.9 SMART in [22] 84.2 C4.5 in [22] 84.5 Backprop in [22] 84.6 IndCART in [22] 84.8 Naive Bayes in [22] 84.9 CASTLE in [22] 85.2 CART in [22] <p> goodness of the results with respect to the memory based methods are not especially surprising as they can be seen as approximations to the 13 70 75 80 85 Success rate (% correct) AUSTRALIAN Flexible Bayes in [15] 78.3 Quadratic discr. in <ref> [22] </ref> 79.3 CN2 in [22] 79.6 ALLOC80 in [22] 79.9 LVQ in [22] 80.3 NewID in [22] 81.9 AC2 in [22] 81.9 Bayes tree in [22] 82.9 SMART in [22] 84.2 C4.5 in [22] 84.5 Backprop in [22] 84.6 IndCART in [22] 84.8 Naive Bayes in [22] 84.9 CASTLE in [22] 85.2 CART in [22] 85.5 RBF in [22] <p> with respect to the memory based methods are not especially surprising as they can be seen as approximations to the 13 70 75 80 85 Success rate (% correct) AUSTRALIAN Flexible Bayes in [15] 78.3 Quadratic discr. in <ref> [22] </ref> 79.3 CN2 in [22] 79.6 ALLOC80 in [22] 79.9 LVQ in [22] 80.3 NewID in [22] 81.9 AC2 in [22] 81.9 Bayes tree in [22] 82.9 SMART in [22] 84.2 C4.5 in [22] 84.5 Backprop in [22] 84.6 IndCART in [22] 84.8 Naive Bayes in [22] 84.9 CASTLE in [22] 85.2 CART in [22] 85.5 RBF in [22] 85.5 DIPOL92 in [22] <p> memory based methods are not especially surprising as they can be seen as approximations to the 13 70 75 80 85 Success rate (% correct) AUSTRALIAN Flexible Bayes in [15] 78.3 Quadratic discr. in <ref> [22] </ref> 79.3 CN2 in [22] 79.6 ALLOC80 in [22] 79.9 LVQ in [22] 80.3 NewID in [22] 81.9 AC2 in [22] 81.9 Bayes tree in [22] 82.9 SMART in [22] 84.2 C4.5 in [22] 84.5 Backprop in [22] 84.6 IndCART in [22] 84.8 Naive Bayes in [22] 84.9 CASTLE in [22] 85.2 CART in [22] 85.5 RBF in [22] 85.5 DIPOL92 in [22] 85.9 Linear discr. in <p> not especially surprising as they can be seen as approximations to the 13 70 75 80 85 Success rate (% correct) AUSTRALIAN Flexible Bayes in [15] 78.3 Quadratic discr. in <ref> [22] </ref> 79.3 CN2 in [22] 79.6 ALLOC80 in [22] 79.9 LVQ in [22] 80.3 NewID in [22] 81.9 AC2 in [22] 81.9 Bayes tree in [22] 82.9 SMART in [22] 84.2 C4.5 in [22] 84.5 Backprop in [22] 84.6 IndCART in [22] 84.8 Naive Bayes in [22] 84.9 CASTLE in [22] 85.2 CART in [22] 85.5 RBF in [22] 85.5 DIPOL92 in [22] 85.9 Linear discr. in [22] 85.9 Logistic discr. <p> can be seen as approximations to the 13 70 75 80 85 Success rate (% correct) AUSTRALIAN Flexible Bayes in [15] 78.3 Quadratic discr. in <ref> [22] </ref> 79.3 CN2 in [22] 79.6 ALLOC80 in [22] 79.9 LVQ in [22] 80.3 NewID in [22] 81.9 AC2 in [22] 81.9 Bayes tree in [22] 82.9 SMART in [22] 84.2 C4.5 in [22] 84.5 Backprop in [22] 84.6 IndCART in [22] 84.8 Naive Bayes in [22] 84.9 CASTLE in [22] 85.2 CART in [22] 85.5 RBF in [22] 85.5 DIPOL92 in [22] 85.9 Linear discr. in [22] 85.9 Logistic discr. in [22] 85.9 ITrule in <p> approximations to the 13 70 75 80 85 Success rate (% correct) AUSTRALIAN Flexible Bayes in [15] 78.3 Quadratic discr. in <ref> [22] </ref> 79.3 CN2 in [22] 79.6 ALLOC80 in [22] 79.9 LVQ in [22] 80.3 NewID in [22] 81.9 AC2 in [22] 81.9 Bayes tree in [22] 82.9 SMART in [22] 84.2 C4.5 in [22] 84.5 Backprop in [22] 84.6 IndCART in [22] 84.8 Naive Bayes in [22] 84.9 CASTLE in [22] 85.2 CART in [22] 85.5 RBF in [22] 85.5 DIPOL92 in [22] 85.9 Linear discr. in [22] 85.9 Logistic discr. in [22] 85.9 ITrule in [22] 86.3 Cal5 in <p> 70 75 80 85 Success rate (% correct) AUSTRALIAN Flexible Bayes in [15] 78.3 Quadratic discr. in <ref> [22] </ref> 79.3 CN2 in [22] 79.6 ALLOC80 in [22] 79.9 LVQ in [22] 80.3 NewID in [22] 81.9 AC2 in [22] 81.9 Bayes tree in [22] 82.9 SMART in [22] 84.2 C4.5 in [22] 84.5 Backprop in [22] 84.6 IndCART in [22] 84.8 Naive Bayes in [22] 84.9 CASTLE in [22] 85.2 CART in [22] 85.5 RBF in [22] 85.5 DIPOL92 in [22] 85.9 Linear discr. in [22] 85.9 Logistic discr. in [22] 85.9 ITrule in [22] 86.3 Cal5 in [22] 86.9 CABINET 87.2 <p> Success rate (% correct) AUSTRALIAN Flexible Bayes in [15] 78.3 Quadratic discr. in <ref> [22] </ref> 79.3 CN2 in [22] 79.6 ALLOC80 in [22] 79.9 LVQ in [22] 80.3 NewID in [22] 81.9 AC2 in [22] 81.9 Bayes tree in [22] 82.9 SMART in [22] 84.2 C4.5 in [22] 84.5 Backprop in [22] 84.6 IndCART in [22] 84.8 Naive Bayes in [22] 84.9 CASTLE in [22] 85.2 CART in [22] 85.5 RBF in [22] 85.5 DIPOL92 in [22] 85.9 Linear discr. in [22] 85.9 Logistic discr. in [22] 85.9 ITrule in [22] 86.3 Cal5 in [22] 86.9 CABINET 87.2 60 65 70 75 <p> AUSTRALIAN Flexible Bayes in [15] 78.3 Quadratic discr. in <ref> [22] </ref> 79.3 CN2 in [22] 79.6 ALLOC80 in [22] 79.9 LVQ in [22] 80.3 NewID in [22] 81.9 AC2 in [22] 81.9 Bayes tree in [22] 82.9 SMART in [22] 84.2 C4.5 in [22] 84.5 Backprop in [22] 84.6 IndCART in [22] 84.8 Naive Bayes in [22] 84.9 CASTLE in [22] 85.2 CART in [22] 85.5 RBF in [22] 85.5 DIPOL92 in [22] 85.9 Linear discr. in [22] 85.9 Logistic discr. in [22] 85.9 ITrule in [22] 86.3 Cal5 in [22] 86.9 CABINET 87.2 60 65 70 75 80 Success rate (% <p> 78.3 Quadratic discr. in <ref> [22] </ref> 79.3 CN2 in [22] 79.6 ALLOC80 in [22] 79.9 LVQ in [22] 80.3 NewID in [22] 81.9 AC2 in [22] 81.9 Bayes tree in [22] 82.9 SMART in [22] 84.2 C4.5 in [22] 84.5 Backprop in [22] 84.6 IndCART in [22] 84.8 Naive Bayes in [22] 84.9 CASTLE in [22] 85.2 CART in [22] 85.5 RBF in [22] 85.5 DIPOL92 in [22] 85.9 Linear discr. in [22] 85.9 Logistic discr. in [22] 85.9 ITrule in [22] 86.3 Cal5 in [22] 86.9 CABINET 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [22] <p> <ref> [22] </ref> 79.3 CN2 in [22] 79.6 ALLOC80 in [22] 79.9 LVQ in [22] 80.3 NewID in [22] 81.9 AC2 in [22] 81.9 Bayes tree in [22] 82.9 SMART in [22] 84.2 C4.5 in [22] 84.5 Backprop in [22] 84.6 IndCART in [22] 84.8 Naive Bayes in [22] 84.9 CASTLE in [22] 85.2 CART in [22] 85.5 RBF in [22] 85.5 DIPOL92 in [22] 85.9 Linear discr. in [22] 85.9 Logistic discr. in [22] 85.9 ITrule in [22] 86.3 Cal5 in [22] 86.9 CABINET 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [22] 67.6 ALLOC80 in [22] <p> <ref> [22] </ref> 79.6 ALLOC80 in [22] 79.9 LVQ in [22] 80.3 NewID in [22] 81.9 AC2 in [22] 81.9 Bayes tree in [22] 82.9 SMART in [22] 84.2 C4.5 in [22] 84.5 Backprop in [22] 84.6 IndCART in [22] 84.8 Naive Bayes in [22] 84.9 CASTLE in [22] 85.2 CART in [22] 85.5 RBF in [22] 85.5 DIPOL92 in [22] 85.9 Linear discr. in [22] 85.9 Logistic discr. in [22] 85.9 ITrule in [22] 86.3 Cal5 in [22] 86.9 CABINET 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [22] 67.6 ALLOC80 in [22] 69.9 CN2 in [22] <p> <ref> [22] </ref> 79.9 LVQ in [22] 80.3 NewID in [22] 81.9 AC2 in [22] 81.9 Bayes tree in [22] 82.9 SMART in [22] 84.2 C4.5 in [22] 84.5 Backprop in [22] 84.6 IndCART in [22] 84.8 Naive Bayes in [22] 84.9 CASTLE in [22] 85.2 CART in [22] 85.5 RBF in [22] 85.5 DIPOL92 in [22] 85.9 Linear discr. in [22] 85.9 Logistic discr. in [22] 85.9 ITrule in [22] 86.3 Cal5 in [22] 86.9 CABINET 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [22] 67.6 ALLOC80 in [22] 69.9 CN2 in [22] 71.1 NewID in [22] <p> <ref> [22] </ref> 80.3 NewID in [22] 81.9 AC2 in [22] 81.9 Bayes tree in [22] 82.9 SMART in [22] 84.2 C4.5 in [22] 84.5 Backprop in [22] 84.6 IndCART in [22] 84.8 Naive Bayes in [22] 84.9 CASTLE in [22] 85.2 CART in [22] 85.5 RBF in [22] 85.5 DIPOL92 in [22] 85.9 Linear discr. in [22] 85.9 Logistic discr. in [22] 85.9 ITrule in [22] 86.3 Cal5 in [22] 86.9 CABINET 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [22] 67.6 ALLOC80 in [22] 69.9 CN2 in [22] 71.1 NewID in [22] 71.1 AC2 in [22] <p> 81.9 AC2 in <ref> [22] </ref> 81.9 Bayes tree in [22] 82.9 SMART in [22] 84.2 C4.5 in [22] 84.5 Backprop in [22] 84.6 IndCART in [22] 84.8 Naive Bayes in [22] 84.9 CASTLE in [22] 85.2 CART in [22] 85.5 RBF in [22] 85.5 DIPOL92 in [22] 85.9 Linear discr. in [22] 85.9 Logistic discr. in [22] 85.9 ITrule in [22] 86.3 Cal5 in [22] 86.9 CABINET 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [22] 67.6 ALLOC80 in [22] 69.9 CN2 in [22] 71.1 NewID in [22] 71.1 AC2 in [22] 72.4 LVQ in [22] 72.8 <p> Bayes tree in <ref> [22] </ref> 82.9 SMART in [22] 84.2 C4.5 in [22] 84.5 Backprop in [22] 84.6 IndCART in [22] 84.8 Naive Bayes in [22] 84.9 CASTLE in [22] 85.2 CART in [22] 85.5 RBF in [22] 85.5 DIPOL92 in [22] 85.9 Linear discr. in [22] 85.9 Logistic discr. in [22] 85.9 ITrule in [22] 86.3 Cal5 in [22] 86.9 CABINET 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [22] 67.6 ALLOC80 in [22] 69.9 CN2 in [22] 71.1 NewID in [22] 71.1 AC2 in [22] 72.4 LVQ in [22] 72.8 Bayes tree in [22] 72.9 <p> 82.9 SMART in <ref> [22] </ref> 84.2 C4.5 in [22] 84.5 Backprop in [22] 84.6 IndCART in [22] 84.8 Naive Bayes in [22] 84.9 CASTLE in [22] 85.2 CART in [22] 85.5 RBF in [22] 85.5 DIPOL92 in [22] 85.9 Linear discr. in [22] 85.9 Logistic discr. in [22] 85.9 ITrule in [22] 86.3 Cal5 in [22] 86.9 CABINET 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [22] 67.6 ALLOC80 in [22] 69.9 CN2 in [22] 71.1 NewID in [22] 71.1 AC2 in [22] 72.4 LVQ in [22] 72.8 Bayes tree in [22] 72.9 IndCART in [22] 72.9 <p> 84.2 C4.5 in <ref> [22] </ref> 84.5 Backprop in [22] 84.6 IndCART in [22] 84.8 Naive Bayes in [22] 84.9 CASTLE in [22] 85.2 CART in [22] 85.5 RBF in [22] 85.5 DIPOL92 in [22] 85.9 Linear discr. in [22] 85.9 Logistic discr. in [22] 85.9 ITrule in [22] 86.3 Cal5 in [22] 86.9 CABINET 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [22] 67.6 ALLOC80 in [22] 69.9 CN2 in [22] 71.1 NewID in [22] 71.1 AC2 in [22] 72.4 LVQ in [22] 72.8 Bayes tree in [22] 72.9 IndCART in [22] 72.9 C4.5 in [22] 73.0 <p> <ref> [22] </ref> 84.9 CASTLE in [22] 85.2 CART in [22] 85.5 RBF in [22] 85.5 DIPOL92 in [22] 85.9 Linear discr. in [22] 85.9 Logistic discr. in [22] 85.9 ITrule in [22] 86.3 Cal5 in [22] 86.9 CABINET 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [22] 67.6 ALLOC80 in [22] 69.9 CN2 in [22] 71.1 NewID in [22] 71.1 AC2 in [22] 72.4 LVQ in [22] 72.8 Bayes tree in [22] 72.9 IndCART in [22] 72.9 C4.5 in [22] 73.0 Quadratic discr. in [22] 73.8 Flexible Bayes in [15] 73.9 CASTLE in [22] 74.2 CART in <p> <ref> [22] </ref> 85.2 CART in [22] 85.5 RBF in [22] 85.5 DIPOL92 in [22] 85.9 Linear discr. in [22] 85.9 Logistic discr. in [22] 85.9 ITrule in [22] 86.3 Cal5 in [22] 86.9 CABINET 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [22] 67.6 ALLOC80 in [22] 69.9 CN2 in [22] 71.1 NewID in [22] 71.1 AC2 in [22] 72.4 LVQ in [22] 72.8 Bayes tree in [22] 72.9 IndCART in [22] 72.9 C4.5 in [22] 73.0 Quadratic discr. in [22] 73.8 Flexible Bayes in [15] 73.9 CASTLE in [22] 74.2 CART in [22] 74.5 Cal5 in <p> <ref> [22] </ref> 85.5 RBF in [22] 85.5 DIPOL92 in [22] 85.9 Linear discr. in [22] 85.9 Logistic discr. in [22] 85.9 ITrule in [22] 86.3 Cal5 in [22] 86.9 CABINET 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [22] 67.6 ALLOC80 in [22] 69.9 CN2 in [22] 71.1 NewID in [22] 71.1 AC2 in [22] 72.4 LVQ in [22] 72.8 Bayes tree in [22] 72.9 IndCART in [22] 72.9 C4.5 in [22] 73.0 Quadratic discr. in [22] 73.8 Flexible Bayes in [15] 73.9 CASTLE in [22] 74.2 CART in [22] 74.5 Cal5 in [22] 75.0 Naive Bayes <p> <ref> [22] </ref> 85.5 DIPOL92 in [22] 85.9 Linear discr. in [22] 85.9 Logistic discr. in [22] 85.9 ITrule in [22] 86.3 Cal5 in [22] 86.9 CABINET 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [22] 67.6 ALLOC80 in [22] 69.9 CN2 in [22] 71.1 NewID in [22] 71.1 AC2 in [22] 72.4 LVQ in [22] 72.8 Bayes tree in [22] 72.9 IndCART in [22] 72.9 C4.5 in [22] 73.0 Quadratic discr. in [22] 73.8 Flexible Bayes in [15] 73.9 CASTLE in [22] 74.2 CART in [22] 74.5 Cal5 in [22] 75.0 Naive Bayes in [15] 75.1 Backprop <p> <ref> [22] </ref> 85.9 Linear discr. in [22] 85.9 Logistic discr. in [22] 85.9 ITrule in [22] 86.3 Cal5 in [22] 86.9 CABINET 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [22] 67.6 ALLOC80 in [22] 69.9 CN2 in [22] 71.1 NewID in [22] 71.1 AC2 in [22] 72.4 LVQ in [22] 72.8 Bayes tree in [22] 72.9 IndCART in [22] 72.9 C4.5 in [22] 73.0 Quadratic discr. in [22] 73.8 Flexible Bayes in [15] 73.9 CASTLE in [22] 74.2 CART in [22] 74.5 Cal5 in [22] 75.0 Naive Bayes in [15] 75.1 Backprop in [22] 75.2 ITrule <p> in <ref> [22] </ref> 85.9 Logistic discr. in [22] 85.9 ITrule in [22] 86.3 Cal5 in [22] 86.9 CABINET 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [22] 67.6 ALLOC80 in [22] 69.9 CN2 in [22] 71.1 NewID in [22] 71.1 AC2 in [22] 72.4 LVQ in [22] 72.8 Bayes tree in [22] 72.9 IndCART in [22] 72.9 C4.5 in [22] 73.0 Quadratic discr. in [22] 73.8 Flexible Bayes in [15] 73.9 CASTLE in [22] 74.2 CART in [22] 74.5 Cal5 in [22] 75.0 Naive Bayes in [15] 75.1 Backprop in [22] 75.2 ITrule in [22] 75.5 RBF <p> in <ref> [22] </ref> 85.9 ITrule in [22] 86.3 Cal5 in [22] 86.9 CABINET 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [22] 67.6 ALLOC80 in [22] 69.9 CN2 in [22] 71.1 NewID in [22] 71.1 AC2 in [22] 72.4 LVQ in [22] 72.8 Bayes tree in [22] 72.9 IndCART in [22] 72.9 C4.5 in [22] 73.0 Quadratic discr. in [22] 73.8 Flexible Bayes in [15] 73.9 CASTLE in [22] 74.2 CART in [22] 74.5 Cal5 in [22] 75.0 Naive Bayes in [15] 75.1 Backprop in [22] 75.2 ITrule in [22] 75.5 RBF in [22] 75.7 SMART in <p> in <ref> [22] </ref> 86.3 Cal5 in [22] 86.9 CABINET 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [22] 67.6 ALLOC80 in [22] 69.9 CN2 in [22] 71.1 NewID in [22] 71.1 AC2 in [22] 72.4 LVQ in [22] 72.8 Bayes tree in [22] 72.9 IndCART in [22] 72.9 C4.5 in [22] 73.0 Quadratic discr. in [22] 73.8 Flexible Bayes in [15] 73.9 CASTLE in [22] 74.2 CART in [22] 74.5 Cal5 in [22] 75.0 Naive Bayes in [15] 75.1 Backprop in [22] 75.2 ITrule in [22] 75.5 RBF in [22] 75.7 SMART in [22] 76.8 CABINET 77.3 <p> in <ref> [22] </ref> 86.9 CABINET 87.2 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in [22] 67.6 ALLOC80 in [22] 69.9 CN2 in [22] 71.1 NewID in [22] 71.1 AC2 in [22] 72.4 LVQ in [22] 72.8 Bayes tree in [22] 72.9 IndCART in [22] 72.9 C4.5 in [22] 73.0 Quadratic discr. in [22] 73.8 Flexible Bayes in [15] 73.9 CASTLE in [22] 74.2 CART in [22] 74.5 Cal5 in [22] 75.0 Naive Bayes in [15] 75.1 Backprop in [22] 75.2 ITrule in [22] 75.5 RBF in [22] 75.7 SMART in [22] 76.8 CABINET 77.3 Linear discr. in [22] <p> 60 65 70 75 80 Success rate (% correct) DIABETES K-NN in <ref> [22] </ref> 67.6 ALLOC80 in [22] 69.9 CN2 in [22] 71.1 NewID in [22] 71.1 AC2 in [22] 72.4 LVQ in [22] 72.8 Bayes tree in [22] 72.9 IndCART in [22] 72.9 C4.5 in [22] 73.0 Quadratic discr. in [22] 73.8 Flexible Bayes in [15] 73.9 CASTLE in [22] 74.2 CART in [22] 74.5 Cal5 in [22] 75.0 Naive Bayes in [15] 75.1 Backprop in [22] 75.2 ITrule in [22] 75.5 RBF in [22] 75.7 SMART in [22] 76.8 CABINET 77.3 Linear discr. in [22] 77.5 Logistic discr. in [22] <p> DIABETES K-NN in <ref> [22] </ref> 67.6 ALLOC80 in [22] 69.9 CN2 in [22] 71.1 NewID in [22] 71.1 AC2 in [22] 72.4 LVQ in [22] 72.8 Bayes tree in [22] 72.9 IndCART in [22] 72.9 C4.5 in [22] 73.0 Quadratic discr. in [22] 73.8 Flexible Bayes in [15] 73.9 CASTLE in [22] 74.2 CART in [22] 74.5 Cal5 in [22] 75.0 Naive Bayes in [15] 75.1 Backprop in [22] 75.2 ITrule in [22] 75.5 RBF in [22] 75.7 SMART in [22] 76.8 CABINET 77.3 Linear discr. in [22] 77.5 Logistic discr. in [22] 77.7 DIPOL92 in [22] 77.6 30 35 40 45 <p> 67.6 ALLOC80 in <ref> [22] </ref> 69.9 CN2 in [22] 71.1 NewID in [22] 71.1 AC2 in [22] 72.4 LVQ in [22] 72.8 Bayes tree in [22] 72.9 IndCART in [22] 72.9 C4.5 in [22] 73.0 Quadratic discr. in [22] 73.8 Flexible Bayes in [15] 73.9 CASTLE in [22] 74.2 CART in [22] 74.5 Cal5 in [22] 75.0 Naive Bayes in [15] 75.1 Backprop in [22] 75.2 ITrule in [22] 75.5 RBF in [22] 75.7 SMART in [22] 76.8 CABINET 77.3 Linear discr. in [22] 77.5 Logistic discr. in [22] 77.7 DIPOL92 in [22] 77.6 30 35 40 45 50 55 Success rate <p> 69.9 CN2 in <ref> [22] </ref> 71.1 NewID in [22] 71.1 AC2 in [22] 72.4 LVQ in [22] 72.8 Bayes tree in [22] 72.9 IndCART in [22] 72.9 C4.5 in [22] 73.0 Quadratic discr. in [22] 73.8 Flexible Bayes in [15] 73.9 CASTLE in [22] 74.2 CART in [22] 74.5 Cal5 in [22] 75.0 Naive Bayes in [15] 75.1 Backprop in [22] 75.2 ITrule in [22] 75.5 RBF in [22] 75.7 SMART in [22] 76.8 CABINET 77.3 Linear discr. in [22] 77.5 Logistic discr. in [22] 77.7 DIPOL92 in [22] 77.6 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR <p> AC2 in <ref> [22] </ref> 72.4 LVQ in [22] 72.8 Bayes tree in [22] 72.9 IndCART in [22] 72.9 C4.5 in [22] 73.0 Quadratic discr. in [22] 73.8 Flexible Bayes in [15] 73.9 CASTLE in [22] 74.2 CART in [22] 74.5 Cal5 in [22] 75.0 Naive Bayes in [15] 75.1 Backprop in [22] 75.2 ITrule in [22] 75.5 RBF in [22] 75.7 SMART in [22] 76.8 CABINET 77.3 Linear discr. in [22] 77.5 Logistic discr. in [22] 77.7 DIPOL92 in [22] 77.6 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M (3) in [3] 41.4 <p> LVQ in <ref> [22] </ref> 72.8 Bayes tree in [22] 72.9 IndCART in [22] 72.9 C4.5 in [22] 73.0 Quadratic discr. in [22] 73.8 Flexible Bayes in [15] 73.9 CASTLE in [22] 74.2 CART in [22] 74.5 Cal5 in [22] 75.0 Naive Bayes in [15] 75.1 Backprop in [22] 75.2 ITrule in [22] 75.5 RBF in [22] 75.7 SMART in [22] 76.8 CABINET 77.3 Linear discr. in [22] 77.5 Logistic discr. in [22] 77.7 DIPOL92 in [22] 77.6 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M (3) in [3] 41.4 Human expert in [19] <p> Bayes tree in <ref> [22] </ref> 72.9 IndCART in [22] 72.9 C4.5 in [22] 73.0 Quadratic discr. in [22] 73.8 Flexible Bayes in [15] 73.9 CASTLE in [22] 74.2 CART in [22] 74.5 Cal5 in [22] 75.0 Naive Bayes in [15] 75.1 Backprop in [22] 75.2 ITrule in [22] 75.5 RBF in [22] 75.7 SMART in [22] 76.8 CABINET 77.3 Linear discr. in [22] 77.5 Logistic discr. in [22] 77.7 DIPOL92 in [22] 77.6 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M (3) in [3] 41.4 Human expert in [19] 42.0 Assistant in [19] <p> 72.9 IndCART in <ref> [22] </ref> 72.9 C4.5 in [22] 73.0 Quadratic discr. in [22] 73.8 Flexible Bayes in [15] 73.9 CASTLE in [22] 74.2 CART in [22] 74.5 Cal5 in [22] 75.0 Naive Bayes in [15] 75.1 Backprop in [22] 75.2 ITrule in [22] 75.5 RBF in [22] 75.7 SMART in [22] 76.8 CABINET 77.3 Linear discr. in [22] 77.5 Logistic discr. in [22] 77.7 DIPOL92 in [22] 77.6 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M (3) in [3] 41.4 Human expert in [19] 42.0 Assistant in [19] 44.0 CABINET 50.4 Naive <p> <ref> [22] </ref> 73.0 Quadratic discr. in [22] 73.8 Flexible Bayes in [15] 73.9 CASTLE in [22] 74.2 CART in [22] 74.5 Cal5 in [22] 75.0 Naive Bayes in [15] 75.1 Backprop in [22] 75.2 ITrule in [22] 75.5 RBF in [22] 75.7 SMART in [22] 76.8 CABINET 77.3 Linear discr. in [22] 77.5 Logistic discr. in [22] 77.7 DIPOL92 in [22] 77.6 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M (3) in [3] 41.4 Human expert in [19] 42.0 Assistant in [19] 44.0 CABINET 50.4 Naive Bayes in [18] 51.0 Successive Bayes in <p> <ref> [22] </ref> 73.8 Flexible Bayes in [15] 73.9 CASTLE in [22] 74.2 CART in [22] 74.5 Cal5 in [22] 75.0 Naive Bayes in [15] 75.1 Backprop in [22] 75.2 ITrule in [22] 75.5 RBF in [22] 75.7 SMART in [22] 76.8 CABINET 77.3 Linear discr. in [22] 77.5 Logistic discr. in [22] 77.7 DIPOL92 in [22] 77.6 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M (3) in [3] 41.4 Human expert in [19] 42.0 Assistant in [19] 44.0 CABINET 50.4 Naive Bayes in [18] 51.0 Successive Bayes in [18] 51.7 75 80 85 <p> in [15] 73.9 CASTLE in <ref> [22] </ref> 74.2 CART in [22] 74.5 Cal5 in [22] 75.0 Naive Bayes in [15] 75.1 Backprop in [22] 75.2 ITrule in [22] 75.5 RBF in [22] 75.7 SMART in [22] 76.8 CABINET 77.3 Linear discr. in [22] 77.5 Logistic discr. in [22] 77.7 DIPOL92 in [22] 77.6 30 35 40 45 50 55 Success rate (% correct) PRIMARY TUMOR C4 in [1] 37.8 M (3) in [3] 41.4 Human expert in [19] 42.0 Assistant in [19] 44.0 CABINET 50.4 Naive Bayes in [18] 51.0 Successive Bayes in [18] 51.7 75 80 85 90 Success rate (% <p> 55 60 65 70 75 80 85 Success rate (% correct) GLASS Naive Bayes in [15] 42.9 1Rw in [14] 62.2 Flexible Bayes in [15] 66.2 CABINET 84.0 itis and Glass datasets 14 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in <ref> [22] </ref> 85.4 ITrule in [22] 86.5 Cal5 in [22] 86.9 SMART in [22] 88.5 NewID in [22] 90.0 AC2 in [22] 90.0 Bayes tree in [22] 90.5 Backprop in [22] 91.2 CART in [22] 91.5 C4.5 in [22] 92.4 IndCART in [22] 92.7 CASTLE in [22] 92.8 Naive Bayes in [22] <p> 75 80 85 Success rate (% correct) GLASS Naive Bayes in [15] 42.9 1Rw in [14] 62.2 Flexible Bayes in [15] 66.2 CABINET 84.0 itis and Glass datasets 14 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in <ref> [22] </ref> 85.4 ITrule in [22] 86.5 Cal5 in [22] 86.9 SMART in [22] 88.5 NewID in [22] 90.0 AC2 in [22] 90.0 Bayes tree in [22] 90.5 Backprop in [22] 91.2 CART in [22] 91.5 C4.5 in [22] 92.4 IndCART in [22] 92.7 CASTLE in [22] 92.8 Naive Bayes in [22] 93.2 Logistic discr. in <p> rate (% correct) GLASS Naive Bayes in [15] 42.9 1Rw in [14] 62.2 Flexible Bayes in [15] 66.2 CABINET 84.0 itis and Glass datasets 14 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in <ref> [22] </ref> 85.4 ITrule in [22] 86.5 Cal5 in [22] 86.9 SMART in [22] 88.5 NewID in [22] 90.0 AC2 in [22] 90.0 Bayes tree in [22] 90.5 Backprop in [22] 91.2 CART in [22] 91.5 C4.5 in [22] 92.4 IndCART in [22] 92.7 CASTLE in [22] 92.8 Naive Bayes in [22] 93.2 Logistic discr. in [22] 93.9 Quadratic discr. <p> Naive Bayes in [15] 42.9 1Rw in [14] 62.2 Flexible Bayes in [15] 66.2 CABINET 84.0 itis and Glass datasets 14 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in <ref> [22] </ref> 85.4 ITrule in [22] 86.5 Cal5 in [22] 86.9 SMART in [22] 88.5 NewID in [22] 90.0 AC2 in [22] 90.0 Bayes tree in [22] 90.5 Backprop in [22] 91.2 CART in [22] 91.5 C4.5 in [22] 92.4 IndCART in [22] 92.7 CASTLE in [22] 92.8 Naive Bayes in [22] 93.2 Logistic discr. in [22] 93.9 Quadratic discr. in [22] 94.1 Linear <p> 42.9 1Rw in [14] 62.2 Flexible Bayes in [15] 66.2 CABINET 84.0 itis and Glass datasets 14 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in <ref> [22] </ref> 85.4 ITrule in [22] 86.5 Cal5 in [22] 86.9 SMART in [22] 88.5 NewID in [22] 90.0 AC2 in [22] 90.0 Bayes tree in [22] 90.5 Backprop in [22] 91.2 CART in [22] 91.5 C4.5 in [22] 92.4 IndCART in [22] 92.7 CASTLE in [22] 92.8 Naive Bayes in [22] 93.2 Logistic discr. in [22] 93.9 Quadratic discr. in [22] 94.1 Linear discr. in [22] 94.1 <p> 62.2 Flexible Bayes in [15] 66.2 CABINET 84.0 itis and Glass datasets 14 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in <ref> [22] </ref> 85.4 ITrule in [22] 86.5 Cal5 in [22] 86.9 SMART in [22] 88.5 NewID in [22] 90.0 AC2 in [22] 90.0 Bayes tree in [22] 90.5 Backprop in [22] 91.2 CART in [22] 91.5 C4.5 in [22] 92.4 IndCART in [22] 92.7 CASTLE in [22] 92.8 Naive Bayes in [22] 93.2 Logistic discr. in [22] 93.9 Quadratic discr. in [22] 94.1 Linear discr. in [22] 94.1 ALLOC80 in [22] 94.3 <p> 66.2 CABINET 84.0 itis and Glass datasets 14 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in <ref> [22] </ref> 85.4 ITrule in [22] 86.5 Cal5 in [22] 86.9 SMART in [22] 88.5 NewID in [22] 90.0 AC2 in [22] 90.0 Bayes tree in [22] 90.5 Backprop in [22] 91.2 CART in [22] 91.5 C4.5 in [22] 92.4 IndCART in [22] 92.7 CASTLE in [22] 92.8 Naive Bayes in [22] 93.2 Logistic discr. in [22] 93.9 Quadratic discr. in [22] 94.1 Linear discr. in [22] 94.1 ALLOC80 in [22] 94.3 DIPOL92 in [22] 95.2 RBF <p> and Glass datasets 14 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in <ref> [22] </ref> 85.4 ITrule in [22] 86.5 Cal5 in [22] 86.9 SMART in [22] 88.5 NewID in [22] 90.0 AC2 in [22] 90.0 Bayes tree in [22] 90.5 Backprop in [22] 91.2 CART in [22] 91.5 C4.5 in [22] 92.4 IndCART in [22] 92.7 CASTLE in [22] 92.8 Naive Bayes in [22] 93.2 Logistic discr. in [22] 93.9 Quadratic discr. in [22] 94.1 Linear discr. in [22] 94.1 ALLOC80 in [22] 94.3 DIPOL92 in [22] 95.2 RBF in [22] 95.9 CABINET <p> 50 55 60 65 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in <ref> [22] </ref> 85.4 ITrule in [22] 86.5 Cal5 in [22] 86.9 SMART in [22] 88.5 NewID in [22] 90.0 AC2 in [22] 90.0 Bayes tree in [22] 90.5 Backprop in [22] 91.2 CART in [22] 91.5 C4.5 in [22] 92.4 IndCART in [22] 92.7 CASTLE in [22] 92.8 Naive Bayes in [22] 93.2 Logistic discr. in [22] 93.9 Quadratic discr. in [22] 94.1 Linear discr. in [22] 94.1 ALLOC80 in [22] 94.3 DIPOL92 in [22] 95.2 RBF in [22] 95.9 CABINET 97.0 50 55 60 <p> 70 75 80 85 90 95 Success rate (% correct) DNA K-NN in <ref> [22] </ref> 85.4 ITrule in [22] 86.5 Cal5 in [22] 86.9 SMART in [22] 88.5 NewID in [22] 90.0 AC2 in [22] 90.0 Bayes tree in [22] 90.5 Backprop in [22] 91.2 CART in [22] 91.5 C4.5 in [22] 92.4 IndCART in [22] 92.7 CASTLE in [22] 92.8 Naive Bayes in [22] 93.2 Logistic discr. in [22] 93.9 Quadratic discr. in [22] 94.1 Linear discr. in [22] 94.1 ALLOC80 in [22] 94.3 DIPOL92 in [22] 95.2 RBF in [22] 95.9 CABINET 97.0 50 55 60 65 70 75 80 <p> 90 95 Success rate (% correct) DNA K-NN in <ref> [22] </ref> 85.4 ITrule in [22] 86.5 Cal5 in [22] 86.9 SMART in [22] 88.5 NewID in [22] 90.0 AC2 in [22] 90.0 Bayes tree in [22] 90.5 Backprop in [22] 91.2 CART in [22] 91.5 C4.5 in [22] 92.4 IndCART in [22] 92.7 CASTLE in [22] 92.8 Naive Bayes in [22] 93.2 Logistic discr. in [22] 93.9 Quadratic discr. in [22] 94.1 Linear discr. in [22] 94.1 ALLOC80 in [22] 94.3 DIPOL92 in [22] 95.2 RBF in [22] 95.9 CABINET 97.0 50 55 60 65 70 75 80 Success rate (% correct) <p> (% correct) DNA K-NN in <ref> [22] </ref> 85.4 ITrule in [22] 86.5 Cal5 in [22] 86.9 SMART in [22] 88.5 NewID in [22] 90.0 AC2 in [22] 90.0 Bayes tree in [22] 90.5 Backprop in [22] 91.2 CART in [22] 91.5 C4.5 in [22] 92.4 IndCART in [22] 92.7 CASTLE in [22] 92.8 Naive Bayes in [22] 93.2 Logistic discr. in [22] 93.9 Quadratic discr. in [22] 94.1 Linear discr. in [22] 94.1 ALLOC80 in [22] 94.3 DIPOL92 in [22] 95.2 RBF in [22] 95.9 CABINET 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert <p> <ref> [22] </ref> 85.4 ITrule in [22] 86.5 Cal5 in [22] 86.9 SMART in [22] 88.5 NewID in [22] 90.0 AC2 in [22] 90.0 Bayes tree in [22] 90.5 Backprop in [22] 91.2 CART in [22] 91.5 C4.5 in [22] 92.4 IndCART in [22] 92.7 CASTLE in [22] 92.8 Naive Bayes in [22] 93.2 Logistic discr. in [22] 93.9 Quadratic discr. in [22] 94.1 Linear discr. in [22] 94.1 ALLOC80 in [22] 94.3 DIPOL92 in [22] 95.2 RBF in [22] 95.9 CABINET 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [19] 64.0 K-NN in <p> 86.5 Cal5 in <ref> [22] </ref> 86.9 SMART in [22] 88.5 NewID in [22] 90.0 AC2 in [22] 90.0 Bayes tree in [22] 90.5 Backprop in [22] 91.2 CART in [22] 91.5 C4.5 in [22] 92.4 IndCART in [22] 92.7 CASTLE in [22] 92.8 Naive Bayes in [22] 93.2 Logistic discr. in [22] 93.9 Quadratic discr. in [22] 94.1 Linear discr. in [22] 94.1 ALLOC80 in [22] 94.3 DIPOL92 in [22] 95.2 RBF in [22] 95.9 CABINET 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [19] 64.0 K-NN in [14] 65.3 Quadratic discr. in <p> SMART in <ref> [22] </ref> 88.5 NewID in [22] 90.0 AC2 in [22] 90.0 Bayes tree in [22] 90.5 Backprop in [22] 91.2 CART in [22] 91.5 C4.5 in [22] 92.4 IndCART in [22] 92.7 CASTLE in [22] 92.8 Naive Bayes in [22] 93.2 Logistic discr. in [22] 93.9 Quadratic discr. in [22] 94.1 Linear discr. in [22] 94.1 ALLOC80 in [22] 94.3 DIPOL92 in [22] 95.2 RBF in [22] 95.9 CABINET 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [19] 64.0 K-NN in [14] 65.3 Quadratic discr. in [14] 65.6 Backprop in [14] <p> in <ref> [22] </ref> 90.0 AC2 in [22] 90.0 Bayes tree in [22] 90.5 Backprop in [22] 91.2 CART in [22] 91.5 C4.5 in [22] 92.4 IndCART in [22] 92.7 CASTLE in [22] 92.8 Naive Bayes in [22] 93.2 Logistic discr. in [22] 93.9 Quadratic discr. in [22] 94.1 Linear discr. in [22] 94.1 ALLOC80 in [22] 94.3 DIPOL92 in [22] 95.2 RBF in [22] 95.9 CABINET 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [19] 64.0 K-NN in [14] 65.3 Quadratic discr. in [14] 65.6 Backprop in [14] 71.5 Linear discr. in [14] <p> in <ref> [22] </ref> 90.0 Bayes tree in [22] 90.5 Backprop in [22] 91.2 CART in [22] 91.5 C4.5 in [22] 92.4 IndCART in [22] 92.7 CASTLE in [22] 92.8 Naive Bayes in [22] 93.2 Logistic discr. in [22] 93.9 Quadratic discr. in [22] 94.1 Linear discr. in [22] 94.1 ALLOC80 in [22] 94.3 DIPOL92 in [22] 95.2 RBF in [22] 95.9 CABINET 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [19] 64.0 K-NN in [14] 65.3 Quadratic discr. in [14] 65.6 Backprop in [14] 71.5 Linear discr. in [14] 71.6 C4 in [14] <p> tree in <ref> [22] </ref> 90.5 Backprop in [22] 91.2 CART in [22] 91.5 C4.5 in [22] 92.4 IndCART in [22] 92.7 CASTLE in [22] 92.8 Naive Bayes in [22] 93.2 Logistic discr. in [22] 93.9 Quadratic discr. in [22] 94.1 Linear discr. in [22] 94.1 ALLOC80 in [22] 94.3 DIPOL92 in [22] 95.2 RBF in [22] 95.9 CABINET 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [19] 64.0 K-NN in [14] 65.3 Quadratic discr. in [14] 65.6 Backprop in [14] 71.5 Linear discr. in [14] 71.6 C4 in [14] 72.0 RMHC-PF1 in [31] <p> Backprop in <ref> [22] </ref> 91.2 CART in [22] 91.5 C4.5 in [22] 92.4 IndCART in [22] 92.7 CASTLE in [22] 92.8 Naive Bayes in [22] 93.2 Logistic discr. in [22] 93.9 Quadratic discr. in [22] 94.1 Linear discr. in [22] 94.1 ALLOC80 in [22] 94.3 DIPOL92 in [22] 95.2 RBF in [22] 95.9 CABINET 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [19] 64.0 K-NN in [14] 65.3 Quadratic discr. in [14] 65.6 Backprop in [14] 71.5 Linear discr. in [14] 71.6 C4 in [14] 72.0 RMHC-PF1 in [31] 72.3 1Rw in [14]
Reference: [23] <author> P. Myllymaki and H. Tirri. </author> <title> Bayesian case-based reasoning with neural networks. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 422-427, </pages> <address> San Francisco, March 1993. </address> <publisher> IEEE, </publisher> <address> Piscataway, NJ. </address>
Reference-contexts: Determining the model class In the Bayesian framework, the optimal number of mixing distributions (clusters) can be determined by evaluating the posterior probability for each model class M K 1 If massively parallel hardware is available, the computations can be made even faster since the algorithms can be parallelized easily <ref> [24, 23] </ref>. 6 given the data: P (M K jD) / P (DjM K )P (M K ); K = 1; : : : ; N; where the normalizing constant P (D) can be omitted since we only need to compare different model classes.
Reference: [24] <author> P. Myllymaki and H. Tirri. </author> <title> Massively parallel case-based reasoning with probabilistic similarity metrics. </title> <editor> In S. Wess, K.-D. Althoff, and M Richter, editors, </editor> <booktitle> Topics in Case-Based Reasoning, volume 837 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 144-154. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Determining the model class In the Bayesian framework, the optimal number of mixing distributions (clusters) can be determined by evaluating the posterior probability for each model class M K 1 If massively parallel hardware is available, the computations can be made even faster since the algorithms can be parallelized easily <ref> [24, 23] </ref>. 6 given the data: P (M K jD) / P (DjM K )P (M K ); K = 1; : : : ; N; where the normalizing constant P (D) can be omitted since we only need to compare different model classes.
Reference: [25] <author> P. Myllymaki and H. Tirri. </author> <title> Constructing computationally efficient Bayesian models via unsupervised clustering. </title> <editor> In A.Gammerman, editor, </editor> <booktitle> Probabilistic Reasoning and Bayesian Belief Networks, </booktitle> <pages> pages 237-248. </pages> <publisher> Alfred Waller Publishers, </publisher> <address> Suffolk, </address> <year> 1995. </year>
Reference-contexts: In the framework of the theory of Bayesian networks, a finite mixture model can be seen as a simple tree, where the root of the tree represents a latent (hidden) variable, corresponding to the mixing distributions, and the leaves represent the actual random variables of the problem domain (see <ref> [25] </ref>). Therefore, instead of searching over the exponentially many Bayesian network structures, we have a fixed model structure. However, in order to find good models, we have to search over the missing values of the unobserved latent variable in the dataset.
Reference: [26] <author> R.E. </author> <title> Neapolitan. Probabilistic Reasoning in Expert Systems. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, NY, </address> <year> 1990. </year>
Reference-contexts: Consequently, most existing Bayesian network systems performing probabilistic reasoning first transform a given (learned) multi-connected BN structure to a singly-connected network by variable clustering or conditioning methods, and then use the existing polynomial-time algorithms (see e.g. <ref> [27, 26] </ref>) for solving probabilistic reasoning tasks.
Reference: [27] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year> <month> 19 </month>
Reference-contexts: Consequently, most existing Bayesian network systems performing probabilistic reasoning first transform a given (learned) multi-connected BN structure to a singly-connected network by variable clustering or conditioning methods, and then use the existing polynomial-time algorithms (see e.g. <ref> [27, 26] </ref>) for solving probabilistic reasoning tasks.
Reference: [28] <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific Pub--lishing Company, </publisher> <year> 1989. </year>
Reference-contexts: Laplace's method [16]. Many information-theoretic measures, such as stochastic complexity <ref> [28] </ref>, can also be seen as approximations of this integral. However, in the experimental results presented in Section 6 we did not use the evidence for automated model class selection, since we found the public domain datasets used too small for reasonable accuracy approximations.
Reference: [29] <author> D. Roth. </author> <title> On the hardness of approximate reasoning. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <volume> volume 1, </volume> <pages> pages 613-618, </pages> <year> 1993. </year>
Reference-contexts: Secondly, even if we succeed in finding a good Bayesian network model, it is well known that probabilistic reasoning used for prediction in multi-connected Bayesian networks is an NP-hard problem <ref> [6, 7, 29] </ref>, and hence very probably not computationally feasible in general.
Reference: [30] <author> D.W. Scott. </author> <title> Multivariate Density Estimation. Theory, Practice, and Visualization. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: When compared to single parametric distribution methods and nonparametric methods (such as kernel density estimators <ref> [30] </ref>), finite mixture models have many 2 appealing properties. Firstly, in natural domains the distribution to be modeled is often so complicated that probabilistic modeling through single parametric distribution (e.g., normal, multinomial, Poisson) does not lead to good approximations.
Reference: [31] <author> D. Skalak. </author> <title> Prototype and feature selection by sampling and random mutation hill climbing algorithms. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 293-301, </pages> <year> 1994. </year>
Reference-contexts: [22] 95.2 RBF in [22] 95.9 CABINET 97.0 50 55 60 65 70 75 80 Success rate (% correct) BREAST CANCER Human expert in [19] 64.0 K-NN in [14] 65.3 Quadratic discr. in [14] 65.6 Backprop in [14] 71.5 Linear discr. in [14] 71.6 C4 in [14] 72.0 RMHC-PF1 in <ref> [31] </ref> 72.3 1Rw in [14] 72.7 CN2 in [14] 73.0 CABINET 76.6 CART in [14] 77.1 Assistant in [14] 78.0 Successive Bayes in [18] 78.4 Naive Bayes in [18] 79.2 M (12) in [3] 80.0 Success rate (% correct) IRIS MC1 in [31] 93.5 RMHC-PF1 in [31] 94.7 Flexible Bayes in <p> [14] 71.6 C4 in [14] 72.0 RMHC-PF1 in <ref> [31] </ref> 72.3 1Rw in [14] 72.7 CN2 in [14] 73.0 CABINET 76.6 CART in [14] 77.1 Assistant in [14] 78.0 Successive Bayes in [18] 78.4 Naive Bayes in [18] 79.2 M (12) in [3] 80.0 Success rate (% correct) IRIS MC1 in [31] 93.5 RMHC-PF1 in [31] 94.7 Flexible Bayes in [15] 95.3 CART in [15] 95.3 C4.5 in [15] 95.3 K-NN in [14] 96.0 Naive Bayes in [15] 96.0 Backprop. in [14] 96.7 Quadratic discr. in [14] 97.3 Linear discr. in [14] 98.0 CABINET 98.0 70 75 80 85 Success rate (% <p> [14] 72.0 RMHC-PF1 in <ref> [31] </ref> 72.3 1Rw in [14] 72.7 CN2 in [14] 73.0 CABINET 76.6 CART in [14] 77.1 Assistant in [14] 78.0 Successive Bayes in [18] 78.4 Naive Bayes in [18] 79.2 M (12) in [3] 80.0 Success rate (% correct) IRIS MC1 in [31] 93.5 RMHC-PF1 in [31] 94.7 Flexible Bayes in [15] 95.3 CART in [15] 95.3 C4.5 in [15] 95.3 K-NN in [14] 96.0 Naive Bayes in [15] 96.0 Backprop. in [14] 96.7 Quadratic discr. in [14] 97.3 Linear discr. in [14] 98.0 CABINET 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE C4 <p> Naive Bayes in [15] 96.0 Backprop. in [14] 96.7 Quadratic discr. in [14] 97.3 Linear discr. in [14] 98.0 CABINET 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE C4 in [1] 75.5 IB3 in [1] 78.0 Flexible Bayes in [15] 80.0 Backprop. in [14] 80.6 MC1 in <ref> [31] </ref> 80.7 RMHC-P in [31] 82.3 Naive Bayes in [15] 83.3 CABINET 84.8 databases 15 dataset size #attrs #classes #clusters test method default Australian 690 15 2 20 10-fold CV 56.0 Breast cancer 286 10 2 21 11-fold CV 70.3 Diabetes 768 9 2 20 12-fold CV 65.0 DNA 3186 181 <p> 96.0 Backprop. in [14] 96.7 Quadratic discr. in [14] 97.3 Linear discr. in [14] 98.0 CABINET 98.0 70 75 80 85 Success rate (% correct) HEART DISEASE C4 in [1] 75.5 IB3 in [1] 78.0 Flexible Bayes in [15] 80.0 Backprop. in [14] 80.6 MC1 in <ref> [31] </ref> 80.7 RMHC-P in [31] 82.3 Naive Bayes in [15] 83.3 CABINET 84.8 databases 15 dataset size #attrs #classes #clusters test method default Australian 690 15 2 20 10-fold CV 56.0 Breast cancer 286 10 2 21 11-fold CV 70.3 Diabetes 768 9 2 20 12-fold CV 65.0 DNA 3186 181 3 13 train&test 50.8
Reference: [32] <author> D. Titterington. </author> <title> Updating a diagnostic system using unconfirmed cases. </title> <journal> Applied Statistics, </journal> <volume> 25 </volume> <pages> 238-247, </pages> <year> 1976. </year>
Reference-contexts: The standard way to overcome this problem is to apply the Bayes theorem sequentially considering only one data vector at a time, and then approximating the resulting posterior in a suitable way. This procedure, called the Quasi-Bayes algorithm [33] or fractional updating <ref> [32] </ref>, was used in our empirical tests in Section 6. Let us first assume that the dataset consists of only one data vector ~ d 1 .
Reference: [33] <author> D.M. Titterington, A.F.M. Smith, and U.E. Makov. </author> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1985. </year> <month> 20 </month>
Reference-contexts: Therefore we adopt here an alternative approach, and restrict ourselves to a simpler, computationally efficient class of probabilistic models, the family of finite mixture models <ref> [10, 33] </ref>. By restricting ourselves to finite mixture models we gain the benefit of efficient probabilistic reasoning phase, and just have to concern ourselves with the complexity of the model construction problem. <p> Thus the instantiation space is divided into K local regions called clusters, each of which consists of the data vectors generated by the corresponding mechanism. An appropriate statistical model for the case is a finite mixture ([10], <ref> [33] </ref>). <p> The convergence is especially slow near the mode (s) of the function being maximized. Nevertheless, the convergence of EM is monotonic and the algorithm is assured to converge to a local optimum. Although the two main competitors of EM, the Method of Scoring and Newton-Rapshon (see <ref> [33] </ref>), have a quadratic convergence rate, the methods lack the nice monotonic convergence property (and may not converge at all), and are also usually more difficult to apply than EM is. <p> The standard way to overcome this problem is to apply the Bayes theorem sequentially considering only one data vector at a time, and then approximating the resulting posterior in a suitable way. This procedure, called the Quasi-Bayes algorithm <ref> [33] </ref> or fractional updating [32], was used in our empirical tests in Section 6. Let us first assume that the dataset consists of only one data vector ~ d 1 . <p> An analysis of the convergence of the Quasi-Bayes procedure and a comparison to other similar methods is given in <ref> [33] </ref>. In the M-step we have to find the mode of (16).
References-found: 33

