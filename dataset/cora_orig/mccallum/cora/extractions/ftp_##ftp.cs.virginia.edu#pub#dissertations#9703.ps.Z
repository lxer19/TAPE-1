URL: ftp://ftp.cs.virginia.edu/pub/dissertations/9703.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/dissertations/README.html
Root-URL: http://www.cs.virginia.edu
Title: Selection of Distance Metrics and Feature Subsets for k-Nearest Neighbor Classifiers  
Author: Allen L. Barker 
Degree: A Thesis Presented to the Faculty of the  In Partial Fulfillment of the Requirements for the Degree of Doctor of Philosophy Computer Science by  
Date: May 1997  
Affiliation: School of Engineering and Applied Science University of Virginia  
Abstract-found: 0
Intro-found: 1
Reference: [BBM92] <author> Frank Z. Brill, Donald E. Brown, and Worthy N. Martin. </author> <title> Fast genetic selection of features for neural network classifiers. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 3(2) </volume> <pages> 324-328, </pages> <month> March, </month> <year> 1992. </year>
Reference-contexts: In practice, * must be estimated from 7 the training samples. Some approaches to feature selection include greedy forward and backward selection [Fuk90], 0-1 integer programming with implicit enumeration [FS87], and genetic algorithms <ref> [BBM92] </ref>. 8 3 The kNN Metric Optimization Method In this section we describe a method for approximately optimizing the matrix A in the distance function Q (x; A; y) of a kNN classifier.
Reference: [Ber85] <author> James O. Berger. </author> <title> Statistical Decision Theory and Bayesian Analysis. </title> <publisher> Springer-Verlag, </publisher> <year> 1985. </year>
Reference-contexts: It is well known <ref> [Fuk90, Ber85] </ref> that the optimal decision rule, in the sense of minimizing the probability of error, 4 is given by the Bayes classifier.
Reference: [Bey81] <author> William H. Beyer, </author> <title> editor. CRC Standard Mathematical Tables. </title> <publisher> CRC Press, </publisher> <year> 1981. </year>
Reference-contexts: on the main diagonal. 16 This follows because the trace is the sum of the eigenvalues, and the eigenvalues of A are the reciprocals of the squared semiaxis lengths. 17 This follows since E x [Q (x; A; 0)] = tr [A var (x)] + Q (E [x]; A; 0) <ref> [Bey81] </ref>. 11 tr (A) fixed at 1. the quadratic form as Q (x; A=tr (A); y), though, we will simply rescale A after each iteration of our descent algorithm.
Reference: [Bro89] <author> Richard Bronson. </author> <title> Schaum's Outline of Theory and Problems of Matrix Operations. </title> <publisher> McGraw-Hill, </publisher> <year> 1989. </year>
Reference: [BT73] <author> G.E.P Box and G.C. Tiao. </author> <title> Bayesian Inference in Statistical Analysis. </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference: [CH67] <author> T. M. Cover and P. E. Hart. </author> <title> Nearest neighbor pattern classification. </title> <journal> IEEE Trans on Info. Theory, </journal> <volume> IT-13(1):21-27, </volume> <month> Jan., </month> <year> 1967. </year>
Reference-contexts: A few examples include handwritten character recognition, phoneme classification, and target-type classification. A popular and effective classifier is the nearest neighbor classifier, which assigns a feature vector the same class as the class of its "closest" neighbor in a training set <ref> [CH67] </ref>. As an example of a classification problem, suppose a researcher has created an artificial nose [Tau96]. The nose works by using the fact that different substances react differently in the presence of different odor molecules. <p> If a k-nearest neighbor density estimate is used for the class conditional densities in the Bayes decision rule, the resulting classifier is known as the volumetric kNN classifier <ref> [Fuk90, CH67] </ref>. While it is possible to use different k values for each class, we assume a single, global k value. In this case the decision rule of the volumetric kNN classifier is given by ! i 2 where n is the dimensionality of the feature vectors.
Reference: [FF84] <author> K. Fukunaga and Thomas E. Flick. </author> <title> An optimal global nearest neighbor distance metric. </title> <journal> IEEE PAMI, </journal> <volume> PAMI-6(3):314-318, </volume> <month> May, 84. </month>
Reference-contexts: The quadratic distance metric is attractive for its simplicity, its relation to the Gaussian density, and its analytical tractability. The special case of squared Euclidean distance, A = I , is often used, but does not generally represent the optimal quadratic distance metric for finite sample data <ref> [FF84] </ref>. A production version of a classifier system may be in service for many years. Therefore it can be worthwhile to expend a fair amount of effort in choosing the distance metric. <p> We consider a quadratic distance metric defined globally over all classes and all points in feature space. A method for approximating this optimal global quadratic metric is described in <ref> [FF84] </ref>. We should note that the optimal metric for density estimation, e.g. in the sense of minimizing the integrated squared error, is generally different from the optimal metric for classification. <p> All are first-nearest-neighbor algorithms, but the A matrices differ in each case. We test the A matrix produced by the Fukunaga and Flick (FF) algorithm <ref> [FF84] </ref>, the sample covariance matrix of the samples in training set Z (NNC), the inverse of this sample covariance matrix (NNIC), the identity matrix (NNI), and some A matrices produced by the smoothed gradient (SG) algorithm with several different parameter settings. The FF algorithm is discussed further in Appendix D. <p> The sampling procedure we employ to to generate a training set and an independent test set on each trial is as follows. From the probability distribution selected on a given trial, we sample a training set of 40n feature vectors, where n is the number of dimensions <ref> [FF84] </ref>. A separate test set 6 times larger than the training set is also generated. <p> The step size h is automatically set to fix the largest absolute change in any C element (before renormalization) to 0.0006 per step. 4.3.1 G-G Distributions: Two-class Gaussian In this case the "family" of distributions is actually a fixed distribution, and is the same as one described in <ref> [FF84] </ref>. It is a two-class distribution, with each class having a conditional 16 density that is Gaussian. Each Gaussian density has identity covariance matrix, and thus a spherical form. Each class has an equal weighting of 0.5. <p> The mean for one class is at the origin, while the other is set to have Euclidean distance 2.6 from the origin (here constrained to have all equal, positive components). The Bayes risk for this distribution is 0.097, and the asymptotic nearest-neighbor risk is 0.143 <ref> [FF84] </ref>. Thus, for finite sample sizes N , 0.143 is a practical lower bound on the achievable nearest-neighbor risk. In Table 1 we present some simulation results for the tested algorithms on G-G distributed data. <p> Note that the asymptotic nearest-neighbor score is 0.143 in all three of these G-G cases <ref> [FF84] </ref>. The rightmost bar charts of Figure 5 show estimated pairwise probabilities for the true mean of a tested algorithm to exceed the true mean of the FF algorithm.
Reference: [FH73] <author> K. Fukunaga and Larry D. Hostetler. </author> <title> Optimization of k-nearest-neighbor density estimates. </title> <journal> IEEE Trans. on Info. Theory, </journal> <volume> IT-19:320-326, </volume> <year> 1973. </year>
Reference-contexts: If fl i is taken as random, rather than as a realization, then the value returned by kNNDensityEstimate becomes a random variable with a mean and variance, etc. 5 While this estimate has many nice properties <ref> [LQ65, FH75, FH73] </ref>, other functional forms could be used. For example, we might alternately use a functional form like NeuralNetDensityEstimate [k; x; A; fl i ; N i ]; though a neural net would likely use a different set of parameters. <p> A method for approximating this optimal global quadratic metric is described in [FF84]. We should note that the optimal metric for density estimation, e.g. in the sense of minimizing the integrated squared error, is generally different from the optimal metric for classification. In <ref> [FH73] </ref> the optimal A matrix, in the sense of minimizing the integrated squared error, is derived for the class of densities which can be made circularly symmetric by a linear transformation. For Gaussian densities this optimal matrix is just the inverse covariance matrix of the density. <p> That is, the sample covariance matrix performs best, the inverse sample covariance matrix performs worst, and the identity matrix is in between. This is somewhat surprising, given that the inverse covariance matrix is the optimal matrix for the kNN density estimation of Gaussian densities <ref> [FH73] </ref>. One might expect that the optimal matrix for density representation would also perform well in classification. The density we are dealing with is actually a mixture of two Gaussian class-conditional densities, though. Regardless, the NNIC algorithm performs so poorly that we do not devote much discussion to it.
Reference: [FH75] <author> K. Fukunaga and Larry D. Hostetler. </author> <title> k-nearest neighbor bayes-risk estimation. </title> <journal> IEEE Trans. on Info. Theory, </journal> <volume> IT-21:285-293, </volume> <year> 1975. </year>
Reference-contexts: If fl i is taken as random, rather than as a realization, then the value returned by kNNDensityEstimate becomes a random variable with a mean and variance, etc. 5 While this estimate has many nice properties <ref> [LQ65, FH75, FH73] </ref>, other functional forms could be used. For example, we might alternately use a functional form like NeuralNetDensityEstimate [k; x; A; fl i ; N i ]; though a neural net would likely use a different set of parameters.
Reference: [FH89] <author> K. Fukunaga and Donald M. Hummels. </author> <title> Leave-one-out procedures for nonpara-metric error estimates. </title> <journal> IEEE PAMI, </journal> <volume> 11(4) </volume> <pages> 421-423, </pages> <month> April 89. </month>
Reference-contexts: A similar approach is taken in [YM91], where a NN classifier is mapped into a backpropagation network and optimized by gradient descent. The function we choose to minimize is a leave-one-out estimate of the current classifier's probability of error <ref> [FH89] </ref>. 3.1 Formulation of the Estimated Probability of Error In order to write the error estimate we first introduce some notation. We define ffi (x; C; fl) to be the decision rule of the kNN classifier constructed using matrix A = C 2 and training set fl.
Reference: [FS87] <author> Iman Foroutan and Jack Sklansky. </author> <title> Feature selection for automatic classification of non-gaussian data. </title> <journal> IEEE SMC, SMC-17(2):187-197, </journal> <volume> March/April 87. </volume>
Reference-contexts: In practice, * must be estimated from 7 the training samples. Some approaches to feature selection include greedy forward and backward selection [Fuk90], 0-1 integer programming with implicit enumeration <ref> [FS87] </ref>, and genetic algorithms [BBM92]. 8 3 The kNN Metric Optimization Method In this section we describe a method for approximately optimizing the matrix A in the distance function Q (x; A; y) of a kNN classifier.
Reference: [Fuk90] <author> K. Fukunaga. </author> <title> Introduction to Statistical Pattern Recognition. </title> <publisher> Academic Press, </publisher> <year> 1990. </year>
Reference-contexts: Even if only small gains are achieved, they are nonetheless multiplied over the service life of the classifier system. Improved nearest-neighbor metrics can also be used to obtain improved bounds on the Bayes error <ref> [Fuk90] </ref>. We show that the optimal quadratic distance metric can be approximated by a gradient descent on the parameters of the distance metric with respect to a smoothed estimate of classifier error probability. <p> is a sample drawn from an unknown probability distribution with density function p (x). 1 Because p (x) is unknown, and because we do not assume any particular functional form for either it or for the classifier decision boundaries, 2 we are describing a nonparametric (or distribution-free) classification problem 3 <ref> [Fuk90] </ref>. Using standard rules for joint and conditional densities we can write p (x) as a weighted sum (or mixture) of L different densities, one for each class. <p> It is well known <ref> [Fuk90, Ber85] </ref> that the optimal decision rule, in the sense of minimizing the probability of error, 4 is given by the Bayes classifier. <p> We write V (r; A) for the volume of the hyperellipsoidal region defined by solution vectors y for the equation 8 Q (0; A; y) 1=2 &lt; r. See Appendix A for more on quadratic forms and their geometric visualization. We now describe the k-nearest neighbor (kNN) density estimate <ref> [LQ65, Ize91, TS92, Fuk90] </ref> of class conditional density p (xj! i ) in terms of the notation just introduced. We first define a function of the sampled data and some other chosen parameters. <p> If a k-nearest neighbor density estimate is used for the class conditional densities in the Bayes decision rule, the resulting classifier is known as the volumetric kNN classifier <ref> [Fuk90, CH67] </ref>. While it is possible to use different k values for each class, we assume a single, global k value. In this case the decision rule of the volumetric kNN classifier is given by ! i 2 where n is the dimensionality of the feature vectors. <p> It is also possible to have different metrics for each class, i.e., to have a distance d i associated 10 To reduce bias it is suggested to use in practice ((k 1)=k)kNNDensityEstimate [k; x; A; fl i ; N i ], with k &gt; 1. <ref> [Fuk90] </ref> 11 Bootstrap samples can be used if this condition is known not to hold. 6 with each different class and thus with each subset fl i of the training set. We consider a quadratic distance metric defined globally over all classes and all points in feature space. <p> In terms of computation time to perform the necessary kNN calculations, good expected time algorithms [RP92] and parallel algorithms [Li91] exist. There are also methods for reducing the number of training samples without greatly changing the NN classifier's performance <ref> [Fuk90, Har68, Gat72] </ref>. Finally, we describe the feature selection problem. The feature selection problem involves selecting a subset of features (coordinate positions in sample vectors) which minimize some given criterion. This is important, for example, in cases where there is a cost associated with measuring each feature. <p> In practice, * must be estimated from 7 the training samples. Some approaches to feature selection include greedy forward and backward selection <ref> [Fuk90] </ref>, 0-1 integer programming with implicit enumeration [FS87], and genetic algorithms [BBM92]. 8 3 The kNN Metric Optimization Method In this section we describe a method for approximately optimizing the matrix A in the distance function Q (x; A; y) of a kNN classifier. <p> One way to think of the feature vectors with additive Gaussian noise is as samples from 27 Parzen density estimates with Gaussian kernels. A Parzen density estimate is essentially an equally weighted mixture of densities with means set to the given sample points <ref> [Fuk90] </ref>. So we can think of a feature vector with additive noise as a sample from a Parzen estimate of the corresponding class conditional density. <p> We do not attempt to optimize the standard deviation of the Gaussian component densities in the Parzen interpretation. We simply take the standard deviation to be 0.5. Our results could perhaps be improved if a formula for selecting an optimal value for this parameter were employed <ref> [Fuk90] </ref>. Note that this technique of adding noise to reduce overfitting is applicable to a wide variety of parameter fitting procedures in addition to the current one. For example, training neural networks with noisy patterns should in many cases help reduce overfitting.
Reference: [Gat72] <author> Geoffrey W. Gates. </author> <title> The reduced nearest neighbor rule. </title> <journal> IEEE Trans. on Info. Theory, </journal> <volume> IT-18:431-433, </volume> <month> May, </month> <year> 1972. </year>
Reference-contexts: In terms of computation time to perform the necessary kNN calculations, good expected time algorithms [RP92] and parallel algorithms [Li91] exist. There are also methods for reducing the number of training samples without greatly changing the NN classifier's performance <ref> [Fuk90, Har68, Gat72] </ref>. Finally, we describe the feature selection problem. The feature selection problem involves selecting a subset of features (coordinate positions in sample vectors) which minimize some given criterion. This is important, for example, in cases where there is a cost associated with measuring each feature.
Reference: [GS91] <author> Shlomo Geva and Joaquin Sitte. </author> <title> Adaptive nearest neighbor pattern classification. </title> <journal> IEEE Trans. on Neural Nets, </journal> <volume> 2(2) </volume> <pages> 318-322, </pages> <month> March, </month> <year> 1991. </year> <month> 40 </month>
Reference-contexts: Similar types of terms occur in learning vector quantization (LVQ) and related algorithms <ref> [KKL90, KK91, Kos91, GS91] </ref>. LVQ is essentially an adaptive nearest neighbor algorithm which sequentially modifies a set of "exemplar" points. These exemplar points are distinct from the points in the training set.
Reference: [Har68] <author> P. E. Hart. </author> <title> The condensed nearest neighbor rule. </title> <journal> IEEE Trans. on Info. Theory, </journal> <volume> IT-14:515-516, </volume> <month> May, </month> <year> 1968. </year>
Reference-contexts: In terms of computation time to perform the necessary kNN calculations, good expected time algorithms [RP92] and parallel algorithms [Li91] exist. There are also methods for reducing the number of training samples without greatly changing the NN classifier's performance <ref> [Fuk90, Har68, Gat72] </ref>. Finally, we describe the feature selection problem. The feature selection problem involves selecting a subset of features (coordinate positions in sample vectors) which minimize some given criterion. This is important, for example, in cases where there is a cost associated with measuring each feature.
Reference: [HT83] <author> Robert V. Hogg and Elliot A. Tanis. </author> <title> Probability and Statistical Inference. </title> <address> Macmil-lan, </address> <year> 1983. </year>
Reference: [HT85] <author> J. J. Hopfield and D. W. Tank. </author> <title> Neural computation of decisions in optimization problems. </title> <journal> Biol. Cybern., </journal> <volume> 52 </volume> <pages> 141-152, </pages> <year> 1985. </year>
Reference-contexts: Then by varying T we can control the steepness of the sigmoid and force it to approach a step function. The use of sigmoids in place of hard step functions is common in the neural network literature [RM86]. In the case of spin-glass models <ref> [HT85] </ref> this substitution corresponds to a mean field approximation to simulated annealing at temperature T . Rather than using this sigmoid, though, we take our sigmoid f (x) to be a 1-D Gaussian cumulative distribution.
Reference: [Ize91] <author> Alan Julian Izenman. </author> <title> Recent developments in nonparametric density estimation. </title> <journal> Journal of the American Statistical Assn., </journal> <volume> 86(413) </volume> <pages> 205-224, </pages> <month> March, 91. </month>
Reference-contexts: Thus we estimate a (conditional) density for each set of feature vectors fl i 2 fl. There are many available techniques for producing a density (or conditional density) estimate when given a training set of samples drawn from the density <ref> [Ize91] </ref>. We use a k-nearest neighbor density estimate, but before discussing k-nearest neighbor density estimation we first introduce some notation which will simplify our presentation. We define the quadratic form Q (x; A; y) = (x y) 0 A (x y), as in the introduction. <p> We write V (r; A) for the volume of the hyperellipsoidal region defined by solution vectors y for the equation 8 Q (0; A; y) 1=2 &lt; r. See Appendix A for more on quadratic forms and their geometric visualization. We now describe the k-nearest neighbor (kNN) density estimate <ref> [LQ65, Ize91, TS92, Fuk90] </ref> of class conditional density p (xj! i ) in terms of the notation just introduced. We first define a function of the sampled data and some other chosen parameters.
Reference: [KKL90] <author> Jari A. Kangas, Teuvo K. Kohonen, and Jorma T. Laaksonen. </author> <title> Variants of self-organizing maps. </title> <journal> IEEE Trans. on Neural Nets, </journal> <volume> 1(1) </volume> <pages> 93-99, </pages> <month> March, </month> <year> 1990. </year>
Reference-contexts: Similar types of terms occur in learning vector quantization (LVQ) and related algorithms <ref> [KKL90, KK91, Kos91, GS91] </ref>. LVQ is essentially an adaptive nearest neighbor algorithm which sequentially modifies a set of "exemplar" points. These exemplar points are distinct from the points in the training set.
Reference: [KK91] <author> Seong-Gon Kong and Bart Kosko. </author> <title> Differential competitive learning for centroid estimation and phoneme recognition. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 2(1) </volume> <pages> 118-124, </pages> <month> Jan., 91. </month>
Reference-contexts: Similar types of terms occur in learning vector quantization (LVQ) and related algorithms <ref> [KKL90, KK91, Kos91, GS91] </ref>. LVQ is essentially an adaptive nearest neighbor algorithm which sequentially modifies a set of "exemplar" points. These exemplar points are distinct from the points in the training set.
Reference: [Kos91] <author> Bart Kosko. </author> <title> Stochastic competitive learning. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 2(5) </volume> <pages> 522-529, </pages> <month> Sept., 91. </month>
Reference-contexts: Similar types of terms occur in learning vector quantization (LVQ) and related algorithms <ref> [KKL90, KK91, Kos91, GS91] </ref>. LVQ is essentially an adaptive nearest neighbor algorithm which sequentially modifies a set of "exemplar" points. These exemplar points are distinct from the points in the training set.
Reference: [Li91] <author> Xiaobo Li. </author> <title> Nearest neighbor classification on two types of simd machines. </title> <journal> Parallel Computing, </journal> <volume> 17 </volume> <pages> 381-407, </pages> <year> 1991. </year>
Reference-contexts: For Gaussian densities this optimal matrix is just the inverse covariance matrix of the density. In terms of computation time to perform the necessary kNN calculations, good expected time algorithms [RP92] and parallel algorithms <ref> [Li91] </ref> exist. There are also methods for reducing the number of training samples without greatly changing the NN classifier's performance [Fuk90, Har68, Gat72]. Finally, we describe the feature selection problem. The feature selection problem involves selecting a subset of features (coordinate positions in sample vectors) which minimize some given criterion.
Reference: [LQ65] <author> D. O. Loftsgaarden and C. P. Quesenberry. </author> <title> A nonparametric estimate of a mul-tivariate density function. </title> <journal> Ann. Math. Stat., </journal> <volume> 36 </volume> <pages> 1049-1051, </pages> <year> 1965. </year>
Reference-contexts: We write V (r; A) for the volume of the hyperellipsoidal region defined by solution vectors y for the equation 8 Q (0; A; y) 1=2 &lt; r. See Appendix A for more on quadratic forms and their geometric visualization. We now describe the k-nearest neighbor (kNN) density estimate <ref> [LQ65, Ize91, TS92, Fuk90] </ref> of class conditional density p (xj! i ) in terms of the notation just introduced. We first define a function of the sampled data and some other chosen parameters. <p> If fl i is taken as random, rather than as a realization, then the value returned by kNNDensityEstimate becomes a random variable with a mean and variance, etc. 5 While this estimate has many nice properties <ref> [LQ65, FH75, FH73] </ref>, other functional forms could be used. For example, we might alternately use a functional form like NeuralNetDensityEstimate [k; x; A; fl i ; N i ]; though a neural net would likely use a different set of parameters.
Reference: [RM86] <author> David E. Rumelhart and James L. McClelland. </author> <title> Parallel Distributed Processing. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Then by varying T we can control the steepness of the sigmoid and force it to approach a step function. The use of sigmoids in place of hard step functions is common in the neural network literature <ref> [RM86] </ref>. In the case of spin-glass models [HT85] this substitution corresponds to a mean field approximation to simulated annealing at temperature T . Rather than using this sigmoid, though, we take our sigmoid f (x) to be a 1-D Gaussian cumulative distribution.
Reference: [RP92] <author> V. Ramasubramanian and Kuldip K. Paliwal. </author> <title> Fast k-dimensional tree algorithms for nearest neighbor search with application to vector quantization encoding. </title> <journal> IEEE Trans. on Signal Processing, </journal> <volume> 40(3) </volume> <pages> 518-531, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: For Gaussian densities this optimal matrix is just the inverse covariance matrix of the density. In terms of computation time to perform the necessary kNN calculations, good expected time algorithms <ref> [RP92] </ref> and parallel algorithms [Li91] exist. There are also methods for reducing the number of training samples without greatly changing the NN classifier's performance [Fuk90, Har68, Gat72]. Finally, we describe the feature selection problem.
Reference: [SF81] <author> Robert D. Short and K. Fukunaga. </author> <title> The optimal distance measure for nearest neighbor classification. </title> <journal> IEEE Trans. on Info. Theory, </journal> <volume> IT-27(5):622-627, </volume> <month> Sept. 81. </month>
Reference-contexts: The horizontal ellipse is for A = diag (a 11 ; a 22 ) with a 11 &lt; a 22 &lt; 0 while the vertical ellipse is diagonal with the same eigenvalues except a 22 &lt; a 11 &lt; 0. In <ref> [SF81] </ref> a method is given for approximating optimal local nearest neighbor metrics, i.e., metrics which vary according to the point, or feature vector, to be classified. We consider global, quadratic metrics, however, which do not vary with the point to be classified.
Reference: [Tau96] <author> Gary Taubes. </author> <title> The electronic nose. </title> <journal> Discover, </journal> <volume> 17(9) </volume> <pages> 40-50, </pages> <month> Sept., </month> <year> 1996. </year>
Reference-contexts: A popular and effective classifier is the nearest neighbor classifier, which assigns a feature vector the same class as the class of its "closest" neighbor in a training set [CH67]. As an example of a classification problem, suppose a researcher has created an artificial nose <ref> [Tau96] </ref>. The nose works by using the fact that different substances react differently in the presence of different odor molecules.
Reference: [TS92] <author> George R. Terrell and David W. Scott. </author> <title> Variable kernel density estimation. </title> <journal> Annals of Statistics, </journal> <volume> 20(3) </volume> <pages> 1236-1265, </pages> <year> 1992. </year> <month> 41 </month>
Reference-contexts: We write V (r; A) for the volume of the hyperellipsoidal region defined by solution vectors y for the equation 8 Q (0; A; y) 1=2 &lt; r. See Appendix A for more on quadratic forms and their geometric visualization. We now describe the k-nearest neighbor (kNN) density estimate <ref> [LQ65, Ize91, TS92, Fuk90] </ref> of class conditional density p (xj! i ) in terms of the notation just introduced. We first define a function of the sampled data and some other chosen parameters. <p> In addition to the functional form we have presented, various other formulations of kNN density estimates exist. 10 In <ref> [TS92] </ref>, kNN estimates are shown to perform well for high dimensional feature spaces relative to fixed-kernel estimates. If a k-nearest neighbor density estimate is used for the class conditional densities in the Bayes decision rule, the resulting classifier is known as the volumetric kNN classifier [Fuk90, CH67].
Reference: [YM91] <author> Hung-Chun Yau and Michael T. Manry. </author> <title> Iterative improvement of a nearest neigh-bor classifier. </title> <booktitle> Neural Networks, </booktitle> <volume> 4 </volume> <pages> 517-524, </pages> <year> 1991. </year> <title> 42 Appendices to: Selection of Distance Metrics and Feature Subsets for k-Nearest Neighbor Classifiers Allen L. </title> <type> Barker </type>
Reference-contexts: Since we are going to optimize C by a gradient descent, we need a function of C to be minimized. The negative gradient of this function will then be the direction to move C at each step of the descent. A similar approach is taken in <ref> [YM91] </ref>, where a NN classifier is mapped into a backpropagation network and optimized by gradient descent.
References-found: 29

