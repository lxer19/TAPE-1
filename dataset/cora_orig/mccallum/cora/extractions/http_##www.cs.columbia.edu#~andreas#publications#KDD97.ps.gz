URL: http://www.cs.columbia.edu/~andreas/publications/KDD97.ps.gz
Refering-URL: http://www.cs.columbia.edu/~andreas/publications/publications.html
Root-URL: http://www.cs.columbia.edu
Email: wfang@cs.columbia.edu  pkc@cs.fit.edu  
Title: JAM: Java Agents for Meta-Learning over Distributed Databases  
Author: Salvatore Stolfo, Andreas L. Prodromidis Shelley Tselepis, Wenke Lee, Dave W. Fan fsal, andreas, sat, wenke, Philip K. Chan 
Address: New York, NY 10027  Melbourne, FL 32901  
Affiliation: Department of Computer Science Columbia University  Computer Science Florida Institute of Technology  
Abstract: In this paper, we describe the JAM system, a distributed, scalable and portable agent-based data mining system that employs a general approach to scaling data mining applications that we call meta-learning. JAM provides a set of learning programs, implemented either as JAVA applets or applications, that compute models over data stored locally at a site. JAM also provides a set of meta-learning agents for combining multiple models that were learned (perhaps) at different sites. It employs a special distribution mechanism which allows the migration of the derived models or classifier agents to other remote sites. We describe the overall architecture of the JAM system and the specific implementation currently under development at Columbia University. One of JAM's target applications is fraud and intrusion detection in financial information systems. A brief description of this learning task and JAM's applicability are also described. Interested users may download JAM from http://www.cs.columbia.edu/~sal/JAM/PROJECT. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L.; Friedman, J. H.; Olshen, R. A.; and Stone, C. J. </author> <year> 1984. </year> <title> Classification and Regression Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth. </publisher>
Reference-contexts: In prior publications we introduced a number of meta-learning techniques including arbitration, combining (Chan & Stolfo 1993) and hierarchical tree-structured meta-learning systems. Other publications have reported performance results on standard test problems and data sets with discussions of related techniques, Wolpert's stacking (Wolpert 1992), Breiman's bagging <ref> (Breiman et al. 1984) </ref> and Zhang's combining (Zhang et al. 1989) to name a few. Recently, Breiman extended his approach (Breiman 1997) to a technique called "pasting bites" that appears to be essentially the same as the meta-learning technique explored here with JAM. <p> Agents JAM's extensible plug-and-play architecture allows snap-in learning agents. The learning and meta-learning agents are designed as objects. JAM provides the definition of the parent agent class and every instance agent (i.e. a program that implements any of your favorite learning algorithms ID3, Ripper, Cart <ref> (Breiman et al. 1984) </ref>, Bayes (Duda & Hart 1973), Wpebls (Cost & Salzberg 1993), CN2 (Clark & Niblett 1989), etc.) are then defined as a subclass of this parent class.
Reference: <author> Breiman, L. </author> <year> 1997. </year> <title> Pasting bites together for prediction in large data sets and on-line. </title> <institution> Statistics Dept., Univ. of California, Berkeley. </institution>
Reference-contexts: Other publications have reported performance results on standard test problems and data sets with discussions of related techniques, Wolpert's stacking (Wolpert 1992), Breiman's bagging (Breiman et al. 1984) and Zhang's combining (Zhang et al. 1989) to name a few. Recently, Breiman extended his approach <ref> (Breiman 1997) </ref> to a technique called "pasting bites" that appears to be essentially the same as the meta-learning technique explored here with JAM. The JAM system architecture is designed to support these and perhaps other approaches to distributed data mining.
Reference: <author> Chan, P., and Stolfo, S. </author> <year> 1993. </year> <title> Toward parallel and distributed learning by meta-learning. </title> <booktitle> In Working Notes AAAI Work. Knowledge Discovery in Databases, </booktitle> <pages> 227-240. </pages>
Reference-contexts: Our meta-learning approach is intended to be scalable as well as portable and extensible. In prior publications we introduced a number of meta-learning techniques including arbitration, combining <ref> (Chan & Stolfo 1993) </ref> and hierarchical tree-structured meta-learning systems. Other publications have reported performance results on standard test problems and data sets with discussions of related techniques, Wolpert's stacking (Wolpert 1992), Breiman's bagging (Breiman et al. 1984) and Zhang's combining (Zhang et al. 1989) to name a few.
Reference: <author> Chan, P., and Stolfo, S. </author> <year> 1995a. </year> <title> A comparative evaluation of voting and meta-learning on partitioned data. </title> <booktitle> In Proc. Twelfth Intl. Conf. Machine Learning, </booktitle> <pages> 90-98. </pages>
Reference: <author> Chan, P., and Stolfo, S. </author> <year> 1995b. </year> <title> Learning arbiter and combiner trees from partitioned data for scaling machine learning. </title> <booktitle> In Proc. Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <pages> 39-44. </pages>
Reference-contexts: We believe meta-learning systems deployed as intelligent agents will be an important contributing technology to deploy intrusion detection facilities in global-scale, integrated information systems. 8 The section detailing the meta-learning strategies in <ref> (Chan & Stolfo 1995b) </ref> describes the various empirically determined bounds placed on the meta-training data sets while still producing accurate meta-classifiers. classifiers.
Reference: <author> Chan, P. </author> <year> 1996. </year> <title> An Extensible Meta-Learning Approach for Scalable and Accurate Inductive Learning. </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Science, Columbia University, </institution> <address> New York, NY. </address>
Reference-contexts: Other parameters include the host of the CFM, the Cross-Validation Fold, the Meta-Learning Fold, the Meta-Learning Level, the names of the local learning agent and the local meta-learning agent, etc. (Refer to <ref> (Chan 1996) </ref> for more information on the meaning and use of these parameters.) Notice that Marmalade has established that Strawberry and Mango are its peer Data-sites, having acquired this information from the CFM.
Reference: <author> Clark, P., and Niblett, T. </author> <year> 1989. </year> <title> The CN2 induction algorithm. </title> <booktitle> Machine Learning 3 </booktitle> <pages> 261-285. </pages>
Reference-contexts: JAM provides the definition of the parent agent class and every instance agent (i.e. a program that implements any of your favorite learning algorithms ID3, Ripper, Cart (Breiman et al. 1984), Bayes (Duda & Hart 1973), Wpebls (Cost & Salzberg 1993), CN2 <ref> (Clark & Niblett 1989) </ref>, etc.) are then defined as a subclass of this parent class. Among other definitions which are inherited by all agent subclasses, the parent agent class provides a very simple and minimal interface that all subclasses have to comply to.
Reference: <author> Cohen, W. W. </author> <year> 1995. </year> <title> Fast effective rule induction. </title> <booktitle> In Proc. Twelfth Intl. </booktitle> <address> Conf.. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Some machine learning algorithms generate concise and readable textual outputs, e.g., the rule sets from Ripper <ref> (Cohen 1995) </ref>. In such cases, JAM simply pretty formats the text output and displays it in the classifier visualization panel.
Reference: <author> Cost, S., and Salzberg, S. </author> <year> 1993. </year> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <booktitle> Machine Learning 10 </booktitle> <pages> 57-78. </pages>
Reference-contexts: The learning and meta-learning agents are designed as objects. JAM provides the definition of the parent agent class and every instance agent (i.e. a program that implements any of your favorite learning algorithms ID3, Ripper, Cart (Breiman et al. 1984), Bayes (Duda & Hart 1973), Wpebls <ref> (Cost & Salzberg 1993) </ref>, CN2 (Clark & Niblett 1989), etc.) are then defined as a subclass of this parent class. Among other definitions which are inherited by all agent subclasses, the parent agent class provides a very simple and minimal interface that all subclasses have to comply to.
Reference: <author> Duda, R., and Hart, P. </author> <year> 1973. </year> <title> Pattern classification and scene analysis. </title> <address> New York, NY: </address> <publisher> Wiley. </publisher>
Reference-contexts: The learning and meta-learning agents are designed as objects. JAM provides the definition of the parent agent class and every instance agent (i.e. a program that implements any of your favorite learning algorithms ID3, Ripper, Cart (Breiman et al. 1984), Bayes <ref> (Duda & Hart 1973) </ref>, Wpebls (Cost & Salzberg 1993), CN2 (Clark & Niblett 1989), etc.) are then defined as a subclass of this parent class.
Reference: <author> Fayyad, U.; Piatetsky-Shapiro, G.; and Smyth, P. </author> <year> 1996. </year> <title> The kdd process for extracting useful knowledge from data. </title> <journal> Comm. of the ACM 39(11) </journal> <pages> 27-34. </pages>
Reference-contexts: Figure 2 displays a snapshot of the system during the animated meta-learning process where JAM's GUI moves icons within the panel displaying the construction of a new meta-classifier. Classifier Visualization JAM provides graph drawing tools to help users understand the learned knowledge <ref> (Fayyad, Piatetsky-Shapiro, & Smyth 1996) </ref>. There are many kinds of classifiers, e.g., a decision tree by ID3, that can be represented as graphs.
Reference: <author> Lee, W.; Barghouti, N. S.; and Moccenigo, J. </author> <year> 1997. </year> <title> Grappa: Graph package in java. In Graph Drawing, </title> <address> Rome, Italy. </address>
Reference-contexts: Classifier Visualization JAM provides graph drawing tools to help users understand the learned knowledge (Fayyad, Piatetsky-Shapiro, & Smyth 1996). There are many kinds of classifiers, e.g., a decision tree by ID3, that can be represented as graphs. In JAM we have employed major components of JavaDot <ref> (Lee, Barghouti, & Moccenigo 1997) </ref>, an extensible visualization system, to display the classifier and allows the user to analyze the graph. 4 For each connection, the Datasite spawns a separate thread.
Reference: <author> Merz, C., and Murphy, P. </author> <year> 1996. </year> <note> UCI repository of machine learning databases [http://www.ics.uci.edu/~mlearn/mlrepository.html]. Dept. </note> <institution> of Info. and Comp., Univ. of California, Irvine. </institution>
Reference-contexts: The snapshot taken is from "Marmalade's point of view". Initially, Marmalade consults the Dat-asite configuration file where the owner of the Dat-asite sets the parameters. In this case, the dataset is a medical database with records <ref> (Merz & Murphy 1996) </ref>, noted by thyroid in the Data Set panel.
Reference: <author> Quinlan, J. R. </author> <year> 1986. </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning 1 </booktitle> <pages> 81-106. </pages>
Reference-contexts: Then, Marmalade partitions the thyroid database (noted as thyroid.1.bld and thyroid.2.bld in the Data Set panel) for the 2-Cross-Validation Fold and computes the local classifier, noted by Marmalade.1 (here by calling the ID3 <ref> (Quinlan 1986) </ref> learning agent). Next, Marmalade imports the remote classifiers, noted by Strawberry.1 and Mango.1 and begins the meta-learning process. Marmalade employs this meta-classifier to predict the classes of input data items (in this case unlabelled medical records).
Reference: <author> Stolfo, S.; Fan, W.; Lee, W.; Prodromidis, A.; and Chan, P. </author> <year> 1997. </year> <title> Credit card fraud detection using meta-learning: Issues and initial results. AAAI Workshop on AI Approaches to Fraud Detection and Risk Management. </title>
Reference-contexts: In the first setting, Bayes combined the three base classifiers with the least correlated error and in the second it combined the four most accurate base classifiers. The experiments, settings, rationale and results have been reported in detail in <ref> (Stolfo et al. 1997) </ref> also available from http://www.cs.columbia.edu/~sal/JAM/PROJECT. Conclusions We believe the concepts embodied by the term meta-learning provide an important step in developing systems that learn from massive databases and that scale.
Reference: <author> Wolpert, D. </author> <year> 1992. </year> <title> Stacked generalization. </title> <booktitle> Neural Networks 5 </booktitle> <pages> 241-259. </pages>
Reference-contexts: In prior publications we introduced a number of meta-learning techniques including arbitration, combining (Chan & Stolfo 1993) and hierarchical tree-structured meta-learning systems. Other publications have reported performance results on standard test problems and data sets with discussions of related techniques, Wolpert's stacking <ref> (Wolpert 1992) </ref>, Breiman's bagging (Breiman et al. 1984) and Zhang's combining (Zhang et al. 1989) to name a few. Recently, Breiman extended his approach (Breiman 1997) to a technique called "pasting bites" that appears to be essentially the same as the meta-learning technique explored here with JAM.
Reference: <author> Zhang, X.; Mckenna, M.; Mesirov, J.; and Waltz, D. </author> <year> 1989. </year> <title> An efficient implementation of the backpropagation algorithm on the connection machine CM-2. </title> <type> Technical Report RL89-1, </type> <institution> Thinking Machines Corp. </institution>
Reference-contexts: Other publications have reported performance results on standard test problems and data sets with discussions of related techniques, Wolpert's stacking (Wolpert 1992), Breiman's bagging (Breiman et al. 1984) and Zhang's combining <ref> (Zhang et al. 1989) </ref> to name a few. Recently, Breiman extended his approach (Breiman 1997) to a technique called "pasting bites" that appears to be essentially the same as the meta-learning technique explored here with JAM.
References-found: 17

