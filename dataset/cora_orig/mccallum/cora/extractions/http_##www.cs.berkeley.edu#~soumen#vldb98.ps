URL: http://www.cs.berkeley.edu/~soumen/vldb98.ps
Refering-URL: http://www.cs.berkeley.edu/~soumen/pub.html
Root-URL: http://www.cs.berkeley.edu/~soumen/pub.html
Email: fsoumen,sunita,domg@almaden.ibm.com  
Title: Mining surprising patterns using temporal description length  
Author: Soumen Chakrabarti Sunita Sarawagi Byron Dom 
Address: 650 Harry Road, San Jose, CA 95120  
Affiliation: IBM Almaden Research Center  
Abstract: We propose a new notion of surprising temporal patterns in market basket data, and algorithms to find such patterns. This is distinct from finding frequent patterns as addressed in the common mining literature. We argue that once the analyst is already familiar with prevalent patterns in the data, the greatest incremental benefit is likely to be from changes in the relationship between item frequencies over time. A simple measure of surprise is the extent of departure from a model, estimated using standard multivariate time series analysis. Unfortunately, such estimation involves models, smoothing windows and parameters whose optimal choices can vary dramatically from one application to another. In contrast, we propose a precise characterization of surprise based on the number of bits in which a basket sequence can be encoded under a carefully chosen coding scheme. In this scheme it is inexpensive to encode sequences of itemsets that have steady, hence likely to be well-known, correlation between items. Conversely, a sequence with large code length hints at a possibly surprising correlation. Our notion of surprise also has the desirable property that the score of a set of items is offset by anything surprising that the user may already know from the marginal distribution of any proper subset. No parameters, such as support, confidence, or smoothing windows, need to be estimated or specified by the user. We experimented with real-life market basket data. The algorithm successfully rejected a large number of frequent sets of items that bore obvious and steady complementary relations to each other, such as cereal and milk. Instead, our algorithm found itemsets that showed statistically strong fluctuations in correlation over time. These items had no obviously complementary roles. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Database mining: A performance perspective. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(6) </volume> <pages> 914-925, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: We concentrate on the problem of boolean market basket data <ref> [1, 2] </ref>. A set of k items is declared as "interesting" not necessarily because its absolute support exceeds a user-defined threshold, but because the relationship between the items changes over time.
Reference: [2] <author> R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A. I. Verkamo. </author> <title> Fast Discovery of Association Rules. </title> <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, chapter 12, </booktitle> <pages> pages 307-328. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: We concentrate on the problem of boolean market basket data <ref> [1, 2] </ref>. A set of k items is declared as "interesting" not necessarily because its absolute support exceeds a user-defined threshold, but because the relationship between the items changes over time. <p> This property, like the absolute support based pruning is also monotonic, meaning if an itemset is pruned all its supersets must also be pruned. Therefore, we can apply the "apriori" technique <ref> [2] </ref> of pruning any itemsets with at least one subsets already pruned, during the candidate generation phase. In addition, we can also prune by looking at the estimated upper envelope on the support sequence of an itemset.
Reference: [3] <author> R. Agrawal and G. Psaila. </author> <title> Active data mining. </title> <booktitle> In Proc. of the 1st Int'l Conference on Knowledge Discovery in Databases and Data Mining, </booktitle> <address> Montreal, Canada, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Data Mining. The issue of efficiently updating mining results incrementally is relatively well-studied <ref> [9, 24, 3] </ref> in the data mining literature. A few recent papers have also addressed the issue of discovering interesting patterns along time for market basket data.
Reference: [4] <author> R. Agrawal, G. Psaila, E. L. Wimmers, and M. Zait. </author> <title> Querying shapes of histories. </title> <booktitle> In Proc. of the 21st Int'l Conference on Very Large Databases, </booktitle> <address> Zurich, Switzerland, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: Their methodology is to first partition the data into a fixed number of segments, find support in each of these segments and then provide a query interface for the resulting timeseries as discussed in <ref> [4] </ref>; querying based on shapes of the time series can be used as a good user interface in front of our system. 8 Conclusion and future work We have proposed and explored a new approach to extracting temporally surprising patterns, as against just prevalent patterns, from market basket databases.
Reference: [5] <author> T. W. Anderson. </author> <title> The statistical analysis of time series. </title> <publisher> John Wiley & Sons, Inc, </publisher> <year> 1971. </year>
Reference-contexts: An approximation that may be valid in one application may behave poorly in another setting. For example, we already saw in Section 6 how different window sizes can give different interest rankings. Such issues are echoed even in textbooks on the subject <ref> [5, Page 54] </ref>: It is difficult to formulate [smoothing] and give a mathematical statistical solution. The practitioner, thus, must proceed on the basis of general experience and intuition : : : Smoothing leads to an estimated trend that is descriptive rather than analytic or explanatory.
Reference: [6] <author> Y. Bishop, S. Fienberg, and P. Holland. </author> <title> Discrete Multivariate Analysis theory and practice. </title> <publisher> The MIT Press, </publisher> <year> 1975. </year>
Reference-contexts: The problem case is when all three-way atomic probabilities are correlated. In this case, there is no explicit formula for computing the expected support ^p 111 from the observed marginals. But there are simple iterative procedures <ref> [6, 8] </ref> that converge to the maximum likelihood (ML) estimate for ^p 111 . The iteration can be used even in cases where direct formulas exist; the iterative process will yield (in one iteration) the same answer as the closed form formulas when they exist [6, page 83]. <p> The iteration can be used even in cases where direct formulas exist; the iterative process will yield (in one iteration) the same answer as the closed form formulas when they exist <ref> [6, page 83] </ref>. We describe a classical algorithm called Bartlett's method for finding the probability of a k itemset given marginal probability of its subsets. Bartlett's iterative procedure: For simplicity we discuss this process for three dimensions.
Reference: [7] <author> A. Blum and P. Chalasani. </author> <title> Learning switching concepts. </title> <booktitle> In Proc. fifth annual workshop on cmputational learning theory, </booktitle> <year> 1992. </year>
Reference-contexts: Similar segmentation problems have been addressed by Dom [11] in the context of image seg Page 11 mentation, Rissanen and Shedler [20] in the context of identifying stretches of production or short-lived items in a factory, and Ron and Freund [13] and Blum and Chalasani <ref> [7] </ref> in the context of learning from a set of distributions. Most of the proposed algorithms are worse than quadratic, and none deal with identifying segments based only on the drift of the relationship between variables, i.e., potentially ignoring drifts that are well-explained by drifts in the marginals. Data Mining.
Reference: [8] <author> B.S.Everitt. </author> <title> The analysis of contingency tables. Monographs on statistics and applied probability 45. </title> <publisher> Chapman & Hall, </publisher> <address> second edition, </address> <year> 1992. </year>
Reference-contexts: The problem case is when all three-way atomic probabilities are correlated. In this case, there is no explicit formula for computing the expected support ^p 111 from the observed marginals. But there are simple iterative procedures <ref> [6, 8] </ref> that converge to the maximum likelihood (ML) estimate for ^p 111 . The iteration can be used even in cases where direct formulas exist; the iterative process will yield (in one iteration) the same answer as the closed form formulas when they exist [6, page 83].
Reference: [9] <author> D. Cheung, J. Han, V. Ng, and C. Wong. </author> <title> Maintenance of discovered association rules in large databases: An incremental updating techniques. </title> <booktitle> In Proc. of 1996 Int'l Conference on Data Engineering, </booktitle> <address> New Orleans, USA, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: Data Mining. The issue of efficiently updating mining results incrementally is relatively well-studied <ref> [9, 24, 3] </ref> in the data mining literature. A few recent papers have also addressed the issue of discovering interesting patterns along time for market basket data. <p> Incremental mining: The attention to time provides a very natural framework for doing incremental mining. Rather than fold new transactions into global estimates of support <ref> [9] </ref>, one can maintain incremental shortest paths and integrate the new segment of data into the existing segmentation. Acknowledgement. Thanks to Rakesh Agrawal, Trang Nguyen, and Ramakrishnan Srikant for helpful discussions and Martin van den Berg for comments on the manuscript.
Reference: [10] <author> T. M. Cover and J. A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1991. </year>
Reference-contexts: Because R knows M , data compression may be greatly enhanced, i.e., L (~xjM ) may be much smaller than L (~x). Specifically, suppose Pr [~xjM ] is the probability of a specific data value ~x given the model M . Then Shannon's classical Information Theorem <ref> [10] </ref> states that the data can be encoded in L (~xjM ) = log Pr [~xjM ] bits. Note that S and R are motivated to pick a coding scheme that minimizes L (M ) + L (~xjM ).
Reference: [11] <author> B. Dom. </author> <title> MDL estimation with small sample sizes including an application to the problem of segmenting binary strings using Bernoulli models. </title> <booktitle> In International Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Computer Society, </booktitle> <month> June </month> <year> 1997. </year> <note> longer version: IBM Research Report RJ 9997 (89085). </note>
Reference-contexts: A system which needs no tuning is closer to the needs of mining systems that must deal with diverse data. Machine Learning. Our segmentation problem is in some sense a one-dimensional unsupervised clustering scenario. Similar segmentation problems have been addressed by Dom <ref> [11] </ref> in the context of image seg Page 11 mentation, Rissanen and Shedler [20] in the context of identifying stretches of production or short-lived items in a factory, and Ron and Freund [13] and Blum and Chalasani [7] in the context of learning from a set of distributions.
Reference: [12] <author> U. M. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. </author> <title> From data mining to knowledge discover: and overview. </title> <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthuruswamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: There may be patterns of both types that are statistically significant. There is broad consensus [15, 21, 22, 23] that the success of data mining will depend critically on the ability to go beyond obvious patterns and find novel and useful patterns <ref> [12] </ref>. Otherwise the results of mining will often be large and lack novelty, making it overwhelming and unrewarding for the analyst to sieve through them. <p> Part of the paper will describe techniques to greatly reduce the computation cost, but the complexity of the algorithms remains. We believe this stand is justified in view of the aforesaid urgency for mining tools to ignore mundane rules and discover novel ones <ref> [12, 15, 21, 22, 23] </ref>. Otherwise, the time saved in computation may well be spent by analysts discarding rules by inspection! 1.3 Organization of the paper In x2 we review Information Theory basics and describe our model for sequences of market baskets.
Reference: [13] <author> Y. Freund and D. Ron. </author> <title> Learning to model sequences generated by switching distributions. </title> <booktitle> In Proceedings of the Eighth Annual ACM Conference on Computational Learning Theory (COLT), </booktitle> <year> 1995. </year>
Reference-contexts: Similar segmentation problems have been addressed by Dom [11] in the context of image seg Page 11 mentation, Rissanen and Shedler [20] in the context of identifying stretches of production or short-lived items in a factory, and Ron and Freund <ref> [13] </ref> and Blum and Chalasani [7] in the context of learning from a set of distributions.
Reference: [14] <author> P. E. Gill, W. Murray, and M. H. Wright. </author> <title> Practical Optimization. </title> <publisher> Academic Press, </publisher> <year> 1981. </year>
Reference-contexts: following constrained nonlinear optimization problem over unknowns p r;s (all other quantities are numerically known): max r;s2f0;1g h r;s log p r;s subject to ae P p 11 = (p 10 + p 11 )(p 01 + p 11 ): This can be solved using a variety of iterative techniques <ref> [14] </ref>. We use a simple steepest ascent algorithm. Finally we turn to the question of the optimal way of encoding the parameters. To do this we produce a particular coding scheme and argue that nothing can be better. Here is the scheme, we omit the argument for lack of space.
Reference: [15] <author> M. Klemettinen, H. Mannila, P. Ronkainen, H. Toivonen, and A. I. Verkamo. </author> <title> Finding interesting rules from large sets of discovered association rules. </title> <booktitle> In Third International Conference on Information and Knowledge Management, </booktitle> <pages> pages 401-407, </pages> <year> 1994. </year>
Reference-contexts: Proceedings of the 24th VLDB Conference New York, USA, 1998 and surprising patterns that are novel, unexpected and non-trivial to explain. There may be patterns of both types that are statistically significant. There is broad consensus <ref> [15, 21, 22, 23] </ref> that the success of data mining will depend critically on the ability to go beyond obvious patterns and find novel and useful patterns [12]. <p> Part of the paper will describe techniques to greatly reduce the computation cost, but the complexity of the algorithms remains. We believe this stand is justified in view of the aforesaid urgency for mining tools to ignore mundane rules and discover novel ones <ref> [12, 15, 21, 22, 23] </ref>. Otherwise, the time saved in computation may well be spent by analysts discarding rules by inspection! 1.3 Organization of the paper In x2 we review Information Theory basics and describe our model for sequences of market baskets.
Reference: [16] <author> P.-S. </author> <title> Laplace. Philosophical Essays on Probabilities. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1995. </year> <title> Translated by A. I. Dale from the 5th French edition of 1825. </title>
Reference-contexts: Each edge of the shortest path is a segment in the optimal segmentation. The capability to find the exact optimum is important as a baseline even if the computation takes more 1 To avoid problems with parameters approaching zero or one we use Laplace's rule <ref> [16] </ref>, but we ignore this detail in our de scription. Page 3 than linear time.
Reference: [17] <author> B. Lent, R. Agrawal, and R. Srikant. </author> <title> Discovering Trends in Text Databases. </title> <booktitle> In Proc. of the 3rd Int'l Conference on Knowledge Discovery in Databases and Data Mining, </booktitle> <address> New-port Beach, California, </address> <month> August </month> <year> 1997. </year>
Reference-contexts: A few recent papers have also addressed the issue of discovering interesting patterns along time for market basket data. Oz-den et al present an algorithm for discovering "cyclic" associations rules provided the user specifies period (s) and segment size (s) of interest [18]. Lent et al. in <ref> [17] </ref> discuss how a plot of support versus time for frequent itemsets can be queried to find interesting trends along time.
Reference: [18] <author> B. Ozden, S. Ramaswamy, and A. Silberschatz. </author> <title> Cyclic association rules. </title> <booktitle> In Proc. Int'l Conference on Data Engineering, </booktitle> <year> 1998. </year>
Reference-contexts: A few recent papers have also addressed the issue of discovering interesting patterns along time for market basket data. Oz-den et al present an algorithm for discovering "cyclic" associations rules provided the user specifies period (s) and segment size (s) of interest <ref> [18] </ref>. Lent et al. in [17] discuss how a plot of support versus time for frequent itemsets can be queried to find interesting trends along time.
Reference: [19] <author> J. Rissanen. </author> <title> Stochastic complexity in statistical inquiry. </title> <booktitle> World scientific seies in computer science, </booktitle> <volume> 15, </volume> <year> 1989. </year>
Reference-contexts: In this scheme it takes relatively few bits to encode sequences of item-sets that have steady correlation between items (which are likely to be well-known). Conversely, a sequence with large code length (relative to a baseline unconstrained coding scheme) hints at a possibly surprising correlation <ref> [19] </ref>. The surprise value of the itemset is related to the difference or ratio between the constrained and unconstrained code lengths. <p> Note that S and R are motivated to pick a coding scheme that minimizes L (M ) + L (~xjM ). The initial choice of the parameters values of M are usually made to maximize Pr [~xjM]. The classical Minimum Description Length (MDL) principle <ref> [19] </ref> argues that this will generally lead to models that capture exactly the regularities in the data and avoid over-fitting to random deviations. Example 1: Suppose we are given two bit strings of identical length and asked to identify the "more complex" one, without any further information.
Reference: [20] <author> J. Rissanen and G. Shedler. </author> <title> Failure-time prediction. </title> <type> Technical Report RJ 9745, </type> <institution> IBM Research Division, Almaden Research Center, </institution> <address> 650 Harry Road, San Jose CA 95120-6099, </address> <year> 1994. </year>
Reference-contexts: Machine Learning. Our segmentation problem is in some sense a one-dimensional unsupervised clustering scenario. Similar segmentation problems have been addressed by Dom [11] in the context of image seg Page 11 mentation, Rissanen and Shedler <ref> [20] </ref> in the context of identifying stretches of production or short-lived items in a factory, and Ron and Freund [13] and Blum and Chalasani [7] in the context of learning from a set of distributions.
Reference: [21] <author> A. Silberschatz and A. Tuzhilin. </author> <title> What makes patterns interesting in knowledge discovery systems. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(6) </volume> <pages> 970-974, </pages> <year> 1996. </year> <note> Special issue on Data Mining. </note>
Reference-contexts: Proceedings of the 24th VLDB Conference New York, USA, 1998 and surprising patterns that are novel, unexpected and non-trivial to explain. There may be patterns of both types that are statistically significant. There is broad consensus <ref> [15, 21, 22, 23] </ref> that the success of data mining will depend critically on the ability to go beyond obvious patterns and find novel and useful patterns [12]. <p> Part of the paper will describe techniques to greatly reduce the computation cost, but the complexity of the algorithms remains. We believe this stand is justified in view of the aforesaid urgency for mining tools to ignore mundane rules and discover novel ones <ref> [12, 15, 21, 22, 23] </ref>. Otherwise, the time saved in computation may well be spent by analysts discarding rules by inspection! 1.3 Organization of the paper In x2 we review Information Theory basics and describe our model for sequences of market baskets.
Reference: [22] <author> C. Silverstein, R. Motwani, and S. Brin. </author> <title> Beyond market baskets: Generalizing association rules to correlations. </title> <booktitle> In SIG-MOD, </booktitle> <year> 1997. </year>
Reference-contexts: Proceedings of the 24th VLDB Conference New York, USA, 1998 and surprising patterns that are novel, unexpected and non-trivial to explain. There may be patterns of both types that are statistically significant. There is broad consensus <ref> [15, 21, 22, 23] </ref> that the success of data mining will depend critically on the ability to go beyond obvious patterns and find novel and useful patterns [12]. <p> Part of the paper will describe techniques to greatly reduce the computation cost, but the complexity of the algorithms remains. We believe this stand is justified in view of the aforesaid urgency for mining tools to ignore mundane rules and discover novel ones <ref> [12, 15, 21, 22, 23] </ref>. Otherwise, the time saved in computation may well be spent by analysts discarding rules by inspection! 1.3 Organization of the paper In x2 we review Information Theory basics and describe our model for sequences of market baskets. <p> Similar concerns have been raised by Brin and others <ref> [22] </ref>. * In order to produce a ranking by surprise measure, the measure should reflect, in a uniform way across different itemsets with diverse absolute support, the complexity of variation of correlation along time. <p> For instance, for three-itemsets, if all two-way marginals are dependent we use the iterative procedure discussed earlier to calculate expected support. The above notion generalizes previous work on estimating the expected value by Brin et al <ref> [22] </ref> that make the simplifying assumption that three-way item-sets are found interesting only when none of the three two-way marginals were dependent. 3.2.1 The single segmentation Recall from x2.1 our scheme of compressing the data w.r.t. a model M chosen from a certain model class.
Reference: [23] <author> C. Stedman. </author> <title> Data mining for fool's gold. </title> <journal> Computerworld, </journal> <volume> 31(48), </volume> <month> Dec. </month> <year> 1997. </year>
Reference-contexts: Proceedings of the 24th VLDB Conference New York, USA, 1998 and surprising patterns that are novel, unexpected and non-trivial to explain. There may be patterns of both types that are statistically significant. There is broad consensus <ref> [15, 21, 22, 23] </ref> that the success of data mining will depend critically on the ability to go beyond obvious patterns and find novel and useful patterns [12]. <p> Part of the paper will describe techniques to greatly reduce the computation cost, but the complexity of the algorithms remains. We believe this stand is justified in view of the aforesaid urgency for mining tools to ignore mundane rules and discover novel ones <ref> [12, 15, 21, 22, 23] </ref>. Otherwise, the time saved in computation may well be spent by analysts discarding rules by inspection! 1.3 Organization of the paper In x2 we review Information Theory basics and describe our model for sequences of market baskets.
Reference: [24] <author> P. Utgoff, N. Berkman, and J. Clouse. </author> <title> Decision tree induction based on efficient tree restructuring. </title> <journal> Machine learning journal, </journal> <month> Oct </month> <year> 1997. </year> <pages> Page 12 </pages>
Reference-contexts: Data Mining. The issue of efficiently updating mining results incrementally is relatively well-studied <ref> [9, 24, 3] </ref> in the data mining literature. A few recent papers have also addressed the issue of discovering interesting patterns along time for market basket data.
References-found: 24

