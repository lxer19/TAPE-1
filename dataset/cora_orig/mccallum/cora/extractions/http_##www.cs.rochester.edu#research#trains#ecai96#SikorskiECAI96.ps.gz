URL: http://www.cs.rochester.edu/research/trains/ecai96/SikorskiECAI96.ps.gz
Refering-URL: http://www.cs.rochester.edu/stats/oldmonths/1998.05/docs-name.html
Root-URL: 
Title: A Task-Based Evaluation of the TRAINS-95 Dialogue System  
Author: Teresa Sikorski and James F. Allen 
Address: Rochester, Rochester, NY 14627, USA  
Affiliation: University of  
Abstract: This paper describes a task-based evaluation methodology appropriate for dialogue systems such as the TRAINS-95 system, where a human and a computer interact and collaborate to solve a given problem. In task-based evaluations, techniques are measured in terms of their affect on task performance measures such as how long it takes to develop a solution using the system, and the quality of the final plan produced. We report recent experiment results which explore the effects of word recognition accuracy and speech versus keyboard input on task perfor mance.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> J. F. Allen, G. Ferguson, B. Miller, and E. Ringger. </author> <title> Spoken Dialogue and Interactive Planning. </title> <booktitle> In Proceedings of the ARPA SLST Workshop, </booktitle> <address> San Mateo California, January 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1 Introduction TRAINS-95 is the first end-to-end implementation in a long-term effort to develop an intelligent planning assistant that is conversationally proficient in natural language. The initial domain is a train route planner, where a human manager and the system must cooperate to develop and execute plans <ref> [1] </ref>. TRAINS-95 provides a real-time multi-modal interface between the human and computer. In addition to making menu selections and clicking on objects using a mouse, the user is able to engage in an English-language dialogue with the computer using either keyboard or speech input.
Reference: 2. <author> M. Boros, W. Eckert, F. Gallwitz, G. Gorz, G. Hanrieder, H. Niemann. </author> <title> Towards Understanding Spontaneous Speech: Word Accuracy Vs. Concept Accuracy. </title> <booktitle> In Proceedings of the International Conference on Spoken Language Processing, </booktitle> <address> Philadel-phia, Pennsylvania, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: Recent research indicates a linear relationship between word recognition accuracy and the understanding of utterances <ref> [2] </ref>. The evaluation of the TRAINS-95 system had coarse granularity in that it measured the amount of time taken to complete the task and the quality of the solution as a whole.
Reference: 3. <author> P. Cohen and S. Oviatt. </author> <title> The Role of Voice Input for Human-Machine Communication. </title> <booktitle> In Proceedings of the National Academy of Sciences, </booktitle> <year> 1994. </year>
Reference-contexts: The experiment results are consistent with our expectations that as speech recognition technology improves and robust techniques for dialogue systems are developed, human-computer interaction using speech input will become increasingly more efficient in comparison with textual input as is the case with human-human interaction <ref> [3] </ref>. task for each of the 5 tasks. Note that in Figures 3 and 4, we give the average for Tasks 1-4, as well as the average for all 5 tasks.
Reference: 4. <author> L. Hirschman, M. Bates, D. Dahl, W. Fisher, J. Garofolo, D. Pallet, K. Hunicke-Smith, P. Price, A. Rudnicky and E. Tzoukermann. </author> <title> Multi-Site Data Collection and Evaluation in Spoken Language Understanding. </title> <booktitle> In Proceedings of the ARPA Human Language Technology Workshop, </booktitle> <address> Princeton, New Jersey, March 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Furthermore, we wanted to be able to use the evaluation results to guide us in system debugging, and to some extent our future research focus. Standard accuracy models used to evaluate speech recognition and data base query tasks such as ATIS <ref> [4] </ref> are not appropriate for a dialogue evaluation. There is no right answer to an utterance in a dialogue. Rather, there are many different possible ways to answer, each of them equally valid. Some may be more efficient along some dimension or another, but there is no single best answer.
Reference: 5. <author> X. D. Huang, F. Alleva, H.W. Hon, M. Y. Hwang, K. F. Lee, and R. Rosenfeld. </author> <title> The Sphinx-II Speech Recognition System: An Overview. </title> <booktitle> Computer, Speech and Language, </booktitle> <year> 1993. </year>
Reference-contexts: Each of the sixteen subjects participated in a session with the TRAINS-95 system which lasted approximately 45 minutes. All sixteen sessions were conducted in the URCS Speech Lab using identical hardware configurations. The software and hardware components used in the experiment included: A Sphinx-II speech recognizer developed at CMU <ref> [5] </ref>, running on a DEC Alpha - TRAINS-95 version 1.3 and the speech recognition post-processor running on a SPARCstation 10 - TrueTalk, a commercial off-the-shelf speech generator (available from En tropics, Inc.) running on a SPARCstation LX Subjects, working at a Sun SPARCstation, wore a headset with a microphone to communicate
Reference: 6. <author> S. Oviatt and P. Cohen. </author> <title> The Contributing Influence of Speech and Interaction on Human Discourse Patterns. </title> <editor> In J. W. Sullivan and S. W. Tyler (eds), </editor> <booktitle> Intelligent User Interfaces. </booktitle> <address> New York, New York. 1991. </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: All subjects were given a choice of whether to use speech or keyboard input to accomplish the final task. While our evaluation appears similar to HCI experiments on whether speech or keyboard is a more effective interface in general (cf. <ref> [6] </ref>), this comparison was not actually our goal. Rather, we were using the various input media as a way of manipulating the input error rate.
Reference: 7. <author> J. Polifroni, L. Hirschman, S. Seneff, and V. Zue. </author> <title> Experiments in Evaluating Interactive Spoken Language Systems. </title> <booktitle> In Proceedings of the DARPA Speech and Natural Language Workshop, </booktitle> <address> Harriman, New York, February 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Once the goal state and solution quality criteria are defined in objective terms, the evaluation can be completely automated. This gives the task-based evaluation method a significant advantage over other techniques <ref> [7, 12] </ref> where human evaluators must intervene to examine individual responses and assess their correctness. <p> Note, however that a comparison of two dialogue systems one taking a conservative approach that only answers when it is confident (cf. [11]), and a robust system that proceeds based on partial understanding showed that the robust system was significantly more successful in completing the same task <ref> [7] </ref>. Random Routes Designers of the TRAINS-95 system feared that since the initial domain is so simple, there would be very limited dialogue between the human and the computer.
Reference: 8. <author> E. Ringger and J. F. Allen. </author> <title> Error Correction Via A Post-Processor For Continuous Speech Recognition. </title> <booktitle> Proceedings of ICASSP-96, </booktitle> <address> Atlanta Georgia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: The speech recognition post-processor has been successful in improving the word recognition accuracy rate by an additional 5%, on average, in the TRAINS-95 domain <ref> [8] </ref>. The routing tasks were chosen with the following restrictions to ensure nontrivial dialogues and avoid drastic complexity variation among tasks: Each task entailed moving three trains to three cities, with no restriction on which train was destined for which city. In each scenario, three cities had conditions causing delays.
Reference: 9. <author> A. Rudnicky. </author> <title> Mode Preferences in a Simple Data Retrieval Task. </title> <booktitle> In Proceedings of the ARPA Human Language Technology Workshop, </booktitle> <address> Princeton, New Jersey, March 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, for each task, the amount of time to develop the plan was significantly lower when speech input was used. Speech input was 28 49% faster. This performance of speech input over keyboard input in our experiment is in contrast with experimental results obtained on some previous systems <ref> [9] </ref>.
Reference: 10. <author> E. Shriberg, E. Wade, and P. Price. </author> <title> Human-Machine Problem Solving Using Spoken Language Systems (SLS): Factors Affecting Performance and User Satisfaction. </title> <booktitle> In Proceedings of the DARPA Speech and Natural Language Workshop, </booktitle> <address> Harriman, New York, February 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The tutorial encouraged the subject to "speak nat-urally, as if to another person". While we are aware that human-computer dialogue is significantly different than human-human dialogue, there is evidence that such instructions given to test subjects does significantly reduce unnatural speech syles such as hyperarticulation <ref> [10] </ref>. Since it was important for the subject to understand how solution quality would be judged, the tutorial also explained how to calculate the amount of time a route takes to travel.
Reference: 11. <author> R. Smith and R. D. </author> <title> Hipp. Spoken Natural Language Dialog Systems: A Practical Approach, </title> <publisher> Oxford University Press. </publisher> <year> 1994. </year>
Reference-contexts: Our experiment demonstates that a robust approach can create a high variance in the effectiveness of an interaction. Note, however that a comparison of two dialogue systems one taking a conservative approach that only answers when it is confident (cf. <ref> [11] </ref>), and a robust system that proceeds based on partial understanding showed that the robust system was significantly more successful in completing the same task [7].
Reference: 12. <author> S. Walter. </author> <title> Neal-Montgomery NLP System Evaluation Methodology. </title> <booktitle> In Proceedings of the DARPA Speech and Natural Language Workshop, </booktitle> <address> Harriman, New York, </address> <month> February </month> <year> 1992. </year> <title> Morgan Kaufmann. This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: Once the goal state and solution quality criteria are defined in objective terms, the evaluation can be completely automated. This gives the task-based evaluation method a significant advantage over other techniques <ref> [7, 12] </ref> where human evaluators must intervene to examine individual responses and assess their correctness.
References-found: 12

