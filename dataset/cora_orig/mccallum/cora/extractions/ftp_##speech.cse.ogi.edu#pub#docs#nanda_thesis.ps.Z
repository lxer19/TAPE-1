URL: ftp://speech.cse.ogi.edu/pub/docs/nanda_thesis.ps.Z
Refering-URL: http://www.cse.ogi.edu/CSLU/publications/publications.html
Root-URL: http://www.cse.ogi.edu
Title: Local Models and Gaussian Mixture Models for Statistical Data Processing  
Author: Nandakishore Kambhatla 
Degree: B.Tech, Institute of Technology, Benaras Hindu University, 1990 A dissertation submitted to the faculty of the Oregon Graduate Institute of Science Technology in partial fulfillment of the requirements for the degree Doctor of Philosophy in Computer Science and Engineering  
Date: January 1996  
Abstract-found: 0
Intro-found: 1
Reference: <author> Ahalt, S. C., Krishnamurthy, A., Cheen, P. & Melton, D. </author> <year> (1990), </year> <title> "Competitive learning algorthms for vector quantization", </title> <booktitle> Neural Networks 3, </booktitle> <pages> 277-290. </pages>
Reference-contexts: In the next section, we describe a 39 class of neural network based learning algorithms for training VQs called competitive learning, which are stochastic versions of GLA. 3.1.3 Competitive learning algorithms In this section, we will present stochastic versions of the GLA called competitive learning algorithms <ref> (Ahalt, Krishnamurthy, Cheen & Melton 1990, Hertz et al. 1991) </ref>. Stochastic algorithms can be faster than batch-mode algorithms (like GLA) and can potentially jump out of local optima of the cost function due to the inherent stochastic noise. By default, competitive learning algorithms are for the Euclidean distance measure. <p> Stochastic algorithms can be faster than batch-mode algorithms (like GLA) and can potentially jump out of local optima of the cost function due to the inherent stochastic noise. By default, competitive learning algorithms are for the Euclidean distance measure. A comparison of different competitive learning algorithms is given in <ref> (Ahalt et al. 1990) </ref>. A competitive learning network (Ahalt et al. 1990, Hertz et al. 1991) with Q neural units corresponds to a VQ with Q cells and the weight vectors of the network are the reference vectors i . <p> By default, competitive learning algorithms are for the Euclidean distance measure. A comparison of different competitive learning algorithms is given in (Ahalt et al. 1990). A competitive learning network <ref> (Ahalt et al. 1990, Hertz et al. 1991) </ref> with Q neural units corresponds to a VQ with Q cells and the weight vectors of the network are the reference vectors i . <p> All other reference vectors are left unchanged. We iteratively present randomly chosen input vectors to the network until the network converges (the reference vectors stop changing above some threshold). In the above algorithm, sometimes neural units can get under-utilized <ref> (Ahalt et al. 1990) </ref>. This can lead to situations where some of the reference vectors never "win". A frequency sensitive competitive learning (FSCL; (Ahalt et al. 1990)) keeps track of how frequently each unit is the winner, and uses this information to modify the distortion measure d (; ) as follows. <p> In the above algorithm, sometimes neural units can get under-utilized <ref> (Ahalt et al. 1990) </ref>. This can lead to situations where some of the reference vectors never "win". A frequency sensitive competitive learning (FSCL; (Ahalt et al. 1990)) keeps track of how frequently each unit is the winner, and uses this information to modify the distortion measure d (; ) as follows. Let u i (n) denote the total number of times unit i has been the winner till the nth iteration.
Reference: <author> Ahmad, S. & Tresp, V. </author> <year> (1993), </year> <title> "Some solutions to the missing feature problem in vision", </title> <editor> in S. Hanson, J. Cowan & C. L. Giles, eds, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <pages> pp. 393-400. </pages>
Reference-contexts: The diagonal assumption has been effectively used for classification tasks (Nowlan 1991) and for solving the missing data problem <ref> (Ahmad & Tresp 1993) </ref>. The spherically symmetric assumption has been widely used for classification and regression tasks (Stensmo & Sejnowski 1995, Nowlan 1991). These assumptions greatly reduce the number of parameters of the mixture models.
Reference: <author> Aitkin, M., Anderson, D. & Hinde, J. </author> <year> (1981), </year> <title> "Statistical modelling of data on teaching styles (with discussion)", </title> <journal> Journal of the Royal Statistical Society A 144, </journal> <pages> 419-461. </pages>
Reference: <author> Aitkin, M. & Wilson, G. </author> <year> (1980), </year> <title> "Mixture models, outliers, and the EM algorithm", </title> <type> Technometrics 22, </type> <pages> 325-331. </pages>
Reference-contexts: Mixture models are extensively used in applications where data can be viewed as arising from several populations mixed in varying proportions. Gaussian mixture models have been used for the identification of outliers <ref> (Aitkin & Wilson 1980) </ref>, and for the investigation of robustness of certain statistics to departures from normality (Srivastava & Lee 1984).
Reference: <author> Anderson, T. </author> <year> (1963), </year> <title> "Asymptotic theory for principal component analysis", </title> <journal> Ann. J. Math. Stat. </journal> <volume> 34, </volume> <pages> 122-148. </pages>
Reference: <author> Anderson, T. W. </author> <year> (1958), </year> <title> An introduction to multivariate statistical analysis, </title> <publisher> John Wiley and Sons Inc., </publisher> <address> New York. </address>
Reference: <author> Barnes, C. & Frost, R. </author> <year> (1990), </year> <title> "Necessary conditions for the optimality of residual vector quantizers", </title> <booktitle> in Abstracts of the 1990 IEEE International Symposium on Information Theory, IEEE, </booktitle> <address> Piscataway, NJ, p. </address> <month> 34. </month>
Reference: <author> Basford, K. & McLachlan, G. </author> <year> (1985), </year> <title> "Estimation of allocation rates in a cluster analysis context", </title> <journal> Journal of American Statistical Association 80, </journal> <pages> 286-293. </pages>
Reference-contexts: (Basford & McLachlan 1985, McLachlan & Basford 1988, Nowlan 1991, Jordan & Jacobs 1 This means that the data surface is diffeomorphic to R k in the neighborhood of any data point on the surface. 2 Soft partitions are referred to as probabilistic clustering in the statistics community, e.g see <ref> (Basford & McLachlan 1985, McLachlan & Basford 1988) </ref>. 4 1994).
Reference: <author> Baum, L. & Eagon, J. </author> <year> (1967), </year> <title> "An inequality with applications to statistical estimation for probabilistic functions of Markov processes and to a model for ecology", </title> <journal> Bulletin of the American Mathematical Society 73(3), </journal> <pages> 360-363. </pages>
Reference: <author> Baum, L., Petrie, T., Soules, G. & Weiss, N. </author> <year> (1970), </year> <title> "A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains", </title> <journal> The Annals of Mathematical Statistics 41(1), </journal> <volume> 164-171. 160 161 Bellman, </volume> <editor> R. </editor> <year> (1961), </year> <title> Adaptive Control Processes, </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ. </address>
Reference: <author> Bourlard, H. & Kamp, Y. </author> <year> (1988), </year> <title> "Auto-association by multilayer perceptrons and singular value decomposition", </title> <booktitle> Biological Cybernetics 59, </booktitle> <pages> 291-294. </pages>
Reference: <author> Bregler, C. & Omohundro, S. M. </author> <year> (1994), </year> <title> "Surface learning with applications to lipreading", </title> <editor> in Cowan, Tesauro & Alspector, eds, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <pages> pp. 43-50. </pages>
Reference: <author> Bregler, C. & Omohundro, S. M. </author> <year> (1995), </year> <title> "Nonlinear image interpolation using manifold learning", </title> <editor> in Tesauro, Touretzky & Leen, eds, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <pages> pp. 973-980. </pages>
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A. & Stone, C. J. </author> <year> (1984), </year> <title> Classification and Regression Trees, </title> <publisher> Wadsworth & Brooks, </publisher> <address> Monterey, California. </address>
Reference-contexts: Examples of a local model include the classification and regression trees (CART) algorithm <ref> (Breiman, Friedman, Olshen & Stone 1984) </ref> and Friedman's (1991) multivariate adaptive regression splines (MARS) algorithm. These algorithms divide the input space into a nested set of regions and fit simple functions (e.g a constant function or a low order polynomial function) within these regions. <p> Ghahramani and Jordan (1994b, 1994a) note that "the mixture of Gaussians competitively partitions the input space, and learns a linear regression surface in each partition". They also note the similarity of the model presented above to classification and regression trees <ref> (Breiman et al. 1984) </ref> and the mixture of experts (Jacobs et al. 1991, Jordan & Jacobs 1994) which also soft partition the data to build local models. <p> We trained multi-layer perceptrons (MLPs) with one hidden layer using a quasi-Newton BFGS (Press et al. 1987) optimization scheme. 7.3.1 Prediction of Boston housing prices Our first task is the prediction of housing prices in Boston (the response variable) from 13 factors (the predictor variables) <ref> (Breiman et al. 1984, Moody & Yarvin 1992) </ref>. We trained the algorithms with four different partitions of the data into training sets containing 380 vectors, validation sets containing 63 vectors and test sets containing 63 vectors. <p> Here 2 is the variance of the test set error across the four different permutations of training, validation and test sets. Breiman et al report a normalized error E norm :22 (in Table 8.8 on page 249 of <ref> (Breiman et al. 1984) </ref>) using the classification and regression trees (CART) algorithm. Moody and Yarvin (1992) train feedforward neural networks with different activation functions and report an error between E norm = 0:15 and E norm = 0:20.
Reference: <author> Broad, D. J. & Clermont, F. </author> <year> (1989), </year> <title> "Formant estimation by linear transformation of the LPC cepstrum", </title> <journal> The Journal of the Acoustical Society of America 86(5), </journal> <pages> 2013-2017. </pages>
Reference: <author> Broomhead, D. S. </author> <year> (1991), </year> <title> "Signal processing for nonlinear systems", </title> <editor> in S. Haykin, ed., </editor> <booktitle> Adaptive Signal Processing, SPIE Proceedings Vol. 1565, SPIE, </booktitle> <pages> pp. 228-243. </pages>
Reference: <author> Cole, R., Muthusamy, Y. & Fanty, M. </author> <year> (1990), </year> <title> "The ISOLET spoken letter database", </title> <type> Technical Report CSE 90-004, </type> <institution> Oregon Graduate Institute of Science & Technology. </institution>
Reference-contexts: We then build a linear map from the encodings to hand-labelled formant frequencies. 94 5.2.1 Experimental setup For the formant mapping experiments, we used vowels extracted from spoken letters. The data is from isolated utterances of the letters A,E,F,O and R by females only from the ISOLET database <ref> (Cole, Muthusamy & Fanty 1990) </ref>. Utterances of A,E,F,O and R contain the phonemes, /ey/,/iy/,/eh/,/ow/ and /aa/ respectively. The input data consists of the lowest 32 DFT coefficients (from 0 to 4KHz frequency range), time-averaged over the central third of the vowel.
Reference: <author> Cole, R., Novick, D., Burnett, D., Hansen, B., Sutton, S. & Fanty, M. </author> <year> (1994), </year> <title> "Towards automatic collection of the U.S census", </title> <booktitle> in Proceedings of the International Conference on Acoustics, Speech and Signal Processing, IEEE, </booktitle> <address> Piscataway, NJ, </address> <pages> pp. </pages> <month> I/93-I/96. </month>
Reference-contexts: The data was drawn from the CENSUS speech corpus <ref> (Cole, Novick, Burnett, Hansen, Sutton & Fanty 1994) </ref>. Each feature vector was 70 dimensional (perceptual linear prediction (PLP) coefficients (Hermansky 1987) over the vowel and surrounding context). <p> 67.3% GMR-prune (1 component; * = 10 4 , 35-D) 67.1% GMR-ridge (1 component; * = 10 4 , = 10 5 ) 67.3% GMR-const (1 component; * = 10 4 ) 11.1% We also experimented with using GMR for classifying 9 vowels using data from the CENSUS speech corpus <ref> (Cole et al. 1994) </ref>. For this task, there are 70 predictor variables and 9 response variables. We summarize our results with GMR algorithms in table 7.5, where we also show the previous results using GMB algorithms (see chapter 6).
Reference: <author> Cottrell, G. W. </author> <year> (1988), </year> <title> "Principal components analysis of images via back propagation", </title> <booktitle> in Visual Communications and Image Processing 1988, SPIE Proceedings Vol. 1001, SPIE, </booktitle> <pages> pp. 1070-1077. </pages>
Reference-contexts: PCA has also been used for the coding of gray-scale images (Cottrell et al. 1987) and image compression <ref> (Cottrell 1988) </ref>. Several artificial neural networks based implementations of PCA exist e.g (Oja 1989, Sanger 1989, Rubner & Tavan 1989, Foldiak 1989, Kung & Diamantaras 1990, Leen 1991). Karhunen and Joursensalo (1995) discuss various generalizations of neural network implementations of PCA.
Reference: <author> Cottrell, G. W. & Metcalfe, J. </author> <year> (1991), </year> <title> "EMPATH: Face, emotion, and gender recognition using holons", </title> <editor> in R. Lippmann, J. Moody & D. Touretzky, eds, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <publisher> Morgan Kauffmann, </publisher> <address> San Mateo, California, </address> <pages> pp. 564-571. </pages> <note> 162 Cottrell, </note> <author> G. W., Munro, P. & Zipser, D. </author> <year> (1987), </year> <title> "Learning internal representations from gray-scale images: an example of extensional programming", </title> <booktitle> in Proceedings of the Ninth Annual Cognitive Science Society Conference, </booktitle> <address> Seattle, Wa, </address> <pages> pp. 461-473. </pages>
Reference-contexts: PCA is widely used as a preprocessor for pattern recognition applications to reduce the input dimension before building classifiers. PCA generated encodings of images have been used to classify faces, emotions and gender <ref> (Cottrell & Metcalfe 1991) </ref>, and the sexes of humans (Golomb et al. 1991). Leen et al (1990) show that using the principal components of speech data for classification reduces the training time significantly without any adverse effect on the accuracy. <p> Each person was asked to feign 8 different emotional states (astonished, happy, pleased, relaxed, sleepy, bored, miserable and angry). Each image is a 64x64, 8-bit/pixel grayscale image. We obtained the data from Gary Cottrell and David DeMers at UCSD <ref> (Cottrell & Metcalfe 1991, DeMers & Cottrell 1993) </ref>. Cottrell and Metcalf (1991) generated this data and used it to classify the identity, gender and emotions of humans using a neural network classifier. We consider each image to be a point in a 4096 dimensional face space.
Reference: <author> Cover, T. M. & Thomas, J. A. </author> <year> (1991), </year> <title> Elements of Information Theory, </title> <publisher> John Wiley and Sons Inc, </publisher> <address> New York. </address>
Reference: <author> Darken, C. & Moody, J. </author> <year> (1991), </year> <title> "Note on learning rate schedules for stochastic optimization", </title> <booktitle> in Advances in Neural Information Processing Systems 3, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <pages> pp. 832-838. </pages>
Reference-contexts: This stochastic noise can be beneficial in helping the network jump out of local minima. We implemented a stochastic gradient descent (SGD) algorithm with a momentum term (Hertz et al. 1991) and an annealed learning rate <ref> (Darken & Moody 1991) </ref>. We also implemented two batch mode optimization schemes: conjugate gradient descent (CGD) and the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm. Let p denote the total number of weights. CGD (Press et al. 1987) finds p mutually conjugate directions in the weight space. <p> We trained the local linear algorithms using Euclidean distance clustering (VQPCA-Eucl), reconstruction distance clustering (VQPCA-Recon), and multi-stage and tree structured clustering for both distance measures. We trained the VQ partition for VQPCA-Eucl using competitive learning (section 3.1.3; a stochastic learning algorithm) with an annealed learning rate <ref> (Darken & Moody 1991) </ref>. We trained the VQ partition of VQPCA-Recon using a GLA (section 3.1.2; a batch-mode algorithm), since stochastic learning with reconstruction distance entails computing the covariance matrices and their eigenvectors after each randomly drawn input vector is presented.
Reference: <author> DeMers, D. & Cottrell, G. </author> <year> (1993), </year> <title> "Non-linear dimensionality reduction", </title> <editor> in Giles, Han-son & Cowan, eds, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California. </address>
Reference-contexts: We extracted the first 50 principal components of each image and use these as our experimental data. This is the same data and preparation that DeMers and Cottrell used in their study of dimension reduction with five layer auto-associative nets <ref> (DeMers & Cottrell 1993) </ref>. They trained auto-associators to reduce the 50 principal components to 5 dimensions. We divided the data into a training set containing 120 images, a validation set (for architecture selection) containing 20 images and a test set containing 20 images.
Reference: <author> Dempster, A., Laird, N. & Rubin, D. </author> <year> (1977), </year> <title> "Maximum likelihood from incomplete data via the EM algorithm", </title> <journal> Journal of the Royal Statistical Society Series B 39, </journal> <pages> 1-38. </pages>
Reference-contexts: In this dissertation, we study GMB classifiers, show their relation to clustering based classification algorithms and explore different ways of regularizing them. We train the 11 mixture models using the Expectation Maximization (EM) algorithm <ref> (Dempster et al. 1977) </ref> (see appendix C); an iterative algorithm for generating maximum likelihood parameter estimates of mixture models. We derive winner-take-all approximations to GMB classifiers and show that clustering based classifiers like the learning vector quantization algorithm (Kohonen 1988) (LVQ; see appendix D) approximate a GMB model. <p> In this example, the class conditional densities are modelled as a mixture of two Gaussians and equal priors are assumed. To implement a GMB classifier we first separate the training data into the different classes. We then use the EM algorithm <ref> (Dempster et al. 1977, Nowlan 1991) </ref> to determine the parameters of the Gaussian mixture density for each class.
Reference: <author> Devijver, P. & Kittler, J. </author> <year> (1982), </year> <title> Pattern recognition: A statistical approach, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference-contexts: When restricted to a finite dimensional case and truncated after a few terms, the K-L expansion is equivalent to a PCA expansion. PCA is used for feature extraction and data compression <ref> (Devijver & Kittler 1982) </ref>, image compression (Jain 1989) and characterization of signals in signal processing (Wax & Kailath 1985).
Reference: <author> Dillon, W. R. & Goldstein, M. </author> <year> (1984), </year> <title> Multivariate Analysis Methods and Applications, </title> <publisher> John Wiley and Sons, </publisher> <address> New York. </address> <month> D.O'Shaughnessy </month> <year> (1987), </year> <title> Speech Communication, </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, Cal-ifornia. </address>
Reference: <author> Draper, N. & Smith, H. </author> <year> (1981), </year> <title> Applied Regression Analysis, 2nd edition, </title> <publisher> John Wiley and Sons, </publisher> <address> New York. </address>
Reference-contexts: We propose two new ways (Leen 1995) of regularizing the regression function: * local ridge regression, where we regularize each of the local predictor functions using ridge regression <ref> (Draper & Smith 1981) </ref> and * principal components pruning (PC pruning), where we prune those directions of each local prediction matrix which induce the least additional error when pruned (similar to (Levin, Leen & Moody 1994)). <p> The standard model <ref> (Draper & Smith 1981, Seber & Wild 1989) </ref> is y = f (x) + *; (7.1) where f () is a mapping from R n to R m , and * is generated by an unobservable zero-mean noise process (i.e E [*] = 0). <p> The mean squared error (7.2) is minimized by the function g fl (x) = E [y j x] = f (x), the conditional expectation of the response, which is called the regression of y on x <ref> (Draper & Smith 1981, Breiman et al. 1984) </ref>. <p> N i=1 If we assume that the noise vectors * i (corresponding to x i , y i pairs) are independent and identically Gaussian distributed with a zero mean and a covariance matrix 2 I mfim , 131 then the least squares estimate of is also the maximum likelihood estimate <ref> (Draper & Smith 1981, Seber & Wild 1989) </ref>. Let Y = fy 1 ; : : : ; y N g denote the set of sample values of y. <p> When the predictor variables are statistically dependent on each other, the autocorrelation matrix ^ R can be close to singular, resulting in unstable parameter estimates. 132 Ridge regression <ref> (Draper & Smith 1981) </ref>, a method originally conceived to tackle this problem, changes the least squares solution ^! by shifting the predictor correlation matrix from ^ R to ^ R + I nfin , ^! = ^r ^ R + I nfin i 1 where is a parameter.
Reference: <author> Duda, R. & Hart, P. </author> <year> (1973), </year> <title> Pattern Classification and Scene Analysis, </title> <publisher> John Wiley and Sons Inc., </publisher> <address> New York. </address>
Reference-contexts: The functions ffi I (x) = p ( I ) p (x j I ) (6.2) are known as the discriminant functions and a feature vector x is assigned to class I if ffi I (x) &gt; ffi J (x) 8J 6= I : A Bayes classifier <ref> (Duda & Hart 1973) </ref> assigns classes to feature vectors based on a model of the class conditional densities, using the discriminant func tions (6.2). Given the class conditional densities, this choice minimizes the classification error rate (Duda & Hart 1973). <p> ffi I (x) &gt; ffi J (x) 8J 6= I : A Bayes classifier <ref> (Duda & Hart 1973) </ref> assigns classes to feature vectors based on a model of the class conditional densities, using the discriminant func tions (6.2). Given the class conditional densities, this choice minimizes the classification error rate (Duda & Hart 1973).
Reference: <author> Everitt, B. & Hand, D. </author> <year> (1981), </year> <title> Finite Mixture Distributions, </title> <publisher> Chapman and Hall, </publisher> <address> New York. </address>
Reference: <author> Farmer, J. D. & Sidorowich, J. J. </author> <year> (1987), </year> <title> "Predicting chaotic time series", </title> <journal> Physical Review Letters 59(8), </journal> <pages> 845-848. </pages>
Reference: <author> Fisher, W. & Doddington, G. </author> <year> (1986), </year> <title> "The DARPA speech recognition research database : specification and status", </title> <booktitle> in Proceedings of the DARPA Speech Recognition Workshop, </booktitle> <address> Palo Alto, CA, </address> <pages> pp. 93-99. </pages> <address> 163 Foldiak, P. </address> <year> (1989), </year> <title> "Adaptive network for optimal linear feature extraction", </title> <booktitle> in Proceedings of the IJCNN, IEEE, </booktitle> <address> Piscataway, NJ, </address> <pages> pp. I 401-405. </pages>
Reference-contexts: We used examples of the twelve monothon-gal vowels extracted from continuous speech drawn from the TIMIT database <ref> (Fisher & Doddington 1986) </ref>. Each input vector consists of the lowest 32 DFT coefficients (spanning the frequency range 0-4kHz), time-averaged over the central third of the utterance. <p> We compute the distortion D for each combination of Q and K. Thus, we get a family of R (D) curves for VQPCA. 5.3.3 Speech compression Our first data set consists of spectral coefficients of speech vowels extracted from continuous speech utterances from the TIMIT database <ref> (Fisher & Doddington 1986) </ref>. This is the same data as in section 5.1.2. Here, we partitioned the 32 dimensional data into a training set containing 1200 vectors and test set containing 816 vectors. We trained VQs with the number of cells Q varying from 1 to 256. <p> For all algorithms, we selected the value of free parameters by varying them over a range of values and picking the value for which the validation set error was the least. 6.3.1 TIMIT data The first task is the classification of 12 monothongal vowels from the TIMIT database <ref> (Fisher & Doddington 1986) </ref>. Each feature vector consists of the lowest 32 DFT coefficients, time-averaged over the central third of the vowel. This is the same data set used in section 5.1.2 and section 5.3.1 earlier in this document. <p> Our first task is the classification of 12 monothongal vowels from the TIMIT database <ref> (Fisher & Doddington 1986) </ref>. There are 32 predictor variables belonging to 12 classes (12 response variables). This is the same data set used in sections 5.1.2, 5.3.1 and 6.3.1 of this document. We summarize our results using GMR algorithms in Table 7.4 along with our previous results with GMB classifiers.
Reference: <author> Fraser, A. & Dimitriadis, A. </author> <year> (1993), </year> <title> "Hidden Markov models with mixed states", </title> <editor> in A. Weigend & N. Gershenfeld, eds, </editor> <title> Predicting the Future and Understanding the Past: a Comparison of Approaches, </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, California, </address> <pages> pp. 265-282. </pages>
Reference: <author> Friedman, J. </author> <year> (1991), </year> <title> "Multivariate adaptive regression splines", </title> <journal> The Annals of Statistics 19, </journal> <pages> 1-141. </pages>
Reference: <author> Frost, R., Barnes, C. & Xu, F. </author> <year> (1991), </year> <title> "Design and performance of residual quantizers", </title> <editor> in J. Storer & J. Reif, eds, </editor> <booktitle> Proceedings of Data Compression Conference, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 129-138. </pages>
Reference: <author> Fukunaga, K. </author> <year> (1972), </year> <title> Introduction to Statistical Pattern Recognition, </title> <publisher> Academic Press Inc., </publisher> <address> San Diego, California. </address>
Reference-contexts: In general, the criteria will depend upon the specific application. For instance, for classification tasks, we want to extract features from the data which are most effective for separating the classes. In this case, "accuracy" refers to the "effectiveness for separating classes". Linear discriminant analysis <ref> (Fukunaga 1972) </ref> finds the optimal linear transformation of the data vectors to extract (linear) features based on their effectiveness for separating classes. Factor analysis (Harman 1976) is a dimension reduction technique, which aims to obtain low dimensional encodings which maximally account for the correlations between the original data variables.
Reference: <author> Fukunaga, K. & Olsen, D. R. </author> <year> (1971), </year> <title> "An algorithm for finding intrinsic dimensionality of data", </title> <journal> IEEE Transactions on Computers C-20(2), </journal> <pages> 176-183. </pages>
Reference: <author> Funahashi, K. </author> <year> (1989), </year> <title> "On the approximate realization of continuous mappings by neural networks", </title> <booktitle> Neural Networks 2, </booktitle> <pages> 183-192. </pages>
Reference-contexts: The activations of the bottleneck (third) layer form the low dimensional encoding. This network can perform a non-linear dimension reduction from n to m dimensions. perform a non-linear dimension reduction. In theory, these nets can compute any continuous mapping from the inputs to the bottleneck layer <ref> (Hornik, Stinchcombe & White 1989, Funahashi 1989) </ref>, and another mapping from the bottleneck layer to the output layer. 2.2.2 Five layered networks (FLNs) for dimension reduction Auto-associative five layered networks (FLNs) are capable of exploiting non-linear dependencies among the input variables to generate more accurate encodings than PCA.
Reference: <author> Funahashi, K. </author> <year> (1990), </year> <title> "On the approximate realization of identity mappings by three-layer neural networks", </title> <type> Technical Report, </type> <institution> Toyohashi University of Technology, Department of Information and Computer Sciences. </institution> <note> Translation of Japanese paper in Denshi Joho Tsushin Gakkai Ronbunshi, J73-A(1), </note> <year> 1990, </year> <pages> pp 139-145. </pages>
Reference: <author> Ganesalingam, S. & McLachlan, G. </author> <year> (1979), </year> <title> "A case study of two clustering methods based on maximum likelihood", </title> <journal> Statistical Neerlandica 33, </journal> <pages> 81-90. </pages>
Reference: <author> Gemen, S., Bienenstock, E. & Doursat, R. </author> <year> (1992), </year> <title> "Neural networks and the bias/variance dilemma", </title> <booktitle> Neural Computation 4, </booktitle> <pages> 1-58. </pages>
Reference-contexts: Local models like CART and MARS often have convergence times orders of magnitude faster than neural network algorithms (Jordan & Jacobs 1994). In terms of the bias/variance tradeoff <ref> (Gemen, Bienenstock & Doursat 1992) </ref>, in general, a local model will have a higher variance than a global model having equal complexity as each local model, since the local parameters are estimated based on a smaller number of data points (subsets of the original training set). <p> Thus, the model may learn spurious dependencies that do not exist in the population, resulting in poor generalization ability. In terms of the bias/variance tradeoff <ref> (Gemen et al. 1992) </ref>, over-parametrized models can have a high model variance, which means that the trained parameter values vary greatly with the particular training set used. Complex models with a large number of parameters can also require a long time to train and a large storage space.
Reference: <author> G.E.Peterson & H.L.Barney (1952), </author> <title> "Control methods used in a study of the vowels", </title> <journal> The Journal of the Acoustical Society of America 24(2), </journal> <pages> 175-184. </pages>
Reference: <author> Gersho, A. </author> <year> (1979), </year> <title> "Asymptotically optimal block quantization", </title> <journal> IEEE Transactions on Information Theory IT-25(4), </journal> <pages> 373-380. </pages> <note> 164 Gersho, </note> <author> A. & Gray, R. M. </author> <year> (1992), </year> <title> Vector Quantization and Signal Compression, </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> Massachusetts. </address>
Reference-contexts: We present a VQPCA training algorithm which uses a VQ with reconstruction distance. Finally, we present algorithms which use multi-stage and tree structured VQs to implement VQPCA. 35 3.1 Vector quantization (VQ) Vector quantization <ref> (Gersho 1979, Gersho & Gray 1992) </ref> is a classical technique for signal coding and data compression. A vector quantizer (VQ) approximates a vector x by one of a predetermined set of prototype vectors called reference vectors or codebook vectors.
Reference: <author> Ghahramani, Z. </author> <year> (1994), </year> <title> "Solving inverse problems using an EM approach to density estimation", </title> <booktitle> in Proceedings of the 1993 Connectionist Models Summer School, </booktitle> <publisher> Lawrence Erlbaum Publishers, </publisher> <pages> pp. 316-323. </pages>
Reference-contexts: One way to obtain numerical stability (avoid singularities) is to impose an artificial lower bound on the volume elements (determinants of covariance matrices) of each Gaussian <ref> (Ghahramani 1994, Ormoneit & Tresp 1995) </ref> during parameter estimation. This is achieved by adding a small diagonal matrix *I nfin to each covariance matrix in each iteration of the EM algorithm. Adding *I nfin avoids singularities and also regularizes the mixture model.
Reference: <author> Ghahramani, Z. & Jordan, M. I. </author> <year> (1994a), </year> <title> "Learning from incomplete data", </title> <type> Technical Report 108, </type> <institution> Center for Biological and Computational Learning; Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology. </institution>
Reference-contexts: 1968, Ganesalingam & McLachlan 1979, Basford & McLach-lan 1985), for supervised learning in the "mixture of experts" architecture (Jacobs et al. 1991, Jordan & Jacobs 1994), for classification using Bayes classifiers (Ghahramani & Jordan 1994a, Ghahramani & Jordan 1994b, Ormoneit & Tresp 1995, Hinton et al. 1995), and for regression <ref> (Ghahramani & Jordan 1994a, Ghahramani & Jordan 1994b, Leen 1995) </ref>. For a much more extensive discussion on mixture models and their myriad applications, see the books by Everitt and Hand (1981), Titterington et al (1985) and McLachlan and Basford (1988). <p> This greatly reduces the number of parameters. However, this assumption is often invalid. For instance, when classifying images using pixel intensities, the intensities of neighboring pixels are very highly correlated and hence the covariance matrix is far from diagonal or spherically symmetric. * Following <ref> (Ghahramani & Jordan 1994a, Ormoneit & Tresp 1995) </ref>, we add a constant diagonal matrix *I nfin to each covariance matrix in each iteration of the EM algorithm for training the mixture model for each class. <p> We model the joint density of the inputs and outputs as a mixture of Q Gaussians (1.2), and derive the regression function E [y j x] as in <ref> (Ghahramani & Jordan 1994a, Ghahramani & Jordan 1994b) </ref>. The regression is a weighted sum of linear models, where the weights are the probabilities of membership to the component Gaussians of the mixture model. <p> In section 6.2, we discuss the following schemes for regularizing GMB classifiers; * assuming that all covariance matrices are diagonal or spherically symmetric, * bounding the covariance matrices by adding a constant diagonal matrix *I nfin to each covariance matrix in each iteration of the EM algorithm <ref> (Ghahramani & Jordan 1994a, Ormoneit & Tresp 1995) </ref>. * We propose a new regularization scheme which prunes those directions of each covariance matrix, which induce the least bias when pruned. <p> We discussed several methods of regularizing GMB classifiers: * The models are regularized by assuming that all covariance matrices are diagonal (GMB-diagonal) or spherically symmetric (as in LVQ). * Following <ref> (Ghahramani & Jordan 1994a) </ref>, a constant diagonal matrix (*I nfin ) is added to each covariance matrix in each iteration of the EM algorithm (for training the mixture models). This enforces numerical stability and regularizes the mixture model. <p> Applying a Bayesian prior to the parameters, Ormoneit et al (1995) obtain the same EM update rule. In our implementation, we also add * to (and renor-malize) the posterior probabilities of Gaussian component membership during each iteration of the EM algorithm as in <ref> (Ghahramani & Jordan 1994a) </ref>. * We propose a new method of regularizing GMB classifiers.
Reference: <author> Ghahramani, Z. & Jordan, M. I. </author> <year> (1994b), </year> <title> "Supervised learning from incomplete data via an EM approach", </title> <editor> in J. D. Cowan, G. Tesauro & J. Alspector, eds, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <publisher> Califor-nia, </publisher> <pages> pp. 120-127. </pages>
Reference-contexts: Marroquin (1995) uses a soft local linear model for regression which is similar to (7.9). 134 However, Marroquin assumes that the covariance matrices of the mixture model are spherically symmetric. The regression function (7.9) (also see <ref> (Ghahramani & Jordan 1994b, Ghahramani & Jordan 1994a) </ref>) is a weighted sum of standard linear regression functions discussed in section 7.1 as follows: E [y j x] = j=1 h y yx j j = j=1 where ~x (x; 1) is an (n + 1) dimensional column vector and ! j
Reference: <author> Gnanadesikan, R. </author> <year> (1977), </year> <title> Methods for Statistical Data Analysis of Multivariate Observations, </title> <publisher> John Wiley and Sons, </publisher> <address> New York. </address>
Reference: <author> Golomb, B., Lawrence, D. & Sejnowski, T. </author> <year> (1991), </year> <title> "Sexnet: A neural network identifies sex from human faces", </title> <editor> in R. Lippmann, J. Moody & D. Touretzky, eds, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <publisher> Morgan Kauffmann, </publisher> <address> San Mateo, </address> <publisher> Cal-ifornia, </publisher> <pages> pp. 572-577. </pages>
Reference-contexts: PCA is widely used as a preprocessor for pattern recognition applications to reduce the input dimension before building classifiers. PCA generated encodings of images have been used to classify faces, emotions and gender (Cottrell & Metcalfe 1991), and the sexes of humans <ref> (Golomb et al. 1991) </ref>. Leen et al (1990) show that using the principal components of speech data for classification reduces the training time significantly without any adverse effect on the accuracy.
Reference: <author> Golub, G. H. & van Loan, C. F. </author> <year> (1983), </year> <title> Matrix Computations, </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, Maryland. </address>
Reference-contexts: Suppose we are reducing data from n to m dimensions. We estimate and by the sample mean ^ = N I=1 the sample covariance matrix ^ = N I=1 respectively. We reduce ^ to a tridiagonal form using the Householder algorithm <ref> (Golub & van Loan 1983, Press, Flannery, Teukolsky & Vetterling 1987) </ref>. We then use the QL algorithm with implicit shifts (Golub & van Loan 1983, Press et al. 1987) to compute the orthonormal eigenvectors e i of ^ . Finally, we use (2.3) to obtain low dimensional encodings. <p> We reduce ^ to a tridiagonal form using the Householder algorithm (Golub & van Loan 1983, Press, Flannery, Teukolsky & Vetterling 1987). We then use the QL algorithm with implicit shifts <ref> (Golub & van Loan 1983, Press et al. 1987) </ref> to compute the orthonormal eigenvectors e i of ^ . Finally, we use (2.3) to obtain low dimensional encodings. <p> In the extreme case when the number of input vectors, N , is less than n, ^ is singular and we can not compute e i by diagonalizing ^ . 23 Singular value decomposition (SVD) <ref> (Golub & van Loan 1983, Press et al. 1987) </ref> is a robust method for estimating the eigenvectors of ^ in such cases. Suppose X is an N fi n matrix whose rows are the N mean-subtracted input samples f (x I ^)jI = 1; : : : ; N g.
Reference: <author> Guckenheimer, J. & Holmes, P. </author> <year> (1983), </year> <title> Nonlinear Oscillations, Dynamical Systems, and Bifurcations of Vector Fields, </title> <booktitle> Vol. 42 of Applied Mathematical Sciences, </booktitle> <publisher> Springer-Verlag. </publisher>
Reference-contexts: A global model may be inappropriate for some data sets. Consider a data set which has a different structure in different parts of the input space. For example, Figure 2.6 (a) shows data distributed on an orbit asymptotic to the Lorenz attractor <ref> (Guckenheimer & Holmes 1983) </ref>. The three dimensional data can be well described locally using only two dimensions, in most of the input space.
Reference: <author> Harman, H. H. </author> <year> (1976), </year> <title> Modern Factor Analysis, 3rd edn, </title> <publisher> The University of Chicago Press, Chicago. </publisher>
Reference-contexts: In this case, "accuracy" refers to the "effectiveness for separating classes". Linear discriminant analysis (Fukunaga 1972) finds the optimal linear transformation of the data vectors to extract (linear) features based on their effectiveness for separating classes. Factor analysis <ref> (Harman 1976) </ref> is a dimension reduction technique, which aims to obtain low dimensional encodings which maximally account for the correlations between the original data variables. Multi-dimensional scaling (Kruskal 1964) is a technique for extracting the underlying criteria or dimensions that people use in perceptually measuring (dis)similarities between objects (data points). <p> eigen-space of the covariance matrix of x is the maximum likelihood signal estimate (4.10) for our Gaussian signal-plus-noise model. 4.1.1 The relation between factor analysis and the maximum likelihood model for PCA The above model is very similar in spirit to the "the basic factor-analytic model" used in factor analysis <ref> (Harman 1976, Gnanadesikan 1977, Dillon & Goldstein 1984) </ref>. In this subsection, we show the relation between factor analysis and the probabilistic model described above. <p> assumed to be independent of the common factors and are assumed to have a zero mean and a diagonal covariance matrix E [** T ] = = B B B B 2 0 2 . . . . . . n C C C C : (4.12) Factor analysis techniques <ref> (Harman 1976, Gnanadesikan 1977, Dillon & Goldstein 1984) </ref> estimate the factor loadings and values of unique factors from sample observations of the random vector x. <p> We assume that f is N ( f ; I mfim ) (this is a standard assumption; see <ref> (Harman 1976, Gnanadesikan 1977) </ref>). Therefore, ~x is N ( = fl f ; ~ = flfl T ). We assume that * is N (0; ) (Harman 1976, Gnanadesikan 1977), where is given by (4.12). <p> We assume that f is N ( f ; I mfim ) (this is a standard assumption; see <ref> (Harman 1976, Gnanadesikan 1977) </ref>). Therefore, ~x is N ( = fl f ; ~ = flfl T ). We assume that * is N (0; ) (Harman 1976, Gnanadesikan 1977), where is given by (4.12). The maximum likelihood signal estimate is x fl = argmax ~x = argmax ~x There is no solution to (4.13) unless ~ is invertible.
Reference: <author> Hartman, E. & Keeler, J. D. </author> <year> (1991), </year> <title> "Predicting the future: Advantages of semilocal units", </title> <booktitle> Neural Computation 3(4), </booktitle> <pages> 566-578. </pages>
Reference: <author> Hasselblad, V. </author> <year> (1966), </year> <title> "Estimation of parameters for a mixture of normal distributions", </title> <type> Technometrics 8, </type> <pages> 431-444. </pages>
Reference: <author> Hasselblad, V. </author> <year> (1969), </year> <title> "Estimation of finite mixture of distributions from the exponential family", </title> <journal> Journal of the American Statistical Association 64, </journal> <pages> 1459-1471. </pages> <note> 165 Haykin, </note> <author> S. </author> <year> (1994), </year> <title> Neural Networks, A Comprehensive Foundation, </title> <publisher> Macmillan College Publishing Company, </publisher> <address> New York. </address>
Reference: <author> Heck, L. & Chou, K. </author> <year> (1994), </year> <title> "Gaussian mixture model classifiers for machine monitoring", </title> <booktitle> in Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, IEEE, </booktitle> <address> New York, </address> <pages> pp. </pages> <month> VI/133-VI/136. </month>
Reference: <author> Hediger, T., Passamante, A. & Farrell, M. E. </author> <year> (1990), </year> <title> "Characterizing attractors using local intrinsic dimensions calculated by singular-value decomposition and information-theoretic criteria", </title> <journal> Physical Review A 41(10), </journal> <pages> 5325-5332. </pages>
Reference-contexts: PCA is used for feature extraction and data compression (Devijver & Kittler 1982), image compression (Jain 1989) and characterization of signals in signal processing (Wax & Kailath 1985). Fukunaga and Olsen (1971) and Hediger et al <ref> (Hediger, Passamante 18 & Farrell 1990) </ref> use PCA in local regions of the input space to determine the intrinsic dimensionality of a data set. PCA is widely used as a preprocessor for pattern recognition applications to reduce the input dimension before building classifiers.
Reference: <author> Hermansky, H. </author> <year> (1987), </year> <title> "Perceptual linear predictive (PLP) analysis of speech", </title> <journal> The Journal of the Acoustical Society of America 4, </journal> <pages> 1738-1752. </pages>
Reference-contexts: The data was drawn from the CENSUS speech corpus (Cole, Novick, Burnett, Hansen, Sutton & Fanty 1994). Each feature vector was 70 dimensional (perceptual linear prediction (PLP) coefficients <ref> (Hermansky 1987) </ref> over the vowel and surrounding context). We partitioned the data into a training set (8997 vectors), a validation set (1362 vectors) for model selection, and a test set (1638 vectors). The training set had close to a 1000 vectors per class.
Reference: <author> Hertz, J., Krogh, A. & Palmer, R. G. </author> <year> (1991), </year> <title> Introduction to the Theory of Neural Computation, </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, California. </address>
Reference-contexts: In such situations, a stochastic algorithm will train significantly faster, though the gradient estimate at each step is inexact and noisy. This stochastic noise can be beneficial in helping the network jump out of local minima. We implemented a stochastic gradient descent (SGD) algorithm with a momentum term <ref> (Hertz et al. 1991) </ref> and an annealed learning rate (Darken & Moody 1991). We also implemented two batch mode optimization schemes: conjugate gradient descent (CGD) and the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm. Let p denote the total number of weights. <p> We picked the network configuration with the least validation set error for each optimization scheme (BFGS, CGD and SGD). We monitored the validation set error while training the FLNs and stopped training when the validation set error started to increase (early stopping; <ref> (Hertz et al. 1991) </ref>). For stochastic gradient descent (SGD), we used Darken and Moody's (1991) annealed learning rate schedule with a momentum (Hertz et al. 1991) term and stopped training the networks when the validation set error stopped decreasing apart from the fluctuations due to the inherent stochastic noise. <p> We monitored the validation set error while training the FLNs and stopped training when the validation set error started to increase (early stopping; <ref> (Hertz et al. 1991) </ref>). For stochastic gradient descent (SGD), we used Darken and Moody's (1991) annealed learning rate schedule with a momentum (Hertz et al. 1991) term and stopped training the networks when the validation set error stopped decreasing apart from the fluctuations due to the inherent stochastic noise. Similarly, we varied the number of local regions of VQPCA-Eucl and VQPCA-Recon from 5 to 50 in increments of 5. <p> We varied the free parameters of different algorithms over a range of values, and chose the value for which the validation set error was the least. We trained several three layered MLPs (feedforward neural networks (see section 2.2 or <ref> (Hertz et al. 1991) </ref>)) with different number of nodes in the hidden layer. The MLPs had sigmoid non-linearities in the hidden layer with asymptotes 0 and 1. Each network had K (number of classes) binary target outputs indicating class membership. <p> We monitored the validation set error while training the MLPs 2 For our data sets described in the next section, we estimated a training time of several weeks to use this procedure to prune eigen-directions. 123 and stopped training when the validation set error started to increase (early stopping; <ref> (Hertz et al. 1991) </ref>). We trained several GMB classifiers using the EM algorithm, varying the number of components, the number of directions pruned (section 6.2.2) and the regularization parameter * (section 6.2.1). We also trained GMB classifiers with a diagonal covariance approximation and the LVQ model.
Reference: <author> Hinton, G., Revon, M. & Dayan, P. </author> <year> (1995), </year> <title> "Recognizing handwritten digits using mixtures of linear models", </title> <editor> in Tesauro, Touretzky & Leen, eds, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <pages> pp. 1015-1022. </pages>
Reference-contexts: These algorithms divide the input space into a nested set of regions and fit simple functions (e.g a constant function or a low order polynomial function) within these regions. Leen and I propose local linear models for dimension reduction (Kambhatla and Leen 1993,1994). Hinton et al <ref> (Hinton, Revon & Dayan 1995) </ref> use local linear models for handwritten digit recognition. Global models can be inefficient (slow to learn and inaccurate) whenever the structure of the data is different in different parts of the input space, i.e whenever the function f () is highly complex.
Reference: <author> Hornik, M., Stinchcombe, M. & White, H. </author> <year> (1989), </year> <title> "Multilayer feedforward networks are universal approximators", </title> <booktitle> Neural Networks 2, </booktitle> <pages> 359-368. </pages>
Reference-contexts: The activations of the bottleneck (third) layer form the low dimensional encoding. This network can perform a non-linear dimension reduction from n to m dimensions. perform a non-linear dimension reduction. In theory, these nets can compute any continuous mapping from the inputs to the bottleneck layer <ref> (Hornik, Stinchcombe & White 1989, Funahashi 1989) </ref>, and another mapping from the bottleneck layer to the output layer. 2.2.2 Five layered networks (FLNs) for dimension reduction Auto-associative five layered networks (FLNs) are capable of exploiting non-linear dependencies among the input variables to generate more accurate encodings than PCA.
Reference: <author> Hotelling, H. </author> <year> (1933), </year> <title> "Analysis of a complex of statistical variables into principal components", </title> <journal> Journal of Educational Psychology 24, </journal> <pages> 498-520. </pages>
Reference-contexts: The mean squared error in reconstructing the original data is E = E k x g (f (x)) k 2 ; (1.3) where E [] denotes an expectation with respect to the random vector x. Principal component analysis <ref> (Hotelling 1933, Oja 1983) </ref> (PCA) is a dimension reduction algorithm which obtains the least error (1.3) among all techniques with linear encoding and decoding functions (f () and g ()).
Reference: <author> Jacobs, R., Jordan, M., Nowlan, S. & Hinton, G. </author> <year> (1991), </year> <title> "Adaptive mixture of local experts", </title> <booktitle> Neural Computation 3, </booktitle> <pages> 79-87. </pages>
Reference-contexts: Thus, a soft local model computes a weighted sum of (local) functions. Jacobs, Jordan et al propose soft local models for supervised learning as a "mixture of experts" architecture <ref> (Jacobs, Jordan, Nowlan & Hinton 1991, Jordan & Jacobs 1994) </ref>. Bregler and Omohundro use soft local linear models to learn non-linear constraint surfaces from the data (1994) and for "manifold learning" (1995). Marroquin (1995) discusses soft local models and local models for classification and regression. <p> Mixture models have been used as a clustering technique (Lazarsfeld & Henry 1968, Ganesalingam & McLachlan 1979, Basford & McLach-lan 1985), for supervised learning in the "mixture of experts" architecture <ref> (Jacobs et al. 1991, Jordan & Jacobs 1994) </ref>, for classification using Bayes classifiers (Ghahramani & Jordan 1994a, Ghahramani & Jordan 1994b, Ormoneit & Tresp 1995, Hinton et al. 1995), and for regression (Ghahramani & Jordan 1994a, Ghahramani & Jordan 1994b, Leen 1995). <p> Ghahramani and Jordan (1994b, 1994a) note that "the mixture of Gaussians competitively partitions the input space, and learns a linear regression surface in each partition". They also note the similarity of the model presented above to classification and regression trees (Breiman et al. 1984) and the mixture of experts <ref> (Jacobs et al. 1991, Jordan & Jacobs 1994) </ref> which also soft partition the data to build local models.
Reference: <author> Jain, A. </author> <year> (1989), </year> <title> Fundamentals of digital image processing, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference-contexts: When restricted to a finite dimensional case and truncated after a few terms, the K-L expansion is equivalent to a PCA expansion. PCA is used for feature extraction and data compression (Devijver & Kittler 1982), image compression <ref> (Jain 1989) </ref> and characterization of signals in signal processing (Wax & Kailath 1985). Fukunaga and Olsen (1971) and Hediger et al (Hediger, Passamante 18 & Farrell 1990) use PCA in local regions of the input space to determine the intrinsic dimensionality of a data set.
Reference: <author> Jordan, M. & Jacobs, R. </author> <year> (1994), </year> <title> "Hierarchical mixtures of experts and the EM algorithm", </title> <booktitle> Neural Computation 6(2), </booktitle> <pages> 181-214. </pages>
Reference-contexts: Local learning algorithms employ a divide-and-conquer approach: they divide a complex problem into simpler problems and combine their solutions to yield a solution to the complex problem <ref> (Jordan & Jacobs 1994) </ref>. A global model builds a single function for the whole data space, while a local model builds separate functions in different regions of the data space. For instance, consider learning a function f : R n ! R m . <p> Simple models (e.g a constant function or a linear function) can quickly learn the structure of the data in local regions of the input space. Local models like CART and MARS often have convergence times orders of magnitude faster than neural network algorithms <ref> (Jordan & Jacobs 1994) </ref>.
Reference: <author> Juang, B.-H. & Jr., A. G. </author> <year> (1982), </year> <title> "Multiple stage vector quantization for speech coding", </title> <booktitle> in Proceeding of the IEEE International Conference on Acoustics and Signal Processing, IEEE, </booktitle> <address> Piscataway, NJ, </address> <pages> pp. 597-600. </pages> <note> 166 Kambhatla, </note> <author> N. & Leen, T. K. </author> <year> (1993), </year> <title> "Fast non-linear dimension reduction", </title> <booktitle> in 1993 IEEE International Conference on Neural Networks, IEEE, </booktitle> <pages> pp. 1213-1218. </pages>
Reference: <author> Kambhatla, N. & Leen, T. K. </author> <year> (1994), </year> <title> "Fast non-linear dimension reduction", </title> <editor> in Cowan, Tesauro & Alspector, eds, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <pages> pp. 152-159. </pages>
Reference: <author> Karhunen, J. & Joutsensalo, J. </author> <year> (1995), </year> <title> "Generalizations of principal component analysis, optimization problem, and neural networks", </title> <booktitle> Neural Networks 8(4), </booktitle> <pages> 549-562. </pages>
Reference: <author> Kohonen, T. </author> <year> (1988), </year> <title> Self-Organization and Associative Memory, 2nd edn, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: We derive winner-take-all approximations to GMB classifiers and show that clustering based classifiers like the learning vector quantization algorithm <ref> (Kohonen 1988) </ref> (LVQ; see appendix D) approximate a GMB model. GMB classifiers suffer from the curse of dimensionality. A GMB classifier with a mixture of Q Gaussians for each of K classes and for n dimensional data has O (K flQfln 2 ) parameters. <p> We present Gaussian mixture Bayes (GMB) classifiers which use a mixture of Gaussians model for each class conditional density. We show the relation between GMB classifiers and hard clustering based classifiers like the learning vector quantization algorithm 4 (LVQ; see <ref> (Kohonen 1988) </ref> and appendix D). We discuss different ways of regularizing GMB classifiers and propose a new regularization scheme, which prunes those eigen-directions of each discriminant function, which induce the least (empirically measured) classification error when removed. This reduces the model variance, and induces additional bias. <p> A stochastic algorithm related to competitive learning algorithms is Kohonen's self-organizing feature map (KSFM) <ref> (Kohonen 1988) </ref>. Kohonen initially formulated KSFM to illustrate the formation of topological feature maps in the brain, though it is often used to train a VQ. <p> each class separately into disjoint regions using only the dominant Gaussian component for each data point. 116 6.1.1 The relation between GMB and VQ clustering based classifiers We will now derive the relationship between GMB classifiers and VQ (see section 3.1) clustering based classifiers like the learning vector quantization (LVQ <ref> (Kohonen 1988) </ref>; appendix D) algorithm. We will use a similar approach to Steven Nowlan's derivation showing the relation between Gaussian mixture models and unsupervised VQ clustering (see (Nowlan 1991) and appendix B).
Reference: <author> Kramer, M. A. </author> <year> (1991), </year> <title> "Nonlinear prinipal component analysis using autoassociative neural networks", </title> <journal> AIChE journal 37(2), </journal> <pages> 233-243. </pages>
Reference-contexts: PCA builds a global linear model of the data: an m dimensional hyperplane spanned by the leading eigenvectors 3 of the data co-variance matrix. PCA incurs a high error whenever the data has non-linear dependencies not eliminated by removing correlations. Five layered auto-associative neural networks (FLNs) <ref> (Kramer 1991, Oja 1991) </ref> are capable of building non-linear encoding f () and decoding g () functions, by capturing any non-linear dependencies among the data variables. These networks have at least one layer with a smaller number of nodes (m nodes) than the number of inputs (n). <p> FLNs have been used for the analysis of simulated chemical batch reaction data <ref> (Kramer 1991) </ref>, for the learning of psychological color attributes from surface spectral reflectance data (Usui, Nakauchi & Nakano 1991), and for image compression (Namphol, Arozullah & Chin 1991).
Reference: <author> Kruskal, J. </author> <year> (1964), </year> <title> "Multi-dimensional scaling by optimizing goodness of fit to a nonmetric hypothesis", </title> <type> Psychometrika 29, </type> <pages> 1-27. </pages>
Reference-contexts: Factor analysis (Harman 1976) is a dimension reduction technique, which aims to obtain low dimensional encodings which maximally account for the correlations between the original data variables. Multi-dimensional scaling <ref> (Kruskal 1964) </ref> is a technique for extracting the underlying criteria or dimensions that people use in perceptually measuring (dis)similarities between objects (data points). We measure the "accuracy of representation" by the mean squared error induced in 7 reconstructing the original data vectors from the low dimensional encodings.
Reference: <author> Kung, S. Y. & Diamantaras, K. I. </author> <year> (1990), </year> <title> "A neural network learning algorithm for adaptive principal component extraction (APEX)", </title> <booktitle> in Proceedings of the IEEE International Conference on Acoustics Speech and Signal Processing, IEEE, </booktitle> <address> Piscataway, NJ, </address> <pages> pp. 861-864. </pages>
Reference: <author> Lapedes, A. & Farber, R. </author> <year> (1987), </year> <title> "Nonlinear signal processing using neural networks: Prediction and system modelling", </title> <type> Technical Report LA-UR-87-2662, </type> <institution> Los Alamos National Laboratory, </institution> <address> Los Alamos, New Mexico. </address>
Reference: <author> Lazarsfeld, P. & Henry, N. </author> <year> (1968), </year> <title> Latent Structure Analysis, </title> <publisher> Houghton Mi*in, </publisher> <address> New York. </address>
Reference: <author> Leen, T. K. </author> <year> (1991), </year> <title> "Dynamics of learning in linear feature-discovery networks", </title> <booktitle> Network : Computation in Neural Systems 2, </booktitle> <pages> 85-105. </pages>
Reference: <author> Leen, T. K. </author> <year> (1995), </year> <title> "Regularized local linear models for regression", </title> <note> Unpublished research notes, </note> <institution> Oregon Graduate Institute of Science & Technology. </institution>
Reference-contexts: The mixture of Gaussians model can suffer from the curse of dimensionality since it has O (Q fl (n + m) 2 ) parameters (the joint density is n + m dimensional). We propose two new ways <ref> (Leen 1995) </ref> of regularizing the regression function: * local ridge regression, where we regularize each of the local predictor functions using ridge regression (Draper & Smith 1981) and * principal components pruning (PC pruning), where we prune those directions of each local prediction matrix which induce the least additional error when <p> The first method uses local ridge regression. The second method prunes the directions of each local linear predictor which contribute the least towards lowering the mean squared error. 7.2.1 GMR with local ridge regression We apply ridge regression (see section 7.1) to each local prediction matrix in (7.11) <ref> (Leen 1995) </ref>. <p> 137 7.2.2 GMR with principal components pruning We now present another method of regularizing the GMR regression function (7.11), which computes the components of each local prediction matrix ^! j along the eigen-directions of ^ R j , and prunes those p components which induce the least bias when removed <ref> (Leen 1995) </ref>. This procedure is an application of the principal components pruning (PC pruning) technique (Levin et al. 1994) applied to mixture regression models.
Reference: <author> Leen, T. K., Rudnick, M. & Hammerstrom, D. </author> <year> (1990), </year> <title> "Hebbian feature discovery improves classifier efficiency", </title> <booktitle> in Proceedings of the IJCNN, IEEE, </booktitle> <address> Piscataway, NJ, </address> <pages> pp. </pages> <note> I-51 to I-56. 167 Levin, </note> <author> A. U., Leen, T. K. & Moody, J. E. </author> <year> (1994), </year> <title> "Fast pruning using principal components", </title> <editor> in J. D. Cowan, G. Tesauro & J. Alspector, eds, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <pages> pp. 35-42. </pages>
Reference: <author> Linde, Y., Buzo, A. & Gray, R. M. </author> <year> (1980), </year> <title> "An algorithm for vector quantizer design", </title> <journal> IEEE Transactions on Communications COM-28(1), </journal> <pages> 84-95. </pages>
Reference: <author> Lloyd, S. </author> <year> (1982), </year> <title> "Least squares quantization in PCM", </title> <journal> IEEE Transactions on Information Theory IT-28(2), </journal> <pages> 129-137. </pages> <institution> Unpublished Bell Laboratories technical note. Portions presented at the Institute of Mathematical Statistics Meeting Atlantic City New Jersey September 1957. </institution> <note> Published in March 1982 special issue on quantization of the IEEE Transactions on Information Theory. </note>
Reference-contexts: Using the terminology in Gersho and Gray (1992), we shall refer to it as the GLA since it is a direct generalization of Lloyd's treatment in 1957 <ref> (Lloyd 1982) </ref>. The GLA for training a VQ with Q cells and distortion measure d (x; ) is given below. 1. Start with an initial codebook C 1 .
Reference: <author> MacQueen, J. </author> <year> (1967), </year> <title> "Some methods for classification and analysis of multivariate observations", </title> <booktitle> in Proceedings of IEEE International Conference on Mathematics, Statistics and Probability, </booktitle> <publisher> University of California Press, </publisher> <pages> pp. 281-297. </pages>
Reference-contexts: We iterate these two steps till convergence. This iterative codebook improvement algorithm for training a VQ is called the generalized Lloyd algorithm (GLA) (Gersho & Gray 1992). It is also known as the k-means algorithm after MacQueen <ref> (MacQueen 1967) </ref> who studied it as a statistical clustering technique, and as the LBG algorithm in the signal processing literature after Linde, Buzo and Gray (1980) who give a detailed description of the algorithm for data compression applications is presented.
Reference: <author> Marroquin, J. </author> <year> (1995), </year> <title> "Measure fields for function approximation", </title> <journal> IEEE Transactions on Neural Networks 6(5), </journal> <pages> 1081-1090. </pages>
Reference: <author> McLachlan, G. & Basford, K. </author> <year> (1988), </year> <title> Mixture Models: Inference and Applications to Clustering, </title> <publisher> Marcel Dekker Inc., </publisher> <address> New York. </address>
Reference: <author> Moody, J. & Darken, C. </author> <year> (1988), </year> <title> "Learning with localized receptive fields", </title> <editor> in D. Touret-zky, G. Hinton & T. Sejnowski, eds, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <pages> pp. 133-143. </pages>
Reference-contexts: The regression function is a weighted average of the means of the response variables, where the weights represent the closeness to the corresponding predictor means. As Ghahramani (1994a) notes, this function has an identical form to normalized radial basis functions <ref> (Moody & Darken 1988, Poggio & Girosi 1990) </ref>. The GMR-const procedure can generate high errors whenever the diagonal assumption is incorrect due to correlations between the predictor and response variables, which is often the case. We propose two new ways of regularizing the local linear regression (7.11).
Reference: <author> Moody, J. & Darken, C. J. </author> <year> (1989), </year> <title> "Fast learning in networks of locally-tuned processing units", </title> <booktitle> Neural Computation 1, </booktitle> <pages> 281-294. </pages>
Reference: <author> Moody, J. & Yarvin, N. </author> <year> (1992), </year> <title> "Networks with learned unit response functions", </title> <editor> in J. Moody, S. Hanson & R. Lippmann, eds, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <pages> pp. 1048-1055. </pages>
Reference-contexts: (which is related to radial basis functions; see section 7.2) performs the worst with a statistically significant margin. 7.3.2 Prediction of the average monthly sunspots count Our next task is the prediction of the average monthly sunspot count in a given year from the values of the twelve previous years <ref> (Moody & Yarvin 1992) </ref>. <p> They note that the best test set error achieved by them or previous testers was about E norm = :085 <ref> (Moody & Yarvin 1992) </ref>. Our results are comparable to theirs. The GMR-prune algorithm (which uses PC pruning) prunes 5 directions from each local prediction matrix, and seems to provide the best performance among all algorithms tested.
Reference: <author> Morrison, D. F. </author> <year> (1976), </year> <title> Multivariate Statistical Methods, </title> <publisher> McGraw-Hill, </publisher> <address> New York. </address>
Reference: <author> Namphol, A., Arozullah, M. & Chin, S. </author> <year> (1991), </year> <title> "Higher order data compression with neural networks", </title> <booktitle> in Proceedings of the IJCNN, IEEE, </booktitle> <address> Piscataway, NJ, </address> <pages> pp. </pages> <month> I-55,I-59. </month>
Reference-contexts: FLNs have been used for the analysis of simulated chemical batch reaction data (Kramer 1991), for the learning of psychological color attributes from surface spectral reflectance data (Usui, Nakauchi & Nakano 1991), and for image compression <ref> (Namphol, Arozullah & Chin 1991) </ref>. For reducing dimension from n to m (m &lt; n), a five layered feedforward neural network has a bottleneck layer containing m nodes (see Figure 2.4).
Reference: <author> Newcomb, S. </author> <title> (1886), "A generalized theory of the combination of observations so as to obtain the best result", </title> <journal> American Journal of Mathematics 8, </journal> <pages> 343-366. </pages> <note> 168 Nowlan, </note> <author> S. </author> <year> (1991), </year> <title> Soft Competitive Adaptation: Neural Network Learning Algorithms based on Fitting Statistical Mixtures, </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University. </institution>
Reference: <author> Oja, E. </author> <year> (1983), </year> <title> Subspace Methods of Pattern Recognition, </title> <publisher> John Wiley and Sons Inc., </publisher> <address> New York. </address>
Reference-contexts: PCA reconstructs x from z as ^x = g (z) = V T z + : (2.4) PCA can also be defined in terms of the eigenvectors of the data correlation matrix R = E [xx T ] (e.g. see <ref> (Oja 1983) </ref>). Throughout this document, we will be using "PCA" to refer to the version which uses the covariance matrix. <p> A comprehensive discussion of the PCA projection and its optimality properties is given in <ref> (Oja 1983) </ref>. In the next subsection, we will outline a proof of the least squares optimality of PCA. 2.1.2 Least squares optimality of PCA In this section, we will outline a proof of the least squares optimality of PCA. <p> This implies that the optimal linear dimension reduction algorithm would project x onto the linear manifold spanned by the leading m eigenvectors of . This is precisely what PCA does. Thus PCA is optimal in the least squares sense. PCA also optimizes other criteria <ref> (Oja 1983) </ref>. The principal components are mutually uncorrelated and are the components along the directions of maximum variance for the n-vector x. In chapter 4, we derive the principal components as the maximum likelihood signal estimate in a probabilistic signal-plus-Gaussian noise model.
Reference: <author> Oja, E. </author> <year> (1989), </year> <title> "Neural networks, principal components, and subspaces", </title> <journal> International Journal of Neural Systems 1, </journal> <pages> 61-68. </pages>
Reference: <author> Oja, E. </author> <year> (1991), </year> <title> "Data compression, feature extraction, </title> <booktitle> and autoassociation in feedfor-ward neural networks", in Artificial Neural Networks, </booktitle> <publisher> Elsevier Science Publishers B.V. (North-Holland), </publisher> <pages> pp. 737-745. </pages>
Reference-contexts: PCA builds a global linear model of the data: an m dimensional hyperplane spanned by the leading eigenvectors 3 of the data co-variance matrix. PCA incurs a high error whenever the data has non-linear dependencies not eliminated by removing correlations. Five layered auto-associative neural networks (FLNs) <ref> (Kramer 1991, Oja 1991) </ref> are capable of building non-linear encoding f () and decoding g () functions, by capturing any non-linear dependencies among the data variables. These networks have at least one layer with a smaller number of nodes (m nodes) than the number of inputs (n).
Reference: <author> Ormoneit, D. & Tresp, V. </author> <year> (1995), </year> <title> "Improved Gaussian mixture density estimates using Bayesian penalty terms and network averaging", </title> <type> Technical Report FKI-205-95, </type> <institution> Technische Universitat Munchen. </institution>
Reference: <author> Pearson, K. </author> <title> (1894), "Contributions to the mathematical theory of evolution", </title> <journal> Philosophical Transactions of the Royal Society of London A 185, </journal> <pages> 71-110. </pages>
Reference: <author> Plomp, R., Pols, L. & van de Geer, J. </author> <year> (1967), </year> <title> "Dimensional analysis of vowel spectra", </title> <journal> The Journal of the Acoustical Society of America 41(3), </journal> <pages> 707-712. </pages>
Reference: <author> Poggio, T. & Girosi, F. </author> <year> (1990), </year> <title> "Networks for approximation and learning", </title> <booktitle> Proceedings of the IEEE 78, </booktitle> <pages> 1481-1497. </pages>
Reference: <author> Pols, L., Tromp, H. & Plomp, R. </author> <year> (1973), </year> <title> "Frequency analysis of Dutch vowels from 50 male speakers", </title> <journal> The Journal of the Acoustical Society of America 53(4), </journal> <pages> 1093-1101. </pages>
Reference-contexts: The formants appear as characteristic peaks in vowel spectra. We are particularly interested in the first three formants (F1, F2 and F3) since they provide the most discriminatory information about vowels (e.g see <ref> (Pols, Tromp & Plomp 1973) </ref>). Peterson and Barney (1952) showed that vowel categories are well separated by formant frequencies in a perceptual study.
Reference: <author> Pols, L., van der Kamp, L. & Plomp, R. </author> <year> (1969), </year> <title> "Perceptual and physical space of vowel sounds", </title> <journal> The Journal of the Acoustical Society of America 46(2), </journal> <pages> 458-467. </pages>
Reference: <author> Press, W., Flannery, B., Teukolsky, S. & Vetterling, W. </author> <year> (1987), </year> <title> Numerical Recipes the Art of Scientific Computing, </title> <publisher> Cambridge University Press, </publisher> <address> New York. </address>
Reference-contexts: The decomposition in (2.11) can always be done, no matter how singular X is. This is particularly useful when the input data is scarce (N is small). We implemented both the methods described above using routines from "The Numerical Recipes in C" <ref> (Press et al. 1987) </ref>. 24 2.1.4 The inadequacies of PCA As discussed in section 1.1.2, PCA is the optimal linear algorithm for minimizing the expected squared error between x and its reconstruction ^x. However, the linear assumption may be a severe restriction for some data sets. <p> We also implemented two batch mode optimization schemes: conjugate gradient descent (CGD) and the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm. Let p denote the total number of weights. CGD <ref> (Press et al. 1987) </ref> finds p mutually conjugate directions in the weight space. These directions are such that minimization along one of them does not spoil the previous minimization along the other directions. <p> These directions are such that minimization along one of them does not spoil the previous minimization along the other directions. CGD requires only O (p) storage and reaches the minimum in p steps if the cost function is quadratic in the weight space. BFGS <ref> (Press et al. 1987) </ref> makes use of the second order Hessian information of the cost function. The BFGS algorithm iteratively estimates the inverse Hessian and uses this curvature information to find new directions for line minimizations. <p> Therefore, the auto-covariance matrix is singular and we cannot use Householder reduction. For image data, we trained PCA using singular value decomposition (SVD; see section 2.1.3). We trained the FLNs using three optimization techniques: conjugate gradient descent (CGD), the BFGS algorithm (a quasi-Newton method <ref> (Press et al. 1987) </ref>), and stochastic 78 gradient descent (SGD). In order to limit the space of architectures, we only considered FLNs with the same number of nodes in both of the mapping (second and fourth) layers. <p> The MLPs had sigmoid non-linearities in the hidden layer with asymptotes 0 and 1. Each network had K (number of classes) binary target outputs indicating class membership. The networks were trained using a conjugate gradient descent (see section 2.2.3 and <ref> (Press et al. 1987) </ref>) optimization scheme to minimize the mean squared error between the targets and the network outputs. <p> For all the local linear models, we added a diagonal matrix *I (m+n)fi (m+n) to each local covariance matrix in each iteration of the EM algorithm. We trained multi-layer perceptrons (MLPs) with one hidden layer using a quasi-Newton BFGS <ref> (Press et al. 1987) </ref> optimization scheme. 7.3.1 Prediction of Boston housing prices Our first task is the prediction of housing prices in Boston (the response variable) from 13 factors (the predictor variables) (Breiman et al. 1984, Moody & Yarvin 1992).
Reference: <author> Priebe, C. & Marchette, D. </author> <year> (1991), </year> <title> "Adaptive mixtures: Recursive nonparametric pattern recognition", </title> <booktitle> Pattern Recognition 24(12), </booktitle> <pages> 1197-1209. </pages>
Reference: <author> Rabiner, L. R. </author> <year> (1989), </year> <title> "A tutorial on hidden Markov models and selected applications in speech recognition", </title> <editor> in A. Waibel & K.-F. Lee, eds, </editor> <booktitle> Readings in Speech Recognition, IEEE, </booktitle> <address> Piscataway, NJ, </address> <pages> pp. 267-296. </pages> <note> 169 Rubner, </note> <author> J. & Tavan, P. </author> <year> (1989), </year> <title> "A self-organizing network for principal component analysis", </title> <journal> Europhysics Letters 20, </journal> <pages> 693-698. </pages>
Reference-contexts: Discriminative cost functions such as the one mentioned above are sometimes used to train hidden Markov models for speech recognition <ref> (Rabiner 1989) </ref>.
Reference: <author> Rumelhart, D., Hinton, G. & Williams, R. </author> <year> (1986), </year> <title> "Learning internal representations by error propagation", </title> <editor> in D. Rumelhart, J. McClelland & the PDP Research Group, eds, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <publisher> The MIT Press, </publisher> <pages> pp. 318-362. </pages>
Reference-contexts: Thus the model is f (x) = &gt; &gt; &gt; &gt; &lt; f 1 (x) if x 2 R 1 . f Q (x) if x 2 R Q . 3 A feedforward neural network <ref> (Rumelhart, Hinton & Williams 1986, Hertz, Krogh & Palmer 1991) </ref> builds a global model of the data. Examples of a local model include the classification and regression trees (CART) algorithm (Breiman, Friedman, Olshen & Stone 1984) and Friedman's (1991) multivariate adaptive regression splines (MARS) algorithm. <p> We trained the FLNs using conjugate gradient descent, the BFGS algorithm and stochastic gradient descent. We computed the gradient of the cost function using the backpropogation algorithm <ref> (Rumelhart et al. 1986) </ref>. A learning algorithm is said to be in batch mode when each weight update uses information from all the data points in the training set.
Reference: <author> Sanger, T. </author> <year> (1989), </year> <title> "An optimality principle for unsupervised learning", </title> <editor> in D. Touretzky, ed., </editor> <booktitle> Advances in Neural Information Processing Systems 1, </booktitle> <publisher> Morgan Kauffmann, </publisher> <address> San Mateo, California. </address>
Reference: <author> Seber, G. & Wild, C. </author> <year> (1989), </year> <title> Nonlinear Regression, </title> <publisher> John Wiley and Sons, </publisher> <address> New York. </address>
Reference-contexts: ; 2 ) = i=1 (2 2 ) m=2 exp 2 2 = (2 2 ) Nm=2 exp 2 2 i=1 # 1 1 The parameter estimates which maximize p (Y j ; 2 ) are = ^ (the least squares estimate) and ^ 2 = S ( ^ )=m <ref> (Seber & Wild 1989) </ref>. 7.1.1 Linear regression model In the standard linear model (Draper & Smith 1981, Breiman et al. 1984, Seber & Wild 1989), g (x) is restricted to g (x) = !x where ! 2 R mfin .
Reference: <author> Srivastava, M. & Lee, G. </author> <year> (1984), </year> <title> "On the distribution of the correlation coefficient when sampling from a mixture of two bivariate normal densities: robustness and outliers", </title> <journal> Canadian Journal of Statistics 2, </journal> <pages> 119-133. </pages>
Reference-contexts: Mixture models are extensively used in applications where data can be viewed as arising from several populations mixed in varying proportions. Gaussian mixture models have been used for the identification of outliers (Aitkin & Wilson 1980), and for the investigation of robustness of certain statistics to departures from normality <ref> (Srivastava & Lee 1984) </ref>. Gaussian mixture models are widely used to tackle the "missing data problem" where one or more of the data variables may be unavailable (Dempster, Laird & Rubin 1977, Ghahramani 10 1994, Ghahramani & Jordan 1994b).
Reference: <author> Stensmo, M. & Sejnowski, T. J. </author> <year> (1995), </year> <title> "A mixture model system for medical and machine diagnosis", </title> <editor> in G. Tesauro, D. S. Touretzky & T. K. Leen, eds, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <publisher> The MIT Press, Cambridge, Mas-sachusetts, </publisher> <pages> pp. 1077-1084. </pages>
Reference-contexts: We explore the following ways of regularizing GMB classifiers: * A commonly used technique is to assume that all covariance matrices are diagonal (Nowlan 1991, Ahmad & Tresp 1993) or spherically symmetric <ref> (Stensmo & Sejnowski 1995, Nowlan 1991) </ref>. This greatly reduces the number of parameters. However, this assumption is often invalid. <p> The diagonal assumption has been effectively used for classification tasks (Nowlan 1991) and for solving the missing data problem (Ahmad & Tresp 1993). The spherically symmetric assumption has been widely used for classification and regression tasks <ref> (Stensmo & Sejnowski 1995, Nowlan 1991) </ref>. These assumptions greatly reduce the number of parameters of the mixture models. Stensmo and Sejnowski (1995) and Nowlan (1991) have argued that the loss in flexibility due to the diagonal or spherical symmetry assumptions can be compensated for by using more mixture components.
Reference: <author> Titterington, D., Smith, A. & Makov, U. </author> <year> (1985), </year> <title> Statistical Analysis of Finite Mixture Distributions, </title> <publisher> John Wiley and Sons Inc., </publisher> <address> New York. </address>
Reference: <author> Tresp, V., Ahmad, S. & Neuneier, R. </author> <year> (1994), </year> <title> "Training neural networks with deficient data", </title> <editor> in J. D. Cowan, G. Tesauro & J. Alspector, eds, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <pages> pp. 128-135. </pages>
Reference-contexts: One way to obtain numerical stability (avoid singularities) is to impose an artificial lower bound on the volume elements (determinants of covariance matrices) of each Gaussian <ref> (Ghahramani 1994, Ormoneit & Tresp 1995) </ref> during parameter estimation. This is achieved by adding a small diagonal matrix *I nfin to each covariance matrix in each iteration of the EM algorithm. Adding *I nfin avoids singularities and also regularizes the mixture model.
Reference: <author> Usui, S., Nakauchi, S. & Nakano, M. </author> <year> (1991), </year> <title> "Internal color representation acquired by a five-layer neural network", </title> <editor> in T. Kohonen, K. Makisara, O. Simula & J. Kangas, eds, </editor> <booktitle> Artificial Neural Networks, </booktitle> <publisher> Elsevier Science Publishers, North-Holland, </publisher> <pages> pp. 867-872. </pages>
Reference-contexts: FLNs have been used for the analysis of simulated chemical batch reaction data (Kramer 1991), for the learning of psychological color attributes from surface spectral reflectance data <ref> (Usui, Nakauchi & Nakano 1991) </ref>, and for image compression (Namphol, Arozullah & Chin 1991). For reducing dimension from n to m (m &lt; n), a five layered feedforward neural network has a bottleneck layer containing m nodes (see Figure 2.4).
Reference: <author> Watanabe, S. </author> <year> (1965), </year> <title> "Karhunen-Loeve expansion and factor analysis, theoretical remarks and applications", </title> <booktitle> in Transactions of the 4th Prague Conference on Information Theory, Statistical Decision Functions and Random Processes, </booktitle> <pages> pp. 645-660. </pages>
Reference-contexts: PCA was first proposed by Hotelling (1933) for dimension reduction. Anderson (1958) and Morrison (1976) use PCA to reduce the number of variables by eliminating linear combinations with small variance. Oja (1983) discusses PCA and related techniques. A closely related orthogonal expansion to PCA is the Karhunen-Loeve (K-L) expansion <ref> (Watanabe 1965) </ref> which was originally conceived in the framework of continuous second-order stochastic processes. When restricted to a finite dimensional case and truncated after a few terms, the K-L expansion is equivalent to a PCA expansion.
Reference: <author> Wax, M. & Kailath, T. </author> <year> (1985), </year> <title> "Detection of signals by information theoretic criteria", </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing ASSP-33(2), </journal> <pages> 387-392. </pages> <note> 170 Widrow, </note> <author> B. & M. Hoff, Jr. </author> <year> (1960), </year> <title> "Adaptive switching circuits", </title> <booktitle> in IRE WESCON Convention Record, </booktitle> <pages> pp. 96-104. </pages>
Reference-contexts: When restricted to a finite dimensional case and truncated after a few terms, the K-L expansion is equivalent to a PCA expansion. PCA is used for feature extraction and data compression (Devijver & Kittler 1982), image compression (Jain 1989) and characterization of signals in signal processing <ref> (Wax & Kailath 1985) </ref>. Fukunaga and Olsen (1971) and Hediger et al (Hediger, Passamante 18 & Farrell 1990) use PCA in local regions of the input space to determine the intrinsic dimensionality of a data set.
Reference: <author> Wolfe, J. </author> <year> (1970), </year> <title> "Pattern clustering by multivariate mixture analysis", </title> <journal> Multivariate Behavioural Research 5, </journal> <pages> 329-350. </pages>
Reference: <author> Yang, J. & Dumont, G. A. </author> <year> (1991), </year> <title> "Classification of acoustic emission signals via Hebbian feature extraction", </title> <booktitle> in Proceedings of the IJCNN, IEEE, </booktitle> <address> Piscataway, NJ, </address> <pages> pp. </pages> <note> I-113 to I-118. </note>
References-found: 110

