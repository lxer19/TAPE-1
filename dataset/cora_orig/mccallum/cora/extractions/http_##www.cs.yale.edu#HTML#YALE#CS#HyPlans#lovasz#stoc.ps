URL: http://www.cs.yale.edu/HTML/YALE/CS/HyPlans/lovasz/stoc.ps
Refering-URL: http://www.cs.yale.edu/HTML/YALE/CS/HyPlans/lovasz/papers.html
Root-URL: http://www.cs.yale.edu
Email: lovasz@cs.yale.edu  pw@research.att.com  
Title: Efficient Stopping Rules for Markov Chains (EXTENDED ABSTRACT)  
Author: L aszl o Lov asz Peter Winkler 
Note: Some of the work described herein is joint with David Aldous.  
Address: New Haven CT 06510;  2D-147, Murray Hill NJ 07974;  
Affiliation: Dept. of Computer Science, Yale University,  AT&T Bell Laboratories  
Abstract: Let M be the transition matrix, and the initial state distribution, for a discrete-time finite-state irreducible Markov chain. A stopping rule for M is an algorithm which observes the progress of the chain and then stops it at some random time ; the distribution of the final state is denoted by . We give a useful characterization for stopping rules which are optimal for given target distribution t, in the sense that = t and the expected stopping time E is minimal. Four classes of optimal stopping rules are described, including a unique "threshold" rule which also minimizes max(). The minimum value of E, which we denote by H(; t), is easily computable from the hitting times of M. For applications in computing, the most important case is when is concentrated on a single starting state s and t is the stationary distribution . We describe a simple, practical stopping rule which achieves target distribution close to in expected time of order T mix = max s H(s;). Finally, we give a stopping rule that runs in time polynomial in the maximum hitting time of M and achieves the stationary distribution exactly, even though the transition probabilities of the chain are unknown. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D.J. Aldous, </author> <title> Reversible Markov Chains and Random Walks on Graphs (book), </title> <note> to appear. </note>
Reference-contexts: Pr (w) = w 0 i=0 The distribution of w t will be denoted by t , so that 0 = and t P It will be useful to us to regard a stopping rule both as a random stopping time and as a (partial) function from S fl to <ref> [0; 1] </ref> giving the probability of continuing from a walk w. Formally: Definition. A stopping rule is a partial map from S fl to [0; 1], such that (w) is defined for w = (w 0 ; : : :; w t ) just when Pr (w) &gt; 0 and (w <p> It will be useful to us to regard a stopping rule both as a random stopping time and as a (partial) function from S fl to <ref> [0; 1] </ref> giving the probability of continuing from a walk w. Formally: Definition. A stopping rule is a partial map from S fl to [0; 1], such that (w) is defined for w = (w 0 ; : : :; w t ) just when Pr (w) &gt; 0 and (w 0 ; : : : ; w i ) is defined and non-zero for each i, 0 i &lt; t. <p> In the case when t = is the stationary distribution, this formula can be simplified using what we call the "random target identity," which says that X j H (i; j) = C (1) is independent of the choice of i (see e.g. the "right averaging principle" in Aldous <ref> [1] </ref>). Thus the mean length of the naive rule to reach is C, independently of the starting distribution. The maximum length max () of a stopping rule is the maximum length of a walk that has positive probability. is said to be bounded (for ) if this quantity is finite. <p> Indeed, versions of Lemma 4.2 and Theorem 4.4 below are proved in [19] using Pitman's "occupation measure identity." We begin with some simple facts about exit frequencies. The first result is from Aldous <ref> [1] </ref>; several related formulas can be derived using relations to electrical networks, as in [9], [11], or [20]. <p> Another rather neat way to think of the chain rule is to assign the real value r (i) = P f (U ) : i 2 U g to each state i. The chain rule is then implemented by choosing a random real r uniformly from <ref> [0; 1] </ref> and walking until a state j with r (j) r is reached. 6 Threshold Rules A stopping rule is said to be of the "threshold type", or for short to be a threshold rule, if there is a threshold vector h = (h 1 ; : : :; h <p> state j with r (j) r is reached. 6 Threshold Rules A stopping rule is said to be of the "threshold type", or for short to be a threshold rule, if there is a threshold vector h = (h 1 ; : : :; h n ), h i 2 <ref> [0; 1] </ref> such that (w 0 ; : : : ; w k ) = 0 if k h w k k h w k otherwise. Thus in effect each state is provided with a critical time.
Reference: [2] <author> D.J. Aldous, </author> <title> On the Markov chain simulation method for uniform combinatorial distributions and simulated annealing, </title> <booktitle> Probability in the Engineering and Informational Sciences, 1 (1987), </booktitle> <pages> 33-46. </pages>
Reference-contexts: We note that similar convergence speed can be achieved using other "averaging" rules, e.g., the Poisson stopping time of Aldous <ref> [2] </ref>. 8 Exact Mixing in an Unknown Chain In the previous section we saw that there is at least one practical stopping rule for approximating the stationary distribution when all that is known about the chain is its mixing time (or a good upper bound for it).
Reference: [3] <author> D.J. Aldous, </author> <title> Applications of random walks on graphs, </title> <note> preprint (1989). </note>
Reference-contexts: 1 Introduction In the past ten years many new applications have been found (see, e.g., <ref> [3] </ref>) for sampling via Markov chains; these have resulted in polynomial-time randomized approximation schemes for computing the volume of a convex body [12] and counting combinatorial objects such as matchings [13], linear extensions [14], and Eulerian orientations [18].
Reference: [4] <author> D.J. Aldous, </author> <title> On simulating a Markov chain stationary distribution when transition probabilities are unknown, </title> <month> prepint </month> <year> (1993). </year>
Reference-contexts: Even this requirement can be circumvented: Aldous <ref> [4] </ref> gives a rule that comes within total variation " of the stationary distribution in expected time O (h=" 2 where h = max i;j H (i; j) is the maximum hitting time of the chain, provided the number n of states is known.
Reference: [5] <author> S. Asmussen, P. W. Glynn and H. Thorisson, </author> <title> Stationary detection in the initial transient problem, </title> <booktitle> ACM Transactions on Modeling and Computer Simulation 2 (1992), </booktitle> <pages> 130-157. </pages>
Reference-contexts: Amazingly, one can even achieve the the stationary distribution exactly in an unknown chain. A stopping rule for doing so is described in Asmussen et al. <ref> [5] </ref>, but it is complex and requires perfect generation of random variables with certain exponential distributions. The expected number of steps required appears to be super-polynomial in the maximum hitting time, although no bound or estimate is given in the paper.
Reference: [6] <author> D.J. Aldous, L. Lovasz and P. Winkler, </author> <title> Fast mixing in a Markov chain, </title> <note> in preparation. </note>
Reference-contexts: The expected number of steps for this rule is nowhere near optimal, but is nonetheless polynomial in the maximum hitting time of the chain. Work described herein will be found in three papers, <ref> [6] </ref>, [15] and [16]; the first of these is joint with David Aldous, of the Dept. of Statistics, U.C. Berkeley. Research of the first author was supported in part by NSF grant No.
Reference: [7] <author> J.R. Baxter and R.V. Chacon, </author> <title> Stopping times for recurrent Markov processes, </title> <journal> Illinois J. Math. </journal> <volume> 20 (1976), </volume> <pages> 467-475. </pages>
Reference-contexts: Such stopping rules (times) have been studied in the theory of Markov chains (see e.g. <ref> [7] </ref>), but not from the point of view of algorithmic efficiency. In this work we study stopping rules that achieve any given distribution, when starting from some other given distribution. <p> The converse is less obvious: we must provide, for any two distributions and t , a stopping rule that has a halting state. The proof makes use of the discrete version of the "filling scheme," introduced by Chacon and Ornstein [8] and shown by Baxter and Chacon <ref> [7] </ref> to minimize expected number of steps. We call it the filling rule t (or ;t when dependence on initial distribution must be made explicit) and define it recursively as follows.
Reference: [8] <author> R.V. Chacon and D.S. Ornstein, </author> <title> A general ergodic theorem, </title> <journal> Illinois J. Math. </journal> <volume> 4 (1960), </volume> <pages> 153-160. </pages>
Reference-contexts: The converse is less obvious: we must provide, for any two distributions and t , a stopping rule that has a halting state. The proof makes use of the discrete version of the "filling scheme," introduced by Chacon and Ornstein <ref> [8] </ref> and shown by Baxter and Chacon [7] to minimize expected number of steps. We call it the filling rule t (or ;t when dependence on initial distribution must be made explicit) and define it recursively as follows.
Reference: [9] <author> A.K. Chandra, P. Raghavan, W.L. Ruzzo, R. Smolen-sky, and P. Tiwari, </author> <title> The Electrical Resistance of a Graph Captures its Commute and Cover Times, </title> <booktitle> Proceedings of the 21st Annual ACM Symposium on Theory of Computing, </booktitle> <month> May </month> <year> (1989). </year>
Reference-contexts: Indeed, versions of Lemma 4.2 and Theorem 4.4 below are proved in [19] using Pitman's "occupation measure identity." We begin with some simple facts about exit frequencies. The first result is from Aldous [1]; several related formulas can be derived using relations to electrical networks, as in <ref> [9] </ref>, [11], or [20].
Reference: [10] <author> D. Coppersmith, P. Tetali, and P. Winkler, </author> <title> Collisions among Random Walks on a Graph, </title> <note> SIAM J. on Discrete Mathematics 6 No. </note> <month> 3 </month> <year> (1993), </year> <pages> 363-374. </pages>
Reference-contexts: H (; t ) + H (t; j). 2 In the important special case of time-reversible chains, we can use the "cycle-reversing" identity H (i; j) + H (j; k) + H (k; i) = H (i; k) + H (k; j) + H (j; i): of Coppersmith et al. <ref> [10] </ref> to obtain the following formula for the exit frequencies of the naive rule: ~x k = k i;j i;j + i ! It follows that the exit frequencies of a mean-optimal rule from to t are x k (; t ) = k X (t i i )H (k; i)
Reference: [11] <author> P.G. Doyle and J.L. Snell, </author> <title> Random Walks and Electric Networks, </title> <publisher> Mathematical Assoc. of America, </publisher> <address> Washing-ton, DC 1984. </address>
Reference-contexts: Indeed, versions of Lemma 4.2 and Theorem 4.4 below are proved in [19] using Pitman's "occupation measure identity." We begin with some simple facts about exit frequencies. The first result is from Aldous [1]; several related formulas can be derived using relations to electrical networks, as in [9], <ref> [11] </ref>, or [20].
Reference: [12] <author> M. Dyer, A. Frieze and R. Kannan, </author> <title> A random polynomial time algorithm for estimating volumes of convex bodies, </title> <booktitle> Proc. 21st Annual ACM Symposium on the Theory of Computing (1989), </booktitle> <pages> 375-381. </pages>
Reference-contexts: 1 Introduction In the past ten years many new applications have been found (see, e.g., [3]) for sampling via Markov chains; these have resulted in polynomial-time randomized approximation schemes for computing the volume of a convex body <ref> [12] </ref> and counting combinatorial objects such as matchings [13], linear extensions [14], and Eulerian orientations [18].
Reference: [13] <author> M. Jerrum and A. Sinclair, </author> <title> Conductance and the rapid mixing property for Markov chains: the approximation of the permanent resolved, </title> <booktitle> Proc. 20nd Annual ACM Symposium on Theory of Computing (1988), </booktitle> <pages> 235-243. </pages>
Reference-contexts: 1 Introduction In the past ten years many new applications have been found (see, e.g., [3]) for sampling via Markov chains; these have resulted in polynomial-time randomized approximation schemes for computing the volume of a convex body [12] and counting combinatorial objects such as matchings <ref> [13] </ref>, linear extensions [14], and Eulerian orientations [18]. Typically, in these applications, sampling from an approximately correct distribution is accomplished by walking randomly on a graph for a fixed number of steps, after which the distribution of the occupied vertex is nearly stationary.
Reference: [14] <author> A. Karzanov and L. Khachiyan, </author> <title> On the conductance of order Markov chains, </title> <type> Technical Report DCS TR 268, </type> <institution> Rutgers University, </institution> <month> June </month> <year> 1990. </year> <month> 6 </month>
Reference-contexts: 1 Introduction In the past ten years many new applications have been found (see, e.g., [3]) for sampling via Markov chains; these have resulted in polynomial-time randomized approximation schemes for computing the volume of a convex body [12] and counting combinatorial objects such as matchings [13], linear extensions <ref> [14] </ref>, and Eulerian orientations [18]. Typically, in these applications, sampling from an approximately correct distribution is accomplished by walking randomly on a graph for a fixed number of steps, after which the distribution of the occupied vertex is nearly stationary.
Reference: [15] <author> L. Lovasz and P. Winkler, </author> <title> Exact mixing in an unknown Markov chain, </title> <note> in preparation. </note>
Reference-contexts: The expected number of steps for this rule is nowhere near optimal, but is nonetheless polynomial in the maximum hitting time of the chain. Work described herein will be found in three papers, [6], <ref> [15] </ref> and [16]; the first of these is joint with David Aldous, of the Dept. of Statistics, U.C. Berkeley. Research of the first author was supported in part by NSF grant No.
Reference: [16] <author> L. Lovasz and P. Winkler, </author> <title> The forget time of a Markov chain, </title> <note> in preparation. </note>
Reference-contexts: The expected number of steps for this rule is nowhere near optimal, but is nonetheless polynomial in the maximum hitting time of the chain. Work described herein will be found in three papers, [6], [15] and <ref> [16] </ref>; the first of these is joint with David Aldous, of the Dept. of Statistics, U.C. Berkeley. Research of the first author was supported in part by NSF grant No.
Reference: [17] <author> L. Lovasz and M. Simonovits, </author> <title> Random walks in a convex body and an improved volume algorithm, Random Structures and Alg. </title> <booktitle> 4 (1993), </booktitle> <pages> 359-412. </pages>
Reference-contexts: The simplest blind stopping rule is the stopping rule used most often: "stop after t steps." Equal practicality and better convergence guarantees can be obtained from lazy random walks, considered e.g. in Lovasz and Simonovits <ref> [17] </ref>.
Reference: [18] <author> M. Mihail and P. Winkler, </author> <title> On the number of Eulerian orientations of a graph, </title> <booktitle> Proc. 3rd ACM-SIAM Symposium on Discrete Algorithms (1992), </booktitle> <pages> 138-145. </pages>
Reference-contexts: In the past ten years many new applications have been found (see, e.g., [3]) for sampling via Markov chains; these have resulted in polynomial-time randomized approximation schemes for computing the volume of a convex body [12] and counting combinatorial objects such as matchings [13], linear extensions [14], and Eulerian orientations <ref> [18] </ref>. Typically, in these applications, sampling from an approximately correct distribution is accomplished by walking randomly on a graph for a fixed number of steps, after which the distribution of the occupied vertex is nearly stationary.
Reference: [19] <author> J.W. </author> <title> Pitman, Occupation measures for Markov chains, </title> <journal> Adv. Appl. Prob. </journal> <volume> 9 (1977), </volume> <pages> 69-86. </pages>
Reference-contexts: Definition. The exit frequencies x = fx i g i2S for are given by setting x i equal to the expected number of times the walk leaves state i before stopping. Exit frequencies are a special case of what Pitman <ref> [19] </ref> calls "pre-T occupation measures" for stopping times T . Indeed, versions of Lemma 4.2 and Theorem 4.4 below are proved in [19] using Pitman's "occupation measure identity." We begin with some simple facts about exit frequencies. <p> Exit frequencies are a special case of what Pitman <ref> [19] </ref> calls "pre-T occupation measures" for stopping times T . Indeed, versions of Lemma 4.2 and Theorem 4.4 below are proved in [19] using Pitman's "occupation measure identity." We begin with some simple facts about exit frequencies. The first result is from Aldous [1]; several related formulas can be derived using relations to electrical networks, as in [9], [11], or [20]. <p> distribution are given by ~x k = k i;j = k X i t j H (i; j) + H (t; k) H (; k) : Counting how often a state i is entered and exited, we get the following relation between exit frequencies and starting and ending distributions (Pitman <ref> [19] </ref>): Lemma 4.2 The exit frequencies of any stopping rule that reaches distribution t from distribution satisfy the equations X i Theorem 4.3 Fix and let and 0 be two finite stopping rules with exit frequencies x and x 0 respectively.
Reference: [20] <author> P. Tetali, </author> <title> Random walks and effective resistance of networks, </title> <journal> J. Theoretical Prob. </journal> <volume> No. 1 (1991), </volume> <pages> 101-109. 7 </pages>
Reference-contexts: The first result is from Aldous [1]; several related formulas can be derived using relations to electrical networks, as in [9], [11], or <ref> [20] </ref>.
References-found: 20

