URL: ftp://ftp.cs.virginia.edu/pub/techreports/CS-94-14.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Dynamic Access Ordering for Symmetric Shared-Memory Multiprocessors  
Author: Sally A. McKee 
Abstract: Computer Science Report No. CS-94-14 May, 1994 
Abstract-found: 1
Intro-found: 1
Reference: [Abu79] <author> Abu-Sufah, W., Kuck, D.J., and Lawrie, D.H., </author> <title> Automatic Program Transformations for Virtual Memory Computers, </title> <booktitle> Proc. 1979 National Computer Conference, </booktitle> <month> June, </month> <year> 1979. </year>
Reference: [Abu86] <author> Abu-Sufah, W., and Malony, A., </author> <title> Vector Processing on the Alliant FX/8 Multiprocessors, </title> <booktitle> Proc. 1986 International Conference on Parallel Processing, </booktitle> <month> August, </month> <year> 1986. </year>
Reference: [Bae91] <author> Baer, J.L., and Chen, T.F., </author> <title> An Effective On-Chip Preloading Scheme to Reduce Data Access Penalty, </title> <booktitle> Proc. Supercomputing 91, </booktitle> <month> November, </month> <year> 1991. </year>
Reference-contexts: Prefetching and nonblocking caches can be used to overlap memory accesses with computation, or to overlap the latencies of more than one access <ref> [Bae91, Cal91, Gup91, Kla91, Mow92, Soh91] </ref>. These techniques can improve processor performance, but techniques that simply mask latency do nothing to increase effective bandwidth. Such schemes are still useful, but they will be most effective when combined with complementary technology to exploit memory component capabilities.
Reference: [Bud71] <author> Budnik, P., and Kuck, D., </author> <title> The Organization and Use of Parallel Memories, </title> <journal> IEEE Trans. Comput., </journal> <volume> 20, 12, </volume> <year> 1971. </year>
Reference-contexts: These techniques will also deliver better performance when integrated with access ordering. Several schemes for avoiding bank contention, either by address transformations, skewing, or prime memory systems, have been published <ref> [Bud71, Gao93, Har87, Har89, Rau91] </ref>; we do not discuss them, other than to note that these, too, are complementary to the access ordering. 8. Conclusions As processors become faster, memory bandwidth is rapidly becoming the performance bottleneck in the application of high performance microprocessors to vector-like algorithms.
Reference: [Cal91] <author> Callahan, D., Kennedy, K., and Porterfield, A., </author> <title> Software Prefetching, </title> <booktitle> Proc. Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April, </month> <year> 1991. </year>
Reference-contexts: Prefetching and nonblocking caches can be used to overlap memory accesses with computation, or to overlap the latencies of more than one access <ref> [Bae91, Cal91, Gup91, Kla91, Mow92, Soh91] </ref>. These techniques can improve processor performance, but techniques that simply mask latency do nothing to increase effective bandwidth. Such schemes are still useful, but they will be most effective when combined with complementary technology to exploit memory component capabilities.
Reference: [Car89] <author> Carr, S., Kennedy, K., </author> <title> Blocking Linear Algebra Codes for Memory Hierarchies, </title> <booktitle> Proc. Fourth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <year> 1989. </year>
Reference-contexts: Such schemes are still useful, but they will be most effective when combined with complementary technology to exploit memory component capabilities. Modifying the computation to increase the reuse of cached data can improve performance dramatically <ref> [Gal87, Gan87, Car89, Por89, Wol90, Lam91, Tem93] </ref>. These techniques will also deliver better performance when integrated with access ordering.
Reference: [Con90] <institution> CONVEX Architecture Reference (C200 Series), CONVEX Computer Corporation Document No. 081-009220-000, 5th ed., </institution> <month> September </month> <year> 1990. </year>
Reference: [Dav91] <author> Davidson, J.W., and Benitez, </author> <title> M.E., Code Generation for Streaming: An Access/Execute Mechanism, </title> <booktitle> Proc. Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April, </month> <year> 1991. </year>
Reference: [Dig92] <institution> Alpha Architecture Handbook, Digital Equipment Corporation, </institution> <year> 1992. </year>
Reference: [Don79] <author> Dongarra, J.J., et. al., </author> <title> Linpack Users Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference: [Don90] <author> Dongarra, J.J., DuCroz, J., Duff, I., and Hammerling, S., </author> <title> A set of Level 3 Basic Linear Algebra Subprograms, </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 16 </volume> <pages> 1-17, </pages> <year> 1990. </year>
Reference: [Far92] <author> Farmwald, M., and Morring, D., </author> <title> A Fast Path to One Memory, </title> <booktitle> in [IEEE92], </booktitle> <pages> pp. 50-51, </pages> <month> October </month> <year> 1992. </year>
Reference: [Gal87] <author> Gallivan, K., Jalby, W., Meier, U., and Sameh, A., </author> <title> The Impact of Hierarchical Memory Systems on Linear Algebra Algorithm Design, </title> <type> Technical Report UIUCSRD 625, </type> <institution> University of Illinois, </institution> <year> 1987. </year>
Reference-contexts: Such schemes are still useful, but they will be most effective when combined with complementary technology to exploit memory component capabilities. Modifying the computation to increase the reuse of cached data can improve performance dramatically <ref> [Gal87, Gan87, Car89, Por89, Wol90, Lam91, Tem93] </ref>. These techniques will also deliver better performance when integrated with access ordering.
Reference: [Gan87] <author> Gannon, D., and Jalby, W., </author> <title> The Inuence of Memory Hierarchy on Dynamic Access Ordering for Symmetric Shared-Memory Multiprocessors 123 Algorithm Organization: Programming FFTs on a Vector Multiprocessor, in The Characteristics of Parallel Algorithms. </title> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Such schemes are still useful, but they will be most effective when combined with complementary technology to exploit memory component capabilities. Modifying the computation to increase the reuse of cached data can improve performance dramatically <ref> [Gal87, Gan87, Car89, Por89, Wol90, Lam91, Tem93] </ref>. These techniques will also deliver better performance when integrated with access ordering.
Reference: [Gao93] <author> Gao, </author> <title> Q.S., The Chinese Remainder Theorem and the Prime Memory System, </title> <booktitle> Proc. 20th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: These techniques will also deliver better performance when integrated with access ordering. Several schemes for avoiding bank contention, either by address transformations, skewing, or prime memory systems, have been published <ref> [Bud71, Gao93, Har87, Har89, Rau91] </ref>; we do not discuss them, other than to note that these, too, are complementary to the access ordering. 8. Conclusions As processors become faster, memory bandwidth is rapidly becoming the performance bottleneck in the application of high performance microprocessors to vector-like algorithms.
Reference: [Gol93] <author> Golub, G., and Ortega, J.M., </author> <title> Scientific Computation: An Introduction with Parallel Computing, </title> <publisher> Academic Press, Inc., </publisher> <year> 1993. </year>
Reference: [Gup91] <author> Gupta, A., Hennessy, J., Gharachorloo, K., Mowry, T., and Weber, W.-D., </author> <title> Comparative Evaluation of Latency Reducing and Tolerating Techniques, </title> <booktitle> Proc. 18th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: Prefetching and nonblocking caches can be used to overlap memory accesses with computation, or to overlap the latencies of more than one access <ref> [Bae91, Cal91, Gup91, Kla91, Mow92, Soh91] </ref>. These techniques can improve processor performance, but techniques that simply mask latency do nothing to increase effective bandwidth. Such schemes are still useful, but they will be most effective when combined with complementary technology to exploit memory component capabilities.
Reference: [Har87] <author> Harper, D. T., Jump, J., </author> <title> Vector Access Performance in Parallel Memories Using a Skewed Storage Scheme, </title> <journal> IEEE Trans. Comput., </journal> <volume> 36, 12, </volume> <year> 1987. </year>
Reference-contexts: These techniques will also deliver better performance when integrated with access ordering. Several schemes for avoiding bank contention, either by address transformations, skewing, or prime memory systems, have been published <ref> [Bud71, Gao93, Har87, Har89, Rau91] </ref>; we do not discuss them, other than to note that these, too, are complementary to the access ordering. 8. Conclusions As processors become faster, memory bandwidth is rapidly becoming the performance bottleneck in the application of high performance microprocessors to vector-like algorithms.
Reference: [Har89] <author> Harper, D. T., </author> <title> Address Transformation to Increase Memory Performance, </title> <booktitle> 1989 International Conference on Supercomputing. </booktitle>
Reference-contexts: These techniques will also deliver better performance when integrated with access ordering. Several schemes for avoiding bank contention, either by address transformations, skewing, or prime memory systems, have been published <ref> [Bud71, Gao93, Har87, Har89, Rau91] </ref>; we do not discuss them, other than to note that these, too, are complementary to the access ordering. 8. Conclusions As processors become faster, memory bandwidth is rapidly becoming the performance bottleneck in the application of high performance microprocessors to vector-like algorithms.
Reference: [Har92] <author> Hart, C., </author> <title> Dynamic RAM as Secondary Cache, </title> <editor> in [IEEE92], p. </editor> <volume> 48, </volume> <month> October, </month> <year> 1992. </year>
Reference: [Hen90] <author> Hennessy, J., and Patterson, D., </author> <title> Computer Architecture: A Quantitative Approach, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: 1. Introduction Processor speeds are increasing much faster than memory speeds <ref> [Kat89, Hen90] </ref>. As a result, memory bandwidth is rapidly becoming the limiting performance factor for many applications. A comprehensive, successful solution to the memory bandwidth problem must exploit the richness of the full memory hierarchy.
Reference: [IEEE92] <author> High-speed DRAMs, </author> <title> Special Report, </title> <journal> IEEE Spectrum, </journal> <volume> vol. 29, no. 10, </volume> <month> October </month> <year> 1992. </year> <title> [Int91] i860 XP Microprocessor Data Book, </title> <publisher> Intel Corporation, </publisher> <year> 1991. </year> <title> [Int92] i860 Microprocessor Family Programmers Reference Manual, </title> <publisher> Intel Corporation, </publisher> <year> 1991. </year>
Reference: [Jon92] <author> Jones, F., </author> <title> A New Era of Fast Dynamic RAMs, </title> <booktitle> in [IEEE92], </booktitle> <pages> pp. 43-49, </pages> <month> October, </month> <year> 1992. </year>
Reference: [Kat89] <author> Katz, R., and Hennessy, J., </author> <title> High Performance Microprocessor Architectures, </title> <institution> University of California, Berkeley, </institution> <note> Report No. UCB/CSD 89/529, </note> <month> August, </month> <year> 1989. </year>
Reference-contexts: 1. Introduction Processor speeds are increasing much faster than memory speeds <ref> [Kat89, Hen90] </ref>. As a result, memory bandwidth is rapidly becoming the limiting performance factor for many applications. A comprehensive, successful solution to the memory bandwidth problem must exploit the richness of the full memory hierarchy.
Reference: [Lai92] <author> Laird, M., </author> <title> A Comparison of Three Current Superscalar Designs, Computer Architecture News, </title> <address> 20:3, </address> <month> June, </month> <year> 1992. </year>
Reference: [Lam91] <author> Lam, M., Rothberg, E., and Wolf, M., </author> <title> The Cache Performance and Optimizations of Blocked Algorithms, </title> <booktitle> Proc. </booktitle> <address> ASPLOS-IV. </address>
Reference-contexts: Such schemes are still useful, but they will be most effective when combined with complementary technology to exploit memory component capabilities. Modifying the computation to increase the reuse of cached data can improve performance dramatically <ref> [Gal87, Gan87, Car89, Por89, Wol90, Lam91, Tem93] </ref>. These techniques will also deliver better performance when integrated with access ordering.
Reference: [Law79] <editor> Lawson, et. al., </editor> <title> Basic Linear Algebra Subprograms for Fortran Usage, Dynamic Access Ordering for Symmetric Shared-Memory Multiprocessors 124 ACM Trans. </title> <journal> Math. Soft., </journal> <volume> 5, 3, </volume> <year> 1979. </year>
Reference: [Lee91] <author> Lee, K. </author> <title> Achieving High Performance on the i860 Microprocessor, </title> <type> NAS Technical Report RNR-91-029, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> October </month> <year> 1991. </year>
Reference: [Lee92] <author> Lee, K. </author> <title> On the Floating Point Performance of the i860 Microprocessor, </title> <type> NAS Technical Report RNR-90-019, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> July </month> <year> 1992. </year>
Reference: [Lee93] <author> Lee, K. </author> <title> The NAS860 Library Users Manual, </title> <type> NAS Technical Report RND-93-003, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> March </month> <year> 1993. </year>
Reference: [Los92] <author> Loshin, D., and Budge, D., </author> <title> Breaking the Memory Bottleneck, Parts 1 & 2, </title> <booktitle> Supercomputing Review, </booktitle> <address> January/February, </address> <year> 1992. </year>
Reference: [Mea92] <author> Meadows, L., Nakamoto, S., Schuster, V., </author> <title> A Vectorizing Software Pipelining Compiler for LIW and Superscalar Architectures, </title> <booktitle> Proc. RISC 92. </booktitle>
Reference: [McK69] <author> McKeller, A.C., and Coffman, E.G., </author> <title> The Organization of Matrices and Matrix Operations in a Paged Multiprogramming Environment, </title> <journal> CACM, </journal> <volume> 12:3, </volume> <year> 1969. </year>
Reference: [McK93a] <author> McKee, S.A, </author> <title> Hardware Support for Access Ordering: Performance of Some Design Options, </title> <institution> University of Virginia, Department of Computer Science, </institution> <type> Technical Report CS-93-08, </type> <month> August </month> <year> 1993. </year>
Reference-contexts: McKee et. al. propose a uniprocessor architecture for performing access ordering at run time [McK94a]. Simulation studies indicate that dynamic access ordering is a valuable technique for improving uniprocessor memory performance for stream computations the SMC, or Stream Memory Controller, consistently delivers almost the entire available bandwidth <ref> [McK93a, McK94b, McK93c] </ref>. The applicability of dynamic access ordering is not limited to uniprocessor environments. This paper discusses the effectiveness of dynamic access ordering with respect to the memory performance of symmetric multiprocessor (SMP) systems. <p> The Stream Memory Controller Access ordering systems span a spectrum ranging from purely compile-time to purely run-time approaches. A general taxonomy of access ordering systems is presented in <ref> [McK93a] </ref>. Based on our analysis and simulations, we believe that the best engineering choice is to detect streams at compile time, and to defer access ordering and issue to run time. <p> Here vector length refers to the amount of data processed by the entire parallel computation, not just by one CE. The 10,000-element vectors facilitate comparisons between SMP and uniprocessor systems, since this is one of the vector lengths used in the uniprocessor SMC studies <ref> [McK93a, McK93c] </ref>. These vectors are long enough that SMC startup transients become insignificant in most cases, but as the number of CEs increases, the amount of data processed by each CE decreases, and startup effects become more evident under certain parallelization techniques. <p> Figure 3 shows an example mapping of memory banks to FIFO positions for a stride-one vector when the length of the FIFOs is less than the number of banks. 3.1 Benchmark Suite The benchmark suite used is the same as in previous SMC studies <ref> [McK93a, McK93c, McK94a, McK94b] </ref>, and is described in Figure 4. These benchmarks represent access patterns found in real scientific codes, including the inner-loops of blocked algorithms. The suite constitutes a representative subset of all possible access patterns for computations involving a small number of vectors. <p> There is no notion of dynamic load balancing with respect to data size or number of CEs. This type of scheduling is particularly appropriate for applications exhibiting functional parallelism, where each CE performs a different task. Since performance on a single CE is relatively independent of access pattern <ref> [McK93a] </ref>, we model prescheduled computations by running the same benchmark on all CEs. The vector is split into approximately equal-size pieces, and each CE performs the computation on a single piece. <p> We therefore omit separate simulation results for this scheduling technique. 3.3 Ordering Policy The overwhelming similarity of the performance curves presented in the uniprocessor SMC studies <ref> [McK93a, McK93c, McK94a, McK94b] </ref> indicates that neither the ordering strategy nor the processors access pattern has a large effect on the MSUs ability to optimize bandwidth. For moderately long vectors whose stride is relatively prime to the number of memory banks, the SMC consistently delivers nearly the full system bandwidth. <p> Figure 8 illustrates the layout of a vector 1. In the uniprocessor SMC study, FC is called A1, Token BC is T1, Token TBC is T2, and Exhaus tive BC is R1. <ref> [McK93a] </ref>. 512 8 4096= Dynamic Access Ordering for Symmetric Shared-Memory Multiprocessors 15 with respect to DRAM pages for cases where the page size times the interleaving factor is slightly less than the amount of data to be processed at each of CEs. <p> Complete results for all benchmarks can be found in Appendix A. Like the uniprocessor SMC systems studied <ref> [McK93a] </ref>, SMP SMC performance approaches (and often exceeds) 90% of the peak system bandwidth for sufficiently long vectors and appropriately-sized FIFOs. schemes on SMP SMC systems with 2, 4, and 8 computational elements. Figure 13 illustrates SMC performance for daxpy using 10,000-element vectors on systems with 2 CEs. <p> percentages of peak bandwidth delivered by SMP SMC systems using this scheduling technique is almost identical to that for the analogous uniprocessor SMC systems: for long vectors, deep FIFOs, and workloads that allow the MSU to fully exploit bank concurrency, the SMC can consistently deliver almost the full system bandwidth <ref> [McK93a] </ref>. for 2-CE systems under the five dynamic access-ordering policies on which weve focused. These results demonstrate that SMP SMC performance is insensitive to variations in the Bank-Centric ordering schemes, and is almost constant for a given ratio of CEs to memory banks. <p> Tail-Off As the number of computational elements increases, the amount of data processed by each element decreases. This contributes to the slight tail-off of the performance curves in workloads and 8-CE systems using 10,000-element vectors, as in Figure 25 (b), uniprocessor SMC systems <ref> [McK93a] </ref>, this phenomenon illustrates the net effect of competing performance factors associated with FIFO depth: 1) The MSU needs sufficiently deep FIFOs to be able to keep the banks busy most of the time and to amortize page-miss costs over a number of page-hits. 2) Deeper FIFOs cause longer startup delays
Reference: [McK93b] <author> McKee, S.A., </author> <title> An Analytic Model of SMC Performance, </title> <institution> University of Virginia, TR CS-93-54, </institution> <month> November, </month> <year> 1993. </year>
Reference: [McK93c] <author> McKee, S.A., </author> <title> Uniprocessor SMC Performance on Vectors with Non-unit Strides, </title> <institution> University of Virginia, TR CS-93-67, </institution> <month> December, </month> <year> 1993. </year>
Reference-contexts: McKee et. al. propose a uniprocessor architecture for performing access ordering at run time [McK94a]. Simulation studies indicate that dynamic access ordering is a valuable technique for improving uniprocessor memory performance for stream computations the SMC, or Stream Memory Controller, consistently delivers almost the entire available bandwidth <ref> [McK93a, McK94b, McK93c] </ref>. The applicability of dynamic access ordering is not limited to uniprocessor environments. This paper discusses the effectiveness of dynamic access ordering with respect to the memory performance of symmetric multiprocessor (SMP) systems. <p> Here vector length refers to the amount of data processed by the entire parallel computation, not just by one CE. The 10,000-element vectors facilitate comparisons between SMP and uniprocessor systems, since this is one of the vector lengths used in the uniprocessor SMC studies <ref> [McK93a, McK93c] </ref>. These vectors are long enough that SMC startup transients become insignificant in most cases, but as the number of CEs increases, the amount of data processed by each CE decreases, and startup effects become more evident under certain parallelization techniques. <p> Figure 3 shows an example mapping of memory banks to FIFO positions for a stride-one vector when the length of the FIFOs is less than the number of banks. 3.1 Benchmark Suite The benchmark suite used is the same as in previous SMC studies <ref> [McK93a, McK93c, McK94a, McK94b] </ref>, and is described in Figure 4. These benchmarks represent access patterns found in real scientific codes, including the inner-loops of blocked algorithms. The suite constitutes a representative subset of all possible access patterns for computations involving a small number of vectors. <p> We therefore omit separate simulation results for this scheduling technique. 3.3 Ordering Policy The overwhelming similarity of the performance curves presented in the uniprocessor SMC studies <ref> [McK93a, McK93c, McK94a, McK94b] </ref> indicates that neither the ordering strategy nor the processors access pattern has a large effect on the MSUs ability to optimize bandwidth. For moderately long vectors whose stride is relatively prime to the number of memory banks, the SMC consistently delivers nearly the full system bandwidth.
Reference: [McK94a] <author> McKee, S.A., Klenke, R.H., Schwab, A.J., Wulf, Wm.A., Moyer, S.A., Hitchcock, C., Aylor, J.H., </author> <title> Experimental Implementation of Dynamic Access Ordering, </title> <booktitle> Proc. </booktitle> <address> HICSS-27, Maui, HI, </address> <note> January 1994; also University of Virginia, TR CS-93-42, </note> <month> August </month> <year> 1993. </year>
Reference-contexts: The overhead time required to do this makes performing such an access significantly slower than one that hits the current page. Dynamic Access Ordering for Symmetric Shared-Memory Multiprocessors 2 system. McKee et. al. propose a uniprocessor architecture for performing access ordering at run time <ref> [McK94a] </ref>. Simulation studies indicate that dynamic access ordering is a valuable technique for improving uniprocessor memory performance for stream computations the SMC, or Stream Memory Controller, consistently delivers almost the entire available bandwidth [McK93a, McK94b, McK93c]. The applicability of dynamic access ordering is not limited to uniprocessor environments. <p> Figure 3 shows an example mapping of memory banks to FIFO positions for a stride-one vector when the length of the FIFOs is less than the number of banks. 3.1 Benchmark Suite The benchmark suite used is the same as in previous SMC studies <ref> [McK93a, McK93c, McK94a, McK94b] </ref>, and is described in Figure 4. These benchmarks represent access patterns found in real scientific codes, including the inner-loops of blocked algorithms. The suite constitutes a representative subset of all possible access patterns for computations involving a small number of vectors. <p> We therefore omit separate simulation results for this scheduling technique. 3.3 Ordering Policy The overwhelming similarity of the performance curves presented in the uniprocessor SMC studies <ref> [McK93a, McK93c, McK94a, McK94b] </ref> indicates that neither the ordering strategy nor the processors access pattern has a large effect on the MSUs ability to optimize bandwidth. For moderately long vectors whose stride is relatively prime to the number of memory banks, the SMC consistently delivers nearly the full system bandwidth.
Reference: [McK94b] <author> McKee, S.A., Moyer, S.A., Wulf, Wm.A., Hitchcock, C., </author> <title> Increasing Memory Bandwidth for Vector Computations, </title> <booktitle> Proc. Conf. on Prog. Lang. and Sys. Arch., </booktitle> <address> Zurich, Switzerland, </address> <note> March 1994; also University of Virginia, TR CS-93-34. </note>
Reference-contexts: McKee et. al. propose a uniprocessor architecture for performing access ordering at run time [McK94a]. Simulation studies indicate that dynamic access ordering is a valuable technique for improving uniprocessor memory performance for stream computations the SMC, or Stream Memory Controller, consistently delivers almost the entire available bandwidth <ref> [McK93a, McK94b, McK93c] </ref>. The applicability of dynamic access ordering is not limited to uniprocessor environments. This paper discusses the effectiveness of dynamic access ordering with respect to the memory performance of symmetric multiprocessor (SMP) systems. <p> Figure 3 shows an example mapping of memory banks to FIFO positions for a stride-one vector when the length of the FIFOs is less than the number of banks. 3.1 Benchmark Suite The benchmark suite used is the same as in previous SMC studies <ref> [McK93a, McK93c, McK94a, McK94b] </ref>, and is described in Figure 4. These benchmarks represent access patterns found in real scientific codes, including the inner-loops of blocked algorithms. The suite constitutes a representative subset of all possible access patterns for computations involving a small number of vectors. <p> We therefore omit separate simulation results for this scheduling technique. 3.3 Ordering Policy The overwhelming similarity of the performance curves presented in the uniprocessor SMC studies <ref> [McK93a, McK93c, McK94a, McK94b] </ref> indicates that neither the ordering strategy nor the processors access pattern has a large effect on the MSUs ability to optimize bandwidth. For moderately long vectors whose stride is relatively prime to the number of memory banks, the SMC consistently delivers nearly the full system bandwidth.
Reference: [McM86] <author> McMahon, F.H., </author> <title> The Livermore Fortran Kernels: A Computer Test of the Numerical Performance Range, </title> <institution> Lawrence Livermore National Laboratory, UCRL-53745, </institution> <month> December </month> <year> 1986. </year> <title> Dynamic Access Ordering for Symmetric Shared-Memory Multiprocessors 125 </title>
Reference: [Mow92] <author> Mowry, T.C., Lam, M., and Gupta, A., </author> <title> Design and Evaluation of a Compiler Algorithm for Prefetching, </title> <booktitle> Proc. Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> September, </month> <year> 1992. </year>
Reference-contexts: Prefetching and nonblocking caches can be used to overlap memory accesses with computation, or to overlap the latencies of more than one access <ref> [Bae91, Cal91, Gup91, Kla91, Mow92, Soh91] </ref>. These techniques can improve processor performance, but techniques that simply mask latency do nothing to increase effective bandwidth. Such schemes are still useful, but they will be most effective when combined with complementary technology to exploit memory component capabilities.
Reference: [Moy93] <author> Moyer, S.A., </author> <title> Access Ordering and Effective Memory Bandwidth, </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Science, University of Virginia, </institution> <type> Technical Report CS-93-18, </type> <month> April </month> <year> 1993. </year>
Reference-contexts: Access ordering yields a dramatic performance improvement over traditional caching of vector operands, especially for non-unit stride vector computations. Access ordering may be performed statically, at compile time, or dynamically, at run time. Moyer derives compile-time access-ordering algorithms relative to a precise analytic model of memory systems <ref> [Moy93] </ref>. This approach unrolls loops and orders non-caching memory operations to exploit architectural and device features of the target memory 1. These devices behave as if implemented with a single on-chip cache line, or page.
Reference: [Ost89] <author> Osterhaug, Anita, ed., </author> <title> Guide to Parallel Programming on Sequent Computer Systems, </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: Three general scheduling techniques are commonly used to parallelize workloads: prescheduling, static scheduling, and dynamic scheduling <ref> [Ost89] </ref>.
Reference: [Ous90] <author> Ousterhout, J. </author> <title> Why arent Operating Systems Getter Faster As Fast As Hardware? Proc. </title> <booktitle> 1990 USENIX Summer Conference, </booktitle> <month> June </month> <year> 1990. </year>
Reference: [Por89] <author> Porterfield, </author> <title> A.K., Software Methods for Improvement of Cache Performance on Supercomputer Applications, </title> <type> Ph.D. Thesis, </type> <institution> Rice University, </institution> <month> May, </month> <year> 1989. </year>
Reference-contexts: Such schemes are still useful, but they will be most effective when combined with complementary technology to exploit memory component capabilities. Modifying the computation to increase the reuse of cached data can improve performance dramatically <ref> [Gal87, Gan87, Car89, Por89, Wol90, Lam91, Tem93] </ref>. These techniques will also deliver better performance when integrated with access ordering.
Reference: [Qui91] <author> Quinnell, R., </author> <title> High-speed DRAMs, </title> <type> EDN, </type> <month> May 23, </month> <year> 1991. </year>
Reference-contexts: Most memory-bandwidth studies focus on cache hit rates, but these only address one aspect of the problem. Most memory devices manufactured in the last decade provide special capabilities that make it possible to perform some access sequences faster than others <ref> [IEE92, Ram92, Qui91] </ref>, and exploiting these component characteristics can dramatically improve effective bandwidth.
Reference: [Ram92] <institution> Architectural Overview, Rambus Inc., Mountain View, </institution> <address> CA, </address> <year> 1992. </year>
Reference-contexts: Most memory-bandwidth studies focus on cache hit rates, but these only address one aspect of the problem. Most memory devices manufactured in the last decade provide special capabilities that make it possible to perform some access sequences faster than others <ref> [IEE92, Ram92, Qui91] </ref>, and exploiting these component characteristics can dramatically improve effective bandwidth.
Reference: [Rau91] <author> Rau, B. R., </author> <title> Pseudo-Randomly Interleaved Memory, </title> <booktitle> 18th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: These techniques will also deliver better performance when integrated with access ordering. Several schemes for avoiding bank contention, either by address transformations, skewing, or prime memory systems, have been published <ref> [Bud71, Gao93, Har87, Har89, Rau91] </ref>; we do not discuss them, other than to note that these, too, are complementary to the access ordering. 8. Conclusions As processors become faster, memory bandwidth is rapidly becoming the performance bottleneck in the application of high performance microprocessors to vector-like algorithms.
Reference: [Soh91] <author> Sohi, G., and Franklin, M., </author> <title> High Bandwidth Memory Systems for Superscalar Processors, </title> <booktitle> Proc. Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April, </month> <year> 1991. </year>
Reference-contexts: Prefetching and nonblocking caches can be used to overlap memory accesses with computation, or to overlap the latencies of more than one access <ref> [Bae91, Cal91, Gup91, Kla91, Mow92, Soh91] </ref>. These techniques can improve processor performance, but techniques that simply mask latency do nothing to increase effective bandwidth. Such schemes are still useful, but they will be most effective when combined with complementary technology to exploit memory component capabilities.
Reference: [Tha81] <author> Thabit, K.O., </author> <title> Cache Management by the Compiler, </title> <type> Ph.D. thesis, </type> <institution> University of Illinois, </institution> <month> October, </month> <year> 1982. </year>
Reference: [Tem93] <author> Temam, O., Granston, E.D., and Jalby, W., </author> <title> To Copy of Not to Copy: A Compile-Time Technique for Assessing When Data Copying Shoud Be Used to Eliminate Cache Conicts, </title> <booktitle> Proc. </booktitle> <address> Supercomputing93, </address> <month> December, </month> <year> 1993. </year>
Reference-contexts: Such schemes are still useful, but they will be most effective when combined with complementary technology to exploit memory component capabilities. Modifying the computation to increase the reuse of cached data can improve performance dramatically <ref> [Gal87, Gan87, Car89, Por89, Wol90, Lam91, Tem93] </ref>. These techniques will also deliver better performance when integrated with access ordering.
Reference: [Wal85] <author> Wallach, S., </author> <title> The CONVEX C-1 64-bit Supercomputer, </title> <booktitle> Compcon Spring 85, </booktitle> <month> February </month> <year> 1985. </year>
Reference: [Wol87] <author> Wolfe, M., </author> <title> Iteration Space Tiling for Memory Hierarchies, </title> <booktitle> Proc. Third SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> December, </month> <year> 1987. </year>
Reference: [Wol89] <author> Wolfe, M., </author> <title> More Iteration Space Tiling, </title> <booktitle> Proc. Supercomputing 89, </booktitle> <year> 1989. </year>
References-found: 53

