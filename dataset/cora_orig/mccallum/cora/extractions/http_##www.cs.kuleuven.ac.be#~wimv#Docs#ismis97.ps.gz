URL: http://www.cs.kuleuven.ac.be/~wimv/Docs/ismis97.ps.gz
Refering-URL: http://www.cs.kuleuven.ac.be/~wimv/ICL/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email:fWimV,LucDRg@cs.kuleuven.ac.be, saso.dzeroski@ijs.si  
Phone: 2  
Title: On Multi-class Problems and Discretization in Inductive Logic Programming  
Author: Wim Van Laer Luc De Raedt Saso Dzeroski 
Keyword: Learning, Knowledge Discovery, Inductive Logic Program ming, Classification, Discretization.  
Address: Celestijnenlaan 200A, B-3001 Heverlee, Belgium  Jamova 39, 1111 Ljubljana, Slovenia  
Affiliation: 1 Department of Computer Science, Katholieke Universiteit Leuven  Department of Intelligent Systems, Jozef Stefan Institute  
Abstract: In practical applications of machine learning and knowledge discovery, handling multi-class problems and real numbers are important issues. While attribute-value learners address these problems as a rule, very few ILP systems do so. The few ILP systems that handle real numbers mostly do so by trying out all real values applicable, thus running into efficiency or overfitting problems. The ILP learner ICL (Inductive Constraint Logic), learns first order logic formulae from positive and negative examples. The main characteristic of ICL is its view on examples, which are seen as interpretations which are true or false for the target theory. The paper reports on the extensions of ICL to tackle multi-class problems and real numbers. We also discuss some issues on learning CNF formulae versus DNF formulae related to these extensions. Finally, we present experiments in the practical domains of predicting mutagenesis, finite element mesh design and predicting biodegradability of chemical compounds. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> H. Blockeel and L. De Raedt. </author> <title> Experiments with top-down induction of logical decision trees. </title> <type> Technical Report CW 247, </type> <institution> Dept. of Computer Science, K.U.Leuven, </institution> <month> January </month> <year> 1997. </year>
Reference-contexts: Accuracies and timings for the four different backgrounds of the mutagene-sis data, with setting S 2 (the other three settings give similar results), using manual discretization. (The results for Progol, Foil and Tilde have been taken from <ref> [1] </ref>.) The results in table 1 are obtained by supplying ICL with the possible bound-aries for real-valued variables manually (the language looks like the dlab in section 2.2, but is more complex). Herefore, we needed some insight in the data. <p> Results of ICL on the Mesh data (learning a multi-class). When comparing the performance of ICL on Exp 1 with other learning systems (results taken from <ref> [1] </ref>), ICL has the highest predictive accuracy: ICL (50%), Tilde (36%), Foil (21%), Indigo (38%), FFoil (44%) and Fors (31%). 6.3 The Biodegradability Experiments While extensive experimentation has been conducted in the other two domains, the biodegradability domain is a relatively new one.
Reference: 2. <author> J. Catlett. </author> <title> On changing continuous attributes into ordered discrete attributes. </title> <editor> In Yves Kodratoff, editor, </editor> <booktitle> Proceedings of the 5th European Working Session on Learning, volume 482 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 164-178. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: Thirdly, and most importantly, (as many other ILP systems) ICL has problems with handling real numbers. In attribute value learning, discretization has recently received a lot of attention (cf. <ref> [2, 9] </ref>) and proven to be a valuable technique. We show how Fayyad and Irani's discretization technique ([11, 9]) can be modified for use in ILP and more specifically in ICL. The paper is structured as follows. <p> Thresholds are thus computed only once (instead of once for each candidate clause considered). The number of interesting thresholds (to be considered when refining clauses) is also kept to a minimum, yielding a smaller branching factor. This has also yielded positive results in attribute value learning, cf. <ref> [2] </ref>. Though we present the procedure as applied in the ICL system, it also generalizes to other ILP systems The discretization procedure is tied with the DLAB parameter of ICL, which defines the syntax of the clauses that may be part of a hypothesis.
Reference: 3. <author> P. Clark and R. Boswell. </author> <title> Rule induction with CN2: Some recent improvements. </title> <editor> In Yves Kodratoff, editor, </editor> <booktitle> Proceedings of the 5th European Working Session on Learning, volume 482 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 151-163. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: This view originates from computational learning theory where it was originally applied to boolean concept-learning [18], but recently upgraded towards 1st order logic [6] where it is known as learning from interpretations [4]. The ICL system can be considered an upgrade of the attribute value learning system CN2 <ref> [3] </ref>. However, whereas CN2 learns boolean concepts in DNF form, ICL learns first order theories in CNF form. In this paper, we import further features of attribute value learning into the ILP system ICL. First, it is shown that ICL can also learn DNF concepts. <p> In ICL, we use a similar strategy as in CN2 (see <ref> [3] </ref>). Given a problem with m classes (m 2), we first learn m separate DNF theories for the m different classes.
Reference: 4. <author> L. De Raedt. </author> <title> Induction in logic. In R.S. </title> <editor> Michalski and Wnek J., editors, </editor> <booktitle> Proceedings of the 3rd International Workshop on Multistrategy Learning, </booktitle> <pages> pages 29-38, </pages> <year> 1996. </year>
Reference-contexts: This view originates from computational learning theory where it was originally applied to boolean concept-learning [18], but recently upgraded towards 1st order logic [6] where it is known as learning from interpretations <ref> [4] </ref>. The ICL system can be considered an upgrade of the attribute value learning system CN2 [3]. However, whereas CN2 learns boolean concepts in DNF form, ICL learns first order theories in CNF form. In this paper, we import further features of attribute value learning into the ILP system ICL.
Reference: 5. <author> L. De Raedt and L. Dehaspe. </author> <title> Clausal discovery. </title> <journal> Machine Learning, </journal> <volume> 26 </volume> <pages> 99-146, </pages> <year> 1997. </year>
Reference-contexts: More details on the algorithm (based on the covering approach of CN2 with unordered rules) and the heuristics can be found in [7]. To specify the hypothesis language, ICL uses the same declarative bias as Claudien, i.e. DLAB (declarative language bias, see <ref> [5] </ref>). DLAB is a formalism for specifying an intensional syntactic definition of the language L H . For CNF, the hypothesis is a conjunction of clauses, and DLAB specifies the allowed syntax for the head and the body. <p> Note that lt (Lumo, 1-1:[-1,-2]) is a shorthand for 1-1:[lt (Lumo, -1), lt (Lumo, -2)]. 3 CNF and DNF Representation Originally, ICL learned a hypothesis in conjunctive normal form (CNF). This is inherited from its older twin system Claudien (see <ref> [5] </ref>). In [15], R. Mooney presents 2 dual algorithms, one for learning CNF and one for learning DNF. <p> On the one hand, there is an efficiency concern, and on the other hand, one may sometimes obtain higher accuracy rates. Procedures currently used to handle numbers in ILP and those used in older versions of Claudien <ref> [5] </ref>, are quite expensive. The reason is that for each candidate clause, all values for a given numeric variable have to be generated and considered in tests. In large databases, the number of such values can be huge, resulting in a high branching factor of the search.
Reference: 6. <author> L. De Raedt and S. Dzeroski. </author> <title> First order jk-clausal theories are PAC-learnable. </title> <journal> Artificial Intelligence, </journal> <volume> 70 </volume> <pages> 375-392, </pages> <year> 1994. </year>
Reference-contexts: It rather takes the view that examples are logical interpretations that are a model (resp. not a model) of the unknown target theory. This view originates from computational learning theory where it was originally applied to boolean concept-learning [18], but recently upgraded towards 1st order logic <ref> [6] </ref> where it is known as learning from interpretations [4]. The ICL system can be considered an upgrade of the attribute value learning system CN2 [3]. However, whereas CN2 learns boolean concepts in DNF form, ICL learns first order theories in CNF form.
Reference: 7. <author> L. De Raedt and W. Van Laer. </author> <title> Inductive constraint logic. </title> <booktitle> In Proceedings of the 5th Workshop on Algorithmic Learning Theory, volume 997 of Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: 1 Introduction The ILP system ICL (Inductive Constraint Logic, see <ref> [7] </ref>) does not employ the traditional ILP semantics in which examples are clauses that are (resp. are not) entailed by the target theory. It rather takes the view that examples are logical interpretations that are a model (resp. not a model) of the unknown target theory. <p> Section 6 describes some experiments that indicate the usefulness of the newly incorporated features in ICL. Section 7 concludes. 2 The Learning System ICL An overview of ICL can be found in <ref> [7] </ref>. Here, we will describe the framework of ICL in an informal way and discuss some practical aspects. But first we will introduce some concepts from logic (for an introduction to first order logic and model theory, we refer to [14, 12]). <p> More details on the algorithm (based on the covering approach of CN2 with unordered rules) and the heuristics can be found in <ref> [7] </ref>. To specify the hypothesis language, ICL uses the same declarative bias as Claudien, i.e. DLAB (declarative language bias, see [5]). DLAB is a formalism for specifying an intensional syntactic definition of the language L H .
Reference: 8. <author> B. Dolsak and S. Muggleton. </author> <title> The application of Inductive Logic Programming to finite element mesh design. </title> <editor> In S. Muggleton, editor, </editor> <booktitle> Inductive logic programming, </booktitle> <pages> pages 453-472. </pages> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: For the experiments, we have used the same four sets of background knowledge as in [17]. The data set for finite element mesh design <ref> [8] </ref>, consists of 5 structures and has 13 classes (= 13 possible number of partitions for an edge in a structure). In total, the 5 structures consist of 278 edges (each edge is taken as an example). <p> Saso Dzeroski is supported by the Slovenian Ministry of Science and Technology. This research is also part of the ESPRIT project no. 20237 on Inductive Logic Programming II. The Mutagenesis and Mesh datasets are made public by King and Srinivasan [17] resp. Dolsak <ref> [8] </ref>, and are available at the ILP data repository [13]. More info on ICL: http://www.cs.kuleuven.ac.be/~wimv/ICL/main.html.
Reference: 9. <author> J. Dougherty, R. Kohavi, and M. Sahami. </author> <title> Supervised and unsupervised discretiza-tion of continuous features. </title> <editor> In A. Prieditis and S. Russell, editors, </editor> <booktitle> Proc. Twelfth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Thirdly, and most importantly, (as many other ILP systems) ICL has problems with handling real numbers. In attribute value learning, discretization has recently received a lot of attention (cf. <ref> [2, 9] </ref>) and proven to be a valuable technique. We show how Fayyad and Irani's discretization technique ([11, 9]) can be modified for use in ILP and more specifically in ICL. The paper is structured as follows. <p> The resulting numeric attributes are then discretized using a simple modification of Fayyad and Irani's method, and the result is fed back into the DLAB template. The details of the Fayyad and Irani's method can be found in [11] and <ref> [9] </ref>. Fayyad and Irani's stopping criterion, which is based on the minimal description length principle, is very strict, in the sense that the method generates very few subintervals. When applying this criterion in ICL almost no subinter-vals would be generated.
Reference: 10. <author> S. Dzeroski, B. Kompare, and W. Van Laer. </author> <title> Predicting biodegradability from chemical structure using ILP. </title> <note> Submitted. </note>
Reference-contexts: In total, the 5 structures consist of 278 edges (each edge is taken as an example). The background knowledge is relatively large and contains information on edge types, boundary conditions, loadings and the geometry of the structure. The task in the biodegradability domain <ref> [10] </ref> is to predict the half-time of aqueous biodegradation of a compound from its chemical structure. The biodegradation time has been discretized into 4 classes: fast, moderate, slow and resistant. The structure of a compound is represented by facts about atoms and bonds, much like in the mutagenesis domain.
Reference: 11. <author> U.M. Fayyad and K.B. Irani. </author> <title> Multi-interval discretization of continuous-valued attributes for classification learning. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1022-1027, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: The resulting numeric attributes are then discretized using a simple modification of Fayyad and Irani's method, and the result is fed back into the DLAB template. The details of the Fayyad and Irani's method can be found in <ref> [11] </ref> and [9]. Fayyad and Irani's stopping criterion, which is based on the minimal description length principle, is very strict, in the sense that the method generates very few subintervals. When applying this criterion in ICL almost no subinter-vals would be generated.
Reference: 12. <author> M. Genesereth and N. Nilsson. </author> <booktitle> Logical foundations of artificial intelligence. </booktitle> <publisher> Mor-gan Kaufmann, </publisher> <year> 1987. </year>
Reference-contexts: Here, we will describe the framework of ICL in an informal way and discuss some practical aspects. But first we will introduce some concepts from logic (for an introduction to first order logic and model theory, we refer to <ref> [14, 12] </ref>). A first order alphabet is a set of predicate symbols, constant symbols and functor symbols. An atom p (t 1 ; :::; t n ) is a predicate symbol p followed by a bracketed n-tuple of terms t i .
Reference: 13. <author> D. Kazakov, L. Popelinsky, and O. Stepankova. </author> <note> ILP datasets page [http://www.gmd.de/ml-archive/datasets/ilp-res.html] , 1996. </note>
Reference-contexts: This research is also part of the ESPRIT project no. 20237 on Inductive Logic Programming II. The Mutagenesis and Mesh datasets are made public by King and Srinivasan [17] resp. Dolsak [8], and are available at the ILP data repository <ref> [13] </ref>. More info on ICL: http://www.cs.kuleuven.ac.be/~wimv/ICL/main.html.
Reference: 14. <author> J.W. Lloyd. </author> <title> Foundations of logic programming. </title> <publisher> Springer-Verlag, </publisher> <address> 2nd edition, </address> <year> 1987. </year>
Reference-contexts: Here, we will describe the framework of ICL in an informal way and discuss some practical aspects. But first we will introduce some concepts from logic (for an introduction to first order logic and model theory, we refer to <ref> [14, 12] </ref>). A first order alphabet is a set of predicate symbols, constant symbols and functor symbols. An atom p (t 1 ; :::; t n ) is a predicate symbol p followed by a bracketed n-tuple of terms t i .
Reference: 15. <author> R.J. Mooney. </author> <title> Encouraging experimental results on learning cnf. </title> <journal> Machine Learning, </journal> <volume> 19 </volume> <pages> 79-92, </pages> <year> 1995. </year>
Reference-contexts: Note that lt (Lumo, 1-1:[-1,-2]) is a shorthand for 1-1:[lt (Lumo, -1), lt (Lumo, -2)]. 3 CNF and DNF Representation Originally, ICL learned a hypothesis in conjunctive normal form (CNF). This is inherited from its older twin system Claudien (see [5]). In <ref> [15] </ref>, R. Mooney presents 2 dual algorithms, one for learning CNF and one for learning DNF. <p> Experiments indicate there is a difference in classification accuracy when learning CNF or DNF. In the mutagenesis case this is very clear. Table 1 shows the theory accuracy for the learned DNF and CNF theory. Theory complexity also differs. This is important in the light of Mooney's <ref> [15] </ref> experiments. He argued that the reason for the differences in accuracy and complexity can be explained by the difference in CNF and DNF. From the property above, it follows that there is an alternative explanation.
Reference: 16. <author> U. Pompe and I. Kononenko. </author> <title> Probabilistic first-order classification, </title> <note> 1997. Submitted. </note>
Reference-contexts: Regarding directions for further work, we note that multiple rules/theories are currently combined using the same approach as in CN2. It might be useful to look into different possibilities to merge several theories in one multi-class theory. We might look, for example, into the Bayesian approach used in <ref> [16] </ref>. Acknowledgements Wim Van Laer and Luc De Raedt are supported by the Fund for Scientific Research, Flanders. Saso Dzeroski is supported by the Slovenian Ministry of Science and Technology. This research is also part of the ESPRIT project no. 20237 on Inductive Logic Programming II.
Reference: 17. <author> A. Srinivasan, S.H. Muggleton, M.J.E. Sternberg, and R.D. King. </author> <title> Theories for mutagenicity: A study in first-order and feature-based induction. </title> <journal> Artificial Intelligence, </journal> <volume> 85, </volume> <year> 1996. </year>
Reference-contexts: The sum of the weights of all values for one numeric attribute or query in one example always equals one, or zero when no values are given. 6 Experiments We have done experiments in three domains. The data in the mutagenesis domain (see <ref> [17] </ref>) consists of 188 molecules, of which 125 are active (thus mutagenic) and 63 are inactive. A molecule is described by listing its atoms atom (AtomID,Element,Type,Charge) (the number of atoms differs between molecules, ranging from 15 to 35) and the bonds bond (Atom1,Atom2,BondType) between atoms. <p> A molecule is described by listing its atoms atom (AtomID,Element,Type,Charge) (the number of atoms differs between molecules, ranging from 15 to 35) and the bonds bond (Atom1,Atom2,BondType) between atoms. For the experiments, we have used the same four sets of background knowledge as in <ref> [17] </ref>. The data set for finite element mesh design [8], consists of 5 structures and has 13 classes (= 13 possible number of partitions for an edge in a structure). In total, the 5 structures consist of 278 edges (each edge is taken as an example). <p> Saso Dzeroski is supported by the Slovenian Ministry of Science and Technology. This research is also part of the ESPRIT project no. 20237 on Inductive Logic Programming II. The Mutagenesis and Mesh datasets are made public by King and Srinivasan <ref> [17] </ref> resp. Dolsak [8], and are available at the ILP data repository [13]. More info on ICL: http://www.cs.kuleuven.ac.be/~wimv/ICL/main.html.
Reference: 18. <author> L. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142, </pages> <year> 1984. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: It rather takes the view that examples are logical interpretations that are a model (resp. not a model) of the unknown target theory. This view originates from computational learning theory where it was originally applied to boolean concept-learning <ref> [18] </ref>, but recently upgraded towards 1st order logic [6] where it is known as learning from interpretations [4]. The ICL system can be considered an upgrade of the attribute value learning system CN2 [3].
References-found: 18

