URL: http://www.specbench.org/hpg/cse.ps
Refering-URL: http://www.specbench.org/hpg/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: eigenman@ecn.purdue.edu  Email: siamak.hassanzadeh@corp.sun.com  
Phone: Phone: 317-494-1741  Phone: 415-336-0118  
Title: Evaluating High-Performance Computer Technology through Industrially Significant Applications  
Author: Rudolf Eigenmann and Siamak Hassanzadeh 
Keyword: Benchmarking, Performance Evaluation, High-Performance Computers, Computational Applications, SPEC, SPEChpc96.  
Address: West Lafayette, Indiana 47907  2550 Garcia Avenue Mountain View, California 94043  
Affiliation: Purdue University School of Electrical and Computer Engineering  Sun Microsystems  
Abstract: The paper gives an overview and a brief status of the activities of the High-Performance Group of the Standard Performance Evaluation Corporation. SPEC/HPG has recently released a first suite of benchmarks, called SPEChpc96, that will be used to evaluate high-performance computer systems across the wide spectrum of available systems. The benchmark suite includes industrially significant applications in different areas. The effort is broadly supported by industrial and academic institutions. The paper also describes roles that participants of this benchmarking activity can play. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. H. Bailey, E. Barszcz, L. Dagum, and H. Simon. </author> <title> NAS parallel benchmark results. </title> <booktitle> In Proc. Supercomputing `92, </booktitle> <pages> pages 386-393. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1992. </year>
Reference-contexts: We need overall system performance numbers combined with detailed studies of the benchmark codes and system characteristics (such results came out of both the Parkbench and Perfect efforts, as well as from many other benchmarking projects <ref> [9, 16, 5, 15, 6, 1, 14] </ref>). We need the application expertise that helps us understand the codes and ensure that they represent the current state of computational problem solving technology (such as the automotive and seismic industry benchmarks).
Reference: [2] <author> A. Beguelin, J. Dongarra, A. Geist, R. Manchek, S. Otto, and J. Walpole. </author> <title> PVM: Experiences, current status and future direction. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> page 765, </pages> <year> 1993. </year>
Reference-contexts: On the other hand, manual tuning of the codes at the machine (assembly) level is disallowed. The SPEChpc96 benchmarks are released in two versions: a serial version and a message-passing version. The current codes are written in Fortran 77 with several C routines. The PVM <ref> [2] </ref> libraries are used in the message passing code variants. Benchmarkers are free to use either version as a starting point for their measurements. Typically, for distributed-memory systems that do not provide a global address space, the message-passing variant will be used.
Reference: [3] <author> M. Berry, D. Chen, P. Koss, D. Kuck, L. Pointer, S. Lo, Y. Pang, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Schwarzmeier, K. Lue, S. Orszag, F. Seidl, O. Johnson, G. Swanson, R. Goodrum, and J. Martin. </author> <title> The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> Int'l. Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: With the formation of the High-Performance Group, SPEC extends its activities to address performance evaluation needs across the large spectrum of today's high-performance systems. The Perfect Benchmarks R fl effort was initiated in 1988 <ref> [3, 4] </ref> to provide a balanced set of realistic scientific and engineering application programs and to use them for evaluating high-performance computers. In coordination with the vendors of such systems, performance results were obtained and documented in several reports [3, 11]. <p> The Perfect Benchmarks R fl effort was initiated in 1988 [3, 4] to provide a balanced set of realistic scientific and engineering application programs and to use them for evaluating high-performance computers. In coordination with the vendors of such systems, performance results were obtained and documented in several reports <ref> [3, 11] </ref>. This effort led to the first comprehensive evaluation of the sustainable application performance of supercomputers at that time.
Reference: [4] <author> George Cybenko, Lyle Kipp, Lynn Pointer, and David Kuck. </author> <title> Supercomputer Performance Evaluation and the Perfect Benchmarks. </title> <booktitle> Proceedings of ICS, </booktitle> <address> Amsterdam, Netherlands, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: With the formation of the High-Performance Group, SPEC extends its activities to address performance evaluation needs across the large spectrum of today's high-performance systems. The Perfect Benchmarks R fl effort was initiated in 1988 <ref> [3, 4] </ref> to provide a balanced set of realistic scientific and engineering application programs and to use them for evaluating high-performance computers. In coordination with the vendors of such systems, performance results were obtained and documented in several reports [3, 11]. <p> In fact - contrary to the expectation the results of a previous effort to evaluate computer systems based on a balanced set of real applications (the Perfect Benchmarks <ref> [4] </ref>) had uncovered the fact that the sustained application performance has improved only little over a decade. Evaluation of new computing technology and new computing paradigm is a challenging task, beyond the realm of possibility for average users.
Reference: [5] <author> J. J. Dongarra. </author> <title> The Linpack benchmark: An explanation. </title> <editor> In A. J. van der Steen, editor, </editor> <booktitle> Evaluating Supercomputers, </booktitle> <pages> pages 1-21. </pages> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1990. </year>
Reference-contexts: We need overall system performance numbers combined with detailed studies of the benchmark codes and system characteristics (such results came out of both the Parkbench and Perfect efforts, as well as from many other benchmarking projects <ref> [9, 16, 5, 15, 6, 1, 14] </ref>). We need the application expertise that helps us understand the codes and ensure that they represent the current state of computational problem solving technology (such as the automotive and seismic industry benchmarks).
Reference: [6] <editor> C. A. Addison et. al. </editor> <title> The GENESIS distributed-memory benchmarks. </title> <editor> In J. J. Dongarra and W. Gentzsch, editors, </editor> <booktitle> Computer Benchmarks, </booktitle> <pages> pages 257-271. </pages> <publisher> Elsevier Science Publishers, </publisher> <address> Amsterdam, </address> <year> 1991. </year>
Reference-contexts: We need overall system performance numbers combined with detailed studies of the benchmark codes and system characteristics (such results came out of both the Parkbench and Perfect efforts, as well as from many other benchmarking projects <ref> [9, 16, 5, 15, 6, 1, 14] </ref>). We need the application expertise that helps us understand the codes and ensure that they represent the current state of computational problem solving technology (such as the automotive and seismic industry benchmarks).
Reference: [7] <author> Myron Ginsberg. </author> <title> Creating an automotive industry benchmark suite for assessing the effectiveness of high-performance computers. </title> <booktitle> In Proc. Ninth Int'l. Conf. on Vehicle Structural Mechanics and CAE, </booktitle> <pages> page 290, </pages> <year> 1995. </year>
Reference-contexts: Thanks to the participation of this effort, SPEC/HPG could start with an important example of an industrially significant application and the corresponding methodology of benchmarking high-performance computers. Another effort to create an area-specific suite is described in <ref> [7] </ref>. The objective of this project is to create benchmarks for evaluating high-performance computers used in the automotive industry. A representative of this effort has joined SPEC/HPG, and forms of collaboration are being discussed. For example, this effort may provide the automotive area benchmarks of the SPEChpc96 suite.
Reference: [8] <author> R. W. Hockney and M. Berry (Editors). </author> <title> PARKBENCH report: Public international benchmarking for parallel computers. </title> <journal> Scientific Programming, </journal> <volume> 3(2) </volume> <pages> 101-146, </pages> <year> 1994. </year>
Reference-contexts: For example, this effort may provide the automotive area benchmarks of the SPEChpc96 suite. The Parkbench (PARallel Kernels and BENCHmarks) committee was founded in 1992 by a group of interested parties from universities, laboratories and industries. Members from both Europe and the USA <ref> [8] </ref> are participating in this effort with the goal of establishing a comprehensive set of parallel benchmarks and to set standards for benchmarking methodology. Maintaining a repository for the benchmarks and results is part of the objective.
Reference: [9] <author> F. McMahon. </author> <title> The Livermore kernels: A computer test of the numerical performance range. </title> <editor> In J.L. Martin, editor, </editor> <booktitle> Performance Evaluation of Supercomputers, </booktitle> <pages> pages 143-186. </pages> <publisher> Elsevier Science Publishers, </publisher> <address> Amsterdam, </address> <year> 1988. </year>
Reference-contexts: We need overall system performance numbers combined with detailed studies of the benchmark codes and system characteristics (such results came out of both the Parkbench and Perfect efforts, as well as from many other benchmarking projects <ref> [9, 16, 5, 15, 6, 1, 14] </ref>). We need the application expertise that helps us understand the codes and ensure that they represent the current state of computational problem solving technology (such as the automotive and seismic industry benchmarks).
Reference: [10] <author> C. C. Mosher and S. </author> <title> Hassanzadeh. ARCO seismic processing performance evaluation suite, User's Guide. </title> <type> ARCO Technical report, </type> <institution> Plano, TX., </institution> <year> 1993. </year>
Reference-contexts: The Seismic Benchmarking initiative, informally known as the "ARCO suite" <ref> [10] </ref> is one area-specific benchmarking effort with the objective to provide a suite of application codes representative of the seismic processing industry. Parts of this suite have already been included in several other benchmarking activities.
Reference: [11] <author> Lynn Pointer. </author> <title> Perfect: Performance Evaluation for Cost-Effective Transformations Report 2. </title> <type> Technical Report 964, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr for Supercomputing Res & Dev, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: The Perfect Benchmarks R fl effort was initiated in 1988 [3, 4] to provide a balanced set of realistic scientific and engineering application programs and to use them for evaluating high-performance computers. In coordination with the vendors of such systems, performance results were obtained and documented in several reports <ref> [3, 11] </ref>. This effort led to the first comprehensive evaluation of the sustainable application performance of supercomputers at that time.
Reference: [12] <author> Bill Pottenger and Rudolf Eigenmann. </author> <title> Targeting a Shared-Address-Space version of the seismic benchmark Seis1.1. </title> <type> Technical Report 1456, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. & Dev., </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: This porting effort could be reduced through the provision of a third variant that includes such program modifications in a generic form. To this end, we have started to study codes <ref> [12] </ref> and evaluate possible languages for expressing the new benchmark variants.
Reference: [13] <author> M. W. Schmidt, K. K. Baldridge, J. A. Boatz, S. T. Elber, M. S. Gordon, J. H. Jensen, S. Koseki, N. Mat-sunaga, K. A. Nguyen, S. Su, T. L. Windus, M. Dupuis, and J. A. Montgonery. </author> <title> The general atomic and molecular electronics structure systems. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 14(11) </volume> <pages> 1347-1363, </pages> <year> 1993. </year>
Reference-contexts: SPECchem96 is based on GAMESS (General Atomic and Molecular Elec--tronic Structure System), an improved version of programs that came from the Department of Energy's National Resource for Computations in Chemistry <ref> [13] </ref>. Many of the functions found in GAMESS are duplicated in commercial packages used in the pharmaceutical and chemical industries for drug design and bonding analysis. Both SPECseis96 and SPECchem96 incorporate a wide range of problem sizes.
Reference: [14] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> Splash: Stanford parallel applications for shared-memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <year> 1992. </year>
Reference-contexts: We need overall system performance numbers combined with detailed studies of the benchmark codes and system characteristics (such results came out of both the Parkbench and Perfect efforts, as well as from many other benchmarking projects <ref> [9, 16, 5, 15, 6, 1, 14] </ref>). We need the application expertise that helps us understand the codes and ensure that they represent the current state of computational problem solving technology (such as the automotive and seismic industry benchmarks).
Reference: [15] <author> A. J. van der Steen. </author> <title> The benchmark of the EuroBen group. </title> <journal> Parallel Computing, </journal> <volume> 17 </volume> <pages> 1211-1221, </pages> <year> 1991. </year>
Reference-contexts: We need overall system performance numbers combined with detailed studies of the benchmark codes and system characteristics (such results came out of both the Parkbench and Perfect efforts, as well as from many other benchmarking projects <ref> [9, 16, 5, 15, 6, 1, 14] </ref>). We need the application expertise that helps us understand the codes and ensure that they represent the current state of computational problem solving technology (such as the automotive and seismic industry benchmarks).
Reference: [16] <author> R. P. Weicker. </author> <title> An overview of common benchmarks. </title> <journal> IEEE Computer, </journal> <volume> 23(12) </volume> <pages> 65-75, </pages> <month> December </month> <year> 1990. </year> <month> 7 </month>
Reference-contexts: We need overall system performance numbers combined with detailed studies of the benchmark codes and system characteristics (such results came out of both the Parkbench and Perfect efforts, as well as from many other benchmarking projects <ref> [9, 16, 5, 15, 6, 1, 14] </ref>). We need the application expertise that helps us understand the codes and ensure that they represent the current state of computational problem solving technology (such as the automotive and seismic industry benchmarks).
References-found: 16

