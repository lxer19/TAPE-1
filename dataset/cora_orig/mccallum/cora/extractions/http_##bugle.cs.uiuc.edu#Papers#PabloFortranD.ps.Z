URL: http://bugle.cs.uiuc.edu/Papers/PabloFortranD.ps.Z
Refering-URL: http://bugle.cs.uiuc.edu/Papers/papers.html
Root-URL: http://www.cs.uiuc.edu
Title: An Integrated Compilation and Performance Analysis Environment for Data Parallel Programs  
Author: Vikram S. Adve John Mellor-Crummey Mark Anderson Ken Kennedy Jhy-Chun Wang Daniel A. Reed 
Address: Houston, Texas 77251-1892  Urbana, Illinois 61801  
Affiliation: Center for Research on Parallel Computation Rice University  Department of Computer Science University of Illinois  
Abstract: To support the transition from programming languages in which parallelism and communication are explicit to high-level languages that rely on compilers to infer such details from data decomposition directives, tools for performance analysis require increased sophistication and integration with other components in the programming system. We explore integration of performance tools with compilers for data parallel languages by integrating the Rice Fortran 77D compiler and the Illinois Pablo Environment. This integration permits analysis and correlation of the compiler-generated code's dynamic behavior and performance with the original data parallel source code. We expect that our strategy can serve as a model for integration of other data parallel compilers and performance tools.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Adve, V. S., Koelbel, C., and Mellor-Crummey, J. M. </author> <title> Performance Analysis of Data-Parallel Programs. </title> <type> Tech. Rep. </type> <institution> CRPC-TR94405, Center for Research on Parallel Computation, Rice University, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Finally, the compiler can exploit dynamic performance data to generate more efficient code, as described elsewhere <ref> [1] </ref>. 3 3 Integrating Compilation and Performance Analysis As illustrated in x2, a simple composition of compiler and performance tools is insufficient to support high-level performance analysis and software tuning of Fortran D programs.
Reference: [2] <institution> Applied Parallel Research. </institution> <note> Forge 90 Distributed Memory Parallelizer: User's Guide, version 8.0 ed. </note> <institution> Placerville, </institution> <address> CA, </address> <year> 1992. </year>
Reference-contexts: Notable exceptions include include Prism [12] and NV [5] for CM-Fortran, Forge90 <ref> [2] </ref> for Fortran 90 and HPF, and the MPP-Apprentice performance tool, which supports C, Fortran and Fortran 90 on the Cray T3D [7]. The CM Fortran compiler lacks aggressive inter-statement optimization, making the mapping of dynamic performance data to source lines for Prism and data objects for NV straightforward.
Reference: [3] <author> Aydt, R. A. SDDF: </author> <title> The Pablo Self-Describing Data Format. </title> <type> Tech. rep., </type> <institution> Department of Computer Science, University of Illinois, </institution> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: When data dependences prevent full parallelism, the compiler often can achieve partial parallelism by pipelining the computation. The Pablo environment's principal components are an extensible data capture library [10], a data meta-format <ref> [3] </ref> for describing the structure of performance data records without constraining their contents, and a user-extensible graphical data analysis toolkit [8]. The data capture library can trace, count, or time code fragments as well as capture entry to and exit from procedures, loops, and message-passing library calls.
Reference: [4] <author> Hiranandani, S., Kennedy, K., and Tseng, C.-W. </author> <title> Preliminary Experiences with the Fortran D Compiler. </title> <booktitle> In Proceedings of Supercomputing '93 (Nov. </booktitle> <year> 1993), </year> <institution> Association for Computing Machinery. </institution>
Reference-contexts: 1 Introduction High-level, data parallel languages such as High Performance Fortran and Fortran D <ref> [4] </ref> have attracted considerable attention because they offer a simple and portable programming model for parallel, scientific programs. In such languages, programmers specify parallelism abstractly using data layout directives; a compiler uses these directives as the basis for synthesizing a program with explicit parallelism and interprocessor communication.
Reference: [5] <author> Irvin, R. B., and Miller, B. P. </author> <title> A Performance Tool for High-Level Parallel Languages. </title> <type> Tech. Rep. 1204, </type> <institution> University of Wisconsin-Madison, </institution> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: Notable exceptions include include Prism [12] and NV <ref> [5] </ref> for CM-Fortran, Forge90 [2] for Fortran 90 and HPF, and the MPP-Apprentice performance tool, which supports C, Fortran and Fortran 90 on the Cray T3D [7].
Reference: [6] <author> Miller, B. P., Clark, M., Hollingsworth, J., Kierstead, S., Lim, S.-S., and Torzewski, T. IPS-2: </author> <title> The Second Generation of a Parallel Program Measurement System. </title> <journal> IEEE Transactions on Computers 1, </journal> <month> 2 (Apr. </month> <year> 1990), </year> <pages> 206-217. </pages>
Reference-contexts: Because the goal of data-parallel languages is to insulate software developers from the idiosyncrasies of message passing, performance tuning should not require them to understand the details of the compiler-generated code. Unfortunately, current performance instrumentation and analysis tools <ref> [9, 8, 6, 11] </ref> for distributed memory parallel systems only capture and present performance data from the generated Fortran 77 code of Figure 3.
Reference: [7] <author> Pase, D. </author> <title> MPP Apprentice: A Non-Event Trace Performance Tool for the CRAY T3D. Presentation at the Workshop on Debugging and Performance Tuning for Parallel Computing Systems, </title> <address> Oct. </address> <year> 1994. </year>
Reference-contexts: Notable exceptions include include Prism [12] and NV [5] for CM-Fortran, Forge90 [2] for Fortran 90 and HPF, and the MPP-Apprentice performance tool, which supports C, Fortran and Fortran 90 on the Cray T3D <ref> [7] </ref>. The CM Fortran compiler lacks aggressive inter-statement optimization, making the mapping of dynamic performance data to source lines for Prism and data objects for NV straightforward. Forge90 supports performance analysis at the level of compiler-generated SPMD code; its performance annotations show the cost of invocations of their communication library.
Reference: [8] <author> Reed, D. A. </author> <title> Performance Instrumentation Techniques for Parallel Systems. In Models and Techniques for Performance Evaluation of Computer and Communications Systems, </title> <editor> L. Donatiello and R. Nelson, Eds. </editor> <booktitle> Springer-Verlag Lecture Notes in Computer Science, </booktitle> <year> 1993, </year> <pages> pp. 463-490. </pages>
Reference-contexts: The Pablo environment's principal components are an extensible data capture library [10], a data meta-format [3] for describing the structure of performance data records without constraining their contents, and a user-extensible graphical data analysis toolkit <ref> [8] </ref>. The data capture library can trace, count, or time code fragments as well as capture entry to and exit from procedures, loops, and message-passing library calls. A group of library extension interfaces allows instrumentation software developers to incrementally extend the library's functionality. <p> Because the goal of data-parallel languages is to insulate software developers from the idiosyncrasies of message passing, performance tuning should not require them to understand the details of the compiler-generated code. Unfortunately, current performance instrumentation and analysis tools <ref> [9, 8, 6, 11] </ref> for distributed memory parallel systems only capture and present performance data from the generated Fortran 77 code of Figure 3.
Reference: [9] <author> Reed, D. A. </author> <title> Experimental Performance Analysis of Parallel Systems: Techniques and Open Problems. </title> <booktitle> In Proceedings of the 7th International Conference on Modelling Techniques and Tools for Computer Performance Evaluation (May 1994), </booktitle> <pages> pp. 25-51. </pages>
Reference-contexts: Because the goal of data-parallel languages is to insulate software developers from the idiosyncrasies of message passing, performance tuning should not require them to understand the details of the compiler-generated code. Unfortunately, current performance instrumentation and analysis tools <ref> [9, 8, 6, 11] </ref> for distributed memory parallel systems only capture and present performance data from the generated Fortran 77 code of Figure 3.
Reference: [10] <author> Reed, D. A., Aydt, R. A., Noe, R. J., Roth, P. C., Shields, K. A., Schwartz, B. W., and Tavera, L. F. </author> <title> Scalable Performance Analysis: The Pablo Performance Analysis Environment. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <editor> A. Skjellum, Ed. </editor> <publisher> IEEE Computer Society, </publisher> <year> 1993, </year> <pages> pp. 104-113. </pages>
Reference-contexts: When data dependences prevent full parallelism, the compiler often can achieve partial parallelism by pipelining the computation. The Pablo environment's principal components are an extensible data capture library <ref> [10] </ref>, a data meta-format [3] for describing the structure of performance data records without constraining their contents, and a user-extensible graphical data analysis toolkit [8]. <p> A common interface provides uniform access to this information regardless of where individual symbolic records are stored. 4 3.2 Program Instrumentation The Pablo instrumentation library <ref> [10] </ref> can count or trace dynamic events and time intervals; specialized instrumentation supports tracing of procedure calls, loops, and message passing library calls. The library has been augmented, via its extension interfaces, to support integration with the Fortran D compiler.
Reference: [11] <author> Ries, B., Anderson, R., Auld, W., Breazeal, D., Callaghan, K., Richards, E., and Smith, W. </author> <title> The Paragon Performance Monitoring Environment. </title> <booktitle> In Proceedings of Supercomputing '93 (Nov. 1993), Association for Computing Machinery, </booktitle> <pages> pp. 850-859. </pages>
Reference-contexts: Because the goal of data-parallel languages is to insulate software developers from the idiosyncrasies of message passing, performance tuning should not require them to understand the details of the compiler-generated code. Unfortunately, current performance instrumentation and analysis tools <ref> [9, 8, 6, 11] </ref> for distributed memory parallel systems only capture and present performance data from the generated Fortran 77 code of Figure 3.

References-found: 11

