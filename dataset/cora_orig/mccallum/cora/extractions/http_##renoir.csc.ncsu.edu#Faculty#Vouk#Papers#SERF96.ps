URL: http://renoir.csc.ncsu.edu/Faculty/Vouk/Papers/SERF96.ps
Refering-URL: http://renoir.csc.ncsu.edu/Faculty/Vouk/vouk.se.html
Root-URL: http://www.csc.ncsu.edu
Email: email: atrivers@magneto.csc.ncsu.edu  email: vouk@adm.csc.ncsu.edu  
Phone: Tel: 919-515-3655  Tel: 919-515-7886  
Title: 111 AN EMPIRICAL EVALUATION OF TESTING EFFICIENCY DURING NONOPERATIONAL TESTING  
Author: Anthony T. Rivers M. A. Vouk 
Address: Box 7911  Raleigh, NC 27695-7911  Box 8206  Raleigh, NC 27695-8206  
Affiliation: Electrical and Computer Engineering Dept.,  North Carolina State University,  Computer Science Department,  North Carolina State University,  
Abstract: In this paper we discuss the question of growth of testing efficiency during nonoperational testing. "Learning" is said to occur if there is an improvement in the efficiency of testing during a testing phase. Our work indicates that in practice very little actual "learning" may take place during nonoperational testing, particularly if test duration and test-suite size is subject to a priori constraints. In fact, some of our results show that "learning" may be an artifact of the program structure. We discuss the testing process that we believe may operate in an nonoperational testing phase, and we develop a fault detection model based on that process. Finally we present and discuss some early empirical results on the growth of testing efficiency. 
Abstract-found: 1
Intro-found: 1
Reference: [Bel95] <author> F. Belli, O. Jack, </author> <title> "A Test Coverage Notion for Logic Programming," </title> <booktitle> Proc. Sixth International Symposium on Software Reliability Engineering, </booktitle> <address> Toulouse, France, </address> <month> October </month> <year> 1995. </year>
Reference: [Bor91] <author> Borger, D.S., and Vouk, M.A., </author> <title> "Modeling the Behaviour of Large Software Projects," </title> <note> Center for Communications and Signal Processing Technical Report TR-91/19, NCSU, </note> <year> 1991. </year>
Reference-contexts: One set is from the paper by Ohba [Ohb84], the second set comes from the NASA Langley Research Center (LaRC) sponsored multi-version multi-university software experiment [Kel88, Vou90, Eck91, Tai94], and the third set comes from the functional test debug stage of a large telecommunications system <ref> [Bor91, Cra92] </ref>. The first (Ohba) dataset represents testand-debug data of a PL/I database application program described in [Ohb84], Appendix F, Table 4. The size of the software is about 1.4 million lines of code, and the failure data were collected using calendar and CPU execution time. <p> Detailed descriptions of the detected faults of the employed testing strategies are given in [Vou90, Eck91, Tai94]. The third (Commercial) dataset derives from real software testing and evaluation efforts conducted during late 116 unit and early integration testing of the elements of a very large commercial telecommunications product <ref> [Bor91, Cra92, Jon95] </ref>. 4.2 Empirical Testing Efficiency We empirically estimate the testing efficiency function, g (Q), from equation (7) by rewriting it as: ( )g Q N E Q i i = -( ) D (16) where ^ g i (Q i ) is the g (Q) estimate at instant i,
Reference: [Che95] <author> Mei-Hwa Chen, Aditya P. Mathur, Vernon J. Rego, </author> <title> "Effect of Testing Techniques on Software Reliability Estimates Obtained Using A Time-Domain Model," </title> <journal> IEEE Transactions on Reliability, </journal> <volume> Vol. 44, No. 1, </volume> <year> 1995 </year> <month> March, </month> <pages> pp. 97-103. </pages>
Reference: [Cra92] <author> Cramp R., Vouk M.A., and Jones W., </author> <title> "On Operational Availability of a Large Software-Based Telecommunications System," </title> <booktitle> Proc. Thrid Intl. Symposium on Software Reliability Engineering, </booktitle> <publisher> IEEE CS, </publisher> <year> 1992, </year> <pages> pp. 358-366 </pages>
Reference-contexts: One set is from the paper by Ohba [Ohb84], the second set comes from the NASA Langley Research Center (LaRC) sponsored multi-version multi-university software experiment [Kel88, Vou90, Eck91, Tai94], and the third set comes from the functional test debug stage of a large telecommunications system <ref> [Bor91, Cra92] </ref>. The first (Ohba) dataset represents testand-debug data of a PL/I database application program described in [Ohb84], Appendix F, Table 4. The size of the software is about 1.4 million lines of code, and the failure data were collected using calendar and CPU execution time. <p> Detailed descriptions of the detected faults of the employed testing strategies are given in [Vou90, Eck91, Tai94]. The third (Commercial) dataset derives from real software testing and evaluation efforts conducted during late 116 unit and early integration testing of the elements of a very large commercial telecommunications product <ref> [Bor91, Cra92, Jon95] </ref>. 4.2 Empirical Testing Efficiency We empirically estimate the testing efficiency function, g (Q), from equation (7) by rewriting it as: ( )g Q N E Q i i = -( ) D (16) where ^ g i (Q i ) is the g (Q) estimate at instant i,
Reference: [Eck91] <author> D.E. Eckhardt, A.K. Caglayan, J.P.J. Kelly, J.C. Knight, L.D. Lee, D.F. McAllister, and M.A. Vouk, </author> <title> "An Experimental Evaluation of Software Redundancy as a Strategy for Improving Reliability," </title> <journal> IEEE Trans. Soft. Eng., </journal> <volume> Vol. 17(7), </volume> <pages> pp. 692-702, </pages> <year> 1991. </year>
Reference-contexts: EMPIRICAL RESULTS 4.1 Data The empirical data used in this investigation derives from three sources. One set is from the paper by Ohba [Ohb84], the second set comes from the NASA Langley Research Center (LaRC) sponsored multi-version multi-university software experiment <ref> [Kel88, Vou90, Eck91, Tai94] </ref>, and the third set comes from the functional test debug stage of a large telecommunications system [Bor91, Cra92]. The first (Ohba) dataset represents testand-debug data of a PL/I database application program described in [Ohb84], Appendix F, Table 4. <p> The cumulative execution coverage of the code by the test cases was determined through post-experimental code instrumentation and execution tracing using BGG [Vou89]. After the functional and random certification tests, the programs were subjected to an independent operational evaluation test <ref> [Eck91] </ref> which found additional defects in the code. Detailed descriptions of the detected faults of the employed testing strategies are given in [Vou90, Eck91, Tai94]. <p> After the functional and random certification tests, the programs were subjected to an independent operational evaluation test [Eck91] which found additional defects in the code. Detailed descriptions of the detected faults of the employed testing strategies are given in <ref> [Vou90, Eck91, Tai94] </ref>. <p> In this illustration we use puse coverage data for three of the 20 functionally equivalent programs, and we have set N to the number of faults eventually detected in these programs through extensive tests that followed the "certification" phase <ref> [Vou90, Eck91] </ref>. In the figures, we plot the "instantaneous" or "per test-case" efficiency, and its arithmetic average over the covered metric space.
Reference: [Fra95] <author> F. Del Frate, P. Garg, A.P. Mathur, A. Pasquini, </author> <title> "On the Correlation Between Code Coverage and Software Reliability," </title> <booktitle> Proc. Sixth International Symposium on Software Reliability Engineering, </booktitle> <address> Toulouse, France, </address> <month> October </month> <year> 1995. </year>
Reference: [Goe 85] <author> Amrit L. </author> <title> Goel "Software Reliability Models: Assumptions, Limitations, and Applicability." </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol SE-11 Number 12, </volume> <month> December </month> <year> 1985, </year> <pages> pages 1411-1423. </pages>
Reference: [Hou94] <author> Rong-Huei Hou, Sy-Yen Kuo, YiPing Chang, </author> <title> "Applying Various Learning Curves to the Hyper-Geometric Distribution Software Reliability 120 Growth Model," </title> <booktitle> 5th International Symposium on Software Reliability Engineering, </booktitle> <month> Nov. </month> <pages> 6-9, </pages> <year> 1994, </year> <pages> pp. 196-205. </pages>
Reference-contexts: It is interesting to note that equation (16), in fact, subsumes the learning factor proposed by Hou et al. <ref> [Hou94] </ref>, and derived from the sensitivity factor defined by Thoma et al. [Tho89], i.e., w i = N-E i-1 where w i is the sensitivity factor. <p> Figures 3 and 4 illustrate the difference between "learning factor" and "testing efficiency" functions using the same data as Hou et al. <ref> [Hou94, Ohb84] </ref>. We assume that the testing schedule of 19 weeks was planned, and that N=358. Therefore, on the horizontal axis we plot the fraction of the schedule that is completed.
Reference: [Jac90] <author> Raymond Jacoby, Yoshihiro Tohma, </author> <title> "The Hyper-Geometric Distribution Software Reliability Growth Model ( HGDM ): Precise Formulation and Applicability," </title> <booktitle> Proc. COMPSAC 1990, </booktitle> <address> Chicago, Illinois, </address> <month> October </month> <year> 1990, </year> <pages> pp. 13-19. </pages>
Reference: [Jac92] <author> Raymond Jacoby, Kaori Masuzawa, </author> <title> "Test Coverage Dependent Software Reliability Estimation by the HGD Model," </title> <booktitle> Proc. Third International Symposium on Software Reliability Engineering, </booktitle> <institution> Research Triangle Park, North Carolina, </institution> <month> October </month> <year> 1992, </year> <pages> pp. 193-204. </pages>
Reference: [Jon95] <author> Jones W. and Vouk M.A.,, </author> <title> "Software Reliability Field Data Analysis," Chapter 11 in Handbook of Software Reliability Engineering, </title> <publisher> McGraw Hill, </publisher> <editor> editor M. Lyu, accepted, in print, </editor> <volume> expected 1 9 9 5 </volume> . 
Reference-contexts: Detailed descriptions of the detected faults of the employed testing strategies are given in [Vou90, Eck91, Tai94]. The third (Commercial) dataset derives from real software testing and evaluation efforts conducted during late 116 unit and early integration testing of the elements of a very large commercial telecommunications product <ref> [Bor91, Cra92, Jon95] </ref>. 4.2 Empirical Testing Efficiency We empirically estimate the testing efficiency function, g (Q), from equation (7) by rewriting it as: ( )g Q N E Q i i = -( ) D (16) where ^ g i (Q i ) is the g (Q) estimate at instant i,
Reference: [Kel88] <author> J. Kelly, D. Eckhardt, A. Caglayan, J. Knight, D. McAllister, M. Vouk, </author> <title> "A Large Scale Second Generation Experiment in Multi-Version Software: Description and Early Results", </title> <booktitle> Proc. </booktitle> <volume> FTCS 18, </volume> <pages> pp. 9-14, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: EMPIRICAL RESULTS 4.1 Data The empirical data used in this investigation derives from three sources. One set is from the paper by Ohba [Ohb84], the second set comes from the NASA Langley Research Center (LaRC) sponsored multi-version multi-university software experiment <ref> [Kel88, Vou90, Eck91, Tai94] </ref>, and the third set comes from the functional test debug stage of a large telecommunications system [Bor91, Cra92]. The first (Ohba) dataset represents testand-debug data of a PL/I database application program described in [Ohb84], Appendix F, Table 4. <p> The second (NASA) dataset derives from the NASA LaRC experiment in which of twenty implementations of a sensor management inertial navigation system were developed to the same specification <ref> [Kel88] </ref>. The programs have between 2000 and 4000 lines of Pascal code, and were developed and tested in a UNIX environment. The data discussed in this paper are from the "certification" phases. Independent certification testing consisted of a functional certification test suite and a random certification test suite.
Reference: [Mal94] <author> Yashwant K. Malaiya, Rick Karcich, </author> <title> "The Relationship Between Test Coverage and Reliability," </title> <booktitle> 5th International Symposium on Software Reliability Engineering, </booktitle> <month> Nov. </month> <pages> 6-9, </pages> <year> 1994, </year> <pages> pp. 186-195. </pages>
Reference: [Mus87] <author> J.D. Musa, A. Iannino, and K. Okumoto, </author> <title> Software R e l i a b i l i t y : M e a s u r e m e n t P r e d i c t i o n s , Application, </title> <publisher> McGraw-Hill Book Co., </publisher> <year> 1987. </year>
Reference-contexts: introduction of the concept of "hidden" constructs, reevaluation of the "plan" and estimation of the additional "time" to target intensity in a manner similar to "classical" time-based models <ref> [e.g., Mus87] </ref>. 4. EMPIRICAL RESULTS 4.1 Data The empirical data used in this investigation derives from three sources.
Reference: [Mus93] <author> J.D. Mua, </author> <title> "Operational profiles in Software-Reliability Engineering," </title> <journal> IEEE Software, </journal> <volume> Vol. 10 (2), </volume> <pages> pp. 14-32, </pages> <month> March </month> <year> 1993. </year>
Reference: [Piw93] <author> P. Piwowarski, M. Ohba, J. Caruso, </author> <title> "Coverage measurement experience during function test," </title> <booktitle> International Conference on Software Engineering (ICSE'93), </booktitle> <year> 1994, </year> <pages> pp. 287-300. </pages>
Reference: [Tai94] <author> Tai K.C., Vouk M.A., Paradkar A., Lu P., </author> <title> "Predicate Based Testing," </title> <journal> IBM Systems Journal, </journal> <volume> Vol 33 (3), p 445, </volume> <year> 1994. </year>
Reference-contexts: EMPIRICAL RESULTS 4.1 Data The empirical data used in this investigation derives from three sources. One set is from the paper by Ohba [Ohb84], the second set comes from the NASA Langley Research Center (LaRC) sponsored multi-version multi-university software experiment <ref> [Kel88, Vou90, Eck91, Tai94] </ref>, and the third set comes from the functional test debug stage of a large telecommunications system [Bor91, Cra92]. The first (Ohba) dataset represents testand-debug data of a PL/I database application program described in [Ohb84], Appendix F, Table 4. <p> After the functional and random certification tests, the programs were subjected to an independent operational evaluation test [Eck91] which found additional defects in the code. Detailed descriptions of the detected faults of the employed testing strategies are given in <ref> [Vou90, Eck91, Tai94] </ref>. <p> This is a strong indicator that a specification-based testing strategy may perform quite differently in different situations, and that a "black box" specification-based test generation needs to be supplemented with a more rigorous test-case selection approach such as predicate-based testing <ref> [Tai94] </ref>. From the figures we see that in two cases average test-case efficiency appears to be more or less constant over the testsuite, while in one case we see a slight upward trend in the efficiency as coverage increases.
Reference: [Toh89] <author> Yoshihiro Tohma, Raymond Jacoby, Yukihisa Murata, Moriki Yamamoto, </author> <title> "Hyper-Geometric Distribution Model to Estimate the Number of Residual Software Faults," </title> <booktitle> Proc. </booktitle> <address> COMMACK-89, Orlando, Florida, </address> <month> September </month> <year> 1989, </year> <pages> pp. 610-617. </pages>
Reference: [Tri82] <author> K.S. Trivedi, </author> <title> "Probability and Statistics with Reliability, Queuing, </title> <booktitle> and Computer Science Applications, </booktitle> <publisher> Prentice-Hall, </publisher> <address> New Jersey, </address> <year> 1982. </year>
Reference: [Vou89] <author> Vouk, M.A., and Coyle, R.E., "BGG: </author> <title> A Testing Coverage Tool," </title> <booktitle> Proc. Seventh Annual Pacific Northwest Software Quality Conference, </booktitle> <publisher> Lawrence and Craig, Inc., </publisher> <address> Portland, OR, pp.212-233, </address> <month> September </month> <year> 1989. </year>
Reference-contexts: During certification testing the correctness of the answers was judged using a program which had been extensively tested and inspected on its own. The cumulative execution coverage of the code by the test cases was determined through post-experimental code instrumentation and execution tracing using BGG <ref> [Vou89] </ref>. After the functional and random certification tests, the programs were subjected to an independent operational evaluation test [Eck91] which found additional defects in the code. Detailed descriptions of the detected faults of the employed testing strategies are given in [Vou90, Eck91, Tai94].
Reference: [Vou90] <author> M.A. Vouk, A. Caglayan, D.E. Eckhardt , J.P.J. Kelly, J. Knight, D.F. McAllister, L. Walker, </author> <title> "Analysis of faults detected in a large-scale multiversion software development experiment," </title> <booktitle> Proc. </booktitle> <volume> DASC '90, </volume> <pages> pp. 378-385, </pages> <year> 1990. </year>
Reference-contexts: EMPIRICAL RESULTS 4.1 Data The empirical data used in this investigation derives from three sources. One set is from the paper by Ohba [Ohb84], the second set comes from the NASA Langley Research Center (LaRC) sponsored multi-version multi-university software experiment <ref> [Kel88, Vou90, Eck91, Tai94] </ref>, and the third set comes from the functional test debug stage of a large telecommunications system [Bor91, Cra92]. The first (Ohba) dataset represents testand-debug data of a PL/I database application program described in [Ohb84], Appendix F, Table 4. <p> After the functional and random certification tests, the programs were subjected to an independent operational evaluation test [Eck91] which found additional defects in the code. Detailed descriptions of the detected faults of the employed testing strategies are given in <ref> [Vou90, Eck91, Tai94] </ref>. <p> In this illustration we use puse coverage data for three of the 20 functionally equivalent programs, and we have set N to the number of faults eventually detected in these programs through extensive tests that followed the "certification" phase <ref> [Vou90, Eck91] </ref>. In the figures, we plot the "instantaneous" or "per test-case" efficiency, and its arithmetic average over the covered metric space.
Reference: [Vou92] <author> M. A. Vouk, </author> <title> "Using Reliability Models During Testing with NonOperational Profiles," </title> <booktitle> Proc. 2nd Bellcore/Purdue workshop issues in Software Reliability Estimation, </booktitle> <month> Oct. </month> <year> 1992, </year> <pages> pp. 103-111. </pages>
Reference: [Wal78] <author> R. E. Walpole, R. H. </author> <title> Myers "Probability and Statistics for Engineers and Scientists," </title> <publisher> Macmillan Publishing Co., Inc., </publisher> <year> 1978. </year>
Reference: [Yam93] <author> Shigeru Yamada, Jun Hishitani, Shunji Osaki, </author> <title> "Software-Reliability Growth with a Weibull Test-Effort: A Model & Application," </title> <journal> IEEE Transactions on Reliability, </journal> <volume> Vol. 42, No. 1, </volume> <year> 1993 </year> <month> March, </month> <pages> pp. 100-106. </pages>
Reference-contexts: It is possible that a Weibull-type testing effort weight function may be responsible <ref> [Yam93] </ref>. On the other hand, the "noise" (variability) on g (Q) is so large that reading more than a general ascending, level or descending trend into the data is probably not justified.
References-found: 24

