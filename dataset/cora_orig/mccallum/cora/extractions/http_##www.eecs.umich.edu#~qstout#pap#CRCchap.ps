URL: http://www.eecs.umich.edu/~qstout/pap/CRCchap.ps
Refering-URL: http://www.eecs.umich.edu/~qstout/papers.html
Root-URL: http://www.cs.umich.edu
Title: Algorithmic Techniques for Networks of Processors  
Author: Russ Miller and Quentin F. Stout 
Affiliation: State University of New York at Buffalo and University of Michigan  
Note: To appear in CRC Handbook of Algorithms and Theory of Computation, M.J. Atallah, ed., 1998.  
Abstract-found: 0
Intro-found: 1
Reference: [Akl and Lyon, 1993] <author> Akl, S.G. and Lyon, K.A. </author> <year> 1993. </year> <title> Parallel Computational Geometry, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: [Atallah and Goodrich, 1986] <author> Atallah, M.J. and Goodrich, </author> <title> M.T. 1986. Efficient parallel solutions to geometric problems, </title> <journal> Journal of Parallel and Distributed Computing 3 (1986), </journal> <pages> pp. 492-507. </pages>
Reference-contexts: The maximal point problem is to determine all maximal points of S. See Figure 4. The following parallel algorithm for the maximal point problem was apparently first noted by Atallah and Goodrich <ref> [Atallah and Goodrich, 1986] </ref>. 1. Sort the n planar points in reverse order by x-coordinate, with ties broken by reverse order by y-coordinate. Let (i x ; i y ) denote the coordinates of the i th point after the sort is 11 complete.
Reference: [Brainerd, Goldberg, and Adams, 1990] <author> Brainerd, W.S., Goldberg, C., and Adams, J.C. </author> <year> 1990. </year> <title> Programmers Guide to FORTRAN 90, </title> <publisher> McGraw-Hill Book Company, </publisher> <address> New York, NY. </address>
Reference-contexts: Further, since several of these operations are widely useful, they are often made available in highly optimized implementations. The language APL provided a model for several of these operations, and some parallel versions of APL have appeared. Languages such as C* [Thinking Machines Corporation, 1991] and FORTRAN 90 <ref> [Brainerd, Goldberg, and Adams, 1990] </ref> also provide for some forms of global operations, as do message-passing systems such as MPI [Snir, Otto, Huss-Lederman, Walker, and Dongarra, 1995]. Reduction operations are so important that most parallelizing compilers detect them automatically, even if they have no explicit support for other global operations.
Reference: [Flynn, 1966] <author> Flynn, M.J. </author> <year> 1966. </year> <title> Very high-speed computing systems, </title> <journal> Proc. of the IEEE, </journal> <volume> 54(12) </volume> <pages> 1901-1909. </pages>
Reference-contexts: a processing element and Mem is used to represent memory.) Flynn's Taxonomy In 1966, Michael Flynn classified computer architectures with respect to the instruction stream, that is, the sequence of operations performed by the computer, and the data stream, that is, the sequence of items operated on by the instructions <ref> [Flynn, 1966] </ref>. While extensions and modifications to Flynn's taxonomy have appeared, Flynn's original taxonomy [Flynn, 1972] is still widely used.
Reference: [Flynn, 1972] <author> Flynn, M.J. </author> <year> 1972. </year> <title> Some computer organizations and their effectiveness, </title> <journal> IEEE Transactions on Computers, C-21:948-960. </journal>
Reference-contexts: While extensions and modifications to Flynn's taxonomy have appeared, Flynn's original taxonomy <ref> [Flynn, 1972] </ref> is still widely used.
Reference: [JaJa, 1992] <author> JaJa, J. </author> <year> 1992. </year> <title> An Introduction to Parallel Algorithms, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference: [Kung, Luccio, and Preparata, 1975] <author> Kung, H.T., Luccio, F., and Preparata, F.P. </author> <year> 1975. </year> <title> On finding the maxima of a set of vectors, </title> <journal> Journal of the ACM 22(4) </journal> <pages> 469-476. </pages>
Reference-contexts: On all parallel architectures known to the authors, P ref ix (n) = O (Sort (n)), and hence on such machines the time of the algorithm is fi (Sort (n)). It is worth noting that for the sequential model, <ref> [Kung, Luccio, and Preparata, 1975] </ref> have shown that the problem of determining maximal points is as hard as sorting. Divide-and-Conquer Divide-and-conquer is a powerful algorithmic paradigm that exploits the repeated subdivision of problems and data into smaller, similar problems.
Reference: [Leighton, 1992] <author> Leighton, F.T. </author> <year> 1992. </year> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, Hypercubes, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: For example, keeping the expansion as close to 1 as is possible (given the restriction that a hypercube has a power of 2 processors), one can map the complete binary tree onto the hypercube with unit link congestion, dilation two, and unit processor contention. See, for example, <ref> [Leighton, 1992] </ref>. In general, however, finding an optimal weak embedding for a given source and target is an NP-complete problem. This problem, sometimes known as the mapping problem, is often solved by various heuristics.
Reference: [Leiserson, 1985] <author> Leiserson, C.E. </author> <year> 1985. </year> <title> Fat-trees: Universal networks for hardware-efficient supercomputing, </title> <journal> IEEE Transactions on Computers, C-34(10):892-901. </journal>
Reference-contexts: One severe disadvantage of a tree is that when extensive communication occurs, all messages traveling from one side of the tree to the other must pass through the root, causing a bottleneck. This is because the bisection bandwidth is only 1. Fat trees, introduced by Leiserson <ref> [Leiserson, 1985] </ref>, alleviate this problem by increasing the bandwidth of the communication links near the root. This increase can come from changing the nature of the links, or, more easily, by using parallel communication links. <p> Link contention may still be a problem in such machines, but some solve this by increasing the bandwidth on links that would have heavy contention. For example, as noted earlier, fat-trees <ref> [Leiserson, 1985] </ref> add bandwidth near the root to avoid the bottlenecks inherent in a tree architecture. This increases the bisection bandwidth, which reduces the link contention for communication that poorly matches the basic tree structure.
Reference: [Li and Stout, 1991] <author> Li, H. and Stout, Q.F. </author> <year> 1991. </year> <title> Reconfigurable Massively Parallel Computers, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: [Miller and Stout, 1996] <author> Miller, R. and Stout, Q.F. </author> <year> 1996. </year> <title> Parallel Algorithms for Regular Architectures: Meshes and Pyramids, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: We utilize a generic parallel divide-and-conquer solution for this problem, given, for example, in <ref> [Miller and Stout, 1996] </ref>. Suppose that the n fi n image has been divided into p subimages, as square as possible, and distributed one subimage per processor.
Reference: [Quinn, 1994] <author> Quinn, M.J. </author> <year> 1994. </year> <title> Parallel Computing Theory and Practice, </title> <publisher> McGraw-Hill, Inc., </publisher> <address> New York, NY. </address>
Reference: [Reif, 1993] <author> Reif, J., ed. </author> <year> 1993. </year> <title> Synthesis of Parallel Algorithms, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA. </address>
Reference: [Snir, Otto, Huss-Lederman, Walker, and Dongarra, 1995] <author> Snir, M., Otto, S.W., Huss-Lederman, S., Walker, D.W., and Dongarra, J. </author> <year> 1995. </year> <title> MPI: The Complete Reference, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Languages such as C* [Thinking Machines Corporation, 1991] and FORTRAN 90 [Brainerd, Goldberg, and Adams, 1990] also provide for some forms of global operations, as do message-passing systems such as MPI <ref> [Snir, Otto, Huss-Lederman, Walker, and Dongarra, 1995] </ref>. Reduction operations are so important that most parallelizing compilers detect them automatically, even if they have no explicit support for other global operations. Besides broadcast, reduction, and shift, other important global operations include the following.
Reference: [Thinking Machines Corporation, 1991] <institution> Thinking Machines Corporation. </institution> <year> 1991. </year> <title> C* Programming Guide, Version 6.0.2, </title> <address> Cambridge, MA. </address>
Reference-contexts: Further, since several of these operations are widely useful, they are often made available in highly optimized implementations. The language APL provided a model for several of these operations, and some parallel versions of APL have appeared. Languages such as C* <ref> [Thinking Machines Corporation, 1991] </ref> and FORTRAN 90 [Brainerd, Goldberg, and Adams, 1990] also provide for some forms of global operations, as do message-passing systems such as MPI [Snir, Otto, Huss-Lederman, Walker, and Dongarra, 1995].

References-found: 15

