URL: http://www.cs.nyu.edu/cs/faculty/overton/papers/psfiles/sumnorms.ps.gz
Refering-URL: http://www.cs.nyu.edu/cs/faculty/overton/papers/sumnorms.html
Root-URL: http://www.cs.nyu.edu
Title: An Efficient Primal-Dual Interior-Point Method for Minimizing a Sum of Euclidean Norms  
Author: Knud D. Andersen Edmund Christiansen Andrew R. Conn Michael L. Overton 
Address: Yorktown Heights, NY.  New York University, New York, NY.  
Note: Dash Associates, Leamington Spa, England. This work was supported by a Danish SNF Research Studentship.  Supported in part by the U.S. Department of Defense Advanced Research Projects Agency.  Supported in part by IBM Thomas J. Watson Research Center, in part by the National Science Foundation, and in part by the U.S. Department of Energy.  
Date: August 17, 1998  
Affiliation: NYU Computer Science Dept  Department of Mathematics and Computer Science, Odense University, Denmark IBM Thomas J. Watson Research Center,  Courant Institute of Mathematical Sciences,  
Pubnum: Technical Report 769  
Abstract: The problem of minimizing a sum of Euclidean norms dates from the 17th century and may be the earliest example of duality in the mathematical programming literature. This nonsmooth optimization problem arises in many different kinds of modern scientific applications. We derive a primal-dual interior-point algorithm for the problem, by applying Newton's method directly to a system of nonlinear equations characterizing primal and dual feasibility and a perturbed complementarity condition. The main work at each step consists of solving a system of linear equations (the Schur complement equations). This Schur complement matrix is not symmetric, unlike in linear programming. We incorporate a Mehrotra-type predictor-corrector scheme and present some experimental results comparing several variations of the algorithm, including, as one option, explicit symmetrization of the Schur complement with a skew corrector term. We also present results obtained from a code implemented to solve large sparse problems, using a symmetrized Schur complement. This has been applied to problems arising in plastic collapse analysis, with hundreds of thousands of variables and millions of nonzeros in the constraint matrix. The algorithm typically finds accurate solutions in less than 50 iterations and deter mines physically meaningful solutions previously unobtainable. 
Abstract-found: 1
Intro-found: 1
Reference: [AA95] <author> I. Adler and F. Alizadeh. </author> <title> Primal-dual interior point algorithms for convex quadratically constrained and semidefinite optimization problems. </title> <type> Technical Report 46-95, </type> <institution> RUTCOR, </institution> <month> Dec </month> <year> 1995. </year>
Reference-contexts: The sum of norms problem is a special case of quadratically constrained quadratic programming (QCQP), also known as optimization over the quadratic cone. Nesterov and Todd [NT98a, NT98b] give a theoretical discussion of algorithms for optimization over homogeneous self-dual cones, including the quadratic cone. See also Adler and Alizadeh <ref> [AA95] </ref> for another primal-dual algorithmic approach to QCQP. Our view is that the sum of norms problem is sufficiently important that a specialized approach is justified.
Reference: [AA97] <author> E.D. Andersen and K.D. Andersen. </author> <title> The APOS linear programming solver: an implementation of the homogeneous algorithm. </title> <type> 24 Technical Report CORE Discussion Paper 9730, CORE, </type> <institution> UCL, Belgium, </institution> <year> 1997. </year>
Reference-contexts: This is the primary cost of the algorithm. Details of this and other numerical linear algebra issues are available in <ref> [AA97] </ref>, [AY97] and [And96a]. The primal steplength rule was used, with t = 0:99 in equation (22). This sparse implementation was developed over several years with large-scale applications in mind. There are two primary differences from the algorithm discussed in Section 3.
Reference: [AC98] <author> K.D. Andersen and E. Christiansen. </author> <title> Minimizing a sum of norms subject to linear equality constraints. </title> <note> Computational Optimization and Applications, 1998. To appear. </note>
Reference-contexts: Using this method, Ander-sen was the first to be able to solve D rapidly and accurately even when the number of variables is large and many norms kz i k are zero at a solution point. In <ref> [AC98] </ref> it was demonstrated how the linearly constrained problem can be reduced to the unconstrained case using an exact l 1 penalty function, while still preserving sparsity structure. In this paper we present a primal-dual interior-point method for solving P and D. <p> kz i k = 0 relgap kAxk 20 20 134 1224 9e 09 4e 11 60 32 4937 14181 7e 09 5e 11 120 29 36516 57127 7e 09 3e 11 Table 9: Problem and solution characteristics for lN a20 22 The last class of test problems is taken from <ref> [AC98] </ref>. <p> In <ref> [AC98] </ref>, it is shown how the ` 1 penalty function approach makes it possible to transform the linearly constrained problem to the unconstrained form (D) in Section 1, and the physical interpretation and setup of the test problems are described. This class of problems is denoted clN 13. <p> In addition to the number l of linear constraints, there is a new column, "constr", indicating the relative infeasibility of these constraints 23 measured by the expression fl fl E T y d fl kdk + 1 For the primal barrier method in <ref> [AC98] </ref>, the number of iterations varies from 30 (for N = 3 and N = 12) to 201 (for N = 300). For the primal-dual method the variation is from 11 (for N = 3) to 24 (for N = 201) and 35 (for N = 399). <p> For the primal-dual method the variation is from 11 (for N = 3) to 24 (for N = 201) and 35 (for N = 399). For the case N = 201 the CPU time is reduced from 36371 seconds in <ref> [AC98] </ref> to 6179 using the primal-dual method. However, we can do even better: in the clN 13 problems there is one column that is relatively dense, resulting in considerable fill-in during the factorization. <p> Using the same technique, the case N = 201 required 4293 CPU seconds, and there were 6367553 nonzero elements in the L factor. We conclude that for nonsmooth problems the primal-dual method is significantly more efficient than the primal barrier method applied in <ref> [And96b, ACO98, AC98] </ref>. The number of iterations increases slowly with the size of the problem. Finally, the primal-dual method appears to be less vulnerable to ill-conditioning near the optimal solution.
Reference: [ACK + 98] <author> J. Alpert, T.F. Chan, A.B. Kahng, I.L. Markov, and P. Mulet. </author> <title> Faster minimization of linear wirelength for global placement. </title> <journal> IEEE Trans. Computer Aided Design for Integrated Circuits and Systems, </journal> <volume> 17(1), </volume> <month> Jan </month> <year> 1998. </year>
Reference-contexts: The algorithm typically finds accurate solutions in less than 50 iterations and determines physically meaningful solutions that were considered unobtainable until now. In fact, problem D arises in many applications. Alpert, Chan, Kahng, Markov and Mulet <ref> [ACK + 98] </ref> have recently applied a variant of our algorithm presented in this paper to the placement of circuits in VLSI design. Chan, Golub and Mulet [CGM96] applied a nonlinear version of the algorithm to some applications in image reconstruction.
Reference: [ACO98] <author> K.D. Andersen, E. Christiansen, </author> <title> and M.L. Overton. Computing limit loads by minimizing a sum of norms. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 19(3) </volume> <pages> 1046-1062, </pages> <year> 1998. </year>
Reference-contexts: We are unaware of any other published results for large sparse problems of the form (D). These test problems are finite dimensional discretizations of collapse problems in rigid plasticity. The discretization step and the physical interpretation of the results can be found in [Chr96] and in <ref> [ACO98] </ref>. The discrete optimization problems are the same as in [And96b]. The m by dn matrix A is a typical finite element matrix which in plastic analysis is not square since the equilibrium equation for the continuum is under-determined. <p> Using the same technique, the case N = 201 required 4293 CPU seconds, and there were 6367553 nonzero elements in the L factor. We conclude that for nonsmooth problems the primal-dual method is significantly more efficient than the primal barrier method applied in <ref> [And96b, ACO98, AC98] </ref>. The number of iterations increases slowly with the size of the problem. Finally, the primal-dual method appears to be less vulnerable to ill-conditioning near the optimal solution.
Reference: [AM93] <author> J.C. Alexander and J.H. Maddocks. </author> <title> Bounds on the friction-dominated motion of a pushed object. </title> <journal> Int. J. Robotics Research, </journal> <volume> 12 </volume> <pages> 231-248, </pages> <year> 1993. </year>
Reference-contexts: Strang [Str79] considered an isoparametric design problem to which Overton [Ove84] applied a version of his algorithm mentioned above. Parks [Par91] has applied related methods to solve minimal surface (soap bubble) problems. Alexander and Maddocks <ref> [AM93] </ref> used the method of [Ove83] to solve friction problems arising in robotics. A key similarity in all these applications is that some, and perhaps many, of the norms in the sum to be minimized can expected to have the value zero at an optimal solution.
Reference: [And96a] <author> K. D. Andersen. </author> <title> A modified Schur complement method for handling dense columns in interior-point methods for linear programming. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 22(3) </volume> <pages> 348-356, </pages> <year> 1996. </year>
Reference-contexts: This is the primary cost of the algorithm. Details of this and other numerical linear algebra issues are available in [AA97], [AY97] and <ref> [And96a] </ref>. The primal steplength rule was used, with t = 0:99 in equation (22). This sparse implementation was developed over several years with large-scale applications in mind. There are two primary differences from the algorithm discussed in Section 3. <p> However, we can do even better: in the clN 13 problems there is one column that is relatively dense, resulting in considerable fill-in during the factorization. Using the technique described in <ref> [And96a] </ref> for handling dense columns these problems can be solved more efficiently, making it possible to solve for larger values of N . The asterisk in the table indicates that the result for N = 399 was obtained by this method.
Reference: [And96b] <author> K.D. Andersen. </author> <title> An efficient Newton barrier method for minimizing a sum of Euclidean norms. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 6(1) </volume> <pages> 74-95, </pages> <year> 1996. </year>
Reference-contexts: P : max i=1 i x i : x i 2 &lt; d ; kx i k 1; i = 1; : : : ; n; i=1 ) This result is an easy generalization of the duality theory in [Kuh67], but may not have explicitly appeared in the literature until <ref> [And96b] </ref>. Although the duality theory has been known in its simplest form for nearly two centuries, it was not understood until relatively recently how to exploit duality in algorithms for minimizing D. <p> The methods of Calamai and Conn and Overton are quite efficient if not 3 many norms kz i k are zero. However, if this number is large, the number of iterations is typically also large, because the active set of zero norms must be updated at every step. Andersen <ref> [And96b] </ref> gave a method for solving D which is based on a primal interior-point method for LP. <p> In this paper we present a primal-dual interior-point method for solving P and D. The basic algorithm is easy to motivate and implement. The number of iterations required is substantially fewer than for the primal interior-point method used by Andersen <ref> [And96b] </ref>. This is consistent with general experience with primal-dual versus primal interior-point methods for LP [Wri97]. The sum of norms problem is a special case of quadratically constrained quadratic programming (QCQP), also known as optimization over the quadratic cone. <p> This condition is not differentiable if kz i k is zero, but it may be replaced by the centering condition z i kz i k + 2 2 where &gt; 0. The following theorem is from <ref> [And96b] </ref>, showing that the centering condition (7) is in fact the complementarity condition for the following pair of smooth optimization problems: D : min i=1 kz i k 2 + 2 2 ) ( n X 2 2 ) Theorem 1 The problems D and P are a primal-dual pair. <p> Specifically, D has the solution (y (); z ()) and P has the solution x (), all satisfying (5), (6) and (7). Proof: The proof is a simple modification of the proof (given in Section 1) that P and D are a primal-dual pair. See <ref> [And96b] </ref> for details. 2 This theorem shows that introducing the centering parameter in the complementarity conditions for the original pair of problems is equivalent to smoothing the norms in D and introducing a cost into P which moves the primal solution away from its boundary. <p> This modification was found to give significant improvements in performance for the large-scale problems. The first three classes of test problems are taken from <ref> [And96b] </ref>, where a primal barrier method was used. We are unaware of any other published results for large sparse problems of the form (D). These test problems are finite dimensional discretizations of collapse problems in rigid plasticity. <p> These test problems are finite dimensional discretizations of collapse problems in rigid plasticity. The discretization step and the physical interpretation of the results can be found in [Chr96] and in [ACO98]. The discrete optimization problems are the same as in <ref> [And96b] </ref>. The m by dn matrix A is a typical finite element matrix which in plastic analysis is not square since the equilibrium equation for the continuum is under-determined. As earlier, H is block diagonal with block size d fi d. <p> As earlier, H is block diagonal with block size d fi d. In the cases reported here d is either 2 or 3. The runs were made on the same Convex 3240 vector machine (using IEEE-compatible double precision) as in <ref> [And96b] </ref> so comparisons of accuracy and CPU time are meaningful. <p> These numbers are the same as in <ref> [And96b] </ref>, except for small variations in sparsity due to improvements in the implementation. The iteration count is denoted by "iter" and "cpu" is the CPU time in seconds. <p> They all have the same structure, 20 but vary in size, depending on the grid in the finite element analysis. In this problem d = 3. This set of problems is characterized by having no zero norms in the solution, i.e., they are, in fact, smooth optimization problems. In <ref> [And96b] </ref> the constraints kx i k 1 are satisfied within a tolerance of order 10 9 . These constraints are satisfied exactly in the primal-dual method. Except for this improvement in accuracy, the primal-dual method shows no significant difference, for these problems, compared to the primal barrier method in [And96b]. <p> In <ref> [And96b] </ref> the constraints kx i k 1 are satisfied within a tolerance of order 10 9 . These constraints are satisfied exactly in the primal-dual method. Except for this improvement in accuracy, the primal-dual method shows no significant difference, for these problems, compared to the primal barrier method in [And96b]. There is a small reduction in the iteration count, but not in the CPU time. This is clearly a consequence of the fact that these problems are smooth. <p> Compared with <ref> [And96b] </ref>, there is 21 a significant reduction in the number of iterations and in CPU time. If we disregard the smallest case, N = 3, the iteration count for the primal-dual method varies from 19 to 34; for the primal barrier method in [And96b] the variation is from 33 to 176. <p> Compared with <ref> [And96b] </ref>, there is 21 a significant reduction in the number of iterations and in CPU time. If we disregard the smallest case, N = 3, the iteration count for the primal-dual method varies from 19 to 34; for the primal barrier method in [And96b] the variation is from 33 to 176. The primal-dual algorithm also obtains significantly more zero norms in the optimal solution. From our physical understanding of the solution we believe this is correct. <p> As shown in Table 9, the number of zero norms varies from 62 to 96 percent of the total number of terms. Compared with <ref> [And96b] </ref> the iteration count is significantly reduced and increases very slowly with the problem size. The CPU time is reduced by a factor 4 or more, and we are able to solve larger instances of the problem. <p> Using the same technique, the case N = 201 required 4293 CPU seconds, and there were 6367553 nonzero elements in the L factor. We conclude that for nonsmooth problems the primal-dual method is significantly more efficient than the primal barrier method applied in <ref> [And96b, ACO98, AC98] </ref>. The number of iterations increases slowly with the size of the problem. Finally, the primal-dual method appears to be less vulnerable to ill-conditioning near the optimal solution.
Reference: [AY97] <author> E.D. Andersen and Y. Ye. </author> <title> On a homogeneous algorithm for a monotone complementarity problem with nonlinear equality constraints. </title> <editor> In M.C. Ferris and J.S. Pang, editors, </editor> <title> Complementarity and Variational Problems. </title> <booktitle> State of the Art, </booktitle> <pages> pages 1-11. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1997. </year>
Reference-contexts: This is the primary cost of the algorithm. Details of this and other numerical linear algebra issues are available in [AA97], <ref> [AY97] </ref> and [And96a]. The primal steplength rule was used, with t = 0:99 in equation (22). This sparse implementation was developed over several years with large-scale applications in mind. There are two primary differences from the algorithm discussed in Section 3.
Reference: [BB] <author> D.V. Byrnes and L.E. </author> <title> Bright. Design of high-accuracy multiple flyby trajectories using constrained optimization. </title> <note> AAS Paper 95-307, AAS/AIAA Astrodynamics Specialist Conference, </note> <institution> Halifax, Nova Scotia, Canada, </institution> <month> August 14-17, </month> <year> 1995. </year>
Reference-contexts: Chan, Golub and Mulet [CGM96] applied a nonlinear version of the algorithm to some applications in image reconstruction. Byrnes and Bright <ref> [BB] </ref> used iteratively reweighted least-squares to solve trajectory optimization problems in space exploration.
Reference: [CC80] <author> P.H. Calamai and A.R. Conn. </author> <title> A stable algorithm for solving the multifacility location problem involving Euclidean distances. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 1 </volume> <pages> 512-526, </pages> <year> 1980. </year>
Reference-contexts: Neither of these algorithms use any aspect of duality. In both cases, the reason for the slow convergence is that, in most interesting applications, some of the norms in the objective D have zero as their optimal value. Calamai and Conn <ref> [CC80, CC87] </ref> and Overton [Ove83] solved D using Newton methods combined with an active set approach to determine which norms kz i k are zero at an optimal solution.
Reference: [CC87] <author> P.H. Calamai and A.R. Conn. </author> <title> A projected Newton method for l p norm location problems. </title> <journal> Mathematical Programming, </journal> <volume> 38 </volume> <pages> 75-109, </pages> <year> 1987. </year>
Reference-contexts: Neither of these algorithms use any aspect of duality. In both cases, the reason for the slow convergence is that, in most interesting applications, some of the norms in the objective D have zero as their optimal value. Calamai and Conn <ref> [CC80, CC87] </ref> and Overton [Ove83] solved D using Newton methods combined with an active set approach to determine which norms kz i k are zero at an optimal solution.
Reference: [CGM96] <author> T.F. Chan, G.H. Golub, and P. Mulet. </author> <title> A nonlinear primal dual method for TV-based image restoration. </title> <editor> In M.Berger et al, editor, </editor> <booktitle> Lecture Notes in Control and Information Sciences, </booktitle> <volume> volume 219, </volume> <pages> pages 241-252, </pages> <year> 1996. </year> <booktitle> Proc. of ICAOS'96, 12 th Int'l Conf. on Analysis and Optimization of Systems: Images, Wavelets and PDE's, </booktitle> <address> Paris, </address> <month> June 26-28, </month> <year> 1996. </year> <note> To appear in SIAM J. Sci. Comp. </note>
Reference-contexts: In fact, problem D arises in many applications. Alpert, Chan, Kahng, Markov and Mulet [ACK + 98] have recently applied a variant of our algorithm presented in this paper to the placement of circuits in VLSI design. Chan, Golub and Mulet <ref> [CGM96] </ref> applied a nonlinear version of the algorithm to some applications in image reconstruction. Byrnes and Bright [BB] used iteratively reweighted least-squares to solve trajectory optimization problems in space exploration.
Reference: [Chr96] <author> E. Christiansen. </author> <title> Limit analysis of collapse states. </title> <editor> In P.G. Cia-rlet and J.L. Lions, editors, </editor> <booktitle> Handbook of Numerical Analysis, </booktitle> <volume> volume 4, </volume> <pages> pages 193-312. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1996. </year>
Reference-contexts: We are unaware of any other published results for large sparse problems of the form (D). These test problems are finite dimensional discretizations of collapse problems in rigid plasticity. The discretization step and the physical interpretation of the results can be found in <ref> [Chr96] </ref> and in [ACO98]. The discrete optimization problems are the same as in [And96b]. The m by dn matrix A is a typical finite element matrix which in plastic analysis is not square since the equilibrium equation for the continuum is under-determined.
Reference: [DO98] <author> D.R. </author> <title> Dreyer and M.L. Overton. Two heuristics for the Euclidean Steiner tree problem. </title> <journal> Journal on Global Optimization, </journal> <volume> 13 </volume> <pages> 95-106, </pages> <year> 1998. </year>
Reference-contexts: These results were obtained using a Matlab implementation run on a set of small topologically-constrained Steiner tree test problems (for more details, see <ref> [DO98] </ref>). The sparsity in the data is determined by the tree structure and its topological constraint, but subject to these qualifications, the data are generated randomly. Each table shows a summary of results from many runs with different random data on the same problem class. Sparsity was not exploited. <p> In Tables 3 through 6 we display results for a different class of topologically constrained Steiner tree examples, based on the Chung-Graham ladder problem (see <ref> [DO98] </ref>). For these examples, n = 85, m = 84, and the median number of indices for which kz i k equals 0 at the optimal solution is 10.
Reference: [Kuh67] <author> H.W. Kuhn. </author> <title> On a pair of dual nonlinear programs. </title> <editor> In J. Abadie, editor, </editor> <booktitle> Nonlinear Programming, </booktitle> <pages> pages 38-54, </pages> <address> Amsterdam, 1967. </address> <publisher> North-Holland. </publisher>
Reference-contexts: In the early 19th century it was realized that this particular convex optimization problem has a natural dual maximization formulation. Kuhn [Kuh91] regards this as the first instance of duality in the mathematical programming literature. Further history is given in <ref> [Kuh67] </ref>. Duality theory for D is easily described using min-max theory. Let x i 2 &lt; d , i = 1; : : : ; n. <p> Therefore, the dual of D is the primal problem P : max i=1 i x i : x i 2 &lt; d ; kx i k 1; i = 1; : : : ; n; i=1 ) This result is an easy generalization of the duality theory in <ref> [Kuh67] </ref>, but may not have explicitly appeared in the literature until [And96b]. Although the duality theory has been known in its simplest form for nearly two centuries, it was not understood until relatively recently how to exploit duality in algorithms for minimizing D.
Reference: [Kuh91] <author> H.W. Kuhn. </author> <title> Nonlinear programming: A historical note. </title> <editor> In J.K. Lenstra, A.H.G. Rinnooy Kan, and A. Schrijver, editors, </editor> <booktitle> History of Mathematical Programming, </booktitle> <pages> pages 82-96, </pages> <address> Amsterdam, 1991. </address> <publisher> North-Holland. </publisher>
Reference-contexts: This amounts to finding the point in &lt; 2 which minimizes the sum of distances from it to three given points. In the early 19th century it was realized that this particular convex optimization problem has a natural dual maximization formulation. Kuhn <ref> [Kuh91] </ref> regards this as the first instance of duality in the mathematical programming literature. Further history is given in [Kuh67]. Duality theory for D is easily described using min-max theory. Let x i 2 &lt; d , i = 1; : : : ; n.
Reference: [Meh92] <author> S. Mehrotra. </author> <title> On the implementation of a primal-dual interior point method. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 2 </volume> <pages> 575-601, </pages> <year> 1992. </year>
Reference-contexts: A key point is that both predictor and corrector use the same matrix factorization; only the right-hand sides of the linear equations defining the steps differ. Another key component is a technique for estimating the centering parameter . Mehrotra's method was originally given in <ref> [Meh92] </ref>; an excellent discussion may be found in [Wri97, Chap. 10]. We now discuss how to adapt Mehrotra's method to our problem.
Reference: [NT98a] <author> Y. Nesterov and M. Todd. </author> <title> Primal-dual interior-point methods for self-scaled cones. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 8 </volume> <pages> 321-364, </pages> <year> 1998. </year>
Reference-contexts: This is consistent with general experience with primal-dual versus primal interior-point methods for LP [Wri97]. The sum of norms problem is a special case of quadratically constrained quadratic programming (QCQP), also known as optimization over the quadratic cone. Nesterov and Todd <ref> [NT98a, NT98b] </ref> give a theoretical discussion of algorithms for optimization over homogeneous self-dual cones, including the quadratic cone. See also Adler and Alizadeh [AA95] for another primal-dual algorithmic approach to QCQP. Our view is that the sum of norms problem is sufficiently important that a specialized approach is justified. <p> Our view is that the sum of norms problem is sufficiently important that a specialized approach is justified. Also taking this view, Xue and Ye [XY97] give a complexity analysis of the sum of norms problem, using an interior-point method and exploiting the general theory given in <ref> [NT98a, NT98b] </ref>. Our primal-dual algorithm is derived in the next section, applying New-ton's method to three conditions: primal and dual feasibility and complementarity. A key point is the derivation of the appropriate complementarity condition.
Reference: [NT98b] <author> Y. Nesterov and M. Todd. </author> <title> Self-scaled barriers and interior-point methods for convex programming. </title> <journal> Mathematics of Operations Research, </journal> <volume> 22 </volume> <pages> 1-42, </pages> <year> 1998. </year>
Reference-contexts: This is consistent with general experience with primal-dual versus primal interior-point methods for LP [Wri97]. The sum of norms problem is a special case of quadratically constrained quadratic programming (QCQP), also known as optimization over the quadratic cone. Nesterov and Todd <ref> [NT98a, NT98b] </ref> give a theoretical discussion of algorithms for optimization over homogeneous self-dual cones, including the quadratic cone. See also Adler and Alizadeh [AA95] for another primal-dual algorithmic approach to QCQP. Our view is that the sum of norms problem is sufficiently important that a specialized approach is justified. <p> Our view is that the sum of norms problem is sufficiently important that a specialized approach is justified. Also taking this view, Xue and Ye [XY97] give a complexity analysis of the sum of norms problem, using an interior-point method and exploiting the general theory given in <ref> [NT98a, NT98b] </ref>. Our primal-dual algorithm is derived in the next section, applying New-ton's method to three conditions: primal and dual feasibility and complementarity. A key point is the derivation of the appropriate complementarity condition.
Reference: [Ove83] <author> M.L. Overton. </author> <title> A quadratically convergent method for minimizing a sum of Euclidean norms. </title> <journal> Mathematical Programming, </journal> <volume> 27 </volume> <pages> 34-63, </pages> <year> 1983. </year>
Reference-contexts: Neither of these algorithms use any aspect of duality. In both cases, the reason for the slow convergence is that, in most interesting applications, some of the norms in the objective D have zero as their optimal value. Calamai and Conn [CC80, CC87] and Overton <ref> [Ove83] </ref> solved D using Newton methods combined with an active set approach to determine which norms kz i k are zero at an optimal solution. These methods were the first to exploit the duality structure of the problem, as they explicitly compute both primal and dual solutions. <p> Strang [Str79] considered an isoparametric design problem to which Overton [Ove84] applied a version of his algorithm mentioned above. Parks [Par91] has applied related methods to solve minimal surface (soap bubble) problems. Alexander and Maddocks [AM93] used the method of <ref> [Ove83] </ref> to solve friction problems arising in robotics. A key similarity in all these applications is that some, and perhaps many, of the norms in the sum to be minimized can expected to have the value zero at an optimal solution.
Reference: [Ove84] <author> M.L. Overton. </author> <title> Numerical solution of a model problem from col-lapse load analysis. </title> <editor> In J.L. Lions and R. Glowinski, editors, </editor> <booktitle> Computing Methods in Applied Sciences and Engineering VI, </booktitle> <pages> pages 421-437, </pages> <address> Amsterdam, 1984. </address> <publisher> North-Holland. </publisher>
Reference-contexts: Strang [Str79] considered an isoparametric design problem to which Overton <ref> [Ove84] </ref> applied a version of his algorithm mentioned above. Parks [Par91] has applied related methods to solve minimal surface (soap bubble) problems. Alexander and Maddocks [AM93] used the method of [Ove83] to solve friction problems arising in robotics.
Reference: [Par91] <author> H.R. Parks. </author> <title> Numerical approximation of parametric oriented area-minimizing hypersurfaces. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <year> 1991. </year>
Reference-contexts: Strang [Str79] considered an isoparametric design problem to which Overton [Ove84] applied a version of his algorithm mentioned above. Parks <ref> [Par91] </ref> has applied related methods to solve minimal surface (soap bubble) problems. Alexander and Maddocks [AM93] used the method of [Ove83] to solve friction problems arising in robotics.
Reference: [Roc70] <author> R.T. Rockafellar. </author> <title> Convex Analysis. </title> <publisher> Princeton University Press, Princeton University, </publisher> <year> 1970. </year>
Reference-contexts: T = max min i y+z i =c i i=1 i z i kx i k1 y i=1 i x i y T i=1 ! ( n X c T n X A i x i = 0 : The first equality follows from Cauchy-Schwartz, the second from min-max theory <ref> [Roc70, Cor. 37.3.2] </ref>, the third trivially, and the fourth because if P n i=1 A i x i is not zero, the minimized value would be 1.
Reference: [Str79] <author> G. Strang. </author> <title> A minimax problem in plasticity theory. In M.Z. Nashed, editor, Functional Analysis Methods in Numerical Analysis, </title> <address> New York, </address> <year> 1979. </year> <note> Springer-Verlag. Lecture Notes in Mathematics 701. </note>
Reference-contexts: In fact, this method (Weiszfeld's method) has long been used at the Jet Propulsion Laboratory as a basic workhorse to solve problems of the form D that arise in spacecraft missions such as the Galileo and Pioneer "fly-by's" of the outer planets. Strang <ref> [Str79] </ref> considered an isoparametric design problem to which Overton [Ove84] applied a version of his algorithm mentioned above. Parks [Par91] has applied related methods to solve minimal surface (soap bubble) problems. Alexander and Maddocks [AM93] used the method of [Ove83] to solve friction problems arising in robotics.
Reference: [Wei37] <author> E. Weiszfeld. </author> <title> Sur le point par lequel la somme des distances de n points donnes est minimum. </title> <journal> Tohoku Mathematics Journal, </journal> <volume> 43 </volume> <pages> 355-386, </pages> <year> 1937. </year>
Reference-contexts: Although the duality theory has been known in its simplest form for nearly two centuries, it was not understood until relatively recently how to exploit duality in algorithms for minimizing D. Iteratively reweighted least squares (Weiszfeld's method <ref> [Wei37] </ref>) has long been used as a robust though slowly converging method to solve D. Another well-known approach is to replace the terms kz i k in the objective by the differentiable quantity p kz i k 2 + 2 , where is a fixed positive number.
Reference: [Wri97] <author> S.J. Wright. </author> <title> Primal-Dual Interior-Point Methods. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1997. </year>
Reference-contexts: The basic algorithm is easy to motivate and implement. The number of iterations required is substantially fewer than for the primal interior-point method used by Andersen [And96b]. This is consistent with general experience with primal-dual versus primal interior-point methods for LP <ref> [Wri97] </ref>. The sum of norms problem is a special case of quadratically constrained quadratic programming (QCQP), also known as optimization over the quadratic cone. Nesterov and Todd [NT98a, NT98b] give a theoretical discussion of algorithms for optimization over homogeneous self-dual cones, including the quadratic cone. <p> Another key component is a technique for estimating the centering parameter . Mehrotra's method was originally given in [Meh92]; an excellent discussion may be found in <ref> [Wri97, Chap. 10] </ref>. We now discuss how to adapt Mehrotra's method to our problem. <p> All variants used the Dual Line Search Rule. For the symmetrized algorithm, we tested both a version which quits if the Cholesky factorization of H fails, and one that modifies the factorization and continues iterating: the latter is standard practice in LP <ref> [Wri97, p.219] </ref>. We also tested a variant of the symmetrized version which omits the skew correction h (3) i . Finally, we also tested the effect of omitting the correction h (2) i , but this had essentially no effect in any case.
Reference: [XY97] <author> G. Xue and Y. Ye. </author> <title> An efficient algorithm for minimizing a sum of Euclidean norms with applications. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 7 </volume> <pages> 1017-1036, </pages> <year> 1997. </year> <month> 27 </month>
Reference-contexts: See also Adler and Alizadeh [AA95] for another primal-dual algorithmic approach to QCQP. Our view is that the sum of norms problem is sufficiently important that a specialized approach is justified. Also taking this view, Xue and Ye <ref> [XY97] </ref> give a complexity analysis of the sum of norms problem, using an interior-point method and exploiting the general theory given in [NT98a, NT98b]. Our primal-dual algorithm is derived in the next section, applying New-ton's method to three conditions: primal and dual feasibility and complementarity.
References-found: 28

