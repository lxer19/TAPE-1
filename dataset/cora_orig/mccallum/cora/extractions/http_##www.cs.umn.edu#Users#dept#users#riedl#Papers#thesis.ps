URL: http://www.cs.umn.edu/Users/dept/users/riedl/Papers/thesis.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/riedl/Papers/
Root-URL: http://www.cs.umn.edu
Title: ADAPTABLE DISTRIBUTED TRANSACTION SYSTEMS  
Author: by John Thomas Riedl 
Degree: A Thesis Submitted to the Faculty of  In Partial Fulfillment of the Requirements for the Degree of Doctor of Philosophy  
Date: May 1990  
Affiliation: Purdue University  
Abstract-found: 0
Intro-found: 1
Reference: <institution> 118 BIBLIOGRAPHY </institution>
Reference: [A + 85] <editor> Anon et al. </editor> <title> A measure of transaction processing power. </title> <journal> Datamation, </journal> <volume> 31(7) </volume> <pages> 112-118, </pages> <month> April </month> <year> 1985. </year>
Reference-contexts: Each data point is an average of 3-10 experiments, depending on the number needed to get a good confidence interval. The presented data were chosen to show particular features of adaptable transaction processing, but are representative of many other experiments. 6.1.2 Benchmark Data Several benchmarks for database systems exist <ref> [BDT83, A + 85] </ref>. However, for distributed database systems there are no equally well-accepted benchmarks. The problem is that both the data and the workload can be distributed among the sites in many different ways, especially in systems that support data replication. <p> We have implemented the Wisconsin Benchmark and run pilot experiments, but do not report data from this benchmark. 6.1.2.2 DebitCredit Benchmark The DebitCredit (or TP1 or ET1) benchmark is described in <ref> [A + 85] </ref>. DebitCredit is intended to be the simplest realistic transaction processing benchmark. There is only one form of DebitCredit transaction, representing a simple banking transaction. This transaction reads and writes a single tuple from each of three relations: the teller relation, the branch relation, and the account relation.
Reference: [AHU74] <author> A. V. Aho, J. E. Hopcroft, and J. D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <address> Adison-Wesley, Reading, MA, </address> <year> 1974. </year>
Reference-contexts: For instance, a separate pointer can be kept to the write actions, a simple rule such as transpose or move-to-front can be used to keep hot items near the front [BM82], or in the extreme case the actions could be organized in a search tree <ref> [AHU74] </ref>.
Reference: [AKW] <author> A. V. Aho, B. W. Kernighan, and P. J. Weinberger. </author> <title> Awk a pattern scanning and processing language. </title>
Reference-contexts: All of these scripts are user scripts. The file system mounting and unmounting is done using a special setuid program 5 that allows users to mount and unmount certain files systems. Data were collected each morning with a program in the awk language <ref> [AKW] </ref>. This program scans the directory where the log files are stored looking for AD simulator log files. For each AD simulator log file it locates the log files of the other servers involved in the same experiment.
Reference: [Ale86] <author> Nikitas A. Alexandridis. </author> <title> Adaptable software and hardware: Problems and solutions. </title> <journal> IEEE Computer, </journal> <volume> 19(2), </volume> <month> February </month> <year> 1986. </year>
Reference-contexts: Subsystems can be replaced without affecting other parts of the system, and the design of each subsystem supports the implementation of new algorithms. With maintenance costs climbing as high as 80% of life-cycle costs for many systems <ref> [Ale86, Weg84] </ref>, a design that supports future changes is becoming an essential part of software development. Flavors of Adaptability. In [BR89a] we classify adaptability into four broad categories: structural static, structural dynamic, algorithmic and heterogeneous. <p> There is a need for distributed transaction systems that are specifically designed for adaptability. The life-cycle for a system is usually at least five to ten years. During this period the development cost is often dwarfed by maintenance and upgrade costs <ref> [Ale86] </ref>. Furthermore, emerging technologies cannot be easily integrated into such a system. While the system is in production use changing environmental conditions will require adaptability. New computers will be purchased, and old ones sold or retired. Computers will fail and recover, networks will partition and merge.
Reference: [Avi76] <author> A. Avizienis. </author> <title> Fault-tolerant systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-25(12):1304-1312, </volume> <month> December </month> <year> 1976. </year>
Reference-contexts: Chapter 2 surveys related work on adaptability, and chapter 7 contains conclusions and suggestions for further work. 11 2. OTHER EFFORTS IN ADAPTABILITY Significant previous work has been done on adaptability. Research on the recovery block scheme [Ran75] and on N-version programming <ref> [Avi76] </ref> has been focussed on switchable software for fault-tolerance. Much effort is underway to build software that can exploit new-found hardware flexibility, including parallel processing capabilities, to increase performance [KK86].
Reference: [BB88] <author> Bharat Bhargava and Shirley Browne. </author> <title> Using replication to implement reliable storage. </title> <booktitle> In Proc. 26th Annual Allerton Conf. on Communication, Control, and Computing, </booktitle> <pages> pages 368-377, </pages> <address> Monticello, Illinois, </address> <month> September </month> <year> 1988. </year>
Reference-contexts: These techniques adapt to failures by tailoring the actions taken on recovery to the length and/or spatial extent of the failure, resulting in less overhead for recovery. The other direction involves using communication with other sites as an alternative to local disk-based recovery <ref> [BB89b, BB88] </ref>. Because network communication speeds are projected to improve more rapidly than disk I/O performance, increasing the reliance on communication is a promising approach for improving recovery performance. 15 [Hel89] investigates protocols that are necessary to cope with failures in a distributed database system.
Reference: [BB89a] <author> Bharat Bhargava and Shirley Browne. </author> <title> Adaptability to failures using dynamic quorum assignments. </title> <type> Technical Report CSD-TR-886, </type> <institution> Purdue University, </institution> <month> June </month> <year> 1989. </year>
Reference-contexts: The distributed system software must be designed to be adaptable to a wide range of modes of operation, corresponding to the wide range of possible failure modes. For 2 instance, <ref> [BB89a] </ref> describes an algorithm for responding to failures by dynamically adjusting quorum assignments. As a failure continues, more and more quorum assignments are modified. Finally, when the failure is repaired those quorums that were changed can be brought back to their original assignments. <p> Adaptability is proving to be an effective way to increase performance and reliability. 2.1 Other Work in Adaptability in the RAID Project Other Purdue researchers are investigating adaptability in the areas of operating systems [Maf89, BMR89], increasing availability with dynamic adaptability of replication control algorithms <ref> [Bro90, BB89a, Hel89] </ref>, and merging multiple RAID servers into a single process [KLB89]. This section summarizes some of that work. <p> For twenty sites, the performance is improved by 28% with respect to the user-level implementation and the degradation from the kernel-level implementation is only 24%. 2.1.2 Increasing Availability with Adaptable Replication Control Other researchers at Purdue are working on the problem of increasing availability with adaptable replication control <ref> [Hel89, Bro90, BB89a] </ref>. Work at other institutions on this problem is described in section 2.3. 14 [BR89a] develops a formal model for adaptability in transaction processing algorithms and identifies the correctness conditions for switching algorithms at run-time. <p> This model has been used for concurrency control, replication control, distributed commitment, and network partition algorithms <ref> [BR89a, Bro90, BB89a] </ref>. It is being generalized for dynamic reconfiguration of the system to deal with failures. As an application of this work, they are extending the quorum-based schemes to adapt the assignments to increase availability and performance. <p> Rather than specifying quorums to be a majority of votes, [Her87] provides for explicitly listing sets of sites that form read and write quorums. Adaptable quorums are further explored in <ref> [BB89a] </ref>. Quorums that have not been changed during a failure 64 can be used after the failure is repaired. The effect is that the system dynamically adapts to the failure as objects are accessed, with more severe failures automatically causing a higher degree of adaptation.
Reference: [BB89b] <author> Bharat Bhargava and Shirley Browne. </author> <title> Communication-based recovery in replicated databases. </title> <type> Technical Report CSD-TR-891, </type> <institution> Purdue University, </institution> <month> July </month> <year> 1989. </year>
Reference-contexts: These techniques adapt to failures by tailoring the actions taken on recovery to the length and/or spatial extent of the failure, resulting in less overhead for recovery. The other direction involves using communication with other sites as an alternative to local disk-based recovery <ref> [BB89b, BB88] </ref>. Because network communication speeds are projected to improve more rapidly than disk I/O performance, increasing the reliance on communication is a promising approach for improving recovery performance. 15 [Hel89] investigates protocols that are necessary to cope with failures in a distributed database system.
Reference: [BDT83] <author> D. Bitton, D.J. DeWitt, and C. Turbyfil. </author> <title> Benchmarking database systems: a systematic approach. </title> <booktitle> In Proc of the VLDB Conference, </booktitle> <month> October </month> <year> 1983. </year>
Reference-contexts: Each data point is an average of 3-10 experiments, depending on the number needed to get a good confidence interval. The presented data were chosen to show particular features of adaptable transaction processing, but are representative of many other experiments. 6.1.2 Benchmark Data Several benchmarks for database systems exist <ref> [BDT83, A + 85] </ref>. However, for distributed database systems there are no equally well-accepted benchmarks. The problem is that both the data and the workload can be distributed among the sites in many different ways, especially in systems that support data replication. <p> In RAID the problem is not as crucial, since RAID's adaptability features allow the same workload and data distribution to be used for each combination of algorithms. Thus, all variations will be measured under the same conditions. 6.1.2.1 Wisconsin Benchmark The Wisconsin Benchmark <ref> [BDT83] </ref> is a scientifically-designed benchmark that allows a wide range of queries with varying degrees of selectivity on integer and character data. The Wisconsin Benchmark emphasizes performance of individual operations of the database system, such as selects of a certain fraction of the database, or joins of two large relations.
Reference: [BFH + 90] <author> Bharat Bhargava, Karl Friesen, Abdelsalam Helal, Srinivasan Jagan-nathan, and John Riedl. </author> <title> Design and implementation of the RAID V2 distributed database system. </title> <type> Technical report, </type> <institution> Purdue University, </institution> <month> March </month> <year> 1990. </year> <note> In preparation. 119 </note>
Reference-contexts: Version 2 has the same principles and goals as version 1, but takes advantage of the lessons learned from the original RAID implementation to offer improved support for adaptability and reliability <ref> [BFH + 90] </ref>. 45 user 1 user 2 remote ADs remote ACs remote AMs remote AMs local database UI AD AM AC CC Legend AC Atomicity Controller UI User Interface CC Concurrency Controller AM Access Manager RC Replication Controller AD Action Driver 46 There are three primary differences between version 1 <p> Transactions searching the index obtain read-locks on each node (tuple) they visit, and transactions updating the index (usually because they change attributes in the indexed column) must obtain write locks on index tuples that they change. This indexing scheme is described in more detail in <ref> [BFH + 90] </ref>. 59 * A relation descriptor identifies a relation and a list of tuples * within the relation. This descriptor uniquely identifies a * subrelation within the database. <p> These implementations enforce the same concurrency control policy, but one uses a generic data structure specifically designed for adaptability, while the other uses an ad hoc data structure designed specifically for T/O. Currently RAID replication control can use either a ROWA policy, or a quorum consensus policy <ref> [BFH + 90] </ref>. Quorum consensus is general enough that a particular 94 choice of quorums makes it equivalent to ROWA. We will call quorum consensus with this choice of quorums QC-ROWA.
Reference: [BFHR90] <author> Bharat Bhargava, Karl Friesen, Abdelsalam Helal, and John Riedl. </author> <title> Adaptability experiments in the RAID distributed database system. </title> <booktitle> In Proceedings of the IEEE Symposium on Reliability in Distributed Systems, </booktitle> <address> Huntsville, Alabama, </address> <month> October </month> <year> 1990. </year> <note> To appear. </note>
Reference-contexts: Chapter 6 gives the results of our experiments and analysis of algorithmic adaptability in the concurrency control, replication control, and atomicity control sub-systems <ref> [BFHR90] </ref>. This section clarifies the costs and benefits of adaptability, providing information to help make decisions about which of several methods would be best for a given situation, and whether the cost of conversion would be balanced by the benefits of running the new algorithm. <p> We will call quorum consensus with this choice of quorums QC-ROWA. To compare the adaptable implementations with the equivalent non-adaptable implementations, we ran T/O and generic T/O over a range of hot spot sizes and ROWA and QC-ROWA over a range of update percents <ref> [BFHR90] </ref>. Hot spot size affects the the number of concurrency control conflicts, while update percent changes the mix of reads and writes, which changes the performance of the replication controller. <p> This experiment compares the performance cost of higher availability by measuring the performance of ROWA against other replication protocols, and by measuring the performance of 2PC against 3PC. 97 6.5.2 Procedure To compare the cost of higher availability we tested QC-ROWA against a quorum consensus protocol called QC-RSW, for read-size-same-as-write-size <ref> [BFHR90] </ref>. We used the database from Experiment I, and set the quorum parameters for QC-RSW so that both reads and writes access two of the three copies of an item.
Reference: [BG81] <author> P. A. Bernstein and N. Goodman. </author> <title> Concurrency control in distributed database systems. </title> <journal> Computing Surveys, </journal> <volume> 13(2) </volume> <pages> 185-221, </pages> <year> 1981. </year>
Reference-contexts: Of particular importance are failures that can be detected to be site failures, rather than possible network partitionings. Site failure algorithms are special cases of network partition control algorithms that usually allow higher availability. There are numerous choices of algorithms for concurrency control <ref> [BG81] </ref>, network partition management [DGMS85], transaction commit/termination [SS83], database recovery [Koh81], etc. It has been found that certain algorithms for each of the above subsystems cooperate well to reduce bookkeeping and to increase the efficiency of the implementation [Bha84].
Reference: [BGMS86] <author> Daniel Barbara, Hector Garcia-Molina, and Annemarie Spauster. </author> <title> Polices for dynamic vote reassignment. </title> <booktitle> In Proceedings of the 6th International Conference on Distributed Computing Systems, </booktitle> <month> May </month> <year> 1986. </year>
Reference-contexts: For instance, an adaptable system may 6 reassign votes in response to a network partitioning, to improve availability in case of further partitionings. Each reassignment of votes gives certain benefits and has certain costs <ref> [BGMS86] </ref>. Depending on the network connectivity, the cost of messages, and the probability of future failure, one of several vote redistribution techniques should be used. An adaptable system may also decide to change between concur-rency control algorithms, based on changes in the mix of active transactions, or in task requirements. <p> The idea is for a partition with a simple majority to reassign the votes so that it has an overwhelming majority. Common methods involve redistributing the votes from sites in other partitions. This way future partitions are less likely to leave the system with no majority partition <ref> [BGMS86] </ref>. Herlihy [Her87] generalizes to non-voting quorum methods. Rather than specifying quorums to be a majority of votes, Herlihy provides for explicitly listing sets of sites that form read and write quorums. <p> RAID currently uses quorum consensus replication control to handle network partitioning. Recent work on quorums has extended to protocols that dynamically change the number of votes assigned to each data copy during a partitioning <ref> [BGMS86] </ref>. [Her87] generalizes to non-voting quorum methods. Rather than specifying quorums to be a majority of votes, [Her87] provides for explicitly listing sets of sites that form read and write quorums. Adaptable quorums are further explored in [BB89a].
Reference: [Bha84] <author> Bharat Bhargava. </author> <title> Performance evaluation of reliability control algorithms for distributed database systems. </title> <journal> Journal of Systems and Software, </journal> <volume> 3 </volume> <pages> 239-264, </pages> <month> July </month> <year> 1984. </year>
Reference-contexts: There are numerous choices of algorithms for concurrency control [BG81], network partition management [DGMS85], transaction commit/termination [SS83], database recovery [Koh81], etc. It has been found that certain algorithms for each of the above subsystems cooperate well to reduce bookkeeping and to increase the efficiency of the implementation <ref> [Bha84] </ref>. For example, optimistic concurrency control methods work well with optimistic network partition treatment, log based database recovery mechanisms, and integrity checking systems in a distributed environment. There is a need to experiment with various combinations of algorithms to understand their interactions.
Reference: [Bha87] <author> Bharat Bhargava. </author> <title> Transaction processing and consistency control of replicated copies during failures. </title> <journal> Journal of Management Information Systems, </journal> <volume> 4(2) </volume> <pages> 93-112, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: Second, the servers must collect information from active servers about the final status of transactions that were involved in commitment before the failure. To keep track of out-of-date data items, old RAID maintains fail-locks during failure <ref> [Bha87] </ref>. The Replication Controller keeps a bitmap that records for each other 65 site which data items were updated while that site was down. When the site recovers, it collects the bitmaps from all other sites and merges them.
Reference: [BHG87] <author> P. Bernstein, V. Hadzilacos, and N. Goodman. </author> <title> Concurrency Control and Recovery in Database Systems. </title> <publisher> Addison-Wesley, </publisher> <year> 1987. </year>
Reference-contexts: This transaction reads and writes a single tuple from each of three relations: the teller relation, the branch relation, and the account relation. In addition, a tuple describing the transaction is appended to a special write-only sequential history file. requires that the entire transaction be serializable and recoverable <ref> [BHG87] </ref>. The DebitCredit transaction requires four relations. The teller, branch, and account tuples are 100 bytes long, and the history tuples are 50 bytes long. Each teller, branch, and account tuple must contain an integer key and an fixed-point dollar value.
Reference: [BHJ + 86] <author> Andrew Black, Norman Hutchinson, Eric Jul, Henry Levy, and Larry Carter. </author> <title> Distribution and abstract types in Emerald. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-12(12), </volume> <month> December </month> <year> 1986. </year>
Reference-contexts: Emerald partially solves the problem for smaller units by making all entities in the system objects (down to the granularity of integer variables), and providing compiler support to make accesses to small objects efficient <ref> [BHJ + 86] </ref>. Another use of control and information flow to improve adaptability is hierarchical design. For instance, the ISO/OSI standard for networks is based on layers of functionality each of which interfaces only with the layer immediately above and 8 immediately below. <p> Conic supports reconfiguration adaptability, both through providing powerful primitives for implementing the server model and through the interconnection language. Conic does not provide consistency guarantees during conversion, and does not support algorithmic adaptability within nodes. 2.4.2 Emerald Emerald <ref> [BHJ + 86] </ref> is a descendent of Eden [J + 82]. Both are distributed systems that support objects.
Reference: [Bir85] <author> Ken Birman. </author> <title> Replication and availability in the ISIS system. </title> <journal> Operating System Review, </journal> <volume> 19(5) </volume> <pages> 79-86, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: For instance, ISIS provides a set of distributed programming primitives that can be used to implement a wide variety of types of distributed systems <ref> [Bir85] </ref>. In particular, ISIS provides a range of reliable multicast functions that can be assembled in different ways to build systems with varying tradeoffs between efficiency and reliability. The toolkit design succeeds at being adaptable by providing new functionality without restricting its use.
Reference: [BLLR88] <author> Bharat Bhargava, Fady Lamaa, Pei-Jyun Leu, and John Riedl. </author> <title> Three experiments in reliable transaction processing in raid. </title> <type> Technical Report CSD-TR-782, </type> <institution> Purdue University, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: Most real systems are likely to use something in between, balancing the need to have copies in case of site failures with the performance cost of performing operations on multiple sites. <ref> [BLLR88] </ref> describes experiments that show that a relatively low degree of replication (two or three) along with a policy of generating new replicas when sites fail has nearly the same availability as full replication, with much lower cost.
Reference: [BM82] <author> Jon Louis Bentley and Catherine McGeoch. </author> <title> Worst-case analysis of self-organizing sequential search heuristics. </title> <booktitle> In Proceedings of 20th Allerton Conference on Communication, Control, and Computing, </booktitle> <pages> pages 452-461, </pages> <institution> University of Illinois, Urbana-Champaign, </institution> <month> October </month> <year> 1982. </year>
Reference-contexts: For instance, a separate pointer can be kept to the write actions, a simple rule such as transpose or move-to-front can be used to keep hot items near the front <ref> [BM82] </ref>, or in the extreme case the actions could be organized in a search tree [AHU74].
Reference: [BM84] <author> J. Bachant and J. McDermot. </author> <title> R1 revisited: Four years in the trenches. </title> <journal> AI Magazine, </journal> <volume> 5(3) </volume> <pages> 21-32, </pages> <month> September </month> <year> 1984. </year>
Reference-contexts: In [BMR89] we discuss the advantages and disadvantages of each approach. Here we briefly outline the main ideas. Digital Equipment Corporation's configuration expert system, XICON, can assist users in the customized configuration of a complete computing system <ref> [BM84] </ref>. It builds a monolithic operating system tuned to a particular application. Hoare proposed the small-kernel approach to operating systems [Hoa72]. Under this model, the kernel provides only basic services, i.e., process and memory management, and interprocess communication.
Reference: [BM89] <author> Kenneth Birman and Keith Marzullo. </author> <title> ISIS and the META project. </title> <booktitle> Sun Technology, </booktitle> <pages> pages 90-104, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Examples of these services include buffer management, file system support, process management, interpro-cess communication, concurrency control, atomicity control, and crash recovery. The services that are present in current operating systems are general-purpose and do not satisfy the demands of distributed transaction processing algorithms <ref> [S + 85, BM89] </ref>. For instance, locking facilities and buffer management are generally implemented by database systems because the services provided in operating systems are inadequate. The Push system consists of a Push machine, a Push assembler, and a set of Push utilities.
Reference: [BMK88] <author> David R. Boggs, Jeffrey C. Mogul, and Christopher A. Kent. </author> <title> Measured capacity of an Ethernet: Myths and reality. </title> <booktitle> In Proceedings of the SIG-COMM Conference, </booktitle> <month> August </month> <year> 1988. </year> <month> 120 </month>
Reference-contexts: The ethernet backoff policy doubles the restart delay after each time an individual transaction is aborted. This policy is very successful in maintaining fairness and throughput in the ethernet as contention increases, because conflicting packets quickly spread out over a large period of time, reducing contention <ref> [BMK88] </ref>. We experimentally compared the eight methods, running each for a range of inter-arrival gaps fi. We measured stability, the ability of the method to avoid excessively high multiprogramming level, and average response time.
Reference: [BMR] <author> Bharat Bhargava, Enrique Mafla, and John Riedl. </author> <title> Communication in the Raid distributed database system. </title> <journal> International Journal on Computers and ISDN Systems, </journal> 1991(21) 81-92. 
Reference-contexts: Servers can fail, recover, adapt, migrate, or be replaced separately. Currently there are six major subsystems in RAID: User Interface, Action Driver, Access Manager, Atomicity Controller, Concurrency Controller, and Replication Controller. RAID has proven useful in supporting experiments in communication <ref> [BMR87, BMR] </ref>, adaptability [BMR89], and transaction processing [BR89b]. However, there were several new features desired in RAID, to support additional experiments in adaptability and reliability, and to support O-RAID, an object-oriented extension to RAID.
Reference: [BMR87] <author> Bharat Bhargava, Tom Mueller, and John Riedl. </author> <title> Experimental analysis of layered Ethernet software. </title> <booktitle> In Proc of the ACM-IEEE Computer Society 1987 Fall Joint Computer Conference, </booktitle> <pages> pages 559-568, </pages> <address> Dallas, Texas, </address> <month> October </month> <year> 1987. </year>
Reference-contexts: Servers can fail, recover, adapt, migrate, or be replaced separately. Currently there are six major subsystems in RAID: User Interface, Action Driver, Access Manager, Atomicity Controller, Concurrency Controller, and Replication Controller. RAID has proven useful in supporting experiments in communication <ref> [BMR87, BMR] </ref>, adaptability [BMR89], and transaction processing [BR89b]. However, there were several new features desired in RAID, to support additional experiments in adaptability and reliability, and to support O-RAID, an object-oriented extension to RAID.
Reference: [BMR89] <author> Bharat Bhargava, Enrique Mafla, and John Riedl. </author> <title> Experimental facility for kernel extensions to support distributed database systems. </title> <type> Technical Report CSD-TR-930, </type> <institution> Purdue University, </institution> <month> April </month> <year> 1989. </year>
Reference-contexts: As a result, distributed systems often duplicate operating system functions in these areas. One solution to these problems is to investigate ways in which operating systems can change their algorithms to accommodate a range of potential uses. PUSH <ref> [BMR89] </ref> is a general facility through which users can tailor operating system algorithms to their needs. A user can write a program in PUSH and load it into the system. PUSH prevents the user from violating security, but provides the power and efficiency of running directly in the operating system. <p> Adaptability is proving to be an effective way to increase performance and reliability. 2.1 Other Work in Adaptability in the RAID Project Other Purdue researchers are investigating adaptability in the areas of operating systems <ref> [Maf89, BMR89] </ref>, increasing availability with dynamic adaptability of replication control algorithms [Bro90, BB89a, Hel89], and merging multiple RAID servers into a single process [KLB89]. This section summarizes some of that work. <p> Complexity and efficiency are characteristic of kernel-resident code, while simplicity and poor performance are characteristic of user-level code. This section briefly describes a system called Push, that facilitates changing the functionality of the operating system kernel dynamically. Push is described in more detail in <ref> [Maf89, BMR89] </ref>. Other efforts in adaptable operating systems are described in section 2.4.3. Push 12 combines the flexibility and safety of user-level code with the efficiency and security of kernel-level code. Push can be used to implement semantically-rich system call interfaces that provide enhanced support for specific transaction processing systems. <p> In this environment, migration can significantly improve performance. 22 2.4.3 Other Paradigms for Extensible Operating Systems Several paradigms to achieve extensibility in operating systems have been proposed and implemented. They include parameterized operating systems, minimal kernels, synthesized code, streams, and the packet filter approach. In <ref> [BMR89] </ref> we discuss the advantages and disadvantages of each approach. Here we briefly outline the main ideas. Digital Equipment Corporation's configuration expert system, XICON, can assist users in the customized configuration of a complete computing system [BM84]. It builds a monolithic operating system tuned to a particular application. <p> Servers can fail, recover, adapt, migrate, or be replaced separately. Currently there are six major subsystems in RAID: User Interface, Action Driver, Access Manager, Atomicity Controller, Concurrency Controller, and Replication Controller. RAID has proven useful in supporting experiments in communication [BMR87, BMR], adaptability <ref> [BMR89] </ref>, and transaction processing [BR89b]. However, there were several new features desired in RAID, to support additional experiments in adaptability and reliability, and to support O-RAID, an object-oriented extension to RAID.
Reference: [BMRS89] <author> Bharat Bhargava, Enrique Mafla, John Riedl, and Bradley Sauder. </author> <title> Implementation and measurements of an efficient communication facility for distributed database systems. </title> <booktitle> In Proceedings of the 5th IEEE Data Engineering Conference, </booktitle> <address> Los Angeles, CA, </address> <month> February </month> <year> 1989. </year>
Reference-contexts: In order to support multicast, this utility has to call the device driver for each member in the multicast group. The kernel-level SE multicast utility uses the multiSE device driver <ref> [BMRS89] </ref>. <p> For instance, we are changing the implementation of the "send to all Atomicity Controllers" primitive to make use of a kernel-level multicast mechanism that we have implemented <ref> [BMRS89] </ref>. The RAID oracle is a server process listening on a well-known port for requests from other servers. The two major functions it provides are lookup and registration. The oracle maintains for each server a notifier list of other servers that wish to know if its address changes.
Reference: [BN84] <author> Andrew D. Birrell and Bruce Jay Nelson. </author> <title> Implementing remote procedure calls. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(1) </volume> <pages> 39-59, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: Since the handler routines run synchronously they cannot block, and even if multiple servers are merged into a single address space there is still only a single thread of control. A better implementation would be based on remote procedure calls (RPCs) <ref> [BN84] </ref> with invocation of asynchronous lightweight processes (LWPs) as needed. The LWPs can block on I/O or messages and different servers are supported by different LWPs. Multiple LWPs can work on the same server if 62 semaphores are used for access control to the server data structures.
Reference: [BNS88] <author> Bharat Bhargava, Paul Noll, and Donna Sabo. </author> <title> An experimental analysis of replicated copy control during site failure and recovery. </title> <booktitle> In Proc. of the 4th IEEE Data Engineering Conference, </booktitle> <pages> pages 82-91, </pages> <address> Los Angeles, CA, </address> <month> February </month> <year> 1988. </year>
Reference-contexts: After 80% of the stale copies have been refreshed in this way (for free!), old RAID issues copier transactions to refresh the rest of the stale copies. Experiments show this to be an effective way to efficiently maintain fault-tolerance <ref> [BNS88] </ref>. We have a complete implementation of this technique in a stand-alone system called mini-RAID, and everything except copier transactions implemented in the full old RAID system.
Reference: [BP88] <author> Brian N. Bershad and C. Brian Pinkerton. Watchdogs: </author> <title> Extending the UNIX file system. </title> <booktitle> In USENIX Winter Conference, </booktitle> <pages> pages 267-275, </pages> <address> Dallas, TX, </address> <month> February </month> <year> 1988. </year>
Reference-contexts: These rules can be quite complex and can be changed dynamically. By running inside the kernel, the packet filter eliminates much of the context switch overhead incurred by user-level demultiplexers. Extensions to the Unix file system have been proposed in <ref> [BP88] </ref>. There, the additional file system services are implemented in user-level servers. The Unix kernel is modified to associate special processing requests with files.
Reference: [BR88a] <author> Bharat Bhargava and John Riedl. </author> <title> Implementation of RAID. </title> <booktitle> In Proc. of the 7th IEEE Symposium on Reliability in Distributed Systems, </booktitle> <address> Columbus, Ohio, </address> <month> October </month> <year> 1988. </year>
Reference-contexts: Control and information flow can support adaptability by carefully managing the interfaces between parts of the distributed program so that pieces can be replaced without disturbing the whole. For instance, the RAID system is constructed as a group of servers communicating only through well-defined messages <ref> [BR88a] </ref>. Any single server can be replaced with a new implementation as long as that implementation understands and sends the same messages. Conic is another system that uses this principle. A Conic distributed program consists of a series of multi-threaded processes ("nodes") connected only by typed ports.
Reference: [BR88b] <author> Bharat Bhargava and John Riedl. </author> <title> A model for adaptable systems for transaction processing. </title> <booktitle> In Proceedings of the 4th IEEE Data Engineering Conference, </booktitle> <pages> pages 40-50, </pages> <address> Los Angeles, CA, </address> <month> February </month> <year> 1988. </year>
Reference-contexts: Furthermore, the syntax and semantics of object references are the same whether the message is to an object in the same address space, in a different address space on the same computer, or even on a different computer. Models of adaptability are another tool. In <ref> [BR88b] </ref> we develop the sequencer model for subsystems of adaptable transaction systems (section 3.1). The sequencer model formalizes three basic approaches to algorithmic adaptability: generic state, 4 converting state, and suffix-sufficient state.
Reference: [BR89a] <author> Bharat Bhargava and John Riedl. </author> <title> A model for adaptable systems for transaction processing. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <month> December </month> <year> 1989. </year>
Reference-contexts: With maintenance costs climbing as high as 80% of life-cycle costs for many systems [Ale86, Weg84], a design that supports future changes is becoming an essential part of software development. Flavors of Adaptability. In <ref> [BR89a] </ref> we classify adaptability into four broad categories: structural static, structural dynamic, algorithmic and heterogeneous. Structural static adaptability consists of software engineering techniques for developing system software that can adapt to requirements changes over its life-cycle. <p> More difficult problems include getting diverse database systems to work together and developing software that takes advantage of the particular strengths of each machine. Techniques for Adaptability. In <ref> [BR89a] </ref> we also describe an infrastructure of software tools for adaptable software: modular design, powerful communications primitives, new languages, and design models. Support for modular design can make it easier to implement adaptability. <p> Work at other institutions on this problem is described in section 2.3. 14 <ref> [BR89a] </ref> develops a formal model for adaptability in transaction processing algorithms and identifies the correctness conditions for switching algorithms at run-time. This model has been used for concurrency control, replication control, distributed commitment, and network partition algorithms [BR89a, Bro90, BB89a]. <p> This model has been used for concurrency control, replication control, distributed commitment, and network partition algorithms <ref> [BR89a, Bro90, BB89a] </ref>. It is being generalized for dynamic reconfiguration of the system to deal with failures. As an application of this work, they are extending the quorum-based schemes to adapt the assignments to increase availability and performance. <p> MODEL FOR ADAPTABILITY 3.1 Sequencer Model In this section we describe the sequencer model for algorithmic adaptability, which applies in a natural way to many subsystems of a transaction system. The model is based on Papadimitriou's model for concurrency control [Pap79], and presented in <ref> [BR89a] </ref>. A primary advantage of this model is that it provides for a clean interface between subsystems. The model is based on the idea of a sequencer, which orders atomic actions in a transaction processing history according to some correctness criterion. <p> The performance, storage requirements, and abort cost of the two generic data structures are compared in <ref> [BR89a] </ref>. 3.2.2 State Conversion Adaptability We now demonstrate the use of the converting state approach to adaptable concur-rency control with several examples of algorithms for converting the data structures. All of the examples require time at most proportional to the union of the sizes of the read-sets of active transactions. <p> We are in the process of implementing the converting state adaptability method to convert between two- and three- phase commit algorithms while processing transactions. This section gives an overview of the method. More details are in <ref> [BR89a] </ref>. The Atomicity Controller tracks each transaction in a state-transition diagram. Adaptability adds transitions from certain states in the two-phase commit protocol to other states in the three-phase commit protocol and vice versa. The complete set of transitions supported in RAID is depicted in figure 5.1. <p> Dashed lines in the figure represent transitions to or from equivalent states that may 66 three-phase commit two-phase commit C C Q W 2 A tocols. 67 be useful in implementations. Solid lines between the two protocols indicate legal adaptations. In <ref> [BR89a] </ref> we show that the transitions shown in the figure are a correct and complete set of adaptations for centralized 2PC and 3PC. We also extend the result to decentralized commit protocols. <p> The complete protocol is given in <ref> [BR89a] </ref>. Commitment is different from many of the other protocols used in distributed systems in that each transaction can run using a different commit method.
Reference: [BR89b] <author> Bharat Bhargava and John Riedl. </author> <title> The RAID distributed database system. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(6), </volume> <month> June </month> <year> 1989. </year>
Reference-contexts: Chapter 5 describes the implementation of adaptability in individual RAID servers. Here we describe the flow of information in RAID, the organization of RAID into servers, and the role of each server. 4.1 RAID RAID is an experimental distributed database system <ref> [BR89b] </ref> being developed on SUNs under the UNIX operating system (Figure 4.1). RAID has been designed to support experiments in adaptable, reliable distributed transaction processing. RAID is structured as a server-based system. <p> Servers can fail, recover, adapt, migrate, or be replaced separately. Currently there are six major subsystems in RAID: User Interface, Action Driver, Access Manager, Atomicity Controller, Concurrency Controller, and Replication Controller. RAID has proven useful in supporting experiments in communication [BMR87, BMR], adaptability [BMR89], and transaction processing <ref> [BR89b] </ref>. However, there were several new features desired in RAID, to support additional experiments in adaptability and reliability, and to support O-RAID, an object-oriented extension to RAID. To achieve these goals the RAID group re-implemented the control flow in RAID, creating version 2 of RAID (RAID V2).
Reference: [Bro90] <author> Shirley Browne. </author> <title> Efficient Recovery in Replicated Database Systems. </title> <type> PhD thesis, </type> <institution> Purdue University, </institution> <month> May </month> <year> 1990. </year> <note> In preparation. 121 </note>
Reference-contexts: Adaptability is proving to be an effective way to increase performance and reliability. 2.1 Other Work in Adaptability in the RAID Project Other Purdue researchers are investigating adaptability in the areas of operating systems [Maf89, BMR89], increasing availability with dynamic adaptability of replication control algorithms <ref> [Bro90, BB89a, Hel89] </ref>, and merging multiple RAID servers into a single process [KLB89]. This section summarizes some of that work. <p> For twenty sites, the performance is improved by 28% with respect to the user-level implementation and the degradation from the kernel-level implementation is only 24%. 2.1.2 Increasing Availability with Adaptable Replication Control Other researchers at Purdue are working on the problem of increasing availability with adaptable replication control <ref> [Hel89, Bro90, BB89a] </ref>. Work at other institutions on this problem is described in section 2.3. 14 [BR89a] develops a formal model for adaptability in transaction processing algorithms and identifies the correctness conditions for switching algorithms at run-time. <p> This model has been used for concurrency control, replication control, distributed commitment, and network partition algorithms <ref> [BR89a, Bro90, BB89a] </ref>. It is being generalized for dynamic reconfiguration of the system to deal with failures. As an application of this work, they are extending the quorum-based schemes to adapt the assignments to increase availability and performance.
Reference: [BRW87] <author> Bharat Bhargava, John Riedl, and Detlef Weber. </author> <title> An expert system to control an adaptable distributed database system. </title> <type> Technical Report CSD-TR-693, </type> <institution> Purdue University, </institution> <month> May </month> <year> 1987. </year>
Reference-contexts: We wish to make the system adaptive, so it automatically responds to changes in its environment and workload. We have experimented with a prototype expert system that determines when to switch to a new concurrency control algorithm <ref> [BRW87] </ref>. The expert system uses a rule database describing relationships between performance data and algorithms. The rules are combined using a forward reasoning process to determine an indication of the suitability of the available algorithms for the current processing situation. <p> The expert system could use the results of experiments like these to determine the approximate costs and benefits of dynamic adaptation in a given situation. We describe some experience with a prototype expert system for controlling an adaptable distributed system in <ref> [BRW87] </ref>. 114 7. CONCLUSIONS AND FURTHER WORK 7.1 Results We believe that adaptability is an achievable goal.
Reference: [Che88] <author> David R. Cheriton. </author> <title> The V distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 31(3), </volume> <month> March </month> <year> 1988. </year>
Reference-contexts: Hoare proposed the small-kernel approach to operating systems [Hoa72]. Under this model, the kernel provides only basic services, i.e., process and memory management, and interprocess communication. In the last decade, several small-kernel operating systems have been proposed and implemented <ref> [Che88, DLS85] </ref>. Operating system services are provided as server processes. These servers can be replaced with servers designed for particular applications. For example, we could have lock managers, atomicity controllers, consistency controllers to support distributed transaction processing.
Reference: [Com79] <author> D. Comer. </author> <title> The ubiquitous b-tree. </title> <journal> ACM Computing Surveys, </journal> <volume> 11(2), </volume> <month> June </month> <year> 1979. </year>
Reference-contexts: The tuples of this index relation all contain key fields from the original relation and tuple ids of other tuples in the index relation. The tuples are organized into a b-tree <ref> [Com79] </ref>, with the leaves of the tree pointing to tuples of the original relation. When the AM wishes to search the index, it proceeds from the root of this b-tree issuing read requests for other nodes 58 in the tree as necessary.
Reference: [Com84] <author> Douglas Comer. </author> <title> Operating System Design: The Xinu Approach. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1984. </year>
Reference-contexts: Blocking concurrency controllers are very sensitive to the timeout chosen for high degrees of conflict. Since 1 A delta list is a list of times in increasing distance from the present <ref> [Com84] </ref>. The time for an element tells how long after the preceding element an alarm should occur. 2 If the transaction is already in commitment it may not be abortable.
Reference: [CS84] <author> Michael Carey and Michael Stonebraker. </author> <title> The performance of concurrency control algorithms for database management systems. </title> <booktitle> In Proceedings of the Tenth International Conference on Very Large Data Bases, </booktitle> <address> Singapore, </address> <month> August </month> <year> 1984. </year>
Reference-contexts: The problem is that at a high degree of concurrency an open system is very unstable, and at a low degree of con-currency the choice of concurrency controller does not matter since there are very few concurrency conflicts <ref> [CS84] </ref>.
Reference: [DGMS85] <author> Susan B. Davidson, Hector Garcia-Molina, and Dale Skeen. </author> <title> Consistency in partitioned networks. </title> <journal> ACM Computing Surveys, </journal> <volume> 17(3), </volume> <month> September </month> <year> 1985. </year>
Reference-contexts: running the new algorithm is determined to be larger than the cost of adaptation, the expert system recommends switching to the new algorithm. 5.2 Network Partitioning Network partitioning control is the task of maintaining consistency in a distributed system despite some sites not being able to communicate with other sites <ref> [DGMS85] </ref>. There are many solutions to this problem, falling broadly into the classes of optimistic and conservative methods [DGMS85]. RAID currently uses quorum consensus replication control to handle network partitioning. <p> switching to the new algorithm. 5.2 Network Partitioning Network partitioning control is the task of maintaining consistency in a distributed system despite some sites not being able to communicate with other sites <ref> [DGMS85] </ref>. There are many solutions to this problem, falling broadly into the classes of optimistic and conservative methods [DGMS85]. RAID currently uses quorum consensus replication control to handle network partitioning. Recent work on quorums has extended to protocols that dynamically change the number of votes assigned to each data copy during a partitioning [BGMS86]. [Her87] generalizes to non-voting quorum methods. <p> Of particular importance are failures that can be detected to be site failures, rather than possible network partitionings. Site failure algorithms are special cases of network partition control algorithms that usually allow higher availability. There are numerous choices of algorithms for concurrency control [BG81], network partition management <ref> [DGMS85] </ref>, transaction commit/termination [SS83], database recovery [Koh81], etc. It has been found that certain algorithms for each of the above subsystems cooperate well to reduce bookkeeping and to increase the efficiency of the implementation [Bha84].
Reference: [DLS85] <author> P. Dasgupta, R. J. LeBlanc, and E. Spafford. </author> <title> The clouds project: Design and implementation of a fault tolerant distributed operating system. </title> <type> Technical Report GIT-ICS-85/29, </type> <institution> Georgia Tech, </institution> <month> October </month> <year> 1985. </year>
Reference-contexts: Both are distributed systems that support objects. The Emerald project was begun in response to the observed problem in systems such as Eden, Argus [LS83], and Clouds <ref> [DLS85] </ref> that the objects provided by the system were relatively expensive to use. These objects provided support for sharing, concurrency, and distribution, and hence were expensive to invoke. <p> Hoare proposed the small-kernel approach to operating systems [Hoa72]. Under this model, the kernel provides only basic services, i.e., process and memory management, and interprocess communication. In the last decade, several small-kernel operating systems have been proposed and implemented <ref> [Che88, DLS85] </ref>. Operating system services are provided as server processes. These servers can be replaced with servers designed for particular applications. For example, we could have lock managers, atomicity controllers, consistency controllers to support distributed transaction processing.
Reference: [DVB89] <author> Prasun Dewan, Ashish Vikram, and Bharat Bhargava. </author> <title> Engineering the object-relation database model in O-Raid. </title> <booktitle> In Proceedings of the Third International Conference on Foundations of Data Organization and Algorithms, </booktitle> <pages> pages 389-403. </pages> <publisher> Springer Verlag, </publisher> <month> June </month> <year> 1989. </year>
Reference-contexts: This protection allows the object to preserve consistency properties of the data. Systems that support this programming methodology include Smalltalk [GR83], and more recently distributed systems such as Argus [LCJS87] and Camelot [S + 86]. RAID is currently being extended to include support for objects <ref> [DVB89] </ref>. O-RAID takes advantage of the interface specification by providing support for dynamically loading functions as they are required. New implementations can be loaded while the system is running as long as they match the specification. A flaw in the object-oriented approach to adaptability is in its limited granularity.
Reference: [EGLT76] <author> K.P. Eswaran, J.N. Gray, R.A. Lorie, </author> <title> and I.L. Traiger. The notions of consistency and predicate locks in a database system. </title> <journal> Communications of the ACM, </journal> 19(11) 624-633, November 1976. 
Reference-contexts: Although both concurrency controllers made locally correct decisions, the combination permitted a non-serializable history. This section applies the techniques of section 3.1 to permit adaptable concurrency control while preserving correctness. We use three classes of concurrency controllers to present our ideas: two-phase locking (2PL) <ref> [EGLT76] </ref>, timestamp ordering (T/O) [Lam78], and optimistic concur-rency control (OPT) [KR81]. The version of 2PL that we are using implicitly acquires read locks when data items are read, implicitly acquires write locks during transaction commit, and releases all locks after commitment. <p> The first transaction does not read the tuples with name "Ian", but if the second transaction serializes first this is an error! One solution is to implement predicate locking in the concurrency controller to specify exactly the database tuples that are being used by a transaction <ref> [EGLT76] </ref>. This solution has poor performance, though, and is difficult to extend to other data models such as those used by object-oriented systems. Our solution is for the CC to read-lock the entire relation for any transaction that performs an index read.
Reference: [GR83] <author> Adele Goldberg and David Robinson. </author> <title> Smalltalk-80: The Language and its Implementation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1983. </year>
Reference-contexts: The data within each object is protected so that it can only be accessed through the operations. This protection allows the object to preserve consistency properties of the data. Systems that support this programming methodology include Smalltalk <ref> [GR83] </ref>, and more recently distributed systems such as Argus [LCJS87] and Camelot [S + 86]. RAID is currently being extended to include support for objects [DVB89]. O-RAID takes advantage of the interface specification by providing support for dynamically loading functions as they are required.
Reference: [Gra81] <author> Jim Gray. </author> <title> The transaction concept: Virtues and limitations. </title> <booktitle> In Proc of the VLDB Conference, </booktitle> <address> Cannes, France, </address> <month> September </month> <year> 1981. </year>
Reference-contexts: The system processes transactions, which are units of processing with specific concurrency and failure atomicity requirements that must be met regardless of failures in the hardware on which the transactions are running <ref> [Gra81] </ref>. Although distribution is still not realized in most commercial computing, transaction systems comprise a substantial fraction of all computing world-wide. The 5 goal is to support adaptation in this environment without sacrificing transaction consistency. <p> The purpose of a transaction system is to process transactions efficiently while maintaining concurrency atomicity and failure atomicity <ref> [Gra81] </ref>. 25 Definition 2 A history is a set of transactions and a total order on the union of the actions of all of the transactions.
Reference: [Hel89] <author> Abdelsalam Helal. </author> <title> Adaptability to failures in distributed systems, De-cember 1989. </title> <type> Preliminary PhD exam. </type>
Reference-contexts: Adaptability is proving to be an effective way to increase performance and reliability. 2.1 Other Work in Adaptability in the RAID Project Other Purdue researchers are investigating adaptability in the areas of operating systems [Maf89, BMR89], increasing availability with dynamic adaptability of replication control algorithms <ref> [Bro90, BB89a, Hel89] </ref>, and merging multiple RAID servers into a single process [KLB89]. This section summarizes some of that work. <p> For twenty sites, the performance is improved by 28% with respect to the user-level implementation and the degradation from the kernel-level implementation is only 24%. 2.1.2 Increasing Availability with Adaptable Replication Control Other researchers at Purdue are working on the problem of increasing availability with adaptable replication control <ref> [Hel89, Bro90, BB89a] </ref>. Work at other institutions on this problem is described in section 2.3. 14 [BR89a] develops a formal model for adaptability in transaction processing algorithms and identifies the correctness conditions for switching algorithms at run-time. <p> The other direction involves using communication with other sites as an alternative to local disk-based recovery [BB89b, BB88]. Because network communication speeds are projected to improve more rapidly than disk I/O performance, increasing the reliance on communication is a promising approach for improving recovery performance. 15 <ref> [Hel89] </ref> investigates protocols that are necessary to cope with failures in a distributed database system. The work emphasizes failure/repair detection protocols (surveillance protocols) as well as adaptable replication methods.
Reference: [Her87] <author> Maurice Herlihy. </author> <title> Dynamic quorum adjustment for partitioned data. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 12(2) </volume> <pages> 170-194, </pages> <month> June </month> <year> 1987. </year> <month> 122 </month>
Reference-contexts: The idea is for a partition with a simple majority to reassign the votes so that it has an overwhelming majority. Common methods involve redistributing the votes from sites in other partitions. This way future partitions are less likely to leave the system with no majority partition [BGMS86]. Herlihy <ref> [Her87] </ref> generalizes to non-voting quorum methods. Rather than specifying quorums to be a majority of votes, Herlihy provides for explicitly listing sets of sites that form read and write quorums. <p> RAID currently uses quorum consensus replication control to handle network partitioning. Recent work on quorums has extended to protocols that dynamically change the number of votes assigned to each data copy during a partitioning [BGMS86]. <ref> [Her87] </ref> generalizes to non-voting quorum methods. Rather than specifying quorums to be a majority of votes, [Her87] provides for explicitly listing sets of sites that form read and write quorums. Adaptable quorums are further explored in [BB89a]. <p> Recent work on quorums has extended to protocols that dynamically change the number of votes assigned to each data copy during a partitioning [BGMS86]. <ref> [Her87] </ref> generalizes to non-voting quorum methods. Rather than specifying quorums to be a majority of votes, [Her87] provides for explicitly listing sets of sites that form read and write quorums. Adaptable quorums are further explored in [BB89a]. Quorums that have not been changed during a failure 64 can be used after the failure is repaired.
Reference: [Hoa72] <author> C. A. R. Hoare. </author> <title> Operating systems: Their purpose, objectives, functions, and scope. </title> <editor> In Hoare and Perrot, editors, </editor> <booktitle> Operating System Techniques, </booktitle> <pages> pages 11-25. </pages> <publisher> Prentice Hall, </publisher> <year> 1972. </year>
Reference-contexts: Here we briefly outline the main ideas. Digital Equipment Corporation's configuration expert system, XICON, can assist users in the customized configuration of a complete computing system [BM84]. It builds a monolithic operating system tuned to a particular application. Hoare proposed the small-kernel approach to operating systems <ref> [Hoa72] </ref>. Under this model, the kernel provides only basic services, i.e., process and memory management, and interprocess communication. In the last decade, several small-kernel operating systems have been proposed and implemented [Che88, DLS85]. Operating system services are provided as server processes.
Reference: [J + 82] <author> W. H. Jessop et al. </author> <title> The Eden transaction-based file system. </title> <booktitle> In Second IEEE Symposium on Reliability in Distributed Software and Database Systems, </booktitle> <address> Pittsburgh, PA, </address> <month> July </month> <year> 1982. </year>
Reference-contexts: Conic supports reconfiguration adaptability, both through providing powerful primitives for implementing the server model and through the interconnection language. Conic does not provide consistency guarantees during conversion, and does not support algorithmic adaptability within nodes. 2.4.2 Emerald Emerald [BHJ + 86] is a descendent of Eden <ref> [J + 82] </ref>. Both are distributed systems that support objects. The Emerald project was begun in response to the observed problem in systems such as Eden, Argus [LS83], and Clouds [DLS85] that the objects provided by the system were relatively expensive to use.
Reference: [KK86] <author> S. Kartashev and S. Kartashev. </author> <title> Guest editor's introduction: Design for adaptability. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 9-15, </pages> <month> February </month> <year> 1986. </year>
Reference-contexts: Research on the recovery block scheme [Ran75] and on N-version programming [Avi76] has been focussed on switchable software for fault-tolerance. Much effort is underway to build software that can exploit new-found hardware flexibility, including parallel processing capabilities, to increase performance <ref> [KK86] </ref>.
Reference: [KLB89] <author> Charles Koelbel, Fady Lamaa, and Bharat Bhargava. </author> <title> Efficient implementation of modularity in RAID. </title> <booktitle> In Proceedings of the USENIX Workshop on Experiences with Building Distributed (and Multiprocessor) Systems, </booktitle> <address> Ft. Lauderdale FL, </address> <month> October </month> <year> 1989. </year>
Reference-contexts: way to increase performance and reliability. 2.1 Other Work in Adaptability in the RAID Project Other Purdue researchers are investigating adaptability in the areas of operating systems [Maf89, BMR89], increasing availability with dynamic adaptability of replication control algorithms [Bro90, BB89a, Hel89], and merging multiple RAID servers into a single process <ref> [KLB89] </ref>. This section summarizes some of that work. The references contain further information. 2.1.1 PUSH: System for Adaptable Operating Systems Operating system services have to be constantly modified and extended in order to adjust the system to changing environments and applications. <p> the context of the RAID distributed database system is used to experimentally study these protocols as well as to validate the proposed availability model. 2.1.3 Merged Server Configurations The process structure of RAID is adaptable in the sense that RAID servers can be grouped into processes in many different ways <ref> [KLB89] </ref>. Server-based systems suffer from performance problems because communication between the separate address spaces becomes a bottleneck. In RAID, merged servers communicate through shared memory in an order of magnitude less time than servers in separate processes.
Reference: [KMN89] <author> J. Kramer, J. Magee, and K. Ng. </author> <title> Graphical support for configuration programming. </title> <booktitle> In Proceedings of the Hawaii International Conference on System Sciences, </booktitle> <pages> pages 860-870, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: In addition to the language in 7 which the processes are implemented, Conic provides a separate configuration language with which the connections between processes can be manipulated. There is even a graphical system that can change process connections dynamically in response to mouse actions on icons <ref> [KMN89] </ref>! These ideas of interface design for adaptability lead to object-based systems. Each component is implemented as an abstract data type with clearly specified interface. Implementations of the abstract data type are called objects, and respond to typed messages that invoke their operations.
Reference: [Koh81] <author> W. H. Kohler. </author> <title> A survey of techiques for synchronization and recovery in decentralized computer systems. </title> <journal> ACM Computing Surveys, </journal> <volume> 13(2) </volume> <pages> 149-183, </pages> <month> June </month> <year> 1981. </year>
Reference-contexts: Site failure algorithms are special cases of network partition control algorithms that usually allow higher availability. There are numerous choices of algorithms for concurrency control [BG81], network partition management [DGMS85], transaction commit/termination [SS83], database recovery <ref> [Koh81] </ref>, etc. It has been found that certain algorithms for each of the above subsystems cooperate well to reduce bookkeeping and to increase the efficiency of the implementation [Bha84].
Reference: [KR81] <author> H.T. Kung and J. Robinson. </author> <title> On optimistic methods for concurrency control. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 6(2) </volume> <pages> 213-226, </pages> <month> June </month> <year> 1981. </year>
Reference-contexts: This section applies the techniques of section 3.1 to permit adaptable concurrency control while preserving correctness. We use three classes of concurrency controllers to present our ideas: two-phase locking (2PL) [EGLT76], timestamp ordering (T/O) [Lam78], and optimistic concur-rency control (OPT) <ref> [KR81] </ref>. The version of 2PL that we are using implicitly acquires read locks when data items are read, implicitly acquires write locks during transaction commit, and releases all locks after commitment. <p> Then, we assign read-locks to the active transactions based on their readsets, and continue processing. There can be no lock conflicts, since the operations are all reads at this point. This conversion method works for converting from any validation technique to 2PL, although <ref> [KR81] </ref> validation must be used during conversion to abort transactions with backward edges. 1 By pure 2PL, we mean the 2PL algorithm used in this paper.
Reference: [Lam78] <author> Leslie Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-564, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: Although both concurrency controllers made locally correct decisions, the combination permitted a non-serializable history. This section applies the techniques of section 3.1 to permit adaptable concurrency control while preserving correctness. We use three classes of concurrency controllers to present our ideas: two-phase locking (2PL) [EGLT76], timestamp ordering (T/O) <ref> [Lam78] </ref>, and optimistic concur-rency control (OPT) [KR81]. The version of 2PL that we are using implicitly acquires read locks when data items are read, implicitly acquires write locks during transaction commit, and releases all locks after commitment.
Reference: [Lau82] <author> G. Lausen. </author> <title> Concurrency control in database systems: a step towards the integration of optimistic methods and locking. </title> <booktitle> In Proceedings of the ACM Annual Conference, </booktitle> <pages> pages 64-68, </pages> <month> October </month> <year> 1982. </year>
Reference-contexts: During the validation phase the transaction checks to see if it had any illegal concurrency conflicts with other transactions during the read phase. If not, the transaction enters the commit phase and makes the temporary copies permanent. Lausen <ref> [Lau82] </ref> describes the basic method, in which optimistic transactions are validated against currently running locking transactions in addition to being validated against other optimistic transactions.
Reference: [LCJS87] <author> Barbara Liskov, Dorothy Curtis, Paul Johnson, and Robert Scheifler. </author> <title> Implementation of Argus. </title> <booktitle> In Proc of the 11th ACM Symposium on Operating Systems Principles, </booktitle> <month> November </month> <year> 1987. </year>
Reference-contexts: The data within each object is protected so that it can only be accessed through the operations. This protection allows the object to preserve consistency properties of the data. Systems that support this programming methodology include Smalltalk [GR83], and more recently distributed systems such as Argus <ref> [LCJS87] </ref> and Camelot [S + 86]. RAID is currently being extended to include support for objects [DVB89]. O-RAID takes advantage of the interface specification by providing support for dynamically loading functions as they are required.
Reference: [LS83] <author> Barbara Liskov and R. Scheifler. </author> <title> Guardians and actions: Lingustic support for distributed programs. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(3) </volume> <pages> 381-404, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: Both are distributed systems that support objects. The Emerald project was begun in response to the observed problem in systems such as Eden, Argus <ref> [LS83] </ref>, and Clouds [DLS85] that the objects provided by the system were relatively expensive to use. These objects provided support for sharing, concurrency, and distribution, and hence were expensive to invoke.
Reference: [Maf89] <author> Enrique Mafla. </author> <title> Experimental studies in distributed transaction processing, </title> <month> April </month> <year> 1989. </year> <type> Preliminary PhD exam. 123 </type>
Reference-contexts: Adaptability is proving to be an effective way to increase performance and reliability. 2.1 Other Work in Adaptability in the RAID Project Other Purdue researchers are investigating adaptability in the areas of operating systems <ref> [Maf89, BMR89] </ref>, increasing availability with dynamic adaptability of replication control algorithms [Bro90, BB89a, Hel89], and merging multiple RAID servers into a single process [KLB89]. This section summarizes some of that work. <p> Complexity and efficiency are characteristic of kernel-resident code, while simplicity and poor performance are characteristic of user-level code. This section briefly describes a system called Push, that facilitates changing the functionality of the operating system kernel dynamically. Push is described in more detail in <ref> [Maf89, BMR89] </ref>. Other efforts in adaptable operating systems are described in section 2.4.3. Push 12 combines the flexibility and safety of user-level code with the efficiency and security of kernel-level code. Push can be used to implement semantically-rich system call interfaces that provide enhanced support for specific transaction processing systems.
Reference: [MKS89] <author> Jeff Magee, Jeff Kramer, and Morris Sloman. </author> <title> Constructing distributed systems in Conic. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(6), </volume> <month> June </month> <year> 1989. </year>
Reference-contexts: An interesting feature of these protocols is that only the data structures are converted; the same transaction processing algorithms are used after conversion. Thus, this adaptability is entirely data-driven. 2.4 Adaptable Systems 2.4.1 Conic The Conic distributed programming environment supports both static and dynamic reconfiguration <ref> [MKS89] </ref>. Conic provides a programming environment with the simplicity and safety of a language-based approach, while preserving the flexibility of distributed programming environments created from low-level communications primitives. Conic provides separate programming languages for module development ("programming in the small") and module interconnection ("programming in the large").
Reference: [MRA87] <author> Jeffrey C. Mogul, Richard F. Rashid, and Michael J. Accetta. </author> <title> The packet filter: An efficient mechanism for user-level network code. </title> <booktitle> In Proceedings of the 11th ACM Symposium on Operating Systems Principles, </booktitle> <month> November </month> <year> 1987. </year>
Reference-contexts: A stream is a two-way connection between a process and a device driver. Modules that process data flowing along this two-way path can be inserted and deleted dynamically, changing the behavior of the user interface. The packet filter presents another alternative to the efficiency/flexibility dilemma for network code implementation <ref> [MRA87] </ref>. The packet filter demultiplexes network 23 packets according to rules specified by the users. These rules can be quite complex and can be changed dynamically. By running inside the kernel, the packet filter eliminates much of the context switch overhead incurred by user-level demultiplexers.
Reference: [Pap79] <author> C. H. Papadimitriou. </author> <title> The serializability of concurrent database updates. </title> <journal> Journal of the ACM, </journal> <volume> 26(4) </volume> <pages> 631-653, </pages> <month> October </month> <year> 1979. </year>
Reference-contexts: MODEL FOR ADAPTABILITY 3.1 Sequencer Model In this section we describe the sequencer model for algorithmic adaptability, which applies in a natural way to many subsystems of a transaction system. The model is based on Papadimitriou's model for concurrency control <ref> [Pap79] </ref>, and presented in [BR89a]. A primary advantage of this model is that it provides for a clean interface between subsystems. The model is based on the idea of a sequencer, which orders atomic actions in a transaction processing history according to some correctness criterion. <p> When concurrency control methods are switched while the system is running, special care must be taken to ensure correctness. Figure 3.5 is an example of how locking depends on the structure of the past history. In this example a concurrency controller implementing the DSR policy <ref> [Pap79] </ref> had been running and it was removed from the 35 start with DSR change to locking W 2 [y] commit T2 commit T1 system and replaced by locking without appropriate preparation. <p> This section exhibits one such conversion termination condition for concurrency control, including a proof of correctness. The following conversion termination condition permits adaptation for all con-currency controllers that accept subsets of the digraph-serializable histories, or DSR <ref> [Pap79] </ref>. DSR includes all known practical concurrency controllers. Using this termination condition, suffix-sufficient state adaptability is possible between any two concurrency controllers in DSR. Let M be the suffix-sufficient state conversion method of Section 3.1.4, and let H = H A ffi H M ffi H B .
Reference: [PM83] <author> Michael L. Powell and Barton P. Miller. </author> <title> Process migration in DEMOS/MP. </title> <booktitle> In Proceedings of the 9th ACM Symposium on Operating Systems Principles, </booktitle> <year> 1983. </year>
Reference-contexts: system provides for adaptability by separating the function of a server from its interface, in the same way that specification and implementation of abstract data types are separate. 69 5.6 Server Relocation Another aspect of RAID adaptability is the ability to move servers to new locations, using process migration techniques <ref> [PM83] </ref>. Reliability is enhanced because servers or entire virtual sites can be moved from hosts before upcoming failures (e.g., periodic maintenance, partial failure that will eventually cause crash).
Reference: [PMI88] <author> Calton Pu, Henry Massalin, and John Ioannidis. </author> <title> The Synthesis kernel. </title> <journal> Computing Systems, </journal> <volume> 1(1) </volume> <pages> 11-32, </pages> <year> 1988. </year>
Reference-contexts: The Synthesis project employs a monolithic kernel and uses several techniques to specialize the kernel code that executes specific requests. These techniques include the elimination of redundant computation and the collapsing of kernel layers. <ref> [PMI88] </ref>. Synthesized code is reported to reduce the conventional execution path of some system calls by a factor of 10-20. Streams increase the modularity and reusability of kernel code in the input-output subsystem [Rit84]. Streams try to eliminate the duplication of functionality existing in conventional device drivers.
Reference: [PW85] <author> Gerald J. Popek and Bruce J. Walker. </author> <title> The LOCUS Distributed System Architecture. </title> <publisher> The MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: Structural dynamic adaptability, also called reconfiguration, is restructuring a running system in response to failures or performance requirements. Reconfiguration includes site failure protocols, network partitioning protocols, and other techniques for reorganizing the way the sites in a distributed system work together <ref> [PW85, chapter 5] </ref>. Algorithmic adaptability is a set of techniques for dynamically changing from the execution of one algorithm for a module to a different algorithm. <p> For instance, a 3 transaction system can change to a new concurrency controller, or a distributed system can change to a new site failure algorithm. Heterogeneity is the form of adaptability that deals with the issues involved in distributed computing using many different types of computing systems <ref> [PW85, chapter 6] </ref>. The simpler problems of heterogeneity include establishing physical connection between the machines, and resolving data type differences (e.g., size of integers, byte order). More difficult problems include getting diverse database systems to work together and developing software that takes advantage of the particular strengths of each machine. <p> Thus, this adaptability is entirely data-driven. Additional work at Purdue on the replication control problem is described in section 2.1.2. 5.3 Reconfiguration Another aspect of adaptability in distributed systems is the reconfiguration problem <ref> [PW85, chapter 5] </ref>. Reconfiguration is the process of adding or deleting sites from a distributed system without violating consistency. When a site leaves the system, either because of a failure or an administrative decision, its transactions must be terminated.
Reference: [Ran75] <author> B. Randell. </author> <title> System structure for software fault tolerance. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-1(2):220-232, </volume> <month> June </month> <year> 1975. </year>
Reference-contexts: Chapter 2 surveys related work on adaptability, and chapter 7 contains conclusions and suggestions for further work. 11 2. OTHER EFFORTS IN ADAPTABILITY Significant previous work has been done on adaptability. Research on the recovery block scheme <ref> [Ran75] </ref> and on N-version programming [Avi76] has been focussed on switchable software for fault-tolerance. Much effort is underway to build software that can exploit new-found hardware flexibility, including parallel processing capabilities, to increase performance [KK86].
Reference: [Ree83] <author> D.P. Reed. </author> <title> Implementing atomic actions on decentralized data. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 1(1) </volume> <pages> 3-23, </pages> <month> February </month> <year> 1983. </year>
Reference-contexts: For the common case of transactions with just a few actions, however, a simple unorganized list will be most efficient. maintained by version-based concurrency control methods <ref> [Ree83] </ref>, except that it 37 data item = record MaxReadTS, MaxWriteTS: timestamp; MaxReadTID, MaxWriteTID: timestamp; item: item id; read actions: action list; write actions: action list; end; maintains only timestamps and not values. Each data item has separate timestamped lists for read and write actions.
Reference: [Rit84] <author> D. M. Ritchie. </author> <title> A stream input-output system. </title> <journal> AT&T Bell Laboratories Technical Journal, </journal> <volume> 63(8) </volume> <pages> 1897-1910, </pages> <month> October </month> <year> 1984. </year>
Reference-contexts: These techniques include the elimination of redundant computation and the collapsing of kernel layers. [PMI88]. Synthesized code is reported to reduce the conventional execution path of some system calls by a factor of 10-20. Streams increase the modularity and reusability of kernel code in the input-output subsystem <ref> [Rit84] </ref>. Streams try to eliminate the duplication of functionality existing in conventional device drivers. A stream is a two-way connection between a process and a device driver. Modules that process data flowing along this two-way path can be inserted and deleted dynamically, changing the behavior of the user interface.
Reference: [S + 85] <author> M. Stonebraker et al. </author> <title> Problems in supporting data base transactions in an operating system transaction manager. Operating System Review, </title> <month> January </month> <year> 1985. </year>
Reference-contexts: Push can be used to implement semantically-rich system call interfaces that provide enhanced support for specific transaction processing systems. Database implementors have suggested that additional support in the underlying operating system is needed for efficiency <ref> [S + 85, Spe86] </ref>. Push provides a facility for experimenting with new or extended operating system services. Examples of these services include buffer management, file system support, process management, interpro-cess communication, concurrency control, atomicity control, and crash recovery. <p> Examples of these services include buffer management, file system support, process management, interpro-cess communication, concurrency control, atomicity control, and crash recovery. The services that are present in current operating systems are general-purpose and do not satisfy the demands of distributed transaction processing algorithms <ref> [S + 85, BM89] </ref>. For instance, locking facilities and buffer management are generally implemented by database systems because the services provided in operating systems are inadequate. The Push system consists of a Push machine, a Push assembler, and a set of Push utilities.
Reference: [S + 86] <author> Alfred Z. Spector et al. </author> <title> The Camelot project. </title> <journal> Database Engineering, </journal> <volume> 9(4), </volume> <month> December </month> <year> 1986. </year>
Reference-contexts: The data within each object is protected so that it can only be accessed through the operations. This protection allows the object to preserve consistency properties of the data. Systems that support this programming methodology include Smalltalk [GR83], and more recently distributed systems such as Argus [LCJS87] and Camelot <ref> [S + 86] </ref>. RAID is currently being extended to include support for objects [DVB89]. O-RAID takes advantage of the interface specification by providing support for dynamically loading functions as they are required. New implementations can be loaded while the system is running as long as they match the specification.
Reference: [Ske81] <author> D. Skeen. </author> <title> Nonblocking commit protocols. </title> <booktitle> In Proceedings of the 1981 ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 133-142, </pages> <address> Ann Arbor, Michigan, </address> <year> 1981. </year>
Reference-contexts: When a site leaves the system, either because of a failure or an administrative decision, its transactions must be terminated. The data can be brought up to date using a multi-phase commit protocol in such a way that the rest of the system can continue processing transactions <ref> [Ske81] </ref>. When the site rejoins the system its data must be brought up to date. Adaptable systems can dynamically incorporate new sites into the system, and provide graceful degradation as sites leave the system. <p> However, under 2PC some site failure conditions will cause active data items to be blocked from further accesses until the failed site recovers. Three-phase commit (3PC) does not block data items during site failures, but requires one extra round of messages for commitment <ref> [Ske81] </ref>.
Reference: [Spe86] <author> Alfred Z. Spector. </author> <title> Communication support in operating systems for distributed transactions. </title> <type> Technical Report CMU-CS-86-165, </type> <institution> Department of Computer Sciences, Carnegie Mellon University, </institution> <month> November </month> <year> 1986. </year> <month> 124 </month>
Reference-contexts: Push can be used to implement semantically-rich system call interfaces that provide enhanced support for specific transaction processing systems. Database implementors have suggested that additional support in the underlying operating system is needed for efficiency <ref> [S + 85, Spe86] </ref>. Push provides a facility for experimenting with new or extended operating system services. Examples of these services include buffer management, file system support, process management, interpro-cess communication, concurrency control, atomicity control, and crash recovery. <p> A timeout mechanism is used to detect site failures. For twenty destinations, we observe a 27% improvement over the user-level program and a 26% degradation from the kernel-level routine. Push is especially efficient when the user-level implementation of a service demands a heavy user-kernel interaction. Commitment Protocol In Camelot <ref> [Spe86] </ref>, the authors suggest that certain distributed transaction protocols can be added to the operating system to improve performance and to raise the level of the operating system interface. In database-oriented operating systems, commitment protocols can be added to the kernel.
Reference: [SS83] <author> D. Skeen and M. Stonebraker. </author> <title> A formal model of crash recovery in a distributed system. </title> <journal> IEEE Transaction on Software Engineering, </journal> <volume> SE-9(3):219-227, </volume> <month> May </month> <year> 1983. </year>
Reference-contexts: These techniques have not yet been implemented in RAID V2. 5.4 Distributed Commit Multiple-phase distributed commit algorithms successfully terminate transactions on all sites in a distributed system. Three-phase algorithms tolerate arbitrary site failures without causing blocking, at the cost of an extra round of messages <ref> [SS83] </ref>. RAID can use either two-phase or three-phase commit, choosing the method on a per-transaction basis. We are in the process of implementing the converting state adaptability method to convert between two- and three- phase commit algorithms while processing transactions. This section gives an overview of the method. <p> Site failure algorithms are special cases of network partition control algorithms that usually allow higher availability. There are numerous choices of algorithms for concurrency control [BG81], network partition management [DGMS85], transaction commit/termination <ref> [SS83] </ref>, database recovery [Koh81], etc. It has been found that certain algorithms for each of the above subsystems cooperate well to reduce bookkeeping and to increase the efficiency of the implementation [Bha84].
Reference: [Sto81] <author> M. Stonebraker. </author> <title> Operating system support for database management. </title> <journal> Communications of the ACM, </journal> <volume> 24(7) </volume> <pages> 412-418, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: Many operating systems provide functions that do not work well for distributed systems design in areas such as communication, buffer management, process management, and file input and output <ref> [Sto81] </ref>. As a result, distributed systems often duplicate operating system functions in these areas. One solution to these problems is to investigate ways in which operating systems can change their algorithms to accommodate a range of potential uses.

References-found: 76

