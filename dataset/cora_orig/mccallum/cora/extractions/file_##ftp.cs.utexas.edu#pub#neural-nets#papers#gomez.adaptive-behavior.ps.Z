URL: file://ftp.cs.utexas.edu/pub/neural-nets/papers/gomez.adaptive-behavior.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/nn/pages/publications/abstracts.html
Root-URL: 
Email: inaki@cs.utexas.edu  risto@cs.utexas.edu  
Title: Incremental Evolution of Complex General Behavior  
Author: Faustino Gomez Risto Miikkulainen 
Address: Austin, TX 78712  Austin, TX 78712  
Affiliation: Department of Computer Sciences The University of Texas at Austin  Department of Computer Sciences The University of Texas at Austin  
Date: 5:317-342, 1997  June 1, 1996  
Note: To appear in Adaptive Behavior,  
Abstract: Several researchers have demonstrated how complex action sequences can be learned through neuro-evolution (i.e. evolving neural networks with genetic algorithms). However, complex general behavior such as evading predators or avoiding obstacles, which is not tied to specific environments, turns out to be very difficult to evolve. Often the system discovers mechanical strategies (such as moving back and forth) that help the agent cope, but are not very effective, do not appear believable and would not generalize to new environments. The problem is that a general strategy is too difficult for the evolution system to discover directly. This paper proposes an approach where such complex general behavior is learned incrementally, by starting with simpler behavior and gradually making the task more challenging and general. The task transitions are implemented through successive stages of delta-coding (i.e. evolving modifications), which allows even converged populations to adapt to the new task. The method is tested in the stochastic, dynamic task of prey capture, and compared with direct evolution. The incremental approach evolves more effective and more general behavior, and should also scale up to harder tasks. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, C. W. </author> <year> (1989). </year> <title> Learning to control an inverted pendulum using neural networks. </title> <journal> IEEE Control Systems Magazine, </journal> <volume> 9 </volume> <pages> 31-37. </pages>
Reference: <author> Barto, A. G., Sutton, R. S., and Anderson, C. W. </author> <year> (1983). </year> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, SMC-13:834-846. </journal>
Reference: <author> Brooks, R. A. </author> <year> (1986). </year> <title> A robust layered control system for a mobile robot. </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> 2(10). </volume>
Reference-contexts: However, at least for Artificial Life and robot control, the task sequences are usually easy to come by, because the goal-task often subsumes natural layers of behavior <ref> (Brooks 1986) </ref>. For example, avoiding moving obstacles while following a wall could be evolved incrementally. The robot could first be evolved to move around avoiding stationary obstacles, then obstacles moving at slow speeds.
Reference: <author> Cliff, D., Harvey, I., and Husbands, P. </author> <year> (1992). </year> <title> Incremental evolution of neural network architectures for adaptive behavior. </title> <type> Technical Report CSRP256, </type> <institution> School of Cognitive and Computing Sciences, University of Sussex, </institution> <address> Brighton, UK. </address>
Reference: <author> Colombetti, M., and Dorigo, M. </author> <year> (1992a). </year> <title> Learning to control an autonomous robot by distributed genetic algorithms. </title> <editor> In Meyer, J. A., Roitblat, H. L., and Wilson, S. W., editors, </editor> <booktitle> From Animals to Animats 2, Proceedings of the 2nd International Conference on Simulation of Adaptive Behavior. </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Colombetti, M., and Dorigo, M. </author> <year> (1992b). </year> <title> Robot shaping: developing situated agents through learning. </title> <type> Technical Report TR-92-040, </type> <institution> International Computer Science Institute, Berkeley,CA. </institution>
Reference: <author> Elman, J. L. </author> <year> (1991). </year> <title> Incremental learning, or The importance of starting small. </title> <booktitle> In Proceedings of the 13th Annual Conference of the Cognitive Science Society, </booktitle> <pages> 443-448. </pages> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Law, D., and Miikkulainen, R. </author> <year> (1994). </year> <title> Grounding robotic control with genetic neural net-works. </title> <type> Technical Report AI93-223, </type> <institution> Department of Computer Sciences, The University of Texas at Austin. </institution>
Reference: <author> Lin, L.-J. </author> <year> (1992). </year> <title> Self-improving reactive agents based on reinforcement learning, planning, and teaching. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 293-321. </pages>
Reference: <author> Lin, L.-J. </author> <year> (1993). </year> <title> Hierarchical learning of robot skills by reinforcement. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks. IEEE. </booktitle>
Reference: <author> Miller, G., and Cliff, D. </author> <year> (1994). </year> <title> Co-evolution of pursuit and evasion i: Biological and gametheoretic foundations. </title> <type> Technical Report CSRP311, </type> <institution> School of Cognitive and Computing Sciences, University of Sussex, </institution> <address> Brighton, UK. </address>
Reference-contexts: Scenarios of this kind are a particularly interesting arena for the study of adaptive behavior because they are ubiquitous in natural ecosystems and offer a relatively simple objective that requires complex sensory-motor coordination with respect to both the environment and another moving entity <ref> (Miller and Cliff 1994) </ref>. To accomplish the Prey Capture task, an agent, moving through a simulated environment, must be able to apprehend another entity, the prey, within a fixed number of time-steps. The agent can detect the prey only within a limited distance.
Reference: <author> Moriarty, D. E. </author> <year> (1997). </year> <title> Symbiotic Evolution of Neural Networks in Sequential Decision Tasks. </title> <type> PhD thesis, </type> <institution> Department of Computer Sciences, The University of Texas at Austin, Austin, TX. </institution> <note> Technical Report UT-AI97-259. </note>
Reference-contexts: As a matter of fact, in the advanced stages of SANE evolution, instead of converging the population around a single individual like the standard GA approaches, the neuron population clusters into "species" or groups of individuals that perform specialized functions in the target behavior <ref> (Moriarty 1997) </ref>. 4.2 Enforced Sub-Populations (ESP) In Enforced Sub-Populations, as in SANE, the population consists of individual neurons instead of full networks, and a subset of neurons are put together to form a complete network. <p> It is difficult to attribute a particular behavior to any particular neuron. The coding of behavior seems to be distributed across the network. These results are in line with those of feedforward SANE networks for controlling a mobile robot <ref> (Moriarty 1997) </ref>, where elementary behaviors such as advancing and turning and stopping in front of obstacles were also found to be distributed across multiple units. Recurrency apparently makes the behaviors even more distributed.
Reference: <author> Moriarty, D. E., and Miikkulainen, R. </author> <year> (1996a). </year> <title> Efficient reinforcement learning through symbiotic evolution. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 11-32. </pages> <note> 20 Moriarty, </note> <author> D. E., and Miikkulainen, R. </author> <year> (1996b). </year> <title> Evolving obstacle avoidance behavior in a robot arm. </title> <editor> In Maes, P., Mataric, M., Meyer, J.-A., and Pollack, J., editors, </editor> <booktitle> From Animals to Animats: The Fourth International Conference on Simulation of Adaptive Behavior (SAB96). </booktitle>
Reference: <author> Nolfi, S., Elman, J. L., and Parisi, D. </author> <year> (1990). </year> <title> Learning and evolution in neural networks. </title> <type> Technical Report 9019, </type> <institution> Center for Research in Language, University of California, </institution> <address> San Diego. </address>
Reference: <author> Nolfi, S., and Parisi, D. </author> <year> (1994). </year> <title> Good teaching inputs do not correspond to desired responses in ecological neural networks. </title> <journal> Neural Processing Letters, </journal> <volume> 1(2) </volume> <pages> 1-4. </pages>
Reference: <author> Nolfi, S., and Parisi, D. </author> <year> (1995). </year> <title> Learning to adapt to changing environments in evolving neural networks. </title> <type> Technical Report 95-15, </type> <institution> Institute of Psychology, National Research Council, Rome, Italy. </institution>
Reference: <author> Pendrith, M. </author> <year> (1994). </year> <title> On reinforcement learning of control actions in noisy and nonMarkovian domains. </title> <type> Technical Report UNSW-CSE-TR-9410, </type> <institution> School of Computer Science and Engineering, The University of New South Wales, </institution> <address> Sydney, Australia. </address>
Reference: <author> Perkins, S., and Hayes, G. </author> <year> (1996). </year> <title> Robot shaping-principles, methods, and architectures. </title> <type> Technical Report No. 795, </type> <institution> Department of Artifical Intelligence, University of Edinburgh. </institution>
Reference: <author> Ring, M. B. </author> <year> (1994). </year> <title> Continual Learning in Reinforcement Environments. </title> <type> PhD thesis, </type> <institution> Department of Computer Sciences, The University of Texas at Austin, Austin, Texas 78712. </institution>
Reference: <author> Saravanan, N., and Fogel, D. B. </author> <year> (1995). </year> <title> Evolving neural control systems. </title> <journal> IEEE Expert, </journal> <pages> 23-27. </pages>
Reference: <author> Singh, S. </author> <year> (1992). </year> <title> Transfer of learning by composing solutions of elemental sequential tasks. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 323-339. </pages>
Reference: <author> Thrun, S. </author> <year> (1996). </year> <title> Explanation-Based Neural Network Learning: A Lifelong Learning Approach. </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Watkins, C. J. C. H., and Dayan, P. </author> <year> (1989). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292. </pages>
Reference: <author> Whitley, D., and Kauth, J. </author> <year> (1988). </year> <title> Genitor: A different genetic algorithm. </title> <booktitle> In Proceedings of the 1988 Rocky Mountain Conference on Artificial Intelligence. </booktitle> <institution> Computer Science Department, Colorado State University. </institution>
Reference: <author> Whitley, D., Mathias, K., and Fitzhorn, P. </author> <year> (1991). </year> <title> Delta-coding: An iterative search strategy for genetic algortihms. </title> <booktitle> In Proceedings of the Fourth International Conference on Genetic Algorithms. </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This is a problem, especially in incremental evolution, because a converged population cannot easily adapt to a new task. To accomplish task transfer despite convergence, ESP is combined with an iterative search technique known as Delta-Coding. 4.3 Delta-Coding The idea of Delta-Coding <ref> (Whitley et al. 1991) </ref> is to search for optimal modifications of the current best solution. In a conventional single-population GA, when the population of candidate solutions has converged, Delta-Coding is invoked by first saving the best solution and then initializing a population of new individuals called -chromosomes.
Reference: <author> Wieland, A. </author> <year> (1990). </year> <title> Evolving controls for unstable systems. </title> <editor> In Touretzky, D. S., Elman, J. L., and Sejnowski, T. J., editors, </editor> <booktitle> Proceedings of the 1990 Connectionist Models Summer School, </booktitle> <pages> 91-102. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. 21 Wieland, A. </publisher> <year> (1991). </year> <title> Evolving neural network controllers for unstable systems. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks (Seattle, WA), </booktitle> <volume> vol. II, </volume> <pages> 667-673. </pages> <address> Piscataway, NJ: </address> <publisher> IEEE. </publisher>
Reference: <author> Wilson, S. W. </author> <year> (1991). </year> <title> The animat path to AI. </title> <editor> In Meyer, J., and Wilson, S., editors, </editor> <booktitle> Proceedings of the First International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> 15-21. </pages> <publisher> MIT Press/Bradford Books. </publisher>
Reference: <author> Yamauchi, B., and Beer, R. </author> <year> (1994). </year> <title> Integrating reactive, sequential, and learning behavior using dynamical neural networks. </title> <editor> In Cliff, D., Husbands, P., Meyer, J. A., and Wilson, S., editors, </editor> <booktitle> From Animals to Animats 3, Proceedings of the 3rd International Conference on Simulation of Adaptive Behavior. </booktitle> <publisher> MIT Press/Bradford Books. </publisher>
References-found: 28

