URL: http://www.cs.concordia.ca/~faculty/gregb/home/PS/kharma-mcs-thesis.ps.gz
Refering-URL: http://www.cs.concordia.ca/~faculty/gregb/home/paper.html
Root-URL: http://www.cs.concordia.ca
Title: REENGINEERING UNIFICATION AND T-ENTAILMENT FOR MANTRA IN C++  
Author: Tania Kharma 
Degree: A thesis in The Department of Computer Science Presented in Partial Fulfillment of the Requirements For the Degree of Master of Computer Science  
Note: c Tania Kharma, 1996  
Date: May 1996  
Address: Montr eal, Qu ebec, Canada  
Affiliation: Concordia University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> R.P. Abelson and R.C. Schank. </author> <title> Scripts, Plans, Goals and Understanding. </title> <publisher> Lawrence Erlbaum Assoc., </publisher> <year> 1977. </year>
Reference-contexts: representation, and the struc ture contains holes to be filled with the correct values. 41 The disadvantages of using Conceptual Dependency are that: a low-level de-composition is needed, and there is a need to represent more than just events. * Scripts were first proposed by Schank and Abelson in 1977 <ref> [1] </ref>. They are used to represent a common sequence of events. For example, what happens when one goes to a restaurant. A script is a description of a class of events in terms of contexts, participants, and sub-events. It consists of a set of slots.
Reference: [2] <author> R. Ackermann. </author> <title> Introduction to Many Valued Logics. </title> <publisher> Dover Publications Inc., </publisher> <year> 1967. </year>
Reference-contexts: So we need our facts to be mapped not only over two values (true and false) but over many values <ref> [2] </ref>. Historically, the first many-valued system (propositional logic) was constructed by Lukasiewicz in 1920 [43] in which he introduced the notion of three-valued logic : true, false, and neutral (an intermediary value).
Reference: [3] <author> M. Arbib, A.J. Kfoury, and R. Moll. </author> <title> A Programming Approach to Computability. </title> <publisher> Springer Verlag, </publisher> <year> 1982. </year>
Reference-contexts: Both the input and output data must have finite descriptions and the algorithm should run for a finite amount of time. A function is computable if there is an algorithm for computing it <ref> [3] </ref>. A problem is decidable if there is an algorithm which can always give the answer to the problem. There is no algorithm that is always able to determine whether an arbitrary formula 30 or sentence of first order logic is valid. That is, first order logic is not decidable.
Reference: [4] <author> N.D Belnap. </author> <title> How a computer should think. </title> <editor> In G. Ryle, editor, </editor> <booktitle> Contemporary Aspects of Philosophy Proceedings of the Oxford International Symposium, </booktitle> <pages> pages 30-56. </pages> <publisher> Oriel Press, </publisher> <year> 1975. </year>
Reference-contexts: After that, many-valued logic started to gain interest in many areas: logic design, switching theory, programming languages, pattern recognition, artificial intelligence and many others. Our concern here is the decidable four-valued logic introduced by Belnap <ref> [4] </ref> and extended to a version suitable for knowledge representation by Patel-Schneider [30, 31]. 2.1.4.1 Why Four-Valued Logic ? There are several reasons behind the need to use four-valued logic: Firstly, there is a trade-off between expressive power and computational tractability in knowledge representation formalisms [24]. <p> Secondly, we should be able to consider cases where something is both true and false (contradiction) and deal with it without polluting the remaining knowledge <ref> [4] </ref>. <p> The four truth values form the set of subsets of ftrue, falseg: ftrueg, ffalseg, ftrue, falseg for both, and fg for none. Table 6 through Table 8 summarise the values taken by compound sentences with the : negation, ^ conjunction, and _ disjunction connectives <ref> [4] </ref>. <p> And the failure of the second means that we cannot conclude that we know something about B just by knowing that A is true. Their absence is welcome because we guarantee that the presence of a contradiction will not contaminate the system <ref> [4] </ref>. * It is rooted in reality where we encounter contradictory facts (both) or we ignore the truth or falsity of some others (none) [4]. 2.2 Knowledge Representation In order to use knowledge in a machine, we must first choose a way of representing it, and a way of manipulating it <p> Their absence is welcome because we guarantee that the presence of a contradiction will not contaminate the system <ref> [4] </ref>. * It is rooted in reality where we encounter contradictory facts (both) or we ignore the truth or falsity of some others (none) [4]. 2.2 Knowledge Representation In order to use knowledge in a machine, we must first choose a way of representing it, and a way of manipulating it in order to create solutions to problems related to this knowledge.
Reference: [5] <author> Wolfgang Bibel. </author> <title> Automated Theorem Proving. </title> <publisher> Vieweg & Sohn, </publisher> <year> 1987. </year>
Reference-contexts: These transformations will always terminate with an answer that either the formula is a tautology or it is not. 2.1.1.5 Summary of Propositional logic theorem proving The following Table 3 is a summary of the theorem provers just described. The information is based on readings from <ref> [17, 19, 16, 5, 12] </ref> 21 Semantic Resolution Hilbert Natural Gentzen Davis Tableau System System Deduction Sequents Putnam Sound yes yes yes yes yes yes Complete Suitable for very good very good very bad very bad very good very good Automation Transformation yes yes no no no yes Required Understanding fair <p> Gentzen Sequents are sound and complete. 2.1.2.5 Summary of Theorem Proving in Predicate Logic The following Table 5 is a summary of the theorem provers just described. The information is based on readings from <ref> [17, 19, 16, 5, 12] </ref> 2.1.3 Computability and Decidability Computability deals with functions that could or could not be computed by algorithms. An algorithm is a well defined set of instructions that work on some input data to produce useful output.
Reference: [6] <author> G. Bittencourt. </author> <title> A hybrid system architecture and its unified semantics. </title> <booktitle> In Proceedings of the 4th International Symposium on Methodologies for Intelligent Systems, </booktitle> <pages> pages 150-158, </pages> <address> Charlotte North Carolina USA, </address> <month> October 12-14 </month> <year> 1989. </year>
Reference-contexts: We read 9xP x "there exists a known individual for which P is true " <ref> [6] </ref>. More generally, S supports the truth of 9xff under the variable map v if there is some domain element (2 D), common across all the situations in S, which when taken as the mapping of x, is such that each situation in S supports the truth of ff. <p> It supports three different knowledge representation formalisms: logic, frames, and semantic nets. Knowledge is represented in one of the formalisms and then inferred from one or a combination of the formalisms. Mantra was developed in Karlsruhe, Germany, in 1991 by G. Bittencourt <ref> [6, 8, 7] </ref> and implemented in Common Lisp. The motivations behind its development were: * To provide a combination of knowledge representation formalisms since each formalism is usually suitable for a particular kind of knowledge.
Reference: [7] <author> G. Bittencourt. </author> <title> The integration of terminological and logical knowledge representation languages. </title> <booktitle> In Proceedings of the 5th International Symposium on Methodologies for Intelligent Systems, </booktitle> <pages> pages 226-234, </pages> <address> Knoxville Tennessee, </address> <month> Oc-tober 25-27 </month> <year> 1990. </year>
Reference-contexts: It supports three different knowledge representation formalisms: logic, frames, and semantic nets. Knowledge is represented in one of the formalisms and then inferred from one or a combination of the formalisms. Mantra was developed in Karlsruhe, Germany, in 1991 by G. Bittencourt <ref> [6, 8, 7] </ref> and implemented in Common Lisp. The motivations behind its development were: * To provide a combination of knowledge representation formalisms since each formalism is usually suitable for a particular kind of knowledge.
Reference: [8] <author> G. Bittencourt. </author> <title> The Mantra Reference Manual. </title> <address> Universitat Karlsruhe, Germany, </address> <month> 12 January </month> <year> 1990. </year>
Reference-contexts: To date no single system optimises all four properties. The main reason behind this is the "trade-off between expressive power and computational tractability" [24]. Mantra <ref> [8, 9] </ref>, a shell for knowledge representation, offers three different formalisms: 1 logic, semantic nets, and frames. The user can represent his knowledge using any of the formalisms. <p> It supports three different knowledge representation formalisms: logic, frames, and semantic nets. Knowledge is represented in one of the formalisms and then inferred from one or a combination of the formalisms. Mantra was developed in Karlsruhe, Germany, in 1991 by G. Bittencourt <ref> [6, 8, 7] </ref> and implemented in Common Lisp. The motivations behind its development were: * To provide a combination of knowledge representation formalisms since each formalism is usually suitable for a particular kind of knowledge. <p> This mechanism is specific to the logic formalism, it provides a decidable way to answer any given question based on the facts stored in the logic formalism of Mantra. We are confident about the correctness of this mechanism, we have tested it with all the examples in Mantra manual <ref> [8] </ref>, some examples from artificial intelligence books [28, 34], and some tests prepared by the author and her supervisor. 117 The importance of this work lies in some major aspects: The selection of the algo-rithms and the use the Object Oriented paradigm in the design and implementation phases.
Reference: [9] <author> G. Bittencourt, J. Calmet, and I.A. Tjandra. Mantra: </author> <title> A shell for hybrid knowledge representation. </title> <booktitle> In Tools for Artificial Intelligence. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year> <month> 120 </month>
Reference-contexts: To date no single system optimises all four properties. The main reason behind this is the "trade-off between expressive power and computational tractability" [24]. Mantra <ref> [8, 9] </ref>, a shell for knowledge representation, offers three different formalisms: 1 logic, semantic nets, and frames. The user can represent his knowledge using any of the formalisms.
Reference: [10] <author> M. Blaha, F. Eddy, W. Lorensen, W. Premerlani, and J. Rumbaugh. </author> <title> Object Oriented Modeling and Design. </title> <publisher> Prentice Hall, </publisher> <year> 1991. </year>
Reference-contexts: Put simply, design patterns help a designer get a design "right" fast [33, page 2]. In this thesis, design patterns are used to solve the design problems encountered. In the design description, the Object Modeling Technique (OMT) notations are used <ref> [10] </ref>. The OMT methodology, is developed by J. Rumbaugh and his colleagues, it supports the same notation during the three different phases of the software life cycle. And the notations and terminologies it uses are not dependent on any programming language.
Reference: [11] <author> Michael A. Carrico, John E. Girard, and Jennifer P. Jones. </author> <title> Building Knowledge Systems. </title> <publisher> Mc Graw Hill Book Company, </publisher> <year> 1989. </year>
Reference-contexts: However, this flexibility tends to become a disadvantage because semantic nets lack a formal structuring, so complex ones become messy and hard to understand. To overcome this problem, some techniques were developed such 40 as the Warnier/Orr approach, and normalisation <ref> [11] </ref>. * Frames are used to represent complex objects from different points of view. They describe an object by placing all the information related to that object in slots. Each slot either describes an aspect of the object, is another frame, or has a default value.
Reference: [12] <author> Donald N. Cohen. </author> <title> Knowledge Based Theorem Proving and Learning. </title> <publisher> UMI Research Press, </publisher> <year> 1981. </year>
Reference-contexts: These transformations will always terminate with an answer that either the formula is a tautology or it is not. 2.1.1.5 Summary of Propositional logic theorem proving The following Table 3 is a summary of the theorem provers just described. The information is based on readings from <ref> [17, 19, 16, 5, 12] </ref> 21 Semantic Resolution Hilbert Natural Gentzen Davis Tableau System System Deduction Sequents Putnam Sound yes yes yes yes yes yes Complete Suitable for very good very good very bad very bad very good very good Automation Transformation yes yes no no no yes Required Understanding fair <p> Gentzen Sequents are sound and complete. 2.1.2.5 Summary of Theorem Proving in Predicate Logic The following Table 5 is a summary of the theorem provers just described. The information is based on readings from <ref> [17, 19, 16, 5, 12] </ref> 2.1.3 Computability and Decidability Computability deals with functions that could or could not be computed by algorithms. An algorithm is a well defined set of instructions that work on some input data to produce useful output.
Reference: [13] <author> M. Davis. </author> <title> Eliminating the irrelevant from mechanical proofs. </title> <booktitle> In Proceedings of Symposia in Applied Mathematics, </booktitle> <volume> volume 15. </volume> <publisher> American Math. Society, </publisher> <year> 1963. </year>
Reference-contexts: In his algorithm, Herbrand described three properties related to the validity of a formula. In 1960, D. Prawitz [32] worked on a logic which did not contain any function symbols, and for which he computed a most general representative of all possible instantiations. In 1963, M. Davis <ref> [13] </ref> published a proof procedure which was implemented on an IBM 7090 at Bell Telephone Laboratories in November 1962, and which used a unification algorithm. It was the first fully implemented unification algorithm.
Reference: [14] <author> M. Davis. </author> <title> Hilbert's tenth problem is unsolvable. </title> <journal> American Mathematical Monthly, </journal> <volume> 80, </volume> <year> 1973. </year>
Reference-contexts: Unification is the process of finding the most general unifier (mgu). The earliest reference for unification of general terms goes back to 1920 when E. Post mentioned in his diary and notes, partially published in <ref> [14] </ref>, a hint of the concept of a unification algorithm that computes a most general representation as opposite to all possible instantiations. The first published unification algorithm was given in J. Herbrand's thesis in 1930 [23]. In his algorithm, Herbrand described three properties related to the validity of a formula.
Reference: [15] <author> M. Davis and H. Putnam. </author> <title> A computing procedure for quantification theory. </title> <journal> Journal of the ACM, </journal> <volume> 7 </volume> <pages> 201-215, </pages> <year> 1960. </year>
Reference-contexts: proof systems designed mainly for the purpose of efficient proof development have a great relationship with sequent calculi. * Tableaux systems, which are very good for automation, correspond to the back ward application of rules of sequent calculus. 2.1.1.4.6 Davis Putnam Procedure The Davis Putnam procedure was introduced in 1960 <ref> [15] </ref>. It is a refutation method. In order to apply the Davis Putnam procedure on a formula, first it must be converted into its CNF. A block is a disjunction of formulae in CNF. The second step involves transforming the blocks into new blocks by applying rules.
Reference: [16] <author> David Duffy. </author> <title> Principles of Automated Theorem Proving. </title> <publisher> John Wiley & Sons, </publisher> <year> 1991. </year>
Reference-contexts: This development started with Frege in 1879, and later in 1928 led to the Hilbert type calculi in which valid formulae are derived from a sequence of basic logical formulae using few forms of inferences <ref> [16] </ref>. Unlike tableaux and resolution mechanisms which start with the formula and try to work with it in order to prove it, Hilbert systems start with known tautologies, derive consequences, derive consequences from consequences until the formula we want to prove is reached. <p> These transformations will always terminate with an answer that either the formula is a tautology or it is not. 2.1.1.5 Summary of Propositional logic theorem proving The following Table 3 is a summary of the theorem provers just described. The information is based on readings from <ref> [17, 19, 16, 5, 12] </ref> 21 Semantic Resolution Hilbert Natural Gentzen Davis Tableau System System Deduction Sequents Putnam Sound yes yes yes yes yes yes Complete Suitable for very good very good very bad very bad very good very good Automation Transformation yes yes no no no yes Required Understanding fair <p> Gentzen Sequents are sound and complete. 2.1.2.5 Summary of Theorem Proving in Predicate Logic The following Table 5 is a summary of the theorem provers just described. The information is based on readings from <ref> [17, 19, 16, 5, 12] </ref> 2.1.3 Computability and Decidability Computability deals with functions that could or could not be computed by algorithms. An algorithm is a well defined set of instructions that work on some input data to produce useful output. <p> If a formulae is a theorem we can show that it is valid in a finite time. However, if it is not a theorem, the procedure may loop for ever. Thus, first order logic is semi-decidable <ref> [16, pages 34-35] </ref>. <p> It was designed using the iterator pattern, and a combination of the composite and visitor patterns. Its implementation is general to accept any two terms to be unified. Testing was done using examples from Martelli and Montanari paper [25], some logic books <ref> [16, 17, 19, 37] </ref> and inputs prepared by the author and her supervisor. The final step was the building of the inference mechanism based on t-entailment, which used the CNF transformation and the unification module.
Reference: [17] <author> Melvin Fitting. </author> <title> First Order Logic and Automated Theorem Proving. </title> <publisher> Springer Verlag, </publisher> <year> 1990. </year>
Reference-contexts: Our interest is theorem proving and knowledge representation using logic. In this section, the classical propositional and predicate logic notations <ref> [17, 37] </ref> are studied and a wide variety of their theorem proving formalisms are discussed namely: tableaux and resolution mechanisms, Hilbert systems, natural deduction, the sequent calculus, and the Davis-Putman procedure. <p> The main interest is to show how the truth 4 value of an atomic sentence extends to the truth of more complex sentences. The most important features studied in propositional logic are given below and the notations used are based on <ref> [17] </ref> and [37]. 2.1.1.1 Syntax * Propositional letters express propositions: P ,Q,... * Formulae are propositional letters connected using propositional connectives. * Propositional connectives: _; ^; ae; oe; j; 6oe; 6ae; 6j; "; #. * Constants: ? false , &gt; true. &gt;, t, and true are used interchangeably. <p> Uniform Notation: In his book <ref> [17] </ref>, M. <p> Reusing formulae over and over in tableaux proofs is a controversial issue: non-classical logicians claim we should, while classical ones claim we should not. The idea behind this is that reusing formulae makes completeness easy to prove by a general method but implementation becomes difficult <ref> [17, page 39] </ref>. 10 2.1.1.4.2 Resolution Method The resolution principle was originally proposed by J.A. Robinson in the early 1960s [36]. The motivation for developing this principle was to improve the efficiency of earlier proof methods. <p> This involved showing that proofs are performed in a direct way, starting with axiom sequents and successively introducing connectives until we get the theorem we want to prove <ref> [17] </ref>. Later, tableaux systems used a backward application of the rules of sequent calculus. Some notations and definitions used in sequent calculus: * The connectives used are _, ^, and oe. * A sequent is a pair &lt; , &gt; of finite sets of formulae. <p> These transformations will always terminate with an answer that either the formula is a tautology or it is not. 2.1.1.5 Summary of Propositional logic theorem proving The following Table 3 is a summary of the theorem provers just described. The information is based on readings from <ref> [17, 19, 16, 5, 12] </ref> 21 Semantic Resolution Hilbert Natural Gentzen Davis Tableau System System Deduction Sequents Putnam Sound yes yes yes yes yes yes Complete Suitable for very good very good very bad very bad very good very good Automation Transformation yes yes no no no yes Required Understanding fair <p> For instance, how to express that a property is true for certain objects or for all objects. The most important features studied in predicate logic are given below and the notations used are also based on <ref> [17, 37] </ref>. 2.1.2.1 Syntax * The quantifiers are: existential quantifier (there exists) 9 and universal quanti fier (for all) 8. * A first order language L (R,F,C) is determined by: A finite set R of relation symbols where each relation has a positive integer which determines its arity. <p> Gentzen Sequents are sound and complete. 2.1.2.5 Summary of Theorem Proving in Predicate Logic The following Table 5 is a summary of the theorem provers just described. The information is based on readings from <ref> [17, 19, 16, 5, 12] </ref> 2.1.3 Computability and Decidability Computability deals with functions that could or could not be computed by algorithms. An algorithm is a well defined set of instructions that work on some input data to produce useful output. <p> It was designed using the iterator pattern, and a combination of the composite and visitor patterns. Its implementation is general to accept any two terms to be unified. Testing was done using examples from Martelli and Montanari paper [25], some logic books <ref> [16, 17, 19, 37] </ref> and inputs prepared by the author and her supervisor. The final step was the building of the inference mechanism based on t-entailment, which used the CNF transformation and the unification module.
Reference: [18] <editor> D.M Gabbay, C.J. Hogger, J.A. Robinson, and J. Siekmann. </editor> <booktitle> Handbook of Logic in Artificial Intelligence and Logic Programming, </booktitle> <volume> volume 2. </volume> <publisher> Clarendon Press, </publisher> <year> 1994. </year>
Reference-contexts: This algorithm requires time exponential in the size of the terms. Robinson introduced unification to implement resolution. Since then, it became universal in automated theorem proving. Due to its pattern matching nature, unification is applicable in a wide variety of areas in computer science <ref> [18, 38, 39] </ref> such as deductive databases, information retrieval, type checking, natural language processing, and logic programming. Unification is at the heart of these applications. Thus, its 62 performance is crucial to the overall efficiency. Some algorithms having almost linear time complexity were published for example [19]: M.S. <p> The representation it uses is very abstract, it does not refer to any specific control, or any data structure for representing terms and systems of equations. This makes the idea very transparent, and it gives freedom of choice to any person who wants to implement the algorithm <ref> [18] </ref>. 2. Their approach is flexible. This is clearly demonstrated by the fact that this algorithm became a standard in the field and its idea was used to describe other unification algorithms [18]. 3. It allows terms of any depth. <p> idea very transparent, and it gives freedom of choice to any person who wants to implement the algorithm <ref> [18] </ref>. 2. Their approach is flexible. This is clearly demonstrated by the fact that this algorithm became a standard in the field and its idea was used to describe other unification algorithms [18]. 3. It allows terms of any depth. This leads to a more efficient performance because the computation of the common part and the frontier is the same for any term. Martelli and Montanari's algorithm has an almost linear time complexity.
Reference: [19] <author> Jean H. Gallier. </author> <title> Logic for Computer Science: Foundations of Automatic Theorem Proving. </title> <publisher> Harper & Row Publishers, </publisher> <year> 1986. </year>
Reference-contexts: These transformations will always terminate with an answer that either the formula is a tautology or it is not. 2.1.1.5 Summary of Propositional logic theorem proving The following Table 3 is a summary of the theorem provers just described. The information is based on readings from <ref> [17, 19, 16, 5, 12] </ref> 21 Semantic Resolution Hilbert Natural Gentzen Davis Tableau System System Deduction Sequents Putnam Sound yes yes yes yes yes yes Complete Suitable for very good very good very bad very bad very good very good Automation Transformation yes yes no no no yes Required Understanding fair <p> Gentzen Sequents are sound and complete. 2.1.2.5 Summary of Theorem Proving in Predicate Logic The following Table 5 is a summary of the theorem provers just described. The information is based on readings from <ref> [17, 19, 16, 5, 12] </ref> 2.1.3 Computability and Decidability Computability deals with functions that could or could not be computed by algorithms. An algorithm is a well defined set of instructions that work on some input data to produce useful output. <p> Unification is at the heart of these applications. Thus, its 62 performance is crucial to the overall efficiency. Some algorithms having almost linear time complexity were published for example <ref> [19] </ref>: M.S. Paterson and M.N. Wegman in 1978, Kapur, Krishnamoorthy, and Narendran in 1982, G. Huet in 1976, L.D. Bax-ter in 1973, Martelli and Montanari in 1982. <p> It was designed using the iterator pattern, and a combination of the composite and visitor patterns. Its implementation is general to accept any two terms to be unified. Testing was done using examples from Martelli and Montanari paper [25], some logic books <ref> [16, 17, 19, 37] </ref> and inputs prepared by the author and her supervisor. The final step was the building of the inference mechanism based on t-entailment, which used the CNF transformation and the unification module.
Reference: [20] <author> E. Gamma, R. Helm, R. Johnson, and J. Vlissides. </author> <title> Design Patterns: Elements of Reusable Object-Oriented Software. </title> <publisher> Addison Wesley Publishing Company, </publisher> <year> 1995. </year>
Reference-contexts: Christopher Alexander says: "Each pattern describes a problem which occurs over and over again in our environment, and then describes the core of the solution to that problem, in such a way that you can use this solution a million times over, without ever doing it the same way twice" <ref> [20, page 2] </ref>. Design patterns make it easier to reuse successful designs, to choose design alternatives, and to improve the documentation and maintenance. Put simply, design patterns help a designer get a design "right" fast [33, page 2]. <p> The iterator pattern <ref> [20, pages 257-273] </ref> is used to access the elements of the list sequentially, regardless of the internal representation of the list. This pattern uses the list class and the list iterator class. The list iterator defines an interface to access and traverse elements of the list. <p> The template method <ref> [20, pages 325-331] </ref> is another pattern, used in the relation between the transformer and its two sub-classes. It allows us to define the skeleton of the CNF transformation in the transformer class and defer some sub-steps to the 52 two sub-classes: universal transformer and existential transformer. <p> The unification algorithm uses a list container to handle intermediate computations and to manipulate the other classes. The list is traversed by list iterator. Besides the ubiquitous iterator pattern, unification uses double dispatching. This is a combination of the composite pattern <ref> [20, pages 163-175] </ref>, which allows us to treat individual objects (variable, constant) and compositions of objects (composite term) in the same manner, and the visitor pattern [20, pages 331-345], which let us add operations to classes without changing them. <p> Besides the ubiquitous iterator pattern, unification uses double dispatching. This is a combination of the composite pattern [20, pages 163-175], which allows us to treat individual objects (variable, constant) and compositions of objects (composite term) in the same manner, and the visitor pattern <ref> [20, pages 331-345] </ref>, which let us add operations to classes without changing them. Double dispatching means that the operation that is executed depends on the request (operation) and the type of each of two arguments.
Reference: [21] <author> G. </author> <title> Gentzen. Investigation into logical deduction (english translation). In M.E. Szabo, editor, Collected Papers of Gerhrad Gentzen. </title> <publisher> North-Holland Publishing Co., </publisher> <year> 1969. </year>
Reference-contexts: But the problem is that in practice proofs tend to be long, tedious and difficult to perform. 2.1.1.4.4 Natural Deduction During 1934-35, Gerhard Gentzen <ref> [21] </ref> developed a system of natural deduction intended to allow proofs to be performed in a way which corresponds to human reasoning. The major principle of natural deduction is that there should be separate rules that allow the introduction and removal of each logical symbol.
Reference: [22] <author> Frank Van Harmelen, Peter Jackson, and Han Reichgelt. </author> <title> Logic-Based Knowledge Representation. </title> <publisher> The MIT Press, </publisher> <year> 1989. </year> <month> 121 </month>
Reference-contexts: This is the reason why we use specific knowledge representation models which have more powerful inference mechanisms to manipulate them. 2.2.1 Logic One way of representing facts is to use the language of logic. This representation has many advantages <ref> [22] </ref>: First, logic has a formal semantics, which gives a precise meaning for each expression. Second, logic has well defined properties for which it is possible to prove their soundness, completeness, and decidability. For powerful logics, proof theories are sound, complete, and semi-decidable.
Reference: [23] <author> J. </author> <title> Herbrand. Recherches sur la Theorie de la Demonstration. </title> <type> PhD thesis, </type> <institution> Sor--bonne Paris, </institution> <year> 1930. </year>
Reference-contexts: Post mentioned in his diary and notes, partially published in [14], a hint of the concept of a unification algorithm that computes a most general representation as opposite to all possible instantiations. The first published unification algorithm was given in J. Herbrand's thesis in 1930 <ref> [23] </ref>. In his algorithm, Herbrand described three properties related to the validity of a formula. In 1960, D. Prawitz [32] worked on a logic which did not contain any function symbols, and for which he computed a most general representative of all possible instantiations. In 1963, M.
Reference: [24] <author> H. Levesque and R. Brachman. </author> <title> Readings in Knowledge Representation. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1985. </year>
Reference-contexts: To date no single system optimises all four properties. The main reason behind this is the "trade-off between expressive power and computational tractability" <ref> [24] </ref>. Mantra [8, 9], a shell for knowledge representation, offers three different formalisms: 1 logic, semantic nets, and frames. The user can represent his knowledge using any of the formalisms. <p> four-valued logic introduced by Belnap [4] and extended to a version suitable for knowledge representation by Patel-Schneider [30, 31]. 2.1.4.1 Why Four-Valued Logic ? There are several reasons behind the need to use four-valued logic: Firstly, there is a trade-off between expressive power and computational tractability in knowledge representation formalisms <ref> [24] </ref>. When the formalism is very expressive like first order logic, then reasoning becomes time consuming and, in the case of first order logic, semi-decidable. Being semi-decidable, first order logic is not suitable for knowledge representation systems. <p> For instance, in predicate logic disjunction could be used to express that either this pen is blue or this pen is red, which is an incomplete knowledge about the colour of the pen. As Levesque and Brachman <ref> [24] </ref> say about the 38 expressive power of logic:"determines not so much what can be said, but what can be left unsaid". 2.2.2 Structured Representations of Knowledge In logic, complex structures and relationships cannot be easily represented so we have other systems for representing complex structured knowledge.
Reference: [25] <author> Alberto Martelli and Ugo Montanari. </author> <title> An efficient unification algorithm. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 4(2) </volume> <pages> 258-282, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: This is the reason why it has been introduced and studied by a number of researchers. In this thesis, we are concerned both in the design and implementation of an efficient unification module for Mantra based on the algorithm described by Martelli and Montanari <ref> [25] </ref>. Since 1980s, the object oriented approach has been getting more popular, because of its many advantages, such as reusability of software components leading to productivity and rapid system development, maintainability where an object can be replaced with a new implementation without affecting the other objects, and modularity. <p> Huet in 1976, L.D. Bax-ter in 1973, Martelli and Montanari in 1982. The Martelli and Montanari unification algorithm and its implementation are described in detail below. 4.1 Martelli and Montanari Unification Algorithm The algorithm of A. Martelli and U. Montanari <ref> [25] </ref> is based on a technique that was first described by Herbrand. It operates on a system of term equations, and it transforms this system until a solved form is reached. In their description of the unification solution, Martelli and Montanari start with a nondeterministic algorithm. <p> It was designed using the iterator pattern, and a combination of the composite and visitor patterns. Its implementation is general to accept any two terms to be unified. Testing was done using examples from Martelli and Montanari paper <ref> [25] </ref>, some logic books [16, 17, 19, 37] and inputs prepared by the author and her supervisor. The final step was the building of the inference mechanism based on t-entailment, which used the CNF transformation and the unification module. <p> The unification algorithm selected, based on Martelli and Montanari work, has been shown to have a better performance than many well-known algorithms <ref> [25, page 44] </ref>. It has a linear-time complexity [25, page 44]. The inference mechanism based on t-entailment is sound, complete and decidable. <p> The unification algorithm selected, based on Martelli and Montanari work, has been shown to have a better performance than many well-known algorithms <ref> [25, page 44] </ref>. It has a linear-time complexity [25, page 44]. The inference mechanism based on t-entailment is sound, complete and decidable. Its performance in the circumstances we are working in is typically in the order of ln k fl k ln ln k , where k is the size of the knowledge base.
Reference: [26] <author> Marvin Minsky. </author> <title> A framework for representing knowledge. </title> <editor> In Patrick H. Winston, editor, </editor> <booktitle> The Psychology of Computer Vision. </booktitle> <publisher> Mc Graw Hill Book Company, </publisher> <year> 1975. </year>
Reference-contexts: Some of the declarative representations are briefly described below: * Semantic Nets are used to describe both events and objects. They are one of the first knowledge representation structures developed, they were proposed by Minsky in 1975 in <ref> [26] </ref>. They were originally designed to represent the meanings of English words. The two components of a semantic net are the nodes and the labelled arcs. Information, an object, an event or a concept, is represented by the nodes and relationships are represented by the labelled arcs between nodes.
Reference: [27] <author> R. Moore. </author> <title> The role of logic in knowledge representation and common-sense reasoning. </title> <booktitle> In Proceedings of American Association for Artificial Intelligence 82, </booktitle> <pages> pages 428-433, </pages> <address> Pittsburg, Pa, </address> <month> August </month> <year> 1982. </year>
Reference-contexts: The final but not least important advantage is its expressive power. Logic can express, not only asserted knowledge, but also incomplete knowledge, or information about incompletely known situations <ref> [27] </ref>. For instance, in predicate logic disjunction could be used to express that either this pen is blue or this pen is red, which is an incomplete knowledge about the colour of the pen.
Reference: [28] <author> Nils J. Nilsson. </author> <booktitle> Principles of Artificial Intelligence. </booktitle> <publisher> Tioga Publishing Company, </publisher> <year> 1980. </year>
Reference-contexts: We are confident about the correctness of this mechanism, we have tested it with all the examples in Mantra manual [8], some examples from artificial intelligence books <ref> [28, 34] </ref>, and some tests prepared by the author and her supervisor. 117 The importance of this work lies in some major aspects: The selection of the algo-rithms and the use the Object Oriented paradigm in the design and implementation phases.
Reference: [29] <author> Peter F. Patel-Schneider. </author> <title> A decidable first order logic for knowledge representation. </title> <booktitle> In Proceedings of the 9th International Joint Conference on Artificial Intelligence, </booktitle> <month> August 18-23, </month> <year> 1985. </year>
Reference-contexts: However, very little was done in defining decidable logics for knowledge representation. Most modifications are either extensions to first-order logic or ad-hoc changes to inference mechanism <ref> [29] </ref>. In Mantra, a four-valued logic, based on the work of Patel-Schneider [31] is used. The four-valued logic weakens the first order logic just enough to achieve a logic with a decidable inference process. The semantic-net and the frame formalisms are also based on the four-valued semantics. <p> The advantage of this is that entailment can explain what its failure to produce an answer means, since it is a semantically motivated and well-defined notion. This explanation could be used by the next stronger method which could be first order logic for instance <ref> [29, page 458] </ref>. Chapter 6 Conclusion In this thesis, a unification module and an inference mechanism based on t-entailment were designed in the Object Oriented paradigm making use of design patterns, and then implemented in C++. <p> This could be done either by converting the semantic-nets and frame knowledge into logic and using the implemented inference mechanism, or by implementing a separate mechanism for each formalism based on the four-valued semantics. I endorse an important suggestion given by Patel-Schneider in <ref> [29, page 458] </ref>: "Any knowledge representation system based on four-valued logic is going to be quite weak. One way of making such a system stronger is to make entailment be only the first method for answering questions.
Reference: [30] <author> Peter F. Patel-Schneider. </author> <title> A four-valued semantics for frame-based description languages. </title> <booktitle> In Proceedings of AAAI-86 5th International Conference on AI, </booktitle> <address> Philadelphia, USA, </address> <month> August 11-15 </month> <year> 1986. </year> <note> Springer-Verlag. </note>
Reference-contexts: After that, many-valued logic started to gain interest in many areas: logic design, switching theory, programming languages, pattern recognition, artificial intelligence and many others. Our concern here is the decidable four-valued logic introduced by Belnap [4] and extended to a version suitable for knowledge representation by Patel-Schneider <ref> [30, 31] </ref>. 2.1.4.1 Why Four-Valued Logic ? There are several reasons behind the need to use four-valued logic: Firstly, there is a trade-off between expressive power and computational tractability in knowledge representation formalisms [24].
Reference: [31] <author> Peter F. Patel-Schneider. </author> <title> A decidable first order logic for knowledge representation. </title> <journal> Journal of Automated Reasoning, </journal> <volume> 6 </volume> <pages> 361-388, </pages> <year> 1990. </year>
Reference-contexts: However, very little was done in defining decidable logics for knowledge representation. Most modifications are either extensions to first-order logic or ad-hoc changes to inference mechanism [29]. In Mantra, a four-valued logic, based on the work of Patel-Schneider <ref> [31] </ref> is used. The four-valued logic weakens the first order logic just enough to achieve a logic with a decidable inference process. The semantic-net and the frame formalisms are also based on the four-valued semantics. <p> After that, many-valued logic started to gain interest in many areas: logic design, switching theory, programming languages, pattern recognition, artificial intelligence and many others. Our concern here is the decidable four-valued logic introduced by Belnap [4] and extended to a version suitable for knowledge representation by Patel-Schneider <ref> [30, 31] </ref>. 2.1.4.1 Why Four-Valued Logic ? There are several reasons behind the need to use four-valued logic: Firstly, there is a trade-off between expressive power and computational tractability in knowledge representation formalisms [24]. <p> The four-valued logic, described in section 2.1.4, has the last two properties since it is based on the language of standard first-order logic. In this section, a decidable inference mechanism <ref> [31] </ref> based on t-entailment is described in detail. <p> What is needed is a decidable algorithm to compute the t-entailment |to decide whether ff ! t fi or ff 6! t fi. Such an algorithm was described by Patel-Schneider: t-entailment Algorithm Theorem 1 <ref> [31, page 378] </ref> Let ff be a formula in conjunctive normal form with no existentially quantified variables, ff = 8~z ^ ff j . Let fi be a formula in t-quantifier normal form. <p> A drawback of this inference mechanism is that its worst case running time is exponential in jffj fl jfij where the magnitude of ff and fi is the number of literals in each <ref> [31, page 380] </ref>. However, in most cases, a knowledge representation system will have the following reasonable assumptions [31, page 380]: Facts are already in CNF. If they are not, converting them to normal form will not increase their size significantly. <p> A drawback of this inference mechanism is that its worst case running time is exponential in jffj fl jfij where the magnitude of ff and fi is the number of literals in each <ref> [31, page 380] </ref>. However, in most cases, a knowledge representation system will have the following reasonable assumptions [31, page 380]: Facts are already in CNF. If they are not, converting them to normal form will not increase their size significantly. Knowledge base has different predicates, thus only a small number will require uni fication. Queries are small even when converted to CNF.
Reference: [32] <author> D. Prawitz. </author> <title> An improved proof procedure. </title> <booktitle> In Theoria, </booktitle> <year> 1960. </year>
Reference-contexts: The first published unification algorithm was given in J. Herbrand's thesis in 1930 [23]. In his algorithm, Herbrand described three properties related to the validity of a formula. In 1960, D. Prawitz <ref> [32] </ref> worked on a logic which did not contain any function symbols, and for which he computed a most general representative of all possible instantiations. In 1963, M.
Reference: [33] <author> Wolfgang Pree. </author> <title> Design Patterns for Object-Oriented Software Development. </title> <publisher> Ad-dison Wesley Publishing Company, </publisher> <year> 1995. </year>
Reference-contexts: Design patterns make it easier to reuse successful designs, to choose design alternatives, and to improve the documentation and maintenance. Put simply, design patterns help a designer get a design "right" fast <ref> [33, page 2] </ref>. In this thesis, design patterns are used to solve the design problems encountered. In the design description, the Object Modeling Technique (OMT) notations are used [10]. The OMT methodology, is developed by J. <p> The selection of OMT for this work, is based on its popularity and enforced by the fact that "All of the object-oriented methodologies, have much in common, and should be contrasted more with non-object-oriented methodologies than with each other" <ref> [33] </ref>. The programming language C++ is used in this work since it supports a combination of object-oriented and traditional procedure-oriented programming. 3 Chapter 2 Background 2.1 Logic Logic studies proofs, theorem inference from other theorems or axioms, and how assertions are combined and connected.
Reference: [34] <author> Elaine Rich. </author> <booktitle> Artificial Intelligence. </booktitle> <publisher> Mc Graw Hill Book Company, </publisher> <year> 1983. </year>
Reference-contexts: To represent any kind of knowledge through a knowledge base system, one is faced with choosing among a broad repertoire of formalisms to achieve this goal. In making this choice, four properties should be taken into consideration <ref> [34] </ref>: * Representational adequacy |the ability to represent the required knowledge. * Inferential adequacy |the ability to manipulate the knowledge and infer new knowledge from the old. * Inferential efficiency |the ability to direct the inference mechanism into the most efficient direction by appropriate guides. * Acquisitional efficiency |the ability to <p> Those systems should have the following four properties <ref> [34] </ref>: * Representational adequacy: be able to represent all the kinds of knowledge needed. * Inferential adequacy: be able to manipulate the representational structures to derive new ones. * Inferential efficiency: be able to incorporate information that will help in the inference mechanism. * Acquisitional efficiency: be able to acquire information <p> We are confident about the correctness of this mechanism, we have tested it with all the examples in Mantra manual [8], some examples from artificial intelligence books <ref> [28, 34] </ref>, and some tests prepared by the author and her supervisor. 117 The importance of this work lies in some major aspects: The selection of the algo-rithms and the use the Object Oriented paradigm in the design and implementation phases.
Reference: [35] <author> David C. Rine. </author> <title> Computer Science and Multiple Valued Logic Theory and Applications. </title> <publisher> North-Holland Publishing Company, </publisher> <year> 1977. </year> <month> 122 </month>
Reference-contexts: In the early 1940s, the first complete algebra of n-valued logic corresponding to the work of Post, was formulated <ref> [35] </ref>. After that, many-valued logic started to gain interest in many areas: logic design, switching theory, programming languages, pattern recognition, artificial intelligence and many others.
Reference: [36] <author> J.A Robinson. </author> <title> A machine-oriented logic based on resolution principle. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 12(1) </volume> <pages> 23-41, </pages> <month> January </month> <year> 1965. </year>
Reference-contexts: The idea behind this is that reusing formulae makes completeness easy to prove by a general method but implementation becomes difficult [17, page 39]. 10 2.1.1.4.2 Resolution Method The resolution principle was originally proposed by J.A. Robinson in the early 1960s <ref> [36] </ref>. The motivation for developing this principle was to improve the efficiency of earlier proof methods. He admitted that the single inference rule of his calculus does not necessarily lead to easily understandable proofs, but he said that all that is required is that it is sound and recursive. <p> In 1963, M. Davis [13] published a proof procedure which was implemented on an IBM 7090 at Bell Telephone Laboratories in November 1962, and which used a unification algorithm. It was the first fully implemented unification algorithm. It was not until 1965 that J.A Robinson <ref> [36] </ref> published a formal unification algorithm for first order terms, and a proof that this algorithm computes a most general unifier. Robinson's algorithm is simple and applicable to any finite nonempty set A of terms.
Reference: [37] <author> Uwe Schoning. </author> <title> Logic for Computer Scientists. </title> <publisher> Birkhauser, </publisher> <year> 1989. </year>
Reference-contexts: Our interest is theorem proving and knowledge representation using logic. In this section, the classical propositional and predicate logic notations <ref> [17, 37] </ref> are studied and a wide variety of their theorem proving formalisms are discussed namely: tableaux and resolution mechanisms, Hilbert systems, natural deduction, the sequent calculus, and the Davis-Putman procedure. <p> The main interest is to show how the truth 4 value of an atomic sentence extends to the truth of more complex sentences. The most important features studied in propositional logic are given below and the notations used are based on [17] and <ref> [37] </ref>. 2.1.1.1 Syntax * Propositional letters express propositions: P ,Q,... * Formulae are propositional letters connected using propositional connectives. * Propositional connectives: _; ^; ae; oe; j; 6oe; 6ae; 6j; "; #. * Constants: ? false , &gt; true. &gt;, t, and true are used interchangeably. <p> For instance, how to express that a property is true for certain objects or for all objects. The most important features studied in predicate logic are given below and the notations used are also based on <ref> [17, 37] </ref>. 2.1.2.1 Syntax * The quantifiers are: existential quantifier (there exists) 9 and universal quanti fier (for all) 8. * A first order language L (R,F,C) is determined by: A finite set R of relation symbols where each relation has a positive integer which determines its arity. <p> Each disjunction is a collection of terms. Terms are the input to the unification module. 3.1 Steps in Transforming a Formula into CNF The steps necessary to transform a formula into CNF are the following <ref> [37] </ref>: 1. Rectify the formula: * No variable should occur both bound and free. For variables occurring bound and free, the free occurrences are renamed. * All quantifiers should refer to different variables. <p> It was designed using the iterator pattern, and a combination of the composite and visitor patterns. Its implementation is general to accept any two terms to be unified. Testing was done using examples from Martelli and Montanari paper [25], some logic books <ref> [16, 17, 19, 37] </ref> and inputs prepared by the author and her supervisor. The final step was the building of the inference mechanism based on t-entailment, which used the CNF transformation and the unification module.
Reference: [38] <author> J.H Siekmann and P. Szabo. </author> <title> Universal unification. </title> <editor> In Wolfgang Wahlster, editor, </editor> <booktitle> 6th German Workshop on Artificial Intelligence, </booktitle> <pages> pages 102-141. </pages> <publisher> Springer-Verlag, </publisher> <month> September 27-October 1, </month> <year> 1982. </year>
Reference-contexts: This algorithm requires time exponential in the size of the terms. Robinson introduced unification to implement resolution. Since then, it became universal in automated theorem proving. Due to its pattern matching nature, unification is applicable in a wide variety of areas in computer science <ref> [18, 38, 39] </ref> such as deductive databases, information retrieval, type checking, natural language processing, and logic programming. Unification is at the heart of these applications. Thus, its 62 performance is crucial to the overall efficiency. Some algorithms having almost linear time complexity were published for example [19]: M.S.
Reference: [39] <author> Jorg H. Siekmann. </author> <title> Universal unification. In R.E. </title> <editor> Shostak, editor, </editor> <booktitle> 7th International Conference on Automated Deduction, </booktitle> <address> Napa California USA, May 14-16, 1984. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: This algorithm requires time exponential in the size of the terms. Robinson introduced unification to implement resolution. Since then, it became universal in automated theorem proving. Due to its pattern matching nature, unification is applicable in a wide variety of areas in computer science <ref> [18, 38, 39] </ref> such as deductive databases, information retrieval, type checking, natural language processing, and logic programming. Unification is at the heart of these applications. Thus, its 62 performance is crucial to the overall efficiency. Some algorithms having almost linear time complexity were published for example [19]: M.S.
Reference: [40] <author> P. Trum and G. Winterstein. </author> <title> Description, implementation, and practical comparison of unification algorithms. </title> <type> Internal Rep. 6/78, </type> <institution> Fachbereich Informatik, Univ. of Kaiserlautern, Germany. </institution>
Reference-contexts: However, in theorem provers, where unification is mostly used, we usually deal with small terms and in this case the asymptotically growing difference between linear and nonlinear algorithms will depend on the efficiency of the implementation. An experiment was carried out by Trum and Winterstein <ref> [40] </ref>: They implemented the three algorithms in Pascal using the same data structures. The result was that Martelli and Montanari's algorithm had the lowest running time for all test data.
Reference: [41] <author> T. Winograd. </author> <title> Understanding Natural Language. </title> <publisher> Academic Press, </publisher> <year> 1972. </year>
Reference-contexts: Procedural representation of a piece of 42 information is a plan on how to use that information. In other words, knowledge is represented by rules about a particular problem. This approach was taken by Winograd <ref> [41, 42] </ref> in his SHRDLU system which takes English sentences as input and produces a set of procedures for doing what the statement requested. Each procedure has a set goals and there is a mechanism to satisfy this goal.
Reference: [42] <author> T. Winograd. </author> <title> A procedural model of language understanding. </title> <editor> In K.M. Colby and R.C. Schank, editors, </editor> <booktitle> Computer Models of Tought and Language. </booktitle> <publisher> Freeman, </publisher> <year> 1973. </year>
Reference-contexts: Procedural representation of a piece of 42 information is a plan on how to use that information. In other words, knowledge is represented by rules about a particular problem. This approach was taken by Winograd <ref> [41, 42] </ref> in his SHRDLU system which takes English sentences as input and produces a set of procedures for doing what the statement requested. Each procedure has a set goals and there is a mechanism to satisfy this goal.
Reference: [43] <author> A.A. Zinov'ev. </author> <title> Philosophical Problems of Many Valued Logic. </title> <address> D. </address> <publisher> Reidel Publishing Company, </publisher> <year> 1963. </year> <month> 123 </month>
Reference-contexts: So we need our facts to be mapped not only over two values (true and false) but over many values [2]. Historically, the first many-valued system (propositional logic) was constructed by Lukasiewicz in 1920 <ref> [43] </ref> in which he introduced the notion of three-valued logic : true, false, and neutral (an intermediary value). Shortly after Lukasiewicz in 1921, Post published his many-valued system [43] where he allowed arguments and functions to take values out of a given number of n values (say 1,2,...,n) regardless of what <p> Historically, the first many-valued system (propositional logic) was constructed by Lukasiewicz in 1920 <ref> [43] </ref> in which he introduced the notion of three-valued logic : true, false, and neutral (an intermediary value). Shortly after Lukasiewicz in 1921, Post published his many-valued system [43] where he allowed arguments and functions to take values out of a given number of n values (say 1,2,...,n) regardless of what meaning each value i could have. In the early 1940s, the first complete algebra of n-valued logic corresponding to the work of Post, was formulated [35].
References-found: 43

