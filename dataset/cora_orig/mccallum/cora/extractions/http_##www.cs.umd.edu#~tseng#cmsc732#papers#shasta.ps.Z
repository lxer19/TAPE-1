URL: http://www.cs.umd.edu/~tseng/cmsc732/papers/shasta.ps.Z
Refering-URL: http://www.cs.umd.edu/~tseng/cmsc732/papers.html
Root-URL: 
Title: Shasta: A Low Overhead, Software-Only Approach for Supporting Fine-Grain Shared Memory  
Author: Daniel J. Scales and Kourosh Gharachorloo Chandramohan A. Thekkath 
Affiliation: Western Research Laboratory Digital Equipment Corporation  Systems Research Center Digital Equipment Corporation  
Abstract: This paper describes Shasta, a system that supports a shared address space in software on clusters of computers with physically distributed memory. A unique aspect of Shasta compared to most other software distributed shared memory systems is that shared data can be kept coherent at a fine granularity. In addition, the system allows the coherence granularity to vary across different shared data structures in a single application. Shasta implements the shared address space by transparently rewriting the application executable to intercept loads and stores. For each shared load or store, the inserted code checks to see if the data is available locally and communicates with other processors if necessary. The system uses numerous techniques to reduce the run-time overhead of these checks. Since Shasta is implemented entirely in software, it also provides tremendous flexibility in supporting different types of cache coherence protocols. We have implemented an efficient cache coherence protocol that incorporates a number of optimizations, including support for multiple communication granularities and use of relaxed memory models. This system is fully functional and runs on a cluster of Alpha workstations. The primary focus of this paper is to describe the techniques used in Shasta to reduce the checking overhead for supporting fine granularity sharing in software. These techniques include careful layout of the shared address space, scheduling the checking code for efficient execution on modern processors, using a simple method that checks loads using only the value loaded, reducing the extra cache misses caused by the checking code, and combining the checks for multiple loads and stores. To characterize the effect of these techniques, we present detailed performance results for the SPLASH-2 applications running on an Alpha processor. Without our optimizations, the checking overheads are excessively high, exceeding 100% for several applications. However, our techniques are effective in reducing these overheads to a range of 5% to 35% for almost all of the applications. We also describe our coherence protocol and present some preliminary results on the parallel performance of several applications running on our workstation cluster. Our experience so far indicates that once the cost of checking memory accesses is reduced using our techniques, the Shasta approach is an attractive software solution for supporting a shared address space with fine-grain access to data. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. E. Bal, M. F. Kaashoek, and A. S. Tanenbaum. Orca: </author> <title> A Language for Parallel Programming of Distributed Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(3) </volume> <pages> 190-205, </pages> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: Midway [2] inserts code at stores to record where writes have occurred in a shared memory block. Some systems that utilize garbage collection record the location of stores in a manner similar to Midway to aid in the process of scavenging for free memory [21]. Objector region-based DSM systems <ref> [1, 2, 11, 14, 16] </ref> communicate data at the object level and therefore support coherence at multiple granularities, but these systems require explicit programmer intervention to partition the application data into objects and to identify when objects are accessed through annotations.
Reference: [2] <author> B. N. Bershad, M. J. Zekauskas, and W. A. Sawdon. </author> <title> The Midway Distributed Shared Memory System. </title> <booktitle> In COMPCON 1993, </booktitle> <pages> pages 528-537, </pages> <month> Mar. </month> <year> 1993. </year>
Reference-contexts: A variety of such distributed shared memory (DSM) systems have been developed, using various techniques to minimize the software overhead for supporting the shared address space. For example, some systems require programmer annotations or explicit calls to access shared data <ref> [2, 11] </ref>. Another approach, called Shared Virtual Memory (SVM), uses the virtual memory hardware to detect access to data that is not available locally [4, 13, 12]. <p> Controlling the coherence granularity in this manner is simpler than approaches adopted by objector region-based DSM systems <ref> [2, 11, 14, 16] </ref>, since the latter approaches can affect correctness and typically require a more substantial change to the application. We currently associate different granularities to different virtual pages and place newly allocated data on the appropriate page. <p> Olden [3] uses the compiler to insert checks at loads and stores to implement a specialized shared memory protocol across workstations. Split-C [6] uses compiler-inserted checks to implement a shared address space without caching in the context of a parallel language. Midway <ref> [2] </ref> inserts code at stores to record where writes have occurred in a shared memory block. Some systems that utilize garbage collection record the location of stores in a manner similar to Midway to aid in the process of scavenging for free memory [21]. <p> Midway [2] inserts code at stores to record where writes have occurred in a shared memory block. Some systems that utilize garbage collection record the location of stores in a manner similar to Midway to aid in the process of scavenging for free memory [21]. Objector region-based DSM systems <ref> [1, 2, 11, 14, 16] </ref> communicate data at the object level and therefore support coherence at multiple granularities, but these systems require explicit programmer intervention to partition the application data into objects and to identify when objects are accessed through annotations.
Reference: [3] <author> M. C. Carlisle and A. Rogers. </author> <title> Software Caching and Computation Migration in Olden. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 29-38, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: We have also measured the Shasta overhead for the appbt application. The Blizzard-S overhead is 1.9, while the Shasta overhead is 1.19. There are several other systems that use compiler-generated checks to aid in implementing a global address space. Olden <ref> [3] </ref> uses the compiler to insert checks at loads and stores to implement a specialized shared memory protocol across workstations. Split-C [6] uses compiler-inserted checks to implement a shared address space without caching in the context of a parallel language.
Reference: [4] <author> J. B. Carter, J. K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: For example, some systems require programmer annotations or explicit calls to access shared data [2, 11]. Another approach, called Shared Virtual Memory (SVM), uses the virtual memory hardware to detect access to data that is not available locally <ref> [4, 13, 12] </ref>. In most such systems, the granularity at which data is accessed and kept coherent is large, because it is related to the size of an application data structure or the size of a virtual page.
Reference: [5] <author> D. Chiou, B. S. Ang, Arvind, M. J. Becherle, A. Boughton, R. Greiner, J. E. Hicks, and J. C. Hoe. StarT-NG: </author> <title> Delivering Seamless Parallel Computing. </title> <booktitle> In Proceedings of EURO-PAR '95, </booktitle> <pages> pages 101-116, </pages> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: We independently developed the flag technique of Section 3.2 for use with all types of loads in the 64-bit Alpha architecture. We recently became aware that a form of the flag technique has been proposed for use in hardware in the StarT-NG machine <ref> [5] </ref>. The Blizzard-S project has also recently incorporated the flag technique by adapting this idea from the StarT-NG design [17]. With this optimization, the Blizzard-S overhead is 3 instructions at most loads and 8 instructions at most stores.
Reference: [6] <editor> D. E. Culler et al. </editor> <booktitle> Parallel Programming in Split-C. In Proceedings of Supercomputing '93, </booktitle> <pages> pages 262-273, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: There are several other systems that use compiler-generated checks to aid in implementing a global address space. Olden [3] uses the compiler to insert checks at loads and stores to implement a specialized shared memory protocol across workstations. Split-C <ref> [6] </ref> uses compiler-inserted checks to implement a shared address space without caching in the context of a parallel language. Midway [2] inserts code at stores to record where writes have occurred in a shared memory block.
Reference: [7] <author> A. Erlichson, N. Nuckolls, G. Chesson, and J. Hennessy. SoftFLASH: </author> <title> Analyzing the Performance of Clustered Distributed Virtual Shared Memory. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Similarly, some page-based systems (e.g., Treadmarks [12]) reduce the required bandwidth by only communicating the differences between copies, but the coherence granularity is still a page. Page-based DSM systems implemented on a cluster of shared-memory multiprocessors, such as MGS [23] and SoftFLASH <ref> [7] </ref>, naturally support two coherence granularities the line size of the multiprocessor hardware and the size of the virtual memory page. However, neither of these granularities can be changed.
Reference: [8] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The optimizations include exploiting relaxed memory models, supporting coherence and communication at multiple granularities, and batching together requests for multiple misses. 4.1 Exploiting Relaxed Memory Models Our protocol aggressively exploits the release consistency model <ref> [8] </ref> by emulating the behavior of a processor with non-blocking loads and stores and a lockup-free cache. Because of our non-blocking load and store operations, a line may be in one of two pending states, pending-invalid and pending-shared.
Reference: [9] <author> R. Gillett, M. Collins, and D. Pimm. </author> <title> Overview of Memory Channel Network for PCI. </title> <booktitle> In Proceedings of COMPCON '96, </booktitle> <pages> pages 244-248, </pages> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: Our protocol also includes optimizations, such as non-stalling stores, that exploit a relaxed memory model. We have implemented the Shasta system on clusters of Alpha workstations connected by an ATM network or a Memory Channel network <ref> [9] </ref>. We use a modified version of ATOM [20] that can insert any Alpha instructions at any point in the executable. As illustrated in Figure 1, our Shasta compiler automatically modifies application executables developed for a hardware shared-memory multiprocessor so that they run on a cluster of workstations. <p> In addition, we present preliminary parallel performance results for some of the SPLASH-2 applications running on a cluster of Alpha workstations connected by Digital's Memory Channel network <ref> [9] </ref>. 5.1 Instruction and Cycle Overhead of Miss Checks Table 1 gives the static overheads for our optimized load and store miss checks, when using the flag technique and exclusive table, in the case that there is no shared data miss.
Reference: [10] <author> M. Horowitz, M. Martonosi, T. C. Mowry, and M. D. Smith. </author> <title> Informing Memory Operations: Providing Memory Performance Feedback in Modern Processors. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 260-270, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: First, there is still much room for more aggressive compiler optimizations, including scheduling application and checking instructions better and extending our techniques to use interprocedural analysis. Second, more of the checking overhead may be hidden on more modern processors with dynamic scheduling capability <ref> [10] </ref>. Finally, the relative effect of the checking overhead is typically less on the parallel execution time due to the overheads arising from communication and synchronization. <p> Support for informing memory operations has also been proposed as a way of invoking handler code whenever there is a miss in the primary data cache <ref> [10] </ref>. While informing operations may be used to implement coherent shared memory, a major disadvantage of the scheme is that the handler is called whenever referenced data is not in the first-level cache, even if it is in local memory.
Reference: [11] <author> K. L. Johnson, M. F. Kaashoek, and D. A. Wallach. </author> <title> CRL: High-Performance All-Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the Fifteenth Symposium on Operating System Principles, </booktitle> <pages> pages 213-228, </pages> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: A variety of such distributed shared memory (DSM) systems have been developed, using various techniques to minimize the software overhead for supporting the shared address space. For example, some systems require programmer annotations or explicit calls to access shared data <ref> [2, 11] </ref>. Another approach, called Shared Virtual Memory (SVM), uses the virtual memory hardware to detect access to data that is not available locally [4, 13, 12]. <p> Controlling the coherence granularity in this manner is simpler than approaches adopted by objector region-based DSM systems <ref> [2, 11, 14, 16] </ref>, since the latter approaches can affect correctness and typically require a more substantial change to the application. We currently associate different granularities to different virtual pages and place newly allocated data on the appropriate page. <p> Midway [2] inserts code at stores to record where writes have occurred in a shared memory block. Some systems that utilize garbage collection record the location of stores in a manner similar to Midway to aid in the process of scavenging for free memory [21]. Objector region-based DSM systems <ref> [1, 2, 11, 14, 16] </ref> communicate data at the object level and therefore support coherence at multiple granularities, but these systems require explicit programmer intervention to partition the application data into objects and to identify when objects are accessed through annotations.
Reference: [12] <author> P. Keleher, A. L. Cox, S. Dwarkadas, and W. Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <pages> pages 115-132, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: For example, some systems require programmer annotations or explicit calls to access shared data [2, 11]. Another approach, called Shared Virtual Memory (SVM), uses the virtual memory hardware to detect access to data that is not available locally <ref> [4, 13, 12] </ref>. In most such systems, the granularity at which data is accessed and kept coherent is large, because it is related to the size of an application data structure or the size of a virtual page. <p> Even though a finer granularity of write detection can reduce the amount of communicated data, the access and coherence granularity is still at an object or page level (depending on the consistency model). Similarly, some page-based systems (e.g., Treadmarks <ref> [12] </ref>) reduce the required bandwidth by only communicating the differences between copies, but the coherence granularity is still a page.
Reference: [13] <author> K. Li and P. Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> Nov. </month> <year> 1989. </year>
Reference-contexts: For example, some systems require programmer annotations or explicit calls to access shared data [2, 11]. Another approach, called Shared Virtual Memory (SVM), uses the virtual memory hardware to detect access to data that is not available locally <ref> [4, 13, 12] </ref>. In most such systems, the granularity at which data is accessed and kept coherent is large, because it is related to the size of an application data structure or the size of a virtual page.
Reference: [14] <author> R. S. Nikhil. Cid: </author> <title> a Parallel, "Shared-memory" C for Distributed-memory Machines. </title> <booktitle> In Seventh Workshopon Languagesand Compilers for Parallel Computing, </booktitle> <pages> pages 376-390, </pages> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: Controlling the coherence granularity in this manner is simpler than approaches adopted by objector region-based DSM systems <ref> [2, 11, 14, 16] </ref>, since the latter approaches can affect correctness and typically require a more substantial change to the application. We currently associate different granularities to different virtual pages and place newly allocated data on the appropriate page. <p> Midway [2] inserts code at stores to record where writes have occurred in a shared memory block. Some systems that utilize garbage collection record the location of stores in a manner similar to Midway to aid in the process of scavenging for free memory [21]. Objector region-based DSM systems <ref> [1, 2, 11, 14, 16] </ref> communicate data at the object level and therefore support coherence at multiple granularities, but these systems require explicit programmer intervention to partition the application data into objects and to identify when objects are accessed through annotations.
Reference: [15] <author> S. K. Reinhardt, R. W. Pfile, and D. A. Wood. </author> <title> Decoupled Hardware Support for Distributed Shared Memory. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 34-43, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: A number of systems attempt to use a small amount of extra hardware to support fine-grain access control to shared data more efficiently. Blizzard-E [18] uses ECC bits at the memory level to cause faults on accesses to particular lines; similarly, Typhoon-0 <ref> [15] </ref> uses hardware at the memory bus to detect an access fault.
Reference: [16] <author> D. J. Scales and M. S. Lam. </author> <title> The Design and Evaluation of a Shared Object System for Distributed Memory Machines. </title> <booktitle> In Proceedings of the First Symposium on Operating System Design and Implementation, </booktitle> <pages> pages 101-114, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Controlling the coherence granularity in this manner is simpler than approaches adopted by objector region-based DSM systems <ref> [2, 11, 14, 16] </ref>, since the latter approaches can affect correctness and typically require a more substantial change to the application. We currently associate different granularities to different virtual pages and place newly allocated data on the appropriate page. <p> Midway [2] inserts code at stores to record where writes have occurred in a shared memory block. Some systems that utilize garbage collection record the location of stores in a manner similar to Midway to aid in the process of scavenging for free memory [21]. Objector region-based DSM systems <ref> [1, 2, 11, 14, 16] </ref> communicate data at the object level and therefore support coherence at multiple granularities, but these systems require explicit programmer intervention to partition the application data into objects and to identify when objects are accessed through annotations.
Reference: [17] <author> I. Schoinas, B. Falsafi, M. D. Hill, J. R. Larus, C. E. Lukas, S. S. Mukherjee, S. K. Reinhardt, E. Schnarr, and D. A. Wood. </author> <title> Implementing Fine-Grain Distributed Shared Memory on Commodity SMP Workstations. </title> <type> Technical Report 1307, </type> <institution> University of Wisconsin Computer Sciences, </institution> <month> Mar. </month> <year> 1996. </year>
Reference-contexts: We recently became aware that a form of the flag technique has been proposed for use in hardware in the StarT-NG machine [5]. The Blizzard-S project has also recently incorporated the flag technique by adapting this idea from the StarT-NG design <ref> [17] </ref>. With this optimization, the Blizzard-S overhead is 3 instructions at most loads and 8 instructions at most stores. <p> Run-time overheads on a 66 MHz HyperSPARC processor (with a 8K first-level data cache and 256K second-level data cache) are reported for five applications, of which only one is a SPLASH-2 application <ref> [17] </ref>. For the Barnes-Hut application, the reported Blizzard-S overhead on the Sparc is 1.6, while the Shasta overhead on the 275 MHz Alpha is 1.08. We have also measured the Shasta overhead for the appbt application. The Blizzard-S overhead is 1.9, while the Shasta overhead is 1.19.
Reference: [18] <author> I. Schoinas, B. Falsafi, A. R. Lebeck, S. K. Reinhardt, J. R. Larus, and D. A. Wood. </author> <title> Fine-grain Access Control for Distributed Shared Memory. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 297-306, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: Fine-grain access to shared data is important to reduce false sharing and the transmission of unneeded data, both of which are potential problems in systems with large coherence granularities. Shasta uses the basic approach of the Blizzard-S system from Wisconsin <ref> [18] </ref>. Shasta implements the shared address space by inserting checks in an application executable at loads and stores. This inline code checks if the data is available locally and sends out the necessary messages to other processors if the operation cannot be serviced locally. <p> In addition, the ability to choose different coherence granularities for different data structures in the same application appears to be highly beneficial for boosting performance. 6 Related Work Shasta's basic approach is derived from the Blizzard-S work <ref> [18] </ref>. However, we have substantially extended the previous work in this area by developing several techniques for reducing the otherwise excessive access control overheads. We have also developed an efficient protocol that provides support for maintaining coherence at variable granularities within a single application. <p> However, neither of these granularities can be changed. A number of systems attempt to use a small amount of extra hardware to support fine-grain access control to shared data more efficiently. Blizzard-E <ref> [18] </ref> uses ECC bits at the memory level to cause faults on accesses to particular lines; similarly, Typhoon-0 [15] uses hardware at the memory bus to detect an access fault.
Reference: [19] <author> J. P. Singh, W. D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: Data in the shared region may be cached by multiple processors at the same time, with copies residing at the same virtual address on each processor. In the current Shasta system, we have adopted the memory model of the original SPLASH applications <ref> [19] </ref>: data that is dynamically allocated is shared, but all static and stack data is private.
Reference: [20] <author> A. Srivastava and A. Eustace. </author> <title> ATOM: A System for Building Customized Program Analysis Tools. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 196-205, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Our protocol also includes optimizations, such as non-stalling stores, that exploit a relaxed memory model. We have implemented the Shasta system on clusters of Alpha workstations connected by an ATM network or a Memory Channel network [9]. We use a modified version of ATOM <ref> [20] </ref> that can insert any Alpha instructions at any point in the executable. As illustrated in Figure 1, our Shasta compiler automatically modifies application executables developed for a hardware shared-memory multiprocessor so that they run on a cluster of workstations.
Reference: [21] <author> P. R. Wilson and T. G. Moher. </author> <title> A Card-marking Scheme for Controlling Intergenerational References in Generation-Based GC on Stock Hardware. </title> <journal> SIGPLAN Notices, </journal> <volume> 24(5) </volume> <pages> 87-92, </pages> <year> 1989. </year>
Reference-contexts: Midway [2] inserts code at stores to record where writes have occurred in a shared memory block. Some systems that utilize garbage collection record the location of stores in a manner similar to Midway to aid in the process of scavenging for free memory <ref> [21] </ref>. Objector region-based DSM systems [1, 2, 11, 14, 16] communicate data at the object level and therefore support coherence at multiple granularities, but these systems require explicit programmer intervention to partition the application data into objects and to identify when objects are accessed through annotations.
Reference: [22] <author> S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22nd International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: We primarily focus on characterizing the overhead of miss checks by presenting the static overheads for individual miss checks, the dynamic overheads for all of the SPLASH-2 applications <ref> [22] </ref>, and the frequency of instrumented accesses in these applications. <p> Our results are for a 275 MHz 21064A dual-issue processor, which has a 16 Kbyte on-chip instruction cache, a 16 Kbyte on-chip data cache, and a 4 Mbyte second-level off-chip data cache. We use the standard SPLASH-2 problem sizes for each of the applications <ref> [22] </ref>, except for Radiosity and Raytrace, where we use smaller test sets so as to avoid running out of swap space on some of our systems. The line size of the Shasta system is configured as 64 bytes.
Reference: [23] <author> D. Yeung, J. Kubiatowicz, and A. Agarwal. MGS: </author> <title> A Multigrain shared Memory System. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 44-55, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Similarly, some page-based systems (e.g., Treadmarks [12]) reduce the required bandwidth by only communicating the differences between copies, but the coherence granularity is still a page. Page-based DSM systems implemented on a cluster of shared-memory multiprocessors, such as MGS <ref> [23] </ref> and SoftFLASH [7], naturally support two coherence granularities the line size of the multiprocessor hardware and the size of the virtual memory page. However, neither of these granularities can be changed.
References-found: 23

