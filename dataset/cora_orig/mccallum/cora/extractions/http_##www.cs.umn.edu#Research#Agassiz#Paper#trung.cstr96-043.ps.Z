URL: http://www.cs.umn.edu/Research/Agassiz/Paper/trung.cstr96-043.ps.Z
Refering-URL: http://www.cs.umn.edu/Research/Agassiz/agassiz_pubs.html
Root-URL: http://www.cs.umn.edu
Email: email: li@cs.umn.edu  
Title: Performance Evaluation of Memory Allocation Schemes on CC-NUMA Multiprocessors  
Author: Trung N. Nguyen Zhiyuan Li Jian Huang Guohua Jin Dongsoo Kim 
Note: Contact  
Abstract: Cache Coherent Non-Uniform Memory Access (CC-NUMA) architectures have received strong interests from both academia and industries. This paper studies the performance impact of design choices at different levels of address and memory mapping on CC-NUMA architectures. Through execution-driven simulations of five numerical programs, we find close interactions between data allocation, global address translation and cache set-addressing. Our results strongly discourage the use of direct-mapped caches in CC-NUMA machines. Results also show that data allocation often makes a great impact on memory miss ratio and execution time. A compiler scheme which allocates data and parallel tasks simultaneously is shown to perform quite well consistently. Keywords - CC-NUMA, Data Allocation, Cache Mapping, Memory Management, Parallel Task Allocation 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, D. Kranz, and V. Natarajan. </author> <title> Automatic partitioning of parallel loops and data arrays for distributed shared memory multiprocessors. </title> <booktitle> In Proc. International Conference on Parallel Processing, volume I: Architecture, </booktitle> <pages> pages 2-11, </pages> <address> St. Charles, IL, </address> <year> 1993. </year>
Reference-contexts: By solving several potentially large systems of equations, array data and loop iterations are mapped to processors. During this process, some parallel loops may be sequentialized in order to eliminate or minimize remote memory references. Agarwal et al. also model this problem in a linear algebra framework <ref> [1] </ref>. Loop iterations and array data are mapped to a hyperplane. The loop iterations are then partitioned into optimal hyperparallelepiped tiles that minimize the interprocessor communication. This scheme, however, only considers a single loop nest. Li and Pingali take a different approach to the data and task allocation problem [9].
Reference: [2] <author> A. Agarwal et al. </author> <title> The MIT alewife machine: A large-scale distributed-memory multiprocessor. </title> <type> Technical Report 454, </type> <institution> MIT/LCS, </institution> <year> 1991. </year>
Reference-contexts: 1 Introduction In recent years, we see a trend toward the Cache Coherent Non-Uniform Memory Access (CC-NUMA) architecture in multiprocessors. Systems based on this architecture includes research prototypes such as the Stanford DASH [8] and FLASH [7], MIT Alewife <ref> [2] </ref>, University of Toronto NUMAchine [19], and Sun's fl This work was supported in part by NSF CAREER Award CCR-9502541, by the Army High Performance Computing Research Center, under the auspices of Army Research Office contract number DAAL03-89-C-0038 with the University of Minnesota, and by the U.S.
Reference: [3] <author> J. M. Anderson and M. S. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proc. ACM SIGPLAN Conf. on Prog. Lang. Design and Imp., </booktitle> <pages> pages 112-125, </pages> <month> June </month> <year> 1993. </year> <month> 27 </month>
Reference-contexts: Anderson and Lam model this problem in a linear algebra framework <ref> [3] </ref>. They assume that loop bounds and array subscripts are affine functions of the loop indices. Parallel loop iterations are mapped to a polytope in an l-dimensional iteration space, where l is the depth of a parallel loop nest. An n-dimensional array is mapped to an n-dimensional rectangular space.
Reference: [4] <institution> Cray Research Inc. CrayT3D Technical Summary, </institution> <year> 1993. </year>
Reference-contexts: Instead, it makes a change at the level of memory module mapping, which is much easier. In addition, by using a modifiable mask to specify the pid bits (similar to the mask used in the Cray T3D Block Transfer Engine <ref> [4] </ref>), the 5 the set-addressing bits. second approach can even allow changes to the overlap between the pid bits and the SA bits for different programs. As will be shown later in this paper, the optimal overlap can differ for different programs.
Reference: [5] <author> J. Gu, Z. Li, and G. Lee. </author> <title> Symbolic array dataflow analysis for array privatization and program parallelization. </title> <booktitle> In Proc. Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: This section discusses an overview of the algorithm and some related works. More details on the algorithm and its implementation can be found in [14, 13]. The algorithm has been implemented in the Panorama compiler <ref> [5, 10, 14] </ref>. We assume that each iteration in a parallel loop is assigned to one unique virtual processor (VP). The VP's are eventually folded to the physical processors available for program execution. How the VP's are folded depends on which static loop scheduling scheme is used. <p> The simulation methodology is described next. 3.1 Simulation methodology Panorama, an interprocedural parallelizing compiler, is used to parallelize and instrument the benchmarking programs for this study. It also provides the framework for the implementation of the data-task co-allocation algorithm <ref> [5, 10, 14] </ref>. After a program is parallelized and instrumented by Panorama, it is compiled by SGI's f77 compiler to a parallel object code executable on the SGI Challenge multiprocessor. The SGI Challenge provides run-time libraries which implement the loop scheduling methods mentioned in the last section.
Reference: [6] <author> Hewlett-Packard Co. </author> <title> HP releases high-end technical-solutions road map with consistent architecture to beyond year 2000. </title> <address> http://www.convex.com/. </address>
Reference-contexts: S. Army Intelligence Center and Fort Huachuca, or the U.S. Government. Additional support is provided by a donation from Cray Research Inc. and by an instrumentation grant from NSF, CDA 9414015. 1 S3.mp [15], as well as commercial products including the Sequent STiNG [12] and Hewlett-Packard SPP <ref> [6] </ref>. On a CC-NUMA machine (c.f. Figure 1), an interconnection network connects a number of nodes, each of which consists of one or a few processors, a cache private to the node, and a local memory accessible remotely by other nodes.
Reference: [7] <author> J. Kuskin et al. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> In Proc. Int. Sym. on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction In recent years, we see a trend toward the Cache Coherent Non-Uniform Memory Access (CC-NUMA) architecture in multiprocessors. Systems based on this architecture includes research prototypes such as the Stanford DASH [8] and FLASH <ref> [7] </ref>, MIT Alewife [2], University of Toronto NUMAchine [19], and Sun's fl This work was supported in part by NSF CAREER Award CCR-9502541, by the Army High Performance Computing Research Center, under the auspices of Army Research Office contract number DAAL03-89-C-0038 with the University of Minnesota, and by the U.S.
Reference: [8] <author> D. Lenoski et al. </author> <title> The Stanford DASH multiprocessor. </title> <journal> Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: 1 Introduction In recent years, we see a trend toward the Cache Coherent Non-Uniform Memory Access (CC-NUMA) architecture in multiprocessors. Systems based on this architecture includes research prototypes such as the Stanford DASH <ref> [8] </ref> and FLASH [7], MIT Alewife [2], University of Toronto NUMAchine [19], and Sun's fl This work was supported in part by NSF CAREER Award CCR-9502541, by the Army High Performance Computing Research Center, under the auspices of Army Research Office contract number DAAL03-89-C-0038 with the University of Minnesota, and by
Reference: [9] <author> W. Li and K. Pingali. </author> <title> Access normalization: Loop restructuring for NUMA computers. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 11(4), </volume> <month> November </month> <year> 1993. </year>
Reference-contexts: Loop iterations and array data are mapped to a hyperplane. The loop iterations are then partitioned into optimal hyperparallelepiped tiles that minimize the interprocessor communication. This scheme, however, only considers a single loop nest. Li and Pingali take a different approach to the data and task allocation problem <ref> [9] </ref>. Access patterns of arrays in each loop nest are summarized in a data access 13 matrix. After calculating or estimating an invertible matrix of the data access matrix, they use it as a basis for transforming and normalizing the loop nest.
Reference: [10] <author> Z. Li. </author> <title> Propagating symbolic relations on an interprocedural and hierarchical control flow graph. </title> <type> Technical Report CSci-93-87, </type> <institution> University of Minnesota, </institution> <year> 1993. </year>
Reference-contexts: This section discusses an overview of the algorithm and some related works. More details on the algorithm and its implementation can be found in [14, 13]. The algorithm has been implemented in the Panorama compiler <ref> [5, 10, 14] </ref>. We assume that each iteration in a parallel loop is assigned to one unique virtual processor (VP). The VP's are eventually folded to the physical processors available for program execution. How the VP's are folded depends on which static loop scheduling scheme is used. <p> The simulation methodology is described next. 3.1 Simulation methodology Panorama, an interprocedural parallelizing compiler, is used to parallelize and instrument the benchmarking programs for this study. It also provides the framework for the implementation of the data-task co-allocation algorithm <ref> [5, 10, 14] </ref>. After a program is parallelized and instrumented by Panorama, it is compiled by SGI's f77 compiler to a parallel object code executable on the SGI Challenge multiprocessor. The SGI Challenge provides run-time libraries which implement the loop scheduling methods mentioned in the last section.
Reference: [11] <author> D. J. Lilja. </author> <title> Exploiting the parallelism available in loops. </title> <journal> IEEE Computer, </journal> <volume> 27(2) </volume> <pages> 13-26, </pages> <year> 1994. </year>
Reference-contexts: Using the above programming model, task allocation involves partitioning a DOALL loop's iterations (or tasks) and assigning them to processors. Task allocation is therefore also referred to as loop scheduling. For a survey of loop scheduling methods, the reader is referred to <ref> [11] </ref>. Scheduling of loop iterations to processors can be determined at compile-time, called static scheduling. One commonly used method of static scheduling is known as chunk-interleave scheduling, in which loop iterations are partitioned into chunks of the same size. These chunks are then assigned to the processors by interleaving.
Reference: [12] <author> T. Lovette and R. Clapp. STiNG: </author> <title> A CC-NUMA computer system for the commercial marketplace. </title> <booktitle> In Proc. Int. Sym. on Computer Architecture, </booktitle> <pages> pages 308-317, </pages> <year> 1996. </year>
Reference-contexts: S. Army Intelligence Center and Fort Huachuca, or the U.S. Government. Additional support is provided by a donation from Cray Research Inc. and by an instrumentation grant from NSF, CDA 9414015. 1 S3.mp [15], as well as commercial products including the Sequent STiNG <ref> [12] </ref> and Hewlett-Packard SPP [6]. On a CC-NUMA machine (c.f. Figure 1), an interconnection network connects a number of nodes, each of which consists of one or a few processors, a cache private to the node, and a local memory accessible remotely by other nodes.
Reference: [13] <author> T. N. Nguyen. </author> <title> Interprocedural Compiler Analysis for Reducing Memory Latency and Network Traffic. </title> <type> PhD thesis, </type> <institution> University of Minnesota, </institution> <note> to be compeleted, </note> <year> 1996. </year>
Reference-contexts: Second, for each array reference, the address is computed by using a new mapping function as opposed to say, the conventional column-major mapping in Fortran. Since these involve compiler technical details, we leave the discussion elsewhere <ref> [13] </ref>. To illustrate the remaining allocation schemes, we show how a two-dimensional array is distributed to processors by each scheme in the following. Per-array interleave Per-array interleave partitions the linear address space of the array into chunks of the cache line size. <p> This section discusses an overview of the algorithm and some related works. More details on the algorithm and its implementation can be found in <ref> [14, 13] </ref>. The algorithm has been implemented in the Panorama compiler [5, 10, 14]. We assume that each iteration in a parallel loop is assigned to one unique virtual processor (VP). The VP's are eventually folded to the physical processors available for program execution.
Reference: [14] <author> T. N. Nguyen, J. Gu, and Z. Li. </author> <title> An interprocedural parallelizing compiler and its support for hierarchical memory research. </title> <editor> In C.-H. Huang, P. Sadayappan, U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Proc. 8th Int. Workshop on Languages and Compilers for Parallel Computing, Lecture Notes in Computer Science 1033, </booktitle> <pages> pages 96-110, </pages> <address> Columbus, Ohio, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: This section discusses an overview of the algorithm and some related works. More details on the algorithm and its implementation can be found in <ref> [14, 13] </ref>. The algorithm has been implemented in the Panorama compiler [5, 10, 14]. We assume that each iteration in a parallel loop is assigned to one unique virtual processor (VP). The VP's are eventually folded to the physical processors available for program execution. <p> This section discusses an overview of the algorithm and some related works. More details on the algorithm and its implementation can be found in [14, 13]. The algorithm has been implemented in the Panorama compiler <ref> [5, 10, 14] </ref>. We assume that each iteration in a parallel loop is assigned to one unique virtual processor (VP). The VP's are eventually folded to the physical processors available for program execution. How the VP's are folded depends on which static loop scheduling scheme is used. <p> The simulation methodology is described next. 3.1 Simulation methodology Panorama, an interprocedural parallelizing compiler, is used to parallelize and instrument the benchmarking programs for this study. It also provides the framework for the implementation of the data-task co-allocation algorithm <ref> [5, 10, 14] </ref>. After a program is parallelized and instrumented by Panorama, it is compiled by SGI's f77 compiler to a parallel object code executable on the SGI Challenge multiprocessor. The SGI Challenge provides run-time libraries which implement the loop scheduling methods mentioned in the last section.
Reference: [15] <author> A. Nowatzyk et al. </author> <title> The S3.mp scalable shared memory multiprocessor. </title> <booktitle> In Proc. Int. Sym. on Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: S. Army Intelligence Center and Fort Huachuca, or the U.S. Government. Additional support is provided by a donation from Cray Research Inc. and by an instrumentation grant from NSF, CDA 9414015. 1 S3.mp <ref> [15] </ref>, as well as commercial products including the Sequent STiNG [12] and Hewlett-Packard SPP [6]. On a CC-NUMA machine (c.f.
Reference: [16] <author> W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. </author> <title> Numerical Recipes: The Art of Scientific Computing (Fortran Version). </title> <publisher> Cambridge University Press, </publisher> <year> 1989. </year>
Reference-contexts: Loops are not interchanged in those sequential codes, since interchange did not improve the sequential execution. We have collected data for the following five numerical Fortran programs: three programs, namely swm256, tomcatv and hydro2d, from the SPEC92 benchmarks, mgrid from the SPEC95 benchmarks, and adi <ref> [16] </ref>. Most programs use all 16 processors with the exception of hydro2d which uses only 8 processors because of its relatively low degree of parallelism.
Reference: [17] <author> P. Stenstrom, J. Truman, and A. Gupta. </author> <title> Comparative performance evaluation of cache-coherent NUMA and COMA architectures. </title> <booktitle> In Proc. Int. Sym. on Comp. Arch., </booktitle> <pages> pages 80-91, </pages> <year> 1992. </year>
Reference-contexts: A remote read miss requires either an additional 2-way network delay for a non-dirty line or an additional 3-way network delay for a dirty line <ref> [17] </ref>. The actual delay for cache and memory misses may vary depending on the availability of the cache ports and the memory ports.
Reference: [18] <author> J. E. Veenstra and R. J. Fowler. </author> <title> MINT tutorial and user manual. </title> <type> Technical Report 452, </type> <institution> University of Rochester, </institution> <month> June </month> <year> 1993. </year> <institution> Dep. of Computer Science. </institution>
Reference-contexts: The SGI Challenge provides run-time libraries which implement the loop scheduling methods mentioned in the last section. Block scheduling, however, is named simple scheduling. 14 The executable object code is then simulated by NUMAsim, an execution-driven CC-NUMA simulator. NUMAsim consists of two main parts: the MINT event generator front-end <ref> [18] </ref> and a CC-NUMA simulation back-end. Figure 10 illustrates how the components of NUMAsim interact with each other. The MINT front-end interprets each instruction in the executable object code and schedules events such as loads and stores for the back-end to simulate.
Reference: [19] <author> Z. Vranesic et al. </author> <title> The NUMAchine multiprocessor. </title> <type> Technical report, </type> <institution> University of Toronto, </institution> <year> 1995. </year> <month> 28 </month>
Reference-contexts: 1 Introduction In recent years, we see a trend toward the Cache Coherent Non-Uniform Memory Access (CC-NUMA) architecture in multiprocessors. Systems based on this architecture includes research prototypes such as the Stanford DASH [8] and FLASH [7], MIT Alewife [2], University of Toronto NUMAchine <ref> [19] </ref>, and Sun's fl This work was supported in part by NSF CAREER Award CCR-9502541, by the Army High Performance Computing Research Center, under the auspices of Army Research Office contract number DAAL03-89-C-0038 with the University of Minnesota, and by the U.S.
References-found: 19

