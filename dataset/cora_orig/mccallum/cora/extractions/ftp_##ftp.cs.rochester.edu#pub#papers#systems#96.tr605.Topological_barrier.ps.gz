URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/96.tr605.Topological_barrier.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/scott/synchronization/pubs.html
Root-URL: 
Email: fscott,michaelg@cs.rochester.edu  
Title: The Topological Barrier: A Synchronization Abstraction for Regularly-Structured Parallel Applications  
Author: Michael L. Scott and Maged M. Michael 
Keyword: barriers, synchronization, abstraction, communication topology  
Date: 1996  
Note: January  This work was supported in part by NSF grants nos. CDA-8822724 and CCR-9319445, and by ONR research grant no. N00014-92-J-1801 (in conjunction with the DARPA Research in Information Science and Technology| High Performance Computing, Software Science and Technology program, ARPA Order no. 8930).  
Address: Rochester, NY 14627-0226  
Affiliation: Department of Computer Science University of Rochester  
Abstract: Barriers are a simple, widely-used technique for synchronization in parallel applications. In regularly-structured programs, however, barriers can overly-constrain execution by forcing synchronization among processes that do not really share data. The topological barrier preserves the simplicity of traditional barriers while performing the minimum amount of synchronization actually required by the application. Topological barriers can easily be retro-fitted into existing programs. The only new burden on the programmer is the construction of a pair of functions to count and enumerate the neighbors of a given process. We describe the topological barrier in pseudo-code and pictures, and illustrate its performance on a pair of applications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Bailey, J. Barton, T. Lasinski, and H. Simon. </author> <title> The NAS Parallel Benchmarks. </title> <type> Report RNR-91-002, </type> <institution> NASA Ames Research Center, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: j's first neighbor. num_neighbors (p : proc) if p in -1, N, N*(N-1)+1, N*N return 2 if p &lt; N or p &gt; N*(N-1) or (p-1) % N in -0, N-1 return 3 return 4 enumerate_neighbors (p : proc; var list : array of proc) if p = 1 list <ref> [1] </ref> := 2; list [2] := N+1 elsif p = N list [1] := N-1; list [2] := N*2 elsif p = N*(N-1)+1 list [1] := N*(N-2)+1; list [2] := N*(N-1)+2 elsif p = N*N list [1] := N*(N-1); list [2] := N*N-1 elsif p &lt; N list [1] := p-1; <p> N*(N-1)+1, N*N return 2 if p &lt; N or p &gt; N*(N-1) or (p-1) % N in -0, N-1 return 3 return 4 enumerate_neighbors (p : proc; var list : array of proc) if p = 1 list <ref> [1] </ref> := 2; list [2] := N+1 elsif p = N list [1] := N-1; list [2] := N*2 elsif p = N*(N-1)+1 list [1] := N*(N-2)+1; list [2] := N*(N-1)+2 elsif p = N*N list [1] := N*(N-1); list [2] := N*N-1 elsif p &lt; N list [1] := p-1; list [2] := p+1; list [3] := p+N elsif (p-1) % N <p> or (p-1) % N in -0, N-1 return 3 return 4 enumerate_neighbors (p : proc; var list : array of proc) if p = 1 list <ref> [1] </ref> := 2; list [2] := N+1 elsif p = N list [1] := N-1; list [2] := N*2 elsif p = N*(N-1)+1 list [1] := N*(N-2)+1; list [2] := N*(N-1)+2 elsif p = N*N list [1] := N*(N-1); list [2] := N*N-1 elsif p &lt; N list [1] := p-1; list [2] := p+1; list [3] := p+N elsif (p-1) % N = 0 list [1] := p-N; list [2] := p+N; list [3] <p> (p : proc; var list : array of proc) if p = 1 list <ref> [1] </ref> := 2; list [2] := N+1 elsif p = N list [1] := N-1; list [2] := N*2 elsif p = N*(N-1)+1 list [1] := N*(N-2)+1; list [2] := N*(N-1)+2 elsif p = N*N list [1] := N*(N-1); list [2] := N*N-1 elsif p &lt; N list [1] := p-1; list [2] := p+1; list [3] := p+N elsif (p-1) % N = 0 list [1] := p-N; list [2] := p+N; list [3] := p+1 elsif (p-1) % N = N-1 list [1] := p-N; <p> 1 list <ref> [1] </ref> := 2; list [2] := N+1 elsif p = N list [1] := N-1; list [2] := N*2 elsif p = N*(N-1)+1 list [1] := N*(N-2)+1; list [2] := N*(N-1)+2 elsif p = N*N list [1] := N*(N-1); list [2] := N*N-1 elsif p &lt; N list [1] := p-1; list [2] := p+1; list [3] := p+N elsif (p-1) % N = 0 list [1] := p-N; list [2] := p+N; list [3] := p+1 elsif (p-1) % N = N-1 list [1] := p-N; list [2] := p+N; list [3] := p-1 elsif p &gt; N*(N-1) <p> [2] := N*2 elsif p = N*(N-1)+1 list <ref> [1] </ref> := N*(N-2)+1; list [2] := N*(N-1)+2 elsif p = N*N list [1] := N*(N-1); list [2] := N*N-1 elsif p &lt; N list [1] := p-1; list [2] := p+1; list [3] := p+N elsif (p-1) % N = 0 list [1] := p-N; list [2] := p+N; list [3] := p+1 elsif (p-1) % N = N-1 list [1] := p-N; list [2] := p+N; list [3] := p-1 elsif p &gt; N*(N-1) list [1] := p-1; list [2] := p+1; list [3] := p-N else list [1] := p-1; list <p> N*N list <ref> [1] </ref> := N*(N-1); list [2] := N*N-1 elsif p &lt; N list [1] := p-1; list [2] := p+1; list [3] := p+N elsif (p-1) % N = 0 list [1] := p-N; list [2] := p+N; list [3] := p+1 elsif (p-1) % N = N-1 list [1] := p-N; list [2] := p+N; list [3] := p-1 elsif p &gt; N*(N-1) list [1] := p-1; list [2] := p+1; list [3] := p-N else list [1] := p-1; list [2] := p+1; list [3] := p-N; list [3] := p+N needs to be written for each different <p> p-1; list [2] := p+1; list [3] := p+N elsif (p-1) % N = 0 list <ref> [1] </ref> := p-N; list [2] := p+N; list [3] := p+1 elsif (p-1) % N = N-1 list [1] := p-N; list [2] := p+N; list [3] := p-1 elsif p &gt; N*(N-1) list [1] := p-1; list [2] := p+1; list [3] := p-N else list [1] := p-1; list [2] := p+1; list [3] := p-N; list [3] := p+N needs to be written for each different sharing topology. 5 Absolute times on one processor are the same for all three barriers. 3 <p> = 0 list <ref> [1] </ref> := p-N; list [2] := p+N; list [3] := p+1 elsif (p-1) % N = N-1 list [1] := p-N; list [2] := p+N; list [3] := p-1 elsif p &gt; N*(N-1) list [1] := p-1; list [2] := p+1; list [3] := p-N else list [1] := p-1; list [2] := p+1; list [3] := p-N; list [3] := p+N needs to be written for each different sharing topology. 5 Absolute times on one processor are the same for all three barriers. 3 Experimental Results To verify the usefulness of the topological barrier abstraction, we retro-fitted <p> We used a small (100 fi 100) grid, to keep the number of rows per processor small and highlight the impact of synchronization. Mgrid is a simplified shared-memory version of the multigrid kernel from the NAS Parallel Benchmarks <ref> [1] </ref>. It performs a more elaborate over-relaxation using multi-grid techniques to compute an approximate solution to the Poisson equation on the unit cube. We ran 10 iterations, with 100 relaxation steps in each iteration, and a grid size of 8 fi 8 fi 120.
Reference: [2] <author> R. Gupta. </author> <title> The Fuzzy Barrier: A Mechanism for High Speed Synchronization of Processors. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 54-63, </pages> <address> Boston, MA, </address> <month> April </month> <year> 1989. </year>
Reference-contexts: To prevent a process from proceeding past a barrier and over-writing its neighbor's flag variables before that neighbor has had a chance to notice them, we alternate use of two different sets of flags in consecutive barrier episodes. Gupta <ref> [2] </ref> has noted that in many applications there is work between iterations that is neither required by neighbors in the next iteration, nor dependent on the work of neighbors in the iteration just completed. <p> (p : proc) if p in -1, N, N*(N-1)+1, N*N return 2 if p &lt; N or p &gt; N*(N-1) or (p-1) % N in -0, N-1 return 3 return 4 enumerate_neighbors (p : proc; var list : array of proc) if p = 1 list [1] := 2; list <ref> [2] </ref> := N+1 elsif p = N list [1] := N-1; list [2] := N*2 elsif p = N*(N-1)+1 list [1] := N*(N-2)+1; list [2] := N*(N-1)+2 elsif p = N*N list [1] := N*(N-1); list [2] := N*N-1 elsif p &lt; N list [1] := p-1; list [2] := p+1; <p> if p &lt; N or p &gt; N*(N-1) or (p-1) % N in -0, N-1 return 3 return 4 enumerate_neighbors (p : proc; var list : array of proc) if p = 1 list [1] := 2; list <ref> [2] </ref> := N+1 elsif p = N list [1] := N-1; list [2] := N*2 elsif p = N*(N-1)+1 list [1] := N*(N-2)+1; list [2] := N*(N-1)+2 elsif p = N*N list [1] := N*(N-1); list [2] := N*N-1 elsif p &lt; N list [1] := p-1; list [2] := p+1; list [3] := p+N elsif (p-1) % N = 0 list [1] <p> in -0, N-1 return 3 return 4 enumerate_neighbors (p : proc; var list : array of proc) if p = 1 list [1] := 2; list <ref> [2] </ref> := N+1 elsif p = N list [1] := N-1; list [2] := N*2 elsif p = N*(N-1)+1 list [1] := N*(N-2)+1; list [2] := N*(N-1)+2 elsif p = N*N list [1] := N*(N-1); list [2] := N*N-1 elsif p &lt; N list [1] := p-1; list [2] := p+1; list [3] := p+N elsif (p-1) % N = 0 list [1] := p-N; list [2] := p+N; list [3] := p+1 elsif (p-1) <p> list : array of proc) if p = 1 list [1] := 2; list <ref> [2] </ref> := N+1 elsif p = N list [1] := N-1; list [2] := N*2 elsif p = N*(N-1)+1 list [1] := N*(N-2)+1; list [2] := N*(N-1)+2 elsif p = N*N list [1] := N*(N-1); list [2] := N*N-1 elsif p &lt; N list [1] := p-1; list [2] := p+1; list [3] := p+N elsif (p-1) % N = 0 list [1] := p-N; list [2] := p+N; list [3] := p+1 elsif (p-1) % N = N-1 list [1] := p-N; list [2] := p+N; <p> 2; list <ref> [2] </ref> := N+1 elsif p = N list [1] := N-1; list [2] := N*2 elsif p = N*(N-1)+1 list [1] := N*(N-2)+1; list [2] := N*(N-1)+2 elsif p = N*N list [1] := N*(N-1); list [2] := N*N-1 elsif p &lt; N list [1] := p-1; list [2] := p+1; list [3] := p+N elsif (p-1) % N = 0 list [1] := p-N; list [2] := p+N; list [3] := p+1 elsif (p-1) % N = N-1 list [1] := p-N; list [2] := p+N; list [3] := p-1 elsif p &gt; N*(N-1) list [1] := p-1; <p> p = N*(N-1)+1 list [1] := N*(N-2)+1; list <ref> [2] </ref> := N*(N-1)+2 elsif p = N*N list [1] := N*(N-1); list [2] := N*N-1 elsif p &lt; N list [1] := p-1; list [2] := p+1; list [3] := p+N elsif (p-1) % N = 0 list [1] := p-N; list [2] := p+N; list [3] := p+1 elsif (p-1) % N = N-1 list [1] := p-N; list [2] := p+N; list [3] := p-1 elsif p &gt; N*(N-1) list [1] := p-1; list [2] := p+1; list [3] := p-N else list [1] := p-1; list [2] := p+1; list <p> N*(N-1); list <ref> [2] </ref> := N*N-1 elsif p &lt; N list [1] := p-1; list [2] := p+1; list [3] := p+N elsif (p-1) % N = 0 list [1] := p-N; list [2] := p+N; list [3] := p+1 elsif (p-1) % N = N-1 list [1] := p-N; list [2] := p+N; list [3] := p-1 elsif p &gt; N*(N-1) list [1] := p-1; list [2] := p+1; list [3] := p-N else list [1] := p-1; list [2] := p+1; list [3] := p-N; list [3] := p+N needs to be written for each different sharing topology. 5 Absolute <p> p+1; list [3] := p+N elsif (p-1) % N = 0 list [1] := p-N; list <ref> [2] </ref> := p+N; list [3] := p+1 elsif (p-1) % N = N-1 list [1] := p-N; list [2] := p+N; list [3] := p-1 elsif p &gt; N*(N-1) list [1] := p-1; list [2] := p+1; list [3] := p-N else list [1] := p-1; list [2] := p+1; list [3] := p-N; list [3] := p+N needs to be written for each different sharing topology. 5 Absolute times on one processor are the same for all three barriers. 3 Experimental Results To verify <p> := p-N; list <ref> [2] </ref> := p+N; list [3] := p+1 elsif (p-1) % N = N-1 list [1] := p-N; list [2] := p+N; list [3] := p-1 elsif p &gt; N*(N-1) list [1] := p-1; list [2] := p+1; list [3] := p-N else list [1] := p-1; list [2] := p+1; list [3] := p-N; list [3] := p+N needs to be written for each different sharing topology. 5 Absolute times on one processor are the same for all three barriers. 3 Experimental Results To verify the usefulness of the topological barrier abstraction, we retro-fitted a pair of regularly-structured
Reference: [3] <author> L. I. Kontothanassis and M. L. Scott. </author> <title> Using Memory-Mapped Network Interfaces to Improve the Performance of Distributed Shared Memory. </title> <booktitle> In Proceedings of the Second International Symposium on High Performance Computer Architecture, </booktitle> <address> San Jose, CA, </address> <month> February </month> <year> 1996. </year> <title> Earlier version available as "Distributed Shared Memory for New Generation Networks," </title> <type> TR 578, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: Clearly nothing prevents the programmer from implementing the minimum amount of synchronization required by the application. Barriers are attractive, however, even if they over-synchronize, because they are so simple. As part of some recent experiments in software-managed cache coherence <ref> [3] </ref>, we re-wrote a banded implementation of successive over-relaxation to use locks on boundary rows, rather than barriers, to synchronize between iterations. <p> N+1 elsif p = N list [1] := N-1; list [2] := N*2 elsif p = N*(N-1)+1 list [1] := N*(N-2)+1; list [2] := N*(N-1)+2 elsif p = N*N list [1] := N*(N-1); list [2] := N*N-1 elsif p &lt; N list [1] := p-1; list [2] := p+1; list <ref> [3] </ref> := p+N elsif (p-1) % N = 0 list [1] := p-N; list [2] := p+N; list [3] := p+1 elsif (p-1) % N = N-1 list [1] := p-N; list [2] := p+N; list [3] := p-1 elsif p &gt; N*(N-1) list [1] := p-1; list [2] := p+1; <p> [1] := N*(N-2)+1; list [2] := N*(N-1)+2 elsif p = N*N list [1] := N*(N-1); list [2] := N*N-1 elsif p &lt; N list [1] := p-1; list [2] := p+1; list <ref> [3] </ref> := p+N elsif (p-1) % N = 0 list [1] := p-N; list [2] := p+N; list [3] := p+1 elsif (p-1) % N = N-1 list [1] := p-N; list [2] := p+N; list [3] := p-1 elsif p &gt; N*(N-1) list [1] := p-1; list [2] := p+1; list [3] := p-N else list [1] := p-1; list [2] := p+1; list [3] := p-N; list <p> N*N-1 elsif p &lt; N list [1] := p-1; list [2] := p+1; list <ref> [3] </ref> := p+N elsif (p-1) % N = 0 list [1] := p-N; list [2] := p+N; list [3] := p+1 elsif (p-1) % N = N-1 list [1] := p-N; list [2] := p+N; list [3] := p-1 elsif p &gt; N*(N-1) list [1] := p-1; list [2] := p+1; list [3] := p-N else list [1] := p-1; list [2] := p+1; list [3] := p-N; list [3] := p+N needs to be written for each different sharing topology. 5 Absolute times on one processor <p> p+N elsif (p-1) % N = 0 list [1] := p-N; list [2] := p+N; list <ref> [3] </ref> := p+1 elsif (p-1) % N = N-1 list [1] := p-N; list [2] := p+N; list [3] := p-1 elsif p &gt; N*(N-1) list [1] := p-1; list [2] := p+1; list [3] := p-N else list [1] := p-1; list [2] := p+1; list [3] := p-N; list [3] := p+N needs to be written for each different sharing topology. 5 Absolute times on one processor are the same for all three barriers. 3 Experimental Results To verify the usefulness of the <p> := p+N; list <ref> [3] </ref> := p+1 elsif (p-1) % N = N-1 list [1] := p-N; list [2] := p+N; list [3] := p-1 elsif p &gt; N*(N-1) list [1] := p-1; list [2] := p+1; list [3] := p-N else list [1] := p-1; list [2] := p+1; list [3] := p-N; list [3] := p+N needs to be written for each different sharing topology. 5 Absolute times on one processor are the same for all three barriers. 3 Experimental Results To verify the usefulness of the topological barrier abstraction, we retro-fitted a pair of regularly-structured applications|SOR and Mgrid|and measured <p> := p+1 elsif (p-1) % N = N-1 list [1] := p-N; list [2] := p+N; list <ref> [3] </ref> := p-1 elsif p &gt; N*(N-1) list [1] := p-1; list [2] := p+1; list [3] := p-N else list [1] := p-1; list [2] := p+1; list [3] := p-N; list [3] := p+N needs to be written for each different sharing topology. 5 Absolute times on one processor are the same for all three barriers. 3 Experimental Results To verify the usefulness of the topological barrier abstraction, we retro-fitted a pair of regularly-structured applications|SOR and Mgrid|and measured their performance on a
Reference: [4] <author> J. M. Mellor-Crummey and M. L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: We ran the applications with the topological barrier, the native (SGI library) barrier, and a tree-based barrier known to provide excellent performance and to scale well to large machines <ref> [4] </ref>. Speedup graphs appear in figure 4. Absolute running times on one processor were essentially the same for all three barriers. SOR computes the steady state temperature of a metal sheet using a banded parallelization of red-black successive over-relaxation.
Reference: [5] <author> S. P. Midkiff and D. A. Padua. </author> <title> Compiler Algorithms for Synchronization. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12), </volume> <month> December </month> <year> 1987. </year> <month> 7 </month>
Reference-contexts: The extra code is not subtle: just tedious. It could be incorporated easily in programs generated by parallelizing compilers <ref> [5] </ref>. For programs written by human beings, however, it is a major nuisance. What we need for hand-written programs is a programming abstraction that preserves the simplicity of barriers from the programmer's point of view, while performing the minimum amount of synchronization necessary in a given application.
References-found: 5

