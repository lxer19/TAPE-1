URL: http://www.cs.uni-bonn.de/III/lehre/vorlesungen/InformationRetrieval/WS96/Harvest95.ps.gz
Refering-URL: http://www.cs.uni-bonn.de/III/lehre/vorlesungen/InformationRetrieval/WS96/
Root-URL: http://cs.uni-bonn.de
Title: Harvest: A Scalable, Customizable Discovery and Access System  
Author: C. Mic Bowman Peter B. Danzig Darren R. Hardy Udi Manber Michael F. Schwartz Duane P. Wessels 
Date: March 12, 1995  
Address: Boulder  Boulder  Boulder  
Affiliation: Transarc Corp.  University of Southern California  University of Colorado  University of Arizona  University of Colorado  University of Colorado  
Abstract: Technical Report CU-CS-732-94 Department of Computer Science University of Colorado Boulder (Original Date: August 1994; Revised March 1995) Abstract Rapid growth in data volume, user base, and data diversity render Internet-accessible information increasingly difficult to use effectively. In this paper we introduce Harvest, a system that provides an integrated set of customizable tools for gathering information from diverse repositories, building topic-specific content indexes, flexibly searching the indexes, widely replicating them, and caching objects as they are retrieved across the Internet. The system interoperates with WWW clients and with HTTP, FTP, Gopher, and NetNews information resources. We discuss the design and implementation of Harvest and its subsystems, give examples of its uses, and provide measurements indicating that Harvest can significantly reduce server load, network traffic, and space requirements when building indexes, compared with previous systems. We also discuss several popular indexes we have built using Harvest, underscoring the customizability and scalability of the system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Marc Andreessen. </author> <title> NCSA Mosaic technical summary. </title> <type> Technical report, </type> <month> May </month> <year> 1993. </year> <note> Available from ftp://zaphod.ncsa.uiuc.edu/Web/mosaic-papers/mosaic.ps.Z. 25 </note>
Reference-contexts: Until 1992, FTP [45] and NetNews [42] were the principal publishing tools. Around 1992, Gopher [39] and WAIS [31] gained popularity because they simplified network interactions and provided better ways to navigate through information. With the introduction of Mosaic <ref> [1] </ref> in 1993, publishing information on the World Wide Web [2] gained widespread use, because of Mosaic's attractive graphical interface and ease of use for accessing multimedia data reachable via WWW links. While Internet publishing has become easy and popular, making effective use of Internet-accessible information has become more difficult.
Reference: [2] <author> T. Berners-Lee, R. Cailliau, J-F. Groff, and B. Pollermann. </author> <title> World-Wide Web: The information universe. </title> <journal> Elec--tronic Networking: Research, Applications and Policy, </journal> <volume> 1(2) </volume> <pages> 52-58, </pages> <month> Spring </month> <year> 1992. </year> <note> Available from ftp://ftp.cern.ch /pub/www/doc/ENRAP 9202.ps. </note>
Reference-contexts: Until 1992, FTP [45] and NetNews [42] were the principal publishing tools. Around 1992, Gopher [39] and WAIS [31] gained popularity because they simplified network interactions and provided better ways to navigate through information. With the introduction of Mosaic [1] in 1993, publishing information on the World Wide Web <ref> [2] </ref> gained widespread use, because of Mosaic's attractive graphical interface and ease of use for accessing multimedia data reachable via WWW links. While Internet publishing has become easy and popular, making effective use of Internet-accessible information has become more difficult.
Reference: [3] <author> Tim Berners-Lee. </author> <title> Uniform Resource Locators. </title> <publisher> CERN, </publisher> <month> July </month> <year> 1993. </year> <type> Internet Draft, </type> <institution> IETF URL Working Group. </institution>
Reference-contexts: It delineates streams of object summaries, and allows for multiple levels of nested detail. SOIF is based on a combination of the Internet Anonymous FTP Archives (IAFA) IETF Working Group templates [19] and BibTeX [34]. Each template contains a type, a Uniform Resource Locator (URL) <ref> [3] </ref>, and a list of byte-count delimited attribute-value pairs. We define a set of mandatory and recommended attributes for Harvest system components. For example, attributes for a Broker describe the server's administrator, location, software version, and the type of objects it contains.
Reference: [4] <author> Kenneth P. Birman. </author> <title> Replication and fault-tolerance in the Isis system. </title> <booktitle> Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 79-86, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: Caching and Replication A great deal of caching and replication research has been carried out in the operating systems community over the past 20 years (e.g., the Andrew File System [29] and ISIS <ref> [4] </ref>). More recently, a number of efforts have begun to build object caches into HTTP servers (e.g., Lagoon [12]), and to support replication [24] in Internet information systems such as Archie.
Reference: [5] <author> Andrew D. Birrell, Roy Levin, Roger M. Needham, and Michael D. Schroeder. Grapevine: </author> <title> An exercise in distributed computing. </title> <journal> Communications of the ACM, </journal> <volume> 25(4) </volume> <pages> 260-274, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: We are currently working on several tools to allow much greater customizations. 8 The Replication Subsystem Harvest replicates its Brokers with the mirror-d replication tool. Mirror-d maintains a weakly consistent, replicated directory tree of files. Mirror-d implements eventual consistency <ref> [5] </ref>: if all new updates cease, the replicas eventually converge. Each replica belongs to one or more data propagation groups and, within a group, mirror-d's companion process, flood-d [15], estimates the available network bandwidth and observed round trip delay between each pair of replicas.
Reference: [6] <author> David C. Blair and M. E. Maron. </author> <title> An evaluation of retrieval effectiveness for a full-text document-retrieval system. </title> <journal> Communications of the ACM, </journal> <volume> 28(3) </volume> <pages> 289-299, </pages> <month> March </month> <year> 1985. </year>
Reference-contexts: records a time-to-live value and unique object identifier (OID) consisting of the object's URL plus information about the Gatherer that generated 8 Intuitively, precision is the probability that all of the retrieved documents will be relevant, while recall is the probability that all of the relevant documents will be retrieved <ref> [6] </ref>. 12 the summary object. The Storage Manager archives a collection of summary objects on disk, storing each as a file in the underlying file system. The Broker also eliminates duplicate objects (e.g., those reached via multiple HTML pointers) based on a combination of MD5 signatures and Gatherer IDs.
Reference: [7] <author> Nathaniel Borenstein and Ned Freed. </author> <title> RFC 1521: MIME (Multipurpose Internet Mail Extensions) part one: Mechanisms for specifying and describing the format of internet message bodies. </title> <type> Technical report, </type> <month> September </month> <year> 1993. </year>
Reference-contexts: We also cache negative responses, so that temporarily unavailable servers are not repeatedly contacted. Currently, Web cache consistency is not altogether satisfactory, since no standard MIME header <ref> [7] </ref> exists that defines an object time-to-live [18]. Mosaic and Netscape cache images and Netscape caches objects, but verifying that an object is consistent requires nearly the same effort as retriving the object again.
Reference: [8] <author> C. Mic Bowman, Peter B. Danzig, Darren R. Hardy, Udi Manber, and Michael F. Schwartz. </author> <title> The Harvest information discovery and access system. </title> <booktitle> Proceedings of the Second International World Wide Web Conference, </booktitle> <pages> pages 763-771, </pages> <month> October </month> <year> 1994. </year> <note> Available from ftp://ftp.cs.colorado.edu/pub/cs/techreports/schwartz /Harvest.Conf.ps.Z or ftp://ftp.cs.colorado.edu/pub/cs/techreports/schwartz/Harvest.Conf.txt.Z. </note>
Reference: [9] <author> C. Mic Bowman, Peter B. Danzig, Udi Manber, and Michael F. Schwartz. </author> <title> Scalable Internet resource discovery: Research problems and approaches. </title> <journal> Communications of the ACM, </journal> <volume> 37(8) </volume> <pages> 98-107, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: Finally, current systems primarily support text and graphics intended for end user viewing; they provide little support for more complex data, as might be found in a digital library. For a more detailed discussion of these problems, the reader is referred to <ref> [9] </ref>. In this paper we discuss a system that addresses these problems using a variety of techniques. We call the system Harvest, to connote its focus on reaping the growing collection of Internet information.
Reference: [10] <author> C. Mic Bowman, Chanda Dharap, Mrinal Baruah, Bill Camargo, and Sunil Potti. </author> <title> A file system for information management. </title> <booktitle> Proceedings of the Conference on Intelligent Information Management Systems, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: We have developed two particular search and index subsystems for Harvest, each optimized for different uses. Glimpse [36] supports space-efficient indexes and flexible interactive queries, while 13 Nebula <ref> [10] </ref> supports fast searches and views based on complex standing queries that scan the data on a regular basis and extract relevant information. <p> We are doing this initially in the context of Network Time Protocol servers [26], but in time we will extend this system for use with locating nearby Cache and Replica servers as well. We are developing tools similar to the Nebula information management system <ref> [10] </ref> to improve searching capabilities. These include recursive query evaluation, iterative query refinement, and integrated support for taxonomies. Recursive query evaluation enables automatic search of several servers located in the HSR. This relieves from the user the burden of manually guiding the search to several candidate Brokers.
Reference: [11] <author> Mic Bowman, Larry L. Peterson, and Andrey Yeatts. Univers: </author> <title> An attribute-based name server. </title> <journal> Software Practice & Experience, </journal> <volume> 20(4) </volume> <pages> 403-424, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Many of the ideas and some of the code for Harvest were derived from our previous work on the agrep string search tool [50], the Essence customized information extraction system [28], the Indie distributed indexing system [17], and the Univers attribute-based name service <ref> [11] </ref>. Caching and Replication A great deal of caching and replication research has been carried out in the operating systems community over the past 20 years (e.g., the Andrew File System [29] and ISIS [4]).
Reference: [12] <author> Paul M. E. De Bra and Reiner D. J. Post. </author> <title> Information Retrieval in the World-Wide Web: Making Client-based searching feasible. </title> <institution> Eindhoven University of Technology. </institution> <note> Available from http://www.win.tue.nl/win/cs/is /reinpost/www94/www94.html. </note>
Reference-contexts: More recently, a number of efforts have begun to build object caches into HTTP servers (e.g., Lagoon <ref> [12] </ref>), and to support replication [24] in Internet information systems such as Archie. In contrast to the flat organization of existing Internet caches, Harvest supports a hierarchical arrangement of object caches, modeled after the Domain Naming System's caching architecture.
Reference: [13] <author> Anawat Chankthod, Peter B. Danzig, Chuck Neerdales, Michael F. Schwartz, and Kurt Worrell. </author> <title> A hierarchical Internet object cache. </title> <booktitle> In prepartion, </booktitle> <year> 1995. </year>
Reference-contexts: of Southern California, the ten laboratory machines mirror off of each other, and one USC lab machine mirrors off of a machine at the University of Colorado. 9 The Object Caching Subsystem To meet ever-increasing demand on network links and information servers, Harvest includes a hierarchically organized Object Caching subsystem <ref> [13] </ref>, as illustrated in Figure 8. At each level there can be several neighbor caches, allowing some load-sharing among cache servers. 18 Below, we describe the resolution policy, implementation philosophy, and performance of the Object Caching subsystem.
Reference: [14] <author> Bhavna Chhabra, Darren R. Hardy, Allan Hundhausen, Dave Merkel, John Noble, and Michael F. Schwartz. </author> <title> Integrating complex data access methods into the Mosaic/WWW environment. </title> <booktitle> Proceedings of the Second International World Wide Web Conference, </booktitle> <pages> pages 909-919, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: For this purpose we will allow organizations to create SOIF descriptions of objects, including types and methods that can be invoked either by fetching executable code to the local machine or by invoking remote TCP services <ref> [14] </ref>. A customized, Mosaic-invokable graphical object browser will allow users to interact with these objects by selecting among available methods, invoking methods, and saving intermediate objects.
Reference: [15] <author> Peter Danzig, Katia Obraczka, Dante DeLucia, and Naveed Alam. </author> <title> Massively replicating services in autonomously managed wide-area internetworks. </title> <type> Technical report, </type> <month> January </month> <year> 1994. </year> <note> Available from ftp://catarina.usc.edu/pub /kobraczk/ToN.ps.Z. </note>
Reference-contexts: Mirror-d maintains a weakly consistent, replicated directory tree of files. Mirror-d implements eventual consistency [5]: if all new updates cease, the replicas eventually converge. Each replica belongs to one or more data propagation groups and, within a group, mirror-d's companion process, flood-d <ref> [15] </ref>, estimates the available network bandwidth and observed round trip delay between each pair of replicas. A group master periodically computes a graph that specifies which replicas should synchronize and propagate updates among each other. We call this the group's logical topology.
Reference: [16] <author> Peter B. Danzig, Richard S. Hall, and Michael F. Schwartz. </author> <title> A case for caching file objects inside internetworks. </title> <booktitle> Proceedings of the SIGCOMM '93, </booktitle> <pages> pages 239-248, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: In contrast to the flat organization of existing Internet caches, Harvest supports a hierarchical arrangement of object caches, modeled after the Domain Naming System's caching architecture. We believe this is the most appropriate arrangement because of a simulation study we performed using NSFNET backbone trace data <ref> [16] </ref>. We also note that one of the biggest scaling problems facing the Andrew File System is its callback-based invalidation protocol, which would be reduced if caches were arranged hierarchically.
Reference: [17] <author> Peter B. Danzig, Shih-Hao Li, and Katia Obraczka. </author> <title> Distributed indexing of autonomous internet services. </title> <journal> Computing Systems, </journal> <volume> 5(4) </volume> <pages> 433-459, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: Many of the ideas and some of the code for Harvest were derived from our previous work on the agrep string search tool [50], the Essence customized information extraction system [28], the Indie distributed indexing system <ref> [17] </ref>, and the Univers attribute-based name service [11]. Caching and Replication A great deal of caching and replication research has been carried out in the operating systems community over the past 20 years (e.g., the Andrew File System [29] and ISIS [4]).
Reference: [18] <author> Peter B. Danzig, Katia Obraczka, and Anant Kumar. </author> <title> An analysis of wide-area name server traffic | a study of the domain name system. </title> <booktitle> Proceedings of the SIGCOMM Symposium, </booktitle> <pages> pages 281-292, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: We also cache negative responses, so that temporarily unavailable servers are not repeatedly contacted. Currently, Web cache consistency is not altogether satisfactory, since no standard MIME header [7] exists that defines an object time-to-live <ref> [18] </ref>. Mosaic and Netscape cache images and Netscape caches objects, but verifying that an object is consistent requires nearly the same effort as retriving the object again.
Reference: [19] <author> Peter Deutsch and Alan Emtage. </author> <title> Publishing Information on the Internet with Anonymous FTP. </title> <institution> Bunyip Information Systems Inc., </institution> <month> May </month> <year> 1994. </year> <note> Available from ftp://nri.reston.va.us/internet-drafts/draft-ietf-iiir-publishing-01.txt. 26 </note>
Reference-contexts: It delineates streams of object summaries, and allows for multiple levels of nested detail. SOIF is based on a combination of the Internet Anonymous FTP Archives (IAFA) IETF Working Group templates <ref> [19] </ref> and BibTeX [34]. Each template contains a type, a Uniform Resource Locator (URL) [3], and a list of byte-count delimited attribute-value pairs. We define a set of mandatory and recommended attributes for Harvest system components.
Reference: [20] <author> Alan Emtage and Peter Deutsch. </author> <title> Archie an electronic directory service for the Internet. </title> <booktitle> Proceedings of the USENIX Winter Conference, </booktitle> <pages> pages 93-110, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Many of the early indexing tools fall into one of two categories: file name or menu name indexes of widely distributed information (such as Archie <ref> [20] </ref>, Veronica [21], or WWWW [38]); and full content indexes of individual databases (such as Gifford's Semantic File System [23], WAIS [31], and local Gopher [39] indexes). Name-only indexes are very space efficient, but support limited queries.
Reference: [21] <author> Steve Foster. </author> <title> About the Veronica service, November 1992. </title> <journal> Electronic bulletin board posting on the comp.infosystems.gopher newsgroup. </journal>
Reference-contexts: Many of the early indexing tools fall into one of two categories: file name or menu name indexes of widely distributed information (such as Archie [20], Veronica <ref> [21] </ref>, or WWWW [38]); and full content indexes of individual databases (such as Gifford's Semantic File System [23], WAIS [31], and local Gopher [39] indexes). Name-only indexes are very space efficient, but support limited queries.
Reference: [22] <author> G. W. Furnas, Thomas K. Landauer, L. M. Gomez, and S. T. Dumais. </author> <title> The vocabulary problem in human-system communication. </title> <journal> Communications of the ACM, </journal> <volume> 30(11) </volume> <pages> 964-971, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: For example, one Broker might index scientific papers for microbiologists, while another Broker indexes PC software archives. By focusing index contents per topic and per community, a Broker can avoid many of the vocabulary <ref> [22] </ref> and scaling problems of unfocused global indexes (such as Archie and WWWW). Harvest includes a distinguished Broker called the Harvest Server Registry (HSR), which allows users to register information about each Harvest Gatherer, Broker, Cache, and Replicator in the Internet.
Reference: [23] <author> David K. Gifford, P. Jouvelot, Mark A. Sheldon, and Jr. James W. O'Toole. </author> <title> Semantic file systems. </title> <booktitle> Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 16-25, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Many of the early indexing tools fall into one of two categories: file name or menu name indexes of widely distributed information (such as Archie [20], Veronica [21], or WWWW [38]); and full content indexes of individual databases (such as Gifford's Semantic File System <ref> [23] </ref>, WAIS [31], and local Gopher [39] indexes). Name-only indexes are very space efficient, but support limited queries. For example, it is only possible to query Archie for "graphics packages" whose file names 2 happen to reflect their contents.
Reference: [24] <author> Richard Golding and Darrell D. E. </author> <title> Long. Quorum-oriented multicast protocols for data replication. </title> <type> Technical report, </type> <month> June </month> <year> 1991. </year>
Reference-contexts: More recently, a number of efforts have begun to build object caches into HTTP servers (e.g., Lagoon [12]), and to support replication <ref> [24] </ref> in Internet information systems such as Archie. In contrast to the flat organization of existing Internet caches, Harvest supports a hierarchical arrangement of object caches, modeled after the Domain Naming System's caching architecture.
Reference: [25] <author> Object Management Group. </author> <title> Common Object Request Broker: Architecture and specification. </title> <type> Technical report, Framingham, </type> <institution> Massachusetts, </institution> <year> 1991. </year>
Reference-contexts: The database community has done a great deal of work with more structured distributed information systems, but to date has fielded no widespread systems on the Internet. Similarly, a number of object-oriented systems have been developed (e.g., CORBA <ref> [25] </ref> and OLE [30]), but have yet to be deployed on the Internet at large. At present, Harvest supports structured data through the use of attribute-value structured indexes.
Reference: [26] <author> James D. Guyton and Michael F. Schwartz. </author> <title> Experiences with a survey tool for discovering Network Time Protocol servers. </title> <booktitle> Proceedings of the USENIX Summer Conference, </booktitle> <pages> pages 257-265, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Clearly, there are several security and architecture-dependency issues involved with this approach, which we are still considering. We are also developing support for selecting among instances of a replicated service. We are doing this initially in the context of Network Time Protocol servers <ref> [26] </ref>, but in time we will extend this system for use with locating nearby Cache and Replica servers as well. We are developing tools similar to the Nebula information management system [10] to improve searching capabilities. These include recursive query evaluation, iterative query refinement, and integrated support for taxonomies.
Reference: [27] <author> Darren R. Hardy and Michael F. Schwartz. </author> <title> Harvest user's manual. </title> <type> Technical report, </type> <month> February </month> <year> 1995. </year> <note> Version 1.1. </note>
Reference-contexts: The HSR is useful when searching for an appropriate Broker, and when constructing new Gatherers and Brokers, to avoid duplication of effort. Gatherers and Brokers communicate using an attribute-value stream protocol called the Summary Object Interchange Format (SOIF) <ref> [27] </ref>, intended to be easily parsed yet sufficiently expressive to handle many kinds of objects. It delineates streams of object summaries, and allows for multiple levels of nested detail. SOIF is based on a combination of the Internet Anonymous FTP Archives (IAFA) IETF Working Group templates [19] and BibTeX [34].
Reference: [28] <author> Darren R. Hardy and Michael F. Schwartz. </author> <title> Customized information extraction as a basis for resource discovery. </title> <type> Technical Report CU-CS-707-94, </type> <month> March </month> <year> 1994. </year> <note> To appear, ACM Transactions on Computer Systems. </note>
Reference-contexts: Many of the ideas and some of the code for Harvest were derived from our previous work on the agrep string search tool [50], the Essence customized information extraction system <ref> [28] </ref>, the Indie distributed indexing system [17], and the Univers attribute-based name service [11]. Caching and Replication A great deal of caching and replication research has been carried out in the operating systems community over the past 20 years (e.g., the Andrew File System [29] and ISIS [4]). <p> This results in a huge reduction in server load. The combination of traversal caching and response streaming can reduce server load significantly. The Gatherer uses four techniques to reduce network traffic substantially. First, it uses the Essence system <ref> [28] </ref> to extract content summaries before passing the data to a remote Broker. Essence uses type-specific procedures to extract the most relevant parts of documents as content summaries | for example, extracting author and title information from LaTeX documents, and routine names from executable files. <p> Here we discuss the first three ways, since the cache is primarily needed for insulating the network from re-retrievals in case of a system crash. The savings from pre-extraction are derived from measurements we performed for an earlier paper about Essence <ref> [28] </ref>. We found that Essence content summaries require from 10-50 times less space than the data they summarize, depending on data type. <p> suspect that Essence's use of content summaries causes the resulting indexes to lose useful keywords, our measurements indicate that Essence achieves nearly the same precision 8 and 70% the recall of WAIS, but requires only 3-11% as much index space and 70% as much summarizing and indexing time as WAIS <ref> [28] </ref>. For users who need the added recall and precision, however, Essence also supports a full-text extraction option.
Reference: [29] <author> John Howard, Michael Kazar, Sherri Menees, David Nichols, M. Satyanarayanan, Robert Sidebotham, and Michael West. </author> <title> Scale and performance in a distributed file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 51-81, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Caching and Replication A great deal of caching and replication research has been carried out in the operating systems community over the past 20 years (e.g., the Andrew File System <ref> [29] </ref> and ISIS [4]). More recently, a number of efforts have begun to build object caches into HTTP servers (e.g., Lagoon [12]), and to support replication [24] in Internet information systems such as Archie.
Reference: [30] <author> Microsoft Inc. </author> <title> OLE 2.01 Design Specification. Microsoft OLE2 Design Team, </title> <month> September </month> <year> 1993. </year> <title> Describes the Object Linking & Embedding environment. </title>
Reference-contexts: The database community has done a great deal of work with more structured distributed information systems, but to date has fielded no widespread systems on the Internet. Similarly, a number of object-oriented systems have been developed (e.g., CORBA [25] and OLE <ref> [30] </ref>), but have yet to be deployed on the Internet at large. At present, Harvest supports structured data through the use of attribute-value structured indexes.
Reference: [31] <author> Brewster Kahle and Art Medlar. </author> <title> An information system for corporate users: Wide Area Information Servers. </title> <journal> ConneXions The Interoperability Report, </journal> <volume> 5(11) </volume> <pages> 2-9, </pages> <month> November </month> <year> 1991. </year> <note> Available from ftp://think.com/wais /wais-corporate-paper.text. </note>
Reference-contexts: 1 Introduction Over the past few years a progression of Internet publishing tools has appeared. Until 1992, FTP [45] and NetNews [42] were the principal publishing tools. Around 1992, Gopher [39] and WAIS <ref> [31] </ref> gained popularity because they simplified network interactions and provided better ways to navigate through information. <p> Many of the early indexing tools fall into one of two categories: file name or menu name indexes of widely distributed information (such as Archie [20], Veronica [21], or WWWW [38]); and full content indexes of individual databases (such as Gifford's Semantic File System [23], WAIS <ref> [31] </ref>, and local Gopher [39] indexes). Name-only indexes are very space efficient, but support limited queries. For example, it is only possible to query Archie for "graphics packages" whose file names 2 happen to reflect their contents.
Reference: [32] <author> Martijn Koster. </author> <title> Guidelines for robot writers. </title> <type> Technical report, </type> <year> 1994. </year> <note> Available from http://web.nexor.co.uk /mak/doc/robots/guidelines.html. </note>
Reference-contexts: However, Harvest can be used to build WHOIS++. Aliweb [33] collects site description templates formatted according to the WHOIS++ specification. An increasingly popular approach to resource discovery is the use of Web robots <ref> [32] </ref>. These are programs that attempt to locate a large number of WWW documents by recursively enumerating hypertext links starting with some known set of documents. <p> Data Collection Efficiency Koster offers a number of guidelines for constructing Web "robots" <ref> [32] </ref>. For example, he suggests using breadth-first rather than depth-first traversal, to minimize rapid-fire requests to a single server. These guidelines make a good deal of sense for uncoordinated information gathering. In contrast, Harvest focuses on coordinating and optimizing the gathering process.
Reference: [33] <author> Martijn Koster. </author> <title> Introduction to ALIWEB. </title> <publisher> NEXOR, Limited, </publisher> <year> 1994. </year> <note> Available from http://web.nexor.co.uk /public/aliweb/doc/introduction.html. </note>
Reference-contexts: In contrast to our approach, WHOIS++ does not provide an automatic data gathering architecture, nor many of the scaling features (such as caching and replication) that Harvest provides. However, Harvest can be used to build WHOIS++. Aliweb <ref> [33] </ref> collects site description templates formatted according to the WHOIS++ specification. An increasingly popular approach to resource discovery is the use of Web robots [32]. These are programs that attempt to locate a large number of WWW documents by recursively enumerating hypertext links starting with some known set of documents.
Reference: [34] <author> Leslie Lamport. </author> <title> LaTeX: A Document Prepartion System. </title> <publisher> Addison Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: It delineates streams of object summaries, and allows for multiple levels of nested detail. SOIF is based on a combination of the Internet Anonymous FTP Archives (IAFA) IETF Working Group templates [19] and BibTeX <ref> [34] </ref>. Each template contains a type, a Uniform Resource Locator (URL) [3], and a list of byte-count delimited attribute-value pairs. We define a set of mandatory and recommended attributes for Harvest system components.
Reference: [35] <author> Ari Luotonen, Henrik Frystyk, and Tim Berners-Lee. </author> <title> CERN HTTPD public domain full-featured hypertext/proxy server with caching, </title> <note> 1994. Available from http://info.cern.ch/hypertext/WWW/Daemon /Status.html. </note>
Reference-contexts: At each level there can be several neighbor caches, allowing some load-sharing among cache servers. 18 Below, we describe the resolution policy, implementation philosophy, and performance of the Object Caching subsystem. Hierarchical Caching Clients request objects through the Cache's proxy-HTTP interface, modeled after the CERN httpd cache <ref> [35] </ref>. The proxy interface allows Mosaic, Netscape and Lynx clients to use the cache simply by setting three environment variables.
Reference: [36] <author> Udi Manber and Sun Wu. Glimpse: </author> <title> A tool to search through entire file systems. </title> <booktitle> Proceedings of the USENIX Winter Conference, </booktitle> <pages> pages 23-32, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: One can therefore use a variety of different backends inside a Broker, such as Ingres or WAIS (although some versions of WAIS do not support all the needed features). We have developed two particular search and index subsystems for Harvest, each optimized for different uses. Glimpse <ref> [36] </ref> supports space-efficient indexes and flexible interactive queries, while 13 Nebula [10] supports fast searches and views based on complex standing queries that scan the data on a regular basis and extract relevant information.
Reference: [37] <author> MARBI, </author> <title> Network Development, and MARC Standards Office. The USMARC Formats: </title> <booktitle> Background and Principles. </booktitle> <year> 1989. </year>
Reference-contexts: A similar effort would allow us to incorporate other forms of information into Harvest Brokers, such as the U.S. Library of Congress Machine Readable card Catalog (MARC) standard <ref> [37] </ref>. Building Harvest Brokers Corresponding to Well-Known Existing Indexes One of the ideas behind Harvest is to provide a customizable system that can be configured in various ways to create many types of Brokers, to reduce the amount of effort that currently goes into building single-purpose indexers.
Reference: [38] <author> Oliver McBryan. </author> <title> Genvl and WWWW: Tools for taming the Web. </title> <booktitle> Proceedings of the First International World Wide Web Conference, </booktitle> <month> May </month> <year> 1994. </year> <note> Available from http://www.cs.colorado.edu/home/mcbryan/mypapers /www94.ps. 27 </note>
Reference-contexts: Many of the early indexing tools fall into one of two categories: file name or menu name indexes of widely distributed information (such as Archie [20], Veronica [21], or WWWW <ref> [38] </ref>); and full content indexes of individual databases (such as Gifford's Semantic File System [23], WAIS [31], and local Gopher [39] indexes). Name-only indexes are very space efficient, but support limited queries. <p> McBryan created an index (called the World Wide Web Worm or WWWW) of anchors and HTML links, by gathering documents from around the World Wide Web <ref> [38] </ref>. Using Harvest we created a related Broker, containing content summaries of Web home pages. <p> We began with a list of Web pointers from various sources, including the WWWW, the "What's New" pages maintained by the National Center for Supercomputing Applications and O'Reilly & Associates, Inc., a list of WWW servers maintained by the European Laboratory for Particle Physics (CERN), McBryan's Mother-of-all-BBS's <ref> [38] </ref>, URLs gleaned from USENET postings, and various other sources. We periodically collect this list, prune it to a set of home pages, gather these home pages, content summarize them, and index them.
Reference: [39] <author> Mark McCahill. </author> <title> The Internet Gopher: A distributed server information system. </title> <journal> ConneXions The Interoper--ability Report, </journal> <volume> 6(7) </volume> <pages> 10-14, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Over the past few years a progression of Internet publishing tools has appeared. Until 1992, FTP [45] and NetNews [42] were the principal publishing tools. Around 1992, Gopher <ref> [39] </ref> and WAIS [31] gained popularity because they simplified network interactions and provided better ways to navigate through information. <p> Many of the early indexing tools fall into one of two categories: file name or menu name indexes of widely distributed information (such as Archie [20], Veronica [21], or WWWW [38]); and full content indexes of individual databases (such as Gifford's Semantic File System [23], WAIS [31], and local Gopher <ref> [39] </ref> indexes). Name-only indexes are very space efficient, but support limited queries. For example, it is only possible to query Archie for "graphics packages" whose file names 2 happen to reflect their contents.
Reference: [40] <author> Lee McLoughlin. </author> <note> FTP mirroring software. Available from ftp://src.doc.ic.ac.uk/package/mirror.shar, </note> <month> August </month> <year> 1991. </year>
Reference-contexts: A group master periodically computes a graph that specifies which replicas should synchronize and propagate updates among each other. We call this the group's logical topology. Replicas synchronize updates with their neighbors in the logical topology graph using the "ftp-mirror" software <ref> [40] </ref>. Basically, mirror-d dynamically configures the ftp-mirror software between replicas in a replication group so that updates propagate along the highest bandwidth, lowest delay network paths. Since groups can be organized hierarchically, mirror-d should be able to scale to thousands of autonomously operated replicas.
Reference: [41] <author> Jeffrey C. Mogul and Venkata N. Padmanabhan. </author> <title> Improving WWW latency. </title> <booktitle> Second International World Wide Web Conference, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: An index of FTP file names can be built more efficiently using recursive directory listing operations, but this still requires an expensive file system traversal. More recently there have also been efforts to reduce the costs of HTTP retrievals by creating servers that do not fork <ref> [41] </ref>. The Gatherer dramatically reduces these inefficiencies through the use of Provider site-resident software optimized for indexing.
Reference: [42] <author> David A. Nowitz and Michael E. Lesk. </author> <title> A Dial-Up Network of UNIX Systems. </title> <institution> Bell Laboratories, </institution> <address> Murray Hill, New Jersey, </address> <month> August </month> <year> 1978. </year>
Reference-contexts: 1 Introduction Over the past few years a progression of Internet publishing tools has appeared. Until 1992, FTP [45] and NetNews <ref> [42] </ref> were the principal publishing tools. Around 1992, Gopher [39] and WAIS [31] gained popularity because they simplified network interactions and provided better ways to navigate through information. <p> We also note that one of the biggest scaling problems facing the Andrew File System is its callback-based invalidation protocol, which would be reduced if caches were arranged hierarchically. Current replication systems (such as USENET news <ref> [42] </ref> and the Archie replication system) require replicas to be placed and configured manually. Harvest's approach is to derive the replica configuration automatically, adapting to measured physical network changes.
Reference: [43] <author> Brian Pinkerton. </author> <title> The WebCrawler. </title> <type> Technical report, </type> <year> 1994. </year> <note> Available from http://www.biotech.washington.edu /WebCrawler/WebCrawler.html. </note>
Reference-contexts: Recently, a number of efforts have been initiated to create indexes of widely distributed sites (e.g., Gifford's Content Router [48] and Pinkerton's WebCrawler <ref> [43] </ref>). One of the goals of Harvest is to provide a flexible and efficient system upon which such systems can be built. We discuss this point in Section 4. <p> One could use Harvest to provide the functionality of many other indexes and indexing systems. One could build a WebCrawler <ref> [43] </ref> by defining an appropriate Gatherer configuration, and using the Essence full-text extraction mechanism. One could build a WHOIS++ [49] system by a combination of a Gatherer configuration and Essence extraction script for constructing the centroids, and then providing a front-end query script to search the gathered SOIF records.
Reference: [44] <author> Jon Postel. </author> <title> RFC 768: User Datagram Protocol. </title> <type> Technical report, </type> <month> August </month> <year> 1980. </year>
Reference-contexts: Each neighbor and parent responds with a hit or miss message, depending on the state of the object in their caches. If the object's home is running a UDP <ref> [44] </ref> echo daemon, the object's home site echos a hit message. The cache fetches the object from the fastest site to return a hit message, whether it be another cache or the object's home site.
Reference: [45] <author> Jon Postel and Joyce Reynolds. </author> <title> RFC 959: File Transfer Protocol (FTP). </title> <type> Technical report, </type> <month> October </month> <year> 1985. </year>
Reference-contexts: 1 Introduction Over the past few years a progression of Internet publishing tools has appeared. Until 1992, FTP <ref> [45] </ref> and NetNews [42] were the principal publishing tools. Around 1992, Gopher [39] and WAIS [31] gained popularity because they simplified network interactions and provided better ways to navigate through information.
Reference: [46] <author> Ronald L. Rivest. </author> <title> Rfc 1321: The MD5 message-digest algorithm. </title> <type> Technical report, </type> <institution> MIT Laboratory for Computer Science, Cambridge, Massachusetts and RSA Data Security, Inc, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: Second, the Gatherer transmits indexing data in compressed form. Third, the Gatherer supports incremental updates by allowing Brokers to request only summaries that have changed since a specified time. For access methods that do not support time stamps, the Gatherer extracts and compares "MD5" <ref> [46] </ref> cryptographic checksums of each object to determine if the object has changed since the last time the Gatherer generated a summary for it. Finally, the Gatherer maintains a cache of recently retrieved objects, to reduce the load of starting the system back up after a crash. 3 .
Reference: [47] <author> Michael F. Schwartz, Alan Emtage, Brewster Kahle, and B. Clifford Neuman. </author> <title> A comparison of Internet resource discovery approaches. </title> <journal> Computing Systems, </journal> <volume> 5(4) </volume> <pages> 461-493, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: means by which site administrators can specify mappings from URL roots to local file system names, allowing URLs listed in the Gatherer configuration file to be accessed via the local file system. 5 We computed this average from measurements of the total number of sites and files indexed by Archie <ref> [47] </ref>. 6 At present this is only possible with the FTP control connection and NetNews. Gopher and HTTP close con nections after each retrieval. 10 pressed form from Gatherers to Brokers, and by using a local cache of recently retrieved objects.
Reference: [48] <author> Mark A. Sheldon, Andrzej Duda, Ron Weiss, Jr. James W. O'Toole, and David K. Gifford. </author> <title> Content routing for distributed information servers. </title> <booktitle> Proceedings of the FOURTH International Conference on Extending Database Technology, </booktitle> <month> March </month> <year> 1994. </year> <note> Queryable via http://paris.lcs.mit.edu/Projects/CRS/content-router.html. </note>
Reference-contexts: Recently, a number of efforts have been initiated to create indexes of widely distributed sites (e.g., Gifford's Content Router <ref> [48] </ref> and Pinkerton's WebCrawler [43]). One of the goals of Harvest is to provide a flexible and efficient system upon which such systems can be built. We discuss this point in Section 4. <p> As a final example of how Harvest can be used to build other Brokers, we plan to implement WAIS gathering support in Harvest. Once we have done that, we will be able to build a Broker of WAIS headlines, similar to that supported by Gifford's Content Router system <ref> [48] </ref>.
Reference: [49] <author> Chris Weider, Jim Fullton, and Simon Spero. </author> <title> Architecture of the WHOIS++ index service. </title> <type> Technical report, </type> <month> November </month> <year> 1992. </year> <note> Available from ftp://nri.reston.va.us/internet-drafts/draft-ietf-wnils-whois-00.txt. </note>
Reference-contexts: We discuss this point in Section 4. The WHOIS and Network Information Look Up Service Working Group in the Internet Engineering Task Force has defined an Internet standard called "WHOIS++" which gathers concise descriptions (called "centroids") of each indexed database <ref> [49] </ref>. In contrast to our approach, WHOIS++ does not provide an automatic data gathering architecture, nor many of the scaling features (such as caching and replication) that Harvest provides. However, Harvest can be used to build WHOIS++. Aliweb [33] collects site description templates formatted according to the WHOIS++ specification. <p> One could use Harvest to provide the functionality of many other indexes and indexing systems. One could build a WebCrawler [43] by defining an appropriate Gatherer configuration, and using the Essence full-text extraction mechanism. One could build a WHOIS++ <ref> [49] </ref> system by a combination of a Gatherer configuration and Essence extraction script for constructing the centroids, and then providing a front-end query script to search the gathered SOIF records.

References-found: 49

