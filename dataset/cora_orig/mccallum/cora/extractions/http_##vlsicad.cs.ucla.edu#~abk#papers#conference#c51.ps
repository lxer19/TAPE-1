URL: http://vlsicad.cs.ucla.edu/~abk/papers/conference/c51.ps
Refering-URL: http://vlsicad.cs.ucla.edu/~abk/publications.html
Root-URL: http://www.cs.ucla.edu
Email: ffukunaga,abkg@cs.ucla.edu  
Title: Improving the Performance of Evolutionary Optimization by Dynamically Scaling the Evaluation Function  
Author: Alex S. Fukunaga and Andrew B. Kahng 
Address: Los Angeles Los Angeles, CA 90095-1596, USA  
Affiliation: Computer Science Department University of California,  
Abstract: Traditional evolutionary optimization algorithms assume a static evaluation function, according to which solutions are evolved. Incremental evolution is an approach through which a dynamic evaluation function is scaled over time in order to improve the performance of evolutionary optimization. In this paper, we present empirical results that demonstrate the effectiveness of this approach for genetic programming. Using two domains, a two-agent pursuit-evasion game and the Tracker [6] trail-following task, we demonstrate that incremental evolution is most successful when applied near the beginning of an evolutionary run. We also show that incremental evolution can be successful when the intermediate evaluation functions are more difficult than the target evaluation function, as well as when they are easier than the target function. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Andre. </author> <title> Evolution of mapmaking: Learning, planning and memory using genetic programming. </title> <booktitle> In Proc. IEEE Int. Conf. Evolutionary Computation, </booktitle> <pages> pages 250-255, </pages> <year> 1994. </year>
Reference-contexts: As we have discovered, it is possible for incremental evolution to be successful when the intermediate task is more difficult than the final task. Another somewhat related technique is the use of a multi-phasic fitness environment <ref> [1] </ref>. In [1], the task (collection of objects in a grid world) was separated into two phases: In the first phase (mapping), the agent looks for valuable objects and "remembers" their location, and in the second phase (planning), the agent plans a sequence of actions (consisting of movements and digging actions). <p> As we have discovered, it is possible for incremental evolution to be successful when the intermediate task is more difficult than the final task. Another somewhat related technique is the use of a multi-phasic fitness environment <ref> [1] </ref>. In [1], the task (collection of objects in a grid world) was separated into two phases: In the first phase (mapping), the agent looks for valuable objects and "remembers" their location, and in the second phase (planning), the agent plans a sequence of actions (consisting of movements and digging actions).
Reference: [2] <author> H. Cobb and J. Grefenstette. </author> <title> Genetic algorithms for tracking changing environments. </title> <booktitle> In Proc. Fifth International Conference on Genetic Algorithms, </booktitle> <pages> pages 523-530, </pages> <year> 1993. </year>
Reference: [3] <author> I. Harvey, P. Husbands, and D. Cliff. </author> <booktitle> Issues in evolutionary robotics. In From Animals to Animats 2: Proceedings of the Second International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 364-374, </pages> <year> 1992. </year>
Reference-contexts: These researchers have considered the problem of adapting to a given dynamic environment. Our work differs fundamentally in that we consider the problem of making the environment dynamic in order to improve performance. Recently, Harvey et al. <ref> [3, 4] </ref> have proposed this strategy of incremental evolution. They reported that evolving a neural network controller to visually guide a robot toward a small target in the environment took less total computational effort if the controllers were first evolved using a larger target. <p> They reported that evolving a neural network controller to visually guide a robot toward a small target in the environment took less total computational effort if the controllers were first evolved using a larger target. Our work differs from that of Harvey et al. <ref> [3, 4] </ref> in at least two major respects: (i) their representation scheme is different (a dynamical neural network), and (ii) they have only considered incremental evolution from an easier task to a harder task.
Reference: [4] <author> I. Harvey, P. Husbands, and D. Cliff. </author> <title> Seeing the light: Artificial evolution, real vision. </title> <booktitle> In From Animals to Animats 3: Proceedings of the Third International Conference on Adaptive Behavior, </booktitle> <pages> pages 392-401, </pages> <year> 1994. </year>
Reference-contexts: These researchers have considered the problem of adapting to a given dynamic environment. Our work differs fundamentally in that we consider the problem of making the environment dynamic in order to improve performance. Recently, Harvey et al. <ref> [3, 4] </ref> have proposed this strategy of incremental evolution. They reported that evolving a neural network controller to visually guide a robot toward a small target in the environment took less total computational effort if the controllers were first evolved using a larger target. <p> They reported that evolving a neural network controller to visually guide a robot toward a small target in the environment took less total computational effort if the controllers were first evolved using a larger target. Our work differs from that of Harvey et al. <ref> [3, 4] </ref> in at least two major respects: (i) their representation scheme is different (a dynamical neural network), and (ii) they have only considered incremental evolution from an easier task to a harder task.
Reference: [5] <author> J. Holland. </author> <title> Adaptation in natural and Artificial Systems. </title> <publisher> University of Michigan Press, </publisher> <year> 1975. </year>
Reference-contexts: 1. Introduction Genetic programming (GP) [8] is an automatic programming method, inspired by biological evolution, which has been successfully applied to a wide variety of program induction tasks. While genetic algorithms <ref> [5] </ref> typically apply biologically-inspired evolutionary operators to fixed-length representations of task solutions, GP applies analogous operators (selection, crossover, mutation) to tree-structured programs (such as LISP S-expressions). Like other approaches to evolutionary optimization, GP is computationally intensive: methods for accelerating the learning process are necessary. <p> The initial vector from pursuer to evader is a random lattice point in <ref> [5; 5] </ref> fi [5; 5]. The final distance is taken to be zero if the evader is captured. <p> The initial vector from pursuer to evader is a random lattice point in <ref> [5; 5] </ref> fi [5; 5]. The final distance is taken to be zero if the evader is captured.
Reference: [6] <author> D. Jefferson, R. Collins, C. Cooper, M. Dyer, M. Flowers, R. Korf, C. Taylor, and A. Wang. </author> <title> Evolution as a theme in artificial life: The genesys/tracker system. </title> <editor> In C. Langton, C. Taylor, J. Farmer, and S. Rasmussen, editors, </editor> <booktitle> Artificial Life II, </booktitle> <pages> pages 549-577. </pages> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference-contexts: Note that other relative weightings of time steps and final distance in the evaluation function are possible. being equal, it is easier for a faster evader to succeed (achieve a higher fitness score) than a slower evader. 3 Our secondary task domain is the Tracker problem <ref> [6] </ref>, a complex task inspired by the trail-following behavior of ants. 4 A hungry, artificial ant is placed in a two-dimensional, toroidal grid world populated by food arranged in an irregular trail, and the task is to generate a controller that maximizes the amount of food picked up by the ant <p> When an ant moves onto a cell containing food, the cell is cleared (i.e., it is assumed that the ant picks up the food). The difficulty of the Tracker problem stems from the irregularity and the "gaps" in the trail; see <ref> [6] </ref> for a thorough analysis. We used the Santa Fe trail [8] (see Figure 1, reproduced from [8]) as the target evaluation function (G 1 ) for optimization. To apply incremental evolution, we first removed all gaps at corners, to obtain the Intermediate trail. <p> The performance of the GP algorithm (best fitness achieved after 50 generations, taking the mean over 3 We tested this intuition experimentally see Section 3.2. 4 Jefferson et al. <ref> [6] </ref> addressed this problem by evolving finite-state-automata and neural network controllers. The same task was subsequently addressed by Koza [7] using genetic programming. 5 Because it is a steady-state GP, by "generation" we mean a "generational equivalent", or 500 individual fitness evaluations.
Reference: [7] <author> J. Koza. </author> <title> Genetic evolution and co-evolution of computer programs. </title> <editor> In C. Langton, C. Taylor, J. Farmer, and S. Ras-mussen, editors, </editor> <booktitle> Artificial Life II, </booktitle> <pages> pages 603-629. </pages> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference-contexts: The performance of the GP algorithm (best fitness achieved after 50 generations, taking the mean over 3 We tested this intuition experimentally see Section 3.2. 4 Jefferson et al. [6] addressed this problem by evolving finite-state-automata and neural network controllers. The same task was subsequently addressed by Koza <ref> [7] </ref> using genetic programming. 5 Because it is a steady-state GP, by "generation" we mean a "generational equivalent", or 500 individual fitness evaluations.
Reference: [8] <author> J. Koza. </author> <title> Genetic Programming: On the Programming of Computers By the Means of Natural Selection. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: 1. Introduction Genetic programming (GP) <ref> [8] </ref> is an automatic programming method, inspired by biological evolution, which has been successfully applied to a wide variety of program induction tasks. <p> Clearly, all else 1 Koza <ref> [8] </ref> evolved both pursuers and evaders using genetic programming. <p> The difficulty of the Tracker problem stems from the irregularity and the "gaps" in the trail; see [6] for a thorough analysis. We used the Santa Fe trail <ref> [8] </ref> (see Figure 1, reproduced from [8]) as the target evaluation function (G 1 ) for optimization. To apply incremental evolution, we first removed all gaps at corners, to obtain the Intermediate trail. <p> The difficulty of the Tracker problem stems from the irregularity and the "gaps" in the trail; see [6] for a thorough analysis. We used the Santa Fe trail <ref> [8] </ref> (see Figure 1, reproduced from [8]) as the target evaluation function (G 1 ) for optimization. To apply incremental evolution, we first removed all gaps at corners, to obtain the Intermediate trail. The trail was further simplified to the Easy trail by replacing double gaps in the trail with single gaps. <p> This approach differs from incremental evolution in that the user intervenes to identify the subproblems that must be solved and manually impose a structure on the solution. Incremental evolution also differs from previously suggested heuristics regarding test case selection. For example, Koza <ref> [8] </ref> suggested that test cases should be 11 Recall that a genetic program is an s-expression, which is naturally represented as a tree. chosen to be a representative sample of the possible inputs to the genetic program. 12 In related work in evolutionary computation, Shultz [16] has studied biasing of test
Reference: [9] <author> J. Koza. </author> <title> Genetic Programming II: Automatic Discovery of Reusable Programs. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Like other approaches to evolutionary optimization, GP is computationally intensive: methods for accelerating the learning process are necessary. A number of techniques for improving the efficiency of GP have been proposed. These include extensions to the basic genetic programming model (e.g., mechanisms such as automatically defined functions <ref> [9] </ref>) and variations on basic genetic operators (e.g., brood selection [17]). These previous approaches concentrate on the search algorithm, i.e., the mechanism by which the space of genetic programs are explored.
Reference: [10] <author> M. Littman and D. Ackley. </author> <title> Adaptation in constant utility non-stationary environments. </title> <booktitle> In Proc. Fourth International Conference on Genetic Algorithms, </booktitle> <pages> pages 136-142, </pages> <year> 1991. </year>
Reference: [11] <author> G. F. Miller and D. Cliff. </author> <title> Protean behavior in dynamic games: Arguments for the co-evolution of pursuit-evasion tactics. </title> <editor> In D. Cliff, P. Husbands, J.-A. Meyer, and S. W. Wilson, editors, </editor> <booktitle> From Animals to Animats 3: Proceedings of the Third International Conference on Adaptive Behavior, </booktitle> <year> 1994. </year>
Reference-contexts: Clearly, all else 1 Koza [8] evolved both pursuers and evaders using genetic programming. Recently, Reynolds [13] has used coevolution to evolve pursuers and evaders, and the merits of this task as a testbed for the evolution of adaptive behavior have been discussed in <ref> [11] </ref>. 2 To be specific: the pursuer moves a distance of 1.0 in every time step, and there are a total of 50 time steps. The initial vector from pursuer to evader is a random lattice point in [5; 5] fi [5; 5].
Reference: [12] <author> C. Ramsey and J. Grefenstette. </author> <title> Case-based initialization of genetic algorithms. </title> <booktitle> In Proc. Fifth International Conference on Genetic Algorithms, </booktitle> <pages> pages 84-91, </pages> <year> 1993. </year>
Reference: [13] <author> C. Reynolds. </author> <title> Competition, coevolution and the game of tag. </title> <booktitle> In Artificial Life IV, </booktitle> <year> 1994. </year>
Reference-contexts: Clearly, all else 1 Koza [8] evolved both pursuers and evaders using genetic programming. Recently, Reynolds <ref> [13] </ref> has used coevolution to evolve pursuers and evaders, and the merits of this task as a testbed for the evolution of adaptive behavior have been discussed in [11]. 2 To be specific: the pursuer moves a distance of 1.0 in every time step, and there are a total of 50
Reference: [14] <author> C. Reynolds. </author> <title> An evolved, vision-based model of obstacle avoidance behavior. </title> <editor> In C. Langton, editor, </editor> <booktitle> Artificial Life III, </booktitle> <pages> pages 327-346. </pages> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: Note that the intermediate and easy trails were shortened at the end to maintain the total amount of food at 89 units; thus, the maximum fitnesses achievable on all three trails are the same. We used steady-state GP <ref> [14] </ref> with tournament selection (Figure 2). Incremental evolution was implemented by changing the fitness function at generation t 0 . 5 No mutation was used. The population size was 500, and there were a total of 50 generations.
Reference: [15] <author> J. Rutkowska. </author> <title> Emergent functionality in human infants. In From Animals to Animats3: </title> <booktitle> Proceedings of the Third International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 179-188. </pages> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: This intuition is consistent with the phenomenon of scaffolding, which has been studied in psychology <ref> [15] </ref>. This paper presents an empirical study of incremental evolution applied to GP. In Section 2, we define the technique more precisely. Section 3 then presents experimental results demonstrating that incremental evolution can yield performance improvements in GP.
Reference: [16] <author> A. C. Schultz. </author> <title> Adapting the evaluation space to improve global learning. </title> <booktitle> In Proc. Fourth International Conference on Genetic Algorithms, </booktitle> <pages> pages 158-164, </pages> <year> 1991. </year>
Reference-contexts: For example, Koza [8] suggested that test cases should be 11 Recall that a genetic program is an s-expression, which is naturally represented as a tree. chosen to be a representative sample of the possible inputs to the genetic program. 12 In related work in evolutionary computation, Shultz <ref> [16] </ref> has studied biasing of test cases in order to improve performance in a genetic algorithm. Last, we note that our approach to optimizing performance using incremental evolution also differs from co-evolution.
Reference: [17] <author> W. Tackett and A. Carmi. </author> <title> The unique implications of brood selection for genetic programming. </title> <booktitle> In Proceedings of the 1994 IEEE World Congress on Computational Intelligence. </booktitle> <publisher> IEEE Press, </publisher> <year> 1994. </year>
Reference-contexts: A number of techniques for improving the efficiency of GP have been proposed. These include extensions to the basic genetic programming model (e.g., mechanisms such as automatically defined functions [9]) and variations on basic genetic operators (e.g., brood selection <ref> [17] </ref>). These previous approaches concentrate on the search algorithm, i.e., the mechanism by which the space of genetic programs are explored. A complementary approach is to focus on the searchability genetic programs that is explored (that is, by the fitness function over the space of possible genetic programs.
References-found: 17

