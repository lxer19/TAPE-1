URL: http://www.uni-paderborn.de/fachbereich/AG/agmadh/Scripts/GENERAL/Interactive/Sudan-PhD.ps.gz
Refering-URL: http://www.uni-paderborn.de/fachbereich/AG/agmadh/WWW/english/scripts.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Efficient Checking of Polynomials and Proofs and the Hardness of Approximation Problems  
Author: by Madhu Sudan B. Professor Umesh V. Vazirani, Chair Professor Richard M. Karp 
Degree: A dissertation submitted in partial satisfaction of the requirements for the degree of Doctor of Philosophy in Computer Science in the GRADUATE DIVISION of the UNIVERSITY of CALIFORNIA at BERKELEY Committee in charge:  Professor Dorit Hochbaum  
Date: 1987  1992  
Address: New Delhi)  
Affiliation: Tech. (Indian Institute of Technology,  
Abstract-found: 0
Intro-found: 1
Reference: [ADP77] <author> G. Ausiello, A. D'Atri, and M. Protasi. </author> <title> On the structure of combinatorial problems and structure preserving reductions. </title> <booktitle> In Proceedings of the 4th International Colloquium on Automata, Languages and Programming, </booktitle> <pages> pages 45-57, </pages> <year> 1977. </year>
Reference-contexts: The lack of approximation preserving reductions among optimization problems seemed to isolate these efforts and the search for such reductions became the goal of a wide body of research <ref> [ADP77, ADP80, AMSP80, PM81] </ref>. The most successful of these efforts seems to be the work of Papadimitriou and Yannakakis [PY91] where they used a syntactic characterization of NP due to Fagin [Fag74] to define a class called MAX SNP. <p> The running time of A * on inputs of length n is bounded by a polynomial in n. (The input here is a description of the solution space S and the function f .) The research efforts of the past two decades <ref> [ADP77, ADP80, AMSP80, GJ79, GJ78, PM81] </ref> have broadly aimed at classifying approximation versions of optimization problems into one of the following classes: 1.
Reference: [ADP80] <author> G. Ausiello, A. D'Atri, and M. Protasi. </author> <title> Structure preserving reductions among convex optimization problems. </title> <journal> Journal of Computer and Systems Sciences, </journal> <volume> 21 </volume> <pages> 136-153, </pages> <year> 1980. </year>
Reference-contexts: The lack of approximation preserving reductions among optimization problems seemed to isolate these efforts and the search for such reductions became the goal of a wide body of research <ref> [ADP77, ADP80, AMSP80, PM81] </ref>. The most successful of these efforts seems to be the work of Papadimitriou and Yannakakis [PY91] where they used a syntactic characterization of NP due to Fagin [Fag74] to define a class called MAX SNP. <p> The running time of A * on inputs of length n is bounded by a polynomial in n. (The input here is a description of the solution space S and the function f .) The research efforts of the past two decades <ref> [ADP77, ADP80, AMSP80, GJ79, GJ78, PM81] </ref> have broadly aimed at classifying approximation versions of optimization problems into one of the following classes: 1.
Reference: [AFK89] <author> M. Abadi, J. Feigenbaum, and J. Kilian. </author> <title> On hiding information from an oracle. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 39 </volume> <pages> 21-50, </pages> <year> 1989. </year>
Reference-contexts: Such an equivalence in the worst case and average case behavior is not known for many functions and might not be true of NP-hard functions. Blum, Luby and Rubinfeld use the notion of random self-reducibility (introduced by <ref> [AFK89] </ref>, see also [Fei93]) to exhibit self-correctors for a large collection of functions. This property can be observed in a number of algebraic functions. <p> Notice that a degree d multivariate polynomial restricted to a line becomes a univariate degree d polynomial. 2.2 Achieving some resilience: Random self-reducibility The notion of random self-reducibility was introduced as a tool to implement instance-hiding schemes. The first formal definition occurs in <ref> [AFK89] </ref> (see also [Fei93] for a survey).
Reference: [AHK] <author> L. Adleman, M. Huang, and K. Kompella. </author> <title> Efficient checkers for number-theoretic computations. </title> <note> Submitted to Information and Computation. </note>
Reference-contexts: A significant amount of success has been achieved in the task of constructing checkers and checkers are available for a wide variety of problems including sorting, linear programming and number theoretic applications <ref> [AHK, BK89, Kan90] </ref>. A particular approach to the task of obtaining checkers that has met with considerable success was introduced by Blum, Luby and Rubinfeld [BLR90]. 8 They decompose the task of checking a program into two phases: a preprocessing phase and a runtime phase.
Reference: [ALM + 92] <author> S. Arora, C. Lund, R. Motwani, M. Sudan, and M. Szegedy. </author> <title> Proof verification and the intractability of approximation problems. </title> <booktitle> In Proceedings of the 33rd IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1992. </year> <note> To appear. </note>
Reference-contexts: They show that unless NP DTIME (n loglog n ), the size of the largest clique in a graph cannot be estimated to within constant factors. Subsequently, by improving the performance of the probabilistically checkable proofs, [AS92] and <ref> [ALM + 92] </ref> have been able to improve this to show that approximating the clique size to within n * (for some positive *) is NP-hard. Intuitively, the connection between the probabilistically checkable proofs and the approximation hardness results are due to the following reason. <p> Yet a key element was missing in the analysis of the tester, which was finally remedied by the work of Arora and Safra [AS92]. The observation that putting [RS92] and [AS92] together yields a test which requires only a constant number of probes is made in <ref> [ALM + 92] </ref>. One interesting open question is: How large must the field size be as a function of the degree d, so that the low-degree test of Theorem 3.7 works. <p> The composition idea shows how to compose these proof systems with each other, eventually giving proof systems where the verifier tosses O (log n) coins and looks at only constantly many bits in the proof to verify it. The results of this chapter appear in <ref> [ALM + 92] </ref>. 4.1 Definitions The basic task of this chapter is the construction of proof systems which magnify errors. Such proof systems should have the feature that if a statement is true then the proof system should admit error-free proofs of the statement. <p> Thus the following corollaries become immediate. Corollary 4.6.1 NE = PCP (n; O (1)) Corollary 4.6.2 NEXPTIME = PCP (poly (n); O (1)) 4.7 Discussion The rPCP (poly (n); O (1)) protocol discussed in this chapter is new to <ref> [ALM + 92] </ref>. The rPCP (log n; polylog n) protocol is based on the work done by Lapidot and Shamir [LS91] and Feige and Lovasz [FL92a] on parallelizing the MIP = NEXPTIME protocol, but has some new elements to it. <p> The recursive proof construction technique described here is almost entirely due to the work of Arora and Safra [AS92]. Some of the formalism was introduced in <ref> [ALM + 92] </ref>. Open Questions The most important question that does remain open is what is the smallest number of bits that need to be read from a transparent proof to achieve a fixed probability of detecting a false proof. <p> The results of this section also appear in <ref> [ALM + 92] </ref>. Consider a language L 2 NP and a transparent proof of membership of an input instance x in the language L.
Reference: [ALRS92] <author> S. Ar, R. Lipton, R. Rubinfeld, and M. Sudan. </author> <title> Reconstructing algebraic functions from mixed data. </title> <booktitle> In Proceedings of the 33rd IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1992. </year> <note> To appear. </note>
Reference-contexts: This corresponds to the maximum likelihood decoding problem in coding theory. This problem is studied by Ar, Lipton, Rubinfeld and Sudan <ref> [ALRS92] </ref>, where a number of other applications for this problem are shown. They solve this problem for a restricted model of error which suffices for their applications. They also show that an extension of the methods from here can reduce the multivariate version of this problem to the univariate case.
Reference: [AMSP80] <author> G. Ausiello, A. Marchetti-Spaccamela, and M. Protasi. </author> <title> Towards a unified approach for the classification of np-complete optimization problems. </title> <journal> Theoretical Computer Science, </journal> <volume> 12 </volume> <pages> 83-96, </pages> <year> 1980. </year> <month> 74 </month>
Reference-contexts: The lack of approximation preserving reductions among optimization problems seemed to isolate these efforts and the search for such reductions became the goal of a wide body of research <ref> [ADP77, ADP80, AMSP80, PM81] </ref>. The most successful of these efforts seems to be the work of Papadimitriou and Yannakakis [PY91] where they used a syntactic characterization of NP due to Fagin [Fag74] to define a class called MAX SNP. <p> The running time of A * on inputs of length n is bounded by a polynomial in n. (The input here is a description of the solution space S and the function f .) The research efforts of the past two decades <ref> [ADP77, ADP80, AMSP80, GJ79, GJ78, PM81] </ref> have broadly aimed at classifying approximation versions of optimization problems into one of the following classes: 1.
Reference: [AS92] <author> S. Arora and S. Safra. </author> <title> Probabilistic checking of proofs: A new characterization of NP. </title> <booktitle> In Proceedings of the 33rd Annual IEEE Symposium of the Foundations of Computer Science, </booktitle> <year> 1992. </year> <note> To appear. </note>
Reference-contexts: This is the defining property of the fundamental complexity class NP. Recently this notion has been extended by allowing the polynomial time verifier access to random bits and extending the notion of a proof to allow the verifier a tiny probability of accepting a fallacious proof (see <ref> [Bab85, GMR89, BOGKW88, FRS88, BFLS91, AS92] </ref>). Such probabilistically checkable proofs are unexpectedly powerful and their power has been explored in several recent papers [BFL91, BFLS91, FGL + 91, AS92]. <p> Such probabilistically checkable proofs are unexpectedly powerful and their power has been explored in several recent papers <ref> [BFL91, BFLS91, FGL + 91, AS92] </ref>. These papers show that even verifiers with severely constrained access to the proof can check proofs of very general statements namely membership for any NP language. <p> But the locality of the way the proof is probed (i.e., in a constant number of entries which are O (polylog n) bits long) allows for the proofs to be checked recursively, using the techniques in <ref> [AS92] </ref>. Repeated employment of this idea yields proofs which can be verified by making O (log (c) n) probes into the proof (here log (c) the log function composed c times). <p> A refinement of the notion of a transparent proof probabilistically checkable proof was defined by Arora and Safra <ref> [AS92] </ref>. Instead of concentrating on the running time of the probabilistic verifier that verifies the proof, the new notion concentrates on the sensitivity of the verifier to the proof. <p> Arora and Safra show that all languages in NP have probabilistically checkable proofs with query complexity O (polyloglog n) (for inputs of size n). Based on this notion of a proof system <ref> [AS92] </ref> define a class of languages PCP, with two parameters: the number of random bits used by the verifier and the query complexity. <p> new terminology of Arora and Safra, the previous results may be stated as NEXPTIME = PCP (poly (n); poly (n)) ([BFL91]), NP PCP (polylog n; polylog n) ([BFLS91]), NP PCP (log n loglog n; log n loglog n) ([FGL + 91]) and NP PCP (log n; polyloglog n) shown by <ref> [AS92] </ref>. The last of these provides an exact characterization of NP (since containment in the other direction follows in a straightforward manner). <p> They show that unless NP DTIME (n loglog n ), the size of the largest clique in a graph cannot be estimated to within constant factors. Subsequently, by improving the performance of the probabilistically checkable proofs, <ref> [AS92] </ref> and [ALM + 92] have been able to improve this to show that approximating the clique size to within n * (for some positive *) is NP-hard. Intuitively, the connection between the probabilistically checkable proofs and the approximation hardness results are due to the following reason. <p> Notice that in the above test, the number of iterations has not been specified yet. This will depend on how strong our theorem is. We will establish here that k = O (d) suffices. A tighter theorem on this was proved more recently by Arora and Safra <ref> [AS92] </ref>. They make very elegant use of the Berlekamp-Welch technique to show that k = O (1) suffices! Since the efficiency of this result becomes important in the next section, we include a statement of their result in this section. <p> The proof of this section is essentially from from [RS92]. The efficiency shown here is better than that shown in [RS92], due to the use of Lemma 3.6 which is from <ref> [AS92] </ref>. Definition 3.2.2 For x; h 2 F m , the set of points f x + t fl hjt 2 F g will be called the line through x with offset h. <p> This was followed up by more efficient versions in <ref> [BFLS91, FGL + 91, AS92] </ref>. All the testers did have one common ingredient though: They test for the degree of each variable individually and thus have an inherent W (m) lower bound on the number of probes required. <p> The tester developed in this chapter was developed in parallel in [GLR + 91, She91, RS92]. By not testing for the degree of each variable explicitly, the tester showed potential to perform better than the testers of <ref> [BFL91, BFLS91, FGL + 91, AS92] </ref>. Yet a key element was missing in the analysis of the tester, which was finally remedied by the work of Arora and Safra [AS92]. <p> Yet a key element was missing in the analysis of the tester, which was finally remedied by the work of Arora and Safra <ref> [AS92] </ref>. The observation that putting [RS92] and [AS92] together yields a test which requires only a constant number of probes is made in [ALM + 92]. <p> Yet a key element was missing in the analysis of the tester, which was finally remedied by the work of Arora and Safra <ref> [AS92] </ref>. The observation that putting [RS92] and [AS92] together yields a test which requires only a constant number of probes is made in [ALM + 92]. One interesting open question is: How large must the field size be as a function of the degree d, so that the low-degree test of Theorem 3.7 works. <p> This in turn reduces 47 to the question of seeing how small the field size may be while the Arora-Safra tester still works. The analysis given in <ref> [AS92] </ref> shows that the field size could be O (d 3 ), and this can be improved to show O (d 2 ) suffices. It seems possible that this number could be reduced to being O (d). <p> The probabilistically checkable proofs that result can be verified by a probabilistic verifier who tosses O (log n) coins and probes the proof in O (polylog n) bits. Next we outline the idea of recursive proof checking of Arora and Safra <ref> [AS92] </ref>. <p> This rather surprising nature of these proof systems will turn out to be useful in Section 4.3. The next notion we study is that of probabilistically checkable proofs due to Arora and Safra <ref> [AS92] </ref>. Instead of characterizing the transparency of a proof system using the running time of a verifier, they characterize it using the number of bits of a proof that are examined by the verifier to verify the proof. <p> This gives an implicit bound on the size of the proof and is also motivated by some of the applications of such proofs systems [FGL + 91] (see also Chapter 5). We define the notion in terms of languages which have efficient probabilistically checkable proofs. Definition 4.1.2 (PCP: <ref> [AS92] </ref>) A language L is in the class PCP (r (n); q (n)) if there exists a tester T such that 8x 2 f0; 1g n , we have: * If x 2 L, then there exists a proof such that T (r; x; ) outputs PASS, for all r 2 <p> In this chapter we work towards showing NP = PCP (log n; O (1)). From here onwards we will use the words transparent proofs and probabilistically checkable proofs interchangeably. The notion we will use will be that of <ref> [AS92] </ref> and precise statements will always be made in terms of PCP. 4.2 A transparent proof for languages in NP The first probabilistically checkable proof we will describe follows in a very simple manner from the following characterization of NP in terms of polynomial construction sequences of very short width and <p> It turns out though that the proof verification system of [BFLS91] can be modified to get the blind checkability even when the input is given in the form of a constant number of encoded entries and this was first observed by <ref> [AS92] </ref>. Thus we restrict our attention to proof systems which have both the properties considered above, namely, segmented proofs and blind checkability when the input is presented by a constant number of encoded pieces and show how to compose such proofs systems to achieve proof systems with improved query complexity. <p> Lastly, E should be polynomial time computable. (Notice that E 1 need not be computable efficiently.) The following lemma is based directly on the work of <ref> [AS92] </ref> and shows that two rPCP proof systems can be composed to get potentially more efficient rPCP proof systems. <p> Moreover, their final protocols does not seem to be able to handle the situation where the input comes in a constant number of error-corrected pieces. The recursive proof construction technique described here is almost entirely due to the work of Arora and Safra <ref> [AS92] </ref>. Some of the formalism was introduced in [ALM + 92]. Open Questions The most important question that does remain open is what is the smallest number of bits that need to be read from a transparent proof to achieve a fixed probability of detecting a false proof.
Reference: [Aza92] <author> Y. Azar. </author> <type> Personal Communication, </type> <year> 1992. </year>
Reference: [Bab85] <author> L. Babai. </author> <title> Trading group theory for randomness. </title> <booktitle> In Proceedings of the 17th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 421-429, </pages> <year> 1985. </year>
Reference-contexts: This is the defining property of the fundamental complexity class NP. Recently this notion has been extended by allowing the polynomial time verifier access to random bits and extending the notion of a proof to allow the verifier a tiny probability of accepting a fallacious proof (see <ref> [Bab85, GMR89, BOGKW88, FRS88, BFLS91, AS92] </ref>). Such probabilistically checkable proofs are unexpectedly powerful and their power has been explored in several recent papers [BFL91, BFLS91, FGL + 91, AS92]. <p> Then how easy is it to persuade the verifier of such a fact. (The terminology used here comes from the area of interactive proofs <ref> [Bab85, BM88, GMR89] </ref>.) We make this setting more precise now: We will expect the prover to provide the additional information about f in the form of an oracle O. <p> Our results improve on the efficiency of such schemes and extend it to include all the codes here. 1.4 Probabilistic Checking of Proofs The notion of interactive proofs was first defined by Babai <ref> [Bab85, BM88] </ref> and Goldwasser, Micali and Rackoff [GMR89]. They study languages which permit interactive proofs of membership which are verifiable by a probabilistic verifier in polynomial time and call the collection of such languages IP.
Reference: [BF90] <author> D. Beaver and J. Feigenbaum. </author> <title> Hiding instances in multioracle queries. </title> <booktitle> In Proceedings of the 7th Annual Symposium on Theoretical Aspects of Computer Science, </booktitle> <publisher> Springer Verlag LNCS 415, </publisher> <pages> pages 37-48, </pages> <year> 1990. </year>
Reference-contexts: Then it can be shown that all linear functions have the random self-reducibility property (see [BLR90]). [BLR90] cite many examples of linear functions: The Mod function, Integer Multiplication, Modular Multiplication, Integer Division, Matrix Multiplication etc. all of which have self-correctors due to this observation. [Lip91] based on the techniques of <ref> [BF90] </ref> has similarly shown the random self-reducibility of multivariate polynomials and thus all multivariate polynomials have self-correctors. While the existence of self-correctors of functions may be a rare occurrence, in the cases where they have been found, proofs of correctness have been straightforward. <p> The following observation was made by Blum, Luby and Rubinfeld [BLR90]. Observation 2.2.2 Every random self-reducible function has a self-corrector. Lipton [Lip91] based on the work of Beaver and Feigenbaum <ref> [BF90] </ref> shows that the family of multivariate polynomials over large finite fields are random self-reducible. Lemma 2.2.3 ([BF90],[Lip91]) Let g : F m 7! F be a degree d polynomial, where F is a finite field such that jF j d + 2. Then g is random self-reducible.
Reference: [BF91] <author> L. Babai and L. Fortnow. Arithmetization: </author> <title> A new method in structural complexity theory. </title> <journal> Computational Complexity, </journal> <volume> 1 </volume> <pages> 41-66, </pages> <year> 1991. </year>
Reference-contexts: But for special polynomials other properties of the function can be used to achieve some resilience and this is indeed the case for the permanent over GF (3) (see Feigenbaum and Fortnow [FF91] and Babai and Fortnow <ref> [BF91] </ref>). The resilience shown by them is inverse polynomial in the dimension of the matrix and it is over a distribution which is not uniform. It would be interesting to improve either of the two aspects. 25 Chapter 3 Low-degree tests In this chapter we discuss issues on testing polynomials.
Reference: [BFKR91] <author> D. Beaver, J. Feigenbaum, J. Kilian, and P. Rogaway. </author> <title> Security with low communication overhead. </title> <booktitle> In Proceedings of Crypto '90, </booktitle> <publisher> Springer Verlag LNCS 537, </publisher> <pages> pages 62-76, </pages> <year> 1991. </year>
Reference-contexts: In particular, this has been used by Beaver et al. <ref> [BFKR91] </ref>, to reduce the number of oracles used in instance hiding schemes. The underlying property that they extract is similar.
Reference: [BFL91] <author> L. Babai, L. Fortnow, and C. Lund. </author> <title> Non-deterministic exponential time has two-prover interactive protocols. </title> <journal> Computational Complexity, </journal> <volume> 1 </volume> <pages> 3-40, </pages> <year> 1991. </year>
Reference-contexts: Such probabilistically checkable proofs are unexpectedly powerful and their power has been explored in several recent papers <ref> [BFL91, BFLS91, FGL + 91, AS92] </ref>. These papers show that even verifiers with severely constrained access to the proof can check proofs of very general statements namely membership for any NP language. <p> Moreover, if a linear function g is given by its values on a subset S of points from the domain, then g has a tester, provided g is uniquely determined by its values on the set S. The results of <ref> [BFL91] </ref> also give function family testers for the family of multilinear functions and this plays an important role in their work on showing MIP=NEXPTIME. <p> Languages which admit efficient proofs of membership under the multiple prover proof system are said to be in the class MIP and the recent result of Babai, Fortnow and Lund <ref> [BFL91] </ref> provides an exact characterization of this class i.e., MIP = NEXPTIME. <p> To formalize this concept, Babai et al. placed bounds on the running time of the probabilistic verifier and considered the kind of languages which have transparent proofs that could be verified in time t (n). They scale down the result in <ref> [BFL91] </ref> to show that all languages in NP have polynomial sized transparent proofs that can be verified in O (polylog n) time, under the assumption that the input was presented in some error-correcting code. <p> The tester for such functions can be obtained from the downward-self-reducibility theorem of Blum, Luby and Rubinfeld [BLR90]. We include a version of their theorem here that is weaker than the original theorem they prove. The tester also resembles the Sum-Check protocols of <ref> [LFKN90, Sha90, BFL91] </ref>. Let ffi &gt; 0 be a constant smaller than the constant of Theorem 3.7. <p> a constant number of probes into O (some constant for the low-degree test, 3 for the questions needed to simulate the tester of Theorem 3.8 and a constant to ensure f (l) 0). 2 3.5 Discussion History A tester for multivariate polynomials was first constructed by Babai, Fortnow and Lund <ref> [BFL91] </ref>. This was followed up by more efficient versions in [BFLS91, FGL + 91, AS92]. All the testers did have one common ingredient though: They test for the degree of each variable individually and thus have an inherent W (m) lower bound on the number of probes required. <p> The tester developed in this chapter was developed in parallel in [GLR + 91, She91, RS92]. By not testing for the degree of each variable explicitly, the tester showed potential to perform better than the testers of <ref> [BFL91, BFLS91, FGL + 91, AS92] </ref>. Yet a key element was missing in the analysis of the tester, which was finally remedied by the work of Arora and Safra [AS92]. <p> We show how the results of the previous chapter can be translated to get probabilistically checkable proofs of fairly general statements namely, statements of the form x 2 L where L is a language in NP. The translation uses the work of <ref> [BFL91, BFLS91] </ref> which shows that the problem of testing satisfiability of construction rules in NP-complete for a certain choice of parameters. The probabilistically checkable proofs that result can be verified by a probabilistic verifier who tosses O (log n) coins and probes the proof in O (polylog n) bits. <p> the verifier for all its random choices and if X is close to the encoding of a false theorem, or not close to the encoding of any valid statement, then it must be rejected by the probabilistic verifier with high probability. 50 Babai et al., based on the work of <ref> [BFL91] </ref>, show that all theorems and proofs can be placed in a transparent form by increasing their size by a slightly superlinear factor. One interesting aspect of this formalism is the rather blind nature of the verification process. <p> Furthermore, for any fixed value of r, T (r; x; ) depends on only O (q (n)) bits of , and its running time is bounded by poly (n; r (n); q (n)). The result of Babai et al. [BFLS91], improving on <ref> [BFL91] </ref>, can be viewed in this setting as showing NP PCP (polylog n; polylog n). (The implicit guarantee on the proof size obtained from this characterization is weaker than that shown by Babai et al. <p> The proof sizes as obtained 51 by Babai et al. are nearly linear in the size of any witness of x 2 L.) Feige et al. [FGL + 91], improved on <ref> [BFL91] </ref> differently to show that NP PCP (log n loglog n; log n loglog n), but their proofs are superpolynomial in size. <p> The characterization is implicit in the work of <ref> [BFL91, BFLS91] </ref>. Lemma 4.2.1 ([BFL91, BFLS91]) Given a language L 2 NP and an instance x 2 f0; 1g n , a construction rule of length log n and width log n for degree log 2 n polynomials in Q ( log n loglog n ) variables from a finite field
Reference: [BFLS91] <author> L. Babai, L. Fortnow, L. Levin, and M. Szegedy. </author> <title> Checking computations in poly-logarithmic time. </title> <booktitle> In Proceedings of the 23rd ACM Annual Symposium on Theory of Computing, </booktitle> <pages> pages 21-31, </pages> <year> 1991. </year>
Reference-contexts: This is the defining property of the fundamental complexity class NP. Recently this notion has been extended by allowing the polynomial time verifier access to random bits and extending the notion of a proof to allow the verifier a tiny probability of accepting a fallacious proof (see <ref> [Bab85, GMR89, BOGKW88, FRS88, BFLS91, AS92] </ref>). Such probabilistically checkable proofs are unexpectedly powerful and their power has been explored in several recent papers [BFL91, BFLS91, FGL + 91, AS92]. <p> Such probabilistically checkable proofs are unexpectedly powerful and their power has been explored in several recent papers <ref> [BFL91, BFLS91, FGL + 91, AS92] </ref>. These papers show that even verifiers with severely constrained access to the proof can check proofs of very general statements namely membership for any NP language. <p> In particular this gives proofs that the verifier needs to examine only O (polylog n) bits to verify membership in NP languages, and this resembles the result of <ref> [BFLS91] </ref> in its efficiency. But the locality of the way the proof is probed (i.e., in a constant number of entries which are O (polylog n) bits long) allows for the proofs to be checked recursively, using the techniques in [AS92]. <p> Similarly, a randomized error-corrector would compute the symbol of the nearest codeword at any one location correctly, with high probability, by examining a corrupted codeword at only a few locations. Such efficient error-detecting and correcting schemes were not known previously. Babai et al. <ref> [BFLS91] </ref> were the first to use this connection to build such fast error-correcting and detecting schemes for some codes. <p> This interpretation inspired Babai, Fortnow, Levin and Szegedy <ref> [BFLS91] </ref> to define the notion of transparent proofs: Informally, a transparent proof of a statement of the form x 2 L either proves a correct statement or mistakes will appear in the proof almost everywhere, thus enabling a probabilistic verifier to spot it by a cursory examination. <p> This was followed up by more efficient versions in <ref> [BFLS91, FGL + 91, AS92] </ref>. All the testers did have one common ingredient though: They test for the degree of each variable individually and thus have an inherent W (m) lower bound on the number of probes required. <p> The tester developed in this chapter was developed in parallel in [GLR + 91, She91, RS92]. By not testing for the degree of each variable explicitly, the tester showed potential to perform better than the testers of <ref> [BFL91, BFLS91, FGL + 91, AS92] </ref>. Yet a key element was missing in the analysis of the tester, which was finally remedied by the work of Arora and Safra [AS92]. <p> We show how the results of the previous chapter can be translated to get probabilistically checkable proofs of fairly general statements namely, statements of the form x 2 L where L is a language in NP. The translation uses the work of <ref> [BFL91, BFLS91] </ref> which shows that the problem of testing satisfiability of construction rules in NP-complete for a certain choice of parameters. The probabilistically checkable proofs that result can be verified by a probabilistic verifier who tosses O (log n) coins and probes the proof in O (polylog n) bits. <p> On the other hand, any proof of an incorrect statement should be riddled with errors. Formalizing this notion takes some effort and here we present two efforts which make this notion precise. The first notion we study is that of transparent proofs due to Babai, Fortnow, Levin and Szegedy <ref> [BFLS91] </ref>. [BFLS91] achieve this formalism by restricting the running time of probabilistic verifier. <p> Formalizing this notion takes some effort and here we present two efforts which make this notion precise. The first notion we study is that of transparent proofs due to Babai, Fortnow, Levin and Szegedy <ref> [BFLS91] </ref>. [BFLS91] achieve this formalism by restricting the running time of probabilistic verifier. <p> Babai et al. get around this by expecting the theorem to be also presented in a transparent form i.e., they expect the input x to be presented in an error-correcting encoding. The following definition is presented somewhat informally. Definition 4.1.1 (transparent proof: <ref> [BFLS91] </ref>) A pair of strings (X; ), where X is a theorem-candidate and is a proof-candidate is in transparent form, if X is encoded in an error-correcting code and the pair (X; ) can be verified by a probabilistic verifier in time polylogarithmic in the size of the theorem plus proof <p> Furthermore, for any fixed value of r, T (r; x; ) depends on only O (q (n)) bits of , and its running time is bounded by poly (n; r (n); q (n)). The result of Babai et al. <ref> [BFLS91] </ref>, improving on [BFL91], can be viewed in this setting as showing NP PCP (polylog n; polylog n). (The implicit guarantee on the proof size obtained from this characterization is weaker than that shown by Babai et al. <p> The characterization is implicit in the work of <ref> [BFL91, BFLS91] </ref>. Lemma 4.2.1 ([BFL91, BFLS91]) Given a language L 2 NP and an instance x 2 f0; 1g n , a construction rule of length log n and width log n for degree log 2 n polynomials in Q ( log n loglog n ) variables from a finite field <p> But the recursion involves statements of the type y 1 y 2 y c 2 L, where the prover can only provide individual encodings of the y i 's. It turns out though that the proof verification system of <ref> [BFLS91] </ref> can be modified to get the blind checkability even when the input is given in the form of a constant number of encoded entries and this was first observed by [AS92]. <p> Our result shows that any proof can be converted into a transparent proof which is within a polynomial factor of the size of the original proof. In contrast to this, the transparent proofs of <ref> [BFLS91] </ref> are nearly linear in the size of the original proof. This raises the question of whether the proofs of this section can be compressed into a nearly linear size.
Reference: [BJL + 91] <author> A. Blum, T. Jiang, M. Li, J. Tromp, and M. Yannakakis. </author> <title> Linear approximation of shortest superstrings. </title> <booktitle> In Proceedings of the 23rd Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 328-336, </pages> <year> 1991. </year>
Reference-contexts: In particular, we show that the class MAX SNP, defined by Papadimitriou and Yannakakis [PY91], contains this problem. A large variety of approximation problems are known to be MAX SNP-hard <ref> [PY91, PY92, BP89, BJL + 91, DJP + 92, Kan91] </ref> and thus the result from Chapter 5 translates into non-approximability result for all these problems unless NP = P. 16 Chapter 2 On the resilience of polynomials In this chapter we consider the task of correcting multivariate polynomials.
Reference: [BK89] <author> M. Blum and S. Kannan. </author> <title> Program correctness checking : : : and the design of programs that check their work. </title> <booktitle> In Proceedings of the 21st Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 86-97, </pages> <year> 1989. </year>
Reference-contexts: In the rest of this chapter we introduce in greater detail the areas to which the problems described above relate and describe the effect of the solutions obtained here to these areas. 1.2 Program Result Checking The notion of program result checking was initiated by Blum and Kannan <ref> [Blu88, BK89, Kan90] </ref> as a new approach for ensuring software reliability. The approach advocates the use of fast checks performed at runtime to obtain guarantees on the correctness of the solution obtained on specific runs of the program. <p> A significant amount of success has been achieved in the task of constructing checkers and checkers are available for a wide variety of problems including sorting, linear programming and number theoretic applications <ref> [AHK, BK89, Kan90] </ref>. A particular approach to the task of obtaining checkers that has met with considerable success was introduced by Blum, Luby and Rubinfeld [BLR90]. 8 They decompose the task of checking a program into two phases: a preprocessing phase and a runtime phase.
Reference: [BLR90] <author> M. Blum, M. Luby, and R. Rubinfeld. </author> <title> Self-testing/correcting with applications to numerical problems. </title> <booktitle> In Proceedings of the 22nd Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 73-83, </pages> <year> 1990. </year> <month> 75 </month>
Reference-contexts: To do this, we have to develop some tools which reveal some new characteristics of low-degree polynomials over finite fields. Our motivation for studying these problems came from the theory of self-testing/correcting programs <ref> [BLR90, Rub90] </ref>. It turns out that there is a fundamental connection between the testing and correcting of polynomials and the existence of efficient probabilistically checkable proofs. <p> This proof system also uses results of low degree testing and in particular the testing of linear (degree 1) functions 7 from <ref> [BLR90] </ref>. Composing the earlier developed proof systems with the new one gives us the final result: Polynomial sized proofs of membership in NP which are verifiable by probing them in a constant number of bits. <p> A particular approach to the task of obtaining checkers that has met with considerable success was introduced by Blum, Luby and Rubinfeld <ref> [BLR90] </ref>. 8 They decompose the task of checking a program into two phases: a preprocessing phase and a runtime phase. In the preprocessing phase they test the correctness of the program on randomly chosen inputs from a carefully chosen distribution. <p> We state the formal definitions next. In the following definitions we consider a function g described over a finite domain and the notation d (P; g) reflects the fraction of inputs on which the program P does not compute g. (The original definitions of <ref> [BLR90] </ref> allow for more general norms that could used to estimate the distance between P and g. Here we only define the concepts for the special case of the uniform norm, since all our results work with such a norm.) Definition 1.2.2 ([BLR90]) For * &gt; 0, a *-self-tester T for <p> Then it can be shown that all linear functions have the random self-reducibility property (see <ref> [BLR90] </ref>). [BLR90] cite many examples of linear functions: The Mod function, Integer Multiplication, Modular Multiplication, Integer Division, Matrix Multiplication etc. all of which have self-correctors due to this observation. [Lip91] based on the techniques of [BF90] has similarly shown the random self-reducibility of multivariate polynomials and thus all multivariate polynomials have <p> Then it can be shown that all linear functions have the random self-reducibility property (see <ref> [BLR90] </ref>). [BLR90] cite many examples of linear functions: The Mod function, Integer Multiplication, Modular Multiplication, Integer Division, Matrix Multiplication etc. all of which have self-correctors due to this observation. [Lip91] based on the techniques of [BF90] has similarly shown the random self-reducibility of multivariate polynomials and thus all multivariate polynomials have self-correctors. <p> The construction of self-testers for functions on the other hand has been invariably a much harder task. The first class of functions for which self-testers were shown was the class of linear functions. <ref> [BLR90] </ref> show that there exists a function family tester for the family of linear functions. <p> The following observation was made by Blum, Luby and Rubinfeld <ref> [BLR90] </ref>. Observation 2.2.2 Every random self-reducible function has a self-corrector. Lipton [Lip91] based on the work of Beaver and Feigenbaum [BF90] shows that the family of multivariate polynomials over large finite fields are random self-reducible. <p> The maximum number of oracle calls made by any rule to evaluate any input is called the width of the rule and is denoted w. The tester for such functions can be obtained from the downward-self-reducibility theorem of Blum, Luby and Rubinfeld <ref> [BLR90] </ref>. We include a version of their theorem here that is weaker than the original theorem they prove. The tester also resembles the Sum-Check protocols of [LFKN90, Sha90, BFL91]. Let ffi &gt; 0 be a constant smaller than the constant of Theorem 3.7. Theorem 3.8 ([BLR90]) Given oracles for a sequence <p> Fact 4.5.2 A function A : F m 7! F is linear if and only if for all x; y 2 F m , A ( x + y) = A ( x) + A ( y). The fact above was strengthened very significantly by Blum, Luby and Rubinfeld <ref> [BLR90] </ref> who show that the property used above is a very robust one, and can hence be used to construct testers for the family of linear functions. Lemma 4.5.3 (linearity tester: [BLR90]) If (A) : F m 7! F satisfies Pr h i then 9 a linear function A such that <p> The fact above was strengthened very significantly by Blum, Luby and Rubinfeld <ref> [BLR90] </ref> who show that the property used above is a very robust one, and can hence be used to construct testers for the family of linear functions. Lemma 4.5.3 (linearity tester: [BLR90]) If (A) : F m 7! F satisfies Pr h i then 9 a linear function A such that d (A; A) ffi, provided ffi 1=3. 56 Blum, Luby and Rubinfeld [BLR90] also show that the family of linear functions is self-correctable. <p> Lemma 4.5.3 (linearity tester: <ref> [BLR90] </ref>) If (A) : F m 7! F satisfies Pr h i then 9 a linear function A such that d (A; A) ffi, provided ffi 1=3. 56 Blum, Luby and Rubinfeld [BLR90] also show that the family of linear functions is self-correctable. In fact, they show that the value of a linear function can be computed correctly anywhere, using two calls to a function that is close to it. Lemma 4.5.4 (linear self-corrector: [BLR90]) If A is ffi-close to a linear function <p> provided ffi 1=3. 56 Blum, Luby and Rubinfeld <ref> [BLR90] </ref> also show that the family of linear functions is self-correctable. In fact, they show that the value of a linear function can be computed correctly anywhere, using two calls to a function that is close to it. Lemma 4.5.4 (linear self-corrector: [BLR90]) If A is ffi-close to a linear function A, then for all x 2 F m y2 U F m A ( x) = A ( y + x) A ( y) 1 2ffi The important point about the lemmas above is that both hold for all finite fields and
Reference: [Blu88] <author> M. Blum. </author> <title> Designing programs to check their work. </title> <type> Technical Report TR-88-009, </type> <institution> International Computer Science Institute, </institution> <year> 1988. </year>
Reference-contexts: In the rest of this chapter we introduce in greater detail the areas to which the problems described above relate and describe the effect of the solutions obtained here to these areas. 1.2 Program Result Checking The notion of program result checking was initiated by Blum and Kannan <ref> [Blu88, BK89, Kan90] </ref> as a new approach for ensuring software reliability. The approach advocates the use of fast checks performed at runtime to obtain guarantees on the correctness of the solution obtained on specific runs of the program.
Reference: [BM88] <author> L. Babai and S. Moran. </author> <title> Arthur-Merlin games: A randomized proof system and a hierarchy of complexity classes. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 36 </volume> <pages> 254-276, </pages> <year> 1988. </year>
Reference-contexts: Then how easy is it to persuade the verifier of such a fact. (The terminology used here comes from the area of interactive proofs <ref> [Bab85, BM88, GMR89] </ref>.) We make this setting more precise now: We will expect the prover to provide the additional information about f in the form of an oracle O. <p> Our results improve on the efficiency of such schemes and extend it to include all the codes here. 1.4 Probabilistic Checking of Proofs The notion of interactive proofs was first defined by Babai <ref> [Bab85, BM88] </ref> and Goldwasser, Micali and Rackoff [GMR89]. They study languages which permit interactive proofs of membership which are verifiable by a probabilistic verifier in polynomial time and call the collection of such languages IP.
Reference: [BOGKW88] <author> M. Ben-Or, S. Goldwasser, J. Kilian, and A. Wigderson. </author> <title> Multi-prover interactive proofs: How to remove intractability assumptions. </title> <booktitle> In Proceedings of the 20th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 113-131, </pages> <year> 1988. </year>
Reference-contexts: This is the defining property of the fundamental complexity class NP. Recently this notion has been extended by allowing the polynomial time verifier access to random bits and extending the notion of a proof to allow the verifier a tiny probability of accepting a fallacious proof (see <ref> [Bab85, GMR89, BOGKW88, FRS88, BFLS91, AS92] </ref>). Such probabilistically checkable proofs are unexpectedly powerful and their power has been explored in several recent papers [BFL91, BFLS91, FGL + 91, AS92]. <p> A related model of interactive proofs which is of more interest to us is the model where the verifier gets to ask questions from more than one non-interacting provers <ref> [BOGKW88] </ref> or equivalently when the prover is assumed to be a non-adaptive entity i.e., an oracle [FRS88].
Reference: [BP89] <author> M. Bern and P. Plassman. </author> <title> The steiner problem with edge lengths 1 and 2. </title> <journal> Information Processing Letters, </journal> <volume> 32 </volume> <pages> 171-176, </pages> <year> 1989. </year>
Reference-contexts: In particular, we show that the class MAX SNP, defined by Papadimitriou and Yannakakis [PY91], contains this problem. A large variety of approximation problems are known to be MAX SNP-hard <ref> [PY91, PY92, BP89, BJL + 91, DJP + 92, Kan91] </ref> and thus the result from Chapter 5 translates into non-approximability result for all these problems unless NP = P. 16 Chapter 2 On the resilience of polynomials In this chapter we consider the task of correcting multivariate polynomials.
Reference: [BS92] <author> P. Berman and G. Schnitger. </author> <title> On the complexity of approximating the independent set problem. </title> <journal> Information and Computation, </journal> <volume> 96 </volume> <pages> 77-94, </pages> <year> 1992. </year>
Reference-contexts: Thus the number of different random strings is polynomial in the input size. Hence the number of constraints is polynomial in jxj. Further, 1 Berman and Schnitger <ref> [BS92] </ref> showed the hardness result mentioned here under the assumption that MAX 3-SAT did not have randomized PTAS. The assumption could be made weaker using some of the known derandomization techniques (say, using the idea of recycling random bits [IZ89]).
Reference: [BW] <author> E. Berlekamp and L. Welch. </author> <title> Error correction of algebraic block codes. </title> <type> US Patent Number 4,633,470. </type>
Reference-contexts: In 20 the general form as it is stated above (with no constraints on the forms of the x i 's), the problem can still be solved efficiently and directly due to an elegant method of Berlekamp and Welch <ref> [BW] </ref>. We state their result here; their proof is included in the appendix. Lemma 2.3.1 (univariate self-corrector: [BW]) Given n points (x i ; y i ) 2 F 2 , there exists an algorithm which finds a degree d polynomial g such that g (x i ) = y i <p> it is stated above (with no constraints on the forms of the x i 's), the problem can still be solved efficiently and directly due to an elegant method of Berlekamp and Welch <ref> [BW] </ref>. We state their result here; their proof is included in the appendix. Lemma 2.3.1 (univariate self-corrector: [BW]) Given n points (x i ; y i ) 2 F 2 , there exists an algorithm which finds a degree d polynomial g such that g (x i ) = y i for all but k values of i, where 2k + d &lt; n, if such a g
Reference: [Coo71] <author> S.A. Cook. </author> <title> The complexity of theorem proving procedures. </title> <booktitle> In Proccedings of the 3rd Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 151-158, </pages> <year> 1971. </year>
Reference-contexts: Here we have tried to highlight this connection be deriving previously known results as well as some of the new results in a uniform manner from our results on the testing of polynomials. 1 The early papers on NP-completeness <ref> [Coo71, Lev73, Kar72] </ref> linked the notion of proof verification 1 The development of the proofs as described here is quite different from the way in which these results evolved. Therefore the ideas from these past developments cannot be fully localized within our exposition. <p> This question seems to get mapped down to the question of the efficiency of the low-degree test and the question about the field sizes required for the Arora-Safra Tester that is raised at the end of Chapter 3. 62 Chapter 5 Hardness of approximations The notion of NP-completeness <ref> [Coo71, Lev73, Kar72] </ref> was developed primarily as an attempt to explain the apparent intractability of a large family of combinatorial optimization problems. The resulting theoretical framework [GJ79] was defined mainly in terms of decision problems obtained by imposing bounds on the value of the objective function.
Reference: [DJP + 92] <author> E. Dalhaus, D.S. Johnson, C.H. Papadimitriou, P.D. Seymour, and M. Yannakakis. </author> <title> The complexity of multiway cuts. </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 241-251, </pages> <year> 1992. </year>
Reference-contexts: In particular, we show that the class MAX SNP, defined by Papadimitriou and Yannakakis [PY91], contains this problem. A large variety of approximation problems are known to be MAX SNP-hard <ref> [PY91, PY92, BP89, BJL + 91, DJP + 92, Kan91] </ref> and thus the result from Chapter 5 translates into non-approximability result for all these problems unless NP = P. 16 Chapter 2 On the resilience of polynomials In this chapter we consider the task of correcting multivariate polynomials.
Reference: [dW70] <editor> Van der Waerden. </editor> <booktitle> Algebra, </booktitle> <volume> volume 1. </volume> <publisher> Frederick Ungar Publishing Co., Inc., </publisher> <year> 1970. </year>
Reference-contexts: The following lemma shows that interpolation (testing if a neighborhood constraint is violated) is a much easier task for evenly spaced points. In fact, the interpolation can be performed without using any multiplication and this makes the tester different from any function evaluating the polynomial. Lemma 3.1.3 (cf. <ref> [dW70] </ref> pages 86-91) The points f (x i ; y i )ji 2 f0; : : : ; d + 1g; x i = x + i fl h; x i ; y i 2 Z p g lie on a degree d polynomial if and only if P d+1 i=0 <p> Instead the summation can be calculated by evaluating all the functions f (j) . In all, this takes O (d 2 ) additions and subtractions, but no multiplications. Furthermore, evenly spaced points suffice to characterize functions that are polynomials. Lemma 3.1.4 (cf. <ref> [dW70] </ref> pages 86-91) f : Z p 7! Z p is a polynomial of degree at most d if and only if 8x; h 2 Z p , P d+1 Proof [Sketch]: Lemma 3.1.3 immediately gives the implication in one direction.
Reference: [Fag74] <author> R. Fagin. </author> <title> Generalized first-order spectra and polynomial-time recognizable sets. </title> <editor> In R. M. Karp, editor, </editor> <booktitle> Complexity of Computation, SIAM-AMS Proceedings, </booktitle> <volume> Vol. 7, </volume> <pages> pages 43-73, </pages> <year> 1974. </year>
Reference-contexts: The most successful of these efforts seems to be the work of Papadimitriou and Yannakakis [PY91] where they used a syntactic characterization of NP due to Fagin <ref> [Fag74] </ref> to define a class called MAX SNP. They also defined a particular approximation preserving reduction called the L-reduction (for linear reductions) used these reductions to find complete problems for this class. <p> Hard problems: These are problems for which no constant factor approximation is known. 5.2 MAX SNP: Constraint Satisfaction Problems The class MAX SNP was defined by Papadimitriou and Yannakakis [PY91] based on the syntactic definition of NP of Fagin <ref> [Fag74] </ref> and on subsequent definition of strict-NP due to Kolaitis and Vardi [KV87]. The formal definitions are presented below. Definition 5.2.1 (NP: [Fag74]) A predicate P on structures I, is in NP if it can be expressed in the form 9S (I; S), where S is a structure and is a <p> is known. 5.2 MAX SNP: Constraint Satisfaction Problems The class MAX SNP was defined by Papadimitriou and Yannakakis [PY91] based on the syntactic definition of NP of Fagin <ref> [Fag74] </ref> and on subsequent definition of strict-NP due to Kolaitis and Vardi [KV87]. The formal definitions are presented below. Definition 5.2.1 (NP: [Fag74]) A predicate P on structures I, is in NP if it can be expressed in the form 9S (I; S), where S is a structure and is a first order predicate. (In the above definition P is equivalent to the problem and I the instance of the problem.) Definition 5.2.2
Reference: [Fei93] <author> J. Feigenbaum. </author> <title> Locally random reductions in interactive complexity theory. </title> <editor> In J. y. Cai, editor, </editor> <booktitle> Complexity Theory, DIMACS Series on Discrete Mathematics and Theoretical Computer Science, </booktitle> <year> 1993. </year> <month> 76 </month>
Reference-contexts: Such an equivalence in the worst case and average case behavior is not known for many functions and might not be true of NP-hard functions. Blum, Luby and Rubinfeld use the notion of random self-reducibility (introduced by [AFK89], see also <ref> [Fei93] </ref>) to exhibit self-correctors for a large collection of functions. This property can be observed in a number of algebraic functions. <p> Notice that a degree d multivariate polynomial restricted to a line becomes a univariate degree d polynomial. 2.2 Achieving some resilience: Random self-reducibility The notion of random self-reducibility was introduced as a tool to implement instance-hiding schemes. The first formal definition occurs in [AFK89] (see also <ref> [Fei93] </ref> for a survey).
Reference: [FF91] <author> J. Feigenbaum and L. Fortnow. </author> <title> On the random self-reducibility of complete sets. </title> <booktitle> In Proceedings of the Sixth IEEE Conference on Structure in Complexity Theory, </booktitle> <pages> pages 121-132, </pages> <year> 1991. </year>
Reference-contexts: But for special polynomials other properties of the function can be used to achieve some resilience and this is indeed the case for the permanent over GF (3) (see Feigenbaum and Fortnow <ref> [FF91] </ref> and Babai and Fortnow [BF91]). The resilience shown by them is inverse polynomial in the dimension of the matrix and it is over a distribution which is not uniform.
Reference: [FGL + 91] <author> U. Feige, S. Goldwasser, L. Lovasz, S. Safra, and M. Szegedy. </author> <title> Approximating clique is almost NP-complete. </title> <booktitle> In Proceedings of the 32nd IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 2-12, </pages> <year> 1991. </year>
Reference-contexts: Such probabilistically checkable proofs are unexpectedly powerful and their power has been explored in several recent papers <ref> [BFL91, BFLS91, FGL + 91, AS92] </ref>. These papers show that even verifiers with severely constrained access to the proof can check proofs of very general statements namely membership for any NP language. <p> Thus, one would hope that the new proof systems should lead to hardness results for finding even approximate solutions to some optimization problem. Feige et al. <ref> [FGL + 91] </ref> were the first to bring out such a connection that shows that approximating the clique size in graphs is hard. Inspired by this result, we bring out a different connection which enables us to show a variety of problems are hard to approximate. <p> Yet, in a surprising twist, Feige et al. <ref> [FGL + 91] </ref>, used the new results on probabilistically checkable proofs to show hardness results for approximating the clique-size. They show that unless NP DTIME (n loglog n ), the size of the largest clique in a graph cannot be estimated to within constant factors. <p> This was followed up by more efficient versions in <ref> [BFLS91, FGL + 91, AS92] </ref>. All the testers did have one common ingredient though: They test for the degree of each variable individually and thus have an inherent W (m) lower bound on the number of probes required. <p> The tester developed in this chapter was developed in parallel in [GLR + 91, She91, RS92]. By not testing for the degree of each variable explicitly, the tester showed potential to perform better than the testers of <ref> [BFL91, BFLS91, FGL + 91, AS92] </ref>. Yet a key element was missing in the analysis of the tester, which was finally remedied by the work of Arora and Safra [AS92]. <p> A second parameter examined by Arora and Safra is the number of random bits used by the verifier. This gives an implicit bound on the size of the proof and is also motivated by some of the applications of such proofs systems <ref> [FGL + 91] </ref> (see also Chapter 5). We define the notion in terms of languages which have efficient probabilistically checkable proofs. <p> The proof sizes as obtained 51 by Babai et al. are nearly linear in the size of any witness of x 2 L.) Feige et al. <ref> [FGL + 91] </ref>, improved on [BFL91] differently to show that NP PCP (log n loglog n; log n loglog n), but their proofs are superpolynomial in size. <p> It hence came as a big surprise when Feige, Goldwasser, Lovasz Safra and Szegedy <ref> [FGL + 91] </ref>, were able to show hardness (under a slightly weaker assumption than P 6= NP) of approximating the clique number of graphs to within constant factors. <p> The assumption could be made weaker using some of the known derandomization techniques (say, using the idea of recycling random bits [IZ89]). The result could also be observed from the reduction of Feige et al. <ref> [FGL + 91] </ref> as a special case. 69 since each T r;x is a constraint on a constant number of the variables i , this fits the definition of a constraint satisfaction problem and thus is a MAX SNP problem. 2 Claim 5.3.2 Approximating MAX PCP to within 10% is NP-hard. <p> The efficient randomized algorithms can be converted into fast deterministic parallel algorithms, and such error detecting and correcting schemes might be of some interest. 71 The surprising connection between efficient probabilistically checkable proofs and the hardness of approximating clique sizes in graphs, due to <ref> [FGL + 91] </ref>, is now much better understood. Here we showed that for every MAX SNP-hard problem, there exists a constant * such that approximating the optimum value to within a relative error of * is NP-hard.
Reference: [FL92a] <author> U. Feige and L. Lovasz. </author> <title> Two-prover one-round proof systems: Their power and their problems. </title> <booktitle> In Proceedings of the 24th ACM Symposium on Theory of Computing, </booktitle> <pages> pages 733-744, </pages> <year> 1992. </year>
Reference-contexts: In order to cut down on the number of questions asked of such an oracle we use some of the work done on parallelizing the MIP = NEXPTIME protocol by Lapidot and Shamir [LS91], and some improvements on it by Feige and Lovasz <ref> [FL92a] </ref>. <p> The rPCP (log n; polylog n) protocol is based on the work done by Lapidot and Shamir [LS91] and Feige and Lovasz <ref> [FL92a] </ref> on parallelizing the MIP = NEXPTIME protocol, but has some new elements to it. In particular, the amount of randomness used in the protocols of [LS91, FL92a] seems to be superlogarithmic and we are able to reduce this to O (log n). <p> The rPCP (log n; polylog n) protocol is based on the work done by Lapidot and Shamir [LS91] and Feige and Lovasz [FL92a] on parallelizing the MIP = NEXPTIME protocol, but has some new elements to it. In particular, the amount of randomness used in the protocols of <ref> [LS91, FL92a] </ref> seems to be superlogarithmic and we are able to reduce this to O (log n). Moreover, their final protocols does not seem to be able to handle the situation where the input comes in a constant number of error-corrected pieces.
Reference: [FL92b] <author> U. Feige and C. Lund. </author> <title> On the hardness of computing the permanent of random matrices. </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 643-654, </pages> <year> 1992. </year>
Reference-contexts: The improvements shown here now imply that computing the permanent on even 1=2 + ffi fraction of all n fi n matrices from a large finite field is hard unless #P = BPP. Improving on this work further, Feige and Lund <ref> [FL92b] </ref>, have shown that unless #P = S P 2 (which in particular implies a collapse of the polynomial hierarchy), the permanent of n fi n matrices cannot be computed on even exponentially small fraction of all matrices (over large finite fields).
Reference: [Fre79] <author> R. Freivalds. </author> <title> Fast probabilistic algorithms. </title> <booktitle> In Lecture Notes in Computer Science, </booktitle> <pages> pages 57-69. </pages> <publisher> Springer-Verlag, </publisher> <year> 1979. </year>
Reference-contexts: At this point the only aspect left to be tested is that b = a ffi a and that c = a ffi b. We now test that these properties hold. These tests will be based on the randomized algorithm for verifying matrix 58 products, due to Freivalds <ref> [Fre79] </ref>. Consider the n fi n matrix X such X ij = b ij and let Y be the n fi n matrix obtained by viewing a ffi a as an n fi n matrix. The property we wish to verify is that X = Y .
Reference: [FRS88] <author> L. Fortnow, J. Rompel, and M. Sipser. </author> <title> On the power of multi-prover interactive protocols. </title> <booktitle> In Proceedings of the 3rd Annual IEEE Symposium on Structure in Complexity Theory, </booktitle> <pages> pages 156-161, </pages> <year> 1988. </year>
Reference-contexts: This is the defining property of the fundamental complexity class NP. Recently this notion has been extended by allowing the polynomial time verifier access to random bits and extending the notion of a proof to allow the verifier a tiny probability of accepting a fallacious proof (see <ref> [Bab85, GMR89, BOGKW88, FRS88, BFLS91, AS92] </ref>). Such probabilistically checkable proofs are unexpectedly powerful and their power has been explored in several recent papers [BFL91, BFLS91, FGL + 91, AS92]. <p> A related model of interactive proofs which is of more interest to us is the model where the verifier gets to ask questions from more than one non-interacting provers [BOGKW88] or equivalently when the prover is assumed to be a non-adaptive entity i.e., an oracle <ref> [FRS88] </ref>. Languages which admit efficient proofs of membership under the multiple prover proof system are said to be in the class MIP and the recent result of Babai, Fortnow and Lund [BFL91] provides an exact characterization of this class i.e., MIP = NEXPTIME.
Reference: [GJ76] <author> M.R. Garey and D.S. Johnson. </author> <title> The complexity of near-optimal graph coloring. </title> <journal> Journal of the ACM, </journal> <volume> 23 </volume> <pages> 43-49, </pages> <year> 1976. </year>
Reference-contexts: Chromatic number. Some initial success was obtained in showing the hardness of even approximating certain problems: For the traveling salesman problem without triangle inequality Sahni and Gonzalez [SG76] showed that finding a solution within any constant factor of optimal is also NP-hard. Garey and Johnson <ref> [GJ76] </ref> showed that the chromatic number a graph could not be approximated to within a factor 63 of 2 *. They also show that if the clique number of a graph cannot be approximated to within some constant factor, then it cannot be approximated to within any constant factor.
Reference: [GJ78] <author> M.R. Garey and D.S. Johnson. </author> <title> strong NP-completeness: Motivation, examples and implications. </title> <journal> Journal of the ACM, </journal> <volume> 25 </volume> <pages> 499-508, </pages> <year> 1978. </year>
Reference-contexts: The running time of A * on inputs of length n is bounded by a polynomial in n. (The input here is a description of the solution space S and the function f .) The research efforts of the past two decades <ref> [ADP77, ADP80, AMSP80, GJ79, GJ78, PM81] </ref> have broadly aimed at classifying approximation versions of optimization problems into one of the following classes: 1.
Reference: [GJ79] <author> R. Garey and David S. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <year> 1979. </year>
Reference-contexts: The resulting theoretical framework <ref> [GJ79] </ref> was defined mainly in terms of decision problems obtained by imposing bounds on the value of the objective function. <p> Problem which seemed equivalent when the goal was to find exact solutions, seems to break apart into problems of widely varying complexity when the goal was relaxed to that of finding approximate solutions. Some problems like the knapsack problem have extremely good approximation algorithms <ref> [GJ79] </ref>. Other problems have algorithms where the error of approximation can be made arbitrarily small, but the penalties paid for improved solutions are heavy. An example of such a problem is the task of minimizing the makespan on a parallel machine a scheduling problem studied in [HS87]. <p> The running time of A * on inputs of length n is bounded by a polynomial in n. (The input here is a description of the solution space S and the function f .) The research efforts of the past two decades <ref> [ADP77, ADP80, AMSP80, GJ79, GJ78, PM81] </ref> have broadly aimed at classifying approximation versions of optimization problems into one of the following classes: 1.
Reference: [GLR + 91] <author> P. Gemmell, R. Lipton, R. Rubinfeld, M. Sudan, and A. Wigderson. </author> <title> Self-testing/correcting for polynomials and for approximate functions. </title> <booktitle> In Proceedings of the 23rd Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 32-42, </pages> <year> 1991. </year>
Reference-contexts: Next, we show how to solve the problem of multivariate self-correction, by giving a technique for picking especially nice univariate subdomains. The results of this section appear in <ref> [GLR + 91] </ref> and [GS92]. 2.3.1 Univariate polynomials: Error correcting codes Here, we wish to solve the following problem: Problem 1 Given: A function f : F 7! F such that D (f; F (d) [x]) 1=2 ffi and a point a 2 F . <p> Notice that their reduction does indeed construct a univariate subdomain by picking D to be a line through the space F m . But this construction only achieves a very weak form of property 3. This is pointed out by Gemmell et al. <ref> [GLR + 91] </ref>, where it is shown, using Markov's Inequality, that if f and g agree on all but * fraction of the inputs from F m , then with probability 1 1 k , f and g agree on all but k* fraction of the inputs from the domain D. <p> This also allows <ref> [GLR + 91] </ref> to show that the family of multivariate polynomials is (1=4 ffi)-resilient. <p> By Lemma 3.1.4 g must be a polynomial of degree at most d. 2 Proof: [of Theorem 3.1] Follows from Lemmas 3.1.5 and 3.1.7. 2 This theorem can now be used to construct a tester for univariate polynomials as follows. This tester first appeared in <ref> [GLR + 91] </ref> and [Rub90]. program Evenly-Spaced-Test Repeat O (d 2 log (1=fi)) times Pick x; t 2 R Z p and verify that P d+1 Reject if any of the test fails Theorem 3.2 If the computation of a program can be expressed by a low-degree polynomial correctly on all <p> All the testers did have one common ingredient though: They test for the degree of each variable individually and thus have an inherent W (m) lower bound on the number of probes required. The tester developed in this chapter was developed in parallel in <ref> [GLR + 91, She91, RS92] </ref>. By not testing for the degree of each variable explicitly, the tester showed potential to perform better than the testers of [BFL91, BFLS91, FGL + 91, AS92].
Reference: [GMR89] <author> S. Goldwasser, S. Micali, and C. Rackoff. </author> <title> The knowledge complexity of interactive proof systems. </title> <journal> SIAM Journal on Computing, </journal> <volume> 18 </volume> <pages> 186-208, </pages> <year> 1989. </year> <month> 77 </month>
Reference-contexts: This is the defining property of the fundamental complexity class NP. Recently this notion has been extended by allowing the polynomial time verifier access to random bits and extending the notion of a proof to allow the verifier a tiny probability of accepting a fallacious proof (see <ref> [Bab85, GMR89, BOGKW88, FRS88, BFLS91, AS92] </ref>). Such probabilistically checkable proofs are unexpectedly powerful and their power has been explored in several recent papers [BFL91, BFLS91, FGL + 91, AS92]. <p> Then how easy is it to persuade the verifier of such a fact. (The terminology used here comes from the area of interactive proofs <ref> [Bab85, BM88, GMR89] </ref>.) We make this setting more precise now: We will expect the prover to provide the additional information about f in the form of an oracle O. <p> Our results improve on the efficiency of such schemes and extend it to include all the codes here. 1.4 Probabilistic Checking of Proofs The notion of interactive proofs was first defined by Babai [Bab85, BM88] and Goldwasser, Micali and Rackoff <ref> [GMR89] </ref>. They study languages which permit interactive proofs of membership which are verifiable by a probabilistic verifier in polynomial time and call the collection of such languages IP.
Reference: [GMW86] <author> O. Goldreich, S. Micali, and A. Wigderson. </author> <title> Proofs that yield nothing but their validity and a methodology of cryptographic protocol design. </title> <booktitle> In Proceedings of the 27th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 174-187, </pages> <year> 1986. </year>
Reference-contexts: They study languages which permit interactive proofs of membership which are verifiable by a probabilistic verifier in polynomial time and call the collection of such languages IP. Goldwasser, Micali and Wigderson <ref> [GMW86] </ref> provided evidence to show that the class IP 13 strictly contains NP, by showing that graph non-isomorphism, a problem not known to be in NP, can be proved efficiently interactively and thus lies in the class IP.
Reference: [GS92] <author> P. Gemmell and M. Sudan. </author> <title> Highly resilient correctors for polynomials. </title> <journal> Information Processing Letters, </journal> <note> 1992. To appear. </note>
Reference-contexts: Next, we show how to solve the problem of multivariate self-correction, by giving a technique for picking especially nice univariate subdomains. The results of this section appear in [GLR + 91] and <ref> [GS92] </ref>. 2.3.1 Univariate polynomials: Error correcting codes Here, we wish to solve the following problem: Problem 1 Given: A function f : F 7! F such that D (f; F (d) [x]) 1=2 ffi and a point a 2 F .
Reference: [HS85] <author> D.S. Hochbaum and D.B. Shmoys. </author> <title> A best possible heuristic for the k-center problem. </title> <journal> Mathematics of Operations Research, </journal> <volume> 10(2) </volume> <pages> 180-184, </pages> <month> May </month> <year> 1985. </year>
Reference-contexts: They also show that if the clique number of a graph cannot be approximated to within some constant factor, then it cannot be approximated to within any constant factor. Hochbaum and Shmoys <ref> [HS85, HS86] </ref> study some minmax problems where they show tight bounds on the factor to which these problems may be approximated unless NP = P.
Reference: [HS86] <author> D.S. Hochbaum and D.B. Shmoys. </author> <title> A unified approach to approximation algorithms for bottleneck problems. </title> <journal> Journal of the ACM, </journal> <volume> 33(3) </volume> <pages> 533-550, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: They also show that if the clique number of a graph cannot be approximated to within some constant factor, then it cannot be approximated to within any constant factor. Hochbaum and Shmoys <ref> [HS85, HS86] </ref> study some minmax problems where they show tight bounds on the factor to which these problems may be approximated unless NP = P.
Reference: [HS87] <author> D.S. Hochbaum and D.B. Shmoys. </author> <title> Using dual approximation algorithms for scheduling problems: Theoretical and practical results. </title> <journal> Journal of the ACM, </journal> <volume> 34(1) </volume> <pages> 144-162, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: Other problems have algorithms where the error of approximation can be made arbitrarily small, but the penalties paid for improved solutions are heavy. An example of such a problem is the task of minimizing the makespan on a parallel machine a scheduling problem studied in <ref> [HS87] </ref>. Yet other problems like the Euclidean TSP and vertex cover seemed approximable to some constant factor but not arbitrarily small ones; and finally we have problems which seem no easier to approximate than to solve exactly e.g. Chromatic number.
Reference: [IZ89] <author> R. Impagliazzo and D. Zuckerman. </author> <title> How to recycle random bits. </title> <booktitle> In Proceedings of the 30th Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <year> 1989. </year>
Reference-contexts: A naive implementation of the tester would thus take O (km log jF j) random bits. But we can save on this randomness by recycling the random bits via the technique of Impagliazzo and Zuckerman <ref> [IZ89] </ref>. This would allow each additional phase to reuse most of the old random bits, and would need only a constant number of fresh random bits per phase. <p> Further, 1 Berman and Schnitger [BS92] showed the hardness result mentioned here under the assumption that MAX 3-SAT did not have randomized PTAS. The assumption could be made weaker using some of the known derandomization techniques (say, using the idea of recycling random bits <ref> [IZ89] </ref>).
Reference: [Kan90] <author> S. Kannan. </author> <title> Program Result Checking with Applications. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <year> 1990. </year>
Reference-contexts: In the rest of this chapter we introduce in greater detail the areas to which the problems described above relate and describe the effect of the solutions obtained here to these areas. 1.2 Program Result Checking The notion of program result checking was initiated by Blum and Kannan <ref> [Blu88, BK89, Kan90] </ref> as a new approach for ensuring software reliability. The approach advocates the use of fast checks performed at runtime to obtain guarantees on the correctness of the solution obtained on specific runs of the program. <p> These features make program checking an attractive alternative to some of the traditional methods for guaranteeing reliability of programs. For a more detailed study of checking and a comparison of its advantages versus other techniques for ensuring software reliability, see <ref> [Kan90, Rub90] </ref>. A significant amount of success has been achieved in the task of constructing checkers and checkers are available for a wide variety of problems including sorting, linear programming and number theoretic applications [AHK, BK89, Kan90]. <p> A significant amount of success has been achieved in the task of constructing checkers and checkers are available for a wide variety of problems including sorting, linear programming and number theoretic applications <ref> [AHK, BK89, Kan90] </ref>. A particular approach to the task of obtaining checkers that has met with considerable success was introduced by Blum, Luby and Rubinfeld [BLR90]. 8 They decompose the task of checking a program into two phases: a preprocessing phase and a runtime phase.
Reference: [Kan91] <author> V. Kann. </author> <title> Maximum bounded 3-dimensional matching is MAX SNP-complete. </title> <journal> Information Processing Letters, </journal> <volume> 37 </volume> <pages> 27-35, </pages> <year> 1991. </year>
Reference-contexts: In particular, we show that the class MAX SNP, defined by Papadimitriou and Yannakakis [PY91], contains this problem. A large variety of approximation problems are known to be MAX SNP-hard <ref> [PY91, PY92, BP89, BJL + 91, DJP + 92, Kan91] </ref> and thus the result from Chapter 5 translates into non-approximability result for all these problems unless NP = P. 16 Chapter 2 On the resilience of polynomials In this chapter we consider the task of correcting multivariate polynomials.
Reference: [Kar72] <author> R.M. Karp. </author> <title> Reducibility among combinatorial problems. In R.E. </title> <editor> Miller and J.W. Thatcher, editors, </editor> <booktitle> Complexity of Computer Computations, </booktitle> <pages> pages 85-103. </pages> <publisher> Plenum Press, </publisher> <year> 1972. </year>
Reference-contexts: Here we have tried to highlight this connection be deriving previously known results as well as some of the new results in a uniform manner from our results on the testing of polynomials. 1 The early papers on NP-completeness <ref> [Coo71, Lev73, Kar72] </ref> linked the notion of proof verification 1 The development of the proofs as described here is quite different from the way in which these results evolved. Therefore the ideas from these past developments cannot be fully localized within our exposition. <p> This question seems to get mapped down to the question of the efficiency of the low-degree test and the question about the field sizes required for the Arora-Safra Tester that is raised at the end of Chapter 3. 62 Chapter 5 Hardness of approximations The notion of NP-completeness <ref> [Coo71, Lev73, Kar72] </ref> was developed primarily as an attempt to explain the apparent intractability of a large family of combinatorial optimization problems. The resulting theoretical framework [GJ79] was defined mainly in terms of decision problems obtained by imposing bounds on the value of the objective function.
Reference: [KMR92] <author> D. Karger, R. Motwani, and G.D.S. Ramkumar. </author> <title> On approximating the longest path in a graph. </title> <type> Manuscript, </type> <year> 1992. </year>
Reference: [KV87] <author> P. Kolaitis and M. Vardi. </author> <title> The decision problem for the probabilities of higher-order properties. </title> <booktitle> In Proceedings of the 19th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 425-435, </pages> <year> 1987. </year> <month> 78 </month>
Reference-contexts: problems: These are problems for which no constant factor approximation is known. 5.2 MAX SNP: Constraint Satisfaction Problems The class MAX SNP was defined by Papadimitriou and Yannakakis [PY91] based on the syntactic definition of NP of Fagin [Fag74] and on subsequent definition of strict-NP due to Kolaitis and Vardi <ref> [KV87] </ref>. The formal definitions are presented below. <p> predicate P on structures I, is in NP if it can be expressed in the form 9S (I; S), where S is a structure and is a first order predicate. (In the above definition P is equivalent to the problem and I the instance of the problem.) Definition 5.2.2 (SNP: <ref> [KV87] </ref>) A predicate P on structures I , is in SNP if it can be expressed in the form 9S8x (x; I; S), where S is a structure and is a quantifier free predicate.
Reference: [Lev73] <author> L. Levin. </author> <title> Universal'nye perebornye zadachi (Universal search problems, in Rus-sian). </title> <journal> Problemy Peredachi Informatsii, </journal> <volume> 9 </volume> <pages> 265-266, </pages> <year> 1973. </year> <note> A corrected English translation appears in an appendix to Trakhtenbrot [Tra84]. </note>
Reference-contexts: Here we have tried to highlight this connection be deriving previously known results as well as some of the new results in a uniform manner from our results on the testing of polynomials. 1 The early papers on NP-completeness <ref> [Coo71, Lev73, Kar72] </ref> linked the notion of proof verification 1 The development of the proofs as described here is quite different from the way in which these results evolved. Therefore the ideas from these past developments cannot be fully localized within our exposition. <p> This question seems to get mapped down to the question of the efficiency of the low-degree test and the question about the field sizes required for the Arora-Safra Tester that is raised at the end of Chapter 3. 62 Chapter 5 Hardness of approximations The notion of NP-completeness <ref> [Coo71, Lev73, Kar72] </ref> was developed primarily as an attempt to explain the apparent intractability of a large family of combinatorial optimization problems. The resulting theoretical framework [GJ79] was defined mainly in terms of decision problems obtained by imposing bounds on the value of the objective function.
Reference: [LFKN90] <author> C. Lund, L. Fortnow, H. Karloff, and N. Nisan. </author> <title> Algebraic methods for interactive proof systems. </title> <booktitle> In Proceedings of the 31st Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 2-10, </pages> <year> 1990. </year>
Reference-contexts: Recent breakthroughs completely characterize the power of IP and the results of Lund, Fortnow, Karloff and Nisan <ref> [LFKN90] </ref> and Shamir [Sha90] shows IP = PSPACE. <p> The tester for such functions can be obtained from the downward-self-reducibility theorem of Blum, Luby and Rubinfeld [BLR90]. We include a version of their theorem here that is weaker than the original theorem they prove. The tester also resembles the Sum-Check protocols of <ref> [LFKN90, Sha90, BFL91] </ref>. Let ffi &gt; 0 be a constant smaller than the constant of Theorem 3.7.
Reference: [Lip91] <author> R. Lipton. </author> <title> New directions in testing. </title> <journal> Distributed Computing and Cryptography, DIMACS Series in Discrete Math and Theoretical Computer Science, American Mathematical Society, </journal> <volume> 2 </volume> <pages> 191-202, </pages> <year> 1991. </year>
Reference-contexts: The former phase is the self-testing phase and the latter phase is referred to as the self-correcting phase. The notion of self-correction was also independently introduced by Lipton <ref> [Lip91] </ref>. We state the formal definitions next. <p> Then it can be shown that all linear functions have the random self-reducibility property (see [BLR90]). [BLR90] cite many examples of linear functions: The Mod function, Integer Multiplication, Modular Multiplication, Integer Division, Matrix Multiplication etc. all of which have self-correctors due to this observation. <ref> [Lip91] </ref> based on the techniques of [BF90] has similarly shown the random self-reducibility of multivariate polynomials and thus all multivariate polynomials have self-correctors. While the existence of self-correctors of functions may be a rare occurrence, in the cases where they have been found, proofs of correctness have been straightforward. <p> The following observation was made by Blum, Luby and Rubinfeld [BLR90]. Observation 2.2.2 Every random self-reducible function has a self-corrector. Lipton <ref> [Lip91] </ref> based on the work of Beaver and Feigenbaum [BF90] shows that the family of multivariate polynomials over large finite fields are random self-reducible. Lemma 2.2.3 ([BF90],[Lip91]) Let g : F m 7! F be a degree d polynomial, where F is a finite field such that jF j d + <p> They also show that an extension of the methods from here can reduce the multivariate version of this problem to the univariate case. The univariate case for general error still remains open. Implications for the permanent Lipton <ref> [Lip91] </ref> observed that the permanent of an n fi n matrix is a polynomial of degree n and is hence random self-reducible (over sufficiently large finite fields). <p> draw the input from the uniform distribution (or any polynomial time sampleable distribution) and still get problems that are very hard? Interestingly enough one of the key techniques relying on the power of polynomials random self-reducibility has already been used to show instances of average case hard problems for #P <ref> [Lip91] </ref>. It would be interesting to see if any of these techniques can be used to show average case hardness for problems in NP. 73
Reference: [LS91] <author> D. Lapidot and A. Shamir. </author> <title> Fully parallelized multi prover protocols for NEXPTIME. </title> <booktitle> In Proceedings of the 32nd IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 13-18, </pages> <year> 1991. </year>
Reference-contexts: In order to cut down on the number of questions asked of such an oracle we use some of the work done on parallelizing the MIP = NEXPTIME protocol by Lapidot and Shamir <ref> [LS91] </ref>, and some improvements on it by Feige and Lovasz [FL92a]. <p> The rPCP (log n; polylog n) protocol is based on the work done by Lapidot and Shamir <ref> [LS91] </ref> and Feige and Lovasz [FL92a] on parallelizing the MIP = NEXPTIME protocol, but has some new elements to it. In particular, the amount of randomness used in the protocols of [LS91, FL92a] seems to be superlogarithmic and we are able to reduce this to O (log n). <p> The rPCP (log n; polylog n) protocol is based on the work done by Lapidot and Shamir [LS91] and Feige and Lovasz [FL92a] on parallelizing the MIP = NEXPTIME protocol, but has some new elements to it. In particular, the amount of randomness used in the protocols of <ref> [LS91, FL92a] </ref> seems to be superlogarithmic and we are able to reduce this to O (log n). Moreover, their final protocols does not seem to be able to handle the situation where the input comes in a constant number of error-corrected pieces.
Reference: [Lub86] <author> M. Luby. </author> <title> A simple parallel algorithm for the maximal independent set problem. </title> <journal> SIAM Journal of Computing, </journal> <volume> 15(4) </volume> <pages> 1036-1053, </pages> <year> 1986. </year>
Reference-contexts: Proof [Sketch]: Let I be the indicator variable for the condition f = g i.e., I ( x) = &lt; 1 if f ( x) = g ( x) 0 otherwise Then by the fact that Chebychev's Inequality holds for pairwise independent variables (see for instance <ref> [Lub86] </ref>) one can conclude that the expected value of I over the domain S is very close to the expected value of I over the domain F m .
Reference: [LY92] <author> C. Lund and M. Yannakakis. </author> <title> On the hardness of approximating minimization problems. </title> <type> Preprint, </type> <month> June </month> <year> 1992. </year>
Reference-contexts: Here we showed that for every MAX SNP-hard problem, there exists a constant * such that approximating the optimum value to within a relative error of * is NP-hard. More recently, Lund and Yannakakis <ref> [LY92] </ref> have shown strong hardness results for approximating the chromatic number of graphs and approximating the minimum set cover size for a family of sets. In the light of all these developments, it appears that this connection between probabilistically checkable proofs and approximation problems is a fundamental one. <p> A concrete example of such a result may be found in <ref> [LY92] </ref>, where they show the hardness of approximating a certain problem thus providing the first proof showing hardness of exact computation for that problem.
Reference: [PM81] <author> A. Paz and S. Moran. </author> <title> Non-deterministic polynomial optimization problems and their approximation. </title> <journal> Theoretical Computer Science, </journal> <volume> 15 </volume> <pages> 251-277, </pages> <year> 1981. </year>
Reference-contexts: The lack of approximation preserving reductions among optimization problems seemed to isolate these efforts and the search for such reductions became the goal of a wide body of research <ref> [ADP77, ADP80, AMSP80, PM81] </ref>. The most successful of these efforts seems to be the work of Papadimitriou and Yannakakis [PY91] where they used a syntactic characterization of NP due to Fagin [Fag74] to define a class called MAX SNP. <p> The running time of A * on inputs of length n is bounded by a polynomial in n. (The input here is a description of the solution space S and the function f .) The research efforts of the past two decades <ref> [ADP77, ADP80, AMSP80, GJ79, GJ78, PM81] </ref> have broadly aimed at classifying approximation versions of optimization problems into one of the following classes: 1.
Reference: [PS92] <author> S. Phillips and S. Safra. </author> <type> Personal Communication, </type> <month> June </month> <year> 1992. </year>
Reference-contexts: the question would ask: How many bits of a proof need to be examined if the verifier is expected to reject false claims with probability half? At present, the best known bound on the number of bits probed in polynomial sized proofs seems to be less than a 100 bits. <ref> [PS92, Sud92] </ref>. Such a result would translate to showing that MAX 3-SAT cannot be approximated to within 1 300 . This, of course, is far from being tight (the best known upper bound for the constant is 1=8).
Reference: [PY91] <author> C. Papadimitriou and M. Yannakakis. </author> <title> Optimization, approximation and complexity classes. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 43 </volume> <pages> 425-440, </pages> <year> 1991. </year>
Reference-contexts: Inspired by this result, we bring out a different connection which enables us to show a variety of problems are hard to approximate. The problems we consider are based on the class MAX SNP, defined by Papadimitriou and Yannakakis <ref> [PY91] </ref>. <p> Almost by definition this problem turns out to be NP-hard to approximate. The structure of the problem turns out be very simple and hence can be reduced to many other optimization problems. In particular, we show that the class MAX SNP, defined by Papadimitriou and Yannakakis <ref> [PY91] </ref>, contains this problem. <p> In particular, we show that the class MAX SNP, defined by Papadimitriou and Yannakakis [PY91], contains this problem. A large variety of approximation problems are known to be MAX SNP-hard <ref> [PY91, PY92, BP89, BJL + 91, DJP + 92, Kan91] </ref> and thus the result from Chapter 5 translates into non-approximability result for all these problems unless NP = P. 16 Chapter 2 On the resilience of polynomials In this chapter we consider the task of correcting multivariate polynomials. <p> The lack of approximation preserving reductions among optimization problems seemed to isolate these efforts and the search for such reductions became the goal of a wide body of research [ADP77, ADP80, AMSP80, PM81]. The most successful of these efforts seems to be the work of Papadimitriou and Yannakakis <ref> [PY91] </ref> where they used a syntactic characterization of NP due to Fagin [Fag74] to define a class called MAX SNP. They also defined a particular approximation preserving reduction called the L-reduction (for linear reductions) used these reductions to find complete problems for this class. <p> Hard problems: These are problems for which no constant factor approximation is known. 5.2 MAX SNP: Constraint Satisfaction Problems The class MAX SNP was defined by Papadimitriou and Yannakakis <ref> [PY91] </ref> based on the syntactic definition of NP of Fagin [Fag74] and on subsequent definition of strict-NP due to Kolaitis and Vardi [KV87]. The formal definitions are presented below. <p> Definition 5.2.3 (MAX SNP: <ref> [PY91] </ref>) An optimization problem P on structures I is in MAX SNP if its objective function can be expressed as max jfx : (x; I ; S)gj 66 The following problem provides an alternate view of MAX SNP. <p> The proof of this claim is straightforward and omitted. Papadimitriou and Yannakakis also introduced the notion of a linear reduction (L-reduction) which is an approximation preserving reduction. The notion of L-reductions allows them to find complete problems for the class MAX SNP. Definition 5.2.6 (linear reduction: <ref> [PY91] </ref>) Optimization problem P L-reduces to P 0 if there exist polynomial time computable functions f; g and constant ff; fi 0 such that 1. f reduces an instance I of P to an instance I 0 of P 0 with the property that OPT (I) ffOPT (I 0 ). 2.
Reference: [PY92] <author> C. H. Papadimitriou and M. Yannakakis. </author> <title> The traveling salesman problem with distances one and two. </title> <note> In Mathematics of Operations Research, 1992. To appear. </note>
Reference-contexts: In particular, we show that the class MAX SNP, defined by Papadimitriou and Yannakakis [PY91], contains this problem. A large variety of approximation problems are known to be MAX SNP-hard <ref> [PY91, PY92, BP89, BJL + 91, DJP + 92, Kan91] </ref> and thus the result from Chapter 5 translates into non-approximability result for all these problems unless NP = P. 16 Chapter 2 On the resilience of polynomials In this chapter we consider the task of correcting multivariate polynomials.
Reference: [RS60] <author> I.S. Reed and G. Solomon. </author> <title> Polynomial codes over certain finite fields. </title> <journal> Journal of the Society of Industrial and Applied Mathematics, </journal> <volume> 8 </volume> <pages> 300-304, </pages> <month> June </month> <year> 1960. </year>
Reference-contexts: In the specific case of low-degree polynomials, the Schwartz's Lemma guarantees large distance between any two message words. Next we give examples of codes which may be obtained from polynomials. Reed Solomon Codes Let F be a finite field of size n. The Generalized Reed Solomon codes (see <ref> [RS60] </ref>) encode messages from the space F k into codewords in the range F n as follows. Let &lt; c 0 ; : : : ; c k1 &gt;2 F k be a message.
Reference: [RS92] <author> R. Rubinfeld and M. Sudan. </author> <title> Testing polynomial functions efficiently and over rational domains. </title> <booktitle> In Proceedings of the 3rd Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 23-43, </pages> <year> 1992. </year> <month> 79 </month>
Reference-contexts: With a bit more work, the same ideas can even be used to test polynomials over the reals and the integers. Such a test is not described here. Details of this tester appear in <ref> [RS92] </ref>. The interesting element of the tester is that it reveals that testing low-degreeness over strongly correlated samples suffices to establish low-degreeness over the whole domain. <p> R F Verify that f (x; y 0 ); : : : ; f (x; y d+1 ) is a degree d poly (in y) Reject if the test fails The following theorem is needed to prove the correctness of the tester and appears as the Matrix Transposition Lemma in <ref> [RS92] </ref>. Notice that in the above test, the number of iterations has not been specified yet. This will depend on how strong our theorem is. We will establish here that k = O (d) suffices. A tighter theorem on this was proved more recently by Arora and Safra [AS92]. <p> The connection yields improved testers for multivariate polynomials and can be used to construct testers for the family of degree d polynomials which look at the value of a function at only O (d) points to test it. The proof of this section is essentially from from <ref> [RS92] </ref>. The efficiency shown here is better than that shown in [RS92], due to the use of Lemma 3.6 which is from [AS92]. <p> The proof of this section is essentially from from <ref> [RS92] </ref>. The efficiency shown here is better than that shown in [RS92], due to the use of Lemma 3.6 which is from [AS92]. Definition 3.2.2 For x; h 2 F m , the set of points f x + t fl hjt 2 F g will be called the line through x with offset h. <p> All the testers did have one common ingredient though: They test for the degree of each variable individually and thus have an inherent W (m) lower bound on the number of probes required. The tester developed in this chapter was developed in parallel in <ref> [GLR + 91, She91, RS92] </ref>. By not testing for the degree of each variable explicitly, the tester showed potential to perform better than the testers of [BFL91, BFLS91, FGL + 91, AS92]. <p> Yet a key element was missing in the analysis of the tester, which was finally remedied by the work of Arora and Safra [AS92]. The observation that putting <ref> [RS92] </ref> and [AS92] together yields a test which requires only a constant number of probes is made in [ALM + 92]. One interesting open question is: How large must the field size be as a function of the degree d, so that the low-degree test of Theorem 3.7 works. <p> As a starting point for this one would need testers which don't depend on the nice properties of finite fields. Such a test does exist and is described in <ref> [RS92] </ref>. But even for univariate functions that are closely approximated by polynomials, no tester seems to be known.
Reference: [Rub90] <author> R. Rubinfeld. </author> <title> A Mathematical Theory of Self-Checking, Self-Testing and Self-Correcting Programs. </title> <type> PhD thesis, </type> <institution> University of California at Berkeley, </institution> <year> 1990. </year>
Reference-contexts: To do this, we have to develop some tools which reveal some new characteristics of low-degree polynomials over finite fields. Our motivation for studying these problems came from the theory of self-testing/correcting programs <ref> [BLR90, Rub90] </ref>. It turns out that there is a fundamental connection between the testing and correcting of polynomials and the existence of efficient probabilistically checkable proofs. <p> These features make program checking an attractive alternative to some of the traditional methods for guaranteeing reliability of programs. For a more detailed study of checking and a comparison of its advantages versus other techniques for ensuring software reliability, see <ref> [Kan90, Rub90] </ref>. A significant amount of success has been achieved in the task of constructing checkers and checkers are available for a wide variety of problems including sorting, linear programming and number theoretic applications [AHK, BK89, Kan90]. <p> By Lemma 3.1.4 g must be a polynomial of degree at most d. 2 Proof: [of Theorem 3.1] Follows from Lemmas 3.1.5 and 3.1.7. 2 This theorem can now be used to construct a tester for univariate polynomials as follows. This tester first appeared in [GLR + 91] and <ref> [Rub90] </ref>. program Evenly-Spaced-Test Repeat O (d 2 log (1=fi)) times Pick x; t 2 R Z p and verify that P d+1 Reject if any of the test fails Theorem 3.2 If the computation of a program can be expressed by a low-degree polynomial correctly on all its inputs from Z
Reference: [Sch80] <author> J. T. Schwartz. </author> <title> Fast probabilistic algorithms for verification of polynomial identities. </title> <journal> Journal of the ACM, </journal> <volume> 27 </volume> <pages> 701-717, </pages> <year> 1980. </year>
Reference-contexts: Using the above notation, we can express the distance property of polynomials as follows: Lemma 1.1.2 (cf. <ref> [Sch80] </ref>) Let f and g be polynomials over F in m variables with total degree at most d. Then d (f; g) 1 d jF j . Thus for sufficiently large finite fields (jF j &gt;> d), the distance between two polynomials is nearly 1. <p> To do this we use the Multivariate Self Corrector given by Theorem 2.1, to compute g 0 ( x) for x 2 I m . Now we can estimate the quantity Pr x2F m g ( x) = g 0 ( x) . By Schwartz's Lemma <ref> [Sch80] </ref> we know that if this quantity is greater than jIjd jIj then g g 0 . Thus using O ( jIj jIjd ) evaluations of g 0 we can test whether g 0 = g or not.
Reference: [SG76] <author> S. Sahni and T. Gonzalez. </author> <title> P-complete approximation problems. </title> <journal> Journal of the ACM, </journal> <volume> 23 </volume> <pages> 555-565, </pages> <year> 1976. </year>
Reference-contexts: Chromatic number. Some initial success was obtained in showing the hardness of even approximating certain problems: For the traveling salesman problem without triangle inequality Sahni and Gonzalez <ref> [SG76] </ref> showed that finding a solution within any constant factor of optimal is also NP-hard. Garey and Johnson [GJ76] showed that the chromatic number a graph could not be approximated to within a factor 63 of 2 *.
Reference: [Sha90] <author> A. Shamir. </author> <title> IP = PSPACE. </title> <booktitle> In Proceedings of the 31st Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 11-15, </pages> <year> 1990. </year>
Reference-contexts: Recent breakthroughs completely characterize the power of IP and the results of Lund, Fortnow, Karloff and Nisan [LFKN90] and Shamir <ref> [Sha90] </ref> shows IP = PSPACE. <p> The tester for such functions can be obtained from the downward-self-reducibility theorem of Blum, Luby and Rubinfeld [BLR90]. We include a version of their theorem here that is weaker than the original theorem they prove. The tester also resembles the Sum-Check protocols of <ref> [LFKN90, Sha90, BFL91] </ref>. Let ffi &gt; 0 be a constant smaller than the constant of Theorem 3.7.
Reference: [She91] <author> A. Shen. </author> <type> Personal Communication, </type> <month> May </month> <year> 1991. </year>
Reference-contexts: All the testers did have one common ingredient though: They test for the degree of each variable individually and thus have an inherent W (m) lower bound on the number of probes required. The tester developed in this chapter was developed in parallel in <ref> [GLR + 91, She91, RS92] </ref>. By not testing for the degree of each variable explicitly, the tester showed potential to perform better than the testers of [BFL91, BFLS91, FGL + 91, AS92].
Reference: [Sud92] <author> M. Sudan. </author> <title> Low-degree tests via degree 2 curves. </title> <note> In preparation, </note> <year> 1992. </year>
Reference-contexts: the question would ask: How many bits of a proof need to be examined if the verifier is expected to reject false claims with probability half? At present, the best known bound on the number of bits probed in polynomial sized proofs seems to be less than a 100 bits. <ref> [PS92, Sud92] </ref>. Such a result would translate to showing that MAX 3-SAT cannot be approximated to within 1 300 . This, of course, is far from being tight (the best known upper bound for the constant is 1=8).

References-found: 69

