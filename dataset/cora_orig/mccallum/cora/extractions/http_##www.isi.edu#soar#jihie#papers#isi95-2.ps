URL: http://www.isi.edu/soar/jihie/papers/isi95-2.ps
Refering-URL: http://www.isi.edu/soar/jihie/chunking.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: jihie@isi.edu, rosenbloom@isi.edu  
Title: Mapping Explanation-Based Learning onto Soar: The Sequel  
Author: Jihie Kim and Paul S. Rosenbloom 
Keyword: Key words: EBL, chunking, Soar, utility problem  
Address: 4676 Admiralty Way Marina del Rey, CA 90292, U.S.A.  
Affiliation: Information Sciences Institute and Computer Science Department University of Southern California  
Abstract: In past work, chunking in Soar has been analyzed as a variant of explanation-based learning. The components and processes underlying EBL have been mapped to their corresponding components and processes in chunking. The cost and generality of the resulting rules have also been compared. Here we extend that work by analyzing an implementation of EBL within Soar as a sequence of transformations from a problem solving episode to a learned rule. The transformations in this sequence, along with their intermediate products, are then evaluated for their effects on the generality and expensiveness of the rules learned, and compared with the results of a similar analysis previously performed for chunking. The analysis reveals that EBL, as implemented for Soar, yields the same sources of expensiveness and overgenerality as does chunking | and that, in fact, these problems stem more from other aspects of Soar than from the details of either learning algorithm. (However, some of these aspects may appear in other AI architectures, even though the analysis is based on Soar.) Moreover this analysis reveals that the differences between EBL and chunking are localized within a single transformation, where chunking overspecializes with respect to EBL. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. M. Mitchell, R. M. Keller, and S. T. Kedar-Cabelli. </author> <title> Explanation-based generalization a unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 47-80, </pages> <year> 1986. </year>
Reference: [2] <author> G. F. DeJong and R. Mooney. </author> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1(2) </volume> <pages> 145-176, </pages> <year> 1986. </year>
Reference: [3] <author> P. S. Rosenbloom, J. E. Laird, A. Newell, and R. McCarl. </author> <title> A preliminary analysis of the Soar architecture as a basis for general intelligence. </title> <journal> Artificial Intelligence, </journal> <volume> 47(1-3):289-325, </volume> <year> 1991. </year>
Reference-contexts: In past work, chunking in Soar <ref> [3] </ref> has been analyzed as a variant of EBL. The four components and the three steps of EBL have been mapped to the components of Soar and to the sub-processes of chunking, respectively [4]. Also, the cost and the generality of the learned rules have been compared [5]. <p> Because no search control is used in the cup domain, the structure of the E-rule is the same as the PS-rule shown in Figure 3. The E-rule acts as an EBL explanation structure. The search-control rules are also missing in chunking <ref> [11, 3] </ref>, based on the assumption that they only affect efficiency, and not correctness of learned rules. The intended purpose of this omission is to increase the generality of the learned rules, by reducing the number of conditions incorporated into learned rules.
Reference: [4] <author> P. S. Rosenbloom and J. E. Laird. </author> <title> Mapping explanation-based generalization onto Soar. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> pages 561-567, </pages> <address> Philadelphia, 1986. </address> <publisher> AAAI. </publisher>
Reference-contexts: In past work, chunking in Soar [3] has been analyzed as a variant of EBL. The four components and the three steps of EBL have been mapped to the components of Soar and to the sub-processes of chunking, respectively <ref> [4] </ref>. Also, the cost and the generality of the learned rules have been compared [5].
Reference: [5] <author> F. Zerr and J. G. Ganascia. </author> <title> Comparison of chunking with EBG implemented onto Soar. </title> <institution> Universite Paris-Sud, Orsay, France and Universite Pierre et Marie Curie, Paris, France. </institution> <month> October, </month> <year> 1989, </year> <note> Unpublished. </note>
Reference-contexts: The four components and the three steps of EBL have been mapped to the components of Soar and to the sub-processes of chunking, respectively [4]. Also, the cost and the generality of the learned rules have been compared <ref> [5] </ref>. We have recently extended this earlier work by implementing EBL within Soar (Version 6) | to yield EBL/Soar | and then analyzing this implementation as a sequence of transformations from a problem solving episode to a learned rule.
Reference: [6] <author> J. Kim and P. S. Rosenbloom. </author> <title> A transformational analysis of expensive chunks. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence. IJCAI, </booktitle> <year> 1995. </year> <note> submitted. </note>
Reference-contexts: In R2, R3, and R4, the training example is accessed through the attribute super-state which links the cup problem-space state and the supergoal state. 3 3 Transforming problem solving to a rule chunking and EBL/Soar. (The chunking part is adapted from <ref> [6] </ref>). Each transformation (except for the last) creates an intermediate structure which is called a pseudo-chunk in chunking and pseudo-rule in EBL/Soar. As the sequences progress, the pseudo-chunks (or pseudo-rules) become more like chunks (or EBL/Soar rules) and less like problem solving. <p> Cost problems may be introduced in going to U-rules (and U-chunks), because the number of instantiations of a rule can be greater than the number of WMEs created from those instantiations, as explained in <ref> [6] </ref>. <p> However, U-rules require the ability to perform non-linear matches, in which conditions are matched hierarchically. They also require the ability to create hierarchically structured tokens. 10 The solution previously proposed for this problem in chunking can be adopted here; preprocessing instantiations before they are used <ref> [6] </ref>.
Reference: [7] <author> M. Tambe, D. Kalp, A. Gupta, C. L. Forgy, B. G. Milnes, and A. Newell. Soar/psm-e: </author> <title> Investigating match parallelism in a learning production system. </title> <booktitle> In Proceedings of the ACM/SIGPLAN Symposium on Parallel Programming: Experience with applications, languages, and systems, </booktitle> <pages> pages 146-160, </pages> <year> 1988. </year>
Reference-contexts: State saving preserves the previous (partial) matches for use in the future. A partial instantiation of a rule, also called a token, is a consistent binding of variables in a subset of the conditions. Because match time per token is known to be approximately constant in Rete <ref> [7, 8] </ref>, we use the number of tokens as a tool for measuring the complexity. 2 The following subsections analyze the transformations underlying EBL/Soar, along with their resulting (pseudo-) rules and their effects on cost and generality.
Reference: [8] <author> M. Tambe. </author> <title> Eliminating combinatorics from production match. </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, </institution> <year> 1991. </year>
Reference-contexts: State saving preserves the previous (partial) matches for use in the future. A partial instantiation of a rule, also called a token, is a consistent binding of variables in a subset of the conditions. Because match time per token is known to be approximately constant in Rete <ref> [7, 8] </ref>, we use the number of tokens as a tool for measuring the complexity. 2 The following subsections analyze the transformations underlying EBL/Soar, along with their resulting (pseudo-) rules and their effects on cost and generality. <p> At each stage from problem solving to an EBL rule (or chunk), match cost is evaluated by counting the number of tokens required during the match to generate the result. So far, the resulting experimental system has been applied to a simple grid-task problem <ref> [8] </ref> which creates one subgoal to break a tie (impasse) among the candidate operators, and creates a search control rule (or chunk). The results of this experiment are shown in Figure 7.
Reference: [9] <author> S. Minton. </author> <title> Quantitative results concerning the utility of explanation-based learning. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 564-569, </pages> <year> 1988. </year>
Reference-contexts: Ignoring these activities can leave holes in the backtrace. So Soar implicitly provides two architectural axioms that model these architectural actions, much as in <ref> [9] </ref>. First, if a (architecture created) WME is obviously based on a supergoal object, a dummy instantiation that links them is created and added to the backtrace.
Reference: [10] <author> S. Minton. </author> <type> Personal communication. </type> <year> 1993. </year>
Reference-contexts: However, in archetypical EBL systems implemented for Prolog-like languages, the problem solving does not employ search-control rules. Even in Prodigy/EBL, where the problem solving involves search control, the explanation ignores search control rules <ref> [10] </ref>. An E-rule (Explanation-structure-like rule) is the intermediate structure which is formed by removing search control (if there is any) from a PS-rule. Because no search control is used in the cup domain, the structure of the E-rule is the same as the PS-rule shown in Figure 3.
Reference: [11] <author> J. E. Laird, P. S. Rosenbloom, and A. Newell. </author> <title> Overgeneralization during knowledge compilation in Soar. </title> <booktitle> In Proceedings of the Workshop on Knowledge Compilation, </booktitle> <pages> pages 46-57, </pages> <year> 1986. </year> <month> 14 </month>
Reference-contexts: Because no search control is used in the cup domain, the structure of the E-rule is the same as the PS-rule shown in Figure 3. The E-rule acts as an EBL explanation structure. The search-control rules are also missing in chunking <ref> [11, 3] </ref>, based on the assumption that they only affect efficiency, and not correctness of learned rules. The intended purpose of this omission is to increase the generality of the learned rules, by reducing the number of conditions incorporated into learned rules.
Reference: [12] <author> J. Kim and P. S. Rosenbloom. </author> <title> Constraining learning with search con-trol. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 174-181, </pages> <year> 1993. </year>
Reference-contexts: consequence of eliminating search control (if there was any) is that the E-rule is not constrained by the path actually taken in the problem space, and thus can perform an exponential amount of search even when the original problem-space search was highly directed (by the control rules), as analyzed in <ref> [12] </ref>. One possible way of avoiding the problem is to incorporate search control into the explanation structure, so that the match process for the learned rule focuses on only the path that was actually followed. <p> This can specialize the learned rule, but in return it enables the rule's cost to remain bounded by the cost of the original problem solving <ref> [12] </ref>. 7 by constraining variables by instantiations. The structure remains the same as in the E-rule (E-chunk) for this example. 3.3 Regress ()R-rule) We can apply the regression process of EBL [13] to the E-rule.
Reference: [13] <author> R. J. Mooney and S. W. Bennett. </author> <title> A domain independent explanation-based generalizer. </title> <booktitle> In Proceedings of AAAI-86, </booktitle> <pages> pages 551-555, </pages> <address> Philadel-phia, </address> <year> 1986. </year> <month> 15 </month>
Reference-contexts: The structure remains the same as in the E-rule (E-chunk) for this example. 3.3 Regress ()R-rule) We can apply the regression process of EBL <ref> [13] </ref> to the E-rule. Replacing the variable names with unique names and then unifying each connection between an action and a condition can create a generalized explanation. EBL/Soar also needs to introduce some additional constraints on variable names in order to produce legal Soar rules.
References-found: 13

