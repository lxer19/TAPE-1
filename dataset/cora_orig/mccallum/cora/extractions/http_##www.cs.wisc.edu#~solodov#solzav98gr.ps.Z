URL: http://www.cs.wisc.edu/~solodov/solzav98gr.ps.Z
Refering-URL: http://www.cs.wisc.edu/~olvi/uwmp/nn-solodov.html
Root-URL: 
Title: ERROR STABILITY PROPERTIES OF GENERALIZED GRADIENT-TYPE ALGORITHMS  
Author: M. V. Solodov and S. K. Zavriev 
Keyword: Key words. error-stability, perturbation analysis, gradient-type methods, incremental algorithms, approximate solutions.  
Date: 3, September 1998  
Note: JOURNAL OF OPTIMIZATION THEORY AND APPLICATIONS Vol. 98, No.  c fl1998 Plenum Publishing Corporation  AMS subject classifications. 90C31, 49M07, 65K10.  
Abstract: We present a unified framework for convergence analysis of the generalized subgradient-type algorithms in the presence of perturbations. One of the principal novel features of our analysis is that perturbations need not tend to zero in the limit. It is established that the iterates of the algorithms are attracted, in a certain sense, to an "-stationary set of the problem, where " depends on the magnitude of perturbations. Characterization of those attraction sets is given in the general (nonsmooth and nonconvex) case. The results are further strengthened for convex, weakly sharp and strongly convex problems. Our analysis extends and unifies previously known results on convergence and stability properties of gradient and subgradient methods, including their incremental, parallel and "heavy ball" modifications. fl The first author is supported in part by CNPq grant 300734/95-6. Research of the second author was supported in part by the International Science Foundation Grant NBY000, the International Science Foundation and Russian Goverment Grant NBY300 and the Russian Foundation for Fundamental Research Grant N 95-01-01448. y Instituto de Matematica Pura e Aplicada, Estrada Dona Castorina 110, Jardim Bot^anico, Rio de Janeiro, RJ, CEP 22460-320, Brazil. Email : solodov@impa.br. z Operations Research Department, Faculty of Computational Mathematics and Cybernetics, Moscow State University, Moscow, Russia, 119899. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ya.I. Alber, </author> <title> A.N. Iusem, and M.V. Solodov. On the projected subgradient methods for nonsmooth convex optimization in a Hilbert space. </title> <journal> Mathematical Programming, </journal> <note> to appear. </note>
Reference-contexts: In the analysis of such methods it is typically assumed that the "-subgradients become asymptotically exact <ref> [1] </ref>. In this paper, we do not assume convexity and, moreover, assume that the error terms are merely bounded. It is clear that in the presence of bounded perturbations, the iterates generated by the methods in consideration need not converge to the exact stationary set of the problem.
Reference: [2] <author> B.D.O. Anderson and J.B. Moore. </author> <title> Optimal Filtering. </title> <publisher> Prentice-Hall, Inc, </publisher> <address> Englewood, New Jersey, </address> <year> 1979. </year>
Reference-contexts: Problems of the form (1.2) arise, for example, in least-norm minimization. Of particular importance are machine learning [9] and control <ref> [2] </ref> applications, where computational significance of in cremental algorithms is well documented. Among some important applications that involve 2 parameters in the objective function, we note adaptive smoothing techniques [15], and neu-ral network training [12, 13].
Reference: [3] <author> D.P. Bertsekas. </author> <title> A new class of incremental gradient methods for least squares problems. </title> <note> SIAM J. on Optimization, to appear. </note>
Reference-contexts: Thus, until recently, there has been no rigorous mathematical analysis for incremental algorithms. First deterministic convergence results for noise-free smooth incremental gradient methods are given in [14, 11]; extended Kalman filter (an incremental least-squares algorithm) is analyzed in [5]. Other very recent work on incremental gradient methods includes <ref> [20, 3, 24] </ref>. In this paper, we fill some of the remaining theoretical gaps, specifically in the perturbation analysis of those methods (Theorems 3.1, 3.2).
Reference: [4] <author> D.P. Bertsekas. </author> <title> Nonlinear programming. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA, </address> <year> 1995. </year>
Reference-contexts: Analysis for perturbed optimization algorithms in a different context (for differentiable functions and different stepsize rules) can be found in [27, 21, 23]. Incremental algorithms <ref> [4, Section 1.5.2] </ref> considered in this paper are designed for minimizing an additive objective function min f (x; ff 0 ) := j=1 in the case when K is large.
Reference: [5] <author> D.P. Bertsekas. </author> <title> Incremental least squares methods and the extended Kalman filter. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 6 </volume> <pages> 807-822, </pages> <year> 1996. </year>
Reference-contexts: Thus, until recently, there has been no rigorous mathematical analysis for incremental algorithms. First deterministic convergence results for noise-free smooth incremental gradient methods are given in [14, 11]; extended Kalman filter (an incremental least-squares algorithm) is analyzed in <ref> [5] </ref>. Other very recent work on incremental gradient methods includes [20, 3, 24]. In this paper, we fill some of the remaining theoretical gaps, specifically in the perturbation analysis of those methods (Theorems 3.1, 3.2).
Reference: [6] <author> J.V. Burke and M.C. Ferris. </author> <title> Weak sharp minima in mathematical programming. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 31(5) </volume> <pages> 1340-1359, </pages> <year> 1993. </year>
Reference-contexts: These include problems with relatively small perturbations, convex and strongly convex problems, and problems with weak sharp minima <ref> [6] </ref>. We start with the following lemma which deals with the case of perturbations small relative to the residual function r () defined in (1.3). Lemma 4.1 Let "(x) maxf"; r (x)g 8x 2 X, where " 0; 2 [0; 1). Then X s (") X s ("). <p> This establishes the first two assertions of the lemma. For the last assertion, just note that (see [18, p.24]) for any x 2 X 2 (f (x) min f (y)) k@f (x)k 2 : Recall that X opt is a set of weak sharp minima <ref> [6] </ref> with parameter &gt; 0 if f (x) min f (y) d (x; X opt ) 8x 2 X: The following lemma shows that for problems with weak sharp minima, "-stationary sets coincide with the set of minima, provided " is small relative to the parameter .
Reference: [7] <author> F.H. Clarke. </author> <title> Optimization and Nonsmooth Analysis. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: 1 Introduction We consider the general optimization problem min f (x); (1.1) where X is a convex compact set in &lt; n . For the objective function, we assume that f : (X + t IB) ! &lt; is at least Lipschitz continuous and regular (in the sense of Clarke <ref> [7] </ref>) on X + t IB for some t 2 (0; +1], where IB is the closed unit ball in &lt; n . <p> sets of problem (1.1) respectively : X opt := fx 2 X j f (x) = min f (y)g and X s := fx 2 X j 0 2 @f (x) + N X (x)g; where @f (x) is the set of all generalized gradients (in the sense of Clarke <ref> [7] </ref>) of f () at x, and N X (x) is the normal cone to the set X at the point x 2 X.
Reference: [8] <author> P. A. Dorofeev. </author> <title> On some properties of quasi-gradient method. </title> <journal> USSR Computational Mathematics and Mathematical Physics, </journal> <volume> 25 </volume> <pages> 181-189, </pages> <year> 1985. </year>
Reference-contexts: As a by-product, we also obtain some new results for the basic generalized subgradient projection method in the presence of bounded perturbations (see Theorem 4.1), thus improving on <ref> [16, 8, 28] </ref>. We now describe notation and some concepts employed in the paper. <p> Theorem 4.1 extends, strengthens and unifies results on convergence and stability properties of the generalized subgradient projection method given in <ref> [16, 8, 28] </ref>.
Reference: [9] <author> T. Khanna. </author> <title> Foundations of neural networks. </title> <publisher> Addison-Wesley, </publisher> <address> New Jersey, </address> <year> 1989. </year>
Reference-contexts: We assume that the set A is bounded and f j (; ff) are Lipschitz continuous and regular on an open neighborhood of X + t IB for every ff 2 A. Problems of the form (1.2) arise, for example, in least-norm minimization. Of particular importance are machine learning <ref> [9] </ref> and control [2] applications, where computational significance of in cremental algorithms is well documented. Among some important applications that involve 2 parameters in the objective function, we note adaptive smoothing techniques [15], and neu-ral network training [12, 13]. <p> next consider a modification of parallel GGPM, where a "heavy ball" term [18] is added in the synchronization step : x i+1 = P X x i + l=1 i;K l +1 # In neural network literature, methods of this type are usually referred to as backpropagation with momentum term <ref> [9, 13] </ref>.
Reference: [10] <author> Z.-Q. Luo. </author> <title> On the convergence of the LMS algorithm with adaptive learning rate for linear feedforward networks. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 226-245, </pages> <year> 1991. </year> <month> 16 </month>
Reference-contexts: process with stepsizes decreasing subject to the following rule : lim i = 0; i=0 7 We point out that the condition of stepsize going to zero is indispensable in the general nons--mooth case (see [16]), as well as in the case of incremental methods (the latter is demonstrated in <ref> [10, Section 2] </ref>). We are now ready to state and prove convergence properties of the parallel GGPM in the presence of perturbations. Algorithm 3.1 (Parallel GGPM) Start with any x 0 2 X.
Reference: [11] <author> Z.-Q. Luo and P. Tseng. </author> <title> Analysis of an approximate gradient projection method with applications to the backpropagation algorithm. </title> <journal> Optimization Methods and Software, </journal> <volume> 4 </volume> <pages> 85-101, </pages> <year> 1994. </year>
Reference-contexts: Thus, until recently, there has been no rigorous mathematical analysis for incremental algorithms. First deterministic convergence results for noise-free smooth incremental gradient methods are given in <ref> [14, 11] </ref>; extended Kalman filter (an incremental least-squares algorithm) is analyzed in [5]. Other very recent work on incremental gradient methods includes [20, 3, 24]. In this paper, we fill some of the remaining theoretical gaps, specifically in the perturbation analysis of those methods (Theorems 3.1, 3.2). <p> In particular, this holds if we choose i M max l K l where M satisfies (3.1). We omit the proof which is quite straightforward, and is similar to <ref> [11, Lemma 2] </ref>. From now on, we assume that the stepsizes satisfy both (3.2) and (3.5). The following lemma will be used for translating Algorithm 3.1 into the framework of Section 2.
Reference: [12] <author> O.L. Mangasarian. </author> <title> Mathematical programming in neural networks. </title> <journal> ORSA Journal on Computing, </journal> <volume> 5(4) </volume> <pages> 349-360, </pages> <year> 1993. </year>
Reference-contexts: Of particular importance are machine learning [9] and control [2] applications, where computational significance of in cremental algorithms is well documented. Among some important applications that involve 2 parameters in the objective function, we note adaptive smoothing techniques [15], and neu-ral network training <ref> [12, 13] </ref>. It should be noted that incremental algorithms are inherently nonmonotone (even in the smooth and noise-free case) which makes standard Lyapunov-type convergence analysis techniques inapplicable. Thus, until recently, there has been no rigorous mathematical analysis for incremental algorithms.
Reference: [13] <editor> O.L. Mangasarian and M.V. Solodov. </editor> <title> Backpropagation convergence via deterministic nonmonotone perturbed minimization. </title> <editor> In G. Tesauro J.D. Cowan and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 383-390, </pages> <address> San Francisco, CA, 1994. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Of particular importance are machine learning [9] and control [2] applications, where computational significance of in cremental algorithms is well documented. Among some important applications that involve 2 parameters in the objective function, we note adaptive smoothing techniques [15], and neu-ral network training <ref> [12, 13] </ref>. It should be noted that incremental algorithms are inherently nonmonotone (even in the smooth and noise-free case) which makes standard Lyapunov-type convergence analysis techniques inapplicable. Thus, until recently, there has been no rigorous mathematical analysis for incremental algorithms. <p> next consider a modification of parallel GGPM, where a "heavy ball" term [18] is added in the synchronization step : x i+1 = P X x i + l=1 i;K l +1 # In neural network literature, methods of this type are usually referred to as backpropagation with momentum term <ref> [9, 13] </ref>.
Reference: [14] <editor> O.L. Mangasarian and M.V. Solodov. </editor> <title> Serial and parallel backpropagation convergence via nonmonotone perturbed minimization. </title> <journal> Optimization Methods and Software, </journal> <volume> 4 </volume> <pages> 103-116, </pages> <year> 1994. </year>
Reference-contexts: Thus, until recently, there has been no rigorous mathematical analysis for incremental algorithms. First deterministic convergence results for noise-free smooth incremental gradient methods are given in <ref> [14, 11] </ref>; extended Kalman filter (an incremental least-squares algorithm) is analyzed in [5]. Other very recent work on incremental gradient methods includes [20, 3, 24]. In this paper, we fill some of the remaining theoretical gaps, specifically in the perturbation analysis of those methods (Theorems 3.1, 3.2). <p> The type of parallelization proposed here is primarily motivated by incremental gradient methods, particularly neural network training <ref> [14] </ref>. Empirical evaluation of parallel neural network 6 training and numerical tests can be found in [17]. We first consider the most general case. Our results can be then specialized by removing parallelism and/or considering the standard (nonadditive) objective function.
Reference: [15] <author> D.Q. Mayne and E. Polak. </author> <title> Nondifferentiable optimization via adaptive smoothing. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 43(4) </volume> <pages> 601-614, </pages> <year> 1984. </year>
Reference-contexts: Problems of the form (1.2) arise, for example, in least-norm minimization. Of particular importance are machine learning [9] and control [2] applications, where computational significance of in cremental algorithms is well documented. Among some important applications that involve 2 parameters in the objective function, we note adaptive smoothing techniques <ref> [15] </ref>, and neu-ral network training [12, 13]. It should be noted that incremental algorithms are inherently nonmonotone (even in the smooth and noise-free case) which makes standard Lyapunov-type convergence analysis techniques inapplicable. Thus, until recently, there has been no rigorous mathematical analysis for incremental algorithms.
Reference: [16] <author> V. S. Mikhalevitch, A. M. Gupal, and V. I. Norkin. </author> <title> Methods of nonconvex optimization. </title> <publisher> Nauka, </publisher> <address> Moscow, </address> <year> 1987. </year> <note> In Russian. </note>
Reference-contexts: As a by-product, we also obtain some new results for the basic generalized subgradient projection method in the presence of bounded perturbations (see Theorem 4.1), thus improving on <ref> [16, 8, 28] </ref>. We now describe notation and some concepts employed in the paper. <p> We consider the process with stepsizes decreasing subject to the following rule : lim i = 0; i=0 7 We point out that the condition of stepsize going to zero is indispensable in the general nons--mooth case (see <ref> [16] </ref>), as well as in the case of incremental methods (the latter is demonstrated in [10, Section 2]). We are now ready to state and prove convergence properties of the parallel GGPM in the presence of perturbations. Algorithm 3.1 (Parallel GGPM) Start with any x 0 2 X. <p> Theorem 4.1 extends, strengthens and unifies results on convergence and stability properties of the generalized subgradient projection method given in <ref> [16, 8, 28] </ref>.
Reference: [17] <author> H. Paugam-Moisy. </author> <title> On parallel algorithm for backpropagation by partitioning the training set. </title> <booktitle> In Neural Networks and Their Applications. Proceedings of Fifth International Conference, </booktitle> <address> Nimes, France, </address> <month> November 2-6, </month> <year> 1992. </year>
Reference-contexts: The type of parallelization proposed here is primarily motivated by incremental gradient methods, particularly neural network training [14]. Empirical evaluation of parallel neural network 6 training and numerical tests can be found in <ref> [17] </ref>. We first consider the most general case. Our results can be then specialized by removing parallelism and/or considering the standard (nonadditive) objective function.
Reference: [18] <author> B.T. Polyak. </author> <title> Introduction to Optimization. Optimization Software, </title> <publisher> Inc., Publications Division, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: This very useful technique can be viewed as a generalization of the Lyapunov Direct Method for convergence analysis of nonlinear iterative processes. The classical Lyapunov Direct Method is a powerful tool for stability analysis of both continuous and discrete time processes <ref> [19, 25, 18] </ref>. Roughly speaking, this approach reduces analysis of stability properties of a process to the analysis of local improvement of this process with respect to some scalar criterion V () (usually called the Lyapunov function). <p> In the classical approach, V () monotonically decreases from one iterate of the process to the next. Some typical choices for V () are the objective function being minimized, or the norm of some optimality measure (see <ref> [18] </ref>). It should be noted that in certain situations, for example in the presence of perturbations or in incremental methods, one cannot exhibit a function which is guaranteed to decrease from one iteration of the algorithm to the next. This makes classical analysis not applicable. <p> Hence x 62 A 0 , and it follows that A 0 X s ("). Now applying Theorem 2.1, we immediately obtain the desired results. We next consider a modification of parallel GGPM, where a "heavy ball" term <ref> [18] </ref> is added in the synchronization step : x i+1 = P X x i + l=1 i;K l +1 # In neural network literature, methods of this type are usually referred to as backpropagation with momentum term [9, 13]. <p> This establishes the first two assertions of the lemma. For the last assertion, just note that (see <ref> [18, p.24] </ref>) for any x 2 X 2 (f (x) min f (y)) k@f (x)k 2 : Recall that X opt is a set of weak sharp minima [6] with parameter &gt; 0 if f (x) min f (y) d (x; X opt ) 8x 2 X: The following lemma shows
Reference: [19] <author> N. Rouche, P. Habets, and M Laloy. </author> <title> Stability Theory by Liapunov's Direct Method. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: This very useful technique can be viewed as a generalization of the Lyapunov Direct Method for convergence analysis of nonlinear iterative processes. The classical Lyapunov Direct Method is a powerful tool for stability analysis of both continuous and discrete time processes <ref> [19, 25, 18] </ref>. Roughly speaking, this approach reduces analysis of stability properties of a process to the analysis of local improvement of this process with respect to some scalar criterion V () (usually called the Lyapunov function).
Reference: [20] <author> M.V. Solodov. </author> <title> Incremental gradient algorithms with stepsizes bounded away from zero. </title> <note> Computational Optimization and Applications, to appear. </note>
Reference-contexts: Thus, until recently, there has been no rigorous mathematical analysis for incremental algorithms. First deterministic convergence results for noise-free smooth incremental gradient methods are given in [14, 11]; extended Kalman filter (an incremental least-squares algorithm) is analyzed in [5]. Other very recent work on incremental gradient methods includes <ref> [20, 3, 24] </ref>. In this paper, we fill some of the remaining theoretical gaps, specifically in the perturbation analysis of those methods (Theorems 3.1, 3.2).
Reference: [21] <author> M.V. Solodov. </author> <title> Convergence analysis of perturbed feasible descent methods. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 93: </volume> <pages> 337-353, </pages> <year> 1997. </year> <month> 17 </month>
Reference-contexts: Analysis for perturbed optimization algorithms in a different context (for differentiable functions and different stepsize rules) can be found in <ref> [27, 21, 23] </ref>. Incremental algorithms [4, Section 1.5.2] considered in this paper are designed for minimizing an additive objective function min f (x; ff 0 ) := j=1 in the case when K is large.
Reference: [22] <author> M.V. Solodov. </author> <title> New inexact parallel variable distribution algorithms. </title> <journal> Computational Optimization and Applications, </journal> <volume> 7 </volume> <pages> 165-182, </pages> <year> 1997. </year>
Reference-contexts: Define " = sup x2X "(x), and D = sup x;y2X kx yk. The following lemma relates the "- stationary sets to the *-optimal sets for the case when f () is convex. This result was also used in <ref> [22] </ref>. Lemma 4.2 Let f () be convex on X. Then X s ("(x)) X opt ("(x)d (x; X opt )). In particular, X s (") X opt ("D).
Reference: [23] <author> M.V. Solodov and B.F. Svaiter. </author> <title> Descent methods with linesearch in the presence of perturbations. </title> <journal> Journal of Computational and Applied Mathematics, </journal> <volume> 80: </volume> <pages> 265-275, </pages> <year> 1997. </year>
Reference-contexts: Analysis for perturbed optimization algorithms in a different context (for differentiable functions and different stepsize rules) can be found in <ref> [27, 21, 23] </ref>. Incremental algorithms [4, Section 1.5.2] considered in this paper are designed for minimizing an additive objective function min f (x; ff 0 ) := j=1 in the case when K is large.
Reference: [24] <author> P. Tseng. </author> <title> Incremental gradient(-projection) method with momentum term and adaptive stepsize rule. </title> <note> SIAM J. on Optimization, to appear. </note>
Reference-contexts: Thus, until recently, there has been no rigorous mathematical analysis for incremental algorithms. First deterministic convergence results for noise-free smooth incremental gradient methods are given in [14, 11]; extended Kalman filter (an incremental least-squares algorithm) is analyzed in [5]. Other very recent work on incremental gradient methods includes <ref> [20, 3, 24] </ref>. In this paper, we fill some of the remaining theoretical gaps, specifically in the perturbation analysis of those methods (Theorems 3.1, 3.2).
Reference: [25] <author> W.I. Zangwill. </author> <title> Nonlinear Programming: A Unified Approach. </title> <publisher> Prentice-Hall, Inc, </publisher> <address> En-glewood Cliffs, New Jersey, </address> <year> 1969. </year>
Reference-contexts: This very useful technique can be viewed as a generalization of the Lyapunov Direct Method for convergence analysis of nonlinear iterative processes. The classical Lyapunov Direct Method is a powerful tool for stability analysis of both continuous and discrete time processes <ref> [19, 25, 18] </ref>. Roughly speaking, this approach reduces analysis of stability properties of a process to the analysis of local improvement of this process with respect to some scalar criterion V () (usually called the Lyapunov function).
Reference: [26] <author> S. K. Zavriev. </author> <title> Stochastic subgradient methods for Minmax problems. </title> <address> Izdatelstvo MGU, Moscow, </address> <year> 1984. </year> <note> In Russian. </note>
Reference-contexts: Let fA fl g; fl 2 be the (unique) decomposition of A 0 into V ()-connected components (see <ref> [26] </ref>), that is A 0 = [ fl2 A fl ; A fl 0 for fl 0 6= fl 00 ; fl 0 ; fl 00 2 : The following theorem will play a central role in the subsequent analysis.
Reference: [27] <author> S. K. Zavriev. </author> <title> Convergence properties of the gradient method under variable level interference. </title> <journal> USSR Computational Mathematics and Mathematical Physics, </journal> <volume> 30 </volume> <pages> 997-1007, </pages> <year> 1990. </year>
Reference-contexts: Analysis for perturbed optimization algorithms in a different context (for differentiable functions and different stepsize rules) can be found in <ref> [27, 21, 23] </ref>. Incremental algorithms [4, Section 1.5.2] considered in this paper are designed for minimizing an additive objective function min f (x; ff 0 ) := j=1 in the case when K is large.
Reference: [28] <author> S.K. Zavriev and A.G. Perevozchikov. </author> <title> Attraction of trajectories of finite-difference inclusions and stability of numerical methods of stochastic nonsmooth optimization. </title> <journal> Soviet Phys. Doklady, </journal> <volume> 313 </volume> <pages> 1373-1376, </pages> <year> 1990. </year>
Reference-contexts: As a by-product, we also obtain some new results for the basic generalized subgradient projection method in the presence of bounded perturbations (see Theorem 4.1), thus improving on <ref> [16, 8, 28] </ref>. We now describe notation and some concepts employed in the paper. <p> Theorem 4.1 extends, strengthens and unifies results on convergence and stability properties of the generalized subgradient projection method given in <ref> [16, 8, 28] </ref>.
Reference: [29] <author> S.K. Zavriev and A.G. Perevozchikov. </author> <title> Direct Lyapunov's method in attraction analysis of finite-difference inclusions. </title> <journal> USSR Computational Mathematics and Mathematical Physics, </journal> <volume> 30(1) </volume> <pages> 22-32, </pages> <year> 1990. </year> <month> 18 </month>
Reference-contexts: In this paper we show that the iterates are attracted, in a certain sense, to an "-stationary set introduced above (see Theorem 3.1). We give a precise characterization of "() in terms of asymptotic behavior of perturbations. Our analysis is based on the novel technique presented in <ref> [29] </ref>. This approach allows us to deal with essentially perturbed problems (i.e. problems with nonvanishing noise : ffi (x i ) 6! 0 as i ! 1), as well as analyze algorithms that are inherently nonmonotone, e.g. incremental methods described below. <p> Section 4 contains some new results for a number of special cases. These include the case of asymptotically (relatively) small perturbations and convex, weakly sharp and strongly convex problems. 2 Generalized Lyapunov Direct Method In this section we outline the novel convergence analysis technique that was first proposed in <ref> [29] </ref> (albeit in a slightly different form). This very useful technique can be viewed as a generalization of the Lyapunov Direct Method for convergence analysis of nonlinear iterative processes. <p> Theorem 2.1 <ref> [29] </ref> For every sequence fx i g generated by the process (2.1)-(2.2), and satis fying (2.3), there exists a fl 2 such that lt V (x i ) = V lt fx i g " A fl : Furthermore, every subsequence fx i m g of fx i g satisfying lim <p> If, in addition, the set V (A 0 ) is nowhere dense in &lt;, then the whole sequence fx i g converges into a connected component of A 0 . We refer the reader to <ref> [29] </ref> for a detailed discussion. 3 Convergence Analysis In this section, we consider a parallel generalized gradient-type projection method (GGPM) for solving the problem of minimizing an additive parametric objective function (1.2). The type of parallelization proposed here is primarily motivated by incremental gradient methods, particularly neural network training [14].
References-found: 29

