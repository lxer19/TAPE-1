URL: http://www.cs.jhu.edu/~sheppard/lazy.ps
Refering-URL: http://www.cs.jhu.edu/~sheppard/pubs.html
Root-URL: http://www.cs.jhu.edu
Email: lastname@cs.jhu.edu  
Title: A Teaching Strategy for Memory-Based Control  
Author: John W. Sheppard and Steven L. Salzberg 
Keyword: lazy learning, nearest neighbor, genetic algorithms, differential games, pursuit games, teaching, reinforcement learning  
Date: Abstract  
Address: Baltimore, Maryland 21218  
Affiliation: Department of Computer Science The Johns Hopkins University  
Abstract: Combining different machine learning algorithms in the same system can produce benefits above and beyond what either method could achieve alone. This paper demonstrates that genetic algorithms can be used in conjunction with lazy learning to solve examples of a difficult class of delayed reinforcement learning problems better than either method alone. This class, the class of differential games, includes numerous important control problems that arise in robotics, planning, game playing, and other areas, and solutions for differential games suggest solution strategies for the general class of planning and control problems. We conducted a series of experiments applying three learning approaches|lazy Q-learning, k-nearest neighbor (k-NN), and a genetic algorithm|to a particular differential game called a pursuit game. Our experiments demonstrate that k-NN had great difficulty solving the problem, while a lazy version of Q-learning performed moderately well and the genetic algorithm performed even better. These results motivated the next step in the experiments, where we hypothesized k-NN was having difficulty because it did not have good examples-a common source of difficulty for lazy learning. Therefore, we used the genetic algorithm as a bootstrapping method for k-NN to create a system to provide these examples. Our experiments demonstrate that the resulting joint system learned to solve the pursuit games with a high degree of accuracy-outperforming either method alone-and with relatively small memory requirements.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D., & Salzberg, S. </author> <year> (1993). </year> <title> Learning to catch: Applying nearest neighbor algorithms to dynamic control tasks. </title> <booktitle> In Proceedings of the Fourth International Workshop on AI and Statistics, </booktitle> <pages> pp. </pages> <address> 363-368 Ft. Lauderdale. </address>
Reference: <author> Aha, D. W. </author> <year> (1992). </year> <title> Tolerating noisy, irrelevant, and novel attributes in instance-based learning algorithms. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 16, </volume> <pages> 267-287. </pages>
Reference: <author> Atkeson, C. </author> <year> (1990). </year> <title> Using local models to control movement. </title> <editor> In Touretzky, D. S. (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pp. </pages> <address> 316-323 San Mateo, CA. </address> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Atkeson, C. G. </author> <year> (1992). </year> <title> Memory-based approaches to approximating continuous functions. </title> <editor> In Casdagli, M., & Eubanks, S. (Eds.), </editor> <booktitle> Nonlinear Modeling and Forecasting, </booktitle> <pages> pp. 503-521. </pages> <publisher> Addison Wesley. </publisher>
Reference-contexts: The advantages to a lazy approach are three-fold. First, minimal computational time is required during training, since training consists primarily of storing examples (in the most traditional lazy approach, k-nearest neighbor). Second, lazy methods have been shown to be good 5 function-approximators in continuous state and action spaces <ref> (Atkeson, 1992) </ref>. As we will see, this will become important for our task of learning to play differential games. Third, traditional eager approaches to reinforcement learning assume the tasks are Markov decision problems.
Reference: <author> Barto, A., Sutton, R., & Anderson, C. </author> <year> (1983). </year> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13, </volume> <pages> 835-846. </pages>
Reference: <author> Barto, A., Sutton, R., & Watkins, C. </author> <year> (1990). </year> <title> Learning and sequential decision making. </title>
Reference: <editor> In Gabriel, & Moore (Eds.), </editor> <booktitle> Learning and Computational Neuroscience, </booktitle> <pages> pp. </pages> <address> 539-602 Cambridge. </address> <publisher> MIT Press. </publisher>
Reference: <author> Basar, T., & Olsder, G. J. </author> <year> (1982). </year> <title> Dynamic Noncooperative Game Theory. </title> <publisher> Academic Press, London. </publisher>
Reference-contexts: The typical formulation has both the car and the pedestrian traveling at fixed speeds, with the car having a fixed minimum radius of curvature, and the pedestrian able to make arbitrarily sharp turns (i.e., the radius of curvature is zero) <ref> (Basar & Olsder, 1982) </ref>.
Reference: <author> Booker, L., Goldberg, D., & Holland, J. </author> <year> (1989). </year> <title> Classifier systems and genetic algorithms. </title> <journal> Artificial Intelligence, </journal> <volume> 40, </volume> <pages> 235-282. </pages>
Reference: <author> Chapman, D. </author> <year> (1987). </year> <title> Planning for conjunctive goals. </title> <journal> Artificial Intelligence, </journal> <volume> 32, </volume> <pages> 333-377. </pages> <note> 25 Clouse, </note> <author> J., & Utgoff, P. </author> <year> (1992). </year> <title> A teaching method for reinforcement learning. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 92-101 Aberdeen, Scotland. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Then we made the task substantially harder to study the limitations of lazy learning methods on this class of problems. The more complicated task, which is described further in Section 3.2, also resembles complicated planning tasks in which an agent has to satisfy several goals simultaneously <ref> (Chapman, 1987) </ref>. As our experiments will show, we were successful at developing a method to solve our 2 difficult reinforcement learning task. The key idea behind our success was the combined use of both lazy learning and GAs.
Reference: <author> Colombetti, M., & Dorigo, M. </author> <year> (1994). </year> <title> Training agents to perform sequential behavior. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 2 (3), </volume> <pages> 247-275. </pages>
Reference: <author> Dasarathy, B. V. (Ed.). </author> <year> (1991). </year> <title> Nearest neighbor (NN) norms: NN pattern classification techniques. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA. </address>
Reference-contexts: Consequently, we decided to take this study one step further, and attempted to reduce the size of the memory store during the lazy learning phase of GLL (Zhang, 1992; Skalak, 1994). In the pattern recognition literature, e.g., in <ref> (Dasarathy, 1991) </ref>, algorithms for reducing memory size are known as editing methods. However, because lazy learning is not usually applied to control tasks, we were not able to find any editing methods specifically tied to our type of problem.
Reference: <author> Devijver, P. A. </author> <year> (1986). </year> <title> On the editing rate of the multiedit algorithm. </title> <journal> In Pattern Recognition Letters, </journal> <volume> Vol. 4, </volume> <pages> pp. 9-12. </pages>
Reference-contexts: The Ritter method, which is similar to Hart's (1968), basically keeps only points near the boundaries between classes, and eliminates examples that are in the midst of a homogenous region. The editing approach we took combined the editing procedure of Ritter et al. and the sampling idea of Tomek <ref> (Devijver, 1986) </ref>. We began by generating ten example sets with = 90 where each set consisted of a single set of examples from the GA. We then selected the set with the best performance on 10,000 test games, which in this case obtained nearly perfect accuracy with 1,700 examples.
Reference: <author> Devijver, P. A., & Kittler, J. </author> <year> (1982). </year> <title> Pattern Recognition: A Statistical Approach. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey. </address>
Reference-contexts: One possible reason for k-NN's poor performance on the two-pursuer task is presence of irrelevant attributes, which is known to cause problems for nearest neighbor algorithms (Aha, 1992; Salzberg, 1991). We experimented with a method similar to stepwise forward selection <ref> (Devijver & Kittler, 1982) </ref> to determine the set of relevant attributes. However, determining relevant attributes in a dynamic environment is difficult for the same reason that determining good examples is difficult: we do not know which attributes to use until many successful examples have been generated.
Reference: <author> Dorigo, M., & Colombetti, M. </author> <year> (1994). </year> <title> Robot shaping: Developing autonomous agents through learning. </title> <journal> Artificial Intelligence, </journal> <volume> 71 (2), </volume> <pages> 321-370. </pages>
Reference: <author> Friedman, A. </author> <year> (1971). </year> <title> Differential Games. </title> <publisher> Wiley Interscience, </publisher> <address> New York. </address>
Reference-contexts: We can also interpret differential games to be a version of optimal control theory in which players' positions develop continuously in time, and where the goal is to optimize competing control laws for the players <ref> (Friedman, 1971) </ref>. 3.1 DIFFERENTIAL GAMES AND PURSUIT GAMES Differential game theory originated in the early 1960s (Isaacs, 1963) as a framework for a more formal analysis of competitive games.
Reference: <author> Goldberg, D. </author> <year> (1989). </year> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts. </address>
Reference-contexts: During testing, the plan with the highest fitness is used to control E. The heart of the learning algorithm lies in the application of two genetic operators: mutation and crossover. Rules within a plan are selected for mutation using fitness proportional selection <ref> (Goldberg, 1989) </ref>. Namely, probability of selection is determined as Pr (r) = strength r (t) X 8s2rules strength s (t) (11) where rules is the set of rules in a plan and r is the rule of interest.
Reference: <author> Gordon, D., & Subramanian, D. </author> <year> (1993a). </year> <title> A multistrategy learning scheme for agent knowledge acquisition. </title> <journal> Informatica, </journal> <volume> 17, </volume> <pages> 331-346. </pages>
Reference: <author> Gordon, D., & Subramanian, D. </author> <year> (1993b). </year> <title> A multistrategy learning scheme for assimilating advice in embedded agents. </title> <booktitle> In Proceedings of the Second International Workshop on Multistrategy Learning, </booktitle> <pages> pp. 218-233. </pages> <institution> George Mason University. </institution>
Reference: <author> Grefenstette, J. </author> <year> (1988). </year> <title> Credit assignment in rule discovery systems based on genetic algorithms. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 225-245. </pages>
Reference-contexts: Following a game the strengths of the rules that fired are updated based on the payoff received from the game (the same payoff used in Q-learning). Given the payoff function, the strength for each rule that fired in a game is updated using the profit sharing plan <ref> (Grefenstette, 1988) </ref> as follows: (t) = (1 c)(t 1) + c (8) strength (t) = (t) (t) (10) where c is the profit sharing rate (c = 0:01 for our experiments), is the payoff received, is an estimate of the mean strength of a rule, and is an estimate of the
Reference: <editor> Grefenstette, J. </editor> <booktitle> (1991). Lamarkian learning in multi-agent environments. In Proceedings of the Fourth International Conference of Genetic Algorithms, </booktitle> <pages> pp. 303-310. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Grefenstette, J., Ramsey, C., & Schultz, A. </author> <year> (1990). </year> <title> Learning sequential decision rules using simulation models and competition. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 355-381. </pages>
Reference-contexts: For this study, we begin by considering a differential game that involved one agent trying to pursue and capture another (i.e., a pursuit game). Earlier research showed that at least one implementation of this task, known as evasive maneuvers <ref> (Grefenstette, Ramsey, & Schultz, 1990) </ref>, can be solved by a genetic algorithm (GA). We developed a lazy learning approach using k-nearest neighbor (k-NN) for the same task, hoping to demonstrate lazy learning could perform as well or better than the GA. <p> State y is the state that follows when action a is applied to state x. Reward is determined using the payoff function in <ref> (Grefenstette et al., 1990) </ref>, namely = 1000 if E evades the pursuers 10t if E is captured at time t. (3) Each of the pairs in the game are then compared with all of the pairs in the database. <p> The knowledge for the evasive maneuvers problem requires rules in which the terms have numeric values; we therefore modified the standard GA representation and operators for this problem, using a formulation similar to <ref> (Grefenstette et al., 1990) </ref>. We call a set of rules a plan.
Reference: <author> Hart, P. </author> <year> (1968). </year> <title> The condensed nearest neighbor rule. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 14 (3), </volume> <pages> 515-516. </pages> <publisher> 26 Holland, </publisher> <editor> J. </editor> <booktitle> (1975). Adaptation in Natural and Artificial Systems. </booktitle> <institution> University of Michigan Press, Ann Arbor, Michigan. </institution>
Reference: <author> Imado, F., & Ishihara, T. </author> <year> (1993). </year> <title> Pursuit-evasion geometry analysis between two missiles and an aircraft. </title> <journal> Computers and Mathematics with Applications, </journal> <volume> 26 (3), </volume> <pages> 125-139. </pages>
Reference-contexts: Our extended version includes a second pursuer, which makes the problem much harder. Unlike the single-pursuer problems, the two-pursuer problem has no known optimal strategy <ref> (Imado & Ishihara, 1993) </ref>, and for some initial states, there is no possibility of escape. Second, we gave the evader additional capabilities: in the one-pursuer game, E only controls its turn angle at each time step.
Reference: <author> Isaacs, R. </author> <year> (1963). </year> <title> Differential games: A mathematical theory with applications to warfare and other topics. </title> <type> Tech. rep. </type> <institution> Research Contribution No. 1, Center for Naval Analysis, </institution> <address> Washington, D.C. </address>
Reference-contexts: The class of RL problems studied here has also been studied in the field of differential game theory. Differential game theory is an extension of traditional game theory in which a game follows a sequence of actions through a continuous state space to achieve some payoff <ref> (Isaacs, 1963) </ref>. This sequence can be modeled with a set of differential equations which are analyzed to determine optimal play by the players. <p> can also interpret differential games to be a version of optimal control theory in which players' positions develop continuously in time, and where the goal is to optimize competing control laws for the players (Friedman, 1971). 3.1 DIFFERENTIAL GAMES AND PURSUIT GAMES Differential game theory originated in the early 1960s <ref> (Isaacs, 1963) </ref> as a framework for a more formal analysis of competitive games.
Reference: <author> Lin, L. </author> <year> (1991). </year> <title> Programming robots using reinforcement learning and teaching. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pp. 781-786. </pages>
Reference: <author> Littman, M. </author> <year> (1994). </year> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Proceedings of the Eleventh International Machine Learning Conference, </booktitle> <pages> pp. </pages> <address> 157-163 New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> McCallum, R. A. </author> <year> (1995). </year> <title> Instance-based state identification for reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 7, </booktitle> <pages> pp. 377-384. </pages>
Reference: <author> Millan, J., & Torras, C. </author> <year> (1992). </year> <title> A reinforcement connectionist approach to robot path finding in non-maze-like environments. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 363-395. </pages>
Reference: <author> Mingers, J. </author> <year> (1989). </year> <title> An empirical comparison of pruning methods for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 4 (2), </volume> <pages> 227-243. </pages>
Reference-contexts: Early work by Wilson (1972) showed that examples could be removed from a set used for classification, and suggested that simply editing would frequently improve classification accuracy (in the same way that pruning improves decision trees <ref> (Mingers, 1989) </ref>). Wilson's algorithm classifies each example in a data set with its own k nearest neighbors. Those points that are incorrectly classified are deleted from the example set, the idea being that such points probably represent noise.
Reference: <author> Moore, A. </author> <year> (1990). </year> <title> Efficient Memory-Based Learning for Robot Control. </title> <type> Ph.D. thesis, </type> <institution> Computer Laboratory, Cambridge University. </institution>
Reference: <author> Moore, A., & Atkeson, C. </author> <year> (1993). </year> <title> Prioritized sweeping: Reinforcement learning with less data and less time. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <pages> 103-130. </pages>
Reference: <author> Nguyen, D., & Widrow, B. </author> <year> (1989). </year> <title> The truck backer-upper: An example of self learning in neural networks. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 357-363. </pages>
Reference: <author> Pell, B. D. </author> <year> (1993). </year> <title> Strategy Generation and Evaluation for Meta-Game Playing. </title> <type> Ph.D. thesis, </type> <institution> University of Cambridge, </institution> <address> Cambridge, England. </address>
Reference: <author> Ramsey, C. L., & Grefenstette, J. J. </author> <year> (1994). </year> <title> Case-based anytime learning. </title> <editor> In Aha, D. W. (Ed.), </editor> <title> Case Based Reasoning: </title> <booktitle> Papers from the 1994 Workshop, </booktitle> <pages> pp. </pages> <address> 91-95 Menlo Park, California. </address> <note> AAAI Press. 27 Ritter, </note> <author> G., Woodruff, H., Lowry, S., & Isenhour, T. </author> <year> (1975). </year> <title> An algorithm for a selective nearest neighbor decision rule. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 21 (6), </volume> <pages> 665-669. </pages>
Reference: <author> Salzberg, S. </author> <year> (1991). </year> <title> Distance metrics for instance-based learning. </title> <booktitle> In Methodologies for Intelligent Systems: 6th International Symposium, </booktitle> <pages> pp. 399-408. </pages>
Reference: <author> Salzberg, S., Delcher, A., Heath, D., & Kasif, S. </author> <year> (1991). </year> <title> Learning with a helpful teacher. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 705-711 Sydney, Australia. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sheppard, J. W., & Salzberg, S. L. </author> <year> (1993). </year> <title> Memory-based learning of pursuit games. </title> <type> Tech. rep. </type> <institution> JHU-93/94-02, Department of Computer Science, Johns Hopkins University, Bal-timore, Maryland. </institution> <note> revised May, </note> <year> 1995. </year>
Reference-contexts: The object of analyzing a differential game is to determine the optimal strategies for each player of the game and to determine the value of the game (i.e., the expected payoff to each player) assuming all of players follow the optimal strategies. For more details, see <ref> (Sheppard & Salzberg, 1993) </ref>. A pursuit game is a special type of differential game that has two players, called the pursuer (P ) and the evader (E). <p> Probability of selection for plans is determined similarly using plan fitness rather than rule strength. For more details of the implementation, see <ref> (Sheppard & Salzberg, 1993) </ref>. 4.4 RESULTS For each of the algorithms and for both variations of the evasive maneuvers game, we ran ten experiments. To produce learning curves, we combined the results of the ten experiments by averaging the algorithm's performance at regular intervals.
Reference: <author> Skalak, D. </author> <year> (1994). </year> <title> Prototype and feature selection by sampling and random mutation hill climbing algorithms. </title> <booktitle> In Proceedings of the Eleventh International Machine Learning Conference, </booktitle> <pages> pp. </pages> <address> 293-301 New Brunswick, NJ. </address> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Smith, R. E., & Gray, B. </author> <year> (1993). </year> <title> Co-adaptive genetic algorithms: An example in othello strategy. </title> <type> Tech. rep. TCGA Report No. 94002, </type> <institution> University of Alabama, Tuscaloosa, Alabama. </institution>
Reference: <author> Sutton, R. </author> <year> (1988). </year> <title> Learning to predict by methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference-contexts: They addressed the problem of teaching a robot to navigate around obstacles. Considerable research has been performed using a form of reinforcement learning called temporal difference learning <ref> (Sutton, 1988) </ref>. Temporal difference methods apply reinforcement throughout a sequence of actions to predict both future reinforcement and appropriate actions in performing the task. Specifically, predictions are refined through a process of identifying differences between the results of temporally successive actions.
Reference: <author> Tesauro, G. </author> <year> (1992). </year> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 257-277. </pages>
Reference-contexts: They found they were able to control the development of niches in the population to handle several different types of opponents. Finally, Tesauro used temporal difference learning <ref> (Tesauro, 1992) </ref> and neural networks (Tesauro & Sejnowski, 1989) to train a backgammon program called td-gammon.
Reference: <author> Tesauro, G., & Sejnowski, T. J. </author> <year> (1989). </year> <title> A parallel network that learns to play backgammon. </title> <journal> Artificial Intelligence, </journal> <volume> 39, </volume> <pages> 357-390. </pages>
Reference-contexts: They found they were able to control the development of niches in the population to handle several different types of opponents. Finally, Tesauro used temporal difference learning (Tesauro, 1992) and neural networks <ref> (Tesauro & Sejnowski, 1989) </ref> to train a backgammon program called td-gammon.
Reference: <author> Tomek, I. </author> <year> (1976). </year> <title> An experiment with the edited nearest-neighbor rule. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> SMC-6 (6), </volume> <pages> 448-452. </pages> <editor> van der Wal, J. </editor> <year> (1981). </year> <title> Stochastic Dynamic Programming. </title> <publisher> Morgan Kaufmann, Amsterdam. </publisher>
Reference: <author> Watkins, C. </author> <year> (1989). </year> <title> Learning with Delayed Rewards. </title> <type> Ph.D. thesis, </type> <institution> Cambridge University, Department of Computer Science, </institution> <address> Cambridge, England. </address>
Reference-contexts: Specifically, predictions are refined through a process of identifying differences between the results of temporally successive actions. Two popular temporal difference algorithms are ACE/ASE (Barto, Sutton, & Anderson, 1983; Barto et al., 1990) and Q-learning <ref> (Watkins, 1989) </ref>. The original work by Barto et al. (1983) demonstrated that the cart and pole problem could be solved using this method. Clouse and Utgoff (1992) later used ACE/ASE with a separate teacher for the cart and pole problem, and applied Q-learning to navigating a race track. <p> We then tried a traditional lazy learning approach, k-nearest neighbors. Finally, we experimented with an eager learning method, genetic algorithms, to compare with the two lazy methods. 4.1 LAZY Q-LEARNING FOR EVASIVE MANEUVERS Q-learning solves delayed reinformement learning problems by using a temporal difference (TD) learning rule <ref> (Watkins, 1989) </ref>. TD methods usually assume that both the feature space and the variables being predicted are discrete (Sutton, 1988; Tesauro, 1992). Q-learning represents a problem using a lookup table that contains all states, which naturally causes problems with large, continuous state spaces such as those encountered in differential games.
Reference: <author> Whitehead, S. </author> <year> (1992). </year> <title> Reinforcement Learning for the Adaptive Control of Perception and Action. </title> <type> Ph.D. thesis, </type> <institution> Department of Computer Science, University of Rochester. </institution> <note> 28 Widrow, </note> <author> B. </author> <year> (1987). </year> <title> The original adaptive neural net broom-balancer. </title> <booktitle> In International Symposium on Circuits and Systems, </booktitle> <pages> pp. 351-357. </pages>
Reference-contexts: McCallum (1995) developed the "nearest sequence memory" algorithm, which is a lazy 3 algorithm for solving control problems plagued by hidden state. Hidden state is an artifact of perceptual aliasing in which the mapping between states and perceptions is not one-to-one <ref> (Whitehead, 1992) </ref>. McCallum showed through his algorithm that lazy methods can reduce the effects of perceptual aliasing by appending history information with state information. Since our approach stores complete sequences, we too have minimized the effects of hidden state.
Reference: <author> Wilson, D. </author> <year> (1972). </year> <title> Asymptotic properties of nearest neighbor rules using edited data. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 2 (3), </volume> <pages> 408-421. </pages>
Reference: <author> Zhang, J. </author> <year> (1992). </year> <title> Selecting typical instances in instance-based learning. </title> <booktitle> In Proceedings of the Ninth International Machine Learning Conferences, </booktitle> <pages> pp. </pages> <address> 470-479 Aberdeen, Scotland. </address> <publisher> Morgan Kaufman. </publisher> <pages> 29 </pages>
References-found: 48

