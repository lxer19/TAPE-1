URL: file://ftp.cc.gatech.edu/pub/groups/architecture/TASS/git.cc.93.63.ps.Z
Refering-URL: http://www.cs.gatech.edu/computing/Architecture/TASS/tass.html
Root-URL: 
Email: e-mail: rama@cc.gatech.edu  
Phone: Phone: (404) 894-5136 Fax: (404) 894-9442  
Title: Abstracting Network Characteristics and Locality Properties of Parallel Systems  
Author: Anand Sivasubramaniam Aman Singla Umakishore Ramachandran H. Venkateswaran 
Keyword: Key words: Parallel Systems, Machine Abstractions, Locality, Execution-driven Simulation, Application driven Studies  
Note: This work has been funded in part by NSF grants MIPS-9058430 and MIPS-9200005, and an equipment grant from DEC.  
Address: Atlanta, GA 30332-0280  
Affiliation: College of Computing Georgia Institute of Technology  
Abstract: Technical Report GIT-CC-93/63 October 1993 Abstract Abstracting features of parallel systems is a technique that has been traditionally used in theoretical and analytical models for program development and performance evaluation. In this paper, we explore the use of abstractions in execution-driven simulators in order to speed up simulation. In particular, we evaluate abstractions for the interconnection network and locality properties of parallel systems in the context of simulating cache-coherent shared memory (CC-NUMA) multiprocessors. We use the recently proposed LogP model to abstract the network. We abstract locality by modeling a cache at each processing node in the system which is maintained coherent, without modeling the overheads associated with coherence maintenance. Such an abstraction tries to capture the true communication characteristics of the application without modeling any hardware induced artifacts. Using a suite of applications and three network topologies simulated on a novel simulation platform, we show that the latency overhead modeled by LogP is fairly accurate. On the other hand, the contention overhead can become pessimistic when the applications display sufficient communication locality. Our abstraction for data locality closely models the behavior of the actual system over the chosen range of applications. The simulation model which incorporated these abstractions was around 250-300% faster than the simulation of the actual machine. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V. S. Adve and M. K. Vernon. </author> <title> Performance analysis of mesh interconnection networks with deterministic routing. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(3) </volume> <pages> 225-246, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: The g-parameter in cLogP is estimated using the bisection bandwidth of the network as suggested in [11]. Such an estimate assumes that every message in the system traverses the bisection and can become very pessimistic when the application displays sufficient communication locality <ref> [1, 2] </ref>. This pessimism increases as the connectivity of the network decreases (as can be seen in Figures 6, 7, and 8) since the impact of communication locality increases. This pessimism is amplified further for applications such as EP that display a significant amount of communication locality.
Reference: [2] <author> A. Agarwal. </author> <title> Limits on Interconnection Network Performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 398-412, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Similarly, considerable effort has been expended in the area of performance evaluation in developing simple analytical abstractions to model the complex behavior of parallel systems. For instance, Agarwal <ref> [2] </ref> and Dally [12] develop mathematical models for abstracting the network and studying network properties. Patel [19] analyzes the impact of caches on multiprocessor performance. But many of these models make simplifying assumptions about the hardware and/or the applications, restricting their ability to model the behavior of real parallel systems. <p> The g-parameter in cLogP is estimated using the bisection bandwidth of the network as suggested in [11]. Such an estimate assumes that every message in the system traverses the bisection and can become very pessimistic when the application displays sufficient communication locality <ref> [1, 2] </ref>. This pessimism increases as the connectivity of the network decreases (as can be seen in Figures 6, 7, and 8) since the impact of communication locality increases. This pessimism is amplified further for applications such as EP that display a significant amount of communication locality.
Reference: [3] <author> A. Agarwal et al. </author> <title> The MIT Alewife machine : A large scale Distributed-Memory Multiprocessor. In Scalable shared memory multiprocessors. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference-contexts: In [29] a Petri net model is used for the application and the hardware. Mehra et al. [17] use application knowledge in abstracting out phases of the execution. The issue of locality has been well investigated in the architecture community. Several studies <ref> [3, 16] </ref> have explored hardware facilities that would help exploit locality in applications, and have clearly illustrated the use of caches in reducing network traffic. There have also been application-driven studies which try to synthesize cache requirements from the application viewpoint.
Reference: [4] <author> A. Aggarwal, A. K. Chandra, and M. Snir. </author> <title> On Communication Latency in PRAM Computations. </title> <booktitle> In Proceedings of the First Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 11-21, </pages> <year> 1989. </year>
Reference-contexts: The PRAM model assumes conflict-free accesses to shared memory (assigning unit cost for memory accesses) and zero cost for synchronization. The PRAM model has been augmented with additional parameters to account for memory access latency <ref> [4] </ref>, memory access conflicts [5], and cost of synchronization [15, 9]. The Bulk Synchronous Parallel (BSP) model [28] and the LogP model [11] are departures from the PRAM models, and attempt to realistically bridge the gap between theory and practice.
Reference: [5] <author> H. Alt, T. Hagerup, K. Mehlhorn, and F. P. Preparata. </author> <title> Deterministic Simulation of Idealized Parallel Computers on More Realistic Ones. </title> <journal> SIAM Journal of Computing, </journal> <volume> 16(5) </volume> <pages> 808-835, </pages> <year> 1987. </year>
Reference-contexts: The PRAM model assumes conflict-free accesses to shared memory (assigning unit cost for memory accesses) and zero cost for synchronization. The PRAM model has been augmented with additional parameters to account for memory access latency [4], memory access conflicts <ref> [5] </ref>, and cost of synchronization [15, 9]. The Bulk Synchronous Parallel (BSP) model [28] and the LogP model [11] are departures from the PRAM models, and attempt to realistically bridge the gap between theory and practice.
Reference: [6] <author> T. E. Anderson. </author> <title> The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: For EP on a cLogP machine, only the first and last accesses to the condition variable use the network, while on the LogP machine a network access would be incurred for each reference to the condition variable as is reflected in Figure 3. Similarly, a test-test&set primitive <ref> [6] </ref>, would behave like an ordinary test&set operation in the LogP machine thus resulting in an increase of network accesses. As can be seen in Figure 20, these effects do not impact the total execution time of EP since computation dominates for this particular application.
Reference: [7] <author> D. Bailey et al. </author> <title> The NAS Parallel Benchmarks. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 5(3) </volume> <pages> 63-73, </pages> <year> 1991. </year>
Reference-contexts: parallel system overheads in scalability studies of parallel systems [25], identifying parallel system (both algorithmic and architectural) bottlenecks [25], and synthesizing architectural requirements from an application viewpoint [27]. 4 Application Characteristics Three of the applications (EP, IS and CG) used in this study are from the NAS parallel benchmark suite <ref> [7] </ref>; CHOLESKY is from the SPLASH benchmark suite [24]; and FFT is the well-known Fast Fourier Transform algorithm. EP and FFT are well-structured applications with regular communication patterns determinable at compile-time, with the difference that EP has a higher computation to communication ratio.
Reference: [8] <author> E. A. Brewer, C. N. Dellarocas, A. Colbrook, and W. E. Weihl. </author> <title> PROTEUS : A high-performance parallel-architecture simulator. </title> <type> Technical Report MIT-LCS-TR-516, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA 02139, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: Patel [19] analyzes the impact of caches on multiprocessor performance. But many of these models make simplifying assumptions about the hardware and/or the applications, restricting their ability to model the behavior of real parallel systems. Execution-driven simulation is becoming increasingly popular for capturing the dynamic behavior of parallel systems <ref> [25, 8, 10, 13, 21] </ref>. Some of these simulators have abstracted out the instruction-set of the processors, since a detailed simulation of the instruction-set is not likely to contribute significantly to the 3 performance analysis of parallel systems. <p> The input to the simulator are parallel applications written in C. These programs are pre-processed (to label shared memory accesses), the compiled assembly code is augmented with cycle counting instructions, and the assembled binary is linked with the simulator code. As with other recent simulators <ref> [8, 13, 10, 21] </ref>, bulk of the instructions is executed at the speed of the native processor (the SPARC in this case) and only instructions (such as LOADs and STOREs on a shared memory platform or SENDs and RECEIVEs on a message-passing platform) that may potentially involve a network access are
Reference: [9] <author> R. Cole and O. Zajicek. </author> <title> The APRAM: Incorporating Asynchrony into the PRAM Model. </title> <booktitle> In Proceedings of the First Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 169-178, </pages> <year> 1989. </year>
Reference-contexts: The PRAM model assumes conflict-free accesses to shared memory (assigning unit cost for memory accesses) and zero cost for synchronization. The PRAM model has been augmented with additional parameters to account for memory access latency [4], memory access conflicts [5], and cost of synchronization <ref> [15, 9] </ref>. The Bulk Synchronous Parallel (BSP) model [28] and the LogP model [11] are departures from the PRAM models, and attempt to realistically bridge the gap between theory and practice.
Reference: [10] <author> R. G. Covington, S. Madala, V. Mehta, J. R. Jump, and J. B. Sinclair. </author> <title> The Rice parallel processing testbed. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1988 Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 4-11, </pages> <address> Santa Fe, NM, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: Patel [19] analyzes the impact of caches on multiprocessor performance. But many of these models make simplifying assumptions about the hardware and/or the applications, restricting their ability to model the behavior of real parallel systems. Execution-driven simulation is becoming increasingly popular for capturing the dynamic behavior of parallel systems <ref> [25, 8, 10, 13, 21] </ref>. Some of these simulators have abstracted out the instruction-set of the processors, since a detailed simulation of the instruction-set is not likely to contribute significantly to the 3 performance analysis of parallel systems. <p> The input to the simulator are parallel applications written in C. These programs are pre-processed (to label shared memory accesses), the compiled assembly code is augmented with cycle counting instructions, and the assembled binary is linked with the simulator code. As with other recent simulators <ref> [8, 13, 10, 21] </ref>, bulk of the instructions is executed at the speed of the native processor (the SPARC in this case) and only instructions (such as LOADs and STOREs on a shared memory platform or SENDs and RECEIVEs on a message-passing platform) that may potentially involve a network access are
Reference: [11] <author> D. Culler et al. </author> <title> LogP : Towards a realistic model of parallel computation. </title> <booktitle> In Proceedings of the 4th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 1-12, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: We explore these two issues in the context of simulating Cache Coherent Non-Uniform Memory Access (CC-NUMA) shared memory machines. For abstracting the interconnection network, we use the recently proposed LogP <ref> [11] </ref> model that incorporates the two defining characteristics of a network, namely, latency and contention. For abstracting the locality properties of a parallel system, we model a private cache at each processing node in the system to capture data locality 2 . <p> The PRAM model has been augmented with additional parameters to account for memory access latency [4], memory access conflicts [5], and cost of synchronization [15, 9]. The Bulk Synchronous Parallel (BSP) model [28] and the LogP model <ref> [11] </ref> are departures from the PRAM models, and attempt to realistically bridge the gap between theory and practice. Similarly, considerable effort has been expended in the area of performance evaluation in developing simple analytical abstractions to model the complex behavior of parallel systems. <p> All the details of the interconnection network and coherence maintenance are exactly modeled. 3.1 The LogP Machine The LogP model proposed by Culler et al. <ref> [11] </ref> assumes a collection of processing nodes executing asynchronously, communicating with each other by small fixed-size messages incurring constant latencies on a network with a finite bandwidth. <p> The L parameter for a message on the LogP and cLogP models is chosen to be 1.6 microseconds assuming 32-byte messages and a link bandwidth of 20 MBytes/sec. Similar to the method used in <ref> [11] </ref>, the g parameter is calculated using the cross-section bandwidth available per processor for each of the above network configurations. <p> But there is a difference in the absolute values. The g-parameter in cLogP is estimated using the bisection bandwidth of the network as suggested in <ref> [11] </ref>. Such an estimate assumes that every message in the system traverses the bisection and can become very pessimistic when the application displays sufficient communication locality [1, 2]. <p> The two sources of disparity are (a) the way g is computed, and (b) the way g is to be used as defined by the model. Since g is computed using only the bisection bandwidth of the network (as is suggested in <ref> [11] </ref>), it fails to capture any communication locality resulting from mapping the application on to a specific network topology. The ensuing pessimism in the observed contention overhead would increase with decreasing connectivity in the network as we have seen in the previous section. <p> For instance, even when total execution time curves were similar the latency and contention overhead curves helped us determine whether the model parameters were accurate in capturing the intended machine abstractions. One can experimentally determine the accuracy of the performance predicted by the LogP model as is done in <ref> [11] </ref> using the CM-5. However, this approach does not validate the individual parameters abstracted using the model.
Reference: [12] <author> W. J. Dally. </author> <title> Performance analysis of k-ary n-cube interconnection networks. </title> <journal> IEEE Transactions on Computer Systems, </journal> <volume> 39(6) </volume> <pages> 775-785, </pages> <month> June </month> <year> 1990. </year> <month> 16 </month>
Reference-contexts: Similarly, considerable effort has been expended in the area of performance evaluation in developing simple analytical abstractions to model the complex behavior of parallel systems. For instance, Agarwal [2] and Dally <ref> [12] </ref> develop mathematical models for abstracting the network and studying network properties. Patel [19] analyzes the impact of caches on multiprocessor performance. But many of these models make simplifying assumptions about the hardware and/or the applications, restricting their ability to model the behavior of real parallel systems.
Reference: [13] <author> H. Davis, S. R. Goldschmidt, and J. L. Hennessy. </author> <title> Multiprocessor Simulation and Tracing Using Tango. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages II 99-107, </pages> <year> 1991. </year>
Reference-contexts: Patel [19] analyzes the impact of caches on multiprocessor performance. But many of these models make simplifying assumptions about the hardware and/or the applications, restricting their ability to model the behavior of real parallel systems. Execution-driven simulation is becoming increasingly popular for capturing the dynamic behavior of parallel systems <ref> [25, 8, 10, 13, 21] </ref>. Some of these simulators have abstracted out the instruction-set of the processors, since a detailed simulation of the instruction-set is not likely to contribute significantly to the 3 performance analysis of parallel systems. <p> The input to the simulator are parallel applications written in C. These programs are pre-processed (to label shared memory accesses), the compiled assembly code is augmented with cycle counting instructions, and the assembled binary is linked with the simulator code. As with other recent simulators <ref> [8, 13, 10, 21] </ref>, bulk of the instructions is executed at the speed of the native processor (the SPARC in this case) and only instructions (such as LOADs and STOREs on a shared memory platform or SENDs and RECEIVEs on a message-passing platform) that may potentially involve a network access are
Reference: [14] <author> S. Fortune and J. Wyllie. </author> <title> Parallelism in random access machines. </title> <booktitle> In Proceedings of the 10th Annual Symposium on Theory of Computing, </booktitle> <pages> pages 114-118, </pages> <year> 1978. </year>
Reference-contexts: Developing algorithms for parallel architectures is also hard if one has to grapple with all parallel system artifacts. Abstracting features of parallel systems is a technique often employed to address both of these issues. For instance, abstracting parallel machines by theoretical models like the PRAM <ref> [14] </ref> has facilitated algorithm development and analysis. Such models try to hide hardware details from the programmer, providing a simplified view of the machine.
Reference: [15] <author> P. B. Gibbons. </author> <title> A More Practical PRAM Model. </title> <booktitle> In Proceedings of the First Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 158-168, </pages> <year> 1989. </year>
Reference-contexts: The PRAM model assumes conflict-free accesses to shared memory (assigning unit cost for memory accesses) and zero cost for synchronization. The PRAM model has been augmented with additional parameters to account for memory access latency [4], memory access conflicts [5], and cost of synchronization <ref> [15, 9] </ref>. The Bulk Synchronous Parallel (BSP) model [28] and the LogP model [11] are departures from the PRAM models, and attempt to realistically bridge the gap between theory and practice.
Reference: [16] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W-D Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford DASH multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: In [29] a Petri net model is used for the application and the hardware. Mehra et al. [17] use application knowledge in abstracting out phases of the execution. The issue of locality has been well investigated in the architecture community. Several studies <ref> [3, 16] </ref> have explored hardware facilities that would help exploit locality in applications, and have clearly illustrated the use of caches in reducing network traffic. There have also been application-driven studies which try to synthesize cache requirements from the application viewpoint.
Reference: [17] <author> P. Mehra, C. H. Schulbach, and J. C. Yan. </author> <title> A comparison of two model-based performance-prediction techniques for message-passing parallel programs. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1994 Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 181-190, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Researchers have tried to use other abstractions for the workload as well as the simulated hardware in order to speed up the simulation. In [29] a Petri net model is used for the application and the hardware. Mehra et al. <ref> [17] </ref> use application knowledge in abstracting out phases of the execution. The issue of locality has been well investigated in the architecture community. Several studies [3, 16] have explored hardware facilities that would help exploit locality in applications, and have clearly illustrated the use of caches in reducing network traffic.
Reference: [18] <institution> Microelectronics and Computer Technology Corporation, Austin, TX 78759. </institution> <note> CSIM User's Guide, </note> <year> 1990. </year>
Reference-contexts: SPASM has been written using CSIM <ref> [18] </ref>, a process oriented sequential simulation package, and currently runs on SPARCstations. The input to the simulator are parallel applications written in C.
Reference: [19] <author> J. H. Patel. </author> <title> Analysis of multiprocessors with private cache memories. </title> <journal> IEEE Transactions on Computer Systems, </journal> <volume> 31(4) </volume> <pages> 296-304, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: Similarly, considerable effort has been expended in the area of performance evaluation in developing simple analytical abstractions to model the complex behavior of parallel systems. For instance, Agarwal [2] and Dally [12] develop mathematical models for abstracting the network and studying network properties. Patel <ref> [19] </ref> analyzes the impact of caches on multiprocessor performance. But many of these models make simplifying assumptions about the hardware and/or the applications, restricting their ability to model the behavior of real parallel systems.
Reference: [20] <author> U. Ramachandran, G. Shah, S. Ravikumar, and J. Muthukumarasamy. </author> <title> Scalability study of the KSR-1. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <pages> pages I-237-240, </pages> <month> August </month> <year> 1993. </year>
Reference: [21] <author> S. K. Reinhardt et al. </author> <title> The Wisconsin Wind Tunnel : Virtual prototyping of parallel computers. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1993 Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 48-60, </pages> <address> Santa Clara, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Patel [19] analyzes the impact of caches on multiprocessor performance. But many of these models make simplifying assumptions about the hardware and/or the applications, restricting their ability to model the behavior of real parallel systems. Execution-driven simulation is becoming increasingly popular for capturing the dynamic behavior of parallel systems <ref> [25, 8, 10, 13, 21] </ref>. Some of these simulators have abstracted out the instruction-set of the processors, since a detailed simulation of the instruction-set is not likely to contribute significantly to the 3 performance analysis of parallel systems. <p> The input to the simulator are parallel applications written in C. These programs are pre-processed (to label shared memory accesses), the compiled assembly code is augmented with cycle counting instructions, and the assembled binary is linked with the simulator code. As with other recent simulators <ref> [8, 13, 10, 21] </ref>, bulk of the instructions is executed at the speed of the native processor (the SPARC in this case) and only instructions (such as LOADs and STOREs on a shared memory platform or SENDs and RECEIVEs on a message-passing platform) that may potentially involve a network access are
Reference: [22] <author> E. Rothberg, J. P. Singh, and A. Gupta. </author> <title> Working sets, cache sizes and node granularity issues for large-scale multiprocessors. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 14-25, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Several studies [3, 16] have explored hardware facilities that would help exploit locality in applications, and have clearly illustrated the use of caches in reducing network traffic. There have also been application-driven studies which try to synthesize cache requirements from the application viewpoint. For instance, Gupta et al. <ref> [22] </ref> show that a small-sized cache of around 64KB can accommodate the important working set of many applications. Similarly, Wood et al. [30] show that the performance of a suite of applications is not very sensitive to different cache coherence protocols.
Reference: [23] <author> J. P. Singh, E. Rothberg, and A. Gupta. </author> <title> Modeling communication in parallel algorithms: </title> <booktitle> A fruitful interaction between theory and systems? In Proceedings of the Sixth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1994. </year> <month> 17 </month>
Reference-contexts: There is a growing awareness for evaluating parallel systems using applications due to the dynamic nature of the interaction between applications and architectures. Execution-driven simulation is becoming an increasingly popular vehicle for performance prediction because of its ability to accurately capture such complex interactions in parallel systems <ref> [25, 23] </ref>. However, simulating every artifact of a parallel system places tremendous requirements on resource usage, both in terms of space and time. A sufficiently abstract simulation model which does not compromise on accuracy can help in easing this problem.
Reference: [24] <author> J. P. Singh, W-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <year> 1991. </year>
Reference-contexts: systems [25], identifying parallel system (both algorithmic and architectural) bottlenecks [25], and synthesizing architectural requirements from an application viewpoint [27]. 4 Application Characteristics Three of the applications (EP, IS and CG) used in this study are from the NAS parallel benchmark suite [7]; CHOLESKY is from the SPLASH benchmark suite <ref> [24] </ref>; and FFT is the well-known Fast Fourier Transform algorithm. EP and FFT are well-structured applications with regular communication patterns determinable at compile-time, with the difference that EP has a higher computation to communication ratio.
Reference: [25] <author> A. Sivasubramaniam, A. Singla, U. Ramachandran, and H. Venkateswaran. </author> <title> An Approach to Scalability Study of Shared Memory Parallel Systems. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1994 Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 171-180, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: There is a growing awareness for evaluating parallel systems using applications due to the dynamic nature of the interaction between applications and architectures. Execution-driven simulation is becoming an increasingly popular vehicle for performance prediction because of its ability to accurately capture such complex interactions in parallel systems <ref> [25, 23] </ref>. However, simulating every artifact of a parallel system places tremendous requirements on resource usage, both in terms of space and time. A sufficiently abstract simulation model which does not compromise on accuracy can help in easing this problem. <p> Patel [19] analyzes the impact of caches on multiprocessor performance. But many of these models make simplifying assumptions about the hardware and/or the applications, restricting their ability to model the behavior of real parallel systems. Execution-driven simulation is becoming increasingly popular for capturing the dynamic behavior of parallel systems <ref> [25, 8, 10, 13, 21] </ref>. Some of these simulators have abstracted out the instruction-set of the processors, since a detailed simulation of the instruction-set is not likely to contribute significantly to the 3 performance analysis of parallel systems. <p> The reader is referred to <ref> [26, 25] </ref> for a detailed description of SPASM where we illustrated its use in studying the scalability of a number of parallel applications on different shared memory [25] and message-passing [26] platforms. <p> The reader is referred to [26, 25] for a detailed description of SPASM where we illustrated its use in studying the scalability of a number of parallel applications on different shared memory <ref> [25] </ref> and message-passing [26] platforms. The input parameters that may be specified to SPASM are the number of processors, the CPU clock speed, the network topology, the link bandwidth and switching delays. SPASM provides a wide range of statistical information about the execution of the program. <p> It gives 6 the total time (simulated time) which is the maximum of the running times of the individual parallel processors. This is the time that would be taken by an execution of the parallel program on the target parallel machine. The profiling capabilities of SPASM (outlined in <ref> [25] </ref>) provide a novel isolation and quantification of different overheads in a parallel system that contribute to the performance of the parallel system. These overheads may be broadly separated into a purely algorithmic component, and an interaction component arising from the interaction of the algorithm with the architecture. <p> In related studies, we have illustrated the importance of separating parallel system overheads in scalability studies of parallel systems <ref> [25] </ref>, identifying parallel system (both algorithmic and architectural) bottlenecks [25], and synthesizing architectural requirements from an application viewpoint [27]. 4 Application Characteristics Three of the applications (EP, IS and CG) used in this study are from the NAS parallel benchmark suite [7]; CHOLESKY is from the SPLASH benchmark suite [24]; and <p> In related studies, we have illustrated the importance of separating parallel system overheads in scalability studies of parallel systems <ref> [25] </ref>, identifying parallel system (both algorithmic and architectural) bottlenecks [25], and synthesizing architectural requirements from an application viewpoint [27]. 4 Application Characteristics Three of the applications (EP, IS and CG) used in this study are from the NAS parallel benchmark suite [7]; CHOLESKY is from the SPLASH benchmark suite [24]; and FFT is the well-known Fast Fourier Transform algorithm.
Reference: [26] <author> A. Sivasubramaniam, A. Singla, U. Ramachandran, and H. Venkateswaran. </author> <title> A Simulation-based Scalability Study of Parallel Systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 1994. To appear. </note>
Reference-contexts: The reader is referred to <ref> [26, 25] </ref> for a detailed description of SPASM where we illustrated its use in studying the scalability of a number of parallel applications on different shared memory [25] and message-passing [26] platforms. <p> The reader is referred to [26, 25] for a detailed description of SPASM where we illustrated its use in studying the scalability of a number of parallel applications on different shared memory [25] and message-passing <ref> [26] </ref> platforms. The input parameters that may be specified to SPASM are the number of processors, the CPU clock speed, the network topology, the link bandwidth and switching delays. SPASM provides a wide range of statistical information about the execution of the program.
Reference: [27] <author> A. Sivasubramaniam, A. Singla, U. Ramachandran, and H. Venkateswaran. </author> <title> Synthesizing network requirements using parallel scientific applications. </title> <type> Technical Report GIT-CC-94/31, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: In related studies, we have illustrated the importance of separating parallel system overheads in scalability studies of parallel systems [25], identifying parallel system (both algorithmic and architectural) bottlenecks [25], and synthesizing architectural requirements from an application viewpoint <ref> [27] </ref>. 4 Application Characteristics Three of the applications (EP, IS and CG) used in this study are from the NAS parallel benchmark suite [7]; CHOLESKY is from the SPLASH benchmark suite [24]; and FFT is the well-known Fast Fourier Transform algorithm.
Reference: [28] <author> Leslie G. Valiant. </author> <title> A Bridging Model for Parallel Computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: The PRAM model has been augmented with additional parameters to account for memory access latency [4], memory access conflicts [5], and cost of synchronization [15, 9]. The Bulk Synchronous Parallel (BSP) model <ref> [28] </ref> and the LogP model [11] are departures from the PRAM models, and attempt to realistically bridge the gap between theory and practice. Similarly, considerable effort has been expended in the area of performance evaluation in developing simple analytical abstractions to model the complex behavior of parallel systems.
Reference: [29] <author> H. Wabnig and G. Haring. </author> <title> PAPS The Parallel Program Performance Prediction Toolset. </title> <booktitle> In Proceedings of the 7th International Conference on Modeling Techniques and Tools for Computer Performance Evaluation, </booktitle> <address> Vienna, Austria, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Researchers have tried to use other abstractions for the workload as well as the simulated hardware in order to speed up the simulation. In <ref> [29] </ref> a Petri net model is used for the application and the hardware. Mehra et al. [17] use application knowledge in abstracting out phases of the execution. The issue of locality has been well investigated in the architecture community.
Reference: [30] <author> D. A. Wood et al. </author> <title> Mechanisms for cooperative shared memory. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 156-167, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: There have also been application-driven studies which try to synthesize cache requirements from the application viewpoint. For instance, Gupta et al. [22] show that a small-sized cache of around 64KB can accommodate the important working set of many applications. Similarly, Wood et al. <ref> [30] </ref> show that the performance of a suite of applications is not very sensitive to different cache coherence protocols. <p> In this paradigm, each processor works with a different portion of the data space, leading to lower coherence related traffic compared to applications where there is a more active sharing of the data space. It may be noted that Wood et al. <ref> [30] </ref> also present simulation results showing that the performance of a suite of applications is not very sensitive to different cache coherence protocols. However, further study with a wider suite of applications is required to validate this claim.
Reference: [31] <author> J. C. Wyllie. </author> <title> The Complexity of Parallel Computations. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, NY, </address> <year> 1979. </year> <title> 18 Latency Overhead 19 Contention Overhead 20 Contention Overhead (contd) 21 Contention Overhead (contd) (Send/Receive Separation) 22 Execution Time 23 Execution Time (contd) 24 </title>
Reference-contexts: The algorithmic overhead arises from factors such as the serial part and work-imbalance in the algorithm, and is captured by the ideal time metric provided by SPASM. Ideal time is the time taken by the parallel program to execute on an ideal machine such as the PRAM <ref> [31] </ref>. This metric includes the algorithmic overheads but does not include any overheads arising from architectural limitations. Of the interaction component, the latency and contention introduced by network limitations are the important overheads that are of relevance to this study.
References-found: 31

