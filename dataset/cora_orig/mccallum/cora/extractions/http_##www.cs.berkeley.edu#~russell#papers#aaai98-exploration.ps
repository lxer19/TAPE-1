URL: http://www.cs.berkeley.edu/~russell/papers/aaai98-exploration.ps
Refering-URL: http://www.cs.berkeley.edu/~russell/publications.html
Root-URL: 
Email: dearden@cs.ubc.ca  nir@cs.berkeley.edu  russell@cs.berkeley.edu  
Title: Bayesian Q-learning  
Author: Richard Dearden Nir Friedman Stuart Russell 
Address: Vancouver, BC V6T 1Z4, Canada  387 Soda Hall  Berkeley, CA 94720  387 Soda Hall  Berkeley, CA 94720  
Affiliation: Department of Computer Science University of British Columbia  Computer Science Division  University of California  Computer Science Division  University of California  
Abstract: A central problem in learning in complex environments is balancing exploration of untested actions against exploitation of actions that are known to be good. The benefit of exploration can be estimated using the classical notion of Value of Infor-mationthe expected improvement in future decision quality that might arise from the information acquired by exploration. Estimating this quantity requires an assessment of the agent's uncertainty about its current value estimates for states. In this paper, we adopt a Bayesian approach to maintaining this uncertain information. We extend Watkins' Q-learning by maintaining and propagating probability distributions over the Q-values. These distributions are used to compute a myopic approximation to the value of information for each action and hence to select the action that best balances exploration and exploitation. We establish the convergence properties of our algorithm and show experimentally that it can exhibit substantial improvements over other well-known model-free exploration strategies. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> M. Abramowitz and I. A. Stegun, editors. </editor> <title> Handbook of Mathematical Functions. </title> <publisher> Dover, </publisher> <year> 1964. </year>
Reference-contexts: The requirement that ff 1 + * is to ensure that ff &gt; 1 so that the normal-gamma distribution is well defined. Although this theorem does not give a closed-form solution for ff, we can find a numerical solution easily since g (y) is a monotonically decreasing function <ref> [1] </ref>. Another complication with this approach is that it requires us to compute E [t s;a ], E [t s;a s;a ], E [t s;a 2 s;a ] and E [log t s;a ] with respect to p mix r;t ( s;a ; t s;a ).
Reference: [2] <author> R. E. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton Univ. Press, </publisher> <year> 1957. </year>
Reference-contexts: Letting V fl (s) denote the optimal expected discounted reward achievable from state s and Q fl (s; a) denote the value of executing a at s, we have the standard Bellman equations <ref> [2] </ref>: V fl (s) = max a Q fl (s; a) X r p r (rjs; a) + fl t a Reinforcement learning procedures attempt to maximize the agent's expected reward when the agent does not know p t and p r .
Reference: [3] <author> D. A. Berry and B. Fristedt. </author> <title> Bandit Problems: Sequential Allocation of Experiments. </title> <publisher> Chapman and Hall, </publisher> <year> 1985. </year>
Reference-contexts: Although this problem is well-defined, given a prior distribution over possible environments, it is not easy to solve exactly. Solutions are known only for very restricted casesmostly the so-called bandit problems in which the environment has a single state, several actions, and unknown rewards <ref> [3] </ref>. Section 2 discusses several existing approaches to exploration, as well as the model-free Q-learning algorithm we use as our underlying learning method. This paper presents two new approaches to exploration: Q-value sampling: Wyatt [17] proposed Q-value sampling as a method for solving bandit problems.
Reference: [4] <author> C. M. Bishop. </author> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford Univ. Press, </publisher> <year> 1995. </year>
Reference-contexts: Finally, it should be possible to use function approximators to extend this work to problems with large and/or continuous state spaces. There is a well-understood theory of Bayesian neural network learning <ref> [4, Ch. 10] </ref> that allows posterior means and variances to be computed for each point in the input space; these can be fed directly into our algorithm. 2 We performed parameter adjustment to find the best-performing parameters for each method.
Reference: [5] <author> B. W. Brown, J. Lovato, and K. Russell. </author> <title> Library of routines for cumulative distribution functions, inverses, and other parameters, </title> <booktitle> 1997. </booktitle> <address> ftp://odin.mdacc. tmc.edu/pub/source/dcdflib.c-1.1. tar.gz. </address>
Reference-contexts: Of course, sampling from a distribution of the form of (2) is nontrivial and requires evaluation of the cumulative distribution P ( &lt; x). Fortunately, T (x : d) can be evaluated efficiently using standard statistical packages. In our experiments, we used the library routines of Brown et al. <ref> [5] </ref>. Q-value sampling resembles, to some extent, Boltzmann exploration. It is a stochastic exploration policy, where the probability of performing an action is related to the distribution of the associated Q-values.
Reference: [6] <author> T. M. Cover and J. A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: We can avoid this complexity by approximating p mix r;t ( s;a ; t s;a ) with a normal-gamma distribution after each update. We compute the best normal-gamma approximation by minimizing the KL-divergence <ref> [6] </ref> from the true distribution. Theorem 3.5: Let q (; t ) be some density measure over and t and let * &gt; 0.
Reference: [7] <editor> M. H. Degroot. Proability and Statistics. </editor> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: We will now define and motivate the choice of the normal-gamma distribution. See <ref> [7] </ref> for more details. A normal-gamma distribution over the mean and the precision t of an unknown normally distributed variable R is determined by a tuple of hyperparameters = h 0 ; ; ff; fii. <p> We say that p (; t ) ~ NG ( 0 ; ; ff; fi) if p (; t ) / t 2 e 1 t ff1 e fit Standard results show how to update such a prior distribution when we receive independent samples of values of R: Theorem 3.1: <ref> [7] </ref> Let p (; t ) ~ NG ( 0 ; ; ff; fi) be a prior distribution over the unknown parameters for a normally distributed variable R, and let r 1 ; : : : ; r n be n independent samples of R with M 1 = 1 n <p> To evaluate this expression, we use the marginal density of given a normal-gamma distribution. (a) (b) for which Q-value sampling has the same exploration policy even though the payoff of exploration in (b) is higher than in (a). Lemma 3.2: <ref> [7] </ref> If p (; t ) ~ NG ( 0 ; ; ff; fi), then p () = 2 2 fi ff G (ff+ 1 G (ff) fi + 2 2 2 ) (2) Pr ( &lt; x) = T ((x 0 ) ff 1 : 2ff) where T (x :
Reference: [8] <author> R. A. Howard. </author> <title> Information value theory. </title> <journal> IEEE Trans. Systems Science and Cybernetics, </journal> <volume> SSC-2:22-26, </volume> <year> 1966. </year>
Reference-contexts: In this work, we extend this approach to multi-state reinforcement learning problems. The primary contribution here is a Bayesian method for representing, updating, and propagating probability distributions over rewards. Myopic-VPI: Myopic value of perfect information <ref> [8] </ref> provides an approximation to the utility of an information-gathering action in terms of the expected improvement in decision quality resulting from the new information. This provides a direct way of evaluating the exploration/exploitation tradeoff. Like Q-value sampling, myopic-VPI uses the current probability distributions over rewards to control exploratory behavior. <p> However, in case (b) exploration seems more useful than in case (a), since the potential for larger rewards is higher for the second action in this case. Myopic-VPI selection This method considers quantitatively the question of policy improvement through exploration. It is based on value of information <ref> [8] </ref>. Its application in this context is reminiscent of its use in tree search [12], which can also be seen as a form of exploration. The idea is to balance the expected gains from explorationin the form of improved policiesagainst the expected cost of doing a potentially suboptimal action.
Reference: [9] <author> L. P. Kaelbling, M. L. Littman, and A. W. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> J. Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 237-285, </pages> <year> 1996. </year>
Reference-contexts: Go to step 1. number of experiments comparing them against other exploration strategies. In our experiments, myopic-VPI was uniformly the best approach. 2 Q-Learning We assume the reader is familiar with the basic concepts of MDPs (see, e.g., Kaelbling et al. <ref> [9] </ref>).
Reference: [10] <author> L. P. Kaelbling. </author> <title> Learning in Embedded Systems. </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: This means that sub-optimal but good actions tend to be selected more often than clearly poor actions. Both these exploration methods are undirected, meaning that no exploration-specific knowledge is used. A number of directed methods have also been proposed, of which the best known is interval estimation <ref> [10] </ref>. Most of the directed techniques can be thought of as selecting an action to perform based on the expected value of the action plus some exploration bonus [11]. <p> A navigation problem. S is the start state. The agent receives a reward upon reaching G based on the number of flags collected. Boltzmann Q-learning with Boltzmann exploration. Interval Q-learning using Kaelbling's interval-estimation algorithm <ref> [10] </ref>. IEQL+ Meuleau's IEQL+ algorithm [11]. Bayes Bayesian Q-learning as presented above, using either Q-value sampling or myopic-VPI to select actions, and either Moment updating or Mixture updating for value updates. These variants are denoted QS, VPI, Mom, Mix, respectively.
Reference: [11] <author> N. Meuleau and P. </author> <title> Bourgine. Exploration of multi-states environments: Local measures and back-propogation of uncertainty. </title> <booktitle> Machine Learning, </booktitle> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: A number of directed methods have also been proposed, of which the best known is interval estimation [10]. Most of the directed techniques can be thought of as selecting an action to perform based on the expected value of the action plus some exploration bonus <ref> [11] </ref>. In the case of interval estimation, we assume a normal distributionfor the observed future values of each action in each state, and select an action by maximizing the upper bound of a 100 (1 ff)% confidence interval (for some confidence coefficient ff) over this distribution. <p> Exploration can also be done globally, selecting actions now that we believe will lead us to less-explored parts of the state space in the future. We can do this by backing up exploration specific information along with the Q-values. Meuleau and Bourgine <ref> [11] </ref>, propose IEQL+, which is closely related to interval estimation in that it backs up Q-values and uses them to compute a local exploration bonus. Unlike interval estimation, IEQL+ also backs up an exploration bonus and combines the two to compute the new exploration value of the action. <p> The parameters of each al gorithm were tuned as well as possible for each domain. The algorithms we have used are as follows: Semi-Uniform Q-learning with semi-uniform random ex ploration. (a) Task 1 <ref> [11] </ref>. (b) Task 2 [14]. (c) Task 3. A navigation problem. S is the start state. The agent receives a reward upon reaching G based on the number of flags collected. Boltzmann Q-learning with Boltzmann exploration. Interval Q-learning using Kaelbling's interval-estimation algorithm [10]. IEQL+ Meuleau's IEQL+ algorithm [11]. <p> (a) Task 1 <ref> [11] </ref>. (b) Task 2 [14]. (c) Task 3. A navigation problem. S is the start state. The agent receives a reward upon reaching G based on the number of flags collected. Boltzmann Q-learning with Boltzmann exploration. Interval Q-learning using Kaelbling's interval-estimation algorithm [10]. IEQL+ Meuleau's IEQL+ algorithm [11]. Bayes Bayesian Q-learning as presented above, using either Q-value sampling or myopic-VPI to select actions, and either Moment updating or Mixture updating for value updates. These variants are denoted QS, VPI, Mom, Mix, respectively.
Reference: [12] <author> S. J. Russell and E. H. Wefald. </author> <title> Do the Right Thing: Studies in Limited Rationality. </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Myopic-VPI selection This method considers quantitatively the question of policy improvement through exploration. It is based on value of information [8]. Its application in this context is reminiscent of its use in tree search <ref> [12] </ref>, which can also be seen as a form of exploration. The idea is to balance the expected gains from explorationin the form of improved policiesagainst the expected cost of doing a potentially suboptimal action.
Reference: [13] <author> S. B. Thrun. </author> <title> The role of exploration in learning control. </title> <editor> In D. A. White and D. A. Sofge, eds., </editor> <title> Handbook of Intelligent Control: Neural, Fuzzy and Adaptive Approaches. </title> <publisher> Van Nostrand Reinhold, </publisher> <year> 1992. </year>
Reference-contexts: Unlike interval estimation, IEQL+ also backs up an exploration bonus and combines the two to compute the new exploration value of the action. For a survey of directed and undirected exploration techniques, see <ref> [13] </ref>. 3 Bayesian Q-learning In this work, we consider a Bayesian approach to Q-learning in which we use probability distributions to represent the uncertainty the agent has about its estimate of the Q-value of each state.
Reference: [14] <author> C. J. Watkins. </author> <title> Models of Delayed Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> Psychology Department, Cambridge University, </institution> <year> 1989. </year>
Reference-contexts: In this paper we focus on Q-learning <ref> [14] </ref>, a simple and elegant model-free method that learns Q-values without learning the model p t . In Section 6, we discuss how our results carry over to model-based learning procedures. A Q-learning agent works by estimating the values of Q fl (s; a) from its experiences. <p> In semi-uniform random exploration [16], the best action is selected with some probability p, and with probability 1 p, an action is chosen at random. In some cases, p is initially set quite low to encourage exploration, and is slowly increased. Boltzmann exploration <ref> [14] </ref> is a more sophisticated approach in which the probability of executing action a in state s is: Pr (a) = P where T is a temperature parameter that can be decreased slowly over time to decrease exploration. <p> The parameters of each al gorithm were tuned as well as possible for each domain. The algorithms we have used are as follows: Semi-Uniform Q-learning with semi-uniform random ex ploration. (a) Task 1 [11]. (b) Task 2 <ref> [14] </ref>. (c) Task 3. A navigation problem. S is the start state. The agent receives a reward upon reaching G based on the number of flags collected. Boltzmann Q-learning with Boltzmann exploration. Interval Q-learning using Kaelbling's interval-estimation algorithm [10]. IEQL+ Meuleau's IEQL+ algorithm [11].
Reference: [15] <author> C. J. Watkins and P. </author> <title> Dayan. </title> <journal> Q-learning. Machine Learning, </journal> <volume> 8(3) </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: It then select actions based on their Q-values. The algorithm is shown in Figure 1. If every action is performed in every state infinitely often, and ff is decayed appropriately, Q (s; a) will eventually converge to Q fl (s; a) for all s and a <ref> [15] </ref>. The strategy used to select an action to perform at each step is crucial to the performance of the algorithm. As with any reinforcement learning algorithm, some balance between exploration and exploitation must be found. Two commonly used methods are semi-uniform random exploration and Boltzmann exploration. <p> If this is the case, then both the Q-value sampling and the myopic-VPI strategies will, eventually, execute an optimal policy. Without going into details, the standard convergence proof <ref> [15] </ref> for Q-learning requires that each action is tried infinitely often in each state in an infinite run, and that P 1 1 and n=0 ff (n) 2 &lt; 1 where ff is the learning rate. <p> This is the case when we select actions by Q-value sampling. If we select actions using myopic-VPI, then we can no longer guarantee that each action is tried infinitely often. More precisely, myopic VPI might starve certain actions and hence we cannot apply the results from <ref> [15] </ref>. Of course, we can define a noisy version of this action selection strategy (e.g., use a Boltzmann distribution over the adjusted expected values), and this will guarantee convergence. At this stage, we do not yet have counterparts to Theorems 4.1 and 4.2 for mixture updating.
Reference: [16] <author> S. D. Whitehead and D. H. Ballard. </author> <title> Learning to per-cieve and act by trial and error. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 45-83, </pages> <year> 1991. </year>
Reference-contexts: As with any reinforcement learning algorithm, some balance between exploration and exploitation must be found. Two commonly used methods are semi-uniform random exploration and Boltzmann exploration. In semi-uniform random exploration <ref> [16] </ref>, the best action is selected with some probability p, and with probability 1 p, an action is chosen at random. In some cases, p is initially set quite low to encourage exploration, and is slowly increased.
Reference: [17] <author> J. Wyatt. </author> <title> Exploration and Inference in Learning from Reinforcement. </title> <type> PhD thesis, </type> <institution> Department of Artificial Intelligence, University of Edinburgh, </institution> <year> 1997. </year>
Reference-contexts: Section 2 discusses several existing approaches to exploration, as well as the model-free Q-learning algorithm we use as our underlying learning method. This paper presents two new approaches to exploration: Q-value sampling: Wyatt <ref> [17] </ref> proposed Q-value sampling as a method for solving bandit problems. The idea is to represent explicitly the agent's knowledge of the available rewards as probability distributions; then, an action is selected stochastically according to the current probability that it is optimal. <p> Thus, the greedy approach would select the action with the greatest mean, and would not attempt to perform exploration. In particular, it does not take into account any uncertainty about the Q-value. Q-value sampling Q-value sampling was first described by Wyatt <ref> [17] </ref> for exploration in multi-armed bandit problems. The idea is to select actions stochastically, based on our current subjective belief that they are optimal.
References-found: 17

