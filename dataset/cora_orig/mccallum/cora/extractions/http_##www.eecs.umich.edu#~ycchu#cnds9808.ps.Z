URL: http://www.eecs.umich.edu/~ycchu/cnds9808.ps.Z
Refering-URL: http://www.eecs.umich.edu/~ycchu/
Root-URL: http://www.eecs.umich.edu
Title: Keyword: Performance Analysis, Operating Systems.  
Abstract: This paper presents the communication overhead measurement of the OSF/1 3.0 kernel on the Alpha Station, DEC 3000 Model 300LX. The purpose of this measurement experiment is to quantize the CPU demand of network I/O for analytic performance analysis of distributed servers that generate significant communication overhead. The measurement methodology used is software kernel instrumentation based on ATOM, a flexible tool-building system. Measurement results are presented as overhead profiles of varied messages sizes. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Apache. </author> <year> 1997. </year> <title> The Apache HTTP Server Project. </title> <address> [URL: http://www.apache.org]. </address>
Reference-contexts: The web server software (httpd) is Apache 1.2 <ref> (Apache 1997) </ref>, and the traffic generator is a popular web benchmark program, the WebStone 2.0 (Trent and Sake 1995). The testbed was configured with 10 client machines connected with the server machine through the department LANs. Three test runs were conducted with different filelists (see appendix) for driving the benchmark.
Reference: <author> Chen, J.B. and A. Eustace. </author> <year> 1995. </year> <title> Kernel Instrumentation Tools and Techniques. </title> <type> Technical Report TR-26-95, </type> <institution> Center for Research in Computing Technology, Harvard University. </institution>
Reference: <author> Chu, Y-C. and T.J. Teorey. </author> <year> 1996. </year> <title> Modeling and Analysis of the Unix Communication Subsystems. </title> <booktitle> In Proc. of the 6th Annual CASCON Conference (Toronto, Canada): </booktitle> <pages> 42-53. </pages>
Reference-contexts: Data-touching overhead, like data movement and checksum computation, grows with messages size. Non-data touching overhead, like protocolspecific processing and device driver, tends to be invariant to message size. As a consequence, it is possible to build analytic models for communication overhead with respect to message sizes <ref> (Chu and Teorey 1996) </ref>. Quantizing the communication overhead on any platform is difficult for several reasons. First, communication is accomplished through system calls in most operating systems. It is usually inaccurate to measure the system call overhead without kernel instrumentation. <p> It should be noticed that the device driver contributes a significant portion of overhead in all four cases. This confirms our previous study of CPU usage in Internet web servers which showed about 33% of total CPU time was spent in the token-ring device driver <ref> (Chu and Teorey 1996) </ref>. It is an interesting question to know how many ACK packets are generated during a TCP session because they cause extra overhead for both ends. According to TCP specification, TCP always delays the ACK packet and hopes it can be piggybacked by later outgoing packets. <p> The general rule is that TCP will acknowledge every other packet or send out a delayed ACK no later than 200 ms (Stevens 1994). Our previous study on TCP packet analysis in web servers indicates that about 40% of TCP packets are control packets <ref> (Chu and Teorey 1996) </ref>.
Reference: <author> Clark, D.D.; V. Jacobson; J. Romkey; and H. Salwen. </author> <year> 1989. </year> <title> An Analysis of TCP Processing Overhead. </title> <journal> IEEE Commu nication Magazine 27, </journal> <volume> no. </volume> <month> 6 (June): </month> <pages> 23-29. </pages>
Reference: <author> Denham, J.M.; P. Long; and J.A. Woodward. </author> <year> 1994. </year> <note> DEC OSF/1 Version 3.0 Symmetric Multiprocessing Implementa tion. Digital Technical Journal 6, no. 3. </note>
Reference: <author> Eustace, A. and A. Srivastava. </author> <year> 1994. </year> <title> ATOM: A Flexible Interface for Building High Performance Program Analysis Tools. </title> <note> WRL Technical Note TN-44, </note> <institution> Western Research Lab oratory, Digital, </institution> <address> Palo Alto, CA. </address>
Reference-contexts: It is impossible to measure interrupt overhead through user-level programs. Third, overhead measurement through kernel instrumentation is a nontrivial and error-prone task; it requires kernel source code. In this paper, we present our work on measuring communication overhead of the OSF/1 3.0 kernel with the ATOM tool-building system <ref> (Eustace and Srivastava 1994) </ref>. Section 2 describes the measurement methodology used in our experiment. The measurement results are presented as the overhead profiles in Section 3. Section 4 presents the communication overhead measurement of the Internet web server.

Reference: <author> Kay, J. and J. Pasquale. </author> <year> 1993. </year> <title> The Importance of Non-Data Touching Processing Overheads in TCP/IP. </title> <booktitle> In Proc. of the 1993 ACM SIGCOMM conference (San Francisco). </booktitle>
Reference-contexts: Analytic analysis of Unix communication subsystems showed that communication overhead can be categorized as data-touching and non-data touching overhead <ref> (Kay and Pasquale 1993) </ref>. Data-touching overhead, like data movement and checksum computation, grows with messages size. Non-data touching overhead, like protocolspecific processing and device driver, tends to be invariant to message size.
Reference: <author> Kay, J. and J. Pasquale. </author> <year> 1996. </year> <title> Profiling and Reducing Processing Overheads in TCP/IP. </title> <journal> IEEE/ACM Transactions on Networking 4, </journal> <volume> no. </volume> <month> 6 (Dec.): </month> <pages> 817-828. </pages>
Reference-contexts: Their results showed that protocol processing overhead is somewhat acceptable considering the rich functionality of the TCP/IP protocols. They also found out that operating systems also contribute a significant portion of overhead, like data movement and memory buffer (mbuf) manipulation, in comparison to the protocol processing overhead <ref> (Kay and Pasquale 1996) </ref>. Analytic analysis of Unix communication subsystems showed that communication overhead can be categorized as data-touching and non-data touching overhead (Kay and Pasquale 1993). Data-touching overhead, like data movement and checksum computation, grows with messages size.
Reference: <author> Mogul, J.C. and K.K. Ramakrishnan. </author> <year> 1995. </year> <note> Eliminating Receive Livelock in an Interrupt-driven Kernel. WRL Research Report 95/8, </note> <institution> Western Research Laboratory, Digital, </institution> <address> Palo Alto, CA. </address>
Reference: <author> Papadopoulos, C. and G.M. Parulkar. </author> <year> 1993. </year> <title> Experimental Evaluation of SunOS IPC and TCP/IP Protocol Implementation. </title> <journal> ACM/IEEE Transactions on Networking 1, </journal> <volume> no. 2 (Apr.): </volume> <pages> 199-216. </pages>
Reference: <author> Site, R.L. </author> <year> 1992. </year> <title> Alpha AXP Architecture. </title> <note> Digital Technical Journal 4, no. 4 (Special Issue). </note>
Reference-contexts: The trace analysis program tells ATOM what actions to be taken at the instrumentation points. In our case, it only records the subroutines name and the content of Alphas cycle counter register, which can be used as timestamp in the kernel trace <ref> (Site 1992) </ref>. 2.3 Packet Generator In order to generate packets of varied sizes, the sock program in (Stevens 1994) was modified to generate equally paced packets. <p> The popular call-graph profiler, gprof, cannot be used to profile the OSF/1 kernel. Instead, a flat kernel profiler, kpro-file, was used in this experiment. Kprofile uses the performance counters on the Alphas EV4 chip to produce a program counter profile <ref> (Site 1992) </ref>. To profile a kernel, a profiled version of kernel should be built first. It should be noticed that kprofile like other kernel profilers generates significant overhead for sampling the program counters.
Reference: <author> Stevens, W.R. </author> <year> 1994. </year> <journal> TCP/IP Illustrated: </journal> <volume> Volume 1. </volume> <month> Addi-son-Wesley. </month>
Reference-contexts: First, communication is accomplished through system calls in most operating systems. It is usually inaccurate to measure the system call overhead without kernel instrumentation. Second, packet receive processing is accomplished in three separate steps inside the kernel; two of them are interrupt driven <ref> (Stevens 1994) </ref>. It is impossible to measure interrupt overhead through user-level programs. Third, overhead measurement through kernel instrumentation is a nontrivial and error-prone task; it requires kernel source code. <p> In our case, it only records the subroutines name and the content of Alphas cycle counter register, which can be used as timestamp in the kernel trace (Site 1992). 2.3 Packet Generator In order to generate packets of varied sizes, the sock program in <ref> (Stevens 1994) </ref> was modified to generate equally paced packets. <p> According to TCP specification, TCP always delays the ACK packet and hopes it can be piggybacked by later outgoing packets. The general rule is that TCP will acknowledge every other packet or send out a delayed ACK no later than 200 ms <ref> (Stevens 1994) </ref>. Our previous study on TCP packet analysis in web servers indicates that about 40% of TCP packets are control packets (Chu and Teorey 1996).
Reference: <author> Trent, G. and M. Sake. </author> <year> 1995. </year> <title> WebSTONE: The First Generation in HTTP Server Benchmarking. </title> <note> [URL: http:// www.sgi.com/Products/WebFORCE/WebStone]. </note>
Reference-contexts: The web server software (httpd) is Apache 1.2 (Apache 1997), and the traffic generator is a popular web benchmark program, the WebStone 2.0 <ref> (Trent and Sake 1995) </ref>. The testbed was configured with 10 client machines connected with the server machine through the department LANs. Three test runs were conducted with different filelists (see appendix) for driving the benchmark.
Reference: <author> Wright, G. and W.R. Stevens. </author> <year> 1995. </year> <journal> TCP/IP Illustrated, </journal> <volume> Volume 2: </volume> <booktitle> the Implementation. </booktitle> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Another point should be noticed is that the overhead of UDP input processing is larger than UDP output processing. It is because UDP input routine prepends an mbuf to hold the senders address in front of the message <ref> (Wright and Stevens 1995) </ref>. 3.3 TCP Sending Overhead Sending a TCP message employs steps similar to the UDP sending case. <p> The overhead component marked tcp includes overhead of both TCP input and output routines. tcp_output () is invoked in TCP input processing to update the receiving window and generate an ACK packet if necessary <ref> (Wright and Stevens 1995) </ref>. 3.5 Average Overhead Profile The average breakdown of communication overhead is listed in Table 2. The overhead is categorized as socket layer, transport layer (TCP/UDP), network layer (IP), device driver, data movement, checksum computation, memory allocation, and software interrupt scheduling.
References-found: 14

