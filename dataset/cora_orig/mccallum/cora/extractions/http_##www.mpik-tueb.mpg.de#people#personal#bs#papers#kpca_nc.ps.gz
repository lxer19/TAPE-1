URL: http://www.mpik-tueb.mpg.de/people/personal/bs/papers/kpca_nc.ps.gz
Refering-URL: http://www.mpik-tueb.mpg.de/people/personal/bs/svm.html
Root-URL: 
Title: Nonlinear Component Analysis as a Kernel Eigenvalue Problem  
Author: Bernhard Scholkopf Alexander Smola, Klaus-Robert Muller 
Address: Spemannstr. 38, 72076 Tubingen, Germany  Rudower Chaussee 5, 12489 Berlin, Germany  
Affiliation: Max-Planck-Institut fur biologische Kybernetik  GMD First (Forschungszentrum Informationstechnik)  
Date: 1996)  
Note: in press: Neural Computation (TR 44, Max-Planck-Institut fur biologische Kybernetik,  
Abstract: A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible 5-pixel products in 16fi16 images. We give the derivation of the method and present first experimental results on polynomial feature extraction for pattern recognition. 
Abstract-found: 1
Intro-found: 1
Reference: <author> M. Aizerman, E. Braverman, and L. Rozonoer. </author> <title> Theoretical foundations of the potential function method in pattern recognition learning. </title> <journal> Automation and Remote Control, </journal> <volume> 25:821 - 837, </volume> <year> 1964. </year>
Reference: <author> B. E. Boser, I .M. Guyon, and V. N. Vapnik. </author> <title> A training algorithm for optimal margin classifiers. </title> <editor> In D. Haussler, editor, </editor> <booktitle> Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 144-152, </pages> <address> Pittsburgh, PA, 1992. </address> <publisher> ACM Press. </publisher>
Reference: <author> C. Bregler and M. Omohundro. </author> <title> Surface learning with applications to lipreading. </title> <editor> In J. D. Cowan, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Precessing Systems 6, </booktitle> <address> San Mateo, CA, 1994. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Locally Linear PCA. In cases where a linear PCA fails because the dependences in the data vary nonlinearly with the region in input space, it can be fruitful to use an approach where linear PCA is applied locally <ref> (e.g. Bregler & Omohundro, 1994) </ref>. Possibly, also kernel PCA could be improved by taking into account locality. Kernel PCA.
Reference: <author> C. J. C. Burges. </author> <title> Simplified support vector decision rules. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Proc. 13th Intl. Conf. on Machine Learning, </booktitle> <address> San Mateo, CA, 1996. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> C. Cortes and V. Vapnik. </author> <title> Support vector networks. </title> <journal> Machine Learning, </journal> <volume> 20:273 - 297, </volume> <year> 1995. </year>
Reference: <author> R. Courant and D. </author> <title> Hilbert. </title> <journal> Methods of Mathematical Physics, </journal> <volume> volume 1. </volume> <publisher> Interscience Publishers, Inc, </publisher> <address> New York, </address> <year> 1953. </year>
Reference-contexts: C Mercer Kernels Mercer's theorem of functional analysis <ref> (e.g. Courant & Hilbert, 1953) </ref> gives con ditions under which we can construct the mapping from the eigenfunction de composition of k.
Reference: <author> Y. Le Cun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. J. Jackel. </author> <title> Backpropagation applied to handwritten zip code recognition. </title> <journal> Neural Computation, </journal> <volume> 1:541 - 551, </volume> <year> 1989. </year>
Reference-contexts: We can, however, compare results to other feature extraction methods which have been used in the past by researchers working on the USPS classification problem (cf. Sec. 5). Our system of kernel PCA feature extraction plus linear support vector machine for instance performed better than LeNet1 <ref> (LeCun et al., 1989) </ref>. Even though the latter result has been obtained a number of years ago, it should be stressed that LeNet1 provides an architecture which contains a great deal of prior information about the handwritten character classification problem. <p> The principal components of a test point x in that case take the form (Fig. 2) P i tanh ((x i ; x) + fi). Principal Curves. An approach with a clear geometric interpretation in input space is the method of principal curves <ref> (Hastie & Stuetzle, 1989) </ref>, which iteratively estimates a curve (or surface) capturing the structure of the data.
Reference: <author> K. I. Diamantaras and S. Y. Kung. </author> <title> Principal Component Neural Networks. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: Furthermore, in the case where we need to use a large number ` of observations, we may want to work with an algorithm for computing only the largest eigenvalues, as for instance the power method with deflation <ref> (for a discussion, see Diamantaras & Kung, 1996) </ref>. In addition, we can consider using an estimate of the matrix K, computed from a subset 7 of M &lt; ` examples, while still extracting principal components from all ` examples (this approach was chosen in some of our experiments described below). <p> Consider a linear 3-layer perceptron with a hidden layer which is smaller than the input. If we train it to reproduce the input values as outputs (i.e. use it in autoassociative mode), then the hidden unit activations form a lower-dimensional representation of the data, closely related to PCA <ref> (see for instance Diamantaras & Kung, 1996) </ref>.
Reference: <author> N. Dunford and J. T. Schwartz. </author> <title> Linear Operators Part II: Spectral Theory, Self Adjoint Operators in Hilbert Space. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1963. </year>
Reference-contexts: Although formulated originally for the case where the integral operator acts on functions f on L 2 ([a; b]), Mercer's theorem also holds if f is defined on a space of arbitrary dimensionality, provided that it is compact <ref> (e.g. Dunford & Schwartz, 1963) </ref>.
Reference: <author> T. Hastie and W. Stuetzle. </author> <title> Principal curves. </title> <journal> JASA, </journal> <volume> 84:502 - 516, </volume> <year> 1989. </year>
Reference-contexts: The principal components of a test point x in that case take the form (Fig. 2) P i tanh ((x i ; x) + fi). Principal Curves. An approach with a clear geometric interpretation in input space is the method of principal curves <ref> (Hastie & Stuetzle, 1989) </ref>, which iteratively estimates a curve (or surface) capturing the structure of the data.
Reference: <author> I. T. Jolliffe. </author> <title> Principal Component Analysis. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1986. </year>
Reference: <author> M. Kirby and L. Sirovich. </author> <title> Application of the Karhunen-Loeve procedure for the characterization of human faces. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12(1) </volume> <pages> 103-108, </pages> <year> 1990. </year>
Reference: <author> E. Oja. </author> <title> A simplified neuron model as a principal component analyzer. </title> <journal> J. Math. Biology, </journal> <volume> 15:267 - 273, </volume> <year> 1982. </year>
Reference: <author> T. Poggio. </author> <title> On optimal nonlinear associative recall. </title> <journal> Biological Cybernetics, </journal> <volume> 19 </volume> <pages> 201-209, </pages> <year> 1975. </year>
Reference: <author> B. Scholkopf, C. Burges, and V. Vapnik. </author> <title> Extracting support data for a given task. </title> <editor> In U. M. Fayyad and R. Uthurusamy, editors, </editor> <booktitle> Proceedings, First Intl. Conference on Knowledge Discovery & Data Mining. </booktitle> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1995. </year>
Reference-contexts: These different kernels allow the construction of Polynomial Classifiers, Radial Basis Function Classifiers and Neural Networks with the Support Vector algorithm which exhibit very similar accuracy. In addition, they all construct their decision functions from an almost identical subset of a small number of training patterns, the Support Vectors <ref> (Scholkopf, Burges, & Vapnik, 1995) </ref>. The application of (16) to our problem is straightforward: we simply substitute an a priori chosen kernel function k (x; y) for all occurences of ((x) (y)). <p> For all numbers of features, the optimal degree of kernels to use is around 4, which is compatible with Support Vector machine results on the same data set <ref> (Scholkopf, Burges, & Vapnik, 1995) </ref>. Moreover, with only one exception, the nonlinear features are superior to their linear counterparts. <p> Clearly, the last point has yet to be evaluated in practice, however, for the support vector machine, the utility of different kernels has already been established. Different kernels (polynomial, sigmoid, Gaussian) led to fine classification performances <ref> (Scholkopf, Burges, & Vap-nik, 1995) </ref>. The general question of how to select the ideal kernel for a given task (i.e. the appropriate feature space), however, is an open problem. We conclude with a twofold outlook.
Reference: <author> B. Scholkopf, A. Smola, and K.-R. Muller. </author> <title> Nonlinear component analysis as a kernel eigenvalue problem. </title> <type> Technical Report 44, </type> <institution> Max-Planck-Institut fur biologische Kybernetik, </institution> <year> 1996. </year>
Reference-contexts: These encouraging results have been reproduced on an object recognition task <ref> (Scholkopf, Smola & Muller, 1996) </ref>. 6 Discussion Feature Extraction for Classification. This paper was devoted to the presentation of a new technique for nonlinear PCA. To develop this technique, we made use of a kernel method so far only used in supervised learning (Vapnik, 1995). <p> It is beyond the scope of the present paper to explore all the possibilities, including many distance-based algorithms, in detail. Some of them are currently being investigated, for instance nonlinear forms of k-means clustering and kernel-based independent component analysis <ref> (Scholkopf, Smola, & Muller, 1996) </ref>. Linear PCA is being used in numerous technical and scientific applications, including noise reduction, density estimation, image indexing and retrieval systems, and the analysis of natural image statistics.
Reference: <author> V. Vapnik. </author> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: Even though this general fact was known (Burges, private communication), the machine learning community has made 1 little use of it, the exception being Support Vector machines <ref> (Vapnik, 1995) </ref>. In this paper, we give an example of applying this method in the domain of unsupervised learning, to obtain a nonlinear form of PCA. In the next section, we will first review the standard PCA algorithm. <p> For instance <ref> (Vapnik, 1995) </ref>, if x = (x 1 ; x 2 ), then C 2 (x) = (x 2 2 ; x 1 x 2 ; x 2 x 1 ), or, yielding the same value of the dot product, 2 (x) = (x 2 2 ; 2 x 1 x 2 <p> These different kernels allow the construction of Polynomial Classifiers, Radial Basis Function Classifiers and Neural Networks with the Support Vector algorithm which exhibit very similar accuracy. In addition, they all construct their decision functions from an almost identical subset of a small number of training patterns, the Support Vectors <ref> (Scholkopf, Burges, & Vapnik, 1995) </ref>. The application of (16) to our problem is straightforward: we simply substitute an a priori chosen kernel function k (x; y) for all occurences of ((x) (y)). <p> For all numbers of features, the optimal degree of kernels to use is around 4, which is compatible with Support Vector machine results on the same data set <ref> (Scholkopf, Burges, & Vapnik, 1995) </ref>. Moreover, with only one exception, the nonlinear features are superior to their linear counterparts. <p> This paper was devoted to the presentation of a new technique for nonlinear PCA. To develop this technique, we made use of a kernel method so far only used in supervised learning <ref> (Vapnik, 1995) </ref>. Kernel PCA constitutes a mere first step towards exploiting this technique for a large class of algorithms.
Reference: <author> V. Vapnik and A. Chervonenkis. </author> <title> Theory of Pattern Recognition [in Russian]. </title> <publisher> Nauka, </publisher> <address> Moscow, </address> <year> 1974. </year> <title> (German Translation: </title> <editor> W. Wapnik & A. Tscherwonenkis, Theorie der Zeichenerkennung, </editor> <publisher> Akademie-Verlag, </publisher> <address> Berlin, </address> <year> 1979). </year> <month> 18 </month>
References-found: 18

