URL: http://www.cs.purdue.edu/homes/sb/papers/DynamicScheduling.ps
Refering-URL: http://www.cs.purdue.edu/homes/sb/papers/LIST.html
Root-URL: http://www.cs.purdue.edu
Title: Dynamic Scheduling of Process Groups  
Author: Kuei Yu Wang, Dan C. Marinescu, and Octavian F. Carbunar 
Note: Work supported in part by NSF grants BIR-9301210 and MCR-9527131, by the Scalable I/O Initiative, by a grant from Intel Corporation and by CNPq Brazil  
Date: March 20, 1997  
Address: West Lafayette, IN 47907  
Affiliation: Computer Sciences Department Purdue University  
Abstract: In this paper we introduce the concept of temporal locality of communication for process groups and a hierarchical decision model for dynamic scheduling of process groups. Empirical evidence suggests that, once a member of a process group starts to communicate with other processes in the group, it will continue to do so, while an independent process will maintain its state of isolation for some time. Other instances of inertial behavior of programs are known. Temporal and spatial locality of reference are example of inertial behavior of programs, exploited by hierarchical storage systems; once a block of information (program or data) is brought into faster storage, it is very likely that it will be referenced again within a short time frame. When process groups exhibit temporal locality of communication, this information can be used to hide the latency of paging and I/O operations, to perform dynamic scheduling to reduce processor fragmentation, and to identify optimal instances of time for check-pointing of process groups. In our scheduling model the supervisory process of a process group collects information about the dynamics of the group and shares it with local and global scheduling agents. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. J. Atallah, C. Lock, D. C. Marinescu, H. J. Siegel, and T. L. Casavant. </author> <title> Models and algorithms for co-scheduling compute-intensive tasks on a network of workstations. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16 </volume> <pages> 319-327, </pages> <year> 1992. </year>
Reference-contexts: When combined with support for demand paging, this strategy leads to wasted CPU cycles and longer execution time [2], [13]. Co-scheduling is an alternative to gang scheduling <ref> [1] </ref>, [10]. In case of co-scheduling, the dynamics of synchronization requirements of the application is taken into account; only those members of the process group which need to communicate with one another are scheduled concurrently.
Reference: [2] <author> D. C. Burger, R. S. Hyder, B. P. Miller, and D. A. Wood. </author> <title> Paging tradeoffs in distributed-shared-memory multiprocessors. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: Gang scheduling with busy waiting, is used extensively by existing MIMD systems, e.g., Paragon, CM5, SP2, Alliant FX/8 for parallel applications which exhibit fine-grain interactions. When combined with support for demand paging, this strategy leads to wasted CPU cycles and longer execution time <ref> [2] </ref>, [13]. Co-scheduling is an alternative to gang scheduling [1], [10]. In case of co-scheduling, the dynamics of synchronization requirements of the application is taken into account; only those members of the process group which need to communicate with one another are scheduled concurrently.
Reference: [3] <author> S.-H. Chiang, R. K. Mansharamani, and M. K. Vernon. </author> <title> Use of application characteristics and limited preemption for run-to-completion parallel processor scheduling policies. </title> <journal> Performance Evaluation Review, </journal> <volume> 22(1) </volume> <pages> 33-44, </pages> <month> May </month> <year> 1994. </year>
Reference: [4] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to algorithms. </title> <publisher> McGraw-Hill Book Company, </publisher> <year> 1991. </year>
Reference-contexts: be either one or zero defined as follows: M [i; j] = 1 A i sends messages to A j during the time window 0 if there is no communication between A i and A j The pseudocode for determining the largest communication working set is illustrated in algorithm in <ref> [4] </ref> for determining the connected components of an undirected graph using the disjoint-set data structure. Figure 3 adapted from [4], illustrates the pseudo code for finding connected components (clusters). Using the union by rank and path compression heuristics presented in [4], the running time for finding connected components is almost linear <p> j during the time window 0 if there is no communication between A i and A j The pseudocode for determining the largest communication working set is illustrated in algorithm in <ref> [4] </ref> for determining the connected components of an undirected graph using the disjoint-set data structure. Figure 3 adapted from [4], illustrates the pseudo code for finding connected components (clusters). Using the union by rank and path compression heuristics presented in [4], the running time for finding connected components is almost linear in the total number of operations. (Make set, Find set and Union operations.) A snapshot of the computation is <p> the largest communication working set is illustrated in algorithm in <ref> [4] </ref> for determining the connected components of an undirected graph using the disjoint-set data structure. Figure 3 adapted from [4], illustrates the pseudo code for finding connected components (clusters). Using the union by rank and path compression heuristics presented in [4], the running time for finding connected components is almost linear in the total number of operations. (Make set, Find set and Union operations.) A snapshot of the computation is illustrated 16 in Figure 4.
Reference: [5] <author> M. A. Cornea-Hasegan, D. C. Marinescu, and Z. Zhang. </author> <title> Data management for a class of iterative computations on distributed memory MIMD systems. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 6(3) </volume> <pages> 205-229, </pages> <year> 1994. </year>
Reference-contexts: The program implements a shared virtual memory and operates in two modes, the DD mode, where the shared virtual memory resides on the external storage device, and the DC mode, where the input data is distributed over the set of compute nodes. Reference <ref> [5] </ref> describes different data management strategies 17 for implementing a shared virtual memory. The entire data set is partitioned into Data Allocation Units, DAUs. The working set of DAU j consists of all DAUs needed to carry out the computations associated with DAU j .
Reference: [6] <author> M. A. Cornea-Hasegan, Z. Zhang, R. E. Lynch, D. C. Marinescu, A. Hadfield, J. K. Muckelbauer, S. Munshi, L. Tong, and M. G. Rossmann. </author> <title> Phase refinement and extension by means of non-crystallographic symmetry averaging using parallel computers. </title> <journal> Acta Crystallographica, </journal> <volume> D51:749-759, </volume> <year> 1995. </year> <month> 23 </month>
Reference-contexts: G 2 ; : : : ; G 4 g after processing C A k of each PE A k . 4.2 The applications We discuss below two programs in the Molecular Replacement suite we have examined, envelope and fftsynth, and results gathered during their execution on a Paragon system <ref> [6] </ref>. The envelope program computes the molecular envelope of a virus. It needs as input a 3-D lattice with up to 10 9 grid points and produces a lattice of equal size as output.
Reference: [7] <author> P. J. Denning. </author> <title> The working set model for program behavior. </title> <journal> Communications of the ACM, </journal> <volume> 11(5) </volume> <pages> 323-333, </pages> <month> May </month> <year> 1968. </year>
Reference-contexts: G i = cluster i. g i = the number of elements of cluster i. Following the definition of working set model introduced by Denning <ref> [7] </ref>, we define the communication working set of process A k at time t as: C A k (t; ) = collection of processes that communicate with process A k during the time interval (t ; t) = working set parameter For a given time window [t ; t], C A
Reference: [8] <author> A. Ieumwananonthachai, A. N. Aizawa, S. R. Schwartz, B. W. Wah, and J. C. Yan. </author> <title> Intelligent mapping of communicating processes in distributed computing systems. </title> <booktitle> In Supercomputing '91, </booktitle> <pages> pages 512-521, </pages> <month> November </month> <year> 1991. </year>
Reference: [9] <author> S. Majumdar, D. L. Eager, and R. B. Bunt. </author> <title> Characterization of programs for scheduling in multiprogrammed parallel systems. </title> <booktitle> Performance Evaluation 13(2) </booktitle> <pages> 109-130, </pages> <year> 1991. </year>
Reference: [10] <author> J. K. Ousterhout. </author> <title> Scheduling techniques for concurrent systems. </title> <booktitle> In Proceedings of the 3rd Intl. Conf. Distributed Computing Systems, </booktitle> <pages> pages 22-30, </pages> <month> October </month> <year> 1982. </year>
Reference-contexts: When combined with support for demand paging, this strategy leads to wasted CPU cycles and longer execution time [2], [13]. Co-scheduling is an alternative to gang scheduling [1], <ref> [10] </ref>. In case of co-scheduling, the dynamics of synchronization requirements of the application is taken into account; only those members of the process group which need to communicate with one another are scheduled concurrently.
Reference: [11] <author> K. C. Sevcik. </author> <title> Characterization of parallelism in applications and their use in scheduling. </title> <journal> Performance Evaluation Review, </journal> <volume> 17 </volume> <pages> 171-180, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Although multiprogramming allows better service to be provided to the users, it also complicates the processor allocation issues in multiprocessor systems. 2 There are two classes of scheduling algorithms for multiprogrammed parallel systems static and dynamic, [3],[8],[9], <ref> [11] </ref>, [12]. The static scheduling algorithms are non-preemptive, each application runs to completion without interruption on the set of processors initially allocated for it.
Reference: [12] <author> K. C. Sevcik. </author> <title> Application scheduling and processor allocation in multipro-grammed parallel processing systems. Performance Evaluation, </title> <address> 19(2-3):107-140, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: Although multiprogramming allows better service to be provided to the users, it also complicates the processor allocation issues in multiprocessor systems. 2 There are two classes of scheduling algorithms for multiprogrammed parallel systems static and dynamic, [3],[8],[9], [11], <ref> [12] </ref>. The static scheduling algorithms are non-preemptive, each application runs to completion without interruption on the set of processors initially allocated for it.
Reference: [13] <author> Tumuri C., Mohan C.K., and Choudhary, A. </author> <title> Unsupervised algorithms for learning emerging spatio-temporal correlations. </title> <booktitle> In Proceedings IEEE-ICNN International Conference in Neural Networks. </booktitle>
Reference-contexts: Gang scheduling with busy waiting, is used extensively by existing MIMD systems, e.g., Paragon, CM5, SP2, Alliant FX/8 for parallel applications which exhibit fine-grain interactions. When combined with support for demand paging, this strategy leads to wasted CPU cycles and longer execution time [2], <ref> [13] </ref>. Co-scheduling is an alternative to gang scheduling [1], [10]. In case of co-scheduling, the dynamics of synchronization requirements of the application is taken into account; only those members of the process group which need to communicate with one another are scheduled concurrently. <p> The supervisory process could also include algorithms to discover patterns and temporal correlations, like the ones described in <ref> [13] </ref> or connected components algorithms like the one used for data analysis in this paper. Another possibility is to include learning modules. When the application is executed repeatedly the supervisory process may collect trace data during early runs and then use these data for resource optimization during later runs.
Reference: [14] <author> K. Y. Wang and D. C. Marinescu. </author> <title> 24 Correlation of the paging activity of individual node programs in the SPMD execution mode. </title> <booktitle> In Proceedings of the 28th Hawaii International Conference on System Sciences, HICSS'28, </booktitle> <pages> pages 61-71. </pages> <publisher> IEEE Press, </publisher> <month> January </month> <year> 1995. </year>
Reference: [15] <author> K. Y. </author> <title> Wang Hiding the latency of paging and I/O operations on massively parallel systems. </title> <publisher> Ph. </publisher> <address> D. </address> <institution> Dissertation, Computer Sciences Department, Purdue University, </institution> <month> August </month> <year> 1996. </year>
Reference-contexts: When only communicating processes in the process group are active, the entire group is propitious for checkpointing. 13 4 Empirical evidence supporting temporal locality of communication To gather experimental evidence to confirm temporal locality of communication among members of a process group, we have monitored several parallel applications, <ref> [15] </ref>. We have instrumented the communication statements and examined the collected data. One possible approach to represent the results is to display the time elapsed between successive communication events for every thread of control.

References-found: 15

