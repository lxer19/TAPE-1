URL: http://www.cs.cornell.edu/Info/People/coleman/PAPERS/CTC96-TR238.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/coleman/papers.html
Root-URL: 
Title: Structure and efficient Jacobian calculation  
Author: Thomas F. Coleman Arun Verma 
Date: April 4, 1996  
Abstract: Cornell Theory Center Technical Report CTC96TR238 Abstract Many computational tasks require the determination of the Jacobian matrix, at a given argument, for a large nonlinear system of equations. Calculation or approximation of a Newton step is a related task. The development of robust automatic differentiation (AD) software allows for "painless" and accurate calculation of these quantities; however, straightforward application of AD software on large-scale problems can require an inordinate amount of computation. Fortunately, large-scale systems of nonlinear equations typically exhibit either sparsity or structure in their Jacobian matrices. In this paper we proffer general approaches for exploiting sparsity and structure to yield efficient ways to determine Jacobian matrices (and Newton steps) via automatic differentiation.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. M. Averick, J. J. More, C. H. Bischof, A. Carle, and A. Griewank, </author> <title> Computing large sparse Jacobian matrices using automatic differentiation, </title> <journal> SIAM Journal on Scientific Computing, </journal> <volume> 15 (1994), </volume> <pages> pp. 285-294. </pages>
Reference-contexts: The purpose of this paper is to show how it is possible to dramatically lower the cost of computing J by exploiting structure and sparsity in the application of AD. Recently, techniques for the efficient determination of sparse Jacobian matrices J , via AD, have been developed <ref> [1, 9, 13] </ref>. <p> corresponding Newton equations: J E B ffi x ffi y 2 C 0 @ 0 1 A ;(5) where J E = 6 ~ J I 0 0 0 J 7 3 Here is a key point: the "extended" Jacobian matrix J E is sparse and clearly sparse AD-techniques, e.g., <ref> [1, 9, 13] </ref>, can be applied with respect to F E (x; y) = B y 1 ~ F (x) F (y 2 ) C to efficiently determine J E . <p> these ideas can be applied more generally: in many cases the natural "coarse-grained" program yields a sparse "extended" Jacobian matrix which in turn, can be efficiently computed by sparse AD-techniques. 2 Calculation of sparse Jacobians via bi-coloring New techniques for the efficient computation of sparse Jacobian matrices are developed in <ref> [1, 9, 13] </ref>. In this section we briefly highlight the approach proposed in [9], coined "bi-coloring".
Reference: [2] <author> C. H. Bischof, A. Bouaricha, P. M. Khademi, and J. J. </author> <title> More, Computing gradients in large-scale optimization using automatic differentiation, </title> <type> Tech. Rep. </type> <institution> MCS-P488-0195, MCS, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: This special case, the efficient determination of a gradient of a partially separable function via the forward-mode of AD, is studied in detail in <ref> [2] </ref>. 4 Examples We discuss three common classes of structured nonlinear systems. In each case the Jacobian matrix is potentially dense; however, by differentiating the natural high-level program to compute F , as discussed in x3, underlying sparsity can be exploited in the use of AD tools.
Reference: [3] <author> C. </author> <title> Broyden, A class of methods for solving nonlinear simultaneous equations, </title> <journal> Mathematics of Computation, </journal> <volume> 19 (1965), </volume> <pages> pp. 577-593. </pages>
Reference-contexts: For example the extended system (5) can be solved directly. This can afford significant savings. To illustrate, consider the following experiment. We define a composite function F (x) following the form described above. The functions ~ F and F are defined to be the Broyden <ref> [3] </ref> function (the Jacobian is tridiagonal). The structure of A is based on the 5-point Laplacian defined on a regular p p n grid.
Reference: [4] <author> T. F. Coleman and J.-Y. Cai, </author> <title> The cyclic coloring problem and estimation of sparse Hessian matrices, </title> <journal> SIAM J. Alg. Disc. Meth., </journal> <volume> 7 (1986), </volume> <pages> pp. 221-235. </pages>
Reference-contexts: The motivation for the bi-coloring approach stems from the sparse finite-differencing literature <ref> [4, 5, 6, 7, 8, 10] </ref> where graph coloring is used to partition the columns of a sparse Jacobian matrix J and subsequently define a matrix V such that J can be determined from the product J V .
Reference: [5] <author> T. F. Coleman, B. S. Garbow, and J. J. </author> <title> More, Software for estimating sparse Jacobian matrices, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 10 (1984), </volume> <pages> pp. </pages> <month> 329-345. </month> <title> [6] , Software for estimating sparse Hessian matrices, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 11 (1985), </volume> <pages> pp. 363-377. </pages>
Reference-contexts: The motivation for the bi-coloring approach stems from the sparse finite-differencing literature <ref> [4, 5, 6, 7, 8, 10] </ref> where graph coloring is used to partition the columns of a sparse Jacobian matrix J and subsequently define a matrix V such that J can be determined from the product J V .
Reference: [7] <author> T. F. Coleman and J. J. </author> <title> More, Estimation of sparse Hessian matrices and graph coloring problems, </title> <journal> Math. Programming, </journal> <volume> 28 (1984), </volume> <pages> pp. </pages> <month> 243-270. </month> <title> [8] , Estimation of sparse Jacobian matrices and graph coloring problems, </title> <journal> SIAM J. on Numerical Analysis, </journal> <volume> 20 (1984), </volume> <pages> pp. 187-209. </pages>
Reference-contexts: The motivation for the bi-coloring approach stems from the sparse finite-differencing literature <ref> [4, 5, 6, 7, 8, 10] </ref> where graph coloring is used to partition the columns of a sparse Jacobian matrix J and subsequently define a matrix V such that J can be determined from the product J V .
Reference: [9] <author> T. F. Coleman and A. Verma, </author> <title> The efficient computation of sparse Jacobian matrices using automatic differentiation, </title> <type> Tech. Rep. </type> <institution> TR95-1557, Computer Science Department, Cornell 12 University, </institution> <month> November, </month> <year> 1995. </year>
Reference-contexts: The purpose of this paper is to show how it is possible to dramatically lower the cost of computing J by exploiting structure and sparsity in the application of AD. Recently, techniques for the efficient determination of sparse Jacobian matrices J , via AD, have been developed <ref> [1, 9, 13] </ref>. <p> Recently, techniques for the efficient determination of sparse Jacobian matrices J , via AD, have been developed [1, 9, 13]. The bi-coloring approach of Coleman and Verma <ref> [9] </ref>, as discussed in x2, rests on the observation that is is usually possible to define "thin" matrices V; W such that the nonzero elements of J can be readily extracted from the pair fl To appear: Proceedings of the Second SIAM International Workshop on Computational Differentiation, Sante Fe, February, 1996. <p> corresponding Newton equations: J E B ffi x ffi y 2 C 0 @ 0 1 A ;(5) where J E = 6 ~ J I 0 0 0 J 7 3 Here is a key point: the "extended" Jacobian matrix J E is sparse and clearly sparse AD-techniques, e.g., <ref> [1, 9, 13] </ref>, can be applied with respect to F E (x; y) = B y 1 ~ F (x) F (y 2 ) C to efficiently determine J E . <p> For example, the work required by the bi-coloring technique developed in <ref> [9] </ref> is !(F E ) = !(F ) where is a "bi-chromatic number" dependent on the sparsity of J E . Typically, &lt;< min (m; n). <p> these ideas can be applied more generally: in many cases the natural "coarse-grained" program yields a sparse "extended" Jacobian matrix which in turn, can be efficiently computed by sparse AD-techniques. 2 Calculation of sparse Jacobians via bi-coloring New techniques for the efficient computation of sparse Jacobian matrices are developed in <ref> [1, 9, 13] </ref>. In this section we briefly highlight the approach proposed in [9], coined "bi-coloring". <p> In this section we briefly highlight the approach proposed in <ref> [9] </ref>, coined "bi-coloring". <p> Again, it is very easy to construct examples, where defining thin W is not possible : consider the case where J has a single dense column. Bi-coloring circumvents the dense row/dense column problem as illustrated in (1). In <ref> [9] </ref> bi-coloring approaches are proposed that allow for both the direct calculation of J the non-zero elements of J are are extracted for the AD-computed products (W T J; J V ) directly, without further computations and determination by substitution where the the non-zero elements of J are are extracted for <p> Hence, the computed Jacobian matrix is usually less accurate than the directly determined Jacobian. Nevertheless, as discussed in <ref> [9] </ref>, the loss of accuracy is usually minimal. Computational experiments are reported in [9] which illustrate the effectiveness of bi-coloring as opposed to strictly 1-sided schemes. <p> Hence, the computed Jacobian matrix is usually less accurate than the directly determined Jacobian. Nevertheless, as discussed in <ref> [9] </ref>, the loss of accuracy is usually minimal. Computational experiments are reported in [9] which illustrate the effectiveness of bi-coloring as opposed to strictly 1-sided schemes. A 1-sided scheme may be column-based: partition the columns of J to define a thin matrix V such that the non-zeroes of J can be determined form the product J V (computed via forward mode AD). <p> Clearly on this set of sparsity structures bi-coloring is a significant win over 1-sided calculations . Moreover, bi-coloring combined with determination by substitution represents 5 the least-cost approach. Additional experiments, with more detail, and further related discussion is given in <ref> [9] </ref>. Bi-coloring 1-sided Coloring Direct Substitution column row 337 270 1753 452 Table 1 Totals for LP Collection (http://www.netlib.org/lp/data) 3 Structure Our thesis can be summarized as follows. Large-scale nonlinear systems F (x) = 0 often exhibit a natural lower Hessenberg form. <p> For example, Jacobians ~ J , J can be determined by applying the bi-coloring technique <ref> [9] </ref> to ~ F and F respectively. And of course if F is simple enough, J will be constant (and trivially available). If the function F is itself non-trivial it may be advantageous to exploit this by expanding the 2-line program. <p> j = ^ F j (w j ) P t Clearly the evaluation of this "group partially separable" function is easily expressed in the lower Hessenberg form given in Figure 2; the corresponding Jacobian matrix J E is likely to be sparse and economically computable via the AD techniques in <ref> [9] </ref> 4.2.1 Product function A special case of the generalized partially separable form is a product function: Suppose that F : &lt; n ! &lt; n is a component-wise product of functions: F (x) = F 1 (x): fl F 2 (x): fl : : : : fl F p (x); <p> i : Clearly it is entirely possible that J is relatively dense even when each component Jacobian J i is very sparse; hence, direct determination of J via AD is usually unattractive compared to the calculation of the extended Jacobian, J E , via AD (e.g., using the bi-coloring approach <ref> [9] </ref>). 4.3 A remark Our two main examples, composite functions and generalized partially separable functions, are complementary in a structural sense. The evaluation of a composite function is a depth computation: each subsequent intermediate variable y i depends on the previous intermediate y i1 .
Reference: [10] <author> A. R. Curtis, M. J. D. Powell, and J. K. Reid, </author> <title> On the estimation of sparse Jacobian matrices, </title> <journal> J. Inst. Math. Appl., </journal> <volume> 13 (1974), </volume> <pages> pp. 117-119. </pages>
Reference-contexts: The motivation for the bi-coloring approach stems from the sparse finite-differencing literature <ref> [4, 5, 6, 7, 8, 10] </ref> where graph coloring is used to partition the columns of a sparse Jacobian matrix J and subsequently define a matrix V such that J can be determined from the product J V .
Reference: [11] <author> A. Griewank, </author> <title> Direct calculation of Newton steps without accumulating Jacobians, in Large Scale Numerical Optimization, </title> <editor> T. F. Coleman and Y. Li, eds., </editor> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1990, </year> <pages> pp. 115-137. </pages> <note> Also appeared as Preprint MCS-P132-0290, </note> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <month> February </month> <year> 1990. </year> <title> [12] , Some Bounds on the Complexity of Gradients, Jacobians, and Hessians, in Complexity in Nonlinear Optimization, </title> <editor> P. Pardalos, ed., </editor> <publisher> World Scientific Publishers, </publisher> <year> 1993, </year> <pages> pp. 128-161. </pages>
Reference-contexts: Given a computer code to evaluate F (x), the techniques of automatic differentiation (AD) can be used to compute J (x). There are two basic modes of automatic differentiation, forward and reverse, e.g., <ref> [11, 12] </ref>. Forward mode AD yields J in time proportional to n !(F ), where !(F ) is the number of flops to evaluate F (x). Alternatively, reverse mode AD yields J (x) in time proportional to m !(F ). <p> Given an arbitrary n-by-t V matrix V , product J V can be directly calculated using automatic differentiation in the "forward mode"; given an arbitrary m-by-t W matrix W , the product W T J can be calculated using automatic differentiation in the "reverse mode", e.g., <ref> [11, 12] </ref>. <p> A natural way to evaluate f at a given argument x is to evaluate F (x) = B B @ f 2 (x) f p (x) C C A 1 Griewank <ref> [11] </ref> has proposed a similar idea in a more extreme form: F E is defined with respect to all intermediate variables. The resulting extended Jacobian matrix J E is huge, but very sparse. 7 and then sum the components of F (x). <p> In many cases this is practical. However, an interesting question is, given an arbitrary (but correct!) program to evaluate F , is it possible to automatically recover the lower Hessenberg form F E ? Of course a fine-grained lower Hessenberg structure is always available from the compiled program <ref> [11] </ref>; however, we are concerned with the natural high-level (coarse-grained) lower-Hessenberg form. This is an open question.
Reference: [13] <author> A. K. M. Hossain and T. Steihaug, </author> <title> Computing a sparse Jacobian matrix by rows and columns, </title> <type> Tech. Rep. 109, </type> <institution> Department of Informatics, University of Bergen, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: The purpose of this paper is to show how it is possible to dramatically lower the cost of computing J by exploiting structure and sparsity in the application of AD. Recently, techniques for the efficient determination of sparse Jacobian matrices J , via AD, have been developed <ref> [1, 9, 13] </ref>. <p> corresponding Newton equations: J E B ffi x ffi y 2 C 0 @ 0 1 A ;(5) where J E = 6 ~ J I 0 0 0 J 7 3 Here is a key point: the "extended" Jacobian matrix J E is sparse and clearly sparse AD-techniques, e.g., <ref> [1, 9, 13] </ref>, can be applied with respect to F E (x; y) = B y 1 ~ F (x) F (y 2 ) C to efficiently determine J E . <p> these ideas can be applied more generally: in many cases the natural "coarse-grained" program yields a sparse "extended" Jacobian matrix which in turn, can be efficiently computed by sparse AD-techniques. 2 Calculation of sparse Jacobians via bi-coloring New techniques for the efficient computation of sparse Jacobian matrices are developed in <ref> [1, 9, 13] </ref>. In this section we briefly highlight the approach proposed in [9], coined "bi-coloring".
References-found: 10

