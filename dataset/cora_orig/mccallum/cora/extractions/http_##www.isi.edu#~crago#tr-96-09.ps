URL: http://www.isi.edu/~crago/tr-96-09.ps
Refering-URL: http://www.isi.edu/~crago/
Root-URL: http://www.isi.edu
Email: crago@isi.edu  despain@isi.edu  
Phone: (310) 822-1511 ext. 713  (310) 822-1511 ext. 377  
Title: Reducing the Traffic of Loop-Based Programs Using a Prefetch Processor  
Author: Stephen P. Crago Alvin M. Despain 
Date: March 25, 1997  
Address: 4676 Admiralty Way Marina del Rey, CA 90292-6695  
Affiliation: University of Southern California Information Sciences Institute  
Abstract-found: 0
Intro-found: 1
Reference: [AgCo87] <author> T.Y. Agawala and J. Cocke. </author> <title> High Performance Reduced Instruction Set Processors. </title> <institution> IBM T.J. Watson Research Center, </institution> <type> Technical Report #55845, </type> <month> March </month> <year> 1987. </year>
Reference-contexts: As transistors continue to shrink and die sizes get larger, computer architects must determine how to best use the larger number of available transistors [FaTP94]. Processors have recently used transistors to provide multiple functional units, using the VLIW [Fish83] or superscalar <ref> [AgCo87] </ref> paradigm, and are deeply pipelined to increase the clock speed. While superscalar, VLIW, and deep pipelining can all provide additional microparallelism, processor performance is limited by memory performance, diminishing the returns of additional microparallelism within uniprocessors.
Reference: [BaCh91] <author> J.-L. Baer and T.-F. Chen. </author> <title> An effective on-chip preloading scheme to reduce data access penalty. </title> <booktitle> Proceedings of Supercomputing 91, </booktitle> <year> 1991. </year>
Reference: [Brig74] <author> E.O. Brigham. </author> <title> The Fast Fourier Transform, </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1974, </year> <note> p. 165. </note>
Reference-contexts: The simulator does not model cache or memory contention and assumes that all instructions hit in an instruction cache. We used C versions of the first five Livermore loops [McMa72], two of our own benchmarks (discrete convolution [Papo80], and the bit-reverse access pattern used in the FFT <ref> [Brig74] </ref>), and Tomcat from SPECfp95. The benchmarks represent a scientific and signal processing workload.
Reference: [BuGK96] <author> D. Burger, J.R. Goodman, and Alain Kgi. </author> <title> Memory bandwidth limitations of future microprocessors. </title> <booktitle> Proceedings of the 23st Annual International Symposium on Computer Architecture, </booktitle> <year> 1996, </year> <pages> pp. 78-89. </pages>
Reference-contexts: Multithreading tolerates latency at the expense of context-switching overhead and additional traffic because of cache pollution and possibly context saving. The problem that all of the above latency tolerating techniques is that they all require additional traffic between the on-chip cache and off-chip memory. However, <ref> [BuGK96] </ref> showed that memory traffic is also quickly becoming a performance limiting factor. A latency tolerating technique is needed that hides latency without placing a larger burden on the bandwidth available across the pins of the microprocessor chip.
Reference: [CrDe96] <author> S.P. Crago and A.M. Despain. </author> <title> Improving the performance of loop-based programs using a prefetch processor. </title> <booktitle> Submitted to the 24th Annual International Symposium on Computer Architecture. </booktitle>
Reference-contexts: In this paper, we present an alternative to traditional microprocessor designs that adds an independent processor for executing prefetches to tolerate memory latency. Furthermore, our architecture tolerates latency while decreasing traffic at the microprocessor chip boundary. We show how the CAPP (Computation And Prefetch Processor) architecture <ref> [CrDe96] </ref> tolerates memory latency using software prefetching without increasing memory traffic by allowing a smaller cache block to be used. In addition, the CAPP architecture reduces cache conflicts caused by prefetching by dynamically adapting prefetch distance based on run-time information about cache performance. <p> Our architecture also overlaps computation but is not limited to statically determined instructions in loops as it is in software pipelining. 3. The CAPP Architecture 3.1 Overview The CAPP (Computation And Prefetch Processor) architecture <ref> [CrDe96] </ref> shown in Figure 2, uses a main processor to execute the program and a prefetch processor to execute the prefetches and their related code. The main processor executes the program as any other uniprocessor would, with the exception of occasional instructions that control the Slip Control Queue.
Reference: [Chen95] <author> T.-F. Chen. </author> <title> An effective programmable prefetch engine for on-chip caches. </title> <address> Micro-28, </address> <month> November </month> <year> 1995, </year> <pages> pp. 237-242. </pages>
Reference: [CSOD96] <author> S.P. Crago, A. Srivastava, K. Obenland, and A.M. Despain. </author> <title> A high-performance, hierarchical decoupled architecture. </title> <booktitle> Submitted to the 20th Annual International Symposium on Computer Architecture. </booktitle>
Reference-contexts: Finally, the cost of the prefetch processor, its instruction cache, and the Slip Control Queue must be evaluated. We have also begun to evaluate another architecture with a decoupled access processor in addition to the decoupled prefetch processor, providing additional instruction-level parallelism and memory latency tolerance <ref> [CSOD96] </ref>. The principles of this study are also applicable to traditional uniprocessors. As we have shown, reducing the cache block size reduces the traffic required for uniprocessors. Software prefetching can be used to improve execution time.
Reference: [Denn74] <author> R. Dennard et al. </author> <title> Design of Ion-Implanted MOSFETs with very small physical dimensions. </title> <journal> IEEE Journal of Solid-State Circuits, v. CS-9, </journal> <volume> No. 5, </volume> <month> Oct. </month> <year> 1974. </year>
Reference-contexts: 1. Introduction The gap between processor speeds and DRAM speeds is increasing at an exponential rate [WuMc94]. Transistor sizes have been shrinking and die sizes have been growing steadily for many years [Sano94]. These trends have allowed processors to get faster <ref> [Denn74] </ref> and more complex. DRAM technology has also benefited from decreasing feature sizes. However, since DRAM technology is optimized for size and not speed, DRAM speeds have not kept pace with processor speeds.
Reference: [FaTP94] <author> M. Farrens, G. Tyson, and A.R. Pleszkun. </author> <title> A Study of Single-Chip Processor/ Cache Organizations for Large Number of Transistors. </title> <booktitle> Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <year> 1994, </year> <pages> pp. 338-347. </pages>
Reference-contexts: However, since DRAM technology is optimized for size and not speed, DRAM speeds have not kept pace with processor speeds. As transistors continue to shrink and die sizes get larger, computer architects must determine how to best use the larger number of available transistors <ref> [FaTP94] </ref>. Processors have recently used transistors to provide multiple functional units, using the VLIW [Fish83] or superscalar [AgCo87] paradigm, and are deeply pipelined to increase the clock speed.
Reference: [Fish83] <author> Fisher, J.A. </author> <title> Very long instruction word architectures and ELSI-512. </title> <booktitle> Proceedings of the Tenth Symposium on Computer Architecture, </booktitle> <year> 1983, </year> <pages> pp. 140-150. </pages>
Reference-contexts: As transistors continue to shrink and die sizes get larger, computer architects must determine how to best use the larger number of available transistors [FaTP94]. Processors have recently used transistors to provide multiple functional units, using the VLIW <ref> [Fish83] </ref> or superscalar [AgCo87] paradigm, and are deeply pipelined to increase the clock speed. While superscalar, VLIW, and deep pipelining can all provide additional microparallelism, processor performance is limited by memory performance, diminishing the returns of additional microparallelism within uniprocessors.
Reference: [KuHC94] <author> L. Kurian, P.T. Hulina, and L.D. Caraor. </author> <title> Memory latency effects in decoupled architectures. </title> <journal> IEEE Transactions on Computers, v. </journal> <volume> 43, no. </volume> <month> 10 (October </month> <year> 1994), </year> <pages> pp. 1129-1139. 15 </pages>
Reference-contexts: Caches exploit spatial and temporal locality to keep frequently used data near the processor. Caches have not been as successful for processors designed 3 for scientific and signal processing applications because of the poor locality exhibited by these applications <ref> [KuHC94] </ref>. Large cache block sizes (32 to 64 bytes) are used to reduce the cache miss ratio and amortize the latency of a cache miss over more data. The efficacy of large cache blocks depends on spatial locality.
Reference: [Lam88] <author> M.S. Lam. </author> <title> Software pipelining: an effective scheduling technique for VLIW machines. </title> <booktitle> Proceedings of the SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1988, </year> <pages> pp. 318-328. </pages>
Reference-contexts: Techniques such as tiling [Wolf92] try to increase locality by making compile time transformations. These techniques are limited to scientific programs which exhibit simple access behavior. In our architecture we can use these techniques as well as others which would not be possible by complier analysis alone. Software pipelining <ref> [Lam88] </ref> is a technique to expose more parallelism so that computation can be overlapped. Our architecture also overlaps computation but is not limited to statically determined instructions in loops as it is in software pipelining. 3.
Reference: [McMa72] <author> F.H. McMahon. </author> <title> Fortran CPU Performance Analysis. </title> <institution> Lawrence Livermore Laboratories, </institution> <year> 1972. </year>
Reference-contexts: The simulator does not model cache or memory contention and assumes that all instructions hit in an instruction cache. We used C versions of the first five Livermore loops <ref> [McMa72] </ref>, two of our own benchmarks (discrete convolution [Papo80], and the bit-reverse access pattern used in the FFT [Brig74]), and Tomcat from SPECfp95. The benchmarks represent a scientific and signal processing workload.
Reference: [Mips95] <institution> MIPS Technologies, Inc. </institution> <note> MIPS R10000 Microprocessor Users Manual, </note> <year> 1995. </year>
Reference-contexts: The simulator allows overlapped execution of integer and floating point instructions and models interlocks for integer multiply and divide and the latencies and issue rates of floating point operations of the MIPS R10000 <ref> [Mips95] </ref>. The simulator does not model cache or memory contention and assumes that all instructions hit in an instruction cache.
Reference: [MoLG92] <author> T.C. Mowry, M.S. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1992, </year> <pages> pp. 62-73. </pages>
Reference-contexts: Increasing the cache block size trades off execution time for traffic between the cache and the level below it, as the graphs of Figure 1 show. Prefetching is another technique used to hide memory latency. Prefetching can be done in hardware [BaCh91][Chen95] or software <ref> [MoLG92] </ref>. Hardware prefetching tolerates latency at the expense of additional hardware and additional traffic when unnecessary data is prefetched or when prefetches conflict with useful data. Cache blocks can be thought of as a form of hardware prefetching.
Reference: [Mowr94] <author> T.C. Mowry. </author> <title> Tolerating Latency Through Software-Controlled Data Prefetching. </title> <type> Ph.D. Thesis, </type> <institution> Stanford University, </institution> <year> 1994. </year>
Reference-contexts: Software prefetching requires the compiler to determine when the instruction overhead of prefetching outweighs the benefit of potentially avoiding cache misses. It also requires the compiler to determine where to insert the prefetch instructions in order to cover a potential miss. <ref> [Mowr94] </ref> and [SaPa96] propose methods for varying prefetch distance at run-time. [WaRa95] proposes to use a processor for prefetching in the context of virtual shared memory. Bandwidth Block Size Execution Time Block Size Execution Time and Bandwidth 4 Multithreading [WeGu89] is also used as a latency tolerating technique.
Reference: [Papo80] <author> A. Papoulis. </author> <title> Circuits and Systems, </title> <publisher> Holt, Rinehart and Winston, Inc., </publisher> <year> 1980, </year> <note> p. 146. </note>
Reference-contexts: The simulator does not model cache or memory contention and assumes that all instructions hit in an instruction cache. We used C versions of the first five Livermore loops [McMa72], two of our own benchmarks (discrete convolution <ref> [Papo80] </ref>, and the bit-reverse access pattern used in the FFT [Brig74]), and Tomcat from SPECfp95. The benchmarks represent a scientific and signal processing workload.
Reference: [SaPa96] <author> R.H. Saavedra and D. Park. </author> <title> Improving the effectiveness of software prefetching with adaptive execution. 1996 Parallel Architectures and Compilation Techniques, </title> <month> October </month> <year> 1996. </year>
Reference-contexts: Software prefetching requires the compiler to determine when the instruction overhead of prefetching outweighs the benefit of potentially avoiding cache misses. It also requires the compiler to determine where to insert the prefetch instructions in order to cover a potential miss. [Mowr94] and <ref> [SaPa96] </ref> propose methods for varying prefetch distance at run-time. [WaRa95] proposes to use a processor for prefetching in the context of virtual shared memory. Bandwidth Block Size Execution Time Block Size Execution Time and Bandwidth 4 Multithreading [WeGu89] is also used as a latency tolerating technique.
Reference: [Sano94] <author> B.J. Sano. </author> <title> Microparallel Processors. </title> <type> Ph.D. Thesis, </type> <institution> University of Southern Cali-fornia, </institution> <year> 1994. </year>
Reference-contexts: 1. Introduction The gap between processor speeds and DRAM speeds is increasing at an exponential rate [WuMc94]. Transistor sizes have been shrinking and die sizes have been growing steadily for many years <ref> [Sano94] </ref>. These trends have allowed processors to get faster [Denn74] and more complex. DRAM technology has also benefited from decreasing feature sizes. However, since DRAM technology is optimized for size and not speed, DRAM speeds have not kept pace with processor speeds.
Reference: [WaRa95] <author> I. Watson and A. Rawsthorne. </author> <title> Decoupled Pre-Fetching for distributed shared memory. </title> <booktitle> Proceedings of the 28th Annual Hawaii International Conference on System Sciences, </booktitle> <month> January </month> <year> 1995, </year> <pages> pp. 252-261. </pages>
Reference-contexts: It also requires the compiler to determine where to insert the prefetch instructions in order to cover a potential miss. [Mowr94] and [SaPa96] propose methods for varying prefetch distance at run-time. <ref> [WaRa95] </ref> proposes to use a processor for prefetching in the context of virtual shared memory. Bandwidth Block Size Execution Time Block Size Execution Time and Bandwidth 4 Multithreading [WeGu89] is also used as a latency tolerating technique.
Reference: [WeGu89] <author> W.D. Weber and A. Gupta. </author> <title> Exploring the benefits of multiple hardware contexts in a multiprocessor architecture: preliminary results. </title> <booktitle> Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <year> 1989, </year> <pages> pp. 273-280. </pages>
Reference-contexts: Bandwidth Block Size Execution Time Block Size Execution Time and Bandwidth 4 Multithreading <ref> [WeGu89] </ref> is also used as a latency tolerating technique. Multithreading tolerates latency at the expense of context-switching overhead and additional traffic because of cache pollution and possibly context saving.
Reference: [Wolf92] <author> M.E. Wolf. </author> <title> Improving Locality and Parallelism in Nested Loops. </title> <type> Ph.D. Thesis, </type> <institution> Stanford University, </institution> <year> 1992. </year>
Reference-contexts: However, [BuGK96] showed that memory traffic is also quickly becoming a performance limiting factor. A latency tolerating technique is needed that hides latency without placing a larger burden on the bandwidth available across the pins of the microprocessor chip. Techniques such as tiling <ref> [Wolf92] </ref> try to increase locality by making compile time transformations. These techniques are limited to scientific programs which exhibit simple access behavior. In our architecture we can use these techniques as well as others which would not be possible by complier analysis alone.
Reference: [WuMc94] <author> W.A. Wulf and S.A. McKee. </author> <title> Hitting the Memory Wall: Implications of the Obvious. Computer Architecture News, </title> <editor> v. </editor> <volume> 23, no. </volume> <month> 1 (December </month> <year> 1994), </year> <pages> pp. 20-24. </pages>
Reference-contexts: 1. Introduction The gap between processor speeds and DRAM speeds is increasing at an exponential rate <ref> [WuMc94] </ref>. Transistor sizes have been shrinking and die sizes have been growing steadily for many years [Sano94]. These trends have allowed processors to get faster [Denn74] and more complex. DRAM technology has also benefited from decreasing feature sizes.
References-found: 23

