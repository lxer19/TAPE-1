URL: http://www.cs.kuleuven.ac.be/~ml/PS/ML98-56.ps
Refering-URL: http://www.cs.wisc.edu/icml98/schedule.html
Root-URL: 
Email: fHendrik.Blockeel,Luc.DeRaedt,Jan.Ramong@cs.kuleuven.ac.be  
Title: Top-down induction of clustering trees  
Author: Hendrik Blockeel Luc De Raedt 
Date: Jan Ramon  
Address: Celestijnenlaan 200A, B-3001 Heverlee, Belgium  
Affiliation: Katholieke Universiteit Leuven, Department of Computer Science  
Abstract: An approach to clustering is presented that adapts the basic top-down induction of decision trees method towards clustering. To this aim, it employs the principles of instance based learning. The resulting methodology is implemented in the TIC (Top down Induction of Clustering trees) system for first order clustering. The TIC system employs the first order logical decision tree representation of the inductive logic programming system Tilde. Various experiments with TIC are presented, in both propositional and re lational domains.
Abstract-found: 1
Intro-found: 1
Reference: [ Bisson, 1992 ] <author> G. Bisson. </author> <title> Conceptual clustering in a first order logic representation. </title> <booktitle> In Proceedings of the 10th Eu-ropean Conference on Artificial Intelligence, </booktitle> <pages> pages 458-462. </pages> <publisher> John Wiley & Sons, </publisher> <year> 1992. </year> [ <editor> Blockeel and De Raedt, 1998 ] H. Blockeel and L. De Raedt. </editor> <title> Top-down induction of first order logical decision trees. </title> <journal> Artificial Intelligence, </journal> <note> 1998. To appear. </note>
Reference-contexts: For instance, the distance could be the Euclidean distance d 1 between the values of one or more numerical attributes, or it could be the distance d 2 as measured by a first order distance measure such as used in RIBL [ Emde and Wettschereck, 1996 ] or KBG <ref> [ Bisson, 1992 ] </ref> or [ Hutchinson, 1997 ] . Given the distance at the level of the examples, the principles of instance based learning can be used to compute the prototypes. <p> employ only propositional distance measures, we obtain first order descriptions of the clusters through the representation of first order logical decision trees. 2.4 PROBLEM-SPECIFICATION By now we are able to formally specify the clustering problem: Given 2 Using Plotkin's [1970] notion of -subsumption or the variants corresponding to structural matching <ref> [ Bisson, 1992; De Raedt et al., 1997 ] </ref> . * a set of examples E (each example is a set of tuples in a relational database or equivalently, a set of facts in Prolog), * a background theory B in the form of a Prolog program, * a distance measure <p> Several experiments were performed that illustrate the type of tasks TIC is useful for. As far as related work is concerned, our work is related to KBG <ref> [ Bisson, 1992 ] </ref> , which also performs first order clustering. In contrast to the current version of TIC, KBG does use a first order similarity measure, which could also be used within TIC. Furthermore, KBG is an agglomerative (bottom-up) clustering algorithm and TIC a divisive one (top-down).
Reference: [ Breiman et al., 1984 ] <author> L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, </address> <year> 1984. </year>
Reference-contexts: Depending on the examples and the distance measure employed one can distinguish two modes. In supervised learning (as in the classical top-down induction of decision trees paradigm), the distance measure only takes into account the class information of each example (see e.g. C4.5 [ Quinlan, 1993 ] , CART <ref> [ Breiman et al., 1984 ] </ref> ). Also, regression trees (SRT [ Kramer, 1996 ] , CART) should be considered supervised learning. In unsupervised learning, the examples may not be classified and the distance measure does not take into account any class information. <p> This reflects the principle that the inter-cluster distance should be as large as possible. If the prototype is simply the mean, then maximizing inter-cluster distances corresponds to minimizing intra-cluster distances, and splitting heuristics such as information gain [ Quinlan, 1993 ] or Gini index <ref> [ Breiman et al., 1984 ] </ref> can be seen as special cases of the above principle, as they minimize intra-cluster class diversity. In the regression context, minimizing intra-cluster variance (e.g. [ Kramer, 1996 ] ) is another instance of this principle. <p> If Q 0 &gt; Q then the tree is pruned. Such a strategy has been successfully followed in the context of classification and regression (e.g. CART <ref> [ Breiman et al., 1984 ] </ref> ) as well as clustering (e.g. [ Fisher, 1996 ] ). Fisher's method is more complex than ours in that for each individual variable a different subset of the original tree will be used for prediction.
Reference: [ Clark and Niblett, 1989 ] <author> P. Clark and T. Niblett. </author> <title> The CN2 algorithm. </title> <journal> Machine Learning, </journal> <volume> 3(4) </volume> <pages> 261-284, </pages> <year> 1989. </year>
Reference: [ De Raedt and Blockeel, 1997 ] <author> L. De Raedt and H. Bloc-keel. </author> <title> Using logical decision trees for clustering. </title> <booktitle> In Proceedings of the 7th International Workshop on Inductive Logic Programming, volume 1297 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 133-141. </pages> <publisher> Springer-Verlag, </publisher> <year> 1997. </year>
Reference-contexts: Using TIC we report on a number of experiments. These experiments demonstrate the power of top-down induction of clustering trees. More specifically, we show that TIC can be used for clustering, for regression, and for learning classifiers. This paper significantly expands on an earlier extended abstract <ref> [ De Raedt and Blockeel, 1997 ] </ref> in that TIC now contains a pruning method and also that this paper provides new experimental evidence. This paper is structured as follows. In Section 2 we discuss the representation of the data and the induced theories.
Reference: [ De Raedt and Dzeroski, 1994 ] <author> L. De Raedt and S. Dzeroski. </author> <title> First order jk-clausal theories are PAC-learnable. </title> <journal> Artificial Intelligence, </journal> <volume> 70 </volume> <pages> 375-392, </pages> <note> 1994. </note> [ <author> De Raedt and Van Laer, 1995 ] L. De Raedt and W. Van Laer. </author> <title> Inductive constraint logic. </title> <booktitle> In Proceedings of the 5th Workshop on Algorithmic Learning Theory, volume 997 of Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference: [ De Raedt et al., 1997 ] <author> L. De Raedt, P. Idestam-Almquist, and G. Sablon. </author> <title> -subsumption for structural matching. </title> <booktitle> In Proceedings of the 9th European Conference on Machine Learning, </booktitle> <pages> pages 73-84. </pages> <publisher> Springer-Verlag, </publisher> <year> 1997. </year>
Reference-contexts: employ only propositional distance measures, we obtain first order descriptions of the clusters through the representation of first order logical decision trees. 2.4 PROBLEM-SPECIFICATION By now we are able to formally specify the clustering problem: Given 2 Using Plotkin's [1970] notion of -subsumption or the variants corresponding to structural matching <ref> [ Bisson, 1992; De Raedt et al., 1997 ] </ref> . * a set of examples E (each example is a set of tuples in a relational database or equivalently, a set of facts in Prolog), * a background theory B in the form of a Prolog program, * a distance measure
Reference: [ De Raedt et al., 1998 ] <author> L. De Raedt, H. Blockeel, L. De-haspe, and W. Van Laer. </author> <title> Three companions for first order data mining. </title> <editor> In N. Lavrac and S. Dzeroski, editors, </editor> <title> Inductive Logic Programming for Knowledge Discovery in Databases, </title> <booktitle> Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1998. </year> <note> To appear. </note>
Reference: [ De Raedt, 1996 ] <author> L. De Raedt. </author> <title> Induction in logic. In R.S. </title> <editor> Michalski and Wnek J., editors, </editor> <booktitle> Proceedings of the 3rd International Workshop on Multistrategy Learning, </booktitle> <pages> pages 29-38, </pages> <year> 1996. </year>
Reference: [ Emde and Wettschereck, 1996 ] <author> W. Emde and D. Wettschereck. </author> <title> Relational instance-based learning. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Proceedings of the 13th International Conference on Machine Learning, </booktitle> <pages> pages 122-130. </pages> <publisher> Mor-gan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: For instance, the distance could be the Euclidean distance d 1 between the values of one or more numerical attributes, or it could be the distance d 2 as measured by a first order distance measure such as used in RIBL <ref> [ Emde and Wettschereck, 1996 ] </ref> or KBG [ Bisson, 1992 ] or [ Hutchinson, 1997 ] . Given the distance at the level of the examples, the principles of instance based learning can be used to compute the prototypes. <p> For KBG, these descriptions have to be derived in a separate step because the clustering process only produces the clusters (i.e. sets of examples) and not their description. The instance-based learner RIBL <ref> [ Emde and Wettschereck, 1996 ] </ref> uses an advanced first order distance metric that might be a good candidate for incorporation in TIC. While [ Fisher, 1993 ] first made the link between TDIDT and clustering, our work is inspired mainly by [ Langley, 1996 ] .
Reference: [ Emde, 1994 ] <author> W. Emde. </author> <title> Inductive learning of characteristic concept descriptions. </title> <editor> In S. Wrobel, editor, </editor> <booktitle> Proceedings of the 4th International Workshop on Inductive Logic Programming, volume 237 of GMD-Studien, </booktitle> <pages> pages 51-70, </pages> <address> Sankt Augustin, Germany, </address> <year> 1994. </year> <institution> Gesellschaft fur Mathematik und Datenverarbeitung MBH. </institution>
Reference-contexts: If the leaves are coherent with respect to classes, this method would yield relatively high classification accuracy with a minimum of class information available. This is quite similar in spirit to Emde's method for learning from few classified examples, implemented in the COLA system <ref> [ Emde, 1994 ] </ref> . A similar reasoning can be followed for regression, leading to "unsupervised regression"; again this may be useful in the case of partially missing information. We conclude that clustering can extend classification and regression towards unsupervised learning. <p> This experiment is similar in spirits to the ones performed with COLA <ref> [ Emde, 1994 ] </ref> . Table 4 shows the results.
Reference: [ Fisher and Langley, 1985 ] <author> D. Fisher and P. Langley. </author> <title> Approaches to conceptual clustering. </title> <booktitle> In Proceedings of the 9th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 691-697, </pages> <address> Los Altos, CA, 1985. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: classification trees and clustering trees has also been noted by Fisher, who points to the possibility of using TDIDT (or TDIDT heuristics) fl The authors are listed in alphabetical order. in the clustering context [ Fisher, 1993 ] and mentions a few clustering systems that work in a TDIDT-like fashion <ref> [ Fisher and Langley, 1985 ] </ref> . Following these views we study top-down induction of clustering trees. A clustering tree is a decision tree where the leaves do not contain classes and where each node as well as each leaf corresponds to a cluster.
Reference: [ Fisher, 1987 ] <author> D. H. Fisher. </author> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 139-172, </pages> <year> 1987. </year>
Reference-contexts: Such taxonomies are not only output by decision tree algorithms but typically also by clustering algorithms such as e.g. COBWEB <ref> [ Fisher, 1987 ] </ref> . Therefore, Langley views both clustering and concept-learning as instantiations of the same general technique, the induction of concept hierarchies. <p> In this case classes should be available for the evaluation of the clustering tree, though not during (unsupervised) learning. Such an evaluation is often done for clusters, see e.g. <ref> [ Fisher, 1987 ] </ref> . 4 TIC: TOP-DOWN INDUCTION OF CLUSTERING TREES A system for top-down induction of clustering trees called TIC has been implemented as a subsystem of the ILP system Tilde [ Blockeel and De Raedt, 1998 ] .
Reference: [ Fisher, 1993 ] <author> D. H. Fisher. </author> <title> Database management and analysis tools of machine induction. </title> <journal> Journal of Intelligent Information Systems, </journal> <volume> 2, </volume> <year> 1993. </year>
Reference-contexts: The similarity between classification trees and clustering trees has also been noted by Fisher, who points to the possibility of using TDIDT (or TDIDT heuristics) fl The authors are listed in alphabetical order. in the clustering context <ref> [ Fisher, 1993 ] </ref> and mentions a few clustering systems that work in a TDIDT-like fashion [ Fisher and Langley, 1985 ] . Following these views we study top-down induction of clustering trees. <p> The instance-based learner RIBL [ Emde and Wettschereck, 1996 ] uses an advanced first order distance metric that might be a good candidate for incorporation in TIC. While <ref> [ Fisher, 1993 ] </ref> first made the link between TDIDT and clustering, our work is inspired mainly by [ Langley, 1996 ] . From this point of view, our work is closely related to SRT [ Kramer, 1996 ] , who builds regression trees in a supervised manner.
Reference: [ Fisher, 1996 ] <author> D. H. Fisher. </author> <title> Iterative optimization and simplification of hierarchical clusterings. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 147-179, </pages> <year> 1996. </year>
Reference-contexts: If Q 0 &gt; Q then the tree is pruned. Such a strategy has been successfully followed in the context of classification and regression (e.g. CART [ Breiman et al., 1984 ] ) as well as clustering (e.g. <ref> [ Fisher, 1996 ] </ref> ). Fisher's method is more complex than ours in that for each individual variable a different subset of the original tree will be used for prediction. In the current implementation of Tilde validation set based pruning is available for all settings. <p> The high accuracies show that most attributes can be predicted very well, which means the clusters are very coherent. The mean accuracy of 81.6% does not differ significantly from the 83 2% reported in <ref> [ Fisher, 1996 ] </ref> . 5.5 EXPERIMENT 4: HANDLING MISSING INFORMATION It can be expected that clustering, making use of more attributes than just class attributes, is more robust with respect to missing values.
Reference: [ Hutchinson, 1997 ] <author> A. Hutchinson. </author> <title> Metrics on terms and clauses. </title> <booktitle> In Proceedings of the 9th European Conference on Machine Learning, Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 138-145. </pages> <publisher> Springer-Verlag, </publisher> <year> 1997. </year>
Reference-contexts: could be the Euclidean distance d 1 between the values of one or more numerical attributes, or it could be the distance d 2 as measured by a first order distance measure such as used in RIBL [ Emde and Wettschereck, 1996 ] or KBG [ Bisson, 1992 ] or <ref> [ Hutchinson, 1997 ] </ref> . Given the distance at the level of the examples, the principles of instance based learning can be used to compute the prototypes.
Reference: [ Kazakov et al., 1996 ] <author> D. Kazakov, L. Popelinsky, and O. Stepankova. </author> <note> ILP datasets page [http://www.gmd.de/ml-archive/- datasets/% ilp-res.html] , 1996. </note>
Reference-contexts: Originally mutagenicity was measured by a real number, but in most experiments with ILP systems this has been discretized into two values (positive and negative). The database is available at the ILP repository <ref> [ Kazakov et al., 1996 ] </ref> . Srinivasan et al. [1995] introduce four levels of background knowledge; the first 2 contain only structural information (atoms and bonds in the molecules), the other 2 contain higher level information (attributes describing the molecule as a whole and higher level submolecular structures).
Reference: [ Ketterlin et al., 1995 ] <author> A. Ketterlin, P. Gancarski, and J.J. Korczak. </author> <title> Conceptual clustering in structured databases : a practical approach. </title> <booktitle> In Proceedings of KDD-95, </booktitle> <year> 1995. </year>
Reference-contexts: Finally, we should also refer to a number of other approaches to first order clustering, which include Kluster [ Kietz and Morik, 1994 ] , [ Yoo and Fisher, 1991 ] , [ Thompson and Langley, 1991 ] and <ref> [ Ketterlin et al., 1995 ] </ref> . Future work on TIC includes extending the system so that it can employ first order distance measures, and investigating the limitations of this approach (which will require further experiments).
Reference: [ Kietz and Morik, 1994 ] <author> J.U. Kietz and K.. Morik. </author> <title> A polynomial approach to the constructive induction of structural knowledge. </title> <journal> Machine Learning, </journal> <volume> 14 </volume> <pages> 193-217, </pages> <year> 1994. </year>
Reference-contexts: TIC can be considered a generalization of SRT in that TIC can also build trees in an unsupervised manner, and can predict multiple values. Finally, we should also refer to a number of other approaches to first order clustering, which include Kluster <ref> [ Kietz and Morik, 1994 ] </ref> , [ Yoo and Fisher, 1991 ] , [ Thompson and Langley, 1991 ] and [ Ketterlin et al., 1995 ] .
Reference: [ Kramer, 1996 ] <author> S. Kramer. </author> <title> Structural regression trees. </title> <booktitle> In Proceedings of the 13th National Conference on Artificial Intelligence (AAAI-96), </booktitle> <year> 1996. </year>
Reference-contexts: In supervised learning (as in the classical top-down induction of decision trees paradigm), the distance measure only takes into account the class information of each example (see e.g. C4.5 [ Quinlan, 1993 ] , CART [ Breiman et al., 1984 ] ). Also, regression trees (SRT <ref> [ Kramer, 1996 ] </ref> , CART) should be considered supervised learning. In unsupervised learning, the examples may not be classified and the distance measure does not take into account any class information. Rather, all attributes or features of the examples are taken into account in the distance measure. <p> The Top-down Induction of Clustering trees approach is implemented in the TIC system. TIC is a first order clustering system as it does not employ the classical attribute value representation but that of first order logical decision trees as in SRT <ref> [ Kramer, 1996 ] </ref> and Tilde [ Blockeel and De Raedt, 1998 ] . So, the clusters corresponding to the tree will have first order definitions. On the other hand, in the current implementation of TIC we only employ propositional distance measures. <p> In the regression context, minimizing intra-cluster variance (e.g. <ref> [ Kramer, 1996 ] </ref> ) is another instance of this principle. Note that our distance-based approach has the advantage of being applicable to both numeric and symbolic data, and thus generalises over regression and classification. 4.2 STOPPING CRITERIA Stopping criteria are often based on significance tests. <p> While [ Fisher, 1993 ] first made the link between TDIDT and clustering, our work is inspired mainly by [ Langley, 1996 ] . From this point of view, our work is closely related to SRT <ref> [ Kramer, 1996 ] </ref> , who builds regression trees in a supervised manner. TIC can be considered a generalization of SRT in that TIC can also build trees in an unsupervised manner, and can predict multiple values.
Reference: [ Langley, 1996 ] <author> P. Langley. </author> <title> Elements of Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: 1 INTRODUCTION Decision trees are usually regarded as representing theories for classification. The leaves of the tree contain the classes and the branches from the root to a leaf contain sufficient conditions for classification. A different viewpoint is taken in Elements of Machine Learning <ref> [ Langley, 1996 ] </ref> . According to Langley, each node of a tree corresponds to a concept or a cluster, and the tree as a whole thus represents a kind of taxonomy or a hierarchy. <p> Throughout this paper we employ only propositional distance measures and the prototype functions that correspond to the instance averaging methods along the lines of <ref> [ Langley, 1996 ] </ref> . However, we stress that - in principle we could use any distance measure. <p> The instance-based learner RIBL [ Emde and Wettschereck, 1996 ] uses an advanced first order distance metric that might be a good candidate for incorporation in TIC. While [ Fisher, 1993 ] first made the link between TDIDT and clustering, our work is inspired mainly by <ref> [ Langley, 1996 ] </ref> . From this point of view, our work is closely related to SRT [ Kramer, 1996 ] , who builds regression trees in a supervised manner.
Reference: [ Merz and Murphy, 1996 ] <author> C.J. Merz and P.M. Mur-phy. </author> <title> UCI repository of machine learning databases [http://www.ics.uci.edu/~mlearn/mlrepository.html] , 1996. </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science. </institution>
Reference: [ Michalski and Chilausky, 1980 ] <author> R.S. Michalski and R.L. Chilausky. </author> <title> Learning by being told and learning from examples: an experimental comparaison of the two methods of knowledge acquisition in the context of developing an expert system for soybean disease diagnosis. </title> <journal> Policy analysis and information systems, </journal> <volume> 4, </volume> <year> 1980. </year>
Reference: [ Plotkin, 1970 ] <author> G. Plotkin. </author> <title> A note on inductive generalization. </title> <booktitle> In Machine Intelligence, </booktitle> <volume> volume 5, </volume> <pages> pages 153-163. </pages> <publisher> Edinburgh University Press, </publisher> <year> 1970. </year>
Reference: [ Quinlan, 1993 ] <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. Morgan Kaufmann series in machine learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Depending on the examples and the distance measure employed one can distinguish two modes. In supervised learning (as in the classical top-down induction of decision trees paradigm), the distance measure only takes into account the class information of each example (see e.g. C4.5 <ref> [ Quinlan, 1993 ] </ref> , CART [ Breiman et al., 1984 ] ). Also, regression trees (SRT [ Kramer, 1996 ] , CART) should be considered supervised learning. In unsupervised learning, the examples may not be classified and the distance measure does not take into account any class information. <p> The best test T is then the one that maximizes this distance. This reflects the principle that the inter-cluster distance should be as large as possible. If the prototype is simply the mean, then maximizing inter-cluster distances corresponds to minimizing intra-cluster distances, and splitting heuristics such as information gain <ref> [ Quinlan, 1993 ] </ref> or Gini index [ Breiman et al., 1984 ] can be seen as special cases of the above principle, as they minimize intra-cluster class diversity. In the regression context, minimizing intra-cluster variance (e.g. [ Kramer, 1996 ] ) is another instance of this principle.
Reference: [ Srinivasan et al., 1995 ] <author> A. Srinivasan, S.H. Muggleton, and R.D. King. </author> <title> Comparing the use of background knowledge by inductive logic programming systems. </title> <editor> In L. De Raedt, editor, </editor> <booktitle> Proceedings of the 5th International Workshop on Inductive Logic Programming, </booktitle> <year> 1995. </year>
Reference: [ Srinivasan et al., 1996 ] <author> A. Srinivasan, S.H. Muggleton, M.J.E. Sternberg, and R.D. King. </author> <title> Theories for muta-genicity: A study in first-order and feature-based induction. </title> <journal> Artificial Intelligence, </journal> <volume> 85, </volume> <year> 1996. </year>
Reference-contexts: For instance, examples for the well-known mutage-nesis problem <ref> [ Srinivasan et al., 1996 ] </ref> can be described by interpretations. Here, an interpretation is simply an enumeration of all the facts we know about one single molecule: its class, lumo and logp values, the atoms and bonds occurring in it, certain high-level structures. . . <p> It contains 3 classes of 50 examples each. There are 4 numerical attributes. * Mutagenesis: this database <ref> [ Srinivasan et al., 1996 ] </ref> contains descriptions of molecules for which the mutagenic activity has to be predicted. Originally mutagenicity was measured by a real number, but in most experiments with ILP systems this has been discretized into two values (positive and negative).
Reference: [ Thompson and Langley, 1991 ] <author> K. Thompson and P. Lan-gley. </author> <title> Concept formation in structured domains. </title> <editor> In D. Fisher, M. Pazzani, and P. Langley, editors, </editor> <title> Concept formation: knowledge and experience in unsupervised learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: Finally, we should also refer to a number of other approaches to first order clustering, which include Kluster [ Kietz and Morik, 1994 ] , [ Yoo and Fisher, 1991 ] , <ref> [ Thompson and Langley, 1991 ] </ref> and [ Ketterlin et al., 1995 ] . Future work on TIC includes extending the system so that it can employ first order distance measures, and investigating the limitations of this approach (which will require further experiments).
Reference: [ Yoo and Fisher, 1991 ] <author> J. Yoo and D. Fisher. </author> <title> Concept formation over explanations and problem-solving experience. </title> <booktitle> In Proceedings of the 12th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 630 - 636. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: TIC can be considered a generalization of SRT in that TIC can also build trees in an unsupervised manner, and can predict multiple values. Finally, we should also refer to a number of other approaches to first order clustering, which include Kluster [ Kietz and Morik, 1994 ] , <ref> [ Yoo and Fisher, 1991 ] </ref> , [ Thompson and Langley, 1991 ] and [ Ketterlin et al., 1995 ] . Future work on TIC includes extending the system so that it can employ first order distance measures, and investigating the limitations of this approach (which will require further experiments).
References-found: 28

