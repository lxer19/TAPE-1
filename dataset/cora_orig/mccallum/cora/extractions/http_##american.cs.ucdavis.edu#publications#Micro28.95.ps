URL: http://american.cs.ucdavis.edu/publications/Micro28.95.ps
Refering-URL: http://american.cs.ucdavis.edu/ArchLabPersonnel/Farrens/PubList.html
Root-URL: http://www.cs.ucdavis.edu
Email: (matthewj@cs.ucdavis.edu) (arp@tosca.colorado.edu)  
Title: d d A Modified Approach to Data Cache Management  
Author: Matthew Farrens Gary Tyson John Matthews Andrew R. Pleszkun 
Address: Riverside, CA Davis, CA 95616  (tyson@cs.ucr.edu) (farrens@cs.ucdavis.edu) Boulder, CO 80309-0425  
Affiliation: Department of Computer Science Computer Science Department Department of Electrical University of California, Riverside University of California, Davis and Computer Engineering  University of Colorado-Boulder  
Abstract: As processor performance continues to improve, more emphasis must be placed on the performance of the memory system. In this paper, a detailed characterization of data cache behavior for individual load instructions is given. We show that by selectively applying cache line allocation according the characteristics of individual load instructions, overall performance can be improved for both the data cache and the memory system. This approach can improve some aspects of memory performance by as much as 60 percent on existing executables. 
Abstract-found: 1
Intro-found: 1
Reference: [ASWR93] <author> S. G. Abraham, R. A. Sugumar, D. Windheiser, B. R. Rau and R. Gupta, </author> <title> ``Predictability of Load/Store Instruction Latencies'', </title> <booktitle> Proceedings of the 26th Annual International Symposium on Microarchitecture, </booktitle> <address> Austin, Texas (December 1-3, </address> <year> 1993), </year> <pages> pp. 139-152. </pages>
Reference-contexts: IBM uses a similar hardware approach [EKPP93] in which they associate previous miss behavior with a load instruction and use that information to do prefetching. Among the most intriguing software approaches to reducing the miss penalty is a study by Abraham, et. al. <ref> [ASWR93] </ref> in which they observe that a very small number of load instructions are responsible for causing a disproportionate percentage of cache misses. <p> One could potentially reduce the miss rate of the data cache by simply not caching those data references that lead to a poor miss rate. As Abraham, et el. <ref> [ASWR93] </ref> point out, a large percentage of the data misses are caused by a very small number of instructions. <p> In addition, if we do not cache the data associated with high-miss-rate instructions, memory bandwidth requirements could be reduced since these references would only request a single word from memory, instead of an entire cache line. Since the study by Abraham, et el. <ref> [ASWR93] </ref> did not look at an extensive set of benchmark programs, we began by performing experiments similar to theirs in which we measured the miss rate associated with individual load and store instructions for a more extensive set of programs. <p> Thus, a cache lookup for an item is unaffected by whether it is marked C/NA or not only the allocation on a miss is affected. We looked at both static (similar to <ref> [ASWR93] </ref>) and dynamic approaches to identifying and marking these C/NA instructions. 4.1. Static Method We began by modeling a simple strategy in which all load instructions that do not meet a threshold for cache hit rate are marked C/NA. <p> Furthermore, the 75% threshold also relates to the memory bandwidth requirements for a cache line replacement (32 bytes) and a 64-bit load reference (8 bytes), and is the same value settled on by <ref> [ASWR93] </ref>. 4.1.1. Hit Rate and Memory Bandwidth Utilization Table 3 shows the change in cache hit rate and required memory bandwidth after the poorest performing instructions were marked C/NA.
Reference: [CaGr94] <author> B. Calder and D. Grunwald, </author> <title> ``Fast and Accurate Instruction Fetch and Branch Prediction'', </title> <booktitle> Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <address> Chicago, Illinois (April 18-21, </address> <year> 1994), </year> <pages> pp. 2-11. </pages>
Reference-contexts: Our goal is to develop a scheme that will provide the same performance enhancement transparently. In order to select which items should be marked C/NA, we turn to the body of work on branch prediction strategies. There has been a great amount written about branch prediction strategies recently <ref> [CaGr94, FiFr92, PaS92, Smit81, YeP91, YeP92, YeP93] </ref>. d d Briefly, dynamic branch prediction strategies collect run-time information about branch behavior to predict whether a branch will be taken in the future. Typically, these strategies associate several bits of information with a branch instruction.
Reference: [CaPo] <author> D. Callahan and A. Porterfield, </author> <title> ``Data Cache Performance and Supercomputer Applications'', </title> <booktitle> Proceedings of Supercomputing '90, </booktitle> <pages> pp. 564-572. </pages>
Reference-contexts: A similar conclusion can be made when increasing the primary cache size much beyond 16K bytes. 2.2. Reducing Miss Penalty A number of studies have proposed techniques (either compiler-based or hardware-based) that reduce the miss penalty of the cache by performing some type of data prefetch <ref> [CaPo, ChBa95, KlLe91] </ref>. If data items are pre-fetched during idle data cache cycles, references to a pre-fetched item will find it already in the cache and thus will not cause a miss and the associated miss penalty will not be experienced.
Reference: [ChBa95] <author> T. Chen and J. Baer, </author> <title> ``Effective Hardware Based Data Prefetching for High-Performance Processors'', </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 44, no. </volume> <month> 5 (May </month> <year> 1995), </year> <pages> pp. 609-623. </pages>
Reference-contexts: A similar conclusion can be made when increasing the primary cache size much beyond 16K bytes. 2.2. Reducing Miss Penalty A number of studies have proposed techniques (either compiler-based or hardware-based) that reduce the miss penalty of the cache by performing some type of data prefetch <ref> [CaPo, ChBa95, KlLe91] </ref>. If data items are pre-fetched during idle data cache cycles, references to a pre-fetched item will find it already in the cache and thus will not cause a miss and the associated miss penalty will not be experienced. <p> An example of hardware-based prefetch-ing is the work by Chen and Baer <ref> [ChBa95] </ref>. In this paper the authors propose keeping a history of the strides of data references, and using that information to make predictions as to what should be prefetched.
Reference: [EKPP93] <author> P. G. Emma, J. W. Knight, J. H. Pomerene, T. R. Puzak and R. N. Rechtschaffen, </author> <title> ``Cache Miss Facility with Stored Sequences for Data Fetching'', </title> <type> U.S. Patent 5,233,702(Issued: </type> <month> August 3, </month> <year> 1993). </year>
Reference-contexts: An example of hardware-based prefetch-ing is the work by Chen and Baer [ChBa95]. In this paper the authors propose keeping a history of the strides of data references, and using that information to make predictions as to what should be prefetched. IBM uses a similar hardware approach <ref> [EKPP93] </ref> in which they associate previous miss behavior with a load instruction and use that information to do prefetching.
Reference: [FiFr92] <author> J. A. Fisher and S. M. Freudenberger, </author> <title> ``Predicting Conditional Branch Directions from Previous Runs of a Program'', </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Boston, MA (October 12-15, </address> <year> 1992), </year> <pages> pp. 85-95. </pages>
Reference-contexts: Our goal is to develop a scheme that will provide the same performance enhancement transparently. In order to select which items should be marked C/NA, we turn to the body of work on branch prediction strategies. There has been a great amount written about branch prediction strategies recently <ref> [CaGr94, FiFr92, PaS92, Smit81, YeP91, YeP92, YeP93] </ref>. d d Briefly, dynamic branch prediction strategies collect run-time information about branch behavior to predict whether a branch will be taken in the future. Typically, these strategies associate several bits of information with a branch instruction.
Reference: [Joup90] <author> N. Jouppi, </author> <title> ``Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers'', </title> <booktitle> Proceedings of the Seventeenth Annual International Symposium on Computer Architecture, </booktitle> <volume> vol. 18, no. </volume> <month> 2 (May </month> <year> 1990), </year> <pages> pp. 364-373. </pages>
Reference-contexts: We simulated a 32-byte line size, and both 8K-byte and 16K-byte caches, which were direct mapped, 2-way set associative, 4-way set associative, and direct-mapped with a victim cache <ref> [Joup90] </ref>. Table 1 presents a detailed breakdown of each benchmark analyzed, the input that was used, the total number of load data references and the hit rates for each of the cache configurations simulated.
Reference: [KlLe91] <author> A. C. Klaiber and H. M. Levy, </author> <title> ``An Architecture for Software-Controlled Data Prefetching'', </title> <booktitle> Proceedings of the Eighteenth Annual International Symposium on Computer Architecture, </booktitle> <address> Toronto, Canada (May 27-30, </address> <year> 1991), </year> <pages> pp. 43-53. </pages>
Reference-contexts: A similar conclusion can be made when increasing the primary cache size much beyond 16K bytes. 2.2. Reducing Miss Penalty A number of studies have proposed techniques (either compiler-based or hardware-based) that reduce the miss penalty of the cache by performing some type of data prefetch <ref> [CaPo, ChBa95, KlLe91] </ref>. If data items are pre-fetched during idle data cache cycles, references to a pre-fetched item will find it already in the cache and thus will not cause a miss and the associated miss penalty will not be experienced.
Reference: [MuQF91] <author> J. M. Mulder, N. T. Quach and M. J. Flynn, </author> <title> ``An Area Model for On-Chip Memories and its Application'', </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> vol. 26, no. </volume> <month> 2 (February </month> <year> 1991), </year> <pages> pp. 98-105. </pages>
Reference-contexts: Several studies have investigated the relationship between cache access time, cache size and cache associativity <ref> [MuQF91, WaRP92] </ref>. These studies carefully parameterized a hardware model of the components of the cache (such as data array, tag array, compare logic, bus delays, etc.) and found, for example, that going from a direct mapped to 2-way associative cache substantially increases the access time to the cache.
Reference: [PaS92] <author> S. Pan, K. So and J. T. Rahmeh, </author> <title> ``Improving the Accuracy of Dynamic Branch Prediction Using Branch Correlation'', </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Boston, MA (October 12-15, </address> <year> 1992), </year> <pages> pp. 76-84. </pages>
Reference-contexts: Our goal is to develop a scheme that will provide the same performance enhancement transparently. In order to select which items should be marked C/NA, we turn to the body of work on branch prediction strategies. There has been a great amount written about branch prediction strategies recently <ref> [CaGr94, FiFr92, PaS92, Smit81, YeP91, YeP92, YeP93] </ref>. d d Briefly, dynamic branch prediction strategies collect run-time information about branch behavior to predict whether a branch will be taken in the future. Typically, these strategies associate several bits of information with a branch instruction.
Reference: [Smit81] <author> J. E. Smith, </author> <title> ``A Study of Branch Prediction Strategies'', </title> <booktitle> Proceedings of the Eighth Annual International Symposium on Computer Architecture, </booktitle> <address> Minneapolis, Minnesota (May 1981), </address> <pages> pp. 135-148. </pages>
Reference-contexts: Our goal is to develop a scheme that will provide the same performance enhancement transparently. In order to select which items should be marked C/NA, we turn to the body of work on branch prediction strategies. There has been a great amount written about branch prediction strategies recently <ref> [CaGr94, FiFr92, PaS92, Smit81, YeP91, YeP92, YeP93] </ref>. d d Briefly, dynamic branch prediction strategies collect run-time information about branch behavior to predict whether a branch will be taken in the future. Typically, these strategies associate several bits of information with a branch instruction.
Reference: [SrWa94] <author> A. Srivastava and D. W. Wall, </author> <title> ``Atom: A system for building customized program analysis tools'', </title> <booktitle> Proceedings of the ACM SIGPLAN Notices 1994 Conference on Programming Languages and Implementations(June 1994), </booktitle> <pages> pp. 196-205. </pages>
Reference-contexts: Using the ATOM program trace facilities <ref> [SrWa94] </ref> and the SPEC92 suite of benchmarks, such statistics were relatively straight-forward to gather. Each program in the SPEC 92 suite was instrumented in order to track the data cache hit rate associated with each unique data address.
Reference: [WaRP92] <author> T. Wada, S. Rajan and S. A. Przybylski, </author> <title> ``An Analytical Access Time Model for On-Chip Cache Memories'', </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> vol. 27, no. </volume> <month> 8 (August </month> <year> 1992), </year> <pages> pp. 1147-1156. </pages>
Reference-contexts: Several studies have investigated the relationship between cache access time, cache size and cache associativity <ref> [MuQF91, WaRP92] </ref>. These studies carefully parameterized a hardware model of the components of the cache (such as data array, tag array, compare logic, bus delays, etc.) and found, for example, that going from a direct mapped to 2-way associative cache substantially increases the access time to the cache.
Reference: [YeP91] <author> T. Yeh and Y. Patt, </author> <title> ``Two-Level Adaptive Training Branch Prediction'', </title> <booktitle> Proceedings of the 24th Annual International Symposium on Microarchitecture, </booktitle> <address> Albuquerque, New Mexico (November 18-20, </address> <year> 1991), </year> <pages> pp. 51-61. </pages>
Reference-contexts: Our goal is to develop a scheme that will provide the same performance enhancement transparently. In order to select which items should be marked C/NA, we turn to the body of work on branch prediction strategies. There has been a great amount written about branch prediction strategies recently <ref> [CaGr94, FiFr92, PaS92, Smit81, YeP91, YeP92, YeP93] </ref>. d d Briefly, dynamic branch prediction strategies collect run-time information about branch behavior to predict whether a branch will be taken in the future. Typically, these strategies associate several bits of information with a branch instruction.
Reference: [YeP92] <author> T. Yeh and Y. Patt, </author> <title> ``Alternative Implementations of Two-Level Adaptive Training Branch Prediction'', </title> <booktitle> Proceedings of the Nineteenth Annual International Symposium on Computer Architecture, </booktitle> <address> Queensland, Australia (May 19-21, </address> <year> 1992), </year> <pages> pp. 124-134. </pages>
Reference-contexts: Our goal is to develop a scheme that will provide the same performance enhancement transparently. In order to select which items should be marked C/NA, we turn to the body of work on branch prediction strategies. There has been a great amount written about branch prediction strategies recently <ref> [CaGr94, FiFr92, PaS92, Smit81, YeP91, YeP92, YeP93] </ref>. d d Briefly, dynamic branch prediction strategies collect run-time information about branch behavior to predict whether a branch will be taken in the future. Typically, these strategies associate several bits of information with a branch instruction.
Reference: [YeP93] <author> T. Yeh and Y. Patt, </author> <title> ``A Comparison of Dynamic Branch Predictors that use Two Levels of Branch History'', </title> <booktitle> Proceedings of the Twentieth Annual International Symposium on Computer Architecture, </booktitle> <address> San Diego, CA (May 16-19, </address> <year> 1993), </year> <pages> pp. 257-266. </pages>
Reference-contexts: Our goal is to develop a scheme that will provide the same performance enhancement transparently. In order to select which items should be marked C/NA, we turn to the body of work on branch prediction strategies. There has been a great amount written about branch prediction strategies recently <ref> [CaGr94, FiFr92, PaS92, Smit81, YeP91, YeP92, YeP93] </ref>. d d Briefly, dynamic branch prediction strategies collect run-time information about branch behavior to predict whether a branch will be taken in the future. Typically, these strategies associate several bits of information with a branch instruction.
References-found: 16

