URL: http://www.cs.umn.edu/Users/dept/users/kumar/web-wits.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Title: Web Page Categorization and Feature Selection Using Association Rule and Principal Component Clustering  
Author: Jerome Moore, Eui-Hong (Sam) Han, Daniel Boley, Maria Gini, Robert Gross, Kyle Hastings, George Karypis, Vipin Kumar, and Bamshad Mobasher 
Address: Minneapolis, MN 55455  
Affiliation: Department of Computer Science and Engineering Army HPC Research Center University of Minnesota,  
Abstract: Clustering techniques have been used by many intelligent software agents in order to retrieve, filter, and categorize documents available on the World Wide Web. Clustering is also useful in extracting salient features of related web documents to automatically formulate queries and search for other similar documents on the Web. Traditional clustering algorithms either use a priori knowledge of document structures to define a distance or similarity among these documents, or use probabilistic techniques such as Bayesian classification. Many of these traditional algorithms, however, falter when the dimensionality of the feature space becomes high relative to the size of the document space. In this paper, we introduce two new clustering algorithms that can effectively cluster documents, even in the presence of a very high dimensional feature space. These clustering techniques. which are based on generalizations of graph partitioning, do not require pre-specified ad hoc distance functions, and are capable of automatically discovering document similarities or associations. We conduct several experiments on real Web data using various feature selection heuristics, and compare our clustering schemes to standard distance-based techniques, such as hierarchical agglomeration clustering, and Bayesian classification methods, AutoClass. fl This work was supported in part by NSF ASC-9634719 & CCR-9405380, by Army Research Office contract DA/DAAH04-95-1-0538, by Army High Performance Computing Research Center cooperative agreement number DAAH04-95-2-0003/contract number DAAH04-95-C-0008, the content of which does not necessarily reflect the position or the policy of the government, and no official endorsement should be inferred. Additional support was provided by the IBM Partnership Award, and by the IBM SUR equipment grant. Access to computing facilities was provided by AHPCRC, Minnesota Supercomputer Institute. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A. Verkamo. </author> <title> Fast discovery of association rules. </title> <editor> In U. Fayyad, G. Piatetsky-Shapiro, P. Smith, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 307-328. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: This method first finds set of items that occur frequently together in transactions using association rule discovery methods <ref> [1] </ref>. These frequent item sets are then used to group items into hypergraph edges, and a hypergraph partitioning algorithm [13] is used to find the item clusters. The similarity among items is captured implicitly by the frequent item sets. <p> We are essentially minimizing the relations that are violated by partitioning the documents into different clusters. Similarly, this method can be applied to word clustering. In this setting, each word would correspond to an item and each document would correspond to a transaction. This method uses the Apriori algorithm <ref> [1] </ref> which has been shown to be very efficient in finding frequent item sets and HMETIS [13] which can partition very large hypergraphs (of size &gt; 100K nodes) in minutes on personal computers.
Reference: [2] <author> M. Balabanovic, Y. Shoham, and Y. Yun. </author> <title> An adaptive agent for automated Web browsing. Journal of Visual Communication and Image Representation, </title> <type> 6(4), </type> <year> 1995. </year> <note> http://www-diglib.stanford.edu/cgi-bin/WP/get/SIDL-WP-1995-0023. </note>
Reference-contexts: Syskill & Webert [17] represents an HTML page with a Boolean feature vector, and then uses naive Bayesian classification to find web pages that are similar, but for only a given single user profile. Also, Balabanovic <ref> [2] </ref> presents a system that uses a single well-defined profile to find similar web documents for a user. Candidate web pages are located using best-first search, comparing their word vectors against a user profile vector, and returning the highest -scoring pages.
Reference: [3] <author> C. Berge. </author> <title> Graphs and Hypergraphs. </title> <publisher> American Elsevier, </publisher> <year> 1976. </year>
Reference-contexts: The association rule discovery algorithm is used to find sets of documents with many features in common (frequent item sets). Each frequent item sets must satisfy a user-specified minimum support criteria which specifies a threshold on the minimum number of features common among documents in the set. A hypergraph <ref> [3] </ref> H = (V; E) is formed with vertices V consisting of the documents and hyperedges E representing the frequent item sets. Hyperedges are edges that can connect more than 2 vertices.
Reference: [4] <author> A. Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig. </author> <title> Syntactic clustering of the web. </title> <booktitle> In Proc. of 6th International World Wide Web Conference, </booktitle> <year> 1997. </year>
Reference-contexts: in Figure 2, "labor-related" clusters 8, 9, and 10, correspond to the subcategories General Labor, Occupational Safety, and Affirmative Action, respectively. 4 Related Work A number of Web agents use various information retrieval techniques [9] and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize these documents <ref> [5, 4, 16, 23, 22] </ref>. For example, HyPursuit [22] uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents and structure an information space.
Reference: [5] <author> C. Chang and C. Hsu. </author> <title> Customizable multi-engine search tool with clustering. </title> <booktitle> In Proc. of 6th International World Wide Web Conference, </booktitle> <year> 1997. </year>
Reference-contexts: in Figure 2, "labor-related" clusters 8, 9, and 10, correspond to the subcategories General Labor, Occupational Safety, and Affirmative Action, respectively. 4 Related Work A number of Web agents use various information retrieval techniques [9] and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize these documents <ref> [5, 4, 16, 23, 22] </ref>. For example, HyPursuit [22] uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents and structure an information space.
Reference: [6] <author> P. Cheeseman and J. Stutz. </author> <title> Baysian classification (autoclass): Theory and results. </title> <editor> In U. Fayyad, G. Piatetsky-Shapiro, P. Smith, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 153-180. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: In a high dimensional space, the cluster means will do a poor job at separating documents. We have found that hierarchical agglomeration clustering (HAC) [7], based on distances between sample cluster means, does a poor job on our examples. Similarly, probabilistic methods such as Bayesian classification used in AutoClass <ref> [6, 21] </ref> do not perform well when the size of the feature space is much larger than the size of the sample set or may depend on the independence of the underlying features. Web documents suffer from both high dimensionality and high correlation among the feature values.
Reference: [7] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and scene analysis. </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: Distance-based schemes generally require the calculation of the mean of document clusters, which are often chosen initially at random. In a high dimensional space, the cluster means will do a poor job at separating documents. We have found that hierarchical agglomeration clustering (HAC) <ref> [7] </ref>, based on distances between sample cluster means, does a poor job on our examples. <p> This is repeated as many times as desired, resulting in a tree-like hierarchy. The leaves of the tree are document clusters, each with a computed mean and principal direction. We use a scatter value, measuring the average distance from the documents in a cluster to the mean <ref> [7] </ref>, to determine the next cluster to split at each stage. The definition of the hyperplane is based on principal component analysis, similar to the Hotelling or Karhunen-Loeve Transformation [7]. We compute the principal direction as the leading eigenvector of the sample covariance matrix. <p> We use a scatter value, measuring the average distance from the documents in a cluster to the mean <ref> [7] </ref>, to determine the next cluster to split at each stage. The definition of the hyperplane is based on principal component analysis, similar to the Hotelling or Karhunen-Loeve Transformation [7]. We compute the principal direction as the leading eigenvector of the sample covariance matrix.
Reference: [8] <author> W. B. Frakes. </author> <title> Stemming algorithms. </title> <editor> In W. B. Frakes and R. Baeza-Yates, editors, </editor> <booktitle> Information Retrieval Data Structures and Algorithms, </booktitle> <pages> pages 131-160. </pages> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: This ensures a stable data sample since some pages are fairly dynamic in content. Table 1: Setup of experiments. The word lists from all documents were filtered with a stop-list and "stemmed" using Porter's suffix-stripping algorithm [18] as implemented by <ref> [8] </ref>. We derived seven experiments and clustered the documents using the four algorithms described earlier. Our objective is to reduce the dimensionality of the clustering problem and retain the important features of the documents.
Reference: [9] <author> W. B. Frakes and R. Baeza-Yates. </author> <title> Information Retrieval Data Structures and Algorithms. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: We show that partitioning clustering methods perform better than traditional distance based clustering. 2 Clustering Methods Most of the existing approaches to document clustering are based on either probabilistic methods, or distance and similarity measures (see <ref> [9] </ref>). Distance-based methods such as k-means analysis, hierarchical clustering [12] and nearest-neighbor clustering [15] use a selected set of words (features) appearing in different documents as the dimensions. Each such feature vector, representing a document, can be viewed as a point in this multi-dimensional space. <p> For example, in Figure 2, "labor-related" clusters 8, 9, and 10, correspond to the subcategories General Labor, Occupational Safety, and Affirmative Action, respectively. 4 Related Work A number of Web agents use various information retrieval techniques <ref> [9] </ref> and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize these documents [5, 4, 16, 23, 22]. For example, HyPursuit [22] uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents and structure an information space.
Reference: [10] <author> G. Golub and C. </author> <title> Van Loan Matrix Computations. </title> <publisher> Johns Hopkins, </publisher> <year> 1996. </year>
Reference-contexts: The definition of the hyperplane is based on principal component analysis, similar to the Hotelling or Karhunen-Loeve Transformation [7]. We compute the principal direction as the leading eigenvector of the sample covariance matrix. This is the most expensive part, for which we use a fast Lanczos-based singular value solver <ref> [10] </ref>. 3 Experimental Results 3.1 Experimental Setup For the experiments we present here we selected 98 web pages in four broad categories: business and finance, electronic communication and networking, labor, and manufacturing. These pages were downloaded, labeled, and archived.
Reference: [11] <author> E. Han, G. Karypis, V. Kumar, and B. Mobasher. </author> <title> Clustering based on association rule hy--pergraphs. </title> <booktitle> In Workshop on Research Issues on Data Mining and Knowledge Discovery, </booktitle> <pages> pages 9-13, </pages> <address> Tucson, Arizona, </address> <year> 1997. </year>
Reference-contexts: We have found AutoClass has performed poorly on our examples. Our proposed clustering algorithms, described below, are designed to efficiently handle very high dimensional spaces, and do not depend on the use of ad hoc distance or similarity metrics. 2.1 Association Rule Hypergraph Partitioning (ARHP) In <ref> [11] </ref>, a new method was proposed for clustering related items in transaction-based databases, such as supermarket bar code data, using association rules and hypergraph partitioning. This method first finds set of items that occur frequently together in transactions using association rule discovery methods [1].
Reference: [12] <author> A. Jain and R. C. Dubes. </author> <title> Algorithms for Clustering Data. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: We show that partitioning clustering methods perform better than traditional distance based clustering. 2 Clustering Methods Most of the existing approaches to document clustering are based on either probabilistic methods, or distance and similarity measures (see [9]). Distance-based methods such as k-means analysis, hierarchical clustering <ref> [12] </ref> and nearest-neighbor clustering [15] use a selected set of words (features) appearing in different documents as the dimensions. Each such feature vector, representing a document, can be viewed as a point in this multi-dimensional space.
Reference: [13] <author> G. Karypis, R. Aggarwal, V. Kumar, and S. Shekhar. </author> <title> Multilevel hypergraph partitioning: Application in VLSI domain. </title> <booktitle> In Proceedings ACM/IEEE Design Automation Conference, </booktitle> <year> 1997. </year>
Reference-contexts: This method first finds set of items that occur frequently together in transactions using association rule discovery methods [1]. These frequent item sets are then used to group items into hypergraph edges, and a hypergraph partitioning algorithm <ref> [13] </ref> is used to find the item clusters. The similarity among items is captured implicitly by the frequent item sets. In document clustering, each document corresponds to an item and each possible feature corresponds to a transaction. <p> Similarly, this method can be applied to word clustering. In this setting, each word would correspond to an item and each document would correspond to a transaction. This method uses the Apriori algorithm [1] which has been shown to be very efficient in finding frequent item sets and HMETIS <ref> [13] </ref> which can partition very large hypergraphs (of size &gt; 100K nodes) in minutes on personal computers.
Reference: [14] <author> T. Kohonen. </author> <title> Self-Organization and Associated Memory. </title> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: A TFIDF scheme [20] is used to calculate the word weights, normalized for document length. The system needs to keep a large dictionary and is limited to one user. The Kohonen Self-Organizing Feature Map <ref> [14] </ref> is a neural network based scheme that projects high dimensional input data into a feature map of a smaller dimension such that the proximity relationships among input data are preserved.
Reference: [15] <author> S. Lu and K. Fu. </author> <title> A sentence-to-sentence clustering procedure for pattern analysis. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 8 </volume> <pages> 381-389, </pages> <year> 1978. </year>
Reference-contexts: We show that partitioning clustering methods perform better than traditional distance based clustering. 2 Clustering Methods Most of the existing approaches to document clustering are based on either probabilistic methods, or distance and similarity measures (see [9]). Distance-based methods such as k-means analysis, hierarchical clustering [12] and nearest-neighbor clustering <ref> [15] </ref> use a selected set of words (features) appearing in different documents as the dimensions. Each such feature vector, representing a document, can be viewed as a point in this multi-dimensional space. There are a number of problems with clustering in a multi-dimensional space using traditional distance- or probability-based methods.
Reference: [16] <author> Y. S. Maarek and I. B. Shaul. </author> <title> Automatically organizing bookmarks per content. </title> <booktitle> In Proc. of 5th International World Wide Web Conference, </booktitle> <year> 1996. </year>
Reference-contexts: in Figure 2, "labor-related" clusters 8, 9, and 10, correspond to the subcategories General Labor, Occupational Safety, and Affirmative Action, respectively. 4 Related Work A number of Web agents use various information retrieval techniques [9] and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize these documents <ref> [5, 4, 16, 23, 22] </ref>. For example, HyPursuit [22] uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents and structure an information space.
Reference: [17] <author> M. Pazzani, J. Muramatsu, and D. Billsus. Syskill & Webert: </author> <title> Identifying interesting Web sites. </title> <booktitle> In National Conference on Artificial Intelligence, </booktitle> <pages> pages 54-61, </pages> <month> Aug. </month> <year> 1996. </year> <note> http://www.ics.uci.edu/ pazzani/RTF/AAAI.html. </note>
Reference-contexts: Pattern recognition methods and word clustering using the Hartigan's K-means partitional clustering algorithm are used in [23] to discover salient HTML document features (words) that can be used in finding similar HTML documents on the Web. Syskill & Webert <ref> [17] </ref> represents an HTML page with a Boolean feature vector, and then uses naive Bayesian classification to find web pages that are similar, but for only a given single user profile.
Reference: [18] <author> M. F. Porter. </author> <title> An algorithm for suffix stripping. </title> <booktitle> Program, </booktitle> <volume> 14(3) </volume> <pages> 130-137, </pages> <year> 1980. </year>
Reference-contexts: This ensures a stable data sample since some pages are fairly dynamic in content. Table 1: Setup of experiments. The word lists from all documents were filtered with a stop-list and "stemmed" using Porter's suffix-stripping algorithm <ref> [18] </ref> as implemented by [8]. We derived seven experiments and clustered the documents using the four algorithms described earlier. Our objective is to reduce the dimensionality of the clustering problem and retain the important features of the documents.
Reference: [19] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: The seven experiments are distinguished by further selection rules, as shown in Table 1. 3.2 Evaluation of Clustering Results Validating clustering algorithms and comparing performance of different algorithms is complex because it is difficult to find an objective measure of quality of clusters. We decided to use entropy <ref> [19] </ref> as a measure of goodness of the clusters. When a cluster contains documents from one class only, the entropy value is 0.0 for the cluster and when a cluster contains documents from many different classes, then entropy of the cluster is higher.
Reference: [20] <author> G. Salton and M. J. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, </publisher> <year> 1983. </year>
Reference-contexts: First, it is not trivial to define a distance measure in this space. Feature vectors must be scaled to avoid skewing the result by different document lengths or possibly by how common a word is across many documents. Techniques such as TFIDF <ref> [20] </ref> have been proposed precisely to deal with some of these problems. Second, the number of different words in all the documents can be very large. Distance-based schemes generally require the calculation of the mean of document clusters, which are often chosen initially at random. <p> Also, Balabanovic [2] presents a system that uses a single well-defined profile to find similar web documents for a user. Candidate web pages are located using best-first search, comparing their word vectors against a user profile vector, and returning the highest -scoring pages. A TFIDF scheme <ref> [20] </ref> is used to calculate the word weights, normalized for document length. The system needs to keep a large dictionary and is limited to one user.
Reference: [21] <author> D. Titterington, A. Smith, and U. Makov. </author> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley & Sons, </publisher> <year> 1985. </year>
Reference-contexts: In a high dimensional space, the cluster means will do a poor job at separating documents. We have found that hierarchical agglomeration clustering (HAC) [7], based on distances between sample cluster means, does a poor job on our examples. Similarly, probabilistic methods such as Bayesian classification used in AutoClass <ref> [6, 21] </ref> do not perform well when the size of the feature space is much larger than the size of the sample set or may depend on the independence of the underlying features. Web documents suffer from both high dimensionality and high correlation among the feature values.
Reference: [22] <author> R. Weiss, B. Velez, M. A. Sheldon, C. Nemprempre, P. Szilagyi, A. Duda, and D. K. Gifford. Hypursuit: </author> <title> A hierarchical network search engine that exploits content-link hypertext clustering. </title> <booktitle> In Seventh ACM Conference on Hypertext, </booktitle> <month> Mar. </month> <year> 1996. </year> <note> http://paris.lcs.mit.edu/ rweiss/. </note>
Reference-contexts: in Figure 2, "labor-related" clusters 8, 9, and 10, correspond to the subcategories General Labor, Occupational Safety, and Affirmative Action, respectively. 4 Related Work A number of Web agents use various information retrieval techniques [9] and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize these documents <ref> [5, 4, 16, 23, 22] </ref>. For example, HyPursuit [22] uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents and structure an information space. <p> For example, HyPursuit <ref> [22] </ref> uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents and structure an information space.
Reference: [23] <author> M. R. Wulfekuhler and W. F. Punch. </author> <title> Finding salient features for personal Web page categories. </title> <booktitle> In 6th International World Wide Web Conference, </booktitle> <month> Apr. </month> <year> 1997. </year> <note> http://proceedings.www6conf.org/HyperNews/get/PAPER118.html. </note>
Reference-contexts: in Figure 2, "labor-related" clusters 8, 9, and 10, correspond to the subcategories General Labor, Occupational Safety, and Affirmative Action, respectively. 4 Related Work A number of Web agents use various information retrieval techniques [9] and characteristics of open hypertext Web documents to automatically retrieve, filter, and categorize these documents <ref> [5, 4, 16, 23, 22] </ref>. For example, HyPursuit [22] uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents and structure an information space. <p> For example, HyPursuit [22] uses semantic information embedded in link structures as well as document content to create cluster hierarchies of hypertext documents and structure an information space. Pattern recognition methods and word clustering using the Hartigan's K-means partitional clustering algorithm are used in <ref> [23] </ref> to discover salient HTML document features (words) that can be used in finding similar HTML documents on the Web.
References-found: 23

