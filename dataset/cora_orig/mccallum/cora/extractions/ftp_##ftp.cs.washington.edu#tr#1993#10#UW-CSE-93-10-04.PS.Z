URL: ftp://ftp.cs.washington.edu/tr/1993/10/UW-CSE-93-10-04.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Email: radhika@cs.washington.edu eggers@cs.washington.edu  
Title: Impact of Sharing-Based Thread Placement on Multithreaded Architectures  
Author: Radhika Thekkath and Susan J. Eggers 
Address: Seattle, WA 98195  
Affiliation: Dept. of Computer Science and Eng.,FR-35 University of Washington  
Abstract: Multithreaded architectures context switch to another instruction stream to hide the latency of memory operations. Although the technique improves processor utilization, it can increase cache interference and degrade overall performance. One technique to reduce the interconnect traffic is to co-locate on the same processor threads that share data. The multi-thread sharing in the cache should reduce compulsory and invalidation misses, benefiting execution time. To test this hypothesis, we compared a variety of thread placement algorithms via trace-driven simulation of fourteen coarse- and medium-grain parallel applications on several multithreaded architectures. Our results contradict the hypothesis. Rather than decreasing, compulsory and invalidation misses remained fairly constant across all placement algorithms, for all processor configurations, even with an infinite cache. That is, sharing-based placement had no (positive) effect on execution time. Instead, load balancing was the critical factor that affected performance. Our result is explained by the insignificant amount of interconnect traffic due to shared data, relative to the total number of memory references, and the sequentiality and uniformity of thread data sharing.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal. </author> <title> Performance tradeoffs in multithreaded processors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(5) </volume> <pages> 525-539, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: In a few rare situations, e.g., Patch with sixteen processors and LOAD-BAL, we observed thrashing when two co-located threads frequently conflicted for the same cache block. This phenomenon has also been reported by Agarwal <ref> [1] </ref>. In our case the thrashing processor had an order of magnitude more inter-thread conflict misses than other processors, and therefore took longer to complete execution. <p> They measured processor efficiency by varying the number of contexts and found substantial improvement. Agarwal presents an analytical performance model that incorporates network traffic, cache interference, context switching overhead and the number of hardware contexts <ref> [1] </ref>. The paper also has a cache model that takes into account the interference in the cache due to multithreading. The paper's main conclusion vindicated multithreading for a wide range of architectural parameters, provided there existed sufficient network bandwidth.
Reference: [2] <author> A. Agarwal and A. Gupta. </author> <title> Memory-reference characteristics of multiprocessor applications under MACH. </title> <booktitle> Proceedings of the ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 215-225, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: 1 Introduction There are two memory system-related reasons for sublinear speedup in multiprocessors. First, data sharing among threads scheduled on different processors can lead to excessive data movement and higher network traffic <ref> [2, 9] </ref>. Second, large-sized networks have long memory access latencies. Together, they can cause considerable performance degradation. Multithreaded architectures address the second problem of long latencies, by context switching to another thread, and executing useful instructions while waiting for a memory access to complete [4, 11, 13, 19].
Reference: [3] <author> A. Agarwal, B-H. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> APRIL: </month> <title> A processor architecture for multiprocessing. </title> <booktitle> Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: We assume a multipath network and do not explicitly model network contention. Instead, we use a latency value of 50 cycles, approximating the average memory latency of a moderately-loaded Alewife-style multiprocessor <ref> [3] </ref>. We vary the number of processors and hardware contexts to study the impact, if any, on caches that are stressed to different degrees.
Reference: [4] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith. </author> <title> The Tera computer system. </title> <booktitle> International Conference on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Second, large-sized networks have long memory access latencies. Together, they can cause considerable performance degradation. Multithreaded architectures address the second problem of long latencies, by context switching to another thread, and executing useful instructions while waiting for a memory access to complete <ref> [4, 11, 13, 19] </ref>. This decreases processor idle time, i.e., improves processor utilization. However, frequent context switching can exacerbate the first problem by increasing inter-thread conflict misses from the combined working sets of multiple threads.
Reference: [5] <author> J. K. Bennett, J. B. Carter, and W. Zwaenepoel. </author> <title> Adaptive software cache management for distributed shared memory architectures. </title> <booktitle> Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 125-134, </pages> <month> May </month> <year> 1990. </year>
Reference: [6] <author> B. N. Bershad, E. D. Lazowska, and H. M. Levy. </author> <title> PRESTO: A system for object-oriented parallel programming. </title> <journal> Software: Practice and Experience, </journal> <volume> 18(8) </volume> <pages> 713-732, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: subsections discuss in greater detail the application suite and its measured program characteristics, and the simulation environment. 3.1 The Application Suite We analyzed two types of explicitly parallel workloads: coarse-grain programs that include some of the SPLASH benchmarks [18], and medium-grain applications that ran under the Presto parallel programming environment <ref> [6, 23] </ref>. We use the length and number of threads in an application as a measure of its granularity. Coarse-grain programs have fewer, but longer threads, 6.4 million instructions on the average, but as high as 100 million instructions (Table 1).
Reference: [7] <author> D. Chaiken, J. Kubiatowicz, and A. Agarwal. </author> <title> LimitLESS directories: A scalable cache coherence scheme. </title> <booktitle> Proceedings of ASPLOS IV, </booktitle> <pages> pages 224-234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The simulator simulates a multiprocessor system in which processors are connected by a simple multipath interconnection network and cache coherency is maintained with a distributed, directory-based cache coherency protocol <ref> [7] </ref>. The simulator comprises three modules: processor, cache and the interconnection network. Each processor models multiple hardware contexts and a round-robin context switch policy. A context switch takes 6 cycles, the time to drain the execution pipeline.
Reference: [8] <author> S. Devadas and A. </author> <title> Newton. Topological optimization of multiple level array logic. </title> <journal> IEEE Transactions on Computer-Aided Design, </journal> <month> November </month> <year> 1987. </year>
Reference-contexts: Threads from the medium-grain suite are shorter, on average 0.8 million instructions each, and more numerous. The programs span a wide range of application domains. LocusRoute, a commercial quality VLSI standard cell router, Pverify [14], which compares boolean circuits for functional equivalence and Topopt <ref> [8] </ref>, which performs 4 Number Mean Thread Application Applications of Length (millions Domain Threads of instructions) LocusRoute 12 n.a. CAD Water 12 4.9 Scientific MP3D 12 9.1 Scientific Cholesky 14 5.8 Math Barnes-Hut 12 5.7 Scientific Pverify 12 101.0 CAD Topopt 9 n.a.
Reference: [9] <author> S. J. Eggers and R. H. Katz. </author> <title> The effect of sharing on the cache and bus performance of parallel programs. </title> <booktitle> Third International Conference on ASPLOS, </booktitle> <pages> pages 257-270, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: 1 Introduction There are two memory system-related reasons for sublinear speedup in multiprocessors. First, data sharing among threads scheduled on different processors can lead to excessive data movement and higher network traffic <ref> [2, 9] </ref>. Second, large-sized networks have long memory access latencies. Together, they can cause considerable performance degradation. Multithreaded architectures address the second problem of long latencies, by context switching to another thread, and executing useful instructions while waiting for a memory access to complete [4, 11, 13, 19]. <p> With an infinite cache, capacity and conflict misses are eliminated and some conflict misses become invalidation misses <ref> [9] </ref>; thus, coherency operations may dominate interconnect traffic. We ran a set of experiments to study this issue. We compared the load balanced version (LOAD-BAL) with the best (static) sharing-based algorithm for each application and the (dynamic) coherency traffic algorithm.
Reference: [10] <author> S. J. Eggers, D. R. Keppel, E. J. Koldinger, and H. M. Levy. </author> <title> Techniques for efficient inline tracing on a shared-memory multiprocessor. </title> <booktitle> ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 37-46, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The simulator modeled a shared-memory multiprocessor whose processors have multiple hardware contexts. Program traces (both data and instruction) from fourteen explicitly parallel applications were generated using the MPtrace <ref> [10] </ref> parallel tracing toolon a Sequent Symmetry [20]. Certain characteristics of the applications were extracted from the trace files and fed to the placement algorithms, which in turn produced maps associating threads with processors. Both maps and program traces were input to the simulator.
Reference: [11] <author> R. H. Halstead and T. Fujita. MASA: </author> <title> A multithreaded processor architecture for parallel symbolic computing. </title> <booktitle> Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 443-451, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Second, large-sized networks have long memory access latencies. Together, they can cause considerable performance degradation. Multithreaded architectures address the second problem of long latencies, by context switching to another thread, and executing useful instructions while waiting for a memory access to complete <ref> [4, 11, 13, 19] </ref>. This decreases processor idle time, i.e., improves processor utilization. However, frequent context switching can exacerbate the first problem by increasing inter-thread conflict misses from the combined working sets of multiple threads.
Reference: [12] <author> T.E. Jeremiassen and S.J. Eggers. </author> <title> Static analysis of barrier synchronization in explicitly parallel programs. </title> <note> Submitted for publication. </note>
Reference-contexts: Traces of the programs were statically analyzed on a per-thread basis for characteristics that provided cluster-combining criteria. A sharing-based placement algorithm that uses information gathered in this way can be approximated by a compiler, using summary side-effect analysis that detects per-thread memory accesses <ref> [12] </ref>. We did not study placement algorithms that rely on program runtime behavior, such as the order of sharing interconnect operations, which would be difficult or even impossible to implement statically. Table 2 shows averaged values of several of the measured characteristics for both workloads. <p> However, our applications do not have a significant amount of false sharing. Shared data in Topopt and Pverify were statically restructured <ref> [12] </ref> to eliminate false sharing. After restructuring, false sharing misses were 1.5% (Pverify) and 1.7% (Topopt) of the total data misses with a 32 KByte cache. <p> In either case, the programs have been optimized for data locality. For most of the programs the programmer was responsible for the partitioning; in two of them (Pverify and Topopt) locality-enhancing compiler optimizations automatically restructured the shared data to achieve the same effect <ref> [12] </ref>. For example, Barnes-Hut does N-body simulation, where in each time step, it computes the net force on a set of particles and updates their position and velocity. The interaction with other particles decreases with distance; hence the algorithm is parallelized by a spatial partitioning of particles into contiguous zones.
Reference: [13] <author> K. Kurihara, D. Chaiken, and A. Agarwal. </author> <title> Latency tolerance through multithreading in large-scale multiprocessing. </title> <booktitle> International Symposium on Shared Memory Multiprocessing, </booktitle> <pages> pages 91-101, </pages> <month> April </month> <year> 1991. </year> <month> 14 </month>
Reference-contexts: Second, large-sized networks have long memory access latencies. Together, they can cause considerable performance degradation. Multithreaded architectures address the second problem of long latencies, by context switching to another thread, and executing useful instructions while waiting for a memory access to complete <ref> [4, 11, 13, 19] </ref>. This decreases processor idle time, i.e., improves processor utilization. However, frequent context switching can exacerbate the first problem by increasing inter-thread conflict misses from the combined working sets of multiple threads.
Reference: [14] <author> H.-K. T. Ma, S. Devadas, R. Wei, and A. Sangiovanni-Vincentelli. </author> <title> Logic verification algorithms and their parallel applications. </title> <journal> IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, </journal> <volume> 8(2) </volume> <pages> 181-189, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: Threads from the medium-grain suite are shorter, on average 0.8 million instructions each, and more numerous. The programs span a wide range of application domains. LocusRoute, a commercial quality VLSI standard cell router, Pverify <ref> [14] </ref>, which compares boolean circuits for functional equivalence and Topopt [8], which performs 4 Number Mean Thread Application Applications of Length (millions Domain Threads of instructions) LocusRoute 12 n.a.
Reference: [15] <author> E. P. Markatos and T. J. LeBlanc. </author> <title> Using processor affinity in loop scheduling on shared-memory multiprocessors. </title> <booktitle> Supercomputing `92, </booktitle> <pages> pages 104-113, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: LOAD-BAL does load balancing based on thread length. The algorithm uses the dynamic length of each thread from the trace, and the resulting placement represents a perfectly load balanced execution. Load balancing is a standard scheduling technique on multiprocessor systems <ref> [15, 16, 21] </ref>; we use it for performance comparison. 8. Load balancing (LB) is added to algorithms SHARE-REFS, SHARE-ADDR, MIN-PRIV, MIN-INVS, MAX-WRITES and MIN-SHARE to generate versions of those algorithms that load balance instead of thread-balance when combining clusters. <p> They also incorporate cache performance degradation in their model. The study shows that few contexts cannot effectively hide very long memory latencies. Scheduling in single-context shared memory multiprocessors has included affinity scheduling [22] in medium-grain, explicitly parallel programs and fine-grain scheduling of loop iterations <ref> [15, 16, 21] </ref>. Affinity scheduling studies the impact of preferentially running a process on the processor where it previously executed, to take advantage of its already loaded cache state to reduce cache misses.
Reference: [16] <author> C. D. Polychronopoulos and D. J. Kuck. </author> <title> Guided self-scheduling: A practical scheduling scheme for parallel supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1425-1439, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: LOAD-BAL does load balancing based on thread length. The algorithm uses the dynamic length of each thread from the trace, and the resulting placement represents a perfectly load balanced execution. Load balancing is a standard scheduling technique on multiprocessor systems <ref> [15, 16, 21] </ref>; we use it for performance comparison. 8. Load balancing (LB) is added to algorithms SHARE-REFS, SHARE-ADDR, MIN-PRIV, MIN-INVS, MAX-WRITES and MIN-SHARE to generate versions of those algorithms that load balance instead of thread-balance when combining clusters. <p> They also incorporate cache performance degradation in their model. The study shows that few contexts cannot effectively hide very long memory latencies. Scheduling in single-context shared memory multiprocessors has included affinity scheduling [22] in medium-grain, explicitly parallel programs and fine-grain scheduling of loop iterations <ref> [15, 16, 21] </ref>. Affinity scheduling studies the impact of preferentially running a process on the processor where it previously executed, to take advantage of its already loaded cache state to reduce cache misses.
Reference: [17] <author> R. H. Saavedra-Barrera, D. E. Culler, and T. von Eicken. </author> <title> Analysis of multithreaded architectures for parallel computing. </title> <booktitle> 2nd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 169-178, </pages> <year> 1990. </year>
Reference-contexts: It also showed that high cache miss rates can hurt multithreading performance and that applications with active data sharing improves it. Saavedra-Barrera et al. developed a Markov chain model for multithreaded processor efficiency that uses the number of contexts, the network latency, context switch times and remote reference rate <ref> [17] </ref>. They also incorporate cache performance degradation in their model. The study shows that few contexts cannot effectively hide very long memory latencies. Scheduling in single-context shared memory multiprocessors has included affinity scheduling [22] in medium-grain, explicitly parallel programs and fine-grain scheduling of loop iterations [15, 16, 21].
Reference: [18] <author> J. P. Singh, W-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared-memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The next two subsections discuss in greater detail the application suite and its measured program characteristics, and the simulation environment. 3.1 The Application Suite We analyzed two types of explicitly parallel workloads: coarse-grain programs that include some of the SPLASH benchmarks <ref> [18] </ref>, and medium-grain applications that ran under the Presto parallel programming environment [6, 23]. We use the length and number of threads in an application as a measure of its granularity.
Reference: [19] <author> B. J. Smith. </author> <title> Architecture and applications of the HEP multiprocessor computer system. </title> <booktitle> SPIE, Real-Time Signal Processing IV, </booktitle> <volume> 298 </volume> <pages> 241-248, </pages> <year> 1981. </year>
Reference-contexts: Second, large-sized networks have long memory access latencies. Together, they can cause considerable performance degradation. Multithreaded architectures address the second problem of long latencies, by context switching to another thread, and executing useful instructions while waiting for a memory access to complete <ref> [4, 11, 13, 19] </ref>. This decreases processor idle time, i.e., improves processor utilization. However, frequent context switching can exacerbate the first problem by increasing inter-thread conflict misses from the combined working sets of multiple threads.
Reference: [20] <institution> Symmetry Technical Summary. Sequent Computer Systems, Inc. </institution>
Reference-contexts: The simulator modeled a shared-memory multiprocessor whose processors have multiple hardware contexts. Program traces (both data and instruction) from fourteen explicitly parallel applications were generated using the MPtrace [10] parallel tracing toolon a Sequent Symmetry <ref> [20] </ref>. Certain characteristics of the applications were extracted from the trace files and fed to the placement algorithms, which in turn produced maps associating threads with processors. Both maps and program traces were input to the simulator.
Reference: [21] <author> T. H. Tzen and L. M. Ni. </author> <title> Dynamic loop scheduling for shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages II:246-250, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: LOAD-BAL does load balancing based on thread length. The algorithm uses the dynamic length of each thread from the trace, and the resulting placement represents a perfectly load balanced execution. Load balancing is a standard scheduling technique on multiprocessor systems <ref> [15, 16, 21] </ref>; we use it for performance comparison. 8. Load balancing (LB) is added to algorithms SHARE-REFS, SHARE-ADDR, MIN-PRIV, MIN-INVS, MAX-WRITES and MIN-SHARE to generate versions of those algorithms that load balance instead of thread-balance when combining clusters. <p> They also incorporate cache performance degradation in their model. The study shows that few contexts cannot effectively hide very long memory latencies. Scheduling in single-context shared memory multiprocessors has included affinity scheduling [22] in medium-grain, explicitly parallel programs and fine-grain scheduling of loop iterations <ref> [15, 16, 21] </ref>. Affinity scheduling studies the impact of preferentially running a process on the processor where it previously executed, to take advantage of its already loaded cache state to reduce cache misses.
Reference: [22] <author> R. Vaswani and J. Zahorjan. </author> <title> The implications of cache affinity on processor scheduling for multiprogrammed, shared memory multiprocessors. </title> <booktitle> Proceedings of the 13th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 26-40, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: They also incorporate cache performance degradation in their model. The study shows that few contexts cannot effectively hide very long memory latencies. Scheduling in single-context shared memory multiprocessors has included affinity scheduling <ref> [22] </ref> in medium-grain, explicitly parallel programs and fine-grain scheduling of loop iterations [15, 16, 21]. Affinity scheduling studies the impact of preferentially running a process on the processor where it previously executed, to take advantage of its already loaded cache state to reduce cache misses.
Reference: [23] <author> D. B. Wagner. </author> <title> Conservative Parallel Discrete-Event Simulation: Principles and Practice. </title> <type> Ph.D. thesis, </type> <institution> University of Washington, </institution> <address> Seattle, </address> <month> September </month> <year> 1989. </year>
Reference-contexts: subsections discuss in greater detail the application suite and its measured program characteristics, and the simulation environment. 3.1 The Application Suite We analyzed two types of explicitly parallel workloads: coarse-grain programs that include some of the SPLASH benchmarks [18], and medium-grain applications that ran under the Presto parallel programming environment <ref> [6, 23] </ref>. We use the length and number of threads in an application as a measure of its granularity. Coarse-grain programs have fewer, but longer threads, 6.4 million instructions on the average, but as high as 100 million instructions (Table 1).
Reference: [24] <author> W-D. Weber and A. Gupta. </author> <title> Exploring the benefits of multiple hardware contexts in a multiprocessor architecture: Preliminary results. </title> <booktitle> Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 273-280, </pages> <month> June </month> <year> 1989. </year> <month> 15 </month>
Reference-contexts: Weber and Gupta estimated, via simulation, the extent to which a multithreaded architecture can overcome the effects of long access latencies <ref> [24] </ref>. They measured processor efficiency by varying the number of contexts and found substantial improvement. Agarwal presents an analytical performance model that incorporates network traffic, cache interference, context switching overhead and the number of hardware contexts [1].
References-found: 24

