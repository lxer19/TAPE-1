URL: ftp://thales.cs.umd.edu/pub/reports/wmlspleq.ps
Refering-URL: ftp://thales.cs.umd.edu/pub/reports/Contents.html
Root-URL: 
Title: On the Weighting Method for Least Squares Problems with Linear Equality Constraints  
Author: G. W. Stewart 
Date: November, 1996 Revised June, 1997  
Affiliation: University of Maryland College Park Institute for Advanced Computer Studies TR-96-79 Department of Computer Science  
Pubnum: TR-3709  
Abstract: The weighting method for solving a least squares problem with linear equality constraints multiplies the constraints by a large number and appends them to the top of the least squares problem, which is then solved by standard techniques. In this paper we give a new analysis of the method, based on the QR decomposition, that exhibits many features of the algorithm. In particular it suggests a natural criterion for chosing the weighting factor. fl This report is available by anonymous ftp from thales.cs.umd.edu in the directory pub/reports or through the web at http://www.cs.umd.edu/ stewart/. y Department of Computer Science and Institute for Advanced Computer Studies, University of Mary-land, College Park, MD 20742. This work was supported in part by the National Science Foundation under grant CCR 95503126. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. L. Barlow. </author> <title> Error analysis and implementation aspects of deferred correction for 2 Genuine numerical problems can occur if plane rotations are used in the order recommended by Gentleman and Kung [7] to parallelize the reduction. A small element in the last row of C 1 can cause the first row of X to be obliterated, even when C 1 is well conditioned. The Weighting Method 7 equality constrained least squares problems. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 25 </volume> <pages> 1340-1358, </pages> <year> 1988. </year>
Reference-contexts: Thus it is usually recommended that the problem be solved by orthogonal triangularization. Column pivoting is also recommended because counterexamples show that without it the method can fail. The weighting method has been analyzed by Lawson and Hanson [9, x22], Van Loan [15], and Barlow <ref> [1] </ref>. These analyses at some point invoke an eigendecomposition that mixes up the columns of the least squares matrix. In this paper, we will give an analysis based on the QR decomposition of the weighted least squares matrix. <p> In some cases (e.g., when the matrices are sparse), column pivoting may be precluded. In such cases, Van Loan [15] proposed an iterative method for refining the approximate solution, a method that has been extended an analyzed by Barlow <ref> [1] </ref>. It is important to stress that the choice of t in these algorithms is more restricted than for the dense unstructured case. In the next section we will review the elimination method. Section 3 gives the analysis of the weighting method.
Reference: [2] <author> A. Bjorck. </author> <title> Contribution no. 22. Iterative refinement of linear least squares solutions by Householder transformations. </title> <journal> BIT, </journal> <volume> 7 </volume> <pages> 322-337, </pages> <year> 1967. </year>
Reference-contexts: At that point we can shift to orthogonal triangularization followed by back substitution to solve the entire problem. This technique has been used by Shepherd and McWhirter [12] in recursive least squares applications. (In a different spirit, Bjorck <ref> [2] </ref> uses an orthogonal decomposition of C to compute the Schur complement.) The perturbation theory for the constrained problem [6] says that, among other things, the condition of the problem depends on the condition number of the constraint matrix C.
Reference: [3] <author> A. Bjorck. </author> <title> Iterative refinement of linear least squares solutions II. </title> <journal> BIT, </journal> <volume> 8 </volume> <pages> 8-30, </pages> <year> 1968. </year>
Reference-contexts: If t is large enough, the residual d Cb will be so small that the constraint is effectively satisfied. The origin of the weighting method is unknown, but an early reference is a paper by Bjorck <ref> [3] </ref>, who observed that as t increases the solution of (1.2) approaches the constrained least squares solution. 2 The Weighting Method The method has the appeal of simplicity | weight the constraints and invoke a least squares solver. In principal, the weighted problem (1.2) can be solved by any method.
Reference: [4] <author> A. Bjorck. </author> <title> Numerical Methods for Least Squares Problems. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1996. </year>
Reference-contexts: In this paper we will be concerned with the following constrained least squares problem: minimize ky Xbk 2 subject to Cb = d; (1.1) where k k is the Euclidean vector norm. There are several ways to solve this problem (for a survey, see <ref> [4, x5.1] </ref>). Here we will be concerned with two: the elimination method and the weighting method. The elimination method uses the constraint equation Cb = d to solve for m components of b in terms of the remaining components.
Reference: [5] <author> A. J. Cox and N. J. Higham. </author> <title> Stability of Householder QR factorization for weighted least squares. Numerical Analysis Report 301, </title> <institution> Department of Mathematics, University of Manchester, </institution> <year> 1997. </year>
Reference-contexts: In particular, examples of "numerical" difficulties in, say, the Householder triangularization are really just manifestations of the ill-conditioning of C 1 , which must make itself felt at some point in the reduction. 2 It is instructive to see what a recent extension by Cox and Higham <ref> [5] </ref> of an analysis by Powell and Reid [11] of Householder triangularization has to say about the problem.
Reference: [6] <author> L. Elden. </author> <title> Perturbation theory for the least squares problem with linear equality constraints. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 17 </volume> <pages> 338-350, </pages> <year> 1980. </year>
Reference-contexts: This technique has been used by Shepherd and McWhirter [12] in recursive least squares applications. (In a different spirit, Bjorck [2] uses an orthogonal decomposition of C to compute the Schur complement.) The perturbation theory for the constrained problem <ref> [6] </ref> says that, among other things, the condition of the problem depends on the condition number of the constraint matrix C. Now it is possible for C to be well conditioned while its submatrix C 1 is ill-conditioned, in which case the elimination algorithm will be unstable.
Reference: [7] <author> W. M. Gentleman and H. T. Kung. </author> <title> Matrix triangularization by systolic arrays. </title> <booktitle> In SPIE Proceedings, </booktitle> <volume> volume 298, </volume> <pages> pages 19-26, </pages> <year> 1982. </year> <note> Cited in [8]. </note>
Reference: [8] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, Maryland, 2nd edition, </address> <year> 1989. </year>
Reference-contexts: To bound the difference between Q 22 and ^ Q 22 note that from (3.4) kQ 21 k t 1 kQ 22 kkX 1 kkC 1 where fl = kX 1 kkC 1 Now by the CS decomposition <ref> [8, x2.6.3] </ref> the matrices Q 21 , Q 22 , and hence ^ Q 22 can be simultaneously diagonalized by orthogonal transformations.
Reference: [9] <author> C. L. Lawson and R. J. Hanson. </author> <title> Solving Least Squares Problems. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1974. </year> <title> Reissued with a survey in recent developments by SIAM, </title> <year> 1995. </year>
Reference-contexts: Thus it is usually recommended that the problem be solved by orthogonal triangularization. Column pivoting is also recommended because counterexamples show that without it the method can fail. The weighting method has been analyzed by Lawson and Hanson <ref> [9, x22] </ref>, Van Loan [15], and Barlow [1]. These analyses at some point invoke an eigendecomposition that mixes up the columns of the least squares matrix. In this paper, we will give an analysis based on the QR decomposition of the weighted least squares matrix.
Reference: [10] <author> M. Moonen and J. Vandewalle. </author> <title> A square root covariance algorithm for constrained recursive least squares estimation. </title> <journal> Journal of VLSI Signal Processing, </journal> <volume> 3 </volume> <pages> 163-172, </pages> <year> 1991. </year>
Reference: [11] <author> M. J. D. Powell and J. K. Reid. </author> <title> On applying Householder's method to linear least squares problems. </title> <booktitle> In Proceedings IFIP Congress, </booktitle> <pages> pages 122-126, </pages> <year> 1968. </year> <note> Cited in [8]. </note>
Reference-contexts: difficulties in, say, the Householder triangularization are really just manifestations of the ill-conditioning of C 1 , which must make itself felt at some point in the reduction. 2 It is instructive to see what a recent extension by Cox and Higham [5] of an analysis by Powell and Reid <ref> [11] </ref> of Householder triangularization has to say about the problem. In essence, the analysis says that if the rows of the matrix being reduced are arranged in descending order of size then the method is backward, rowwise stable provided the elements of the intermediate matrices do not grow.
Reference: [12] <author> T. J. Shepherd and J. G. McWhirter. </author> <title> A pipelined array for linearly constrained least squares optimization. </title> <editor> In T. S. Durrani, J. B. Abbiss, J. E. Hudson, R. W. Madan, J. G. McWhirter, and T. A. Moore, editors, </editor> <booktitle> Mathematics in Signal Processing, </booktitle> <pages> pages 607-635, </pages> <address> Oxford, </address> <year> 1987. </year> <note> Clarendon Press. Cited in [10]. </note>
Reference-contexts: At that point we can shift to orthogonal triangularization followed by back substitution to solve the entire problem. This technique has been used by Shepherd and McWhirter <ref> [12] </ref> in recursive least squares applications. (In a different spirit, Bjorck [2] uses an orthogonal decomposition of C to compute the Schur complement.) The perturbation theory for the constrained problem [6] says that, among other things, the condition of the problem depends on the condition number of the constraint matrix C.
Reference: [13] <author> G. W. Stewart. </author> <title> The efficient generation of random orthogonal matrices with an application to condition estimators. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 17 </volume> <pages> 403-404, </pages> <year> 1980. </year> <title> 8 The Weighting Method </title>
Reference-contexts: However, it is possible to have an ill-conditioned C 1 even when C is nice. In this light, the recommendation that column pivoting be employed in the triangulariza-tion can be seen as an attempt to get a well conditioned C 1 (this pivoting strategy is rather good at it <ref> [13] </ref>). However, column pivoting furnishes one more reason for using a generous criterion such as (3.8) for choosing t . Since the concern is with the matrix C, the pivoting should be controled by the norms of the columns of C.
Reference: [14] <author> G. W. Stewart. </author> <title> On the asymptotic behavior of scaled singular value and QR de-compostions. </title> <journal> Mathematics of Computation, </journal> <volume> 43 </volume> <pages> 483-489, </pages> <year> 1984. </year>
Reference-contexts: In this paper, we will give an analysis based on the QR decomposition of the weighted least squares matrix. The approach shows that the method is closely related to the elimination method (first noted in <ref> [14] </ref>). Moreover, it suggests a natural value of t and throws light on why column pivoting is necessary. The matrices C and X are assumed to be dense and unstructured. In some cases (e.g., when the matrices are sparse), column pivoting may be precluded.
Reference: [15] <author> C. F. Van Loan. </author> <title> On the method of weighting for equality constrained least squares. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 22 </volume> <pages> 851-864, </pages> <year> 1985. </year>
Reference-contexts: Thus it is usually recommended that the problem be solved by orthogonal triangularization. Column pivoting is also recommended because counterexamples show that without it the method can fail. The weighting method has been analyzed by Lawson and Hanson [9, x22], Van Loan <ref> [15] </ref>, and Barlow [1]. These analyses at some point invoke an eigendecomposition that mixes up the columns of the least squares matrix. In this paper, we will give an analysis based on the QR decomposition of the weighted least squares matrix. <p> Moreover, it suggests a natural value of t and throws light on why column pivoting is necessary. The matrices C and X are assumed to be dense and unstructured. In some cases (e.g., when the matrices are sparse), column pivoting may be precluded. In such cases, Van Loan <ref> [15] </ref> proposed an iterative method for refining the approximate solution, a method that has been extended an analyzed by Barlow [1]. It is important to stress that the choice of t in these algorithms is more restricted than for the dense unstructured case.
References-found: 15

