URL: ftp://ftp.idsia.ch/pub/juergen/lstm.ps.gz
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00228.html
Root-URL: 
Email: hochreit@informatik.tu-muenchen.de  juergen@idsia.ch  
Title: LONG SHORT-TERM MEMORY Neural Computation 9(8):1735-1780, 1997  
Author: Sepp Hochreiter Jurgen Schmidhuber 
Web: http://www7.informatik.tu-muenchen.de/~hochreit  http://www.idsia.ch/~juergen  
Address: 80290 Munchen, Germany  Corso Elvezia 36 6900 Lugano, Switzerland  
Affiliation: Fakultat fur Informatik Technische Universitat Munchen  IDSIA  
Abstract: Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insufficient, decaying error back flow. We briefly review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called "Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error flow through "constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long time lag tasks that have never been solved by previous recurrent network algorithms.
Abstract-found: 1
Intro-found: 1
Reference: <author> Almeida, L. B. </author> <year> (1987). </year> <title> A learning rule for asynchronous perceptrons with feedback in a combinatorial environment. </title> <booktitle> In IEEE 1st International Conference on Neural Networks, San Diego, </booktitle> <volume> volume 2, </volume> <pages> pages 609-618. </pages>
Reference: <author> Baldi, P. and Pineda, F. </author> <year> (1991). </year> <title> Contrastive learning and neural oscillator. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 526-545. </pages>
Reference: <author> Bengio, Y. and Frasconi, P. </author> <year> (1994). </year> <title> Credit assignment through time: Alternatives to backpropagation. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 75-82. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Bengio, Y., Simard, P., and Frasconi, P. </author> <year> (1994). </year> <title> Learning long-term dependencies with gradient descent is difficult. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(2) </volume> <pages> 157-166. </pages>
Reference: <author> Cleeremans, A., Servan-Schreiber, D., and McClelland, J. L. </author> <year> (1989). </year> <title> Finite-state automata and simple recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 372-381. </pages> <editor> de Vries, B. and Principe, J. C. </editor> <year> (1991). </year> <title> A theory for neural networks with time delays. </title> <editor> In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 162-168. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: To correctly predict the symbol before last, the net has to remember the second symbol. Comparison. We compare LSTM to "Elman nets trained by Elman's training procedure" (ELM) <ref> (results taken from Cleeremans et al. 1989) </ref>, Fahlman's "Recurrent Cascade-Correlation" (RCC) (results taken from Fahlman 1991), and RTRL (results taken from Smith and Zipser (1989), where only the few successful trials are listed). <p> 4 blocks, size 1 264 0.5 97 9,500 LSTM 3 blocks, size 2 276 0.5 100 8,440 Table 1: EXPERIMENT 1: Embedded Reber grammar: percentage of successful trials and number of sequence presentations until success for RTRL (results taken from Smith and Zipser 1989), "Elman net trained by Elman's procedure" <ref> (results taken from Cleeremans et al. 1989) </ref>, "Recurrent Cascade-Correlation" (results taken from Fahlman 1991) and our new approach (LSTM). Weight numbers in the first 4 rows are estimates | the corresponding papers do not provide all the technical details.
Reference: <author> Doya, K. </author> <year> (1992). </year> <title> Bifurcations in the learning of recurrent neural networks. </title> <booktitle> In Proceedings of 1992 IEEE International Symposium on Circuits and Systems, </booktitle> <pages> pages 2777-2780. </pages>
Reference: <author> Doya, K. and Yoshizawa, S. </author> <year> (1989). </year> <title> Adaptive neural oscillator using continuous-time backpropagation learning. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 375-385. </pages>
Reference: <author> Elman, J. L. </author> <year> (1988). </year> <title> Finding structure in time. </title> <type> Technical Report CRL Technical Report 8801, </type> <institution> Center for Research in Language, University of California, </institution> <address> San Diego. </address>
Reference: <author> Fahlman, S. E. </author> <year> (1991). </year> <title> The recurrent cascade-correlation learning algorithm. </title> <editor> In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 190-196. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Our LSTM architectures are selected quite arbitrarily. If nothing is known about the complexity of a given problem, a more systematic approach would be: start with a very small net consisting of one memory cell. If this does not work, try two cells, etc. Alternatively, use sequential network construction <ref> (e.g., Fahlman 1991) </ref>. Outline of experiments. * Experiment 1 focuses on a standard benchmark test for recurrent nets: the embedded Reber grammar. Since it allows for training sequences with short time lags, it is not a long time lag problem. <p> To correctly predict the symbol before last, the net has to remember the second symbol. Comparison. We compare LSTM to "Elman nets trained by Elman's training procedure" (ELM) (results taken from Cleeremans et al. 1989), Fahlman's "Recurrent Cascade-Correlation" (RCC) <ref> (results taken from Fahlman 1991) </ref>, and RTRL (results taken from Smith and Zipser (1989), where only the few successful trials are listed). It should be mentioned that Smith and Zipser actually make the task easier by increasing the probability of short time lag exemplars. We didn't do this for LSTM. <p> 3 blocks, size 2 276 0.5 100 8,440 Table 1: EXPERIMENT 1: Embedded Reber grammar: percentage of successful trials and number of sequence presentations until success for RTRL (results taken from Smith and Zipser 1989), "Elman net trained by Elman's procedure" (results taken from Cleeremans et al. 1989), "Recurrent Cascade-Correlation" <ref> (results taken from Fahlman 1991) </ref> and our new approach (LSTM). Weight numbers in the first 4 rows are estimates | the corresponding papers do not provide all the technical details. Only LSTM almost always learns to solve the task (only two failures out of 150 trials).
Reference: <author> Hochreiter, J. </author> <year> (1991). </year> <title> Untersuchungen zu dynamischen neuronalen Netzen. </title> <type> Diploma thesis, </type> <institution> Institut fur Informatik, Lehrstuhl Prof. Brauer, Technische Universitat Munchen. </institution> <note> See www7.informatik.tu-muenchen.de/~hochreit. 30 Hochreiter, </note> <author> S. and Schmidhuber, J. </author> <year> (1995). </year> <title> Long short-term memory. </title> <type> Technical Report FKI-207--95, </type> <institution> Fakultat fur Informatik, Technische Universitat Munchen. </institution>
Reference-contexts: The most difficult task (Task 2c) involves hundreds of distractor symbols at random positions, and minimal time lags of 1000 steps. LSTM solves it, while BPTT and RTRL already fail in case of 10-step minimal time lags <ref> (see also, e.g., Hochreiter 1991 and Mozer 1992) </ref>. For this reason RTRL and BPTT are omitted in the remaining, more complex experiments, all of which involve much longer time lags. * Experiment 3 addresses long time lag problems with noise and signal on the same input line.
Reference: <author> Hochreiter, S. and Schmidhuber, J. </author> <year> (1996). </year> <title> Bridging long time lags by weight guessing and "Long Short-Term Memory". </title> <editor> In Silva, F. L., Principe, J. C., and Almeida, L. B., editors, </editor> <booktitle> Spa-tiotemporal models in biological and artificial systems, </booktitle> <pages> pages 65-72. </pages> <publisher> IOS Press, </publisher> <address> Amsterdam, Netherlands. Serie: </address> <booktitle> Frontiers in Artificial Intelligence and Applications, </booktitle> <volume> Volume 37. </volume>
Reference-contexts: See, e.g., Hochreiter (1991) and Mozer (1992). A second important requirement is that the tasks should be complex enough such that they cannot be solved quickly by simple-minded strategies such as random weight guessing. Guessing can outperform many long time lag algorithms. Recently we discovered <ref> (Schmidhuber and Hochreiter 1996, Hochreiter and Schmidhuber 1996, 1997) </ref> that many long time lag tasks used in previous work can be solved more quickly by simple random weight guessing than by the proposed algorithms.
Reference: <author> Hochreiter, S. and Schmidhuber, J. </author> <year> (1997). </year> <title> LSTM can solve hard long time lag problems. </title> <booktitle> In Advances in Neural Information Processing Systems 9. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address> <note> Presented at NIPS 96. </note>
Reference: <author> Lang, K., Waibel, A., and Hinton, G. E. </author> <year> (1990). </year> <title> A time-delay neural network architecture for isolated word recognition. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 23-43. </pages>
Reference: <author> Miller, C. B. and Giles, C. L. </author> <year> (1993). </year> <title> Experimental comparison of the effect of order in recurrent neural networks. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 7(4) </volume> <pages> 849-872. </pages>
Reference: <author> Mozer, M. C. </author> <year> (1989). </year> <title> A focused back-propagation algorithm for temporal sequence recognition. </title> <journal> Complex Systems, </journal> <volume> 3 </volume> <pages> 349-381. </pages>
Reference: <author> Mozer, M. C. </author> <year> (1992). </year> <title> Induction of multiscale temporal structure. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 275-282. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Pearlmutter, B. A. </author> <year> (1989). </year> <title> Learning state space trajectories in recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 1(2) </volume> <pages> 263-269. </pages>
Reference: <author> Pearlmutter, B. A. </author> <year> (1995). </year> <title> Gradient calculations for dynamic recurrent neural networks: A survey. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 6(5) </volume> <pages> 1212-1228. </pages>
Reference: <author> Pineda, F. J. </author> <year> (1987). </year> <title> Generalization of back-propagation to recurrent neural networks. </title> <journal> Physical Review Letters, </journal> 19(59) 2229-2232. 
Reference: <author> Pineda, F. J. </author> <year> (1988). </year> <title> Dynamics and architecture for neural computation. </title> <journal> Journal of Complexity, </journal> <volume> 4 </volume> <pages> 216-245. </pages>
Reference: <author> Plate, T. A. </author> <year> (1993). </year> <title> Holographic recurrent networks. </title> <editor> In S. J. Hanson, J. D. C. and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 34-41. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Pollack, J. B. </author> <year> (1991). </year> <title> Language induction by phase transition in dynamical recognizers. </title> <editor> In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 619-626. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Puskorius, G. V. and Feldkamp, L. A. </author> <year> (1994). </year> <title> Neurocontrol of nonlinear dynamical systems with Kalman filter trained recurrent networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(2) </volume> <pages> 279-297. </pages>
Reference: <author> Ring, M. B. </author> <year> (1993). </year> <title> Learning sequential tasks by incrementally adding higher orders. </title> <editor> In S. J. Han-son, J. D. C. and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 115-122. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Robinson, A. J. and Fallside, F. </author> <year> (1987). </year> <title> The utility driven dynamic error propagation network. </title> <type> Technical Report CUED/F-INFENG/TR.1, </type> <institution> Cambridge University Engineering Department. </institution>
Reference: <author> Schmidhuber, J. </author> <year> (1989). </year> <title> The Neural Bucket Brigade: A local learning algorithm for dynamic feedforward and recurrent networks. </title> <journal> Connection Science, </journal> <volume> 1(4) </volume> <pages> 403-412. </pages> <note> 31 Schmidhuber, J. </note> <year> (1992a). </year> <title> A fixed size storage O(n 3 ) time complexity learning algorithm for fully recurrent continually running networks. </title> <journal> Neural Computation, </journal> 4(2):243-248. 
Reference: <author> Schmidhuber, J. </author> <year> (1992b). </year> <title> Learning complex, extended sequences using the principle of history compression. </title> <journal> Neural Computation, </journal> <volume> 4(2) </volume> <pages> 234-242. </pages>
Reference-contexts: To predict the final element, the net has to learn to store a representation of the first element for p time steps. We compare "Real-Time Recurrent Learning" for fully recurrent nets (RTRL), "Back-Propagation Through Time" (BPTT), the sometimes very successful 2-net "Neural Sequence Chunker" <ref> (CH, Schmidhuber 1992b) </ref>, and our new method (LSTM). In all cases, weights are initialized in [-0.2,0.2]. <p> Each layer has connections from all layers below. All units use the logistic activation function sigmoid in [0,1]. BPTT: same architecture as the one trained by RTRL. CH: both net architectures like RTRL's, but one has an additional output for predicting the hidden unit of the other one <ref> (see Schmidhuber 1992b for details) </ref>. LSTM: like with RTRL, but the hidden unit is replaced by a memory cell and an input gate (no output gate required). g is the logistic sigmoid, and h is the identity function h : h (x) = x; 8x. <p> Future work. To find out about LSTM's practical limitations we intend to apply it to real world data. Application areas will include (1) time series prediction, (2) music composition, and (3) speech processing. It will also be interesting to augment sequence chunkers <ref> (Schmidhuber 1992b, 1993) </ref> by LSTM to combine the advantages of both. 8 ACKNOWLEDGMENTS Thanks to Mike Mozer, Wilfried Brauer, Nic Schraudolph, and several anonymous referees for valuable comments and suggestions that helped to improve a previous version of this paper (Hochreiter and Schmidhuber 1995).
Reference: <author> Schmidhuber, J. </author> <year> (1992c). </year> <title> Learning unambiguous reduced sequence descriptions. </title> <editor> In Moody, J. E., Hanson, S. J., and Lippman, R. P., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 291-298. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: LSTM, however, always learned to solve the task. Comparing successful trials only, LSTM learned much faster. See Table 2 for details. It should be mentioned, however, that a hierarchical chunker can also always quickly solve this task <ref> (Schmidhuber 1992c, 1993) </ref>. Task 2b: no local regularities. With the task above, the chunker sometimes learns to correctly predict the final element, but only because of predictable local regularities in the input stream that allow for compressing the sequence.
Reference: <author> Schmidhuber, J. </author> <year> (1993). </year> <institution> Netzwerkarchitekturen, Zielfunktionen und Kettenregel. Habilitations-schrift, Institut fur Informatik, Technische Universitat Munchen. </institution>
Reference: <author> Schmidhuber, J. and Hochreiter, S. </author> <year> (1996). </year> <title> Guessing can outperform many long time lag algorithms. </title> <type> Technical Report IDSIA-19-96, </type> <institution> IDSIA. </institution>
Reference-contexts: See, e.g., Hochreiter (1991) and Mozer (1992). A second important requirement is that the tasks should be complex enough such that they cannot be solved quickly by simple-minded strategies such as random weight guessing. Guessing can outperform many long time lag algorithms. Recently we discovered <ref> (Schmidhuber and Hochreiter 1996, Hochreiter and Schmidhuber 1996, 1997) </ref> that many long time lag tasks used in previous work can be solved more quickly by simple random weight guessing than by the proposed algorithms.
Reference: <author> Silva, G. X., Amaral, J. D., Langlois, T., and Almeida, L. B. </author> <year> (1996). </year> <title> Faster training of recurrent networks. </title> <editor> In Silva, F. L., Principe, J. C., and Almeida, L. B., editors, </editor> <booktitle> Spatiotemporal models in biological and artificial systems, </booktitle> <pages> pages 168-175. </pages> <publisher> IOS Press, </publisher> <address> Amsterdam, Netherlands. Serie: </address> <booktitle> Frontiers in Artificial Intelligence and Applications, </booktitle> <volume> Volume 37. </volume>
Reference: <author> Smith, A. W. and Zipser, D. </author> <year> (1989). </year> <title> Learning sequential structures with the real-time recurrent learning algorithm. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1(2) </volume> <pages> 125-131. </pages>
Reference-contexts: 0.1 100 21,730 LSTM 3 blocks, size 2 276 0.2 97 14,060 LSTM 4 blocks, size 1 264 0.5 97 9,500 LSTM 3 blocks, size 2 276 0.5 100 8,440 Table 1: EXPERIMENT 1: Embedded Reber grammar: percentage of successful trials and number of sequence presentations until success for RTRL <ref> (results taken from Smith and Zipser 1989) </ref>, "Elman net trained by Elman's procedure" (results taken from Cleeremans et al. 1989), "Recurrent Cascade-Correlation" (results taken from Fahlman 1991) and our new approach (LSTM).
Reference: <author> Sun, G., Chen, H., and Lee, Y. </author> <year> (1993). </year> <title> Time warping invariant neural networks. </title> <editor> In S. J. Hanson, J. D. C. and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 180-187. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Watrous, R. L. and Kuhn, G. M. </author> <year> (1992). </year> <title> Induction of finite-state languages using second-order recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 406-414. </pages>
Reference: <author> Werbos, P. J. </author> <year> (1988). </year> <title> Generalization of backpropagation with application to a recurrent gas market model. Neural Networks, </title> <type> 1. </type>
Reference: <author> Williams, R. J. </author> <year> (1989). </year> <title> Complexity of exact gradient computation algorithms for recurrent neural networks. </title> <type> Technical Report Technical Report NU-CCS-89-27, </type> <institution> Boston: Northeastern University, College of Computer Science. </institution>
Reference: <author> Williams, R. J. and Peng, J. </author> <year> (1990). </year> <title> An efficient gradient-based algorithm for on-line training of recurrent network trajectories. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 491-501. </pages>
Reference-contexts: Net activations are reset after each processed input sequence. For comparisons with recurrent nets taught by gradient descent, we give results only for RTRL, except for comparison 2a, which also includes BPTT. Note, however, that untruncated BPTT <ref> (see, e.g., Williams and Peng 1990) </ref> computes exactly the same gradient as o*ine RTRL.
Reference: <author> Williams, R. J. and Zipser, D. </author> <year> (1992). </year> <title> Gradient-based learning algorithms for recurrent networks and their computational complexity. In Back-propagation: Theory, Architectures and Applications. </title> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher> <pages> 32 </pages>
References-found: 38

