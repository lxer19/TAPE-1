URL: http://www.ai.univie.ac.at/~juffi/publications/ijcai-97.ps.gz
Refering-URL: http://www.ai.univie.ac.at/~juffi/publications/publications.html
Root-URL: 
Email: E-mail: juffi@ai.univie.ac.at  
Title: Noise-Tolerant Windowing  
Author: Johannes F urnkranz 
Address: Schottengasse 3, A-1010 Wien, Austria  
Affiliation: Austrian Research Institute for Artificial Intelligence  
Abstract: Windowing has been proposed as a procedure for efficient memory use in the ID3 decision tree learning algorithm. However, it was shown that it may often lead to a decrease in performance, in particular in noisy domains. Following up on previous work, where we have demonstrated that the ability of rule learning algorithms to learn rules independently can be exploited for more efficient win-dowing procedures, we demonstrate in this paper how this property can be exploited to achieve noise tolerance in windowing.
Abstract-found: 1
Intro-found: 1
Reference: [ Breiman et al., 1984 ] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth & Brooks, </publisher> <address> Pacific Grove, CA, </address> <year> 1984. </year>
Reference-contexts: For decision tree algorithms it has been proposed to use dynamic subsampling at each node in order to determine the optimal test. This idea has been originally proposed, but not evaluated in <ref> [ Breiman et al., 1984 ] </ref> . This approach was further explored in Catlett's work on peepholing [ Catlett, 1992 ] , which is a sophisticated procedure for using subsampling to eliminate unpromising attributes and thresholds from consideration.
Reference: [ Catlett, 1991 ] <author> Jason Catlett. </author> <title> Megainduction: A test flight. In L.A. </title> <editor> Birnbaum and G.C. Collins, editors, </editor> <booktitle> Proceedings of the 8th International Workshop on Machine Learning (ML-91), </booktitle> <pages> pages 596 599, </pages> <address> Evanston, IL, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It can be assumed that many more examples have to be added to the window in order to recover the structure that is inherent in the data. This hypothesis is consistent with the results of [ Wirth and Catlett, 1988 ] and <ref> [ Catlett, 1991 ] </ref> , where it was shown that windowing is highly sensitive to noise. 4 A Noise-Tolerant Version of Windowing The windowing algorithm described in [ Furnkranz, 1997a ] , which is only applicable to noise-free domains, is based on the observation that rule learning algorithms will re-discover good
Reference: [ Catlett, 1992 ] <author> Jason Catlett. Peepholing: </author> <title> Choosing attributes efficiently for megainduction. </title> <booktitle> In Proceedings of the 9th International Conference on Machine Learning (ML-91), </booktitle> <pages> pages 4954. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: We expect that the fact that fewer thresholds have to be considered at lower example set sizes will have a positive effect on the run-time performance of windowing, but may have a negative effect on the accuracy of the learned rules. This hypothesis has been stated before <ref> [ Catlett, 1992 ] </ref> , but has never been empirically verified. In fact, we would not be surprised, if a lower set of potential thresholds, like the ones contained in the current window, gave the algorithm less chance for overfitting and could thus even increase predictive accuracy. <p> For decision tree algorithms it has been proposed to use dynamic subsampling at each node in order to determine the optimal test. This idea has been originally proposed, but not evaluated in [ Breiman et al., 1984 ] . This approach was further explored in Catlett's work on peepholing <ref> [ Catlett, 1992 ] </ref> , which is a sophisticated procedure for using subsampling to eliminate unpromising attributes and thresholds from consideration. Most closely related to windowing is uncertainty sampling [ Lewis and Catlett, 1994 ] .
Reference: [ Clark and Niblett, 1989 ] <author> Peter Clark and Tim Niblett. </author> <title> The CN2 induction algorithm. </title> <booktitle> Machine Learning, </booktitle> <address> 3(4):261283, </address> <year> 1989. </year>
Reference-contexts: Thus, a more elaborate criterion must be used. We have experimented with a variety of criteria known from the literature, but found that they are insufficient for our purposes. For example, it turned out that, at higher training set sizes, CN2's likelihood ratio significance test <ref> [ Clark and Niblett, 1989 ] </ref> will deem almost any rule learned by I-RIP as significant, even if the distribution of covered positive and negative examples deviates only slightly from their distribution in the entire training set.
Reference: [ Cohen, 1995 ] <author> William W. Cohen. </author> <title> Fast effective rule induction. </title> <editor> In A. Prieditis and S. Russell, editors, </editor> <booktitle> Proceedings of the 12th International Conference on Machine Learning (ML-95), </booktitle> <pages> pages 115 123, </pages> <address> Lake Tahoe, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The basic learning algorithm we use, I-RIP, is based on I-REP [ Furnkranz and Wid-mer, 1994 ] and its successor RIPPER <ref> [ Cohen, 1995 ] </ref> . However, the algorithms presented in this paper do not depend on this choice; any other effective noise-tolerant rule learning algorithm could be used in I-RIP's place. <p> The resulting rule is added to the theory, and all examples that it covers are removed from the training set. The remaining training examples are used for learning another rule until no more meaningful rules can be discovered. In <ref> [ Cohen, 1995 ] </ref> it was shown that some of the parameters of the I-REP algorithm, like the pruning and stopping criteria, were not chosen optimally. <p> We have not implemented RIPPER's rule optimization heuristics. Thus our I-RIP algorithm is half-way between I-REP and RIPPER. As such, it is quite similar to I-REP*, which is also described in <ref> [ Cohen, 1995 ] </ref> , but it differs from it in that its implementation is closer to the original I-REP. For example, I-RIP considers every condition in a rule for pruning, while I-REP* only considers to delete a final sequence of conditions.
Reference: [ Domingos, 1996 ] <author> Pedro Domingos. </author> <title> Efficient specific-to-general rule induction. </title> <editor> In E. Simoudis and J. Han, editors, </editor> <booktitle> Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining (KDD-96), </booktitle> <pages> pages 319322. </pages> <publisher> AAAI Press, </publisher> <year> 1996. </year>
Reference-contexts: Work on partitioning, i.e. splitting the example space into segments of equal size and combining the rules learned on each partition, has also produced promising results in noisy domains, but has substantially decreased learning accuracy in non-noisy domains <ref> [ Domingos, 1996 ] </ref> . Besides, the technique seems to be tailored to a specific learning algorithm and not generally applicable. 8 Summary We have presented a noise-tolerant version of windowing that is based on a separate-and-conquer strategy.
Reference: [ Furnkranz and Widmer, 1994 ] <author> Johannes Furnkranz and Gerhard Widmer. </author> <title> Incremental Reduced Error Pruning. </title> <editor> In W. Cohen and H. Hirsh, editors, </editor> <booktitle> Proceedings of the 11th International Conference on Machine Learning (ML-94), </booktitle> <pages> pages 7077, </pages> <address> New Brunswick, NJ, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In [ Cohen, 1995 ] it was shown that some of the parameters of the I-REP algorithm, like the pruning and stopping criteria, were not chosen optimally. We have implemented the I-REP algorithm as described in <ref> [ Furnkranz and Widmer, 1994 ] </ref> , but used RIPPER's rule-value-metric pruning criterion and its 0:5-rule-accuracy stopping criterion. We have not implemented RIPPER's rule optimization heuristics. Thus our I-RIP algorithm is half-way between I-REP and RIPPER. <p> In terms of accuracy, no significant differences can be observed between I-RIP, WIN-RIP, and I-WIN (0.0), although the latter is able to compensate some of the weakness of I-RIP at low example set sizes that is due to its internal split of the data <ref> [ Furnkranz and Widmer, 1994 ] </ref> . I WIN with ff = 0:5 and ff = 1:0 has a significantly worse performance, because these versions are often content with slightly over-general rules, which is detrimental in this noise-free domain.
Reference: [ Furnkranz, 1997a ] <author> Johannes Furnkranz. </author> <title> More efficient window-ing. </title> <booktitle> In Proceedings of the 14th National Conference on Artificial Intelligence (AAAI-97), </booktitle> <address> Providence, RI, 1997. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: The best results were achieved in noise-free domains, such as the Mushroom domain, where it was able to perform on the same level as ID3 without window-ing, while its performance in noisy domains was considerably worse. In <ref> [ Furnkranz, 1997a ] </ref> , we have demonstrated that rule learning algorithms are better suited for windowing in noise-free domains, because they learn each rule independently. <p> This hypothesis is consistent with the results of [ Wirth and Catlett, 1988 ] and [ Catlett, 1991 ] , where it was shown that windowing is highly sensitive to noise. 4 A Noise-Tolerant Version of Windowing The windowing algorithm described in <ref> [ Furnkranz, 1997a ] </ref> , which is only applicable to noise-free domains, is based on the observation that rule learning algorithms will re-discover good rules again and again in subsequent iterations of the windowing procedure. <p> If these rules could be detected early on, they could be saved and the examples they cover could be removed from the window, thus gaining computational efficiency. The algorithm discussed in <ref> [ Furnkranz, 1997a ] </ref> achieves this by separating the examples that are covered by rules that have been consistent for a larger procedure I-WIN (Examples,InitSize,MaxIncSize) Train = RANDOMSAMPLE (Examples,InitSize) Theory = ; repeat NewTheory = I-RIP (Train) NewTr = Train NewEx = Examples Candidates = ; for Rule 2 NewTheory if <p> With a setting of ff = 0, I-WIN is very similar to the WIN DOS-95 algorithm described in <ref> [ Furnkranz, 1997a ] </ref> with the difference that WINDOS-95 only tests a theory until it has collected MaxIncSize new examples to add to the current window. <p> Furthermore, all algorithms discussed in this paper attempt to remove semantically redundant rules in a post-processing phase. Such rules only cover training examples that are also covered by other rules. We refer to <ref> [ Furnkranz, 1997a ] </ref> for more details. 5 Experimental Evaluation In each of the experiments described in this section, we report the average results of 10 different subsets of the specified training set size, selected from the entire set of preclas-sified examples. <p> These settings have been found to perform well on noise-free domains <ref> [ Furnkranz, 1997a ] </ref> . We have not yet made an attempt to evaluate their appropriateness for noisy domains. First we have tested the algorithms on the 8124 example Mushroom database. <p> However, we have shown that our windowing algorithm is in fact able to achieve significant gains in run-time without losing accuracy, thus confirming our previous results <ref> [ Furnkranz, 1997a ] </ref> . For testing the algorithms' noise-handling capabilities we have performed a series of experiments in a propositional version of the well-known KRK classification task, which is commonly used as a benchmark for relational learning algorithms. <p> In all experiments in this paper we have set the initial window size to 100, and the maximum window increment to 50. We have found these parameters to perform well on noise-free domains <ref> [ Furnkranz, 1997a ] </ref> , but in some experiments we have encountered evidence that larger values of these parameters could be more suitable for noisy domains. Another crucial parameter is the ff parameter used in the significance test we have employed. <p> It lies in the nature of windowing that it can only work successfully, if there is some redundancy in the domain, i.e. that at least some of the rules of good theory can be learned from a subset of the given training examples. In <ref> [ Furnkranz, 1997a ] </ref> we present an example for a noise-free dataset, where this assumption does not hold, and consequently windowing is not effective.
Reference: [ Furnkranz, 1997b ] <author> Johannes Furnkranz. </author> <title> Separate-and-conquer rule learning. </title> <journal> Artificial Intelligence Review, </journal> <note> 1997. To appear. </note>
Reference-contexts: In this paper, we will show how this property can be exploited in order to achieve noise-tolerance. 2 The I-RIP algorithm We have conducted our study in the framework of separate-and-conquer rule learning algorithms that has recently gained in popularity <ref> [ Furnkranz, 1997b ] </ref> . The basic learning algorithm we use, I-RIP, is based on I-REP [ Furnkranz and Wid-mer, 1994 ] and its successor RIPPER [ Cohen, 1995 ] .
Reference: [ John and Langley, 1996 ] <author> George H. John and Pat Langley. </author> <title> Static versus dynamic sampling for data mining. </title> <editor> In E. Simoudis and J. Han, editors, </editor> <booktitle> Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining (KDD-96), </booktitle> <pages> pages 367370. </pages> <publisher> AAAI Press, </publisher> <year> 1996. </year>
Reference-contexts: The examples that are classified with the least confidence will be added to the training set in the next iteration. A different approach that successively increases the current learning window is presented in <ref> [ John and Langley, 1996 ] </ref> . Here examples are added until an extrapolation of the learning curve does no longer promise significant gains. However, the authors note that this technique can in general only gain efficiency for incremental learning algorithms.
Reference: [ Kivinen and Mannila, 1994 ] <author> Jyrki Kivinen and Heikki Mannila. </author> <title> The power of sampling in knowledge discovery. </title> <booktitle> In Proceedings of the 13th ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems (PODS-94), </booktitle> <pages> pages 7785, </pages> <year> 1994. </year>
Reference-contexts: One reason for this certainly is the rapid development of computer hardware, which made the motivation for windowing seem less compelling. However, recent work in the areas of Knowledge Discovery in Databases <ref> [ Kivinen and Mannila, 1994; Toivonen, 1996 ] </ref> and Intelligent Information Retrieval [ Lewis and Catlett, 1994; Yang, 1996 ] has recognized the importance of subsampling procedures for reducing both, learning time and memory requirements.
Reference: [ Lewis and Catlett, 1994 ] <author> David D. Lewis and Jason Catlett. </author> <title> Heterogeneous uncertainty sampling for supervised learning. </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning (ML-94), </booktitle> <pages> pages 148156, </pages> <address> New Brunswick, NJ, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: One reason for this certainly is the rapid development of computer hardware, which made the motivation for windowing seem less compelling. However, recent work in the areas of Knowledge Discovery in Databases [ Kivinen and Mannila, 1994; Toivonen, 1996 ] and Intelligent Information Retrieval <ref> [ Lewis and Catlett, 1994; Yang, 1996 ] </ref> has recognized the importance of subsampling procedures for reducing both, learning time and memory requirements. <p> This approach was further explored in Catlett's work on peepholing [ Catlett, 1992 ] , which is a sophisticated procedure for using subsampling to eliminate unpromising attributes and thresholds from consideration. Most closely related to windowing is uncertainty sampling <ref> [ Lewis and Catlett, 1994 ] </ref> . Here the new window is not selected on the basis of misclassified examples, but on the basis of the learner's confidence in the learned theory.
Reference: [ Quinlan, 1983 ] <author> John Ross Quinlan. </author> <title> Learning efficient classification procedures and their application to chess end games. </title> <editor> In Ryszard S. Michalski, Jaime G. Carbonell, and Tom M. Mitchell, editors, </editor> <booktitle> Machine Learning. An Artificial Intelligence Approach, </booktitle> <pages> pages 463482. </pages> <publisher> Tioga, </publisher> <address> Palo Alto, CA, </address> <year> 1983. </year>
Reference-contexts: The gain in efficiency is obtained by identifying an appropriate subset of the given training examples, from which a theory of sufficient quality can be induced. Such procedures are also known as subsampling. Windowing has been proposed in <ref> [ Quinlan, 1983 ] </ref> as a supplement to the inductive decision tree learner ID3 to enable it to tackle tasks which would otherwise have exceeded the memory capacity of the computers of those days. Despite first successful experiments in the KRKN chess endgame domain [ Quinlan, 1983 ] , windowing has <p> Windowing has been proposed in <ref> [ Quinlan, 1983 ] </ref> as a supplement to the inductive decision tree learner ID3 to enable it to tackle tasks which would otherwise have exceeded the memory capacity of the computers of those days. Despite first successful experiments in the KRKN chess endgame domain [ Quinlan, 1983 ] , windowing has not played a major role in machine learning research. One reason for this certainly is the rapid development of computer hardware, which made the motivation for windowing seem less compelling. <p> However, these are no principle limitations to the algorithm, and standard enhancements for dealing with these problems could easily be added to all algorithms described in this paper. 3 Windowing and Noise The windowing algorithm described in <ref> [ Quinlan, 1983 ] </ref> starts by picking a random sample of a user-settable size InitSize from the total set of Examples and uses it for inducing a classifier with a given learning algorithm, in our case the I-RIP algorithm briefly described in the last section.
Reference: [ Quinlan, 1993 ] <author> John Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: For example, windowing with the decision tree learner ID3 could not achieve significant run-time gains over pure ID3 [ Wirth and Catlett, 1988 ] , while the slightly modified version of win-dowing used in C4.5 is able to achieve a run-time improvement of only about 15% (p. 59 of <ref> [ Quinlan, 1993 ] </ref> ). The left column of figure 2 shows the accuracy and run-time results for I-RIP, WIN-RIP, and three versions of I 1 Measured in CPU seconds of a microSPARC 110MHz running compiled Allegro Common Lisp code under SUN Unix 4.1.3. version of the Thyroid domain.
Reference: [ Toivonen, 1996 ] <author> Hannu Toivonen. </author> <title> Sampling large databases for association rules. </title> <booktitle> In Proceedings of the 22nd Conference on Very Large Data Bases (VLDB-96), </booktitle> <pages> pages 134145, </pages> <address> Mumbai, India, </address> <year> 1996. </year>
Reference-contexts: One reason for this certainly is the rapid development of computer hardware, which made the motivation for windowing seem less compelling. However, recent work in the areas of Knowledge Discovery in Databases <ref> [ Kivinen and Mannila, 1994; Toivonen, 1996 ] </ref> and Intelligent Information Retrieval [ Lewis and Catlett, 1994; Yang, 1996 ] has recognized the importance of subsampling procedures for reducing both, learning time and memory requirements.
Reference: [ Wirth and Catlett, 1988 ] <author> Jarryl Wirth and Jason Catlett. </author> <title> Experiments on the costs and benefits of windowing in ID3. </title> <editor> In J. Laird, editor, </editor> <booktitle> Proceedings of the 5th International Conference on Machine Learning (ML-88), </booktitle> <pages> pages 8799, </pages> <address> Ann Arbor, MI, 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A good deal of this lack of interest can be attributed to an empirical study <ref> [ Wirth and Catlett, 1988 ] </ref> which showed that windowing is unlikely to gain any efficiency. The authors studied windowing with ID3 in various domains and concluded that it cannot be recommended as a procedure for improving efficiency. <p> It can be assumed that many more examples have to be added to the window in order to recover the structure that is inherent in the data. This hypothesis is consistent with the results of <ref> [ Wirth and Catlett, 1988 ] </ref> and [ Catlett, 1991 ] , where it was shown that windowing is highly sensitive to noise. 4 A Noise-Tolerant Version of Windowing The windowing algorithm described in [ Furnkranz, 1997a ] , which is only applicable to noise-free domains, is based on the observation <p> Although this database is known to be noise-free, it forms an interesting test-bed for our algorithms, because it allows a rough comparison to previous results. For example, windowing with the decision tree learner ID3 could not achieve significant run-time gains over pure ID3 <ref> [ Wirth and Catlett, 1988 ] </ref> , while the slightly modified version of win-dowing used in C4.5 is able to achieve a run-time improvement of only about 15% (p. 59 of [ Quinlan, 1993 ] ).
Reference: [ Yang, 1996 ] <author> Yiming Yang. </author> <title> Sampling strategies and learning efficiency in text categorization. </title> <editor> In M. Hearst and H. Hirsh, editors, </editor> <booktitle> Proceedings of the AAAI Spring Symposium on Machine Learning in Information Access, </booktitle> <pages> pages 8895. </pages> <publisher> AAAI Press, </publisher> <year> 1996. </year> <note> Technical Report SS-96-05. </note>
Reference-contexts: One reason for this certainly is the rapid development of computer hardware, which made the motivation for windowing seem less compelling. However, recent work in the areas of Knowledge Discovery in Databases [ Kivinen and Mannila, 1994; Toivonen, 1996 ] and Intelligent Information Retrieval <ref> [ Lewis and Catlett, 1994; Yang, 1996 ] </ref> has recognized the importance of subsampling procedures for reducing both, learning time and memory requirements.
References-found: 17

