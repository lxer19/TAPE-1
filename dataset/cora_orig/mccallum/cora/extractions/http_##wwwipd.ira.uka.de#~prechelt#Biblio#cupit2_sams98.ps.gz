URL: http://wwwipd.ira.uka.de/~prechelt/Biblio/cupit2_sams98.ps.gz
Refering-URL: 
Root-URL: 
Email: (hoppjprechelt@ira.uka.de)  
Phone: Phone: +49/721/608-7344, Fax: -7343  
Title: CuPit-2: Portable and Efficient High-Level Parallel Programming of Neural Networks for the Systems Analysis Modelling
Author: Holger Hopp, Lutz Prechelt 
Note: Submission to the Special Issue on Simulation of Artificial Neural Networks  
Date: 12th January 1998  
Address: D-76128 Karlsruhe, Germany  
Affiliation: Universitat Karlsruhe, Fakultat fur Informatik,  
Abstract: CuPit-2 is a special-purpose programming language designed for expressing dynamic neural network learning algorithms. It provides most of the flexibility of general-purpose languages such as C or C ++ , but is more expressive. It allows writing much clearer and more elegant programs, in particular for algorithms that change the network topology dynamically (constructive algorithms, pruning algorithms). In contrast to other languages, CuPit-2 programs can be compiled into efficient code for parallel machines without any changes in the source program, thus providing an easy start for using parallel platforms. This article analyzes the circumstances under which the CuPit-2 approach is the most useful one, presents a description of most language constructs and reports performance results for CuPit-2 on symmetric multiprocessors (SMPs). It concludes that in many cases CuPit-2 is a good basis for neural learning algorithm research on small-scale parallel machines. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> 15th IMACS World Congress on Scientific Computation, Modelling, and Applied Mathematics, </institution> <year> 1997. </year>
Reference: [2] <author> Nikolaus Almassy. </author> <title> Ein Compiler f ur CONDELA-III. </title> <type> Master's thesis, </type> <institution> Institut fur praktische Informatik, TU Wien, </institution> <month> February </month> <year> 1990. </year>
Reference-contexts: It has special operations for manipulating network topology and allows for parallel operations by parallel procedure calls at the network, node group, and connection level. Various proposals for network description languages, often with less clear aims, have been made by other researchers, e.g. <ref> [2, 8, 7, 9, 21] </ref>. Most of these cover only static network topologies and are not full programming languages, thus still exhibit most of the problems of hand-written implementations.
Reference: [3] <author> Andrew A. Chien. </author> <title> Concurrent Aggregates: Supporting Modularity in Massively-Parallel Programs. </title> <publisher> MIT Press Cambridge, </publisher> <address> Massachusetts, London, England, </address> <year> 1993. </year>
Reference-contexts: The same is true for neural network library approaches. Handwritten implementations in standard languages such as C or C ++ require low-level parallel programming (data distribution, message passing, work partitioning, thread management, load distribution, etc.) and typically result in non-portable programs. Finally, high-level general-purpose parallel languages (e.g. Concurrent Aggregates <ref> [3] </ref>) would be acceptable from the programming point of view, but they cannot yet be translated into sufficiently efficient parallel code. Thus we propose to use a special-purpose parallel programming language for neural network learning algorithms.
Reference: [4] <author> William Finnoff, Ferdinand Hergert, and Hans Georg Zimmermann. </author> <title> Improving model selection by nonconvergent methods. Neural Networks, </title> <address> 6:771783, </address> <year> 1993. </year>
Reference-contexts: To investigate the performance in this case, we perform some benchmarks with the autoprune learning algorithm <ref> [4] </ref>. This algorithm removes 17 some fixed fraction of the connections when overfitting is detected during training. Training then proceeds with the thus pruned network.
Reference: [5] <author> Holger Hopp and Lutz Prechelt. CuPit-2: </author> <title> A parallel language for neural algorithms: Language reference and tutorial. </title> <type> Technical Report 4/1997, </type> <institution> Fakultat fur Informatik, Universitat Karlsruhe, Germany, </institution> <month> mar </month> <year> 1997. </year>
Reference-contexts: The most advanced of the above proposals is CONNECT [8], which repre 5 sents a mostly complete programming language, but still has only incomplete support for dynamic changes of network topology. 3 CuPit-2 language overview The programming language CuPit-2 <ref> [5] </ref> views a neural network as a graph of nodes and connections. In the neural network literature, the nodes are often called units or neurons, the connections are often called links or, misleadingly, weights.
Reference: [6] <author> Marwan Jabri, Edward Tinker, and Laurens Leerink. MUME: </author> <title> An environment for multi-net and multi-architectures neural simulation. </title> <type> Technical report, </type> <institution> System Engineering and Design Automation Laboratory, University of Sydney, </institution> <address> NSW 2006, Australia, </address> <year> 1993. </year> <month> 21 </month>
Reference-contexts: Constructive algorithms make network handling very complicated for the simulation system. In this case, most users end up with handwritten programs in conventional programming languages such as C or C ++ . Sometimes, special neural network libraries such as MUME <ref> [6] </ref> or Sesame [10] may be used to simplify parts of the programming. 3 Obviously such flexibility requirements impede or even contradict ease of use.
Reference: [7] <author> Gerd Kock and Thomas Becher. </author> <title> Mind: An environment for the development, integration, and acceleration of connectionist systems. </title> <booktitle> In [1], </booktitle> <pages> pages 499504, </pages> <year> 1997. </year>
Reference-contexts: It has special operations for manipulating network topology and allows for parallel operations by parallel procedure calls at the network, node group, and connection level. Various proposals for network description languages, often with less clear aims, have been made by other researchers, e.g. <ref> [2, 8, 7, 9, 21] </ref>. Most of these cover only static network topologies and are not full programming languages, thus still exhibit most of the problems of hand-written implementations.
Reference: [8] <author> Gerd Kock and N.B. Serbedzija. </author> <title> Artificial neural networks: From compact descriptions to C++. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks, </booktitle> <year> 1994. </year>
Reference-contexts: It has special operations for manipulating network topology and allows for parallel operations by parallel procedure calls at the network, node group, and connection level. Various proposals for network description languages, often with less clear aims, have been made by other researchers, e.g. <ref> [2, 8, 7, 9, 21] </ref>. Most of these cover only static network topologies and are not full programming languages, thus still exhibit most of the problems of hand-written implementations. <p> Most of these cover only static network topologies and are not full programming languages, thus still exhibit most of the problems of hand-written implementations. The most advanced of the above proposals is CONNECT <ref> [8] </ref>, which repre 5 sents a mostly complete programming language, but still has only incomplete support for dynamic changes of network topology. 3 CuPit-2 language overview The programming language CuPit-2 [5] views a neural network as a graph of nodes and connections.
Reference: [9] <author> Russel R. Leighton. </author> <title> The Aspirin/MIGRAINES neural network software, user's manual, release v6.0. </title> <type> Technical Report MP-91W00050, </type> <institution> MITRE Corp., </institution> <month> October </month> <year> 1999. </year>
Reference-contexts: It has special operations for manipulating network topology and allows for parallel operations by parallel procedure calls at the network, node group, and connection level. Various proposals for network description languages, often with less clear aims, have been made by other researchers, e.g. <ref> [2, 8, 7, 9, 21] </ref>. Most of these cover only static network topologies and are not full programming languages, thus still exhibit most of the problems of hand-written implementations.
Reference: [10] <author> Alexander Linden and Christoph Tietz. </author> <title> Combining multiple neural network paradigms and applications using SESAME. </title> <booktitle> In Proc. of the Int. Joint Conf. on Neural Networks, </booktitle> <address> Baltimore, </address> <month> June </month> <year> 1992. </year> <note> IEEE. </note>
Reference-contexts: Constructive algorithms make network handling very complicated for the simulation system. In this case, most users end up with handwritten programs in conventional programming languages such as C or C ++ . Sometimes, special neural network libraries such as MUME [6] or Sesame <ref> [10] </ref> may be used to simplify parts of the programming. 3 Obviously such flexibility requirements impede or even contradict ease of use.
Reference: [11] <editor> NeuralWorks Reference Guide, </editor> <publisher> NeuralWare Inc. </publisher> <address> http://www.neuralware.com/. </address>
Reference-contexts: Furthermore, the software should be easy to learn. This sort of requirement is usually best handled by integrated neural network simulators such as SNNS [23], Xerion [19], NeuroGraph [22], NeuralWorks <ref> [11] </ref> etc. These programs package multiple well-known learning algorithms with data manipulation functions and interactive graphical network exploration and manipulation capabilities. They are relatively easy to learn and will often provide the fastest path to a reasonable solution of a given engineering problem.
Reference: [12] <author> Lutz Prechelt. </author> <title> CuPit a parallel language for neural algorithms: Language reference and tutorial. </title> <type> Technical Report 4/94, </type> <institution> Fakultat f ur Informatik, Universitat Karlsruhe, Germany, </institution> <note> Jan-uary 1994. Anonymous FTP: /pub/papers/techreports/1994/1994-04.ps.gz on ftp.ira.uka.de. </note>
Reference-contexts: Finally, it is possible to incorporate program parts written in other languages such as C. 4 Implementations and performance results We have implemented prototype compilers for the massively parallel MasPar MP-1/MP-2 [14, 15] (this compiler implements the language CuPit <ref> [12, 16] </ref>, which is very similar to CuPit-2), for sequential computers, and for symmetric multiprocessors (SMP).
Reference: [13] <author> Lutz Prechelt. </author> <title> PROBEN1 A set of benchmarks and benchmarking rules for neural network training algorithms. </title> <type> Technical Report 21/94, </type> <institution> Fakultat f ur Informatik, Universitat Karl-sruhe, Germany, </institution> <month> September </month> <year> 1994. </year> <note> Anonymous FTP: /pub/papers/techreports/1994/1994-21.ps.gz on ftp.ira.uka.de. </note>
Reference-contexts: The first is the nettalk problem shown above (with 203+120+26 nodes and 32758 connections initially). The other is the soybean1 data set taken from the Proben1 benchmark set <ref> [13] </ref> (with 83+16+8+19 nodes and 4153 connections initially). See [13, 17] for details of the data and algorithm setup. As we see, the nettalk problem initially shows rather low performance. As discussed above, this is because the network is too large to fit into the processor cache. <p> The first is the nettalk problem shown above (with 203+120+26 nodes and 32758 connections initially). The other is the soybean1 data set taken from the Proben1 benchmark set [13] (with 83+16+8+19 nodes and 4153 connections initially). See <ref> [13, 17] </ref> for details of the data and algorithm setup. As we see, the nettalk problem initially shows rather low performance. As discussed above, this is because the network is too large to fit into the processor cache.
Reference: [14] <author> Lutz Prechelt. </author> <title> The CuPit compiler for the MasPar a literate programming document. </title> <type> Technical Report 1/95, </type> <institution> Fakultat f ur Informatik, Universitat Karlsruhe, Germany, </institution> <month> January </month> <year> 1995. </year> <month> ftp.ira.uka.de. </month>
Reference-contexts: Finally, it is possible to incorporate program parts written in other languages such as C. 4 Implementations and performance results We have implemented prototype compilers for the massively parallel MasPar MP-1/MP-2 <ref> [14, 15] </ref> (this compiler implements the language CuPit [12, 16], which is very similar to CuPit-2), for sequential computers, and for symmetric multiprocessors (SMP).
Reference: [15] <author> Lutz Prechelt. </author> <title> Data locality and load balancing for parallel neural network learning. </title> <editor> In Emilio Zapata, editor, </editor> <booktitle> Proc. Fifth Workshop on Compilers for Parallel Computers, </booktitle> <pages> pages 22 111127, </pages> <address> Malaga, Spain, </address> <month> June 28-31, </month> <year> 1995. </year> <institution> Dept. of Computer Architecture, University of Malaga, UMA-DAC-95/09. </institution>
Reference-contexts: Finally, it is possible to incorporate program parts written in other languages such as C. 4 Implementations and performance results We have implemented prototype compilers for the massively parallel MasPar MP-1/MP-2 <ref> [14, 15] </ref> (this compiler implements the language CuPit [12, 16], which is very similar to CuPit-2), for sequential computers, and for symmetric multiprocessors (SMP).
Reference: [16] <author> Lutz Prechelt. </author> <title> A parallel programming model for irregular dynamic neural networks. In W.K. Giloi, </title> <editor> S. Jahnichen, and B.D. Shriver, editors, </editor> <booktitle> Proc. Programming Models for Massively Parallel Computers, </booktitle> <address> page ?, Berlin, Germany, </address> <month> October </month> <year> 1995. </year> <note> GMD First, IEEE CS Press. by accident the article was not printed in the proceedings volume, but see http://wwwipd.ira.uka.de/ prechelt/Biblio. </note>
Reference-contexts: Finally, it is possible to incorporate program parts written in other languages such as C. 4 Implementations and performance results We have implemented prototype compilers for the massively parallel MasPar MP-1/MP-2 [14, 15] (this compiler implements the language CuPit <ref> [12, 16] </ref>, which is very similar to CuPit-2), for sequential computers, and for symmetric multiprocessors (SMP).
Reference: [17] <author> Lutz Prechelt. </author> <title> Connection pruning with static and adaptive pruning schedules. </title> <address> Neurocom-puting, 16:4961, </address> <year> 1997. </year>
Reference-contexts: The first is the nettalk problem shown above (with 203+120+26 nodes and 32758 connections initially). The other is the soybean1 data set taken from the Proben1 benchmark set [13] (with 83+16+8+19 nodes and 4153 connections initially). See <ref> [13, 17] </ref> for details of the data and algorithm setup. As we see, the nettalk problem initially shows rather low performance. As discussed above, this is because the network is too large to fit into the processor cache.
Reference: [18] <author> Martin Riedmiller and Heinrich Braun. </author> <title> A direct adaptive method for faster backpropagation learning: The RPROP algorithm. </title> <booktitle> In Proc. of the IEEE Intl. Conf. on Neural Networks, </booktitle> <pages> pages 586591, </pages> <address> San Francisco, CA, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: If there is more than one processor per network replicate, node parallelism will be used. Our results show performance variation depending on the size of the network: Example parallelism is bad for networks larger than the cache, see Figure 2. The figure shows RPROP <ref> [18] </ref> performance expressed Fig. 2 !!! in Million Connection Updates Per Second (MCUPS) for a large network (SNNS version of nettalk, 203+120+26 nodes, 27480 connections, 200 patterns).
Reference: [19] <author> Drew van Camp. </author> <title> A users guide for the Xerion neural network simulator version 3.1. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Toronto, Toronto, Canada, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Furthermore, the software should be easy to learn. This sort of requirement is usually best handled by integrated neural network simulators such as SNNS [23], Xerion <ref> [19] </ref>, NeuroGraph [22], NeuralWorks [11] etc. These programs package multiple well-known learning algorithms with data manipulation functions and interactive graphical network exploration and manipulation capabilities. They are relatively easy to learn and will often provide the fastest path to a reasonable solution of a given engineering problem.
Reference: [20] <author> Sven Wahle. </author> <title> Untersuchungen zur Parallelisierbarkeit von Backpropagation-Netzwerken auf dem CNAPS Neurocomputer. </title> <type> Master's thesis, </type> <institution> Fakultat f ur Informatik, Universitat Karlsruhe, Germany, </institution> <month> October </month> <year> 1994. </year>
Reference-contexts: Not all kinds of learning algorithms can be implemented using given matrix libraries or special-purpose hardware, the latter may also suffer from problems with computational precision for some problem domains <ref> [20] </ref>. On the other hand, general-purpose parallel processors are quite flexible, yet are typically difficult to program. 3. Flexibility.
Reference: [21] <editor> Alfredo Weitzenfeld. </editor> <booktitle> NSL neural simulation language. In Proceedings of the International Workshop on Artificial Neural Networks, number 930 in LNCS, </booktitle> <pages> pages 683688, </pages> <address> Malaga-Torremolinos, Spain, June 1995. </address> <publisher> Springer. </publisher>
Reference-contexts: It has special operations for manipulating network topology and allows for parallel operations by parallel procedure calls at the network, node group, and connection level. Various proposals for network description languages, often with less clear aims, have been made by other researchers, e.g. <ref> [2, 8, 7, 9, 21] </ref>. Most of these cover only static network topologies and are not full programming languages, thus still exhibit most of the problems of hand-written implementations.
Reference: [22] <author> Peter Wilke and Christian Jacob. </author> <title> The NeuroGraph neural network simulator. </title> <booktitle> In Proceedings of MASCOTS'93, </booktitle> <address> San Diego, CA, </address> <year> 1993. </year> <month> 23 </month>
Reference-contexts: Furthermore, the software should be easy to learn. This sort of requirement is usually best handled by integrated neural network simulators such as SNNS [23], Xerion [19], NeuroGraph <ref> [22] </ref>, NeuralWorks [11] etc. These programs package multiple well-known learning algorithms with data manipulation functions and interactive graphical network exploration and manipulation capabilities. They are relatively easy to learn and will often provide the fastest path to a reasonable solution of a given engineering problem.

References-found: 22

