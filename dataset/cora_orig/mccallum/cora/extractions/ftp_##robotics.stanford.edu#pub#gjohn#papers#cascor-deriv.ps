URL: ftp://robotics.stanford.edu/pub/gjohn/papers/cascor-deriv.ps
Refering-URL: http://www.robotics.stanford.edu/~gjohn/pubs.html
Root-URL: http://www.robotics.stanford.edu
Email: gjohn@cs.Stanford.EDU  
Title: Cascade Correlation: Derivation of a More Numerically Stable Update Rule  
Author: George H. John 
Note: In International Conference on Neural Networks, Perth, Western Australia, 1995, IEEE Press.  
Web: http://robotics.stanford.edu/~gjohn  
Address: Stanford, CA 94305  
Affiliation: Computer Science Department Stanford University  
Abstract: We discuss the weight update rule in the Cascade Correlation neural net learning algorithm. The weight update rule implements gradient descent optimization of the correlation between a new hidden unit's output and the previous network's error. We present a derivation of the gradient of the correlation function and show that our resulting weight update rule results in slightly faster training. We also show that the new rule is mathematically equivalent to the one presented in the original Cascade Correlation paper and discuss numerical issues underlying the difference in performance. Since a derivation of the Cascade Correlation weight update rule was not published, this paper should be useful to those who wish to understand the rule. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Fahlman, S. E. </author> <year> (1988), </year> <title> An empirical study of learning speed in back-propagation networks, </title> <type> Technical Report CMU-CS-88-162, </type> <institution> Carnegie Mel-lon University. </institution>
Reference-contexts: When new hidden units are added, their weights are repeatedly adjusted to maximize this objective function using some optimization algorithm requiring first-order derivative information. In fact, the algorithm uses either gradient descent or quickprop, an optimization method borrowing ideas from Gauss-Newton methods <ref> (Fahlman 1988, Gill, Murray & Wright 1981) </ref>. To use these methods, one must first mathematically derive the equations for the first derivatives of the objective function with respect to the parameters (the weights, in this case). <p> Numerical errors in calculating the derivative of the objective function do not accumulate but rather are overcome by the deviations of the objective function's surface itself from the linear model assumed by the gradient descent algorithm. More aggressive optimization algorithms, such as Quick-prop <ref> (Fahlman 1988) </ref>, might be more sensitive to errors in the gradient, and might be more improved by using Equation 6. 5.
Reference: <author> Fahlman, S. E. & Lebiere, C. </author> <year> (1990), </year> <title> The cascade-correlation learning architecture, </title> <editor> in D. Touret-zky, ed., </editor> <booktitle> "Advances in Neural Information Processing Systems", </booktitle> <volume> Vol. 2, </volume> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Gill, P. E., Murray, W. & Wright, M. H. </author> <year> (1981), </year> <title> Practical Optimization, </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference: <author> McKenna, S. J., Ricketts, I. W., Cairns, A. Y. & Hussein, K. A. </author> <year> (1992), </year> <title> Cascade-correlation neural networks for the classification of cer-vical cells, </title> <booktitle> in "IEE Colloquium on Neual Networks for image processing applications", IEE, </booktitle> <address> London, UK, </address> <pages> pp. 51-54. </pages>
Reference-contexts: This frees the user from having to guess at a good network topology. Because of this freedom, and because of the computational strengths of Cascade Correlation, it has begun to enjoy widespread use in applications <ref> (McKenna, Ricketts, Cairns & Hussein 1992, Yamamoto & Zenios 1993) </ref>. At the core of the algorithm is the novel objective function used to train new hidden units, the correlation function.
Reference: <author> Murphy, P. M. & Aha, D. W. </author> <year> (1994), </year> <note> "UCI repository of machine learning databases", Available by anonymous ftp to ics.uci.edu in the pub/machine-learning-databases directory. </note>
Reference: <author> Neely, P. M. </author> <year> (1966), </year> <title> "Comparison of several algorithms for computation of means and correlation coefficients", </title> <journal> Communications of the ACM 9, </journal> <pages> 496-499. </pages>
Reference: <author> White, M. </author> <year> (1995), </year> <title> "Cascade correlation learning algorithm v1.2", </title> <note> Available by ftp to ftp.cs.cmu.edu in the /afs/cs/project/ connect/code/supported directory. </note>
Reference: <author> White, M. & Fahlman, S. E. </author> <year> (1994), </year> <title> "Neural-bench benchmark collection", </title> <note> Available by anonymous ftp to ftp.cs.cmu.edu in the /afs/cs/project/connect/bench directory. </note>
Reference: <author> Yamamoto, Y. & Zenios, S. A. </author> <year> (1993), </year> <title> "Predicting prepayment rates for mortgate-backed securities using the cascade-correlation learning algorithm", </title> <journal> The Journal of Fixed Income 2(4), </journal> <pages> 86-96. </pages>
References-found: 9

