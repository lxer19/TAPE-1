URL: http://theory.lcs.mit.edu/~andrews/stoc96.ps
Refering-URL: http://theory.lcs.mit.edu/~andrews/
Root-URL: 
Email: Email: ftl@math.mit.edu.  Email: pmetaxas@wellesley.edu.  Email: ylz@math.mit.edu.  
Title: Automatic Methods for Hiding Latency in High Bandwidth Networks (Extended Abstract)  using n=  
Author: Matthew Andrews Tom Leighton P. Takis Metaxas Lisa Zhang p andrews@ math.mit.edu. 
Keyword: with unit link delays using slowdown O(d  ave log 5=3 n)-node array with average link delay d ave  
Affiliation: Department of Mathematics and Laboratory for Computer Science, MIT.  Department of Mathematics and Laboratory for Computer Science, MIT.  Department of Computer Science, Wellesley College.  Department of Mathematics and Laboratory for Computer Science, MIT.  
Note: steps  2=3 on an (n 2 d 2=3  Supported by NSF contract 9302476-CCR and ARPA contract N00014-95-1-1246. Email:  Supported by ARMY grant DAAH04-95-1-0607 and ARPA contract N00014-95-1-1246.  Supported by NSF contract 9504421-CCR and ARPA contract N00014-95-1-1246.  Supported by an NSF graduate fellowship and ARPA contract N00014-95-1-1246.  
Abstract: In this paper we describe methods for mitigating the degradation in performance caused by high latencies in parallel and distributed networks. Our approach is similar in spirit to the "complementary slackness" technique for latency hiding but has the advantage that the slackness does not need to be provided by the programmer and that large slowdowns are not needed in order to hide the latency. For example, given any algorithm that runs in T steps on an n-node ring with unit link delays, we show how to run the algorithm in O(T ) steps on any n-node bounded-degree connected network with average link delay O(1). This is a significant improvement over prior approaches to latency hiding, which require slowdowns proportional to the maximum link delay (which can be quite large in comparison to the average delay). In the case when the network has aver age link delay d ave , our simulation runs in O( d ave processors, thereby preserving efficiency. We also show how to simulate an n fi n array We anticipate that our results will be of interest in the context of parallel and distributed computing on networks of workstations (NOWs). NOWs typically have the luxury of very high bandwidth links but suffer from latency problems caused by long wires or delays in accessing the network. Our work suggests an approach for overcoming such problems in a way that can be made transparent to the programmer (e.g., by making the network appear to function as if it were comprised of low-latency links). p
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> The Connection Machine CM-5 Technical Summary. Thinking Machines Corporation, </institution> <address> 245 First Street, Cambridge, MA 02154-1264, </address> <year> 1991. </year>
Reference-contexts: Such an approach is clearly less than desirable in the context of a NOW with high-latency links. An alternative approach is to organize the network in a hierarchical fashion so that the latencies are consistent with the hierarchy. For example, in the CM-5 <ref> [10, 1] </ref> the highest latency links are segregated into the top levels of the network hierarchy. This type of architecture works well for applications in which most of the computation is local since local computation can proceed using the low-level low-latency links.
Reference: [2] <author> M. Andrews, T. Leighton, P. T. Metaxas, and L. Zhang. </author> <title> Improved methods for hiding latency in networks of workstations. </title> <note> Submitted to SPAA, </note> <year> 1996. </year>
Reference-contexts: This model is sufficiently general so as to be applicable for many algorithms, such as matrix operations, Fourier transforms, etc. However, it does not include applications where the computation performed by p depends on the state of a potentially large local memory or database. In <ref> [2] </ref> we consider a different model which handles this situation. In this new model we show how to simulate a linear array with unit delay links using slowdown O p on an n-node network with average link delay d ave .
Reference: [3] <author> Y. Aumann and M. Ben-Or. </author> <title> Computing with faulty arrays. </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 162-169, </pages> <year> 1992. </year>
Reference-contexts: The technique also forms a critical component of Valiant's bulk synchronous model of parallel computing [15, 16] and it has been employed in several algorithms papers <ref> [11, 7, 3, 12, 6] </ref>. In all of the preceding examples, however, it is incumbent on the programmer to provide the slackness or pipelining needed to overcome the latencies in the network.
Reference: [4] <author> G. E. Blelloch, S. Chatterjee, J. C. Hardwick, J. Sipelstein, and M. Zagha. </author> <title> Implementation of a portable nested data-parallel language. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming PPoPP, </booktitle> <address> San Diego, CA, </address> <pages> pages 102-112. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: Processors on the HEP machine [13] swapped between unrelated threads while waiting for the data. The CM-1 and CM-2 were designed to emulate much larger virtual machines so that a single processor would perform the computation of many virtual processors <ref> [14, 4] </ref>. The technique also forms a critical component of Valiant's bulk synchronous model of parallel computing [15, 16] and it has been employed in several algorithms papers [11, 7, 3, 12, 6].
Reference: [5] <author> R. Cole, B. Maggs, and R. Sitaraman. </author> <title> Multi-scale self-simulation: A technique for reconfiguring arrays with faults. </title> <booktitle> In Proceedings of the 25th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 561-572, </pages> <year> 1993. </year>
Reference-contexts: Redundant computation is another approach that has been used to hide the effects of latency. Here the idea is to avoid latency by recomputing data locally instead of waiting to receive it through a high-latency link. Although this approach has worked well in some special examples <ref> [9, 5] </ref> we have found that is is not helpful in the scenarios we consider in this paper. Perhaps the best and most generally applicable method of hiding latency is the approach known as complementary slackness.
Reference: [6] <author> C. Kaklamanis, A. R. Karlin, F. T. Leighton, V. Milenkovic, P. Raghavan, S. Rao, C. Thom-borson, and A. Tsantilas. </author> <title> Asymptotically tight bounds for computing with faulty arrays of processors. </title> <booktitle> In Proceedings of the 31st Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 285-296, </pages> <year> 1990. </year>
Reference-contexts: The technique also forms a critical component of Valiant's bulk synchronous model of parallel computing [15, 16] and it has been employed in several algorithms papers <ref> [11, 7, 3, 12, 6] </ref>. In all of the preceding examples, however, it is incumbent on the programmer to provide the slackness or pipelining needed to overcome the latencies in the network.
Reference: [7] <author> R. Koch, T. Leighton, B. Maggs, S. Rao, and A. Rosenberg. </author> <title> Work-preserving emulations of fixed-connection networks. </title> <booktitle> In Proceedings of the 21st Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 227-240, </pages> <year> 1989. </year>
Reference-contexts: The technique also forms a critical component of Valiant's bulk synchronous model of parallel computing [15, 16] and it has been employed in several algorithms papers <ref> [11, 7, 3, 12, 6] </ref>. In all of the preceding examples, however, it is incumbent on the programmer to provide the slackness or pipelining needed to overcome the latencies in the network.
Reference: [8] <author> T. Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays * Trees * Hypercubes. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1992. </year>
Reference-contexts: The following fact <ref> [8] </ref> is used in our argument. Fact 7 An n-node linear array can be one-to-one em bedded with dilation 3 in any connected n-node network. Let N be the n-node linear array embedded in N by Fact 7. <p> The following fact [8] is used in our argument. Fact 7 An n-node linear array can be one-to-one em bedded with dilation 3 in any connected n-node network. Let N be the n-node linear array embedded in N by Fact 7. The proof of Fact 3 <ref> [8, page 470] </ref> implies that if N has bounded degree ffi then N has average delay at most ffid ave . By Corollary 4, N can simulate G with a slowdown of O ( p ffid ave ). <p> Hence, the slowdown is at least maxf p n=m; mg n 1=4 , whereas the average delay is a constant. Notice that all the results in this section apply to rings, since a linear array can simulate a ring with a slowdown of 2 <ref> [8] </ref>. 3 Two Dimensional Arrays 3.1 The Analogue of the One Dimensional Case Let the guest network G be an nfin 2-dimensional array with unit delay on all the edges. Let the host network H be an n fi n 2-dimensional array with arbitrary delays.
Reference: [9] <author> T. Leighton, B. Maggs, and R. Sitaraman. </author> <title> On the fault tolerance of some popular bounded-degree networks. </title> <booktitle> In Proceedings of the 33rd Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 542-552, </pages> <year> 1992. </year>
Reference-contexts: Redundant computation is another approach that has been used to hide the effects of latency. Here the idea is to avoid latency by recomputing data locally instead of waiting to receive it through a high-latency link. Although this approach has worked well in some special examples <ref> [9, 5] </ref> we have found that is is not helpful in the scenarios we consider in this paper. Perhaps the best and most generally applicable method of hiding latency is the approach known as complementary slackness.
Reference: [10] <author> C. E. Leiserson, Z. Abuhamdeh, D. Douglas, C. Feynman, M. Ganmukhi, J. Hill, D. Hillis, B. Kuszmaul, M. St. Pierre, D. Wells, M. Wong, S. Yang, and R. Zak. </author> <title> The network architecture of the connection machine CM-5. </title> <booktitle> In Proceedings of the 4th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 272-285, </pages> <year> 1992. </year>
Reference-contexts: Such an approach is clearly less than desirable in the context of a NOW with high-latency links. An alternative approach is to organize the network in a hierarchical fashion so that the latencies are consistent with the hierarchy. For example, in the CM-5 <ref> [10, 1] </ref> the highest latency links are segregated into the top levels of the network hierarchy. This type of architecture works well for applications in which most of the computation is local since local computation can proceed using the low-level low-latency links.
Reference: [11] <author> C. E. Leiserson, S. Rao, and S. Toledo. </author> <title> Efficient out-of-core algorithms for linear relaxation using blocking covers. </title> <booktitle> In Proceedings of the 34th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 704-713, </pages> <year> 1993. </year>
Reference-contexts: The technique also forms a critical component of Valiant's bulk synchronous model of parallel computing [15, 16] and it has been employed in several algorithms papers <ref> [11, 7, 3, 12, 6] </ref>. In all of the preceding examples, however, it is incumbent on the programmer to provide the slackness or pipelining needed to overcome the latencies in the network.
Reference: [12] <author> M. O. Rabin. </author> <title> Efficient dispersal of information for security, load balancing and fault tolerance. </title> <journal> Journal of the ACM, </journal> <volume> 36(2) </volume> <pages> 335-348, </pages> <year> 1989. </year>
Reference-contexts: The technique also forms a critical component of Valiant's bulk synchronous model of parallel computing [15, 16] and it has been employed in several algorithms papers <ref> [11, 7, 3, 12, 6] </ref>. In all of the preceding examples, however, it is incumbent on the programmer to provide the slackness or pipelining needed to overcome the latencies in the network.
Reference: [13] <author> B. J. Smith. </author> <title> Architecture and applications of the HEP multiprocessor computer system. </title> <booktitle> In SPIE, </booktitle> <pages> pages 298 241-248, </pages> <year> 1981. </year>
Reference-contexts: There are many implementations and incarnations of this method. For example, each processor in the CRAY YMP C-90 keeps busy by operating on a pipeline of 128 64-bit words. Processors on the HEP machine <ref> [13] </ref> swapped between unrelated threads while waiting for the data. The CM-1 and CM-2 were designed to emulate much larger virtual machines so that a single processor would perform the computation of many virtual processors [14, 4].
Reference: [14] <author> Lewis W. Tucker and George G. Robertson. </author> <title> Architecture and applications of the connection machine. </title> <journal> Computer, </journal> <volume> 21(8) </volume> <pages> 26-38, </pages> <year> 1988. </year>
Reference-contexts: Processors on the HEP machine [13] swapped between unrelated threads while waiting for the data. The CM-1 and CM-2 were designed to emulate much larger virtual machines so that a single processor would perform the computation of many virtual processors <ref> [14, 4] </ref>. The technique also forms a critical component of Valiant's bulk synchronous model of parallel computing [15, 16] and it has been employed in several algorithms papers [11, 7, 3, 12, 6].
Reference: [15] <author> L. G. Valiant. </author> <title> Bulk-synchronous parallel computers. </title> <type> Technical report TR-08-89, </type> <institution> Center for Research in Computing Technology, Harvard University, </institution> <year> 1989. </year>
Reference-contexts: The CM-1 and CM-2 were designed to emulate much larger virtual machines so that a single processor would perform the computation of many virtual processors [14, 4]. The technique also forms a critical component of Valiant's bulk synchronous model of parallel computing <ref> [15, 16] </ref> and it has been employed in several algorithms papers [11, 7, 3, 12, 6]. In all of the preceding examples, however, it is incumbent on the programmer to provide the slackness or pipelining needed to overcome the latencies in the network.
Reference: [16] <author> L. G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <year> 1990. </year>
Reference-contexts: The CM-1 and CM-2 were designed to emulate much larger virtual machines so that a single processor would perform the computation of many virtual processors [14, 4]. The technique also forms a critical component of Valiant's bulk synchronous model of parallel computing <ref> [15, 16] </ref> and it has been employed in several algorithms papers [11, 7, 3, 12, 6]. In all of the preceding examples, however, it is incumbent on the programmer to provide the slackness or pipelining needed to overcome the latencies in the network.
References-found: 16

