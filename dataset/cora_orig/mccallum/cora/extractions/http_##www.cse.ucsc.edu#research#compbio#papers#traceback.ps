URL: http://www.cse.ucsc.edu/research/compbio/papers/traceback.ps
Refering-URL: http://www.cse.ucsc.edu/research/compbio/sam.html
Root-URL: http://www.cse.ucsc.edu
Title: Reduced Space Sequence Alignment  
Author: J Alicia Grice Richard Hughey Don Speck REPRINT CABIOS ():-, 
Address: Santa Cruz, CA 95064.  
Affiliation: Computer Engineering, University of California,  
Abstract: 1 Abstract 
Abstract-found: 1
Intro-found: 1
Reference: <author> Edmiston, E. W., Core, N. G., Saltz, J. H., & Smith, R. M. </author> <year> (1988). </year> <title> Parallel processing of biological sequence comparison algorithms. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17 (3), </volume> <pages> 259-275. </pages>
Reference-contexts: Minimizing over all members of the column will produce a point through which the optimal alignment will pass. This enables the division of the problem into two subproblems of combined size n 2 =2, which can then be solved recursively. Edmiston, Core, Saltz, and Smith <ref> (Edmiston et al., 1988) </ref> proposed an extension to Hirschberg's algorithm for use on parallel processors by dividing the problem into H segments rather than Hirschberg's original two segments.
Reference: <author> Gotoh, O. </author> <year> (1982). </year> <title> An improved algorithm for matching biological sequences. </title> <journal> J. Mol. Biol. </journal> <volume> 162 (3), </volume> <pages> 705-708. </pages>
Reference-contexts: Sequence comparison using affine gap penalties involves three interconnected recurrences of a similar form <ref> (Gotoh, 1982) </ref>. The extra cost for starting a sequence of insertions or deletions will, for example, make the second alignment of Figure 1 preferred over the alignment with four gaps.
Reference: <author> Gribskov, M., Luthy, R., & Eisenberg, D. </author> <year> (1990). </year> <title> Profile analysis. </title> <booktitle> Methods in Enzymology, </booktitle> <volume> 183, </volume> <pages> 146-159. </pages>
Reference: <author> Hirschberg, D. S. </author> <year> (1975). </year> <title> A linear space algorithm for computing maximal common subsequences. </title> <journal> Communications of the ACM, </journal> <volume> 18 (6), </volume> <pages> 341-343. </pages>
Reference-contexts: The algorithms presented in this paper will enable the new parallel processor, called Kestrel, to perform sequence 2 alignment and hidden Markov model (HMM) training despite having only a tiny 256 bytes of local memory per processing element (Hirschberg et al., 1996). 2.1 Related Work Hirschberg <ref> (Hirschberg, 1975) </ref> discovered the first linear-space algorithm for sequence alignment, which Myers and Miller then popularized and extended (Myers & Miller, 1988).
Reference: <author> Hirschberg, J. D., Hughey, R., Karplus, K., & Speck, D. </author> <year> (1996). </year> <title> Kestrel: A programmable array for sequence analysis. </title> <booktitle> In: Proc. Int. Conf. Application Specific Array Processors pp. </booktitle> <pages> 25-34, </pages> <address> Los Alamitos, CA: </address> <publisher> IEEE CS. </publisher>
Reference-contexts: The algorithms presented in this paper will enable the new parallel processor, called Kestrel, to perform sequence 2 alignment and hidden Markov model (HMM) training despite having only a tiny 256 bytes of local memory per processing element <ref> (Hirschberg et al., 1996) </ref>. 2.1 Related Work Hirschberg (Hirschberg, 1975) discovered the first linear-space algorithm for sequence alignment, which Myers and Miller then popularized and extended (Myers & Miller, 1988).
Reference: <author> Huang, X. </author> <year> (1989). </year> <title> A space-efficient parallel sequence comparison algorithm for a message-passing multiprocessor. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 18 (3), </volume> <pages> 223-239. </pages>
Reference-contexts: Edmiston, Core, Saltz, and Smith (Edmiston et al., 1988) proposed an extension to Hirschberg's algorithm for use on parallel processors by dividing the problem into H segments rather than Hirschberg's original two segments. Huang <ref> (Huang, 1989) </ref> further improved the parallelization by noting that if one partitions along a pair of diagonals rather than a column, the problem will be reduced to equally-sized subproblems, a critical issue in creating a load-balanced parallel algorithm.
Reference: <author> Hughey, R. & Krogh, A. </author> <year> (1996). </year> <title> Hidden Markov models for sequence analysis: Extension and analysis of the basic method. </title> <journal> CABIOS, </journal> <volume> 12 (2), </volume> <pages> 95-107. </pages>
Reference-contexts: Experimental evaluation will be able to optimize the method for any architecture's memory hierarchy. The algorithms are particularly appropriate to hidden Markov model systems such as SAM <ref> (Hughey & Krogh, 1996) </ref>. SAM's core forward-backward (Baum-Welch) training phase requires information about all possible alignments of each sequence to the model, rather than just the best alignment.
Reference: <author> Ibarra, O. H., Jiang, T., & Wang, H. </author> <year> (1992). </year> <title> String editing on a one-way linear array of finite-state machines. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41 (1), </volume> <pages> 112-118. </pages>
Reference-contexts: Ibarra, Jiang, and Wang solved many of these problems for performing sequence alignment on a one-way linear array of finite state machines, a restrictive, and hence realistic, machine model in which data can only flow from left to right <ref> (Ibarra et al., 1992) </ref>. Their parallelization of Hirschberg's algorithm balances the computation by performing problem division on the larger of the two dimensions of the dynamic programming sub-matrix during any given recursive call.
Reference: <author> Krogh, A., Brown, M., Mian, I. S., Sjolander, K., & Haussler, D. </author> <year> (1994). </year> <title> Hidden Markov models in computational biology: Applications to protein modeling. </title> <journal> J. Mol. Biol. </journal> <volume> 235, </volume> <pages> 1501-1531. </pages>
Reference-contexts: The most computationally-intensive step of the former is a sequence of training iterations that uses an alignment-like calculation on the dynamic programming matrix that requires a summation of all possible paths through the dynamic programming matrix, adding and multiplying probabilities rather than minimizing and adding costs <ref> (Krogh et al., 1994) </ref>.
Reference: <author> Masek, W. J. & Paterson, M. S. </author> <year> (1983). </year> <title> How to compute string-edit distances quickly. In: Time Warps, String Edits, and Macromolecules: The Theory and Practice of Sequence Comparison pp. </title> <address> 337-349. </address> <publisher> Addison-Wesley Reading, </publisher> <address> MA. </address>
Reference-contexts: c i;j1 + dist (; b j ) delete; 1 Masek and Paterson describe an O (n 2 = log n) algorithm for strings of equal length from a finite alphabet with restrictions on the cost function, but it has a large constant factor and is not amenable to parallelization <ref> (Masek & Paterson, 1983) </ref>. 1 T R A C E B A C K B C T A K 6 6 6 6 I 6 I 6 6 6 6 I 6 6 6 I 6 6 6 I I I I I T R ACE B A C K TRACE
Reference: <author> Myers, E. W. & Miller, W. </author> <year> (1988). </year> <title> Optimal alignments in linear space. </title> <journal> CABIOS, </journal> <volume> 4 (1), </volume> <pages> 11-17. </pages>
Reference-contexts: to perform sequence 2 alignment and hidden Markov model (HMM) training despite having only a tiny 256 bytes of local memory per processing element (Hirschberg et al., 1996). 2.1 Related Work Hirschberg (Hirschberg, 1975) discovered the first linear-space algorithm for sequence alignment, which Myers and Miller then popularized and extended <ref> (Myers & Miller, 1988) </ref>. In reducing space from O (n 2 ) to O (n), these algorithms introduce a small constant (1.8 for My-ers and Miller) slowdown to the O (n 2 ) time algorithm.
Reference: <author> Needleman, S. B. & Wunsch, C. D. </author> <year> (1970). </year> <title> A general method applicable to the search for similarities in the amino acid sequences of two proteins. </title> <journal> J. Mol. Biol. </journal> <volume> 48, </volume> <pages> 443-453. </pages>
Reference-contexts: Good alignments and sequence comparisons come from solutions of an appropriately chosen optimization problem. The problem formulation defines a set of edit primitives, including match, insert, and delete, and assigns them values or costs in a distance function. Optimization then maximizes the total match value <ref> (Needleman & Wunsch, 1970) </ref> or minimizes the total cost of mismatches and insert and delete gaps. Dynamic programming organizes sequence comparison by comparing shorter subsequences first, so their costs can be made available in a table (Figure 1) for the next longer subsequence comparisons.
Reference: <author> Sellers, P. H. </author> <year> (1974). </year> <title> On the theory and computation of evolutionary distances. </title> <journal> SIAM J. Appl. Math. </journal> <volume> 26, </volume> <pages> 787-793. </pages>
Reference-contexts: 6 6 I 6 6 6 I I I I I T R ACE B A C K TRACE B A C K B A C K T R A C - K I 6 c i1;j1 c i;j1 cost edit of "BACKTRACK" into "TRACEBACK" using Sellers' evolutionary distance metric <ref> (Sellers, 1974) </ref>. Below the dynamic programming table are two possible alignments and an illustration of the data dependencies.
Reference: <author> Smith, T. F. & Waterman, M. S. </author> <year> (1981). </year> <title> Identification of common molecular subsequences. </title> <journal> J. Mol. Biol. </journal> <volume> 147, </volume> <pages> 195-197. 10 </pages>
Reference-contexts: ; b j ); c I i1;j + g M!I ; c I c D i1;j + g D!I ) + dist (a i ; ); c D i;j1 + g M!D ; c I c D i;j1 + g D!D ) + dist (; b j ): Local alignment <ref> (Smith & Waterman, 1981) </ref>, which finds the most similar subsequences of two sequences, adds a fourth, constant term to each minimization (such as zero) representing the cost of starting the correspondence at an internal (i; j) pair, in which case highly similar regions must, for the equations above, have negative cost.
References-found: 14

