URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/umsi-98-10.ps.gz
Refering-URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/
Root-URL: http://www.cs.umn.edu
Title: Inexact Newton Preconditioning Techniques for Eigenvalue Problems  
Author: Kesheng Wu Yousef Saad Andreas Stathopoulos 
Date: March 11, 1998  
Abstract: The focus of this paper is on numerical methods for finding a few eigenvalues and eigenvectors of a large sparse matrix. New preconditioning schemes are proposed for improving the effectiveness of a few methods for computing eigenvalues and eigenvec-tors. The basic framework of the preconditioned eigenvalue methods we consider is that of the Arnoldi method and the related Davidson method. Within this framework, it is possible to unravel new and more effective alternatives by varying the right-hand side and the matrix of the preconditioning equation. This paper first studies the effects of selecting various such right-hand sides. These comparisons, indicate that that a scheme based on the inexact-Newton method outperforms the others. We further study a number of Newton schemes for eigenvalue problems and test their potential as preconditioners. The experiments reveal that two schemes related to the Newton preconditioning can constitute good alternatives to other commonly used schemes.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> O. Axelsson and P. S. Vassilevski. </author> <title> A survey of multilevel preconditioned iterative methods. </title> <journal> BIT, </journal> <volume> 29(4), </volume> <year> 1989. </year>
Reference-contexts: Research on preconditioners for linear systems continues to be very active <ref> [1, 3, 22, 28, 37, 55] </ref>. <p> Research on preconditioning linear systems focuses on generating an approximate solution z of this preconditioning equation. Some of the common strategies include applying iterative methods on the preconditioning equation [2, 19, 35], constructing an approximation to A that is easy to invert <ref> [1, 3, 37, 55] </ref>, and constructing an approximation to the inverse of A [9, 20, 22]. This paper will attempt to exploit results of this research and apply them to precondition eigenvalue problems. <p> We used each one of them to form an standard eigenvalue problem, then compute 5 smallest eigenvalues and the corresponding eigenvectors of each problem. All methods start with the same initial guess, a vector of all ones: <ref> [1; 1; : : : ; 1] </ref> T . Only one initial guess is used so that comparisons can be made with ARPACK [44] to validate the implementation of the methods. <p> The largest negative eigenvalue is 48; 501, the smallest positive eigenvalue is 0:068. There are 28 well separated negative eigenvalues and 160 eigenvalues between zero and one. The condition number of the matrix is 10 10 . The initial guess used in all tests is <ref> [1; 1; : : : ; 1] </ref> T . The experiments are carried on using matlab 3 which is internally using 64-bit IEEE floating-point arithmetic. Table 5 shows the Ritz values and their corresponding residual norms computed during iterations of the constrained Newton recurrence.
Reference: [2] <author> O. Axelsson and P. S. Vassilevski. </author> <title> A block generalized Conjugate Gradient solver with inner iterations and variable step preconditioning. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 12, </volume> <year> 1991. </year>
Reference-contexts: We refer to this auxiliary equation as the preconditioning equation. Research on preconditioning linear systems focuses on generating an approximate solution z of this preconditioning equation. Some of the common strategies include applying iterative methods on the preconditioning equation <ref> [2, 19, 35] </ref>, constructing an approximation to A that is easy to invert [1, 3, 37, 55], and constructing an approximation to the inverse of A [9, 20, 22]. This paper will attempt to exploit results of this research and apply them to precondition eigenvalue problems. <p> We apply Conjugate Gradient (CG) method to solve Equations (1), (2), (7), and (15). This is how the Jacobi-Davidson method is usually implemented. It is similar to the inner-outer iteration schemes for linear systems <ref> [2, 19, 35, 54] </ref>. For simplicity, a maximum of 100 steps are allowed for each call to the CG routine. The CG routine is stopped when the residual norm has decreased by a factor of 10 4 .
Reference: [3] <author> Owe Axelsson. </author> <title> A survey of preconditioned iterative methods for linear systems of equations. </title> <journal> BIT, </journal> <volume> 25 </volume> <pages> 166-187, </pages> <year> 1985. </year>
Reference-contexts: Research on preconditioners for linear systems continues to be very active <ref> [1, 3, 22, 28, 37, 55] </ref>. <p> Research on preconditioning linear systems focuses on generating an approximate solution z of this preconditioning equation. Some of the common strategies include applying iterative methods on the preconditioning equation [2, 19, 35], constructing an approximation to A that is easy to invert <ref> [1, 3, 37, 55] </ref>, and constructing an approximation to the inverse of A [9, 20, 22]. This paper will attempt to exploit results of this research and apply them to precondition eigenvalue problems.
Reference: [4] <author> Owe Axelsson. </author> <title> Iterative Solution Methods. </title> <institution> Press Syndicate of the University of Cam-bridge, </institution> <address> Cambridge, UK, </address> <year> 1994. </year>
Reference-contexts: The preconditioned solvers compute the solution of one of the following equations instead of the original one <ref> [4, 39] </ref>, M 1 Ax = M 1 b; AM 1 (M x) = b; M 1 R (M R x) = M 1 With appropriate choices of preconditioners, M or M L M R , the linear system solvers converge faster on the above equations than on the original equation. <p> The SOR preconditioner solves the preconditioning equation with one iteration of Gauss-Seidel iteration which is a special case of the Successive Over-Relaxation (SOR) method. ILU0 is an incomplete factorization where the LU factors have the same nonzero pattern as the original matrix <ref> [4, 28, 39] </ref>. <p> Even though the Jacobi-Davidson method was not derived based on this idea, this constitutes a valid interpretation. In the Jacobi-Davidson method, a Krylov subspace method is used to solve Equation (2). The Krylov subspace method approximates pseudoinverse by computing a solution in the range of the iteration matrix <ref> [4] </ref>. One difference between Equation (3) and Equation (4) is that the later one generates a correction that is orthogonal to the exact eigenvector x fl , i.e., the correction is in the range of the iteration matrix A fl I.
Reference: [5] <author> J. Baglama, D. Calvetti, and L. Reichel. </author> <title> Iterative methods for the computation of a few eigenvalues of a large symmetric matrix. </title> <journal> BIT, </journal> <volume> 36 </volume> <pages> 400-421, </pages> <year> 1996. </year>
Reference-contexts: When the basis size reaches m and the eigenvalue approximations are not satisfactory, the basis is compressed to a smaller one and the above algorithm is repeated. This process of restarting is a crucial part of an eigenvalue routine <ref> [5, 45, 47] </ref>. Since it is not the main concern of this paper, we will only use a very simply scheme. In fact for all tests, we set k = m=2, which is a good choice for most test problems, even though it is not optimal [47].
Reference: [6] <author> J. G. L. Booten, H. A. van der Vorst, P. M. Meijer, and H. J. J. te Riele. </author> <title> A preconditioned Jacobi-Davidson method for solving large generalized eigenvalue problems. </title> <type> Technical Report NM-R9414, </type> <institution> Mathematisch Centrum, Centrum voor Wiskunde en Informatica, </institution> <address> Amsterdam, </address> <year> 1994. </year> <month> 18 </month>
Reference-contexts: Stathopoulos and colleagues have shown that using a biased shift (see Section 2) ffi in the preconditioning matrix, A ffiI, can significantly reduce the number of iterations [46]. More recently, the following Jacobi-Davidson preconditioning has attracted much attention <ref> [6, 15, 16, 41, 42, 43] </ref>. (I xx T )(A I)(I xx T )z = r ; (2) in which again, x is a current approximation to the eigenvector being computed. The Jacobi--Davidson preconditioning can often significantly outperform the original Davidson scheme. <p> This explains why the use of CG to solve the Davidson preconditioning equation is effective. On the other hand, the advantages of using the other preconditioning schemes are significant in some cases. For example, there are a number of reports on the successes of the Jacobi-Davidson method <ref> [6, 15, 16, 41, 42, 43] </ref>. We recommend two new preconditioning schemes to the users of the Davidson method. These are the inflated Newton preconditioning and the constrained Newton preconditioning. The inflated Newton scheme is only slightly more complex than the Davidson scheme.
Reference: [7] <author> J. H. Bramble, A. V. Knyazev, and J. E. Pasciak. </author> <title> A subspace preconditioning algorithm for eigenvector/eigenvalue computation. </title> <type> Technical Report 66, </type> <institution> Center for Computational Mathematics, University of Colorado at Denver, </institution> <year> 1995. </year>
Reference-contexts: The Jacobi--Davidson preconditioning can often significantly outperform the original Davidson scheme. Other preconditioned eigenvalue methods include the preconditioned Arnoldi method [27, 36, 51], the preconditioned subspace techniques <ref> [7, 21, 40] </ref>, and the inexact rational Krylov method [24]. All these eigenvalue methods generate an basis first, then use the Rayleigh-Ritz projection on the basis to obtain the desired eigenvalue approximation. Most of them build orthonormal basis progressively.
Reference: [8] <author> F. Chatelin. </author> <title> Eigenvalues of Matrices. </title> <publisher> Wiley, </publisher> <year> 1993. </year>
Reference-contexts: The Rayleigh-Ritz projection method is used to project a large eigenvalue problem onto a small basis <ref> [8, 18, 32, 36, 50] </ref>. The preconditioning equation is used to introduce new vectors to the basis. For convenience, the basis V = [v 1 ; : : : ; v m ] used in the Rayleigh-Ritz projection will be an orthonormal basis.
Reference: [9] <author> E. Chow and Y. Saad. </author> <title> Approximate inverse preconditioners via sparse-sparse iterations. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 18(6) </volume> <pages> 1657-1675, </pages> <year> 1997. </year>
Reference-contexts: Some of the common strategies include applying iterative methods on the preconditioning equation [2, 19, 35], constructing an approximation to A that is easy to invert [1, 3, 37, 55], and constructing an approximation to the inverse of A <ref> [9, 20, 22] </ref>. This paper will attempt to exploit results of this research and apply them to precondition eigenvalue problems. One of our first goals for this purpose is to identify an effective preconditioning equation for eigenvalue problems. Davidson's method is one of the best known preconditioned eigenvalue solution methods.
Reference: [10] <author> M. Crouzeix, B. Philippe, and M. Sadkane. </author> <title> The Davidson method. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 15 </volume> <pages> 62-76, </pages> <year> 1994. </year>
Reference-contexts: Later on, the proper preconditioning equation was generally considered to be (AI)z = r <ref> [10, 12, 29, 30] </ref>. A number of potential problems have been observed [53]. For example, if the preconditioning equation is actually solved accurately, the solution is z = x, which will cause the Davidson method to stagnate.
Reference: [11] <author> Ernest R. Davidson. </author> <title> The iterative calculation of a few of the lowest eigenvalues and corresponding eigenvectors of large real-symmetric matrices. </title> <journal> J. Comput. Phys., </journal> <volume> 17 </volume> <pages> 87-94, </pages> <year> 1975. </year>
Reference-contexts: One of our first goals for this purpose is to identify an effective preconditioning equation for eigenvalue problems. Davidson's method is one of the best known preconditioned eigenvalue solution methods. Its original version employs the following preconditioning step <ref> [11] </ref>, diag (A I)z = r; where r is the residual vector of the current approximate solution (; x), r = Ax x. <p> However, a crucial difference is that the Davidson method does not require Equation (1) to be solved exactly. In the original Davidson method, the diagonal of (A I) is taken as an approximation to the whole matrix <ref> [11] </ref>. Later, incomplete LU factorization schemes were used [12, 29]. In recent years, a number of modifications to the preconditioning Equation (1) have been developed. <p> However, the data shown in Tables 2 and 3 are representative of their relative strength [53]. 3 Newton methods for eigenvalue problems In Davidson's original paper, the proposed eigenvalue method was given as a combination of the Lanczos method and the Newton method for minimizing the Rayleigh quotient <ref> [11] </ref>.
Reference: [12] <author> Ernest R. Davidson. </author> <title> Super-matrix methods. </title> <journal> Computer Physics Communications, </journal> <volume> 53 </volume> <pages> 49-60, </pages> <year> 1989. </year>
Reference-contexts: However, a crucial difference is that the Davidson method does not require Equation (1) to be solved exactly. In the original Davidson method, the diagonal of (A I) is taken as an approximation to the whole matrix [11]. Later, incomplete LU factorization schemes were used <ref> [12, 29] </ref>. In recent years, a number of modifications to the preconditioning Equation (1) have been developed. For example, Olsen et al. [31] proposed to modify the right-hand side of Equation (1) to make the solution z orthogonal to computed approximate eigenvectors x. <p> Later on, the proper preconditioning equation was generally considered to be (AI)z = r <ref> [10, 12, 29, 30] </ref>. A number of potential problems have been observed [53]. For example, if the preconditioning equation is actually solved accurately, the solution is z = x, which will cause the Davidson method to stagnate.
Reference: [13] <author> J. J. Dongarra, C. B. Moler, and J. H. Wilkingson. </author> <title> Improving the accuracy of computed eigenvalues and eigenvectors. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 20 </volume> <pages> 23-45, </pages> <year> 1983. </year>
Reference-contexts: The biased estimate may also be regarded as an attempt to estimate fl based on the Ritz value [46]. The above equation is also known as the correction equation since it was first used to refine the eigenvectors found by other means <ref> [13] </ref>. 7 Since fl is an exact eigenvalue, the Jacobian matrix (A fl I) is singular, and the above Newton recurrence is not well defined.
Reference: [14] <author> I. S. Duff, R. G. Grimes, and J. G. Lewis. </author> <title> Sparse matrix test problems. </title> <journal> ACM Trans. Math. Soft., </journal> <pages> pages 1-14, </pages> <year> 1989. </year>
Reference-contexts: diagonal 0 0 0 28 SOR 0 0 0 34 ILUTP 0 2 0 26 Table 3: Number of cases where the method reached convergence in the least amount of time. 5 To compare the four preconditioning schemes, we applied them on 45 non-diagonal sym-metric matrices in the Harwell-Boeing collection <ref> [14] </ref>. We used each one of them to form an standard eigenvalue problem, then compute 5 smallest eigenvalues and the corresponding eigenvectors of each problem. All methods start with the same initial guess, a vector of all ones: [1; 1; : : : ; 1] T . <p> The basis size for the Davidson method is 20. The maximum number of matrix-vector multiplications allowed is 5,000. The test is performed on a SPARC 10 running at 40MHz. We will also show results from another test problem PLAT362. The matrix PLAT362 is in the Harwell/Boeing collection <ref> [14] </ref>. It is positive definite, but the smallest eigenvalues are 12 order of magnitude smaller than the largest ones and the smallest ones are very close to each other. As before we apply Davidson method with basis size 20 to computed the smallest five eigenvalues and the corresponding eigenvectors.
Reference: [15] <author> A. Edelman, T. Arias, and S. T. Smith. </author> <title> Conjugate Gradient and Newton's method on the Grassmann and Stiefel manifolds, </title> <note> 1996. Submitted to SIAM J. Matrix Anal. Appl. </note>
Reference-contexts: Stathopoulos and colleagues have shown that using a biased shift (see Section 2) ffi in the preconditioning matrix, A ffiI, can significantly reduce the number of iterations [46]. More recently, the following Jacobi-Davidson preconditioning has attracted much attention <ref> [6, 15, 16, 41, 42, 43] </ref>. (I xx T )(A I)(I xx T )z = r ; (2) in which again, x is a current approximation to the eigenvector being computed. The Jacobi--Davidson preconditioning can often significantly outperform the original Davidson scheme. <p> This explains why the use of CG to solve the Davidson preconditioning equation is effective. On the other hand, the advantages of using the other preconditioning schemes are significant in some cases. For example, there are a number of reports on the successes of the Jacobi-Davidson method <ref> [6, 15, 16, 41, 42, 43] </ref>. We recommend two new preconditioning schemes to the users of the Davidson method. These are the inflated Newton preconditioning and the constrained Newton preconditioning. The inflated Newton scheme is only slightly more complex than the Davidson scheme.
Reference: [16] <author> D. R. Fokkema, G. L. G. Sleijpen, and H. A. van der Vorst. </author> <title> Jacobi-Davidson style QR and QZ algorithms for the partial reduction of matrix pencils. </title> <type> Technical Report nr. 941, </type> <institution> Department of mathematics, Universiteit Utrecht, </institution> <year> 1996. </year>
Reference-contexts: Stathopoulos and colleagues have shown that using a biased shift (see Section 2) ffi in the preconditioning matrix, A ffiI, can significantly reduce the number of iterations [46]. More recently, the following Jacobi-Davidson preconditioning has attracted much attention <ref> [6, 15, 16, 41, 42, 43] </ref>. (I xx T )(A I)(I xx T )z = r ; (2) in which again, x is a current approximation to the eigenvector being computed. The Jacobi--Davidson preconditioning can often significantly outperform the original Davidson scheme. <p> This explains why the use of CG to solve the Davidson preconditioning equation is effective. On the other hand, the advantages of using the other preconditioning schemes are significant in some cases. For example, there are a number of reports on the successes of the Jacobi-Davidson method <ref> [6, 15, 16, 41, 42, 43] </ref>. We recommend two new preconditioning schemes to the users of the Davidson method. These are the inflated Newton preconditioning and the constrained Newton preconditioning. The inflated Newton scheme is only slightly more complex than the Davidson scheme.
Reference: [17] <author> Albert T. Galick. </author> <title> Efficient solution of large sparse eigenvalue problems in microelectronic simulation. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1993. </year>
Reference-contexts: I i (A i I) 1 i (A i I) 1 x i : we can define yet another recurrence which computes ffi x as follows, ffi x = J 1 i : (15) The same iteration matrix J I has been used before in a so-called Inflated Inverse Iterations <ref> [17] </ref>. Thus we refer to the recurrence formed from Equations (15) and (8) as the Inflated Newton Recurrence. This recurrence is well defined since it is always possible to choose an ff to make J I non-singular.
Reference: [18] <author> G. H. Golub and C. F. van Loan. </author> <title> Matrix Computations. </title> <publisher> The John Hopkins University Press, </publisher> <address> Baltimore, MD 21211, thrid edition, </address> <year> 1996. </year>
Reference-contexts: The Rayleigh-Ritz projection method is used to project a large eigenvalue problem onto a small basis <ref> [8, 18, 32, 36, 50] </ref>. The preconditioning equation is used to introduce new vectors to the basis. For convenience, the basis V = [v 1 ; : : : ; v m ] used in the Rayleigh-Ritz projection will be an orthonormal basis. <p> To correct this, the following recursion can be used instead of Equation (3), x i+1 = x i (A fl I) + r i ; (4) where the superscript + indicates a pseudoinverse <ref> [18] </ref>. By the definition of pseudoinverses, AA + A = A. <p> The matrix in front of r i in Equation (13) closely resembles the Sherman-Woodbury formula for the inverse of A i I + ffx i x T i where ff is an arbitrary constant <ref> [18, Equation (2.1.4)] </ref>, i ) 1 = (A i I) 1 I i (A i I) 1 i (A i I) 1 x i : we can define yet another recurrence which computes ffi x as follows, ffi x = J 1 i : (15) The same iteration matrix J I
Reference: [19] <author> A. Greenbraum and L. N. Trefethen. </author> <title> GMRES/CR and Arnoldi/Lanczos matrix approximation problems. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 15(2) </volume> <pages> 359-368, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: We refer to this auxiliary equation as the preconditioning equation. Research on preconditioning linear systems focuses on generating an approximate solution z of this preconditioning equation. Some of the common strategies include applying iterative methods on the preconditioning equation <ref> [2, 19, 35] </ref>, constructing an approximation to A that is easy to invert [1, 3, 37, 55], and constructing an approximation to the inverse of A [9, 20, 22]. This paper will attempt to exploit results of this research and apply them to precondition eigenvalue problems. <p> We apply Conjugate Gradient (CG) method to solve Equations (1), (2), (7), and (15). This is how the Jacobi-Davidson method is usually implemented. It is similar to the inner-outer iteration schemes for linear systems <ref> [2, 19, 35, 54] </ref>. For simplicity, a maximum of 100 steps are allowed for each call to the CG routine. The CG routine is stopped when the residual norm has decreased by a factor of 10 4 .
Reference: [20] <author> M. J. Grote and T. Huckle. </author> <title> Parallel preconditioning with sparse approximate inverses. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 18(3) </volume> <pages> 838-853, </pages> <year> 1997. </year>
Reference-contexts: Some of the common strategies include applying iterative methods on the preconditioning equation [2, 19, 35], constructing an approximation to A that is easy to invert [1, 3, 37, 55], and constructing an approximation to the inverse of A <ref> [9, 20, 22] </ref>. This paper will attempt to exploit results of this research and apply them to precondition eigenvalue problems. One of our first goals for this purpose is to identify an effective preconditioning equation for eigenvalue problems. Davidson's method is one of the best known preconditioned eigenvalue solution methods.
Reference: [21] <author> A. V. Knyazev and A. L. Skorokhodov. </author> <title> Preconditioned gradient-type iterative methods in a subspace for partial generalized symmetric eigenvalue problems. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 31 </volume> <pages> 1226-1239, </pages> <year> 1994. </year>
Reference-contexts: The Jacobi--Davidson preconditioning can often significantly outperform the original Davidson scheme. Other preconditioned eigenvalue methods include the preconditioned Arnoldi method [27, 36, 51], the preconditioned subspace techniques <ref> [7, 21, 40] </ref>, and the inexact rational Krylov method [24]. All these eigenvalue methods generate an basis first, then use the Rayleigh-Ritz projection on the basis to obtain the desired eigenvalue approximation. Most of them build orthonormal basis progressively.
Reference: [22] <author> L. Yu. Kolotilina and A. Yu. Yeremin. </author> <title> Factorized sparse approximate inverse precondi-tionings I. </title> <journal> theory. SIAM J. Matrix Anal. Appl., </journal> <volume> 14(1) </volume> <pages> 45-58, </pages> <year> 1993. </year> <month> 19 </month>
Reference-contexts: Research on preconditioners for linear systems continues to be very active <ref> [1, 3, 22, 28, 37, 55] </ref>. <p> Some of the common strategies include applying iterative methods on the preconditioning equation [2, 19, 35], constructing an approximation to A that is easy to invert [1, 3, 37, 55], and constructing an approximation to the inverse of A <ref> [9, 20, 22] </ref>. This paper will attempt to exploit results of this research and apply them to precondition eigenvalue problems. One of our first goals for this purpose is to identify an effective preconditioning equation for eigenvalue problems. Davidson's method is one of the best known preconditioned eigenvalue solution methods.
Reference: [23] <author> R. B. Lehoucq. </author> <title> Restart an Arnoldi reduction. </title> <type> Technical Report MCS-P591-0496, </type> <institution> Ar--gonne National Laboratory, Argonne, IL, </institution> <year> 1996. </year> <note> Submitted to SIAM Journal on Scientific Computing. </note>
Reference-contexts: In this framework, the differences among the different methods include different restarting schemes and different preconditioning schemes. The process of restarting can have a strong influence on the overall effectiveness of the eigenvalue method <ref> [23, 45, 48] </ref>. However, the focus of this paper will be on the preconditioning aspects only. Different preconditioners can be obtained by using different right-hand sides and different matrices in the preconditioning equation. We will compare the results of using a number of different preconditioning schemes.
Reference: [24] <author> R. B. Lehoucq and K. Meerbergen. </author> <title> The inexact rational Krylov sequence method. </title> <type> Technical Report MCS-P612-1096, </type> <institution> Argonne National Laboratory, Argonne, IL, </institution> <year> 1997. </year>
Reference-contexts: The Jacobi--Davidson preconditioning can often significantly outperform the original Davidson scheme. Other preconditioned eigenvalue methods include the preconditioned Arnoldi method [27, 36, 51], the preconditioned subspace techniques [7, 21, 40], and the inexact rational Krylov method <ref> [24] </ref>. All these eigenvalue methods generate an basis first, then use the Rayleigh-Ritz projection on the basis to obtain the desired eigenvalue approximation. Most of them build orthonormal basis progressively.
Reference: [25] <author> R. B. Lehoucq and J. A. Scott. </author> <title> An evaluation of software for computing eigenvalues of sparse nonsymmetric matrices. </title> <type> Technical Report MCS-P547-1195, </type> <institution> Argonne National Laboratory, Argonne, IL, </institution> <year> 1996. </year> <note> Submitted to ACM TOMS. </note>
Reference-contexts: Experiments show that this biased estimate of the eigenvalue is more effective as a shift for the preconditioners than the Ritz value [46, 53]. This biased estimate is used as shift in all reported test cases on preconditioning. Comparison results on nonsymmetric eigenvalue problems can be found elsewhere <ref> [25] </ref>. The above algorithm starts with k initial vectors and increment the basis size by fi vectors at a time. When the basis size reaches m and the eigenvalue approximations are not satisfactory, the basis is compressed to a smaller one and the above algorithm is repeated.
Reference: [26] <author> T. Manteuffel, S. McCormick, L. Adams, S. Ashby, H. Elman, R. Freund, A. Greenbaum, S. Parter, P. Saylor, N. Trefethen, H. van der Vorst, H. Walker, and O. Wildlund, </author> <title> editors. </title> <booktitle> Proceedings of Copper Mountain Conference on Iterative Methods, </booktitle> <address> Copper Mountain, Colorado, </address> <year> 1996. </year>
Reference: [27] <author> Karl Meerbergen. </author> <title> Robust methods for the calculation of rightmost eigenvalues of nonsymmetric eigenvalue problems. </title> <type> PhD thesis, K. </type> <institution> U. Leuven, Heverlee, Belgium, </institution> <year> 1996. </year>
Reference-contexts: The Jacobi--Davidson preconditioning can often significantly outperform the original Davidson scheme. Other preconditioned eigenvalue methods include the preconditioned Arnoldi method <ref> [27, 36, 51] </ref>, the preconditioned subspace techniques [7, 21, 40], and the inexact rational Krylov method [24]. All these eigenvalue methods generate an basis first, then use the Rayleigh-Ritz projection on the basis to obtain the desired eigenvalue approximation. Most of them build orthonormal basis progressively. <p> Extensive tests have been conducted for a number of different choices for s [53]. Here, we will recall the results of four schemes: the Arnoldi method, the Davidson method, the orthogonalized Arnoldi method [51], and the Cayley-Arnoldi method <ref> [27] </ref>. The different right-hand sides used in the corresponding preconditioning equations are shown in Table 1. The rationale for the orthogonalized Arnoldi method is that the right-hand side used by the Davidson method is orthogonal to the basis. <p> The Cayley-Arnoldi scheme builds a basis for the Krylov subspace with the matrix M 1 (A I). If is an eigenvalue of A, then zero is an eigenvalue of M 1 (A I). In addition, the corresponding eigenvectors are identical <ref> [27] </ref>. This is an advantage over the Arnoldi method because usually M 1 A and A have no eigenvector in common.
Reference: [28] <author> J. A. Meijerink and H. A. van der Vorst. </author> <title> An iterative solution method for lineat systems of which the coefficient matrix is a symmetric M-matrix. </title> <journal> Math. Comp., </journal> <volume> 31 </volume> <pages> 148-162, </pages> <year> 1977. </year>
Reference-contexts: Research on preconditioners for linear systems continues to be very active <ref> [1, 3, 22, 28, 37, 55] </ref>. <p> The SOR preconditioner solves the preconditioning equation with one iteration of Gauss-Seidel iteration which is a special case of the Successive Over-Relaxation (SOR) method. ILU0 is an incomplete factorization where the LU factors have the same nonzero pattern as the original matrix <ref> [4, 28, 39] </ref>.
Reference: [29] <author> R. B. Morgan and D. S. Scott. </author> <title> Generalizations of Davidson's method for computing eigenvalues of sparse symmetric matrices. </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 7 </volume> <pages> 817-825, </pages> <year> 1986. </year>
Reference-contexts: Davidson's method is one of the best known preconditioned eigenvalue solution methods. Its original version employs the following preconditioning step [11], diag (A I)z = r; where r is the residual vector of the current approximate solution (; x), r = Ax x. Morgan and Scott <ref> [29] </ref> were among the first researchers to generalize this to the following form, (A I)z = r: (1) They then adapted standard preconditioning techniques developed for linear systems for solving the above equation. <p> However, a crucial difference is that the Davidson method does not require Equation (1) to be solved exactly. In the original Davidson method, the diagonal of (A I) is taken as an approximation to the whole matrix [11]. Later, incomplete LU factorization schemes were used <ref> [12, 29] </ref>. In recent years, a number of modifications to the preconditioning Equation (1) have been developed. For example, Olsen et al. [31] proposed to modify the right-hand side of Equation (1) to make the solution z orthogonal to computed approximate eigenvectors x. <p> Later on, the proper preconditioning equation was generally considered to be (AI)z = r <ref> [10, 12, 29, 30] </ref>. A number of potential problems have been observed [53]. For example, if the preconditioning equation is actually solved accurately, the solution is z = x, which will cause the Davidson method to stagnate.
Reference: [30] <author> R. B. Morgan and D. S. Scott. </author> <title> Preconditioning the Lanczos algorithm for sparse symmetric eigenvalue problems. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 14 </volume> <pages> 585-593, </pages> <year> 1993. </year>
Reference-contexts: Later on, the proper preconditioning equation was generally considered to be (AI)z = r <ref> [10, 12, 29, 30] </ref>. A number of potential problems have been observed [53]. For example, if the preconditioning equation is actually solved accurately, the solution is z = x, which will cause the Davidson method to stagnate.
Reference: [31] <author> J. Olsen, P. Jorgensen, and J. Simons. </author> <title> Passing the one-billion limit in full configuration-interaction (FCI) calculations. </title> <journal> Chemical Physics Letters, </journal> <volume> 169 </volume> <pages> 463-472, </pages> <year> 1990. </year>
Reference-contexts: In the original Davidson method, the diagonal of (A I) is taken as an approximation to the whole matrix [11]. Later, incomplete LU factorization schemes were used [12, 29]. In recent years, a number of modifications to the preconditioning Equation (1) have been developed. For example, Olsen et al. <ref> [31] </ref> proposed to modify the right-hand side of Equation (1) to make the solution z orthogonal to computed approximate eigenvectors x. Stathopoulos and colleagues have shown that using a biased shift (see Section 2) ffi in the preconditioning matrix, A ffiI, can significantly reduce the number of iterations [46]. <p> One difference between Equation (3) and Equation (4) is that the later one generates a correction that is orthogonal to the exact eigenvector x fl , i.e., the correction is in the range of the iteration matrix A fl I. Both the Olsen preconditioning scheme <ref> [31] </ref> and the Jacobi-Davidson preconditioning scheme [42] generate orthogonal corrections. They can viewed as ways of mimicking the Newton recurrence with pseudoinverse. An alternative strategy to address the same issue is to find a Newton recurrence with a nonsingular Jacobian matrix. <p> symbolically, ffi x can be expressed as follows, ffi x = (A i I) 1 I i (A i I) 1 i (A i I) 1 x i r i : (13) This expression can be regarded as the exact form of the Olsen modification to the Davidson preconditioning scheme <ref> [31] </ref>. The goal of the Olsen modification is to make the vector resulting from preconditioning orthogonal to the current Ritz vector. It is easy to verify that ffi x from Equation (12) is orthogonal to x i . <p> Table 11 shows the results of using the Olsen preconditioning schemes with the diagonal preconditioner, ILU0 and ILUTP. Compared to Table 9, the Olsen scheme improves the effectiveness of the three incomplete factorizations. The improvement with the ILU0 preconditioner is particularly pronounced <ref> [31] </ref>. Table 12 shows the results of using CG as preconditioner. The results of both EX2 and PLAT362 are shown. The table shows the number of matrix-vector multiplications (MATVEC) used by the Davidson method, the total number of matrix-vector multiplications, and the time.
Reference: [32] <author> Beresford N. Parlett. </author> <title> The symmetric eigenvalue problem. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1980. </year>
Reference-contexts: A brief summary is presented in Section 5. 2 Right-hand sides for the preconditioning equation The eigenvalue methods studied here have two logically distinct parts, one to generate a basis V and the other to project the original eigenvalue problem onto the basis and generate approximate solutions <ref> [32, 53] </ref>. The Rayleigh-Ritz projection method is used to project a large eigenvalue problem onto a small basis [8, 18, 32, 36, 50]. The preconditioning equation is used to introduce new vectors to the basis. <p> The Rayleigh-Ritz projection method is used to project a large eigenvalue problem onto a small basis <ref> [8, 18, 32, 36, 50] </ref>. The preconditioning equation is used to introduce new vectors to the basis. For convenience, the basis V = [v 1 ; : : : ; v m ] used in the Rayleigh-Ritz projection will be an orthonormal basis. <p> The Lanczos method is also equivalent to the unpreconditioned Arnoldi method and it uses even fewer arithmetic operations per step. If an eigenvalue problem can be solved without preconditioning, the Lanczos method is a more efficient method than the Arnoldi method <ref> [32] </ref>. Because the Lanczos method cannot use the preconditioning scheme described here, it is not discussed further. In the preconditioned cases, the Davidson method reaches convergence faster than most others no matter what preconditioner is used.
Reference: [33] <author> G. Peters and J. H. Wilkinson. </author> <title> Inverse iteration, ill-conditioned equations and Newton's method. </title> <journal> SIAM Review, </journal> <volume> 21(3) </volume> <pages> 339-360, </pages> <month> July </month> <year> 1979. </year>
Reference-contexts: condition number of J C in this case is about 100 times larger than the condition number of EX2. 3.2 Augmented Newton Recurrence Another way of formulating the eigenvalue problem is to treat it as an (n + 1)-dimensional optimization problem which can again be solved using a Newton method <ref> [33] </ref>. Earlier researchers have formulated the eigenvalue problem using different normalization schemes. Here we choose to normalize the eigenvectors using the 2-norm. The eigenvalue problem can be restated as follows, ( (A I)x = 0; 2 x T x + 1 (9) This is an unconstrained quadratic problem.
Reference: [34] <author> Yousef Saad. SPARSKIT: </author> <title> A basic toolkit for sparse matrix computations. </title> <type> Technical Report 90-20, </type> <institution> Research Institute for Advanced Computer Science, NASA Ames Research Center, Moffet Field, </institution> <address> CA, </address> <year> 1990. </year> <note> Software currently available at ftp://ftp.cs.umn.edu/dept/sparse/. </note>
Reference-contexts: A Ritz pair is considered to have converged if its residual norm is less than 10 12 kAk F . The timing results are obtained on a SPARC-10 workstation. The preconditioning equations are solved with selected schemes from SPARSKIT: diagonal preconditioner, SOR, ILU0 and ILUTP <ref> [34] </ref>. The diagonal pre-conditioner only inverts the diagonal of (A I) as in the original Davidson scheme. The SOR preconditioner solves the preconditioning equation with one iteration of Gauss-Seidel iteration which is a special case of the Successive Over-Relaxation (SOR) method. <p> We wanted to test the potential benefit of using Equation (12) as the preconditioning equation. This preconditioning scheme will be called augmented Newton preconditioning. The preconditioning equation will be solved with two incomplete LU factorizations: ILU0, ILUTP <ref> [34] </ref>. ILUTP uses a level of fill that is equal to half of the average number of nonzero elements per row. In other word, the ILUTP factorization stores slightly more nonzero elements than ILU0. <p> In other word, the ILUTP factorization stores slightly more nonzero elements than ILU0. The drop tolerance is 3 fi 10 5 and the pivot threshold is 0:1, see documentation of SPARSKIT for definition of these parameters <ref> [34] </ref>. * Olsen preconditioners. The Olsen preconditioning scheme is implemented on top of the regular Davidson preconditioning schemes. We replace (A ffiI) in Equation (13) by the following three schemes: the diagonal preconditioner, ILU0, and ILUTP. * Iterative solvers.
Reference: [35] <author> Yousef Saad. </author> <title> A flexible inner-outer preconditioned GMRES algorithm. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 14(2) </volume> <pages> 461-469, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: We refer to this auxiliary equation as the preconditioning equation. Research on preconditioning linear systems focuses on generating an approximate solution z of this preconditioning equation. Some of the common strategies include applying iterative methods on the preconditioning equation <ref> [2, 19, 35] </ref>, constructing an approximation to A that is easy to invert [1, 3, 37, 55], and constructing an approximation to the inverse of A [9, 20, 22]. This paper will attempt to exploit results of this research and apply them to precondition eigenvalue problems. <p> We apply Conjugate Gradient (CG) method to solve Equations (1), (2), (7), and (15). This is how the Jacobi-Davidson method is usually implemented. It is similar to the inner-outer iteration schemes for linear systems <ref> [2, 19, 35, 54] </ref>. For simplicity, a maximum of 100 steps are allowed for each call to the CG routine. The CG routine is stopped when the residual norm has decreased by a factor of 10 4 .
Reference: [36] <author> Yousef Saad. </author> <title> Numerical Methods for Large Eigenvalue Problems. </title> <publisher> Manchester University Press, </publisher> <year> 1993. </year> <month> 20 </month>
Reference-contexts: The Jacobi--Davidson preconditioning can often significantly outperform the original Davidson scheme. Other preconditioned eigenvalue methods include the preconditioned Arnoldi method <ref> [27, 36, 51] </ref>, the preconditioned subspace techniques [7, 21, 40], and the inexact rational Krylov method [24]. All these eigenvalue methods generate an basis first, then use the Rayleigh-Ritz projection on the basis to obtain the desired eigenvalue approximation. Most of them build orthonormal basis progressively. <p> The Rayleigh-Ritz projection method is used to project a large eigenvalue problem onto a small basis <ref> [8, 18, 32, 36, 50] </ref>. The preconditioning equation is used to introduce new vectors to the basis. For convenience, the basis V = [v 1 ; : : : ; v m ] used in the Rayleigh-Ritz projection will be an orthonormal basis.
Reference: [37] <author> Yousef Saad. </author> <title> Highly parallel preconditioners for general sparse matrices. </title> <editor> In G. Golub, A. Greenbaum, and M. Luskin, editors, </editor> <booktitle> Recent advances in Iterative Methods, </booktitle> <pages> pages 165-199. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Research on preconditioners for linear systems continues to be very active <ref> [1, 3, 22, 28, 37, 55] </ref>. <p> Research on preconditioning linear systems focuses on generating an approximate solution z of this preconditioning equation. Some of the common strategies include applying iterative methods on the preconditioning equation [2, 19, 35], constructing an approximation to A that is easy to invert <ref> [1, 3, 37, 55] </ref>, and constructing an approximation to the inverse of A [9, 20, 22]. This paper will attempt to exploit results of this research and apply them to precondition eigenvalue problems.
Reference: [38] <author> Yousef Saad. ILUT: </author> <title> a dual threshold incomplete ILU factorization. Numerical Linear Algebra with Applications, </title> <booktitle> 1 </booktitle> <pages> 387-402, </pages> <year> 1994. </year> <type> Technical Report 92-38, </type> <institution> Minnesota Supercomputer Institute, University of Minnesota, </institution> <year> 1992. </year>
Reference-contexts: of LU factors with small absolute values are dropped, (2) a maximum number of nonzero elements in a row of LU factors, i.e., level of fill, is controlled by the user, (3) column pivoting is performed if the diagonal element is significantly smaller than another element on the same row <ref> [38] </ref>. Generally, the preconditioners are considered more effective for linear systems toward the bottom of the Tables 2 and 3, i.e., ILUTP is better than ILU0, which is in turn better than SOR and diagonal preconditioning [52].
Reference: [39] <author> Yousef Saad. </author> <title> Iterative Methods for Sparse Linear Systems. </title> <publisher> PWS publishing, </publisher> <address> Boston, MA, </address> <year> 1996. </year>
Reference-contexts: The preconditioned solvers compute the solution of one of the following equations instead of the original one <ref> [4, 39] </ref>, M 1 Ax = M 1 b; AM 1 (M x) = b; M 1 R (M R x) = M 1 With appropriate choices of preconditioners, M or M L M R , the linear system solvers converge faster on the above equations than on the original equation. <p> The SOR preconditioner solves the preconditioning equation with one iteration of Gauss-Seidel iteration which is a special case of the Successive Over-Relaxation (SOR) method. ILU0 is an incomplete factorization where the LU factors have the same nonzero pattern as the original matrix <ref> [4, 28, 39] </ref>.
Reference: [40] <author> A. H. Sameh and J. A. Wisniewski. </author> <title> A trace minimization algorithm for the generalized eigenvalues problem. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 19(6) </volume> <pages> 1243-1259, </pages> <year> 1982. </year>
Reference-contexts: The Jacobi--Davidson preconditioning can often significantly outperform the original Davidson scheme. Other preconditioned eigenvalue methods include the preconditioned Arnoldi method [27, 36, 51], the preconditioned subspace techniques <ref> [7, 21, 40] </ref>, and the inexact rational Krylov method [24]. All these eigenvalue methods generate an basis first, then use the Rayleigh-Ritz projection on the basis to obtain the desired eigenvalue approximation. Most of them build orthonormal basis progressively.
Reference: [41] <author> G. L. G. Sleijpen, A. G. L. Booten, D. R. Fokkema, and H. A. van der Vorst. </author> <title> Jacobi-Davidson type methods for generalized eigenproblems and polynomial eigenproblems: Part I. </title> <type> Technical Report nr. 923, </type> <institution> Department of mathematics, Universiteit Utrecht, </institution> <year> 1995. </year>
Reference-contexts: Stathopoulos and colleagues have shown that using a biased shift (see Section 2) ffi in the preconditioning matrix, A ffiI, can significantly reduce the number of iterations [46]. More recently, the following Jacobi-Davidson preconditioning has attracted much attention <ref> [6, 15, 16, 41, 42, 43] </ref>. (I xx T )(A I)(I xx T )z = r ; (2) in which again, x is a current approximation to the eigenvector being computed. The Jacobi--Davidson preconditioning can often significantly outperform the original Davidson scheme. <p> This explains why the use of CG to solve the Davidson preconditioning equation is effective. On the other hand, the advantages of using the other preconditioning schemes are significant in some cases. For example, there are a number of reports on the successes of the Jacobi-Davidson method <ref> [6, 15, 16, 41, 42, 43] </ref>. We recommend two new preconditioning schemes to the users of the Davidson method. These are the inflated Newton preconditioning and the constrained Newton preconditioning. The inflated Newton scheme is only slightly more complex than the Davidson scheme.
Reference: [42] <author> G. L. G. Sleijpen and H. A. van der Vorst. </author> <title> A Jacobi-Davidson iteration method for linear eigenvalue problems. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 17(2), </volume> <year> 1996. </year>
Reference-contexts: Stathopoulos and colleagues have shown that using a biased shift (see Section 2) ffi in the preconditioning matrix, A ffiI, can significantly reduce the number of iterations [46]. More recently, the following Jacobi-Davidson preconditioning has attracted much attention <ref> [6, 15, 16, 41, 42, 43] </ref>. (I xx T )(A I)(I xx T )z = r ; (2) in which again, x is a current approximation to the eigenvector being computed. The Jacobi--Davidson preconditioning can often significantly outperform the original Davidson scheme. <p> Both the Olsen preconditioning scheme [31] and the Jacobi-Davidson preconditioning scheme <ref> [42] </ref> generate orthogonal corrections. They can viewed as ways of mimicking the Newton recurrence with pseudoinverse. An alternative strategy to address the same issue is to find a Newton recurrence with a nonsingular Jacobian matrix. <p> This explains why the use of CG to solve the Davidson preconditioning equation is effective. On the other hand, the advantages of using the other preconditioning schemes are significant in some cases. For example, there are a number of reports on the successes of the Jacobi-Davidson method <ref> [6, 15, 16, 41, 42, 43] </ref>. We recommend two new preconditioning schemes to the users of the Davidson method. These are the inflated Newton preconditioning and the constrained Newton preconditioning. The inflated Newton scheme is only slightly more complex than the Davidson scheme.
Reference: [43] <author> G. L. G. Sleijpen and H. A. van der Vorst. </author> <title> The Jacobi-Davidson method for eigenvalue problems and its relation with accelerated inexact newton schemes, </title> <year> 1996. </year>
Reference-contexts: Stathopoulos and colleagues have shown that using a biased shift (see Section 2) ffi in the preconditioning matrix, A ffiI, can significantly reduce the number of iterations [46]. More recently, the following Jacobi-Davidson preconditioning has attracted much attention <ref> [6, 15, 16, 41, 42, 43] </ref>. (I xx T )(A I)(I xx T )z = r ; (2) in which again, x is a current approximation to the eigenvector being computed. The Jacobi--Davidson preconditioning can often significantly outperform the original Davidson scheme. <p> This explains why the use of CG to solve the Davidson preconditioning equation is effective. On the other hand, the advantages of using the other preconditioning schemes are significant in some cases. For example, there are a number of reports on the successes of the Jacobi-Davidson method <ref> [6, 15, 16, 41, 42, 43] </ref>. We recommend two new preconditioning schemes to the users of the Davidson method. These are the inflated Newton preconditioning and the constrained Newton preconditioning. The inflated Newton scheme is only slightly more complex than the Davidson scheme.
Reference: [44] <author> D. Sorensen, R. Lehoucq, P. Vu, and C. Yang. ARPACK: </author> <title> an implementation of the Implicitly Restarted Arnoldi iteration that computes some of the eigenvalues and eigenvectors of a large sparse matrix. </title> <note> Available from ftp.caam.rice.edu, directory pub/people/sorensen/ARPACK, </note> <year> 1995. </year>
Reference-contexts: All methods start with the same initial guess, a vector of all ones: [1; 1; : : : ; 1] T . Only one initial guess is used so that comparisons can be made with ARPACK <ref> [44] </ref> to validate the implementation of the methods. A Ritz pair is considered to have converged if its residual norm is less than 10 12 kAk F . The timing results are obtained on a SPARC-10 workstation.
Reference: [45] <author> D. S. Sorensen. </author> <title> Implicit application of polynomial filters in a K-step Arnoldi method. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 13(1) </volume> <pages> 357-385, </pages> <year> 1992. </year>
Reference-contexts: In this framework, the differences among the different methods include different restarting schemes and different preconditioning schemes. The process of restarting can have a strong influence on the overall effectiveness of the eigenvalue method <ref> [23, 45, 48] </ref>. However, the focus of this paper will be on the preconditioning aspects only. Different preconditioners can be obtained by using different right-hand sides and different matrices in the preconditioning equation. We will compare the results of using a number of different preconditioning schemes. <p> When the basis size reaches m and the eigenvalue approximations are not satisfactory, the basis is compressed to a smaller one and the above algorithm is repeated. This process of restarting is a crucial part of an eigenvalue routine <ref> [5, 45, 47] </ref>. Since it is not the main concern of this paper, we will only use a very simply scheme. In fact for all tests, we set k = m=2, which is a good choice for most test problems, even though it is not optimal [47].
Reference: [46] <author> A. Stathopoulos, Y. Saad, and C. F. Fischer. </author> <title> Robust preconditioning of large, sparse, symmetric eigenvalue problems. </title> <journal> Journal of Computational and Applied Mathematics, </journal> <volume> 64 </volume> <pages> 197-215, </pages> <year> 1995. </year>
Reference-contexts: Stathopoulos and colleagues have shown that using a biased shift (see Section 2) ffi in the preconditioning matrix, A ffiI, can significantly reduce the number of iterations <ref> [46] </ref>. More recently, the following Jacobi-Davidson preconditioning has attracted much attention [6, 15, 16, 41, 42, 43]. (I xx T )(A I)(I xx T )z = r ; (2) in which again, x is a current approximation to the eigenvector being computed. <p> For largest eigenvalues the minus sign in the above equation is replaced by a plus. Experiments show that this biased estimate of the eigenvalue is more effective as a shift for the preconditioners than the Ritz value <ref> [46, 53] </ref>. This biased estimate is used as shift in all reported test cases on preconditioning. Comparison results on nonsymmetric eigenvalue problems can be found elsewhere [25]. The above algorithm starts with k initial vectors and increment the basis size by fi vectors at a time. <p> The Davidson preconditioning can be regarded an approximate form of this Newton iteration. The biased estimate may also be regarded as an attempt to estimate fl based on the Ritz value <ref> [46] </ref>. The above equation is also known as the correction equation since it was first used to refine the eigenvectors found by other means [13]. 7 Since fl is an exact eigenvalue, the Jacobian matrix (A fl I) is singular, and the above Newton recurrence is not well defined.
Reference: [47] <author> A. Stathopoulos, Y. Saad, and K. Wu. </author> <title> Dynamic thick restarting of the davidson and the implicitly restarted arnoldi methods. </title> <type> Technical Report UMSI 96/123, </type> <institution> University of Minnesota Supercomputer Institute, </institution> <year> 1996. </year>
Reference-contexts: When the basis size reaches m and the eigenvalue approximations are not satisfactory, the basis is compressed to a smaller one and the above algorithm is repeated. This process of restarting is a crucial part of an eigenvalue routine <ref> [5, 45, 47] </ref>. Since it is not the main concern of this paper, we will only use a very simply scheme. In fact for all tests, we set k = m=2, which is a good choice for most test problems, even though it is not optimal [47]. <p> Since it is not the main concern of this paper, we will only use a very simply scheme. In fact for all tests, we set k = m=2, which is a good choice for most test problems, even though it is not optimal <ref> [47] </ref>. In step 2 of the above algorithm, the right-hand sides of the preconditioning equation are called s i to suggest that they do not have to be residual vectors as in the Davidson method.
Reference: [48] <author> A. Stathopoulos, Y. Saad, and K. Wu. </author> <title> Thick restarting of the Davidson method: an extension to implicit restarting. </title> <editor> In Manteuffel et al. </editor> <volume> [26]. </volume>
Reference-contexts: In this framework, the differences among the different methods include different restarting schemes and different preconditioning schemes. The process of restarting can have a strong influence on the overall effectiveness of the eigenvalue method <ref> [23, 45, 48] </ref>. However, the focus of this paper will be on the preconditioning aspects only. Different preconditioners can be obtained by using different right-hand sides and different matrices in the preconditioning equation. We will compare the results of using a number of different preconditioning schemes.
Reference: [49] <author> R. A. Tapia. </author> <title> Newton's method for optimization problems with equality constraints. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 11 </volume> <pages> 874-886, </pages> <year> 1974. </year>
Reference-contexts: One way of stating the eigenvalue problem is to write it as follows: Ax xx T Ax = 0; kxk = 1: (5) With the eigenvalue problem stated in this form, Tapia's algorithm for constrained optimization can be directly applied after we evaluate the Jacobian matrix of this Newton recurrence <ref> [49] </ref>. <p> It is easy to show that the above constrained Newton recurrence is well defined near a nonzero simple eigenvalue [53, Lemma 3.5]. Based on a result of Tapia <ref> [49, Theorem 3.3] </ref>, the above Newton recurrence should converge quadratically near a nonzero simple eigenvalue. In addition, it is also very easy to show that this Newton recurrence is mathematically equivalent to the Rayleigh quotient iteration [53, Lemma 3.6].
Reference: [50] <author> J. H. Wilkinson. </author> <title> Algebraic Eigenvalue Problem. Monographs on Numerical Analysis. </title> <publisher> Oxford Science Publications, </publisher> <address> New York, </address> <year> 1965. </year> <month> 21 </month>
Reference-contexts: The Rayleigh-Ritz projection method is used to project a large eigenvalue problem onto a small basis <ref> [8, 18, 32, 36, 50] </ref>. The preconditioning equation is used to introduce new vectors to the basis. For convenience, the basis V = [v 1 ; : : : ; v m ] used in the Rayleigh-Ritz projection will be an orthonormal basis.
Reference: [51] <author> K. Wu, Y. Saad, and A. Stathopoulos. </author> <title> Preconditioned Krylov subspace methods for eigenvalue problems. </title> <editor> In Manteuffel et al. </editor> <volume> [26]. </volume>
Reference-contexts: The Jacobi--Davidson preconditioning can often significantly outperform the original Davidson scheme. Other preconditioned eigenvalue methods include the preconditioned Arnoldi method <ref> [27, 36, 51] </ref>, the preconditioned subspace techniques [7, 21, 40], and the inexact rational Krylov method [24]. All these eigenvalue methods generate an basis first, then use the Rayleigh-Ritz projection on the basis to obtain the desired eigenvalue approximation. Most of them build orthonormal basis progressively. <p> Extensive tests have been conducted for a number of different choices for s [53]. Here, we will recall the results of four schemes: the Arnoldi method, the Davidson method, the orthogonalized Arnoldi method <ref> [51] </ref>, and the Cayley-Arnoldi method [27]. The different right-hand sides used in the corresponding preconditioning equations are shown in Table 1. The rationale for the orthogonalized Arnoldi method is that the right-hand side used by the Davidson method is orthogonal to the basis.
Reference: [52] <author> Kesheng Wu. </author> <title> An experimental study of Krylov subspace accelerators. </title> <type> Technical Report UMSI 96/20, </type> <institution> Minnesota Supercomputing Institite, University of Minnesota, </institution> <year> 1996. </year>
Reference-contexts: Generally, the preconditioners are considered more effective for linear systems toward the bottom of the Tables 2 and 3, i.e., ILUTP is better than ILU0, which is in turn better than SOR and diagonal preconditioning <ref> [52] </ref>. Table 2 sums up the total number of eigenvalue problems solved with each method. Without preconditioning, the four methods converged on the same problems. With preconditioning, the Davidson method converges on more problems than any other method.
Reference: [53] <author> Kesheng Wu. </author> <title> Preconditioned Techniques for Large Eigenvalue Problems. </title> <type> PhD thesis, </type> <institution> University of Minnesota, </institution> <year> 1997. </year> <note> An updated version also appears as Technical Report TR97-038 at the Computer Science Department. </note>
Reference-contexts: A brief summary is presented in Section 5. 2 Right-hand sides for the preconditioning equation The eigenvalue methods studied here have two logically distinct parts, one to generate a basis V and the other to project the original eigenvalue problem onto the basis and generate approximate solutions <ref> [32, 53] </ref>. The Rayleigh-Ritz projection method is used to project a large eigenvalue problem onto a small basis [8, 18, 32, 36, 50]. The preconditioning equation is used to introduce new vectors to the basis. <p> For largest eigenvalues the minus sign in the above equation is replaced by a plus. Experiments show that this biased estimate of the eigenvalue is more effective as a shift for the preconditioners than the Ritz value <ref> [46, 53] </ref>. This biased estimate is used as shift in all reported test cases on preconditioning. Comparison results on nonsymmetric eigenvalue problems can be found elsewhere [25]. The above algorithm starts with k initial vectors and increment the basis size by fi vectors at a time. <p> Extensive tests have been conducted for a number of different choices for s <ref> [53] </ref>. Here, we will recall the results of four schemes: the Arnoldi method, the Davidson method, the orthogonalized Arnoldi method [51], and the Cayley-Arnoldi method [27]. The different right-hand sides used in the corresponding preconditioning equations are shown in Table 1. <p> There are a number of implementation details that may alter the overall performance of the methods tested. However, the data shown in Tables 2 and 3 are representative of their relative strength <ref> [53] </ref>. 3 Newton methods for eigenvalue problems In Davidson's original paper, the proposed eigenvalue method was given as a combination of the Lanczos method and the Newton method for minimizing the Rayleigh quotient [11]. <p> Later on, the proper preconditioning equation was generally considered to be (AI)z = r [10, 12, 29, 30]. A number of potential problems have been observed <ref> [53] </ref>. For example, if the preconditioning equation is actually solved accurately, the solution is z = x, which will cause the Davidson method to stagnate. <p> It is easy to show that the above constrained Newton recurrence is well defined near a nonzero simple eigenvalue <ref> [53, Lemma 3.5] </ref>. Based on a result of Tapia [49, Theorem 3.3], the above Newton recurrence should converge quadratically near a nonzero simple eigenvalue. In addition, it is also very easy to show that this Newton recurrence is mathematically equivalent to the Rayleigh quotient iteration [53, Lemma 3.6]. <p> Based on a result of Tapia [49, Theorem 3.3], the above Newton recurrence should converge quadratically near a nonzero simple eigenvalue. In addition, it is also very easy to show that this Newton recurrence is mathematically equivalent to the Rayleigh quotient iteration <ref> [53, Lemma 3.6] </ref>. To test this constrained Newton recurrence, we have chosen a small finite element matrix named EX2 as a test problem. This test matrix is generated from solving for a fully coupled Navier-Stokes equation using the FIDAP package 1 and is available from MatrixMarket 2 . <p> ; x 0 ), the Newton recurrence can be described as follows, i+1 = x i ! A (A i I)x i 2 x T 2 ; (10) x T ! It is easy to see that the matrix J A is non-singular when i is near a simple eigenvalue <ref> [53, Lemma 3.1] </ref>. Table 6 shows the Ritz values and associated residual norms produced by the augmented Newton recurrence on the test problem. One observation we make here is that the eigenvalue converges to a different number compared with Tables 4 and 5. <p> It is easy to verify that ffi x from Equation (12) is orthogonal to x i . Using Equation (13), it can be shown that the normalized augmented Newton recurrence is equivalent to the Rayleigh quotient iteration <ref> [53, Lemma 3.2] </ref>. Using the iteration matrix of the Jacobi-Davidson preconditioning scheme, we can also define ffi x as follows ffi x = (I x i x T i ) r i : (14) It is easy to see that Equation (13) and Equation (14) produce the same results. <p> The Jacobi-Davidson preconditioning scheme is known to be a form of inexact Newton method. This is another interpretation in the framework of the constrained Newton recurrence. The recurrence formed from Equation (14) and (8) is equivalent to the Rayleigh quotient iteration <ref> [53] </ref>.
Reference: [54] <author> Ulrike Meier Yang. </author> <title> Preconditioned Conjugate Gradient-like methods for nonsymmetric linear system. </title> <type> Technical Report 1210, </type> <institution> CSRD, University of Illinois at Urbaba-Champaign, </institution> <year> 1994. </year>
Reference-contexts: We apply Conjugate Gradient (CG) method to solve Equations (1), (2), (7), and (15). This is how the Jacobi-Davidson method is usually implemented. It is similar to the inner-outer iteration schemes for linear systems <ref> [2, 19, 35, 54] </ref>. For simplicity, a maximum of 100 steps are allowed for each call to the CG routine. The CG routine is stopped when the residual norm has decreased by a factor of 10 4 .
Reference: [55] <author> Harry Yserentant. </author> <title> Old and new convergence proofs for multigrid methods. </title> <journal> Acta Nu-merica, </journal> <pages> pages 285-326, </pages> <year> 1993. </year> <month> 22 </month>
Reference-contexts: Research on preconditioners for linear systems continues to be very active <ref> [1, 3, 22, 28, 37, 55] </ref>. <p> Research on preconditioning linear systems focuses on generating an approximate solution z of this preconditioning equation. Some of the common strategies include applying iterative methods on the preconditioning equation [2, 19, 35], constructing an approximation to A that is easy to invert <ref> [1, 3, 37, 55] </ref>, and constructing an approximation to the inverse of A [9, 20, 22]. This paper will attempt to exploit results of this research and apply them to precondition eigenvalue problems.
References-found: 55

