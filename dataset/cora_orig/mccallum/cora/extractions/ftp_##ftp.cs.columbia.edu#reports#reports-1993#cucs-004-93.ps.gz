URL: ftp://ftp.cs.columbia.edu/reports/reports-1993/cucs-004-93.ps.gz
Refering-URL: http://www.cs.columbia.edu/~library/1993.html
Root-URL: http://www.cs.columbia.edu
Email: li@cs.columbia.edu kar@cs.columbia.edu  
Title: On the Cost of Transitive Closures in Relational Databases  
Author: Zhe Li Kenneth A. Ross 
Address: New York, NY 10027 New York, NY 10027  
Affiliation: Computer Science Department Computer Science Department Columbia University Columbia University  
Abstract: Technical Report No. CUCS-004-93 February 1993 Abstract We consider the question of taking transitive closures on top of pure relational systems (Sybase and Ingres in this case). We developed three kinds of transitive closure programs, one using a stored procedure to simulate a built-in transitive closure operator, one using the C language embedded with SQL statements to simulate the iterated execution of the transitive closure operation, and one using Floyd's matrix algorithm to compute the transitive closure of an input graph. By comparing and analyzing the respective performances of their different versions in terms of elapsed time spent on taking the transitive closure, we identify some of the bottlenecks that arise when defining the transitive closure operator on top of existing relational systems. The main purpose of the work is to estimate the costs of taking transitive closures on top of relational systems, isolate the different cost factors (such as logging, network transmission cost, etc.), and identify some necessary enhancements to existing relational systems in order to support transitive closure operation efficiently. We argue that relational databases should be augmented with efficient transitive closure operators if such queries are made frequently. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho and J. D. Ullman. </author> <title> Universality of data retrieval languages. </title> <booktitle> In 6th ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 110-120, </pages> <year> 1979. </year>
Reference-contexts: 1 Introduction It has been argued in <ref> [1, 9] </ref> that declarativeness and stronger expressive power are important features of deductive database systems. Declarative formulations of queries allow for a clear expression of exactly what should be the answer to a query instead of how to get the answer tuples. <p> 1 Introduction It has been argued in [1, 9] that declarativeness and stronger expressive power are important features of deductive database systems. Declarative formulations of queries allow for a clear expression of exactly what should be the answer to a query instead of how to get the answer tuples. In <ref> [1] </ref> it was shown that the least fixpoint operation is not expressible by relational algebra and relational calculus. Typical relational query languages (such as SQL/QUEL) have the favorable property of declarativeness but lack the expressive power for the least 2 fixpoint operation.
Reference: [2] <author> F. Bancilhon, D. Maier, Y. Sagiv, and J. D. Ullman. </author> <title> Magic sets and other strange ways to implement logic programs. </title> <booktitle> In Proceedings of the Fifth ACM Symposium on Principles of Database Systems, </booktitle> <year> 1986. </year>
Reference-contexts: Finally in section 8 we discuss the conclusions we draw from this experiment and some practical issues related to taking transitive closure operation. 2 Algorithm, Sample Database and Environment In the context of a deductive database, many techniques have recently been proposed for optimizing recursive queries <ref> [3, 2, 4] </ref>. The query evaluation method adopted in our experiment is "semi-naive" evaluation [3].
Reference: [3] <author> Francois Bancilhon and Raghu Ramakrishnan. </author> <title> An amateur's introduction to recursive query processing strategies. </title> <booktitle> In Proceedings o the ACM SIGMOD 1986 International Conference on Management of Data, </booktitle> <pages> pages 16-52, </pages> <year> 1986. </year>
Reference-contexts: Finally in section 8 we discuss the conclusions we draw from this experiment and some practical issues related to taking transitive closure operation. 2 Algorithm, Sample Database and Environment In the context of a deductive database, many techniques have recently been proposed for optimizing recursive queries <ref> [3, 2, 4] </ref>. The query evaluation method adopted in our experiment is "semi-naive" evaluation [3]. <p> The query evaluation method adopted in our experiment is "semi-naive" evaluation <ref> [3] </ref>.
Reference: [4] <author> C. Beeri and R. Ramakrishnan. </author> <title> On the power of magic. </title> <journal> Journal of Logic Programming, </journal> <volume> 10 </volume> <pages> 255-300, </pages> <year> 1991. </year> <note> Preliminary version appeared in the 6th ACM Symposium on Principles of Database Systems, </note> <year> 1987. </year>
Reference-contexts: Finally in section 8 we discuss the conclusions we draw from this experiment and some practical issues related to taking transitive closure operation. 2 Algorithm, Sample Database and Environment In the context of a deductive database, many techniques have recently been proposed for optimizing recursive queries <ref> [3, 2, 4] </ref>. The query evaluation method adopted in our experiment is "semi-naive" evaluation [3].
Reference: [5] <author> Jiawei Han and Hongjun Lu. </author> <title> Some performance results on recursive query processing in relational database systems. </title> <booktitle> In IEEE 1986 International Conference on Data Engineering, </booktitle> <pages> pages 533-541, </pages> <year> 1986. </year>
Reference-contexts: For the single source case, the relations are very small, thus the saving of join cost achieved by indexing is not that obvious. 7 Comparisons with Other Related Work Some other work has done regarding the performance analysis of taking transitive closures on relational systems <ref> [5, 7, 11, 10] </ref>. In [5], both analytical and experimental results are presented to suggest some useful heuristics for efficient recursive database processing. <p> In <ref> [5] </ref>, both analytical and experimental results are presented to suggest some useful heuristics for efficient recursive database processing.
Reference: [6] <author> Ellis Horowitz. </author> <title> Fundamentals of Data Structures in C. </title> <publisher> Computer Science Press, </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: developed three kinds of transitive closure programs, one using a stored procedure 1 to simulate a built-in transitive closure operator, one using the C language embedded with SQL statements to simulate the iterated execution of a transitive closure operation, and one using the version of Floyd's matrix algorithm presented in <ref> [6] </ref> to compute the transitive closure of an input graph. Floyd's algorithm will serve as an "ideal" algorithm for performing the transitive closure. By seeing how far from the ideal the results for transitive closures involving Ingres and Sybase come, we can demonstrate the presence of a significant performance gap.
Reference: [7] <author> Hongjun Lu, Krishna Mikkilineni, and James Richardson. </author> <title> Design and evaluation of algorithms to compute the transitive closure of a database relation. </title> <booktitle> In Proceedings of the Third International Conference on Data Engineering, </booktitle> <pages> pages 112-119, </pages> <year> 1987. </year>
Reference-contexts: For the single source case, the relations are very small, thus the saving of join cost achieved by indexing is not that obvious. 7 Comparisons with Other Related Work Some other work has done regarding the performance analysis of taking transitive closures on relational systems <ref> [5, 7, 11, 10] </ref>. In [5], both analytical and experimental results are presented to suggest some useful heuristics for efficient recursive database processing. <p> Among them, performing selection first, making use of wavefront relations (saving previous processing results to avoid redundant processing), and grouping those joins which reduce the size of intermediate results are demonstrated to be beneficial on a WISS (Wisconsin Storage System) platform. In <ref> [7] </ref>, an adaptation of Warren's algorithm [12] is proposed, its performance is compared with that of an iterative algorithm and an improved version of the iterative algorithm (logarithmic). They evaluate the performance of the algorithms for different source relation sizes, available memory sizes, join selectivities, and maximum path length.
Reference: [8] <author> G. Phipps, M. Derr, and K. A. Ross. </author> <title> Glue-Nail: A deductive database system. </title> <booktitle> In Proceedings of the ACM-SIGMOD International Conference on Management of Data, </booktitle> <year> 1991. </year>
Reference-contexts: Aside from the impedance mismatch problem (incompatible data types and separate compiler optimizations) and loss of declarativeness (an ad hoc evaluation algorithm has to be chosen and specified procedurally), this paradigm also suffers a severe performance penalty. We quantify this performance penalty in this paper. It was noted in <ref> [8] </ref> that attempting to build a deductive database system on top of an existing relational system is a mistake. In a relational database, the relations are typically large in size and persistent in nature. In deductive database things are just the opposite.
Reference: [9] <author> J. D. Ullman. </author> <title> Principles of Database and Knowledge Base Systems. </title> <publisher> Computer Science Press, </publisher> <address> Rockville, MD, </address> <year> 1989. </year> <title> (Two volumes). </title>
Reference-contexts: 1 Introduction It has been argued in <ref> [1, 9] </ref> that declarativeness and stronger expressive power are important features of deductive database systems. Declarative formulations of queries allow for a clear expression of exactly what should be the answer to a query instead of how to get the answer tuples. <p> In a relational database, the relations are typically large in size and persistent in nature. In deductive database things are just the opposite. Most least fixpoint query evaluation algorithms try to simulate sideways information passing <ref> [9] </ref> to restrict redundant computation. These algorithms usually create some supplementary relations to pass bindings and these relations are typically small and temporary in nature. <p> Usually the updates on these relations have to be logged for future recovery if the system crashes. Since the relations are long-lived, caching them will improve the performance. In contrast, when evaluating a transitive closure query, especially when using algorithms simulate sideways information passing <ref> [9] </ref>, many supplementary relations are generated. These relations are typically small and short-lived (usually their lifetime lasts only one iteration). The majority of operations performed on these relations are updates. Logging seems to be overkill since the disk I/O incurred by logging is exorbitant.
Reference: [10] <author> P. Valduriez. </author> <title> Join indices. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 12(2) </volume> <pages> 219-246, </pages> <year> 1987. </year>
Reference-contexts: For the single source case, the relations are very small, thus the saving of join cost achieved by indexing is not that obvious. 7 Comparisons with Other Related Work Some other work has done regarding the performance analysis of taking transitive closures on relational systems <ref> [5, 7, 11, 10] </ref>. In [5], both analytical and experimental results are presented to suggest some useful heuristics for efficient recursive database processing. <p> The general conclusion is that no algorithm has uniformly superior performance; the adaptation of Warren's algorithm is superior when the source and result relations can be held in main memory. In <ref> [11, 10] </ref>, a new data structure called "join index" is proposed to facilitate the relational join operation. A join index is a binary relation that captures the semantic links that exist between tuples. <p> So hashing or indexing on base relation would be cost effective to reduce the join cost. For instance, join indices <ref> [11, 10] </ref> would give good overall performance since it only focus on the relevant join attributes and delay the actual tuple fetching until the time to assemble the final result.
Reference: [11] <author> P. Valduriez and H. Boral. </author> <title> Evaluation of recursive queries using join indices. </title> <booktitle> In Proceedings of the First International Conference on Expert Database Systems, </booktitle> <pages> pages 271-293, </pages> <year> 1986. </year>
Reference-contexts: For the single source case, the relations are very small, thus the saving of join cost achieved by indexing is not that obvious. 7 Comparisons with Other Related Work Some other work has done regarding the performance analysis of taking transitive closures on relational systems <ref> [5, 7, 11, 10] </ref>. In [5], both analytical and experimental results are presented to suggest some useful heuristics for efficient recursive database processing. <p> The general conclusion is that no algorithm has uniformly superior performance; the adaptation of Warren's algorithm is superior when the source and result relations can be held in main memory. In <ref> [11, 10] </ref>, a new data structure called "join index" is proposed to facilitate the relational join operation. A join index is a binary relation that captures the semantic links that exist between tuples. <p> The idea is to apply all complex operations (join, union) on join indices and to access the data at the very end. Because the length of an index is shorter than that of a tuple, the size of the data to be iteratively joined will be reduced considerably. In <ref> [11] </ref>, some analytical results are shown for two transitive closure algorithms, "brute-force" and "logarithmic". The relative performances between two versions (using join index vs. not using join index) for each algorithm are contrasted. <p> So hashing or indexing on base relation would be cost effective to reduce the join cost. For instance, join indices <ref> [11, 10] </ref> would give good overall performance since it only focus on the relevant join attributes and delay the actual tuple fetching until the time to assemble the final result.
Reference: [12] <author> H.S. Warren. </author> <title> A modification of warshall's algorithm for the transitive closure of binary relations. </title> <journal> CACM, </journal> <volume> 18(4) </volume> <pages> 218-220, </pages> <year> 1975. </year>
Reference-contexts: Among them, performing selection first, making use of wavefront relations (saving previous processing results to avoid redundant processing), and grouping those joins which reduce the size of intermediate results are demonstrated to be beneficial on a WISS (Wisconsin Storage System) platform. In [7], an adaptation of Warren's algorithm <ref> [12] </ref> is proposed, its performance is compared with that of an iterative algorithm and an improved version of the iterative algorithm (logarithmic). They evaluate the performance of the algorithms for different source relation sizes, available memory sizes, join selectivities, and maximum path length.
References-found: 12

