URL: http://vibes.cs.uiuc.edu/Publications/Theses/TottyPhD.ps.gz
Refering-URL: http://vibes.cs.uiuc.edu/Publications/Theses/theses.htm
Root-URL: http://www.cs.uiuc.edu
Title: c  
Author: flCopyright by Brian Keith Totty 
Date: 1994  
Abstract-found: 0
Intro-found: 1
Reference: [AB86] <author> James Archibald and Jean-Loup Baer. </author> <title> Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4) </volume> <pages> 273-298, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: Private caches have been thoroughly studied as a mechanism for reducing remote memory access latency [Smi82]. Mechanisms to keep cached data consistent have been one of the major fields of research in computer systems design in the past decade. Public caches [Sta91] and snooping caches <ref> [AB86] </ref> are simple architectural solutions, but do not scale to massive numbers of processors. Directory based coherence techniques [CF78, ASHH88, CFKA90] are a popular solution, being used in the DASH [LLG + 92] and Alewife [Aea91] multiprocessors. Directory techniques maintain tables of current cache line copies.
Reference: [Aea91] <author> Anant Agarwal and et. al. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <type> MIT/LCS Technical Memo 454, </type> <institution> Massachusetts Institute of Technology, Laboratory for Computer Science, </institution> <year> 1991. </year>
Reference-contexts: These two models are sketched in Figure 1.1 Because of the commonality of the models, it is possible to build "mixed-mode" systems, supporting both direct access and message passing communication models. Indeed, several emerging distributed-memory architectures provide efficient support for both models via a common communication subsystem <ref> [NWD93, Aea91] </ref>. In such a mixed-mode system, a programmer is free to choose the communication model based on performance and programmability. For example, discrete event simulations often model inherently asynchronous communications, making message passing a logical communication model. <p> Public caches [Sta91] and snooping caches [AB86] are simple architectural solutions, but do not scale to massive numbers of processors. Directory based coherence techniques [CF78, ASHH88, CFKA90] are a popular solution, being used in the DASH [LLG + 92] and Alewife <ref> [Aea91] </ref> multiprocessors. Directory techniques maintain tables of current cache line copies. The memory controller initiates a coherence protocol on each write, sending directed messages through the communication fabric to implement the policy. Directories are typically placed in fixed locations. <p> Furthermore, hardware schemes are traditionally inflexible, because design decisions and policies are hardwired. There is a current trend in multiprocessor architecture research to design machines with at most a small amount of custom hardware, fast traps, and to implement the bulk of the coherence protocols in software <ref> [Aea91, RHL + 93, RLW94] </ref>. Operating system global memory was first applied to large-scale distributed memory multiprocessors in the IVY system [LH89, Li86] which exploited paging hardware and software page fault handlers to provide a paged, virtual memory over distributed memory multiprocessors. <p> Munin permits the assignment of coherence protocols to individual pages of the virtual memory system. The Mach [Ras86] operating system also permits a degree of tunability by providing external pagers which implement page-replacement and other paging services. Newer, hybrid systems, such as Alewife <ref> [Aea91] </ref> and the proposed Typhoon [RLW94] make judicious use of hardware and rely on software for portions of the protocol. <p> The operating system may also require costly saving and reconstruction of virtual process state. If the trap is to be serviced by user-level software, some operating systems require yet more overhead. Emerging systems with fast context switching <ref> [NWD93, Aea91] </ref> allow dispatch and return from traps in a few tens of processor cycles. In contrast, servicing traps with UNIX user-level signal handlers can require tens of thousands of cycles. <p> We choose delay values based on the representative values collected in x3.3. We selected 10 cycles, 1000 cycles, and 10000 cycles for the small, medium, and large values of dispatch cost D. The small value represents systems such as Alewife <ref> [Aea91] </ref> which are optimized for very fast traps, and procedure call interfaces (see Table 3.4). The medium value is appropriate for workstation signal handlers (as demonstrated in Table 3.3). Expensive signal handlers, like those on the Intel Paragon are represented by the large value. <p> If a Trap policy is used, then a higher latency signal handler is used to catch the fault and invoke the handler. Trap based virtual memory support is a conventional mechanism for shared virtual memory systems <ref> [LH89, BCZ89, LP93, Aea91, LLG + 92] </ref>. In such systems, pages or cache lines which are not locally present, or cannot be be accessed in a particular mode, are protected. Subsequent accesses will fault, invoking a handler. The handler will examine the memory address, and perform the requested operation.
Reference: [Arl88] <author> Ramune Arlauskas. </author> <title> iPSC/2 System: A Second Generation Hypercube. </title> <booktitle> In Proceed ings of the 3 rd Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 38-42, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: The Cosmic Cube provided subroutine libraries to send regions of memory to another processing node, which then performs a matching receive to obtain the data into a local memory buffer. The Cosmic Cube led to the commercial development of the Intel hypercube multiprocessors <ref> [Rat85, Arl88, Bok90] </ref>, and the NX message passing system [Int94], later used for the Intel Touchstone [Lil90] and Paragon [Int91] systems. Other vendors provide libraries with different message passing conventions. PVM [Oak94] and UNIX interprocess communication facilities (such as sockets [Bac86]) also serve as message passing subsystems.
Reference: [ASHH88] <author> Anant Agarwal, Richard Simoni, John Hennessy, and Mark Horowitz. </author> <title> An Evalua tion of Directory Schemes for Cache Coherence. </title> <booktitle> In Proceedings of the 15 th International Symposium on Computer Architecture, </booktitle> <pages> pages 280-289, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Mechanisms to keep cached data consistent have been one of the major fields of research in computer systems design in the past decade. Public caches [Sta91] and snooping caches [AB86] are simple architectural solutions, but do not scale to massive numbers of processors. Directory based coherence techniques <ref> [CF78, ASHH88, CFKA90] </ref> are a popular solution, being used in the DASH [LLG + 92] and Alewife [Aea91] multiprocessors. Directory techniques maintain tables of current cache line copies. The memory controller initiates a coherence protocol on each write, sending directed messages through the communication fabric to implement the policy.
Reference: [Bac86] <author> Maurice J. Bach. </author> <title> The Design of the UNIX Operating System. </title> <publisher> Prentice-Hall, </publisher> <year> 1986. </year>
Reference-contexts: Other vendors provide libraries with different message passing conventions. PVM [Oak94] and UNIX interprocess communication facilities (such as sockets <ref> [Bac86] </ref>) also serve as message passing subsystems. The MPI standard [For94] is an effort to stardardize message passing interfaces across platforms. 11 The MIT J-Machine [NWD93] provides better hardware support for message passing, dras-tically reducing the number of instructions required to send and receive messages. <p> Processors connect to distributed memories through a multi-stage Omega network. The `Local Area Network' represents a typical distributed system of contemporary workstations. As a case study, we selected a lightly loaded Ethernet of Sun SPARCstation 2 workstations in an academic research environment, communicating via UNIX sockets <ref> [Bac86] </ref>. The LAN represents an extreme example of a distributed memory multiprocessor, yielding extremely high latency, and extremely low throughput. We have collected approximate values of instruction rates and single link network characteristics, for the architectures described above.
Reference: [Ban88] <author> Utpal Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <booktitle> Parallel Processing and Fifth Generation Computing. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1988. </year>
Reference-contexts: Recently there has been an avalanche of published research on compilation techniques to provide the illusion of shared memory. The vast majority of these techniques exploit dependence analysis technology <ref> [Wol82, Ban88, Zim91] </ref> to extract and aggregate communication from nested loops of scientific programs. The generated programs are loosely synchronous in that they are data parallel, but synchronize only at communication points.
Reference: [Bar89] <author> Joel F. Bartlett. Scheme!C, </author> <title> a Portable Scheme-to-C Compiler. </title> <note> WRL Research Report 89/1, </note> <institution> Digital Equipment Corporation, Western Research Laboratory, </institution> <year> 1989. </year>
Reference-contexts: The translator converts programs composed in Poli-C, an extended version of C, into conventional C source programs. The translator is implemented in the Scheme dialect of Lisp [RCea], and was compiled to an executable using the DEC Scheme!C scc compiler <ref> [Bar89] </ref>. The translation process synthesizes code for run-time data structure initialization, remote procedure call stubs, and distributed data structure operations. The resulting C target code is then compiled by a convential 104 C compiler and linked with the Poli-C library, producing the resulting node executable.
Reference: [BCZ89] <author> John K. Bennett, John B. Carter, and Willy Zwaenepoel. Munin: </author> <title> Shared Memory for Distributed Memory Multiprocessors. </title> <type> Technical Report COMP TR89-91, </type> <institution> Rice University, Department of Computer Science, </institution> <month> April </month> <year> 1989. </year>
Reference-contexts: Our distributed data structures employ software emulation of paging hardware to support arbitrary page sizes, permitting control over the granularity of sharing and communication. Furthermore, software page tables permits the optimization of local memory hierarchy performance. With the exception of a few systems <ref> [BCZ89, RLW94] </ref>, shared memory implementations provide a single remote memory access policy across the entire system. Our distributed data structures permit the assignment of individual, user-level access policies to each data structure. <p> In addition, pages are sufficiently large to amortize the large communication latency existing on early multicomputers. The KOAN sys 13 tem [LP93] is a more aggressive implementation of paged virtual memory for Intel hypercube multiprocessors. DUnX [RPLE90], Clouds [RAK89], and Munin <ref> [BCZ89] </ref> are examples of distributed operating systems which support forms of object-specific access policy control. The Munin project [BCZ89] was an early exploitation of software coherence protocols for system tuning. Munin permits the assignment of coherence protocols to individual pages of the virtual memory system. <p> The KOAN sys 13 tem [LP93] is a more aggressive implementation of paged virtual memory for Intel hypercube multiprocessors. DUnX [RPLE90], Clouds [RAK89], and Munin <ref> [BCZ89] </ref> are examples of distributed operating systems which support forms of object-specific access policy control. The Munin project [BCZ89] was an early exploitation of software coherence protocols for system tuning. Munin permits the assignment of coherence protocols to individual pages of the virtual memory system. The Mach [Ras86] operating system also permits a degree of tunability by providing external pagers which implement page-replacement and other paging services. <p> This idea has been independently advanced in other recent projects <ref> [BCZ89, RAK89, RLW94, KOH + 94] </ref>. Our data structure policies vary in message quantity and message size. Both algorithmic locality and access policy affect the message count and the message volume. Communication latency and bandwidth will further affect the total execution time of the algorithm. <p> The prototype Poli-C system currently supports the Intel iPSC/2, iPSC/860, and Paragon XP/S distributed memory multiprocessors. Poli-C extends the native message-passing communication model provided by these systems to permit a variety of globally-accessible data structure mechanisms. Similar to other virtually-shared distributed memory implementations for multicomputers <ref> [LH89, BCZ89, CG89] </ref>, Poli-C dynamically moves and replicates data between the memories of the multicomputer. There are three primary distinctions of the Poli-C style of distributed data management. <p> First, whereas most virtually-shared distributed memory systems offer a single, system-imposed data management policy for all global data structures, Poli-C permits the programmer to assign data-management policies to individual data structures, and to tune the performance to data-structure usage (in a fashion similar to the pioneering Munin <ref> [BCZ89] </ref> work). Secondly, the data management policies are constructed and applied at the user-level, as opposed to being compiled into the operating system. This encourages the construction and use of a rich set of data management policies. <p> If a Trap policy is used, then a higher latency signal handler is used to catch the fault and invoke the handler. Trap based virtual memory support is a conventional mechanism for shared virtual memory systems <ref> [LH89, BCZ89, LP93, Aea91, LLG + 92] </ref>. In such systems, pages or cache lines which are not locally present, or cannot be be accessed in a particular mode, are protected. Subsequent accesses will fault, invoking a handler. The handler will examine the memory address, and perform the requested operation.
Reference: [BJR88] <author> M. Balakrishnan, R. Jain, and C. S. Raghavendra. </author> <title> On Array Storage for Conflict-Free Memory Access For Parallel Processors. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 103-107, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Early work involved the organization of data for efficient access to interleaved memories. The problem became more severe with the advent of multiprocessors. The way data is mapped into a linear memory space was shown to be critical in avoiding memory conflicts among competing data access streams <ref> [Law75, BJR88] </ref>. Data organization has also been shown to be important in the placement of data within pages of a virtual memory system [Sta84]. By grouping related objects within pages, thrashing is reduced and prefetch benefits are gained.
Reference: [BKA93] <author> Rajeev Barua, David Kranz, and Anant Agarwal. </author> <title> Addressing Partitioned Arrays in Distributed Memory Multiprocessors | the Software Virtual Memory Approach. </title> <month> November </month> <year> 1993. </year> <note> Draft Submitted for Publication. </note>
Reference-contexts: Page size can be chosen to balance communication performance (larger is usually better) against false sharing (smaller is usually better). The simulation of page tables generates additional overhead, though we show in this thesis, the overhead is often acceptable, for the flexibility we obtain. And, as suggested in <ref> [BKA93] </ref>, software controlled page tables can also be useful for implement complex data distributions which are difficult to express in closed form, and for optimizing the performance of the local memory hierarchy by insightfully allocating page frames. 16 Chapter 3 Analysis of Access Interfaces This chapter explores the implications of distributed
Reference: [Bok90] <author> Shahid Bokhari. </author> <title> Communication Overhead on the Intel iPSC-860 Hypercube. </title> <type> ICASE Interim Report 10, </type> <institution> Institute for Computer Applications in Science and Engineering (ICASE), </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: The Cosmic Cube provided subroutine libraries to send regions of memory to another processing node, which then performs a matching receive to obtain the data into a local memory buffer. The Cosmic Cube led to the commercial development of the Intel hypercube multiprocessors <ref> [Rat85, Arl88, Bok90] </ref>, and the NX message passing system [Int94], later used for the Intel Touchstone [Lil90] and Paragon [Int91] systems. Other vendors provide libraries with different message passing conventions. PVM [Oak94] and UNIX interprocess communication facilities (such as sockets [Bac86]) also serve as message passing subsystems.
Reference: [CF78] <author> Lucien M. Censier and Paul Feautrier. </author> <title> A New Solution to Coherence Problems in Multicache Systems. </title> <journal> IEEE Computer, </journal> <volume> C-27(12):1112-1118, </volume> <month> December </month> <year> 1978. </year> <month> 156 </month>
Reference-contexts: Mechanisms to keep cached data consistent have been one of the major fields of research in computer systems design in the past decade. Public caches [Sta91] and snooping caches [AB86] are simple architectural solutions, but do not scale to massive numbers of processors. Directory based coherence techniques <ref> [CF78, ASHH88, CFKA90] </ref> are a popular solution, being used in the DASH [LLG + 92] and Alewife [Aea91] multiprocessors. Directory techniques maintain tables of current cache line copies. The memory controller initiates a coherence protocol on each write, sending directed messages through the communication fabric to implement the policy.
Reference: [CFKA90] <author> David Chaiken, Craig Fields, Kiyoshi Kurihara, and Anant Agarwal. </author> <title> Directory--Based Cache Coherence in Large-Scale Multiprocessors. </title> <booktitle> IEEE Computer, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: Mechanisms to keep cached data consistent have been one of the major fields of research in computer systems design in the past decade. Public caches [Sta91] and snooping caches [AB86] are simple architectural solutions, but do not scale to massive numbers of processors. Directory based coherence techniques <ref> [CF78, ASHH88, CFKA90] </ref> are a popular solution, being used in the DASH [LLG + 92] and Alewife [Aea91] multiprocessors. Directory techniques maintain tables of current cache line copies. The memory controller initiates a coherence protocol on each write, sending directed messages through the communication fabric to implement the policy.
Reference: [CG89] <author> N. Carriero and D. Gelernter. </author> <title> How to Write Parallel Programs: A Guide to the Perplexed. </title> <journal> ACM Computing Surveys, </journal> <pages> pages 323-357, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: The prototype Poli-C system currently supports the Intel iPSC/2, iPSC/860, and Paragon XP/S distributed memory multiprocessors. Poli-C extends the native message-passing communication model provided by these systems to permit a variety of globally-accessible data structure mechanisms. Similar to other virtually-shared distributed memory implementations for multicomputers <ref> [LH89, BCZ89, CG89] </ref>, Poli-C dynamically moves and replicates data between the memories of the multicomputer. There are three primary distinctions of the Poli-C style of distributed data management.
Reference: [Che89] <author> Ding-Kai Chen. </author> <title> MaxPar: An Execution Driven Simulator for Studying Parallel Systems. </title> <type> CSRD Report 917, </type> <institution> University of Illinois Center for Supercomputing Research and Development, </institution> <month> October </month> <year> 1989. </year>
Reference-contexts: Such instrumentation is a painstaking and error-prone task. Automation is necessary. We developed a tool called ForTrace to automatically instrument source code. ForTrace is a source-to-source Fortran preprocessor with an associated library of run-time routines. ForTrace traces its distant origins to MaxPar <ref> [Che89] </ref>, a tool designed at the Center for Supercomputing Research and Development (CSRD). The ForTrace annotator consists of 6800 lines of C source code.
Reference: [Chi90] <author> Andrew Andai Chien. </author> <title> Concurrent Aggregates (CA): An Object-Oriented Language for Fine-Grained Message-Passing Machines. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: Object management schemes to efficiently locate and migrate objects in distributed databases have been described in [LS80]. Some of these techniques have been applied to a segment-based, distributed memory multiprocessor in [Tot88]. The Concurrent Aggregates system <ref> [Chi90] </ref> provides mechanisms to construct distributed objects with user-defined access behavior. Access policy can be tailored to object needs.
Reference: [CKP] <author> Andrew Chien, Vijay Karamcheti, and John Plevyak. </author> <title> The Concert System: Compiler and Runtime Support for Fine-Grained Concurrent Object-Oriented Languages. </title> <type> UIUC DCS Technical Report R-93-1815, </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science. </institution>
Reference-contexts: Some of these techniques have been applied to a segment-based, distributed memory multiprocessor in [Tot88]. The Concurrent Aggregates system [Chi90] provides mechanisms to construct distributed objects with user-defined access behavior. Access policy can be tailored to object needs. The Concert system <ref> [CKP] </ref> exploits compiler analysis and efficient run-time systems to improve the performance of object-based systems. 2.2.3 Shared Memory Shared-memory systems are appealing because they foster an illusion of a single, global memory for distributed memory multiprocessors. There are many techniques to provide such an illusion of shared memory.
Reference: [Cor92] <institution> Thinking Machines Corporation. </institution> <type> Connection Machine CM-5 Technical Summary, </type> <month> November </month> <year> 1992. </year> <note> Revised Edition. </note>
Reference-contexts: The J-Machine also provides executable messages that invoke code upon arrival. Such active messages [vECGS92] have been found useful, and have been implemented on other systems include the Thinking Machine's CM-5 <ref> [Cor92] </ref>. 2.2.2 Object-Based Systems Conventional message passing systems isolate the data from the communication services. Messages send bulk regions of one memory space to another. Object-based systems integrate communication services into the memory model, supporting object-oriented communication between memory objects.
Reference: [DBMS79] <author> J. J. Dongarra, J. R. Bunch, C. B. Moler, and G. W. Stewart. </author> <title> LINPACK User's Guide. </title> <publisher> SIAM, </publisher> <year> 1979. </year>
Reference-contexts: To Upper Hessenberg Form rgeig Find Eigenvectors Of General Matrix Sparse Matrix Kernels smvmult Sparse Matrix/Vector Multiply sgentojag Convert Sparse Matrix To Jagged Diagonal Format sfelim Forward Elimination Of Sparse System Miscellaneous Scientific Algorithms fft 2-D Fast Fourier Transform nws Wavefront Needleman-Wunsch-Sellers Sequence Match Table 4.2: Scientific Algorithm Benchmark Set <ref> [DBMS79] </ref> and EISPACK [Smi76] libraries. These dense matrix codes provide a set of easily analyzed, generally regular access patterns. Although contemporary compilation schemes have been somewhat successful at static data management, for a large domain of applications, purely compile-time analysis cannot make acceptable data management decisions. <p> For these reasons, this benchmark performs relatively poorly, with execution time slowdown similar to the degree of code explosion, as summarized in Table 7.6. 7.2.3 SGEFA The sgefa benchmark, is a variant of the Linpack <ref> [DBMS79] </ref> SGEFA Gaussian elimination routine.
Reference: [DSB91] <author> Raja Das, Joel Saltz, and Harry Berryman. </author> <title> A Manual for PARTI Runtime Primitives. </title> <type> ICASE Interim Report 17, </type> <institution> Institute for Computer Applications in Science and Engineering (ICASE), </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: If the object is sufficiently large, the communication latency can be amortized by the volume of data sent. However, for programs with more irregular data accesses, achieving successful latency tolerance with object transfer is more difficult, because the accesses may leapfrog through numerous objects. 8.2.1 Coherent Scatter-Gather Competitive approaches <ref> [MMB90, DSB91] </ref> gather index lists at runtime and subsequently perform bulk transfers of data. While such schemes can more effectively tolerate latencies in irregular programs, they are not integrated into a general-purpose demand-fetch environment, and they do not support sophisticated caching.
Reference: [Ede92] <author> Daniel R. Edelson. </author> <title> Fault Interpretation: Fine-Grain Monitoring of Page Accesses. </title> <type> Technical Report UCSC-CRL-92-32, </type> <institution> The University of California at Santa Cruz, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: The data in Table 3.3 summarizes the time to service and return from a virtual memory protection fault using relatively heavyweight UNIX signal handlers on three commercial systems. The benchmarks are variants of those presented in <ref> [Ede92] </ref>. In a typical signal handler use, accesses to protected pages invoke a user-level signal handler. The data in Table 3.3 indicates delays to invoke and return from such a signal handler.
Reference: [For94] <author> Message Passing Interface Forum. </author> <title> Mpi: A message-passing interface standard. </title> <institution> Technical Report Computer Science Department Technical Report CS-94-230, University of Tennessee, Knoxville, TN, </institution> <month> May 5 </month> <year> 1994. </year> <booktitle> To appear in the International Journal of Supercomputing Applications, </booktitle> <volume> Volume 8, Number 3/4, </volume> <year> 1994. </year>
Reference-contexts: Other vendors provide libraries with different message passing conventions. PVM [Oak94] and UNIX interprocess communication facilities (such as sockets [Bac86]) also serve as message passing subsystems. The MPI standard <ref> [For94] </ref> is an effort to stardardize message passing interfaces across platforms. 11 The MIT J-Machine [NWD93] provides better hardware support for message passing, dras-tically reducing the number of instructions required to send and receive messages. The J-Machine also provides executable messages that invoke code upon arrival.
Reference: [Ger90] <author> Michael Gerndt. </author> <title> Updating Distributed Variables in Local Computations, </title> <address> pages 171-193. </address> <publisher> John Wiley And Sons, </publisher> <month> September </month> <year> 1990. </year>
Reference-contexts: Data organization has also been shown to be important in the placement of data within pages of a virtual memory system [Sta84]. By grouping related objects within pages, thrashing is reduced and prefetch benefits are gained. Distributed-memory programming languages <ref> [RSW89, HKT91, Ger90, Lov93] </ref> support distribution annotations, which specify how the data structures are distributed to memories. For regular computations, the decomposition problem is typically viewed as one of domain alignment. <p> The vast majority of these techniques exploit dependence analysis technology [Wol82, Ban88, Zim91] to extract and aggregate communication from nested loops of scientific programs. The generated programs are loosely synchronous in that they are data parallel, but synchronize only at communication points. Fortran D [HKT91] and SUPERB <ref> [Ger90] </ref> are two compilation systems which synthesize data movement instructions by a dependence analysis of programs. Kali [MR90] and related projects [SCMB90] exploit run-time dependence analysis to extract communication patterns from programs which cannot be statically analyzed due to data dependent execution patterns.
Reference: [Gru88] <author> Dirk C. Grunwald. </author> <title> User's Guide to AWESIME II A Widely Extensible Simulation Environment. Internal Document, </title> <month> July </month> <year> 1988. </year>
Reference-contexts: of the parallel trace from the program in Figure 4.3 is shown in readable form in Figure 4.4. 4.2 The Policy Simulator: PoliSim To study the effects of various data management alternatives, we built a multi-threaded simulation system using the C++ programming language and the AWESIME-II discrete event simulation library <ref> [Gru88] </ref>. The simulation system is named PoliSim [Tot92], for "Policy Simulator." PoliSim consists of 10,000 lines of C++ code, in addition to the AWESIME-II and C++ libraries.
Reference: [Guz87] <author> Mark D. Guzzi. </author> <title> Cedar Fortran Programmer's Handbook. </title> <type> CSRD Report 601, </type> <institution> University of Illinois Center for Supercomputing Research and Development, </institution> <month> June </month> <year> 1987. </year>
Reference-contexts: The resulting programs are expressed in the parallel language Cedar Fortran <ref> [Guz87] </ref>. Manual modification was occasionally needed to augment the parallelism. The remainder of the parallel benchmarks were hand-coded parallel algorithms. 4.5 Summary Systems with steep memory hierarchies are subject to severe performance penalties if data is managed inappropriately.
Reference: [HKT91] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiler Optimizations for Fortran D on MIMD Distributed-Memory Machines. </title> <booktitle> In Proceedings of the 4 th Annual Conference on High-Performance Computing (Supercomputing '91), </booktitle> <pages> pages 86-100, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Data organization has also been shown to be important in the placement of data within pages of a virtual memory system [Sta84]. By grouping related objects within pages, thrashing is reduced and prefetch benefits are gained. Distributed-memory programming languages <ref> [RSW89, HKT91, Ger90, Lov93] </ref> support distribution annotations, which specify how the data structures are distributed to memories. For regular computations, the decomposition problem is typically viewed as one of domain alignment. <p> Explicit user maps can be used to associate elements with memories <ref> [HKT91] </ref>. Automatic partitioners can perform this partitioning using network flow methods or knowledge of application behavior. Automatic data organization is much more difficult. General layout optimization (the "shapes problem") has been shown to be NP-complete [Mac87]. <p> The vast majority of these techniques exploit dependence analysis technology [Wol82, Ban88, Zim91] to extract and aggregate communication from nested loops of scientific programs. The generated programs are loosely synchronous in that they are data parallel, but synchronize only at communication points. Fortran D <ref> [HKT91] </ref> and SUPERB [Ger90] are two compilation systems which synthesize data movement instructions by a dependence analysis of programs. Kali [MR90] and related projects [SCMB90] exploit run-time dependence analysis to extract communication patterns from programs which cannot be statically analyzed due to data dependent execution patterns.
Reference: [HL84] <author> P. Heidelberger and S. S. Lavenberg. </author> <title> Computer Performance Evaluation Methodology. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> C-33:1195-1220, </volume> <month> December </month> <year> 1984. </year>
Reference-contexts: Second, for various access interface and architectural delays, we analyze exeucution time, to determine conditions when gains in locality offset additional costs. 46 4.1 Capturing Traces with ForTrace Memory reference (access) tracing is a standard approach for evaluating computer memory system performance <ref> [HL84, Smi82, SF89] </ref>. By recording an application program's accesses to data (a data memory reference trace) from an existing computer system, and then replaying the access trace to a simulator, we can obtain performance predictions for hypothetical computer systems under actual program workloads.
Reference: [Int91] <author> Intel Corporation. </author> <title> Paragon XP/S Product Overview, </title> <booktitle> 1991. </booktitle> <pages> 157 </pages>
Reference-contexts: The Cosmic Cube led to the commercial development of the Intel hypercube multiprocessors [Rat85, Arl88, Bok90], and the NX message passing system [Int94], later used for the Intel Touchstone [Lil90] and Paragon <ref> [Int91] </ref> systems. Other vendors provide libraries with different message passing conventions. PVM [Oak94] and UNIX interprocess communication facilities (such as sockets [Bac86]) also serve as message passing subsystems.
Reference: [Int94] <author> Intel Corporation. </author> <title> Paragon C System Calls Reference Manual, </title> <month> June </month> <year> 1994. </year>
Reference-contexts: The Cosmic Cube led to the commercial development of the Intel hypercube multiprocessors [Rat85, Arl88, Bok90], and the NX message passing system <ref> [Int94] </ref>, later used for the Intel Touchstone [Lil90] and Paragon [Int91] systems. Other vendors provide libraries with different message passing conventions. PVM [Oak94] and UNIX interprocess communication facilities (such as sockets [Bac86]) also serve as message passing subsystems. <p> This section examines some of these "miss costs" in the context of the particular Poli-C implementation. 6.4.3 Message Reception Mechanisms As discussed in x6.3.4.6, Poli-C supports both interrupt-driven and polling-driven scheduling of incoming message activations. The Poli-C prototype is layered on top of the Intel NX message-passing system <ref> [Int94] </ref>. NX provides routines to support sundry paradigms for message sending and reception. Three such paradigms might be called blocking receive, polled receive, and interrupt receive. The blocking receive constructs explicitly wait for a message to arrive, blocking the current instruction thread.
Reference: [Kal] <author> L. V. Kale. </author> <title> A Tutorial Introduction to Charm. </title> <note> Can be accessed with the WWW URL http://charm.cs.uiuc.edu/manuals.html. </note>
Reference-contexts: Object-based systems integrate communication services into the memory model, supporting object-oriented communication between memory objects. This approach provides protection and abstraction, while controlling the units of communication. In object-based systems, messages are directed to objects rather than memory nodes. Charm <ref> [Kal] </ref> is a simple, portable, object-based system which permits communication between active objects. Information sharing extensions [KS94] have been added to the Charm system. These extensions support an illusion of shared data across distributed name spaces using "information-sharing abstractions," that are tuned to particular types of conceptual sharing patterns.
Reference: [KLS90] <author> Kathleen Knobe, Joan D. Lukas, and Guy L. Steele Jr. </author> <title> Data Optimization: Allocation of Arrays to Reduce Communication on SIMD Machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 102-118, </pages> <year> 1990. </year>
Reference-contexts: Automatic data organization is much more difficult. General layout optimization (the "shapes problem") has been shown to be NP-complete [Mac87]. Some success has been made for automatic data organization for very regular data accesses which occur in systolic array computations <ref> [KLS90, LC90] </ref>. 2.2 Data Motion and Access It is not generally possible to select a single, static allocation of data that eliminates communication. For this reason, once data is assigned to the memories of a distributed memory processor, mechanisms are needed to read and write data in remote memories.
Reference: [KOH + 94] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Simoni, K. Gharachorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21 st International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <year> 1994. </year>
Reference-contexts: This idea has been independently advanced in other recent projects <ref> [BCZ89, RAK89, RLW94, KOH + 94] </ref>. Our data structure policies vary in message quantity and message size. Both algorithmic locality and access policy affect the message count and the message volume. Communication latency and bandwidth will further affect the total execution time of the algorithm.
Reference: [KS94] <author> L. V. Kale and A. B. Sinha. </author> <title> Information sharing in parallel programs. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: This approach provides protection and abstraction, while controlling the units of communication. In object-based systems, messages are directed to objects rather than memory nodes. Charm [Kal] is a simple, portable, object-based system which permits communication between active objects. Information sharing extensions <ref> [KS94] </ref> have been added to the Charm system. These extensions support an illusion of shared data across distributed name spaces using "information-sharing abstractions," that are tuned to particular types of conceptual sharing patterns. Object management schemes to efficiently locate and migrate objects in distributed databases have been described in [LS80].
Reference: [Kuc88] <institution> Kuck And Associates, Champaign, IL. </institution> <note> KAP User's Guide, Version 6, </note> <year> 1988. </year>
Reference-contexts: Cedar Fortran provides constructs for loop-level parallelism, including doall and doacross parallel loops, and await and advance synchronization primitives. The commercial parallelization tool KAP <ref> [Kuc88] </ref> converts sequential Fortran programs into Cedar Fortran, providing a simple and direct way of obtaining parallel benchmarks. KAP and ForTrace both use only a subset of the concurrency features of Cedar Fortran. Most importantly, both systems support parallel loops with only backward dependences. <p> 1,375,918 675,893 2,051,811 33% fft 64x64 215,046 120,832 335,878 36% Table 4.3: Benchmark Set Characteristics 4.4 The Parallelization of the Serial Algorithms A majority of the benchmarks discussed in x4.3 are members of popular uniprocessor scientific libraries that have been converted into parallel algorithms using the KAP Cedar Fortran preprocessor <ref> [Kuc88] </ref>. The resulting programs are expressed in the parallel language Cedar Fortran [Guz87]. Manual modification was occasionally needed to augment the parallelism. The remainder of the parallel benchmarks were hand-coded parallel algorithms. 4.5 Summary Systems with steep memory hierarchies are subject to severe performance penalties if data is managed inappropriately.
Reference: [Law75] <author> Duncan H. Lawrie. </author> <title> Access and Alignment of Data in an Array Processor. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-24(12):1145-1155, </volume> <month> December </month> <year> 1975. </year>
Reference-contexts: Early work involved the organization of data for efficient access to interleaved memories. The problem became more severe with the advent of multiprocessors. The way data is mapped into a linear memory space was shown to be critical in avoiding memory conflicts among competing data access streams <ref> [Law75, BJR88] </ref>. Data organization has also been shown to be important in the placement of data within pages of a virtual memory system [Sta84]. By grouping related objects within pages, thrashing is reduced and prefetch benefits are gained.
Reference: [LC90] <author> Jingke Li and Marina Chen. </author> <title> Index Domain Alignment: Minimizing Cost of Cross-Referencing Between Distributed Arrays. </title> <booktitle> In Proceedings of Frontiers of Massively Parallel Processing, </booktitle> <pages> pages 424-431, </pages> <year> 1990. </year>
Reference-contexts: Automatic data organization is much more difficult. General layout optimization (the "shapes problem") has been shown to be NP-complete [Mac87]. Some success has been made for automatic data organization for very regular data accesses which occur in systolic array computations <ref> [KLS90, LC90] </ref>. 2.2 Data Motion and Access It is not generally possible to select a single, static allocation of data that eliminates communication. For this reason, once data is assigned to the memories of a distributed memory processor, mechanisms are needed to read and write data in remote memories.
Reference: [LeB86] <author> Thomas J. LeBlanc. </author> <title> Shared Memory Versus Message-Passing in a Tightly Coupled Multiprocessor: A Case Study. </title> <type> Butterfly Project Report 3, </type> <institution> University of Rochester, Computer Science Department, </institution> <month> January </month> <year> 1986. </year>
Reference-contexts: The subsequent three sections highlight architectural schemes, operating system and hybrid hardware/software implementations, and run-time and compiler synthesized software schemes. 12 2.2.3.1 Hardware Shared Memory Hardware-supported remote access to remote memory modules has been implemented in many multiprocessors, including the BBN Butterfly <ref> [LeB86] </ref>, the IBM RP3 [Pea85], and the Denelcor HEP [Smi78]. Private caches have been thoroughly studied as a mechanism for reducing remote memory access latency [Smi82]. Mechanisms to keep cached data consistent have been one of the major fields of research in computer systems design in the past decade.
Reference: [LH89] <author> Kai Li and Paul Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <month> November </month> <year> 1989. </year>
Reference-contexts: Operating system global memory was first applied to large-scale distributed memory multiprocessors in the IVY system <ref> [LH89, Li86] </ref> which exploited paging hardware and software page fault handlers to provide a paged, virtual memory over distributed memory multiprocessors. Data was shared in units of virtual memory pages, representing several thousand bytes. <p> The prototype Poli-C system currently supports the Intel iPSC/2, iPSC/860, and Paragon XP/S distributed memory multiprocessors. Poli-C extends the native message-passing communication model provided by these systems to permit a variety of globally-accessible data structure mechanisms. Similar to other virtually-shared distributed memory implementations for multicomputers <ref> [LH89, BCZ89, CG89] </ref>, Poli-C dynamically moves and replicates data between the memories of the multicomputer. There are three primary distinctions of the Poli-C style of distributed data management. <p> If a Trap policy is used, then a higher latency signal handler is used to catch the fault and invoke the handler. Trap based virtual memory support is a conventional mechanism for shared virtual memory systems <ref> [LH89, BCZ89, LP93, Aea91, LLG + 92] </ref>. In such systems, pages or cache lines which are not locally present, or cannot be be accessed in a particular mode, are protected. Subsequent accesses will fault, invoking a handler. The handler will examine the memory address, and perform the requested operation.
Reference: [Li86] <author> Kai Li. </author> <title> Shared Virtual Memory on Loosely Coupled Multiprocessors. </title> <type> PhD thesis, </type> <institution> Yale University, Department of Computer Science, </institution> <month> October </month> <year> 1986. </year>
Reference-contexts: Operating system global memory was first applied to large-scale distributed memory multiprocessors in the IVY system <ref> [LH89, Li86] </ref> which exploited paging hardware and software page fault handlers to provide a paged, virtual memory over distributed memory multiprocessors. Data was shared in units of virtual memory pages, representing several thousand bytes.
Reference: [Lil90] <author> Sigurd L. Lillevik. </author> <title> Touchstone Program Overview. </title> <booktitle> In Proceedings of the 5 th Distributed Memory Computing Conference, </booktitle> <volume> volume II, </volume> <pages> pages 647-657, </pages> <year> 1990. </year>
Reference-contexts: The Cosmic Cube led to the commercial development of the Intel hypercube multiprocessors [Rat85, Arl88, Bok90], and the NX message passing system [Int94], later used for the Intel Touchstone <ref> [Lil90] </ref> and Paragon [Int91] systems. Other vendors provide libraries with different message passing conventions. PVM [Oak94] and UNIX interprocess communication facilities (such as sockets [Bac86]) also serve as message passing subsystems. <p> The iPSC/2 and the iPSC/860, "second-generation" hypercubes, exploit much faster circuit-switching interconnects. The iPSC/860 uses the Intel i860 processing nodes in place of the iPSC/2's i386. The Intel Paragon XP/S multiprocessor is a "third-generation" distributed memory multiprocessor. It is the commercial offspring of the Touchstone project <ref> [Lil90] </ref>. The processing nodes are Intel i860 XP microprocessors with a 50 MHz clock frequency. The processors communicate via a high-speed, two-dimensional mesh. Messages are wormhole-pipelined through the network. The architecture is intended to scale to many thousands of processors. The J-Machine is a distributed memory multiprocessor constructed at MIT.
Reference: [LLG + 92] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam. </author> <title> The Stanford DASH Multiprocessor. </title> <booktitle> IEEE Computer, </booktitle> <month> March </month> <year> 1992. </year>
Reference-contexts: Public caches [Sta91] and snooping caches [AB86] are simple architectural solutions, but do not scale to massive numbers of processors. Directory based coherence techniques [CF78, ASHH88, CFKA90] are a popular solution, being used in the DASH <ref> [LLG + 92] </ref> and Alewife [Aea91] multiprocessors. Directory techniques maintain tables of current cache line copies. The memory controller initiates a coherence protocol on each write, sending directed messages through the communication fabric to implement the policy. Directories are typically placed in fixed locations. <p> If a Trap policy is used, then a higher latency signal handler is used to catch the fault and invoke the handler. Trap based virtual memory support is a conventional mechanism for shared virtual memory systems <ref> [LH89, BCZ89, LP93, Aea91, LLG + 92] </ref>. In such systems, pages or cache lines which are not locally present, or cannot be be accessed in a particular mode, are protected. Subsequent accesses will fault, invoking a handler. The handler will examine the memory address, and perform the requested operation.
Reference: [Lov93] <author> David B. Loveman. </author> <title> High Performance Fortran. </title> <journal> IEEE Parallel and Ditributed Technology, </journal> <volume> 1(1) </volume> <pages> 25-42, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Furthermore, while distributed data structures provide a useful stand-alone programming system, they can also be used to advantage as a run-time data management system for distributed-memory compilers such as HPF <ref> [Lov93] </ref>. Distributed data structures can simply support sophisticated relocation and replication of data at execution time, for arbitrary access patterns, when static analysis can not determine the data motion at compiler time. Heuristics about read write mixtures and access footprints can be used as hints when selecting data management policies. <p> Data organization has also been shown to be important in the placement of data within pages of a virtual memory system [Sta84]. By grouping related objects within pages, thrashing is reduced and prefetch benefits are gained. Distributed-memory programming languages <ref> [RSW89, HKT91, Ger90, Lov93] </ref> support distribution annotations, which specify how the data structures are distributed to memories. For regular computations, the decomposition problem is typically viewed as one of domain alignment. <p> In an attempt to provide these three attributes, integrated compilation and runtime approaches <ref> [Lov93, MMB90] </ref> position themselves as a compromise between message-passing and shared-memory. Our approach also attempts to achieve portability, programmability and performance by combining the best features of message-passing and shared-memory. The system we present in this thesis associates policies with individual data structures. <p> Partition determines the constituent object containing the indexed element. Together, the organization and partition functions map the elements of data structures into constituent objects, that are allocated to processors. The organization and partition functions serve an analogous role to data decomposition functions in languages like HPF <ref> [Lov93] </ref>, but are formulated as a generalization of conventional memory system functionality.
Reference: [LP93] <author> Zakaria Lahjomri and Thierry Priol. KOAN Reference Manual 2.1, </author> <month> May 13 </month> <year> 1993. </year>
Reference-contexts: The use of pages as constituent objects (cache lines) allows the use of existing virtual memory management hardware to trap writes to shared pages. In addition, pages are sufficiently large to amortize the large communication latency existing on early multicomputers. The KOAN sys 13 tem <ref> [LP93] </ref> is a more aggressive implementation of paged virtual memory for Intel hypercube multiprocessors. DUnX [RPLE90], Clouds [RAK89], and Munin [BCZ89] are examples of distributed operating systems which support forms of object-specific access policy control. The Munin project [BCZ89] was an early exploitation of software coherence protocols for system tuning. <p> If a Trap policy is used, then a higher latency signal handler is used to catch the fault and invoke the handler. Trap based virtual memory support is a conventional mechanism for shared virtual memory systems <ref> [LH89, BCZ89, LP93, Aea91, LLG + 92] </ref>. In such systems, pages or cache lines which are not locally present, or cannot be be accessed in a particular mode, are protected. Subsequent accesses will fault, invoking a handler. The handler will examine the memory address, and perform the requested operation. <p> In this chapter, we briefly demonstrate the tradeoffs between paged virtual memory and tunable software shared memory approaches. We compare programs running under Poli-C, and the paged virtual memory system KOAN <ref> [LP93] </ref>. KOAN is a kernel-based paged global memory system for the Intel iPSC/2 hypercube multiprocessor. Sections of the address space of each processor are reserved for shared data. Page protection hardware is used to detect shared writes and reads to non-local data.
Reference: [LS80] <author> Bruce Lindsay and Patricia G. Selinger. </author> <title> Site Autonomy Issues in R*: A Distributed Database Management System. </title> <type> Technical Report RJ2927, </type> <institution> IBM Research Laboratory, </institution> <address> San Jose, </address> <month> September 15, </month> <year> 1980. </year>
Reference-contexts: These extensions support an illusion of shared data across distributed name spaces using "information-sharing abstractions," that are tuned to particular types of conceptual sharing patterns. Object management schemes to efficiently locate and migrate objects in distributed databases have been described in <ref> [LS80] </ref>. Some of these techniques have been applied to a segment-based, distributed memory multiprocessor in [Tot88]. The Concurrent Aggregates system [Chi90] provides mechanisms to construct distributed objects with user-defined access behavior. Access policy can be tailored to object needs.
Reference: [Mac87] <author> Mary E. Mace. </author> <title> Memory Storage Patterns in Parallel Processing. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1987. </year>
Reference-contexts: Explicit user maps can be used to associate elements with memories [HKT91]. Automatic partitioners can perform this partitioning using network flow methods or knowledge of application behavior. Automatic data organization is much more difficult. General layout optimization (the "shapes problem") has been shown to be NP-complete <ref> [Mac87] </ref>. Some success has been made for automatic data organization for very regular data accesses which occur in systolic array computations [KLS90, LC90]. 2.2 Data Motion and Access It is not generally possible to select a single, static allocation of data that eliminates communication.
Reference: [MMB90] <author> Seema Mirchandaney, Joel Saltz Piyush Mehrotra, and Harry Berryman. </author> <title> A Scheme for Supporting Automatic Data Migration on Multicomputers. </title> <booktitle> In Proceedings of the 5 th Distributed Memory Computing Conference, </booktitle> <pages> pages 1028-1037, </pages> <year> 1990. </year> <month> 158 </month>
Reference-contexts: Kali [MR90] and related projects [SCMB90] exploit run-time dependence analysis to extract communication patterns from programs which cannot be statically analyzed due to data dependent execution patterns. To reduce unnecessary data motion, software caching has been proposed <ref> [MMB90] </ref>. Compilation techniques have also been applied to the cache consistency problem [OA89], by using dependence information to insure coherence only at critical points. <p> In an attempt to provide these three attributes, integrated compilation and runtime approaches <ref> [Lov93, MMB90] </ref> position themselves as a compromise between message-passing and shared-memory. Our approach also attempts to achieve portability, programmability and performance by combining the best features of message-passing and shared-memory. The system we present in this thesis associates policies with individual data structures. <p> If the object is sufficiently large, the communication latency can be amortized by the volume of data sent. However, for programs with more irregular data accesses, achieving successful latency tolerance with object transfer is more difficult, because the accesses may leapfrog through numerous objects. 8.2.1 Coherent Scatter-Gather Competitive approaches <ref> [MMB90, DSB91] </ref> gather index lists at runtime and subsequently perform bulk transfers of data. While such schemes can more effectively tolerate latencies in irregular programs, they are not integrated into a general-purpose demand-fetch environment, and they do not support sophisticated caching.
Reference: [MR90] <author> Piyush Mehrotra and John Van Rosendale. </author> <title> Programming Distributed Memory Ar--chitectures Using Kali. </title> <type> ICASE Report 90-69, </type> <institution> Institute for Computer Applications in Science and Engineering (ICASE), </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: The generated programs are loosely synchronous in that they are data parallel, but synchronize only at communication points. Fortran D [HKT91] and SUPERB [Ger90] are two compilation systems which synthesize data movement instructions by a dependence analysis of programs. Kali <ref> [MR90] </ref> and related projects [SCMB90] exploit run-time dependence analysis to extract communication patterns from programs which cannot be statically analyzed due to data dependent execution patterns. To reduce unnecessary data motion, software caching has been proposed [MMB90].
Reference: [NWD93] <author> Michael D. Noakes, Deborah Wallach, and William J. Dally. </author> <title> The J-Machine Mul ticomputer: An Architectural Evaluation. </title> <booktitle> In Proceedings of the 20 th International Symposium on Computer Architecture, </booktitle> <pages> pages 647-657, </pages> <year> 1993. </year>
Reference-contexts: These two models are sketched in Figure 1.1 Because of the commonality of the models, it is possible to build "mixed-mode" systems, supporting both direct access and message passing communication models. Indeed, several emerging distributed-memory architectures provide efficient support for both models via a common communication subsystem <ref> [NWD93, Aea91] </ref>. In such a mixed-mode system, a programmer is free to choose the communication model based on performance and programmability. For example, discrete event simulations often model inherently asynchronous communications, making message passing a logical communication model. <p> Other vendors provide libraries with different message passing conventions. PVM [Oak94] and UNIX interprocess communication facilities (such as sockets [Bac86]) also serve as message passing subsystems. The MPI standard [For94] is an effort to stardardize message passing interfaces across platforms. 11 The MIT J-Machine <ref> [NWD93] </ref> provides better hardware support for message passing, dras-tically reducing the number of instructions required to send and receive messages. The J-Machine also provides executable messages that invoke code upon arrival. <p> The operating system may also require costly saving and reconstruction of virtual process state. If the trap is to be serviced by user-level software, some operating systems require yet more overhead. Emerging systems with fast context switching <ref> [NWD93, Aea91] </ref> allow dispatch and return from traps in a few tens of processor cycles. In contrast, servicing traps with UNIX user-level signal handlers can require tens of thousands of cycles.
Reference: [OA89] <author> Susan Owicki and Anant Agarwal. </author> <title> Evaluating the Performance of Software Cache Coherence. </title> <booktitle> In Proceedings of the 3 rd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 230-242, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Kali [MR90] and related projects [SCMB90] exploit run-time dependence analysis to extract communication patterns from programs which cannot be statically analyzed due to data dependent execution patterns. To reduce unnecessary data motion, software caching has been proposed [MMB90]. Compilation techniques have also been applied to the cache consistency problem <ref> [OA89] </ref>, by using dependence information to insure coherence only at critical points.
Reference: [Oak94] <institution> Oak RIdge National Laboratory, Oak Ridge, TN 37831. </institution> <note> PVM 3 User's Guide and Refernce Manual, </note> <month> May </month> <year> 1994. </year>
Reference-contexts: The Cosmic Cube led to the commercial development of the Intel hypercube multiprocessors [Rat85, Arl88, Bok90], and the NX message passing system [Int94], later used for the Intel Touchstone [Lil90] and Paragon [Int91] systems. Other vendors provide libraries with different message passing conventions. PVM <ref> [Oak94] </ref> and UNIX interprocess communication facilities (such as sockets [Bac86]) also serve as message passing subsystems.
Reference: [Pea85] <author> G. F. Pfister and et. al. </author> <title> The IBM Research Parallel Processor Prototype (RP3): Introduction and Architecture. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 764-771, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: The subsequent three sections highlight architectural schemes, operating system and hybrid hardware/software implementations, and run-time and compiler synthesized software schemes. 12 2.2.3.1 Hardware Shared Memory Hardware-supported remote access to remote memory modules has been implemented in many multiprocessors, including the BBN Butterfly [LeB86], the IBM RP3 <ref> [Pea85] </ref>, and the Denelcor HEP [Smi78]. Private caches have been thoroughly studied as a mechanism for reducing remote memory access latency [Smi82]. Mechanisms to keep cached data consistent have been one of the major fields of research in computer systems design in the past decade.
Reference: [RAK89] <author> Umakishore Ramachandran, Mustaque Ahamad, and M. </author> <title> Yousef A Khalidi. Coherence of Distributed Shared Memory: Unifying Synchronization and Data Transfer. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages II-160-II-169, </pages> <year> 1989. </year>
Reference-contexts: In addition, pages are sufficiently large to amortize the large communication latency existing on early multicomputers. The KOAN sys 13 tem [LP93] is a more aggressive implementation of paged virtual memory for Intel hypercube multiprocessors. DUnX [RPLE90], Clouds <ref> [RAK89] </ref>, and Munin [BCZ89] are examples of distributed operating systems which support forms of object-specific access policy control. The Munin project [BCZ89] was an early exploitation of software coherence protocols for system tuning. Munin permits the assignment of coherence protocols to individual pages of the virtual memory system. <p> This idea has been independently advanced in other recent projects <ref> [BCZ89, RAK89, RLW94, KOH + 94] </ref>. Our data structure policies vary in message quantity and message size. Both algorithmic locality and access policy affect the message count and the message volume. Communication latency and bandwidth will further affect the total execution time of the algorithm. <p> By locking an object (or an element contained in an object) explicitly in the source code of the program, the access interface can omit the conditional tests. Furthermore, locking permits numerous accesses to an object before the object is relocated to remote nodes, for improved performance <ref> [RAK89] </ref>. 8.1.3 Object-Level Programming The organization and partition phases of access interfaces can be removed if programming is done directly at the object level.
Reference: [Ras86] <author> R. F. Rashid. </author> <title> From RIG to Accent to Mach: The Evolution of a Network Operating System. </title> <booktitle> In Proceedings of the 1986 Fall Joint Computer Conference, </booktitle> <pages> pages 1128-1137, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: The Munin project [BCZ89] was an early exploitation of software coherence protocols for system tuning. Munin permits the assignment of coherence protocols to individual pages of the virtual memory system. The Mach <ref> [Ras86] </ref> operating system also permits a degree of tunability by providing external pagers which implement page-replacement and other paging services. Newer, hybrid systems, such as Alewife [Aea91] and the proposed Typhoon [RLW94] make judicious use of hardware and rely on software for portions of the protocol.
Reference: [Rat85] <author> Justin Rattner. </author> <title> Concurrent Processing: A New Direction in Scientific Computing. </title> <booktitle> In Proceedings of the 1985 National Computer Conference, </booktitle> <pages> pages 157-166. </pages> <publisher> AFIPS Press, </publisher> <year> 1985. </year>
Reference-contexts: The Cosmic Cube provided subroutine libraries to send regions of memory to another processing node, which then performs a matching receive to obtain the data into a local memory buffer. The Cosmic Cube led to the commercial development of the Intel hypercube multiprocessors <ref> [Rat85, Arl88, Bok90] </ref>, and the NX message passing system [Int94], later used for the Intel Touchstone [Lil90] and Paragon [Int91] systems. Other vendors provide libraries with different message passing conventions. PVM [Oak94] and UNIX interprocess communication facilities (such as sockets [Bac86]) also serve as message passing subsystems.
Reference: [RCea] <editor> J. Rees, W. Clinger, and et. al. </editor> <title> Revised 3 Report on the Algorithmic Language Scheme. </title>
Reference-contexts: The translator converts programs composed in Poli-C, an extended version of C, into conventional C source programs. The translator is implemented in the Scheme dialect of Lisp <ref> [RCea] </ref>, and was compiled to an executable using the DEC Scheme!C scc compiler [Bar89]. The translation process synthesizes code for run-time data structure initialization, remote procedure call stubs, and distributed data structure operations.
Reference: [Res92] <institution> Kendall Square Research. Kendall Square Research Technical Summary, </institution> <year> 1992. </year>
Reference-contexts: Directories are typically placed in fixed locations. If data accesses are issued far from the directory, substantial network communication may be generated to access the directory. COMA systems, such as the KSR multiprocessors <ref> [Res92] </ref>, reduce this traffic by effectively caching directory information close to the accessing processors. 2.2.3.2 Operating System Shared Memory Pure hardware schemes offer high performance, but at a high cost, due to the need for special purpose components.
Reference: [RHL + 93] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, and David A. Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers. </title> <booktitle> In Proceedings to the 1993 ACM Sigmetrics Conference on Measurement and Modelling of Computer Systems, </booktitle> <pages> pages 48-60, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Furthermore, hardware schemes are traditionally inflexible, because design decisions and policies are hardwired. There is a current trend in multiprocessor architecture research to design machines with at most a small amount of custom hardware, fast traps, and to implement the bulk of the coherence protocols in software <ref> [Aea91, RHL + 93, RLW94] </ref>. Operating system global memory was first applied to large-scale distributed memory multiprocessors in the IVY system [LH89, Li86] which exploited paging hardware and software page fault handlers to provide a paged, virtual memory over distributed memory multiprocessors.
Reference: [RLW94] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21 st International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <year> 1994. </year>
Reference-contexts: Our distributed data structures employ software emulation of paging hardware to support arbitrary page sizes, permitting control over the granularity of sharing and communication. Furthermore, software page tables permits the optimization of local memory hierarchy performance. With the exception of a few systems <ref> [BCZ89, RLW94] </ref>, shared memory implementations provide a single remote memory access policy across the entire system. Our distributed data structures permit the assignment of individual, user-level access policies to each data structure. <p> Furthermore, hardware schemes are traditionally inflexible, because design decisions and policies are hardwired. There is a current trend in multiprocessor architecture research to design machines with at most a small amount of custom hardware, fast traps, and to implement the bulk of the coherence protocols in software <ref> [Aea91, RHL + 93, RLW94] </ref>. Operating system global memory was first applied to large-scale distributed memory multiprocessors in the IVY system [LH89, Li86] which exploited paging hardware and software page fault handlers to provide a paged, virtual memory over distributed memory multiprocessors. <p> Munin permits the assignment of coherence protocols to individual pages of the virtual memory system. The Mach [Ras86] operating system also permits a degree of tunability by providing external pagers which implement page-replacement and other paging services. Newer, hybrid systems, such as Alewife [Aea91] and the proposed Typhoon <ref> [RLW94] </ref> make judicious use of hardware and rely on software for portions of the protocol. <p> This idea has been independently advanced in other recent projects <ref> [BCZ89, RAK89, RLW94, KOH + 94] </ref>. Our data structure policies vary in message quantity and message size. Both algorithmic locality and access policy affect the message count and the message volume. Communication latency and bandwidth will further affect the total execution time of the algorithm.
Reference: [RPLE90] <author> Jr. Richard P. LaRowe and Carla Schlatter Ellis. </author> <title> Experimental Comparison of Memory Management Policies for NUMA Multiprocessors. </title> <type> Technical Report CS-1990-10, </type> <institution> Duke University, Department of Computer Science, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: In addition, pages are sufficiently large to amortize the large communication latency existing on early multicomputers. The KOAN sys 13 tem [LP93] is a more aggressive implementation of paged virtual memory for Intel hypercube multiprocessors. DUnX <ref> [RPLE90] </ref>, Clouds [RAK89], and Munin [BCZ89] are examples of distributed operating systems which support forms of object-specific access policy control. The Munin project [BCZ89] was an early exploitation of software coherence protocols for system tuning. Munin permits the assignment of coherence protocols to individual pages of the virtual memory system.
Reference: [RSW89] <author> Matthew Rosing, Robert B. Schnabel, and Robert P. Weaver. </author> <title> Expressing Complex Parallel Algorithms in DINO. </title> <type> Technical Report CU-CS-430-88, </type> <institution> University of Colorado at Boulder, Department of Computer Science, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: Data organization has also been shown to be important in the placement of data within pages of a virtual memory system [Sta84]. By grouping related objects within pages, thrashing is reduced and prefetch benefits are gained. Distributed-memory programming languages <ref> [RSW89, HKT91, Ger90, Lov93] </ref> support distribution annotations, which specify how the data structures are distributed to memories. For regular computations, the decomposition problem is typically viewed as one of domain alignment.
Reference: [SCMB90] <author> Joel Saltz, Kathleen Crowley, Ravi Mirchandaney, and Harry Berryman. </author> <title> Run-Time Scheduling and Execution of Loops on Message Passing Machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 303-312, </pages> <year> 1990. </year> <month> 159 </month>
Reference-contexts: The generated programs are loosely synchronous in that they are data parallel, but synchronize only at communication points. Fortran D [HKT91] and SUPERB [Ger90] are two compilation systems which synthesize data movement instructions by a dependence analysis of programs. Kali [MR90] and related projects <ref> [SCMB90] </ref> exploit run-time dependence analysis to extract communication patterns from programs which cannot be statically analyzed due to data dependent execution patterns. To reduce unnecessary data motion, software caching has been proposed [MMB90].
Reference: [Sei85] <author> Charles L. Seitz. </author> <title> The Cosmic Cube. </title> <journal> Communications of the ACM, </journal> <volume> 28(1) </volume> <pages> 23-25, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: Message passing provides directed, bulk data transfers between memories, though the specific functionality varies widely across packages. The Caltech Cosmic Cube <ref> [Sei85] </ref> popularized message-passing multiprocessors built of commodity hardware. The Cosmic Cube provided subroutine libraries to send regions of memory to another processing node, which then performs a matching receive to obtain the data into a local memory buffer.
Reference: [SF89] <author> Craig B. Stunkel and W. Kent Fuchs. TRAPEDS: </author> <title> Producing Traces for Multi-computers Via Execution Driven Simulation. </title> <journal> ACM SIGMETRICS Performance Evaluation Review, </journal> <volume> 17(1) </volume> <pages> 70-78, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Second, for various access interface and architectural delays, we analyze exeucution time, to determine conditions when gains in locality offset additional costs. 46 4.1 Capturing Traces with ForTrace Memory reference (access) tracing is a standard approach for evaluating computer memory system performance <ref> [HL84, Smi82, SF89] </ref>. By recording an application program's accesses to data (a data memory reference trace) from an existing computer system, and then replaying the access trace to a simulator, we can obtain performance predictions for hypothetical computer systems under actual program workloads.
Reference: [Smi76] <author> Brian T. Smith. </author> <title> Matrix Eigensystems Routines: EISPACK. </title> <booktitle> In Lecture Notes In Computer Science, </booktitle> <volume> volume 6. </volume> <publisher> Springer-Verlag, </publisher> <year> 1976. </year>
Reference-contexts: Form rgeig Find Eigenvectors Of General Matrix Sparse Matrix Kernels smvmult Sparse Matrix/Vector Multiply sgentojag Convert Sparse Matrix To Jagged Diagonal Format sfelim Forward Elimination Of Sparse System Miscellaneous Scientific Algorithms fft 2-D Fast Fourier Transform nws Wavefront Needleman-Wunsch-Sellers Sequence Match Table 4.2: Scientific Algorithm Benchmark Set [DBMS79] and EISPACK <ref> [Smi76] </ref> libraries. These dense matrix codes provide a set of easily analyzed, generally regular access patterns. Although contemporary compilation schemes have been somewhat successful at static data management, for a large domain of applications, purely compile-time analysis cannot make acceptable data management decisions.
Reference: [Smi78] <author> Burton J. Smith. </author> <title> A Pipelined, Shared Resource MIMD Computer. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 6-8, </pages> <month> August </month> <year> 1978. </year>
Reference-contexts: subsequent three sections highlight architectural schemes, operating system and hybrid hardware/software implementations, and run-time and compiler synthesized software schemes. 12 2.2.3.1 Hardware Shared Memory Hardware-supported remote access to remote memory modules has been implemented in many multiprocessors, including the BBN Butterfly [LeB86], the IBM RP3 [Pea85], and the Denelcor HEP <ref> [Smi78] </ref>. Private caches have been thoroughly studied as a mechanism for reducing remote memory access latency [Smi82]. Mechanisms to keep cached data consistent have been one of the major fields of research in computer systems design in the past decade.
Reference: [Smi82] <author> A. J. Smith. </author> <title> Cache Memories. </title> <journal> ACM Computing Surveys, </journal> <volume> 14(3) </volume> <pages> 473-530, </pages> <year> 1982. </year>
Reference-contexts: Private caches have been thoroughly studied as a mechanism for reducing remote memory access latency <ref> [Smi82] </ref>. Mechanisms to keep cached data consistent have been one of the major fields of research in computer systems design in the past decade. Public caches [Sta91] and snooping caches [AB86] are simple architectural solutions, but do not scale to massive numbers of processors. <p> Second, for various access interface and architectural delays, we analyze exeucution time, to determine conditions when gains in locality offset additional costs. 46 4.1 Capturing Traces with ForTrace Memory reference (access) tracing is a standard approach for evaluating computer memory system performance <ref> [HL84, Smi82, SF89] </ref>. By recording an application program's accesses to data (a data memory reference trace) from an existing computer system, and then replaying the access trace to a simulator, we can obtain performance predictions for hypothetical computer systems under actual program workloads.
Reference: [Sta84] <author> James W. Stamos. </author> <title> Static Grouping of Small Objects to Enhance Performance of a Paged Virtual Memory. </title> <journal> ACM Transactions on Computer Systems, </journal> <pages> pages 155-179, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: The way data is mapped into a linear memory space was shown to be critical in avoiding memory conflicts among competing data access streams [Law75, BJR88]. Data organization has also been shown to be important in the placement of data within pages of a virtual memory system <ref> [Sta84] </ref>. By grouping related objects within pages, thrashing is reduced and prefetch benefits are gained. Distributed-memory programming languages [RSW89, HKT91, Ger90, Lov93] support distribution annotations, which specify how the data structures are distributed to memories. For regular computations, the decomposition problem is typically viewed as one of domain alignment.
Reference: [Sta91] <author> CSRD Staff. </author> <title> The CEDAR Project. </title> <type> CSRD Report 1122, </type> <institution> University of Illinois Center for Supercomputing Research and Development, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: Private caches have been thoroughly studied as a mechanism for reducing remote memory access latency [Smi82]. Mechanisms to keep cached data consistent have been one of the major fields of research in computer systems design in the past decade. Public caches <ref> [Sta91] </ref> and snooping caches [AB86] are simple architectural solutions, but do not scale to massive numbers of processors. Directory based coherence techniques [CF78, ASHH88, CFKA90] are a popular solution, being used in the DASH [LLG + 92] and Alewife [Aea91] multiprocessors. Directory techniques maintain tables of current cache line copies. <p> Changes in the temporal memory access stream can have significant effects on the performance of the memory management policy. 48 4.1.1 Cedar Fortran and KAP ForTrace recognizes a subset of Cedar Fortran. Cedar Fortran is a parallel version of Fortran, designed for the Cedar multiprocessor <ref> [Sta91] </ref> built at the University of Illinois Center for Supercomputing Research and Development (CSRD). Cedar Fortran provides constructs for loop-level parallelism, including doall and doacross parallel loops, and await and advance synchronization primitives.
Reference: [SW] <author> Youcef Saad and Harry A. G. Wijshoff. </author> <title> SPARK: A Benchmark Package for Sparse Computations. </title> <note> Available via FTP from icarus.riacs.edu in pub/benchmark. </note>
Reference-contexts: The majority of matrices are from the Harwell/Boeing sparse matrix set. The Harwell/Boeing matrix set is a large collection of practical matrices contributed from a variety of scientific disciplines. Additional matrices were generated by the SPARK sparse matrix benchmark package 81 (mc ca 2) (abb313) (spark3) (steam1) <ref> [SW] </ref>. Table 5.1 lists the sparse matrices used in our experiments. The pattern of some of the matrices are shown in Figure 5.8. 5.3.3 SMVMULT The smvmult benchmark performs a fully parallel multiplication of a sparse matrix by a vector (b = A fi x). <p> Prefetch gains a great deal, though at some point, the gains from prefetch outweigh the false sharing costs. 5.3.5 SGENTOJAG The sgentojag benchmark converts a sparse matrix in general columnar encoding into a jagged diagonal format. The benchmark was derived from the SPARK manipulation kernel number 3 <ref> [SW] </ref>.
Reference: [Tot88] <author> Brian Totty. </author> <title> An Operating Environment for the Jellybean Machine. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1070, </pages> <institution> Massachusetts Institute of Technology, Artificial Intelligence Laboratory, </institution> <month> May </month> <year> 1988. </year>
Reference-contexts: Object management schemes to efficiently locate and migrate objects in distributed databases have been described in [LS80]. Some of these techniques have been applied to a segment-based, distributed memory multiprocessor in <ref> [Tot88] </ref>. The Concurrent Aggregates system [Chi90] provides mechanisms to construct distributed objects with user-defined access behavior. Access policy can be tailored to object needs. <p> Such object-based programming is appropriate for many programs, and is supported by systems such as <ref> [Tot88] </ref>. 147 8.2 Latency and Locality Optimizations The current prototype Poli-C implementation typically uses objects as the unit of communication. Entire objects are moved back and forth between memories on demand. If the object is sufficiently large, the communication latency can be amortized by the volume of data sent.
Reference: [Tot92] <author> Brian K. Totty. </author> <title> Experimental Analysis of Data Management for Distributed Data Structures. </title> <type> UIUC DCS Technical Report, </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: The simulation system is named PoliSim <ref> [Tot92] </ref>, for "Policy Simulator." PoliSim consists of 10,000 lines of C++ code, in addition to the AWESIME-II and C++ libraries. <p> The precise details of the interaction of policy and application are not important for the purpose of this discussion. It suffices to demonstrate the variation in memory locality across policies and applications. For a more detailed investigation of this simulation data, see <ref> [Tot92] </ref>. 1 This implementation of the Update protocol simulates infinite caches. Copies are never purged or replaced. While this increases the read locality (the likelihood of performing a local read), it decreases the write locality.
Reference: [vECGS92] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Kalus Erik Schauser. </author> <title> Active messages: A mechanism for integrated communication and com putation. </title> <booktitle> In Proceedings of the 19 th International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The J-Machine also provides executable messages that invoke code upon arrival. Such active messages <ref> [vECGS92] </ref> have been found useful, and have been implemented on other systems include the Thinking Machine's CM-5 [Cor92]. 2.2.2 Object-Based Systems Conventional message passing systems isolate the data from the communication services. Messages send bulk regions of one memory space to another. <p> The critical section is required in the default access interface, which uses interrupt-driven active messages <ref> [vECGS92] </ref> to implement the access protcols. In this active message model, requests from other processors can interrupt the local processor at any time.
Reference: [Wol82] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <year> 1982. </year>
Reference-contexts: Recently there has been an avalanche of published research on compilation techniques to provide the illusion of shared memory. The vast majority of these techniques exploit dependence analysis technology <ref> [Wol82, Ban88, Zim91] </ref> to extract and aggregate communication from nested loops of scientific programs. The generated programs are loosely synchronous in that they are data parallel, but synchronize only at communication points.
Reference: [Zim91] <author> Hans Zima. </author> <title> Supercompilers for Parallel and Vector Computers. Frontier Series. </title> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1991. </year> <month> 160 </month>
Reference-contexts: Recently there has been an avalanche of published research on compilation techniques to provide the illusion of shared memory. The vast majority of these techniques exploit dependence analysis technology <ref> [Wol82, Ban88, Zim91] </ref> to extract and aggregate communication from nested loops of scientific programs. The generated programs are loosely synchronous in that they are data parallel, but synchronize only at communication points.
References-found: 74

