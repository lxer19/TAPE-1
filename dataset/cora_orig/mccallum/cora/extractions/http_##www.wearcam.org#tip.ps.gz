URL: http://www.wearcam.org/tip.ps.gz
Refering-URL: http://www.wearcam.org/
Root-URL: 
Email: steve@media.mit.edu, picard@media.mit.edu  
Phone: (Tel) 617-253-0611 (Fax) 617-253-8874  
Title: Video Orbits of the Projective Group: A simple approach to featureless estimation of parameters.  
Author: Steve Mann and Rosalind W. Picard 
Note: This work sponsored in part by Hewlett-Packard (HP) Research Labs and BT, PLC.  
Date: October 15, 1996  
Address: 20 Ames Street; Cambridge, MA 02139  
Affiliation: MIT Media Lab;  
Abstract: We present direct featureless methods for estimating the 8 parameters of an "exact" projective (homographic) coordinate transformation to register pairs of images, together with the application of seamlessly combining a plurality of images of the same scene, resulting in a single image (or new image sequence) of greater resolution or spatial extent. The approach is "exact" for two cases of static scenes: (1) images taken from the same location of an arbitrary 3-D scene, with a camera that is free to pan, tilt, rotate about its optical axis, and zoom or (2) images of a flat scene taken from arbitrary locations. The featureless projective approach generalizes inter-frame camera motion estimation methods which have previously used an affine model (which lacks the degrees of freedom to "exactly" characterize such phenomena as camera pan and tilt) and/or which have relied upon finding points of correspondence between the image frames. The featureless projective approach, which operates directly on the image pixels, is shown to be superior in accuracy and ability to enhance resolution. The proposed methods work well on image data collected from both good-quality and poor-quality video under a wide variety of conditions (sunny, cloudy, day, night). These new fully-automatic methods are also shown to be robust to deviations from the assumptions of static scene and no parallax. permission to publish this abstract separetely is granted
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. B. J.L. Barron, </author> <title> D.J. Fleet, "Systems and experiment performance of optical flow techniques," </title> <journal> International journal of computer vision, </journal> <pages> pp. 43-77, </pages> <year> 1994. </year>
Reference-contexts: in the final production, many more images were used, resulting in a high resolution full-color composite showing most of the room. (Figure reproduced from [6], courtesy of IS&T.) 2 Background Hundreds of papers have been published on the problems of motion estimation and frame alignment. (For review and comparison, see <ref> [1] </ref>.) In this section we review the basic differences between coordinate transformations and emphasize the importance of using the "exact" 8-parameter projective coordinate transformation. 2.1 Coordinate transformations A coordinate transformation maps the image coordinates, x = [x; y] T to a new set of coordinates, x 0 = [x 0 ; <p> b x + E x As discussed earlier, the calculation can be simplified by judicious alteration of the weighting, in particular, multiplying each term of the summation (12) by (cx + 1), and solving, gives: x ! X (x E t =E x )(x) (13) where the regressor is = <ref> [x; 1; xE t =E x x 2 ] </ref> T . For `projective-flow' (`p-flow'), we substitute u m = ax+b cx+1 x into (8). <p> deal with the 2-D translational flow, but with the 2-D projective flow, estimating the eight parameters in the coordinate transformation: x 0 = 6 x 0 3 5 = c T [x; y] T + 1 Ax + b (20) The desired eight scalar parameters are denoted by p = <ref> [A; b; c; 1] </ref>, A 2 IR 2fi2 , b 2 IR 2fi1 , and c 2 IR 2fi1 .
Reference: [2] <author> A. Tekalp, M. Ozkan, and M. Sezan, </author> <title> "High-resolution image reconstruction from lower-resolution image sequences and space-varying image restoration," </title> <booktitle> in Proc. of the Int. Conf. on Acoust., Speech and Sig. Proc., </booktitle> <address> (San Francisco, CA), </address> <pages> pp. </pages> <address> III-169, </address> <publisher> IEEE, </publisher> <month> Mar. </month> <pages> 23-26, </pages> <year> 1992. </year>
Reference-contexts: An illustration showing the effects possible with each of these forms is shown in Fig. 3. The most common assumption (especially in motion estimation for coding, and optical flow for computer vision) is that the coordinate transformation between frames is translation. Tekalp, Ozkan, and Sezan <ref> [2] </ref> have applied this assumption to high-resolution image reconstruction. Although translation is the least constraining and simplest to implement of the seven coordinate transformations in Table 1, it is poor at handling large changes due to camera zoom, rotation, pan and tilt.
Reference: [3] <author> Q. Zheng and R. Chellappa, </author> <title> "A Computational Vision Approach to Image Registration ," IEEE Transactions Image Processing, </title> <month> July </month> <year> 1993. </year> <pages> pages 311-325. </pages>
Reference-contexts: Tekalp, Ozkan, and Sezan [2] have applied this assumption to high-resolution image reconstruction. Although translation is the least constraining and simplest to implement of the seven coordinate transformations in Table 1, it is poor at handling large changes due to camera zoom, rotation, pan and tilt. Zheng and Chellappa <ref> [3] </ref> considered the image registration problem using a subset of the affine model | translation, rotation and scale.
Reference: [4] <author> M. Irani and S. Peleg, </author> <title> "Improving Resolution by Image Registration," </title> <journal> CVGIP, </journal> <volume> vol. 53, </volume> <pages> pp. 231-239, </pages> <month> May </month> <year> 1991. </year>
Reference: [5] <author> L. Teodosio and W. Bender, </author> <title> "Salient video stills: Content and context preserved," </title> <booktitle> Proc. ACM Multimedia Conf., </booktitle> <month> August </month> <year> 1993. </year>
Reference: [6] <author> S. Mann, </author> <title> "Compositing multiple pictures of the same scene," </title> <booktitle> in Proceedings of the 46th Annual IS&T Conference, </booktitle> <address> (Cambridge, </address> <institution> Massachusetts), The Society of Imaging Science and Technology, </institution> <month> May 9-14 </month> <year> 1993. </year>
Reference-contexts: visited by the zooming-in, compared to other areas. (See magnified portions of composite at sides.) This composite only shows the result of combining three images, but in the final production, many more images were used, resulting in a high resolution full-color composite showing most of the room. (Figure reproduced from <ref> [6] </ref>, courtesy of IS&T.) 2 Background Hundreds of papers have been published on the problems of motion estimation and frame alignment. (For review and comparison, see [1].) In this section we review the basic differences between coordinate transformations and emphasize the importance of using the "exact" 8-parameter projective coordinate transformation. 2.1 <p> The 8-parameter projective model gives the desired 8 parameters that exactly account for all possible zero-parallax camera motions; hence, there is an important need for a featureless estimator of these parameters. To the best of our knowledge, the only algorithms proposed to date for such an estimator are <ref> [6] </ref>, and shortly after, [7]. In both of these, a computationally expensive nonlinear optimization method was presented. In [6], a direct method was also proposed. This direct method uses simple linear algebra, and is non-iterative insofar as methods such as Levenberg-Marquardt and the like are in no way required. <p> To the best of our knowledge, the only algorithms proposed to date for such an estimator are <ref> [6] </ref>, and shortly after, [7]. In both of these, a computationally expensive nonlinear optimization method was presented. In [6], a direct method was also proposed. This direct method uses simple linear algebra, and is non-iterative insofar as methods such as Levenberg-Marquardt and the like are in no way required.
Reference: [7] <author> R. Szeliski and J. Coughlan, </author> <title> "Hierarchical spline-based image registration," </title> <address> CVPR, </address> <year> 1994. </year>
Reference-contexts: To the best of our knowledge, the only algorithms proposed to date for such an estimator are [6], and shortly after, <ref> [7] </ref>. In both of these, a computationally expensive nonlinear optimization method was presented. In [6], a direct method was also proposed. This direct method uses simple linear algebra, and is non-iterative insofar as methods such as Levenberg-Marquardt and the like are in no way required.
Reference: [8] <author> G. Wolberg, </author> <title> Digital Image Warping. </title> <address> 10662 Los Vaqueros Circle, Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <address> 1990. </address> <publisher> IEEE Computer Society Press Monograph. </publisher> <pages> 29 </pages>
Reference-contexts: The physical camera model fits exactly in the 8-parameter projective group; therefore, we know that "eight is enough." Hence, it seems reasonable to have a preference for approximate models with exactly eight parameters. The 8-parameter bilinear model is perhaps the most widely-used <ref> [8] </ref> in the fields of image processing, medical imaging, remote sensing, and computer graphics. This model is easily obtained from the biquadratic model by removing the four x 2 and y 2 terms. <p> Then set h k = p 0;k ffi h. (This should have nearly the same effect as applying p k to h k1 , except that it will avoid additional interpolation and anti-aliasing errors you would get by resampling an already resampled image <ref> [8] </ref>). Repeat until either the error between h k and g falls below a threshold, or until some maximum number of iterations is achieved.
Reference: [9] <author> G. Adiv, </author> <title> "Determining 3D Motion and structure from optical flow generated by several moving objects," </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <pages> pp. 304-401, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: This model is easily obtained from the biquadratic model by removing the four x 2 and y 2 terms. Although the resulting bilinear model captures the effect of converging lines, it completely fails to capture the effect of chirping. The 8-parameter pseudo-perspective model <ref> [9] </ref> and an 8 parameter `relative-projective' model both do, in fact, capture both the converging lines and the chirping of a projective coordinate transformation.
Reference: [10] <author> N. Navab and S. Mann, </author> <title> "Recovery of relative affine structure using the motion flow field of a rigid planar patch.," </title> <booktitle> Mustererkennung 1994, </booktitle> <address> Tagungsband., </address> <year> 1994. </year>
Reference-contexts: The pseudo-perspective model, for example, may be thought of as first, removal of two of the quadratic terms (bf q x 0 y 2 = q y 0 x 2 = 0), which results in a ten parameter model (the `q-chirp' of <ref> [10] </ref>) and then constraining the four remaining quadratic parameters to have two degrees of freedom.
Reference: [11] <author> R. Y. Tsai and T. S. Huang, </author> <title> "Estimating Three-Dimensional Motion Parameters of a Rigid Planar Patch ," Trans. Accoust., Speech, and Sig. </title> <booktitle> Proc., </booktitle> <year> 1981. </year>
Reference-contexts: Of course, the desired "exact" eight parameters come from the projective model, but they have been perceived as being notoriously difficult to estimate. The parameters for this model have been solved by Tsai and Huang <ref> [11] </ref>, but their solution assumed that features had been identified in the two frames, along with their correspondences. The main contribution of this paper is a simple featureless means of automatically solving for these 8 parameters. Other researchers have looked at projective estimation in the context of obtaining 3D models. <p> We also assume that each element in the camera sensor array returns a quantity that is linearly proportional to the quantity of light received 4 . With these assumptions, the exact camera motion that can be recovered is summarized in Table 2. 2.3 Video orbits Tsai and Huang <ref> [11] </ref> pointed out that the elements of the projective group give the true camera motions with respect to a planar surface.
Reference: [12] <author> O. D. Faugeras and F. Lustman, </author> <title> "Motion and structure from motion in a piecewise planar environment," </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> vol. 2, no. 3, </volume> <pages> pp. 485-508, </pages> <year> 1988. </year>
Reference-contexts: The main contribution of this paper is a simple featureless means of automatically solving for these 8 parameters. Other researchers have looked at projective estimation in the context of obtaining 3D models. Faugeras and Lustman <ref> [12] </ref>, Shashua and Navab [13], and Sawhney [14] have considered the problem of estimating the projective parameters while computing the motion of a rigid planar patch, as part of a larger problem of finding 3-D motion and structure using parallax relative to an arbitrary plane in the scene.
Reference: [13] <author> A. Shashua and N. Navab, </author> <title> "Relative Affine: Theory and Application to 3D Reconstruction From Perspective Views.," </title> <booktitle> Proc. IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <month> June </month> <year> 1994. 1994. </year>
Reference-contexts: The main contribution of this paper is a simple featureless means of automatically solving for these 8 parameters. Other researchers have looked at projective estimation in the context of obtaining 3D models. Faugeras and Lustman [12], Shashua and Navab <ref> [13] </ref>, and Sawhney [14] have considered the problem of estimating the projective parameters while computing the motion of a rigid planar patch, as part of a larger problem of finding 3-D motion and structure using parallax relative to an arbitrary plane in the scene.
Reference: [14] <author> H. Sawhney, </author> <title> "Simplifying motion and structure analysis using planar parallax and image warping," </title> <journal> ICPR, </journal> <volume> vol. 1, </volume> <month> October </month> <year> 1994. </year> <note> 12th IAPR. </note>
Reference-contexts: The main contribution of this paper is a simple featureless means of automatically solving for these 8 parameters. Other researchers have looked at projective estimation in the context of obtaining 3D models. Faugeras and Lustman [12], Shashua and Navab [13], and Sawhney <ref> [14] </ref> have considered the problem of estimating the projective parameters while computing the motion of a rigid planar patch, as part of a larger problem of finding 3-D motion and structure using parallax relative to an arbitrary plane in the scene.
Reference: [15] <author> R. Kumar, P. Anandan, and K. Hanna, </author> <title> "Shape recovery from multiple views: a parallax based approach," </title> <booktitle> ARPA image understanding workshop, </booktitle> <month> 10 Nov </month> <year> 1994. </year>
Reference-contexts: Kumar et al. <ref> [15] </ref> have also suggested registering frames of video by computing the flow along the epipolar lines, for which 6 Scene assumptions Camera assumptions Case 1: arbitrary 3-D free to zoom, rotate, pan, and tilt, fixed center of projection Case 2: planar free to zoom, rotate, pan, and tilt, free to translate
Reference: [16] <author> L. Campbell and A. Bobick, </author> <title> "Correcting for radial lens distortion: A simple implementation," </title> <type> TR 322, </type> <institution> M.I.T. Media Lab Perceptual Computing Section, </institution> <address> Cambridge, Ma, </address> <month> Apr </month> <year> 1995. </year>
Reference-contexts: They explored the group structure associated with images of a 3-D rigid planar 1 When using low cost wide-angle lenses, there is usually some barrel distortion which we correct using the method of <ref> [16] </ref>. 2 The principal point is where the optical axis intersects the film. 3 Isotropic means that magnification in the x and y directions is the same.
Reference: [17] <author> C. W. Wyckoff, </author> <title> "An experimental extended response film," </title> <booktitle> S.P.I.E. NEWSLETTER, </booktitle> <month> JUNE-JULY </month> <year> 1962. </year>
Reference: [18] <author> S. Mann and R. </author> <title> Picard, "Being `undigital' with digital cameras: Extending dynamic range by combining differently exposed pictures," </title> <type> Tech. Rep. 323, </type> <institution> M.I.T. Media Lab Perceptual Computing Section, Boston, Massachusetts, </institution> <year> 1994. </year> <title> Also appears, </title> <booktitle> IS&T's 46th annual conference, </booktitle> <pages> pages 422-428, </pages> <month> May </month> <year> 1995. </year>
Reference: [19] <author> M. </author> <title> Artin, Algebra. </title> <publisher> Prentice Hall, </publisher> <year> 1991. </year>
Reference-contexts: That single coordinate transformation is given by the law of composition in the group. The orbit of a particular element of the set, under the group operation <ref> [19] </ref> is the new set formed by applying to it, all possible operators from the group. In this paper, the orbit is a collection of pictures formed from one picture through applying all possible projective coordinate transformations to that picture. <p> When a 6= 0 and c = 0, the projective group becomes the affine group of coordinate transformations, and 5 also known as a group action or G-set <ref> [19] </ref>. 6 In this 2-D world, the "camera" consists of a center of projection (pinhole "lens") and a line (1-D sensor array or 1-D "film"). 8 element of the operator group.
Reference: [20] <author> S. Mann, </author> <title> "Wavelets and chirplets: Time-frequency perspectives, with applications," in Advances in Machine Vision, Strategies and Applications (P. </title> <editor> Archibald, ed.), </editor> <publisher> World Scientific, </publisher> <year> 1992. </year>
Reference-contexts: The resulting two (1-D) frames taken by the camera, are related by the coordinate transformation from x 1 to x 2 , given by <ref> [20] </ref>: x 2 = z 2 tan (arctan (x 1 =z 1 ) ); 8x 1 6= o 1 = (ax 1 + b)=(cx 1 + 1); 8x 1 6= o 1 (1) where a = z 2 =z 1 , b = z 2 tan (), c = tan ()=z <p> We should mention that c, the degree of perspective, has been given the interpretation of a chirp-rate <ref> [20] </ref>. The coordinate transformations of (1) form a group operation.
Reference: [21] <author> S. Mann and R. W. </author> <title> Picard, "Virtual bellows: constructing high-quality images from video," </title> <booktitle> in Proceedings of the IEEE first international conference on image processing, </booktitle> <address> (Austin, Texas), </address> <month> Nov. 13-16 </month> <year> 1994. </year>
Reference-contexts: The coordinate transformations of (1) form a group operation. This result, and the proof of this group's isomorphism to the group corresponding to nonsingular projections of a flat object are given in <ref> [21] </ref>. 2.3.2 Projective group in 2-D coordinates The theory for the projective, affine, and translation groups also holds for the familiar 2-D images taken of the 3-D world. <p> The frames of Fig 6 were brought into register using the differential parameter estimation, and "cemented" together seamlessly on a common canvas. "Cementing" involves piecing the frames together, for example, by median, mean, or trimmed mean, or combining on a subpixel grid <ref> [21] </ref>. (Trimmed mean was used here, but the particular method made little visible difference.) Fig 8 shows this result ("projective/projective"), with a comparison to two non-projective cases. The first comparison is to "affine/affine" where affine parameters were estimated (also multiscale) and used for the coordinate transformation.
Reference: [22] <author> R. Y. Tsai and T. S. Huang, </author> <title> "Multiframe image restoration and registration," </title> <publisher> ACM, </publisher> <year> 1984. </year>
Reference: [23] <author> T. S. Huang and A. Netravali, </author> <title> "Motion and structure from feature correspondences: a review," </title> <booktitle> Proc. IEEE, </booktitle> <month> Feb </month> <year> 1984. </year>
Reference: [24] <author> N. Navab and A. Shashua, </author> <title> "Algebraic Description of Relative Affine Structure: Connections to Eu-clidean, Affine and Projective Structure.," MIT Media Lab Memo No. </title> <type> 270, </type> <year> 1994. </year>
Reference-contexts: A major difficulty with feature-based methods is finding the features. Good features are often hand-selected, or computed, possibly with some degree of human intervention <ref> [24] </ref>. A second problem with features is their sensitivity to noise and occlusion.
Reference: [25] <author> H. L. </author> <title> Van Trees, Detection, Estimation, and Modulation Theory (Part I). </title> <publisher> John Wiley and Sons, </publisher> <year> 1968. </year>
Reference-contexts: p are unknown. 7 We can find, by exhaustive search (applying all possible operators, p, to h), the "best" p as the one which maximizes 7 In the presence of additive white Gaussian noise, this method, also known as "matched filtering", leads to a maximum likelihood estimate of the parameters <ref> [25] </ref>. 11 the inner product: Z 1 g (x) R 1 dx (3) where we have normalized the energy of each coordinate-transformed h before making the comparison.
Reference: [26] <author> S. Mann and S. Haykin, </author> <title> "The chirplet transform: Physical considerations," </title> <journal> Trans. Signal Processing, </journal> <volume> vol. 43, </volume> <month> November </month> <year> 1995. </year>
Reference-contexts: Expanding the right hand side of (4) in a Taylor series, and canceling 0th order terms gives the well-known 8 Symplectomorphisms of the time-frequency plane [27][28] have been applied to signal analysis <ref> [26] </ref>, giving rise to the so-called q-chirplet [26], which differs from the projective chirplet discussed here. 12 optical flow equation: u f E x + E t + h:o:t: = 0, where E x and E t are the spatial and temporal derivatives respectively, and h:o:t: denotes higher order terms. <p> Expanding the right hand side of (4) in a Taylor series, and canceling 0th order terms gives the well-known 8 Symplectomorphisms of the time-frequency plane [27][28] have been applied to signal analysis <ref> [26] </ref>, giving rise to the so-called q-chirplet [26], which differs from the projective chirplet discussed here. 12 optical flow equation: u f E x + E t + h:o:t: = 0, where E x and E t are the spatial and temporal derivatives respectively, and h:o:t: denotes higher order terms.
Reference: [27] <author> A. Berthon, </author> <title> "Operator Groups and Ambiguity Functions in Signal Processing," in Wavelets: Time-Frequency Methods and Phase Space (J. </title> <editor> Combes, ed.), </editor> <publisher> Springer Verlag, </publisher> <year> 1989. </year> <month> 30 </month>
Reference: [28] <author> A. Grossmann and T. Paul, </author> <title> "Wave functions on subgroups of the group of affine cannonical tranfor--mations," </title> <booktitle> Lecture notes in physics, </booktitle> <volume> No. 211: </volume> <booktitle> Resonances | Models and Phenomena, </booktitle> <pages> pp. 128-138, </pages> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference: [29] <author> R. Young, </author> <title> "Wavelet theory and its applications," </title> <year> 1993. </year>
Reference-contexts: A computationally efficient algorithm for the cross-wavelet transform has recently been presented <ref> [29] </ref>. (See [30] for a good review on wavelet-based estimation of affine coordinate transformations.) Adaptive variants of the chirplet transforms have been previously reported in the literature [31].
Reference: [30] <author> L. G. Weiss, </author> <title> "Wavelets and wideband correlation processing," </title> <journal> IEEE Signal Processing Magazine, </journal> <pages> pp. 13-32, </pages> <year> 1993. </year>
Reference-contexts: A computationally efficient algorithm for the cross-wavelet transform has recently been presented [29]. (See <ref> [30] </ref> for a good review on wavelet-based estimation of affine coordinate transformations.) Adaptive variants of the chirplet transforms have been previously reported in the literature [31].
Reference: [31] <author> S. Mann and S. Haykin, </author> <title> "Adaptive "Chirplet" Transform: an adaptive generalization of the wavelet transform," </title> <journal> Optical Engineering, </journal> <volume> vol. 31, </volume> <pages> pp. 1243-1256, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: A computationally efficient algorithm for the cross-wavelet transform has recently been presented [29]. (See [30] for a good review on wavelet-based estimation of affine coordinate transformations.) Adaptive variants of the chirplet transforms have been previously reported in the literature <ref> [31] </ref>.
Reference: [32] <author> B. Horn and B. Schunk, </author> <title> "Determining Optical Flow," </title> <journal> Artificial Intelligence, </journal> <year> 1981. </year>
Reference-contexts: However, there are still many problems with the adaptive chirplet approach; thus, for the remainder of this paper, we consider featureless methods based on spatiotemporal derivatives. 3.3 Featureless methods based on spatiotemporal derivatives 3.3.1 Optical flow (`translation flow') When the change from one image to another is small, optical flow <ref> [32] </ref> may be used. <p> Now we describe these algorithms for 2-D images. The brightness constancy constraint equation for 2-D images <ref> [32] </ref> which gives the flow velocity components in the x and y directions, analogous to (5) is: u f 16 As is well-known [32] the optical flow field in 2-D is underconstrained 10 . <p> Now we describe these algorithms for 2-D images. The brightness constancy constraint equation for 2-D images <ref> [32] </ref> which gives the flow velocity components in the x and y directions, analogous to (5) is: u f 16 As is well-known [32] the optical flow field in 2-D is underconstrained 10 .
Reference: [33] <author> J. Y. Wang and E. H. Adelson, </author> <title> "Spatio-Temopral Segmentation of Video Data ," in SPIE Image and Video Processing II, </title> <address> (San Jose, California), </address> <pages> pp. 120-128, </pages> <month> February 7-9 </month> <year> 1994. </year>
Reference-contexts: Wang and Adelson have proposed fitting an affine model to the optical flow field <ref> [33] </ref> between two 2-D images. We briefly examine their approach with 1-D images; the reduction in dimensions simplifies analysis and comparison to `affine flow'. Denote coordinates in the original image, g, by x, and in the new image, h, by x 0 .
Reference: [34] <author> J. Bergen, P. Burt, R. Hingorini, and S. Peleg, </author> <title> "Computing two motions from three frames," </title> <booktitle> in Proc. Third Int'l Conf. Comput. Vision, </booktitle> <address> (Osaka, Japan), </address> <pages> pp. 27-32, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Bergen et al. <ref> [34] </ref> have proposed this method, which we will call `affine flow', to distinguish it from the `affine fit' model of Wang and Adelson (7). Let us show how `affine flow' and `affine fit' are related. <p> An analogous variant of the `affine flow' method involves multiscale iteration as well, but in this case the iteration and multiscale hierarchy 14 are incorporated directly into the affine estimator <ref> [34] </ref>. With the addition of multiscale analysis, the `fit' and `flow' methods differ in additional respects beyond just the weighting. Our intuition and experience indicates that the direct multiscale `affine flow' performs better than the `affine fit' to the multiscale flow.
Reference: [35] <author> Lucas and T. Kanade, </author> <title> "An iterative image-registration technique with an application to stereo vision ," in Image Understanding Workshop, </title> <journal> pp. </journal> <pages> 121-130, </pages> <year> 1981. </year>
Reference-contexts: Both our intuition and our practical experience tends to favor the `affine flow' weighting, but, more generally, perhaps we should ask "What is the best weighting?" Lucas and Kanade <ref> [35] </ref>, among others, have considered weighting issues, though the rather obvious difference in weighting between fit and flow doesn't appear to have been pointed out previously in the literature.
Reference: [36] <author> J. Y. A. Wang and E. H. Adelson, </author> <title> "Representing moving images with layers," Image Processing Spec. Iss: </title> <booktitle> Image Seq. Compression, </booktitle> <volume> vol. 12, </volume> <month> September </month> <year> 1994. </year>
Reference-contexts: Multiscale optical flow makes the assumption that blocks of the image are moving with pure translational motion, and then, paradoxically, the affine fit refutes this pure-translation assumption. However, `fit' provides some utility over `flow' when it is desired to segment the image into regions undergoing different motions <ref> [36] </ref>, or to gain robustness by rejecting portions of the image not obeying the assumed model. 3.3.3 `Projective fit' and `projective flow': new techniques Analogous to the "affine fit" and "affine flow" of the previous section, we now propose the two new methods: `projective fit' and `projective flow'.
Reference: [37] <author> R. Wilson and G. H. </author> <title> Granlund, </title> <booktitle> "The Uncertainty Principle in Image Processing ," IEEE Transactions on Pattern Analysis and Machine Intelligence, </booktitle> <month> November </month> <year> 1984. </year>
Reference-contexts: The algorithm (in Matlab on an HP 735) takes about six seconds per iteration for a pair of 320x240 images. 4.3 Exploiting commutativity for parameter estimation There is a fundamental uncertainty <ref> [37] </ref> involved in the simultaneous estimation of parameters of a noncommutative group, akin to the Heisenberg uncertainty relation of quantum mechanics.
Reference: [38] <author> J. Segman, J. Rubinstein, and Y. Y. Zeevi, </author> <title> "The canonical coordinates method for pattern deformation: </title> <journal> Theoretical and computational considerations," </journal> <volume> vol. 14, </volume> <pages> pp. 1171-1183, </pages> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: 13 group (in the absence of noise), we can obtain the exact coordinate transformation. 13 A commutative (or Abelian) group is one in which elements of the group commute, for example, translation along the x-axis commutes with translation along the y-axis, so the 2-D translation group is commutative. 21 Segman <ref> [38] </ref> considered the problem of estimating the parameters of a commutative group of coordinate transformations, in particular, the parameters of the affine group [39]. His work also deals with noncommutative groups, in particular, in the incorporation of scale in the Heisenberg group 14 [40].
Reference: [39] <author> J. Segman, </author> <title> "Fourier cross correlation and invariance transformations for an optimal recognition of functions deformed by affine groups," </title> <journal> vol. </journal> <volume> 9, </volume> <pages> pp. 895-902, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: which elements of the group commute, for example, translation along the x-axis commutes with translation along the y-axis, so the 2-D translation group is commutative. 21 Segman [38] considered the problem of estimating the parameters of a commutative group of coordinate transformations, in particular, the parameters of the affine group <ref> [39] </ref>. His work also deals with noncommutative groups, in particular, in the incorporation of scale in the Heisenberg group 14 [40]. Estimating the parameters of a commutative group is computationally efficient, e.g., through the use of Fourier cross-spectra [41].
Reference: [40] <author> J. Segman and W. Schempp, </author> <title> Two methods of incorporating scale in the Heisenberg group. </title> <note> 1993. JMIV special issue on wavelets. </note>
Reference-contexts: His work also deals with noncommutative groups, in particular, in the incorporation of scale in the Heisenberg group 14 <ref> [40] </ref>. Estimating the parameters of a commutative group is computationally efficient, e.g., through the use of Fourier cross-spectra [41]. We exploit this commutativity for estimating the parameters of the noncommutative 2-D projective group by first estimating the parameters that commute.
Reference: [41] <author> B. Girod and D. Kuo, </author> <title> "Direct estimation of displacement histograms," </title> <booktitle> OSA Meeting on IMAGE UNDERSTANDING AND MACHINE VISION, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: His work also deals with noncommutative groups, in particular, in the incorporation of scale in the Heisenberg group 14 [40]. Estimating the parameters of a commutative group is computationally efficient, e.g., through the use of Fourier cross-spectra <ref> [41] </ref>. We exploit this commutativity for estimating the parameters of the noncommutative 2-D projective group by first estimating the parameters that commute. For example, we improve performance if we first estimate the two parameters of translation, correct for the translation, and then proceed to estimate the eight projective parameters. <p> In practice, we run through the following `commutative initialization' before estimating the parameters of the projective group of coordinate transformations: 1. Assume that h is merely a translated version of g. (a) Estimate this translation using the method of Girod <ref> [41] </ref>. (b) Shift h by the amount indicated by this estimate. (c) Compute the MSE between the shifted h and g, and compare to the original MSE before shifting. (d) If an improvement has resulted, use the shifted h from now on. 2.
Reference: [42] <author> Y. Sheng, C. Lejeune, and H. H. Arsenault, </author> <title> "Frequency-domain Fourier-Mellin descriptors for invariant pattern recognition," Optical Engineering, </title> <month> May </month> <year> 1988. </year>
Reference-contexts: We can also simultaneously estimate both the isotropic-zoom and the rotation about the optical axis by applying a log-polar coordinate transformation followed by a translation estimator. This process may also be achieved by a direct application of the Fourier-Mellin transform <ref> [42] </ref>. Similarly, if the only difference between g and h is a camera pan, then the pan may be estimated through a coordinate transformation to cylindrical coordinates, followed by a translation estimator.
Reference: [43] <author> P. J. Burt and P. Anandan, </author> <title> "Image stabilization by registration to a reference mosaic," </title> <booktitle> ARPA image understanding workshop, </booktitle> <month> 10 Nov </month> <year> 1994. </year>
Reference: [44] <author> M. Hansen, P. Anandan, K. Dana, G. van der Wal, and P. Burt, </author> <title> "Real-time scene stabilization and mosaic construction," </title> <booktitle> ARPA image understanding workshop, </booktitle> <month> 10 Nov </month> <year> 1994. </year>
Reference: [45] <author> S. Intille, </author> <title> "Computers watching football," 1995. http://www-white.media.mit.edu/vismod/demos/football/football.html. Additional technical notes, examples, computer code, and video sequences, some containing images in the same orbit of the projective group (contributions are also welcome), are available from: </title> <address> http://wearcam.org/pencigraphy, or http://www-white.media.mit.edu/~steve/pencigraphy 31 </address>
Reference-contexts: In fact, a coordinate system other than one chosen from the input images could also be used. In particular, a coordinate system where parallel lines never meet, and periodic structures are "dechirped" (Fig 12 (b)) lends itself well to machine vision and player-tracking algorithms <ref> [45] </ref>.
References-found: 45

