URL: http://skew2.kellogg.nwu.edu/~ecm/ftp/nlpca.ps.gz
Refering-URL: http://skew2.kellogg.nwu.edu/~ecm/ftp/nlfe.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Principal Components Analysis  
Author: Edward C. Malthouse 
Keyword: nonlinear principal components analysis, feature extraction, data compression, principal curves, principal surfaces.  
Date: September 19, 1996  
Abstract: Some Theoretical Results on Nonlinear Abstract Nonlinear principal components analysis (NLPCA) neural networks are feedforward autoassociative networks with five layers. The third layer has fewer nodes than the input or output layers. NLPCA has been shown to give better solutions to several feature extraction problems than existing methods, but very little is know about the theoretical properties of this method or its estimates. This paper studies NLPCA. It proposes a geometric interpretation by showing that NLPCA fits a lower-dimensional curve or surface through the training data. The first three layers project observations onto the curve or surface giving scores. The last three layers define the curve or surface. The first three layers are a continuous function, which I show has several implications: NLPCA "projections" are suboptimal producing larger approximation error, NLPCA is unable to model curves and surfaces that intersect themselves, and NLPCA cannot parameterize curves with parameterizations having discontinuous jumps. I establish results on the identification of score values and discuss their implications on interpreting score values. I discuss the relationship between NLPCA and principal curves and surfaces, another nonlinear feature extraction method. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. Baldi and K. Hornik, </author> <title> "Neural networks and principal component analysis: Learning from examples without local minima," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 2, </volume> <pages> pp. 53-58, </pages> <year> 1989. </year>
Reference-contexts: Data compression therefore occurs in the bottleneck layer. Note that this architecture estimates the PCA solution because it minimizes the same objective function as PCA (Equation 8). An early description of this application of neural networks is the encoder-decoder problem described in [14, pp. 335-339]. <ref> [1] </ref> later develops some theoretical properties of this method. 3 The statement is true for any partial sphere "greater" than a hemisphere, e.g., Equation (12) with s 1 2 [0; 2), s 2 2 [0; a), and a &gt; =2. 9 ` - ` - 7 S S Sw S S
Reference: [2] <author> D. J. Bartholomew, </author> <title> Latent Variable Models and Factor Analysis, </title> <publisher> Charles Griffin & Company LTD, </publisher> <address> London, </address> <year> 1987. </year>
Reference-contexts: I generated 100 equally-spaced s values in the interval <ref> [0; 2] </ref> and their corresponding 100 points along the perimeter of a circle using (2). <p> The function f and latent variable values can be estimated with a suitable feature extraction method. Latent variable problems are common in the physical, social, and engineering sciences. <ref> [2] </ref> gives a thorough account of the theory of linear latent variable models for continuous and discrete data. A second application is data visualization. When the dimension of the predictor variables is large (usually p 4), it is difficult to visualize the data graphically. <p> This change of basis is called a rotation in the social science literature. See [13, x 9.6] or <ref> [2, x 3.4] </ref> for further discussion. Before the surface-score values can be interpreted, similar work must be done for the nonlinear feature extraction methods.
Reference: [3] <author> G. Cybenko, </author> <title> "Approximation by superpositions of a sigmoidal function," </title> <journal> Math. Control Signals Systems, </journal> <volume> vol. 2, </volume> <pages> pp. 303-314, </pages> <year> 1989. </year>
Reference-contexts: A three-layer neural network with nonlinear activation functions in the hidden layer can represent any continuous function under weak assumptions <ref> [3] </ref>. NLPCA adds hidden layers with nonlinear activation functions between the input and bottleneck layers and between the bottleneck and output layers giving a network with a total of 5 layers. The network models a composition of functions. Fig. 1 shows an example of an NLPCA network.
Reference: [4] <author> David DeMers and Garrison Cottrell, </author> <title> "Nonlinear dimensionality reduction," </title> <booktitle> Neural Information processing systems, </booktitle> <volume> vol. 5, </volume> <pages> pp. 580-587, </pages> <year> 1993. </year>
Reference-contexts: NLPCA and related nonlinear feature extraction methods have been successfully applied to many different problems. For example, <ref> [4] </ref> shows how NLPCA can be applied to image compression problems. [17] show how a version of NLPCA can be used for another data compression problem. [5] shows how another version of NLPCA can be used to construct control charts. These examples suggest that NLPCA is a promising method. <p> In this case the score values provide a compressed version of the original data. Note that the score values are not interpreted or used to make any decisions. [17, Example 4] capture 95% of the variation in p = 65 observed x's with r = 3 nonlinear components. <ref> [4] </ref> uses a version of NLPCA for image compression. The authors analyze 64 fi 64 pixel images (p = 4; 096) and found r = 5 dimensional representations of the images. A second application is noise reduction.
Reference: [5] <author> D. Dong and T.J. McAvoy, </author> <title> "Nonlinear principal component analysis | based on principal curves and neural networks," </title> <journal> Computers and Chemical Engineering, </journal> <volume> vol. 20, no. 1, </volume> <pages> pp. 65-78, </pages> <year> 1996. </year>
Reference-contexts: NLPCA and related nonlinear feature extraction methods have been successfully applied to many different problems. For example, [4] shows how NLPCA can be applied to image compression problems. [17] show how a version of NLPCA can be used for another data compression problem. <ref> [5] </ref> shows how another version of NLPCA can be used to construct control charts. These examples suggest that NLPCA is a promising method. But very little is known about the theoretical properties of NLPCA and its estimates. <p> The goal is to use these data to determine if the fabrication process is operating normally (in control) or if something is wrong (out of control). It is often difficult to distinguish between in-control and out-of-control states when p is large. <ref> [5] </ref> demonstrates that the score values from a nonlinear feature extraction method can be plotted on a control chart to detect out-of-control conditions. Not all applications of nonlinear feature extraction methods require interpreting scores. One application is data/image compression. <p> Ba sic properties like convexity are not preserved under such transformations. 10 Function f is a homeomorphism if and only if it is continuous and has a continuous inverse. 19 Consider the following hypothetical quality control example, which is similar to the application discussed in <ref> [5] </ref>. Suppose that a large number of process variable (p large) are monitored and that the p-dimensional observations lie on a 2-dimensional surface, i.e., the intrinsic dimension of the process variables is r = 2.
Reference: [6] <author> J.H. Friedman, </author> <title> "Exploratory projection pursuit," </title> <journal> JASA, </journal> <volume> vol. 82, no. 397, </volume> <pages> pp. 249-267, </pages> <year> 1987. </year>
Reference-contexts: NLPCA generalizes the objective function given in (8). There are several methods that generalize (7) including <ref> [6] </ref>, [7], [10], and [15]. Also see Section 10 in [12] for additional discussion and references on these other approaches. 2.2 Principal Curves and Surfaces Principal curves (PC) were first proposed in [8] and [9]. PCA finds a unit-length vector u that satisfies the minimum distance property in (8).
Reference: [7] <author> Colin Fyfe and Roland Badderey, </author> <title> "Non-linear data structure extraction using simple hebbian networks," </title> <type> Tech. Rep., </type> <institution> Department of Computer Science, University of Strathclyde, </institution> <year> 1995. </year>
Reference-contexts: NLPCA generalizes the objective function given in (8). There are several methods that generalize (7) including [6], <ref> [7] </ref>, [10], and [15]. Also see Section 10 in [12] for additional discussion and references on these other approaches. 2.2 Principal Curves and Surfaces Principal curves (PC) were first proposed in [8] and [9]. PCA finds a unit-length vector u that satisfies the minimum distance property in (8).
Reference: [8] <author> T. Hastie, </author> <title> Principal Curves and Surfaces, </title> <type> Ph.D. thesis, </type> <institution> Stanford University, </institution> <month> November </month> <year> 1984. </year>
Reference-contexts: The unit-speed property can be generalized for surfaces in terms of areas and volumes, but these parameterizations are not unique. See <ref> [8] </ref> for further discussion. 2.1 Principal Components Analysis Principal components analysis is a well-established feature extraction method that assumes f in (1) is linear. <p> NLPCA generalizes the objective function given in (8). There are several methods that generalize (7) including [6], [7], [10], and [15]. Also see Section 10 in [12] for additional discussion and references on these other approaches. 2.2 Principal Curves and Surfaces Principal curves (PC) were first proposed in <ref> [8] </ref> and [9]. PCA finds a unit-length vector u that satisfies the minimum distance property in (8). PC extends PCA by fitting a unit-speed curve f under a similar objective function. <p> Under this definition of principal curves, they show that a curve is a principal curve if and only if it satisfies the objective function in (11). 7 Two-dimensional principal surfaces (PS) were proposed in <ref> [8] </ref> and were later extended in [12] for higher dimensional surfaces. Principal surfaces extend PCs by fitting a surface that satisfies a minimum distance objective function similar to (11). <p> For example, a 3=4 sphere (Equation 12 with s 1 2 [0; 2) and s 2 2 [0; 3=4]) cannot be parameterized in this way, but it could be parameterized with the spherical coordinate or stereographic projection approach. Example 6.4 in <ref> [8] </ref> uses stereographic projections to parameterize a full sphere. I revisit this discussion in section 3.2 and show that the spherical coordinates for this hemisphere as defined in (12) cannot be used as a parameterization if the projection index is continuous. Principal curves are defined to have unit speed. <p> A "projection" is 4 They would not necessarily be equal for points in the set of ambiguity points, which have measure zero <ref> [8] </ref>. 11 suboptimal when an x is mapped to a point on the curve other than the point that is closest to it (the "inf" part of Equation 9). I illustrate the reason for this with an example. <p> The center is mapped to the wrong point because the fit is not perfect and the points around 5=4 are closer to the center than the ends. <ref> [8] </ref> also examines some related questions and shows that if s f (as defined in Equation 9) is continuous at a point x, then x is not an ambiguity point (Theorem 4.4).
Reference: [9] <author> T. Hastie and W. Stuetzle, </author> <title> "Principal curves," </title> <journal> JASA, </journal> <volume> vol. 84, no. 406, </volume> <pages> pp. 502-516, </pages> <year> 1989. </year>
Reference-contexts: There are several methods that generalize (7) including [6], [7], [10], and [15]. Also see Section 10 in [12] for additional discussion and references on these other approaches. 2.2 Principal Curves and Surfaces Principal curves (PC) were first proposed in [8] and <ref> [9] </ref>. PCA finds a unit-length vector u that satisfies the minimum distance property in (8). PC extends PCA by fitting a unit-speed curve f under a similar objective function. <p> be linear, the principal curves algorithm is equivalent to the power method of extracting the dominant eigenvalue from X 0 X, and therefore extracts the first principal component. 2 More formally, let X be a random vector defined on R p from continuous probability density with E (X) = 0. <ref> [9] </ref> defines a principal curve of to be the set of curves that do not intersect themselves and are self-consistent, i.e., E (Xjs f (X) = s) = f (s); for all s. <p> Fig. 7 shows the principal curve fit of 3=4 circle from noisy data. The projections are all good, but the principal curves method has some difficulty with the endpoints of the curve. <ref> [9] </ref> also notes this problem (see their Fig. 4). The projection index should map the center of the circle to one of the endpoints, depending on whether the parameterization increases in a clockwise or counter-clockwise direction. <p> A third application is curve estimation. The nonlinear feature extraction methods (with a discontinuous projection index) fit a curve passing through the middle of a set of data points. <ref> [9] </ref> uses principal curves to determine the path of particles 18 in a collider chamber. The following result partially resolves the parameterization question. The result shows that any two parameterizations of a curve or surface satisfying the least-squares objection function in (14) can be related by a homeomor-phism 10 .
Reference: [10] <author> Juha Karhunen and Jyrki Joutsensalo, </author> <title> "Representation and separation of singnals using nonlinear pca type learning," </title> <booktitle> Neural Networks, </booktitle> <pages> pp. 113-127, </pages> <year> 1994. </year>
Reference-contexts: NLPCA generalizes the objective function given in (8). There are several methods that generalize (7) including [6], [7], <ref> [10] </ref>, and [15]. Also see Section 10 in [12] for additional discussion and references on these other approaches. 2.2 Principal Curves and Surfaces Principal curves (PC) were first proposed in [8] and [9]. PCA finds a unit-length vector u that satisfies the minimum distance property in (8).
Reference: [11] <author> M.A. Kramer, </author> <title> "Nonlinear principal component analysis using autoas-sociative neural networks," </title> <journal> AIChE Journal, </journal> <volume> vol. 37, no. 2, </volume> <pages> pp. 233-243, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction Nonlinear principal components analysis (NLPCA) <ref> [11] </ref> is one approach to nonlinear feature extraction. It uses five-layer autoassociative neural networks with a bottleneck layer of nodes (e.g., Fig. 1) to reduce the dimension of the input variables. <p> To address the issue of scaling for principal surfaces, [12] suggests rescaling each score vector s (j) so that 0 s ij 1 with min s (j) = 0 and max s (j) = 1. 2.3 Nonlinear Principal Components Analysis Nonlinear principal components analysis (NLPCA) <ref> [11] </ref> was proposed independently of principal curves and surfaces and was motivated by a PCA implementation using feedforward neural networks. Before presenting the NLPCA method, I first sketch the neural-network implementation of PCA. Feedforward neural networks can be used to extract principal components with the architecture shown in Fig. 3.
Reference: [12] <author> M. LeBlanc and R. Tibshirani, </author> <title> "Adaptive principal surfaces," </title> <journal> JASA, </journal> <volume> vol. 89, no. 425, </volume> <pages> pp. 53-64, </pages> <year> 1994. </year>
Reference-contexts: NLPCA generalizes the objective function given in (8). There are several methods that generalize (7) including [6], [7], [10], and [15]. Also see Section 10 in <ref> [12] </ref> for additional discussion and references on these other approaches. 2.2 Principal Curves and Surfaces Principal curves (PC) were first proposed in [8] and [9]. PCA finds a unit-length vector u that satisfies the minimum distance property in (8). <p> Under this definition of principal curves, they show that a curve is a principal curve if and only if it satisfies the objective function in (11). 7 Two-dimensional principal surfaces (PS) were proposed in [8] and were later extended in <ref> [12] </ref> for higher dimensional surfaces. Principal surfaces extend PCs by fitting a surface that satisfies a minimum distance objective function similar to (11). <p> the x 1 -x 2 plane is given by the point of inter section between the x 1 -x 2 plane and the line passing through the "North pole" (0; 0; 1) and x; the stereographic projection of (0; 0; 1) onto the x 1 -x 2 plane is undefined. <ref> [12] </ref> uses the second approach. A problem with this 8 approach is that surfaces that bend back on themselves like a full sphere 3 cannot be parameterized this way. <p> I revisit this discussion in section 3.2 and show that the spherical coordinates for this hemisphere as defined in (12) cannot be used as a parameterization if the projection index is continuous. Principal curves are defined to have unit speed. To address the issue of scaling for principal surfaces, <ref> [12] </ref> suggests rescaling each score vector s (j) so that 0 s ij 1 with min s (j) = 0 and max s (j) = 1. 2.3 Nonlinear Principal Components Analysis Nonlinear principal components analysis (NLPCA) [11] was proposed independently of principal curves and surfaces and was motivated by a PCA
Reference: [13] <author> K.V. Mardia, J.T. Kent, and J.M. Bibby, </author> <title> Multivariate Analysis, </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1979. </year>
Reference-contexts: The principal component transformation is given by S = XU; (4) where S (n fi p) is a matrix of score values. The PCA transformation has several important properties (e.g., see <ref> [13] </ref>). 1. The principal component transformation is a change of basis from the basis of "unit vectors" I to a basis consisting of the column vectors of U. Column vector u (j) (pfi 1) is called the j th principal axis or principal direction. 2. <p> This change of basis is called a rotation in the social science literature. See <ref> [13, x 9.6] </ref> or [2, x 3.4] for further discussion. Before the surface-score values can be interpreted, similar work must be done for the nonlinear feature extraction methods.
Reference: [14] <author> D.E. Rumelhart, G.E. Hinton, and J.L. McClelland, </author> <title> "Learning internal representations by error propagation," </title> <booktitle> in Parallel Distributed Processing Volume 1: Foundations, </booktitle> <pages> pp. 318-362. </pages> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Data compression therefore occurs in the bottleneck layer. Note that this architecture estimates the PCA solution because it minimizes the same objective function as PCA (Equation 8). An early description of this application of neural networks is the encoder-decoder problem described in <ref> [14, pp. 335-339] </ref>. [1] later develops some theoretical properties of this method. 3 The statement is true for any partial sphere "greater" than a hemisphere, e.g., Equation (12) with s 1 2 [0; 2), s 2 2 [0; a), and a &gt; =2. 9 ` - ` - 7 S S
Reference: [15] <author> Terence D. Sanger, </author> <title> "Optimal hidden units for two-layer nonlinear feed-forward neural networks," </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> vol. 5, no. 4, </volume> <pages> pp. 545-561, </pages> <year> 1991. </year>
Reference-contexts: NLPCA generalizes the objective function given in (8). There are several methods that generalize (7) including [6], [7], [10], and <ref> [15] </ref>. Also see Section 10 in [12] for additional discussion and references on these other approaches. 2.2 Principal Curves and Surfaces Principal curves (PC) were first proposed in [8] and [9]. PCA finds a unit-length vector u that satisfies the minimum distance property in (8).
Reference: [16] <author> D.J. Struik, </author> <title> Lectures on Classical Differential Geometry, </title> <publisher> Dover Publication, Inc., </publisher> <address> New York, </address> <note> second edition, </note> <year> 1961. </year>
Reference: [17] <author> Shufeng Tan and Michael Mavrovouniotis, </author> <title> "Reducing data dimensionality through optimizing neural network inputs," </title> <journal> AIChE Journal, </journal> <note> 1995, in press. </note>
Reference-contexts: NLPCA and related nonlinear feature extraction methods have been successfully applied to many different problems. For example, [4] shows how NLPCA can be applied to image compression problems. <ref> [17] </ref> show how a version of NLPCA can be used for another data compression problem. [5] shows how another version of NLPCA can be used to construct control charts. These examples suggest that NLPCA is a promising method. <p> I generated 100 equally-spaced s values in the interval [0; 2] and their corresponding 100 points along the perimeter of a circle using (2). I fitted an NLPCA model and then evaluated the model on a cross-validation grid of 9 <ref> [17] </ref> independently noted that NLPCA has problems in approximating curves that intersect themselves. 16 points in R 2 to understand the NLPCA solution. To insure thorough train-ing, I trained the network for 6000 iterations with the quasi-Newton BFGS nonlinear optimization algorithm and the resulting FVU was 0:0097. <p> One application is data/image compression. The intrinsic dimension of the data can be substantially less than the superficial dimension. In this case the score values provide a compressed version of the original data. Note that the score values are not interpreted or used to make any decisions. <ref> [17, Example 4] </ref> capture 95% of the variation in p = 65 observed x's with r = 3 nonlinear components. [4] uses a version of NLPCA for image compression.
Reference: [18] <author> J.A. Thorpe, </author> <title> Elementary Topics in Differential Geometry, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1979. </year> <month> 24 </month>
References-found: 18

