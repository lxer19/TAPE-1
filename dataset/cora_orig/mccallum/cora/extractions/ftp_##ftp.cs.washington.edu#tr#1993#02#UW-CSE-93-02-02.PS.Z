URL: ftp://ftp.cs.washington.edu/tr/1993/02/UW-CSE-93-02-02.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Email: travis@cs.washington.edu  
Title: Building FIFO and Priority-Queuing Spin Locks from Atomic Swap  
Author: Travis S. Craig 
Note: This research was supported in part by the National Science Foundation under grant number CCR 9200858 and by a National Defense Science and Engineering Graduate Fellowship.  
Date: February 1, 1993  
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering, FR-35 University of Washington  
Pubnum: Technical Report 93-02-02  
Abstract: We present practical new algorithms for FIFO or priority-ordered spin locks on shared-memory multiprocessors with an atomic swap instruction. Different versions of these queuing spin locks are designed for machines with coherent-cache and NUMA memory models. We include extensions to provide nested lock acquisition, conditional locking, timeout of lock requests, and preemption of waiters. These locks apply to both real time and non-real-time parallel systems and we include a comparison of the traits of several lock schemes aimed at those environments. Our main technical contributions are our techniques and algorithms that provide tight control over lock grant order, use only the atomic swap instruction, use at most one (local only) spin for lock acquisition and no spinning for lock release, and need only O(L + P ) space on either a coherent cache or NUMA machine. 
Abstract-found: 1
Intro-found: 1
Reference: [And90] <author> Thomas E. Anderson. </author> <title> The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: The chosen waiter is unblocked and runs when scheduled. To wait actively, a process typically enters a tight loop in which it repeatedly checks the status of the lock and/or attempts to acquire it. Once it acquires the lock, it simply proceeds to access the protected structure. Both Anderson <ref> [And90] </ref> and Mellor-Crummey and Scott [MCS91] provide extensive discussions of the relative strengths and weaknesses of relinquishing and spinning. Together with Graunke and Thakkar [GT90], they discuss numerous architectural and software considera tions and evaluate several spin lock schemes in the context of non-real-time parallel systems. <p> did not receive the lock. * Preemption of Waiters | Ensure that a lock is not granted to a waiter that is not running, but don't prevent the scheduler from preempting a process that is waiting for a lock. 1.1 Previous Work on Queuing Spin Locks The works cited earlier <ref> [And90, GT90, MCS91] </ref> use queue locks for general-purpose (non-real-time) multiprocessor synchronization in coherent-cache and/or NUMA machines. Their goal, therefore, is good average-case behavior.
Reference: [BD91] <author> Eric A. Brewer and Chrysthanos N. Dellarocas. </author> <title> PROTEUS User Documentation, Version 0.2. </title> <publisher> MIT, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1991. </year>
Reference-contexts: That protocol didn't involve a swap, but would not be reliable under some reasonable assumptions about read/write ordering in a NUMA machine. We have also implemented a nesting version of our coherent-FIFO lock on the Proteus multiprocessor simulator <ref> [BD91] </ref>. One benefit of working on the simulator is that we can monitor the functional performance of the lock scheme. Our prototype includes a detector 17 for violations of mutual exclusion that has been triggered only when we purposely altered the lock acquisition routine to test the detector.
Reference: [BRE92] <author> Brian N. Bershad, David D. Redell, and John R. Ellis. </author> <title> Fast Mutual Exclusion for Uniprocessors. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 223-233, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: In any case, we must still work out the details of detecting that a process is waiting for a lock and backing it out of that condition. Recent work with non-queuing locks <ref> [BRE92] </ref> offers some promising techniques. 6 Discussion 6.1 Testing We have implemented nesting versions of all four of our algorithms and performed the minimal functional testing that we could do in a non-multiprogrammed environment (simple mutual exclusion, lock acquisition and lock release).
Reference: [Bur78] <author> J. E. Burns. </author> <title> Mutual Exclusion with Linear Waiting using Binary Shared Variables. </title> <journal> SIGACT News, </journal> <volume> 10(2), </volume> <month> Summer </month> <year> 1978. </year>
Reference-contexts: Their goal, therefore, is good average-case behavior. Molesky, Shen, and Zlokapa [MSZ90] start from work by Burns <ref> [Bur78] </ref> and their own work on characterizing round-robin buses to produce schemes that grant locks in round-robin fashion, using test-and-set as their only atomic instruction. By doing so, they at least bound priority inversion by bounding the waiting time for any given processor to get a lock. <p> Markatos and LeBlanc [ML91] attack the same priority spin lock problem as we do, but for (1) coherent-cache machines with test-and-set instructions and (2) coherent-cache and NUMA machines with both swap and compare-and-swap instructions. They base their solutions on their improved version of Burns <ref> [Bur78] </ref> and on Mellor-Crummey and Scott [MCS91], respectively. In the course of their work, they develop a number of important concepts that are basic to priority spin locks. We cite their work when we present or use their concepts. <p> The "linear" waiting in the case of Markatos and LeBlanc's coherent-cache lock is due to its derivation from Burns's lock <ref> [Bur78] </ref>. Linear waiting is not first-come-first-served but says that when a process (say P) is waiting, no other process will get the lock more than once before P gets the lock. a coherent-cache or NUMA machine. An additional contribution is our outline of techniques to extend the basic algorithms.
Reference: [Gra92] <author> Gary Graunke. </author> <type> Personal communication, </type> <month> December </month> <year> 1992. </year>
Reference-contexts: After finding that the request it's watching has not been granted, the requester immediately executes the timeout action. Graunke has also developed a method to provide a conditional lock and remove the failed request from the queue <ref> [Gra92] </ref>. 5.5 Preemption As discussed by Markatos and LeBlanc [ML91], another lock-related issue is that of preemption. Clearly, if the holder of a lock is descheduled, use of the critical section stops until the 16 holder is scheduled again.
Reference: [GT90] <author> Gary Graunke and Shreekant Thakkar. </author> <title> Synchronization Algorithms for Shared-Memory Multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 23(6) </volume> <pages> 60-69, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Once it acquires the lock, it simply proceeds to access the protected structure. Both Anderson [And90] and Mellor-Crummey and Scott [MCS91] provide extensive discussions of the relative strengths and weaknesses of relinquishing and spinning. Together with Graunke and Thakkar <ref> [GT90] </ref>, they discuss numerous architectural and software considera tions and evaluate several spin lock schemes in the context of non-real-time parallel systems. Molesky, etal [MSZ90] and Markatos and LeBlanc [ML91] discuss the considerations for real-time systems. <p> did not receive the lock. * Preemption of Waiters | Ensure that a lock is not granted to a waiter that is not running, but don't prevent the scheduler from preempting a process that is waiting for a lock. 1.1 Previous Work on Queuing Spin Locks The works cited earlier <ref> [And90, GT90, MCS91] </ref> use queue locks for general-purpose (non-real-time) multiprocessor synchronization in coherent-cache and/or NUMA machines. Their goal, therefore, is good average-case behavior. <p> We assume that processes and data structures do not migrate between processors on the NUMA machine. 3 Queuing Spin Locks on Machines with Coherent Caches Starting with Graunke and Thakkar's FIFO-queuing spin lock <ref> [GT90] </ref>, we must solve two problems to adapt it to a priority queue. First, a process that has just granted the lock to an arbitrary waiter must be able to know when its Request record is available for it to request the lock again. <p> Note, however, that the Request will be reached eventually in a FIFO queue and it will be reached on the next release or two in a priority queue that is completely scanned on each release. 5.4 Conditional Lock A problem noted by Graunke and Thakkar <ref> [GT90] </ref> is that of allowing a conditional request for a lock, one that acquires the lock if it's immediately available and fails otherwise. We observe that a conditional request is very much like one with a zero timeout. <p> Table 1 presents a number of queuing spin lock schemes in terms of these characteristics. For coherent-cache machines, it includes the locks of Graunke and Thakkar <ref> [GT90] </ref>, Markatos and LeBlanc [ML91], and ours. <p> that provide tight control over lock grant order, use only the atomic swap instruction, use at most one (local only) spin for lock acquisition and no spinning for lock release, and need only O (L + P ) space on either 19 [ML91] Our [MCS91] [MCS91] [ML91] Our Category Trait <ref> [GT90] </ref> Coherent Coherent (C&S) (Swap only) NUMA NUMA Memory Coh.
Reference: [HM92] <author> Maurice Herlihy and J. Eliot B. Moss. </author> <title> Transactional Memory: Architectural Support for Lock-Free Data Structures. </title> <type> Technical Report CRL 92/07, </type> <institution> Digital Equipment Corporation, Cambridge Research Lab, </institution> <month> December </month> <year> 1992. </year> <month> 21 </month>
Reference-contexts: The actual performance difference would depend on the relative expense of the swap and compare-and-swap instructions on a given machine. Finally, we have not yet looked seriously at our work in the context of other fetch-and-op instructions, load-linked/store-conditional [KH92], or transactional memory <ref> [HM92] </ref>. * Order in which the lock is granted to waiters (random, FIFO, or priority). For general parallel programming, FIFO order might be marginally better than random order, because it's fair and prevents starvation.
Reference: [Int90] <institution> Intel Corporation, Santa Clara, California. Microprocessors, </institution> <year> 1990. </year>
Reference-contexts: We haven't done a serious survey of available architectures, but we believe that the most generally applicable lock would use just test-and-set. Given a machine with an atomic swap instruction (eg. multiprocessors that use the swap available in the Intel 80x86 <ref> [Int90] </ref> or Motorola 88x00 [Mot91]), we also believe that our algorithms offer better characteristics than would a queuing spin lock that used the swap as a test-and-set. The algorithms that use compare-and-swap 18 offer some potential performance advantages over ours in NUMA machines, but they also require a swap instruction.
Reference: [KH92] <author> Gerry Kane and Joe Heinrich. </author> <title> MIPS RISC Architecture. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: Neither swap nor compare-and-swap may be trivially emulated by the other. The actual performance difference would depend on the relative expense of the swap and compare-and-swap instructions on a given machine. Finally, we have not yet looked seriously at our work in the context of other fetch-and-op instructions, load-linked/store-conditional <ref> [KH92] </ref>, or transactional memory [HM92]. * Order in which the lock is granted to waiters (random, FIFO, or priority). For general parallel programming, FIFO order might be marginally better than random order, because it's fair and prevents starvation.
Reference: [MCS91] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: To wait actively, a process typically enters a tight loop in which it repeatedly checks the status of the lock and/or attempts to acquire it. Once it acquires the lock, it simply proceeds to access the protected structure. Both Anderson [And90] and Mellor-Crummey and Scott <ref> [MCS91] </ref> provide extensive discussions of the relative strengths and weaknesses of relinquishing and spinning. Together with Graunke and Thakkar [GT90], they discuss numerous architectural and software considera tions and evaluate several spin lock schemes in the context of non-real-time parallel systems. <p> did not receive the lock. * Preemption of Waiters | Ensure that a lock is not granted to a waiter that is not running, but don't prevent the scheduler from preempting a process that is waiting for a lock. 1.1 Previous Work on Queuing Spin Locks The works cited earlier <ref> [And90, GT90, MCS91] </ref> use queue locks for general-purpose (non-real-time) multiprocessor synchronization in coherent-cache and/or NUMA machines. Their goal, therefore, is good average-case behavior. <p> They base their solutions on their improved version of Burns [Bur78] and on Mellor-Crummey and Scott <ref> [MCS91] </ref>, respectively. In the course of their work, they develop a number of important concepts that are basic to priority spin locks. We cite their work when we present or use their concepts. <p> space of using the heap would not pay off unless the number of waiters were quite high, but use of the heap can be switched on and off adaptively, for any given lock, by the releasing routine. 5.2 Nested Locks With the other schemes that use O (L+P ) space <ref> [MCS91, ML91] </ref>, a lock-holding process that needs to acquire another (nested) lock must provide an additional Request record. Therefore, with nesting they require O (L + P fl D) space, where D is the depth of the nesting. <p> First, the cache hit rate is much less sensitive to contention with our queuing lock than it is with a simple test-and-set lock. Second, we get a very high hit rate when we have only one process repeatedly entering and leaving the critical section. 6.2 Characterizing Spin Lock Schemes <ref> [MCS91] </ref> nicely identifies four general traits by which to characterize and compare spin lock algorithms. We generalize these traits and add a fifth one of our own (atomic instruction set) to create the following list of traits and our comments on them. <p> Table 1 presents a number of queuing spin lock schemes in terms of these characteristics. For coherent-cache machines, it includes the locks of Graunke and Thakkar [GT90], Markatos and LeBlanc [ML91], and ours. For NUMA machines, it includes the locks of Mellor-Crummey and Scott (with and without compare-and-swap) <ref> [MCS91] </ref>, Markatos and LeBlanc, and ours. 7 Summary and Future Work Our main technical contributions are our techniques and algorithms that provide tight control over lock grant order, use only the atomic swap instruction, use at most one (local only) spin for lock acquisition and no spinning for lock release, and <p> contributions are our techniques and algorithms that provide tight control over lock grant order, use only the atomic swap instruction, use at most one (local only) spin for lock acquisition and no spinning for lock release, and need only O (L + P ) space on either 19 [ML91] Our <ref> [MCS91] </ref> [MCS91] [ML91] Our Category Trait [GT90] Coherent Coherent (C&S) (Swap only) NUMA NUMA Memory Coh. <p> are our techniques and algorithms that provide tight control over lock grant order, use only the atomic swap instruction, use at most one (local only) spin for lock acquisition and no spinning for lock release, and need only O (L + P ) space on either 19 [ML91] Our <ref> [MCS91] </ref> [MCS91] [ML91] Our Category Trait [GT90] Coherent Coherent (C&S) (Swap only) NUMA NUMA Memory Coh.
Reference: [ML91] <author> Evangelos P. Markatos and Thomas J. LeBlanc. </author> <title> Multiprocessor Synchronization Primitives with Priorities. </title> <type> Technical report, </type> <institution> University of Rochester, Rochester, </institution> <address> NY, </address> <year> 1991. </year>
Reference-contexts: Together with Graunke and Thakkar [GT90], they discuss numerous architectural and software considera tions and evaluate several spin lock schemes in the context of non-real-time parallel systems. Molesky, etal [MSZ90] and Markatos and LeBlanc <ref> [ML91] </ref> discuss the considerations for real-time systems. Each of these sources also introduces one or more schemes for queuing spin locks, in which each waiting process spins on a separate location (a separate cache block on coherent-cache machines) and ownership of the lock is explicitly passed from process to process. <p> By doing so, they at least bound priority inversion by bounding the waiting time for any given processor to get a lock. They also cover some subtle hardware considerations for attaining predictability. Markatos and LeBlanc <ref> [ML91] </ref> attack the same priority spin lock problem as we do, but for (1) coherent-cache machines with test-and-set instructions and (2) coherent-cache and NUMA machines with both swap and compare-and-swap instructions. They base their solutions on their improved version of Burns [Bur78] and on Mellor-Crummey and Scott [MCS91], respectively. <p> In Figure 5 (e), P 1 has removed itself and its new Request from the list. The next step is to find the highest-priority waiter, which is done simply by traversing the stable portion of the list. As noted by Markatos and LeBlanc <ref> [ML91] </ref>, we must first choose the highest-priority waiter and then grant it the lock. There is no atomic action to do both. <p> to Waiter G Granter Swaps First Request (watcher, state) nil Request (watcher, state) PPointer to Waiter G Granter (watcher, dummy) P nil Granter (watcher, dummy) P nil (dummy, state) Waiter P nil Request (watcher, state) nil G 13 5 Other Features 5.1 Speed of Selecting a Grantee Markatos and LeBlanc <ref> [ML91] </ref> assume in their model that process priorities are dynamic, as they are in some real-time scheduling schemes (eg. earliest deadline first (EDF), least slack time). If priorities can change at any time, then it's necessary for each lock holder to consider all waiters when choosing a successor. <p> space of using the heap would not pay off unless the number of waiters were quite high, but use of the heap can be switched on and off adaptively, for any given lock, by the releasing routine. 5.2 Nested Locks With the other schemes that use O (L+P ) space <ref> [MCS91, ML91] </ref>, a lock-holding process that needs to acquire another (nested) lock must provide an additional Request record. Therefore, with nesting they require O (L + P fl D) space, where D is the depth of the nesting. <p> After finding that the request it's watching has not been granted, the requester immediately executes the timeout action. Graunke has also developed a method to provide a conditional lock and remove the failed request from the queue [Gra92]. 5.5 Preemption As discussed by Markatos and LeBlanc <ref> [ML91] </ref>, another lock-related issue is that of preemption. Clearly, if the holder of a lock is descheduled, use of the critical section stops until the 16 holder is scheduled again. <p> Once preempted, the waiter is no longer waiting and is not at risk to acquire the lock. With queuing locks, however, preemption of a process does not prevent that process from being granted the lock, which would turn it into a preempted lock holder. As noted in <ref> [ML91] </ref>, however, we might want to allow schedulers to preempt waiters. To do so, we need a way for the scheduler to determine that a process is spinning for a lock and to deactivate the lock's request for the duration of the preemption. <p> Table 1 presents a number of queuing spin lock schemes in terms of these characteristics. For coherent-cache machines, it includes the locks of Graunke and Thakkar [GT90], Markatos and LeBlanc <ref> [ML91] </ref>, and ours. <p> main technical contributions are our techniques and algorithms that provide tight control over lock grant order, use only the atomic swap instruction, use at most one (local only) spin for lock acquisition and no spinning for lock release, and need only O (L + P ) space on either 19 <ref> [ML91] </ref> Our [MCS91] [MCS91] [ML91] Our Category Trait [GT90] Coherent Coherent (C&S) (Swap only) NUMA NUMA Memory Coh. <p> our techniques and algorithms that provide tight control over lock grant order, use only the atomic swap instruction, use at most one (local only) spin for lock acquisition and no spinning for lock release, and need only O (L + P ) space on either 19 <ref> [ML91] </ref> Our [MCS91] [MCS91] [ML91] Our Category Trait [GT90] Coherent Coherent (C&S) (Swap only) NUMA NUMA Memory Coh.
Reference: [Mot91] <author> Motorola, Inc., </author> <title> Phoenix, Arizona. MC88110: Second Generation RISC Microprocessor User's Manual, </title> <year> 1991. </year>
Reference-contexts: We haven't done a serious survey of available architectures, but we believe that the most generally applicable lock would use just test-and-set. Given a machine with an atomic swap instruction (eg. multiprocessors that use the swap available in the Intel 80x86 [Int90] or Motorola 88x00 <ref> [Mot91] </ref>), we also believe that our algorithms offer better characteristics than would a queuing spin lock that used the swap as a test-and-set. The algorithms that use compare-and-swap 18 offer some potential performance advantages over ours in NUMA machines, but they also require a swap instruction.
Reference: [MSZ90] <author> Lory D. Molesky, Chia Shen, and Goran Zlokapa. </author> <title> Predictable Synchronization Mechanisms for Real-Time Systems. </title> <booktitle> Real-Time Systems, </booktitle> <volume> 2(3) </volume> <pages> 163-180, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Together with Graunke and Thakkar [GT90], they discuss numerous architectural and software considera tions and evaluate several spin lock schemes in the context of non-real-time parallel systems. Molesky, etal <ref> [MSZ90] </ref> and Markatos and LeBlanc [ML91] discuss the considerations for real-time systems. <p> Their goal, therefore, is good average-case behavior. Molesky, Shen, and Zlokapa <ref> [MSZ90] </ref> start from work by Burns [Bur78] and their own work on characterizing round-robin buses to produce schemes that grant locks in round-robin fashion, using test-and-set as their only atomic instruction.
Reference: [SRL90] <author> Lui Sha, Ragunathan Rajkumar, and John P. Lehoczky. </author> <title> Priority Inheritance Protocols: An Approach to Real-Time Synchronization. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(9) </volume> <pages> 1175-1185, </pages> <month> September </month> <year> 1990. </year> <month> 22 </month>
Reference-contexts: In priority-scheduled real-time systems, however, a common goal is to reduce priority inversion (where a lower-priority process is holding a resource that is needed by a 2 higher-priority process <ref> [SRL90] </ref>). Therefore, we are interested in mechanisms to grant locks to waiting processes in priority order. We provide a priority-queuing spin lock for coherent cache machines and one for NUMA machines.
References-found: 14

