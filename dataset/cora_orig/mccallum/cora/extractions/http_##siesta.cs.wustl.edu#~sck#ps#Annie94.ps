URL: http://siesta.cs.wustl.edu/~sck/ps/Annie94.ps
Refering-URL: http://siesta.cs.wustl.edu/~sck/
Root-URL: 
Title: AN ADAPTIVE NEURAL NETWORK PARSER  
Author: SAHNNY JOHNSON STAN C. KWASNY AND BARRY L. KALMAN 
Address: Knox College, Galesburg, IL  St. Louis, MO  
Affiliation: Dept. of Computer Science,  Dept. of Computer Science, Washington University,  
Abstract: We inv estigate the applicability of an adaptive neural network to problems with time-dependent input by demonstrating that a deterministic parser for natural language inputs of significant syntactic complexity can be developed using recurrent connectionist architectures. The traditional stacking mechanism, known to be necessary for proper treatment of context-free languages in symbolic systems, is absent from the design, having been subsumed by recurrency in the network.
Abstract-found: 1
Intro-found: 1
Reference: <author> Das, S., Giles, C.L., & Sun, G.Z. </author> <year> (1992). </year> <title> Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory. </title> <booktitle> Proc 14th Ann Conf Cog Sci Soc. </booktitle> <institution> Bloomington: Indiana University. </institution>
Reference-contexts: Unlike recurrent parsing networks described elsewhere <ref> (see, for example, Das et al., 1992) </ref>, our system contains no explicit internal or external stack. The stack's role is assumed to a sufficient degree by the recurrence in the network.
Reference: <author> Elman, J. L. </author> <year> (1990). </year> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14, </volume> <pages> 179-212. </pages>
Reference: <author> Kalman, B. L. </author> <year> (1990). </year> <title> Super linear learning in back propagation neural nets. </title> <type> Tech Rep WUCS-90-21, </type> <institution> Dept of Comp Sci, Washington University, St. Louis, MO. </institution>
Reference-contexts: For the parser, the number of input units was reduced from 38 to 34, effectively yielding an architecture of 64-30-39 for training and 68-30-39 for testing. Training itself is performed using a modified conjugate-gradient method <ref> (Kalman, 1990) </ref>. RESULTS For training purposes, we constructed a set of 109 sentences that collectively exercised 32 of the 39 possible actions and covered a variety of syntactic forms, concentrating on basic sentence structure and relative clauses. A representative sample is shown in the first column of Figure 2.
Reference: <author> Kalman, B. L., Kwasny, S. C ., & Abella, A. </author> <year> (1993). </year> <title> Decomposing input patterns to facilitate training. </title> <booktitle> Proc World Congress on Neural Networks, V.III, </booktitle> <pages> 503-506. </pages>
Reference: <author> Kwasny, S. C ., & Faisal, K. A. </author> <year> (1990). </year> <title> Connectionism and determinism in a syntactic parser. </title> <journal> Connection Science, </journal> <volume> 2, </volume> <pages> 63-82. </pages>
Reference: <author> Kwasny, S. C ., & Faisal, K. A. </author> <year> (1992). </year> <title> Symbolic parsing via sub-symbolic rules. </title> <editor> In J. Dinsmore (Ed.), </editor> <title> Closing the gap: Symbolism vs. connectionism. </title> <address> Hillsdale, NJ: </address> <publisher> LEA. </publisher>
Reference: <author> Kwasny, S. C ., & Kalman, B. L. </author> <year> (1991). </year> <title> The case of the unknown word: Imposing syntactic constraints on words missing from the lexicon. </title> <booktitle> Proc 3rd Midwest Art Intel & Cog Sci Soc Conf. </booktitle>
Reference: <author> Lee, S. E., & Holt, B. R. </author> <year> (1992). </year> <title> Regression analysis of spectroscopic process data using a combined architecture of linear and nonlinear neural networks. </title> <booktitle> Proc Intl Joint Conf on Neural Networks V.IV, </booktitle> <address> Piscataway, NJ: </address> <publisher> IEEE. </publisher>
Reference-contexts: These activation patterns encode information about relevant past events so that current and future decisions can be inuenced, much as the stack would inuence symbolic processing. Since there is evidence that isolating the linearly separable relationship from the nonlinear part makes for more effective training <ref> (Lee & Holt, 1992) </ref>, and since parsing seems to require approximately linear processing for most of the steps, we adapted the recurrent network designed by Elman to include direct connections from input to output units.
Reference: <author> Marcus, M. P. </author> <year> (1980). </year> <title> A theory of syntactic recognition for natural language. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Starting from the basic design of a deterministic natural language parser, we are introducing and evaluating connectionist techniques in a series of evolutionary steps toward a fully connectionist language understanding system. Deterministic or wait-and-see parsing <ref> (Marcus, 1980) </ref> is an appropriate model for the design of a neural net based parser in two important ways. First, it concentrates on syntax, which has the advantage of being relatively well understood.
Reference: <author> Pinker, S., & Prince, A. </author> <year> (1988). </year> <title> On language and connectionism: Analysis of a parallel distributed processing model of language acquisition. </title> <editor> In S. Pinker & J. Mehler (Eds), </editor> <title> Connections and symbols. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
References-found: 10

