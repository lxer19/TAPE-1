URL: http://www.cs.ubc.ca/spider/cebly/Papers/multirl.ps
Refering-URL: http://euler.mcs.utulsa.edu/~sandip/wshop/97/
Root-URL: 
Email: fcclaus,ceblyg@cs.ubc.ca  
Title: The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems  
Author: Caroline Claus and Craig Boutilier 
Address: Vancouver, B.C., Canada V6T 1Z4  
Affiliation: Department of Computer Science University of British Columbia  
Abstract: Reinforcement learning can provide a robust and natural means for agents to learn how to coordinate their action choices in multiagent systems. We examine some of the factors that can influence the dynamics of the learning process in such a setting. We first distinguish reinforcement learners that are unaware of (or ignore) the presence of other agents from those that explicitly attempt to learn the value of joint actions and the strategies of their counterparts. We study Q-learning in cooperative multiagent systems under these two perspectives, focusing on the influence of partial action observability, game structure, and exploration strategies on convergence to (optimal and suboptimal) Nash equilibria and on learned Q-values. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Robert Axelrod. </author> <title> The Evolution of Cooperation. </title> <publisher> Basic Books, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: Here we entertain the suggestion that coordinated action choice might be learned through repeated play of the game with the same agents [5, 6, 10, 13]. (Repeated play with a random selection of similar agents from a large population has also been the object of considerable study <ref> [1, 18, 11, 24] </ref>.) One especially simple, yet often effective, learning model for achieving coordination is fictitious play [4, 5].
Reference: [2] <author> Craig Boutilier. </author> <title> Learning conventions in multiagent stochastic domains using likelihood estimates. </title> <booktitle> In Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 106-114, </pages> <address> Portland, OR, </address> <year> 1996. </year>
Reference-contexts: This very simple adaptive strategy will converge to an equilibrium in our simple cooperative games, and can be made to converge to an optimal equilibrium if appropriate mechanisms are adopted <ref> [24, 2] </ref>; that is, the probability of coordinated equilibrium after k interactions can be made arbitrarily high by increasing k sufficiently. <p> We note that most game theoretic models assume that each agent can observe the actions executed by its counterparts with certainty. As pointed out and addressed in <ref> [2, 8] </ref>, this assumption is often unrealistic. We will be interested below in the more general case where each agent obtains an observation which is related stochastically to the actual joint action selected. Formally, we assume an observation set O, and an observation model : A ! (O). <p> Following <ref> [2] </ref>, we use a simple Bayesian updating rule for beliefs: Pr (a [j] = a j ja [i] = a i ; o) = Pr (oja [i] = a i ) Agent i then updates its distribution over j's probabilities us ing this stochastic observation; in particular, C j a k <p> This is one area where JALs have a distinct advantage over ILs: even if they have converged to an equilibrium, they can tellsince they have access to joint Q-valuesif a better equilibrium exists. Coordination learning techniques (see, e.g., <ref> [2] </ref>) might then be applied, as could other exploration techniques that attempt to induce a shift from one equilibrium to another. Finally, we hope to explore the details associated with general, multistate sequential decision problems and investigate the application of generalization techniques in domains with large state spaces.
Reference: [3] <author> Craig Boutilier. </author> <title> Planning, learning and coordination in mul-tiagent decision processes. </title> <booktitle> In Proceedings of the Sixth Conference on Theoretical Aspects of Rationality and Knowledge, </booktitle> <pages> pages 195-210, </pages> <address> Amsterdam, </address> <year> 1996. </year>
Reference-contexts: Given a profile i , a strategy i is a best response for agent i if the expected value of the strategy profile 1 Most of our conclusions hold mutatis mutandis for sequential, multiagent Markov decision processes <ref> [3] </ref> with multiple states. i [ f i g is maximal for agent i; that is, agent i could not do better using any other strategy 0 i .
Reference: [4] <author> George W. Brown. </author> <title> Iterative solution of games by fictitious play. </title> <editor> In T. C. Koopmans, editor, </editor> <title> Activity Analysis of Production and Allocation. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1951. </year>
Reference-contexts: of the game with the same agents [5, 6, 10, 13]. (Repeated play with a random selection of similar agents from a large population has also been the object of considerable study [1, 18, 11, 24].) One especially simple, yet often effective, learning model for achieving coordination is fictitious play <ref> [4, 5] </ref>. Each agent i keeps a count C j a j , for each j 2 ff and a j 2 A j , of the number of times agent j has used action a j in the past.
Reference: [5] <author> Drew Fudenberg and David M. Kreps. </author> <title> Lectures on Learning and Equilibrium in Strategic Form Games. CORE Foundation, </title> <address> Louvain-La-Neuve, Belgium, </address> <year> 1992. </year>
Reference-contexts: Independent learners (ILs) apply Q-learning in the classic sense, ignoring the existence of other agents. Joint action learners (JALs), in contrast, learn the value of their own actions in conjunction with those of other agents via integration of RL with equilibrium (or coordination) learning methods <ref> [24, 6, 5, 10] </ref>. We also examine the influence of partial observability on JALs, and how game structure and exploration strategies influence the dynamics of the learning process and the convergence to equilibrium. <p> For instance, communication between agents might be admitted [22, 23] or one could impose conventions or rules that restrict behavior so as to ensure coordination [12, 19]. Here we entertain the suggestion that coordinated action choice might be learned through repeated play of the game with the same agents <ref> [5, 6, 10, 13] </ref>. (Repeated play with a random selection of similar agents from a large population has also been the object of considerable study [1, 18, 11, 24].) One especially simple, yet often effective, learning model for achieving coordination is fictitious play [4, 5]. <p> of the game with the same agents [5, 6, 10, 13]. (Repeated play with a random selection of similar agents from a large population has also been the object of considerable study [1, 18, 11, 24].) One especially simple, yet often effective, learning model for achieving coordination is fictitious play <ref> [4, 5] </ref>. Each agent i keeps a count C j a j , for each j 2 ff and a j 2 A j , of the number of times agent j has used action a j in the past. <p> At this point, (with high probability) B will perform b1, allowing A to respond in a similar fashion. These arguments can be put together to show that, with proper exploration and proper use of best responses (i.e., that are at least asymptotically myopic <ref> [5] </ref>, which our suggested methods are), we will eventually converge to an equilibrium.
Reference: [6] <author> Drew Fudenberg and David K. Levine. </author> <title> Steady state learning and Nash equilibrium. </title> <journal> Econometrica, </journal> <volume> 61(3) </volume> <pages> 547-573, </pages> <year> 1993. </year>
Reference-contexts: Independent learners (ILs) apply Q-learning in the classic sense, ignoring the existence of other agents. Joint action learners (JALs), in contrast, learn the value of their own actions in conjunction with those of other agents via integration of RL with equilibrium (or coordination) learning methods <ref> [24, 6, 5, 10] </ref>. We also examine the influence of partial observability on JALs, and how game structure and exploration strategies influence the dynamics of the learning process and the convergence to equilibrium. <p> For instance, communication between agents might be admitted [22, 23] or one could impose conventions or rules that restrict behavior so as to ensure coordination [12, 19]. Here we entertain the suggestion that coordinated action choice might be learned through repeated play of the game with the same agents <ref> [5, 6, 10, 13] </ref>. (Repeated play with a random selection of similar agents from a large population has also been the object of considerable study [1, 18, 11, 24].) One especially simple, yet often effective, learning model for achieving coordination is fictitious play [4, 5].
Reference: [7] <author> John C. Harsanyi and Reinhard Selten. </author> <title> A General Theory of Equilibrium Selection in Games. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1988. </year>
Reference-contexts: If they choose them randomly, or in some way reflecting personal biases, then they risk choosing a suboptimal, or uncoordinated joint action. The general problem of equilibrium selection <ref> [14, 7] </ref> can be addressed in several ways. For instance, communication between agents might be admitted [22, 23] or one could impose conventions or rules that restrict behavior so as to ensure coordination [12, 19].
Reference: [8] <author> Junling Hu and Michael P. Wellman. </author> <title> Self-fulfilling bias in multiagent learning. </title> <booktitle> In Proceedings of the Second International Conference on Multiagent Systems, </booktitle> <pages> pages 118-125, </pages> <address> Ky-oto, </address> <year> 1996. </year>
Reference-contexts: 1 Introduction The application of learning to the problem of coordination in multiagent systems (MASs) has become increasingly popular in AI and game theory. The use of reinforcement learning (RL), in particular, has attracted recent attention <ref> [22, 17, 16, 13, 23, 8, 15] </ref>. As noted in [17], using RL as a means of achieving coordinated behavior is attractive because of its generality and robustness. Standard techniques for RL, for example, Q-learning [21], have been applied directly to MASs with some success. <p> We note that most game theoretic models assume that each agent can observe the actions executed by its counterparts with certainty. As pointed out and addressed in <ref> [2, 8] </ref>, this assumption is often unrealistic. We will be interested below in the more general case where each agent obtains an observation which is related stochastically to the actual joint action selected. Formally, we assume an observation set O, and an observation model : A ! (O).
Reference: [9] <author> Leslie Pack Kaelbling, Michael L. Littman, and Andrew W. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 237-285, </pages> <year> 1996. </year>
Reference-contexts: In such a case, reinforcement learning can be used by the agents to estimate, based on past experience, the expected reward associated with individual or joint actions. We refer to <ref> [9] </ref> for a survey of RL techniques. A simple, well-understood algorithm for single agent learning is Q-learning [21]. The formulation of Q-learning for general sequential decision processes is more sophisticated than we need here.
Reference: [10] <author> Ehud Kalai and Ehud Lehrer. </author> <title> Rational learning leads to Nash equilibrium. </title> <journal> Econometrica, </journal> <volume> 61(5) </volume> <pages> 1019-1045, </pages> <year> 1993. </year>
Reference-contexts: Independent learners (ILs) apply Q-learning in the classic sense, ignoring the existence of other agents. Joint action learners (JALs), in contrast, learn the value of their own actions in conjunction with those of other agents via integration of RL with equilibrium (or coordination) learning methods <ref> [24, 6, 5, 10] </ref>. We also examine the influence of partial observability on JALs, and how game structure and exploration strategies influence the dynamics of the learning process and the convergence to equilibrium. <p> For instance, communication between agents might be admitted [22, 23] or one could impose conventions or rules that restrict behavior so as to ensure coordination [12, 19]. Here we entertain the suggestion that coordinated action choice might be learned through repeated play of the game with the same agents <ref> [5, 6, 10, 13] </ref>. (Repeated play with a random selection of similar agents from a large population has also been the object of considerable study [1, 18, 11, 24].) One especially simple, yet often effective, learning model for achieving coordination is fictitious play [4, 5].
Reference: [11] <author> Michihiro Kandori, George Mailath, and Rafael Rob. </author> <title> Learning, mutation and long run equilibria in games. </title> <journal> Econometrica, </journal> <volume> 61(1) </volume> <pages> 29-56, </pages> <year> 1993. </year>
Reference-contexts: Here we entertain the suggestion that coordinated action choice might be learned through repeated play of the game with the same agents [5, 6, 10, 13]. (Repeated play with a random selection of similar agents from a large population has also been the object of considerable study <ref> [1, 18, 11, 24] </ref>.) One especially simple, yet often effective, learning model for achieving coordination is fictitious play [4, 5].
Reference: [12] <author> David K. Lewis. </author> <title> Conventions, A Philosophical Study. </title> <publisher> Har-vard University Press, </publisher> <address> Cambridge, </address> <year> 1969. </year>
Reference-contexts: The general problem of equilibrium selection [14, 7] can be addressed in several ways. For instance, communication between agents might be admitted [22, 23] or one could impose conventions or rules that restrict behavior so as to ensure coordination <ref> [12, 19] </ref>.
Reference: [13] <author> Michael L. Littman. </author> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 157-163, </pages> <address> New Brunswick, NJ, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction The application of learning to the problem of coordination in multiagent systems (MASs) has become increasingly popular in AI and game theory. The use of reinforcement learning (RL), in particular, has attracted recent attention <ref> [22, 17, 16, 13, 23, 8, 15] </ref>. As noted in [17], using RL as a means of achieving coordinated behavior is attractive because of its generality and robustness. Standard techniques for RL, for example, Q-learning [21], have been applied directly to MASs with some success. <p> For instance, communication between agents might be admitted [22, 23] or one could impose conventions or rules that restrict behavior so as to ensure coordination [12, 19]. Here we entertain the suggestion that coordinated action choice might be learned through repeated play of the game with the same agents <ref> [5, 6, 10, 13] </ref>. (Repeated play with a random selection of similar agents from a large population has also been the object of considerable study [1, 18, 11, 24].) One especially simple, yet often effective, learning model for achieving coordination is fictitious play [4, 5].
Reference: [14] <author> Roger B. Myerson. </author> <title> Game Theory: Analysis of Conflict. </title> <publisher> Har-vard University Press, </publisher> <address> Cambridge, </address> <year> 1991. </year>
Reference-contexts: If they choose them randomly, or in some way reflecting personal biases, then they risk choosing a suboptimal, or uncoordinated joint action. The general problem of equilibrium selection <ref> [14, 7] </ref> can be addressed in several ways. For instance, communication between agents might be admitted [22, 23] or one could impose conventions or rules that restrict behavior so as to ensure coordination [12, 19].
Reference: [15] <author> Tuomas Sandholm and Robert Crites. </author> <title> Learning in the iterated prisoner's dilemma. </title> <journal> Biosystems, </journal> <volume> 37 </volume> <pages> 147-166, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction The application of learning to the problem of coordination in multiagent systems (MASs) has become increasingly popular in AI and game theory. The use of reinforcement learning (RL), in particular, has attracted recent attention <ref> [22, 17, 16, 13, 23, 8, 15] </ref>. As noted in [17], using RL as a means of achieving coordinated behavior is attractive because of its generality and robustness. Standard techniques for RL, for example, Q-learning [21], have been applied directly to MASs with some success. <p> This is one of the questions we explore below. We note that application of Q-learning and other RL methods (or RL-like methods) have met with some success in the past <ref> [22, 17, 16, 18, 15] </ref>. There are two distinct ways in which Q-learning could be applied to a multiagent system. We say a MARL algorithm is an independent learner (IL) algorithm if the agents learn Q-values for their individual actions based on Equation (1).
Reference: [16] <author> Sandip Sen and Mahendra Sekaran. </author> <title> Multiagent coordination with learning classifier systems. </title> <booktitle> In Proceedings of the IJCAI Workshop on Adaptation and Learning in Multiagent Systems, </booktitle> <pages> pages 84-89, </pages> <address> Montreal, </address> <year> 1995. </year>
Reference-contexts: 1 Introduction The application of learning to the problem of coordination in multiagent systems (MASs) has become increasingly popular in AI and game theory. The use of reinforcement learning (RL), in particular, has attracted recent attention <ref> [22, 17, 16, 13, 23, 8, 15] </ref>. As noted in [17], using RL as a means of achieving coordinated behavior is attractive because of its generality and robustness. Standard techniques for RL, for example, Q-learning [21], have been applied directly to MASs with some success. <p> This is one of the questions we explore below. We note that application of Q-learning and other RL methods (or RL-like methods) have met with some success in the past <ref> [22, 17, 16, 18, 15] </ref>. There are two distinct ways in which Q-learning could be applied to a multiagent system. We say a MARL algorithm is an independent learner (IL) algorithm if the agents learn Q-values for their individual actions based on Equation (1).
Reference: [17] <author> Sandip Sen, Mahendra Sekaran, and John Hale. </author> <title> Learning to coordinate without sharing information. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 426-431, </pages> <address> Seattle, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction The application of learning to the problem of coordination in multiagent systems (MASs) has become increasingly popular in AI and game theory. The use of reinforcement learning (RL), in particular, has attracted recent attention <ref> [22, 17, 16, 13, 23, 8, 15] </ref>. As noted in [17], using RL as a means of achieving coordinated behavior is attractive because of its generality and robustness. Standard techniques for RL, for example, Q-learning [21], have been applied directly to MASs with some success. <p> 1 Introduction The application of learning to the problem of coordination in multiagent systems (MASs) has become increasingly popular in AI and game theory. The use of reinforcement learning (RL), in particular, has attracted recent attention [22, 17, 16, 13, 23, 8, 15]. As noted in <ref> [17] </ref>, using RL as a means of achieving coordinated behavior is attractive because of its generality and robustness. Standard techniques for RL, for example, Q-learning [21], have been applied directly to MASs with some success. <p> This is one of the questions we explore below. We note that application of Q-learning and other RL methods (or RL-like methods) have met with some success in the past <ref> [22, 17, 16, 18, 15] </ref>. There are two distinct ways in which Q-learning could be applied to a multiagent system. We say a MARL algorithm is an independent learner (IL) algorithm if the agents learn Q-values for their individual actions based on Equation (1).
Reference: [18] <author> Yoav Shoham and Moshe Tennenholtz. </author> <title> Emergent conventions in multi-agent systems: Initial experimental results and observations. </title> <booktitle> In Proceedings of the Third International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 225-231, </pages> <address> Cambridge, </address> <year> 1992. </year>
Reference-contexts: Here we entertain the suggestion that coordinated action choice might be learned through repeated play of the game with the same agents [5, 6, 10, 13]. (Repeated play with a random selection of similar agents from a large population has also been the object of considerable study <ref> [1, 18, 11, 24] </ref>.) One especially simple, yet often effective, learning model for achieving coordination is fictitious play [4, 5]. <p> This is one of the questions we explore below. We note that application of Q-learning and other RL methods (or RL-like methods) have met with some success in the past <ref> [22, 17, 16, 18, 15] </ref>. There are two distinct ways in which Q-learning could be applied to a multiagent system. We say a MARL algorithm is an independent learner (IL) algorithm if the agents learn Q-values for their individual actions based on Equation (1).
Reference: [19] <author> Yoav Shoham and Moshe Tennenholtz. </author> <title> On the synthesis of useful social laws for artificial agent societies. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 276-281, </pages> <address> San Jose, </address> <year> 1992. </year>
Reference-contexts: The general problem of equilibrium selection [14, 7] can be addressed in several ways. For instance, communication between agents might be admitted [22, 23] or one could impose conventions or rules that restrict behavior so as to ensure coordination <ref> [12, 19] </ref>.
Reference: [20] <author> John H. Tsitsiklis. </author> <title> Asynchronous stochastic approximation and Q-learning. </title> <journal> Machine Learning, </journal> <volume> 16 </volume> <pages> 185-202, </pages> <year> 1994. </year>
Reference-contexts: Here ff is the learning rate (0 ff 1), governing to what extent the new sample replaces the current estimate. If ff is decreased slowly during learning and all actions are sampled infinitely, Q-learning will converge to true Q-values for all actions in the single agent setting <ref> [21, 20] </ref>. Convergence of Q-learning does not depend on the exploration strategy used. An agent can try its actions at any timethere is no requirement to perform actions that are currently estimated to be best.
Reference: [21] <author> Christopher J. C. H. Watkins and Peter Dayan. </author> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: The use of reinforcement learning (RL), in particular, has attracted recent attention [22, 17, 16, 13, 23, 8, 15]. As noted in [17], using RL as a means of achieving coordinated behavior is attractive because of its generality and robustness. Standard techniques for RL, for example, Q-learning <ref> [21] </ref>, have been applied directly to MASs with some success. However, a general understanding of the conditions under which RL can be usefully applied, and exactly what form RL might take in MASs, are problems that have not yet been tackled in depth. <p> In such a case, reinforcement learning can be used by the agents to estimate, based on past experience, the expected reward associated with individual or joint actions. We refer to [9] for a survey of RL techniques. A simple, well-understood algorithm for single agent learning is Q-learning <ref> [21] </ref>. The formulation of Q-learning for general sequential decision processes is more sophisticated than we need here. In our stateless setting, we assume a Q-value, Q (a), that provides an estimate of the value of performing (individual or joint) action a. <p> Here ff is the learning rate (0 ff 1), governing to what extent the new sample replaces the current estimate. If ff is decreased slowly during learning and all actions are sampled infinitely, Q-learning will converge to true Q-values for all actions in the single agent setting <ref> [21, 20] </ref>. Convergence of Q-learning does not depend on the exploration strategy used. An agent can try its actions at any timethere is no requirement to perform actions that are currently estimated to be best.
Reference: [22] <author> Gerhard Wei. </author> <title> Learning to coordinate actions in multi-agent systems. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 311-316, </pages> <address> Cham-bery, FR, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction The application of learning to the problem of coordination in multiagent systems (MASs) has become increasingly popular in AI and game theory. The use of reinforcement learning (RL), in particular, has attracted recent attention <ref> [22, 17, 16, 13, 23, 8, 15] </ref>. As noted in [17], using RL as a means of achieving coordinated behavior is attractive because of its generality and robustness. Standard techniques for RL, for example, Q-learning [21], have been applied directly to MASs with some success. <p> If they choose them randomly, or in some way reflecting personal biases, then they risk choosing a suboptimal, or uncoordinated joint action. The general problem of equilibrium selection [14, 7] can be addressed in several ways. For instance, communication between agents might be admitted <ref> [22, 23] </ref> or one could impose conventions or rules that restrict behavior so as to ensure coordination [12, 19]. <p> This is one of the questions we explore below. We note that application of Q-learning and other RL methods (or RL-like methods) have met with some success in the past <ref> [22, 17, 16, 18, 15] </ref>. There are two distinct ways in which Q-learning could be applied to a multiagent system. We say a MARL algorithm is an independent learner (IL) algorithm if the agents learn Q-values for their individual actions based on Equation (1).
Reference: [23] <author> Holly Yanco and Lynn Andrea Stein. </author> <title> An adaptive communication protocol for cooperating mobile robots. </title> <editor> In J. A. Meyer, H. L. Roitblat, and S.W. Wilson, editors, </editor> <booktitle> From Animals to An-imats: Proceedings of the Second International Conference on the Simulation of Adaptive Behavior, </booktitle> <pages> pages 478-485. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction The application of learning to the problem of coordination in multiagent systems (MASs) has become increasingly popular in AI and game theory. The use of reinforcement learning (RL), in particular, has attracted recent attention <ref> [22, 17, 16, 13, 23, 8, 15] </ref>. As noted in [17], using RL as a means of achieving coordinated behavior is attractive because of its generality and robustness. Standard techniques for RL, for example, Q-learning [21], have been applied directly to MASs with some success. <p> If they choose them randomly, or in some way reflecting personal biases, then they risk choosing a suboptimal, or uncoordinated joint action. The general problem of equilibrium selection [14, 7] can be addressed in several ways. For instance, communication between agents might be admitted <ref> [22, 23] </ref> or one could impose conventions or rules that restrict behavior so as to ensure coordination [12, 19].
Reference: [24] <author> H. Peyton Young. </author> <title> The evolution of conventions. </title> <journal> Economet-rica, </journal> <volume> 61(1) </volume> <pages> 57-84, </pages> <year> 1993. </year>
Reference-contexts: Independent learners (ILs) apply Q-learning in the classic sense, ignoring the existence of other agents. Joint action learners (JALs), in contrast, learn the value of their own actions in conjunction with those of other agents via integration of RL with equilibrium (or coordination) learning methods <ref> [24, 6, 5, 10] </ref>. We also examine the influence of partial observability on JALs, and how game structure and exploration strategies influence the dynamics of the learning process and the convergence to equilibrium. <p> Here we entertain the suggestion that coordinated action choice might be learned through repeated play of the game with the same agents [5, 6, 10, 13]. (Repeated play with a random selection of similar agents from a large population has also been the object of considerable study <ref> [1, 18, 11, 24] </ref>.) One especially simple, yet often effective, learning model for achieving coordination is fictitious play [4, 5]. <p> This very simple adaptive strategy will converge to an equilibrium in our simple cooperative games, and can be made to converge to an optimal equilibrium if appropriate mechanisms are adopted <ref> [24, 2] </ref>; that is, the probability of coordinated equilibrium after k interactions can be made arbitrarily high by increasing k sufficiently.
References-found: 24

