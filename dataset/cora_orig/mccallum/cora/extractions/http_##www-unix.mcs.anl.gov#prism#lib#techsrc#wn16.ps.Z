URL: http://www-unix.mcs.anl.gov/prism/lib/techsrc/wn16.ps.Z
Refering-URL: http://www-unix.mcs.anl.gov/prism/lib/tech.html
Root-URL: http://www.mcs.anl.gov
Title: A Study of the Invariant Subspace Decomposition Algorithm for Banded Symmetric Matrices  
Author: Christian Bischof Xiaobai Sun Anna Tsao Thomas Turnbull 
Date: (revised) 18 April 1994  
Pubnum: SRC-TR-94-114  
Abstract: In this paper, we give an overview of the Invariant Subspace Decomposition Algorithm for banded symmetric matrices and describe a sequential implementation of this algorithm. Our implementation uses a specialized routine for performing banded matrix multiplication together with successive band reduction, yielding a sequential algorithm that is competitive for large problems with the LAPACK QR code in computing all of the eigenvalues and eigenvectors of a dense symmetric matrix. Performance results are given on a variety of machines. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Auslander, L. & A. Tsao, </author> <title> On parallelizable eigensolvers, </title> <journal> Adv. Appl. Math. </journal> <volume> 13 (1992), </volume> <pages> 253-261. </pages>
Reference-contexts: The goal of the PRISM project is the development of algorithms and software for solving large-scale eigenvalue problems based on the invariant subspace decomposition approach originally suggested by Auslander and Tsao <ref> [1] </ref>. The algorithm described here is a promising variant of the Symmetric Invariant Subspace Decomposition Algorithm (SYISDA) [2] that requires significantly less overall work than SYISDA. Let A be a symmetric matrix. <p> Let A be a symmetric matrix. Recall that the SYISDA proceeds as follows: Scaling: Compute bounds on the spectrum (A) of A and use these bounds to compute ff and fi such that for B = ffA + fiI, (B) <ref> [0; 1] </ref>, with the mean eigenvalue of A being mapped to 1 2 . Eigenvalue Smoothing: Let p i (x), i = 1; 2; : : : be polynomials such that in the limit all values are mapped to either 0 or 1. <p> Let kk denote the 2-norm. We have Theorem. Assume that X is a symmetric matrix of band b with all of its eigenvalues in <ref> [0; 1] </ref>. Let X 1 = lim i!1 B 1 (X) and E = X X 1 . Then fl fl B 1 (X) B 1 (X) fl 2 (k1) Proof. X 1 only has eigenvalues 0, 1, and 1=2. Notice that kEk 1=2.
Reference: [2] <author> Bischof, C. H., S. Huss-Lederman, X. Sun, & A. Tsao, </author> <title> The PRISM Project: infrastructure and algorithms for parallel eigensolvers, </title> <booktitle> Proceedings, Scalable Parallel Libraries Conference (Starksville, </booktitle> <address> MS, </address> <month> Oct. </month> <pages> 6-8, </pages> <year> 1993), </year> <note> IEEE, 1993, (also PRISM Working Note #12). </note>
Reference-contexts: The goal of the PRISM project is the development of algorithms and software for solving large-scale eigenvalue problems based on the invariant subspace decomposition approach originally suggested by Auslander and Tsao [1]. The algorithm described here is a promising variant of the Symmetric Invariant Subspace Decomposition Algorithm (SYISDA) <ref> [2] </ref> that requires significantly less overall work than SYISDA. Let A be a symmetric matrix. <p> In our current code, reducing matrices to tridiagonal form results in the fastest algorithm. Since the SBR strategy reduces a dense matrix to narrow band very quickly <ref> [2] </ref>, one might expect that only reducing to narrow band might be superior. Unfortunately, when the band starts out larger, matrices of greater bandwidth must be multiplied, resulting in a significant increase in the matrix multiplication time, since the MRK algorithm is more efficient for smaller bandwidths. <p> Matrices with clustered eigenvalues arise in many important applications, but are problematical for some algorithms. As described in <ref> [2] </ref>, the SYISDA algorithms tend to isolate clusters of eigenvalues. Subproblems consisting of tightly clustered eigenvalues can often be detected using the heuristic described in [2] together with deflation-checking. <p> Matrices with clustered eigenvalues arise in many important applications, but are problematical for some algorithms. As described in <ref> [2] </ref>, the SYISDA algorithms tend to isolate clusters of eigenvalues. Subproblems consisting of tightly clustered eigenvalues can often be detected using the heuristic described in [2] together with deflation-checking. In Table 4, we show relative timings for xSYEV, xSTEBZ, and xBSYISDA for test matrices generated by xLATMS having only three repeated eigenvalues (MODE = 2).
Reference: [3] <author> Bischof, C., M. Marques, & X. Sun, </author> <title> Parallel bandreduction and tridiagonalization, </title> <booktitle> Proceedings, Sixth SIAM Conference on Parallel Processing for Scientific Computing (Norfolk, </booktitle> <address> Virginia, </address> <month> March 22-24, </month> <editor> 1993) (R. F. Sincovec, eds.), </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1993, </year> <note> (also PRISM Working Note #8). </note>
Reference-contexts: Therefore, after a few applications of B 1 , kEk becomes small enough so that the growth in the effective bandwidth slows significantly. An analogous result holds for B 3 . The band reductions are performed using successive band reduction <ref> [5, 3, 6] </ref>. Instead of reducing a matrix from banded form directly to narrow band, SBR reduces a banded matrix to some intermediate band using a blocked algorithm and then reduces this intermediate matrix to the required band. The motivation for SBR is two-fold.
Reference: [4] <author> Bischof, C. & X. Sun, </author> <title> A divide-and-conquer method for tridiagonalizing symmetric matrices with repeated eigenvalues, </title> <type> Preprint MCS-P286-0192, </type> <institution> Argonne National Laboratory (1992), </institution> <note> (also PRISM Working Note #1). </note>
Reference-contexts: Instead of using QR factorization with column pivoting as in [10], we use the rank-revealing tridiagonalization strategy of Bischof and Sun <ref> [4] </ref> to compute the invariant subspaces of C 1 . The rank-revealing tridiagonalization technique exploits both symmetry and the special structure of matrices having only two eigenvalues. This special structure allows for a significant computational savings when performing tridiagonalization [4]. <p> [10], we use the rank-revealing tridiagonalization strategy of Bischof and Sun <ref> [4] </ref> to compute the invariant subspaces of C 1 . The rank-revealing tridiagonalization technique exploits both symmetry and the special structure of matrices having only two eigenvalues. This special structure allows for a significant computational savings when performing tridiagonalization [4]. Since the converged matrix resulting from the polynomial iteration can have eigenvalues at 0, 1=2, and 1, we needed to modify our algorithm to ensure that only 0 and 1 eigenvalues are produced. Fortunately, we have discovered an inexpensive means 3 of detecting this situation.
Reference: [5] <author> Bischof, C. & X. Sun, </author> <title> A framework for symmetric band reduction and tridiagonalization, </title> <type> Preprint MCS-P298-0392, </type> <institution> Argonne National Laboratory (1992), </institution> <note> (also PRISM Working Note #3). </note>
Reference-contexts: In banded SYISDA, we have modified the Eigenvalue Smoothing step to perform periodic band reductions to allow the polynomial iteration to be performed with narrow banded matrices. The band reductions are performed using the successive band reduction (SBR) algorithm of Bischof and Sun <ref> [5] </ref>. In Section 3, we demonstrate that the run times for our implementation of banded SYISDA, when applied to dense matrices, are competitive with the symmetric QR algorithm. We present conclusions in Section 4. 2. <p> Therefore, after a few applications of B 1 , kEk becomes small enough so that the growth in the effective bandwidth slows significantly. An analogous result holds for B 3 . The band reductions are performed using successive band reduction <ref> [5, 3, 6] </ref>. Instead of reducing a matrix from banded form directly to narrow band, SBR reduces a banded matrix to some intermediate band using a blocked algorithm and then reduces this intermediate matrix to the required band. The motivation for SBR is two-fold. <p> The motivation for SBR is two-fold. First, we wanted to take advantage of the fact that the reduction of a narrow banded matrix requires significantly less work than reduction of a dense matrix. Second, SBR allows for effective blocking in both stages <ref> [5, 6] </ref>, unlike previous algorithms that allow only limited blocking. In our current implementation, the first stage is blocked, but the second stage utilizes an improved, but as yet unblocked, version of Lang's algorithm [9, 6]. We hope to incorporate the block Householder approach of Bischof and Sun [6] soon.
Reference: [6] <author> Bischof, C. H. & X. Sun, </author> <title> Parallel tridiagonalization through two-step band reduction, </title> <booktitle> Proceedings: Scalable High Performance Computing Conference '94, </booktitle> <address> Knoxville, Tennessee, May 1994, </address> <publisher> 5 IEEE Computer Society Press, </publisher> <year> 1994, </year> <note> (also PRISM Working Note #17) (to appear). </note>
Reference-contexts: Therefore, after a few applications of B 1 , kEk becomes small enough so that the growth in the effective bandwidth slows significantly. An analogous result holds for B 3 . The band reductions are performed using successive band reduction <ref> [5, 3, 6] </ref>. Instead of reducing a matrix from banded form directly to narrow band, SBR reduces a banded matrix to some intermediate band using a blocked algorithm and then reduces this intermediate matrix to the required band. The motivation for SBR is two-fold. <p> The motivation for SBR is two-fold. First, we wanted to take advantage of the fact that the reduction of a narrow banded matrix requires significantly less work than reduction of a dense matrix. Second, SBR allows for effective blocking in both stages <ref> [5, 6] </ref>, unlike previous algorithms that allow only limited blocking. In our current implementation, the first stage is blocked, but the second stage utilizes an improved, but as yet unblocked, version of Lang's algorithm [9, 6]. We hope to incorporate the block Householder approach of Bischof and Sun [6] soon. <p> Second, SBR allows for effective blocking in both stages [5, 6], unlike previous algorithms that allow only limited blocking. In our current implementation, the first stage is blocked, but the second stage utilizes an improved, but as yet unblocked, version of Lang's algorithm <ref> [9, 6] </ref>. We hope to incorporate the block Householder approach of Bischof and Sun [6] soon. In order to minimize the time spent performing updates with the orthogonal matrices resulting from the band reductions performed, we accumulate these matrices and apply the resulting matrix where needed. <p> In our current implementation, the first stage is blocked, but the second stage utilizes an improved, but as yet unblocked, version of Lang's algorithm [9, 6]. We hope to incorporate the block Householder approach of Bischof and Sun <ref> [6] </ref> soon. In order to minimize the time spent performing updates with the orthogonal matrices resulting from the band reductions performed, we accumulate these matrices and apply the resulting matrix where needed.
Reference: [7] <author> Huo, Y. & R. Schreiber, </author> <title> Efficient, massively parallel eigenvalue computation, </title> <type> RIACS Technical Report 93.02, </type> <institution> Research Institute for Advanced Computer Science (1993). </institution>
Reference-contexts: 1. Introduction Computation of eigenvalues and eigenvectors is an essential kernel in many applications, and several promising parallel algorithms have been investigated <ref> [8, 11, 7] </ref>. The work presented in this paper is part of the PRISM (Parallel Research on Invariant Subspace Methods) Project, which involves researchers from Argonne National Laboratory, the Supercomputing Research Center, the University of California at Berkeley, and the University of Kentucky.
Reference: [8] <author> Ipsen, I. & E. Jessup, </author> <title> Solving the symmetric tridiagonal eigenvalue problem on the hypercube, </title> <type> Tech. Rep. </type> <institution> RR-548, Yale University (1987). </institution>
Reference-contexts: 1. Introduction Computation of eigenvalues and eigenvectors is an essential kernel in many applications, and several promising parallel algorithms have been investigated <ref> [8, 11, 7] </ref>. The work presented in this paper is part of the PRISM (Parallel Research on Invariant Subspace Methods) Project, which involves researchers from Argonne National Laboratory, the Supercomputing Research Center, the University of California at Berkeley, and the University of Kentucky.
Reference: [9] <author> Lang, B., </author> <title> A parallel algorithm for reducing symmetric banded matrices to tridiagonal form, </title> <journal> SIAM J. Sci. Stat. Comp. </journal> <volume> 14 (1993), no. </volume> <pages> 6. </pages>
Reference-contexts: Second, SBR allows for effective blocking in both stages [5, 6], unlike previous algorithms that allow only limited blocking. In our current implementation, the first stage is blocked, but the second stage utilizes an improved, but as yet unblocked, version of Lang's algorithm <ref> [9, 6] </ref>. We hope to incorporate the block Householder approach of Bischof and Sun [6] soon. In order to minimize the time spent performing updates with the orthogonal matrices resulting from the band reductions performed, we accumulate these matrices and apply the resulting matrix where needed.
Reference: [10] <author> Huss-Lederman, S., A. Tsao, & T. Turnbull, </author> <title> A parallelizable eigensolver for real diagonalizable matrices with real eigenvalues, </title> <type> Technical Report TR-91-042, </type> <institution> Supercomputing Research Center (1991). </institution>
Reference-contexts: In order to minimize the time spent performing updates with the orthogonal matrices resulting from the band reductions performed, we accumulate these matrices and apply the resulting matrix where needed. Instead of using QR factorization with column pivoting as in <ref> [10] </ref>, we use the rank-revealing tridiagonalization strategy of Bischof and Sun [4] to compute the invariant subspaces of C 1 . The rank-revealing tridiagonalization technique exploits both symmetry and the special structure of matrices having only two eigenvalues.
Reference: [11] <author> Li, T.-Y., H. Zhang, & X.-H. Sun, </author> <title> Parallel homotopy algorithm for symmetric tridiagonal eigenvalue problems, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 12 (1991), no. 3, </volume> <pages> 469-87. </pages>
Reference-contexts: 1. Introduction Computation of eigenvalues and eigenvectors is an essential kernel in many applications, and several promising parallel algorithms have been investigated <ref> [8, 11, 7] </ref>. The work presented in this paper is part of the PRISM (Parallel Research on Invariant Subspace Methods) Project, which involves researchers from Argonne National Laboratory, the Supercomputing Research Center, the University of California at Berkeley, and the University of Kentucky.
Reference: [12] <author> Tsao, A. & T. Turnbull, </author> <title> A comparison of algorithms for banded matrix multiplication, </title> <type> Technical Report SRC-TR-093-092, </type> <institution> Supercomputing Research Center (1993), </institution> <note> (also PRISM Working Note #6). </note>
Reference-contexts: Since a substantial number of these multiplications are required for each divide, we were interested in reducing the cost of this step. Reduction of the original matrix to narrow band would allow multiplication of banded rather than dense matrices. As seen in <ref> [12] </ref>, banded matrix multiplication can be performed quite efficiently through the use of specialized routines. Rapid band growth normally occurs when multiplying random matrices together. <p> Multiplication of two nfin matrices of bandwidths b 1 and b 2 results in a matrix of bandwidth b 1 +b 2 , requiring only O (b 1 b 2 n) work versus O (n 3 ) for two dense matrices. Based on our studies of banded matrix multiplication <ref> [12] </ref>, we decided to use the Madsen-Rodrigue-Karush (MRK) algorithm to perform the required matrix multiplications. Although blocked algorithms for banded matrix multiplication are about twice as fast as the MRK algorithm [12], we found that it is difficult to realize an actual time savings through the use of these algorithms in <p> Based on our studies of banded matrix multiplication <ref> [12] </ref>, we decided to use the Madsen-Rodrigue-Karush (MRK) algorithm to perform the required matrix multiplications. Although blocked algorithms for banded matrix multiplication are about twice as fast as the MRK algorithm [12], we found that it is difficult to realize an actual time savings through the use of these algorithms in the overall context of banded SYISDA. First, the optimal block size varies with problem size.
References-found: 12

