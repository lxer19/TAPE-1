URL: http://gn.www.media.mit.edu/groups/gn/publications/embodied2.ps
Refering-URL: http://gn.www.media.mit.edu/groups/gn/publications.html
Root-URL: http://www.media.mit.edu
Title: Embodied Conversation: Integrating Face and Gesture into Automatic Spoken Dialogue Systems  
Author: Justine Cassell 
Affiliation: MIT Media Laboratory  
Abstract-found: 0
Intro-found: 1
Reference: <author> Andr, E., Rist, T. and Mueller, J. </author> <title> (forthcoming). Employing AI methods to control the behavior of animated interface agents. </title> <journal> Applied Artificial Intelligence. </journal>
Reference: <author> Badler, N., Phillips, C. and Webber, B. </author> <year> (1993). </year> <title> Simulating Humans: Computer Graphics Animation and Control. </title> <publisher> Oxford: Oxford University Press. </publisher>
Reference-contexts: At that point in time we were not ready to address the problems of understanding human multimodal behavior, and so we built two embodied dialogue systems that could converse with one another, using Badlers work on human figure animation <ref> (Badler et al., 1993) </ref> as the body.
Reference: <author> Ball, G., Ling, D., Kurlander, D., Miller, D., Pugh, D., Skelly, T., Stankosky, A., Thiel, D., Van Dantzich, M. and T. </author> <title> Wax (1997). Lifelike computer characters: the persona project at Microsoft Research. </title> <editor> In J. M. Bradshaw (ed.) </editor> <booktitle> Software Agents, </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Beattie, G. W. </author> <year> (1981). </year> <title> Sequential temporal patterns of speech and gaze in dialogue. In T.A. </title>
Reference: <editor> Sebeok and J. Umiker-Sebeok (eds.), </editor> <title> Nonverbal Communication, Interaction, and Gesture: Selections from Semiotica. The Hague: </title> <publisher> Mouton. </publisher>
Reference: <author> Bavelas, J. Chovil, N., Lawrie, D. and Wade, A. </author> <year> (1992). </year> <title> Interactive Gestures. </title> <booktitle> Discourse Processes, </booktitle> <volume> 15 </volume> <pages> 469-489. </pages>
Reference-contexts: Beat gestures may also serve to maintain conversation as dyadic: to check on the attention of the listener, and to ensure that the listener is following <ref> (Bavelas et al., 1992) </ref>. The importance of these four types of gestures is that (a) they convey content that is not conveyed by speech, (b) their placement tells us something about the state of the conversation, (c) their placement tells us something about the structure of the discourse.
Reference: <author> Beskow, J. and McGlashan, S. </author> <year> (1997). </year> <title> Olgaa conversational agent with gestures. </title> <booktitle> Proceedings of Workshop on Animated Interface Agents. IJCAI-97 (August, </booktitle> <address> Nagoya, Japan). </address>
Reference: <author> Bolt, R.A. </author> <year> (1980). </year> <title> Putthat-there: voice and gesture at the graphics interface. </title> <journal> Computer Graphics, </journal> <volume> 14(3): </volume> <pages> 262-270. </pages>
Reference-contexts: These gestures can be particularly important in certain types of task-oriented talk, as discussed in the well-known paper PutThat-There: Voice and Gesture at the Graphics Interface <ref> (Bolt, 1980) </ref>. Gestures such as these are found notably in communicative situations where the physical world in which the conversation is taking place is also the topic of conversation.
Reference: <author> Bregler, C., Hild, H., Manke, S., and Waibel, A. </author> <year> (1993). </year> <title> Improving connected letter recognition by lipreading. </title> <booktitle> Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (IEEE-ICASSP) (Minneapolis, </booktitle> <address> MN). </address>
Reference-contexts: Some facial displays have a phonological functionlip shapes that change with the phonemes uttered. Although it has been shown that such lip shapes can significantly improve the facility with which people understand talking heads <ref> (Bregler et al, 1993) </ref>, this function of facial displays will not be further discussed in the current chapter. Some facial displays fulfill a semantic function, for example nodding rather than saying yes. Some facial displays convey transitory emotional states (emotional feedback to the conversation).
Reference: <author> Brennan, S. and Hulteen, E. </author> <year> (1995). </year> <title> Interaction and feedback in a spoken language system: a theoretical framework. </title> <booktitle> Knowledge-Based Systems, </booktitle> <pages> 8(2-3). </pages>
Reference: <author> Cassell, J. and Prevost, S. </author> <title> (in preparation). Embodied natural language generation: a framework for generating speech and gesture. </title>
Reference: <author> Cassell, J. and Thrisson, K. </author> <title> (forthcoming). The power of a nod and a glance: envelope vs. emotional feedback in animated conversational agents. </title> <journal> Applied Artificial Intelligence. </journal>
Reference: <author> Cassell, J., McNeill, D. and McCullough, K.E. </author> <year> (1998). </year> <title> Speech-gesture mismatches: evidence for one underlying representation of linguistic and nonlinguistic information. </title> <journal> Pragmatics and Cognition., </journal> <volume> 6(2). </volume>
Reference-contexts: People even gesture while they are speaking on the telephone (Rim, 1982). We know that listeners attend to such gestures in faceto-face conversation, and that they use gesture in these situations to form a mental representation of the communicative intent of the speaker <ref> (Cassell et al, 1998) </ref>. Likewise, faces change expressions continuously, and many of these changes are synchronized to what is going on in concurrent conversation. <p> In the same way, we appear to lose access to 8 the form of gestures (Krauss, Morrel-Samuels & Colasante, 1991), even though we attend to the information that they convey <ref> (Cassell et al, 1998) </ref>. The spontaneous unplanned, more common coverbal gestures are of four types: Iconic gestures depict by the form of the gesture some feature of the action or event being described. <p> or task-oriented 8 For a more complete treatment of the role of body orientation and other whole body behaviors in conversation, see our BodyChat system, a 3D-graphical world in which the conversational body behaviors of avatars are automatically generated as a function of stage of the conversation, social distance, etc. <ref> (Vilhjlmsson & Cassell, 1998) </ref>. Reactive Layer Behavior Requests Behavior Requests Motor Feedback Blackboard Content Blackboard Functional Blackboard Content Layer Process Control Layer Action Scheduler 37 activity. <p> It still remains to test the function of gestures in embodied dialogue systems. The research mentioned above <ref> (Cassell et al., 1998) </ref> shows that users do take gesture into account when constructing a representation of the content of a monologue, and that the information that they received only in the gestural channel is just as likely to be re-narrated in speech.
Reference: <author> Cassell, J., Pelachaud, C., Badler, N.I., Steedman, M., Achorn, B., Beckett, T., Douville, B., Prevost, S. and Stone, M. </author> <year> (1994). </year> <title> Animated conversation: rule-based generation of facial display, gesture and spoken intonation for multiple conversational agents. </title> <booktitle> Computer Graphics (SIGGRAPH proceedings). </booktitle> <volume> 28(4): </volume> <pages> 413-420. </pages>
Reference-contexts: The first example is taken from life (another one of my colleagues) while the second <ref> (from Cassell et al., 1994) </ref> is, in fact, a conversation that took place 23 between two computer-generated animated agents, and in which all verbal and nonverbal behaviors were generated automatically, by rule.
Reference: <author> Cassell, J., Torres, O. and S. </author> <title> Prevost (in press). Turn taking vs. Discourse structure: how best to model multimodal conversation. </title> <editor> In Wilks (ed.), </editor> <title> Machine Conversations. The Hague: </title> <type> Kluwer. 51 Condon, W.S. </type> <institution> and Osgton, W.D. </institution> <year> (1971). </year> <title> Speech and body motion synchrony of the speaker-hearer. </title> <editor> In D.H. Horton and J.J. Jenkins (eds.), </editor> <booktitle> The Perception of Language, </booktitle> <pages> pp. 150-184. </pages> <publisher> Academic Press. </publisher>
Reference: <author> Dittman, A.T. </author> <year> (1974). </year> <title> The body movementspeech rhythm relationship as a cue to speech encoding. </title> <editor> In S. Weitz (Ed.), </editor> <booktitle> Nonverbal Communication. </booktitle> <address> New York: </address> <publisher> Oxford University Press. </publisher>
Reference-contexts: Moreover, some movements reflect encoding-decoding difficulties and therefore coincide with hesitations and pauses inside clauses <ref> (Dittman, 1974) </ref>. Many hesitation pauses are produced at the beginning of speech, and correlate with avoidance of gaze, presumably to help the speaker concentrate on what s/he is going to say.
Reference: <author> Duncan, S. </author> <year> (1974). </year> <title> Some signals and rules for taking speaking turns in conversations. </title> <editor> In S. Weitz (ed.), </editor> <booktitle> Nonverbal Communication. </booktitle> <address> New York: </address> <publisher> Oxford University Press. </publisher>
Reference-contexts: Thus, the occurrence of the word bank, with its accompanying gesture, affected the occurrence of the gesture that accompanied account. At the level of the turn, the hands being in motion is one of the most robust cues to turn-taking <ref> (Duncan, 1974) </ref>. Speakers bring their hands into gesture space as they think about taking the turn, and at the end of a turn the hands of the speaker come to rest, before the next speaker begins to talk. <p> distinction between content and envelope is made by Takeuchi & Nagao (1993) when they refer to the difference between "object-level communication" (relevant to the communication goal) and "meta-level processing" (relevant to communication regulation). 5 Content nods tend to be fewer and produced more emphatically and more slowly than envelope nods <ref> (Duncan, 1974) </ref>. 19 Cultural Differences Note that, like emblem gestures, facial displays with a semantic function can vary from culture to culture. To indicate agreement, for example, one nods in the United States but shakes ones head in Greece or Albania. <p> But the listeners behavior is dependent on the behavior of the speaker; one is much less likely to find feedback in the absence of a feedback elicitation signal <ref> (Duncan, 1974) </ref>. In the description just given, facial behavior is described as a function of the turn-taking structure of a conversation rather than as a function of information structure. <p> For example, gaze is an indicator to the participants of a conversation of who should speak when: when the current speaker looks at the listener and pauses, this serves as a signal that the speaker is giving up the turn <ref> (Duncan, 1974) </ref>. On the other hand, some communicative behavior controls the propositional content of communication. For example, the content of speech, and the content of iconic gestures determine the direction that the conversation is taking.
Reference: <author> Efron, D. </author> <year> (1941). </year> <editor> Gesture and Environment.. </editor> <address> New York: </address> <publisher> Kings Crown Press. </publisher>
Reference: <author> Ekman, P. </author> <year> (1979). </year> <title> About brows: emotional and conversational signals. </title> <editor> In M. von Cranach, </editor> <publisher> K. </publisher>
Reference-contexts: Facial displays are linked to the content of speech (winking when teasing somebody), emotion (wrinkling ones eyebrows with worry), personality (pouting all the time), and other behavioral variables. Facial displays can replace sequences of words (she was dressed [wrinkle nose, stick out tongue]) as well as accompany them <ref> (Ekman, 1979) </ref>, and they can serve to help disambiguate what is being said when the acoustic signal is degraded. They do not occur randomly but rather are synchronized to ones own speech, or to the speech of others (Condon & Osgton, 1971; Kendon, 1972). <p> In this literature, emotional feedback has meant emotional emblems, facial displays that reference a particular emotion without requiring 41 the person showing the expression to feel that emotion at the moment of expression <ref> (Ekman, 1979) </ref>. In the literature on anthropomorphism in interface systems, emotional feedback as displayed by the animated agents emotional emblems in response to a users input is held to be a feature that an embodied agent-based interface couldand shouldadd to human-computer interaction (cf.
Reference: <editor> Foppa, W. Lepenies, and D. Ploog (eds.), </editor> <title> Human Ethology: Claims and Limits of a New Discipline, </title> <journal> pp. </journal> <pages> 169-249. </pages> <address> New York: </address> <publisher> Cambridge University Press. </publisher>
Reference: <author> Ekman, P. and Friesen, W.V. </author> <year> (1984). </year> <title> Unmasking the Face. </title> <address> Palo Alto: </address> <publisher> Consulting Psychologists Press. </publisher>
Reference-contexts: When we talk about facial displays we are really most interested in precisely-timed changes in eyebrow position, expressions of the mouth, movement of the head and eyes, and gestures of the hands. For example, raised eyebrows + a smiling mouth, is taken to be a happy expression <ref> (Ekman & Friesen, 1984) </ref>, while moving one's head up and down is taken to be a nod.
Reference: <author> Ekman, P. and Friesen, W. </author> <year> (1969). </year> <title> The repertoire of nonverbal behavioral categoriesorigins, usage, and coding. </title> <journal> Semiotica, </journal> <volume> 1: </volume> <pages> 49-98. </pages>
Reference: <author> Elhadad, M., McKeown, K. and Robin, J. </author> <title> (to appear). Floating constraints in lexical choice. </title> <note> Computational Linguistics. </note>
Reference: <author> Elliott, C. </author> <year> 1997. </year> <title> I picked up Catapia and other stories: a mulitmodal approach to expressivity for emotionally intelligent agents. </title> <booktitle> Proceedings of the First International Conference on Autonomous Agents, </booktitle> <pages> pp. 451-457, </pages> <month> February 5-8, </month> <institution> Marina del Rey, California, USA. </institution>
Reference: <author> Hajicova, E. and Sgall, P. </author> <year> (1987). </year> <title> The ordering principle. </title> <journal> Journal of Pragmatics 11: </journal> <pages> 435-454. </pages>
Reference: <author> Halliday, M. </author> <year> (1967). </year> <title> Intonation and Grammar in British English. The Hague: </title> <publisher> Mouton. </publisher>
Reference: <author> Hinrichs, E. and Polanyi, L. </author> <year> (1986). </year> <title> Pointing the way: a unified treatment of referential gesture in interactive contexts. </title> <editor> In A. Farley, P. Farley and K.E. McCullough (eds.), </editor> <booktitle> Proceedings of the Parasession of the Chicago Linguistics Society Annual Meetings (Pragmatics and Grammatical Theory). </booktitle> <address> Chicago: </address> <publisher> Chicago Linguistics Society. </publisher>
Reference-contexts: Propositional Gestures Another conscious gesture that has been the subject of some study in the interface community is the socalled 'propositional gesture' <ref> (Hinrichs & Polanyi, 1986) </ref>. An example is the use of the hands to measure the size of a symbolic space while the speaker says "it was this big". Another example 7 is pointing at a chair and then pointing at another spot and saying move that over there.
Reference: <author> Hirschberg, J. </author> <title> (this volume) Intonation in Spoken Dialogue. </title>
Reference: <author> Iverson, J. and Goldin-Meadow, S. </author> <year> (1996). </year> <title> Gestures in blind children. </title> <type> Manuscript, </type> <institution> Department of Psychology, University of Chicago. </institution>
Reference-contexts: Even among the blind, semantic features are distributed across speech and gesturestrong evidence that gesture is a product of the same generative process that produces speech. Children who have been blind from birth and have never experienced the communicative value of gestures do produce gestures along with their speech <ref> (Iverson & Goldin-Meadow, 1996) </ref>. The blind perform gestures during problem solving tasks, such as the Piagetian conservation task.
Reference: <author> Johnson, M., Bransford, J., Solomon, S. </author> <year> (1973). </year> <title> Memory for tacit implications of sentences. </title> <journal> Journal of Experimental Psychology, </journal> <volume> 98(1): </volume> <pages> 203-205. </pages>
Reference-contexts: The fact that we lose access to the form of a whole class of gestures may seem odd, but consider the analogous situation with speech. For the most part, in most situations, we lose access to the surface structure of utterances immediately after hearing or producing them <ref> (Johnson et al., 1973) </ref>. That is, if listeners are asked whether they heard the word couch or the word sofa to refer to the same piece of furniture, unless one of these words sounds odd to them, they probably will not be able to remember which they heard.
Reference: <author> Johnston, M., Cohen, P. R., McGee, D., Pittman, J., Oviatt, S. L., and Smith, I. </author> <year> (1997). </year> <title> Unification-based multimodal integration. </title> <booktitle> Proceedings of the Annual Meeting of the Association for Computational Linguistics. </booktitle> <address> (ACL-97/EACL-97), July, Madrid, Spain. </address>
Reference-contexts: Such managers have been described for multimodal integration for generation of 7 For more details on the facial animation, see Pelachaud et al., 1994 34 text and graphics (Wahlster et al., 1991), and multimodal integration in input <ref> (Johnston et al., 1997) </ref>. Second, we lacked an understanding of what shape a particular gesture would take: how did we describe which particular gesture would be generated? This is similar to the problem of word choice in text generation (Elhadad et al., to appear).
Reference: <author> Kendon, A. </author> <year> (1993). </year> <title> Gestures as illocutionary and discourse structure markers in southern Italian conversation. </title> <booktitle> Proceedings of the Linguistic Society of America Symposium on Gesture in the Context of Talk. </booktitle> <volume> 52 Kendon, </volume> <editor> A. </editor> <year> (1980). </year> <title> Gesticulation and speech: two aspects of the process. In M.R. Key (ed.), The Relation Between Verbal and Nonverbal Communication. The Hague: </title> <publisher> Mouton. </publisher>
Reference-contexts: Examples of emblems in American culture are the thumb-and-index-finger ring gesture that signals 'okay' or the 'thumbs up' gesture. Many more of these "emblems" appear to exist in French and Italian culture than in America <ref> (Kendon, 1993) </ref>, but in few cultures do these gestures appear to constitute more than 10% of the gestures produced by speakers. Despite the paucity of emblematic gestures in everyday communication, it is gestures such as these that have interested computer scientists.
Reference: <author> Kendon, A. </author> <year> (1974). </year> <title> Movement coordination in social interaction: some examples described. </title> <editor> In S. Weitz (ed.), </editor> <booktitle> Nonverbal Communication. </booktitle> <address> New York: </address> <publisher> Oxford University Press. </publisher>
Reference: <author> Kendon, A. </author> <year> (1972). </year> <title> Some relationships between body motion and speech. </title> <editor> In A.W. Siegman and B. Pope (eds.), </editor> <title> Studies in Dyadic Communication. </title> <address> New York: </address> <publisher> Pergamon Press. </publisher>
Reference: <author> Koda, T. and Maes, P. </author> <year> (1996). </year> <title> Agents with faces: the effects of personification of agents. Proceedings of Human-Computer Interaction 96, August, </title> <address> London, UK. </address>
Reference: <author> Krauss, R., Morrel-Samuels, P. and Colasante, C. </author> <year> (1991). </year> <title> Do conversational hand gestures communicate? Journal of Personality and Social Psychology, </title> <booktitle> 61(5): </booktitle> <pages> 743-754. </pages>
Reference-contexts: That is because (so it is hypothesized), we listen to speech in order to extract meaning, and we throw away the words once the meaning has been extracted. In the same way, we appear to lose access to 8 the form of gestures <ref> (Krauss, Morrel-Samuels & Colasante, 1991) </ref>, even though we attend to the information that they convey (Cassell et al, 1998). The spontaneous unplanned, more common coverbal gestures are of four types: Iconic gestures depict by the form of the gesture some feature of the action or event being described.
Reference: <author> Lester, J.C., Voerman, J.L., Towns S.G., and Callaway, C.B. </author> <title> (forthcoming) Deictic Believability: Coordinating Gesture, Locomotion, and Speech in Lifelike Pedagogical Agents. </title> <journal> Applied Artificial Intelligence. </journal>
Reference: <author> Levelt, W. </author> <year> (1989). </year> <title> Speaking: From Intention to Articulation. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Likewise, slight variations in pronunciation of the speech we are listening to are difficult to remember, even right after hearing them <ref> (Levelt, 1989) </ref>. That is because (so it is hypothesized), we listen to speech in order to extract meaning, and we throw away the words once the meaning has been extracted.
Reference: <author> Liberman, M. and Pierrehumbert, J. </author> <year> (1984). </year> <title> Intonational invariance under changes in pitch range and length. </title> <editor> In M. Aronoff and R.T. Oehrle (eds.), </editor> <title> Language Sound Structure: Studies in Phonology Presented to Morris Halle by His Teacher and Students, </title> <journal> pp. </journal> <pages> 157-233. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Unimportant information is never accented or assigned a separate intonational contour. The result is English text annotated with intonational cues. This text is converted automatically to a form suitable for input to the AT&T Bell Laboratories TTS synthesizer <ref> (Liberman & Pierrehumbert, 1984) </ref>. The resulting speech and timing information is then critical for synchronizing the facial and gestural animation. Symbolic Gesture Specification This discourse and intonation infrastructure allows us to generate types of gestures, and placement of gestures as follows.
Reference: <author> Loyall, A. and Bates, J. </author> <year> (1997). </year> <title> Personality-rich believable agents that use language. </title> <booktitle> Proceedings of Agents 97. </booktitle> <address> Marina del Rey, CA. </address>
Reference: <author> McNeill, D. </author> <title> (forthcoming) Models of speaking (to their amazement) meet speech-synchronized gestures. </title> <editor> In D. McNeill (ed.) </editor> <title> Language and Gesture: Window into Thought and Action. </title> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> McNeill, D. </author> <year> (1992). </year> <title> Hand and Mind: What Gestures Reveal about Thought. </title> <publisher> Chicago: University of Chicago Press. </publisher>
Reference-contexts: Interestingly, and perhaps not surprisingly, the form of metaphoric gestures appears to differ from language community to language community. Conduit metaphoric gestures are not found in narrations in all languages: neither Chinese nor Swahili narrators use them, for example <ref> (McNeill, 1992) </ref>. These narratives do contain abundant metaphoric gestures of other kinds, but do not depict abstract ideas as bounded containers. The metaphoric use of space, however, appears in all narratives collected, regardless of the language spoken.
Reference: <author> Nagao, K. and Takeuchi, A. </author> <year> (1994). </year> <title> Social interaction: multimodal conversation with social agents. </title> <booktitle> Proceedings of the 12th National Conference on Artificial Intelligence (AAAI-94), </booktitle> <volume> vol. 1, </volume> <pages> pp. 22-28. (August 1-4, </pages> <address> Seattle, Washington, USA). </address>
Reference: <author> Noma, T. and Badler, N. </author> <year> (1997). </year> <title> A virtual human presenter. </title> <booktitle> Proceedings of Workshop on Animated Interface Agents. IJCAI-97 (August, </booktitle> <address> Nagoya, Japan). </address>
Reference: <author> Oviatt, S. L. </author> <year> (1995). </year> <title> Predicting spoken disfluencies during human-computer interaction. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 9(1): </volume> <pages> 19-35. </pages>
Reference-contexts: Users repeat themselves needlessly, mistake when it is their turn to speak, and otherwise behave in ways that make dialogue systems less likely to function well <ref> (Oviatt, 1995) </ref>. It is in situations just like these in life that the nonverbal modalities come in to play: in noisy situations, humans depend on access to more than one modality (Rogers, 1978).
Reference: <author> Pelachaud, C., Badler, N. and Steedman, M. </author> <year> (1996). </year> <title> Generating facial expressions for speech. </title> <journal> Cognitive Science, </journal> <volume> 20(1): </volume> <pages> 1-46. </pages>
Reference: <author> Perlin, K. and Goldberg, A. </author> <year> (1996). </year> <title> Improv: a system for interactive actors in virtual worlds. </title> <booktitle> Proceedings of SIGGRAPH 96, </booktitle> <pages> pp. 205-216. </pages> <address> (New Orleans, LA, USA). </address>
Reference: <author> Power, R. </author> <year> (1977). </year> <title> The organisation of purposeful dialogues. </title> <journal> Linguistics, </journal> <volume> 17: </volume> <pages> 107-152. </pages> <note> 53 Prevost, </note> <author> S. </author> <year> (1996). </year> <title> An information structural approach to monologue generation. </title> <booktitle> Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (June, </booktitle> <address> Santa Cruz, California, USA). </address>
Reference: <author> Prevost, S. and Steedman, M. </author> <year> (1994). </year> <title> Specifying intonation from context for speech synthesis. </title> <journal> Speech Communication, </journal> <volume> 15: </volume> <pages> 139-153. </pages>
Reference: <author> Reeves, B. and Nass, C. </author> <year> (1996). </year> <title> The Media Equation: How People Treat Computers, Television and New Media Like Real People and Places . Cambridge: </title> <publisher> Cambridge University Press. </publisher>
Reference-contexts: While humans have long years of practicing communication with other humans (some might even say this ability is innate; Trevarthen, 1986), communication with machines is learned. And yet, it has been shown that given the slightest chance, humans will attribute social responses, behaviors, and internal states to computers <ref> (Reeves & Nass, 1996) </ref>. 3 If we can skillfully build on that social response to computers, channel it even into the kind of response that we give one another in human conversation, and build a system that gives back the response (verbal and nonverbal) that humans give, then we may evoke
Reference: <author> Rim, B. </author> <year> (1982). </year> <title> The elimination of visible behavior from social interactions: effects of verbal, nonverbal and interpersonal variables. </title> <journal> European Journal of Social Psychology, </journal> <volume> 12: </volume> <pages> 113-129. </pages>
Reference-contexts: People even gesture while they are speaking on the telephone <ref> (Rim, 1982) </ref>. We know that listeners attend to such gestures in faceto-face conversation, and that they use gesture in these situations to form a mental representation of the communicative intent of the speaker (Cassell et al, 1998).
Reference: <author> Rogers, W.T. </author> <year> (1978). </year> <title> The contribution of kinesic illustrators toward the comprehension of verbal behavior within utterances. </title> <journal> Human Communication Research, </journal> <volume> 5: </volume> <pages> 54-62. </pages>
Reference-contexts: It is in situations just like these in life that the nonverbal modalities come in to play: in noisy situations, humans depend on access to more than one modality <ref> (Rogers, 1978) </ref>. This leads us to the third reason we might wish to add the nonverbal modalities to dialogue systems. While humans have long years of practicing communication with other humans (some might even say this ability is innate; Trevarthen, 1986), communication with machines is learned.
Reference: <author> Scherer, K.R. </author> <year> (1980). </year> <title> The functions of nonverbal signs in conversation. </title> <editor> In and R.N. St. Clair and H. Giles (eds.), </editor> <booktitle> The Social and Psychological Contexts of Language, </booktitle> <pages> pp. 225-243. </pages>
Reference-contexts: Kinds of Facial Displays Let us turn now to the use of the face during conversation. Like hand gestures, facial displays can be classified according to their placement with respect to the linguistic utterance and their significance in transmitting information <ref> (Scherer, 1980) </ref>. When we talk about facial displays we are really most interested in precisely-timed changes in eyebrow position, expressions of the mouth, movement of the head and eyes, and gestures of the hands.
Reference: <editor> Hillsdale, </editor> <address> N.J.: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Scoble, J. </author> <year> (1993). </year> <title> Stuttering blocks the flow of speech and gesture: the speech-gesture relationship in chronic stutters. M.A. </title> <type> thesis, </type> <institution> McGill University. </institution>
Reference-contexts: Even clinical stuttering, despite massive disruptions of the flow of speech, does not interrupt speech-gesture synchrony. Gestures during stuttering bouts freeze into holds until the bout is over, 12 and then speech and gesture resume in synchrony <ref> (Scoble 1993) </ref>. In each of these cases, the linkage of gesture and language strongly resists interruption. Semantic Integration Speech and the nonverbal behaviors that accompany it are sometimes redundant, and sometimes they present complementary but nonoverlapping information. This complementarity can be seen at several levels.
Reference: <author> Steedman, M. </author> <title> (1991) Structure and intonation. </title> <booktitle> Language: </booktitle> <volume> 67(2): </volume> <pages> 260-296. </pages>
Reference: <author> Takeuchi, A. and Nagao, K. </author> <year> (1993). </year> <title> Communicative facial displays as a new conversational modality. </title> <booktitle> Proceedings of InterCHI, </booktitle> <pages> pp. </pages> <address> 187-193 (April, Amsterdam, Netherlands). </address>
Reference: <author> Thrisson, K. R. </author> <title> (forthcoming). A mind model of multimodal communicative creatures and humanoids. </title> <journal> Applied Artificial Intelligence. </journal>
Reference: <author> Thrisson, K. R. </author> <year> (1994). </year> <title> Faceto-face communication with computer agents. </title> <booktitle> AAAI Spring Symposium on Believable Agents Working Notes, </booktitle> <pages> pp. 86-90 (March 19-20, </pages> <institution> Stanford University, California). </institution>
Reference-contexts: Gandalf was built within Ymir, a testbed system especially designed for prototyping multimodal agents that understand human communicative behavior, and generate integrated spontaneous verbal and nonverbal behavior of their own <ref> (see Thrisson, 1994, forthcoming for more details about the system) </ref>. Ymir is constructed as a layered system. It provides a foundation for accommodating any number of interpretive processes, running in parallel, working in concert to interpret and respond to the user's behavior.
Reference: <author> Trevarthen, C. </author> <year> (1986). </year> <title> Sharing makes sense: intersubjectivity and the making of an infant's meaning. </title> <editor> In R. Steele and T. Threadgold (eds.), </editor> <booktitle> Language Topics: Essays in Honour of M. Halliday, </booktitle> <volume> vol. 1, </volume> <pages> pp. 177-200. </pages> <address> Amsterdam: </address> <publisher> J. Benjamins. </publisher>
Reference: <author> Vilhjalmsson, H. and Cassell, J. </author> <year> (1998). </year> <title> BodyChat: autonomous communicative behaviors in avatars. </title> <booktitle> Proceedings of ACM International Conference on Autonomous Agents. </booktitle>
Reference: <author> Wahlster, W., Andr, E., Graf, W. and Rist, T. </author> <year> (1991). </year> <title> Designing illustrated texts. </title> <booktitle> In Proceedings of the 5th EACL: </booktitle> <pages> 8-14. </pages>
Reference-contexts: Such managers have been described for multimodal integration for generation of 7 For more details on the facial animation, see Pelachaud et al., 1994 34 text and graphics <ref> (Wahlster et al., 1991) </ref>, and multimodal integration in input (Johnston et al., 1997).
Reference: <author> Whittaker, S. and OConaill, B. </author> <year> (1997). </year> <title> The role of vision in faceto-face and mediated communication. </title> <editor> In K.E. Finn, A.J. Sellen, and S.B. Wilbur (eds.), </editor> <booktitle> Video-Mediated Communication , pp. </booktitle> <pages> 23-49. </pages> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Wilson, A., Bobick, A. and Cassell, J. </author> <year> (1996). </year> <title> Recovering the temporal structure of natural gesture. </title> <booktitle> Proceedings of the Second International Conference on Automatic Face and Gesture Recognition. </booktitle>
Reference-contexts: That is, the number of phases corresponds to type of meaning: representational vs. non-representational. And it is in the second phasethe strokethat we look for the meaning features that allow us to interpret the gesture <ref> (Wilson, Bobick & Cassell, 1996) </ref>.
References-found: 64

