URL: http://www.cs.toronto.edu/~frey/papers/percepts.ps.Z
Refering-URL: http://www.cs.toronto.edu/~frey/papers/percepts.abs.html
Root-URL: http://www.cs.toronto.edu
Email: ffrey@cs.toronto.edu, dayan@ai.mit.edu, hinton@cs.toronto.edug  
Title: A simple algorithm that discovers efficient perceptual codes  
Author: Brendan J. Frey, Peter Dayan and Geoffrey E. Hinton In M. Jenkin and L. R. Harris 
Note: (editors), Computational and Biological Mech anisms of Visual Coding, Cambridge University Press,  
Address: Toronto, Ontario, M5S 1A4, Canada  New York NY, 1997.  
Affiliation: Department of Computer Science, University of Toronto  
Abstract: We describe the "wake-sleep" algorithm that allows a multilayer, unsupervised, neural network to build a hierarchy of representations of sensory input. The network has bottom-up "recognition" connections that are used to convert sensory input into underlying representations. Unlike most artificial neural networks, it also has top-down "generative" connections that can be used to reconstruct the sensory input from the representations. In the "wake" phase of the learning algorithm, the network is driven by the bottom-up recognition connections and the top-down generative connections are trained to be better at reconstructing the sensory input from the representation chosen by the recognition process. In the "sleep" phase, the network is driven top-down by the generative connections to produce a fantasized representation and a fantasized sensory input. The recognition connections are then trained to be better at recovering the fantasized representation from the fantasized sensory input. In both phases, the synaptic learning rule is simple and local. The combined effect of the two phases is to create representations of the sensory input that are efficient in the following sense: On average, it takes more bits to describe each sensory input vector directly than to first describe the representation of the sensory input chosen by the recognition process and then describe the difference between the sensory input and its reconstruction from the chosen representation. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Dayan, P., and Hinton, G. E. </author> <year> 1996. </year> <title> Varieties of Helmholtz Machine. Neural Networks, </title> <note> to appear. </note>
Reference-contexts: Although the wake-sleep algorithm performs moderately well, we believe that there is room for considerable further development in order to make it more useful in practice <ref> (Frey, Hinton and Dayan 1996) </ref> and more realistic as 20 a neural model (Dayan and Hinton 1996). The simple form of the algorithm presented here is unable to account for top-down influences during recognition (Palmer 1975) because it lacks top-down recognition weights. <p> Although the wake-sleep algorithm performs moderately well, we believe that there is room for considerable further development in order to make it more useful in practice (Frey, Hinton and Dayan 1996) and more realistic as 20 a neural model <ref> (Dayan and Hinton 1996) </ref>. The simple form of the algorithm presented here is unable to account for top-down influences during recognition (Palmer 1975) because it lacks top-down recognition weights.
Reference: <author> Dayan, P., Hinton, G. E., Neal, R. M., and Zemel, R. S. </author> <year> 1995. </year> <title> The Helmholtz Machine. </title> <booktitle> Neural Computation 7, </booktitle> <pages> 889-904. </pages>
Reference: <author> Dayan, P., and Zemel, R. S. </author> <year> 1995. </year> <title> Competition and multiple cause models. </title> <booktitle> Neural Computation 7, </booktitle> <pages> 565-579. </pages>
Reference: <author> Foldiak, P. </author> <year> 1990. </year> <title> Forming sparse representations by local anti-Hebbian learning. </title> <booktitle> Biological Cybernetics 64, </booktitle> <pages> 165-170. </pages>
Reference: <author> Frey, B. J., and Hinton, G. E. </author> <year> 1996. </year> <title> Free energy coding. </title> <booktitle> To appear in Proceedings of the Data Compression Conference 1996, </booktitle> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Although the wake-sleep algorithm performs moderately well, we believe that there is room for considerable further development in order to make it more useful in practice <ref> (Frey, Hinton and Dayan 1996) </ref> and more realistic as 20 a neural model (Dayan and Hinton 1996). The simple form of the algorithm presented here is unable to account for top-down influences during recognition (Palmer 1975) because it lacks top-down recognition weights.
Reference: <author> Frey, B. J., Hinton, G. E., and Dayan, P. </author> <year> 1996. </year> <title> Does the wake-sleep algorithm produce good density estimators? In D. </title> <editor> S. Touretzky, M. C. Mozer, and M. E. Hasselmo (editors), </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Although the wake-sleep algorithm performs moderately well, we believe that there is room for considerable further development in order to make it more useful in practice <ref> (Frey, Hinton and Dayan 1996) </ref> and more realistic as 20 a neural model (Dayan and Hinton 1996). The simple form of the algorithm presented here is unable to account for top-down influences during recognition (Palmer 1975) because it lacks top-down recognition weights.
Reference: <author> Hinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M. </author> <year> 1995. </year> <title> The wake-sleep algorithm for unsupervised neural networks. </title> <booktitle> Science 268, </booktitle> <pages> 1158-1161. </pages>
Reference: <author> Hinton, G. E., and Zemel, R. S. </author> <year> 1994. </year> <title> Autoencoders, Minimum Description Length, and Helmholtz Free Energy. </title> <editor> In J. D. Cowan, G. Tesauro, and J. Alspector (editors), </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address> <note> 21 Palmer, </note> <author> S. E. </author> <year> 1975. </year> <title> Visual perception and world knowledge: Notes on a model of sensory-cognitive interaction. </title> <editor> In D. A. Norman and D. E. Rumel-hart (editors), Explorations in Cognition, W. </editor> <publisher> H Freeman and Co., </publisher> <address> San Fran-cisco, CA. </address>
Reference: <author> Rissanen, J. </author> <year> 1989. </year> <title> Stochastic complexity in Statistical Inquiry, </title> <publisher> World Scientific, Singapore. </publisher>
Reference: <author> Saund, E. </author> <year> 1995. </year> <title> A multiple cause mixture model for unsupervised learning. </title> <booktitle> Neural Computation 7, </booktitle> <pages> 51-71. </pages>
Reference: <author> Somers, D. C., Nelson, S. B., and Sur, M. </author> <year> 1995. </year> <title> An emergent model of orientation selectivity in cat visual cortical simple cells. </title> <journal> Journal of Neuroscience 15, </journal> <pages> 5448-5465. </pages>
Reference-contexts: Nor does it explain the role of lateral connections within a cortical area which seem to be important in modelling a number of psychophysical phenomena <ref> (Somers, Nelson and Sur 1995) </ref>. The whole approach would be more plausible if the learning could be driven by the discrepancy between recognition probabilities and generative expectations during recognition. How to achieve this without making recognition tediously slow is an open issue.
References-found: 11

