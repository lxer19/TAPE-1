URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1997/tr-97-018.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1997.html
Root-URL: http://www.icsi.berkeley.edu
Title: Empirical Observations of Probabilistic Heuristics for the Clustering Problem  
Phone: (510) 643-9153 FAX (510) 643-7684  
Author: Jeff Bilmes Amin Vahdat Windsor Hsu Eun-Jin Im 
Address: I 1947 Center St. Suite 600 Berkeley, California 94704-1198  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  ICSI and CS Division, Department of EECS, U.C. Berkeley CS Division, Department of EECS, U.C. Berkeley CS Division, Department of EECS, U.C. Berkeley CS Division, Department of EECS, U.C. Berkeley  
Pubnum: TR-97-018  
Abstract: We empirically investigate a number of strategies for solving the clustering problem under the minimum variance error criterion. First, we compare the behavior of four algorithms, 1) randomized minimum spanning tree, 2) hierarchical grouping, 3) randomized maximum cut, and 4) standard k-means. We test these algorithms with a large corpus of both contrived and real-world data sets and find that standard k-means performs best. We found, however, that standard k-means can, with non-negligible probability, do a poor job optimizing the minimum variance criterion. We therefore investigate various randomized k-means modifications. We empirically find that by running randomized k-means only a modest number of times, the probability of a poor solution becomes negligible. Using a large number of CPU hours to experimentally derive the apparently optimal solutions, we also find that randomized k-means has the best rate of convergence to this apparent optimum. 
Abstract-found: 1
Intro-found: 1
Reference: [GJ79] <author> M.R. Garey and D.S. Johnson, </author> <title> "Computers and intractability: A guide to the theory of NP-completeness," W.H. </title> <publisher> Freeman and Co, </publisher> <year> 1979. </year>
Reference-contexts: The problem remains NP-complete regardless of the distance measure d (; ) used [B78], <ref> [GJ79] </ref>. Furthermore, there are various versions of J e for which the problem remain NP-complete including: 1. J e (X 1 ; : : : ; X k ; d (; )) = d (x; y) 8x; y 2 X i 8i = 1 : : : k.
Reference: [B78] <author> P. Brucker, </author> <title> "On the complexity of clustering problems," </title> <editor> in R. Henn, B. Korte, and W. Oletti (eds.), </editor> <booktitle> Optimization and Operations Research, Lecture Notes in Economics and Mathematical Systems, </booktitle> <publisher> Springer, </publisher> <address> Berlin 157. </address>
Reference-contexts: The problem remains NP-complete regardless of the distance measure d (; ) used <ref> [B78] </ref>, [GJ79]. Furthermore, there are various versions of J e for which the problem remain NP-complete including: 1. J e (X 1 ; : : : ; X k ; d (; )) = d (x; y) 8x; y 2 X i 8i = 1 : : : k.
Reference: [HS86] <author> Hochbaum, D. S., and Shmoys, D. B. </author> <year> (1986), </year> <title> "A unified approach to approximation algorithms for bottleneck problems", </title> <editor> J. </editor> <booktitle> ACM 33, </booktitle> <pages> 533-550. </pages>
Reference-contexts: This criterion function asks that the maximum distance between any two points in any cluster be less than B. Under this J e and a distance metric (i.e., d (; ) satisfying the triangle inequality), it has been shown <ref> [HS86] </ref> that the problem is efficiently approximable within 2, but is not efficiently approximable within 2 * for any * &gt; 0. P P x;y2X i d (x; y).
Reference: [SG78] <author> Sahni, S. K., and Gonzalez, T. </author> <year> (1976), </year> <title> "P-complete approximation problems", </title> <editor> J. </editor> <booktitle> ACM 23, </booktitle> <pages> 555-565. </pages>
Reference-contexts: P P x;y2X i d (x; y). This criterion function asks for the sum of the intra-cluster point distances to be less than B with a penalty for many points in a cluster. It has been shown <ref> [SG78] </ref> that under this J e , the problem is not *-approximable unless P=NP.
Reference: [DO74] <author> B.S. </author> <title> Duran and P.L. Odell "Cluster analysis; a survey", </title> <booktitle> Lecture notes in economics and mathematical systems, 100, </booktitle> <address> Berlin, New York, </address> <publisher> Springer-Verlag, </publisher> <year> 1974. </year>
Reference: [JD88] <author> Jain, </author> <title> A.K. and R.C. Dubes, "Algorithms for Clustering Data." </title> <address> Englewood Cliffs, N.J.: </address> <publisher> Prentice Hall, </publisher> <year> 1988 </year>
Reference-contexts: And lastly, section 7 outlines future work and presents our conclusions. 2 Background There are a large collection of approximation algorithms for the clustering algorithm <ref> [JD88] </ref>, [DH73],[DO74],[A73], [H75],[DJ87],[Z71],[T89], each with performance sensitive to particular classes of data sets.
Reference: [A73] <author> Michael R. Anderberg, </author> <title> "Cluster Analysis for Applications." </title> <journal> Probability and mathematical statistics, </journal> <volume> 19. </volume> <publisher> Academic Press, </publisher> <year> 1973. </year>
Reference: [H75] <author> John A. Hartigan, </author> <title> "Clustering Algorithms", </title> <publisher> John Wiley & Sons, </publisher> <year> 1975. </year> <month> 17 </month>
Reference: [DJ87] <author> E. Diday, M. Jambu, C. Hayashi, and N. Ohsumi eds. </author> <title> "Recent Developments in Clustering and Data Analysis." </title> <booktitle> Proceedings of the Japanese-French Scientific Seminar, </booktitle> <address> March 24-26, 1987, </address> <publisher> Academic Press, inc. </publisher>
Reference: [DH73] <author> Richard O. Duda and Peter E. Hart. </author> <title> "Pattern Classification and Scene Analysis", 1973, </title> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: This criterion function, called the sum-of-squared-error or minimum variance criterion <ref> [DH73] </ref>, is the most widely used in practice. Attempts to optimize it correspond to finding a set of well-separated clusters whose intra-cluster distances are small. Unfortunately, it has not yet been proven that under this J e the problem remains NP-complete.
Reference: [Z71] <author> Charles T. Zahn, </author> <title> "Graph-Theoretical Methods for Detecting and Describing Gestalt Clusters", </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-20, No. 1, </volume> <month> January </month> <year> 1971 </year>
Reference-contexts: Those algorithms include i) the method of minimum spanning tree, ii) maximum cut, iii) a hierarchical method and iv), the k-means algorithm. These algorithms are described in the following subsections. 2.1 Minimum Spinning Tree The version of the minimum spanning tree algorithm presented in the literature <ref> [Z71] </ref> works as follows: * Construct a complete graph with each element as a node and inter-element distances as edge weights. * Compute the minimum spanning tree of the resulting graph. * Choose the k longest edges in the minimum spanning tree and delete them. * The remaining k connected components
Reference: [T89] <author> Charles W. Therrien, </author> <title> "Decision Estimation and Classification: An Introduction to Pattern Recognition and Related Topics," </title> <publisher> John Wiley & Sons, </publisher> <year> 1989. </year>
Reference: [M67] <author> J. MacQueen, </author> <title> "Some methods for classification and analysis of multivariate observations," </title> <booktitle> in Proc. Fifth Berkeley Symposium on Math. Stat. and Prob., </booktitle> <volume> I, </volume> <pages> 281-297, </pages> <editor> L.M. LeCam and J. Neyman, eds. </editor> <publisher> University of California Press, </publisher> <year> 1967. </year>
Reference-contexts: Step 5: Output the resulting clustering. Needless to say, one main advantage of k-means is that it is designed specifically to optimize our J e . Further advantages include that it can recover from earlier errors, and that convergence to a stable state is guaranteed <ref> [M67] </ref>. See section 5 for our empirical results regarding convergence.
Reference: [J89] <author> Leland B. Jackson, </author> <title> "Digital Filters and Signal Processing, Second Edition", </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1989. </year> <month> 18 </month>
Reference-contexts: Once we have these averages, we wish to find the value t such that the curve y (t) = e t=t best fits the data averages. We do this using the auto-regressive all-pole method of linear prediction <ref> [J89] </ref> rather than non-linear least squares because the former is much simpler to implement than the latter, and the results are comparable.
References-found: 14

