URL: ftp://dino.ph.utexas.edu/wolf/Papers/MutInd.ps
Refering-URL: http://dino.ph.utexas.edu/~wolf/
Root-URL: 
Email: wolf@lanl.gov  
Title: Mutual Information as a Bayesian Measure of Independence  
Author: David Wolf 
Note: 1.0 Introduction.  
Address: Los Alamos, NM 87545.  
Affiliation: Department of Physics, University of Texas, Austin 78712. Los Alamos National Laboratory,  
Date: 1994  
Abstract: 0.0 Abstract. The problem of hypothesis testing is examined from both the historical and the Bayesian points of view in the case that sampling is from an underlying joint probability distribution and the hypotheses tested for are those of independence and dependence of the underlying distribution. Exact results for the Bayesian method are provided. Asymptotic Bayesian results and historical method quantities are compared, and historical method quantities are interpreted in terms of clearly defined Bayesian quantities. The asymptotic Bayesian test relies upon a statistic that is predominantly mutual information. Problems of hypothesis testing arise ubiquitously in situations where observed data is produced by an unknown process and the question is asked From what process did this observed data arise? Historically, the hypothesis testing problem is approached from the point of view of sampling, whereby several fixed hypotheses to be tested for are given, and all measures of the test and its quality are found directly from the likelihood, i.e. by what amounts to sampling the likelihood [2] [3]. (To be specific, a hypothesis is a set of possible parameter vectors, each parameter vector completely specifying a sampling distribution. A simple hypothesis is a hypothesis set that contains one parameter vector. A composite hypothesis occurs when the (nonempty) hypothesis set is not a single parameter vector.) Generally, the test procedure chooses as true the hypothesis that gives the largest test value, although the notion of procedure is not specific and may refer to any method for choosing the hypothesis given the test values. Since it is of interest to quantify the quality of the test, a level of significance is generated, this level being the probability that, under the chosen hypothesis and test procedure, an incorrect hypothesis choice is made. The significance is generated using the sampling distribution, or likelihood. For simple hypotheses the level of significance is found using the single parameter value of the hypothesis. When a test is applied in the case of a composite hypothesis, a size for the test is found that is given by the supremum probability (ranging over the parameter vectors in the hypothesis set) that under the chosen 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sir Maurice Kendall and Alan Stewart, </author> <booktitle> The Advanced Theory of Statistics, Volume 1, 4th Edition, </booktitle> <publisher> Macmillan Publishing, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: When the underlying pf is represented by a vector , and observed counts are represented by a vector , (both of dimension ), the chi-squared test is based upon the observation that for the distribution of the statistic converges to a distribution with degrees of freedom (dof) <ref> [1] </ref> [2] [3] . (Explicitly, the probability density function with dof is given by , where .) (For small the distribution of is somewhat different: we must examine the exact sampling distribution of in order to find the sampling distribution of .) In the historical framework the hypotheses being tested are
Reference: [2] <author> Sir Maurice Kendall and Alan Stewart, </author> <booktitle> The Advanced Theory of Statistics, Volume 2, 4th Edition, </booktitle> <publisher> Macmillan Publishing Inc., </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: observed data arise? Historically, the hypothesis testing problem is approached from the point of view of sampling, whereby several fixed hypotheses to be tested for are given, and all measures of the test and its quality are found directly from the likelihood, i.e. by what amounts to sampling the likelihood <ref> [2] </ref> [3]. (To be specific, a hypothesis is a set of possible parameter vectors, each parameter vector completely specifying a sampling distribution. A simple hypothesis is a hypothesis set that contains one parameter vector. <p> When the underlying pf is represented by a vector , and observed counts are represented by a vector , (both of dimension ), the chi-squared test is based upon the observation that for the distribution of the statistic converges to a distribution with degrees of freedom (dof) [1] <ref> [2] </ref> [3] . (Explicitly, the probability density function with dof is given by , where .) (For small the distribution of is somewhat different: we must examine the exact sampling distribution of in order to find the sampling distribution of .) In the historical framework the hypotheses being tested are and <p> The quantities and are estimated from the data by their maximum likelihood values and . With the estimated marginal distributions found in this manner, the statistic is asymptotically ( ) distributed as a rv with dof. <ref> [2] </ref>. Using this distribution, the test for a given significance goes through as before. There are several criticisms that must be made, but we leave these until Sec. 6 where this historical testing method is compared to the Bayesian method. 4.0 Bayesian test for independence. <p> The Bayesian approach clarifies a confusion associated with the historical test. This confusion concerns whether the cutoff for the historical test should be on high values of the test statistic alone, or whether both a high cutoff and a low cutoff are reasonable <ref> [2] </ref>. Recall from the discussion of Sec. 1 that, in the context of the historical test procedure, the significance level sets a condition only on the probability of rejection of independence, given independence.
Reference: [3] <author> Morris H. </author> <title> DeGroot, Probability and Statistics, Second Edition, </title> <publisher> Addison-Wesley Publishing, Inc., </publisher> <year> 1986. </year>
Reference-contexts: data arise? Historically, the hypothesis testing problem is approached from the point of view of sampling, whereby several fixed hypotheses to be tested for are given, and all measures of the test and its quality are found directly from the likelihood, i.e. by what amounts to sampling the likelihood [2] <ref> [3] </ref>. (To be specific, a hypothesis is a set of possible parameter vectors, each parameter vector completely specifying a sampling distribution. A simple hypothesis is a hypothesis set that contains one parameter vector. <p> Another framework exists for hypothesis testing that does not make implicit assumptions about the probabilities of occurrence of hypotheses and parameter vectors. In the Bayesian framework <ref> [3] </ref>, a prior distribution is chosen that quantifies how the various hypotheses occur. Instead of grouping all parameter vectors in a given hypothesis into a group with unclear probabilities of occurrence, the prior quantifies the probability of occurrence of the parameter vectors. <p> When the underlying pf is represented by a vector , and observed counts are represented by a vector , (both of dimension ), the chi-squared test is based upon the observation that for the distribution of the statistic converges to a distribution with degrees of freedom (dof) [1] [2] <ref> [3] </ref> . (Explicitly, the probability density function with dof is given by , where .) (For small the distribution of is somewhat different: we must examine the exact sampling distribution of in order to find the sampling distribution of .) In the historical framework the hypotheses being tested are and .
Reference: [4] <author> D. R. </author> <title> Wolf , Factorable Prior Distributions, </title> <note> available from the author. </note>
Reference-contexts: Similarly, for the dependent case we parameterize the space using itself, so that . It is not difficult to show that, with the priors chosen in this manner, the densities on the corresponding surfaces of constraint are indeed constant <ref> [4] </ref>. The assumption of uniformity is driven by the subjective desire to put as little knowledge as possible into the prior over the pfs, and to provide for simplicity of calculation.
Reference: [5] <author> D. H. Wolpert and D. R. Wolf, </author> <title> Estimating Functions of Probability Distributions from a Finite Set of Samples. Part 1: Bayes Estimators and the Shannon Entropy., </title> <publisher> LA-UR-4369. </publisher>
Reference-contexts: The integrals in Eqs. (5) and (6) are found using convolution and Laplace transform techniques described in <ref> [5] </ref>. The integration shows that the normalization constant for the independent prior is , while that of the dependent prior is . Taking into account the normalization constants for the priors, the results for the hypothesis-conditioned sampling distributions are , (7a) Let and .
Reference: [6] <author> D. R. Wolf and D. H. Wolpert, </author> <title> Estimating Functions of Probability Distributions from a Finite Set of Samples. Part 2: Bayes Estimators for Mutual Information, Chi-Squared, Covari-ance, and other Statistics., </title> <publisher> LA-UR-93-833. </publisher>
Reference-contexts: Uniformity is one requirement that we may drop: when the prior is not uniform the method for computing results in a manner similar to those presented in this paper is presented in <ref> [6] </ref>. Related hypothesis testing work appears in [8]. A desired quantity is the posterior probability of independence given data, . From it, because there are only two mutually exclusive hypotheses, we find that the probability of dependence given data is .
Reference: [7] <author> M. Abramowitz and I. A. </author> <title> Stegun, Handbook of Mathematical Functions, </title> <publisher> (Dover, </publisher> <address> New York 1972). </address>
Reference-contexts: The other terms are chi-squared statistics distributed as and respectively, and a constant asymptotically proportional to the true mutual information. Because a distribution has mean and variance <ref> [7] </ref>, then, for sufficiently large values of and , it is possible to conclude that the dominant term in Eq. (15) is the distributed term, . 6.0 Comparisons of the tests. The calculations of the last section allow several definitive comparisons of the Bayesian and historical tests to be made.
Reference: [8] <author> D.H. Wolpert, </author> <title> Determining whether two data sets are from the same distribution. The author defines same and different distribution spaces in a manner similar to the way independent and dependent distribution spaces are defined here. </title> <publisher> Unpub. </publisher>
Reference-contexts: Uniformity is one requirement that we may drop: when the prior is not uniform the method for computing results in a manner similar to those presented in this paper is presented in [6]. Related hypothesis testing work appears in <ref> [8] </ref>. A desired quantity is the posterior probability of independence given data, . From it, because there are only two mutually exclusive hypotheses, we find that the probability of dependence given data is .
References-found: 8

