URL: ftp://ftp.cs.umass.edu/pub/techrept/techreport/1993/UM-CS-1993-077.ps
Refering-URL: http://HTTP.CS.Berkeley.EDU/~vassilis/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: LEARNING TO SOLVE MARKOVIAN DECISION PROCESSES  
Author: Satinder P. Singh 
Degree: A Dissertation Presented by  Submitted to the Graduate School of the  in partial fulfillment of the requirements for the degree of Doctor of Philosophy  
Date: February 1994  
Affiliation: University of Massachusetts  Department of Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> P.E. Agre. </author> <title> The Dynamic Structure of Everyday Life. </title> <type> PhD thesis, </type> <institution> M.I.T., </institution> <year> 1988. </year>
Reference-contexts: Recently, some AI researchers have turned attention away from studying isolated aspects of intelligence and towards studying intelligent behavior in complete agents embedded in real-world environments (e.g., Agre <ref> [1] </ref>, Brooks [24, 23], and Kaelbling [60]). Much of this research on building embedded agents has followed the approach of hand-coding the agent's behavior (Maes [68]).
Reference: [2] <author> C.W. Anderson. </author> <title> Learning and Problem Solving with Multilayer Connectionist Systems. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1986. </year>
Reference-contexts: This raises the following important question: if practical concerns dictate that value functions be approximated, how might performance be affected 2 ? Is it possible that, despite some empirical evidence to the contrary (e.g., Barto et al. [13], Anderson <ref> [2] </ref>, Tesauro [112]), small errors in approximations could result in arbitrarily bad performance in principle? If so, this could raise significant concerns about the use of function approximation in DP-based learning. 4.5.1 An Upper Bound on the Loss from Approximate Optimal-Value Functions This section extends a result by Bertsekas [17] which
Reference: [3] <author> K.J. Astrom and B. Wittenmark. </author> <title> Adaptive Control. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: Later, control theorists developed indirect and direct adaptive control methods for dealing with problems in which a model was unavailable (e.g., Astrom and Wittenmark <ref> [3] </ref>). Indirect methods estimate a model of the environment incrementally and use the estimated model to design a solution. The same interleaving procedure of system identification and off-line design can be followed in AI problem solving. <p> Algorithms that estimate a model on-line and do model-based control design on the estimated model are also called indirect algorithms (see, e.g., Astrom and Wittenmark <ref> [3] </ref>, and Barto et al. [14]). * Model-Free A model-free algorithm does not use a model of the environment and therefore does not have access to the state transition matrices or the payoff function for the different policies.
Reference: [4] <author> J.R. Bachrach. </author> <title> A connectionist learning control architecture for navigation. </title> <editor> In R.P. Lippmann, J.E. Moody, and D.S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 457-463, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The new navigation tasks are in a navigational test bed that simulates a planar robot that can translate simultaneously and independently in both x and y directions (Figure 6.11). This testbed is similar to the one developed by Bachrach <ref> [4] </ref>. of the figure shows the robot's environment as seen from above. The circle represents the robot and the radial line inside the circle represents the robot's orientation. The 84 BC room with walls and obstacles that are painted in grayscale. <p> The 84 BC room with walls and obstacles that are painted in grayscale. The circular robot has 16 grayscale sensors and 16 distance sensors distributed evenly around its perimeter. The upper panel shows the robot's view. This testbed is identical to one developed by Bachrach <ref> [4] </ref>. See text for details. 85 walls of the simulated environment and the obstacles are shaded in grayscale. The robot can move one radius in any direction on each time step. The robot has 8 distance sensors and 8 grayscale sensors evenly placed around its perimeter.
Reference: [5] <author> E. Barnard. </author> <title> Temporal-difference methods and markov models. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 23(2) </volume> <pages> 357-365, </pages> <year> 1993. </year>
Reference-contexts: In the late 1980s, Watkins [118] observed that the TD algorithm solves the linear policy evaluation problem for multi-stage MDTs (see, also Dayan [33], Barnard <ref> [5] </ref>). Further, Watkins developed the Q-learning algorithm for solving MDTs and noted the approximate relationship between TD, Q-learning, and DP algorithms (see also Barto et al. [15, 10], and Werbos [122]). In this chapter, the connection between RL and MDTs is made precise.
Reference: [6] <author> A.G. Barto. </author> <type> personal communication. </type>
Reference-contexts: The algorithm presented here is closely related to a set of asynchronous algorithms presented by Williams and Baird [127] that were later shown by Barto <ref> [6] </ref> to be a form of k-step M-PI. 3.3.2 Asynchronous Update Operators For ease of exposition, let us denote the one-step backed-up value for state x under action a, given a value function V , by Q V (x; a).
Reference: [7] <author> A.G. Barto. </author> <title> Connectionist learning for control: An overview. </title> <editor> In T. Miller, R.S. Sutton, and P.J. Werbos, editors, </editor> <title> Neural Networks for Control. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address> <note> To appear. </note>
Reference-contexts: The current focus on embedded agents in AI has fortunately come at a time when a confluence of ideas from artificial intelligence, machine learning, robotics, and control engineering is taking place (Werbos [124], Barto <ref> [7] </ref>, Barto et al. [10], Sutton et al. [108, 110], Dean and Wellman [36]).
Reference: [8] <author> A.G. Barto. </author> <title> Learning to act: A perspective from control theory, </title> <month> July </month> <year> 1992. </year> <note> AAAI invited talk. </note>
Reference-contexts: RL algorithms and classical DP algorithms are related methods because they solve the same system of equations, and because RL algorithms estimate the same quantities that are computed by DP algorithms (see Watkins [118], Barto et al. [14], and Werbos [123, 124]). More recently, Barto <ref> [8] </ref> has identified the separate dimensions along which the different RL algorithms have weakened the strong constraints required by classical DP algorithms (see also Sutton [108]). <p> The first misconception, as pointed out by Barto <ref> [8] </ref>, is that while RL algorithms are indeed slow, there is little evidence that they are slower than any other method that can be applied with the same generality and under similar constraints. <p> The differences among the various iterative algorithms for solving MDTs are: 1) the definition of the operator U , and 2) the order in which the state-update equation is applied to the states of the MDT. Following Barto <ref> [8] </ref>, it is convenient to represent MDTs, both conceptually and pictorially, as directed stochastic graphs. Figure 3.1 shows the outgoing transitions for an arbitrary state x from a stochastic graph representation of an MDT that in turn is an abstraction of some real-world environment.
Reference: [9] <author> A.G. Barto and P. Anandan. </author> <title> Pattern recognizing stochastic learning automata. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 15 </volume> <pages> 360-375, </pages> <year> 1985. </year>
Reference-contexts: Later Barto et al. <ref> [9] </ref> developed an associative RL algorithm, they called the A RP algorithm, that solves single-stage MDTs with multiple-state environments. Single-stage MDTs do not involve the temporal credit assignment problem. Therefore algorithms for solving single-stage MDTs are unrelated to DP algorithms (except in the trivial sense).
Reference: [10] <author> A.G. Barto, S.J. Bradtke, and S.P. Singh. </author> <title> Learning to act using real-time dynamic programming. </title> <type> Technical Report 93-02, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1993. </year> <note> Submitted to: AI Journal. </note>
Reference-contexts: Accordingly, the framework adopted here abstracts the task to that of learning a behavior that approximates optimization of a preset objective functional defined over the space of possible behaviors of the agent. The framework presented here closely follows the work of Barto et al. <ref> [14, 10] </ref> and Sutton [108, 110]. 2.1 Controlling Dynamic Environments In general, the environment in which an agent is embedded can be dynamic, that is, can undergo transformations over time. <p> The current focus on embedded agents in AI has fortunately come at a time when a confluence of ideas from artificial intelligence, machine learning, robotics, and control engineering is taking place (Werbos [124], Barto [7], Barto et al. <ref> [10] </ref>, Sutton et al. [108, 110], Dean and Wellman [36]). <p> Part of the motivation behind this current research is to combine the complementary strengths of research on planning and problem solving in AI and of research on DP in optimal control to get the best of both worlds (e.g., Sutton [108], Barto et al. <ref> [10] </ref>, and Moore and Atkeson [79]). For example, techniques for dealing with uncertainty and stochasticity developed in control theory are now of interest to AI researchers developing architectures for agents embedded in real-world environments. <p> At least in theory, there is no need to find optimal actions for states that are not on the set of optimal paths from the set of possible start states (see Korf [65], and Barto et al. <ref> [10] </ref>). Following Sutton et al. [110] and Barto et al. [14], in this dissertation the adaptive optimal control framework is used to formulate tasks faced by autonomous embedded agents. <p> The set of states whose values are updated at iteration i is denoted S i (as in Barto et al. <ref> [10] </ref>). * On-line An on-line algorithm is one that not only learns a value function but also simultaneously controls a real environment. <p> Further, Watkins developed the Q-learning algorithm for solving MDTs and noted the approximate relationship between TD, Q-learning, and DP algorithms (see also Barto et al. <ref> [15, 10] </ref>, and Werbos [122]). In this chapter, the connection between RL and MDTs is made precise. But first, a brief detour has to be taken to explain the Robbins-Monro [86] stochastic approximation method for solving systems of equations. <p> Therefore, the agent cannot choose actions from the sole perspective of learning the value function in as few updates as possible. This dilemma is called the exploration versus exploitation tradeoff (Barto et al. <ref> [10] </ref>, Thrun [113, 114]). The exploration strategy adopted by an agent determines the order in which the states are visited and updated. Exploration: A simple strategy adopted by many researchers is to execute a non-stationary and probabilistic policy defined by the Gibbs distribution over the Q-values (Watkins [118], Sutton [107]).
Reference: [11] <author> A.G. Barto and S.P. Singh. </author> <title> Reinforcement learning and dynamic programming. </title> <booktitle> In Sixth Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> pages 83-88, </pages> <address> New Haven, CT, </address> <year> 1990. </year>
Reference-contexts: Indeed, there is some evidence that RL algorithms may be faster than their only known competitor that is applicable with the same level of generality, namely classical DP methods (Barto and Singh <ref> [12, 11] </ref>, Moore and Atkeson [79], Gullapalli [48]). The second misconception is the view that RL algorithms can only be used as weak methods. <p> Second, during the early stages of learning, the model will be highly inaccurate and can interfere with learning the value function by adding a "bias" to the full backup operator (see Barto and Singh <ref> [12, 11] </ref>). Nevertheless, in general it is difficult to predict which of the two algorithms, one based on sample backups, and the other based on full backups on an estimated model, will outperform the other.
Reference: [12] <author> A.G. Barto and S.P. Singh. </author> <title> On the computational economics of reinforcement learning. </title> <booktitle> In Proceedings of the 1990 Connectionist Models Summer School, </booktitle> <address> San Mateo, CA, Nov. 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Indeed, there is some evidence that RL algorithms may be faster than their only known competitor that is applicable with the same level of generality, namely classical DP methods (Barto and Singh <ref> [12, 11] </ref>, Moore and Atkeson [79], Gullapalli [48]). The second misconception is the view that RL algorithms can only be used as weak methods. <p> Second, during the early stages of learning, the model will be highly inaccurate and can interfere with learning the value function by adding a "bias" to the full backup operator (see Barto and Singh <ref> [12, 11] </ref>). Nevertheless, in general it is difficult to predict which of the two algorithms, one based on sample backups, and the other based on full backups on an estimated model, will outperform the other. <p> In such a case, parts of the state space that have not been visited will have higher values than those that have been visited often. The greedy policy will then automatically explore unvisited regions of the state space. Barto and Singh <ref> [12] </ref> developed an algorithm that keeps a frequency count of how often each action is executed in each state (see also Sato et al. [91]). If an action is neglected for too long its execution probability 62 is increased.
Reference: [13] <author> A.G. Barto, R.S. Sutton, and C.W. Anderson. </author> <title> Neuronlike elements that can solve difficult learning control problems. </title> <journal> IEEE SMC, </journal> <volume> 13 </volume> <pages> 835-846, </pages> <year> 1983. </year>
Reference-contexts: The second misconception is the view that RL algorithms can only be used as weak methods. This misconception was perhaps generated inadvertently by the early developmental work on RL that used as illustrations applications with very little domain knowledge (Barto et.al. <ref> [13] </ref>, Sutton [106]). However, RL architectures can easily incorporate many different kinds of domain knowledge. <p> Single-stage MDTs do not involve the temporal credit assignment problem. Therefore algorithms for solving single-stage MDTs are unrelated to DP algorithms (except in the trivial sense). In the early 1980s, in a landmark paper Barto et al. <ref> [13] </ref> described a technique for addressing the temporal credit assignment problem that culminated in Sutton's [106] paper on a class of techniques he called temporal difference (TD) methods (see also Sutton [105]). <p> This raises the following important question: if practical concerns dictate that value functions be approximated, how might performance be affected 2 ? Is it possible that, despite some empirical evidence to the contrary (e.g., Barto et al. <ref> [13] </ref>, Anderson [2], Tesauro [112]), small errors in approximations could result in arbitrarily bad performance in principle? If so, this could raise significant concerns about the use of function approximation in DP-based learning. 4.5.1 An Upper Bound on the Loss from Approximate Optimal-Value Functions This section extends a result by Bertsekas <p> The fourth scaling issue concerns the temporal generalization problem, i.e., the problem of doing backups that transfer information among states that are not one-step neighbors. Few researchers have developed RL architectures that explicitly address the need to do temporal generalization. Barto et al.'s <ref> [13] </ref> used eligibility traces to update states visited many steps in the past. Sutton [106] developed a family of algorithms called TD () that use multi-step backup operators. Watkins [118] defined a multi-step version of Q-learning. All these approaches are model-free.
Reference: [14] <author> A.G. Barto, R.S. Sutton, and C. Watkins. </author> <title> Learning and sequential decision making. </title> <editor> In M. Gabriel and J.W. Moore, editors, </editor> <title> Learning and Computational Neuroscience. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year> <month> 131 </month>
Reference-contexts: Researchers within these diverse fields have developed a number of different methods under different names for solving RL tasks, e.g., dynamic programming (DP) algorithms (Bellman [16]), classifier systems (Holland et al. [51]), and reinforcement learning algorithms (Barto et al. <ref> [14] </ref>, Werbos [121]). 1 The different algorithms assume different amounts of domain knowledge and work under different constraints, but they can all solve RL tasks and should perhaps all be called RL methods. <p> RL algorithms and classical DP algorithms are related methods because they solve the same system of equations, and because RL algorithms estimate the same quantities that are computed by DP algorithms (see Watkins [118], Barto et al. <ref> [14] </ref>, and Werbos [123, 124]). More recently, Barto [8] has identified the separate dimensions along which the different RL algorithms have weakened the strong constraints required by classical DP algorithms (see also Sutton [108]). <p> Accordingly, the framework adopted here abstracts the task to that of learning a behavior that approximates optimization of a preset objective functional defined over the space of possible behaviors of the agent. The framework presented here closely follows the work of Barto et al. <ref> [14, 10] </ref> and Sutton [108, 110]. 2.1 Controlling Dynamic Environments In general, the environment in which an agent is embedded can be dynamic, that is, can undergo transformations over time. <p> At least in theory, there is no need to find optimal actions for states that are not on the set of optimal paths from the set of possible start states (see Korf [65], and Barto et al. [10]). Following Sutton et al. [110] and Barto et al. <ref> [14] </ref>, in this dissertation the adaptive optimal control framework is used to formulate tasks faced by autonomous embedded agents. <p> One commonly used measure for policies is the expected value of the discounted sum of payoffs over the time horizon of the agent as a function of the start state (see Barto et al. <ref> [14] </ref>). 6 This dissertation focuses on agents that have infinite life-times and therefore will have infinite horizons. <p> Algorithms that estimate a model on-line and do model-based control design on the estimated model are also called indirect algorithms (see, e.g., Astrom and Wittenmark [3], and Barto et al. <ref> [14] </ref>). * Model-Free A model-free algorithm does not use a model of the environment and therefore does not have access to the state transition matrices or the payoff function for the different policies. A model-free algorithm is limited to applying the state-update equation to the state of the real environment.
Reference: [15] <author> A.G. Barto, R.S. Sutton, and C. Watkins. </author> <title> Sequential decision problems and neural networks. </title> <editor> In D.S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 686-693, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Further, Watkins developed the Q-learning algorithm for solving MDTs and noted the approximate relationship between TD, Q-learning, and DP algorithms (see also Barto et al. <ref> [15, 10] </ref>, and Werbos [122]). In this chapter, the connection between RL and MDTs is made precise. But first, a brief detour has to be taken to explain the Robbins-Monro [86] stochastic approximation method for solving systems of equations.
Reference: [16] <author> R.E. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1957. </year>
Reference-contexts: Therefore, a wide variety of tasks of interest in operations research, control theory, robotics, AI, can be formulated as RL tasks. Researchers within these diverse fields have developed a number of different methods under different names for solving RL tasks, e.g., dynamic programming (DP) algorithms (Bellman <ref> [16] </ref>), classifier systems (Holland et al. [51]), and reinforcement learning algorithms (Barto et al. [14], Werbos [121]). 1 The different algorithms assume different amounts of domain knowledge and work under different constraints, but they can all solve RL tasks and should perhaps all be called RL methods. <p> Dynamic programming (DP) is a set of iterative methods, developed in the classical literature on control theory and operations research, that are capable of solving such equations (Bellman <ref> [16] </ref>). 2 Control architectures that use DP algorithms require a model of the environment, either one that is known a priori, or one that is estimated on-line. <p> The set of recurrence relations, 8x 2 X, V fl (x) = max (R a (x) + fl y2X is known in the DP literature as the Bellman equation for infinite-horizon MDTs (Bellman <ref> [16] </ref>). A policy is greedy with respect to any finite value function V if it prescribes to each state an action that maximizes the sum of the immediate payoff and the discounted expected value of the next state as determined by the value function V . <p> In general, an algorithm that needs knowledge of the transition probabilities cannot be applied without a model of the environment. 20 3.2 Dynamic Programming DP is a collection of algorithms based on Bellman's <ref> [16] </ref> powerful principle of optimality which states that "an optimal policy has the property that whatever the initial state and action are, the remaining actions must constitute an optimal policy with regard to the state resulting from the first action." The optimal control equation 3.2 can be derived directly from Bellman's
Reference: [17] <author> D.P. Bertsekas. </author> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1987. </year>
Reference-contexts: This dissertation will focus on DP and RL algorithms. Nevertheless, it should be noted that no definitive comparison has yet been made between optimization methods based on genetic algorithms and RL or DP algorithms. 4 a system of nonlinear recursive equations (Ross [87], Bertsekas <ref> [17] </ref>). <p> In such a case, R a t (x t ) : 6 The average payoff per time step received by an agent is another measure for policies that is used in the classical DP literature (Bertsekas <ref> [17] </ref>), and more recently in the RL literature (Schwartz [94], Singh [100]). This dissertation will only deal with the discounted measure for policies. 12 be modeled as MDTs by discretizing the state space and choosing actions at some fixed frequency. <p> In general, the coarser the resolution in space and time, the easier it should be to find a solution. But at the same time better solutions may be found at finer resolutions. This tradeoff is a separate topic of research (e.g., Bertsekas <ref> [17] </ref>) and will only be partially addressed in this dissertation (see Chapter 8). The MDT framework for control tasks is a natural extension to stochastic environments of the AI state-space search framework for problem solving tasks. <p> The smaller the value of fl and the smaller the value of 2 , the faster the convergence. 23 Stopping Conditions: For Jacobi value iteration, V i+1 = B (V i ), it is possible to define the following error bounds (Bertsekas <ref> [17] </ref>): h i = 1 fl x2X h i = 1 fl x2X such that 8x 2 X, B x (V i ) + h i V fl (x) B x (V i ) + h i . <p> Anderson [2], Tesauro [112]), small errors in approximations could result in arbitrarily bad performance in principle? If so, this could raise significant concerns about the use of function approximation in DP-based learning. 4.5.1 An Upper Bound on the Loss from Approximate Optimal-Value Functions This section extends a result by Bertsekas <ref> [17] </ref> which guarantees that small errors in the approximation of a task's optimal value function cannot produce arbitrarily bad performance when actions are selected greedily. <p> The resulting loss in value, L ~ V = V fl V ~ V , is bounded above by (2fl*)=(1 fl). in Bertsekas <ref> [17] </ref>, and in deriving the corresponding upper bound for the newer Q-learning algorithm. These results also provide a theoretical justification for a practice that is common in RL. The material presented in this section was developed in collaboration with Yee and is reported in Singh and Yee [103]. <p> is a function such that 8x 2 X; fi fiV fl (x) ~ V (x) fi fi *, and ~ V is a greedy policy for ~ V , then 8x, L ~ V (x) 1 fl The upper bound of Theorem 7 is tighter than the result in Bertsekas <ref> [17] </ref> by a factor of fl (Cf. [p. 236, #14 (c)]).
Reference: [18] <author> D.P. Bertsekas and J.N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: If the states are updated in some fixed order and the operator U always uses the most recent approximation to the value function, the algorithm is said to perform a Gauss-Sidel iteration. * Asynchronous: Different researchers have used different models of asynchrony in iterative algorithms (e.g., Bertsekas and Tsitsiklis <ref> [18] </ref>). In this dissertation the term asynchronous is used for algorithms that place no constraints on the order in which the state-update equation is applied, except that in the limit the value of each state will be updated infinitely often. <p> Therefore, the synchronous successive approximation algorithm converges to V by the application of the contraction mapping theorem (see, e.g., Bertsekas and Tsit sklis <ref> [18] </ref>). Convergence can be proven for asynchronous successive approximation by applying the asynchronous convergence theorem of Bertsekas and Tsitsiklis [18]. 3.2.2 Optimal Control Determining the optimal value function requires solving the nonlinear system of equations 3.2. <p> Therefore, the synchronous successive approximation algorithm converges to V by the application of the contraction mapping theorem (see, e.g., Bertsekas and Tsit sklis <ref> [18] </ref>). Convergence can be proven for asynchronous successive approximation by applying the asynchronous convergence theorem of Bertsekas and Tsitsiklis [18]. 3.2.2 Optimal Control Determining the optimal value function requires solving the nonlinear system of equations 3.2. <p> Therefore, the synchronous value iteration algorithm can be proven to converge to V fl by the application of the contraction mapping theorem. Convergence can be proven for asynchronous value iteration with fl &lt; 1 by applying the asynchronous convergence theorem of Bertsekas and Tsitsiklis <ref> [18] </ref>. The rate of convergence is governed by fl and the second largest eigenvalue, 2 , of the transition probability matrix for the optimal policy. <p> If 0 fl &lt; 1, and a finite constant 2 &lt; is added uniformly to all the payoffs, the partial order of the policies does not change. The development of the convergence proofs closely follows Bertsekas and Tsitsik-lis <ref> [18] </ref>. Condition 1. 0 fl &lt; 1 Condition 2. 8 x 2 X; 8 a 2 A, R a (x) 0. This is not a restriction for MDTs with 0 fl &lt; 1, because of Fact 8. Throughout this section Conditions 1 and 2 are assumed true. <p> Proof : Using Proposition 1 and the contraction mapping theorem, the iterative algorithm defined by Equation 3.14 converges to a unique fixed point. Corollary 2A: Let V fl be the optimal value function. Then lim p!1 V fl p = V fl . Proof : Bertsekas and Tsitsiklis <ref> [18] </ref> show that the iteration V t+1 = B (V t ), where the operator B is as defined in Equation 3.12, converges to the optimal value function V fl . Therefore, lim p!1 B (p) = B ) lim p!1 V fl p = V fl . <p> For the asynchronous case the proof requires the application of the asynchronous convergence theorem in Bertsekas and Tsitsiklis <ref> [18] </ref>. Another advantage of using Q-values is that it becomes possible to do stochastic approximation by sampling not only the state-action pair but the next state as well. <p> Then if V l 2 V u , jjV l+h V fl jj 1 fljjV l V fl jj 1 . Proof: This is a simple extension of a result in Bertsekas and Tsitsiklis <ref> [18] </ref>. Fact 2: Consider the following algorithm: (V 0 k+1 ; k+1 ) = U k (V 0 k ; k ), where U k 2 fB x j x 2 Xg.
Reference: [19] <author> J.R. Blum. </author> <title> Multidimensional stochastic approximation method. </title> <journal> Ann. of Math. Stat., </journal> <volume> 25, </volume> <year> 1954. </year>
Reference-contexts: Equation 4.2 will play an essential role in establishing the connection between RL and DP in Section 4.3. Following Robbin and Monro's work, several authors extended their results to multi-dimensional equations and derived convergence with probability one under weaker conditions (Blum <ref> [19] </ref>, Dvoretzky [39], Schmetterer [92]).
Reference: [20] <author> S.J. Bradtke. </author> <title> Reinforcement learning applied to linear quadratic regulation. </title> <booktitle> In Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 295-302, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Specifically, the extension is in deriving an upper bound on performance loss which is slightly tighter than that 1 Sutton [106] and Dayan [33] have proved convergence for the TD algorithm when linear networks are used to represent the value functions. Bradtke <ref> [20] </ref> adapted Q-learning to solve linear quadratic regulation (LQR) problems and proved convergence under the assumption that a linear-in-the-parameters network is used to store the Q-value function. 2 Even with lookup-table representations, in practice it may be difficult to visit every state-action pair often enough to ensure convergence to the optimal
Reference: [21] <author> J. Bridle. </author> <title> Probablistic interpretation of feedforward classification network outputs with relationships to statistical pattern recognition. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: An advantage of using B () is that it requires less computation than the operator B (p). The operator B () is similar in spirit to the "soft-max" function (Bridle <ref> [21] </ref>) used by Jacobs et al. [56] and may provide a probabilistic framework for action selection in DP-based algorithms. Several researchers are investigating the advantages of combining nonlinear neural networks with traditional adaptive control techniques (e.g., [57, 42]).
Reference: [22] <author> R.A. Brooks. </author> <title> A robot that walks: Emergent behaviors from a carefully evolved network. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 23-262, </pages> <year> 1989. </year>
Reference-contexts: Further work is required to test this conjecture. 6.5 Related Work An architecture similar to CQ-L is the subsumption architecture for autonomous intelligent agents (Brooks <ref> [22] </ref>), which is composed of several task-achieving modules along with precompiled switching circuitry that controls which module should be active at any time. In most implementations of the subsumption architecture both the task-achieving modules as well as the switching circuitry are hardwired by the agent designer. <p> of hitting an obstacle. 8.2.4 Behavior-Based Reinforcement Learning Behavior-based robotics is the name given to a body of relatively recent work in robotics that builds robots equipped with a set of useful "behaviors" and solves problems by switching these behaviors on and off appropriately (e.g., the subsumption architecture of Brooks <ref> [22] </ref>). The term behavior is often used loosely to include all kinds of open-loop, closed-loop, and mixed control policies. The closed-loop Dirichlet and Neumann policies are examples of goal-seeking behaviors.
Reference: [23] <author> R.A. Brooks. </author> <title> Intelligence without reason. </title> <journal> A.I. </journal> <volume> Memo. 1293, </volume> <publisher> M.I.T., </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: Recently, some AI researchers have turned attention away from studying isolated aspects of intelligence and towards studying intelligent behavior in complete agents embedded in real-world environments (e.g., Agre [1], Brooks <ref> [24, 23] </ref>, and Kaelbling [60]). Much of this research on building embedded agents has followed the approach of hand-coding the agent's behavior (Maes [68]).
Reference: [24] <author> R.A. Brooks. </author> <title> Intelligence without representation. </title> <journal> Artificial Intelligence, </journal> <volume> 47 </volume> <pages> 139-159, </pages> <year> 1991. </year>
Reference-contexts: Recently, some AI researchers have turned attention away from studying isolated aspects of intelligence and towards studying intelligent behavior in complete agents embedded in real-world environments (e.g., Agre [1], Brooks <ref> [24, 23] </ref>, and Kaelbling [60]). Much of this research on building embedded agents has followed the approach of hand-coding the agent's behavior (Maes [68]).
Reference: [25] <author> R.R. Bush and F. Mosteller. </author> <title> Stochastic Models for Learning. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1955. </year>
Reference-contexts: and addresses some of them. 4.1 A Brief History of Reinforcement Learning Early research in RL developed non-associative algorithms for solving single-stage Markovian decision tasks (MDTs) in environments with only one state, and had its roots in the work of psychologists working on mathematical learning theory (e.g., Bush and Mosteller <ref> [25] </ref>), and in the work of learning automata theorists (e.g., Narendra and Thatachar [63]). Later Barto et al. [9] developed an associative RL algorithm, they called the A RP algorithm, that solves single-stage MDTs with multiple-state environments. Single-stage MDTs do not involve the temporal credit assignment problem.
Reference: [26] <author> D. </author> <title> Chapman and L.P. Kaelbling. Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. </title> <booktitle> In Proceedings of the 1991 International Joint Conference on Artificial Intelligence, </booktitle> <year> 1991. </year>
Reference-contexts: Moore showed that his approach develops a model that has a high resolution around the optimal trajectory in state space and a coarse resolution elsewhere (see, also Yee [128]). Chapman and Kaelbling <ref> [26] </ref> developed an algorithm that builds a tree structured Q-table. Each node splits on one bit of the state representation, and that bit is chosen based on its relevance in predicting short-term and long-term payoffs. Relevance is measured via statistical tests.
Reference: [27] <author> L. Chrisman. </author> <title> Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. </title> <booktitle> In AAAI-92, </booktitle> <year> 1992. </year>
Reference-contexts: Researchers are currently developing methods that attempt to build state information by either memorizing past perceptions, or by controlling the perceptual system of the agent to generate multiple perceptions (Whitehead and Ballard [126], Lin and Mitchell [67], Chrisman <ref> [27] </ref>, and McCallum [73]). In both cases the hope is that techniques other than RL can be used to convert a non-Markovian problem into a Markovian one so that conventional RL can be applied.
Reference: [28] <author> J.A. Clouse and P.E. Utgoff. </author> <title> A teaching method for reinforcement learning. </title> <booktitle> In Machine Learning: Proceedings of teh Ninth International Conference, </booktitle> <pages> pages 92-101, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Indeed, a significant proportion of the current research on RL is about incorporating domain knowledge into RL architectures to alleviate some of their problems (Singh [99], Yee et al. [129], Mitchell and Thrun [75], Whitehead [125], Lin [66], Clouse and Utgoff <ref> [28] </ref>). Despite the fact that under certain conditions RL algorithms may be the best available methods, conventional RL architectures are slow enough to make them impractical for many real-world problems. <p> Bias in Policy Selection: Some model-free RL architectures control the order in which states are updated by using task-specific knowledge to bias the policy selection in such a way as to direct the state trajectories into a useful and informative part of the state space. Clouse and Utgoff <ref> [28] </ref> have developed an architecture in which a human expert monitors the performance of the RL agent. If the agent is performing badly, the expert replaces the agent's action by an optimal action.
Reference: [29] <author> C.I. Connolly. </author> <title> Applications of harmonic functions to robotics. </title> <booktitle> In The 1992 International Symposium on Intelligent Control. IEEE, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: The robustness and greatly accelerated learning resulting from the above factors can more than offset the cost of learning the abstract actions. The next section presents the optimal motion planning problem and a brief sketch of the harmonic function approach to path planning developed by Connolly <ref> [29] </ref> that is used to compute the abstract actions. 8.2 Motion Planning Problem The motion planning problem arises from the need to give an autonomous robot the capability of planning its own motion, i.e., deciding what motions to execute in order to achieve a task specified by initial and goal spatial
Reference: [30] <author> C.I. Connolly and R.A. Grupen. </author> <title> Harmonic control. </title> <booktitle> In The 1992 International Symposium on Intelligent Control. IEEE, </booktitle> <month> August </month> <year> 1992. </year> <month> 132 </month>
Reference-contexts: The problem with this approach is that it is not guaranteed that the particle will avoid spurious minima, i.e., minima not at goal location. Harmonic functions have been proposed by Connolly et al. <ref> [31, 30] </ref> as an alternative to local potential functions. Harmonic functions are guaranteed to have no spurious local minima. The description presented here closely follows that of Connolly et al. [31]. <p> Solutions derived from the Neumann boundary condition are denoted N . 8.2.2 Policy generation The gradient of a harmonic function, r, defines streamlines, or paths, in configuration space that are guaranteed to be a) smooth, b) avoid all obstacles, and c) terminate in a goal state (Connolly et al. <ref> [30] </ref>). The paths generated by D as well as by N have these properties but are qualitatively very different from one other. The Dirichlet paths are perpendicular to the boundary, while the Neumann paths are parallel to the boundary. Examples of both types of paths are shown later in problem.
Reference: [31] <author> C.I. Connolly and R.A. Grupen. </author> <title> On the applications of harmonic functions to robotics. </title> <journal> Journal of Robotic Systems, </journal> <note> in press 1993. </note>
Reference-contexts: The problem with this approach is that it is not guaranteed that the particle will avoid spurious minima, i.e., minima not at goal location. Harmonic functions have been proposed by Connolly et al. <ref> [31, 30] </ref> as an alternative to local potential functions. Harmonic functions are guaranteed to have no spurious local minima. The description presented here closely follows that of Connolly et al. [31]. <p> Harmonic functions have been proposed by Connolly et al. [31, 30] as an alternative to local potential functions. Harmonic functions are guaranteed to have no spurious local minima. The description presented here closely follows that of Connolly et al. <ref> [31] </ref>. A harmonic function on a domain R n is a function that satisfies Laplace's equation: r 2 = i=1 @x 2 = 0: (8.1) In practice Equation 8.1 is solved numerically by finite difference methods.
Reference: [32] <author> T.H. Cormen, C.E. Leiserson, and R.L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press and McGraw Hill, </publisher> <year> 1990. </year>
Reference-contexts: all weak learning algorithms are doomed to scale poorly to complex tasks (e.g., Mataric [72]). 2 Within theoretical computer science, the term DP is applied to a general class of methods for efficiently solving recursive systems of equations for many different kinds of structured optimization problems (e.g., Cormen et al. <ref> [32] </ref>), not just the recursive equations derived for agents controlling external environments.
Reference: [33] <author> P. Dayan. </author> <title> The convergence of TD() for general . Machine Learning, </title> 8(3/4):341-362, May 1992. 
Reference-contexts: In the late 1980s, Watkins [118] observed that the TD algorithm solves the linear policy evaluation problem for multi-stage MDTs (see, also Dayan <ref> [33] </ref>, Barnard [5]). Further, Watkins developed the Q-learning algorithm for solving MDTs and noted the approximate relationship between TD, Q-learning, and DP algorithms (see also Barto et al. [15, 10], and Werbos [122]). In this chapter, the connection between RL and MDTs is made precise. <p> Q 0 is finite, and 3. 8 (x; a) 2 (X fi A); i=0 i (x; a) = 1 and i=0 2 4.3.3 Discussion Convergence proofs for TD and Q-learning that do not use stochastic approximation theory already existed in the RL literature (Sutton [106] and Dayan <ref> [33] </ref> for TD, and Watkins [118] and Watkins and Dayan [119] for Q-learning). But these proofs, especially the one for Q-learning, are based on special mathematical constructions that obscure the underlying simplicity of the algorithms. <p> Specifically, the extension is in deriving an upper bound on performance loss which is slightly tighter than that 1 Sutton [106] and Dayan <ref> [33] </ref> have proved convergence for the TD algorithm when linear networks are used to represent the value functions.
Reference: [34] <author> P. Dayan. </author> <title> Improving generalization for temporal difference learning: </title> <booktitle> The successor representation. Neural Computation, </booktitle> <volume> 5(4) </volume> <pages> 613-624, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Barto et al.'s [13] used eligibility traces to update states visited many steps in the past. Sutton [106] developed a family of algorithms called TD () that use multi-step backup operators. Watkins [118] defined a multi-step version of Q-learning. All these approaches are model-free. Dayan <ref> [34] </ref> and Sutton and Pinette [111] developed an algorithm that tackles the temporal generalization problem in policy evaluation problems by changing the agent's state representation.
Reference: [35] <author> P. Dayan and G.E. Hinton. </author> <title> Feudal reinforcement learning. </title> <editor> In S.J. Hanson, J.D. Cowan, and C.L. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 271-278. </pages> <publisher> Morgan-Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Lin's work assumes deterministic environments but does not assume the compositional structure on tasks assumed in this authors work. It also does not build models of the elementary tasks. Dayan and Hinton <ref> [35] </ref> have developed a hierarchical architecture, called feudal RL, that also does not assume compositional structure on tasks. It consists of a predefined hierarchy of managers, each of whom has the power to set goals and payoffs for their immediate sub-managers.
Reference: [36] <author> T.L. Dean and M.P. Wellman. </author> <title> Planning and Control. </title> <publisher> Morgan Kauffman, </publisher> <year> 1991. </year>
Reference-contexts: The current focus on embedded agents in AI has fortunately come at a time when a confluence of ideas from artificial intelligence, machine learning, robotics, and control engineering is taking place (Werbos [124], Barto [7], Barto et al. [10], Sutton et al. [108, 110], Dean and Wellman <ref> [36] </ref>).
Reference: [37] <author> J. Denker, D. Schwartz, B. Wittner, S. Solla, R. Howard, L. Jackel, and J. </author> <title> Hopfield. Large automatic learning, rule extraction, and generalization. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 877-922, </pages> <year> 1987. </year>
Reference-contexts: Multi-task agents can also achieve computational and monetary savings over multiple single-task agents simply by sharing hardware and software across tasks. Transfer of training across tasks is different from the phenomenon of generalization as it is commonly studied in supervised learning tasks (Denker et al. <ref> [37] </ref>). Generalization refers to the ability of a learner to produce (hopefully correct) outputs when tested on inputs that are not part of the training set. Typically generalization is studied within a single task that can be thought of as a mapping from some input space to an output space.
Reference: [38] <author> R.O. Duda and P.E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: finite, stationary, and Markovian environments degrade more gracefully for small violations of the previous two assumptions relative to small violations of the Markovian assumption. 1.3 Why Reinforcement Learning? A set of training examples is required if a problem is to be formulated as a supervised learning task (Duda and Hart <ref> [38] </ref>). For agents embedded in dynamic environments, actions executed in the short term can impact the long term dynamics 3 of the environment.
Reference: [39] <author> A. Dvoretzky. </author> <title> On stochastic approximation. </title> <booktitle> In Proceedings of the third Berkeley symposium on Mathematical Statisitics and Probability, </booktitle> <pages> pages 39-55, </pages> <year> 1956. </year>
Reference-contexts: Equation 4.2 will play an essential role in establishing the connection between RL and DP in Section 4.3. Following Robbin and Monro's work, several authors extended their results to multi-dimensional equations and derived convergence with probability one under weaker conditions (Blum [19], Dvoretzky <ref> [39] </ref>, Schmetterer [92]). Appendix B presents a theorem by Dvoretzky [39] that is more complex but more closely related to the material presented in the following sections. 4.3 Reinforcement Learning Algorithms This section uses Equation 4.2 to derive stochastic approximation algorithms to solve the policy evaluation and optimal control problems. <p> Following Robbin and Monro's work, several authors extended their results to multi-dimensional equations and derived convergence with probability one under weaker conditions (Blum [19], Dvoretzky <ref> [39] </ref>, Schmetterer [92]). Appendix B presents a theorem by Dvoretzky [39] that is more complex but more closely related to the material presented in the following sections. 4.3 Reinforcement Learning Algorithms This section uses Equation 4.2 to derive stochastic approximation algorithms to solve the policy evaluation and optimal control problems. <p> For the purposes of Chapter 4 a result derived by Dvoretzky is the most relevant. Dvoretzky <ref> [39] </ref> studied a problem more general than finding roots of equations. He studied convergent deterministic iterative processes of the form V n+1 = T n (V n ), where T n is a deterministic operator.
Reference: [40] <author> S.E. Fahlman and C. Lebiere. </author> <title> The cascade-correlation learning architecture. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 524-532, </pages> <address> San Mateo, CA, 1990. </address> <publisher> IEEE, Morgan Kaufmann. </publisher>
Reference-contexts: This is not possible to know in advance because V fl (and Q fl ) is not known in advance. However, constructive function approximation approaches (e.g., Fahlman and Lebiere <ref> [40] </ref>) may be able to alleviate this problem. * A more fundamental issue is that a non lookup-table function approximation method can generalize an update in such a way that the essential "contraction based" convergence of DP-related algorithms may be thwarted.
Reference: [41] <author> R.E. Fikes, P.E. Hart, and N.J. Nilsson. </author> <title> Learning and executing generalised robot plans. </title> <journal> Artificial Inteligence, </journal> <volume> 3 </volume> <pages> 189-208, </pages> <year> 1972. </year>
Reference-contexts: AI researchers have long used macro-operators, which are labels for useful sequences of operators/actions, to build abstract models of the problem environment (e.g., Fikes et al. <ref> [41] </ref>). The problem solver uses such abstract environment-models to plan in terms of the macro-operators instead of the primitive operators (Korf [64]). Planning in the abstract model achieves temporal abstraction because it allows the problem solver to ignore unnecessary detail.
Reference: [42] <author> W. Fun and M.I. Jordan. </author> <title> The moving basin: Effective action-search in adaptive control, </title> <note> 1992. submitted to Neural Computation. </note>
Reference-contexts: The operator B () is similar in spirit to the "soft-max" function (Bridle [21]) used by Jacobs et al. [56] and may provide a probabilistic framework for action selection in DP-based algorithms. Several researchers are investigating the advantages of combining nonlinear neural networks with traditional adaptive control techniques (e.g., <ref> [57, 42] </ref>). The algorithms presented in this section have the dual advantages of leading to more robust solutions and of employing a differentiable backup operator.
Reference: [43] <author> D.E. Goldberg. </author> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, </address> <year> 1989. </year>
Reference-contexts: introduced briefly in the next two sections. 1.3.1 Reinforcement Learning Algorithms vis-a-vis Classical Dynamic Pro gramming Algorithms The problem of determining the optimal behavior for agents embedded in finite, stationary, and Markovian environments can be reduced to the problem of solving 1 Combinatorial optimization methods, such as genetic algorithms (Goldberg <ref> [43] </ref>), can also be used to solve RL tasks (Grefenstette [45]). However, unlike DP and RL algorithms that use the agent's experience to adapt directly its architecture, genetic algorithms have to evaluate the fitness of a "population" of agents before making any changes to their architectures.
Reference: [44] <author> G.C. Goodwin and K.W. </author> <title> Sin. Adaptive Filtering Prediction and Control. </title> <address> Englewood Cliffs, </address> <year> 1984. </year>
Reference-contexts: For several decades control engineers have been designing controllers that are able to transform a variety of dynamical systems to a desired goal state or that can track a desired state trajectory over time (e.g., Goodwin and Sin <ref> [44] </ref>). Such tasks are called regulation and tracking tasks respectively.
Reference: [45] <author> J.J. Grefenstette. </author> <title> Incremental learning of control strategies with genetic algorithms. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <pages> pages 340-344, </pages> <address> Ithaca, New York, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Algorithms vis-a-vis Classical Dynamic Pro gramming Algorithms The problem of determining the optimal behavior for agents embedded in finite, stationary, and Markovian environments can be reduced to the problem of solving 1 Combinatorial optimization methods, such as genetic algorithms (Goldberg [43]), can also be used to solve RL tasks (Grefenstette <ref> [45] </ref>). However, unlike DP and RL algorithms that use the agent's experience to adapt directly its architecture, genetic algorithms have to evaluate the fitness of a "population" of agents before making any changes to their architectures.
Reference: [46] <author> R. Grupen. </author> <title> Planning grasp strategies for multifingered robot hands. </title> <booktitle> In Proceedings of the 1991 Conference on Robotics and Automation, </booktitle> <pages> pages 646-651, </pages> <address> Sacramento, CA, </address> <month> April </month> <year> 1991. </year> <journal> IEEE. </journal> <volume> 133 </volume>
Reference-contexts: Determining stable behaviors and rules for composing them that are useful across a variety of complex tasks with multi-purpose robots is an active area of research (e.g., Grupen <ref> [47, 46] </ref>). RL architectures, such as the one described here, offer a technique for using existing closed-loop behaviors as primitives by learning mixing functions to solve new complex tasks.
Reference: [47] <author> R. Grupen and R. Weiss. </author> <title> Integrated control for interpreting and manipulating the environment. </title> <journal> Robotica, </journal> <note> page accepted for publication, </note> <year> 1992. </year>
Reference-contexts: Determining stable behaviors and rules for composing them that are useful across a variety of complex tasks with multi-purpose robots is an active area of research (e.g., Grupen <ref> [47, 46] </ref>). RL architectures, such as the one described here, offer a technique for using existing closed-loop behaviors as primitives by learning mixing functions to solve new complex tasks.
Reference: [48] <author> V. Gullapalli. </author> <title> Reinforcement Learning and its application to control. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA 01003, </address> <year> 1992. </year>
Reference-contexts: Indeed, there is some evidence that RL algorithms may be faster than their only known competitor that is applicable with the same level of generality, namely classical DP methods (Barto and Singh [12, 11], Moore and Atkeson [79], Gullapalli <ref> [48] </ref>). The second misconception is the view that RL algorithms can only be used as weak methods. This misconception was perhaps generated inadvertently by the early developmental work on RL that used as illustrations applications with very little domain knowledge (Barto et.al. [13], Sutton [106]). <p> Equation 2.2). If the size of the action set A is large, finding the best action in a state can itself become computationally expensive and is the subject of current research (e.g., Gullapalli <ref> [48] </ref>). 15 complete agents in real-life environments. This dissertation will deal exclusively with learning tasks that can be formulated as MDTs. In particular, the next two chapters will present theoretical results about the application of DP and RL algorithms to abstract MDTs without reference to any real application. <p> There are other general approaches to scaling RL outside the categories considered here, e.g., using "shaping" techniques to guide the agent through a sequence of tasks of increasing difficulty culminating with the desired task (e.g., Gullapalli <ref> [48] </ref>), and hierarchical and modular RL architectures that use the principle of divide and conquer.
Reference: [49] <author> G.H. Hardy, J.E. Littlewood, and G. Polya. </author> <title> Inequalities. </title> <publisher> University Press, </publisher> <address> Cambridge, England, 2 edition, </address> <year> 1952. </year>
Reference-contexts: This section introduces a family of iterative approximation algorithms constructed by replacing the hard max operator in DP-based algorithms by "soft" generalized means <ref> [49] </ref> of order p (e.g., a non-linearly weighted l p norm). These soft DP algorithms converge to solutions that are more robust than those of classical DP. <p> Define A (p) = [ 1 n i=1 (a i ) p ] p , called a gener alized mean of order p. The following facts are proved in Hardy et al. <ref> [49] </ref> under the conditions that a i ; a 0 i 2 R + for all i, 1 i n. Fact 1. (Convergence) lim p!1 A (p) = A (max). Fact 2. (Differentiability) While @A (max) @a i is not defined, @A (p) n [ a i p &lt; 1.
Reference: [50] <author> P.E. Hart, N.J. Nilsson, and B. Rapahel. </author> <title> A formal basis for the heuristic determination of minimum cost paths. </title> <journal> IEEE Trans. Sys. Sci. Cybernet., </journal> <volume> SSC-4(2):100-107, </volume> <year> 1968. </year>
Reference-contexts: At the same time, techniques for reducing search in AI problem solving can play a role in making optimal control algorithms more efficient in their exploration of the solution space (Moore [77]). Another feature of AI problem solving algorithms, e.g., A fl (Hart et al. <ref> [50] </ref>, Nilsson [80]), that should be incorporated into optimal control algorithms is that of determining solutions only in parts of the problem space that matter. Algorithms from optimal control theory, such as DP, find complete optimal solutions that prescribe optimal actions to every possible state of the environment.
Reference: [51] <author> J.H. Holland, K.J. Holoyak, R.E. Nisbet, and P.R. Thagard. </author> <title> Induction: Processes of Inference, Learning and Discovery. </title> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Researchers within these diverse fields have developed a number of different methods under different names for solving RL tasks, e.g., dynamic programming (DP) algorithms (Bellman [16]), classifier systems (Holland et al. <ref> [51] </ref>), and reinforcement learning algorithms (Barto et al. [14], Werbos [121]). 1 The different algorithms assume different amounts of domain knowledge and work under different constraints, but they can all solve RL tasks and should perhaps all be called RL methods.
Reference: [52] <author> R. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1960. </year>
Reference-contexts: more finely asynchronous than the algorithms reviewed in this section because it can sample both in predecessor-state space and in action space. 3.3 A New Asynchronous Policy Iteration Algorithm An alternative classical DP method for solving the optimal control problem that converges in a finite number of iterations is Howard's <ref> [52] </ref> policy iteration method. 3 In policy iteration one computes a sequence of control policies and value functions as follows: 1 ! V 1 greedy ! 2 ! V 2 : : : n ! V n greedy ! n+1 , were PE ! is the policy evaluation operator that solves
Reference: [53] <author> T. Jaakkola, M.I. Jordan, and S.P. Singh. </author> <title> Stochastic convergence of iterative DP algorithms, </title> <note> 1993. Submitted to Neural Computation. </note>
Reference-contexts: Despite all the progress in connecting DP and RL algorithms, the following question was unanswered: can TD and Q-learning be derived by the straightforward application of some classical method for solving systems of equations? Recently this author and others (Singh et al. [102], Jaakkola et al. <ref> [53] </ref>, and Tsitsiklis [115]) have answered that question. In this dissertation it is shown that RL algorithms, such as TD and Q-learning, are instances of asynchronous stochastic approximation methods for solving the recursive system of equations associated with RL tasks. <p> An asymptotic convergence proof for the synchronous algorithm (Equation 4.4) can be easily derived from Dvoretzky's theorem presented in Appendix B. However, more recently, Jaakkola, Jordan and Singh <ref> [53] </ref> have derived the following theorem for the more general asynchronous case (of which the synchronous algorithm is a special case) by extending Dvoretzky's stochastic approximation results: Theorem 5: (Jaakkola, Jordan and Singh [53]) For finite MDPs, the algorithm defined by Equation 4.5 converges with probability one to V under the <p> However, more recently, Jaakkola, Jordan and Singh <ref> [53] </ref> have derived the following theorem for the more general asynchronous case (of which the synchronous algorithm is a special case) by extending Dvoretzky's stochastic approximation results: Theorem 5: (Jaakkola, Jordan and Singh [53]) For finite MDPs, the algorithm defined by Equation 4.5 converges with probability one to V under the following conditions: 1. every state in set X is updated infinitely often, 37 2. <p> The TD algorithm discussed above is just one algorithm from that family, specifically the TD (0) algorithm. See Jaakkola et al. <ref> [53] </ref> for a discussion of the connection between stochastic approximation and the general class of TD () algorithms. Also, see Section 5.1.4 in Chapter 5 of this dissertation for further discussion about TD () algorithms. <p> As in the case of policy evaluation, an asymptotic convergence proof for the synchronous version of Equation 4.10 can be derived in a straightforward manner from Dvoretzky's results. However, more recently, Jaakkola, Jordan, and Singh <ref> [53] </ref> have proved convergence for the asynchronous algorithm that subsumes the synchronous algorithm: Theorem 6: (Jaakkola, Jordan and Singh [53]) For finite MDPs, the algorithm defined in Equation 4.10 converge with probability one to Q fl if the following conditions hold true: 1. every state-action pair in (X fi A) is <p> However, more recently, Jaakkola, Jordan, and Singh <ref> [53] </ref> have proved convergence for the asynchronous algorithm that subsumes the synchronous algorithm: Theorem 6: (Jaakkola, Jordan and Singh [53]) For finite MDPs, the algorithm defined in Equation 4.10 converge with probability one to Q fl if the following conditions hold true: 1. every state-action pair in (X fi A) is updated infinitely often, 2.
Reference: [54] <author> R.A. Jacobs. </author> <title> Increased rates of convergence through learning rate adaptation. Neural Networks, </title> <type> 1, </type> <year> 1988. </year>
Reference-contexts: Therefore, the ability to achieve transfer of training across tasks must play a crucial role in building useful multi-task 2 Sutton [109] has adapted Kesten's method for adapting learning rates for individual parameters in a function approximator (see, also Jacobs <ref> [54] </ref>). 66 agent architectures based on RL. Multi-task agents can also achieve computational and monetary savings over multiple single-task agents simply by sharing hardware and software across tasks.
Reference: [55] <author> R.A. Jacobs. </author> <title> Task decomposition through competition in a modular connectionist architecture. </title> <type> PhD thesis, </type> <institution> COINS dept Univ. of Massachusetts, </institution> <address> Amherst, Mass. U.S.A., </address> <year> 1990. </year>
Reference-contexts: Transfer of training across tasks must play a crucial role in building autonomous embedded agents for complex real-world applications. Although studying architectures that solve multiple tasks is not a new idea (e.g. Korf [64], Jacobs <ref> [55] </ref>), achieving transfer of training within the RL framework requires the formulation of, and the solution to, several unique issues. <p> Section, a modular RL architecture is presented that simultaneously solves the composition problem for composite tasks and implements Equation 6.1. 6.2.1.1 CQ-L: The CQ-Learning Architecture The compositional Q-learning architecture, or CQ-L, is a modification and extension of Jacobs et.al.'s associative Gaussian mixture model (GMM) architecture described in Jacobs et al. <ref> [56, 55] </ref>. GMM consists of several expert modules and a gating module that has an output for each expert module. <p> This simulation shows that CQ-L is able to partition its "across-trial" experience and learn to engage a distinct Q-module for each elemental task. This is similar in spirit to the simulations reported by Jacobs <ref> [55] </ref>, except that he applies his 78 CQ-Learning Architecture One-for-One Architecture | | | | 0.0 | 200.0 Trial Number Avg. Steps per Trial were trained on the intermixed trials of the three elemental tasks T 1 , T 2 , and T 3 .
Reference: [56] <author> R.A. Jacobs, M.I. Jordan, S.J. Nowlan, and G.E. Hinton. </author> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3(1), </volume> <year> 1991. </year>
Reference-contexts: An advantage of using B () is that it requires less computation than the operator B (p). The operator B () is similar in spirit to the "soft-max" function (Bridle [21]) used by Jacobs et al. <ref> [56] </ref> and may provide a probabilistic framework for action selection in DP-based algorithms. Several researchers are investigating the advantages of combining nonlinear neural networks with traditional adaptive control techniques (e.g., [57, 42]). <p> Section, a modular RL architecture is presented that simultaneously solves the composition problem for composite tasks and implements Equation 6.1. 6.2.1.1 CQ-L: The CQ-Learning Architecture The compositional Q-learning architecture, or CQ-L, is a modification and extension of Jacobs et.al.'s associative Gaussian mixture model (GMM) architecture described in Jacobs et al. <ref> [56, 55] </ref>. GMM consists of several expert modules and a gating module that has an output for each expert module. <p> Simultaneously, the gating module is adjusted so that the a priori probability of selecting each Q-module becomes equal to the a posteriori probability of selecting that 3 The interested reader is referred to the descriptions and derivations of GMM in Jacobs et al. <ref> [56] </ref>, Nowlan [81], and Jordan and Jacobs [58]. 73 Q-module, given the estimated desired output. <p> Action Action Action Module 1 Module 2 Module Q Q Q o l e + + + + Bias Module 1 1 g 1 2 State State Q n nn . . . s State Gating Task White Noise N (0, s) Augmenting bits Augmenting bits K Jacobs et al. <ref> [56] </ref>. The Q-modules learn the Q-values for elemental tasks. The gating module has an output for each Q-module and determines the probability of selecting a particular Q-module. <p> Such subsequences can be learned through experience in learning to solve other tasks. This chapter presented CQ-L, an architecture that combines the Q-learning algorithm of Watkins [118] and the modular architecture of Jacobs et al. <ref> [56] </ref> to achieve transfer of training by sharing the solutions of elemental tasks across multiple composite tasks. 7 This assumes that the state representation is rich enough to distinguish repeated performances of the same elemental task.
Reference: [57] <author> M.I. Jordan and R.A. Jacobs. </author> <title> Learning to control an unstable system with forward modeling. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The operator B () is similar in spirit to the "soft-max" function (Bridle [21]) used by Jacobs et al. [56] and may provide a probabilistic framework for action selection in DP-based algorithms. Several researchers are investigating the advantages of combining nonlinear neural networks with traditional adaptive control techniques (e.g., <ref> [57, 42] </ref>). The algorithms presented in this section have the dual advantages of leading to more robust solutions and of employing a differentiable backup operator.
Reference: [58] <author> M.I. Jordan and R.A. Jacobs. </author> <title> Hierarchies of adaptive experts. </title> <editor> In J.E. Moody, S.J. Hanson, and R.P. Lippman, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 985-992. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Simultaneously, the gating module is adjusted so that the a priori probability of selecting each Q-module becomes equal to the a posteriori probability of selecting that 3 The interested reader is referred to the descriptions and derivations of GMM in Jacobs et al. [56], Nowlan [81], and Jordan and Jacobs <ref> [58] </ref>. 73 Q-module, given the estimated desired output. Because of the different initial values of the free parameters in the different Q-modules, over time, different Q-modules start winning the competition for different elemental tasks, and the gating module learns to select the appropriate Q-module for each elemental task.
Reference: [59] <author> M.I. Jordan and D.E. Rumelhart. </author> <title> Internal world models and supervised learning. </title> <editor> In L. Birnbaum and G. Collins, editors, </editor> <booktitle> Machine Learning: Proceedings of the Eighth International Workshop, </booktitle> <pages> pages 70-74, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Instead, the action to be executed at time step t is computed by adding Gaussian noise to the estimated greedy action in the current state. The greedy action in state x t is found by using a network inversion method (see Jordan and Rumelhart <ref> [59] </ref>). As learning proceeds the variance of the Gaussian noise is reduced over time so as to increase the likelihood of selecting the greedy action. Aside from this difference, the training algorithm for CQ-L is similar to the algorithm presented in Section 6.2.1.1.
Reference: [60] <author> L.P. Kaelbling. </author> <title> Learning in Embedded Systems. </title> <type> PhD thesis, </type> <institution> Stanford University, Department of Computer Science, Stanford, </institution> <address> CA, </address> <year> 1990. </year> <note> Technical Report TR-90-04. </note>
Reference-contexts: Recently, some AI researchers have turned attention away from studying isolated aspects of intelligence and towards studying intelligent behavior in complete agents embedded in real-world environments (e.g., Agre [1], Brooks [24, 23], and Kaelbling <ref> [60] </ref>). Much of this research on building embedded agents has followed the approach of hand-coding the agent's behavior (Maes [68]). <p> As a result the environment makes 3 Note that Figure 2.1 is just one possible block diagram representation; other researchers have used more complex block diagrams to capture some of the more subtle intricacies of tasks faced by embedded agents (e.g., Whitehead [125], Kaelbling <ref> [60] </ref>). 4 For ease of exposition it is assumed that the same set of actions are available to the agent in each state of the environment. <p> Another simple, yet effective, approach in the model-free case has been to use optimistic initial value functions (Sutton [107], Kaelbling <ref> [60] </ref>). In such a case, parts of the state space that have not been visited will have higher values than those that have been visited often. The greedy policy will then automatically explore unvisited regions of the state space. <p> If an action is neglected for too long its execution probability 62 is increased. The effect is to ensure that every action is executed infinitely often in each state for ergodic MDTs. Kaelbling <ref> [60] </ref> has developed an algorithm based on interval-estimation techniques that maintains confidence intervals for all actions. The action with the highest upper bound on expected payoff is selected for execution.
Reference: [61] <author> H. Kesten. </author> <title> Accelerated stochastic approximation. </title> <journal> Ann. Math. Statist., </journal> <volume> 29 </volume> <pages> 41-59, </pages> <year> 1958. </year> <month> 134 </month>
Reference-contexts: Most researchers optimize the parameters of the function f by trial and error. Another approach would be to use the experience of the agent to adapt the learning rate online. Kesten's <ref> [61] </ref> method for accelerating stochastic approximation can be adapted 65 to do on-line adaptation of individual learning rates. For each state, the sign of the last change in its value is stored.
Reference: [62] <author> D.E. Kirk. </author> <title> Optimal control theory: an introduction. </title> <address> Englewood Cliffs, </address> <year> 1970. </year>
Reference-contexts: This perspective of optimal control as search is the important common link to the view of problem solving as search developed within the AI community. For some optimal control problems, gradient-based search techniques, such as calculus of variations (e.g., Kirk <ref> [62] </ref>), can be used for finding the extrema. For other optimal control problems, where non-linearities or stochasticity make gradient-based search difficult, dynamic programming (DP) is the only known general class of algorithms for finding an optimal solution.
Reference: [63] <author> K.Narendra and M.A.L. Thathachar. </author> <title> Learning Automata: An Introduction. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: research in RL developed non-associative algorithms for solving single-stage Markovian decision tasks (MDTs) in environments with only one state, and had its roots in the work of psychologists working on mathematical learning theory (e.g., Bush and Mosteller [25]), and in the work of learning automata theorists (e.g., Narendra and Thatachar <ref> [63] </ref>). Later Barto et al. [9] developed an associative RL algorithm, they called the A RP algorithm, that solves single-stage MDTs with multiple-state environments. Single-stage MDTs do not involve the temporal credit assignment problem. Therefore algorithms for solving single-stage MDTs are unrelated to DP algorithms (except in the trivial sense).
Reference: [64] <author> R.E. Korf. </author> <title> Learning to Solve Problems by Searching for Macro-Operators. </title> <publisher> Pitman Publishers, </publisher> <address> Massachusetts, </address> <year> 1985. </year>
Reference-contexts: Both theoretical and empirical support are provided for each of the three new RL architectures. Transfer of training across tasks must play a crucial role in building autonomous embedded agents for complex real-world applications. Although studying architectures that solve multiple tasks is not a new idea (e.g. Korf <ref> [64] </ref>, Jacobs [55]), achieving transfer of training within the RL framework requires the formulation of, and the solution to, several unique issues. <p> AI researchers have long used macro-operators, which are labels for useful sequences of operators/actions, to build abstract models of the problem environment (e.g., Fikes et al. [41]). The problem solver uses such abstract environment-models to plan in terms of the macro-operators instead of the primitive operators (Korf <ref> [64] </ref>). Planning in the abstract model achieves temporal abstraction because it allows the problem solver to ignore unnecessary detail. This chapter extends the familiar idea of macro-operators developed for problem solving in deterministic domains into the reinforcement learning (RL) framework for solving stochastic tasks.
Reference: [65] <author> R.E. Korf. </author> <title> Real-time heuristic search. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 189-211, </pages> <year> 1990. </year>
Reference-contexts: At least in theory, there is no need to find optimal actions for states that are not on the set of optimal paths from the set of possible start states (see Korf <ref> [65] </ref>, and Barto et al. [10]). Following Sutton et al. [110] and Barto et al. [14], in this dissertation the adaptive optimal control framework is used to formulate tasks faced by autonomous embedded agents.
Reference: [66] <author> L.J. Lin. </author> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1993. </year>
Reference-contexts: However, RL architectures can easily incorporate many different kinds of domain knowledge. Indeed, a significant proportion of the current research on RL is about incorporating domain knowledge into RL architectures to alleviate some of their problems (Singh [99], Yee et al. [129], Mitchell and Thrun [75], Whitehead [125], Lin <ref> [66] </ref>, Clouse and Utgoff [28]). Despite the fact that under certain conditions RL algorithms may be the best available methods, conventional RL architectures are slow enough to make them impractical for many real-world problems. <p> Whitehead [125] has developed an architecture called learning by watching where an agent observes the behavior of an expert agent and shares its experiences. The learning agent does state-updates on the states experienced by the expert. Lin's <ref> [66] </ref> architecture stores the experience of the agent from the start to the goal state and performs repeated state-updates on the stored states. <p> Several researchers have achieved this by using function approximation methods other than lookup-tables, e.g., neural networks, to store and maintain the value function (cf. Chapter 6, see, also Lin <ref> [66] </ref>). Supervised learning methods such as the backpropagation algorithm (Rumelhart et al. [88]) can be used to update the parameters of the function approximator. Because these function approximators have fewer free parameters than do lookup-tables, each application of the update equation affects the value of a set of states. <p> Lin <ref> [66] </ref> developed an architecture that first learns elementary tasks and then learns how to compose them to solve more complex tasks. Lin's work assumes deterministic environments but does not assume the compositional structure on tasks assumed in this authors work. It also does not build models of the elementary tasks.
Reference: [67] <author> L.J. Lin and T.M. Mitchell. </author> <title> Reinforcement learning with hidden states. </title> <booktitle> In In Proceedings of the Second International Conference on Simulation of Adaptive Behavior: From Animals to Animats, </booktitle> <year> 1992. </year>
Reference-contexts: Researchers are currently developing methods that attempt to build state information by either memorizing past perceptions, or by controlling the perceptual system of the agent to generate multiple perceptions (Whitehead and Ballard [126], Lin and Mitchell <ref> [67] </ref>, Chrisman [27], and McCallum [73]). In both cases the hope is that techniques other than RL can be used to convert a non-Markovian problem into a Markovian one so that conventional RL can be applied.
Reference: [68] <editor> P. Maes, editor. </editor> <title> Designing Autonomous Agents: Theory and Practice from Biology to Engineering and Back. </title> <address> MIT/Elsevier, </address> <year> 1991. </year>
Reference-contexts: Much of this research on building embedded agents has followed the approach of hand-coding the agent's behavior (Maes <ref> [68] </ref>). The success of such agents has depended heavily on their designers' prior knowledge of the dynamics of the interaction between the agent and its intended environment and on the careful choice of the agent's repertoire of behaviors. Such hand-coded agents lack flexibility and robustness.
Reference: [69] <author> P. Maes and R. Brooks. </author> <title> Learning to coordinate behaviours. </title> <booktitle> In Proceedings of the Eighth AAAI, </booktitle> <pages> pages 796-802. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: In most implementations of the subsumption architecture both the task-achieving modules as well as the switching circuitry are hardwired by the agent designer. Maes and Brooks <ref> [69] </ref> showed how reinforcement learning can be used to learn the switching circuitry for a robot with hardwired task modules. Mahadevan and Connell [71], on the other hand, showed how Q-learning can be used to acquire behaviors that can then be controlled using a hardwired switching scheme. <p> The learning architecture presented in this section is called BB-RL, for behavior-based RL, because it uses RL to learn an optimal policy that maps states to a mixture of Dirichlet and Neumann behaviors. Maes and Brooks <ref> [69] </ref> used a simple form of RL to learn the switching circuitry for a walking robot with hardwired behavior modules. <p> Their formulation of the RL problem was as a single stage decision task in which the learner's goal was to select at each time step the behavior that maximizes the immediate payoff. BB-RL extends Maes and Brooks' <ref> [69] </ref> system to multi-stage decision tasks and to policies that assign linear combinations of behaviors to states instead of a single behavior to each state. (see, also Gullapalli et al. [117]). 8.3 Simulation Results in this section.
Reference: [70] <author> S. Mahadevan. </author> <title> Enhancing transfer in reinforcement learning by building stochastic models of robot actions. </title> <booktitle> In Machine Learning: Proceedings of the Ninth International Conference, </booktitle> <pages> pages 290-299. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: transfer of knowledge that is invariant 101 (1:1) (10:1) 0 5020 10040 | 1000.0 | 3000.0 Number of backups Absolute error in Value function increasing the ration of the number of backups in M -2 to the number of backups in the real environment. 102 across tasks (see, also Mahadevan <ref> [70] </ref>), and the computational expense of building the model can be amortized across the set of tasks. In both the single and multiple task contexts, the nature of the model will play a role in determining its usefulness.
Reference: [71] <author> S. Mahadevan and J. Connell. </author> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <type> Technical report, </type> <institution> IBM Research Division, T.J.Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <year> 1990. </year>
Reference-contexts: In most implementations of the subsumption architecture both the task-achieving modules as well as the switching circuitry are hardwired by the agent designer. Maes and Brooks [69] showed how reinforcement learning can be used to learn the switching circuitry for a robot with hardwired task modules. Mahadevan and Connell <ref> [71] </ref>, on the other hand, showed how Q-learning can be used to acquire behaviors that can then be controlled using a hardwired switching scheme. <p> The closed-loop Dirichlet and Neumann policies are examples of goal-seeking behaviors. In most behavior-based architectures for robots the switching circuitry and the behaviors are designed by the roboticist. (see Mahadevan and Connell <ref> [71] </ref> for an exception that learns behaviors but has fixed switching circuitry). The learning architecture presented in this section is called BB-RL, for behavior-based RL, because it uses RL to learn an optimal policy that maps states to a mixture of Dirichlet and Neumann behaviors.
Reference: [72] <author> M.J. Mataric. </author> <title> A comparative analysis of reinforcement learning methods. </title> <type> Technical report, </type> <institution> M.I.T., </institution> <year> 1991. </year> <note> A.I. Memo No.1322. </note>
Reference-contexts: In fact, the common view is that RL algorithms can only be used as weak learning algorithms in the AI sense, i.e., they can use little domain knowledge, and hence like all weak learning algorithms are doomed to scale poorly to complex tasks (e.g., Mataric <ref> [72] </ref>). 2 Within theoretical computer science, the term DP is applied to a general class of methods for efficiently solving recursive systems of equations for many different kinds of structured optimization problems (e.g., Cormen et al. [32]), not just the recursive equations derived for agents controlling external environments.
Reference: [73] <author> R.A. McCallum. </author> <title> Overcoming incomplete perception with utile distinction memory. </title> <editor> In P. Utgoff, editor, </editor> <booktitle> Machine Learning: Proceedings of the Tenth International Conference, </booktitle> <pages> pages 190-196. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Researchers are currently developing methods that attempt to build state information by either memorizing past perceptions, or by controlling the perceptual system of the agent to generate multiple perceptions (Whitehead and Ballard [126], Lin and Mitchell [67], Chrisman [27], and McCallum <ref> [73] </ref>). In both cases the hope is that techniques other than RL can be used to convert a non-Markovian problem into a Markovian one so that conventional RL can be applied. State estimation techniques developed in control engineering, e.g., Kalman filters, can also be used in conjunction with RL.
Reference: [74] <author> D.V. McDermott. </author> <title> Planning and acting. </title> <journal> Cognitive Science, </journal> <volume> 2, </volume> <year> 1978. </year>
Reference-contexts: As stated before, part of the motivation for RL researchers has been to develop direct methods for solving learning tasks involving embedded agents. 1 Some of the recent work on planning produces closed-loop plans by performing a cycle of sensing and open-loop planning (e.g., McDermott <ref> [74] </ref>). 9 2.3 Learning and Optimal Control Of more relevance to the theory of learning agents embedded in dynamic environments is a class of control problems studied by optimal control theorists in which the desired state trajectory is not known in advance but is part of the solution to be determined
Reference: [75] <author> T.M. Mitchell and S.B. Thrun. </author> <title> Explanation-based neural network learning for robot control. </title> <editor> In S.J. Hanson, J.D. Cowan, and C.L. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 287-294. </pages> <publisher> Morgan-Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: However, RL architectures can easily incorporate many different kinds of domain knowledge. Indeed, a significant proportion of the current research on RL is about incorporating domain knowledge into RL architectures to alleviate some of their problems (Singh [99], Yee et al. [129], Mitchell and Thrun <ref> [75] </ref>, Whitehead [125], Lin [66], Clouse and Utgoff [28]). Despite the fact that under certain conditions RL algorithms may be the best available methods, conventional RL architectures are slow enough to make them impractical for many real-world problems. <p> After every training episode, a form of explanation-based generalization (Mitchell et al. [76]) is used to determine a set of predecessor states that should have the same value. Any errors in generalization are handled via a mechanism for storing exceptions to concepts. Mitchell and Thrun <ref> [75] </ref> extended this approach to situations where a symbolic domain theory may be unavailable. They use on-line learning experiences to estimate a neural-network based environment model. Network inversion techniques are used to determine the slope of the value function in a local region around the predecessor state.
Reference: [76] <author> Tom M. Mitchell, Richard Keller, and S. Kedar-Cabelli. </author> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 47-80, </pages> <year> 1986. </year>
Reference-contexts: Yee et al. [129] developed a method that uses a symbolic domain theory to do structural generalization. After every training episode, a form of explanation-based generalization (Mitchell et al. <ref> [76] </ref>) is used to determine a set of predecessor states that should have the same value. Any errors in generalization are handled via a mechanism for storing exceptions to concepts. Mitchell and Thrun [75] extended this approach to situations where a symbolic domain theory may be unavailable.
Reference: [77] <author> A.W. Moore. </author> <type> personal communication. 135 </type>
Reference-contexts: At the same time, techniques for reducing search in AI problem solving can play a role in making optimal control algorithms more efficient in their exploration of the solution space (Moore <ref> [77] </ref>). Another feature of AI problem solving algorithms, e.g., A fl (Hart et al. [50], Nilsson [80]), that should be incorporated into optimal control algorithms is that of determining solutions only in parts of the problem space that matter.
Reference: [78] <author> A.W. Moore. </author> <title> Variable resolution dynamic programming: Efficiently learning action maps in multivariate real-valued state-spaces. In L.A. </title> <editor> Birnbaum and G.C. Collins, editors, </editor> <booktitle> Maching Learning: Proceedings of the Eighth International Workshop, </booktitle> <pages> pages 333-337, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Another technique for improving the quality of information returned by a backup is to use system identification methods to estimate a model on-line and to use algorithms that employ full backups on the estimated model. Several researchers have used such indirect control methods to solve RL tasks (e.g., Moore <ref> [78] </ref>) and it is also the usual method in classical Markov chain control (e.g., Sato et al. [91]). <p> The value function network is then trained to implement that slope. Both Yee et al. and Mitchell and Thrun demonstrate greatly accelerated learning. Other researchers have developed approaches that start with a coarse resolution model of the environment and selectively increase the resolution where it is needed. Moore <ref> [78] </ref> uses the trie data structure to store a coarse environment model. A DP method is used to find a good solution to the abstract problem defined by the coarse model.
Reference: [79] <author> A.W. Moore and C.G. Atkeson. </author> <title> Prioritized sweeping: Reinforcement learning with less data and less real time. </title> <journal> Machine Learning, </journal> <volume> 13(1), </volume> <month> October </month> <year> 1993. </year>
Reference-contexts: Indeed, there is some evidence that RL algorithms may be faster than their only known competitor that is applicable with the same level of generality, namely classical DP methods (Barto and Singh [12, 11], Moore and Atkeson <ref> [79] </ref>, Gullapalli [48]). The second misconception is the view that RL algorithms can only be used as weak methods. This misconception was perhaps generated inadvertently by the early developmental work on RL that used as illustrations applications with very little domain knowledge (Barto et.al. [13], Sutton [106]). <p> Part of the motivation behind this current research is to combine the complementary strengths of research on planning and problem solving in AI and of research on DP in optimal control to get the best of both worlds (e.g., Sutton [108], Barto et al. [10], and Moore and Atkeson <ref> [79] </ref>). For example, techniques for dealing with uncertainty and stochasticity developed in control theory are now of interest to AI researchers developing architectures for agents embedded in real-world environments. <p> In both subcases a and b there is still the question of exploration, only unlike the model-free case the agent is not constrained by the dynamics of the environment. Moore and Atkeson <ref> [79] </ref> and Peng and Williams [82] independently developed an algorithm that estimates an inverse model and uses it to maintain a priority queue of states.
Reference: [80] <author> N.J. Nilsson. </author> <booktitle> Problem-Solving Methods in Artificial Intelligence. </booktitle> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: At the same time, techniques for reducing search in AI problem solving can play a role in making optimal control algorithms more efficient in their exploration of the solution space (Moore [77]). Another feature of AI problem solving algorithms, e.g., A fl (Hart et al. [50], Nilsson <ref> [80] </ref>), that should be incorporated into optimal control algorithms is that of determining solutions only in parts of the problem space that matter. Algorithms from optimal control theory, such as DP, find complete optimal solutions that prescribe optimal actions to every possible state of the environment.
Reference: [81] <author> S.J. Nowlan. </author> <title> Competing experts: An experimental investigation of associative mixture models. </title> <type> Technical Report CRG-TR-90-5, </type> <institution> Department of Computer Sc., Univ. of Toronto, Toronto, Canada, </institution> <year> 1990. </year>
Reference-contexts: Simultaneously, the gating module is adjusted so that the a priori probability of selecting each Q-module becomes equal to the a posteriori probability of selecting that 3 The interested reader is referred to the descriptions and derivations of GMM in Jacobs et al. [56], Nowlan <ref> [81] </ref>, and Jordan and Jacobs [58]. 73 Q-module, given the estimated desired output.
Reference: [82] <author> J. Peng and R.J. Williams. </author> <title> Efficient learning and planning within the dyna framework. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 1(4) </volume> <pages> 437-454, </pages> <month> Spring </month> <year> 1993. </year>
Reference-contexts: In both subcases a and b there is still the question of exploration, only unlike the model-free case the agent is not constrained by the dynamics of the environment. Moore and Atkeson [79] and Peng and Williams <ref> [82] </ref> independently developed an algorithm that estimates an inverse model and uses it to maintain a priority queue of states. The states in the priority queue are ordered by the estimated magnitude by which their value would change if the update equation were applied to those states.
Reference: [83] <author> M.L. Puterman and S.L. Brumelle. </author> <title> The analytic theory of policy iteration. In Dynamic Programming and its Applications, </title> <address> New York, 1978. </address> <publisher> Academic Press. </publisher>
Reference-contexts: Notice that the PE ! operator is not by itself a local operator, even though it can be solved by repeated application of local operators as 3 Puterman and Brumelle <ref> [83] </ref> have shown that policy iteration is a Newton-Raphson method for solving for the Bellman equation. 24 shown in Section 3.2.1. The operator greedy ! computes the policy that is greedy with respect to the value function on the left-hand side.
Reference: [84] <author> M.L. Puterman and M.C. Shin. </author> <title> Modified policy iteration algorithms for discounted markov decision problems. </title> <journal> Management Science, </journal> <volume> 24(11), </volume> <month> July </month> <year> 1978. </year>
Reference-contexts: Puterman and Shin <ref> [84] </ref> have shown that it is more appropriate to think of the two classical methods of policy iteration and value iteration as two extremes of a continuation of iterative methods, which they called modified policy iteration (M-PI). <p> This dissertation developed a new asynchronous algorithm that allows the agent to sample both in predecessor state space as well as in action space. It was shown that the new algorithm converges under more general initial conditions than the related algorithms of modified policy iteration (Puterman and Shin <ref> [84] </ref>) and the asynchronous algorithms developed by Williams and Baird [127]. Stochastic Approximation Framework for RL: A hitherto unknown connection between stochastic approximation theory and RL algorithms such as TD and Q-learning was developed.
Reference: [85] <author> R.L. Rivest. </author> <title> Game tree searching by min/max approximation. </title> <journal> Artificial Intelligence, </journal> <volume> 34 </volume> <pages> 77-96, </pages> <year> 1988. </year>
Reference-contexts: Being able to compute derivatives allows sensitivity analysis and may lead to some new ways of addressing the difficult exploration versus exploitation issue [107] in optimal control tasks. Indeed, the motivation for Rivest's work <ref> [85] </ref>, which inspired the development of the algorithms presented in this section, was to use sensitivity analysis to address the analogous exploration issue in game tree search. Note that derivatives of the values of states with respect to the transition probabilities and the immediate payoffs can also be derived. <p> Note that derivatives of the values of states with respect to the transition probabilities and the immediate payoffs can also be derived. As discussed by Rivest <ref> [85] </ref>, other forms of generalized means exist, e.g., for any continuous monotone increasing function, f , one can consider mean values of the form f 1 ( 1 n i=1 f (a i )).
Reference: [86] <author> H. Robbins and S. Monro. </author> <title> A stochastic approximation method. </title> <journal> Ann. Math. Stat., </journal> <volume> 22 </volume> <pages> 400-407, </pages> <year> 1951. </year>
Reference-contexts: In this chapter, the connection between RL and MDTs is made precise. But first, a brief detour has to be taken to explain the Robbins-Monro <ref> [86] </ref> stochastic approximation method for solving systems of equations. Stochastic approximation theory forms the basis for connecting RL and DP. 34 4.2 Stochastic Approximation for Solving Systems of Equations Consider the problem of solving a scalar equation G (V ) = 0, where V is a scalar variable. <p> Further suppose that for any V we can observe Y (V ) = G (V ) + *, where * represents some random error with mean zero and variance 2 &gt; 0. Robbins and Monro <ref> [86] </ref> suggested using the following recursion V k+1 = V k k Y (V k ); (4.2) where f k g are positive constants, such that P k &lt; 1, and k = 1, and proved convergence in probability for Equation 4.2 to the root of G under the conditions that
Reference: [87] <author> S. Ross. </author> <title> Introduction to Stochastic Dynamic Programming. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: This dissertation will focus on DP and RL algorithms. Nevertheless, it should be noted that no definitive comparison has yet been made between optimization methods based on genetic algorithms and RL or DP algorithms. 4 a system of nonlinear recursive equations (Ross <ref> [87] </ref>, Bertsekas [17]). <p> Fortunately, infinite-horizon MDTs are simpler to solve than finite-horizon MDTs because with an infinite horizon there always exists a policy that is independent of time, called a stationary policy, that is optimal (see Ross <ref> [87] </ref>). Therefore, throughout this dissertation one need only consider stationary policies : X ! A that assign an action to each state. <p> It can be shown that the following system of linear fixed-point equations of size jXj, written in vector form: V = R + fl [P ] V (2.3) always has a unique solution, and that the solution is V , under the assumption that R is finite (Ross <ref> [87] </ref>). * Optimal Control: The control problem for an MDT, shown in Figure 2.3, is that of finding an optimal control policy fl . The search for an optimal policy has to be conducted only in the set of stationary policies, denoted P, that is of size jAj jXj . <p> The value function for an optimal policy fl is called the optimal value function and is denoted V fl . There may be more than one optimal policy, 14 but the optimal value function is always unique (Ross <ref> [87] </ref>).
Reference: [88] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D.E. Rumelhart and J.L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, vol.1: Foundations. </booktitle> <publisher> Bradford Books/MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: jAj a2A &lt; " P V (x) z2X @V (z) 9 ; Note that one can use the chain rule to compute the above derivatives for states that are not neighbors in the state graph of the MDT in much the same way as the 32 backpropagation (Rumelhart et al. <ref> [88] </ref>, Werbos [120]) algorithm for multi-layer connectionist networks. Being able to compute derivatives allows sensitivity analysis and may lead to some new ways of addressing the difficult exploration versus exploitation issue [107] in optimal control tasks. <p> Several researchers have achieved this by using function approximation methods other than lookup-tables, e.g., neural networks, to store and maintain the value function (cf. Chapter 6, see, also Lin [66]). Supervised learning methods such as the backpropagation algorithm (Rumelhart et al. <ref> [88] </ref>) can be used to update the parameters of the function approximator. Because these function approximators have fewer free parameters than do lookup-tables, each application of the update equation affects the value of a set of states. The set will depend on the generalization bias of the function approximator. <p> Aside from this difference, the training algorithm for CQ-L is similar to the algorithm presented in Section 6.2.1.1. The weights of the networks are trained by using the backpropagation algorithm of Rumelhart et al. <ref> [88] </ref>. 6.4.2 Simulation Results As before, task commands were represented by standard unit basis vectors (Table 6.1), and thus the architecture could not "parse" the task command to solve the composition problem for a composite task.
Reference: [89] <editor> D.E. Rumelhart and J.L. McClelland, editors. </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol.1: Foundations, </booktitle> <volume> Vol. </volume> <month> 2: </month> <title> Psychological and Biological models. </title> <publisher> Bradford Books/MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: The parameters of the network can be adapted. Several learning rules based on gradient descent have been developed for adapting the parameters. A good place for learning about artificial neural networks and about learning rules is Rumelhart et al.'s <ref> [89] </ref> 1986 book on parallel distributed processing. A subset of nodes are called input nodes and their activations are set from the outside. Some special nodes are called output nodes and their activation can be read by the outside world.
Reference: [90] <author> E.D. Sacerdoti. </author> <title> Planning in a hierarchy of abstraction spaces. </title> <journal> Artificial Intelligence, </journal> <volume> 5 </volume> <pages> 115-135, </pages> <year> 1974. </year>
Reference-contexts: Research on abstraction hierarchies in AI focused on deterministic domains and assumed that the problem solver was provided apriori with a hierarchy of state-space models of the problem environment (Sacerdoti <ref> [90] </ref>). The main contribution of this chapter is in extending the advantages of using a hierarchy of state-space models of the environment to RL agents that are embedded in stochastic environments and that learn a hierarchy of environment-models on-line.
Reference: [91] <author> M. Sato, K. Abe, and H. Takeda. </author> <title> Learning control of finite markov chains with unknown transition probabilities. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 27 </volume> <pages> 502-505, </pages> <year> 1982. </year> <month> 136 </month>
Reference-contexts: Several researchers have used such indirect control methods to solve RL tasks (e.g., Moore [78]) and it is also the usual method in classical Markov chain control (e.g., Sato et al. <ref> [91] </ref>). <p> The greedy policy will then automatically explore unvisited regions of the state space. Barto and Singh [12] developed an algorithm that keeps a frequency count of how often each action is executed in each state (see also Sato et al. <ref> [91] </ref>). If an action is neglected for too long its execution probability 62 is increased. The effect is to ensure that every action is executed infinitely often in each state for ergodic MDTs. Kaelbling [60] has developed an algorithm based on interval-estimation techniques that maintains confidence intervals for all actions.
Reference: [92] <author> L. Schmetterer. </author> <title> Stochastic approximation. </title> <booktitle> In Proceedings of the fourth Berkeley Symposium on Mathematics and Probability, </booktitle> <pages> pages 587-609, </pages> <year> 1960. </year>
Reference-contexts: Equation 4.2 will play an essential role in establishing the connection between RL and DP in Section 4.3. Following Robbin and Monro's work, several authors extended their results to multi-dimensional equations and derived convergence with probability one under weaker conditions (Blum [19], Dvoretzky [39], Schmetterer <ref> [92] </ref>). Appendix B presents a theorem by Dvoretzky [39] that is more complex but more closely related to the material presented in the following sections. 4.3 Reinforcement Learning Algorithms This section uses Equation 4.2 to derive stochastic approximation algorithms to solve the policy evaluation and optimal control problems.
Reference: [93] <author> J.H. Schmidhuber. </author> <title> A possibility for implementing curiosity and boredom in model-building neural controllers. </title> <booktitle> In From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 222-227, </pages> <address> Cambridge, MA, 1991. </address> <publisher> MIT Press. </publisher>
Reference-contexts: In subcase a, the the agent must explore the real environment to construct the model efficiently and that can conflict with exploitation, raising a different kind of exploration versus exploitation tradeoff. Schmidhuber <ref> [93] </ref> and Thrun and Moller [114] have developed methods that explicitly estimate the accuracy of the learned model and execute actions taking the agent to those parts of the state space where the model has a low accuracy.
Reference: [94] <author> A. Schwartz. </author> <title> A reinforcement learning method for maximizing undiscounted rewards. </title> <booktitle> In Proceedings of the Tenth Machine Learning Conference, </booktitle> <year> 1993. </year>
Reference-contexts: In such a case, R a t (x t ) : 6 The average payoff per time step received by an agent is another measure for policies that is used in the classical DP literature (Bertsekas [17]), and more recently in the RL literature (Schwartz <ref> [94] </ref>, Singh [100]). This dissertation will only deal with the discounted measure for policies. 12 be modeled as MDTs by discretizing the state space and choosing actions at some fixed frequency. However, it is important to keep in mind that an MDT is only an abstraction of the physical task.
Reference: [95] <author> S.P. Singh. </author> <title> Transfer of learning across compositions of sequential tasks. </title> <editor> In L. Birnbaum and G. Collins, editors, </editor> <booktitle> Machine Learning: Proceedings of the Eighth International Workshop, </booktitle> <pages> pages 348-352, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The architecture achieves transfer of training by constructing the value function for a complex task by computationally efficient modifications to the value functions of tasks that are lower in the hierarchy. The material presented in this chapter is also published in Singh <ref> [99, 96, 95] </ref>. 6.1 Compositionally-Structured Markovian Decision Tasks Much of everyday human activity involves multi-stage decision tasks that have compositional structure, i.e., complex tasks are built up in a systematic way from simpler subtasks. As an example consider the routine task of driving to work. <p> Solving new tasks by doing state-updates in abstract models can lead to low-depth solutions and that can accelerate convergence to the optimal value function. 7.8.1 Subsequent Related Work After the publication of this author's work (Singh <ref> [95, 98, 97] </ref>) on CQ-L and H-DYNA, other authors have developed hierarchical RL architectures that make different assumptions and have different strengths and weaknesses. Lin [66] developed an architecture that first learns elementary tasks and then learns how to compose them to solve more complex tasks.
Reference: [96] <author> S.P. Singh. </author> <title> The efficient learning of multiple task sequences. </title> <editor> In J.E. Moody, S.J. Hanson, and R.P. Lippman, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 251-258, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kauffman. </publisher>
Reference-contexts: The architecture achieves transfer of training by constructing the value function for a complex task by computationally efficient modifications to the value functions of tasks that are lower in the hierarchy. The material presented in this chapter is also published in Singh <ref> [99, 96, 95] </ref>. 6.1 Compositionally-Structured Markovian Decision Tasks Much of everyday human activity involves multi-stage decision tasks that have compositional structure, i.e., complex tasks are built up in a systematic way from simpler subtasks. As an example consider the routine task of driving to work.
Reference: [97] <author> S.P. Singh. </author> <title> Reinforcement learning with a hierarchy of abstract models. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 202-207, </pages> <address> San Jose,CA, July 1992. </address> <publisher> AAAI Press/MIT Press. </publisher>
Reference-contexts: Transfer of training is achieved by sharing the abstract environment model learned while solving the elemental tasks across multiple composite tasks. The material presented in this chapter is also published in Singh <ref> [98, 97] </ref>. 7.1 Hierarchy of Environment Models Building abstract models to speed up the process of learning the value function in RL tasks is not in itself a new idea. <p> Solving new tasks by doing state-updates in abstract models can lead to low-depth solutions and that can accelerate convergence to the optimal value function. 7.8.1 Subsequent Related Work After the publication of this author's work (Singh <ref> [95, 98, 97] </ref>) on CQ-L and H-DYNA, other authors have developed hierarchical RL architectures that make different assumptions and have different strengths and weaknesses. Lin [66] developed an architecture that first learns elementary tasks and then learns how to compose them to solve more complex tasks.
Reference: [98] <author> S.P. Singh. </author> <title> Scaling reinforcement learning algorithms by learning variable temporal resolution models. </title> <editor> In D. Sleeman and P. Edwards, editors, </editor> <booktitle> Proceedings of the Ninth Machine Learning Conference, </booktitle> <pages> pages 406-415, </pages> <year> 1992. </year>
Reference-contexts: Transfer of training is achieved by sharing the abstract environment model learned while solving the elemental tasks across multiple composite tasks. The material presented in this chapter is also published in Singh <ref> [98, 97] </ref>. 7.1 Hierarchy of Environment Models Building abstract models to speed up the process of learning the value function in RL tasks is not in itself a new idea. <p> Solving new tasks by doing state-updates in abstract models can lead to low-depth solutions and that can accelerate convergence to the optimal value function. 7.8.1 Subsequent Related Work After the publication of this author's work (Singh <ref> [95, 98, 97] </ref>) on CQ-L and H-DYNA, other authors have developed hierarchical RL architectures that make different assumptions and have different strengths and weaknesses. Lin [66] developed an architecture that first learns elementary tasks and then learns how to compose them to solve more complex tasks.
Reference: [99] <author> S.P. Singh. </author> <title> Transfer of learning by composing solutions for elemental sequential tasks. </title> <journal> Machine Learning, </journal> 8(3/4):323-339, May 1992. 
Reference-contexts: However, RL architectures can easily incorporate many different kinds of domain knowledge. Indeed, a significant proportion of the current research on RL is about incorporating domain knowledge into RL architectures to alleviate some of their problems (Singh <ref> [99] </ref>, Yee et al. [129], Mitchell and Thrun [75], Whitehead [125], Lin [66], Clouse and Utgoff [28]). Despite the fact that under certain conditions RL algorithms may be the best available methods, conventional RL architectures are slow enough to make them impractical for many real-world problems. <p> The architecture achieves transfer of training by constructing the value function for a complex task by computationally efficient modifications to the value functions of tasks that are lower in the hierarchy. The material presented in this chapter is also published in Singh <ref> [99, 96, 95] </ref>. 6.1 Compositionally-Structured Markovian Decision Tasks Much of everyday human activity involves multi-stage decision tasks that have compositional structure, i.e., complex tasks are built up in a systematic way from simpler subtasks. As an example consider the routine task of driving to work.
Reference: [100] <author> S.P. Singh. </author> <title> New reinforcement learning algorithms for maximizing average payoff, </title> <note> 1993. Submitted. </note>
Reference-contexts: In such a case, R a t (x t ) : 6 The average payoff per time step received by an agent is another measure for policies that is used in the classical DP literature (Bertsekas [17]), and more recently in the RL literature (Schwartz [94], Singh <ref> [100] </ref>). This dissertation will only deal with the discounted measure for policies. 12 be modeled as MDTs by discretizing the state space and choosing actions at some fixed frequency. However, it is important to keep in mind that an MDT is only an abstraction of the physical task.
Reference: [101] <author> S.P. Singh. </author> <title> Soft dynamic programming algorithms: Convergence proofs, </title> <note> 1993. Poster at CLNL93. </note>
Reference-contexts: It is known that 9* &gt; 0 such that if jjV V fl jj 1 * then the greedy policy with respect to V is optimal (Singh <ref> [101] </ref>). Because the sequence fV k g is non-decreasing it can be concluded that 1 2 f fl g. Q.E.D.
Reference: [102] <author> S.P. Singh, A.G. Barto, M.I. Jordan, and T. Jaakkolla. </author> <title> Understanding reinforcement learning. </title> <note> in preparation. </note>
Reference-contexts: Despite all the progress in connecting DP and RL algorithms, the following question was unanswered: can TD and Q-learning be derived by the straightforward application of some classical method for solving systems of equations? Recently this author and others (Singh et al. <ref> [102] </ref>, Jaakkola et al. [53], and Tsitsiklis [115]) have answered that question. In this dissertation it is shown that RL algorithms, such as TD and Q-learning, are instances of asynchronous stochastic approximation methods for solving the recursive system of equations associated with RL tasks.
Reference: [103] <author> S.P. Singh and R.C. Yee. </author> <title> An upper bound on the loss from approximate optimal-value functions. </title> <journal> Machine Learning. </journal> <note> to appear. </note>
Reference-contexts: These results also provide a theoretical justification for a practice that is common in RL. The material presented in this section was developed in collaboration with Yee and is reported in Singh and Yee <ref> [103] </ref>. Policies can be derived from value functions in a straightforward way.
Reference: [104] <author> B.F. Skinner. </author> <title> The Behavior of Organisms: An experimental analysis. </title> <editor> D. </editor> <booktitle> Appleton Century, </booktitle> <address> New York, </address> <year> 1938. </year>
Reference-contexts: This roughly corresponds to the "shaping" procedures used by psychologists to train animals to do complex motor tasks (see Skinner <ref> [104] </ref>). A simple simulation to illustrate shaping was constructed by training CQ-L with one Q-module on one elemental task, T 3 , for 1; 000 trials and then training on the composite task C 2 .
Reference: [105] <author> R.S. Sutton. </author> <title> Temporal Credit Assignment in Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1984. </year>
Reference-contexts: independently for each state because an optimal action in any state is simply an action that leads to the highest immediate payoff, i.e., fl (x) = argmax a2A R a (x) (2:2) MDTs with a horizon greater than one, or multi-stage MDTs, face a difficult temporal credit assignment problem (Sutton <ref> [105] </ref>) because actions executed in the short-term can have long-term consequences on the payoffs received by the agent. Hence, to search for an optimal action in a state it may be necessary to examine the consequences of all action sequences of length equal to the horizon of the MDT. <p> In the early 1980s, in a landmark paper Barto et al. [13] described a technique for addressing the temporal credit assignment problem that culminated in Sutton's [106] paper on a class of techniques he called temporal difference (TD) methods (see also Sutton <ref> [105] </ref>). In the late 1980s, Watkins [118] observed that the TD algorithm solves the linear policy evaluation problem for multi-stage MDTs (see, also Dayan [33], Barnard [5]).
Reference: [106] <author> R.S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year> <month> 137 </month>
Reference-contexts: One of the main innovations in RL algorithms for solving problems traditionally solved by DP is that they are model-free because they do not require a model of the environment. Examples of such model-free RL algorithms are Sutton's <ref> [106] </ref> temporal differences (TD) algorithm and Watkins [118] Q-learning algorithm. <p> The second misconception is the view that RL algorithms can only be used as weak methods. This misconception was perhaps generated inadvertently by the early developmental work on RL that used as illustrations applications with very little domain knowledge (Barto et.al. [13], Sutton <ref> [106] </ref>). However, RL architectures can easily incorporate many different kinds of domain knowledge. <p> Therefore algorithms for solving single-stage MDTs are unrelated to DP algorithms (except in the trivial sense). In the early 1980s, in a landmark paper Barto et al. [13] described a technique for addressing the temporal credit assignment problem that culminated in Sutton's <ref> [106] </ref> paper on a class of techniques he called temporal difference (TD) methods (see also Sutton [105]). In the late 1980s, Watkins [118] observed that the TD algorithm solves the linear policy evaluation problem for multi-stage MDTs (see, also Dayan [33], Barnard [5]). <p> (x) (R (x) (x) + flV k (y))); for x 2 S k The on-line version of Equation 4.5, i.e., where the sets S t = fx t g contain only the current state of the environment is identical to the TD (0) algorithm commonly used in RL applications (Sutton <ref> [106] </ref>). An asymptotic convergence proof for the synchronous algorithm (Equation 4.4) can be easily derived from Dvoretzky's theorem presented in Appendix B. <p> V 0 is finite, and 3. 8x 2 X; i=0 i (x) = 1 and i=0 2 Sutton <ref> [106] </ref> has defined a family of temporal difference algorithms called TD (), where 0 1 is a scalar parameter. The TD algorithm discussed above is just one algorithm from that family, specifically the TD (0) algorithm. <p> Q 0 is finite, and 3. 8 (x; a) 2 (X fi A); i=0 i (x; a) = 1 and i=0 2 4.3.3 Discussion Convergence proofs for TD and Q-learning that do not use stochastic approximation theory already existed in the RL literature (Sutton <ref> [106] </ref> and Dayan [33] for TD, and Watkins [118] and Watkins and Dayan [119] for Q-learning). But these proofs, especially the one for Q-learning, are based on special mathematical constructions that obscure the underlying simplicity of the algorithms. <p> Specifically, the extension is in deriving an upper bound on performance loss which is slightly tighter than that 1 Sutton <ref> [106] </ref> and Dayan [33] have proved convergence for the TD algorithm when linear networks are used to represent the value functions. <p> Much of the early developmental work in RL focused on establishing an abstract and general theoretical framework. In this early phase, RL architectures that used very little task-specific knowledge were developed and applied to simple and abstract problems to illustrate and help understand RL algorithms (e.g., Sutton <ref> [106] </ref>, Watkins [118]). In the 1990s RL research has moved out of the developmental phase and is increasingly focused on complex and diverse applications. As a result several researchers are investigating techniques for reducing the learning time of RL architectures to acceptable levels. <p> The DP and RL algorithms defined in Chapters 3 and 4 use one-step estimators in their backup operators. Sutton <ref> [106] </ref> extended the TD algorithm defined in Section 4.3.1 to a family of algorithms called TD (), where 0 1 is a scalar parameter. For 0 &lt; &lt; 1, TD () combines an infinite sequence of multi-step (n) estimators in a geometric sum (cf. Watkins [118]). <p> Watkins [118]). In general, as one increases n in the n-step estimator, the value returned has a lower bias but may have a larger variance. Empirical results have shown that TD () can outperform TD if is chosen carefully (Sutton <ref> [106] </ref>). Whitehead [125] developed an approach he called learning with an external critic that assumes an external critic that knows the optimal policy in advance. The external critic occasionally rewards the agent when it executes an optimal action. This reward is in addition to the standard payoff function. <p> Few researchers have developed RL architectures that explicitly address the need to do temporal generalization. Barto et al.'s [13] used eligibility traces to update states visited many steps in the past. Sutton <ref> [106] </ref> developed a family of algorithms called TD () that use multi-step backup operators. Watkins [118] defined a multi-step version of Q-learning. All these approaches are model-free.
Reference: [107] <author> R.S. Sutton. </author> <title> Integrating architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proc. of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 216-224, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In practice, either restrictive assumptions are placed on the nature of the MDT, such as ergodicity, or appropriate constraints are imposed on the control policy followed while learning, such as using probabilistic policies (Sutton <ref> [107] </ref>). A model-based relaxation algorithm can be either synchronous or asynchronous. An algorithm that does not require a model of the environment can always be applied to a task in which a model is available simply by using the model to simulate the real environment. <p> Being able to compute derivatives allows sensitivity analysis and may lead to some new ways of addressing the difficult exploration versus exploitation issue <ref> [107] </ref> in optimal control tasks. Indeed, the motivation for Rivest's work [85], which inspired the development of the algorithms presented in this section, was to use sensitivity analysis to address the analogous exploration issue in game tree search. <p> The exploration strategy adopted by an agent determines the order in which the states are visited and updated. Exploration: A simple strategy adopted by many researchers is to execute a non-stationary and probabilistic policy defined by the Gibbs distribution over the Q-values (Watkins [118], Sutton <ref> [107] </ref>). The probability of executing action a in state x at time step t is: P (ajx; t) = e t Q t (x;a) a 0 2A e t Q t (x;a 0 ) , where t is the temperature at time index t. <p> Another simple, yet effective, approach in the model-free case has been to use optimistic initial value functions (Sutton <ref> [107] </ref>, Kaelbling [60]). In such a case, parts of the state space that have not been visited will have higher values than those that have been visited often. The greedy policy will then automatically explore unvisited regions of the state space.
Reference: [108] <author> R.S. Sutton. </author> <title> Planning by incremental dynamic programming. </title> <editor> In L. Birnbaum and G. Collins, editors, </editor> <booktitle> Machine Learning: Proceedings of the Eighth International Workshop, </booktitle> <pages> pages 353-357, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: More recently, Barto [8] has identified the separate dimensions along which the different RL algorithms have weakened the strong constraints required by classical DP algorithms (see also Sutton <ref> [108] </ref>). <p> Accordingly, the framework adopted here abstracts the task to that of learning a behavior that approximates optimization of a preset objective functional defined over the space of possible behaviors of the agent. The framework presented here closely follows the work of Barto et al. [14, 10] and Sutton <ref> [108, 110] </ref>. 2.1 Controlling Dynamic Environments In general, the environment in which an agent is embedded can be dynamic, that is, can undergo transformations over time. <p> The current focus on embedded agents in AI has fortunately come at a time when a confluence of ideas from artificial intelligence, machine learning, robotics, and control engineering is taking place (Werbos [124], Barto [7], Barto et al. [10], Sutton et al. <ref> [108, 110] </ref>, Dean and Wellman [36]). <p> Part of the motivation behind this current research is to combine the complementary strengths of research on planning and problem solving in AI and of research on DP in optimal control to get the best of both worlds (e.g., Sutton <ref> [108] </ref>, Barto et al. [10], and Moore and Atkeson [79]). For example, techniques for dealing with uncertainty and stochasticity developed in control theory are now of interest to AI researchers developing architectures for agents embedded in real-world environments. <p> Therefore, the abstract model explicitly stores only the state-independent goal state for each abstract action. The payoff function is not stored explicitly because it already exists in the value function table. 7.4 Hierarchical DYNA Sutton <ref> [108] </ref> developed an on-line RL architecture called DYNA that uses the agent's experience to learn simultaneously a value function and build a primitive model of the environment. In the time interval between two actions in the real environment, DYNA updates the value function by doing backups on the primitive model.
Reference: [109] <author> R.S. Sutton. </author> <title> Adapting bias by gradient descent: An incremental version of delta-bar-delta. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <address> San Jose,CA, July 1992. </address> <publisher> AAAI Press/MIT Press. </publisher>
Reference-contexts: Therefore, the ability to achieve transfer of training across tasks must play a crucial role in building useful multi-task 2 Sutton <ref> [109] </ref> has adapted Kesten's method for adapting learning rates for individual parameters in a function approximator (see, also Jacobs [54]). 66 agent architectures based on RL. Multi-task agents can also achieve computational and monetary savings over multiple single-task agents simply by sharing hardware and software across tasks.
Reference: [110] <author> R.S. Sutton, A.G. Barto, and R.J. Williams. </author> <title> Reinforcement learning is direct adaptive optimal control. </title> <booktitle> In Proceedings of the American Control Conference, </booktitle> <pages> pages 2143-2146, </pages> <address> Boston, MA, </address> <year> 1991. </year>
Reference-contexts: Accordingly, the framework adopted here abstracts the task to that of learning a behavior that approximates optimization of a preset objective functional defined over the space of possible behaviors of the agent. The framework presented here closely follows the work of Barto et al. [14, 10] and Sutton <ref> [108, 110] </ref>. 2.1 Controlling Dynamic Environments In general, the environment in which an agent is embedded can be dynamic, that is, can undergo transformations over time. <p> The current focus on embedded agents in AI has fortunately come at a time when a confluence of ideas from artificial intelligence, machine learning, robotics, and control engineering is taking place (Werbos [124], Barto [7], Barto et al. [10], Sutton et al. <ref> [108, 110] </ref>, Dean and Wellman [36]). <p> At least in theory, there is no need to find optimal actions for states that are not on the set of optimal paths from the set of possible start states (see Korf [65], and Barto et al. [10]). Following Sutton et al. <ref> [110] </ref> and Barto et al. [14], in this dissertation the adaptive optimal control framework is used to formulate tasks faced by autonomous embedded agents.
Reference: [111] <author> R.S. Sutton and B. Pinette. </author> <title> The learning of world models by connectionist networks. </title> <booktitle> In Proceedings of the Seventh Annual Conference of the Cognitive Science Society, </booktitle> <address> Irvine, CA, </address> <year> 1985. </year>
Reference-contexts: Sutton [106] developed a family of algorithms called TD () that use multi-step backup operators. Watkins [118] defined a multi-step version of Q-learning. All these approaches are model-free. Dayan [34] and Sutton and Pinette <ref> [111] </ref> developed an algorithm that tackles the temporal generalization problem in policy evaluation problems by changing the agent's state representation. It learns to represent the i th element of the state set by the i th row of the matrix (I fl [P ] ) 1 .
Reference: [112] <author> G.J. Tesauro. </author> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> 8(3/4):257-277, May 1992. 
Reference-contexts: In this dissertation, however, the term DP will be used exclusively to refer to algorithms for solving optimal control problems. 3 While Tesauro's <ref> [112] </ref> backgammon player is certainly a complex application, it is not an on-line RL system. 5 However, the common view is misleading in two respects. <p> This raises the following important question: if practical concerns dictate that value functions be approximated, how might performance be affected 2 ? Is it possible that, despite some empirical evidence to the contrary (e.g., Barto et al. [13], Anderson [2], Tesauro <ref> [112] </ref>), small errors in approximations could result in arbitrarily bad performance in principle? If so, this could raise significant concerns about the use of function approximation in DP-based learning. 4.5.1 An Upper Bound on the Loss from Approximate Optimal-Value Functions This section extends a result by Bertsekas [17] which guarantees that
Reference: [113] <author> S.B. Thrun. </author> <title> Efficient exploration in reinforcement learning. </title> <type> Technical Report CMU-CS-92-102, </type> <institution> Carnegie Mellon University, </institution> <year> 1992. </year>
Reference-contexts: Therefore, the agent cannot choose actions from the sole perspective of learning the value function in as few updates as possible. This dilemma is called the exploration versus exploitation tradeoff (Barto et al. [10], Thrun <ref> [113, 114] </ref>). The exploration strategy adopted by an agent determines the order in which the states are visited and updated. Exploration: A simple strategy adopted by many researchers is to execute a non-stationary and probabilistic policy defined by the Gibbs distribution over the Q-values (Watkins [118], Sutton [107]).
Reference: [114] <author> S.B. Thrun and K.Moller. </author> <title> Active exploration in dynamic environments. </title> <editor> In J.E. Moody, S.J. Hanson, and R.P. Lippman, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kauffman. </publisher>
Reference-contexts: Therefore, the agent cannot choose actions from the sole perspective of learning the value function in as few updates as possible. This dilemma is called the exploration versus exploitation tradeoff (Barto et al. [10], Thrun <ref> [113, 114] </ref>). The exploration strategy adopted by an agent determines the order in which the states are visited and updated. Exploration: A simple strategy adopted by many researchers is to execute a non-stationary and probabilistic policy defined by the Gibbs distribution over the Q-values (Watkins [118], Sutton [107]). <p> In subcase a, the the agent must explore the real environment to construct the model efficiently and that can conflict with exploitation, raising a different kind of exploration versus exploitation tradeoff. Schmidhuber [93] and Thrun and Moller <ref> [114] </ref> have developed methods that explicitly estimate the accuracy of the learned model and execute actions taking the agent to those parts of the state space where the model has a low accuracy.
Reference: [115] <author> J. Tsitsiklis. </author> <title> Asynchronous stochastic approximation and Q-learning, </title> <month> February </month> <year> 1993. </year> <note> Submitted. </note>
Reference-contexts: Despite all the progress in connecting DP and RL algorithms, the following question was unanswered: can TD and Q-learning be derived by the straightforward application of some classical method for solving systems of equations? Recently this author and others (Singh et al. [102], Jaakkola et al. [53], and Tsitsiklis <ref> [115] </ref>) have answered that question. In this dissertation it is shown that RL algorithms, such as TD and Q-learning, are instances of asynchronous stochastic approximation methods for solving the recursive system of equations associated with RL tasks.
Reference: [116] <author> P.E. Utgoff and J.A. Clouse. </author> <title> Two kinds of training information for evaluation function learning. </title> <booktitle> In Proceedings of the Ninth Annual Conference on Artificial Intelligence, </booktitle> <pages> pages 596-600, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: They demonstrated that if the human expert carefully selects the states in which to offer advice to the agent, very little supervised advice may be needed to improve dramatically the learning speed (see, also Utgoff and Clouse <ref> [116] </ref>). Another technique for biasing the policy selection mechanism is to use a nominal controller that implements the best initial guess of the optimal control policy.
Reference: [117] <author> R.A. Grupen V. Gullapalli, J. Coelho and A.G. Barto. </author> <title> Learning to grasp using a multi-fingered hand. </title> <note> In preparation. </note>
Reference-contexts: BB-RL extends Maes and Brooks' [69] system to multi-stage decision tasks and to policies that assign linear combinations of behaviors to states instead of a single behavior to each state. (see, also Gullapalli et al. <ref> [117] </ref>). 8.3 Simulation Results in this section. The environment in the top panel consists of two rooms connected 108 by a corridor, and the environment in the lower panel is a horseshoe-shaped corridor.
Reference: [118] <author> C.J.C.H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge Univ., </institution> <address> Cambridge, England, </address> <year> 1989. </year>
Reference-contexts: One of the main innovations in RL algorithms for solving problems traditionally solved by DP is that they are model-free because they do not require a model of the environment. Examples of such model-free RL algorithms are Sutton's [106] temporal differences (TD) algorithm and Watkins <ref> [118] </ref> Q-learning algorithm. RL algorithms and classical DP algorithms are related methods because they solve the same system of equations, and because RL algorithms estimate the same quantities that are computed by DP algorithms (see Watkins [118], Barto et al. [14], and Werbos [123, 124]). <p> of such model-free RL algorithms are Sutton's [106] temporal differences (TD) algorithm and Watkins <ref> [118] </ref> Q-learning algorithm. RL algorithms and classical DP algorithms are related methods because they solve the same system of equations, and because RL algorithms estimate the same quantities that are computed by DP algorithms (see Watkins [118], Barto et al. [14], and Werbos [123, 124]). More recently, Barto [8] has identified the separate dimensions along which the different RL algorithms have weakened the strong constraints required by classical DP algorithms (see also Sutton [108]). <p> However, for the algorithms discussed in this dissertation, U is assumed fixed for all iterations. 2 One can define both DP and RL algorithms that use operators that do more than a one-step search and access information about states that are not one-step neighbors, e.g., the multi-step Q-learning of Watkins <ref> [118] </ref>. Most of the theoretical results stated in this dissertation will also hold for algorithms with multi-step operators. 18 state a m x predecessor states successor a 1 a m y a m R (x) a fragment of an MDT. <p> In the early 1980s, in a landmark paper Barto et al. [13] described a technique for addressing the temporal credit assignment problem that culminated in Sutton's [106] paper on a class of techniques he called temporal difference (TD) methods (see also Sutton [105]). In the late 1980s, Watkins <ref> [118] </ref> observed that the TD algorithm solves the linear policy evaluation problem for multi-stage MDTs (see, also Dayan [33], Barnard [5]). <p> is finite, and 3. 8 (x; a) 2 (X fi A); i=0 i (x; a) = 1 and i=0 2 4.3.3 Discussion Convergence proofs for TD and Q-learning that do not use stochastic approximation theory already existed in the RL literature (Sutton [106] and Dayan [33] for TD, and Watkins <ref> [118] </ref> and Watkins and Dayan [119] for Q-learning). But these proofs, especially the one for Q-learning, are based on special mathematical constructions that obscure the underlying simplicity of the algorithms. The connection to stochastic approximation provides a uniform framework for proving convergence of all the different RL algorithms. <p> Much of the early developmental work in RL focused on establishing an abstract and general theoretical framework. In this early phase, RL architectures that used very little task-specific knowledge were developed and applied to simple and abstract problems to illustrate and help understand RL algorithms (e.g., Sutton [106], Watkins <ref> [118] </ref>). In the 1990s RL research has moved out of the developmental phase and is increasingly focused on complex and diverse applications. As a result several researchers are investigating techniques for reducing the learning time of RL architectures to acceptable levels. <p> Sutton [106] extended the TD algorithm defined in Section 4.3.1 to a family of algorithms called TD (), where 0 1 is a scalar parameter. For 0 &lt; &lt; 1, TD () combines an infinite sequence of multi-step (n) estimators in a geometric sum (cf. Watkins <ref> [118] </ref>). In general, as one increases n in the n-step estimator, the value returned has a lower bias but may have a larger variance. Empirical results have shown that TD () can outperform TD if is chosen carefully (Sutton [106]). <p> The exploration strategy adopted by an agent determines the order in which the states are visited and updated. Exploration: A simple strategy adopted by many researchers is to execute a non-stationary and probabilistic policy defined by the Gibbs distribution over the Q-values (Watkins <ref> [118] </ref>, Sutton [107]). The probability of executing action a in state x at time step t is: P (ajx; t) = e t Q t (x;a) a 0 2A e t Q t (x;a 0 ) , where t is the temperature at time index t. <p> Few researchers have developed RL architectures that explicitly address the need to do temporal generalization. Barto et al.'s [13] used eligibility traces to update states visited many steps in the past. Sutton [106] developed a family of algorithms called TD () that use multi-step backup operators. Watkins <ref> [118] </ref> defined a multi-step version of Q-learning. All these approaches are model-free. Dayan [34] and Sutton and Pinette [111] developed an algorithm that tackles the temporal generalization problem in policy evaluation problems by changing the agent's state representation. <p> Such subsequences can be learned through experience in learning to solve other tasks. This chapter presented CQ-L, an architecture that combines the Q-learning algorithm of Watkins <ref> [118] </ref> and the modular architecture of Jacobs et al. [56] to achieve transfer of training by sharing the solutions of elemental tasks across multiple composite tasks. 7 This assumes that the state representation is rich enough to distinguish repeated performances of the same elemental task. <p> The robot is simulated as a unit-mass, and the only dynamical constraint is a bound on the acceleration. 8.3.1 Two-Room Environment The learning task is to find a policy that minimize time to reach goal region. Q-learning <ref> [118] </ref> was used to learn the mixing function, k. Figure 8.2 shows the 2-layer neural network architecture used to store the Q-values (see Appendix F for a brief review of layered neural networks). <p> The inputs to the net are the 4-dimensional state and the 1-dimensional action. The network was trained using backpropagation with target outputs determined by the Q-learning <ref> [118] </ref> algorithm. 111 GOAL GOAL trajectories from two different starting points. The black-filled circles mark the Dirichlet trajectory, white-filled circles mark the Neumann trajectory, and the grey-filled circles mark the trajectory that results from learning. Each trajectory is shown by showing the position of the robot after every time step.
Reference: [119] <author> C.J.C.H. Watkins and P. </author> <title> Dayan. </title> <journal> Q-learning. Machine Learning, </journal> 8(3/4):279-292, May 1992. 
Reference-contexts: (x; a) 2 (X fi A); i=0 i (x; a) = 1 and i=0 2 4.3.3 Discussion Convergence proofs for TD and Q-learning that do not use stochastic approximation theory already existed in the RL literature (Sutton [106] and Dayan [33] for TD, and Watkins [118] and Watkins and Dayan <ref> [119] </ref> for Q-learning). But these proofs, especially the one for Q-learning, are based on special mathematical constructions that obscure the underlying simplicity of the algorithms. The connection to stochastic approximation provides a uniform framework for proving convergence of all the different RL algorithms.
Reference: [120] <author> P.J. Werbos. </author> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <year> 1974. </year> <month> 138 </month>
Reference-contexts: &lt; " P V (x) z2X @V (z) 9 ; Note that one can use the chain rule to compute the above derivatives for states that are not neighbors in the state graph of the MDT in much the same way as the 32 backpropagation (Rumelhart et al. [88], Werbos <ref> [120] </ref>) algorithm for multi-layer connectionist networks. Being able to compute derivatives allows sensitivity analysis and may lead to some new ways of addressing the difficult exploration versus exploitation issue [107] in optimal control tasks.
Reference: [121] <author> P.J. Werbos. </author> <title> Building and understanding adaptive systems: A statistical/numerical approach to factory automation and brain research. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 17(1) </volume> <pages> 7-20, </pages> <year> 1987. </year>
Reference-contexts: Researchers within these diverse fields have developed a number of different methods under different names for solving RL tasks, e.g., dynamic programming (DP) algorithms (Bellman [16]), classifier systems (Holland et al. [51]), and reinforcement learning algorithms (Barto et al. [14], Werbos <ref> [121] </ref>). 1 The different algorithms assume different amounts of domain knowledge and work under different constraints, but they can all solve RL tasks and should perhaps all be called RL methods.
Reference: [122] <author> P.J. Werbos. </author> <title> Consistency of HDP applied to a simple reinforcement learning problem. </title> <booktitle> Neural Networks, </booktitle> <volume> 3(2) </volume> <pages> 179-189, </pages> <year> 1990. </year>
Reference-contexts: Further, Watkins developed the Q-learning algorithm for solving MDTs and noted the approximate relationship between TD, Q-learning, and DP algorithms (see also Barto et al. [15, 10], and Werbos <ref> [122] </ref>). In this chapter, the connection between RL and MDTs is made precise. But first, a brief detour has to be taken to explain the Robbins-Monro [86] stochastic approximation method for solving systems of equations.
Reference: [123] <author> P.J. Werbos. </author> <title> Neurocontrol and related techniques. </title> <editor> In A.J. Maren, editor, </editor> <booktitle> Handbook of Neural Computer Applications. </booktitle> <publisher> Academic Press, </publisher> <year> 1990. </year>
Reference-contexts: RL algorithms and classical DP algorithms are related methods because they solve the same system of equations, and because RL algorithms estimate the same quantities that are computed by DP algorithms (see Watkins [118], Barto et al. [14], and Werbos <ref> [123, 124] </ref>). More recently, Barto [8] has identified the separate dimensions along which the different RL algorithms have weakened the strong constraints required by classical DP algorithms (see also Sutton [108]).
Reference: [124] <author> P.J. Werbos. </author> <title> Approximate dynamic programming for real-time control and neural modelling. In D.A. White and D.A. </title> <editor> Sofge, editors, </editor> <booktitle> Handbook of Intelligent Control: Neural, Fuzzy and Adaptive Approaches, </booktitle> <pages> pages 493-525. </pages> <publisher> Van Nostrand Reinhold, </publisher> <year> 1992. </year>
Reference-contexts: RL algorithms and classical DP algorithms are related methods because they solve the same system of equations, and because RL algorithms estimate the same quantities that are computed by DP algorithms (see Watkins [118], Barto et al. [14], and Werbos <ref> [123, 124] </ref>). More recently, Barto [8] has identified the separate dimensions along which the different RL algorithms have weakened the strong constraints required by classical DP algorithms (see also Sutton [108]). <p> The current focus on embedded agents in AI has fortunately come at a time when a confluence of ideas from artificial intelligence, machine learning, robotics, and control engineering is taking place (Werbos <ref> [124] </ref>, Barto [7], Barto et al. [10], Sutton et al. [108, 110], Dean and Wellman [36]).
Reference: [125] <author> S.D. Whitehead. </author> <title> Reinforcement Learning for the Adaptive Control of Perception and Action. </title> <type> PhD thesis, </type> <institution> University of Rochester, </institution> <year> 1992. </year>
Reference-contexts: However, RL architectures can easily incorporate many different kinds of domain knowledge. Indeed, a significant proportion of the current research on RL is about incorporating domain knowledge into RL architectures to alleviate some of their problems (Singh [99], Yee et al. [129], Mitchell and Thrun [75], Whitehead <ref> [125] </ref>, Lin [66], Clouse and Utgoff [28]). Despite the fact that under certain conditions RL algorithms may be the best available methods, conventional RL architectures are slow enough to make them impractical for many real-world problems. <p> As a result the environment makes 3 Note that Figure 2.1 is just one possible block diagram representation; other researchers have used more complex block diagrams to capture some of the more subtle intricacies of tasks faced by embedded agents (e.g., Whitehead <ref> [125] </ref>, Kaelbling [60]). 4 For ease of exposition it is assumed that the same set of actions are available to the agent in each state of the environment. <p> Watkins [118]). In general, as one increases n in the n-step estimator, the value returned has a lower bias but may have a larger variance. Empirical results have shown that TD () can outperform TD if is chosen carefully (Sutton [106]). Whitehead <ref> [125] </ref> developed an approach he called learning with an external critic that assumes an external critic that knows the optimal policy in advance. The external critic occasionally rewards the agent when it executes an optimal action. This reward is in addition to the standard payoff function. <p> Exploration can then be confined to small perturbations around the nominal trajectory thereby reducing the number of state-updates performed in regions of the state space that are unlikely to be part of the optimal solution. Whitehead <ref> [125] </ref> has developed an architecture called learning by watching where an agent observes the behavior of an expert agent and shares its experiences. The learning agent does state-updates on the states experienced by the expert.
Reference: [126] <author> S.D. Whitehead and D.H. Ballard. </author> <title> Active perception and reinforcement learning. </title> <booktitle> In Proc. of the Seventh International Conference on Machine Learning, </booktitle> <address> Austin, TX, </address> <month> June </month> <year> 1990. </year> <note> M. </note>
Reference-contexts: Researchers are currently developing methods that attempt to build state information by either memorizing past perceptions, or by controlling the perceptual system of the agent to generate multiple perceptions (Whitehead and Ballard <ref> [126] </ref>, Lin and Mitchell [67], Chrisman [27], and McCallum [73]). In both cases the hope is that techniques other than RL can be used to convert a non-Markovian problem into a Markovian one so that conventional RL can be applied.
Reference: [127] <editor> R.J. Williams and L.C. Baird. </editor> <title> A mathematical analysis of actor-critic architectures for learning optimal controls through incremental dynamic programming. </title> <booktitle> In Proceedings of the Sixth Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> pages 96-101, </pages> <year> 1990. </year>
Reference-contexts: The algorithm presented here is closely related to a set of asynchronous algorithms presented by Williams and Baird <ref> [127] </ref> that were later shown by Barto [6] to be a form of k-step M-PI. 3.3.2 Asynchronous Update Operators For ease of exposition, let us denote the one-step backed-up value for state x under action a, given a value function V , by Q V (x; a). <p> Define the following asynchronous update operators (cf. Williams and Baird <ref> [127] </ref>): 1. A single-sided policy evaluation operator T x (V k ; k ), that takes the current value function V k and the current policy k and does one step of policy evaluation for state x. <p> It was shown that the new algorithm converges under more general initial conditions than the related algorithms of modified policy iteration (Puterman and Shin [84]) and the asynchronous algorithms developed by Williams and Baird <ref> [127] </ref>. Stochastic Approximation Framework for RL: A hitherto unknown connection between stochastic approximation theory and RL algorithms such as TD and Q-learning was developed. The stochastic approximation framework clearly delineates the contribution made by RL researchers to the entire class of algorithms for solving RL tasks.
Reference: [128] <author> R.C. Yee. </author> <title> Abstraction in control learning. </title> <type> Technical Report COINS Technical Report 92-16, </type> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, MA 01003, </address> <year> 1992. </year> <title> A dissertation proposal. </title>
Reference-contexts: This two-step process is repeated until the agent finally finds a good solution for the underlying physical problem. Moore showed that his approach develops a model that has a high resolution around the optimal trajectory in state space and a coarse resolution elsewhere (see, also Yee <ref> [128] </ref>). Chapman and Kaelbling [26] developed an algorithm that builds a tree structured Q-table. Each node splits on one bit of the state representation, and that bit is chosen based on its relevance in predicting short-term and long-term payoffs. Relevance is measured via statistical tests.
Reference: [129] <author> R.C. Yee, S. Saxena, P.E. Utgoff, and A.G. Barto. </author> <title> Explaining temporal differences to create useful concepts for evaluating states. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 882-888, </pages> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: However, RL architectures can easily incorporate many different kinds of domain knowledge. Indeed, a significant proportion of the current research on RL is about incorporating domain knowledge into RL architectures to alleviate some of their problems (Singh [99], Yee et al. <ref> [129] </ref>, Mitchell and Thrun [75], Whitehead [125], Lin [66], Clouse and Utgoff [28]). Despite the fact that under certain conditions RL algorithms may be the best available methods, conventional RL architectures are slow enough to make them impractical for many real-world problems. <p> The wrong generalization bias can prevent convergence to the optimal value function and lead to sub-optimal solutions. Other researchers are exploring techniques for generalizing the values in state space in a way that takes the dynamics of the environment and the payoff function into account. Yee et al. <ref> [129] </ref> developed a method that uses a symbolic domain theory to do structural generalization. After every training episode, a form of explanation-based generalization (Mitchell et al. [76]) is used to determine a set of predecessor states that should have the same value.
References-found: 129

