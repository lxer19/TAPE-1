URL: http://www.csc.ncsu.edu/faculty/mpsingh/papers/mas/jpl.ps
Refering-URL: http://www.csc.ncsu.edu/faculty/mpsingh/papers/mas/
Root-URL: http://www.csc.ncsu.edu
Email: msingh@cs.utexas.edu  
Title: A Logic of Intentions and Beliefs flyz  
Author: Munindar P. Singh Nicholas M. Asher 
Address: Austin, TX 78712 USA  
Affiliation: Center for Cognitive Science The University of Texas  
Date: 1993  
Note: To appear in Journal of Philosophical Logic,  
Abstract: Intentions are an important concept in Artificial Intelligence and Cognitive Science. We present a formal theory of intentions and beliefs based on Discourse Representation Theory that captures many of their important logical properties. Unlike possible worlds approaches, this theory does not assume that agents are perfect reasoners, and gives a realistic view of their internal architecture; unlike most representational approaches, it has an objective semantics, and does not rely on an ad hoc labeling of the internal states of agents. We describe a minimal logic for intentions and beliefs that is sound and complete relative to our semantics. We discuss several additional axioms, and the constraints on the models that validate them. fl This is a considerably extended and revised version of a paper entitled "Towards a Formal Theory of Intentions" that appears in the proceedings of the European Workshop on Logic in AI. y We are indebted to Allen Emerson and Rob Koons and to two anonymous referees for comments. z This work was partially supported by the Microelectronics and Computer Technology Corporation, and by the National Science Foundation (through grant # IRI-8945845 to the Center for Cognitive Science, University of Texas). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Philip Agre and David Chapman. Pengi: </author> <title> An implementation of a theory of activity. </title> <booktitle> In AAAI, </booktitle> <pages> pages 268-272, </pages> <year> 1987. </year>
Reference-contexts: in the antecedent a universal quantifier reading. 8. (K 1 _ K 2 ) fl L 1 and K 2 fl L 2 and K 2 fl Commutativity of _ under both intention and belief. 3 We can thus use conceptual individuals to model the "indexical-functional" aspects of the environment <ref> [1] </ref>.
Reference: [2] <author> James Allen and C. Raymond Perrault. </author> <title> Participating in dialogues: Understanding via plan deduction. </title> <booktitle> In Proceedings of CSCSI, </booktitle> <year> 1978. </year> <month> 23 </month>
Reference-contexts: 1 Introduction An understanding of intentions is important to several subfields of Artificial Intelligence (AI), especially, speech act theory [3, 4, 10, 12], discourse processing [18], planning [17], and plan recognition <ref> [2, 24, 28] </ref>. We present a formal theory of intentions and beliefs that is based on Discourse Representation Theory (DRT) [5, 6, 19]. Our theory involves a formal model of time and possibility, and also explicitly models the structure of the agents' internal states.
Reference: [3] <author> James F. Allen and C. Raymond Perrault. </author> <title> Analyzing intention in utterances. </title> <journal> Artificial Intelligence, </journal> <volume> 15 </volume> <pages> 143-178, </pages> <year> 1980. </year>
Reference-contexts: 1 Introduction An understanding of intentions is important to several subfields of Artificial Intelligence (AI), especially, speech act theory <ref> [3, 4, 10, 12] </ref>, discourse processing [18], planning [17], and plan recognition [2, 24, 28]. We present a formal theory of intentions and beliefs that is based on Discourse Representation Theory (DRT) [5, 6, 19].
Reference: [4] <author> Douglas Appelt. </author> <title> Planning English Sentences. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK, </address> <year> 1986. </year>
Reference-contexts: 1 Introduction An understanding of intentions is important to several subfields of Artificial Intelligence (AI), especially, speech act theory <ref> [3, 4, 10, 12] </ref>, discourse processing [18], planning [17], and plan recognition [2, 24, 28]. We present a formal theory of intentions and beliefs that is based on Discourse Representation Theory (DRT) [5, 6, 19]. <p> Intentions are important attitudes of intelligent agents and, for resource-bounded agents, cannot be reduced to simple considerations of the optimality of decisions. Their importance to cognition and their use in AI has been defended extensively in the literature <ref> [4, 8, 11, 10, 12, 17, 18, 30] </ref>. The roles of intentions mentioned below are especially important when one is interested in agents who would not otherwise (because of their limitations) be able to make appropriate or rational decisions. <p> We sketch just enough of DRT in this paper so that the presentation here is self-contained. A study of the (mostly AI) literature <ref> [4, 8, 10, 17, 18, 25, 27, 28, 30] </ref> yields the following important properties of intentions. Intentions are about future events. An agent with an intention should believe that it can be realized, at least along some future. <p> Points (1) and (2) directly relate to speech act planning and discourse understanding <ref> [4, 10, 18, 27] </ref> as well. Such a logic would also be useful in multiagent systems where agents must reason about each other's intentions and beliefs to effectively negotiate among themselves. We now turn to a deductive system for our semantics.
Reference: [5] <author> Nicholas Asher. </author> <title> Belief in discourse representation theory. </title> <journal> Journal of Philosophical Logic, </journal> <volume> 15 </volume> <pages> 127-189, </pages> <year> 1986. </year>
Reference-contexts: We present a formal theory of intentions and beliefs that is based on Discourse Representation Theory (DRT) <ref> [5, 6, 19] </ref>. Our theory involves a formal model of time and possibility, and also explicitly models the structure of the agents' internal states. <p> Such a unified theory has been evolving in the DRT framework over the last few years. Kamp's thesis of the Unity of Thought and Information [20], and Asher's work on the attitudes <ref> [5] </ref>, and on their relationship with information [7] must be cited in this context. DRT is a useful framework for the general project for several reasons. Firstly, DRT is a theory of discourse meaning that captures many aspects of the information typically encoded in natural language utterances. <p> structure of the agent entirely; the choices made by an agent, and his (most especially, verbal) behavior do not just depend on the anchoring of his internal state in the world, but on its structure as well: Kripke's `London' versus `Londres' example [22], discussed in a DRT framework by Asher <ref> [5, pp. 142-143] </ref> and by Kamp [20, pp. 253-254], is a case in point. Kripke develops a convincing story in which a character, Pierre, ends up with contradictory beliefs about `London' and `Londres,' respectively, even though they are both anchored to the same metropolis in England. <p> We will come back to this point in x3.4. The semantic conditions for beliefs and intentions are a simplification of the ones given by Asher in his "complete theory" for the case of beliefs <ref> [5, pp. 171-173] </ref>. This simplification results in part because we consider an explicit assignment function assigning cognitive states to agents. As a result, we have also been able to separate out the components of content and honesty, yielding a more perspicuous analysis of beliefs and intentions. <p> This is important since it brings us closer to the ultimate goal of a unified theory of actions, beliefs and intentions. The theory presented here is a theory of beliefs and intentions, not of belief and intention reports|a theory of belief reports being a contribution of Asher <ref> [5] </ref>. It considers 4 the logical aspects of these concepts and the consequences of making different assumptions about the model. These aspects and consequences underlie a theory of belief and intention reports, but are distinct from it. In x3, we present the formal language and model. <p> In x4, we motivate a minimal logic for intentions and beliefs. In x5, we list some important extensions to the basic logic in terms of axioms and and the constraints on models in which they are validated. 3 Formal Language and Formal Semantics Our sentences (DRS's) <ref> [5] </ref> are members of the language, DRS, generated by the following semi-formal grammar. The temporal part of the grammar is inspired by CTL*, with the addition of the "sometimes in the past" operator, P [14]. 1. <p> But, since it is clear that the language does not allow DRS's to be referred to, there is no problem here. 6 3.1 Satisfaction conditions The satisfaction conditions for :, _, !, and predicates as given below are standard in the DRT literature (and are adapted from those in <ref> [5, 6] </ref>); the ones for the temporal operators too are standard (and are adapted from those in [14]). <p> This ensures that the embedding of DRS N above is not incoherent. This compatibility condition is weaker than the one given by Asher, who requires that ae be a 1-1 mapping <ref> [5, p. 173] </ref>. Thus this definition is a generalization of his. <p> One can, however, validate this axiom in our theory by adding the appropriate constraint (e.g., see Theorem 15 above). While, by and large, this paper is in the spirit of Asher's paper on beliefs <ref> [5] </ref>, it differs from it in some respects. Asher is more concerned with the philosophical issues involved in testing the correctness of belief reports; here we are interested in the logic of beliefs and intentions themselves and have, therefore, made some simplifications on grounds of technical clarity.
Reference: [6] <author> Nicholas Asher. </author> <title> A typology for attitude verbs and their anaphoric properties. </title> <journal> Linguistics and Philosophy, </journal> <volume> 10 </volume> <pages> 125-197, </pages> <year> 1987. </year>
Reference-contexts: We present a formal theory of intentions and beliefs that is based on Discourse Representation Theory (DRT) <ref> [5, 6, 19] </ref>. Our theory involves a formal model of time and possibility, and also explicitly models the structure of the agents' internal states. <p> But, since it is clear that the language does not allow DRS's to be referred to, there is no problem here. 6 3.1 Satisfaction conditions The satisfaction conditions for :, _, !, and predicates as given below are standard in the DRT literature (and are adapted from those in <ref> [5, 6] </ref>); the ones for the temporal operators too are standard (and are adapted from those in [14]).
Reference: [7] <author> Nicholas Asher. </author> <title> Information, interpretation, and attitudes. </title> <editor> In P. Hanson, editor, </editor> <booktitle> British Columbia Studies in Cognitive Science, </booktitle> <volume> volume 1. </volume> <publisher> University of British Columbia Press, </publisher> <address> Vancouver, Canada, </address> <year> 1990. </year>
Reference-contexts: Such a unified theory has been evolving in the DRT framework over the last few years. Kamp's thesis of the Unity of Thought and Information [20], and Asher's work on the attitudes [5], and on their relationship with information <ref> [7] </ref> must be cited in this context. DRT is a useful framework for the general project for several reasons. Firstly, DRT is a theory of discourse meaning that captures many aspects of the information typically encoded in natural language utterances.
Reference: [8] <author> Michael E. Bratman. </author> <title> Intention, Plans, and Practical Reason. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: Intentions are important attitudes of intelligent agents and, for resource-bounded agents, cannot be reduced to simple considerations of the optimality of decisions. Their importance to cognition and their use in AI has been defended extensively in the literature <ref> [4, 8, 11, 10, 12, 17, 18, 30] </ref>. The roles of intentions mentioned below are especially important when one is interested in agents who would not otherwise (because of their limitations) be able to make appropriate or rational decisions. <p> We sketch just enough of DRT in this paper so that the presentation here is self-contained. A study of the (mostly AI) literature <ref> [4, 8, 10, 17, 18, 25, 27, 28, 30] </ref> yields the following important properties of intentions. Intentions are about future events. An agent with an intention should believe that it can be realized, at least along some future. <p> this definition, the following properties of intentions are accounted for: (1) an agent with an intention tacitly considers it possible that his intention will be fulfilled (this is motivated in [25]), and (2) is tacitly restricted by his intention to scenarios in which it is achieved (this is motivated in <ref> [8] </ref>). These are two of the most important properties of intentions. The content of any set of attitudes is the intersection of their respective contents.
Reference: [9] <author> Brian F. Chellas. </author> <title> Modal Logic. </title> <publisher> Cambridge University Press, </publisher> <address> New York, NY, </address> <year> 1980. </year>
Reference-contexts: Schema (WA) above is validated by the definition of Content and the special clause in the definition of . As a result, it is clear that the axiomatization is sound. Completeness too is simple. The proof sketched below adapts the canonical model technique discussed by Chellas <ref> [9, pp. 60, 173] </ref> for our ends. Let the above logic be called . Define a canonical model M = hW; T; &lt;; I; A; C; [[ ]]i for as follows: 1.
Reference: [10] <author> Philip R. Cohen and Hector J. Levesque. </author> <title> Rational interaction as the basis for communication. </title> <type> Technical Report 433, </type> <institution> SRI International, </institution> <address> Menlo Park, CA, </address> <month> April </month> <year> 1988. </year>
Reference-contexts: 1 Introduction An understanding of intentions is important to several subfields of Artificial Intelligence (AI), especially, speech act theory <ref> [3, 4, 10, 12] </ref>, discourse processing [18], planning [17], and plan recognition [2, 24, 28]. We present a formal theory of intentions and beliefs that is based on Discourse Representation Theory (DRT) [5, 6, 19]. <p> Intentions are important attitudes of intelligent agents and, for resource-bounded agents, cannot be reduced to simple considerations of the optimality of decisions. Their importance to cognition and their use in AI has been defended extensively in the literature <ref> [4, 8, 11, 10, 12, 17, 18, 30] </ref>. The roles of intentions mentioned below are especially important when one is interested in agents who would not otherwise (because of their limitations) be able to make appropriate or rational decisions. <p> We sketch just enough of DRT in this paper so that the presentation here is self-contained. A study of the (mostly AI) literature <ref> [4, 8, 10, 17, 18, 25, 27, 28, 30] </ref> yields the following important properties of intentions. Intentions are about future events. An agent with an intention should believe that it can be realized, at least along some future. <p> These properties, while not valid in the minimal logic, are expressible in extensions of it (see x5). 2 More Motivations Our theory, like the so-called sentential theories [21], and unlike most possible-worlds based theories <ref> [10] </ref>, avoids attributing logical omniscience to agents, since it does not require that agents' intentions or beliefs be closed under logical equivalence (thus it also avoids validating closure under logical consequence). At the same time, this theory has advantages over the sentential theories as well. <p> Points (1) and (2) directly relate to speech act planning and discourse understanding <ref> [4, 10, 18, 27] </ref> as well. Such a logic would also be useful in multiagent systems where agents must reason about each other's intentions and beliefs to effectively negotiate among themselves. We now turn to a deductive system for our semantics.
Reference: [11] <author> Philip R. Cohen and Hector J. Levesque. </author> <title> Intention is choice with commitment. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 213-261, </pages> <year> 1990. </year>
Reference-contexts: Intentions are important attitudes of intelligent agents and, for resource-bounded agents, cannot be reduced to simple considerations of the optimality of decisions. Their importance to cognition and their use in AI has been defended extensively in the literature <ref> [4, 8, 11, 10, 12, 17, 18, 30] </ref>. The roles of intentions mentioned below are especially important when one is interested in agents who would not otherwise (because of their limitations) be able to make appropriate or rational decisions. <p> About the only other formal theory of intentions is that of Cohen & Levesque <ref> [11] </ref>. This is a modal approach based on a possible worlds model. As a consequence, it validates closure under logical equivalence. It even validates a slight variation of closure under logical consequence, though an irrelevant reason prevents closure under standard logical consequence.
Reference: [12] <author> Philip R. Cohen and C. Raymond Perrault. </author> <title> Elements of a plan-based theory of speech acts. </title> <journal> Cognitive Science, </journal> <volume> 3 </volume> <pages> 117-212, </pages> <year> 1979. </year>
Reference-contexts: 1 Introduction An understanding of intentions is important to several subfields of Artificial Intelligence (AI), especially, speech act theory <ref> [3, 4, 10, 12] </ref>, discourse processing [18], planning [17], and plan recognition [2, 24, 28]. We present a formal theory of intentions and beliefs that is based on Discourse Representation Theory (DRT) [5, 6, 19]. <p> Intentions are important attitudes of intelligent agents and, for resource-bounded agents, cannot be reduced to simple considerations of the optimality of decisions. Their importance to cognition and their use in AI has been defended extensively in the literature <ref> [4, 8, 11, 10, 12, 17, 18, 30] </ref>. The roles of intentions mentioned below are especially important when one is interested in agents who would not otherwise (because of their limitations) be able to make appropriate or rational decisions.
Reference: [13] <author> Daniel C. Dennett. </author> <title> The Intentional Stance. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: This, of course, depends on what one might want to do with such a theory. A theory of intentions is needed at the foundational level of study in AI and Cognitive Science in order to complete an account of intelligent and, possibly, rational agency <ref> [13, 31] </ref>. Intentions are important attitudes of intelligent agents and, for resource-bounded agents, cannot be reduced to simple considerations of the optimality of decisions. Their importance to cognition and their use in AI has been defended extensively in the literature [4, 8, 11, 10, 12, 17, 18, 30].
Reference: [14] <author> E. A. Emerson. </author> <title> Temporal and modal logic. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, volume B. </booktitle> <publisher> North-Holland Publishing Company, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1990. </year>
Reference-contexts: The temporal part of the grammar is inspired by CTL*, with the addition of the "sometimes in the past" operator, P <ref> [14] </ref>. 1. DRS ! (a) hU, Condi j (b) predicate (variable, . . . , variable) j (c) : DRS j (e) DRS ! DRS j (f) variable Believes DRS j (g) variable Intends DRS j (h) PDRS j (i) ADRS' j (j) EDRS' 2. <p> Let S w;t be the class of all scenarios at world w and time t, and let S be the class of all scenarios, i.e., the union of S w;t over all w; t. For discrete histories, scenarios correspond to the fullpaths of Emerson <ref> [14] </ref>. An embedding function, f , yields at each w 2 W an embedding, f w , that maps variables to individuals in that world. An embedding, g, extends an embedding, h, written g w h, if it agrees with h on the domain of h. <p> there is no problem here. 6 3.1 Satisfaction conditions The satisfaction conditions for :, _, !, and predicates as given below are standard in the DRT literature (and are adapted from those in [5, 6]); the ones for the temporal operators too are standard (and are adapted from those in <ref> [14] </ref>).
Reference: [15] <author> Ronald Fagin and Joseph Y. Halpern. </author> <title> Belief, awareness, and limited reasoning. </title> <journal> Artificial Intelligence, </journal> <volume> 34 </volume> <pages> 39-76, </pages> <year> 1988. </year>
Reference-contexts: Thus our approach also avoids the charges of ad hoc-ism often levied against the sentential approaches, e.g., by Levesque [23], and Fagin & Halpern <ref> [15] </ref>. The main advantage of our approach is that it allows us to model the internal architecture of intelligent agents far more realistically than the other formal approaches can.
Reference: [16] <author> Ronald Fagin, Joseph Y. Halpern, and Moshe Y. Vardi. </author> <title> A nonstandard approach to the logical omniscience problem. </title> <booktitle> In Proceedings of the Third Conference on Theoretical Aspects of Reasoning About Knowledge. </booktitle> <publisher> Morgan Kaufmann Inc., </publisher> <year> 1990. </year>
Reference-contexts: Clearly that is not true for all such p and q. Thus this inference fails for the right reasons. This inference cannot be avoided in any possible worlds approach, not even those that consider "impossible worlds" [23], or "buddy worlds" <ref> [16] </ref> (roughly, because p and q are true at exactly the same worlds). One can, however, validate this axiom in our theory by adding the appropriate constraint (e.g., see Theorem 15 above).
Reference: [17] <author> Michael P. Georgeff. </author> <title> Planning. </title> <editor> In J. F. Traub, editor, </editor> <booktitle> Annual Review of Computer Science, </booktitle> <volume> Vol 2. </volume> <publisher> Annual Reviews Inc., </publisher> <address> Palo Alto, CA, </address> <year> 1987. </year>
Reference-contexts: 1 Introduction An understanding of intentions is important to several subfields of Artificial Intelligence (AI), especially, speech act theory [3, 4, 10, 12], discourse processing [18], planning <ref> [17] </ref>, and plan recognition [2, 24, 28]. We present a formal theory of intentions and beliefs that is based on Discourse Representation Theory (DRT) [5, 6, 19]. Our theory involves a formal model of time and possibility, and also explicitly models the structure of the agents' internal states. <p> Intentions are important attitudes of intelligent agents and, for resource-bounded agents, cannot be reduced to simple considerations of the optimality of decisions. Their importance to cognition and their use in AI has been defended extensively in the literature <ref> [4, 8, 11, 10, 12, 17, 18, 30] </ref>. The roles of intentions mentioned below are especially important when one is interested in agents who would not otherwise (because of their limitations) be able to make appropriate or rational decisions. <p> We sketch just enough of DRT in this paper so that the presentation here is self-contained. A study of the (mostly AI) literature <ref> [4, 8, 10, 17, 18, 25, 27, 28, 30] </ref> yields the following important properties of intentions. Intentions are about future events. An agent with an intention should believe that it can be realized, at least along some future. <p> However, for many purposes in several important subfields of AI, it would be useful to also have a logic that corresponds to the above semantics. These purposes include (1) reasoning by an agent to determine his own plans, and to reason about their consequences <ref> [17, 25] </ref>, (2) and to do the same for the plans of others [28], and (3) a principled approach to the design of multiagent intelligent systems [29]. Points (1) and (2) directly relate to speech act planning and discourse understanding [4, 10, 18, 27] as well.
Reference: [18] <author> Barbara Grosz and Candace Sidner. </author> <title> Attentions, intentions, and discourse structure. </title> <journal> Computational Linguistics, </journal> <volume> 12(3) </volume> <pages> 175-204, </pages> <year> 1986. </year> <month> 24 </month>
Reference-contexts: 1 Introduction An understanding of intentions is important to several subfields of Artificial Intelligence (AI), especially, speech act theory [3, 4, 10, 12], discourse processing <ref> [18] </ref>, planning [17], and plan recognition [2, 24, 28]. We present a formal theory of intentions and beliefs that is based on Discourse Representation Theory (DRT) [5, 6, 19]. Our theory involves a formal model of time and possibility, and also explicitly models the structure of the agents' internal states. <p> Intentions are important attitudes of intelligent agents and, for resource-bounded agents, cannot be reduced to simple considerations of the optimality of decisions. Their importance to cognition and their use in AI has been defended extensively in the literature <ref> [4, 8, 11, 10, 12, 17, 18, 30] </ref>. The roles of intentions mentioned below are especially important when one is interested in agents who would not otherwise (because of their limitations) be able to make appropriate or rational decisions. <p> We sketch just enough of DRT in this paper so that the presentation here is self-contained. A study of the (mostly AI) literature <ref> [4, 8, 10, 17, 18, 25, 27, 28, 30] </ref> yields the following important properties of intentions. Intentions are about future events. An agent with an intention should believe that it can be realized, at least along some future. <p> Points (1) and (2) directly relate to speech act planning and discourse understanding <ref> [4, 10, 18, 27] </ref> as well. Such a logic would also be useful in multiagent systems where agents must reason about each other's intentions and beliefs to effectively negotiate among themselves. We now turn to a deductive system for our semantics.
Reference: [19] <author> Hans Kamp. </author> <title> A theory of truth and semantic representation. </title> <editor> In J. Groenendijk, T. Jansenn, and M. Stokhof, editors, Truth, </editor> <booktitle> Interpretation and Information, </booktitle> <pages> pages 1-41. </pages> <publisher> Foris Publications, </publisher> <address> Dordrecht, The Netherlands, </address> <year> 1984. </year>
Reference-contexts: We present a formal theory of intentions and beliefs that is based on Discourse Representation Theory (DRT) <ref> [5, 6, 19] </ref>. Our theory involves a formal model of time and possibility, and also explicitly models the structure of the agents' internal states.
Reference: [20] <author> Hans Kamp. </author> <title> Context, </title> <booktitle> thought and communication. The Proceedings of the Aristotelian Society, New Series, </booktitle> <address> LXXXV(XIII):239-261, 1984/1985. </address>
Reference-contexts: Such a unified theory has been evolving in the DRT framework over the last few years. Kamp's thesis of the Unity of Thought and Information <ref> [20] </ref>, and Asher's work on the attitudes [5], and on their relationship with information [7] must be cited in this context. DRT is a useful framework for the general project for several reasons. <p> choices made by an agent, and his (most especially, verbal) behavior do not just depend on the anchoring of his internal state in the world, but on its structure as well: Kripke's `London' versus `Londres' example [22], discussed in a DRT framework by Asher [5, pp. 142-143] and by Kamp <ref> [20, pp. 253-254] </ref>, is a case in point. Kripke develops a convincing story in which a character, Pierre, ends up with contradictory beliefs about `London' and `Londres,' respectively, even though they are both anchored to the same metropolis in England.
Reference: [21] <author> Kurt Konolige. </author> <title> A Deduction Model of Belief. </title> <publisher> Morgan Kaufmann, Inc., </publisher> <year> 1986. </year>
Reference-contexts: These properties, while not valid in the minimal logic, are expressible in extensions of it (see x5). 2 More Motivations Our theory, like the so-called sentential theories <ref> [21] </ref>, and unlike most possible-worlds based theories [10], avoids attributing logical omniscience to agents, since it does not require that agents' intentions or beliefs be closed under logical equivalence (thus it also avoids validating closure under logical consequence).
Reference: [22] <author> Saul Kripke. </author> <title> A puzzle about belief. </title> <editor> In A. Margalit, editor, </editor> <title> Meaning and Use. </title> <publisher> Dordrecht Reidel, </publisher> <address> Dordrecht, The Netherlands, </address> <year> 1979. </year>
Reference-contexts: it is not acceptable to ignore the internal structure of the agent entirely; the choices made by an agent, and his (most especially, verbal) behavior do not just depend on the anchoring of his internal state in the world, but on its structure as well: Kripke's `London' versus `Londres' example <ref> [22] </ref>, discussed in a DRT framework by Asher [5, pp. 142-143] and by Kamp [20, pp. 253-254], is a case in point.
Reference: [23] <author> Hector Levesque. </author> <title> A logic of implicit and explicit belief. </title> <booktitle> In AAAI, </booktitle> <year> 1984. </year>
Reference-contexts: Thus our approach also avoids the charges of ad hoc-ism often levied against the sentential approaches, e.g., by Levesque <ref> [23] </ref>, and Fagin & Halpern [15]. The main advantage of our approach is that it allows us to model the internal architecture of intelligent agents far more realistically than the other formal approaches can. <p> Clearly that is not true for all such p and q. Thus this inference fails for the right reasons. This inference cannot be avoided in any possible worlds approach, not even those that consider "impossible worlds" <ref> [23] </ref>, or "buddy worlds" [16] (roughly, because p and q are true at exactly the same worlds). One can, however, validate this axiom in our theory by adding the appropriate constraint (e.g., see Theorem 15 above).
Reference: [24] <author> Diane J. Litman and James F. Allen. </author> <title> A plan recognition model for subdialogues in conversations. </title> <journal> Cognitive Science, </journal> <volume> 11 </volume> <pages> 163-200, </pages> <year> 1987. </year>
Reference-contexts: 1 Introduction An understanding of intentions is important to several subfields of Artificial Intelligence (AI), especially, speech act theory [3, 4, 10, 12], discourse processing [18], planning [17], and plan recognition <ref> [2, 24, 28] </ref>. We present a formal theory of intentions and beliefs that is based on Discourse Representation Theory (DRT) [5, 6, 19]. Our theory involves a formal model of time and possibility, and also explicitly models the structure of the agents' internal states.
Reference: [25] <author> Drew McDermott. </author> <title> A temporal logic for reasoning about processes and plans. </title> <journal> Cognitive Science, </journal> <volume> 6(2) </volume> <pages> 101-155, </pages> <year> 1982. </year>
Reference-contexts: We sketch just enough of DRT in this paper so that the presentation here is self-contained. A study of the (mostly AI) literature <ref> [4, 8, 10, 17, 18, 25, 27, 28, 30] </ref> yields the following important properties of intentions. Intentions are about future events. An agent with an intention should believe that it can be realized, at least along some future. <p> g : t 0 2 S^g w w f w ^M j= w;t 0 ;g K))g As a consequence of this definition, the following properties of intentions are accounted for: (1) an agent with an intention tacitly considers it possible that his intention will be fulfilled (this is motivated in <ref> [25] </ref>), and (2) is tacitly restricted by his intention to scenarios in which it is achieved (this is motivated in [8]). These are two of the most important properties of intentions. The content of any set of attitudes is the intersection of their respective contents. <p> However, for many purposes in several important subfields of AI, it would be useful to also have a logic that corresponds to the above semantics. These purposes include (1) reasoning by an agent to determine his own plans, and to reason about their consequences <ref> [17, 25] </ref>, (2) and to do the same for the plans of others [28], and (3) a principled approach to the design of multiagent intelligent systems [29]. Points (1) and (2) directly relate to speech act planning and discourse understanding [4, 10, 18, 27] as well.
Reference: [26] <author> Barbara Hall Partee. </author> <title> Belief-sentences and the limits of semantics. </title> <editor> In Stanley Peters and Esa Saarinen, editors, </editor> <title> Processes, Beliefs, and Questions. </title> <address> D. </address> <publisher> Reidel Publishing Company, </publisher> <address> Dordrecht, The Netherlands, </address> <year> 1982. </year>
Reference-contexts: Barbara Partee notes that the lack of valid axioms involving beliefs (and, by extension, intentions) provides only negative evidence against specific proposals for their semantics <ref> [26, p. 95] </ref>. We feel that positive evidence may be generated when agents of different architectures and computational power are considered. That is, while no axiom seems to hold in general, it is important methodologically to consider axioms that hold under different conditions.
Reference: [27] <author> Raymond Perrault. </author> <title> An application of default logic to speech act theory. </title> <type> Technical Report 90, </type> <institution> Center for the Study of Language and Information, Stanford, </institution> <address> CA, </address> <month> March </month> <year> 1987. </year>
Reference-contexts: We sketch just enough of DRT in this paper so that the presentation here is self-contained. A study of the (mostly AI) literature <ref> [4, 8, 10, 17, 18, 25, 27, 28, 30] </ref> yields the following important properties of intentions. Intentions are about future events. An agent with an intention should believe that it can be realized, at least along some future. <p> Points (1) and (2) directly relate to speech act planning and discourse understanding <ref> [4, 10, 18, 27] </ref> as well. Such a logic would also be useful in multiagent systems where agents must reason about each other's intentions and beliefs to effectively negotiate among themselves. We now turn to a deductive system for our semantics.
Reference: [28] <author> Martha E. Pollack. </author> <title> Inferring Domain Plans in Question Answering. </title> <type> PhD thesis, </type> <institution> University of Pennsylvania, </institution> <year> 1986. </year>
Reference-contexts: 1 Introduction An understanding of intentions is important to several subfields of Artificial Intelligence (AI), especially, speech act theory [3, 4, 10, 12], discourse processing [18], planning [17], and plan recognition <ref> [2, 24, 28] </ref>. We present a formal theory of intentions and beliefs that is based on Discourse Representation Theory (DRT) [5, 6, 19]. Our theory involves a formal model of time and possibility, and also explicitly models the structure of the agents' internal states. <p> We sketch just enough of DRT in this paper so that the presentation here is self-contained. A study of the (mostly AI) literature <ref> [4, 8, 10, 17, 18, 25, 27, 28, 30] </ref> yields the following important properties of intentions. Intentions are about future events. An agent with an intention should believe that it can be realized, at least along some future. <p> These purposes include (1) reasoning by an agent to determine his own plans, and to reason about their consequences [17, 25], (2) and to do the same for the plans of others <ref> [28] </ref>, and (3) a principled approach to the design of multiagent intelligent systems [29]. Points (1) and (2) directly relate to speech act planning and discourse understanding [4, 10, 18, 27] as well. <p> These axioms also may be used to motivate certain inferences that are invalid in general but may be acceptable in special circumstances, e.g., as models of how agents of a particular class deliberate, or used as heuristics or conjectures for reasoning in areas such as plan recognition <ref> [28] </ref>. In future work, we plan to incorporate an explicit account of actions and ability into this theory and also to extend it to the intentions of groups of agents.
Reference: [29] <author> Stanley J. Rosenschein. </author> <title> Formal theories of knowledge in AI and robotics. </title> <journal> New Generation Computing, </journal> <volume> 3(4), </volume> <year> 1985. </year>
Reference-contexts: These purposes include (1) reasoning by an agent to determine his own plans, and to reason about their consequences [17, 25], (2) and to do the same for the plans of others [28], and (3) a principled approach to the design of multiagent intelligent systems <ref> [29] </ref>. Points (1) and (2) directly relate to speech act planning and discourse understanding [4, 10, 18, 27] as well. Such a logic would also be useful in multiagent systems where agents must reason about each other's intentions and beliefs to effectively negotiate among themselves.
Reference: [30] <author> John R. Searle. Intentionality: </author> <title> An essay in the Philosophy of Mind. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK, </address> <year> 1983. </year>
Reference-contexts: Intentions are important attitudes of intelligent agents and, for resource-bounded agents, cannot be reduced to simple considerations of the optimality of decisions. Their importance to cognition and their use in AI has been defended extensively in the literature <ref> [4, 8, 11, 10, 12, 17, 18, 30] </ref>. The roles of intentions mentioned below are especially important when one is interested in agents who would not otherwise (because of their limitations) be able to make appropriate or rational decisions. <p> We sketch just enough of DRT in this paper so that the presentation here is self-contained. A study of the (mostly AI) literature <ref> [4, 8, 10, 17, 18, 25, 27, 28, 30] </ref> yields the following important properties of intentions. Intentions are about future events. An agent with an intention should believe that it can be realized, at least along some future.
Reference: [31] <author> Robert C. Stalnaker. </author> <title> Inquiry. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1984. </year> <month> 25 </month>
Reference-contexts: This, of course, depends on what one might want to do with such a theory. A theory of intentions is needed at the foundational level of study in AI and Cognitive Science in order to complete an account of intelligent and, possibly, rational agency <ref> [13, 31] </ref>. Intentions are important attitudes of intelligent agents and, for resource-bounded agents, cannot be reduced to simple considerations of the optimality of decisions. Their importance to cognition and their use in AI has been defended extensively in the literature [4, 8, 11, 10, 12, 17, 18, 30].
References-found: 31

