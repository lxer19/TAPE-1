URL: http://www.cs.utah.edu/~ldurbeck/sound-shapes.ps
Refering-URL: http://www.cs.utah.edu/~ldurbeck/seminar.html
Root-URL: 
Email: fkvdoel j paig@cs.ubc.ca  
Title: The Sounds of Physical Shapes user to obtain important auditory clues about the objects in
Author: Kees van den Doel and Dinesh K. Pai 
Note: This allows the  The framework has been implemented in a Sonic Explorer program, which simulates  This work was supported in part by grants from the  Advanced Systems Institute, and NSERC.  
Date: July 30, 1996  
Address: Vancouver, Canada  BC  
Affiliation: Department of Computer Science University of British Columbia  Institute for Robotics and Intelligent Systems, the  
Abstract: We propose a general framework for the simulation of sounds produced by colliding physical objects in a virtual reality environment. The framework is based on the vibration dynamics of bodies. The computed sounds depend on the material of the body, its shape, and the location of the contact. Specifically, we show how to compute (1) the spectral signature of each body (its natural frequencies), which depends on the material and the shape, (2) the "timbre" of the vibration (the relative amplitudes of the spectral components) generated by an impulsive force applied to the object at a grid of locations, (3) the decay rates of the various frequency components which correlates with the type of material, based on the its internal friction parameter and finally (4) the mapping of sounds on to the object's geometry for real time rendering of the resulting sound. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> MATLAB Reference Guide. The MathWorks, Inc., </institution> <year> 1992. </year>
Reference-contexts: We have computed the eigenfunctions and the spectrum with an adaptation of the method of partial solutions, [7]. As an aside, we note that the first eigenfunction features prominently on the cover of the MATLAB reference guide <ref> [1] </ref>. We have implemented a Sonic Explorer, which allows us to create a graphical scene with objects of the above types. The sounds associated with a grid of points on each object are precomputed and stored as digital samples.
Reference: [2] <author> J. Airey, J. Rohlf, and Jr. F. P. Brooks. </author> <title> Towards image realism with interactive updates in complex virtual building environments. </title> <booktitle> Computer Graphics: Proc. 1990 Symposium on Interactive 3D Graphics, </booktitle> <volume> 24(2) </volume> <pages> 41-50, </pages> <year> 1990. </year>
Reference-contexts: The cognitive importance of realistic sounds is well known in the entertainment industry where sampled sound effects are added to the scene to enhance realism. As simulations become more interactive, for instance in large architectural walkthroughs <ref> [2] </ref> and virtual reality, synthesizing realistic object sounds directly from physical models and rendering them in real time will be increasingly important. The generation of sounds can be characterized as shown in Figure 1, which depicts the process as a pipeline similar to the sound rendering pipeline of [24].
Reference: [3] <author> Durand R. Begault. </author> <title> 3-D Sound for Virtual Reality and Multimedia. </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1994. </year>
Reference-contexts: Off-the-shelf audio hardware is already available to "place" a sound in space, using the "head related transfer function". For a review of this topic we refer to <ref> [3] </ref>. O*ine computation of acoustical properties of performance halls, in the context of graphical visualization techniques was investigated in [22]. <p> Our implementation allows the optional use of a set of HRTF filters to spatialize the sounds completely in three dimensions using the public domain Kemar HRTF filters [8]. However, as we have no specialized hardware, this introduces unacceptable delays. A simple left-right localization based on interaural delays <ref> [3] </ref> does perform in real time. 16 7 Discussion and Conclusions We have developed a framework to add an auditory component to real time simulation environments.
Reference: [4] <author> Albert S. Bregman. </author> <title> Auditory Scene Analysis. </title> <publisher> MIT Press, </publisher> <address> London, </address> <year> 1990. </year>
Reference-contexts: For a book on vibration analysis we refer to [6]. For an application of vibration analysis to animation, see [18]. A survey of the use of the auditory channel in virtual reality is given in [5, Chapter 3]. More general treatises on sound and hearing are <ref> [16, 4] </ref>. 1.2 Overview In this article we investigate the computation and rendering of sounds emitted by colliding bodies. <p> The click or onset has some role in identifying the sound. For example, try listening to a recording of a flute played backwards. The sound is no longer as clearly recognizable as a flute, even though the sustained part of the sound is unchanged. See also <ref> [4, 16] </ref>. 4 Nevertheless, most information about the nature of the object is present in the sustained part. To obtain this, we need to compute the vibrations of an object when it is struck, and compute the resulting sound emitted. <p> Perception of timbre is a complex subject, see for example <ref> [4, 16] </ref> for a discussion, so we can not expect to be able to formulate such a sound-distance measure easily and accurately.
Reference: [5] <author> N. I. Durlach and A. S. Mavor, </author> <title> editors. Virtual Reality, Scientific and technological challenges. </title> <publisher> National Academy Press, </publisher> <address> Washington, D. C., </address> <year> 1995. </year>
Reference-contexts: For completeness we also briefly describe other parts of the pipeline such as impact dynamics and environment modeling. 1.1 Related Work A review of the scientific and technological issues of auditory displays can be found in <ref> [5] </ref>. An overview of the physical factors involved in producing natural sounds was presented in 3 [10]. Several synthesis methods for impact, scraping, and composite sounds were described in [9]. <p> Recently counterexamples have been found [11]. A standard work on acoustics is [17]. For a book on vibration analysis we refer to [6]. For an application of vibration analysis to animation, see [18]. A survey of the use of the auditory channel in virtual reality is given in <ref> [5, Chapter 3] </ref>. More general treatises on sound and hearing are [16, 4]. 1.2 Overview In this article we investigate the computation and rendering of sounds emitted by colliding bodies. <p> An interesting direction for future research is to integrate the Sonic Explorer with a haptic interface <ref> [5, Chapter 4] </ref>. We expect that a virtual reality environment with three sensory feedback channels (sight, hearing, and touch) will provide a significant enhancement. As digital samples tend to take large amount of storage, the question of how many sounds need to be stored for each object comes to mind.
Reference: [6] <author> Frank Fahy. </author> <title> Sound and Structural Vibration. Radiation, Transmission and Response. </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1985. </year>
Reference-contexts: Recently counterexamples have been found [11]. A standard work on acoustics is [17]. For a book on vibration analysis we refer to <ref> [6] </ref>. For an application of vibration analysis to animation, see [18]. A survey of the use of the auditory channel in virtual reality is given in [5, Chapter 3]. <p> Of course, the a S n are only defined up to a multiplicative constant (corresponding to the volume setting of the audio hardware). For a more detailed treatment of the radiation of vibrating plates, we refer to books on vibration analysis <ref> [19, 20, 6] </ref>. 13 4 Sounds and Material Properties When the object is struck, each frequency mode is excited with an initial amplitude a i , which depends on where the object is struck. The relative magnitudes of the amplitudes a i determines the "timbre" of the sound. <p> We have considered systems that can be described by a linear wave equation on some domain, which covers many systems. We have not taken the directionality of the sound emitted into account. To do so involves radiation theory, see for example <ref> [6] </ref>, but this falls in the next stage of the pipeline depicted in Figure 1. These ideas were implemented in a Sonic Explorer, which allows the user to explore the environment by hitting various objects with a virtual drumstick.
Reference: [7] <author> L. Fox, P. Henrici, and C. Moler. </author> <title> Approximations and bounds for eigenvalues of elliptical operators. </title> <journal> SIAM J. Num. Analy., </journal> <volume> 4 </volume> <pages> 89-102, </pages> <year> 1967. </year>
Reference-contexts: In general one has to resort to numerical methods. For membranes, the problem reduces to the solution of the Laplace equation on a given domain, which is a well studied problem. We mention the method of particular solutions <ref> [7] </ref>, which we have adapted for the example of the L-shaped membrane, described below in Section 6. For plates, the operator A is fourth order, and a more general finite element method can be used. <p> This problem has received some attention in the literature, as the resulting boundary value problem requires some refined numerical methods. We have computed the eigenfunctions and the spectrum with an adaptation of the method of partial solutions, <ref> [7] </ref>. As an aside, we note that the first eigenfunction features prominently on the cover of the MATLAB reference guide [1]. We have implemented a Sonic Explorer, which allows us to create a graphical scene with objects of the above types.
Reference: [8] <author> Bill Gardner and Keith Martin. </author> <title> HRTF measurements of a KEMAR dummy-head microphone. </title> <type> Technical Report 280, </type> <institution> MIT Media Lab Perceptual Computing, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Our implementation allows the optional use of a set of HRTF filters to spatialize the sounds completely in three dimensions using the public domain Kemar HRTF filters <ref> [8] </ref>. However, as we have no specialized hardware, this introduces unacceptable delays. A simple left-right localization based on interaural delays [3] does perform in real time. 16 7 Discussion and Conclusions We have developed a framework to add an auditory component to real time simulation environments.
Reference: [9] <author> W. W. Gaver. </author> <title> Synthesizing auditory icons. </title> <booktitle> In Proceedings of the ACM INTERCHI'93, </booktitle> <pages> pages 228-235, </pages> <year> 1993. </year>
Reference-contexts: An overview of the physical factors involved in producing natural sounds was presented in 3 [10]. Several synthesis methods for impact, scraping, and composite sounds were described in <ref> [9] </ref>. However, no method was given to compute the free parameters of the synthesis methods, such as the set of eigenfrequencies, the relative amplitudes of the partials, and the bandwidths of the frequencies. With the methods described in this paper, the parameters of these synthesis methods can be computed.
Reference: [10] <author> W. W. Gaver. </author> <title> What in the world do we hear?: An ecological approach to auditory event perception. </title> <journal> Ecological Psychology, </journal> <volume> 5(1) </volume> <pages> 1-29, </pages> <year> 1993. </year>
Reference-contexts: An overview of the physical factors involved in producing natural sounds was presented in 3 <ref> [10] </ref>. Several synthesis methods for impact, scraping, and composite sounds were described in [9]. However, no method was given to compute the free parameters of the synthesis methods, such as the set of eigenfrequencies, the relative amplitudes of the partials, and the bandwidths of the frequencies.
Reference: [11] <author> C. Gordon, D. Webb, and S. Wolpert. </author> <title> Isospectral plane domains and surfaces via Riemannian orbifolds. </title> <journal> Invent. Math., </journal> <volume> 110 </volume> <pages> 1-22, </pages> <year> 1992. </year>
Reference-contexts: For a more mathematically oriented example of the relation between shape and sound for membranes, we refer to the long standing open problem "Can one hear the shape of a drum?", which was posed in 1966 [14]. Recently counterexamples have been found <ref> [11] </ref>. A standard work on acoustics is [17]. For a book on vibration analysis we refer to [6]. For an application of vibration analysis to animation, see [18]. A survey of the use of the auditory channel in virtual reality is given in [5, Chapter 3].
Reference: [12] <author> James K. Hahn, Joe Geigel, Jong Won Lee, Larry Gritz, Tapio Takala, and Suneil Mishra. </author> <title> An integrated approach to motion and sound. </title> <journal> Journal of Visualization and Computer Animation, </journal> <volume> 6(2) </volume> <pages> 109-123, </pages> <year> 1992. </year> <month> 18 </month>
Reference-contexts: Takala and Hahn introduced the concept of sound rendering for computer animation [24]. They associated a characteristic sound with each object which could then be rendered after filtering the sound to model the environmental effects. Recently <ref> [12] </ref> they proposed "Timbre Trees," which, like shade trees, provide a scene description language for sounds. While [24] indicated that the collision sounds could, in principle, be generated from vibration analysis, they were concerned mainly with the modulation of sound due to material properties.
Reference: [13] <author> Claes Johnson. </author> <title> Numerical solutions of partial differential equations by the finite element method. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1987. </year>
Reference-contexts: We mention the method of particular solutions [7], which we have adapted for the example of the L-shaped membrane, described below in Section 6. For plates, the operator A is fourth order, and a more general finite element method can be used. See for example <ref> [13] </ref>. 11 12 3 Sound Sources from Vibrating Shapes Suppose one has obtained the frequency spectrum and the eigenfunctions, as shown in Section 2.
Reference: [14] <author> M. Kac. </author> <title> Can one hear the shape of a drum? Mer. </title> <journal> Math. Mon., </journal> <volume> 73(II):1-23, </volume> <year> 1966. </year>
Reference-contexts: For a more mathematically oriented example of the relation between shape and sound for membranes, we refer to the long standing open problem "Can one hear the shape of a drum?", which was posed in 1966 <ref> [14] </ref>. Recently counterexamples have been found [11]. A standard work on acoustics is [17]. For a book on vibration analysis we refer to [6]. For an application of vibration analysis to animation, see [18].
Reference: [15] <author> Eric Krotkov and Roberta Klatzky. </author> <title> Robotic perception of material: Experiments with shape-invariant acoustic measures of material type. </title> <booktitle> In Preprints of the Fourth International Symposium on Experimental Robotics, </booktitle> <address> ISER '95, Stanford, California, </address> <year> 1995. </year>
Reference-contexts: Off-the-shelf audio hardware is already available to "place" a sound in space, using the "head related transfer function". For a review of this topic we refer to [3]. O*ine computation of acoustical properties of performance halls, in the context of graphical visualization techniques was investigated in [22]. In <ref> [26, 15] </ref> the problem of recovering the material type from impact sounds was investigated, and it was proposed to use the internal friction parameter, which is an approximate material property, as a characteristic signature of the material. <p> The decay rate of each mode is assumed to be determined by the internal friction parameter, which is an approximate material property <ref> [26, 15] </ref>. In effect, the decay rate of a component is assumed to be proportional to the frequency, with the constant determined by the internal friction parameter. Besides the natural frequencies there is a brief transient, a "click", mentioned before, which we model by a short burst of white noise. <p> In [26] a method was proposed to identify the material type from the sound emitted by a struck object, by extracting the internal friction parameter of the material via Equation 12. Such a model is also used in [24] to simulate object sounds. Some experiments were reported in <ref> [15] </ref>, where it was concluded that a rough characterization of material was indeed possible. However, the internal friction parameter is only approximately invariant over object shape. See also [25]. To emulate external damping of the object, we add an overall decay factor of e t=t 0 .
Reference: [16] <author> Brian C. J. Moore. </author> <title> An Introduction to the Psychology of Hearing. </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1986. </year>
Reference-contexts: For a book on vibration analysis we refer to [6]. For an application of vibration analysis to animation, see [18]. A survey of the use of the auditory channel in virtual reality is given in [5, Chapter 3]. More general treatises on sound and hearing are <ref> [16, 4] </ref>. 1.2 Overview In this article we investigate the computation and rendering of sounds emitted by colliding bodies. <p> The click or onset has some role in identifying the sound. For example, try listening to a recording of a flute played backwards. The sound is no longer as clearly recognizable as a flute, even though the sustained part of the sound is unchanged. See also <ref> [4, 16] </ref>. 4 Nevertheless, most information about the nature of the object is present in the sustained part. To obtain this, we need to compute the vibrations of an object when it is struck, and compute the resulting sound emitted. <p> Perception of timbre is a complex subject, see for example <ref> [4, 16] </ref> for a discussion, so we can not expect to be able to formulate such a sound-distance measure easily and accurately.
Reference: [17] <author> Philip Morse. </author> <title> Vibration and Sound. </title> <journal> American Institute of Physics for the Acoustical Society of America, </journal> <note> fourth edition, </note> <year> 1976. </year>
Reference-contexts: For a more mathematically oriented example of the relation between shape and sound for membranes, we refer to the long standing open problem "Can one hear the shape of a drum?", which was posed in 1966 [14]. Recently counterexamples have been found [11]. A standard work on acoustics is <ref> [17] </ref>. For a book on vibration analysis we refer to [6]. For an application of vibration analysis to animation, see [18]. A survey of the use of the auditory channel in virtual reality is given in [5, Chapter 3]. <p> We will illustrate it with a rectangular membrane, but the framework is quite general; we have used it to generate sounds of strings, bars, plates and other objects. The framework is based on the well developed models in the literature on vibration or acoustics, for example <ref> [17] </ref> for the calculus involved we refer to [23]. 6 7 2.1 Vibration Modes from Shape The vibration of the object is described by a function (x; t), which represents the deviation from equilibrium of the surface, defined on some region S, which defines the shape of the object. <p> Typically, we will only use the frequencies in the audible range. For more details and a more rigorous treatment of this problem for the special cases of the ideal string and the circular membrane, see <ref> [17] </ref>. Using equations 8 and 9, and substituting them in equations 4 and 5 we obtain the amplitudes of the vibration modes as a function of the impact location as a n = cff n ! n and 10 The energy of the vibration is determined by the impact strength.
Reference: [18] <author> Alex Pentland and J. Williams. </author> <title> Good vibrations: Modal dynamics for graphics and animation. </title> <journal> Proc. SIGGRAPH'89, ACM Computer Graphics, </journal> <volume> 23(3) </volume> <pages> 215-222, </pages> <year> 1992. </year>
Reference-contexts: Recently counterexamples have been found [11]. A standard work on acoustics is [17]. For a book on vibration analysis we refer to [6]. For an application of vibration analysis to animation, see <ref> [18] </ref>. A survey of the use of the auditory channel in virtual reality is given in [5, Chapter 3]. More general treatises on sound and hearing are [16, 4]. 1.2 Overview In this article we investigate the computation and rendering of sounds emitted by colliding bodies.
Reference: [19] <author> A. A. Shabana. </author> <title> Theory of Vibration, Volume I: An Introduction. </title> <publisher> Springer-Verlag, </publisher> <address> London, </address> <year> 1991. </year>
Reference-contexts: Of course, the a S n are only defined up to a multiplicative constant (corresponding to the volume setting of the audio hardware). For a more detailed treatment of the radiation of vibrating plates, we refer to books on vibration analysis <ref> [19, 20, 6] </ref>. 13 4 Sounds and Material Properties When the object is struck, each frequency mode is excited with an initial amplitude a i , which depends on where the object is struck. The relative magnitudes of the amplitudes a i determines the "timbre" of the sound.
Reference: [20] <author> A. A. Shabana. </author> <title> Theory of Vibration, Volume II: Discrete and Continuous Systems. </title> <publisher> Springer-Verlag, </publisher> <address> London, </address> <year> 1991. </year>
Reference-contexts: Of course, the a S n are only defined up to a multiplicative constant (corresponding to the volume setting of the audio hardware). For a more detailed treatment of the radiation of vibrating plates, we refer to books on vibration analysis <ref> [19, 20, 6] </ref>. 13 4 Sounds and Material Properties When the object is struck, each frequency mode is excited with an initial amplitude a i , which depends on where the object is struck. The relative magnitudes of the amplitudes a i determines the "timbre" of the sound.
Reference: [21] <author> Juhani Siira and Dinesh K. Pai. </author> <title> Haptic Textures A Stochastic Approach. </title> <booktitle> In Proceedings of the 1996 International Conference on Robotics and Automation, </booktitle> <pages> pages 557-562, </pages> <year> 1996. </year>
Reference: [22] <author> Adam Stettner and Donald P. Greenberg. </author> <title> Computer graphics visualization for acoustic simulation. </title> <journal> Proc. SIGGRAPH'89, Computer Graphics, </journal> <volume> 23(3) </volume> <pages> 195-206, </pages> <year> 1989. </year>
Reference-contexts: Off-the-shelf audio hardware is already available to "place" a sound in space, using the "head related transfer function". For a review of this topic we refer to [3]. O*ine computation of acoustical properties of performance halls, in the context of graphical visualization techniques was investigated in <ref> [22] </ref>. In [26, 15] the problem of recovering the material type from impact sounds was investigated, and it was proposed to use the internal friction parameter, which is an approximate material property, as a characteristic signature of the material.
Reference: [23] <author> Gilbert Strang. </author> <title> Introduction to Applied mathematics. </title> <publisher> Wellesley-Cambridge Press, </publisher> <year> 1986. </year>
Reference-contexts: The framework is based on the well developed models in the literature on vibration or acoustics, for example [17] for the calculus involved we refer to <ref> [23] </ref>. 6 7 2.1 Vibration Modes from Shape The vibration of the object is described by a function (x; t), which represents the deviation from equilibrium of the surface, defined on some region S, which defines the shape of the object.
Reference: [24] <author> Tapio Takala and James Hahn. </author> <title> Sound rendering. </title> <journal> Proc. SIGGRAPH'92, ACM Computer Graphics, </journal> <volume> 26(2) </volume> <pages> 211-220, </pages> <year> 1992. </year>
Reference-contexts: The generation of sounds can be characterized as shown in Figure 1, which depicts the process as a pipeline similar to the sound rendering pipeline of <ref> [24] </ref>. While this is a simplification, it indicates the major computational tasks in going from a collision event to the sound heard by a human ear. <p> With the methods described in this paper, the parameters of these synthesis methods can be computed. Takala and Hahn introduced the concept of sound rendering for computer animation <ref> [24] </ref>. They associated a characteristic sound with each object which could then be rendered after filtering the sound to model the environmental effects. Recently [12] they proposed "Timbre Trees," which, like shade trees, provide a scene description language for sounds. While [24] indicated that the collision sounds could, in principle, be <p> introduced the concept of sound rendering for computer animation <ref> [24] </ref>. They associated a characteristic sound with each object which could then be rendered after filtering the sound to model the environmental effects. Recently [12] they proposed "Timbre Trees," which, like shade trees, provide a scene description language for sounds. While [24] indicated that the collision sounds could, in principle, be generated from vibration analysis, they were concerned mainly with the modulation of sound due to material properties. They did not synthesize sounds which account for the shapes of the colliding objects or the location of the collision on the objects. <p> In [26] a method was proposed to identify the material type from the sound emitted by a struck object, by extracting the internal friction parameter of the material via Equation 12. Such a model is also used in <ref> [24] </ref> to simulate object sounds. Some experiments were reported in [15], where it was concluded that a rough characterization of material was indeed possible. However, the internal friction parameter is only approximately invariant over object shape. See also [25]. <p> The sound becomes brighter for impacts near the ends of the string. The frequency spectrum is harmonic, i.e. all frequencies are integer multiples of the lowest (fundamental) frequency. The amplitudes a n are inversely proportional to n, for large n, in contrast to a plucked string, considered in <ref> [24] </ref>, where they decay as 1=n 2 . This is one factor accounting for the difference between a piano and a guitar sound, for example. 15 2. The rigid bar.
Reference: [25] <author> C. A. Wert. </author> <title> Internal friction in solids. </title> <journal> Journal of Applied Physics, </journal> <volume> 60(6) </volume> <pages> 1888-1895, </pages> <year> 1986. </year>
Reference-contexts: Such a model is also used in [24] to simulate object sounds. Some experiments were reported in [15], where it was concluded that a rough characterization of material was indeed possible. However, the internal friction parameter is only approximately invariant over object shape. See also <ref> [25] </ref>. To emulate external damping of the object, we add an overall decay factor of e t=t 0 . This also allows us to adjust the length of the emitted sound, while maintaining its "material character", which is determined by .
Reference: [26] <author> Richard P. Wildes and Whitman A. Richards. </author> <title> Recovering material properties from sound. </title> <editor> In Whitman Richards, editor, </editor> <booktitle> Natural Computation, </booktitle> <address> Cambridge, Massachusetts, 1988. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Off-the-shelf audio hardware is already available to "place" a sound in space, using the "head related transfer function". For a review of this topic we refer to [3]. O*ine computation of acoustical properties of performance halls, in the context of graphical visualization techniques was investigated in [22]. In <ref> [26, 15] </ref> the problem of recovering the material type from impact sounds was investigated, and it was proposed to use the internal friction parameter, which is an approximate material property, as a characteristic signature of the material. <p> The decay rate of each mode is assumed to be determined by the internal friction parameter, which is an approximate material property <ref> [26, 15] </ref>. In effect, the decay rate of a component is assumed to be proportional to the frequency, with the constant determined by the internal friction parameter. Besides the natural frequencies there is a brief transient, a "click", mentioned before, which we model by a short burst of white noise. <p> Each mode is assumed to decay exponentially, with decay time t i = f i tan where is the internal friction parameter. The internal friction parameter is roughly invariant over object shape, and depends on the material only. In <ref> [26] </ref> a method was proposed to identify the material type from the sound emitted by a struck object, by extracting the internal friction parameter of the material via Equation 12. Such a model is also used in [24] to simulate object sounds.
References-found: 26

