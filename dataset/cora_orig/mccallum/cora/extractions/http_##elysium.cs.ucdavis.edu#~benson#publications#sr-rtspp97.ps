URL: http://elysium.cs.ucdavis.edu/~benson/publications/sr-rtspp97.ps
Refering-URL: http://elysium.cs.ucdavis.edu/~benson/publications/publications.html
Root-URL: http://www.cs.ucdavis.edu
Email: fbenson,olssong@cs.ucdavis.edu  
Title: A Portable Run-Time System for the SR Concurrent Programming Language  
Author: Gregory D. Benson and Ronald A. Olsson tel:()- fax:()- 
Address: CA 95616, U.S.A  
Affiliation: Department of Computer Science University of California, Davis,  
Abstract: A run-time system is the glue between the compiler-generated code and the underlying platform. In addition to having a significant impact on performance, run-time systems are also largely responsible for the degree of portability of a language implementation. Concurrent programming languages are particularly dependent on threads and communication, both of which can vary greatly from platform to platform. This paper presents our current work in developing a portable run-time system for the SR programming language. We argue that portability must be addressed at multiple layers in a run-time system, and not just in a single virtual machine layer. We also present detailed design issues of our run-time system, especially with respect to the thread interface. Performance results are presented for three Linux-based thread packages. Although our work focused on the SR implementation, our overall approach and general observations should also be applicable to implementations of other concurrent programming languages.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Accetta et al. </author> <title> A new kernel foundation for UNIX development. </title> <booktitle> In Proceedings of the Summer 1986 USENIX Conference, </booktitle> <pages> pages 93-112, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: In MkSR [6, 8] we are investigating how to directly support the SR run-time system on top of the Mach microkernel <ref> [1] </ref>. Likewise, SR on Panda [23] was a project in which the SR run-time system was modified to run on the Panda [9] portability layer. In both projects, we started with the original SR implementation, which we refer to as UnixSR. <p> Our approach to I/O is to remove any notion of Unix I/O from all RTS components. This enables us to more easily target non-Unix platforms, such as Windows 95, Windows NT 5 , Mach <ref> [1] </ref>, and Fluke [12]. A more difficult issue in dealing with portable I/O is whether or not the underlying thread package or operating system supports blocking I/O threads. If the underlying interfaces allow threads to block on I/O operations, then the RTS is greatly simplified.
Reference: [2] <author> G. R. Andrews and R. A. Olsson. </author> <title> The SR Programming Language: Concurrency in Practice. </title> <publisher> The Ben-jamin/Cummings Publishing Co., </publisher> <address> Redwood City, California, </address> <year> 1993. </year>
Reference-contexts: This approach is more of a "philosophy" rather than a methodology and is complementary to the virtual machine approach. We are using the SR concurrent programming language <ref> [2] </ref> as a vehicle to explore various run-time support issues. In particular, we have been investigating the design and implementation of SR's run-time support on top of microkernel operating systems [6, 8]. <p> However, the component approach, at least initially, may result in more complex run-time systems from the flexibility of specialization. 3 The SR Programming Language SR is a language for writing concurrent programs, both parallel and distributed <ref> [2, 3] </ref>. Many of SR's abstractions correspond to modern hardware, including shared memory multiprocessors and networks of workstations. While SR does not provide expensive abstractions such as distributed shared memory or distributed shared objects [5], it does provide simple and consistent mechanisms for expressing several forms of explicit communication.
Reference: [3] <author> G. R. Andrews, R. A. Olsson, M. Coffin, I. Elshoff, K. Nilsen, T. Purdin, and G. Townsend. </author> <title> An overview of the SR language and implementation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(1) </volume> <pages> 51-86, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: However, the component approach, at least initially, may result in more complex run-time systems from the flexibility of specialization. 3 The SR Programming Language SR is a language for writing concurrent programs, both parallel and distributed <ref> [2, 3] </ref>. Many of SR's abstractions correspond to modern hardware, including shared memory multiprocessors and networks of workstations. While SR does not provide expensive abstractions such as distributed shared memory or distributed shared objects [5], it does provide simple and consistent mechanisms for expressing several forms of explicit communication.
Reference: [4] <author> M. S. Atkins and R. A. Olsson. </author> <title> Performance of multi-tasking and synchronization mechanisms in the programming language SR. </title> <journal> Software Practice and Experience, </journal> <volume> 18(9) </volume> <pages> 879-895, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: The test machine is a Pentium 166Mhz with 64 MBs of RAM running Linux 2.0.24. The microbenchmarks presented in this section come from a set of benchmarks designed to evaluate the performance of SR's concurrency mechanisms <ref> [4] </ref>. The benchmarks were run on a lightly loaded system and the results are the medians of several runs, where each run consisted of 10 6 to 10 8 iterations depending on the benchmark. The benchmarks do not include program startup time or RTS initialization.
Reference: [5] <author> H. E. Bal, M. F. Kaashoek, and A. S. Tanenbaum. Orca: </author> <title> A language for parallel programming of distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(3) </volume> <pages> 190-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Many of SR's abstractions correspond to modern hardware, including shared memory multiprocessors and networks of workstations. While SR does not provide expensive abstractions such as distributed shared memory or distributed shared objects <ref> [5] </ref>, it does provide simple and consistent mechanisms for expressing several forms of explicit communication. SR's syntax and semantics allow the programmer to concentrate on the communication and synchronization structure of a program without having to be concerned with low-level interfaces or interface definition languages.
Reference: [6] <author> G. D. Benson and R. A. Olsson. </author> <title> Towards microkernel support for the SR concurrent programmiing language. </title> <booktitle> In Proceedings of the International Conference on Parallel and Distributed Processing Techniques and Applications, </booktitle> <pages> pages 1513-1524, </pages> <address> Sunnyvale, CA, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: We are using the SR concurrent programming language [2] as a vehicle to explore various run-time support issues. In particular, we have been investigating the design and implementation of SR's run-time support on top of microkernel operating systems <ref> [6, 8] </ref>. This work combined with a project in which SR was implemented using Panda [23] has prompted us to develop a more flexible run-time system that is intended to run on a wide range of platforms. <p> Operation capabilities serve as pointers to operations; they can be passed as variables, thus allowing the creation of arbitrary communication topologies. 4 Design and Implementation Issues The impetus for developing a portable run-time system grew out of two projects: MkSR and SR on Panda. In MkSR <ref> [6, 8] </ref> we are investigating how to directly support the SR run-time system on top of the Mach microkernel [1]. Likewise, SR on Panda [23] was a project in which the SR run-time system was modified to run on the Panda [9] portability layer.
Reference: [7] <author> G. D. Benson, R. A. Olsson, and R. Pandey. </author> <title> On the decomposition of run-time support for concurrent programming languages. </title> <booktitle> In Proceedings of the Workshop on High-Level Programming Models and Supportive Environments, </booktitle> <pages> pages 86-94, </pages> <address> Honolulu, Hawaii, </address> <month> April </month> <year> 1996. </year> <booktitle> held in conjuction with the 10th International Parallel Processing Symposium (IPPS'96). </booktitle>
Reference-contexts: Various forms of the component approach have been proposed in <ref> [7] </ref> and [18]. The basic idea is to isolate system dependencies into components so that they can be specialized on a per-platform basis.
Reference: [8] <author> G.D. Benson and R.A. Olsson. </author> <title> The Design of Microkernel Support for the SR Concurrent Programming Language, </title> <booktitle> chapter 17, </booktitle> <pages> pages 227-240. </pages> <booktitle> Languages, Compilers, and Run-Time Systems for Scalable Computers. </booktitle> <publisher> Kluwer Academic Publishing, </publisher> <address> Boston, MA, </address> <year> 1996. </year> <editor> B. K. Szymanski and B. </editor> <title> Sinharoy (editors). </title>
Reference-contexts: We are using the SR concurrent programming language [2] as a vehicle to explore various run-time support issues. In particular, we have been investigating the design and implementation of SR's run-time support on top of microkernel operating systems <ref> [6, 8] </ref>. This work combined with a project in which SR was implemented using Panda [23] has prompted us to develop a more flexible run-time system that is intended to run on a wide range of platforms. <p> Operation capabilities serve as pointers to operations; they can be passed as variables, thus allowing the creation of arbitrary communication topologies. 4 Design and Implementation Issues The impetus for developing a portable run-time system grew out of two projects: MkSR and SR on Panda. In MkSR <ref> [6, 8] </ref> we are investigating how to directly support the SR run-time system on top of the Mach microkernel [1]. Likewise, SR on Panda [23] was a project in which the SR run-time system was modified to run on the Panda [9] portability layer.
Reference: [9] <author> R. Bhoedjang, T. Ruhl, R. Hofman, K. Langendoen, H. Bal, and F. Kaashoek. Panda: </author> <title> A portable platform to support parallel programming languages. </title> <booktitle> In Symposium on Experience with Distributed and Multiprocessor Systems IV, </booktitle> <pages> pages 213-226, </pages> <address> San Diego, California, </address> <month> September </month> <year> 1993. </year> <booktitle> USENIX. </booktitle>
Reference-contexts: The interfaces define a virtual machine, which is accessed by higher-level modules of a run-time system. Platform dependencies are restricted to the implementation of the virtual machine. Both Panda <ref> [9] </ref> and Nexus [14] are based on this approach. These systems have demonstrated fl Appears in Proceedings of the Workshop on Run-Time Systems for Parallel Programming, IR-417, Department of Mathematics and Computer Science, pages 21-30, Vrije Universiteit, Amsterdam, The Netherlands, April 1997. <p> The virtual machine is simply a layer between the run-time system and the kernel primitives or the hardware. This layer provides an abstraction of the underlying operating system and architecture. The Panda system <ref> [9] </ref> is a portable platform for supporting parallel programming languages. The platform consists of two layers: the system layer and the Panda layer. The system layer interfaces with operating system kernel primitives; this level is kernel dependent. The Panda layer provides a virtual machine for supporting run-time systems. <p> In addition, virtual machines can be designed to be extremely modular "underneath" the fixed interface. Such modularity results in virtual machines that are easier to port (e.g., both Panda <ref> [9] </ref> and Nexus [14] have modular designs). However, the fixed interface can hide system dependent details that may be important to the implementation of a run-time system. If a language implementor "codes around" the fixed interface, the advantages of the virtual machine are lost. <p> In MkSR [6, 8] we are investigating how to directly support the SR run-time system on top of the Mach microkernel [1]. Likewise, SR on Panda [23] was a project in which the SR run-time system was modified to run on the Panda <ref> [9] </ref> portability layer. In both projects, we started with the original SR implementation, which we refer to as UnixSR. The run-time system for this implementation is monolithic and tightly coupled to a custom, user-level thread interface, to Unix I/O, and to Unix sockets. <p> We sought an approach to portability that would allows us to adapt the SR RTS to a wide range of environments (including virtual machines such as Panda <ref> [9] </ref>, microkernels, and direct hardware). Our observations of the virtual machine approach led to the general principles of the component approach presented in Section 2.2. This section describes several design and implementation issues we encountered while developing our portable run-time system for SR. <p> We also have implementations using CThreads [10] and Panda Threads <ref> [9] </ref>, but we do not provide performance numbers here because the results are not directly comparable to the results for the Linux-based thread packages. These implementations currently run only on architectures that differ from our Linux test machine.
Reference: [10] <author> E. Cooper and R. Draves. </author> <title> C threads. </title> <type> Technical Report CMU-CS-88-154, </type> <institution> Carnegie Mellon University, Department of Computer Science, </institution> <year> 1988. </year>
Reference-contexts: We also have implementations using CThreads <ref> [10] </ref> and Panda Threads [9], but we do not provide performance numbers here because the results are not directly comparable to the results for the Linux-based thread packages. These implementations currently run only on architectures that differ from our Linux test machine.
Reference: [11] <author> B. </author> <title> Ford and Flux Project Members. The Flux Operating System Toolkit. </title> <institution> Department of Computer Science, Universiy of Utah, </institution> <year> 1996. </year> <note> http://www.cs.utah.edu/projects/flux/oskit/html. </note>
Reference-contexts: A very straightforward approach is to clearly document dependencies and any state that is shared among components, therefore when a component is specialized, all the information needed for replacement is provided. This approach is being used successfully in the Flux Operating System Toolkit <ref> [11] </ref>, a toolkit for building kernels. Another crucial concern is the mitigation of specialization costs. We foresee a technique and corresponding tool that helps track components and their compositions. A good model for such a technique is the x-kernel [16], which is an architecture for specifying and composing micro-protocols.
Reference: [12] <author> B. Ford, M. Hibler, J. Lepreau, P. Tullmann, G. Back, and S. Clawson. </author> <title> Microkernels meet recursive virtual machines. </title> <booktitle> In Proceedings of the Second Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 137-151, </pages> <address> Seattle, WA, </address> <month> October </month> <year> 1996. </year> <institution> USENIX Association. </institution>
Reference-contexts: Our approach to I/O is to remove any notion of Unix I/O from all RTS components. This enables us to more easily target non-Unix platforms, such as Windows 95, Windows NT 5 , Mach [1], and Fluke <ref> [12] </ref>. A more difficult issue in dealing with portable I/O is whether or not the underlying thread package or operating system supports blocking I/O threads. If the underlying interfaces allow threads to block on I/O operations, then the RTS is greatly simplified.
Reference: [13] <author> I. Foster and K. M. Chandy. Fotran M: </author> <title> A Language for Modular Parallel Programming. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 25(1), </volume> <year> 1995. </year>
Reference-contexts: Panda targets run-time systems that provide distributed shared data or distributed shared objects. Nexus [14] is a distributed thread library that provides a virtual machine layer similar to Panda. Nexus was designed to be used as a back-end for concurrent programming languages, e.g., CC++ and Fortran-M <ref> [13] </ref>.
Reference: [14] <author> I. Foster, C. Kesselman, and S. Tuecke. </author> <title> The Nexus approach to integrating multithreading and communication. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 37(1) </volume> <pages> 70-82, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: The interfaces define a virtual machine, which is accessed by higher-level modules of a run-time system. Platform dependencies are restricted to the implementation of the virtual machine. Both Panda [9] and Nexus <ref> [14] </ref> are based on this approach. These systems have demonstrated fl Appears in Proceedings of the Workshop on Run-Time Systems for Parallel Programming, IR-417, Department of Mathematics and Computer Science, pages 21-30, Vrije Universiteit, Amsterdam, The Netherlands, April 1997. <p> The Panda layer provides a virtual machine for supporting run-time systems. The virtual machine supports threads, message passing, remote procedure call, totally-ordered group communication, and collective communication. Panda targets run-time systems that provide distributed shared data or distributed shared objects. Nexus <ref> [14] </ref> is a distributed thread library that provides a virtual machine layer similar to Panda. Nexus was designed to be used as a back-end for concurrent programming languages, e.g., CC++ and Fortran-M [13]. <p> In addition, virtual machines can be designed to be extremely modular "underneath" the fixed interface. Such modularity results in virtual machines that are easier to port (e.g., both Panda [9] and Nexus <ref> [14] </ref> have modular designs). However, the fixed interface can hide system dependent details that may be important to the implementation of a run-time system. If a language implementor "codes around" the fixed interface, the advantages of the virtual machine are lost. <p> In addition, depending on the particular virtual machine, the component approach may lead to quicker development time for new platforms because only the low-level functionality that is required by the run-time system is implemented. This is true when compared to virtual machines, such as Nexus <ref> [14] </ref>, that support several abstractions, many of which may not be needed by a particular run-time system.
Reference: [15] <author> M. Haines. </author> <title> On designing lightweight threads for substrate software. </title> <booktitle> In The Proceeding of the Annual Technical Conference on UNIX and Advanced Computing Systems, </booktitle> <address> Anaheim, California, </address> <month> January </month> <year> 1997. </year> <booktitle> USENIX. </booktitle>
Reference-contexts: This is true despite standardization efforts such as MPI [24] and PThreads [17]. It is also not clear if the standard interfaces are always desirable. For example, it has been observed that many current thread interfaces, including PThreads, are not very flexible <ref> [15] </ref>. Portability can be viewed from two perspectives: from portability clients (programs for which portability is desired or required) and from portability providers (programs, libraries, or interfaces designed to provide portability).
Reference: [16] <author> N. C. Hutchinson and L. L. Peterson. </author> <title> The x-Kernel: An architecture for implementing network protocols. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1) </volume> <pages> 64-76, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Another crucial concern is the mitigation of specialization costs. We foresee a technique and corresponding tool that helps track components and their compositions. A good model for such a technique is the x-kernel <ref> [16] </ref>, which is an architecture for specifying and composing micro-protocols.
Reference: [17] <institution> Institute for Electrical and Electronic Engineers. POSIX P1003.1c, Threads Extension for Portable Operating Systems, </institution> <year> 1995. </year>
Reference-contexts: Unfortunately, the interfaces for threads and communication can vary greatly from platform to platform, thus making it difficult to implement a run-time system for multiple targets. This is true despite standardization efforts such as MPI [24] and PThreads <ref> [17] </ref>. It is also not clear if the standard interfaces are always desirable. For example, it has been observed that many current thread interfaces, including PThreads, are not very flexible [15].
Reference: [18] <author> L. V. Kale, Milind Bhandarkar, Narain Jagathesan, Sanjeev Krishnan, and Joshua Yelon. </author> <title> Converse: An In-teroperable Framework for Parallel Programming. </title> <booktitle> In Proceedings of the 10th International Parallel Processing Symposium, </booktitle> <pages> pages 212-217, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: Various forms of the component approach have been proposed in [7] and <ref> [18] </ref>. The basic idea is to isolate system dependencies into components so that they can be specialized on a per-platform basis. Note that this approach can be complementary to the virtual machine approach, in that system-dependent components may be implemented in terms of virtual machine interfaces for a particular platform.
Reference: [19] <author> G. Kiczales. </author> <title> Foil for the workshop on open implementation. </title> <type> Technical report, </type> <note> Xerox PARC, 1994. http://www.parc.xerox.com/PARC/spl/eca/io/workshop-94/foil/main.html. </note>
Reference-contexts: A component is a very general entity; it encapsulates some type of functionality. We do not place any constraints on components; for example, a component can be implemented as an object-oriented class. However, due to performance requirements it is often necessary to avoid traditional black-box techniques <ref> [19] </ref>. As a consequence, some components may have exposed state that can be manipulated by other components. As a specific example, consider a run-time system that supports processes and semaphores (see Figure 3).
Reference: [20] <author> X. Leroy. LinuxThreads, </author> <year> 1996. </year> <note> http://sunsite.unc.edu/pub/Linux/docs/faqs/Threads-FAQ/html/. </note>
Reference-contexts: In addition to presenting specific design and implementation details of our run-time system, we also give performance results for three Linux-based thread packages: a custom user-level package, MIT PThreads [22], and LinuxThreads <ref> [20] </ref>. These results reveal general performance implications for these thread packages and the performance differences based on thread-package functionality. Although our work focused on the SR implementation, our overall approach and general observations should also be applicable to implementations of other concurrent programming languages.
Reference: [21] <author> D. MacKenzie. Autoconf. </author> <title> Free Software Foundation, </title> <publisher> Inc., </publisher> <address> 2.8 edition, </address> <year> 1996. </year>
Reference-contexts: It is infeasible for portability clients to completely rely on portability providers, as a particular provider usually only targets a subset of possible platforms. To this end, clients must take measures to ensure that multiple providers can be utilized. For example, the GNU Autoconf program <ref> [21] </ref> is a tool to aid portability clients in assessing the capabilities of a target platform so appropriate, automatic adjustments can be made to the source code.
Reference: [22] <editor> C. Provenzano. </editor> <publisher> MIT PThreads, </publisher> <year> 1996. </year> <note> http://www.mit.edu:8001/people/proven/pthreads.html. </note>
Reference-contexts: In addition to presenting specific design and implementation details of our run-time system, we also give performance results for three Linux-based thread packages: a custom user-level package, MIT PThreads <ref> [22] </ref>, and LinuxThreads [20]. These results reveal general performance implications for these thread packages and the performance differences based on thread-package functionality. Although our work focused on the SR implementation, our overall approach and general observations should also be applicable to implementations of other concurrent programming languages.
Reference: [23] <author> T. Ruhl, H. Bal, R. Bhoedjang, K. G. Langendoen, and G. D. Benson. </author> <title> Experience with a portability layer for implementing parallel programming systems. </title> <booktitle> In Proceedings of the International Conference on Parallel and Distributed Processing Techniques and Applications, </booktitle> <pages> pages 1477-1488, </pages> <address> Sunnyvale, CA, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: In particular, we have been investigating the design and implementation of SR's run-time support on top of microkernel operating systems [6, 8]. This work combined with a project in which SR was implemented using Panda <ref> [23] </ref> has prompted us to develop a more flexible run-time system that is intended to run on a wide range of platforms. This paper presents our approach to portability in the SR run-time system and some general observations on portable run-time systems based on our experience. <p> In MkSR [6, 8] we are investigating how to directly support the SR run-time system on top of the Mach microkernel [1]. Likewise, SR on Panda <ref> [23] </ref> was a project in which the SR run-time system was modified to run on the Panda [9] portability layer. In both projects, we started with the original SR implementation, which we refer to as UnixSR.
Reference: [24] <author> The MPI Forum. </author> <title> MPI: A message-passing interface standard. </title> <journal> The International Journal of Supercomputing Applications, </journal> <volume> 8(3/4), </volume> <year> 1994. </year>
Reference-contexts: However, concurrent programming languages make extensive use of threads and communication interfaces. Unfortunately, the interfaces for threads and communication can vary greatly from platform to platform, thus making it difficult to implement a run-time system for multiple targets. This is true despite standardization efforts such as MPI <ref> [24] </ref> and PThreads [17]. It is also not clear if the standard interfaces are always desirable. For example, it has been observed that many current thread interfaces, including PThreads, are not very flexible [15].
References-found: 24

