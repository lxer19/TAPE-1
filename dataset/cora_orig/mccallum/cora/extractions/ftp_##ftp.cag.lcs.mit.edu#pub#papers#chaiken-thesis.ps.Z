URL: ftp://ftp.cag.lcs.mit.edu/pub/papers/chaiken-thesis.ps.Z
Refering-URL: http://www.cag.lcs.mit.edu/alewife/papers/chaiken-thesis.html
Root-URL: 
Title: Cache Coherence Protocols for Large-Scale Multiprocessors  
Author: by David Lars Chaiken Anant Agarwal Arthur C. Smith 
Degree: Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Master of Science at the  The author hereby grants to MIT permission to reproduce and to distribute copies of this thesis document in whole or in part. Signature of Author  Certified by  Assistant Professor of Computer Science and Electrical Engineering Thesis Supervisor Accepted by  Chairman, Departmental Committee on Graduate Students  
Note: c Massachusetts Institute of Technology,  
Date: September 1990  1990  August 31, 1990  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak Ordering ANew Definition. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: Otherwise, individual processors may observe write operations in different orders, leading to a violation of the memory model. Several memory system designers have identified less stringent memory models that allow write requests to be overlapped with other processor accesses to shared memory <ref> [1, 19, 21] </ref>. These weakly-ordered models permit the memory system to overlap certain memory transactions by forbidding certain kinds of data sharing semantics. For example, weakly-ordered systems do not guarantee the appropriate behavior in the scenario depicted by Figure 2-1, unless the two processors synchronize between accesses to location X. <p> ASIM's upper bound is valid only for the original definition of weak ordering [19]. The approximation method does not properly simulate the behavior of other definitions of weak ordering, such as the one proposed in <ref> [1] </ref>. Furthermore, ASIM does not account for techniques that can improve the performance of weakly ordered systems. For instance, a data prefetch mechanism could be used to allow a weakly ordered system to reduce the effects of read misses.
Reference: [2] <author> Yehuda Afek, Geoffrey Brown, and Michael Merritt. </author> <title> A Lazy Cache Algorithm. </title> <booktitle> In Proceedings of the 1989 ACM Symposium on Parallel Algorithms and Architectures, ACM, </booktitle> <address> New York, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Correctness refers to the fact that a protocol actually provides the shared memory model (e.g. sequential consistency) specified for the multiprocessor system. Liveness refers to the fact that the protocol will guarantee forward progress in memory accesses. Recent attempts to prove protocol correctness using the I/O Automata Model <ref> [2, 34] </ref> or home-grown models [7, 16, 43] show that the subject is a difficult one. This difficulty may indicate that correctness proofs for coherence protocols are truly complex in nature, or that a better abstraction is needed between memory model specifications and shared memory implementations.
Reference: [3] <author> Anant Agarwal. </author> <title> Overview of the Alewife Project. </title> <month> July </month> <year> 1990. </year> <note> Alewife Systems Memo #10. </note>
Reference-contexts: If this is the case, then a scalable system somehow must counteract the effects of interprocessor communication latency. As described in <ref> [3] </ref>, the Alewife architecture accomplishes the goal of scalability by exploiting communication locality: A program running on a parallel machine displays communication locality if the probability of communication with various nodes decreases with physical distance.
Reference: [4] <author> Anant Agarwal and Anoop Gupta. </author> <title> Temporal, Processor, and Spatial Locality in Multiprocessor Memory References, </title> <booktitle> chapter 8, </booktitle> <pages> pages 271-295. </pages> <publisher> Plenum Press, </publisher> <year> 1989. </year> <note> Stu Tewksbury Ed. Also appears as MIT VLSI Memo, </note> <year> 1989. </year>
Reference-contexts: Processor locality is defined as the tendency of a processor to repeatedly access a block of data before the block must be relinquished upon a request (typically a write) from another processor <ref> [4] </ref>. 20 2.2 The Cache Coherence Problem The fact that more than one processor can cache a single block of data leads to the cache coherence problem, which is usually explained by an example such as the one that follows: Suppose that processor A and processor B both try to add
Reference: [5] <author> Anant Agarwal, Beng-Hong Lim, David Kranz, and John Kubiatowicz. </author> <month> APRIL: </month>
Reference-contexts: the interplay between software and hardware that is required to enhance locality within processing nodes. 3.1.3 Latency Tolerance When the system can not avoid interprocessor communication, Alewife attempts to tolerate the latency by switching between hardware contexts on the SPARCLE processor, an implementation of the APRIL processor architecture for multiprocessing <ref> [5] </ref>. Each of the four hardware contexts on SPARCLE can contain the state of an executable task.
References-found: 5

