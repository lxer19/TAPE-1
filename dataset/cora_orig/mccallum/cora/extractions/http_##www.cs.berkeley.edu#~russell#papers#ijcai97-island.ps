URL: http://www.cs.berkeley.edu/~russell/papers/ijcai97-island.ps
Refering-URL: http://www.cs.berkeley.edu/~russell/publications.html
Root-URL: 
Title: Space-efficient inference in dynamic probabilistic networks  
Author: John Binder, Kevin Murphy, Stuart Russell 
Note: This research was funded by the National Science Foundation under grant no. FD96-34215, and by ARO under the MURI program "Integrated Approach to Intelligent Sys tems," grant number DAAH04-96-1-0341.  
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division University of California  
Abstract: Dynamic probabilistic networks (DPNs) are a useful tool for modeling complex stochastic processes. The simplest inference task in DPNs is monitoring | that is, computing a posterior distribution for the state variables at each time step given all observations up to that time. Recursive, constant-space algorithms are well-known for monitoring in DPNs and other models. This paper is concerned with hindsight | that is, computing a posterior distribution given both past and future observations. Hindsight is an essential subtask of learning DPN models from data. Existing algorithms for hindsight in DPNs use O(SN ) space and time, where N is the total length of the observation sequence and S is the state space size for each time step. They are therefore impractical for hindsight in complex models with long observation sequences. This paper presents an O(S log N ) space, O(SN log N ) time hindsight algorithm. We demonstrates the effectiveness of the algorithm in two real-world DPN learning problems. We also discuss the possibility of an O(S)-space, O(SN )-time algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [ Alag and Agogino, 1996 ] <author> S. Alag and A. Agogino. </author> <title> Inference using message propagation and topology transformation in vector gaussian continuous networks. </title> <booktitle> In Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence (UAI-96), </booktitle> <pages> pages 20-27, </pages> <address> Portland, Oregon, 1996. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In control theory, this task is called filtering. The well-known Kalman filter [ Kalman, 1960 ] can be viewed as a special case of a continuous-variable DPN in which the sensor model is restricted to be Gaussian and the process model is restricted to be linear with Gaussian noise <ref> [ Alag and Agogino, 1996 ] </ref> . We note that DPNs can handle a much larger variety of processes than Kalman filters.
Reference: [ Dean and Kanazawa, 1989 ] <author> Thomas Dean and Keiji Kanazawa. </author> <title> A model for reasoning about persistence and causation. </title> <journal> Computational Intelligence, </journal> <volume> 5(3) </volume> <pages> 142-150, </pages> <year> 1989. </year>
Reference: [ Forbes et al., 1995 ] <author> Jeff Forbes, Tim Huang, Keiji Kanazawa, and Stuart Russell. </author> <title> The BATmobile: Towards a Bayesian automated taxi. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <address> Montreal, Canada, August 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The sensor model describes P (E t jX t ) and the process model describes P (X t+1 jX t ). the behavior of cars driving on the freeway <ref> [ Forbes et al., 1995 ] </ref> . They are therefore impractical for hindsight in complex models with long observation sequences.
Reference: [ Ghahramani and Jordan, 1995 ] <author> Z. Ghahramani and M. I. Jordan. </author> <title> Factorial hidden Markov models. </title> <type> Technical Report 9502, </type> <institution> MIT Computational Cognitive Science Report, </institution> <year> 1995. </year>
Reference: [ Jensen et al., 1990 ] <author> Finn V. Jensen, Steffen L. Lau-ritzen, and Kristian G. Olesen. </author> <title> Bayesian updating in causal probabilistic networks by local computations. </title> <journal> Computational Statistics Quarterly, </journal> <volume> 5(4) </volume> <pages> 269-282, </pages> <year> 1990. </year>
Reference-contexts: compromise is k = p N , which takes only twice as long as the standard algorithm and needs only O (S N ) space. 5 Experimental results We have implemented the abstract operators FwdOp, BackOp, and CombineOp in terms of a modified version of the Jensen join tree algorithm <ref> [ Jensen et al., 1990 ] </ref> . The details will be presented in another paper, but the basic idea is to modify the triangulation heuristic to ensure that the resulting join tree has a repeating structure.
Reference: [ Kalman, 1960 ] <author> R. E. </author> <title> Kalman. A new approach to linear filtering and prediction problems. </title> <journal> Journal of Basic Engineering, </journal> <pages> pages 35-46, </pages> <month> March </month> <year> 1960. </year>
Reference-contexts: For example, one could estimate the progress of a disease in a patient from clinical observations over time, or estimate the position of a missile from a sequence of radar observations. In control theory, this task is called filtering. The well-known Kalman filter <ref> [ Kalman, 1960 ] </ref> can be viewed as a special case of a continuous-variable DPN in which the sensor model is restricted to be Gaussian and the process model is restricted to be linear with Gaussian noise [ Alag and Agogino, 1996 ] .
Reference: [ Kjaerulff, 1992 ] <author> U. Kjaerulff. </author> <title> A computational scheme for reasoning in dynamic probabilistic networks. </title> <booktitle> In Proceedings of the Eighth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 121-129, </pages> <year> 1992. </year>
Reference: [ Lauritzen, 1995 ] <author> S. L. Lauritzen. </author> <title> The EM algorithm for graphical association models with missing data. </title> <journal> Computational Statistics and Data Analysis, </journal> <volume> 19 </volume> <pages> 191-201, </pages> <year> 1995. </year>
Reference-contexts: In AI, perhaps the most important use of hindsight is in learning. In order to learn a DPN model from observation sequences, it is necessary to compute likelihoods for the hidden variables given all available data <ref> [ Lauritzen, 1995; Russell et al., 1995 ] </ref> , hence hindsight is an integral part of DPN learning.
Reference: [ Russell and Norvig, 1995 ] <author> Stuart J. Russell and Peter Norvig. </author> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1995. </year>
Reference-contexts: We note that DPNs can handle a much larger variety of processes than Kalman filters. Standard arguments (see e.g., <ref> [ Russell and Norvig, 1995, p.509 ] </ref> ) show that monitoring corresponds to the following recursive update equation P (X t+1 jE 0:t+1 ) = ffP (E t+1 jX t+1 )fi x t where ff is a normalizing constant.
Reference: [ Russell et al., 1995 ] <author> Stuart Russell, John Binder, Daphne Koller, and Keiji Kanazawa. </author> <title> Local learning in probabilistic networks with hidden variables. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <pages> pages 1146-52, </pages> <address> Montreal, Canada, August 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In AI, perhaps the most important use of hindsight is in learning. In order to learn a DPN model from observation sequences, it is necessary to compute likelihoods for the hidden variables given all available data <ref> [ Lauritzen, 1995; Russell et al., 1995 ] </ref> , hence hindsight is an integral part of DPN learning.
Reference: [ Smyth et al., 1996 ] <author> P. Smyth, D. Heckerman, and M. Jordan. </author> <title> Probabilistic independence networks for hidden Markov probability models. </title> <type> Technical Report MSR-TR-96-03, </type> <institution> Microsoft Research, </institution> <address> Redmond, Washington, </address> <year> 1996. </year>
Reference-contexts: storing F t and B t at each time step (i.e., caching intermediate results), we get an algorithm that is essentially identical to Pearl's - message-passing algorithm for chains, to the join-tree algorithm operating on an explicitly represented DPN with N slices, or to the forward-backward HMM algorithm (see also <ref> [ Smyth et al., 1996 ] </ref> ). All these approaches require O (SN ) time, but unfortunately require O (SN ) space, which is impractical.
References-found: 11

