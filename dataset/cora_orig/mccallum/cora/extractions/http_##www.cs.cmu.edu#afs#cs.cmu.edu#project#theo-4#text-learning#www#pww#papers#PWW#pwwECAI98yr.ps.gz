URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-4/text-learning/www/pww/papers/PWW/pwwECAI98yr.ps.gz
Refering-URL: 
Root-URL: 
Title: Turning Yahoo into an Automatic Web-Page Classifier  
Author: Dunja Mladenic 
Abstract: The paper describes an approach to automatic Web-page classification based on the Yahoo hierarchy. Machine learning techniques developed for learning on text data are used here on the hierarchical classification structure. The high number of features is reduced by taking into account the hierarchical structure and using feature subset selection based on the method known from information retrieval. Documents are represented as feature-vectors that include n-grams instead of including only single words (unigrams) as commonly used when learning on text data. Based on the hierarchical structure the problem is divided into subproblems, each representing one on the categories included in the Yahoo hierarchy. The result of learning is a set of independent classifiers, each used to predict the probability that a new example is a member of the corresponding category. Experimental evaluation on real-world data shows that the proposed approach gives good results. For more than a half of testing examples a correct category is among the 3 categories with the highest predicted probability. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Agrawal, R., Mannila, H., Srikant, R., Toivonen, H., Verkamo, A.I., </author> <year> 1996. </year> <title> Fast Discovery of Association Rules, </title> <editor> In Fayyad et al. (eds.), </editor> <booktitle> Advances in Knowledge Discovery and Data Mining AAAI Press/The MIT Press, </booktitle> <pages> pp. 307-328. </pages>
Reference-contexts: Each new pass gen-erates features of length i+1 only from the candidate features of length i generated in the previous pass. This process is similar to the large k-itemset generation used in association rules algorithm described in <ref> [1] </ref>. We divide the whole problem into subproblems each corresponding to the individual category. For each of the subprob-lems, a classifier is constructed that predicts the probability that a document is a member of the corresponding category.
Reference: [2] <author> Filo, D., Yang, J., </author> <year> 1997. </year> <institution> Yahoo! Inc. </institution> <note> www.yahoo.com/docs/pr/ </note>
Reference-contexts: 1 Introduction Yahoo <ref> [2] </ref>, a well known Web-pages hierarchy is human constructed and designed for human Web browsing. Already classified documents used to build a hierarchy are Web documents, making the hierarchy biased toward human knowledge areas that are represented in Web documents.
Reference: [3] <author> Grobelnik, M., Mladenic, D., </author> <year> 1998. </year> <title> Learning Machine: design and implementation, </title> <type> Technical Report IJS-DP-7824, SI. </type>
Reference-contexts: In this way we reduce the time and space needed to collect and store training data. The actual Web documents are used as testing examples. 3 Experimental results Our experiments are performed using our recently developed machine learning system Learning Machine <ref> [3] </ref> that supports usage of different machine learning techniques on large data sets with especially designed modules for learning on text and collecting data from the Web.
Reference: [4] <author> Joachims, T., </author> <year> 1997. </year> <title> A Probabilistic Analysis of the Roc-chio Algorithm with TFIDF for Text Categorization, </title> <booktitle> Proc. of the 14th International Conference on Machine Learning ICML97, </booktitle> <pages> pp. 143-151. </pages>
Reference-contexts: in Figure 1 the part of the first-level subcategories in the `Science' top category ranging from `Acoustics' to `Weights and Measures'. 2 Machine learning setting We use a Naive Bayesian classifier on text documents represented as feature-vectors using the bag-of-words representation as commonly used in learning on text data (eg., <ref> [4] </ref>, [6], [9]). Our document representation additionally includes not only single words (unigrams) but also up to 5 words (1-grams, 2-grams, . . . 5-grams) occurring in a document as a sequence (eg., `machine learning', `world wide web').
Reference: [5] <author> Koller, D., Sahami, M., </author> <year> 1997. </year> <title> Hierarchically classifying documents using very few words, </title> <booktitle> Proc. of the 14th International Conference on Machine Learning ICML97, </booktitle> <pages> pp. 170-178. </pages>
Reference-contexts: The final result of learning is a set of specialized classifiers each based only on a small subset of features (similar to the learning a classifier for each split in the Reuters hierarchy <ref> [5] </ref>). Since the idea is that each classifier can distinguish between the documents that should be assigned the category it represents and the other documents, we define a set of negative examples as examples from the whole hierarchy.
Reference: [6] <author> Mladenic, D., </author> <year> 1996. </year> <title> Personal WebWatcher: Implementation and Design, </title> <type> Technical Report IJS-DP-7472, SI. </type>
Reference-contexts: Figure 1 the part of the first-level subcategories in the `Science' top category ranging from `Acoustics' to `Weights and Measures'. 2 Machine learning setting We use a Naive Bayesian classifier on text documents represented as feature-vectors using the bag-of-words representation as commonly used in learning on text data (eg., [4], <ref> [6] </ref>, [9]). Our document representation additionally includes not only single words (unigrams) but also up to 5 words (1-grams, 2-grams, . . . 5-grams) occurring in a document as a sequence (eg., `machine learning', `world wide web').
Reference: [7] <author> Mladenic, D., </author> <year> 1998. </year> <title> Feature subset selection in text-learning, </title> <booktitle> Proc. of the 10th European Conference on Machine Learning. </booktitle>
Reference-contexts: In all three domains the best performance is achieved when only a small number of features is used and features are selected using the Odds ratio as scoring measure as suggested in <ref> [7] </ref> and [8].
Reference: [8] <author> Mladenic, D., Grobelnik, M., </author> <year> 1998. </year> <title> Feature selection for classification based on text hierarchy, Working notes of Learning from Text and the Web, </title> <booktitle> Conference on Automated Learning and Discovery CONALD-98. </booktitle>
Reference-contexts: In all three domains the best performance is achieved when only a small number of features is used and features are selected using the Odds ratio as scoring measure as suggested in [7] and <ref> [8] </ref>.
Reference: [9] <author> Pazzani, M., Billsus, D., </author> <year> 1997. </year> <title> Learning and Revising User Profiles: The Identification of Interesting Web Sites, Machine Learning 27, Kluwer Academic Publishers. Yahoo as an automatic document classifier 474 D.Mladenic </title>
Reference-contexts: 1 the part of the first-level subcategories in the `Science' top category ranging from `Acoustics' to `Weights and Measures'. 2 Machine learning setting We use a Naive Bayesian classifier on text documents represented as feature-vectors using the bag-of-words representation as commonly used in learning on text data (eg., [4], [6], <ref> [9] </ref>). Our document representation additionally includes not only single words (unigrams) but also up to 5 words (1-grams, 2-grams, . . . 5-grams) occurring in a document as a sequence (eg., `machine learning', `world wide web').
References-found: 9

