URL: http://www.cs.bu.edu/techreports/96-025-web-server-measurements.ps.Z
Refering-URL: http://cs-www.bu.edu/techreports/Home.html
Root-URL: 
Email: (jussara@dcc.ufmg.br)  (virgilio@bu.edu)  (djy@bu.edu)  
Title: Measuring the Behavior of a World-Wide Web Server  
Author: Jussara Almeida Virglio Almeida David J. Yates 
Note: On sabbatical at Boston University from Universidade Federal de Minas Gerais. Partially supported by CNPq-Brazil  
Address: Minas Gerais, Belo Horizonte, MG 30161, Brazil.  Boston, MA 02215, USA.  
Affiliation: Depto. de Ciencia da Computac~ao, Universidade Federal de  Computer Science Department, Boston University,  
Abstract: Technical Report CS 96-025 October 29, 1996 Abstract Server performance has become a crucial issue for improving the overall performance of the World-Wide Web. This paper describes Webmonitor, a tool for evaluating and understanding server performance, and presents new results for a realistic workload. Webmonitor measures activity and resource consumption, both within the kernel and in HTTP processes running in user space. Webmonitor is implemented using an efficient combination of sampling and event-driven techniques that exhibit low overhead. Our initial implementation is for the Apache World-Wide Web server running on the Linux operating system. We demonstrate the utility of Webmonitor by measuring and understanding the performance of a Pentium-based PC acting as a dedicated WWW server. Our workload uses a file size distribution with a heavy tail. This captures the fact that Web servers must concurrently handle some requests for large audio and video files, and a large number of requests for small documents, containing text or images. Our results show that in a Web server saturated by client requests, over 90% of the time spent handling HTTP requests is spent in the kernel. Furthermore, keeping TCP connections open, as required by TCP, causes a factor of 2-9 increase in the elapsed time required to service an HTTP request. Data gathered from Webmonitor provide insight into the causes of this performance penalty. Specifically, we observe a significant increase in resource consumption along three dimensions: the number of HTTP processes running at the same time, CPU utilization, and memory utilization. These results emphasize the important role of operating system and network protocol implementation in determining Web server performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V. Almeida, A. Bestravos, M. Crovella, and A. Oliveira. </author> <title> Characterizing reference locality in the WWW. Proceedings of IEEE-ACM PDIS'96, </title> <month> December </month> <year> 1996. </year>
Reference-contexts: Thus, we decided to build a specific tool to monitor the behavior of Web servers and to measure resource usage. In this section, we describe the guidelines and principles we followed to design a Web server performance monitor. 2.1 Characteristics of Web Servers As pointed out in <ref> [3, 8, 14, 15, 1] </ref>, there are several characteristics that distinguish Web servers from traditional distributed systems.
Reference: [2] <author> P. Aoki, A. Woodruff, E. Brewer, P. Gauthier, and L. Rowe. </author> <title> An investigation of documents for the world wide web. </title> <booktitle> Proc. of the Fifth World Wide Web Conference, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: One possible explanation is the presence of large multimedia files that contribute to increase the tail of file size distribution. 3 2.1.2 Short-lived Processes Most HTTP server implementations use a new TCP connection for almost every request. Several references <ref> [3, 8, 2] </ref> report that over 90% of client requests are for small HTML or image files. For instance, reference [2] examines over 2.6 million HTML documents in the Internet and shows that the mean size of the documents is 4.4 KB. <p> Several references [3, 8, 2] report that over 90% of client requests are for small HTML or image files. For instance, reference <ref> [2] </ref> examines over 2.6 million HTML documents in the Internet and shows that the mean size of the documents is 4.4 KB.
Reference: [3] <author> M. Arlitt and C. Williamson. </author> <title> Web server workload characterization. </title> <booktitle> Proc. of the 1996 SIGMETRICS Con ference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Such distributions occur in the size of files requested at servers, and in files requested by clients <ref> [3, 8] </ref>. This heterogeneity in workload stresses the limits of the underlying operating system much further than traditional applications [20]. One other important characteristic of our workload (and experiments) is that we do not reuse TCP connections for multiple HTTP requests, as described in [16] and the Apache documentation [18]. <p> Thus, we decided to build a specific tool to monitor the behavior of Web servers and to measure resource usage. In this section, we describe the guidelines and principles we followed to design a Web server performance monitor. 2.1 Characteristics of Web Servers As pointed out in <ref> [3, 8, 14, 15, 1] </ref>, there are several characteristics that distinguish Web servers from traditional distributed systems. <p> The following two characteristics have a profound impact on the behavior of Web servers. 2.1.1 Heavy Tailed Distributions Recent studies <ref> [3, 8] </ref> have shown that file sizes in the World-Wide Web exhibit heavy tails, including files stored on servers, files requested by clients and transmitted over the network. <p> One possible explanation is the presence of large multimedia files that contribute to increase the tail of file size distribution. 3 2.1.2 Short-lived Processes Most HTTP server implementations use a new TCP connection for almost every request. Several references <ref> [3, 8, 2] </ref> report that over 90% of client requests are for small HTML or image files. For instance, reference [2] examines over 2.6 million HTML documents in the Internet and shows that the mean size of the documents is 4.4 KB.
Reference: [4] <author> M. F. Arlitt. </author> <title> A performance study of Internet web servers. M.Sc. </title> <type> Thesis, </type> <institution> Department of Computer Science, University of Saskatchewan, Saskatoon, Saskatchewan, </institution> <month> June </month> <year> 1996. </year> <month> 18 </month>
Reference-contexts: We present results for a workload generated by WebStone [21], which is a configurable tool for benchmarking Web server performance, available from Silicon Graphics. We parameterized the server workload generated by WebStone to capture the heterogeneous nature of HTTP requests, using values from <ref> [4] </ref>. Specifically, we used a file size distribution with a heavy tail to capture the fact that Web servers must concurrently handle some requests for huge multimedia files and a large number of requests for small HTML and image documents. <p> A request for a page means a request for each file that makes up the page. Table 1 gives baseline information for the HTTP workload used in our experiments. The parameters that define the workload are representative of the kinds of workload typically found in busy WWW 6 servers <ref> [4] </ref>.
Reference: [5] <author> K. Birman and R. van Renesse. </author> <title> Software for reliable networks. </title> <publisher> Scientific American, </publisher> <month> May </month> <year> 1996. </year>
Reference-contexts: The overall performance of the Web depends on the performance of its main components; namely clients, the network, and servers. The explosive growth of the Web is placing a heavy demand on servers <ref> [5, 10] </ref>. As a result, users see slow response times on the most popular sites, which are overrun by millions of requests per day. Thus, server performance has become a critical issue for improving the quality of service on the World-Wide Web.
Reference: [6] <author> J. Bradley Chen, Yasuhiro Endo, Kee Chan, David Mazieres, Antonio Dias, Margo I. Seltzer, and Michael D. Smith. </author> <title> The measured performance of personal computer operating systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 14(1) </volume> <pages> 3-40, </pages> <month> February </month> <year> 1996. </year>
Reference: [7] <author> A. Cockcroft. </author> <title> Watching your web server. SunWorld Online, </title> <month> March </month> <year> 1996. </year> <note> URL: http://www.sun.com/ sunworldonline/swol-03-1996/. </note>
Reference-contexts: In Linux, all of them collect information from /proc filesystem [22]. Although these tools can provide insight into server behavior, they reflect the performance only from a system-wide standpoint. Furthermore, those standard tools may introduce unbearable overhead during the monitoring of a busy Web server. In <ref> [7] </ref>, the author notes that in a highly loaded Web server (100 http/sec) netstat -s took several seconds to run and stalled the server for that time period. In order to obtain in-depth information about the server behavior, we also need to collect data at the HTTP server level.
Reference: [8] <author> M. Crovella and A. Bestavros. </author> <title> Self-similarity in world wide web traffic: Evidence and possible causes. </title> <booktitle> Proc. of the 1996 SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Such distributions occur in the size of files requested at servers, and in files requested by clients <ref> [3, 8] </ref>. This heterogeneity in workload stresses the limits of the underlying operating system much further than traditional applications [20]. One other important characteristic of our workload (and experiments) is that we do not reuse TCP connections for multiple HTTP requests, as described in [16] and the Apache documentation [18]. <p> Thus, we decided to build a specific tool to monitor the behavior of Web servers and to measure resource usage. In this section, we describe the guidelines and principles we followed to design a Web server performance monitor. 2.1 Characteristics of Web Servers As pointed out in <ref> [3, 8, 14, 15, 1] </ref>, there are several characteristics that distinguish Web servers from traditional distributed systems. <p> The following two characteristics have a profound impact on the behavior of Web servers. 2.1.1 Heavy Tailed Distributions Recent studies <ref> [3, 8] </ref> have shown that file sizes in the World-Wide Web exhibit heavy tails, including files stored on servers, files requested by clients and transmitted over the network. <p> A heavy-tailed distribution (e.g., Pareto) is given by P [X &gt; x] ~ x ff , as x ! 1 and 0 &lt; ff &lt; 2. Theoretical heavy-tailed distributions have infinite variance, which, in practical terms, means that very large observations are possible with non-negligible probability. In <ref> [8] </ref>, the authors surveyed a number of WWW servers in the Internet and found evidence of heavy-tailed distributions of sizes of files on the servers. <p> One possible explanation is the presence of large multimedia files that contribute to increase the tail of file size distribution. 3 2.1.2 Short-lived Processes Most HTTP server implementations use a new TCP connection for almost every request. Several references <ref> [3, 8, 2] </ref> report that over 90% of client requests are for small HTML or image files. For instance, reference [2] examines over 2.6 million HTML documents in the Internet and shows that the mean size of the documents is 4.4 KB.
Reference: [9] <author> G. Serazzi D. Ferrari and A. Zeigner. </author> <title> Measurement and Tuning of Computer Systems. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, </address> <year> 1983. </year>
Reference-contexts: This division in based on the interaction between the monitor and system, the technique of instrumentation used and the nature of the data collected <ref> [9] </ref>. The Kernel Module runs independently of the Web server and collects information about the operating system as a whole. The code of the Server Module is actually linked with the server code, and therefore runs as part of the server.
Reference: [10] <author> Simson L. Garfinkel. </author> <title> The wizard of Netscape. </title> <journal> WebServer Magazine, </journal> <volume> 1(2) </volume> <pages> 59-63, </pages> <year> 1996. </year>
Reference-contexts: The overall performance of the Web depends on the performance of its main components; namely clients, the network, and servers. The explosive growth of the Web is placing a heavy demand on servers <ref> [5, 10] </ref>. As a result, users see slow response times on the most popular sites, which are overrun by millions of requests per day. Thus, server performance has become a critical issue for improving the quality of service on the World-Wide Web.
Reference: [11] <author> Nicholas Gloy, Cliff Young, J. Bradley Chen, and Michael D. Smith. </author> <title> An analysis of dynamic branch pre diction schemes on system workloads. </title> <booktitle> In Proc. of 23rd International Symposium on Computer Architecture. </booktitle> <address> ACM/IEEE, </address> <month> May </month> <year> 1996. </year>
Reference: [12] <author> K. Lai and M. Baker. </author> <title> A performance comparison of UNIX operating systems on the Pentium. </title> <booktitle> In Proceedings of the 1996 USENIX Conference, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1996. </year> <booktitle> USENIX. </booktitle>
Reference: [13] <author> D. Menasce, V. Almeida, and L. Dowdy. </author> <title> Capacity Planning and Performance Modeling. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, </address> <year> 1994. </year>
Reference-contexts: Thus, Webmonitor was designed to provide data for analytical models also. The basic input data required by queueing network models are service demands of a request at a server <ref> [13] </ref>. Those demands specify the total amount of service time required by a request during its execution at each major component of the server. It is worth mentioning that service demand refers only to the time a request spends actually receiving service. It does not include waiting times.
Reference: [14] <author> Jeffery C. Mogul. </author> <title> Network behavior of a busy Web server and its clients. </title> <type> Research Report 95/5, </type> <institution> DEC Western Research Laboratory, </institution> <month> October </month> <year> 1995. </year>
Reference-contexts: It is necessary to keep TCP connections open (in the TIME WAIT state) at the server to guard against old data being received by a new connection. Although such problems with the way TCP interacts with HTTP have been pointed out by others <ref> [16, 14, 17] </ref>, we isolate and quantify their impact. <p> Thus, we decided to build a specific tool to monitor the behavior of Web servers and to measure resource usage. In this section, we describe the guidelines and principles we followed to design a Web server performance monitor. 2.1 Characteristics of Web Servers As pointed out in <ref> [3, 8, 14, 15, 1] </ref>, there are several characteristics that distinguish Web servers from traditional distributed systems. <p> The combination of these facts explains a common phenomenon that has been observed during the operation of busy Web servers: the creation of a large number of short-lived processes <ref> [14, 15] </ref>. This brings new challenges to some operating systems that are not tuned for handling a large number of short-lived processes. Short-lived processes also represent new problems for performance monitoring. <p> The usual holding time in this state is 60 seconds, after which the connection is closed (put in the TCP CLOSE state). It has been observed by others <ref> [16, 14, 17] </ref> that the holding time in the TIME WAIT state is a possible performance problem for WWW servers, however, we are the first to quantify this and give some insight into possible causes. Table 10 gives the average number of connections seen in different TCP states. <p> The most interesting number in Table 10 is the large number (over 900) connections in the TIME WAIT state, when its holding time is 60 15 seconds. These results are consistent with those in <ref> [14, 16] </ref>. It is also interesting to note that more time is spent in the closed state (TCP CLOSE), than in the state where the connections are actually performing useful work (the ESTABLISHED state).
Reference: [15] <author> Jeffery C. Mogul. </author> <title> Operating system support for busy Internet servers. </title> <booktitle> In Proceedings of the Fifth Workshop on Hot Topics in Operating Systems, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: Thus, we decided to build a specific tool to monitor the behavior of Web servers and to measure resource usage. In this section, we describe the guidelines and principles we followed to design a Web server performance monitor. 2.1 Characteristics of Web Servers As pointed out in <ref> [3, 8, 14, 15, 1] </ref>, there are several characteristics that distinguish Web servers from traditional distributed systems. <p> The combination of these facts explains a common phenomenon that has been observed during the operation of busy Web servers: the creation of a large number of short-lived processes <ref> [14, 15] </ref>. This brings new challenges to some operating systems that are not tuned for handling a large number of short-lived processes. Short-lived processes also represent new problems for performance monitoring.
Reference: [16] <author> Jeffrey C. </author> <title> Mogul. </title> <booktitle> The case for persistent-connection HTTP. In SIGCOMM Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 299-313, </pages> <address> Cambridge, MA, </address> <month> August </month> <year> 1995. </year> <note> ACM. </note>
Reference-contexts: This heterogeneity in workload stresses the limits of the underlying operating system much further than traditional applications [20]. One other important characteristic of our workload (and experiments) is that we do not reuse TCP connections for multiple HTTP requests, as described in <ref> [16] </ref> and the Apache documentation [18]. Thus, we open a new TCP connection for every request. We therefore capture the costs of servicing our workload under the "worst case" assumption of being unable to use persistent connections. We present two new results from data collected using Webmonitor. <p> It is necessary to keep TCP connections open (in the TIME WAIT state) at the server to guard against old data being received by a new connection. Although such problems with the way TCP interacts with HTTP have been pointed out by others <ref> [16, 14, 17] </ref>, we isolate and quantify their impact. <p> Apache incorporates some features of HTTP 1.1 since it can accept more than one HTTP request per connection <ref> [16] </ref>. Our Apache server was configured to run in standalone mode. The number of KeepAlive requests per connection [18] was set to 0 (only one HTTP request was serviced per connection). <p> The usual holding time in this state is 60 seconds, after which the connection is closed (put in the TCP CLOSE state). It has been observed by others <ref> [16, 14, 17] </ref> that the holding time in the TIME WAIT state is a possible performance problem for WWW servers, however, we are the first to quantify this and give some insight into possible causes. Table 10 gives the average number of connections seen in different TCP states. <p> The most interesting number in Table 10 is the large number (over 900) connections in the TIME WAIT state, when its holding time is 60 15 seconds. These results are consistent with those in <ref> [14, 16] </ref>. It is also interesting to note that more time is spent in the closed state (TCP CLOSE), than in the state where the connections are actually performing useful work (the ESTABLISHED state).
Reference: [17] <author> Venkata N. Padmanabhan and Jeffrey C. Mogul. </author> <title> Improving HTTP latency. </title> <booktitle> In Proceedings of Second WWW Conference '94: Mosaic and the Web, </booktitle> <pages> pages 995-1005, </pages> <address> Chicago, IL, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: It is necessary to keep TCP connections open (in the TIME WAIT state) at the server to guard against old data being received by a new connection. Although such problems with the way TCP interacts with HTTP have been pointed out by others <ref> [16, 14, 17] </ref>, we isolate and quantify their impact. <p> The usual holding time in this state is 60 seconds, after which the connection is closed (put in the TCP CLOSE state). It has been observed by others <ref> [16, 14, 17] </ref> that the holding time in the TIME WAIT state is a possible performance problem for WWW servers, however, we are the first to quantify this and give some insight into possible causes. Table 10 gives the average number of connections seen in different TCP states.
Reference: [18] <author> D. Robinson and the Apache Group. </author> <note> APACHE An HTTP Server, Reference Manual, 1995. URL: http://www.apache.org. </note>
Reference-contexts: This heterogeneity in workload stresses the limits of the underlying operating system much further than traditional applications [20]. One other important characteristic of our workload (and experiments) is that we do not reuse TCP connections for multiple HTTP requests, as described in [16] and the Apache documentation <ref> [18] </ref>. Thus, we open a new TCP connection for every request. We therefore capture the costs of servicing our workload under the "worst case" assumption of being unable to use persistent connections. We present two new results from data collected using Webmonitor. <p> The server software is Apache, version 1.1.1, a public domain HTTP server <ref> [18] </ref>. Apache was originally based on code and ideas found in NCSA HTTP server [20]. It is "A PAtCHy server", in the sense that it was based on some existing code and a series of "patch files". <p> Apache incorporates some features of HTTP 1.1 since it can accept more than one HTTP request per connection [16]. Our Apache server was configured to run in standalone mode. The number of KeepAlive requests per connection <ref> [18] </ref> was set to 0 (only one HTTP request was serviced per connection). The lower and upper bounds in the number of idle processes were set to 5 and 10, respectively; and the number of requests a child process serves before dying was set to 30.
Reference: [19] <author> Y. Somin S. Agrawal, M. Forsyth. </author> <title> Measurement and analysis of process & workload CPU times in UNIX environments. </title> <booktitle> Proceedings of the CMG'96, </booktitle> <month> December </month> <year> 1996. </year>
Reference-contexts: This brings new challenges to some operating systems that are not tuned for handling a large number of short-lived processes. Short-lived processes also represent new problems for performance monitoring. Although UNIX provides accurate measurements for processor usage by processes of moderately long duration, the authors in <ref> [19] </ref> point out the problems in trying to measure CPU time used by short-lived individual processes. 2.2 Measurement Principles The fundamental characteristics of a good measurement tool are low overhead, low interference in the system being measured, and high accuracy.
Reference: [20] <author> R. McGrath T. Kwan and D. Reed. </author> <title> NCSA's world wide web server: Design and performance. </title> <booktitle> IEEE Computer, </booktitle> <month> November </month> <year> 1995. </year>
Reference-contexts: Such distributions occur in the size of files requested at servers, and in files requested by clients [3, 8]. This heterogeneity in workload stresses the limits of the underlying operating system much further than traditional applications <ref> [20] </ref>. One other important characteristic of our workload (and experiments) is that we do not reuse TCP connections for multiple HTTP requests, as described in [16] and the Apache documentation [18]. Thus, we open a new TCP connection for every request. <p> The server software is Apache, version 1.1.1, a public domain HTTP server [18]. Apache was originally based on code and ideas found in NCSA HTTP server <ref> [20] </ref>. It is "A PAtCHy server", in the sense that it was based on some existing code and a series of "patch files". It supports the notion of optional modules, that are compiled and linked to the main code.
Reference: [21] <author> G. Trent and M. Sake. WebStone: </author> <title> The First Generation in HTTP Server Benchmarking, </title> <month> February </month> <year> 1995. </year> <note> URL: http://www.sgi.com/Products/WebFORCE/WebStone/paper.html. </note>
Reference-contexts: Our initial implementation is for the Apache WWW server running on the Linux operating system. We demonstrate the utility of Webmonitor by measuring and understanding the performance of a Pentium-based PC acting as a dedicated WWW server. We present results for a workload generated by WebStone <ref> [21] </ref>, which is a configurable tool for benchmarking Web server performance, available from Silicon Graphics. We parameterized the server workload generated by WebStone to capture the heterogeneous nature of HTTP requests, using values from [4]. <p> It has a standard 10 Megabit/second Ethernet card. Linux was installed on the disk on a partition of 416 Megabytes, and a partition of 36 Megabytes was allocated for swap space. 3.2 Workload To generate a representative WWW workload, we used WebStone <ref> [21] </ref> (version 2.0.0), which is an industry-standard benchmark for generating HTTP requests. WebStone is a configurable client-server benchmark for HTTP servers, that uses workload parameters and client processes to generate Web requests. This allows a server to be evaluated in a number of different ways.
Reference: [22] <author> M. Welsh. </author> <title> The Linux Bible. Yggdrasil Computing Incorporated, </title> <address> 2 nd edition, </address> <year> 1994. </year> <month> 19 </month>
Reference-contexts: Finally, section 5 summarizes the paper. 2 Measuring a Web Server The standard performance tools provided by Unix operating systems include ps, vmstat and netstat. In Linux, all of them collect information from /proc filesystem <ref> [22] </ref>. Although these tools can provide insight into server behavior, they reflect the performance only from a system-wide standpoint. Furthermore, those standard tools may introduce unbearable overhead during the monitoring of a busy Web server. <p> We describe the workload, hardware, and software used to perform the measurements and collect the performance data. 3.1 The Server System The operating system used is Linux version 2.0.0, which is distributed under the terms of GNU General Public License <ref> [22] </ref>. The server software is Apache, version 1.1.1, a public domain HTTP server [18]. Apache was originally based on code and ideas found in NCSA HTTP server [20]. It is "A PAtCHy server", in the sense that it was based on some existing code and a series of "patch files". <p> However, since our results show that the vast majority of system resources are consumed by the HTTP processes, we only present results for these processes. Usually the Linux kernel keeps performance data internally. They can be read by user programs through the /proc filesystem <ref> [22] </ref>. This is a "virtual file system", in the sense that its contents are not located on disk but in memory. A read of any file below /proc causes data in the kernel to be copied to memory in user space. <p> After logging, a server process is ready to handle a new request. Unfortunately, the Linux timing routines are not accurate enough to account for the three components of the execution time of a short request. The timing resolution is on the order of 10 milliseconds <ref> [22] </ref>. In order to measure 9 parsing, processing, and logging times with greater accuracy, we implemented a "stopwatch" scheme using the gettimeofday routine, that returns the elapsed seconds and microseconds since a predefined date. This resolution is because gettimeofday reads the time directly from the hardware timer. <p> per second Kernel Module Measures my get procstats cpu (%) percentage of cpu used by all copies of the monitored program mem (%) percentage of memory used by all copies of the monitored program maj flt/s number of major faults all copies of the monitored program have made per second <ref> [22] </ref> started processes total number of copies of the monitored program running processes number of copies of the monitored program waiting for run time. Table 2: Description of Server and Kernel Module measures.
References-found: 22

