URL: ftp://ftp.sics.se/pub/SICS-reports/Reports/SICS-R--91-19--SE.ps.Z
Refering-URL: http://www.sics.se/libindex.html
Root-URL: 
Title: DDM A CACHE-ONLY MEMORY ARCHITECTURE  
Author: Erik Hagersten, Anders Landin and Seif Haridi 
Keyword: Multiprocessor, COMA, hierarchical architecture, hierarchical buses, multilevel cache, shared memory, split-transaction bus, cache coherence, cache-only memory architectures.  
Date: November 1991.  
Pubnum: SICS Research Report R91:19  
Abstract: We introduce a new class of architectures called Cache Only Memory Architectures (COMA). These architectures provide the programming paradigm of the shared-memory architectures, but have no physically shared memory; instead, the caches attached to the processors contain all the memory in the system, and their size is therefore large. A datum is allowed to be in any or many of the caches, and will automatically be moved to where it is needed by a cache-coherence protocol, which also ensures that the last copy of a datum is never lost. The location of a datum in the machine is completely decoupled from its address. We also introduce one example of COMA: the Data Diffusion Machine (DDM), and its simulated performance for large applications. The DDM is based on a hierarchical network structure, with processor/memory pairs at its tips. Remote accesses generally cause only a limited amount of traffic over a limited part of the machine. 
Abstract-found: 1
Intro-found: 1
Reference: [BD91] <author> L. Barroso and M. Dubois. </author> <title> Cache Coherence on a Slotted Ring. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 230-237, </pages> <year> 1991. </year>
Reference-contexts: Still, the bandwidth has not been the limiting factor in our simulation studies. The bandwidth of a bus can be increased many times by using other structures. The slotted ring bus proposed by Barosso and Dubois <ref> [BD91] </ref> has a one order of magnitude higher bandwidth. The DDM uses the normal procedures for translating virtual addresses to physical addresses, as implemented in standard MMUs, for translations to item identifiers. This implies that the knowledge of physical pages appears to an operating system.
Reference: [BW88] <author> J-L. Baer and W-H. Wang. </author> <title> On the Inclusion Properties for Multi-Level Cache Hierarchies. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 73-88, </pages> <year> 1988. </year>
Reference-contexts: The home also differ from NUMAs in it being a bus, i.e. any attraction memory on that bus will do. The details of the directory protocols can be found elsewhere [HHW90]. 4.4 Replacement in a Directory Baer and Wang have studied the multi-level inclusion property <ref> [BW88] </ref> with the following implications for our system: a directory at level i + 1 has to be a superset of the directories, or attraction memories, at level i, i.e. the size of a directory and its associativity (number of ways) must be B i times that of the underlying level
Reference: [CKA91] <author> D. Chaiken, J. Kubiatowicz, and A. Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Proceedings of the 4th Annual ASP-LOS, </booktitle> <year> 1991. </year>
Reference-contexts: NU-MAs often have networks other than a single bus, and the network delay might vary to different nodes. The earlier NUMAs did not have coherent caches, and left the coherence problem to the programmer. Research activities today are striving toward coherent NUMAs with directory-based cache-coherence protocols, e.g. Alewife <ref> [CKA91] </ref> and Dash [LLG + 90]. Programs are often optimized for NUMAs by statically partitioning the work and data. Given a partitioning where the processors make most of their accesses to their part of the shared memory, a better scalability than for UMAs can be achieved.
Reference: [GW88] <author> J.R. Goodman and P.J. Woest. </author> <title> The Wisconsin Multicube: a new large-scale cache-coherent multiprocessor. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, Honolulu, Hawaii, </booktitle> <pages> pages 442-431, </pages> <year> 1988. </year>
Reference-contexts: This is not part of the atomic snooping action of the bus. The hierarchical DDM and its protocol have several similarities with the proposed architectures by Wilson [Wil86], Vernon [VJS88] and Goodman <ref> [GW88] </ref>.
Reference: [HALH91] <author> E. Hagersten, P. Andersson, A. Landin, and S. Haridi. </author> <title> A Performance Study of the DDM a Cache-Only Memory Architecture. </title> <institution> Swedish Institute of Computer Science, </institution> <note> Report R91:17. Submitted to ISCA92, </note> <year> 1991. </year>
Reference-contexts: For instance, we have measured a speedup of 50% when false sharing was removed from an application <ref> [HALH91] </ref>. Hardware-based schemes maintain coherence without involving software and can therefore be implemented more efficiently. Examples of hardware-based protocols are snooping-cache protocols and directory-based protocols. Snooping-cache protocols have a distributed implementation. <p> In a COMA with a small item size, the alternative approach, write-broadcast, could also be attractive where, on a write, the new value is broadcast to all "caches" with a shared copy of the item <ref> [HALH91] </ref>. 5 The protocol also handles the attraction of data (read) and replacement when a set in an attraction memory gets full. <p> Looser forms of consistency providing a higher performance, can also be supported in an efficient way by the hierarchical structure [LHH91]. Yet another protocol which is write invalidate by default but changes strategy to write broadcast on a per-item basis has been proposed <ref> [HALH91] </ref> 5 INCREASING THE BANDWIDTH Although most memory accesses tend to be localized in the machine, the higher level in the hierarchy may nevertheless demand a higher bandwidth than the lower systems, which creates a bottleneck. <p> All programs were originally written for UMA architectures (Sequent Symmetry or Encore Multimax) and use static or dynamic scheduler algorithms. They adapt well for a COMA without any changes. The details of this study can be found elsewhere <ref> [HALH91] </ref>. All programs take in the order of one CPU minute to run sequentially, without any simulations, on a SUN SPARCstation.
Reference: [HHW90] <author> E. Hagersten, S. Haridi, and D.H.D. Warren. </author> <title> The Cache-Coherence Protocol of the Data Diffusion Machine. </title> <editor> In M. Dubois and S. Thakkar, editors, </editor> <title> Cache and Interconnect Architectures in Multiprocessors. </title> <publisher> Kluwer Academic Publisher, Norwell, </publisher> <address> Mass, </address> <year> 1990. </year>
Reference-contexts: more than one new transaction for the bus, a requirement necessary for deadlock avoidance. 3.1 The Protocol of the Single-Bus DDM We developed a new protocol, similar in many ways to the snooping-cache protocol, limiting broadcast requirements to a smaller subsystem and adding support for replacement, described in detail in <ref> [HHW90] </ref>. The write coherence part of the protocol is of write-invalidate type; i.e., in order to keep data coherent, all copies of the item but the one to be updated are erased on a write. <p> When the item is not there, its place can be used by other items. The home also differ from NUMAs in it being a bus, i.e. any attraction memory on that bus will do. The details of the directory protocols can be found elsewhere <ref> [HHW90] </ref>. 4.4 Replacement in a Directory Baer and Wang have studied the multi-level inclusion property [BW88] with the following implications for our system: a directory at level i + 1 has to be a superset of the directories, or attraction memories, at level i, i.e. the size of a directory and <p> A shared item occupies space in many attraction memories, but only one space in the directories above them. Directory replacement is implemented in the DDM by an extension to the existing protocol, which requires one extra state and two extra transactions <ref> [HHW90] </ref>. 4.5 Other Protocols The described protocol provides a sequentially consistent [Lam79] system to the programmer. While fulfilling the strongest memory access model, performance is degraded by waiting for the acknowledge before the write can be performed.
Reference: [HomBC] <author> Homer. </author> <title> Odyssey. </title> <type> 800 BC. 19 </type>
Reference-contexts: The transient states, R and A in the directories, mark the request's path through the hierarchy, shown in Figure 7, like rolling out a red thread when walking in a maze <ref> [HomBC] </ref>. A flow control mechanism in the protocol prevents deadlock if too many processors try to roll out a red thread to the same set in a directory.
Reference: [Lam79] <author> L. Lamport. </author> <title> How to Make a Multiprocessor Computer that Correctly Exe--cutes Multiprocess Programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 28(9) </volume> <pages> 690-691, </pages> <month> September </month> <year> 1979. </year>
Reference-contexts: Directory replacement is implemented in the DDM by an extension to the existing protocol, which requires one extra state and two extra transactions [HHW90]. 4.5 Other Protocols The described protocol provides a sequentially consistent <ref> [Lam79] </ref> system to the programmer. While fulfilling the strongest memory access model, performance is degraded by waiting for the acknowledge before the write can be performed.
Reference: [Lei85] <author> C.E. Leiserson. </author> <title> Fat Trees: Universal Networks for Hardware-Efficient Supercomputing. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 892-901, </pages> <month> Oct. </month> <year> 1985. </year>
Reference-contexts: This solution, however, increases the levels in the hierarchy, resulting in a longer remote access delay and an increased memory overhead. The higher levels of the hierarchy can instead be widened to become a fat tree <ref> [Lei85] </ref>. A directory can be split into two directories of half the size. The two directories deal with different address domains (even and odd). The communication with other directories is also split, which doubles the bandwidth.
Reference: [LHH91] <author> A. Landin, E. Hagersten, and S. Haridi. </author> <title> Race-free Interconnection Networks and Multiprocessor Consistency. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <year> 1991. </year>
Reference-contexts: This not only reduces the remote delay, but also cuts down the number of transactions in the system. The writer might actually receive the acknowledge before all copies are erased. Still sequential consistency can be guaranteed <ref> [LHH91] </ref>. Looser forms of consistency providing a higher performance, can also be supported in an efficient way by the hierarchical structure [LHH91]. <p> The writer might actually receive the acknowledge before all copies are erased. Still sequential consistency can be guaranteed <ref> [LHH91] </ref>. Looser forms of consistency providing a higher performance, can also be supported in an efficient way by the hierarchical structure [LHH91].
Reference: [LLG + 90] <author> D. Lenoski, J. Laundo, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <year> 1990. </year>
Reference-contexts: The earlier NUMAs did not have coherent caches, and left the coherence problem to the programmer. Research activities today are striving toward coherent NUMAs with directory-based cache-coherence protocols, e.g. Alewife [CKA91] and Dash <ref> [LLG + 90] </ref>. Programs are often optimized for NUMAs by statically partitioning the work and data. Given a partitioning where the processors make most of their accesses to their part of the shared memory, a better scalability than for UMAs can be achieved.
Reference: [P + 85] <author> G.F. Pfister et al. </author> <title> The IBM Research Parallel Processor Prototype (RP3). </title> <booktitle> In Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <address> Chigago, </address> <year> 1985. </year>
Reference-contexts: Often many processors try to read the same item, creating the "hot-spot in the attraction memory of processor Py. Its path is marked with states reading and answering (R and A), which will guide the data reply back to Px. phenomenon <ref> [P + 85] </ref>." Combined reads and broadcasts are simple to implement in the DDM.
Reference: [Ras86] <author> R Rashid. </author> <title> Threds of a new system (Mach). </title> <journal> UNIX Review, </journal> <volume> 4(8) </volume> <pages> 386-392, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: This is especially true for operating systems with an eager reclaiming of unused work space like Mach <ref> [Ras86] </ref>. In the DDM, the unused item space may be used to increase the degree of sharing, if the unused items are purged. <p> MUSE was run on an earlier version of the simulator, and some of the statistics are therefore not reported in Table 1. 9 RELATED ACTIVITIES An operating system targeted for the DDM prototype is under development at SICS. This work is based on the Mach operating system from CMU <ref> [Ras86] </ref> that is modified to efficiently support the DDM. Other related activities at SICS involve a hardware prefetching scheme that dynamically prefetches items to the attraction memory, especially useful when a process is started or migrated. We are also experimenting with alternative protocols.
Reference: [RW91] <author> S. Raina and D.H.D Warren. </author> <title> Traffic Patterns in a Scalable Multiprocessor through Transputer Emulation. </title> <booktitle> In International Hawaii Conference on System Science, </booktitle> <year> 1991. </year>
Reference-contexts: Other related activities at SICS involve a hardware prefetching scheme that dynamically prefetches items to the attraction memory, especially useful when a process is started or migrated. We are also experimenting with alternative protocols. An emulator of the DDM is currently under development at the University of Bris-tol <ref> [RW91] </ref>. The emulator runs on the Meiko Transputer platform. The modeled architecture has a tree-shaped link-based structure with Transputers as directories. Their four links allow for a branch factor of three at each level. The Transputers at the leaves execute the application.
Reference: [Ste90] <author> P. Stenstrom. </author> <title> A Survey of Cache Coherence for Multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 23(6), </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: The contents of the caches are kept coherent by a cache-coherence protocol, in which each cache snoops the traffic on the common bus and prevents any inconsistencies from occurring <ref> [Ste90] </ref>. The architecture fl A revised version of R90:17. Swedish Institute of Computer Science; Box 1263 ; 164 28 KISTA ; SWEDEN. Email: fhag,landin,seifg@sics.se 1 provides a uniform access time to the whole shared memory, and is consequently called uniform memory architecture (UMA). <p> Snooping-cache protocols have a distributed implementation. Each cache is responsible for snooping traffic on the bus and taking necessary actions if an incoherence is about to occur. An example of such a protocol is the write-once protocol introduced by Goodman and discussed by Stenstrom <ref> [Ste90] </ref>. In that protocol, shown in Figure 2, each cache line can be in one of the four states INVALID, VALID, RESERVED, or DIRTY. Many caches might have the same cache line in the state VALID at the same time, and may read it locally. <p> Snooping caches, as described above, rely on broadcasting and are not suited for general interconnection networks: unrestricted broadcasting would drastically reduce the available bandwidth, thereby obviating the advantage of general networks. Instead, directory-based schemes send messages directly between nodes <ref> [Ste90] </ref>. A read request is sent to main memory, without any snooping. The main memory knows if the cache line is cached, in which cache or caches, and whether or not it has been modified.
Reference: [SWG91] <author> J.S. Sing, W.-D. Weber, and A Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. </title> <institution> Stanford University, </institution> <type> Report, </type> <month> April </month> <year> 1991. </year>
Reference-contexts: Note that an increased working set results in less load on the busses for Water and Cholesky. We have studied the parallel execution of the Stanford Parallel Applications for Shared Memory (SPLASH) <ref> [SWG91] </ref>, the OR-parallel Prolog system MUSE and a matrix multiplication program, representing applications from engineering computing and symbolic computing. All programs were originally written for UMA architectures (Sequent Symmetry or Encore Multimax) and use static or dynamic scheduler algorithms. They adapt well for a COMA without any changes.
Reference: [VJS88] <author> M.K. Vernon, R Jog, and G.S. Sohi. </author> <title> Performance Analysis of Hierarchical Cache-Consistent Multiprocessors. </title> <booktitle> In Conference Proceedings of International Seminar on Performance of Distributed and Parallel Systems, </booktitle> <pages> pages 111 - 126, </pages> <year> 1988. </year>
Reference-contexts: A directory reads from IB when it has the time and space to do a lookup in its status memory. This is not part of the atomic snooping action of the bus. The hierarchical DDM and its protocol have several similarities with the proposed architectures by Wilson [Wil86], Vernon <ref> [VJS88] </ref> and Goodman [GW88]. <p> A way of taking the load off the higher levels is to have a smaller branch factor at the top of the hierarchy than lower down <ref> [VJS88] </ref>. This solution, however, increases the levels in the hierarchy, resulting in a longer remote access delay and an increased memory overhead. The higher levels of the hierarchy can instead be widened to become a fat tree [Lei85]. A directory can be split into two directories of half the size.
Reference: [WH88] <author> D. H. D. Warren and S. Haridi. </author> <title> Data Diffusion Machine-a scalable shared virtual memory multiprocessor. </title> <booktitle> In International Conference on Fifth Generation Computer Systems 1988. </booktitle> <publisher> ICOT, </publisher> <year> 1988. </year>
Reference-contexts: Writing can now be performed with direct messages between all caches with copies. 3 A MINIMAL COMA We will introduce the COMA architecture by first looking at the smallest instance of our architecture, the Data Diffusion Machine (DDM) <ref> [WH88] </ref>. The minimal DDM, as presented, can be a COMA on its own or a subsystem of a larger COMA. The attraction memories of the minimal DDM are connected by a single bus.
Reference: [Wil86] <author> A. Wilson. </author> <title> Hierarchical cache/bus architecture for shared memory multiprocessor. </title> <type> Technical report ETR 86-006, </type> <institution> Encore Computer Corporation, </institution> <year> 1986. </year> <month> 20 </month>
Reference-contexts: A directory reads from IB when it has the time and space to do a lookup in its status memory. This is not part of the atomic snooping action of the bus. The hierarchical DDM and its protocol have several similarities with the proposed architectures by Wilson <ref> [Wil86] </ref>, Vernon [VJS88] and Goodman [GW88].
References-found: 19

