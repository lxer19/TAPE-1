URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/experts-laser.ps.Z
Refering-URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: mangeas@cs.colorado.edu  andreas@cs.colorado.edu  
Title: First experiments using a mixture of nonlinear experts for time series prediction  
Author: Morgan Mangeas , av. du g en eral de Gaulle, Clamart, France, and Andreas S. Weigend 
Address: Boulder, CO 80309-0430  Boulder, CO 80309-0430  
Affiliation: Electricit e de France, Direction des Etudes et Recherches  Department of Computer Science University of Colorado,  Department of Computer Science and Institute of Cognitive Science University of Colorado  
Abstract: This paper investigates the advantages and disadvantages of the mixture of experts (ME) model (introduced to the connectionist community in [JJNH91] and applied to time series analysis in [WM95]) on two time series where the dynamics is well understood. The first series is a computer-generated series, consisting of a mixture between a noise-free process (the quadratic map) and a noisy process (a composition of a noisy linear autoregressive and a hyperbolic tangent). There are three main results: (1) the ME model produces significantly better results than single networks; (2) it discovers the regimes correctly and also allows us to characterize the sub-processes through their variances. (3) due to the correct matching of the noise level of the model to that of the data it avoids overfitting. The second series is the laser series used in the Santa Fe competition; the ME model also obtains excellent out-of-sample predictions, allows for analysis and shows no overfitting.
Abstract-found: 1
Intro-found: 1
Reference: [Fri91] <author> J. H. Friedman. </author> <title> Multivariate adaptive regression splines. </title> <journal> Annals of Statistics, </journal> <volume> 19 </volume> <pages> 1-142, </pages> <year> 1991. </year>
Reference-contexts: It is difficult for a global model to take into account such features. In this case, it would be preferable to split the series and learn each part separately. While this strategy is not new (e.g., Threshold autoregressive (TAR) model [TL80] and Multivariate Adaptive Regression Splines (MARS) <ref> [Fri91] </ref>, applied to prediction of financial data by [LRS94]), the mixture of experts (ME) model ([JJNH91, JX93, JJ94, WM95]) has a number of potentially promising advantages: * gating and experts are learned simultaneously * there is a natural smooth transition between regions * both splitting and prediction model (gate and experts)
Reference: [JJ94] <author> M. I. Jordan and R. A. Jacobs. </author> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 181-214, </pages> <year> 1994. </year>
Reference: [JJNH91] <author> R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. </author> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 79-87, </pages> <year> 1991. </year>
Reference: [JX93] <author> M. I. Jordan and L. Xu. </author> <title> Convergence results for the EM approach to mixtures of experts architectures. Submitted to Neural Networks, </title> <year> 1993. </year>
Reference-contexts: The usual way is to maximize the log likelihood with respect to the parameters. However, the log of the likelihood (Eq. 1) is quite complicated, and it is difficult for simple optimization methods (such as gradient descent) to find the splittings. Following <ref> [JX93] </ref>, we use the Expectation-Maximization (EM) algorithm.
Reference: [LRS94] <author> P. A. W. Lewis, B. K. Ray, and J. G. Stevens. </author> <title> Modeling time series using multivariate adaptive regression splines (MARS). </title> <editor> In A. S. Weigend and N. A. Gershenfeld, editors, </editor> <title> Time Series Prediction: </title> <booktitle> Forecasting the Future and Understanding the Past, </booktitle> <pages> pages 296-318, </pages> <address> Reading, MA, 1994. </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: In this case, it would be preferable to split the series and learn each part separately. While this strategy is not new (e.g., Threshold autoregressive (TAR) model [TL80] and Multivariate Adaptive Regression Splines (MARS) [Fri91], applied to prediction of financial data by <ref> [LRS94] </ref>), the mixture of experts (ME) model ([JJNH91, JX93, JJ94, WM95]) has a number of potentially promising advantages: * gating and experts are learned simultaneously * there is a natural smooth transition between regions * both splitting and prediction model (gate and experts) can be nonlinear * in the Gaussian case,
Reference: [TL80] <author> H. Tong and K. S. Lim. </author> <title> Threshold autoregression, limit cycles and cyclical data. </title> <journal> J. Roy. Stat. Soc. B, </journal> <volume> 42 </volume> <pages> 245-292, </pages> <year> 1980. </year>
Reference-contexts: It is difficult for a global model to take into account such features. In this case, it would be preferable to split the series and learn each part separately. While this strategy is not new (e.g., Threshold autoregressive (TAR) model <ref> [TL80] </ref> and Multivariate Adaptive Regression Splines (MARS) [Fri91], applied to prediction of financial data by [LRS94]), the mixture of experts (ME) model ([JJNH91, JX93, JJ94, WM95]) has a number of potentially promising advantages: * gating and experts are learned simultaneously * there is a natural smooth transition between regions * both
Reference: [WM95] <author> A. S. Weigend and M. Mangeas. </author> <title> Experts for prediction: discovering regimes and avoiding overfitting. </title> <type> Technical Report CU-CS-764-95, </type> <institution> University of Colorado at Boulder, ftp://ftp.cs.colorado.edu/pub/Time-Series/MyPapers/experts.ps.Z, </institution> <year> 1995. </year>
References-found: 7

