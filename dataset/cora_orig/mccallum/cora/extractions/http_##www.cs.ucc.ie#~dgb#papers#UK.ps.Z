URL: http://www.cs.ucc.ie/~dgb/papers/UK.ps.Z
Refering-URL: http://www.cs.ucc.ie/~dgb/publist.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: ftony|dgbg@minster.york.ac.uk  
Title: Formalising the knowledge content of case memory systems  
Author: A D Griffiths and D G Bridge 
Date: December 22, 1994  
Address: York, YORK YO1 5DD, UK  
Affiliation: Department of Computer Science, University of  
Abstract: Discussions of case-based reasoning often reflect an implicit assumption that a case memory system will become better informed, i.e. will increase in knowledge, as more cases are added to the case-base. This paper considers formalisations of this `knowledge content' which are a necessary preliminary to more rigourous analysis of the performance of case-based reasoning systems. In particular we are interested in modelling the learning aspects of case-based reasoning in order to study how the performance of a case-based reasoning system changes as it accumlates problem-solving experience. The current paper presents a `case-base semantics' which generalises recent formalisations of case-based classification. Within this framework, the paper explores various issues in assuring that these sematics are well-defined, and illustrates how the knowledge content of the case memory system can be seen to reside in both the chosen similarity measure and in the cases of the case-base.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D W Aha, D Kibler, and M K Albert. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference-contexts: For the purposes of the model we consider a similarity measure to be a function over pairs of descriptions returning a normalised real value indicating the degree of similarity between the two objects: : (X 1 fi X 1 ) ! <ref> [0; 1] </ref> Given a problem description x * X 1 we can therefore define the set of nearest neighbours of x with respect to CB and : 8x * X 1 N N (x; CB; ) ^= f (x 1 ; x 2 )j (x 1 ; x 2 ) * <p> )gj &gt; 1, and equation (9) necessarily follows from the case where w I is admissible with respect to and F . 2 3 Consistency of case-based learning algorithms The similarity measure has been defined above only as a binary function from the space (X 1 fi X 1 ! <ref> [0; 1] </ref>); the property of consistency motivates some minimum constraints on a useful similarity measure. A consistent learner is one which, having seen some training sample, is able to correctly reproduce the examples it has seen in that sample.
Reference: [2] <author> M Anthony and N Biggs. </author> <title> Computational Learning Theory. </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: Since we all assume that the functions being represented are defined on a finite domain, consistency is sufficient to guarantee that as more training examples are seen the system will eventually converge to a good approximation of the target function <ref> [2, Chs 3 & 4] </ref>. This holds even in the worst case where the system is able to make little or no suitable generalisation of the seen examples. Since consistency is a property of learning algorithms, we must state explicitly the case-based learning algorithm we wish to consider.
Reference: [3] <author> A M Dearden and M D Harrison. </author> <title> The engineering of case memory systems. </title> <note> submitted to the Journal of Intelligent Information Systems. </note>
Reference-contexts: In contrast the view adopted here is a functional one in that the knowledge content of a case memory is modelled as a mapping between input and output domains. A functional viewpoint has been used in the work of Dearden and Harrison <ref> [3] </ref> in the engineering of the user interface of case memory systems and, of more relevance to our own work, it has also been assumed in much of `computational learning theory'. <p> Additionally, X 1 and X 2 are both finite sets. Using the terminology of Dearden & Harrison <ref> [3] </ref> we refer to the elements of X 1 & X 2 as `descriptions' and `reports' respectively. In [3], the knowledge content of the case memory system is embodied in the `retrieve' function, which maps from a domain of problem statements into the space of partial orders over the cases in <p> Additionally, X 1 and X 2 are both finite sets. Using the terminology of Dearden & Harrison <ref> [3] </ref> we refer to the elements of X 1 & X 2 as `descriptions' and `reports' respectively. In [3], the knowledge content of the case memory system is embodied in the `retrieve' function, which maps from a domain of problem statements into the space of partial orders over the cases in the case-base.
Reference: [4] <author> A D Griffiths and D G Bridge. </author> <title> On concept space and hypothesis space in case-based learning algorithms. </title> <note> Submitted to ECML-95: Eighth European Conference on Machine Learning 1995. Available by WWW via http://dcs2.cs.york.ac.uk:9876/isg/tony/tonydox.html. </note>
Reference-contexts: This theoretical study of learning systems models the current state of a learner as a function which is adjusted to accommodate the data presented to the system. We are currently making progress in the study of various case-based learning algorithms within this framework <ref> [4] </ref>. 1 2 Generalised Case-Base Semantics We consider the general case where the case memory system is intended to respresent a mapping between an input domain X 1 and an output domain X 2 . Additionally, X 1 and X 2 are both finite sets. <p> Given a problem description from X 1 , we require that our system outputs a `yes' or `no' result classifying the problem description as an instance of some concept. Elsewhere in our work <ref> [4] </ref> we have used the following equation as semantics for a case-based classifier, related to the `standard semantics' of Jantke and Lange [6, p.142] ( see also [9, p.84] ). f hCB;i (x) = 1 if 9 (x pos ; 1) * CB 8 (x neg ; 0) * CB (x; <p> As above, we illustrate this result with reference to the special case of classification systems. In <ref> [4] </ref> we prove a version of Theorem 9 in terms of the following definition of `special' predictivity for classification functions: 8f * F 8x; x 0 * X 1 (x; x 0 ) (x; x) ! f (x) = 1 ! f (x 0 ) = 1 (18) The following result <p> of Theorem 9 in terms of the following definition of `special' predictivity for classification functions: 8f * F 8x; x 0 * X 1 (x; x 0 ) (x; x) ! f (x) = 1 ! f (x 0 ) = 1 (18) The following result re-expresses the special case <ref> [4, Thm 5] </ref> as a corollary of the general framework presented here: Corollary 11 Consistency of Case-based classifiers CB1 () is a consistent learning algorithm for space of classification functions F (X 1 ! f0; 1g) iff the similarity measure is predictive of F according to the `special' definition of equations <p> We are finding that this allows some progress in understanding analytically the learning behaviour of various case-based reasoning systems <ref> [4] </ref>. The formalisation presented here might also be of use in emphasising the insight of Wess & Globig [9] that the knowledge content of a case memory system rests in the similarity measure as well as the stored cases.
Reference: [5] <editor> K P Jantke. </editor> <title> Case-based learning in inductive inference. </title> <booktitle> In COLT92 Proceedings of 5th ACM Worshop on Computational Learning Theory, </booktitle> <month> July 92, </month> <pages> pages 218-223. </pages> <publisher> ACM Press, </publisher> <year> 1992. </year>
Reference-contexts: Following the work of Jantke <ref> [5] </ref> a case memory system is modelled as a pair hCB; i consisting of a case-base CB and a similarity measure . The case-base is a set of description-report pairs. i.e. <p> That is, where the set of nearest neighbours defined in equation (2) contains more than one exemplar from the case-base, we must specify a way of choosing the case used to justify the system's output. Jantke <ref> [5] </ref> offers three suggestions for resolving such ties: 1. In the case of ties, the output of the system is undefined. That is, the case memory system is interpreted as a partial function. 2 2. Ties are resolved by a preference ordering over descriptions. 3. <p> Option 2 is passed over for the moment principally because this knowledge is potentially subsumed in the similarity measure. Thus we define the function f hCB;i represented by a case-memory system hCB; i as (c.f. <ref> [5, p.219] </ref>): f hCB;i (x 1 ) = x 2 where max w fx 0 2 j (x 0 2 ) * N N (x 1 ; CB; )g = fx 2 g (3) f hCB;i (x 1 ) = x 2 where N N (x 1 ; CB; ) =
Reference: [6] <editor> K P Jantke and S Lange. </editor> <booktitle> Case-based representation and learning of pattern languages. In EWCBR-93 Working Notes of the first European Workshop on Case-Based Reasoning, </booktitle> <volume> volume 1, </volume> <pages> pages 139-144. </pages> <institution> University of Kaiserslautern, </institution> <year> 1993. </year>
Reference-contexts: Elsewhere in our work [4] we have used the following equation as semantics for a case-based classifier, related to the `standard semantics' of Jantke and Lange <ref> [6, p.142] </ref> ( see also [9, p.84] ). f hCB;i (x) = 1 if 9 (x pos ; 1) * CB 8 (x neg ; 0) * CB (x; x pos ) &gt; (x; x neg ) 0 otherwise (6) Informally, a point x from X 1 is positively classified by
Reference: [7] <author> P Koton and M P Chase. </author> <title> Knowledge representation in a case-based reasoning system: Defaults and exceptions. </title> <editor> In R J Brachman and H J Levesque, editors, </editor> <booktitle> Proceedings of the First International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 203-211. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: Some theoreticians <ref> [7] </ref> [8] have attempted to describe knowledge content from a logical point of view, i.e. in terms of the logical propositions that might be derived from a case memory system.
Reference: [8] <author> M Mehl. </author> <title> Retrieval in case-based reasoning using preferred subtheories. </title> <editor> In G Brewka, K P Jantke, and P Schmitt, editors, </editor> <booktitle> Nonmonotonic and inductive logic (2nd International Workshop), </booktitle> <pages> pages 284-297. </pages> <publisher> Springer Verlag, </publisher> <year> 1993. </year>
Reference-contexts: Some theoreticians [7] <ref> [8] </ref> have attempted to describe knowledge content from a logical point of view, i.e. in terms of the logical propositions that might be derived from a case memory system.
Reference: [9] <author> S Wess and C Globig. </author> <title> Case-based and symbolic classification algorithms A case study using version space. </title> <editor> In S Wess, K-D Althoff, and M M Richter, editors, </editor> <booktitle> Topics in CBR: EWCBR-93 First European Workshop on Case-Based Reasoning, Kaiserslautern, Germany, </booktitle> <volume> November '93, LNCS 837, </volume> <pages> pages 77-91. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year> <month> 8 </month>
Reference-contexts: Elsewhere in our work [4] we have used the following equation as semantics for a case-based classifier, related to the `standard semantics' of Jantke and Lange [6, p.142] ( see also <ref> [9, p.84] </ref> ). f hCB;i (x) = 1 if 9 (x pos ; 1) * CB 8 (x neg ; 0) * CB (x; x pos ) &gt; (x; x neg ) 0 otherwise (6) Informally, a point x from X 1 is positively classified by h hCB;i if and only <p> We are finding that this allows some progress in understanding analytically the learning behaviour of various case-based reasoning systems [4]. The formalisation presented here might also be of use in emphasising the insight of Wess & Globig <ref> [9] </ref> that the knowledge content of a case memory system rests in the similarity measure as well as the stored cases. Future work will attempt to make use of this framework in a model of case-based reasoning systems suitable for simple instances of the design task.
References-found: 9

