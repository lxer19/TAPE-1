URL: http://www.cs.princeton.edu/~ristad/papers/eurospeech97.ps.gz
Refering-URL: http://www.cs.princeton.edu/~ristad/bibliography.html
Root-URL: http://www.cs.princeton.edu
Title: STRUCTURE AND PERFORMANCE OF A DEPENDENCY LANGUAGE MODEL  
Author: Ciprian Chelba, David Engle, Frederick Jelinek, Victor Jimenez, Sanjeev Khudanpur, Lidia Mangu Harry Printz, Eric Ristad, Ronald Rosenfeld, Andreas Stolcke, Dekai Wu 
Abstract: We present a maximum entropy language model that incorporates both syntax and semantics via a dependency grammar. Such a grammar expresses the relations between words by a directed graph. Because the edges of this graph may connect words that are arbitrarily far apart in a sentence, this technique can incorporate the predictive power of words that lie outside of bigram or trigram range. We have built several simple dependency models, as we call them, and tested them in a speech recognition experiment. We report experimental results for these models here, including one that has a small but statistically significant advantage (p &lt; :02) over a bigram language model. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Berger, S. Della Pietra, V. Della Pietra, </author> <title> "A Maximum Entropy Approach to Natural Language Processing," </title> <booktitle> Computational Linguistics, </booktitle> <year> 1996. </year>
Reference-contexts: Maximum Entropy Formulation Even with the map h 7! [h], there are still too many distinct [h] to estimate the probabilities P (wd j [h]) as ratios of counts. To circumvent this difficulty, we formulated our model using the method of constrained maximum entropy <ref> [1] </ref>. The maximum entropy formalism allows us to treat each of the numerous elements of [h] as a distinct predictor variable.
Reference: [2] <author> M. J. Collins, </author> <title> "A New Statistical Parser Based on Bigram Lexical Dependencies," </title> <booktitle> Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 184-191, </pages> <month> May, </month> <year> 1996. </year>
Reference-contexts: Finally, we point out various practical obstacles we encountered in carrying out our plan, and discuss the changes they forced upon us. 2.1. Elements of the Model Our model is based upon a dependency grammar <ref> [2] </ref>, and the closely related notion of a link grammar [10, 5]. Such grammars express the linguistic structure of a sentence in terms of a planar, directed graph: two related words are connected by a graph edge, which bears a label that encodes the nature of their linguistic relationship. <p> Tagging and Parsing Our model operates on parsed utterances. To obtain the required parse K of an utterance S, we used the dependency parser of Michael Collins <ref> [2] </ref>, chosen because of its speed of operation, accuracy, and trainability. This parser processes a linguistically complete utterance S|what we normally think of as a sentence|that has been labeled with part-of-speech tags. It yields a parse K, and a probability P (K j S) of this parse.
Reference: [3] <author> S. Della Pietra, V. Della Pietra, and J. Lafferty, </author> <title> Inducing Features of Random Fields, </title> <type> Technical Report CMU-CS-95-144, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: Training of the Dependency Model To find the maximum entropy model subject to a given set of constraints, we used the Maximum Entropy Modeling Toolkit [8]. This program implements the Improved Iterative Scaling algorithm, described in <ref> [3] </ref>. It proved to be highly efficient: a large trigram model, containing 12,412 unigram features, 36,191 bigram features, and 120,116 trigram features, completed 10 training iterations on a single Sun UltraSparc workstation in under 2 1/2 hours.
Reference: [4] <author> J. J. Godfrey, E. C. Holliman, and J. McDaniel, </author> <title> "SWITCHBOARD: Telephone Speech Corpus for Research and Development," </title> <booktitle> Proceedings of the IEEE Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> Volume I, </volume> <pages> pages 517-520, </pages> <address> San Francisco, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: For this we used the maximum entropy tagger of Adwait Ratnaparkhi [7], again chosen because of its trainability and high accuracy. All training and testing data were drawn from the Switchboard corpus of spontaneous conversational English speech <ref> [4] </ref>, and from the Treebank corpus, which is a hand-annotated and hand-parsed version of the Switchboard text. We used these corpora as follows. First we trained the tagger, using approximately 1 million words of hand-tagged training data.
Reference: [5] <author> John Lafferty, Daniel Sleator, Davy Temperley, </author> <title> "Grammatical Trigrams: A Probabilistic Model of Link Grammar," </title> <booktitle> Proceedings of the 1992 AAAI Fall Symposium on Probabilistic Approaches to Natural Language, </booktitle> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference-contexts: Finally, we point out various practical obstacles we encountered in carrying out our plan, and discuss the changes they forced upon us. 2.1. Elements of the Model Our model is based upon a dependency grammar [2], and the closely related notion of a link grammar <ref> [10, 5] </ref>. Such grammars express the linguistic structure of a sentence in terms of a planar, directed graph: two related words are connected by a graph edge, which bears a label that encodes the nature of their linguistic relationship.
Reference: [6] <author> Raymond Lau, Ronald Rosenfeld, Salim Roukos, </author> <title> "Trigger-Based Language Models: A Maximum Entropy Approach," </title> <booktitle> Proceedings of the International Conference on Acoustics, Speech and Signal Processing, </booktitle> <address> ii:45-48, Minneapolis, MN, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: Typically, there are many such functions, distinguished from one another by the unigram or bigram they constrain. These notions are more fully described in <ref> [6, 8, 9] </ref>. The novel element of our model is the link bigram con-straint. It is here that we condition the probability of the predicted word w upon linguistically related words in the past, possibly out of N gram range.
Reference: [7] <author> A. Ratnaparkhi, </author> <title> "A Maximum Entropy Model for Part-of-Speech Tagging," </title> <booktitle> Proceedings of the Conference on Empirical Methods in Natural Language Processing, </booktitle> <pages> pages 133-142, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Second, because the parser does not operate in an incremental, left-to-right fashion, we were forced to adopt an N -best rescoring strategy. Finally, because the parser requires part-of-speech tags on its input, a prior tagging step is required. For this we used the maximum entropy tagger of Adwait Ratnaparkhi <ref> [7] </ref>, again chosen because of its trainability and high accuracy. All training and testing data were drawn from the Switchboard corpus of spontaneous conversational English speech [4], and from the Treebank corpus, which is a hand-annotated and hand-parsed version of the Switchboard text. We used these corpora as follows.
Reference: [8] <author> E. Ristad, </author> <title> "Maximum Entropy Modeling Toolkit," </title> <type> Technical Report, </type> <institution> Department of Computer Science, Princeton University, </institution> <year> 1996. </year>
Reference-contexts: Typically, there are many such functions, distinguished from one another by the unigram or bigram they constrain. These notions are more fully described in <ref> [6, 8, 9] </ref>. The novel element of our model is the link bigram con-straint. It is here that we condition the probability of the predicted word w upon linguistically related words in the past, possibly out of N gram range. <p> For all features, we used ratios of counts, or ratios of smoothed counts, to compute empirical expectations. 3.2. Training of the Dependency Model To find the maximum entropy model subject to a given set of constraints, we used the Maximum Entropy Modeling Toolkit <ref> [8] </ref>. This program implements the Improved Iterative Scaling algorithm, described in [3]. It proved to be highly efficient: a large trigram model, containing 12,412 unigram features, 36,191 bigram features, and 120,116 trigram features, completed 10 training iterations on a single Sun UltraSparc workstation in under 2 1/2 hours.
Reference: [9] <author> R. Rosenfeld, </author> <title> "A Maximum Entropy Approach to Adaptive Statistical Language Modeling," </title> <booktitle> Computer Speech and Language, </booktitle> <pages> pages 187-228, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: Typically, there are many such functions, distinguished from one another by the unigram or bigram they constrain. These notions are more fully described in <ref> [6, 8, 9] </ref>. The novel element of our model is the link bigram con-straint. It is here that we condition the probability of the predicted word w upon linguistically related words in the past, possibly out of N gram range.
Reference: [10] <author> Daniel D. K. Sleator and Davy Temperley, </author> <title> Parsing English with a Link Grammar, </title> <type> Technical Report CMU-CS-91-196, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA, </address> <year> 1991. </year>
Reference-contexts: Finally, we point out various practical obstacles we encountered in carrying out our plan, and discuss the changes they forced upon us. 2.1. Elements of the Model Our model is based upon a dependency grammar [2], and the closely related notion of a link grammar <ref> [10, 5] </ref>. Such grammars express the linguistic structure of a sentence in terms of a planar, directed graph: two related words are connected by a graph edge, which bears a label that encodes the nature of their linguistic relationship.
Reference: [11] <author> A. Stolcke, C. Chelba, D. Engle, V. Jimenez, L. Mangu, H. Printz, E. Ristad, R. Rosenfeld, D. Wu, F. Jelinek and S. Khudanpur, </author> <title> "Dependency Language Modeling," 1996 Large Vocabulary Continuous Speech Recognition Summer Research Workshop Technical Reports, Research Note 24, Center for Language and Speech Processing, </title> <institution> Johns Hopkins University, Baltimore, MD, </institution> <month> April </month> <year> 1997. </year> <note> http://www.speech.sri.com/people/ stolcke/papers/ws96-report.ps.Z </note>
Reference-contexts: Model 2g24c2 included beyond 2g24 all fully-labeled link bigrams of count 2. Finally, for model 2g24c5mi, we ap plied an information-theoretic measure to link selection: we included beyond 2g24 all link bigrams of count 5, for which the average link gain <ref> [11] </ref> exceeded 1 bit. 4.2. Model Performance Table 1 above lists word error rate scores for these models. Column dm reports results with fi; fl = 0, in expression (6), and ff and ffi fixed at nominal values. <p> We intend to pursue it, con structing more elaborate models, training them on larger corpora, and testing them more thoroughly. A more thorough discussion of the methods and results presented here may be found in reference <ref> [11] </ref>. ACKNOWLEDGEMENTS We gratefully acknowledge the contributions and assistance of Michael Collins and Adwait Ratnaparkhi, both of the University of Pennsylvania, who generously donated their software and energy to this effort.
References-found: 11

