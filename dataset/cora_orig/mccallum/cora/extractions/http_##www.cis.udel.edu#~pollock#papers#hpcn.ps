URL: http://www.cis.udel.edu/~pollock/papers/hpcn.ps
Refering-URL: http://www.cis.udel.edu/~jochen/passages/pubs.htm
Root-URL: http://www.cis.udel.edu
Email: carroll@cis.udel.edu  pollock@cis.udel.edu  
Phone: (302) 831-1953  
Title: Design and Implementation of a General Purpose Parallel Programming System  
Author: Mark Chu-Carroll Lori L. Pollock 
Date: November 14, 1995  
Address: 19716  
Affiliation: Department of Computer and Information Sciences University of Delaware Newark, DE  
Note: (presenting author)  
Abstract-found: 0
Intro-found: 1
Reference: [AGG + 95] <author> J. Auerbach, A. Goldberg, G. Goldszmidt, A. Gopal, M. Kennedy, J. Russell, and S. Yemini. </author> <title> Concert/C Specification and Reference, Definition of a Language for Distributed C Programming. </title> <institution> IBM T. J. Watson Research Center, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: Rendezvous calls are a remote procedure call mechanism that allows synchronization and communication between two nodes on the same level of a composite tree. The rendezvous mechanism is closely modelled on the remote procedure call mechanism of the Concert/C <ref> [AGG + 95] </ref> distributed programming system. Rendezvous calls require two constructs: a call send for the call requester to make a rendezvous call, and a call receive for the call receiver to permit a rendezvous call of one of its methods.
Reference: [AH92] <author> G. Agha and C. Houck. Hal: </author> <title> A high level actor language and its distributed implementation. </title> <booktitle> In Proceedings of the 21st International Conference on Parallel Processing, </booktitle> <pages> pages 158-165, </pages> <year> 1992. </year>
Reference-contexts: Current approaches to parallel programming for network clusters can be divided into explicit message passing <ref> [AH92, Chi92, GKSK94, JH93, SGDM94, MPI94] </ref>, coarse grain data parallel [LRV92, RSW91, Ble90, Sch92], and distributed shared memory systems [CG90, MFL93].
Reference: [App92] <author> A. Appel. </author> <title> Compiling with Continuations. </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: Once these high level transformations are complete, the program is translated into intermediate code. The intermediate program representation which we have developed, called the Parallel Continuation Graph (PCG), is a parallel extension of the continuation passing style (CPS) intermediate representation described in <ref> [App92] </ref>. Our extensions to the CPS intermediate representation allow us to capture dynamic nested parallelism, communication, and synchronization in a simple flexible high level representation.
Reference: [Ble90] <author> G. Blelloch. Nesl: </author> <title> A nested data parallel language. </title> <type> Technical Report CMU-CS-92-103, CMU, </type> <year> 1990. </year>
Reference-contexts: Current approaches to parallel programming for network clusters can be divided into explicit message passing [AH92, Chi92, GKSK94, JH93, SGDM94, MPI94], coarse grain data parallel <ref> [LRV92, RSW91, Ble90, Sch92] </ref>, and distributed shared memory systems [CG90, MFL93]. Explicit message passing and distributed shared memory systems are both difficult to use for complex data structures in general purpose applications, since they do not provide any explicit support for the design and implementation of parallel data structures.
Reference: [Bra90] <author> W. Brainerd. </author> <title> Programmer's Guide to Fortran 90. </title> <publisher> McGraw-Hill Book Co., </publisher> <year> 1990. </year>
Reference-contexts: Data parallel languages may provide access to the parallel memory explicitly (like C* [TMC90]), implicitly (like Fortran-90 <ref> [Bra90] </ref>), or some combination of the two (like Fortran/D [HKT91]). We altered data parallelism in three ways to arrive at the composite parallel model.
Reference: [CAC + 81] <author> Gregory Chaitin, Marc Auslander, Ashok Chandra, John Cocke, Martin Hopkins, and Peter Markstein. </author> <title> Register allocation via coloring. </title> <booktitle> Computer Languages, </booktitle> <pages> pages 47-57, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: These two code fragments implement a graph coloring register allocator <ref> [CAC + 81, MP95] </ref>, which is a common but time consuming pass of an optimizing compiler. Sequential code for this register allocator is presented in part a of the figure, and a parallel version is presented in part b.
Reference: [CG90] <author> Nicholas Carriero and David Gelernter. </author> <title> How to Write Parallel Programs. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mass, </address> <year> 1990. </year>
Reference-contexts: Current approaches to parallel programming for network clusters can be divided into explicit message passing [AH92, Chi92, GKSK94, JH93, SGDM94, MPI94], coarse grain data parallel [LRV92, RSW91, Ble90, Sch92], and distributed shared memory systems <ref> [CG90, MFL93] </ref>. Explicit message passing and distributed shared memory systems are both difficult to use for complex data structures in general purpose applications, since they do not provide any explicit support for the design and implementation of parallel data structures.
Reference: [Chi92] <author> Andrew Chien. </author> <title> Concurrent Aggregates: Supporting Modularity in Massively Parallel Programs. </title> <publisher> MIT Press, </publisher> <address> 2nd edition, </address> <year> 1992. </year>
Reference-contexts: Current approaches to parallel programming for network clusters can be divided into explicit message passing <ref> [AH92, Chi92, GKSK94, JH93, SGDM94, MPI94] </ref>, coarse grain data parallel [LRV92, RSW91, Ble90, Sch92], and distributed shared memory systems [CG90, MFL93].
Reference: [CP94] <author> M. Carroll and L. Pollock. </author> <title> Composites: trees for data parallel programming. </title> <booktitle> In Proceedings of the 1994 International Conference on Computer Languages, </booktitle> <pages> pages 43-54. </pages> <publisher> IEEE, </publisher> <year> 1994. </year>
Reference-contexts: procedure, and reduction is performed by the combination procedure. 2.2 Language Extensions for Composites Support for composite parallelism can be added to any object oriented base language using a simple set of language constructs. (For an example of this portability, a version of composite support for C++ is presented in <ref> [CP94] </ref>. For our 2 1. Type Declarations 1 (define-composite &lt;a-composite-type&gt; ( &lt;composite&gt; ) 2 (a b c)) 2. Parallel Invocations 1 (par 2 or ;; combining function 3 (child node) ;; indexing clause 4 ;; body 5 (some-method child) 3.
Reference: [GKSK94] <author> A. Gursoy, S. Krishnan, A. Sinha, and L. Kale. </author> <title> The Charm(4.3) Programming Language Manual. </title> <institution> University of Illinois, </institution> <year> 1994. </year>
Reference-contexts: Current approaches to parallel programming for network clusters can be divided into explicit message passing <ref> [AH92, Chi92, GKSK94, JH93, SGDM94, MPI94] </ref>, coarse grain data parallel [LRV92, RSW91, Ble90, Sch92], and distributed shared memory systems [CG90, MFL93].
Reference: [Hil85] <author> W. D. Hillis. </author> <title> The Connection Machine. </title> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: This has allowed scientific programmers to write code that runs efficiently on a variety of high performance architectures. Data parallelism has been implemented in a variety of data parallel languages, including CmLisp <ref> [Hil85] </ref>, C* [TMC90], Fortran-D [HKT91], and a huge number of others.
Reference: [HKT91] <author> S. Hiranandani, K. Kennedy, and C. W. Tseng. </author> <title> An overview of the fortran-D programming system. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1991. </year>
Reference-contexts: This has allowed scientific programmers to write code that runs efficiently on a variety of high performance architectures. Data parallelism has been implemented in a variety of data parallel languages, including CmLisp [Hil85], C* [TMC90], Fortran-D <ref> [HKT91] </ref>, and a huge number of others. <p> Data parallel languages may provide access to the parallel memory explicitly (like C* [TMC90]), implicitly (like Fortran-90 [Bra90]), or some combination of the two (like Fortran/D <ref> [HKT91] </ref>). We altered data parallelism in three ways to arrive at the composite parallel model. First, for code to be executed efficiently on very coarse architectures such as workstation clusters, the grain of computation must be larger than what is found in typical data parallel programs.
Reference: [HQ92] <author> P. Hatcher and M. Quinn. </author> <title> Data parallel programming on MIMD computers. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: Our current communication optimizations fall into several categories: extended footprinting, message combination, and message elimination. Here, we describe only the extended footprinting optimization. Footprinting [LA94] is one of a variety of code motion optimizations which minimize effective communication delays by performing computations while a communication is in progress <ref> [HQ92] </ref>. Footprinting splits a synchronous communication into an asynchronous 5 communication initiation and a synchronous communication completion.
Reference: [JH93] <author> M. Jones and P. Hudak. </author> <title> Implicit and explicit parallel programming in haskell. </title> <type> Technical Report YALEU/DCS/RR-982, </type> <institution> Yale University, </institution> <year> 1993. </year>
Reference-contexts: Current approaches to parallel programming for network clusters can be divided into explicit message passing <ref> [AH92, Chi92, GKSK94, JH93, SGDM94, MPI94] </ref>, coarse grain data parallel [LRV92, RSW91, Ble90, Sch92], and distributed shared memory systems [CG90, MFL93].
Reference: [LA94] <author> K. Landry and J. Arthur. </author> <title> Achieving asynchronous speedup while preserving synchronous semantics: an implementation of instruction footprinting in linda. </title> <booktitle> In Proceedings of the 1994 International Conference on Computer Languages, </booktitle> <pages> pages 55-63. </pages> <publisher> IEEE, </publisher> <year> 1994. </year>
Reference-contexts: We briefly discuss communication and structure optimization here. Our current communication optimizations fall into several categories: extended footprinting, message combination, and message elimination. Here, we describe only the extended footprinting optimization. Footprinting <ref> [LA94] </ref> is one of a variety of code motion optimizations which minimize effective communication delays by performing computations while a communication is in progress [HQ92]. Footprinting splits a synchronous communication into an asynchronous 5 communication initiation and a synchronous communication completion.
Reference: [LRV92] <author> J. Larus, B. Richards, and G. Viwsanathan. </author> <title> C**: A large grain, object-oriented, data-parallel programming language. </title> <type> Technical report, </type> <institution> University of Wisconsin, Madison, </institution> <year> 1992. </year>
Reference-contexts: Current approaches to parallel programming for network clusters can be divided into explicit message passing [AH92, Chi92, GKSK94, JH93, SGDM94, MPI94], coarse grain data parallel <ref> [LRV92, RSW91, Ble90, Sch92] </ref>, and distributed shared memory systems [CG90, MFL93]. Explicit message passing and distributed shared memory systems are both difficult to use for complex data structures in general purpose applications, since they do not provide any explicit support for the design and implementation of parallel data structures.
Reference: [MFL93] <author> Stephan Murer, Jerome Feldman, and Chu-Cheow Lim. pSather: </author> <title> Layered extensions to an object-oriented language for efficient parallel computation. </title> <type> Technical Report TR-93-029, </type> <institution> ICSI/Computer Science Division, University of California at Berkeley, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Current approaches to parallel programming for network clusters can be divided into explicit message passing [AH92, Chi92, GKSK94, JH93, SGDM94, MPI94], coarse grain data parallel [LRV92, RSW91, Ble90, Sch92], and distributed shared memory systems <ref> [CG90, MFL93] </ref>. Explicit message passing and distributed shared memory systems are both difficult to use for complex data structures in general purpose applications, since they do not provide any explicit support for the design and implementation of parallel data structures.
Reference: [MP95] <author> Christine Makowski and Lori Pollock. </author> <title> Efficient register allocation via parallel graph coloring. </title> <booktitle> In Proceedings of the ACM Symposium on Applied Computing, Programming Languages Track, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: These two code fragments implement a graph coloring register allocator <ref> [CAC + 81, MP95] </ref>, which is a common but time consuming pass of an optimizing compiler. Sequential code for this register allocator is presented in part a of the figure, and a parallel version is presented in part b. <p> The results of preliminary performance tests lead us to believe that composite parallelism will deliver significant speedups when applied to general purpose applications. For example, an implementation of a parallel register allocator using a tree based structure produced speedups between three and four on eight processors <ref> [MP95] </ref>. We believe that similar results can be expected on a wide range of similar applications. 6
Reference: [MPI94] <author> Mpi: </author> <title> A message-passing interface standard. Technical report, Message Passing Interface Forum, </title> <month> May </month> <year> 1994. </year>
Reference-contexts: Current approaches to parallel programming for network clusters can be divided into explicit message passing <ref> [AH92, Chi92, GKSK94, JH93, SGDM94, MPI94] </ref>, coarse grain data parallel [LRV92, RSW91, Ble90, Sch92], and distributed shared memory systems [CG90, MFL93].
Reference: [RSW91] <author> M. Rosing, R. Schnabel, and R. Weaver. </author> <title> The dino parallel programming language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(1) </volume> <pages> 30-42, </pages> <month> Sept </month> <year> 1991. </year>
Reference-contexts: Current approaches to parallel programming for network clusters can be divided into explicit message passing [AH92, Chi92, GKSK94, JH93, SGDM94, MPI94], coarse grain data parallel <ref> [LRV92, RSW91, Ble90, Sch92] </ref>, and distributed shared memory systems [CG90, MFL93]. Explicit message passing and distributed shared memory systems are both difficult to use for complex data structures in general purpose applications, since they do not provide any explicit support for the design and implementation of parallel data structures.
Reference: [Sch92] <author> H. Schmidt. </author> <title> Data-parallel object-oriented programming. </title> <booktitle> In Proceedings of the Fifth Australian Supercomputing Conference, </booktitle> <pages> pages 263-272, </pages> <year> 1992. </year>
Reference-contexts: Current approaches to parallel programming for network clusters can be divided into explicit message passing [AH92, Chi92, GKSK94, JH93, SGDM94, MPI94], coarse grain data parallel <ref> [LRV92, RSW91, Ble90, Sch92] </ref>, and distributed shared memory systems [CG90, MFL93]. Explicit message passing and distributed shared memory systems are both difficult to use for complex data structures in general purpose applications, since they do not provide any explicit support for the design and implementation of parallel data structures.
Reference: [SGDM94] <author> V. S. Sundcram, G. S. Geist, J. Dongarra, and R. Mancheck. </author> <title> Pvm concurrent computing system. evolution, experiences, and trends. </title> <journal> Parallel Computing, </journal> <volume> 20(4) </volume> <pages> 531-545, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Current approaches to parallel programming for network clusters can be divided into explicit message passing <ref> [AH92, Chi92, GKSK94, JH93, SGDM94, MPI94] </ref>, coarse grain data parallel [LRV92, RSW91, Ble90, Sch92], and distributed shared memory systems [CG90, MFL93].
Reference: [TMC90] <institution> C* Reference Manual. Thinking Machines Corp., </institution> <year> 1990. </year>
Reference-contexts: This has allowed scientific programmers to write code that runs efficiently on a variety of high performance architectures. Data parallelism has been implemented in a variety of data parallel languages, including CmLisp [Hil85], C* <ref> [TMC90] </ref>, Fortran-D [HKT91], and a huge number of others. <p> Data parallel languages may provide access to the parallel memory explicitly (like C* <ref> [TMC90] </ref>), implicitly (like Fortran-90 [Bra90]), or some combination of the two (like Fortran/D [HKT91]). We altered data parallelism in three ways to arrive at the composite parallel model.
References-found: 23

