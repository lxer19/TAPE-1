URL: ftp://ftp.idsia.ch/pub/jieyu/sab96.ps.gz
Refering-URL: http://www.cs.bham.ac.uk/~wbl/biblio/gp-bibliography.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: jieyu,juergen@idsia.ch  
Title: INCREMENTAL SELF-IMPROVEMENT FOR LIFE-TIME MULTI-AGENT REINFORCEMENT LEARNING  
Author: Jieyu Zhao Jurgen Schmidhuber 
Web: http://www.idsia.ch  
Address: Corso Elvezia 36, CH-6900-Lugano, Switzerland  
Affiliation: IDSIA,  
Abstract: Previous approaches to multi-agent reinforcement learning are either very limited or heuristic by nature. The main reason is: each agent's or "animat's" environment continually changes because the other learning animats keep changing. Traditional reinforcement learning algorithms cannot properly deal with this. Their convergence theorems require repeatable trials and strong (typically Markovian) assumptions about the environment. In this paper, however, we use a novel, general, sound method for multiple, reinforcement learning "animats", each living a single life with limited computational resources in an unrestricted, changing environment. The method is called "incremental self-improvement" (IS | Schmidhuber, 1994). IS properly takes into account that whatever some animat learns at some point may affect learning conditions for other animats or for itself at any later point. The learning algorithm of an IS-based animat is embedded in its own policy | the animat cannot only improve its performance, but in principle also improve the way it improves etc. At certain times in the animat's life, IS uses reinforcement/time ratios to estimate from a single training example (namely the entire life so far) which previously learned things are still useful, and selectively keeps them but gets rid of those that start appearing harmful. IS is based on an efficient, stack-based backtracking procedure which is guaranteed to make each animat's learning history a history of long-term reinforcement accelerations. Experiments demonstrate IS' effectiveness. In one experiment, IS learns a sequence of more and more complex function approximation problems. In another, a multi-agent system consisting of three co-evolving, IS-based animats chasing each other learns interesting, stochastic predator and prey strategies. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A. G. </author> <year> (1989). </year> <title> Connectionist approaches for control. </title> <type> Technical Report COINS 89-89, </type> <institution> University of Massachusetts, </institution> <address> Amherst MA 01003. </address>
Reference: <author> Berry, D. A. and Fristedt, B. </author> <year> (1985). </year> <title> Bandit Problems: Sequential Allocation of Experiments. </title> <publisher> Chapman and Hall, London. </publisher>
Reference: <author> Boddy, M. and Dean, T. L. </author> <year> (1994). </year> <title> Deliberation scheduling for problem solving in time-constrained environments. </title> <journal> Artificial Intelligence, </journal> <volume> 67 </volume> <pages> 245-285. </pages>
Reference: <author> Gittins, J. C. </author> <year> (1989). </year> <title> Multi-armed Bandit Allocation Indices. Wiley-Interscience series in systems and optimization. </title> <publisher> Wiley, </publisher> <address> Chichester, NY. </address>
Reference: <author> Greiner, R. </author> <year> (1996). </year> <title> PALO: A probabilistic hill-climbing algorithm. </title> <journal> Artificial Intelligence, </journal> <volume> 83(2). </volume>
Reference: <author> Jaakkola, T., Singh, S. P., and Jordan, M. I. </author> <year> (1995). </year> <title> Reinforcement learning algorithm for partially observable Markov decision problems. </title> <editor> In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, to appear. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference: <author> Kaelbling, L., Littman, M., and Cassandra, A. </author> <year> (1995). </year> <title> Planning and acting in partially observable stochastic domains. </title> <type> Technical report, </type> <institution> Brown University, Providence RI. </institution>
Reference: <author> Kumar, P. R. and Varaiya, P. </author> <year> (1986). </year> <title> Stochastic Systems: Estimation, Identification, and Adaptive Control. </title> <publisher> Prentice Hall. </publisher>
Reference: <author> Levin, L. A. </author> <year> (1973). </year> <title> Universal sequential search problems. </title> <journal> Problems of Information Transmission, </journal> <volume> 9(3) </volume> <pages> 265-266. </pages>
Reference-contexts: For instance, an action may correspond to a Bayesian analysis of previous events. While this analysis is running, time is running, too. Thus, the complexity of the Bayesian approach is automatically taken into account. Or, as in (Wiering and Schmidhu-ber, 1996), actions may be calls of Levin search <ref> (Levin, 1973) </ref>, a theoretically optimal algorithm for a wide variety of non-incremental search problems. Or, actions may be calls of a Q-learning variant. This makes sense in situations where the applicability of Q-learning is questionable because the environment does not satisfy the preconditions that would make Q-learning sound. respectively.
Reference: <author> Littman, M., Cassandra, A., and Kaelbling, L. </author> <year> (1995). </year> <title> Learning policies for partially observable environments. </title> <type> Technical report, </type> <institution> Brown University, Providence RI. </institution>
Reference: <author> McCallum, R. A. </author> <year> (1993). </year> <title> Overcoming incomplete perception with utile distinction memory. </title> <booktitle> In Machine Learning: Proceedings of the Tenth International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Amherst, MA. </address>
Reference: <author> Ring, M. B. </author> <year> (1994). </year> <title> Continual Learning in Reinforcement Environments. </title> <type> PhD thesis, </type> <institution> University of Texas at Austin, Austin, Texas 78712. </institution>
Reference: <author> Russell, S. and Wefald, E. </author> <year> (1991). </year> <title> Principles of Metar-easoning. </title> <journal> Artificial Intelligence, </journal> <volume> 49 </volume> <pages> 361-395. </pages>
Reference: <author> Schmidhuber, J. </author> <year> (1995a). </year> <title> Environment-independent reinforcement acceleration. </title> <type> Technical Note IDSIA-59-95, </type> <institution> IDSIA. Invited talk at Hongkong University of Science and Technology. </institution>
Reference: <author> Schmidhuber, J. </author> <year> (1996a). </year> <title> A general method for incremental self-improvement and multi-agent learning in unrestricted environments. </title> <editor> In Yao, X., editor, </editor> <booktitle> Evolutionary Computation: Theory and Applications. </booktitle> <publisher> Scientific Publ. Co., Singapore. </publisher>
Reference: <author> Schmidhuber, J. </author> <year> (1996b). </year> <title> A general method for multi-agent learning in unrestricted environments. In Adaptation, Co-evolution and Learning in Multia-gent Systems, </title> <type> Technical Report SS-96-01, </type> <pages> pages 84-87. </pages> <booktitle> American Association for Artificial Intelligence, </booktitle> <address> Menlo Park, Calif. </address>
Reference: <author> Schmidhuber, J. H. </author> <year> (1991). </year> <title> Reinforcement learning in Markovian and non-Markovian environments. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 500-506. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schmidhuber, J. H. </author> <year> (1994). </year> <title> On learning how to learn learning strategies. </title> <type> Technical Report FKI-198-94, </type> <institution> Fakultat fur Informatik, Technische Universitat Munchen. </institution> <note> Revised January 1995. </note>
Reference-contexts: given time in its single life, how can the animat estimate from a single training example (namely its entire life so far) which previously learned things will remain useful? How can it selectively keep them but get rid of those that start appearing harmful under the changed conditions? Central idea <ref> (Schmidhuber, 1994) </ref> | details in section 2.2. To address these questions in a principled way, we will introduce the concept of a reinforcement/time ratio. <p> An initially empty stack is used to store information about currently valid policy changes computed by SMSs. Occasionally, at times called "checkpoints", this information is used to restore previous policies, such that RAC holds. The method is based on two complementary kinds of processes: pushing and checking <ref> (Schmidhuber, 1994, 1995a, 1996a,b) </ref>. (1) PUSHING. Suppose the animat uses IncP rob to modify one of its probability distributions in step 3 of the basic loop from section 2.1. <p> The time consumed by pushing processes, checking processes, and all other computations is taken into account (for instance, time goes on as popping takes place). Theoretical soundness. Using induction, it can be shown that this backtracking procedure ensures that RAC holds after each checking process <ref> (Schmidhuber, 1994, 1995a, 1996a,b) </ref>. At each "checkpoint", the ani-mat will get rid of M a if t 1 a was not followed by long-term reinforcement speed-up (note that before countermanding M a , the animat will already have countermanded all M b ; b &gt; a).
Reference: <author> Schmidhuber, J. H. </author> <year> (1995b). </year> <title> Discovering solutions with low Kolmogorov complexity and high generalization capability. </title> <editor> In Prieditis, A. and Russell, S., editors, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 488-496. </pages> <publisher> Morgan Kauf-mann Publishers, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: Each animat makes use of an assembler-like programming language similar to the one in <ref> (Schmidhuber, 1995b) </ref>. It has n addressable work cells with addresses ranging from 0 to n 1. The variable, real-valued contents of the work cell with address k are denoted c k . Processes in the external environment occasionally write inputs into certain work cells.
Reference: <author> Watkins, C. J. C. H. and Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292. </pages>
Reference: <author> Whitehead, S. </author> <year> (1992). </year> <title> Reinforcement Learning for the adaptive control of perception and action. </title> <type> PhD thesis, </type> <institution> University of Rochester. </institution>
Reference: <author> Wiering, M. and Schmidhuber, J. </author> <year> (1996). </year> <title> Solving POMDPs with Levin search and EIRA. </title> <editor> In Saitta, L., editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference. </booktitle> <publisher> Morgan Kauf-mann Publishers, </publisher> <address> San Francisco, CA. </address> <note> To appear. </note>
Reference-contexts: Primitive actions can actually be almost anything. For instance, an action may correspond to a Bayesian analysis of previous events. While this analysis is running, time is running, too. Thus, the complexity of the Bayesian approach is automatically taken into account. Or, as in <ref> (Wiering and Schmidhu-ber, 1996) </ref>, actions may be calls of Levin search (Levin, 1973), a theoretically optimal algorithm for a wide variety of non-incremental search problems. Or, actions may be calls of a Q-learning variant.
Reference: <author> Williams, R. J. </author> <year> (1992). </year> <title> Simple statistical gradient-following algorithms for connectionist reinforcement learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 229-256. </pages>
References-found: 23

