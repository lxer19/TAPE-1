URL: http://polaris.cs.uiuc.edu/reports/988.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: ACKNOWLEDGMENT  
Note: iii  
Abstract: I would like to express my deep gratitude to my advisor, Dr. Williams Ludwell Harrison III, who guided and always encouraged me with patience and insight throughout this work. I am appreciated, in addition, for his understanding of my many strange manners originated from my second language (English). I would also like to thank Dr. David Padua who helped me to be a member of CSRD, and introduced me to my advisor. I also wish to thank many people: Hoich Cheong, Jyh-Herng Chow, Jeannie Covert, Perry Emrath, Mohammad Heghighat, Mike Lake, David Lilja, Tim McDaniel, Sam Midkiff, Paul Petersen, David Sehr, Steve Turner, and all the secretaries in CSRD. They all friendly helped me on this work. I also thank Sunil Kim, Taewhan Kim, and Byungjoon Park, for the many hours we spent together. I would like to thank my wife, who decided to stand by me from the early days of my graduate study sacrificing her promising career. Finally, I owe too much thing to my parents and family members. I am afraid I can't explain properly in words my gratitude to them. "Thank you, Mom and Dad. Gam Sa Ham Ni Da!" 
Abstract-found: 1
Intro-found: 1
Reference: [ADU71] <author> Alfred V. Aho, Peter J. Denning, and Jeffrey D. Ullman. </author> <title> Principles of optimal page replacement. </title> <journal> Journal of the ACM, </journal> <volume> 18(1), </volume> <month> January </month> <year> 1971. </year>
Reference: [AH87] <editor> Samson Abramsky and Chris Hankin. </editor> <title> An introduction of abstract interpretation. </title> <editor> In Samson Abramsky and Chris Hankin, editors, </editor> <title> Abstract Interpretation of Declarative Languages. </title> <publisher> Ellis Horwood Limited, </publisher> <year> 1987. </year>
Reference-contexts: The parallelizing compiler's restructuring procedures involve three phases: interproce-dural analysis, preparatory optimizations, and parallelization transformation. The interprocedural analysis phase provides key information to the compiler for par-allelization. During interprocedural analysis, the user's program is analyzed within the abstract interpretation framework <ref> [AH87, CC77] </ref>. Through this analysis phase, information on the dependence structures of user's program is collected, facilitating the discovery of interprocedural side-effects.
Reference: [AHH89] <author> A. Agarwal, M. Horowitz, and J. Hennessy. </author> <title> An analytical cache model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(2), </volume> <month> May </month> <year> 1989. </year>
Reference: [BM84] <author> Richard B. Bunt and Jennifer M. Murphy. </author> <title> The measurement of locality and the behavior of programs. </title> <journal> The Computer Journal, </journal> <volume> 27(3), </volume> <year> 1984. </year>
Reference-contexts: Even though they proposed a way to ease the space requirement by means of two-pass processing of the reference string using secondary storage and external sorting, it still requires a large amount of time and memory space. Bunt <ref> [BM84] </ref> concentrated on the measurement of the locality behavior of programs and proposed a method to measure the inherent locality of a program, which is not the working set size.
Reference: [CC77] <author> Patrick Cousot and Radhia Cousot. </author> <title> Abstract interpretation: A unified lattice model for static analysis of programs by construction or approximation of fix-points. </title> <booktitle> In Conference Record of the 4th ACM Symposium on Principles of Programming Languages, </booktitle> <year> 1977. </year>
Reference-contexts: The parallelizing compiler's restructuring procedures involve three phases: interproce-dural analysis, preparatory optimizations, and parallelization transformation. The interprocedural analysis phase provides key information to the compiler for par-allelization. During interprocedural analysis, the user's program is analyzed within the abstract interpretation framework <ref> [AH87, CC77] </ref>. Through this analysis phase, information on the dependence structures of user's program is collected, facilitating the discovery of interprocedural side-effects.
Reference: [CJ72] <author> E. G. Coffman Jr. and Thomas A. Ryan Jr. </author> <title> A study of storage partitioning using a mathematical model of locality. </title> <journal> Communications of the ACM, </journal> <volume> 15(3), </volume> <month> March </month> <year> 1972. </year>
Reference: [Cor86] <institution> Alliant Computer Corporation. Fx/series architecture manual, </institution> <month> January </month> <year> 1986. </year>
Reference-contexts: QSort is the quick sort program where data is represented as a linked list and the trace is collected with 200 randomly selected integers. All reference strings have been collected from the Alliant FX/8 <ref> [Cor86] </ref> emulator. The reference string is a sequence of virtual addresses for non-instructional memory accesses. Though the Alliant FX/8 is a 50 shared memory multiprocessor system with eight identical computational processors, the collected traces are from sequential executions of the programs.
Reference: [CV76] <author> P.J. Courtois and H. Vantilborgh. </author> <title> A decomposable model of program paging behaviour. </title> <journal> Acta Informatica, </journal> <volume> 6(3), </volume> <month> November </month> <year> 1976. </year>
Reference-contexts: size on the fly with constant amount of memory. 8 CHAPTER 2 RELATED WORKS Methods of computing the average working set size can be found either in works purely focused on calculating working set statistics [DS72, ST74, EB77, Spi77], or as by-products of the efforts to model the program behavior <ref> [DS72, OC75, CV76] </ref>. Since the intended application of the method is to obtain fast, reliable, and economical estimates, the time and space spent in analyzing a program trace must be significantly small. Keeping our intended application in mind, we discuss the feasibility of these methods. <p> Not only does the calculation of m i require much time and space, but the estimation of the two parameters requires measuring the mean and the coefficient of variation of the inter-reference interval distribution for a given program. In addition, we should compute the integration part in numerical way. <ref> [CV76] </ref> has a formula more complex than [OC75]'s. Their formula is based on "near-complete decomposable" program behavior model, where memory blocks are treated as the states of a Markov chain and the concept of decomposition based on the program locality 9 is applied.
Reference: [Den68] <author> P. J. Denning. </author> <title> The working set model for program behavior. </title> <journal> Communications of the ACM, </journal> <volume> 11(5), </volume> <month> May </month> <year> 1968. </year>
Reference-contexts: On the way to apply the Markov chain technique to the average working set size calculation, a strong assumption is introduced to make the processing steps involved in the method simple, which is definitely required for the economical hardware implementation of 1 The working set <ref> [Den68] </ref> of a program at a point in its execution is a set of memory blocks the program referenced during the past t references. The parameter t is called window size. Thus, given a window size, a small average working set size indicates good temporal locality. 4 the method.
Reference: [Den80] <author> Peter J. Denning. </author> <title> Working sets past and present. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-6(1), </volume> <month> January </month> <year> 1980. </year>
Reference-contexts: all models fails in accounting for 1 In Chapter 3 the assumption we have is that "the (i+1) st foreign access occurs if and only if i consecutive accesses are to `other' blocks." 28 ws (t) t multiple program phases over different locality set sizes and correlations between distant phases <ref> [Den80] </ref>. The filtering method estimates the early behavior of the average working set size curve in terms of the increment requirement of the window size to increase the average working set size by one. See Figure 4.1.
Reference: [DS72] <author> Peter J. Denning and Stuart C. Schwartz. </author> <title> Properties of the working set model. </title> <journal> Communications of the ACM, </journal> <volume> 15(3), </volume> <month> March </month> <year> 1972. </year>
Reference-contexts: In the following chapters we presents two hardware methods to measure the average working set size on the fly with constant amount of memory. 8 CHAPTER 2 RELATED WORKS Methods of computing the average working set size can be found either in works purely focused on calculating working set statistics <ref> [DS72, ST74, EB77, Spi77] </ref>, or as by-products of the efforts to model the program behavior [DS72, OC75, CV76]. Since the intended application of the method is to obtain fast, reliable, and economical estimates, the time and space spent in analyzing a program trace must be significantly small. <p> size on the fly with constant amount of memory. 8 CHAPTER 2 RELATED WORKS Methods of computing the average working set size can be found either in works purely focused on calculating working set statistics [DS72, ST74, EB77, Spi77], or as by-products of the efforts to model the program behavior <ref> [DS72, OC75, CV76] </ref>. Since the intended application of the method is to obtain fast, reliable, and economical estimates, the time and space spent in analyzing a program trace must be significantly small. Keeping our intended application in mind, we discuss the feasibility of these methods. <p> The simplest method of calculating the average working set size has been proposed in <ref> [DS72, Spi77] </ref>. Based on the independent reference model about the reference string, they devised a fairly simple algorithm. It turned out that the same algorithm can be used, without any stochastic assumption at all about the reference string, to calculate the average working set size without error [ST74]. <p> Based on the independent reference model about the reference string, they devised a fairly simple algorithm. It turned out that the same algorithm can be used, without any stochastic assumption at all about the reference string, to calculate the average working set size without error [ST74]. Their method <ref> [DS72, ST74] </ref> is: Let r 1 r 2 r K be the total reference string of length K. r i is the memory block accessed by the i th memory reference. <p> However, the use of the associative memory is problematic, as we discussed on page 11, concerning the problem with the method of <ref> [DS72, ST74] </ref>. Another problem that we observe concerns the accuracy of the measuring technique. When we maintain as many Markov chains as the number of all the accessed memory blocks throughout the whole trace, the 1= = 1 + b=a value easily becomes too close to 1. <p> The Figures 3.4 and 3.5 show the real average working set sizes and the predicted average working set sizes over varying time granule size (300, 500, 800, and 1000). The real 24 working set sizes are calculated according to the method of <ref> [DS72, ST74] </ref>.
Reference: [EB77] <author> M. C. Easton and B. T. Bennett. </author> <title> Transient-free working set statistics. </title> <journal> Communications of the ACM, </journal> <volume> 20(2), </volume> <month> February </month> <year> 1977. </year>
Reference-contexts: In the following chapters we presents two hardware methods to measure the average working set size on the fly with constant amount of memory. 8 CHAPTER 2 RELATED WORKS Methods of computing the average working set size can be found either in works purely focused on calculating working set statistics <ref> [DS72, ST74, EB77, Spi77] </ref>, or as by-products of the efforts to model the program behavior [DS72, OC75, CV76]. Since the intended application of the method is to obtain fast, reliable, and economical estimates, the time and space spent in analyzing a program trace must be significantly small.
Reference: [Har89] <author> Luddy Harrison. </author> <title> The Interprocedural Analysis and Automatic Parallelization of Scheme Programs. </title> <type> PhD thesis, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urabana-Champaign, </institution> <month> February </month> <year> 1989. </year> <month> 59 </month>
Reference-contexts: Run-time system's effect on program locality is 5 excluded here even though it is also interesting topic to study. The example of parallelizing compiler systems referenced here is Parcel <ref> [Har89, IP88] </ref>. The system has been successfully developed for the automatic parallelization of Scheme programs for parallel shared memory architectures, at the Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign. The parallelizing compiler's restructuring procedures involve three phases: interproce-dural analysis, preparatory optimizations, and parallelization transformation. <p> The compiler has two central algorithms to parallelize recursive calls: exit-loop translation and recursion splitting. Here we will not focus our attention on how those algorithms proceed, rather, we would concentrate on their effect on program locality behavior. <ref> [Har89, IP88] </ref> provides details on the algorithms. Recursive calls are transformed into iterative versions either by the recursion splitting algorithm or by tail-recursion elimination. The iterative version is transformed into a collection of do-loops.
Reference: [IP88] <author> Williams Ludwell Harrison III and David A. Padua. </author> <title> Parcel: Project for the automatic restructuring and concurrent evaluation of lisp. </title> <booktitle> In Proceedings of the 1988 International Conference on Supercomputing, </booktitle> <year> 1988. </year>
Reference-contexts: Run-time system's effect on program locality is 5 excluded here even though it is also interesting topic to study. The example of parallelizing compiler systems referenced here is Parcel <ref> [Har89, IP88] </ref>. The system has been successfully developed for the automatic parallelization of Scheme programs for parallel shared memory architectures, at the Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign. The parallelizing compiler's restructuring procedures involve three phases: interproce-dural analysis, preparatory optimizations, and parallelization transformation. <p> The compiler has two central algorithms to parallelize recursive calls: exit-loop translation and recursion splitting. Here we will not focus our attention on how those algorithms proceed, rather, we would concentrate on their effect on program locality behavior. <ref> [Har89, IP88] </ref> provides details on the algorithms. Recursive calls are transformed into iterative versions either by the recursion splitting algorithm or by tail-recursion elimination. The iterative version is transformed into a collection of do-loops.
Reference: [OC75] <author> H. Opderbeck and W. W. Chu. </author> <title> The renewal model for program behavior. </title> <journal> SIAM Journal on Computing, </journal> <volume> 4(3), </volume> <month> September </month> <year> 1975. </year>
Reference-contexts: size on the fly with constant amount of memory. 8 CHAPTER 2 RELATED WORKS Methods of computing the average working set size can be found either in works purely focused on calculating working set statistics [DS72, ST74, EB77, Spi77], or as by-products of the efforts to model the program behavior <ref> [DS72, OC75, CV76] </ref>. Since the intended application of the method is to obtain fast, reliable, and economical estimates, the time and space spent in analyzing a program trace must be significantly small. Keeping our intended application in mind, we discuss the feasibility of these methods. <p> Keeping our intended application in mind, we discuss the feasibility of these methods. Of course, there are other dimensions along which to evaluate the research. <ref> [OC75] </ref> has a formula for the average working set size based on the renewal process model of the reference string.
Reference: [PL71] <author> Gaver Jr. Donald P. and Peter A. W. Lewis. </author> <title> Probability models for buffer storage allocation problems. </title> <journal> Journal of the ACM, </journal> <volume> 18(2), </volume> <month> April </month> <year> 1971. </year>
Reference: [Rao78] <author> Gururaj S. Rao. </author> <title> Performance analysis of cache memories. </title> <journal> Journal of the ACM, </journal> <volume> 25(3), </volume> <month> July </month> <year> 1978. </year>
Reference: [Ros76] <author> Sheldon M. Ross. </author> <title> A First Course in Probability. </title> <publisher> Macmillan Publishing Co., Inc., </publisher> <address> NewYork, </address> <year> 1976. </year>
Reference: [Ros80] <author> Sheldon M. Ross. </author> <title> Introduction to Probability Models. 2nd ed. </title> <publisher> Academic Press, Inc., </publisher> <year> 1980. </year>
Reference: [Spi77] <author> Jeffrey R. Spirn. </author> <title> Program Behavior: Models and Measurements. </title> <booktitle> Operating and programming systems series. </booktitle> <publisher> Elsevier North-Holland, </publisher> <address> NewYork, </address> <year> 1977. </year>
Reference-contexts: In the following chapters we presents two hardware methods to measure the average working set size on the fly with constant amount of memory. 8 CHAPTER 2 RELATED WORKS Methods of computing the average working set size can be found either in works purely focused on calculating working set statistics <ref> [DS72, ST74, EB77, Spi77] </ref>, or as by-products of the efforts to model the program behavior [DS72, OC75, CV76]. Since the intended application of the method is to obtain fast, reliable, and economical estimates, the time and space spent in analyzing a program trace must be significantly small. <p> The simplest method of calculating the average working set size has been proposed in <ref> [DS72, Spi77] </ref>. Based on the independent reference model about the reference string, they devised a fairly simple algorithm. It turned out that the same algorithm can be used, without any stochastic assumption at all about the reference string, to calculate the average working set size without error [ST74].
Reference: [ST74] <author> D. R. Slutz and I. L. Traiger. </author> <title> A note on the calculation of average working set size. </title> <journal> Communications of the ACM, </journal> <volume> 17(10), </volume> <month> October </month> <year> 1974. </year> <month> 60 </month>
Reference-contexts: In the following chapters we presents two hardware methods to measure the average working set size on the fly with constant amount of memory. 8 CHAPTER 2 RELATED WORKS Methods of computing the average working set size can be found either in works purely focused on calculating working set statistics <ref> [DS72, ST74, EB77, Spi77] </ref>, or as by-products of the efforts to model the program behavior [DS72, OC75, CV76]. Since the intended application of the method is to obtain fast, reliable, and economical estimates, the time and space spent in analyzing a program trace must be significantly small. <p> Based on the independent reference model about the reference string, they devised a fairly simple algorithm. It turned out that the same algorithm can be used, without any stochastic assumption at all about the reference string, to calculate the average working set size without error <ref> [ST74] </ref>. Their method [DS72, ST74] is: Let r 1 r 2 r K be the total reference string of length K. r i is the memory block accessed by the i th memory reference. <p> Based on the independent reference model about the reference string, they devised a fairly simple algorithm. It turned out that the same algorithm can be used, without any stochastic assumption at all about the reference string, to calculate the average working set size without error [ST74]. Their method <ref> [DS72, ST74] </ref> is: Let r 1 r 2 r K be the total reference string of length K. r i is the memory block accessed by the i th memory reference. <p> Then not only would the hashing function 11 be fixed according to the memory block size, but also the potential collision problem of the hashing table remains, together with complex hardware. Another alternative is to implement the least recently used (LRU) stack data structure <ref> [ST74] </ref>. The scheme on LRU stack data structure requires much time for we must maintain the stack for every reference and still requires unbounded storage depending on the total number of accessed memory blocks during a program's execution and the range of the window size. <p> However, the use of the associative memory is problematic, as we discussed on page 11, concerning the problem with the method of <ref> [DS72, ST74] </ref>. Another problem that we observe concerns the accuracy of the measuring technique. When we maintain as many Markov chains as the number of all the accessed memory blocks throughout the whole trace, the 1= = 1 + b=a value easily becomes too close to 1. <p> The Figures 3.4 and 3.5 show the real average working set sizes and the predicted average working set sizes over varying time granule size (300, 500, 800, and 1000). The real 24 working set sizes are calculated according to the method of <ref> [DS72, ST74] </ref>. <p> Our method predicts well the window sizes that would have 2; 3; : : :; and 11 unique memory blocks in average. The calculation of the real data was done by the method of <ref> [ST74] </ref> without any estimation. The Figures 5.1 and 5.2 show, at a glance, the accuracy of our method.
References-found: 21

