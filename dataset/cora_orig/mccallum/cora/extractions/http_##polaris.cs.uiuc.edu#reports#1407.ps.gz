URL: http://polaris.cs.uiuc.edu/reports/1407.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Email: e-mail: meier@csrd.uiuc.edu, gallivan@csrd.uiuc.edu  
Title: A NEW FAMILY OF PRECONDITIONED ITERATIVE SOLVERS FOR NONSYMMETRIC LINEAR SYSTEMS  
Author: Ulrike Meier Yang and Kyle A. Gallivan 
Address: 1308 W. Main St. Urbana, IL 61801, USA  
Affiliation: Coordinated Science Laboratory University of Illinois  
Abstract: A new family of iterative methods, the family of EN-like methods, is introduced, and its relationship to other methods is investigated. The complexity and convergence behavior of the new methods as well as their restarted and truncated versions are examined. The methods are also shown to be suitable in the context of inner/outer iteration schemes. Their adaptive versions are included into a robust software package PARASPAR, and numerical experiments are presented, which demonstrate the efficiency of several members of this new family in comparison with other known methods.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> M. Arioli, I. Duff, D. Ruiz, </author> <title> "Stopping criteria for iterative solvers", </title> <type> Tech. Report, </type> <institution> CERFACS, Toulouse, France, </institution> <year> 1992. </year>
Reference-contexts: were obtained through a standard five point finite difference discretization of the following two-dimensional partial differential equation, which was taken from [29], u xx u yy + fi (u x + u y ) = f on (56) with Dirichlet boundary conditions u = c on @; (57) where = <ref> [0; 1] </ref> fi [0; 1]. We chose step size h = 0:01, which leads to a matrix of order 9801. 7.1. Problem 1 For our first problem we chose fi = 1. This is an example for which CGS and BiCGSTAB converge fairly quickly. <p> a standard five point finite difference discretization of the following two-dimensional partial differential equation, which was taken from [29], u xx u yy + fi (u x + u y ) = f on (56) with Dirichlet boundary conditions u = c on @; (57) where = <ref> [0; 1] </ref> fi [0; 1]. We chose step size h = 0:01, which leads to a matrix of order 9801. 7.1. Problem 1 For our first problem we chose fi = 1. This is an example for which CGS and BiCGSTAB converge fairly quickly.
Reference: 2. <author> S. Ashby, M. Holst, T. Manteuffel, P. </author> <title> Saylor, "The role of the inner product in stopping criteria for conjugate gradient iterations", </title> <type> Tech. Report, </type> <institution> Lawrence Livermore National Laboratory, </institution> <year> 1992. </year>
Reference: 3. <author> C. </author> <title> Broyden, "A new method of solving nonlinear simultaneous equations", </title> <journal> Computer J., </journal> <pages> 12(1969) 94-99. </pages>
Reference-contexts: These methods, however, assume symmetric (and often positive definite) matrices, and we will not consider them here, since our goal is to solve nonsymmetric linear systems. Instead, we will focus on some variants of Broyden's method <ref> [3] </ref>, a quasi-Newton method, which is suitable for solving nonsymmetric linear systems. F is defined here by F (x) = Ax b, and its Jacobian equals A. In its most general form, Broyden's method is given by Algorithm 1. <p> There are two undefined variables, f k and ff k , whose choice must be considered. For the original Broyden's method, which is also often called Broyden's 'good' method (GBM), Broyden used f k = H H k p k <ref> [3] </ref>. <p> Broyden suggested in <ref> [3] </ref> to choose ff k , so that kr k+1 k &lt; kr k k.
Reference: 4. <author> C. </author> <title> Broyden, "The convergence of single-rank quasi-Newton methods", </title> <journal> Math. Comp., </journal> <pages> 24(1970) 365-382. </pages>
Reference: 5. <author> C. Broyden, J. Dennis, J. </author> <title> More, "On the local and superlinear convergence of Quasi-Newton methods", </title> <journal> J. Inst. Maths. Applics, </journal> <volume> Vol. 12, </volume> <pages> pp. 223-245, </pages> <year> 1973. </year>
Reference: 6. <author> J. Demmel, M. Heath, H. van der Vorst, </author> <title> "Parallel numerical linear algebra", </title> <booktitle> Acta Numerica (1993), </booktitle> <pages> 111-197. </pages>
Reference-contexts: In this case its operation count as given in Table 1 is identical to that of eGCR. Since it also can lead to instability, it has been suggested to use the classical Gram-Schmidt algorithm twice <ref> [6] </ref>. In order to get an idea of the actual computational complexity in terms of flops, in Table 2 and 3 the number of flops per iteration step for the truncated and restarted versions are given.
Reference: 7. <author> J. Dennis, Jr., J. </author> <title> More, "Quasi-Newton methods, motivation and theory", </title> <journal> SIAM Review 1(1977), </journal> <pages> 46-89. </pages>
Reference-contexts: CCR-9120105. 2. Two Families of Iterative Linear Solvers 2.1. The Family of Broyden Methods An important class of methods based on rank-k updates are the quasi-Newton methods <ref> [7] </ref>. The purpose of quasi-Newton methods is to determine the zero of a function F or minimize a function G. They approximate the Jacobian of F or the Hessian of G, which is symmetric and often positive definite, or their inverses. <p> They approximate the Jacobian of F or the Hessian of G, which is symmetric and often positive definite, or their inverses. There are a variety of effective quasi-Newton methods, such as the Fletcher-Powell-Davidon method and the BFGS method <ref> [7] </ref>. These methods, however, assume symmetric (and often positive definite) matrices, and we will not consider them here, since our goal is to solve nonsymmetric linear systems. Instead, we will focus on some variants of Broyden's method [3], a quasi-Newton method, which is suitable for solving nonsymmetric linear systems.
Reference: 8. <author> J. Dennis, Jr., R. Schnabel, </author> <title> "Numerical methods for unconstrained optimization and nonlinear equations", </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1983. </year>
Reference-contexts: It can be proved that, with this choice, H k+1 is the solution to the minimization problem (min kH 1 k Bk F ) on the set of all matrices B that fulfill the secant condition Bp k = q k ; (6) see <ref> [8] </ref>. (k:k F denotes the Frobenius norm.) By a similar argument, minimizing kH k Hk F where H is an element of the set of all matrices that fulfill the following form of the secant condition Hq k = p k ; (7) one obtains the choice f k = q
Reference: 9. <author> P. Deuflhard, R. Freund, A. Walter, </author> <title> "Fast secant methods for the iterative solution of large nonsymmetric linear systems", </title> <booktitle> IMPACT of Computing in Science and Engineering Vol. </booktitle> <volume> 2, </volume> <pages> pp. 244-276, </pages> <year> 1990. </year>
Reference-contexts: In the past, methods of family (i) had a bad reputation for solving linear systems, but recent efforts <ref> [9] </ref> have shown that different line search principles lead to versions that are competitive with GMRES [22]. Under certain assumptions, members of both families will terminate after a finite number of steps and have local superlinear convergence. <p> He also states that this choice of ff k can lead to worse results than choosing an ff k that does not necessarily fulfill kr k+1 k &lt; kr k k, e.g., ff k = 1. Deuflhard et al. <ref> [9] </ref> propose ff k = k r k k q k and they show that the best ff k for a method depends on the choice of f k . Their experiments show that this choice produces Broyden's methods that are competitive with GMRES. <p> This lemma shows that one step of an EN-like method can be considered as an iteration step of the corresponding Broyden's method with ff k = 1 followed by an iteration step of Broyden's method using the optimal line search principle of <ref> [9] </ref> without updating the approximation to A 1 . In our experiments in Section 7, we will focus our attention specifically on EN, GEN and the EN-like method with f k = q k , which we will call BEN. 2.3. <p> If we replace H k+1 by its definition, we get H k+1 x = H k x + k x k q k = H 0 x + i=0 i x i q i Using this, we can rewrite Broyden's method in the following form (see also <ref> [9] </ref>) Algorithm 7. <p> One would expect the latter approach to lead to better convergence, since more information is being kept. We will see in Section 7 that this not always true. In fact, Deuflhard, Freund and Walter <ref> [9] </ref> saw in their experiments that for Broyden's methods this approach in general is worse than restarting. Our experiments will however show that this result does not necessarily transfer to the EN-like methods. <p> We have included GCR and the equivalent GMRES [22] here since they are related to the considered methods (see next section), and CGS [24] and BiCGSTAB [27] since they are very popular solvers. For Broyden's methods, only the operation counts for optimal line search according to <ref> [9] </ref> are given. Those for GBM or BBM with ff k = 1 would be slightly lower. 'dmv' stands here for dense matrix-vector multiplication. <p> We ignore memory requirements of order k or k 2 , since we assume that k is in general small compared to n. It is possible to save a substantial amount of storage for GBM (see <ref> [9] </ref> [17]). This change requires however an additional k daxpys per iteration step if ff i 6= 1. Unfortunately, it is not possible to use a similar trick for GEN. We see here that unless m is small, CGS and BiCGSTAB require less memory than the other methods. <p> The finite termination property is more of theoretical than of practical interest. Therefore, it is important to examine the convergence behavior of the methods. Due to space limitation, we present the following theorems in condensed form. Most of the results for GBM and BBM can also be found in <ref> [9] </ref>. The proofs for the results for the EN-like methods and the additional results for the Broyden methods can be found in [17]. The following theorem characterizes convergence for BBM, BEN, EN and the projected Broyden's method.
Reference: 10. <author> S. Eisenstat, H. Elman, M. Schultz, </author> <title> "Variational iterative methods for nonsymmetric systems of linear equations", </title> <journal> SIAM J. Numer. Anal. </journal> <year> (1983) </year> <month> 345-357. </month>
Reference-contexts: This method was developed independently by Gay and Schnabel [15] who call it Broyden's method with projected updates. There is another choice of f k that leads to a version that is equivalent to the general conjugate residual method (GCR) <ref> [10] </ref> or GMRES with the initial vector ~x 0 = x 0 + H 0 r 0 and consequently terminates within n steps [31]. For this method we choose f k = (I AH k ) H (I AH k )q k . <p> For truncated methods, it is far more difficult to derive any convergence results. It is however possible for one of the methods considered to prove convergence using an argument similar to that used for ORTHOMIN <ref> [10] </ref> [12]. Let us investigate the tSEN method for a real linear system Ax = b. Since one can prove convergence for the full SEN method and the restarted method SEN (m) in the same way, we summarize the result in the following theorem. <p> If we recall the equivalent estimate for ORTHOMIN <ref> [10] </ref> [12] kr k+1 k (1 min (AH 0 ) 0 A T AH 0 ) we see that the main difference between the two estimates is the factor 1 i=km+1 which equals 1 only when AH 0 r k is orthogonal to all c i ; i = k m
Reference: 11. <author> T. Eirola, O. Nevanlinna, </author> <title> "Accelerating with rank-one updates", </title> <journal> Lin. Alg. and its Appl. </journal> <volume> Vol. 121, </volume> <pages> pp. 511-520, </pages> <year> 1989. </year>
Reference-contexts: Specifically, two families of algorithms are considered: (i) the family of Broyden algorithms for nonsymmetric linear systems (ii) the family of EN-like methods, a new family of methods, which includes a method proposed by Eirola and Nevanlinna <ref> [11] </ref>. In the past, methods of family (i) had a bad reputation for solving linear systems, but recent efforts [9] have shown that different line search principles lead to versions that are competitive with GMRES [22]. <p> Their experiments show that this choice produces Broyden's methods that are competitive with GMRES. In our experiments, this choice of ff k will be used. 2.2. The Family of EN-like Methods The EN method was first proposed by Eirola and Nevanlinna in <ref> [11] </ref>. The main idea is to improve an approximation H k to A 1 via a rank-one update ~u k v H k on each iteration of the method while simultaneously improving an approximation x k to the solution of the linear system. <p> Since the evaluation of f k is more complicated than for GEN or BEN, the number of operations is however increased. For detail see <ref> [11] </ref>, [17] and [31]. If we do not require the evaluation of x k+1 in each iteration step, it is possible to achieve further savings in EN and SEN by avoiding the evaluation of the updates for x k+1 in each iterations step and accumulating the coefficients instead. <p> Relationships between Methods In the previous sections, we have indicated that Broyden's methods and the EN-like methods are related to various other known methods, particularly GCR. In this section, we summarize these relationships. As mentioned in <ref> [11] </ref> and proved in [31], one can derive GCR from the EN method by replacing ~u k = H k E k r k through ~u k = H k r k : (30) A more thorough investigation shows that GCR and ORTHOMIN are related to SEN and tSEN.
Reference: 12. <author> H. Elman, </author> <title> "Iterative methods for large, sparse, nonsymmetric systems of linear equations", </title> <type> Research Report 229, </type> <institution> Yale University, </institution> <year> 1982. </year>
Reference-contexts: For truncated methods, it is far more difficult to derive any convergence results. It is however possible for one of the methods considered to prove convergence using an argument similar to that used for ORTHOMIN [10] <ref> [12] </ref>. Let us investigate the tSEN method for a real linear system Ax = b. Since one can prove convergence for the full SEN method and the restarted method SEN (m) in the same way, we summarize the result in the following theorem. <p> If we recall the equivalent estimate for ORTHOMIN [10] <ref> [12] </ref> kr k+1 k (1 min (AH 0 ) 0 A T AH 0 ) we see that the main difference between the two estimates is the factor 1 i=km+1 which equals 1 only when AH 0 r k is orthogonal to all c i ; i = k m +
Reference: 13. <author> K. Gallivan, A. Sameh, Z. Zlatev, </author> <title> "A parallel hybrid sparse linear system solver", </title> <booktitle> Computing Systems in Engineering Vol. </booktitle> <volume> 1, </volume> <pages> pp. 183-195, </pages> <year> 1990. </year>
Reference-contexts: For the former type of preconditioning the new algorithms are considered as an inner as well as an outer method. The latter preconditioner is taken from PARASPAR, a robust parallel software package based on Y12M <ref> [13] </ref>, which has many other interesting features. The new family of methods appears to be very suitable for the strategy used in PARASPAR that gives it its robustness. <p> Due to space limitations, we will not pursue this approach in this paper, but it is considered in [17]. 6.2. PARASPAR A hybrid software package called PARASPAR, which is based on Y12M <ref> [13] </ref> [32] and combines both iterative and direct methods, has been used as a framework for some of our experiments. Direct methods, e.g., sparse Gauss elimination schemes, while achieving in general high accuracy, are often too time consuming and have only a low level of parallelism. <p> A more detailed description of PARASPAR, including the stopping criterion and the choice of the drop tolerance can be found in <ref> [13] </ref> and [32]. 6.3. Adaptive Versions All the methods under investigation have been added to PARASPAR.
Reference: 14. <author> D. Gay, </author> <title> "Some convergence properties of Broyden's method", </title> <journal> SIAM J. Numer. Anal. </journal> <volume> Vol. 16, </volume> <pages> pp. 623-630, </pages> <year> 1979. </year>
Reference-contexts: Let us now turn our attention to the second undetermined parameter, ff k . The most obvious choice for ff k is 1. One can show that for this case Broyden's method terminates within at most 2n steps <ref> [14] </ref> (see also Section 5). Nevertheless, this is not always a desirable choice. <p> In Section 7, we will show that in practice they often perform significantly better. One of the amazing, unexpected properties of Broyden's method is its finite termination property, which occurs for ff k = 1. Gay showed that Broyden's method terminates within at most 2n steps <ref> [14] </ref>. Recently, O'Leary [18] characterized the vectors that cause the finite termination. Now, due to the relationship between Broyden's method and the EN-like method, it is also possible to prove finite termination for the EN-like method (see [17]).
Reference: 15. <author> D. Gay, R. Schnabel, </author> <title> "Solving systems of nonlinear equations by Broyden's method with projected updates", </title> <editor> in O. L. Mangasarian, R. Meyer, S. Robinson (eds.), </editor> <booktitle> Nonlinear Programming Vol. </booktitle> <volume> 3, </volume> <pages> pp. 245-281, </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: This method was developed independently by Gay and Schnabel <ref> [15] </ref> who call it Broyden's method with projected updates.
Reference: 16. <author> D. Luenberger, </author> <title> "Linear and nonlinear programming", </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1984. </year>
Reference-contexts: Then, Broyden's method will terminate within at most n steps, since the algorithm constructs a better approximation H k to A 1 on each iteration, until finally H n = A 1 , if f H i q i 6= 0; i = 0; :::; n 1, (see also <ref> [16] </ref>). Unfortunately, this is not the case if A is nonsymmetric, and finite termination within n steps is no longer guaranteed for this choice of f k .
Reference: 17. <author> U. Meier Yang, </author> <title> "A family of preconditioned iterative solvers for sparse linear systems", </title> <type> Ph.D. Thesis, Report UIUCDCS-R-95-1904, </type> <institution> Department of Computer Science, University of Illinois, </institution> <year> 1995, </year> <note> (also available as CSRD-Report 1408 through anonymous ftp to sp2.csrd.uiuc.edu in directory CSRD Info/reports). </note>
Reference-contexts: The new family of methods appears to be very suitable for the strategy used in PARASPAR that gives it its robustness. A more detailed discussion of the results of this paper as well as the proofs for the theorems and lemmas can be found in <ref> [17] </ref>. fl This research was supported in part by the National Science Foundation under Grant No. CCR-9120105. 2. Two Families of Iterative Linear Solvers 2.1. The Family of Broyden Methods An important class of methods based on rank-k updates are the quasi-Newton methods [7]. <p> be decomposed in the following way: ~x k+1 = x k + H k r k (24) x k+1 = ~x k+1 + ff k H k ~r k+1 (26) where ff k = k r k k q k The proof is straight forward and can be found in <ref> [17] </ref>. <p> An induction-based proof can be found in <ref> [17] </ref>. Even though an arbitrary z theoretically guarantees scaling-invariance, its choice is important practical matter. An unfortunate choice of z might make no difference or even degrade the convergence of the method, whereas a well-chosen z might lead to an improvement in the number of iterations. <p> Fortunately, for GBM and GEN, those can be performed without significantly increasing the number of operations or the storage needed, if one chooses the order of computations carefully. Unfortunately, the evaluation is highly recursive and leads to a decrease in parallelism. The complete algorithms are given in <ref> [17] </ref>. For the EN and the SEN method, where f k = E H k E k q k , it is possible to make use of the orthogonality of some vectors and so avoid the decrease in parallelism we encountered for GBM and GEN. <p> Since the evaluation of f k is more complicated than for GEN or BEN, the number of operations is however increased. For detail see [11], <ref> [17] </ref> and [31]. If we do not require the evaluation of x k+1 in each iteration step, it is possible to achieve further savings in EN and SEN by avoiding the evaluation of the updates for x k+1 in each iterations step and accumulating the coefficients instead. <p> Such an approach has been used for GCR in [26]. These new even more efficient versions, which we will call eEN, eSEN and eGCR can be found in <ref> [17] </ref>. Even the efficient versions are computationally expensive, since the gradual increase of the underlying subspace leads to an increase of both operation count and memory requirement with each iteration step. We will therefore also consider their restarted and truncated versions. <p> We ignore memory requirements of order k or k 2 , since we assume that k is in general small compared to n. It is possible to save a substantial amount of storage for GBM (see [9] <ref> [17] </ref>). This change requires however an additional k daxpys per iteration step if ff i 6= 1. Unfortunately, it is not possible to use a similar trick for GEN. We see here that unless m is small, CGS and BiCGSTAB require less memory than the other methods. <p> However, local convergence considerations show, that for some choices of f k ! k converges towards 1, when H 0 is a good approximation for A 1 , and in this case H k+1 acts on r k in a manner similar to the Newton iterate <ref> [17] </ref>. 5. Convergence Theory As mentioned in Section 3.2, the EN-like methods need to converge at least twice as fast as the Broyden methods, in order to be competitive. We will show here, that theoretically this is often the case. <p> Gay showed that Broyden's method terminates within at most 2n steps [14]. Recently, O'Leary [18] characterized the vectors that cause the finite termination. Now, due to the relationship between Broyden's method and the EN-like method, it is also possible to prove finite termination for the EN-like method (see <ref> [17] </ref>). Theorem 3 If f H k q k 6= 0, k = 0; 1; :::, the EN-like method converges within at most n steps. The finite termination property is more of theoretical than of practical interest. Therefore, it is important to examine the convergence behavior of the methods. <p> Due to space limitation, we present the following theorems in condensed form. Most of the results for GBM and BBM can also be found in [9]. The proofs for the results for the EN-like methods and the additional results for the Broyden methods can be found in <ref> [17] </ref>. The following theorem characterizes convergence for BBM, BEN, EN and the projected Broyden's method. <p> Due to space limitations, we will not pursue this approach in this paper, but it is considered in <ref> [17] </ref>. 6.2. PARASPAR A hybrid software package called PARASPAR, which is based on Y12M [13] [32] and combines both iterative and direct methods, has been used as a framework for some of our experiments. <p> The latter characteristic can lead to a significant improvement in efficiency, when the iterative method fails due to a low quality preconditioner, but does not diverge drastically. A detailed discussion can be found in <ref> [17] </ref>. 7. Numerical Experiments In this section, we illustrate some of the properties of the algorithms described in the previous sections with numerical experiments. For our experiments, we use several matrices derived from partial differential equations as well as a few test matrices from the Harwell-Boeing collection. <p> Overall, the results show that the use of EN in the context of inner/outer iterations schemes is competitive with GMRESR and superior when memory is limited. See <ref> [17] </ref> for a more detailed discussion, including the orthogonality preserving methods, which were mentioned in Section 6.1. 8. Conclusions and Future Work We introduced a new family of methods, the EN-like methods. Its complexity, relationships to other methods and convergence behavior was examined. <p> Therefore, we plan to investigate further more sophisticated truncation schemes possibly exploiting application-specific information in the context of an adaptive hybrid of restarted and truncated methods. Additionally, due to the connection of EN-like methods to Broyden methods, they can also be used as nonlinear solvers <ref> [17] </ref>, which may also be significant for some applications. This is also under investigation.
Reference: 18. <author> D. O'Leary, </author> <title> "Why Broyden's nonsymmetric method terminates on linear equations", </title> <type> Tech. Report CS-TR-3045, </type> <institution> University of Maryland, </institution> <year> 1993, </year> <note> to appear in SIAM J. on Optimization. </note>
Reference-contexts: In Section 7, we will show that in practice they often perform significantly better. One of the amazing, unexpected properties of Broyden's method is its finite termination property, which occurs for ff k = 1. Gay showed that Broyden's method terminates within at most 2n steps [14]. Recently, O'Leary <ref> [18] </ref> characterized the vectors that cause the finite termination. Now, due to the relationship between Broyden's method and the EN-like method, it is also possible to prove finite termination for the EN-like method (see [17]).
Reference: 19. <author> H. </author> <title> Rutishauser, "Theory of gradient methods", </title> <institution> Mitteilungen aus dem Institut fuer Angewandte Mathematik 8(1959) 24-29. </institution>
Reference-contexts: Preconditioning with Iterative Methods The idea of using an inner iterative method as a preconditioner can be found in the CGT method introduced by Rutishauser <ref> [19] </ref> who used the Chebyshev method as inner method to precondition the conjugate gradient method, an approach equivalent to Chebyshev polynomial preconditioning.
Reference: 20. <author> Y. Saad, </author> <title> "A flexible inner-outer preconditioned GMRES algorithm", </title> <type> Tech. Report, </type> <institution> Computer Science Department and MSI, University of Minnesota, </institution> <year> 1991. </year>
Reference-contexts: There are different ways to precondition iterative methods. We will consider here two different types of preconditioners, the use of an inner iterative method as a preconditioner similar to GMRESR [29] or FGMRES <ref> [20] </ref> and an incomplete LU factorization with numerical dropping. For the former type of preconditioning the new algorithms are considered as an inner as well as an outer method. <p> For the nonsymmetric case, there are various other methods, such as FGMRES <ref> [20] </ref> with GMRES as the outer method, or GMRESR [29] with GCR as the outer method and GMRES as the inner method, and others we consider further in this and the following section.
Reference: 21. <author> Y. Saad, M. Schultz, </author> <title> "Conjugate gradient-like algorithms for solving nonsymmetric linear systems", </title> <journal> Math. Comp, </journal> <pages> 44(1985) 417-424. </pages>
Reference-contexts: We will therefore also consider their restarted and truncated versions. An overview of restarted and truncated algorithms can be found in <ref> [21] </ref>. The methods can be restarted after m + 1 iterations by using x m+1 as the new starting guess x 0 . We truncate these methods by including only updates of the last m iterations.
Reference: 22. <author> Y. Saad, M. Schultz, </author> <title> "GMRES: a generalized minimal residual algorithm for solving nonsymmetric linear system", </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <pages> 7(1986) 856-869. </pages>
Reference-contexts: In the past, methods of family (i) had a bad reputation for solving linear systems, but recent efforts [9] have shown that different line search principles lead to versions that are competitive with GMRES <ref> [22] </ref>. Under certain assumptions, members of both families will terminate after a finite number of steps and have local superlinear convergence. As with other iterative methods such as GMRES, the full methods are too expensive, and restarted, truncated and adaptive versions must be considered. <p> In Table 1 the types and number of operations are given for a variety of methods. We have included GCR and the equivalent GMRES <ref> [22] </ref> here since they are related to the considered methods (see next section), and CGS [24] and BiCGSTAB [27] since they are very popular solvers. For Broyden's methods, only the operation counts for optimal line search according to [9] are given.
Reference: 23. <author> G. Schultz, </author> <title> "Iterative Berechnung der reziproken Matrix", </title> <journal> Z. Angew. Math. Mech. </journal> <pages> 13(1933) 57-59. </pages>
Reference-contexts: Since the EN-like method generates an approximation H k to A 1 , a relationship to matrix iterations that compute the inverse of a matrix is also likely. Such a method can be found in <ref> [23] </ref>.
Reference: 24. <author> P. Sonneveld, </author> <title> "CGS, a fast Lanczos-type solver for nonsymmetric linear systems", </title> <journal> SIAM J. Sci. Stat. Comp., </journal> <pages> 10(1989) 36-52. </pages>
Reference-contexts: In Table 1 the types and number of operations are given for a variety of methods. We have included GCR and the equivalent GMRES [22] here since they are related to the considered methods (see next section), and CGS <ref> [24] </ref> and BiCGSTAB [27] since they are very popular solvers. For Broyden's methods, only the operation counts for optimal line search according to [9] are given. Those for GBM or BBM with ff k = 1 would be slightly lower. 'dmv' stands here for dense matrix-vector multiplication.
Reference: 25. <author> E. de Sturler, D. Fokkema, </author> <title> "Nested Krylov methods and preserving the orthogonality", </title> <editor> in T. Manteuffel and S. McCormick, eds., </editor> <booktitle> Proceedings of the Sixth Copper Mountain Multigrid Conference on Multigrid Methods, </booktitle> <address> VA, </address> <year> 1993. </year>
Reference-contexts: Our experiments will however show that this result does not necessarily transfer to the EN-like methods. It is also possible to use more sophisticated truncated schemes, which can lead to better convergence (see <ref> [25] </ref> and [30]). The application of these to the family of EN-like methods is left as future work. Most of the efficient versions we have mentioned here truncate easily. The truncation of eEN, eSEN and eGCR is far more complicated. <p> A potential drawback of the inner-outer iteration schemes is that the generated Krylov space of the outer iteration is ignored when applying the inner iterative method to Az = y. This drawback was observed by de Sturler and Fokkema <ref> [25] </ref> for the case of GMRESR. The outer method generates a minimal residual polynomial, which is ignored by the inner iteration, which searches for a new minimal residual polynomial.
Reference: 26. <author> E. de Sturler, </author> <title> "Nested Krylov methods based on GCR", </title> <type> Technical Report 93-50, </type> <institution> Faculty of Technical Mathematics and Informatics, Delft University of Technology, Delft, </institution> <address> Holland, </address> <year> 1993, </year> <note> to appear in Journal of Computational and Applied Mathematics. </note>
Reference-contexts: Such an approach has been used for GCR in <ref> [26] </ref>. These new even more efficient versions, which we will call eEN, eSEN and eGCR can be found in [17]. Even the efficient versions are computationally expensive, since the gradual increase of the underlying subspace leads to an increase of both operation count and memory requirement with each iteration step.
Reference: 27. <author> H. van der Vorst, </author> <title> "BI-CGSTAB: a fast and smoothly converging variant of BI-CG for the solution of nonsymmetric linear systems", </title> <journal> SIAM J. Sci. Stat. Comp. </journal> <month> 13 </month> <year> (1992) </year> <month> 631-644. </month>
Reference-contexts: In Table 1 the types and number of operations are given for a variety of methods. We have included GCR and the equivalent GMRES [22] here since they are related to the considered methods (see next section), and CGS [24] and BiCGSTAB <ref> [27] </ref> since they are very popular solvers. For Broyden's methods, only the operation counts for optimal line search according to [9] are given. Those for GBM or BBM with ff k = 1 would be slightly lower. 'dmv' stands here for dense matrix-vector multiplication.
Reference: 28. <author> H. van der Vorst, </author> <title> "Conjugate gradient type methods for nonsymmetric linear systems", </title> <editor> in R. Beauwens and P. de Groen, eds., </editor> <title> Iterative Methods in Linear Algebra, </title> <publisher> North Holland, </publisher> <year> 1992, </year> <pages> 67-76. </pages>
Reference-contexts: The test problem is taken from <ref> [28] </ref>.
Reference: 29. <author> H. van der Vorst, C. Vuik, "GMRESR: </author> <title> A family of nested GMRES methods", </title> <note> to appear in Numerical Linear Algebra and its Applications. </note>
Reference-contexts: Iterative methods in both families require, like most other methods, a good preconditioner in order to be robust. There are different ways to precondition iterative methods. We will consider here two different types of preconditioners, the use of an inner iterative method as a preconditioner similar to GMRESR <ref> [29] </ref> or FGMRES [20] and an incomplete LU factorization with numerical dropping. For the former type of preconditioning the new algorithms are considered as an inner as well as an outer method. <p> For the nonsymmetric case, there are various other methods, such as FGMRES [20] with GMRES as the outer method, or GMRESR <ref> [29] </ref> with GCR as the outer method and GMRES as the inner method, and others we consider further in this and the following section. <p> For the problems considered here the use of classical Gram-Schmidt did not effect the stability. The first three matrices were obtained through a standard five point finite difference discretization of the following two-dimensional partial differential equation, which was taken from <ref> [29] </ref>, u xx u yy + fi (u x + u y ) = f on (56) with Dirichlet boundary conditions u = c on @; (57) where = [0; 1] fi [0; 1]. We chose step size h = 0:01, which leads to a matrix of order 9801. 7.1.
Reference: 30. <author> C. Vuik, </author> <title> "Further experiences with GMRESR", </title> <type> Technical Report 92-12, </type> <institution> Delft University of Technology, </institution> <year> 1992. </year>
Reference-contexts: Our experiments will however show that this result does not necessarily transfer to the EN-like methods. It is also possible to use more sophisticated truncated schemes, which can lead to better convergence (see [25] and <ref> [30] </ref>). The application of these to the family of EN-like methods is left as future work. Most of the efficient versions we have mentioned here truncate easily. The truncation of eEN, eSEN and eGCR is far more complicated.
Reference: 31. <author> C. Vuik, H. van der Vorst, </author> <title> "A comparison of some GMRES-like methods", </title> <journal> Linear Algebra and its Applications Vol. </journal> <volume> 160, </volume> <pages> pp. 131-162, </pages> <year> 1992. </year>
Reference-contexts: There is another choice of f k that leads to a version that is equivalent to the general conjugate residual method (GCR) [10] or GMRES with the initial vector ~x 0 = x 0 + H 0 r 0 and consequently terminates within n steps <ref> [31] </ref>. For this method we choose f k = (I AH k ) H (I AH k )q k . Its convergence behavior is similar to that of GCR or GMRES. <p> An example of this can be found in Figure 1, where for case (1) z is the eigenvector belonging to the eigenvalue 1 and for case (2) the one belonging to the largest eigenvalue max . Vuik and van der Vorst <ref> [31] </ref> suggest another scaling-invariant version of the EN method, which we will call the SEN method. <p> Since the evaluation of f k is more complicated than for GEN or BEN, the number of operations is however increased. For detail see [11], [17] and <ref> [31] </ref>. If we do not require the evaluation of x k+1 in each iteration step, it is possible to achieve further savings in EN and SEN by avoiding the evaluation of the updates for x k+1 in each iterations step and accumulating the coefficients instead. <p> Relationships between Methods In the previous sections, we have indicated that Broyden's methods and the EN-like methods are related to various other known methods, particularly GCR. In this section, we summarize these relationships. As mentioned in [11] and proved in <ref> [31] </ref>, one can derive GCR from the EN method by replacing ~u k = H k E k r k through ~u k = H k r k : (30) A more thorough investigation shows that GCR and ORTHOMIN are related to SEN and tSEN.
Reference: 32. <author> Z. Zlatev, </author> <title> "Computational methods for general sparse matrices", </title> <publisher> Kluwer Academic Publishers, Dordrecht, Holland, </publisher> <year> 1991. </year>
Reference-contexts: Due to space limitations, we will not pursue this approach in this paper, but it is considered in [17]. 6.2. PARASPAR A hybrid software package called PARASPAR, which is based on Y12M [13] <ref> [32] </ref> and combines both iterative and direct methods, has been used as a framework for some of our experiments. Direct methods, e.g., sparse Gauss elimination schemes, while achieving in general high accuracy, are often too time consuming and have only a low level of parallelism. <p> A more detailed description of PARASPAR, including the stopping criterion and the choice of the drop tolerance can be found in [13] and <ref> [32] </ref>. 6.3. Adaptive Versions All the methods under investigation have been added to PARASPAR.
References-found: 32

