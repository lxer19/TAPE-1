URL: http://www.cse.psu.edu/~barlow/zha-intro.ps
Refering-URL: http://www.cse.psu.edu/~barlow/papers.html
Root-URL: http://www.cse.psu.edu
Title: STABLE CHASING ALGORITHMS FOR MODIFYING COMPLETE AND PARTIAL SINGULAR VALUE DECOMPOSITIONS perturbation theory for updating
Author: JESSE L. BARLOW HONGYUAN ZHA AND PETER A. YOON 
Keyword: Key words. singular value decomposition, partial singular value decomposition, bidiagonal form, updating, downdating, stability  
Note: A  is also presented.  AMS(MOS) subject classifications. primary 15A18, 15A21, 65F15; secondary 62H20  
Abstract: Methods for updating and downdating singular value decompositions (SVDs) and partially reduced bidiagonal forms (partial SVDs) are introduced. The methods are based upon chasing procedures for updating the SVD and downdating procedures for orthogonal decompositions. The main feature of these methods is the ability to separate the singular values into "large" and "small" sets and then obtain an updated bidiagonal form with corresponding "large" and "small" columns. This makes for a more accurate update or downdate. 1. Introduction. We discuss methods for updating and downdating two different types of complete orthogonal decomposition of an m fi n matrix A where m n. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Abdallah and Y. Hu. </author> <title> Parallel VLSI computing array implementation for signal subspace updating algorithm. </title> <journal> IEEE Trans. Acoust., Speech, Signal Processing, </journal> <volume> ASSP-37:742-748, </volume> <year> 1989. </year>
Reference-contexts: Our approaches to downdating the decompositions (1.2)-(1.3) use ideas from "chasing" algorithms <ref> [25, 1, 33, 18] </ref> and from the downdating algorithm due to Gill et al [12, 23]. A systolic array implementation of a scheme with similar data movement patterns to this one (but not similar numerical properties) is implemented in [19]. <p> Our numerical tests show some improvement in the accuracy of downdated singular values using our algorithm instead of general chasing. Thus we see potential use of these algorithms in subspace tracking because of the possibility of systolic array implementation <ref> [1, 19] </ref>. Acknowledgement. Dr. Zhenyue Zhang made his helpful suggestions that improved this paper.
Reference: [2] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. DuCroz, A. Greenbaum, S. Ham-marling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <title> LAPACK User's Guide. </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: These tests were performed using FORTRAN on a SUN 4/20 in IEEE Standard single precision with machine precision 2 23 10 7 . In checking the accuracy of the algorithms we used as a reference the LAPACK routine DGESVD <ref> [2] </ref> which implements in double precision the SVD via bidiagonalization followed by computing the SVD of the bidiagonal matrix [7]. All test matrices were generated by the LAPACK Test Matrix Suite [8]. <p> However, the application the LAPACK <ref> [2, 7] </ref> bidiagonal SVD algorithm obtained the same singular values again to machine accuracy. We tried other values of diagonal elements of B and dimensions n, but we always obtained very accurate singular values except when underflow became an issue.
Reference: [3] <author> J.L. Barlow. </author> <title> Error analysis of update methods for the symmetric eigenvalue problem. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 14 </volume> <pages> 598-618, </pages> <year> 1993. </year>
Reference-contexts: The alternative to our approach is that of finding the zeroes of a particular spectral function <ref> [13, 6, 21, 26, 3, 16, 15] </ref>. That approach, as yet, does not allow us to separate the singular values into separate blocks in the manner discussed in this paper. There are five sections after this introductory one.
Reference: [4] <author> J.L. Barlow and J.W. Demmel. </author> <title> Computing accurate eigensystems of scaled diagonally dominant matrices. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 27 </volume> <pages> 762-791, </pages> <year> 1990. </year>
Reference-contexts: This form preserves more of the accuracy of the small singular values and is not achieved by standard chasing procedures. We can then use one of several algorithms to find the singular values of the bidiagonal matrix B to relative accuracy <ref> [7, 4, 11] </ref>. The singular vector matrix can be modified by a procedure due to Gu and Eisenstat [16] in O (n 2 ) operations (the constant on n 2 depends upon machine precision). <p> We can then call a bidiagonal SVD routine <ref> [7, 4, 11] </ref> which will obtain the singular values of ^ B to relative accuracy. 3.2. The LINPACK downdating algorithm and its relation to the chasing schemes. <p> Let be a scaling matrix. Define s = 1 ffiz; E = ffiB 1 (4.24) ^ M = ^ B 1 ; M = B 1 ; ^z = 1 z (4.25) We now use some perturbation results from Barlow and Demmel <ref> [4] </ref> and Demmel and Veselic' [9] to obtain error bounds on the singular values and singular vectors of the updated matrix ^ B. Lemma 4.1. Let i (H; D) denote the eigenvalues of the pencil H D where H and D are symmetric and D is positive definite. <p> This is similar to results in <ref> [9, 4, 31] </ref>. Lemma 4.4. Assume the hypothesis and terminology of the previous lemma. Let v i be the i th singular vector of ~ B and let ^v i be the i th singular vector of ^ B. <p> The effect of * B is less critical. Let j be a constant such that k ffi ^ Bx k j k ^ Bx k (4.32) for all x 2 &lt; n . Then from <ref> [4, 9, 31] </ref> we have the following bound on the singular values. Lemma 4.6. Let ^ B and ^ B + ffi ^ B satisfiy the bound (4.32). <p> A T A = V B T BV T MODIFYING THE SVD AND PARTIAL SVD 24 then V 1 A T AV = B T B (I + E): Both sides of the above equality are similar to the pencil B T Bx = (I + E) 1 x: From <ref> [4] </ref>, the factor k E k makes only a small relative difference in the singular values. 5. Numerical Experiments. In this section, we present the results of numerical experiments. We verify that our algorithms preserve more of the accuracy, especially, of the small singular values.
Reference: [5] <author> A. Bjorck, H. Park, and L. Elden. </author> <title> Accurate downdating of least squares solutions. </title> <type> Preprint Series 947, </type> <institution> Institute for Mathematics and its Applications, University of Minnesota, Min-neapolis, MN, </institution> <month> April </month> <year> 1992. </year> <title> MODIFYING THE SVD AND PARTIAL SVD 28 </title>
Reference-contexts: The LINPACK downdating algorithm and its relation to the chasing schemes. The following downdating procedure due to Gill et al [12] is considered MODIFYING THE SVD AND PARTIAL SVD 11 the most accurate downdating procedure that does not require information from the original matrix A <ref> [5] </ref>. It is the procedure that is implemented in LINPACK [10]. We make use of the fact that B T B zz T is positive definite if and only if k B T z k&lt; 1. Algorithm 3.2 (LINPACK downdating procedure). 1.
Reference: [6] <author> J.R. Bunch and C.P. Nielsen. </author> <title> Updating the singular value decomposition. </title> <journal> Numerical Mathe-matik, </journal> <volume> 31 </volume> <pages> 111-129, </pages> <year> 1978. </year>
Reference-contexts: The alternative to our approach is that of finding the zeroes of a particular spectral function <ref> [13, 6, 21, 26, 3, 16, 15] </ref>. That approach, as yet, does not allow us to separate the singular values into separate blocks in the manner discussed in this paper. There are five sections after this introductory one.
Reference: [7] <author> J.W. Demmel and W.H. Kahan. </author> <title> Accurate singular values of bidiagonal matrices. </title> <journal> SIAM J. Sci. Stat. Computing, </journal> <volume> 11 </volume> <pages> 873-912, </pages> <year> 1990. </year>
Reference-contexts: This form preserves more of the accuracy of the small singular values and is not achieved by standard chasing procedures. We can then use one of several algorithms to find the singular values of the bidiagonal matrix B to relative accuracy <ref> [7, 4, 11] </ref>. The singular vector matrix can be modified by a procedure due to Gu and Eisenstat [16] in O (n 2 ) operations (the constant on n 2 depends upon machine precision). <p> We can then call a bidiagonal SVD routine <ref> [7, 4, 11] </ref> which will obtain the singular values of ^ B to relative accuracy. 3.2. The LINPACK downdating algorithm and its relation to the chasing schemes. <p> Thus if oe k &gt; tol &gt; oe k+1 , we preserve the rank revealing nature of the decomposition. MODIFYING THE SVD AND PARTIAL SVD 14 Since we now have algorithms that obtain the singular value of B to relative accuracy <ref> [7, 11] </ref>, we can obtain more accurate singular values and singular vectors of the updated matrix. Alternatively, we could use the following algorithm that more closely mimics Algorithm 3.1. <p> In checking the accuracy of the algorithms we used as a reference the LAPACK routine DGESVD [2] which implements in double precision the SVD via bidiagonalization followed by computing the SVD of the bidiagonal matrix <ref> [7] </ref>. All test matrices were generated by the LAPACK Test Matrix Suite [8]. <p> However, the application the LAPACK <ref> [2, 7] </ref> bidiagonal SVD algorithm obtained the same singular values again to machine accuracy. We tried other values of diagonal elements of B and dimensions n, but we always obtained very accurate singular values except when underflow became an issue.
Reference: [8] <author> J.W. Demmel and A. McKenney. </author> <title> A test matrix generation suite. </title> <type> Technical report, </type> <institution> Computer Science Department, Courant Institutute, </institution> <address> New York, NY, </address> <year> 1989. </year> <note> LAPACK Working Note #9. </note>
Reference-contexts: In checking the accuracy of the algorithms we used as a reference the LAPACK routine DGESVD [2] which implements in double precision the SVD via bidiagonalization followed by computing the SVD of the bidiagonal matrix [7]. All test matrices were generated by the LAPACK Test Matrix Suite <ref> [8] </ref>. In a sliding window method with the window size p, a data matrix with p rows is constructed from an observation matrix A by adding a new row to the previous data matrix and deleting the oldest row from the data matrix.
Reference: [9] <author> J.W. Demmel and K. Veselic'. </author> <title> Jacobi's method is more accurate than QR. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 13 </volume> <pages> 1204-1243, </pages> <year> 1992. </year>
Reference-contexts: Let be a scaling matrix. Define s = 1 ffiz; E = ffiB 1 (4.24) ^ M = ^ B 1 ; M = B 1 ; ^z = 1 z (4.25) We now use some perturbation results from Barlow and Demmel [4] and Demmel and Veselic' <ref> [9] </ref> to obtain error bounds on the singular values and singular vectors of the updated matrix ^ B. Lemma 4.1. Let i (H; D) denote the eigenvalues of the pencil H D where H and D are symmetric and D is positive definite. <p> This is similar to results in <ref> [9, 4, 31] </ref>. Lemma 4.4. Assume the hypothesis and terminology of the previous lemma. Let v i be the i th singular vector of ~ B and let ^v i be the i th singular vector of ^ B. <p> The effect of * B is less critical. Let j be a constant such that k ffi ^ Bx k j k ^ Bx k (4.32) for all x 2 &lt; n . Then from <ref> [4, 9, 31] </ref> we have the following bound on the singular values. Lemma 4.6. Let ^ B and ^ B + ffi ^ B satisfiy the bound (4.32).
Reference: [10] <author> J.J. Dongarra, J.R. Bunch, C.B. Moler, and G.W. Stewart. </author> <title> LINPACK User's Guide. </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: The following downdating procedure due to Gill et al [12] is considered MODIFYING THE SVD AND PARTIAL SVD 11 the most accurate downdating procedure that does not require information from the original matrix A [5]. It is the procedure that is implemented in LINPACK <ref> [10] </ref>. We make use of the fact that B T B zz T is positive definite if and only if k B T z k&lt; 1. Algorithm 3.2 (LINPACK downdating procedure). 1.
Reference: [11] <author> K.V. Fernando and B.N. Parlett. </author> <title> Accurate singular values and differential QD algorithms. </title> <type> Technical Report PAM-554, </type> <institution> Center for Pure and Applied Mathematics, University of Cal-ifornia, Berkeley, </institution> <address> CA, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: This form preserves more of the accuracy of the small singular values and is not achieved by standard chasing procedures. We can then use one of several algorithms to find the singular values of the bidiagonal matrix B to relative accuracy <ref> [7, 4, 11] </ref>. The singular vector matrix can be modified by a procedure due to Gu and Eisenstat [16] in O (n 2 ) operations (the constant on n 2 depends upon machine precision). <p> Finally, we specify the procedure qd. It produces the orthogonal factorization ^ B = QB where ^ B is lower bidiagonal and B is upper bidiagonal. This is the same as one unshifted Fernando-Parlett qd step <ref> [11] </ref>, hence the name. procedure qd (fl (1 : n); OE (1 : n 1); n); (* Produces the orthogonal factorization of a lower bidiagonal matrix *) (* fl (1 : n) is the diagonal on input and output. *) (* OE (1 : n 1) is the subdiagonal on input <p> We can then call a bidiagonal SVD routine <ref> [7, 4, 11] </ref> which will obtain the singular values of ^ B to relative accuracy. 3.2. The LINPACK downdating algorithm and its relation to the chasing schemes. <p> Thus if oe k &gt; tol &gt; oe k+1 , we preserve the rank revealing nature of the decomposition. MODIFYING THE SVD AND PARTIAL SVD 14 Since we now have algorithms that obtain the singular value of B to relative accuracy <ref> [7, 11] </ref>, we can obtain more accurate singular values and singular vectors of the updated matrix. Alternatively, we could use the following algorithm that more closely mimics Algorithm 3.1.
Reference: [12] <author> P.E. Gill, G.H. Golub, W. Murray, and M.A. Saunders. </author> <title> Methods for modifying matrix factorizations. </title> <journal> Math. Comp., </journal> <volume> 28 </volume> <pages> 505-535, </pages> <year> 1974. </year>
Reference-contexts: Our approaches to downdating the decompositions (1.2)-(1.3) use ideas from "chasing" algorithms [25, 1, 33, 18] and from the downdating algorithm due to Gill et al <ref> [12, 23] </ref>. A systolic array implementation of a scheme with similar data movement patterns to this one (but not similar numerical properties) is implemented in [19]. <p> n fi n1 1 C C (3.7) fi 1 = 1; fi i = s i j i ; j i = l=1 fl i = cabs (oe i ; j i z i ); i = 1; : : :; n:(3.10) This form is given by Gill et al <ref> [12] </ref> and given as the procedure computeRu in Algorithm 3.1. The matrix can be reduced to upper bidiagonal form in O (n 2 ) operations by exploiting the form (3.7). <p> We can then call a bidiagonal SVD routine [7, 4, 11] which will obtain the singular values of ^ B to relative accuracy. 3.2. The LINPACK downdating algorithm and its relation to the chasing schemes. The following downdating procedure due to Gill et al <ref> [12] </ref> is considered MODIFYING THE SVD AND PARTIAL SVD 11 the most accurate downdating procedure that does not require information from the original matrix A [5]. It is the procedure that is implemented in LINPACK [10]. <p> (a 1 ; a 2 ; : : :; a n ) T where a i = z i =oe i ; i = 1; 2; : : :; n: Thus ff is given by ff = u t 1 i=1 We now use a recurrence from Gill et al <ref> [12] </ref>.
Reference: [13] <author> G.H. Golub. </author> <title> Some modified matrix eigenvalue problems. </title> <journal> SIAM Review, </journal> <volume> 15 </volume> <pages> 318-344, </pages> <year> 1973. </year>
Reference-contexts: The alternative to our approach is that of finding the zeroes of a particular spectral function <ref> [13, 6, 21, 26, 3, 16, 15] </ref>. That approach, as yet, does not allow us to separate the singular values into separate blocks in the manner discussed in this paper. There are five sections after this introductory one.
Reference: [14] <author> G.H. Golub and C.F. Van Loan. </author> <title> Matrix Computations, Second Edition. </title> <publisher> The Johns Hopkins Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: This is just a classical backward error bound and will yield only the standard error bounds on the singular values <ref> [14] </ref>. A similar analysis works in downdating.
Reference: [15] <author> W.B. Gragg, J.R. Thorton, </author> <title> and D.D. Warner. Parallel divide and conquer algorithms for the symmetric tridiagonal eigenproblem and bidiagonal singular value problem. </title> <type> Technical report, </type> <institution> Department of Mathematics, Naval Postgraduate School, </institution> <address> Monterey, CA, </address> <year> 1992. </year>
Reference-contexts: The alternative to our approach is that of finding the zeroes of a particular spectral function <ref> [13, 6, 21, 26, 3, 16, 15] </ref>. That approach, as yet, does not allow us to separate the singular values into separate blocks in the manner discussed in this paper. There are five sections after this introductory one.
Reference: [16] <author> M. Gu and S.C. Eisenstat. </author> <title> A stable and efficient algorithm for the rank-one modification of the symmetric eigenproblem. </title> <type> Technical Report YALEU/DCS/RR-916, </type> <institution> Department of Computer Science, Yale University, </institution> <address> New Haven, CT, </address> <month> September </month> <year> 1992. </year>
Reference-contexts: We can then use one of several algorithms to find the singular values of the bidiagonal matrix B to relative accuracy [7, 4, 11]. The singular vector matrix can be modified by a procedure due to Gu and Eisenstat <ref> [16] </ref> in O (n 2 ) operations (the constant on n 2 depends upon machine precision). <p> The alternative to our approach is that of finding the zeroes of a particular spectral function <ref> [13, 6, 21, 26, 3, 16, 15] </ref>. That approach, as yet, does not allow us to separate the singular values into separate blocks in the manner discussed in this paper. There are five sections after this introductory one.
Reference: [17] <author> S. Van Huffel. </author> <title> Analysis of the Total Least Squares Problem and its Use in Parameter Estimation. </title> <type> PhD thesis, </type> <institution> Katholieke Unvisiteit Leuven, Leuven, Belguim, </institution> <year> 1987. </year>
Reference-contexts: We assume that the (k 1) st superdiagonal of B 1 is zero. For (1.2) we presume oe k tol. The form (1.2) is the familiar singular value decomposition (SVD). The form (1.3) is a special case of the partial singular value decomposition (partial SVD) described by Van Huffel <ref> [20, 17] </ref>. The updating problem is that of obtaining the SVD (partial SVD) of A where A = A fl Department of Computer Science and Engineering, The Pennsylvania State University, University Park, PA 16802-6106. e-mail: barlow@cse.psu.edu, zha@cse.psu.edu. The research of Jesse L.
Reference: [18] <author> S. Van Huffel and H. Park. </author> <title> Efficient reduction algorithms for bordered band matrices. </title> <type> Technical report, </type> <institution> ESAT Laboratory, Katholieke Universiteit Leuven, Leuven, Belgium, </institution> <year> 1992. </year>
Reference-contexts: Our approaches to downdating the decompositions (1.2)-(1.3) use ideas from "chasing" algorithms <ref> [25, 1, 33, 18] </ref> and from the downdating algorithm due to Gill et al [12, 23]. A systolic array implementation of a scheme with similar data movement patterns to this one (but not similar numerical properties) is implemented in [19].
Reference: [19] <author> S. Van Huffel and H. Park. </author> <title> Parallel tri- and bi-diagonalization of bordered bi-diagonal matrices. </title> <type> Techinical Report 93-024, </type> <institution> Army High Performance Computing Research Center, University of Minnesota, Minneapolis, MN, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: A systolic array implementation of a scheme with similar data movement patterns to this one (but not similar numerical properties) is implemented in <ref> [19] </ref>. <p> Our numerical tests show some improvement in the accuracy of downdated singular values using our algorithm instead of general chasing. Thus we see potential use of these algorithms in subspace tracking because of the possibility of systolic array implementation <ref> [1, 19] </ref>. Acknowledgement. Dr. Zhenyue Zhang made his helpful suggestions that improved this paper.
Reference: [20] <author> S. Van Huffel and J. Vandewalle. </author> <title> The Total Least Squares Problem: Computational Aspects and Analysis. </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: We assume that the (k 1) st superdiagonal of B 1 is zero. For (1.2) we presume oe k tol. The form (1.2) is the familiar singular value decomposition (SVD). The form (1.3) is a special case of the partial singular value decomposition (partial SVD) described by Van Huffel <ref> [20, 17] </ref>. The updating problem is that of obtaining the SVD (partial SVD) of A where A = A fl Department of Computer Science and Engineering, The Pennsylvania State University, University Park, PA 16802-6106. e-mail: barlow@cse.psu.edu, zha@cse.psu.edu. The research of Jesse L.
Reference: [21] <author> E. R. Jessup and D.C. Sorensen. </author> <title> A parallel algorithm for computing the singular value decomposition. </title> <type> Tech. Rep. </type> <institution> MCS-TM-102, Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, IL, </institution> <year> 1987. </year>
Reference-contexts: The alternative to our approach is that of finding the zeroes of a particular spectral function <ref> [13, 6, 21, 26, 3, 16, 15] </ref>. That approach, as yet, does not allow us to separate the singular values into separate blocks in the manner discussed in this paper. There are five sections after this introductory one.
Reference: [22] <author> T. Kato. </author> <title> A Short Introduction to Perturbation Theory for Linear Operators. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: ! oe j + O (* 2 2 oe n ( ^ M )relgap (oe i ; oe j ) + O (* 2 MODIFYING THE SVD AND PARTIAL SVD 19 where relgap (oe i ; oe j ) = oe i oe j Proof: Using the expansion from Kato <ref> [22] </ref>, we have that ^v j = v j + p6=i p [(z + ffiz)(z + ffiz) T zz T ]v j oe 2 p z ) Thus jv T jv T i [(z + ffiz)(z + ffiz) T zz T ]v j j oe 2 j z ): If we
Reference: [23] <author> C.-T. Pan. </author> <title> A modification to the LINPACK downdating algorithm. </title> <journal> BIT, </journal> <volume> 30 </volume> <pages> 707-722, </pages> <year> 1990. </year>
Reference-contexts: Our approaches to downdating the decompositions (1.2)-(1.3) use ideas from "chasing" algorithms [25, 1, 33, 18] and from the downdating algorithm due to Gill et al <ref> [12, 23] </ref>. A systolic array implementation of a scheme with similar data movement patterns to this one (but not similar numerical properties) is implemented in [19]. <p> Pan <ref> [23] </ref> show that for B upper triangular, this method can be sped up by combining the forward substitution phase with the application of the Givens rotations. He gives two recurrences which produce the same result.
Reference: [24] <author> H. Park and L. Elden. </author> <title> Downdating the rank-revealing URV decomposition. </title> <type> Technical Report LiTH-MAT-R-1992-47, </type> <institution> Department of Mathematics, Linkoping University, Linkoping, Sweden, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: In that case, define * Z =k 1 ffiZ k; ff = q and ^ B and ~ B are defined analogously. Thus this theory applies to the problem of doing several downdates at once. This problem is discussed but without considering the scaling in <ref> [24] </ref>. MODIFYING THE SVD AND PARTIAL SVD 21 4.2. Error Bounds for Blockwise Algorithms for the SVD and Partial SVD. We now give error bounds for the process of one update or downdate using the procedures in section three.
Reference: [25] <author> H. </author> <title> Rutishauser. On Jacobi rotation patterns. </title> <booktitle> In Experimental Arithmetic, </booktitle> <pages> pages 219-239, </pages> <address> Providence, RI, </address> <year> 1963. </year> <journal> Amer. Math. Soc. </journal>
Reference-contexts: Our approaches to downdating the decompositions (1.2)-(1.3) use ideas from "chasing" algorithms <ref> [25, 1, 33, 18] </ref> and from the downdating algorithm due to Gill et al [12, 23]. A systolic array implementation of a scheme with similar data movement patterns to this one (but not similar numerical properties) is implemented in [19].
Reference: [26] <author> D.C. Sorensen and P.T. Tang. </author> <title> On the orthogonality of eigenvectors computed by the divide-and-conquer techniques. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 28 </volume> <pages> 1752-1775, </pages> <year> 1991. </year>
Reference-contexts: The alternative to our approach is that of finding the zeroes of a particular spectral function <ref> [13, 6, 21, 26, 3, 16, 15] </ref>. That approach, as yet, does not allow us to separate the singular values into separate blocks in the manner discussed in this paper. There are five sections after this introductory one.
Reference: [27] <author> G.W. Stewart. </author> <title> The effects of rounding error on an algorithm for downdating a Cholesky factorization. </title> <journal> J. Inst. Maths. Applics., </journal> <volume> 23 </volume> <pages> 203-213, </pages> <year> 1979. </year>
Reference-contexts: Pan [23] show that for B upper triangular, this method can be sped up by combining the forward substitution phase with the application of the Givens rotations. He gives two recurrences which produce the same result. Stewart <ref> [27] </ref> presented an error analysis of the LINPACK procedure and pointed out that it has a mixed stability property. The algorithm is based on the following reasoning. <p> are able to perform a updating or downdating procedure which produces ^ B such that B T B (z + ffiz)(z + ffiz) T = ( ^ B + ffi ^ B) T ( ^ B + ffi ^ B):(4.23) This is the so-called mixed stability criterion described by Stewart <ref> [27] </ref>. Let be a scaling matrix. <p> This is just a classical backward error bound and will yield only the standard error bounds on the singular values [14]. A similar analysis works in downdating. From Stewart <ref> [27] </ref>, we have that pro duceRd obtains U T 0 = R + ffiR z T + ffiz T where k ffir i ffiz i MODIFYING THE SVD AND PARTIAL SVD 23 and f 4 (n) = 13n+5 p n (n + 2).
Reference: [28] <author> G.W. Stewart. </author> <title> The efficient generation of random orthogonal matrices with an application to condition estimators. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 17 </volume> <pages> 403-409, </pages> <year> 1980. </year>
Reference-contexts: For each test matrix, we generated a random diagonal matrix D with diagonal elements, in a geometric sequence, ranging from 1 to 10 6 . Then we generated orthogonal matrices U and V uniformly distributed with respect to Haar measure <ref> [28] </ref>, and formed U DV T . The SVD of the initial data matrix was then computed by DGESVD and the decomposition was truncated to single precision.
Reference: [29] <author> G.W. Stewart. </author> <title> An updating algorithm for subspace tracking. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 40 </volume> <pages> 1535-1541, </pages> <year> 1992. </year>
Reference-contexts: Thus we use a variant of a technique due to Stewart <ref> [29, 30] </ref>.
Reference: [30] <author> G.W. Stewart. </author> <title> Updating a rank-revealing ULV decomposition. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 14 </volume> <pages> 494-499, </pages> <year> 1993. </year>
Reference-contexts: Thus we use a variant of a technique due to Stewart <ref> [29, 30] </ref>.
Reference: [31] <author> K. Veselic' and I. Slapnicar. </author> <title> Floating-point perturbations of Hermitian matrices. </title> <type> Research report, </type> <institution> Fernuniversitat Hagen, Lehrgebiet Math. Physik, Hagen, Germany, </institution> <year> 1991. </year>
Reference-contexts: This is similar to results in <ref> [9, 4, 31] </ref>. Lemma 4.4. Assume the hypothesis and terminology of the previous lemma. Let v i be the i th singular vector of ~ B and let ^v i be the i th singular vector of ^ B. <p> The effect of * B is less critical. Let j be a constant such that k ffi ^ Bx k j k ^ Bx k (4.32) for all x 2 &lt; n . Then from <ref> [4, 9, 31] </ref> we have the following bound on the singular values. Lemma 4.6. Let ^ B and ^ B + ffi ^ B satisfiy the bound (4.32).
Reference: [32] <author> J.H. Wilkinson. </author> <title> The Algebraic Eigenvalue Problem. </title> <publisher> Oxford University Press, </publisher> <address> London, </address> <year> 1965. </year>
Reference-contexts: from Theorems 4.2 and 4.5 is = diag (k B k F I k ; cabs (OE k+1 ; fl k+1 ); k B 2 k F I nk1 ): For Algorithm 2.1, the best bound would be something like k E k f 3 (n) k B from Wilkinson <ref> [32] </ref> where f 3 (n) = O (n). This is just a classical backward error bound and will yield only the standard error bounds on the singular values [14]. A similar analysis works in downdating. <p> computed with error bounds jfl (x) xj = jfl (V T 1 wj (n + 1) k z k u + O (u 2 )(4.33) 2 w) V T 2 wj (n + 1) maxfk y k; oe k+1 gu + O (u 2 ):(4.34) The inequality (4.33) is standard <ref> [32] </ref>. For downdating, we always have k y k oe k+1 , but the inequality (4.34) cannot be expected in floating point arithmetic.
Reference: [33] <author> H. Zha. </author> <title> A two-way chasing scheme for reducing a symmetric arrowhead matrix to tridiagonal form. </title> <journal> J. Numerical Linear Algebra with Applications, </journal> <volume> 1 </volume> <pages> 49-57, </pages> <year> 1992. </year>
Reference-contexts: Our approaches to downdating the decompositions (1.2)-(1.3) use ideas from "chasing" algorithms <ref> [25, 1, 33, 18] </ref> and from the downdating algorithm due to Gill et al [12, 23]. A systolic array implementation of a scheme with similar data movement patterns to this one (but not similar numerical properties) is implemented in [19].
References-found: 33

