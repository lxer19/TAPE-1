URL: http://www.cs.colostate.edu/~pyeatt/FLAIRS-98.ps
Refering-URL: http://www.cs.colostate.edu/~pyeatt/
Root-URL: 
Email: email: fpyeatt,howeg@cs.colostate.edu  
Title: Learning to Race: Experiments with a Simulated Race Car  
Author: Larry D. Pyeatt Adele E. Howe 
Web: URL: http://www.cs.colostate.edu/~fpyeatt,howeg  
Address: Fort Collins, CO 80523  
Affiliation: Colorado State University  
Abstract: We have implemented a reinforcement learning architecture as the reactive component of a two layer control system for a simulated race car. We have found that separating the layers has expedited gradually improving competition and mult-agent interaction. We ran experiments to test the tuning, decomposition and coordination of the low level behaviors. We then extended our control system to allow passing of other cars and tested its ability to avoid collisions. The best design used reinforcement learning with separate networks for each behavior, coarse coded input and a simple rule based coordination mechanism. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, C. W. </author> <year> 1989. </year> <title> Strategy learning with multilayer connectionist representations. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <pages> 103-114. </pages>
Reference-contexts: The possible wheel commands are: adjust steering left by 0.1 radians, do not adjust steering, or adjust steerint right by 0.1 radians. Limiting the set to three possible actions is similar to the bang-bang control strategy used by Anderson <ref> (Anderson 1989) </ref> for controlling an inverted pendulum and has the advantage that it limits the number of possible actions and, therefore, limits the control functions which must be learned. At this stage, our strategy selection mechanism is quite simple.
Reference: <author> Arkin, R. C. </author> <year> 1990. </year> <title> Integrating behavioral, perceptual, and world knowledge in reactive navigation. </title> <booktitle> Robotics and Autonomous Systems 6 </booktitle> <pages> 105-122. </pages>
Reference-contexts: In Cypress (Wilkins et al. 1994), the planner is responsible for high level operation, leaving the details to an execution module. Control plans may direct the activation of situation-triggered behaviors (Hayes-Roth 1995). The Autonomous Robot Architecture used a deliberative planner to configure a modular reactive control system <ref> (Arkin 1990) </ref>. Because of the fast response and built-in tun-ing capabilities, neural network components have been used as both the controller and the behaviors. For example, a neural based robot controller showed better adaptability and noise rejection than a standard model based control algorithm (Poo et al. 1992).
Reference: <author> Arkin, R. C. </author> <year> 1994. </year> <title> Reactive robotic systems. </title> <note> available at http://www.cc.gatech.edu /ai/robot-lab/mrl-online-publications.html. </note>
Reference-contexts: For an agent to be effective in its environment, it must have a large repertoire of behaviors and must be able to coordinate the use of those behaviors effectively. Reactive systems have been favored for applications requiring quick responses, such as robotic or process control applications <ref> (Arkin 1994) </ref>. Unfortunately, it is difficult to tune, decompose and coordinate reactive behaviors while ensuring consistency. In this paper, we present a case study of designing a mostly reactive autonomous agent for learning to perform a task in a multi-agent environment.
Reference: <author> Barto, A.; Sutton, R.; and Watkins, C. </author> <year> 1989. </year> <title> Learning and sequential decision making. </title> <type> Technical report, COINS Technical Report 89-95, </type> <institution> Dept. of Computer and Information Science, University of Massachusetts. </institution>
Reference-contexts: However, some of the agents exhibit quite good performance. Low Level Behaviors In our architecture, each low level behavior can be implemented as either a heuristic control strategy or as a reinforcement learning network similar to those used by Barto, Sutton and Watkins <ref> (Barto, Sutton, & Watkins 1989) </ref>, Anderson (An-derson 1989) and Lin (Lin 1992). Behaviors are implemented as heuristic control strategies when the target behavior is simple to program or as a placeholder during testing or development of the reinforcement learning networks.
Reference: <author> Dorigo, M., and Colombetti, M. </author> <year> 1994. </year> <title> Robot shaping: developing autonomous agents through learning. </title> <booktitle> Artificial Intelligence 71(2) </booktitle> <pages> 321-370. </pages>
Reference-contexts: In addition, using a behavior based architecture speeds up reinforcement learning by converting the problem of learning a complex task into that of learning a simple set of reactive subtasks. Dorigo and Colom-betti <ref> (Dorigo & Colombetti 1994) </ref> used reinforcement learning to shape a robot to perform a predefined target behavior. They trained the agent to display five different behaviors and then defined four ways to combine these behaviors.
Reference: <author> Hayes-Roth, B. </author> <year> 1995. </year> <title> An architecture for adaptive intelligent systems. </title> <journal> Artificial Intelligence 72(1-2):329-365. </journal>
Reference: <author> Lin, L.-H. </author> <year> 1992. </year> <title> Self-improving reactive agents based on reinforcement learning, planning and teaching. </title> <booktitle> Machine Learning </booktitle> 8(3/4):69-97. 
Reference-contexts: Low Level Behaviors In our architecture, each low level behavior can be implemented as either a heuristic control strategy or as a reinforcement learning network similar to those used by Barto, Sutton and Watkins (Barto, Sutton, & Watkins 1989), Anderson (An-derson 1989) and Lin <ref> (Lin 1992) </ref>. Behaviors are implemented as heuristic control strategies when the target behavior is simple to program or as a placeholder during testing or development of the reinforcement learning networks. For the reinforcement learning networks, we used a popular reinforcement learning strategy known as Q-learning.
Reference: <author> Mahadevan, S., and Connell, J. </author> <year> 1992. </year> <title> Automatic programming of behaviour-based robots using reinforcement learning. </title> <booktitle> Artificial Intelligence </booktitle> 55(2/3):311-365. 
Reference-contexts: For example, a neural based robot controller showed better adaptability and noise rejection than a standard model based control algorithm (Poo et al. 1992). Reinforcement learning of individual behaviors in a robot can sometimes outperform a hand coded solution <ref> (Mahadevan & Connell 1992) </ref>. In addition, using a behavior based architecture speeds up reinforcement learning by converting the problem of learning a complex task into that of learning a simple set of reactive subtasks.
Reference: <author> Poo, A.; Jr., M. A.; Teo, C.; and Li, Q. </author> <year> 1992. </year> <title> Performance of a neuro-model-based robot controller: adaptability and noise rejection. </title> <booktitle> Intelligent Systems Engineering 1(1) </booktitle> <pages> 50-62. </pages>
Reference-contexts: Because of the fast response and built-in tun-ing capabilities, neural network components have been used as both the controller and the behaviors. For example, a neural based robot controller showed better adaptability and noise rejection than a standard model based control algorithm <ref> (Poo et al. 1992) </ref>. Reinforcement learning of individual behaviors in a robot can sometimes outperform a hand coded solution (Mahadevan & Connell 1992).
Reference: <author> Sutton, R. S. </author> <year> 1988. </year> <title> Learning to predict by the methods of temporal differences. </title> <booktitle> Machine Learning 3 </booktitle> <pages> 9-44. </pages>
Reference-contexts: For the reinforcement learning networks, we used a popular reinforcement learning strategy known as Q-learning. Q-learning networks Our implementation of Q-learning uses the standard back-propagation learning algorithm with the addition of temporal difference methods <ref> (Sutton 1988) </ref> to learn Q (s; a), the value of performing action a in state s. In standard back-propagation, the error term for the output node is calculated by subtracting the actual output of the network from the desired output.
Reference: <author> Timin, M. E. </author> <year> 1995. </year> <title> Robot Auto Racing Simulator. </title> <note> available from http://www.ebc.ee /~mremm/rars/rars.htm. </note>
Reference-contexts: In this paper, we present a case study of designing a mostly reactive autonomous agent for learning to perform a task in a multi-agent environment. The agent controls a race car in the Robot Automobile Racing Simulator (RARS) system <ref> (Timin 1995) </ref>. To be successful, the agent must quickly respond to its environment and must have mastered several skills: steering, accelerating, and passing.
Reference: <author> Wilkins, D. E.; Myers, K. L.; Lowrance, J. D.; and Wesley, L. P. </author> <year> 1994. </year> <title> Planning and reacting in uncertain and dynamic environments. </title> <journal> Journal of Experimental and Theoretical AI 7. </journal>
Reference-contexts: Once the agent has learned its "survival" behavior, new behaviors can be added for interaction with other agents. Controlling Reactivity Reactivity relies on the synergistic composition of low level behaviors. Authority for composing behaviors may be resident in a deliberative system. In Cypress <ref> (Wilkins et al. 1994) </ref>, the planner is responsible for high level operation, leaving the details to an execution module. Control plans may direct the activation of situation-triggered behaviors (Hayes-Roth 1995). The Autonomous Robot Architecture used a deliberative planner to configure a modular reactive control system (Arkin 1990).
References-found: 12

