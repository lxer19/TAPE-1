URL: ftp://pascal.math.yale.edu/pub/wavelets/papers/ldb.ps.gz
Refering-URL: http://www.mathsoft.com/wavelets.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Phone: 2  
Title: Local discriminant bases  
Author: Naoki Saito ; and Ronald R. Coifman 
Address: Quarry Road, Ridgefield, CT 06877-4108  10 Hillhouse Avenue, New Haven, CT 06520  
Affiliation: 1 Schlumberger-Doll Research Old  Department of Mathematics Yale University  
Abstract: We describe an extension to the "best-basis" method to construct an orthonormal basis which maximizes a class separability for signal classification problems. This algorithm reduces the dimensionality of these problems by using basis functions which are well localized in time-frequency plane as feature extractors. We tested our method using two synthetic datasets: extracted features (expansion coefficients of input signals in these basis functions), supplied them to the conventional pattern classifiers, then computed the misclassification rates. These examples show the superiority of our method over the direct application of these classifiers on the input signals. As a further application, we also describe a method to extract signal component from data consisting of signal and textured background. keywords: wavelet packets, local trigonometric transforms, classification, feature extraction, dimen sionality reduction, linear discriminant analysis, classification and regression trees
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Basseville, </author> <title> "Distance measures for signal processing and pattern recognition", </title> <booktitle> Signal Processing, </booktitle> <volume> Vol. 8, no. 4, </volume> <pages> pp. 349-369, </pages> <year> 1989. </year> <month> 9 </month>
Reference-contexts: There are many choices for the discriminant measure (see e.g., <ref> [1] </ref>); all of them essentially measure "statistical distances" among classes. For simplicity, let us first consider the two-class case. <p> See <ref> [1, 8] </ref> for more examples. We note that this step can also be viewed as a restricted version of the projection pursuit algorithm [8]. Step 5 reduces the dimensionality of the problem from n to k without losing the discriminant information in terms of time-frequency energy distributions among classes.
Reference: [2] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, </author> <title> Classification and Regression Trees, </title> <publisher> Chapman and Hall, Inc., </publisher> <address> New York, </address> <year> 1993, </year> <note> previously published by Wadsworth and Brooks/Cole in 1984. </note>
Reference-contexts: As a classifier g, we adopt Linear Discriminant Analysis (LDA) of R. A. Fisher [6] (see also [7]) (in fact LDA itself does further feature extraction followed by a simple classification scheme) and Classification and Regression Trees (CART) <ref> [2] </ref> although other classifiers such as k-nearest neighbor (k-NN) [7], or artificial neural networks (ANN) [10] are all possible. The reader interested in comparisons of different classifiers is referred to the excellent review article of Ripley [10]. <p> Then the class label is assigned for each terminal node usually by majority vote of the samples belonging to that node. The pruning process to eliminate unimportant branches is usually applied after growing the initial tree to avoid the "overtraining." We refer the reader to <ref> [2] </ref> for the details of splitting, stopping, and pruning rules. <p> Finally the test signals were projected onto the LDB functions and fed into these classifiers; then the misclassification rates were computed. For each method, we also computed the misclassification rate on the training dataset. Example 1 Triangular waveform classification. This is an example for classification originally examined in <ref> [2] </ref>. The dimensionality of the signal was extended from 21 in [2] to 32 for the dyadic dimensionality requirement of the bases under consideration. <p> For each method, we also computed the misclassification rate on the training dataset. Example 1 Triangular waveform classification. This is an example for classification originally examined in <ref> [2] </ref>. The dimensionality of the signal was extended from 21 in [2] to 32 for the dyadic dimensionality requirement of the bases under consideration. <p> We would like to note that according to Breiman et al. <ref> [2] </ref>, the Bayes error of this example is about 14 %. Example 2 Signal shape classification. The second example is a signal shape classification problem. In this example, we try to classify synthetic noisy signals with various shapes, amplitudes, lengths, and positions into three possible classes.
Reference: [3] <author> R. R. Coifman and N. Saito, </author> <title> "Constructions of local orthonormal bases for classification and regression", </title> <journal> Comptes Rendus Acad. Sci. Paris, Serie I, </journal> <volume> Vol. 319, </volume> <year> 1994, </year> <note> to appear. </note>
Reference-contexts: This is immediately followed by examples in Section 3. Then we discuss a method of signal/"background" separation as a further application of such a basis in Section 4 and finally conclude in Section 5. We note that the concise version of this paper was announced earlier in <ref> [3] </ref> which also contains an algorithm for constructing a local basis for regression problems.
Reference: [4] <author> R. R. Coifman and M. V. Wickerhauser, </author> <title> "Entropy-based algorithms for best basis selection", </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol. 38, no. 2, </volume> <pages> pp. 713-719, </pages> <year> 1992. </year>
Reference-contexts: This improves both accuracy and speed of these classifiers. In 2 this paper, we address how to construct good feature extractors. In particular, we use the "best-basis" paradigm <ref> [4] </ref> which permits a rapid [e.g., O (n log n)] search among a large collection of orthogonal bases for the problem at hand; we select basis functions which are well localized in the time-frequency (or space-wave number) plane and most discriminate given classes, and then the coordinates (expansion coefficients) of these <p> be found in [12]. 2 CONSTRUCTION OF LOCAL DISCRIMINANT BASIS In this section, we describe a fast algorithm to construct an adaptive orthonormal basis which is localized in the time-frequency plane and which discriminates given signal classes. 2.1 Review of the best-basis algorithm The best-basis algorithm of Coifman and Wickerhauser <ref> [4] </ref> was developed mainly for signal compression. <p> Now let A j;k be the best basis for the signal x restricted to the span of B j;k and let I be an information cost functional measuring the goodness of nodes (subspaces) for compression. Then, the best-basis algorithm works as: Algorithm 1 (The Best-Basis Algorithm <ref> [4] </ref>) Given a vector x, 3 Step 0: Choose a time-frequency decomposition method [i.e., specify wavelet packet transform (i.e., a pair of quadrature mirror filters), local cosine transform, or local sine transform].
Reference: [5] <author> I. Daubechies, </author> <title> Ten Lectures on Wavelets, </title> <booktitle> CBMS-NSF Regional Conference Series in Applied Mathematics, </booktitle> <volume> Vol. 61, </volume> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: Figure 1 shows five sample waveforms from each class. The LDB was computed from the wavelet packet coefficients with the 6-tap coiflet filter <ref> [5] </ref>. Then the five most discriminant coordinates were selected. In Figure 2, we compare the top five vectors from LDA and LDB. Only top two vectors were useful in LDA in this case. <p> The 12-tap coiflet filter <ref> [5] </ref> was used for the LDB selection. Then the 10 most important coordinates were selected. In Figure 4, we compare the top 10 LDA and LDB vectors. Again, only the top two vectors were used for classification in LDA case.
Reference: [6] <author> R. A. Fisher, </author> <title> "The use of multiple meaurements in taxonomic problems", </title> <journal> Ann. Eugenics, </journal> <volume> Vol. 7, </volume> <pages> pp. 179-188, </pages> <year> 1936. </year>
Reference-contexts: As a classifier g, we adopt Linear Discriminant Analysis (LDA) of R. A. Fisher <ref> [6] </ref> (see also [7]) (in fact LDA itself does further feature extraction followed by a simple classification scheme) and Classification and Regression Trees (CART) [2] although other classifiers such as k-nearest neighbor (k-NN) [7], or artificial neural networks (ANN) [10] are all possible.
Reference: [7] <author> K. Fukunaga, </author> <title> Introduction to Statistical Pattern Recognition, 2nd ed., </title> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1990. </year>
Reference-contexts: As a classifier g, we adopt Linear Discriminant Analysis (LDA) of R. A. Fisher [6] (see also <ref> [7] </ref>) (in fact LDA itself does further feature extraction followed by a simple classification scheme) and Classification and Regression Trees (CART) [2] although other classifiers such as k-nearest neighbor (k-NN) [7], or artificial neural networks (ANN) [10] are all possible. <p> As a classifier g, we adopt Linear Discriminant Analysis (LDA) of R. A. Fisher [6] (see also <ref> [7] </ref>) (in fact LDA itself does further feature extraction followed by a simple classification scheme) and Classification and Regression Trees (CART) [2] although other classifiers such as k-nearest neighbor (k-NN) [7], or artificial neural networks (ANN) [10] are all possible. The reader interested in comparisons of different classifiers is referred to the excellent review article of Ripley [10].
Reference: [8] <author> P. J. Huber, </author> <title> "Projection pursuit (with discussions)", </title> <journal> Ann. Statist., </journal> <volume> Vol. 13, no. 2, </volume> <pages> pp. 435-525, </pages> <year> 1985. </year>
Reference-contexts: See <ref> [1, 8] </ref> for more examples. We note that this step can also be viewed as a restricted version of the projection pursuit algorithm [8]. Step 5 reduces the dimensionality of the problem from n to k without losing the discriminant information in terms of time-frequency energy distributions among classes. <p> See [1, 8] for more examples. We note that this step can also be viewed as a restricted version of the projection pursuit algorithm <ref> [8] </ref>. Step 5 reduces the dimensionality of the problem from n to k without losing the discriminant information in terms of time-frequency energy distributions among classes. Thus many interesting statistical techniques which are usually computationally too expensive for n dimensional problems become feasible.
Reference: [9] <author> S. Kullback and R. A. Leibler, </author> <title> "On information and sufficiency", </title> <journal> Ann. Math. Statist., </journal> <volume> Vol. 22, </volume> <pages> pp. 79-86, </pages> <year> 1951. </year>
Reference-contexts: The discriminant information functional D (p; q) between these two sequences should measure how differently p and q are distributed. One natural choice for D is the so-called relative entropy (also known as cross entropy, Kullback-Leibler distance, or I-divergence) <ref> [9] </ref>: I (p; q) = i=1 p i ;(3) with the convention, log 0 = 1, log (x=0) = +1 for x 0, 0 (1) = 0. It is clear that I (p; q) 0 and equality holds iff p q. <p> The relative entropy (3) is asymmetric in p and q. For certain applications the asymmetry is preferred (see e.g., Section 4). If, however, a symmetric quantity is preferred, one should use the J divergence between p and q <ref> [9] </ref>: J (p; q) = I (p; q) + I (q; p):(4) Another possibility of the measure D is a ` 2 analogue of I (p; q) [14]: W (p; q) = kp qk 2 = i=1 Clearly, ` p (p 1) versions of this measure are all possible.
Reference: [10] <author> B. D. Ripley, </author> <title> "Statistical aspects of neural networks", Networks and Chaos: Statistical and Probabilistic Aspects, </title> <editor> O. E. Barndorff-Nielsen, J. L. Jensen, D. R. Cox, and W. S. Kendall, </editor> <booktitle> eds., </booktitle> <pages> pp. 40-123, </pages> <publisher> Chapman and Hall, Inc., </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: A. Fisher [6] (see also [7]) (in fact LDA itself does further feature extraction followed by a simple classification scheme) and Classification and Regression Trees (CART) [2] although other classifiers such as k-nearest neighbor (k-NN) [7], or artificial neural networks (ANN) <ref> [10] </ref> are all possible. The reader interested in comparisons of different classifiers is referred to the excellent review article of Ripley [10]. <p> feature extraction followed by a simple classification scheme) and Classification and Regression Trees (CART) [2] although other classifiers such as k-nearest neighbor (k-NN) [7], or artificial neural networks (ANN) <ref> [10] </ref> are all possible. The reader interested in comparisons of different classifiers is referred to the excellent review article of Ripley [10]. <p> From these examples, we can see that it is more important to select the good features than to select the best possible classifier without supplying the good features; each classifier has its advantages and 8 disadvantages <ref> [10] </ref>, i.e., the best classifier heavily depends on the problem (e.g., LDA was better than CART in Example 1 whereas the situation was opposite in Example 2.) By supplying a handful of good features, we can greatly enhance the performance of classifiers. 4 SIGNAL/BACKGROUND SEPARATION BY LDB LDB vectors can also
Reference: [11] <author> J. Rissanen, </author> <title> Stochastic Complexity in Statistical Inquiry, </title> <publisher> World Scientific, </publisher> <address> Singapore, </address> <year> 1989. </year>
Reference-contexts: Thus many interesting statistical techniques which are usually computationally too expensive for n dimensional problems become feasible. How to select the best k is a tough interesting question. One possibility is to use model selection methods such as the minimum description length (MDL) criterion <ref> [11, 13] </ref>. 3 EXAMPLES To demonstrate the capability of the local discriminant basis, we conducted two classification experiments using synthetic signals. In both cases, we specified three classes of signals by analytic formulas. For each class, we generated 100 training signals and 1000 test signals.
Reference: [12] <author> N. Saito, </author> <title> Local Feature Extraction and Its Applications Using a Library of Bases, </title> <type> Ph.D. thesis, </type> <institution> Dept. of Mathematics, Yale University, </institution> <address> New Haven, CT 06520 USA, </address> <year> 1994, </year> <title> in preparation. [13] , "Simultaneous noise suppression and signal compression using a library of orthonormal bases and the minimum description length criterion", Wavelets in Geophysics, </title> <editor> E. Foufoula-Georgiou and P. Kumar, </editor> <booktitle> eds., </booktitle> <pages> pp. 299-324, </pages> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1994. </year>
Reference-contexts: The details of all these algorithms, their applications to regression problems, and examples using both synthetic and real datasets can be found in <ref> [12] </ref>. 2 CONSTRUCTION OF LOCAL DISCRIMINANT BASIS In this section, we describe a fast algorithm to construct an adaptive orthonormal basis which is localized in the time-frequency plane and which discriminates given signal classes. 2.1 Review of the best-basis algorithm The best-basis algorithm of Coifman and Wickerhauser [4] was developed mainly
Reference: [14] <author> S. Watanabe, </author> <title> Pattern Recognition: Human and Mechanical, </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1985. </year> <title> 10 The nodes selected as LDB. 11 The nodes selected as LDB. 12 in (a). (c) Top 20 LDB vectors using the local sine library on the frequency domain. (d) Reconstructed spikes after removing the "background". </title> <type> 13 </type>
Reference-contexts: If, however, a symmetric quantity is preferred, one should use the J divergence between p and q [9]: J (p; q) = I (p; q) + I (q; p):(4) Another possibility of the measure D is a ` 2 analogue of I (p; q) <ref> [14] </ref>: W (p; q) = kp qk 2 = i=1 Clearly, ` p (p 1) versions of this measure are all possible.
References-found: 13

