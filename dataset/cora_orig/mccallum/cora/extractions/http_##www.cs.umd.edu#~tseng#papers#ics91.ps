URL: http://www.cs.umd.edu/~tseng/papers/ics91.ps
Refering-URL: http://www.cs.umd.edu/~tseng/papers.html
Root-URL: 
Title: Analysis and Transformation in the ParaScope Editor  
Author: Ken Kennedy Kathryn S. McKinley Chau-Wen Tseng 
Note: Center for Research on Parallel Computation  In Proceedings of the 1991 International Conference, on Supercomputing, Cologne, Germany, June 1991.  
Date: December 1990  
Address: 90106  P.O. Box 1892 Houston, TX 77251-1892  
Affiliation: CRPC-TR  Rice University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> F. Allen, M. Burke, P. Charles, R. Cytron, and J. Ferrante. </author> <title> An overview of the PTRAN analysis system for multiprocessing. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Athens, Greece, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: Ptran is also an automatic parallelizer with extensive program analysis. It computes the SSA and program dependence graphs, and performs constant propagation and interprocedural analysis [22]. Ptran introduces both task and loop parallelism, but currently the only other program transformations are variable priva-tization and loop distribution <ref> [1] </ref>. Sigmacs, a programmable interactive parallelizer in the Faust programming environment, computes and displays call graphs, process graphs, and a statement dependence graph [26, 48]. In a process graph each node represents a task or a process, which is a separate entity running in parallel.
Reference: [2] <author> F. Allen and J. Cocke. </author> <title> A catalogue of optimizing transformations. </title> <editor> In J. Rustin, editor, </editor> <booktitle> Design and Optimization of Compilers. </booktitle> <publisher> Prentice-Hall, </publisher> <year> 1972. </year>
Reference-contexts: Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for enhancing the use of the memory hierarchy. These transformations are described in detail in the literature <ref> [2, 4, 12, 32, 33, 36, 41, 56] </ref>. We classify the transformations in Ped as follows. <p> The update algorithm is explained more thoroughly elsewhere [32]. 7.4 Unroll and Jam Unroll and jam is a transformation that unrolls an outer loop in a loop nest, then jams (or fuses) the resulting inner loops <ref> [2, 13] </ref>. Unroll and jam can be used to convert dependences carried by the outer loop into loop independent dependences or dependences carried by some inner loop.
Reference: [3] <author> J. R. Allen. </author> <title> Dependence Analysis for Subscripted Variables and Its Application to Program Transformations. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> April </month> <year> 1983. </year>
Reference-contexts: a non-null path p, from x to y, such that y postdominates every node between x and y on p, and 2. y does not postdominate x. 1 Input dependences do not restrict statement order. 2.3 Loop-Carried and Loop-Independent Dependence Dependences are also characterized as either being loop-carried or loop-independent <ref> [3, 4] </ref>. Consider the following loop: DO i = 2, n S 2 : : : = A (i) ENDDO The true dependence S 1 ffiS 2 is loop-independent because it exists regardless of the surrounding loops. <p> Loop-carried dependences are important because they inhibit loops from executing in parallel without synchronization. When there are nested loops, the level of any carried dependence is the outermost loop on which it first arises <ref> [3, 4] </ref>. 2.4 Dependence Testing Determining the existence of data dependence between array references is more difficult than for scalars, because the subscript expressions must be considered. The process of differentiating between two subscripted references in a loop nest is called dependence testing.
Reference: [4] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: Hence, the results may differ each time the program is executed. This kind of anomaly, usually called a data race, precludes the parallelization of the above loop. In the literature of compilation for parallel execution, a potential data race is referred to as a loop-carried dependence <ref> [4, 35] </ref>. Without explicit synchronization, only loops with no carried dependences may be safely run in parallel. Automatic parallelizers use this principle by constructing a dependence graph for the entire program and then parallelizing every loop that does not carry a dependence. <p> a non-null path p, from x to y, such that y postdominates every node between x and y on p, and 2. y does not postdominate x. 1 Input dependences do not restrict statement order. 2.3 Loop-Carried and Loop-Independent Dependence Dependences are also characterized as either being loop-carried or loop-independent <ref> [3, 4] </ref>. Consider the following loop: DO i = 2, n S 2 : : : = A (i) ENDDO The true dependence S 1 ffiS 2 is loop-independent because it exists regardless of the surrounding loops. <p> Loop-carried dependences are important because they inhibit loops from executing in parallel without synchronization. When there are nested loops, the level of any carried dependence is the outermost loop on which it first arises <ref> [3, 4] </ref>. 2.4 Dependence Testing Determining the existence of data dependence between array references is more difficult than for scalars, because the subscript expressions must be considered. The process of differentiating between two subscripted references in a loop nest is called dependence testing. <p> Direction vectors, introduced by Wolfe [55], are useful for calculating loop-carried dependences. A dependence is carried by the outermost loop for which the element in the direction vector is not an `='. Additionally, direction vectors are used to determine the safety and profitability of loop interchange <ref> [4, 55] </ref>. Distance vectors, first used by Kuck and Muraoka [38, 43], are more precise versions of direction vectors that specify the actual number of loop iterations between two accesses to the same memory location. <p> The current implementation of Ped can determine if event style synchronization is sufficient to protect a particular dependence. 5.8 Utilizing External Analysis To overcome gaps in the current implementation of dependence analysis, Ped may import dependence information from Pfc, the Rice system for automatic vec-torization and parallelization <ref> [4] </ref>. Pfc's dependence analyzer is more mature and contains symbolic analysis, interprocedural regular sections and constants, as well as control and data dependence analysis. Pfc produces a file of dependence information that Ped converts into its own internal representation. <p> Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for enhancing the use of the memory hierarchy. These transformations are described in detail in the literature <ref> [2, 4, 12, 32, 33, 36, 41, 56] </ref>. We classify the transformations in Ped as follows. <p> The purpose, mechanics, and safety of these transformations are presented, followed by their profitability estimates, user advice, and incremental dependence update algorithms. 7.1 Loop Interchange Loop interchange is a key transformation that modifies the traversal order of the iteration space for the selected loop nest <ref> [4, 55] </ref>. It has been used extensively in vector-izing and parallelizing compilers to adjust the granularity of parallel loops and to expose parallelism [4, 36, 56]. Ped interchanges pairs of adjacent loops. Loop permutations may be performed as a series of pairwise interchanges. <p> It has been used extensively in vector-izing and parallelizing compilers to adjust the granularity of parallel loops and to expose parallelism <ref> [4, 36, 56] </ref>. Ped interchanges pairs of adjacent loops. Loop permutations may be performed as a series of pairwise interchanges. Ped supports interchange of triangular or skewed loops. It also interchanges hexagonal loops that result after skewed loops are interchanged. <p> the original distance vectors (d 1 ; d 2 ) for all dependences in the nest to (d 1 ; ffd 1 + d 2 ), and then updates their interchange flags. 7.3 Loop Distribution Loop distribution separates independent statements inside a single loop into multiple loops with identical headers <ref> [4, 37] </ref>. It is used to expose partial parallelism by separating statements which may be paral-lelized from those that must be executed sequentially. It is a cornerstone of vectorization and parallelization. In Ped the user can specify whether distribution is for the purpose of vectorization or parallelization. <p> Our work on interactive paralleliza-tion bears similarities to Sigmacs, Pat, and Superb. Ped has been greatly influenced by the Rice Parallel Fortran Converter (Pfc), which has focused on the problem of automatically vectorizing and paralleliz-ing sequential Fortran <ref> [4] </ref>. Pfc has a mature dependence analyzer which performs data dependence analysis, control dependence analysis, interprocedural constant propagation [15], interprocedural side effect analysis of scalars [20], and interprocedural array section analysis [16, 28].
Reference: [5] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> An interactive environment for data partitioning and distribution. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: The analysis and representation of dependence in the ParaScope Editor have also proven very useful in the development of several other advanced tools, including a compiler [29] and data decomposition tools <ref> [5, 6] </ref> for distributed-memory machines, and an on-the-fly access anomaly detection system for shared-memory machines [30]. 11 Acknowledgments We would like to thank Vasanth Balasundaram, Pre-ston Briggs, Keith Cooper, Paul Havlak, Marina Kalem, Rhonda Reese, Jaspal Subhlok, and Linda Torczon for their many contributions to this work.
Reference: [6] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> A static performance estimator to guide data partitioning decisions. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: The analysis and representation of dependence in the ParaScope Editor have also proven very useful in the development of several other advanced tools, including a compiler [29] and data decomposition tools <ref> [5, 6] </ref> for distributed-memory machines, and an on-the-fly access anomaly detection system for shared-memory machines [30]. 11 Acknowledgments We would like to thank Vasanth Balasundaram, Pre-ston Briggs, Keith Cooper, Paul Havlak, Marina Kalem, Rhonda Reese, Jaspal Subhlok, and Linda Torczon for their many contributions to this work.
Reference: [7] <author> V. Balasundaram and K. Kennedy. </author> <title> A technique for summarizing data access and its use in parallelism enhancing transformations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: To provide more precise analysis, array accesses can be summarized in terms of regular sections or data access descriptors that describe subsections of arrays such as rows, columns, and rectangles <ref> [7, 16, 28] </ref>. Local symbolic analysis and interprocedural constants are required to build accurate regular sections. Once constructed, regular sections may be quickly intersected during interprocedural analysis and dependence testing to determine whether dependences exist.
Reference: [8] <author> V. Balasundaram, K. Kennedy, U. Kremer, K. S. M c Kinley, and J. Subhlok. </author> <title> The ParaScope Editor: An interactive parallel programming tool. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <address> Reno, NV, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: Clearly a tool with this much functionality is bound to be complex. Ped incorporates a complete source editor and supports dependence analysis, dependence display, and a large variety of program transformations to enhance parallelism. Previous work has described the usage and motivation of the ParaScope Editor <ref> [8, 23, 33] </ref> and the ParaScope parallel programming environment [14]. In this paper, we focus on the implementation of Ped's analysis and transformation features. <p> Each element of the vector can represent a dependence distance or direction. Dependence edges are organized for the user interface using a higher level data abstraction, called the edge list. The edge list provides the user a configurable method of filtering, sorting, and selecting dependences <ref> [8, 33] </ref>. 4.2.2 Level Vectors Dependence edges hold most of the dependence information for a program, but level vectors provide the glue which links them together and to the AST. Every executable statement in a loop nest involved with a dependence has a level vector.
Reference: [9] <author> U. Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Proceedings of the Third Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Distance vectors, first used by Kuck and Muraoka [38, 43], are more precise versions of direction vectors that specify the actual number of loop iterations between two accesses to the same memory location. They are utilized by transformations to exploit parallelism <ref> [9, 39, 54, 56] </ref> and the memory hierarchy [12, 24]. 3 Work Model Ped is designed to exploit loop-level parallelism, which comprises most of the usable parallelism in scientific codes when synchronization costs are considered [18]. <p> It can be applied in conjunction with loop interchange, strip mining, and loop reversal to obtain effective loop-level parallelism in a loop nest <ref> [9, 54, 57] </ref>. All of these transformations are supported in Ped. Loop skewing is applied to a pair of perfectly nested loops that both carry dependences, even after loop interchange.
Reference: [10] <author> M. Burke. </author> <title> An interval-based approach to exhaustive and incremental interprocedural data-flow analysis. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(3) </volume> <pages> 341-395, </pages> <year> 1990. </year>
Reference-contexts: Several algorithms for incremental analysis can be found in the literature; e.g., dataflow analysis [47, 58], interprocedural analysis <ref> [10, 46] </ref>, interprocedural recompilation analysis [11], as well as dependence analysis [45]. However, few of these algorithms have been implemented and evaluated in an interactive environment. Rather than tackle all these problems at once, we chose a simple yet practical strategy for the current implementation of Ped.
Reference: [11] <author> M. Burke, K. Cooper, K. Kennedy, and L. Torczon. </author> <title> Inter-procedural optimization: Eliminating unnecessary recompilation. </title> <type> Technical Report TR90-126, </type> <institution> Dept. of Computer Science, Rice University, </institution> <year> 1990. </year>
Reference-contexts: Several algorithms for incremental analysis can be found in the literature; e.g., dataflow analysis [47, 58], interprocedural analysis [10, 46], interprocedural recompilation analysis <ref> [11] </ref>, as well as dependence analysis [45]. However, few of these algorithms have been implemented and evaluated in an interactive environment. Rather than tackle all these problems at once, we chose a simple yet practical strategy for the current implementation of Ped.
Reference: [12] <author> D. Callahan, S. Carr, and K. Kennedy. </author> <title> Improving register allocation for subscripted variables. </title> <booktitle> In Proceedings of the ACM SIGPLAN '90 Conference on Program Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Distance vectors, first used by Kuck and Muraoka [38, 43], are more precise versions of direction vectors that specify the actual number of loop iterations between two accesses to the same memory location. They are utilized by transformations to exploit parallelism [9, 39, 54, 56] and the memory hierarchy <ref> [12, 24] </ref>. 3 Work Model Ped is designed to exploit loop-level parallelism, which comprises most of the usable parallelism in scientific codes when synchronization costs are considered [18]. In the work model best supported by Ped, the user first selects a loop for parallelization. <p> Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for enhancing the use of the memory hierarchy. These transformations are described in detail in the literature <ref> [2, 4, 12, 32, 33, 36, 41, 56] </ref>. We classify the transformations in Ped as follows. <p> It brings two accesses to the same memory location closer together and can significantly improve performance by enabling reuse of either registers or cache. When applied in conjunction with scalar replacement on scientific codes, unroll and jam has resulted in integer factor speedups, even for single processors <ref> [12] </ref>. Unroll and jam may also be applied to imperfectly nested loops or loops with complex iteration spaces. Figure 5 shows an example iteration space before and after unroll and jam of degree 1. <p> Unroll and jam is profitable if it brings the balance of a loop closer to the balance of the underlying machine. Ped automatically calculates the optimal unroll and jam degree for a loop nest, including loops with complex iteration spaces <ref> [12] </ref>. Update An algorithm for the incremental update of the dependence graph after unroll and jam is described elsewhere [12]. However, we chose a different strategy. <p> Ped automatically calculates the optimal unroll and jam degree for a loop nest, including loops with complex iteration spaces <ref> [12] </ref>. Update An algorithm for the incremental update of the dependence graph after unroll and jam is described elsewhere [12]. However, we chose a different strategy. Since no global dataflow or symbolic information is changed by unroll and jam, Ped rebuilds the scalar dependence graph for the loop nest and refines it with dependence tests.
Reference: [13] <author> D. Callahan, J. Cocke, and K. Kennedy. </author> <title> Estimating interlock and improving balance for pipelined machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(4) </volume> <pages> 334-358, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: The update algorithm is explained more thoroughly elsewhere [32]. 7.4 Unroll and Jam Unroll and jam is a transformation that unrolls an outer loop in a loop nest, then jams (or fuses) the resulting inner loops <ref> [2, 13] </ref>. Unroll and jam can be used to convert dependences carried by the outer loop into loop independent dependences or dependences carried by some inner loop. <p> If any of these dependences cross between the imperfectly nested statements and the statements in the inner loop, they inhibit unroll and jam. Specifically, the intervening statements cannot be moved and prevent fusion of the inner loops. Profitability Balance describes the ratio between computation and memory access rates <ref> [13] </ref>. Unroll and jam is profitable if it brings the balance of a loop closer to the balance of the underlying machine. Ped automatically calculates the optimal unroll and jam degree for a loop nest, including loops with complex iteration spaces [12].
Reference: [14] <author> D. Callahan, K. Cooper, R. Hood, K. Kennedy, and L. Tor-czon. </author> <title> ParaScope: A parallel programming environment. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 2(4) </volume> <pages> 84-99, </pages> <month> Winter </month> <year> 1988. </year>
Reference-contexts: Ped incorporates a complete source editor and supports dependence analysis, dependence display, and a large variety of program transformations to enhance parallelism. Previous work has described the usage and motivation of the ParaScope Editor [8, 23, 33] and the ParaScope parallel programming environment <ref> [14] </ref>. In this paper, we focus on the implementation of Ped's analysis and transformation features. Particular attention is paid to the representation of dependences, the construction of the dependence graph, and how dependences are used and incrementally reconstructed for Page 1 each program transformation in an efficient and flexible manner.
Reference: [15] <author> D. Callahan, K. Cooper, K. Kennedy, and L. Torczon. </author> <title> In-terprocedural constant propagation. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <address> Palo Alto, CA, </address> <month> July </month> <year> 1986. </year>
Reference-contexts: Interprocedural analysis is required so that worst case assumptions need not be made when calls are encountered. ParaScope performs conventional interprocedural analysis that discovers constants, aliasing, flow insensitive side effects such as ref and mod, and flow sensitive side effects such as use and kill <ref> [15, 20] </ref>. However, improvements are limited because arrays are treated as monolithic objects, making it impossible to determine whether two references to an array actually access the same memory location. <p> Ped has been greatly influenced by the Rice Parallel Fortran Converter (Pfc), which has focused on the problem of automatically vectorizing and paralleliz-ing sequential Fortran [4]. Pfc has a mature dependence analyzer which performs data dependence analysis, control dependence analysis, interprocedural constant propagation <ref> [15] </ref>, interprocedural side effect analysis of scalars [20], and interprocedural array section analysis [16, 28]. Ped expands on Pfc's analysis and transformation capabilities and makes them available to the user in an interactive environment. Parafrase was the first automatic vectorizing and par-allelizing compiler [36].
Reference: [16] <author> D. Callahan and K. Kennedy. </author> <title> Analysis of interprocedural side effects in a parallel programming environment. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Athens, Greece, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: To provide more precise analysis, array accesses can be summarized in terms of regular sections or data access descriptors that describe subsections of arrays such as rows, columns, and rectangles <ref> [7, 16, 28] </ref>. Local symbolic analysis and interprocedural constants are required to build accurate regular sections. Once constructed, regular sections may be quickly intersected during interprocedural analysis and dependence testing to determine whether dependences exist. <p> Pfc has a mature dependence analyzer which performs data dependence analysis, control dependence analysis, interprocedural constant propagation [15], interprocedural side effect analysis of scalars [20], and interprocedural array section analysis <ref> [16, 28] </ref>. Ped expands on Pfc's analysis and transformation capabilities and makes them available to the user in an interactive environment. Parafrase was the first automatic vectorizing and par-allelizing compiler [36]. It supports program analysis and performs a large number of program transformations to improve parallelism.
Reference: [17] <author> D. Callahan, K. Kennedy, and J. Subhlok. </author> <title> Analysis of event synchronization in a parallel programming tool. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle, WA, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: Establishing that the order specified by certain dependences will always be observed has been shown to be co-NP-hard, but techniques have been developed to identify dependences that are satisfied by existing synchronization under restricted circumstances <ref> [17, 52] </ref>. The current implementation of Ped can determine if event style synchronization is sufficient to protect a particular dependence. 5.8 Utilizing External Analysis To overcome gaps in the current implementation of dependence analysis, Ped may import dependence information from Pfc, the Rice system for automatic vec-torization and parallelization [4].
Reference: [18] <author> D. Chen, H. Su, and P. Yew. </author> <title> The impact of synchronization and granularity on parallel systems. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <address> Seattle, WA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: They are utilized by transformations to exploit parallelism [9, 39, 54, 56] and the memory hierarchy [12, 24]. 3 Work Model Ped is designed to exploit loop-level parallelism, which comprises most of the usable parallelism in scientific codes when synchronization costs are considered <ref> [18] </ref>. In the work model best supported by Ped, the user first selects a loop for parallelization. Ped then displays all of its carried dependences. The user may sort or filter the dependences to help discover and delete dependences that are due to overly conservative dependence analysis.
Reference: [19] <author> K. Cooper, M. Hall, and L. Torczon. </author> <title> An experiment with inline substitution. </title> <type> Technical Report TR90-128, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> July </month> <year> 1990. </year> <note> To appear in Software|Practice and Experience. </note>
Reference-contexts: Local symbolic analysis and interprocedural constants are required to build accurate regular sections. Once constructed, regular sections may be quickly intersected during interprocedural analysis and dependence testing to determine whether dependences exist. We are integrating existing ParaScope interprocedu-ral analysis and transformations such as inlining and cloning into Ped <ref> [19, 20] </ref>. The implementation of regular sections is also under way. 5.6 Dependence Testing The dependence testing phase refines the coarse dependence graph for array variables created by scalar analysis and sharpened by interprocedural analysis.
Reference: [20] <author> K. Cooper, K. Kennedy, and L. Torczon. </author> <title> The impact of interprocedural analysis and optimization in the IR n programming environment. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 491-523, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: Interprocedural analysis is required so that worst case assumptions need not be made when calls are encountered. ParaScope performs conventional interprocedural analysis that discovers constants, aliasing, flow insensitive side effects such as ref and mod, and flow sensitive side effects such as use and kill <ref> [15, 20] </ref>. However, improvements are limited because arrays are treated as monolithic objects, making it impossible to determine whether two references to an array actually access the same memory location. <p> Local symbolic analysis and interprocedural constants are required to build accurate regular sections. Once constructed, regular sections may be quickly intersected during interprocedural analysis and dependence testing to determine whether dependences exist. We are integrating existing ParaScope interprocedu-ral analysis and transformations such as inlining and cloning into Ped <ref> [19, 20] </ref>. The implementation of regular sections is also under way. 5.6 Dependence Testing The dependence testing phase refines the coarse dependence graph for array variables created by scalar analysis and sharpened by interprocedural analysis. <p> Pfc has a mature dependence analyzer which performs data dependence analysis, control dependence analysis, interprocedural constant propagation [15], interprocedural side effect analysis of scalars <ref> [20] </ref>, and interprocedural array section analysis [16, 28]. Ped expands on Pfc's analysis and transformation capabilities and makes them available to the user in an interactive environment. Parafrase was the first automatic vectorizing and par-allelizing compiler [36].
Reference: [21] <author> R. Cytron, J. Ferrante, B. Rosen, M. Wegman, and K. Zadeck. </author> <title> An efficient method of computing static single assignment form. </title> <booktitle> In Conference Record of the Sixteenth ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Austin, TX, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: It also provides a framework for the later phases of the dependence analyzer. Ped first constructs the control flow graph and postdominator tree. It then computes dominance frontiers for each scalar variable and uses them to build the static single assignment (SSA) graph for each procedure <ref> [21] </ref>. Edges in the SSA graph correspond to precise true dependences for scalar variables. Next, Ped constructs a coarse dependence graph for array variables in each loop nest by connecting fDefsg with fDefs [ Usesg. These edges are later refined through dependence testing to construct dependence Page 6 edges.
Reference: [22] <author> J. Ferrante, K. Ottenstein, and J. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: The following formal definitions of control dependence and the postdominance relation are taken from the literature <ref> [22] </ref>. Def: x is postdominated by y in G f , the control flow graph, if every path from x to stop contains y, where stop is the exit node of G f . <p> More advanced interprocedural and symbolic analysis is planned [27]. Parafrase-2 uses Faust as a front end to provide interactive paralleliza-tion and graphical displays [26]. Ptran is also an automatic parallelizer with extensive program analysis. It computes the SSA and program dependence graphs, and performs constant propagation and interprocedural analysis <ref> [22] </ref>. Ptran introduces both task and loop parallelism, but currently the only other program transformations are variable priva-tization and loop distribution [1]. Sigmacs, a programmable interactive parallelizer in the Faust programming environment, computes and displays call graphs, process graphs, and a statement dependence graph [26, 48].
Reference: [23] <author> K. Fletcher, K. Kennedy, K. S. M c Kinley, and S. Warren. </author> <title> The ParaScope Editor: User interface goals. </title> <type> Technical Report TR90-113, </type> <institution> Dept. of Computer Science, Rice University, </institution> <year> 1990. </year>
Reference-contexts: Clearly a tool with this much functionality is bound to be complex. Ped incorporates a complete source editor and supports dependence analysis, dependence display, and a large variety of program transformations to enhance parallelism. Previous work has described the usage and motivation of the ParaScope Editor <ref> [8, 23, 33] </ref> and the ParaScope parallel programming environment [14]. In this paper, we focus on the implementation of Ped's analysis and transformation features.
Reference: [24] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformations. </title> <booktitle> In Proceedings of the First International Confer Page 14 ence on Supercomputing. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Athens, Greece, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: Distance vectors, first used by Kuck and Muraoka [38, 43], are more precise versions of direction vectors that specify the actual number of loop iterations between two accesses to the same memory location. They are utilized by transformations to exploit parallelism [9, 39, 54, 56] and the memory hierarchy <ref> [12, 24] </ref>. 3 Work Model Ped is designed to exploit loop-level parallelism, which comprises most of the usable parallelism in scientific codes when synchronization costs are considered [18]. In the work model best supported by Ped, the user first selects a loop for parallelization.
Reference: [25] <author> G. Goff, K. Kennedy, and C. Tseng. </author> <title> Practical dependence testing. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Data flow along SSA edges is analyzed to deduce constraints and relationships on symbolic expressions, which may be in turn propagated. Studies have shown that symbolic analysis is essential for precise analysis of scientific programs <ref> [25, 27, 49] </ref>. The symbolic analyzer performs the following steps. Constant propagation uses the sparse conditional constant algorithm to eliminate as many symbolics as initially feasible [53]. Auxiliary induction variable substitution replaces auxiliary induction variables by functions of loop index variables. <p> Dependence edges are eliminated if dependence between the references can be disproved. Otherwise, dependence testing characterizes the dependences with a minimal set of hybrid distance/direction vectors. This dependence information is vital for guiding transformations. Ped's dependence tests are discussed in detail elsewhere <ref> [25] </ref>. Most dependence tests have been implemented in the current version of Ped; we are in the process of extending them to handle symbolic expressions, complex iteration spaces, and regular sections. Page 7 5.7 Analysis of Synchronization In a sophisticated parallel program, the user may wish to employ complex synchronization.
Reference: [26] <author> V. Guarna, D. Gannon, Y. Gaur, and D. Jablonowski. </author> <title> Faust: An environment for programming parallel scientific applications. </title> <booktitle> In Proceedings of Supercomputing '88, </booktitle> <address> Orlando, FL, </address> <month> November </month> <year> 1988. </year>
Reference-contexts: Parafrase-2 adds scheduling and improved program analysis and transformations [44]. More advanced interprocedural and symbolic analysis is planned [27]. Parafrase-2 uses Faust as a front end to provide interactive paralleliza-tion and graphical displays <ref> [26] </ref>. Ptran is also an automatic parallelizer with extensive program analysis. It computes the SSA and program dependence graphs, and performs constant propagation and interprocedural analysis [22]. Ptran introduces both task and loop parallelism, but currently the only other program transformations are variable priva-tization and loop distribution [1]. <p> Ptran introduces both task and loop parallelism, but currently the only other program transformations are variable priva-tization and loop distribution [1]. Sigmacs, a programmable interactive parallelizer in the Faust programming environment, computes and displays call graphs, process graphs, and a statement dependence graph <ref> [26, 48] </ref>. In a process graph each node represents a task or a process, which is a separate entity running in parallel. The call and process graphs may be animated dynamically at run time. Sigmacs also performs several interactive program transformations, and is working on automatic updating of dependence information.
Reference: [27] <author> M. Haghighat and C. Polychronopoulos. </author> <title> Symbolic dependence analysis for high performance parallelizing compilers. </title> <booktitle> In Proceedings of the Third Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Data flow along SSA edges is analyzed to deduce constraints and relationships on symbolic expressions, which may be in turn propagated. Studies have shown that symbolic analysis is essential for precise analysis of scientific programs <ref> [25, 27, 49] </ref>. The symbolic analyzer performs the following steps. Constant propagation uses the sparse conditional constant algorithm to eliminate as many symbolics as initially feasible [53]. Auxiliary induction variable substitution replaces auxiliary induction variables by functions of loop index variables. <p> In Parafrase, program transformations are structured in phases and are always applied where applicable. Batch analysis is performed after each transformation phase to update the dependence information for the entire program. Parafrase-2 adds scheduling and improved program analysis and transformations [44]. More advanced interprocedural and symbolic analysis is planned <ref> [27] </ref>. Parafrase-2 uses Faust as a front end to provide interactive paralleliza-tion and graphical displays [26]. Ptran is also an automatic parallelizer with extensive program analysis. It computes the SSA and program dependence graphs, and performs constant propagation and interprocedural analysis [22].
Reference: [28] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interproce-dural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3), </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: To provide more precise analysis, array accesses can be summarized in terms of regular sections or data access descriptors that describe subsections of arrays such as rows, columns, and rectangles <ref> [7, 16, 28] </ref>. Local symbolic analysis and interprocedural constants are required to build accurate regular sections. Once constructed, regular sections may be quickly intersected during interprocedural analysis and dependence testing to determine whether dependences exist. <p> Pfc has a mature dependence analyzer which performs data dependence analysis, control dependence analysis, interprocedural constant propagation [15], interprocedural side effect analysis of scalars [20], and interprocedural array section analysis <ref> [16, 28] </ref>. Ped expands on Pfc's analysis and transformation capabilities and makes them available to the user in an interactive environment. Parafrase was the first automatic vectorizing and par-allelizing compiler [36]. It supports program analysis and performs a large number of program transformations to improve parallelism.
Reference: [29] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <type> Technical Report TR90-149, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> February </month> <year> 1991. </year> <note> To appear in J. </note> <editor> Saltz and P. Mehrotra, editors, </editor> <title> Compilers and Runtime Software for Scalable Multiprocessors, </title> <publisher> Elsevier, </publisher> <year> 1991. </year>
Reference-contexts: The analysis and representation of dependence in the ParaScope Editor have also proven very useful in the development of several other advanced tools, including a compiler <ref> [29] </ref> and data decomposition tools [5, 6] for distributed-memory machines, and an on-the-fly access anomaly detection system for shared-memory machines [30]. 11 Acknowledgments We would like to thank Vasanth Balasundaram, Pre-ston Briggs, Keith Cooper, Paul Havlak, Marina Kalem, Rhonda Reese, Jaspal Subhlok, and Linda Torczon for their many contributions to this
Reference: [30] <author> R. Hood, K. Kennedy, and J. Mellor-Crummey. </author> <title> Parallel program debugging with on-the-fly anomaly detection. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: The analysis and representation of dependence in the ParaScope Editor have also proven very useful in the development of several other advanced tools, including a compiler [29] and data decomposition tools [5, 6] for distributed-memory machines, and an on-the-fly access anomaly detection system for shared-memory machines <ref> [30] </ref>. 11 Acknowledgments We would like to thank Vasanth Balasundaram, Pre-ston Briggs, Keith Cooper, Paul Havlak, Marina Kalem, Rhonda Reese, Jaspal Subhlok, and Linda Torczon for their many contributions to this work. Their efforts have made Ped the useful research tool it is today.
Reference: [31] <author> F. Irigoin and R. Triolet. </author> <title> Supernode partitioning. </title> <booktitle> In Conference Record of the Fifteenth ACM Symposium on the Principles of Programming Languages, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1988. </year>
Reference-contexts: Finally, the interchange flags are recalculated for dependences in the loop nest. 7.2 Loop Skewing Loop skewing is a transformation that changes the shape of the iteration space to expose parallelism across a wavefront <ref> [31, 39, 43, 56] </ref>. It can be applied in conjunction with loop interchange, strip mining, and loop reversal to obtain effective loop-level parallelism in a loop nest [9, 54, 57]. All of these transformations are supported in Ped.
Reference: [32] <author> K. Kennedy and K. S. M c Kinley. </author> <title> Loop distribution with arbitrary control flow. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for enhancing the use of the memory hierarchy. These transformations are described in detail in the literature <ref> [2, 4, 12, 32, 33, 36, 41, 56] </ref>. We classify the transformations in Ped as follows. <p> The user may then apply or reject the distribution partition. Safety To maintain the meaning of the original loop, the partition must not put statements that are involved in recurrences into different loops <ref> [32, 37] </ref>. Recurrences are calculated by finding strongly connected regions in the subgraph composed of loop-independent dependences and dependences carried on the loop to be distributed. <p> These decisions correspond to loop-independent control dependences that cross between partitions. We use Kennedy and M c Kinley's method to insert new arrays, called execution variables, that record these "crossing" decisions <ref> [32] </ref>. Given a partition, this algorithm introduces the minimal number of execution variables necessary to effect the partition, even for loops with arbitrary control flow. Page 10 Profitability Currently Ped does not change the or-der of statements in the loop during partitioning. <p> First, loop-independent data dependences are introduced between the definitions and uses of execution variables representing the crossing decision. A control dependence is then inserted from the test on the execution variable to the sink of the original control dependence. The update algorithm is explained more thoroughly elsewhere <ref> [32] </ref>. 7.4 Unroll and Jam Unroll and jam is a transformation that unrolls an outer loop in a loop nest, then jams (or fuses) the resulting inner loops [2, 13].
Reference: [33] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Interactive parallel programming using the ParaScope Editor. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3), </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: Clearly a tool with this much functionality is bound to be complex. Ped incorporates a complete source editor and supports dependence analysis, dependence display, and a large variety of program transformations to enhance parallelism. Previous work has described the usage and motivation of the ParaScope Editor <ref> [8, 23, 33] </ref> and the ParaScope parallel programming environment [14]. In this paper, we focus on the implementation of Ped's analysis and transformation features. <p> Each element of the vector can represent a dependence distance or direction. Dependence edges are organized for the user interface using a higher level data abstraction, called the edge list. The edge list provides the user a configurable method of filtering, sorting, and selecting dependences <ref> [8, 33] </ref>. 4.2.2 Level Vectors Dependence edges hold most of the dependence information for a program, but level vectors provide the glue which links them together and to the AST. Every executable statement in a loop nest involved with a dependence has a level vector. <p> Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for enhancing the use of the memory hierarchy. These transformations are described in detail in the literature <ref> [2, 4, 12, 32, 33, 36, 41, 56] </ref>. We classify the transformations in Ped as follows.
Reference: [34] <author> U. Kremer, H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> Advanced tools and techniques for automatic parallelization. </title> <journal> Parallel Computing, </journal> <volume> 7 </volume> <pages> 387-393, </pages> <year> 1988. </year>
Reference-contexts: Superb provides a set of interactive program transformations, including transformations that exploit data parallelism. The user specifies a data partitioning, then node programs with the necessary send and receive operations are automatically generated. Algorithms are also described for incremental update of use-def and def-use chains following structured program transformations <ref> [34] </ref>. 10 Conclusions Our experience with the ParaScope Editor has shown that dependence analysis can be used in an interactive tool with acceptable efficiency.
Reference: [35] <author> D. Kuck. </author> <title> The Structure of Computers and Computations, Volume 1. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, NY, </address> <year> 1978. </year>
Reference-contexts: Hence, the results may differ each time the program is executed. This kind of anomaly, usually called a data race, precludes the parallelization of the above loop. In the literature of compilation for parallel execution, a potential data race is referred to as a loop-carried dependence <ref> [4, 35] </ref>. Without explicit synchronization, only loops with no carried dependences may be safely run in parallel. Automatic parallelizers use this principle by constructing a dependence graph for the entire program and then parallelizing every loop that does not carry a dependence. <p> There are four types of data dependence <ref> [35] </ref>: True (flow) dependence occurs when S 1 writes a memory location that S 2 later reads. Anti dependence occurs when S 1 reads a memory location that S 2 later writes. Output dependence occurs when S 1 writes a memory location that S 2 later writes.
Reference: [36] <author> D. Kuck, R. Kuhn, B. Leasure, and M. J. Wolfe. </author> <title> The structure of an advanced retargetable vectorizer. In Supercomputers: </title> <booktitle> Design and Applications, </booktitle> <pages> pages 163-178. </pages> <publisher> IEEE Computer Society Press, </publisher> <address> Silver Spring, MD, </address> <year> 1984. </year>
Reference-contexts: Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for enhancing the use of the memory hierarchy. These transformations are described in detail in the literature <ref> [2, 4, 12, 32, 33, 36, 41, 56] </ref>. We classify the transformations in Ped as follows. <p> It has been used extensively in vector-izing and parallelizing compilers to adjust the granularity of parallel loops and to expose parallelism <ref> [4, 36, 56] </ref>. Ped interchanges pairs of adjacent loops. Loop permutations may be performed as a series of pairwise interchanges. Ped supports interchange of triangular or skewed loops. It also interchanges hexagonal loops that result after skewed loops are interchanged. <p> Ped expands on Pfc's analysis and transformation capabilities and makes them available to the user in an interactive environment. Parafrase was the first automatic vectorizing and par-allelizing compiler <ref> [36] </ref>. It supports program analysis and performs a large number of program transformations to improve parallelism. In Parafrase, program transformations are structured in phases and are always applied where applicable. Batch analysis is performed after each transformation phase to update the dependence information for the entire program.
Reference: [37] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. J. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eighth ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Williamsburg, VA, </address> <month> January </month> <year> 1981. </year>
Reference-contexts: the original distance vectors (d 1 ; d 2 ) for all dependences in the nest to (d 1 ; ffd 1 + d 2 ), and then updates their interchange flags. 7.3 Loop Distribution Loop distribution separates independent statements inside a single loop into multiple loops with identical headers <ref> [4, 37] </ref>. It is used to expose partial parallelism by separating statements which may be paral-lelized from those that must be executed sequentially. It is a cornerstone of vectorization and parallelization. In Ped the user can specify whether distribution is for the purpose of vectorization or parallelization. <p> The user may then apply or reject the distribution partition. Safety To maintain the meaning of the original loop, the partition must not put statements that are involved in recurrences into different loops <ref> [32, 37] </ref>. Recurrences are calculated by finding strongly connected regions in the subgraph composed of loop-independent dependences and dependences carried on the loop to be distributed.
Reference: [38] <author> D. Kuck, Y. Muraoka, and S. Chen. </author> <title> On the number of operations simultaneously executable in Fortran-like programs and their resulting speedup. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-21(12):1293-1310, </volume> <month> December </month> <year> 1972. </year>
Reference-contexts: A dependence is carried by the outermost loop for which the element in the direction vector is not an `='. Additionally, direction vectors are used to determine the safety and profitability of loop interchange [4, 55]. Distance vectors, first used by Kuck and Muraoka <ref> [38, 43] </ref>, are more precise versions of direction vectors that specify the actual number of loop iterations between two accesses to the same memory location.
Reference: [39] <author> L. Lamport. </author> <title> The parallel execution of DO loops. </title> <journal> Communications of the ACM, </journal> <volume> 17(2) </volume> <pages> 83-93, </pages> <month> February </month> <year> 1974. </year>
Reference-contexts: Distance vectors, first used by Kuck and Muraoka [38, 43], are more precise versions of direction vectors that specify the actual number of loop iterations between two accesses to the same memory location. They are utilized by transformations to exploit parallelism <ref> [9, 39, 54, 56] </ref> and the memory hierarchy [12, 24]. 3 Work Model Ped is designed to exploit loop-level parallelism, which comprises most of the usable parallelism in scientific codes when synchronization costs are considered [18]. <p> Finally, the interchange flags are recalculated for dependences in the loop nest. 7.2 Loop Skewing Loop skewing is a transformation that changes the shape of the iteration space to expose parallelism across a wavefront <ref> [31, 39, 43, 56] </ref>. It can be applied in conjunction with loop interchange, strip mining, and loop reversal to obtain effective loop-level parallelism in a loop nest [9, 54, 57]. All of these transformations are supported in Ped.
Reference: [40] <author> B. Leasure, </author> <title> editor. PCF Fortran: Language Definition, version 3.1. </title> <booktitle> The Parallel Computing Forum, </booktitle> <address> Champaign, IL, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: 1 Introduction The ParaScope Editor is a tool designed to help skilled users interactively transform a sequential Fortran 77 program into a parallel program with explicit parallel constructs, such as those in PCF Fortran <ref> [40] </ref>. In a language like PCF Fortran, the principal mechanism for the introduction of parallelism is the parallel loop, which specifies that its iterations may be run in parallel according to any schedule. The fundamental problem introduced by such languages is the possibility of nondeterministic execution.
Reference: [41] <author> D. Loveman. </author> <title> Program improvement by source-to-source transformations. </title> <journal> Journal of the ACM, </journal> <volume> 17(2) </volume> <pages> 121-145, </pages> <month> Jan-uary </month> <year> 1977. </year>
Reference-contexts: Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for enhancing the use of the memory hierarchy. These transformations are described in detail in the literature <ref> [2, 4, 12, 32, 33, 36, 41, 56] </ref>. We classify the transformations in Ped as follows.
Reference: [42] <author> K. S. McKinley. </author> <title> Evaluating the potential for automatic parallel code generation. </title> <type> Technical Report TR90-148, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: We are, as always, in the process of further extending Ped's transformation capabilities. A major effort is underway to incorporate automatic parallelization strategies within Ped in order to provide users with further assis tance in the parallelization process <ref> [42] </ref>. 8 Edits Editing is fundamental for any program development tool because it is the most flexible means of making program changes. The ParaScope Editor therefore provides advanced editing features.
Reference: [43] <author> Y. Muraoka. </author> <title> Parallelism Exposure and Exploitation in Programs. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> February </month> <year> 1971. </year> <note> Report No. 71-424. </note>
Reference-contexts: A dependence is carried by the outermost loop for which the element in the direction vector is not an `='. Additionally, direction vectors are used to determine the safety and profitability of loop interchange [4, 55]. Distance vectors, first used by Kuck and Muraoka <ref> [38, 43] </ref>, are more precise versions of direction vectors that specify the actual number of loop iterations between two accesses to the same memory location. <p> Finally, the interchange flags are recalculated for dependences in the loop nest. 7.2 Loop Skewing Loop skewing is a transformation that changes the shape of the iteration space to expose parallelism across a wavefront <ref> [31, 39, 43, 56] </ref>. It can be applied in conjunction with loop interchange, strip mining, and loop reversal to obtain effective loop-level parallelism in a loop nest [9, 54, 57]. All of these transformations are supported in Ped.
Reference: [44] <author> C. Polychronopoulos, M. Girkar, M. Haghighat, C. Lee, B. Leung, and D. Schouten. </author> <title> The structure of Parafrase-2: An advanced parallelizing compiler for C and Fortran. </title> <editor> In D. Gel-ernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing. </booktitle> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: In Parafrase, program transformations are structured in phases and are always applied where applicable. Batch analysis is performed after each transformation phase to update the dependence information for the entire program. Parafrase-2 adds scheduling and improved program analysis and transformations <ref> [44] </ref>. More advanced interprocedural and symbolic analysis is planned [27]. Parafrase-2 uses Faust as a front end to provide interactive paralleliza-tion and graphical displays [26]. Ptran is also an automatic parallelizer with extensive program analysis.
Reference: [45] <author> C. Rosene. </author> <title> Incremental Dependence Analysis. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: Several algorithms for incremental analysis can be found in the literature; e.g., dataflow analysis [47, 58], interprocedural analysis [10, 46], interprocedural recompilation analysis [11], as well as dependence analysis <ref> [45] </ref>. However, few of these algorithms have been implemented and evaluated in an interactive environment. Rather than tackle all these problems at once, we chose a simple yet practical strategy for the current implementation of Ped. First, the scope of each program change is evaluated.
Reference: [46] <author> B. Ryder and M. Carroll. </author> <title> An incremental algorithm for software analysis. </title> <booktitle> In Proceedings of the Second ACM SIGSOFT/SIGPLAN Software Engineering Symposium on Practical Software Development Environments, </booktitle> <address> Palo Alto, CA, </address> <month> December </month> <year> 1986. </year>
Reference-contexts: Several algorithms for incremental analysis can be found in the literature; e.g., dataflow analysis [47, 58], interprocedural analysis <ref> [10, 46] </ref>, interprocedural recompilation analysis [11], as well as dependence analysis [45]. However, few of these algorithms have been implemented and evaluated in an interactive environment. Rather than tackle all these problems at once, we chose a simple yet practical strategy for the current implementation of Ped.
Reference: [47] <author> B. Ryder and M. Paull. </author> <title> Incremental data flow analysis algorithms. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(1) </volume> <pages> 1-50, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: In order to calculate precise dependence information, Ped may need to incrementally update the control flow, control dependence, SSA, and call graphs, as well as recalculate live range, constant, symbolic, interprocedural, and dependence testing information. Several algorithms for incremental analysis can be found in the literature; e.g., dataflow analysis <ref> [47, 58] </ref>, interprocedural analysis [10, 46], interprocedural recompilation analysis [11], as well as dependence analysis [45]. However, few of these algorithms have been implemented and evaluated in an interactive environment.
Reference: [48] <author> B. Shei and D. Gannon. SIGMACS: </author> <title> A programmable programming environment. </title> <booktitle> In Proceedings of the Third Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Ptran introduces both task and loop parallelism, but currently the only other program transformations are variable priva-tization and loop distribution [1]. Sigmacs, a programmable interactive parallelizer in the Faust programming environment, computes and displays call graphs, process graphs, and a statement dependence graph <ref> [26, 48] </ref>. In a process graph each node represents a task or a process, which is a separate entity running in parallel. The call and process graphs may be animated dynamically at run time. Sigmacs also performs several interactive program transformations, and is working on automatic updating of dependence information.
Reference: [49] <author> J. Singh and J. Hennessy. </author> <title> An empirical investigation of the effectiveness of and limitations of automatic parallelization. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessors, </booktitle> <address> Tokyo, Japan, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Data flow along SSA edges is analyzed to deduce constraints and relationships on symbolic expressions, which may be in turn propagated. Studies have shown that symbolic analysis is essential for precise analysis of scientific programs <ref> [25, 27, 49] </ref>. The symbolic analyzer performs the following steps. Constant propagation uses the sparse conditional constant algorithm to eliminate as many symbolics as initially feasible [53]. Auxiliary induction variable substitution replaces auxiliary induction variables by functions of loop index variables.
Reference: [50] <author> K. Smith and W. Appelbe. </author> <title> PAT an interactive Fortran parallelizing assistant tool. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: The call and process graphs may be animated dynamically at run time. Sigmacs also performs several interactive program transformations, and is working on automatic updating of dependence information. Pat is also an interactive parallelization tool <ref> [50] </ref>. Its dependence analysis is restricted to Fortran programs where only one write occurs to each variable in a loop. In addition, Pat uses simple dependence tests that do not calculate distance or direction vectors. Hence, it is incapable of applying loop level transformations such as loop interchange and skewing.
Reference: [51] <author> K. Smith, W. Appelbe, and K. Stirewalt. </author> <title> Incremental dependence analysis for interactive parallelization. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: It can also insert synchronization to protect specific dependences. Pat divides analysis into scalar and dependence phases, but does not perform symbolic or interprocedu-ral analysis. The incremental dependence update that follows transformations is simplified due to its austere analysis <ref> [51] </ref>. Superb interactively converts sequential programs into data parallel SPMD programs that can be executed on the Suprenum distributed memory multiprocessor [59]. Superb provides a set of interactive program transformations, including transformations that exploit data parallelism.
Reference: [52] <author> J. Subhlok. </author> <title> Analysis of Synchronization in a Parallel Programming Environment. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: Establishing that the order specified by certain dependences will always be observed has been shown to be co-NP-hard, but techniques have been developed to identify dependences that are satisfied by existing synchronization under restricted circumstances <ref> [17, 52] </ref>. The current implementation of Ped can determine if event style synchronization is sufficient to protect a particular dependence. 5.8 Utilizing External Analysis To overcome gaps in the current implementation of dependence analysis, Ped may import dependence information from Pfc, the Rice system for automatic vec-torization and parallelization [4].
Reference: [53] <author> M. Wegman and K. Zadeck. </author> <title> Constant propagation with conditional branches. </title> <type> Technical Report CS-89-36, </type> <institution> Brown University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Studies have shown that symbolic analysis is essential for precise analysis of scientific programs [25, 27, 49]. The symbolic analyzer performs the following steps. Constant propagation uses the sparse conditional constant algorithm to eliminate as many symbolics as initially feasible <ref> [53] </ref>. Auxiliary induction variable substitution replaces auxiliary induction variables by functions of loop index variables. Expression folding propagates symbolic expressions along edges in the SSA graph. Loop invariant expression detection detects sym-bolics that may be eliminated by the symbolic expression simplifier.
Reference: [54] <author> M. E. Wolf and M. Lam. </author> <title> Maximizing parallelism via loop transformations. </title> <booktitle> In Proceedings of the Third Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Distance vectors, first used by Kuck and Muraoka [38, 43], are more precise versions of direction vectors that specify the actual number of loop iterations between two accesses to the same memory location. They are utilized by transformations to exploit parallelism <ref> [9, 39, 54, 56] </ref> and the memory hierarchy [12, 24]. 3 Work Model Ped is designed to exploit loop-level parallelism, which comprises most of the usable parallelism in scientific codes when synchronization costs are considered [18]. <p> It can be applied in conjunction with loop interchange, strip mining, and loop reversal to obtain effective loop-level parallelism in a loop nest <ref> [9, 54, 57] </ref>. All of these transformations are supported in Ped. Loop skewing is applied to a pair of perfectly nested loops that both carry dependences, even after loop interchange.
Reference: [55] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1982. </year>
Reference-contexts: Since several different values of ff and fi may satisfy the dependence equations, a set of distance and direction vectors may be needed to completely describe the dependences arising between a pair of array references. Direction vectors, introduced by Wolfe <ref> [55] </ref>, are useful for calculating loop-carried dependences. A dependence is carried by the outermost loop for which the element in the direction vector is not an `='. Additionally, direction vectors are used to determine the safety and profitability of loop interchange [4, 55]. <p> Direction vectors, introduced by Wolfe [55], are useful for calculating loop-carried dependences. A dependence is carried by the outermost loop for which the element in the direction vector is not an `='. Additionally, direction vectors are used to determine the safety and profitability of loop interchange <ref> [4, 55] </ref>. Distance vectors, first used by Kuck and Muraoka [38, 43], are more precise versions of direction vectors that specify the actual number of loop iterations between two accesses to the same memory location. <p> The purpose, mechanics, and safety of these transformations are presented, followed by their profitability estimates, user advice, and incremental dependence update algorithms. 7.1 Loop Interchange Loop interchange is a key transformation that modifies the traversal order of the iteration space for the selected loop nest <ref> [4, 55] </ref>. It has been used extensively in vector-izing and parallelizing compilers to adjust the granularity of parallel loops and to expose parallelism [4, 36, 56]. Ped interchanges pairs of adjacent loops. Loop permutations may be performed as a series of pairwise interchanges.
Reference: [56] <author> M. J. Wolfe. </author> <title> Loop skewing: The wavefront method revisited. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 15(4) </volume> <pages> 279-293, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Distance vectors, first used by Kuck and Muraoka [38, 43], are more precise versions of direction vectors that specify the actual number of loop iterations between two accesses to the same memory location. They are utilized by transformations to exploit parallelism <ref> [9, 39, 54, 56] </ref> and the memory hierarchy [12, 24]. 3 Work Model Ped is designed to exploit loop-level parallelism, which comprises most of the usable parallelism in scientific codes when synchronization costs are considered [18]. <p> Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for enhancing the use of the memory hierarchy. These transformations are described in detail in the literature <ref> [2, 4, 12, 32, 33, 36, 41, 56] </ref>. We classify the transformations in Ped as follows. <p> It has been used extensively in vector-izing and parallelizing compilers to adjust the granularity of parallel loops and to expose parallelism <ref> [4, 36, 56] </ref>. Ped interchanges pairs of adjacent loops. Loop permutations may be performed as a series of pairwise interchanges. Ped supports interchange of triangular or skewed loops. It also interchanges hexagonal loops that result after skewed loops are interchanged. <p> Finally, the interchange flags are recalculated for dependences in the loop nest. 7.2 Loop Skewing Loop skewing is a transformation that changes the shape of the iteration space to expose parallelism across a wavefront <ref> [31, 39, 43, 56] </ref>. It can be applied in conjunction with loop interchange, strip mining, and loop reversal to obtain effective loop-level parallelism in a loop nest [9, 54, 57]. All of these transformations are supported in Ped.
Reference: [57] <author> M. J. Wolfe. </author> <title> More iteration space tiling. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <address> Reno, NV, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: It can be applied in conjunction with loop interchange, strip mining, and loop reversal to obtain effective loop-level parallelism in a loop nest <ref> [9, 54, 57] </ref>. All of these transformations are supported in Ped. Loop skewing is applied to a pair of perfectly nested loops that both carry dependences, even after loop interchange.
Reference: [58] <author> F. Zadeck. </author> <title> Incremental data flow analysis in a structured program editor. </title> <booktitle> In Proceedings of the SIGPLAN '84 Symposium on Compiler Construction, </booktitle> <address> Montreal, Canada, </address> <month> June </month> <year> 1984. </year>
Reference-contexts: In order to calculate precise dependence information, Ped may need to incrementally update the control flow, control dependence, SSA, and call graphs, as well as recalculate live range, constant, symbolic, interprocedural, and dependence testing information. Several algorithms for incremental analysis can be found in the literature; e.g., dataflow analysis <ref> [47, 58] </ref>, interprocedural analysis [10, 46], interprocedural recompilation analysis [11], as well as dependence analysis [45]. However, few of these algorithms have been implemented and evaluated in an interactive environment.
Reference: [59] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> Superb: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1986. </year> <pages> Page 15 </pages>
Reference-contexts: The incremental dependence update that follows transformations is simplified due to its austere analysis [51]. Superb interactively converts sequential programs into data parallel SPMD programs that can be executed on the Suprenum distributed memory multiprocessor <ref> [59] </ref>. Superb provides a set of interactive program transformations, including transformations that exploit data parallelism. The user specifies a data partitioning, then node programs with the necessary send and receive operations are automatically generated.
References-found: 59

