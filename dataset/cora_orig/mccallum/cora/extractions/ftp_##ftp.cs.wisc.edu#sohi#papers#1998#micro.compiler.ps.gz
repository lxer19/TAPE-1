URL: ftp://ftp.cs.wisc.edu/sohi/papers/1998/micro.compiler.ps.gz
Refering-URL: http://www.cs.wisc.edu/mscalar/publications.html
Root-URL: http://www.cs.wisc.edu
Email: vijay@ecn.purdue.edu  sohi@cs.wisc.edu  
Title: Task Selection for a Multiscalar Processor  
Author: T. N. Vijaykumar Gurindar S. Sohi 
Affiliation: School of Electrical and Computer Engineering Purdue University  Computer Sciences Department University of Wisconsin-Madison  
Date: Dec. 1998  
Note: To appear in the 31st International Symposium on Microarchitecture,  
Abstract: The Multiscalar architecture advocates a distributed processor organization and task-level speculation to exploit high degrees of instruction level parallelism (ILP) in sequential programs without impeding improvements in clock speeds. The main goal of this paper is to understand the key implications of the architectural features of distributed processor organization and task-level speculation for compiler task selection from the point of view of performance. We identify the fundamental performance issues to be: control ow speculation, data communication, data dependence speculation, load imbalance, and task overhead. We show that these issues are intimately related to a few key characteristics of tasks: task size, inter-task control ow, and inter-task data dependence. We describe compiler heuristics to select tasks with favorable characteristics. We report experimental results to show that the heuristics are successful in boosting overall performance by establishing larger ILP windows. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: Due to the prevalence of pointers in most of our benchmarks, we rely on the memory dependence synchronization mechanism [11] to avoid excessive squashing and the ARB to ensure correctness. But register dependences are identified and specified entirely by the compiler using traditional def-use dataow equations <ref> [1] </ref>. Space constraints do not permit us to list the equations here. dependence_task () in Figure 3 integrates the data dependence heuristic with the control ow heuristic. For each data def-use dependence, we try to include the dependence within a task, without exceeding the limit on the number of successors.
Reference: [2] <author> J. Allen, K. Kennedy, C. Porterfield, and J. Warren. </author> <title> Conversion of control dependence to data dependence. </title> <booktitle> In Proceed-ingsof the 19th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 177189, </pages> <address> Austin, TX, </address> <month> Jan. </month> <year> 1983. </year> <institution> Association for Computing Machinery. </institution>
Reference-contexts: When all the control ow paths are terminated, the feasible task so obtained demarcates the task. There are a myriad of techniques to alleviate the problems caused by control ow for scheduling of superscalar code, such as trace scheduling [4], predication [8], and if-conversion <ref> [2] </ref>. The key point for Multiscalar is that as long as control ow is included within tasks the primary problem of mispredictions is alleviated.
Reference: [3] <author> S. Breach, T. Vijaykumar, and G. Sohi. </author> <title> The anatomy of the register file in a multiscalar processor. </title> <booktitle> In Proceedingsof the 25th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 181190, </pages> <address> San Jose, CA, </address> <month> Nov. </month> <year> 1994. </year> <institution> Association for Computing Machinery. </institution>
Reference-contexts: Intra-task dependences are handled by the processing units, similar to superscalar processors. In the case of inter-task register data dependences, a producer task communicates the required value to the consumer task when it has been computed <ref> [3] </ref>. In the case of inter-task memory data dependences, memory dependence speculation is employed; a task begins by speculating that it does not depend on any previous task for memory values and executes loads from the specified addresses. <p> There are many data dependence detection techniques for memory dependences through memory disambiguation schemes <ref> [3] </ref> [20]. These techniques work well for programs that do not employ intricate pointers. Due to the prevalence of pointers in most of our benchmarks, we rely on the memory dependence synchronization mechanism [11] to avoid excessive squashing and the ARB to ensure correctness.
Reference: [4] <author> J. Fisher. </author> <title> Trace scheduling: A technique for global microcode compaction. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 30:478490, </volume> <year> 1981. </year>
Reference-contexts: When all the control ow paths are terminated, the feasible task so obtained demarcates the task. There are a myriad of techniques to alleviate the problems caused by control ow for scheduling of superscalar code, such as trace scheduling <ref> [4] </ref>, predication [8], and if-conversion [2]. The key point for Multiscalar is that as long as control ow is included within tasks the primary problem of mispredictions is alleviated.
Reference: [5] <author> M. Franklin. </author> <title> The Multiscalar Architecture. </title> <type> Ph.D. thesis, </type> <institution> University of Wisconsin-Madison, Madison, WI 53706, </institution> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: The Multiscalar architecture <ref> [5] </ref> [6] [14] advocates a distributed processor organization to avail of the advantages of large windows and wide-issue pipeline without impeding improvements in clock speeds.
Reference: [6] <author> M. Franklin and G. S. Sohi. </author> <title> The expandable split window paradigm for exploiting fine-grain parallelism. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Com puter Architecture, </booktitle> <pages> pages 5867. </pages> <institution> Association for Computing Machinery, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: The Multiscalar architecture [5] <ref> [6] </ref> [14] advocates a distributed processor organization to avail of the advantages of large windows and wide-issue pipeline without impeding improvements in clock speeds. The key idea is to split one large window into multiple smaller windows and one wide-issue processing unit (PU) into multiple narrow-issue processing units connected together.
Reference: [7] <author> M. Franklin and G. S. Sohi. ARB: </author> <title> A hardware mechanism for dynamic reordering of memory references. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 45(5):552571, </volume> <month> May </month> <year> 1996. </year>
Reference-contexts: If the speculation is incorrect (i.e., a previous task performs a store to the same address as a load performed by this task), a memory dependence violation is detected by the Address Resolution Buffer (ARB) <ref> [7] </ref> and the offending task that performed the load (and all its successor tasks) is squashed. In a real implementation, it may be difficult to isolate tasks that are dependent on the offending task and squash only those.
Reference: [8] <author> P.-T. Hsu and E. Davidson. </author> <title> Highly concurrent scalar processing. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 386395. </pages> <institution> Association for Computing Machinery, </institution> <month> June </month> <year> 1986. </year>
Reference-contexts: When all the control ow paths are terminated, the feasible task so obtained demarcates the task. There are a myriad of techniques to alleviate the problems caused by control ow for scheduling of superscalar code, such as trace scheduling [4], predication <ref> [8] </ref>, and if-conversion [2]. The key point for Multiscalar is that as long as control ow is included within tasks the primary problem of mispredictions is alleviated.
Reference: [9] <author> Q. Jacobson, S. Bennett, N. Sharma, and J. E. Smith. </author> <title> Control ow speculation in multiscalar processors. </title> <booktitle> In Proceedings of the Third IEEE Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 218229, </pages> <month> Feb. </month> <year> 1997. </year>
Reference-contexts: The PUs are configured to use 2-way issue, 16-entry reorder buffer, and 8-entry issue list with two integer, one oating point, one branch, and one memory functional units. The intra-task prediction uses gshare with 16-bit history, and 64K-entry table of 2-bit counters. The inter-task prediction uses a path-based scheme <ref> [9] </ref> with 16-bit history, 64K-entry table of 2-bit counters and 2-bit target numbers. The register communication ring can carry 2 values per cycle and bypass values in the same cycle between adjacent PUs.
Reference: [10] <author> E. P. Markatos and T. J. LeBlanc. </author> <title> Load balancing vs. locality management in shared-memory multiprocessors. </title> <type> Technical Report URCSD-TR 399, </type> <institution> University of Rochester, </institution> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: Load imbalance causes performance loss similar to large-scale, parallel machines <ref> [10] </ref>. Large variations in the amount of computation of adjacent tasks causes load imbalance resulting in successor tasks waiting for predecessor task to complete and retire. There are two kinds of overhead associated with tasks: (1) task start overhead, and (2) task end overhead.
Reference: [11] <author> A. Moshovos, S. E. Breach, T. N. Vijaykumar, and G. S. Sohi. </author> <title> Dynamic speculation and synchronization of data dependences. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 181 193, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: There are many data dependence detection techniques for memory dependences through memory disambiguation schemes [3] [20]. These techniques work well for programs that do not employ intricate pointers. Due to the prevalence of pointers in most of our benchmarks, we rely on the memory dependence synchronization mechanism <ref> [11] </ref> to avoid excessive squashing and the ARB to ensure correctness. But register dependences are identified and specified entirely by the compiler using traditional def-use dataow equations [1]. <p> The ARB characteristics are: 32 entries/PU, 32 x #PU bytes/entry, 4KB (4PU)/8KB (8PU), fully associative, 2 cycle hit, interleaved as many banks as the number of PUs, lock-up free, pipelined, and augmented with a 256-entry memory synchronization table <ref> [11] </ref>. The L2 cache is 4MB, and 2-way associative with 12 cycle hit latency and 16 bytes per cycle transfer. Finally, the main memory is infinite capacity with 58 cycle latency, and 8 bytes per cycle transfer.
Reference: [12] <author> K. Olukotun, B. A. Nayfeh, L. Hammond, K. W. n, and K.- Y. Chang. </author> <title> The case for a single-chip multiprocessor. </title> <booktitle> In Proceedings of the Seventh International Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 211, </pages> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: not exist for centralized microarchitectures that do not perform task-level speculation (e.g., superscalar) have not been studied before in the context of sequential programs and microar-chitectures, but are germane to several recent proposals for distributed microarchitectures employing some form of task level speculation, including the MIT RAW [19], Stanford Hydra <ref> [12] </ref>, CMU STAMPede [16], and Minnesota Super-threaded architecture [17]. In Section 3, we describe our compiler heuristics to select tasks with favorable character istics. In Section 4, we analyze the effects of the various heuristics on overall performance and present measurements of the key task characteristics.
Reference: [13] <author> V. Sarkar and J. Hennessy. </author> <title> Partitioning parallel programs for macro-dataow. </title> <booktitle> In Conference Proceeedings of the 1986 ACM Conference on Lisp and Functional Programming, pages 192201. Association for Computing Machinery, </booktitle> <year> 1986. </year>
Reference-contexts: Data dependences, control dependences, load imbalance, and task overhead often impose conicting requirements. Sarkar showed that given the communication costs and amount of work associated with each function invocation, partitioning simple functional programs into non-speculative tasks to optimize the execution-time on a multiprocessor is NP-Complete <ref> [13] </ref>. Due to the intractable nature of obtaining optimal tasks, we rely on heuristics to approach the problem. This section describes how our task selection heuristics produce tasks with favorable characteristics.
Reference: [14] <author> G. Sohi, S. Breach, and T. Vijaykumar. </author> <title> Multiscalar processors. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 414425. </pages> <institution> Association for Computing Machinery, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: The Multiscalar architecture [5] [6] <ref> [14] </ref> advocates a distributed processor organization to avail of the advantages of large windows and wide-issue pipeline without impeding improvements in clock speeds. The key idea is to split one large window into multiple smaller windows and one wide-issue processing unit (PU) into multiple narrow-issue processing units connected together.
Reference: [15] <institution> SPEC newsletter, </institution> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: Experimental evaluation The heuristics described in the preceding sections have been implemented in the Gnu C Compiler, gcc. SPEC95 benchmark <ref> [15] </ref> source files are input to the compiler which produces executables. The binary generated by the compiler is executed by a simulator which faithfully captures the behavior of a Multiscalar processor on a cycle per cycle basis by simulating all instructions except for system calls. 4.1.
Reference: [16] <author> J. G. Steffan and T. C. Mowry. </author> <title> The potential for thread-level data speculation in tightly-coupled multiprocessors. </title> <booktitle> In Proceedings of the Fourth International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 213, </pages> <month> Feb. </month> <year> 1998. </year>
Reference-contexts: centralized microarchitectures that do not perform task-level speculation (e.g., superscalar) have not been studied before in the context of sequential programs and microar-chitectures, but are germane to several recent proposals for distributed microarchitectures employing some form of task level speculation, including the MIT RAW [19], Stanford Hydra [12], CMU STAMPede <ref> [16] </ref>, and Minnesota Super-threaded architecture [17]. In Section 3, we describe our compiler heuristics to select tasks with favorable character istics. In Section 4, we analyze the effects of the various heuristics on overall performance and present measurements of the key task characteristics.
Reference: [17] <author> J.-Y. Tsai and P.-C. Yew. </author> <title> The superthreaded architecture: Thread pipelining with run-time data dependence checking and control speculation. </title> <booktitle> In Proceedings of the 1996 Conference on Parallel Architectures and Compilation Techniques, </booktitle> <pages> pages 3546, </pages> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: perform task-level speculation (e.g., superscalar) have not been studied before in the context of sequential programs and microar-chitectures, but are germane to several recent proposals for distributed microarchitectures employing some form of task level speculation, including the MIT RAW [19], Stanford Hydra [12], CMU STAMPede [16], and Minnesota Super-threaded architecture <ref> [17] </ref>. In Section 3, we describe our compiler heuristics to select tasks with favorable character istics. In Section 4, we analyze the effects of the various heuristics on overall performance and present measurements of the key task characteristics. We draw some conclusions in Section 5. 2.
Reference: [18] <author> T. N. Vijaykumar. </author> <title> Compiling for the Multiscalar Architecture. </title> <type> Ph.D. thesis, </type> <institution> University of Wisconsin-Madison, Madi-son, WI 53706, </institution> <month> Jan. </month> <year> 1998. </year>
Reference-contexts: For loops, we move the induction variable increments to the top of the loops so that later iterations get the values of the induction variables from earlier iterations without any delay. Register communication scheduling is not discussed in this paper; details are available in <ref> [18] </ref>. 3.4. Data dependence heuristic The key problem with a data dependence is that if the producer is encountered late and the consumer is encoun tered early, then many cycles may be wasted waiting for the value to be communicated. <p> A simple solution to this difficulty is to prioritize the dependences using the execution frequency of the dependences, obtained by profiling. More details on the heuristics are in <ref> [18] </ref>. 3.5. Tasks selected by the heuristics data dependence edge is considered. Figure 4 (a1) shows a part of the CFG of a program including a data dependence edge from the top basic block to the bottom basic block. indicates a control ow edge and indicates a data dependence edge. <p> For Fortran programs, we use f2c and then compile the C code with our compiler. Multiscalar-specific optimizations including task selection, loop restructuring, dead register analysis for register communication, and register communication scheduling and generation are also used <ref> [18] </ref>. The compiler uses basic block frequency, obtained via dynamic profiling, for register communication scheduling and task selection. Profiling was done using the inputs specified by the SPEC95 suite. 4.3. Experiments and Results 4.3.1.
Reference: [19] <author> E. Waingold et al. </author> <title> Baring it all to software: Raw machines. </title> <booktitle> Computer, </booktitle> <address> 30(9):8693, </address> <month> Sept. </month> <year> 1997. </year>
Reference-contexts: issues, which do not exist for centralized microarchitectures that do not perform task-level speculation (e.g., superscalar) have not been studied before in the context of sequential programs and microar-chitectures, but are germane to several recent proposals for distributed microarchitectures employing some form of task level speculation, including the MIT RAW <ref> [19] </ref>, Stanford Hydra [12], CMU STAMPede [16], and Minnesota Super-threaded architecture [17]. In Section 3, we describe our compiler heuristics to select tasks with favorable character istics. In Section 4, we analyze the effects of the various heuristics on overall performance and present measurements of the key task characteristics.
Reference: [20] <author> R. P. Wilson and M. Lam. </author> <title> Efficient context-sensitive pointer analysis for c programs. </title> <booktitle> In Proceedings of the 1995 ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 112, </pages> <year> 1995. </year>
Reference-contexts: There are many data dependence detection techniques for memory dependences through memory disambiguation schemes [3] <ref> [20] </ref>. These techniques work well for programs that do not employ intricate pointers. Due to the prevalence of pointers in most of our benchmarks, we rely on the memory dependence synchronization mechanism [11] to avoid excessive squashing and the ARB to ensure correctness.
References-found: 20

