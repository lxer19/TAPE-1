URL: http://cag-www.lcs.mit.edu/~rinard/paper/popl92.ps
Refering-URL: http://cag-www.lcs.mit.edu/~rinard/paper/index.html
Root-URL: 
Title: Semantic Foundations of Jade  
Author: Martin C. Rinard and Monica S. Lam 
Address: CA 94305  
Affiliation: Computer Systems Laboratory Stanford University,  
Abstract: This paper describes the Jade constructs and defines both a serial and a parallel formal operational semantics for Jade. The paper proves that the two semantics are equivalent. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Berry et al. </author> <title> The perfect club benchmarks: Effective performance evaluation of supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <year> 1989. </year>
Reference-contexts: We have found it possible to parallelize sequential programs with a reasonable programming effort. Implemented applications include a parallel sparse Cholesky factorization algorithm due to Roth-berg and Gupta [11], the Perfect Club benchmark MDG <ref> [1] </ref>, LocusRoute, a VLSI routing system due to Rose [10], a parallel make program, and a program simulating the flow of smog in the Los Angeles basin. We have revised the definition of Jade so that it can now be implemented on machines with separate address spaces.
Reference: [2] <author> N. Carriero and D. Gelernter. </author> <title> How to Write Parallel Programs: A Guide to the Perplexed. </title> <journal> ACM Computing Surveys, </journal> <volume> 21(3) </volume> <pages> 323-357, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: The parallel semantics, however, can execute the two tasks in parallel, producing either an infinite loop or an error depending on how the tasks' transitions interleave. 4 Comparison with Other Work Explicitly parallel programming languages such as CSP [4], Ada [9], Linda <ref> [2] </ref>, Occam [5] and Concur-rentSmalltalk [12] force the programmer to manage concurrency using low-level operations to synchronize parallel tasks. This explicitly parallel approach often leads to complicated, nondeterministic programs that are difficult to debug and maintain.
Reference: [3] <author> R. T. Hammel and D. K. Gifford. </author> <title> FX-87 Performance Measurements: Dataflow Implementation. </title> <type> Technical Report MIT/LCS/TR-421, </type> <institution> MIT, </institution> <month> November </month> <year> 1988. </year>
Reference-contexts: The FX-87 compiler can then verify the correspondence between the declared and actual data accesses, scheduling conflict-free pieces of the program for concurrent execution. Making regions a static concept severely limits the amount of concurrency the implementation can extract <ref> [3] </ref>. At run time, multiple dynamic objects must be mapped to the same static region. Therefore, the compiler cannot exploit concurrency available between parts of the program that access disjoint sets of objects from the same region.
Reference: [4] <author> C. A. R. Hoare. </author> <title> Communicating Sequential Processes. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1985. </year>
Reference-contexts: The parallel semantics, however, can execute the two tasks in parallel, producing either an infinite loop or an error depending on how the tasks' transitions interleave. 4 Comparison with Other Work Explicitly parallel programming languages such as CSP <ref> [4] </ref>, Ada [9], Linda [2], Occam [5] and Concur-rentSmalltalk [12] force the programmer to manage concurrency using low-level operations to synchronize parallel tasks. This explicitly parallel approach often leads to complicated, nondeterministic programs that are difficult to debug and maintain.
Reference: [5] <author> Inmos Ltd. </author> <title> Occam Programming Manual. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1984. </year>
Reference-contexts: The parallel semantics, however, can execute the two tasks in parallel, producing either an infinite loop or an error depending on how the tasks' transitions interleave. 4 Comparison with Other Work Explicitly parallel programming languages such as CSP [4], Ada [9], Linda [2], Occam <ref> [5] </ref> and Concur-rentSmalltalk [12] force the programmer to manage concurrency using low-level operations to synchronize parallel tasks. This explicitly parallel approach often leads to complicated, nondeterministic programs that are difficult to debug and maintain.
Reference: [6] <author> M. S. Lam and M. C. Rinard. </author> <title> Coarse-grain parallel programming in Jade. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 94-105, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Because Jade hides the low-level coordination of parallel activity from the programmer, these applications are portable across different parallel architectures. We introduced the basic concepts of the data-oriented approach to concurrency in a previous paper <ref> [6] </ref>. The previous version of Jade was designed for machines with shared address spaces. We have implemented Jade as an extension to C, C++ and FORTRAN on the Encore Multimax, the Silicon Graphics IRIS 4D/240S, and Stanford DASH multiprocessor [7]. <p> This explicitly parallel approach often leads to complicated, nondeterministic programs that are difficult to debug and maintain. Jade, on the other hand, adopts an implicitly parallel approach that maintains the programming advantages of serial languages. In <ref> [6] </ref> we present a detailed analysis of the differences between Jade and such explicitly parallel languages. FX-87 [8] is similar to Jade in that it contains constructs that allow the programmer to express how the program accesses data.
Reference: [7] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. L. Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 94-105, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The previous version of Jade was designed for machines with shared address spaces. We have implemented Jade as an extension to C, C++ and FORTRAN on the Encore Multimax, the Silicon Graphics IRIS 4D/240S, and Stanford DASH multiprocessor <ref> [7] </ref>. We have found it possible to parallelize sequential programs with a reasonable programming effort.
Reference: [8] <author> J. M. Lucassen. </author> <title> Types and Effects: Towards the Integration of Functional and Imperative Programming. </title> <type> Technical Report MIT/LCS/TR-408, </type> <institution> MIT, </institution> <month> August </month> <year> 1987. </year>
Reference-contexts: Jade, on the other hand, adopts an implicitly parallel approach that maintains the programming advantages of serial languages. In [6] we present a detailed analysis of the differences between Jade and such explicitly parallel languages. FX-87 <ref> [8] </ref> is similar to Jade in that it contains constructs that allow the programmer to express how the program accesses data. In FX-87, memory locations are partitioned into a finite, statically determined set of regions.
Reference: [9] <author> United States Department of Defense. </author> <title> Reference Manual for the Ada programming language. DoD, </title> <address> Washington, D.C., </address> <month> January </month> <year> 1983. </year> <month> ANSI/MIL-STD-1815A. </month>
Reference-contexts: The parallel semantics, however, can execute the two tasks in parallel, producing either an infinite loop or an error depending on how the tasks' transitions interleave. 4 Comparison with Other Work Explicitly parallel programming languages such as CSP [4], Ada <ref> [9] </ref>, Linda [2], Occam [5] and Concur-rentSmalltalk [12] force the programmer to manage concurrency using low-level operations to synchronize parallel tasks. This explicitly parallel approach often leads to complicated, nondeterministic programs that are difficult to debug and maintain.
Reference: [10] <author> J. S. Rose. LocusRoute: </author> <title> A Parallel Global Router for Standard Cells. </title> <booktitle> In Proceedings of the 25th Design Automation Conference, </booktitle> <pages> pages 189-195, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: We have found it possible to parallelize sequential programs with a reasonable programming effort. Implemented applications include a parallel sparse Cholesky factorization algorithm due to Roth-berg and Gupta [11], the Perfect Club benchmark MDG [1], LocusRoute, a VLSI routing system due to Rose <ref> [10] </ref>, a parallel make program, and a program simulating the flow of smog in the Los Angeles basin. We have revised the definition of Jade so that it can now be implemented on machines with separate address spaces.
Reference: [11] <author> E. Rothberg and A. Gupta. </author> <title> Efficient sparse matrix factorization on high-performance workstations exploiting the memory hierarchy. </title> <note> To appear in ACM Transactions on Mathematical Software. </note>
Reference-contexts: We have found it possible to parallelize sequential programs with a reasonable programming effort. Implemented applications include a parallel sparse Cholesky factorization algorithm due to Roth-berg and Gupta <ref> [11] </ref>, the Perfect Club benchmark MDG [1], LocusRoute, a VLSI routing system due to Rose [10], a parallel make program, and a program simulating the flow of smog in the Los Angeles basin.
Reference: [12] <author> Y. Yokote and M. Tokoro. </author> <title> Concurrent Programming in ConcurrentSmalltalk. </title> <editor> In A. Yonezawa and M. Tokoro, editors, </editor> <booktitle> Object-Oriented Concurrent Programming, </booktitle> <pages> pages 129-158. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: The parallel semantics, however, can execute the two tasks in parallel, producing either an infinite loop or an error depending on how the tasks' transitions interleave. 4 Comparison with Other Work Explicitly parallel programming languages such as CSP [4], Ada [9], Linda [2], Occam [5] and Concur-rentSmalltalk <ref> [12] </ref> force the programmer to manage concurrency using low-level operations to synchronize parallel tasks. This explicitly parallel approach often leads to complicated, nondeterministic programs that are difficult to debug and maintain. Jade, on the other hand, adopts an implicitly parallel approach that maintains the programming advantages of serial languages.
References-found: 12

