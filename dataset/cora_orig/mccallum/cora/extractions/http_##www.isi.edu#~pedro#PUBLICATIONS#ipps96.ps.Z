URL: http://www.isi.edu/~pedro/PUBLICATIONS/ipps96.ps.Z
Refering-URL: http://www.isi.edu/~pedro/PUBLICATIONS/ipps96.html
Root-URL: http://www.isi.edu
Email: (martin@cs.ucsb.edu) Pedro Diniz (pedro@cs.ucsb.edu)  
Title: Commutativity Analysis: A Technique for Automatically Parallelizing Pointer-Based Computations  
Author: Martin Rinard 
Address: Santa Barbara, CA 93106-5110  
Affiliation: Department of Computer Science, University of California at Santa Barbara  
Abstract: This paper introduces an analysis technique, commu-tativity analysis, for automatically parallelizing computations that manipulate dynamic, pointer-based data structures. Commutativity analysis views computations as composed of operations on objects. It then analyzes the program to discover when operations commute, i.e. leave the objects in the same state regardless of the order in which they execute. If all of the operations required to perform a given computation commute, the compiler can automatically generate parallel code. Commutativity analysis eliminates many of the limitations that have prevented existing compilers, which use data dependence analysis, from successfully parallelizing pointer-based applications. It enables compilers to parallelize computations that manipulate graphs and eliminates the need to analyze the data structure construction code to extract global properties of the data structure topology. This paper shows how to use symbolic execution and expression manipulation to statically determine that operations commute and how to exploit the extracted com-mutativity information to generate parallel code. It also presents performance results that demonstrate that commu-tativity analysis can be used to successfully parallelize the Barnes-Hut hierarchical N-body solver, an important scientific application that manipulates a complex pointer-based data structure. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> U. Banerjee, R. Eigenmann, A. Nicolau, and D. Padua. </author> <title> Automatic program parallelization. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 211-243, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Current parallelizing compilers preserve the semantics of the original serial program by preserving the data dependences <ref> [1] </ref>. These compilers attempt to identify independent pieces of computation (two pieces of computation are inde fl This research was supported in part by an Alfred P. Sloan Research Fellowship.
Reference: [2] <author> J. Barnes and P. Hut. </author> <title> A hierarchical O(NlogN) force-calculation algorithm. </title> <booktitle> Nature, </booktitle> <pages> pages 446-449, </pages> <month> December </month> <year> 1976. </year>
Reference-contexts: We performed this feasibility study on a complete scientific application the Barnes-Hut hierarchical N-body solver <ref> [2] </ref>. The Barnes-Hut is a challenging application because it manipulates a recursive, pointer-based data structure a space subdivision tree.
Reference: [3] <author> P. Barth, R. Nikhil, and Arvind. M-structures: </author> <title> Extending a parallel, non-strict, functional language with state. </title> <booktitle> In Proceedings of the Fifth ACM Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 538-568. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1991. </year>
Reference-contexts: Commutativity analysis is designed to recognize when programmer-defined operations commute and to exploit this property in the context of arbitrary computations on arbitrary programmer-defined objects. Other researchers have recognized the value of including support for commuting operations in parallel computing systems <ref> [3, 15, 14] </ref>. These systems focus on exploiting commuting operations and rely on some external mechanism, typically the programmer, to specify when the operations actually commute. The goal of the presented research is to automatically recognize and exploit commuting operations. 7.
Reference: [4] <author> D. Callahan. </author> <title> Recognizing and parallelizing bounded recurrences. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Several existing compilers can recognize when a loop performs a reduction of many values into a single value <ref> [6, 4] </ref>. These compilers recognize when the reduction primitive (typically addition) is associative. They then exploit this algebraic property to eliminate the data dependence associated with the serial accumulation of values into the result. The generated program computes the reduction in parallel.
Reference: [5] <author> P. Diniz and M. Rinard. </author> <title> Exploiting commuting operations in parallelizing serial programs. </title> <type> Technical Report TRCS95-11, </type> <institution> Dept. of Computer Science, University of California at Santa Barbara, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: The commutativity analysis algorithm is capable of exploiting the concurrency in the force calculation and position/velocity update phases. Because of space constraints we outline here the force calculation step only. The position/velocity update phases are in essence similar and have been omitted. In <ref> [5] </ref> we present a full description of the analyzed code and a discussion of commuting operations. The force computation step consists of a loop that invokes the force calculation computation for each body. Each force computation recursively traverses the space subdivision tree.
Reference: [6] <author> A. Fisher and A. Ghuloum. </author> <title> Parallelizing complex scans and reductions. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Program Language Design and Implementation, </booktitle> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: The algorithm builds the table by recursively traversing the outer conditional expressions to identify the minimal conjunctions of basic terms that select each maximal conditional-free subexpression as the value of the original expression. It is possible to further simplify the table using logic minimization techniques as pro posed in <ref> [6] </ref>. * Invoked Method Expressions : The algorithm sorts the sequence of invoked method expressions. This sort facilitates the comparison of sets of invoked method expressions by making it easier to identify isomorphic expressions. To compare two expressions for equality the algorithm performs a simple recursive isomorphism test. <p> Several existing compilers can recognize when a loop performs a reduction of many values into a single value <ref> [6, 4] </ref>. These compilers recognize when the reduction primitive (typically addition) is associative. They then exploit this algebraic property to eliminate the data dependence associated with the serial accumulation of values into the result. The generated program computes the reduction in parallel.
Reference: [7] <author> L. Hendren, J. Hummel, and A. Nicolau. </author> <title> Abstractions for recursive pointer data structures: improving the analysis and transformation of imperative programs. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: A significant limitation of applying data dependence analysis to computations that manipulate dynamic, pointer-based computations is the difficulty of performing analysis that is precise enough to expose the concurrency. To statically discover independent pieces of code, the compiler must recognize global topological properties of the manipulated data structures <ref> [7] </ref>. It must therefore analyze the code that builds the data structure and propagate the results of this analysis through the program to the sections that use the data. <p> This is a very encouraging result considering the amount of effort and sophistication of the optimizations in the barnes code. 6. Related Work Compiler research on automatically parallelizing serial codes that manipulate dynamic, pointer-based data structures has focused on techniques that precisely represent the run-time topology of the heap <ref> [7, 10] </ref>. Ideally the compiler can use this representation to discover independent pieces of code. Unlike commutativity analysis, these techniques must analyze the code that builds the data structure and must propagate the results of this analysis through the program to the parallel sections that use the data structure.
Reference: [8] <author> D. Lenoski. </author> <title> The Design and Analysis of DASH: A Scalable Directory-Based Multiprocessor. </title> <type> PhD thesis, </type> <institution> Stanford, </institution> <address> CA, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: The Barnes-Hut is a challenging application because it manipulates a recursive, pointer-based data structure a space subdivision tree. We applied commutativity analysis by hand to this application to generate parallel code. 2 We then executed the generated code on a shared memory multiprocessor (the Stanford DASH machine <ref> [8] </ref>), comparing its performance with that of a highly optimized, explicitly parallel version of the same computation from the SPLASH-2 benchmark set.
Reference: [9] <author> E. Mohr, D. Kranz, and R. Halstead. </author> <title> Lazy task creation: a technique for increasing the granularity of parallel programs. </title> <booktitle> In Proceedings of the 1990 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 185-197, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: This lock ensures that each invocation of parallel visit executes atomically with respect to all other invocations with the same receiver. The parallel visit method acquires the lock before accessing the receiver and releases the lock before invoking any methods. Application of lazy task creation techniques <ref> [9] </ref> can increase the granularity of the resulting parallel computation. 3 The Basic Approach Commutativity analysis is designed for programs written using a pure object-based paradigm. Such programs structure the computation as a sequence of operations on objects.
Reference: [10] <author> J. Plevyak, V. Karamcheti, and A. Chien. </author> <title> Analysis of dynamic structures for efficient parallel execution. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: This is a very encouraging result considering the amount of effort and sophistication of the optimizations in the barnes code. 6. Related Work Compiler research on automatically parallelizing serial codes that manipulate dynamic, pointer-based data structures has focused on techniques that precisely represent the run-time topology of the heap <ref> [7, 10] </ref>. Ideally the compiler can use this representation to discover independent pieces of code. Unlike commutativity analysis, these techniques must analyze the code that builds the data structure and must propagate the results of this analysis through the program to the parallel sections that use the data structure.
Reference: [11] <author> M. Rinard and P. Diniz. </author> <title> Automatically parallelizing serial programs using commutativity analysis. </title> <type> Technical Report TRCS95-13, </type> <institution> Dept. of Computer Science, University of Cali-fornia at Santa Barbara, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: Separability imposes no expressibility limitations it is possible to convert any method into a collection of separable methods via the introduction of auxiliary meth ods. In <ref> [11] </ref> we present a formal treatment of commutativity analysis as applied to separable operations. The foundation of commutativity analysis is a set of conditions that a compiler can use to test if two operations A and B commute.
Reference: [12] <author> D. Scales and M. S. Lam. </author> <title> An efficient shared memory system for distributed memory machines. </title> <type> Technical Report CSL-TR-94-627, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: Programmers that parallelize serial applications by hand do not simply write code that executes independent pieces of code concurrently. For example, four (Water, MP3D, LocusRoute and Cholesky) of the six parallel applications in the SPLASH benchmark suite [13] and three of the four parallel applications described in <ref> [12] </ref> violate the data dependences of the original serial program. They instead rely on commuting operations (operations that generate the same result regardless of their execution order) to preserve the semantics of the original serial program.
Reference: [13] <author> J. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Programmers that parallelize serial applications by hand do not simply write code that executes independent pieces of code concurrently. For example, four (Water, MP3D, LocusRoute and Cholesky) of the six parallel applications in the SPLASH benchmark suite <ref> [13] </ref> and three of the four parallel applications described in [12] violate the data dependences of the original serial program. They instead rely on commuting operations (operations that generate the same result regardless of their execution order) to preserve the semantics of the original serial program.
Reference: [14] <author> J. Solworth and B. Reagan. </author> <title> Arbitrary order operations on trees. </title> <booktitle> In Languages and Compilers for Parallel Computing, Fourth International Workshop, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Commutativity analysis is designed to recognize when programmer-defined operations commute and to exploit this property in the context of arbitrary computations on arbitrary programmer-defined objects. Other researchers have recognized the value of including support for commuting operations in parallel computing systems <ref> [3, 15, 14] </ref>. These systems focus on exploiting commuting operations and rely on some external mechanism, typically the programmer, to specify when the operations actually commute. The goal of the presented research is to automatically recognize and exploit commuting operations. 7.
Reference: [15] <author> G. Steele. </author> <title> Making asynchronous parallelism safe for the world. </title> <booktitle> In Proceedings of the Seventeenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 218-231, </pages> <address> San Francisco, CA, </address> <month> January </month> <year> 1990. </year>
Reference-contexts: Commutativity analysis is designed to recognize when programmer-defined operations commute and to exploit this property in the context of arbitrary computations on arbitrary programmer-defined objects. Other researchers have recognized the value of including support for commuting operations in parallel computing systems <ref> [3, 15, 14] </ref>. These systems focus on exploiting commuting operations and rely on some external mechanism, typically the programmer, to specify when the operations actually commute. The goal of the presented research is to automatically recognize and exploit commuting operations. 7.
Reference: [16] <author> S. Woo, M. Ohara, E. Torrie, J.P. Singh, and A. Gupta. </author> <title> The SPLASH-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proceedings of the 22th International Symposium on Computer Architecture, </booktitle> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: We expect the compiler to generate the same code and use the same library. We ran this version on the Stanford DASH machine. We compare its performance with that of the barnes code, a highly tuned hand-parallelized code from the SPLASH-2 benchmark set <ref> [16] </ref>. Tables 4 and 5 present the timing results. Number Processors of Bodies 1 2 4 8 16 1024 7.94 4.05 2.19 1.38 0.92 4096 51.23 26.82 14.20 8.09 5.16 16384 301.4 146.9 77.56 45.60 26.13 sion (seconds).
References-found: 16

