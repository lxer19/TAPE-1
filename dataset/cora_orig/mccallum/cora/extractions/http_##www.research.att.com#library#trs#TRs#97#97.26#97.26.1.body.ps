URL: http://www.research.att.com/library/trs/TRs/97/97.26/97.26.1.body.ps
Refering-URL: http://www.research.att.com/library/trs/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: walker,diane,cak,abella@research.att.com  
Title: PARADISE: A Framework for Evaluating Spoken Dialogue Agents  
Author: Marilyn A. Walker, Diane J. Litman, Candace A. Kamm and Alicia Abella 
Address: 180 Park Avenue Florham Park, NJ 07932-0971 USA  
Affiliation: AT&T LabsResearch  
Abstract: This paper presents PARADISE (PARAdigm for DIalogue System Evaluation), a general framework for evaluating spoken dialogue agents. The framework decouples task requirements from an agent's dialogue behaviors, supports comparisons among dialogue strategies, enables the calculation of performance over subdialogues and whole dialogues, specifies the relative contribution of various factors to performance, and makes it possible to compare agents performing different tasks by normaliz ing for task complexity.
Abstract-found: 1
Intro-found: 1
Reference: <author> Abella, Alicia, Michael K Brown, and Bruce Buntschuh. </author> <year> 1996. </year> <title> Development principles for dialog-based interfaces. </title> <booktitle> In ECAI-96 Spoken Dialog Processing Workshop, </booktitle> <address> Budapest, Hungary. </address>
Reference: <author> Bates, Madeleine and Damaris Ayuso. </author> <year> 1993. </year> <title> A proposal for incremental dialogue evaluation. </title> <booktitle> In Proceedings of the DARPA Speech and NL Workshop, </booktitle> <pages> pages 319-322. </pages>
Reference: <author> Carberry, S. </author> <year> 1989. </year> <title> Plan recognition and its use in understanding dialogue. </title> <editor> In A. Kobsa and W. Wahlster, editors, </editor> <title> User Models in Dialogue Systems. </title> <publisher> Springer Verlag, Berlin, </publisher> <pages> pages 133-162. </pages>
Reference: <author> Carletta, Jean C. </author> <year> 1996. </year> <title> Assessing the reliability of subjective codings. </title> <journal> Computational Linguistics, </journal> <volume> 20(4). </volume>
Reference: <author> Ciaremella, A. </author> <year> 1993. </year> <title> A prototype performance evaluation report. </title> <type> Technical Report Project Esprit 2218 SUNDIAL, </type> <institution> WP8000-D3. </institution>
Reference: <author> Cohen, Paul. R. </author> <year> 1995. </year> <title> Empirical Methods for Artificial Intelligence. </title> <publisher> MIT Press, </publisher> <address> Boston. </address>
Reference-contexts: for any (sub)dialogue D is defined as follows: 11 Performance = (ff fl N ()) n X w i fl N (c i ) Here ff is a weight on , the cost functions c i are weighted by w i , and N is a Z score normalization function <ref> (Cohen, 1995) </ref>. <p> If so, continuing use of a simple additive function might require a transformation of the data, a reworking of the model shown in Figure 1, or the inclusion of interaction terms in the model <ref> (Cohen, 1995) </ref>. below), , number of utterances (#utt) and number of repair utterances (#rep) for each of these users. Users 5 and 11 correspond to the dialogues in Figures 2 and 3 respectively.
Reference: <author> Danieli, M., W. Eckert, N. Fraser, N. Gilbert, M. Guy-omard, P. Heisterkamp, M. Kharoune, J. Magadur, S. McGlashan, D. Sadek, J. Siroux, and N. Youd. </author> <year> 1992. </year> <title> Dialogue manager design evaluation. </title> <type> Technical Report Project Esprit 2218 SUNDIAL, </type> <institution> WP6000-D3. Danieli, Morena and Elisabetta Gerbino. </institution> <year> 1995. </year> <title> Metrics for evaluating dialogue strategies in a spoken language system. </title> <booktitle> In Proceedings of the 1995 AAAI Spring Symposium on Empirical Methods in Discourse Interpretation and Generation, </booktitle> <pages> pages 34-39. </pages>
Reference: <author> Doyle, Jon. </author> <year> 1992. </year> <title> Rationality and its roles in reasoning. </title> <journal> Computational Intelligence, </journal> <volume> 8(2) </volume> <pages> 376-409. </pages>
Reference-contexts: PARADISE supports comparisons among dialogue strategies by providing a task representation that decouples what an agent needs to achieve in terms of the task requirements from how the agent carries out the dialogue performance task via dialogue. PARADISE uses a decision-theoretic framework <ref> (Doyle, 1992) </ref> to specify the relative contribution of various factors to an agent's overall performance.
Reference: <author> Fraser, Norman M. </author> <year> 1995. </year> <title> Quality standards for spoken dialogue systems: a report on progress in eagles. </title> <booktitle> In ESCA Workshop on Spoken Dialogue Systems Vigso, Denmark, </booktitle> <pages> pages 157-160. </pages>
Reference-contexts: However, one limitation of both this approach and the reference answer approach is the inability to generalize results to other tasks and environments <ref> (Fraser, 1995) </ref>. Such generalization requires the identification of factors that affect performance (Cohen, 1995; Sparck-Jones and Galliers, 1996).
Reference: <author> Gale, William, Ken W. Church, and David Yarowsky. </author> <year> 1992. </year> <title> Estimating upper and lower bounds on the performance of word-sense disambiguation programs. </title> <booktitle> In Proc. of 30th ACL, </booktitle> <pages> pages 249-256, </pages> <address> Newark, Delaware. </address>
Reference-contexts: Thus, the observed user/agent interactions are modeled as a coder, and the ideal interactions as an expert coder. total agreement, = 1. is superior to other measures of success such as transaction success (Danieli and Gerbino, 1995), concept accuracy (Simpson and Fraser, 1993), and percent agreement <ref> (Gale, Church, and Yarowsky, 1992) </ref> because takes into account the inherent complexity of the task by correcting for chance expected agreement. Thus provides a basis for comparisons across agents that are performing different tasks.
Reference: <author> Grosz, Barbara J. </author> <year> 1977. </year> <title> The representation and use of focus in dialogue understanding. </title> <type> Technical Report 151, </type> <institution> SRI International, </institution> <address> 333 Ravenswood Ave, Menlo Park, Ca. </address> <month> 94025. </month>
Reference: <author> Grosz, Barbara J. and Candace L. Sidner. </author> <year> 1986. </year> <title> Attentions, intentions and the structure of discourse. </title> <journal> Computational Linguistics, </journal> <volume> 12 </volume> <pages> 175-204. </pages>
Reference: <author> Hirschberg, Julia and Christine Nakatani. </author> <year> 1996. </year> <title> A prosodic analysis of discourse segments in direction-giving monologues. </title> <booktitle> In 34th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 286-293. </pages>
Reference: <author> Hirschman, Lynette, Deborah A. Dahl, Donald P. McKay, Lewis M. Norton, and Marcia C. Linebarger. </author> <year> 1990. </year> <title> Beyond class a: A proposal for automatic evaluation of discourse. </title> <booktitle> In Proceedings of the Speech and Natural Language Workshop, </booktitle> <pages> pages 109-113. </pages>
Reference-contexts: However, a critical obstacle to progress in this area is the lack of a general framework for evaluating and comparing the performance of different dialogue agents. One widely used approach to evaluation is based on the notion of a reference answer <ref> (Hirschman et al., 1990) </ref>. An agent's responses to a query are compared with a predefined key of minimum and maximum reference answers; performance is the proportion of responses that match the key.
Reference: <author> Hirschman, Lynette and Christine Pao. </author> <year> 1993. </year> <title> The cost of errors in a spoken language system. </title> <booktitle> In Proceedings of the Third European Conference on Speech Communication and Technology, </booktitle> <pages> pages 1419-1422. </pages>
Reference-contexts: The next section explains how to combine with a set of c i to yield an overall performance measure. 10 Previous work has shown that this can be done with high reliability <ref> (Hirschman and Pao, 1993) </ref>. 2.4 Estimating a Performance Function Given the definition of success and costs above and the model in Figure 1, performance for any (sub)dialogue D is defined as follows: 11 Performance = (ff fl N ()) n X w i fl N (c i ) Here ff is
Reference: <editor> Joshi, Aravind K., Bonnie Lynn Webber, </editor> <booktitle> and Ralph M. </booktitle>
Reference: <author> Weischedel. </author> <year> 1984. </year> <title> Preventing false inferences. </title> <booktitle> In COLING84: Proc. 10th International Conference on Computational Linguistics., </booktitle> <pages> pages 134-138. </pages>
Reference: <author> Kamm, Candace. </author> <year> 1995. </year> <title> User interfaces for voice applications. </title> <editor> In David Roe and Jay Wilpon, editors, </editor> <title> Voice Communication between Humans and Machines. </title> <publisher> National Academy Press, </publisher> <pages> pages 422-442. </pages>
Reference: <author> Keeney, Ralph and Howard Raiffa. </author> <year> 1976. </year> <title> Decisions with Multiple Objectives: Preferences and Value Tradeoffs. </title> <publisher> John Wiley and Sons. </publisher>
Reference-contexts: Table 5 shows user satisfaction (US) ratings (discussed 11 We assume an additive performance (utility) function because it appears that and the various cost factors c i are utility independent and additive independent <ref> (Keeney and Raiffa, 1976) </ref>. It is possible however that user satisfaction data collected in future experiments (or other data such as willingness to pay or use) would suggest that these factors are not utility or additive independent.
Reference: <author> Krippendorf, Klaus. </author> <year> 1980. </year> <title> Content Analysis: An Introduction to its Methodology. </title> <publisher> Sage Publications, </publisher> <address> Bev-erly Hills, Ca. </address>
Reference: <author> Litman, Diane and James Allen. </author> <year> 1990. </year> <title> Recognizing and relating discourse intentions and task-oriented plans. </title>
Reference: <editor> In Philip Cohen, Jerry Morgan, and Martha Pollack, editors, </editor> <title> Intentions in Communication. </title> <publisher> MIT Press. </publisher>
Reference: <author> Moser, Margaret G. and Johanna Moore. </author> <year> 1995. </year> <title> Investigating cue selection and placement in tutorial discourse. </title> <booktitle> In ACL 95, </booktitle> <pages> pages 130-137. </pages>
Reference: <author> Passonneau, Rebecca J. and Diane Litman. </author> <year> 1996. </year> <title> Empirical analysis of three dimension of spoken discourse: Segmentation, coherence and linguistic devices. </title> <editor> In Donia Scott and Eduard Hovy, editors, </editor> <booktitle> Burning Issues in Discourse. </booktitle>
Reference: <author> Polifroni, Joseph, Lynette Hirschman, Stephanie Seneff, and Victor Zue. </author> <year> 1992. </year> <title> Experiments in evaluating interactive spoken language systems. </title> <booktitle> In Proceedings of the DARPA Speech and NL Workshop, </booktitle> <pages> pages 28-33. </pages>
Reference: <author> Pollack, Martha, Julia Hirschberg, and Bonnie Webber. </author> <year> 1982. </year> <title> User participation in the reasoning process of expert systems. </title> <booktitle> In Proceedings First National Conference on Artificial Intelligence, </booktitle> <pages> pages pp. 358-361. </pages>
Reference: <author> Shriberg, Elizabeth, Elizabeth Wade, and Patti Price. </author> <year> 1992. </year> <title> Human-machine problem solving using spoken language systems (sls): Factors affecting performance and user satisfaction. </title> <booktitle> In Proceedings of the DARPA Speech and NL Workshop, </booktitle> <pages> pages 49-54. </pages>
Reference: <author> Siegel, Sidney. </author> <year> 1956. </year> <title> Nonparametric Statistics for the Behavioral Sciences. </title> <publisher> McGraw Hill. </publisher>
Reference: <author> Simpson, A. and N. A. Fraser. </author> <year> 1993. </year> <title> Black box and glass box evaluation of the sundial system. </title> <booktitle> In Proceedings of the Third European Conference on Speech Communication and Technology, </booktitle> <pages> pages 1423-1426. </pages>
Reference-contexts: Thus, the observed user/agent interactions are modeled as a coder, and the ideal interactions as an expert coder. total agreement, = 1. is superior to other measures of success such as transaction success (Danieli and Gerbino, 1995), concept accuracy <ref> (Simpson and Fraser, 1993) </ref>, and percent agreement (Gale, Church, and Yarowsky, 1992) because takes into account the inherent complexity of the task by correcting for chance expected agreement. Thus provides a basis for comparisons across agents that are performing different tasks.
Reference: <author> Smith, Ronnie W. and Steven A. Gordon. </author> <year> 1997. </year> <title> Effects of variable initiative on linguistic behavior in human-computer spoken natural language dialog. </title> <journal> Computational Linguistics, </journal> 22(2):???140-170. 
Reference-contexts: Finally, U5 illustrates a user request for an agent action, and is tagged with the RT attribute. The value of RT in the AVM instantiation for the dialogue would be reserve. Second, consider the very different domain and task of diagnosing a fault and repairing a circuit <ref> (Smith and Gor-don, 1997) </ref>.
Reference: <author> Sparck-Jones, Karen and Julia R. Galliers. </author> <year> 1996. </year> <title> Evaluating Natural Language Processing Systems. </title> <publisher> Springer. </publisher>
Reference: <author> Wahlster, Wolfgang. </author> <year> 1986. </year> <title> The role of natural language in advanced knowledge-based systems. </title> <editor> In H. Winter, editor, </editor> <booktitle> Artificial Intelligence and Man-Machine Systems. </booktitle> <publisher> Springer, Berlin, </publisher> <pages> pages 62-83. </pages>
Reference: <author> Walker, Marilyn A. </author> <year> 1996. </year> <title> The Effect of Resource Limits and Task Complexity on Collaborative Planning in Dialogue. </title> <journal> Artificial Intelligence Journal, 85(1-2):181-243. </journal>
Reference-contexts: The model further posits that two types of factors are potential relevant contributors to user satisfaction (namely task success and dialogue costs), and that two types of factors are potential relevant contributors to costs <ref> (Walker, 1996) </ref>.
Reference: <editor> Webber, Bonnie and Aravind Joshi. </editor> <year> 1982. </year> <title> Taking the initiative in natural language database interaction: Justifying why. </title> <booktitle> In Coling 82, </booktitle> <pages> pages 413-419. </pages>
References-found: 34

