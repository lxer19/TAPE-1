URL: http://www.cs.jhu.edu/~brill/Parsing_Tech_93.ps
Refering-URL: http://www.cs.jhu.edu/~brill/acadpubs.html
Root-URL: 
Email: email: brill@goldilocks.lcs.mit.edu  
Title: Transformation-Based Error-Driven Parsing  
Author: Eric Brill 
Affiliation: Spoken Language Systems Group Laboratory for Computer Science M.I.T.  
Abstract: In this paper we describe a new technique for parsing free text: a transformational grammar 1 is automatically learned that is capable of accurately parsing text into binary-branching syntactic trees. The algorithm works by beginning in a very naive state of knowledge about phrase structure. By repeatedly comparing the results of bracketing in the current state to proper bracketing provided in the training corpus, the system learns a set of simple structural transformations that can be applied to reduce the number of errors. After describing the algorithm, we present results and compare these results to other recent results in automatic grammar induction. 
Abstract-found: 1
Intro-found: 1
Reference: [Bak79] <author> J. Baker. </author> <title> Trainable grammars for speech recognition. </title> <booktitle> In Speech communication papers presented at the 97th Meeting of the Acoustical Society of America, </booktitle> <year> 1979. </year>
Reference-contexts: The most promising results to date have been based on the inside-outside algorithm, which can be used to train stochastic context-free grammars. The inside-outside algorithm is an extension of the finite-state based Hidden Markov Model (by <ref> [Bak79] </ref>), which has been applied successfully in many areas, including speech recognition and part of speech tagging. A number of recent papers have explored the potential of using the inside-outside algorithm to automatically learn a grammar [LY90, SJM90, PS92, BW92, CC92, SRO93].
Reference: [BM92a] <author> E. Brill and M. Marcus. </author> <title> Automatically acquiring phrase structure using distributional analysis. </title> <booktitle> In Darpa 12 Eric Brill Workshop on Speech and Natural Language, </booktitle> <address> Harriman, N.Y., </address> <year> 1992. </year>
Reference-contexts: DAAL 03-89-C0031 PRI. 1 Not in the traditional sense of the term. parse new text. [Sam86] defines a function to score the quality of parse trees, and then uses simulated annealing to heuristically explore the entire space of possible parses for a given sentence. In <ref> [BM92a] </ref>, distributional analysis techniques are applied to a large corpus to learn a context-free grammar. The most promising results to date have been based on the inside-outside algorithm, which can be used to train stochastic context-free grammars.
Reference: [BM92b] <author> E. Brill and M. Marcus. </author> <title> Tagging an unfamiliar text with minimal human supervision. </title> <booktitle> In Proceedings of the Fall Symposium on Probabilistic Approaches to Natural Language AAAI Technical Report. American Association for Artificial Intelligence, </booktitle> <year> 1992. </year>
Reference-contexts: This learning paradigm, illustrated in figure 1, has proven to be successful in a number of different natural language applications. In [Bri93] (see also <ref> [Bri92, BM92b] </ref>), transformation-based learning is applied to part of speech tagging. It is shown that the transformation-based approach outperforms stochastic taggers ([MM91]) when trained on small corpora, and obtains performance comparable to stochastic taggers on larger corpora.
Reference: [BR93] <author> E. Brill and P. </author> <title> Resnik. A transformation based approach to prepositional phrase attachment. </title> <type> Technical report, </type> <institution> Department of Computer and Information Science, University of Pennsylvania, </institution> <year> 1993. </year> <month> Forthcoming. </month>
Reference-contexts: It is shown that the transformation-based approach outperforms stochastic taggers ([MM91]) when trained on small corpora, and obtains performance comparable to stochastic taggers on larger corpora. This is significant in light of the fact that the transformation-based tagger is completely symbolic. In <ref> [BR93] </ref>, this technique is applied to prepositional phrase attachment. The transformation-based approach is shown to significantly outperform the t-score technique for prepositional phrase attachment described in [HR91]. In its initial state, the transformation-based learner is capable of annotating text but is not very good at doing so. <p> In addition to meta-rules, postprocessors addressing particular parsing problems such as prepositional phrase attachment and coordination could lead to significant system performance improvements. Progress has already been made on a transformation-based prepositional phrase attachment program (see <ref> [BR93] </ref>). 6 ASSIGING NONTER MINAL LABELS Once a tree is bracketed, the next step is to label the nonterminal nodes. Transformation-based error-driven learning is once again used for learning how to label nonterminals. Currently, a node is labelled based solely on the labels of its daughters.
Reference: [Bri92] <author> E. Brill. </author> <title> A simple rule-based part of speech tagger. </title> <booktitle> In Proceedings of the Third Conference on Applied Natural Language Processing, ACL, </booktitle> <address> Trento, Italy, </address> <year> 1992. </year>
Reference-contexts: This learning paradigm, illustrated in figure 1, has proven to be successful in a number of different natural language applications. In [Bri93] (see also <ref> [Bri92, BM92b] </ref>), transformation-based learning is applied to part of speech tagging. It is shown that the transformation-based approach outperforms stochastic taggers ([MM91]) when trained on small corpora, and obtains performance comparable to stochastic taggers on larger corpora. <p> The list of possible transformation types is prespecified. Transformations involve making a simple change triggered by a simple environment. In the current implementation, there are twelve allowable transformation types: 2 This is the same output given by systems described in <ref> [MM90, Bri92, PS92, SRO93] </ref>. * (1-8) (Addjdelete) a (leftjright) parenthesis to the (leftjright) of part of speech tag X. * (9-12) (Addjdelete) a (leftjright) paren thesis between tags X and Y.
Reference: [Bri93] <author> E. Brill. </author> <title> A Corpus-Based Approach to Language Learning. </title> <type> PhD thesis, </type> <institution> Department of Computer and Information Science, University of Penn-sylvania, </institution> <year> 1993. </year>
Reference-contexts: This learning paradigm, illustrated in figure 1, has proven to be successful in a number of different natural language applications. In <ref> [Bri93] </ref> (see also [Bri92, BM92b]), transformation-based learning is applied to part of speech tagging. It is shown that the transformation-based approach outperforms stochastic taggers ([MM91]) when trained on small corpora, and obtains performance comparable to stochastic taggers on larger corpora.
Reference: [BW92] <author> T. Briscoe and N. Waegner. </author> <title> Robust stochastic parsing using the inside-outside algorithm. </title> <booktitle> In Workshop notes from the AAAI Statistically-Based NLP Techniques Workshop, </booktitle> <year> 1992. </year>
Reference-contexts: A number of recent papers have explored the potential of using the inside-outside algorithm to automatically learn a grammar <ref> [LY90, SJM90, PS92, BW92, CC92, SRO93] </ref>. Below, we describe a new technique for grammar induction. The algorithm works by beginning in a very naive state of knowledge about phrase structure.
Reference: [CC92] <author> G. Carroll and E. Charniak. </author> <title> Learning probabilistic dependency grammars from labelled text aaai technical report. </title> <booktitle> In Proceedings of the Fall Symposium on Probabilistic Approaches to Natural Language. American Association for Artificial Intelligence, </booktitle> <year> 1992. </year>
Reference-contexts: A number of recent papers have explored the potential of using the inside-outside algorithm to automatically learn a grammar <ref> [LY90, SJM90, PS92, BW92, CC92, SRO93] </ref>. Below, we describe a new technique for grammar induction. The algorithm works by beginning in a very naive state of knowledge about phrase structure.
Reference: [ea91] <author> E. Black et al. </author> <title> A procedure for quantitatively comparing the syntactic coverage of English grammars. </title> <booktitle> In Proceedings of Fourth DARPA Speech and Natural Language Workshop, </booktitle> <pages> pages 306-311, </pages> <year> 1991. </year>
Reference-contexts: The measure we have chosen for our experiments is the same measure described in [PS92], which is one of the measures that arose out of a parser evaluation workshop <ref> [ea91] </ref>. The measure is the percentage of constituents (strings of words between matching parentheses) from sentences output by our system which do not cross any constituents in the Penn Treebank structural description of the sentence.
Reference: [HGD90] <author> C. Hemphill, J. Godfrey, and G. Doddington. </author> <title> The ATIS spoken language systems pilot corpus. </title> <booktitle> In Proceedings of the DARPA Speech and Natural Language Workshop, </booktitle> <year> 1990. </year>
Reference-contexts: results in the correct structure: ( ( ( We ( called them ) ) ( , ( but ( they left ) ) ) ) . ) 4 RESULTS In the first experiment we ran, training and testing were done on the Texas Instruments Air Travel Information System (ATIS) corpus <ref> [HGD90] </ref>. 7 In table 1, we compare results we obtained to results cited in [PS92] using the inside-outside algorithm on the same corpus. Accuracy is measured in terms of the percentage of noncrossing constituents in the test corpus, as described above.
Reference: [HR91] <author> D. Hindle and M. Rooth. </author> <title> Structural ambiguity and lexical relations. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Berke-ley, Ca., </address> <year> 1991. </year>
Reference-contexts: This is significant in light of the fact that the transformation-based tagger is completely symbolic. In [BR93], this technique is applied to prepositional phrase attachment. The transformation-based approach is shown to significantly outperform the t-score technique for prepositional phrase attachment described in <ref> [HR91] </ref>. In its initial state, the transformation-based learner is capable of annotating text but is not very good at doing so. The initial state annotator is typically very easy to create.
Reference: [LY90] <author> K. Lari and S. Young. </author> <title> The estimation of stochastic context-free grammars using the inside-outside algorithm. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 4, </volume> <year> 1990. </year>
Reference-contexts: A number of recent papers have explored the potential of using the inside-outside algorithm to automatically learn a grammar <ref> [LY90, SJM90, PS92, BW92, CC92, SRO93] </ref>. Below, we describe a new technique for grammar induction. The algorithm works by beginning in a very naive state of knowledge about phrase structure.
Reference: [MM90] <author> D. Magerman and M. Marcus. </author> <title> Parsing a natural language using mutual information statistics. </title> <booktitle> In Proceedings, Eighth National Conference on Artificial Intelligence (AAAI 90), </booktitle> <year> 1990. </year>
Reference-contexts: Given the difficulty inherent in manually building a robust parser, along with the availability of large amounts of training material, automatic grammar induction seems like a path worth pursuing. A number of systems have been built that can be trained automatically to bracket text into syntactic constituents. In <ref> [MM90] </ref> mutual information statistics are extracted from a corpus of text and this information is then used to fl This work was done while the author was at the University of Pennsylvania. This work was supported by DARPA and AFOSR jointly under grant No. AFOSR-90-0066, and by ARO grant No. <p> The list of possible transformation types is prespecified. Transformations involve making a simple change triggered by a simple environment. In the current implementation, there are twelve allowable transformation types: 2 This is the same output given by systems described in <ref> [MM90, Bri92, PS92, SRO93] </ref>. * (1-8) (Addjdelete) a (leftjright) parenthesis to the (leftjright) of part of speech tag X. * (9-12) (Addjdelete) a (leftjright) paren thesis between tags X and Y.
Reference: [MM91] <author> R. Weischedel M. Meteer, R. Schwartz. </author> <title> Empirical studies in part of speech labelling. </title> <booktitle> In Proceedings of the fourth DARPA Workshop on Speech and Natural Language, </booktitle> <year> 1991. </year>
Reference: [MSM93] <author> M. Marcus, B. Santorini, and M. Marcinkiewicz. </author> <title> Building a large annotated corpus of English: the Penn Treebank. </title> <note> To appear in Computational Linguistics, </note> <year> 1993. </year>
Reference-contexts: The scoring function. 4. The search strategy. 3 LEARNING PHRASE STRUCTURE The phrase structure learning algorithm is trained on a small corpus of partially bracketed text which is also annotated with part of speech information. All of the experiments presented below were done using the Penn Treebank annotated corpus <ref> [MSM93] </ref>. The Transformation-Based Error-Driven Parsing 3 learner begins in a naive initial state, knowing very little about the phrase structure of the target corpus. In particular, all that is initially known is that English tends to be right branching and that final punctuation is final punctuation.
Reference: [PS92] <author> F. Pereira and Y. Schabes. </author> <title> Inside-outside reestimation from partially bracketed corpora. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Newark, De., </address> <year> 1992. </year>
Reference-contexts: A number of recent papers have explored the potential of using the inside-outside algorithm to automatically learn a grammar <ref> [LY90, SJM90, PS92, BW92, CC92, SRO93] </ref>. Below, we describe a new technique for grammar induction. The algorithm works by beginning in a very naive state of knowledge about phrase structure. <p> The list of possible transformation types is prespecified. Transformations involve making a simple change triggered by a simple environment. In the current implementation, there are twelve allowable transformation types: 2 This is the same output given by systems described in <ref> [MM90, Bri92, PS92, SRO93] </ref>. * (1-8) (Addjdelete) a (leftjright) parenthesis to the (leftjright) of part of speech tag X. * (9-12) (Addjdelete) a (leftjright) paren thesis between tags X and Y. <p> The measure we have chosen for our experiments is the same measure described in <ref> [PS92] </ref>, which is one of the measures that arose out of a parser evaluation workshop [ea91]. The measure is the percentage of constituents (strings of words between matching parentheses) from sentences output by our system which do not cross any constituents in the Penn Treebank structural description of the sentence. <p> ( , ( but ( they left ) ) ) ) . ) 4 RESULTS In the first experiment we ran, training and testing were done on the Texas Instruments Air Travel Information System (ATIS) corpus [HGD90]. 7 In table 1, we compare results we obtained to results cited in <ref> [PS92] </ref> using the inside-outside algorithm on the same corpus. Accuracy is measured in terms of the percentage of noncrossing constituents in the test corpus, as described above. <p> numbers presented above allow us to compare the transformation learner with systems trained and tested on comparable corpora, these results are all based upon the assumption that the test data is tagged fairly reliably (manually tagged text was used in all of these experiments, as well in the experiments of <ref> [PS92, SRO93] </ref>.) When parsing free text, we cannot assume that the text will be tagged with the accuracy of a human annotator. Instead, an automatic tagger would have to be Transformation-Based Error-Driven Parsing 9 Lengths in the WSJ Corpus. used to first tag the text before parsing.
Reference: [Sam86] <author> G. Sampson. </author> <title> A stochastic approach to parsing. </title> <booktitle> In Proceedings of COL-ING 1986, </booktitle> <address> Bonn, </address> <year> 1986. </year> <title> Transformation-Based Error-Driven Parsing 13 </title>
Reference-contexts: This work was supported by DARPA and AFOSR jointly under grant No. AFOSR-90-0066, and by ARO grant No. DAAL 03-89-C0031 PRI. 1 Not in the traditional sense of the term. parse new text. <ref> [Sam86] </ref> defines a function to score the quality of parse trees, and then uses simulated annealing to heuristically explore the entire space of possible parses for a given sentence. In [BM92a], distributional analysis techniques are applied to a large corpus to learn a context-free grammar.
Reference: [SJM90] <author> R. Sharman, F. Jelinek, and R. Mercer. </author> <title> Generating a grammar for statistical training. </title> <booktitle> In Proceedings of the 1990 Darpa Speech and Natural Language Workshop, </booktitle> <year> 1990. </year>
Reference-contexts: A number of recent papers have explored the potential of using the inside-outside algorithm to automatically learn a grammar <ref> [LY90, SJM90, PS92, BW92, CC92, SRO93] </ref>. Below, we describe a new technique for grammar induction. The algorithm works by beginning in a very naive state of knowledge about phrase structure.

References-found: 18

