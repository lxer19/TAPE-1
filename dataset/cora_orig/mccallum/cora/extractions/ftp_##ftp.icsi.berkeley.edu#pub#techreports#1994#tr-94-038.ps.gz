URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1994/tr-94-038.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1994.html
Root-URL: http://www.icsi.berkeley.edu
Title: MBP on T0: mixing floating- and fixed-point formats in BP learning  
Phone: (510) 643-9153 FAX (510) 643-7684  
Author: Davide Anguita Benedict A. Gomes 
Date: August 1994  
Address: I 1947 Center St. Suite 600 Berkeley, California 94704-1198  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  International Computer Science Institute (ICSI), Berkeley, USA Department of Biophysical and Electronic Engineering, University of Genova, Italy Dept. of Electrical Engineering and Computer Science, University of California, Berkeley, USA  
Pubnum: TR-94-038  
Abstract: We examine the efficient implementation of back prop type algorithms on T0 [4], a vector processor with a fixed point engine, designed for neural network simulation. A matrix formulation of back prop, Matrix Back Prop [1], has been shown to be very efficient on some RISCs [2]. Using Matrix Back Prop, we achieve an asymptotically optimal performance on T0 (about 0.8 GOPS) for both forward and backward phases, which is not possible with the standard on-line method. Since high efficiency is futile if convergence is poor (due to the use of fixed point arithmetic), we use a mixture of fixed and floating point operations. The key observation is that the precision of fixed point is sufficient for good convergence, if the range is appropriately chosen. Though the most expensive computations are implemented in fixed point, we achieve a rate of convergence that is comparable to the floating point version. The time taken for conversion between fixed and floating point is also shown to be reasonable. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D.Anguita, G.Parodi, and R.Zunino. </author> <title> An Efficient Implementation of BP on RISC-based Workstations. </title> <type> Neurocomputing 6, </type> <year> 1994. </year>
Reference-contexts: We show here a mixed floating- fixed-point implementation of Matrix Back Propagation (MBP) <ref> [1] </ref> on T0 that isolates the most computationally expensive steps of the algorithm and implements them efficiently in fixed-point format. Other parts of the algorithm with less demand in terms of computational power but with more critical needs in terms of accuracy are implemented in conventional floating-point format. <p> It has already been shown <ref> [1, 2] </ref> that with a relative small effort these constraints can be satisfied reasonably well on some RISCs.
Reference: [2] <author> D.Anguita. </author> <title> Matrix Back Propagation. </title> <type> Technical Report, </type> <institution> DIBE, University of Genova, Italy, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: It has already been shown <ref> [1, 2] </ref> that with a relative small effort these constraints can be satisfied reasonably well on some RISCs.
Reference: [3] <author> D.Anguita, M.Pampolini, G.Parodi, and R.Zunino. YPROP: </author> <title> Yet Another Accelerating Technique for the Back-Propagation. </title> <booktitle> ICANN '93, </booktitle> <address> Amsterdam, The Netherlands, </address> <year> 1993. </year>
Reference-contexts: Section 4 describes some experiments with the mixed model approach. In Section 5 some comments on the algorithm are presented. 2 Matrix Back Propagation In Table 1 the MBP algorithm is summarized. It can be used to represent several batch/block learning algorithms with adaptive step and momentum <ref> [29, 3, 27] </ref>. We are interested in finding the most computational expensive part of the algorithm, so we will use the left column of Table 1 that shows the number of operations needed by each step of MBP.
Reference: [4] <author> K.Asanovic. </author> <title> T0 Reference Manual. Internal document, </title> <institution> International Computer Science Institute and UC Berkeley, </institution> <year> 1993. </year>
Reference-contexts: This operation is not directly implemented on T0 and needs ~ 20 cycles for a vector of V L = 32 words. This problem is known and could be eventually solved in future releases of the processor <ref> [4] </ref>. In any case, the overhead due to the absence of the dot-product is not particularly annoying when dealing with matrix products: in fact, partial dot-products of length V L can be kept in vector registers and the final result can be computed at the end of the inner loop.
Reference: [5] <author> K.Asanovic. </author> <title> Torrent Architecture Manual. Internal document, </title> <institution> International Computer Science Institute and UC Berkeley, </institution> <year> 1993. </year>
Reference-contexts: T0 belongs to this family of processors and it will be the first implementation of the Torrent architecture <ref> [5] </ref>. It is tailored for neural-networks calculations and inherits some of the features of a previous neuro-processor [30]. The next implementation (T1) will be the building block for a massively parallel computer [6].
Reference: [6] <author> K.Asanovic, J.Beck, T.Callahan, J.Feldman, B.Irissou, B.Kingsbury, P.Kohn, J.Lazzaro, N.Morgan, D.Stoutamire and J.Wawrzynek. </author> <title> CNS-1 Architecture Specification. </title> <type> ICSI Technical Report TR-93-021, </type> <month> April </month> <year> 1993. </year>
Reference-contexts: It is tailored for neural-networks calculations and inherits some of the features of a previous neuro-processor [30]. The next implementation (T1) will be the building block for a massively parallel computer <ref> [6] </ref>. The common characteristic of these processors is the use of a fixed-point engine, typically 16-bits wide or less, for fast computation.
Reference: [7] <author> K.Asanovic and N.Morgan. </author> <title> Experimental Determination of Precision Requirements for Back-Propagation Training of Artificial Neural Networks. </title> <booktitle> In Proc. of 2nd Int. Conf. on Microeletronics for Neural Networks, </booktitle> <address> Munich, Germany, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: This is not a new problem, in fact both analog and digital implementations of neural networks suffer from some constraint due to physical limitations. For this reason, the effect of discretization on feed-forward networks and back-propagation learning received some attention shortly after the introduction of the algorithm <ref> [9, 7, 15] </ref>. Most of the results indicate that a representation of 16 bits for the fixed-point format is reliable enough to obtain reasonable results with on-line backpropagation.
Reference: [8] <author> H.Boulard and Y.Kamp. </author> <title> Auto-association by Multilayer Perceptrons and Singular Value Decomposition. </title> <journal> Byological Cybernetics, </journal> <volume> No. 59, </volume> <year> 1988. </year>
Reference-contexts: The dependency from size of the training set is controlled by the number of neurons of the output layer (N L ), so we expect better performances when dealing with networks with a small number of outputs (e.g. classification problems, as opposed to encoding problems <ref> [8] </ref>). Furthermore, some techniques to reduce the number of output neurons in classification problems are available in the literature [20]. There is also an explicit dependency in the first order term (20) on the number of layers of the network (L).
Reference: [9] <author> D.D.Caviglia, M.Valle, G.M.Bisio. </author> <title> Effect of Weight Discretization on the Back Propagation Learning Method: Algorithm Design and Hardware Realization. </title> <booktitle> IJCNN 90, </booktitle> <address> San Diego, USA, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: This is not a new problem, in fact both analog and digital implementations of neural networks suffer from some constraint due to physical limitations. For this reason, the effect of discretization on feed-forward networks and back-propagation learning received some attention shortly after the introduction of the algorithm <ref> [9, 7, 15] </ref>. Most of the results indicate that a representation of 16 bits for the fixed-point format is reliable enough to obtain reasonable results with on-line backpropagation.
Reference: [10] <author> A.Corana, C.Rolando, S.Ridella. </author> <title> A Highly Efficient Implementation of Backpropagation Algorithm on SIMD Computers. In High Performance Computing, </title> <editor> J.- L.Delhaye and E.Gelenbe (Eds.), </editor> <publisher> Elsevier, </publisher> <year> 1989. </year>
Reference-contexts: The computation of the activation function is quite expensive if it is done using the floating-point math library <ref> [10] </ref>, and it would cause a large penalty on T0 due to the absence of a FPU. <p> Then the number of cycles for these steps of the algorithm will be n flp cycles = k f n flp op with k f 2 <ref> [10; 50] </ref>. <p> We assume k 1 = 6 <ref> [10] </ref> to compute n op and the worst-case for floating-point and conversion routines on T0 (k f = 50, k fx = 4, k xf = 3) to compute n cycles .
Reference: [11] <author> J.Dongarra. </author> <title> Linear Algebra Library for High-Performance Computers. </title> <booktitle> Frontiers of Supercomputing II. </booktitle> <editor> K.R.Ames and A.Brenner (Eds.), </editor> <publisher> University of California Press, </publisher> <year> 1994. </year>
Reference-contexts: We will show here that dealing with matrices is, in general, more efficient from the computational point of view on the vast majority of architectures <ref> [11] </ref>. On the other hand, it has the disadvantage of requiring more memory.
Reference: [12] <author> E.Fiesler, A.Choudry, and H.J.Caulfield. </author> <title> A Universal Weight Discretization Method for Multi-Layer Neural Networks. </title> <note> To appear in IEEE Trans. on SMC. </note>
Reference-contexts: One solution to overcome the limitations of a BP implementation is to mix conventional floating-point operations with fixed-point operations when required. An example of this approach is <ref> [12] </ref> where the feed-forward and the backward phase are done in fixed- and floating-point format respectively.
Reference: [13] <author> D.Hammerstrom. </author> <title> A VLSI architecture for High-Performance, Low-Cost, On-Chip Learning. </title> <booktitle> IJCNN 90, </booktitle> <address> S.Diego, USA, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Most of these dedicated processors are oriented to the efficient execution of various learning algorithms with a strong accent on back-propagation. Some of the best-known processors in this field are CNAPS <ref> [13] </ref>, GENES-IV [18] and MA-16 [21]: they are the building blocks for larger system that exploit massive parallelism to achieve performances orders of magnitude greater than conventional workstations [22, 28]. T0 belongs to this family of processors and it will be the first implementation of the Torrent architecture [5].
Reference: [14] <institution> M.Hoehfeld and S.E.Fahlman Learning with Numerical Precision Using the Cascade-Correlation Algorithm. </institution> <note> IEEE Trans. on NN, Vol.3, No.4, </note> <month> July </month> <year> 1992. </year>
Reference-contexts: Most of the results indicate that a representation of 16 bits for the fixed-point format is reliable enough to obtain reasonable results with on-line backpropagation. On the other hand, despite this general agreement, there has been some effort to reduce the precision needed during the computation <ref> [14, 24] </ref>, mainly because the effect of the discretization during the learning is not completely understood and it seems to be both problem and algorithm dependent. One solution to overcome the limitations of a BP implementation is to mix conventional floating-point operations with fixed-point operations when required.
Reference: [15] <author> P.W.Hollis, J.S.Harper, and J.J.Paulos. </author> <title> The effect of precision constraints in a backpropagation learning network. Neural Computation, </title> <type> Vol.2, No.3, </type> <year> 1990. </year> <month> 15 </month>
Reference-contexts: This is not a new problem, in fact both analog and digital implementations of neural networks suffer from some constraint due to physical limitations. For this reason, the effect of discretization on feed-forward networks and back-propagation learning received some attention shortly after the introduction of the algorithm <ref> [9, 7, 15] </ref>. Most of the results indicate that a representation of 16 bits for the fixed-point format is reliable enough to obtain reasonable results with on-line backpropagation.
Reference: [16] <author> N.Kambhatla and T.K.Leen. </author> <title> Fast Non-Linear Dimension Reduction. NIPS 6, </title> <editor> J.D.Cowan, G.Tesauro and J.Alspector (Eds.), </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: However, this is not a common case, as the number of layers is seldom greater than four in practical applications (see, for example, <ref> [16] </ref> for a real problem that requires such an architecture).
Reference: [17] <author> G.Kane, J.Heinrich. </author> <title> MIPS RISC architecture. </title> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: The next implementation (T1) will be the building block for a massively parallel computer [6]. The common characteristic of these processors is the use of a fixed-point engine, typically 16-bits wide or less, for fast computation. In particular, T0 is composed of a standard MIPS RISC engine <ref> [17] </ref> with no floating-point unit and a fixed-point vector unit that can execute up to two operations per cycle on 8-word vectors or, in other words, compute 16 results in a single cycle.
Reference: [18] <author> P.Ienne and M.A.Viredaz. </author> <title> GENES IV: A Bit-Serial Processing Element for a Multi-Model Neural-Network Accelerator. </title> <note> available in Neuroprose. </note>
Reference-contexts: Most of these dedicated processors are oriented to the efficient execution of various learning algorithms with a strong accent on back-propagation. Some of the best-known processors in this field are CNAPS [13], GENES-IV <ref> [18] </ref> and MA-16 [21]: they are the building blocks for larger system that exploit massive parallelism to achieve performances orders of magnitude greater than conventional workstations [22, 28]. T0 belongs to this family of processors and it will be the first implementation of the Torrent architecture [5].
Reference: [19] <author> M.Moller. </author> <title> Supervised Learning on Large Redundant Training Sets. </title> <journal> Int. J. of Neural Systems, Vol.4, </journal> <volume> No.1, </volume> <year> 1993. </year>
Reference-contexts: It has been suggested that the possible poor performance of bE methods is directly related to the redundancy of the data in the training set <ref> [19] </ref>, therefore it can be worthwhile to start the learning with a subset of the training set and increase its dimension as the learning proceed. Using this technique, a bE method is usually faster in convergence than a bP method (see [19] for more details). <p> the redundancy of the data in the training set <ref> [19] </ref>, therefore it can be worthwhile to start the learning with a subset of the training set and increase its dimension as the learning proceed. Using this technique, a bE method is usually faster in convergence than a bP method (see [19] for more details). This technique can be seen as a bB method that can be implemented through MBP; the computational efficiency will grow as the learning proceed due to the increase of the learning set dimension.
Reference: [20] <author> N.Morgan and H.Boulard. </author> <title> Factoring Networks by a Statistical Method. Neural Computation, </title> <type> Vol.4, </type> <institution> No.6, </institution> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: Furthermore, some techniques to reduce the number of output neurons in classification problems are available in the literature <ref> [20] </ref>. There is also an explicit dependency in the first order term (20) on the number of layers of the network (L). This term is of small importance being of first order, but we can expect an increase of overhead in networks with a very large number of layers.
Reference: [21] <editor> U.Ramacher et al. (Eds). </editor> <booktitle> VLSI Design of Neural Networks. </booktitle> <publisher> Kluwer Academic, </publisher> <year> 1991. </year>
Reference-contexts: Most of these dedicated processors are oriented to the efficient execution of various learning algorithms with a strong accent on back-propagation. Some of the best-known processors in this field are CNAPS [13], GENES-IV [18] and MA-16 <ref> [21] </ref>: they are the building blocks for larger system that exploit massive parallelism to achieve performances orders of magnitude greater than conventional workstations [22, 28]. T0 belongs to this family of processors and it will be the first implementation of the Torrent architecture [5].
Reference: [22] <editor> U.Ramacher et al. SYNAPSE-X: a general-purpose neurocomputer. </editor> <booktitle> Proc. of the 2nd Int. Conf. on Microelectronics for Neural Networks, </booktitle> <address> Munich, Germany, </address> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: Some of the best-known processors in this field are CNAPS [13], GENES-IV [18] and MA-16 [21]: they are the building blocks for larger system that exploit massive parallelism to achieve performances orders of magnitude greater than conventional workstations <ref> [22, 28] </ref>. T0 belongs to this family of processors and it will be the first implementation of the Torrent architecture [5]. It is tailored for neural-networks calculations and inherits some of the features of a previous neuro-processor [30].
Reference: [23] <author> S.Renals and N.Morgan. </author> <title> Connectionist Probability Estimation in HMM Speech Recognition. </title> <type> ICSI Technical Report TR-92-081, </type> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: Table 4: Some real-world applications. Name Network Size Description N 0 N 1 N 2 N 3 NETtalk [25] 203 80 26 Pronunciation of text. Neurogammon [26] 459 24 24 1 Backgammon player. Speech <ref> [23] </ref> 234 1000 69 Speech recognition. performance is attained (n 1 2 ) is reasonably small (N P ~ 500). problem with different ranges of the fixed-point variables. In particular, EXPbp is the exponent of the most significant digit of the fixed-point variables in the backward phase.
Reference: [24] <author> S.Sakaue, T.Kohda, H.Yamamoto, S.Maruno, and Y.Shimeki. </author> <title> Reduction of Required Precision Bits for Back-Propagation Applied to Pattern Recognition. </title> <journal> IEEE Trans. on NN, Vol.4, No.2, </journal> <month> March </month> <year> 1993. </year>
Reference-contexts: Most of the results indicate that a representation of 16 bits for the fixed-point format is reliable enough to obtain reasonable results with on-line backpropagation. On the other hand, despite this general agreement, there has been some effort to reduce the precision needed during the computation <ref> [14, 24] </ref>, mainly because the effect of the discretization during the learning is not completely understood and it seems to be both problem and algorithm dependent. One solution to overcome the limitations of a BP implementation is to mix conventional floating-point operations with fixed-point operations when required.
Reference: [25] <author> T.J.Sejnowsky and C.R.Rosenberg. </author> <title> Parallel Networks that Learn to Pronounce English Text. </title> <journal> Complex Systems 1, </journal> <year> 1987. </year>
Reference-contexts: For this reason we show here the performance of MBP on T0 with networks that have been used in some real-world applications (Table 4). Table 4: Some real-world applications. Name Network Size Description N 0 N 1 N 2 N 3 NETtalk <ref> [25] </ref> 203 80 26 Pronunciation of text. Neurogammon [26] 459 24 24 1 Backgammon player. Speech [23] 234 1000 69 Speech recognition. performance is attained (n 1 2 ) is reasonably small (N P ~ 500). problem with different ranges of the fixed-point variables.
Reference: [26] <author> G.Tesauro and B.Janssens. </author> <title> A Neural Network That Learns to Play Backgammon. NIPS, </title> <editor> D.Z.Anderson (Ed.), </editor> <year> 1988. </year>
Reference-contexts: Table 4: Some real-world applications. Name Network Size Description N 0 N 1 N 2 N 3 NETtalk [25] 203 80 26 Pronunciation of text. Neurogammon <ref> [26] </ref> 459 24 24 1 Backgammon player. Speech [23] 234 1000 69 Speech recognition. performance is attained (n 1 2 ) is reasonably small (N P ~ 500). problem with different ranges of the fixed-point variables.
Reference: [27] <author> T.Tollenaere. SuperSAB: </author> <title> fast adaptive back propagation with good scaling properties. Neural Networks, </title> <type> Vol.3, </type> <institution> No.5, </institution> <year> 1990. </year>
Reference-contexts: Section 4 describes some experiments with the mixed model approach. In Section 5 some comments on the algorithm are presented. 2 Matrix Back Propagation In Table 1 the MBP algorithm is summarized. It can be used to represent several batch/block learning algorithms with adaptive step and momentum <ref> [29, 3, 27] </ref>. We are interested in finding the most computational expensive part of the algorithm, so we will use the left column of Table 1 that shows the number of operations needed by each step of MBP.
Reference: [28] <author> M.A.Viredaz. MANTRA I: </author> <title> An SIMD Processor Array for Neural Computation. </title> <type> Euro-ARCH 93, </type> <institution> Munchen, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: Some of the best-known processors in this field are CNAPS [13], GENES-IV [18] and MA-16 [21]: they are the building blocks for larger system that exploit massive parallelism to achieve performances orders of magnitude greater than conventional workstations <ref> [22, 28] </ref>. T0 belongs to this family of processors and it will be the first implementation of the Torrent architecture [5]. It is tailored for neural-networks calculations and inherits some of the features of a previous neuro-processor [30].
Reference: [29] <author> T.P.Vogl, J.K.Mangis, A.K.Rigler, W.T.Zink, and D.L.Alkon. </author> <title> Accelerating the Cover-gence of the Back-Propagation Method. </title> <booktitle> Biological Cybernetics 59, </booktitle> <year> 1989. </year>
Reference-contexts: Section 4 describes some experiments with the mixed model approach. In Section 5 some comments on the algorithm are presented. 2 Matrix Back Propagation In Table 1 the MBP algorithm is summarized. It can be used to represent several batch/block learning algorithms with adaptive step and momentum <ref> [29, 3, 27] </ref>. We are interested in finding the most computational expensive part of the algorithm, so we will use the left column of Table 1 that shows the number of operations needed by each step of MBP.
Reference: [30] <author> J.Wawrzynek, K.Asanovic, and N.Morgan. </author> <title> The Design of a Neuro-Microprocessor. </title> <journal> IEEE Trans. on NN, Vol.4, </journal> <volume> No.3, </volume> <month> May </month> <year> 1993. </year> <month> 16 </month>
Reference-contexts: T0 belongs to this family of processors and it will be the first implementation of the Torrent architecture [5]. It is tailored for neural-networks calculations and inherits some of the features of a previous neuro-processor <ref> [30] </ref>. The next implementation (T1) will be the building block for a massively parallel computer [6]. The common characteristic of these processors is the use of a fixed-point engine, typically 16-bits wide or less, for fast computation.
References-found: 30

