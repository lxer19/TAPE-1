URL: http://www.cs.rice.edu/~aron/papers/atm-xsim.ps
Refering-URL: http://www.cs.rice.edu:80/~aron/research.html
Root-URL: 
Title: Techniques for Efficient Cell-Level ATM Simulations  
Author: Mohit Aron Lawrence Brakmo 
Address: TR98-316  
Affiliation: Department of Computer Science Rice University  Western Research Laboratory Digital Equipment Corporation  Department of Computer Science Rice University  
Abstract: This paper investigates the efficiency of a network simulator in simulating ATM networks with UBR service and compares it to the the efficiency of simulating conventional packet networks. A network simulator can be highly inefficient for cell-level ATM simulations as compared to analogous packet network simulations. We motivate and describe three techniques for improving the efficiency of ATM simulations. These techniques afford an increase in efficiency of 29-34% for ATM simulations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Atkinson. </author> <title> Default IP MTU for use over ATM AAL5. </title> <type> IETF Internet Draft, </type> <month> Feb. </month> <year> 1994. </year> <note> Available by anonymous ftp from ds.internic.net:internet-drafts/internet-drafts/draft-ietf-atm-mtu-07.txt. </note>
Reference-contexts: Whenever a cell is dropped due to switch-buffer overflow, the original AAL5 frame is located by looking up the hash table (as explained in Section 4.1). The dropFlag attribute is set and the numInactive attribute is incremented. Similarly, 8 The default MTU of IP over ATM is 9K bytes <ref> [1] </ref> which would be transmitted over the network in about 193 ATM cells. 9 when an uncorrupted cell is received by the AAL5 at the destination, the numInactive is incremented. <p> Our simulator does support these enhancements, but as this paper does not focus on TCP performance aspects, all simulations reported use switches with the FCFS queuing discipline. The TCP segment size was chosen based on the default MTU of IP over ATM which is 9180 bytes <ref> [1] </ref>. 5.3 Results In this subsection, we present results showing the impact of our improvements on the efficiency of the simulator. We present some of our experiments on the network configuration shown in Figure 8.
Reference: [2] <author> L. S. Brakmo and L. L. Peterson. </author> <title> Experiences with Network Simulation. </title> <booktitle> In Proceedings of the ACM SIGMETRICS '96 Conference, </booktitle> <year> 1996. </year>
Reference-contexts: However, cell-level simulations with a faithful implementation of the ATM protocol on a general network simulator can be very inefficient as compared to corresponding simulations involving packet network technologies. Our study is based on our experiences with ATM simulations on the x-sim <ref> [2] </ref> network simulator. We found that the simulations involving ATM networks can take upto 40 times longer than those involving packet networks for the same simulated time and analogous network topologies. <p> The simulations were done using the x-sim network simulator <ref> [2] </ref>, which is based on the x-kernel [6]. x-sim is an execution-driven network simulator, where the actions of network protocols are simulated by executing its actual protocol implementation code, rather than an abstract behavioral model of the protocol. x-sim supports multiple hosts, each running a full protocol stack, and several abstract
Reference: [3] <author> R. Brown. </author> <title> Calendar queues: A fast O(1) priority queue implementation for the simulation event set problem. </title> <journal> Communications of the ACM, </journal> <volume> 31(10) </volume> <pages> 1220-1227, </pages> <month> Oct. </month> <year> 1988. </year>
Reference-contexts: First, we propose a low overhead cell representation in the simulator. Second, we propose an efficient method for detecting corrupt AAL5 frames in the simulator. Finally, we propose improvements to the event management algorithms in the simulator. Specifically, we propose a Calendar Queue <ref> [3] </ref> based priority queue implementation enhanced with a technique called Lateral Insertion. Our first two proposals result in a slight variation to the implementation of the ATM protocol in the simulator, that captures all the essential features of a faithful implementation, yet imposes lesser computational overhead. <p> The priority queue implementation being originally used in the simulator was Timing Wheels based on Scheme 5 described in [13]. In Section 4.3.1 we describe this implementation and the problems associated with it. In Section 4.3.2, we describe a Calendar Queue <ref> [3] </ref> based priority queue. In Section 4.3.3, we present our improvements to the Calendar Queue algorithm. 10 4.3.1 Timing Wheels The timing wheels implementation in x-sim is simply an array of slots where each slot contains a list of events ordered by the time at which they are to fire. <p> Thus, get next event can impose a huge overhead when the average time separation between consecutive events is large. This effect will be quantified in Section 5. 4.3.2 Calendar Queues Calendar queues <ref> [3] </ref> aim towards providing O (1) schedule event and get next event operations by associating a width with each bucket 9 . This bucket width is computed as the average separation between the times at which consecutive events fire.
Reference: [4] <author> T. Cormen, C. Leiserson, and R. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: This function also increments the variable maintaining the simulated time to the time at which this event is to be fired. The above two operations can be modelled by a priority queue algorithm <ref> [4] </ref>. The operations schedule event and get next event are modelled by the INSERT and EXTRACT MIN operations in the priority queue respectively. The priority queue implementation being originally used in the simulator was Timing Wheels based on Scheme 5 described in [13].
Reference: [5] <author> P. Danzig and S. Jamin. tcplib: </author> <title> A Library of TCP Internetwork Characteristics. </title> <type> Technical Report CS-SYS-91-495, </type> <institution> Computer Science Department, USC, </institution> <year> 1991. </year>
Reference-contexts: direct result of the better cache locality and the reduction in the overhead of memory allocation routines (malloc and free) due to the improved ATM implementation. 5.3.2 Background Traffic Experiments We next performed simulations by running a single bulk transfer when the network is loaded with traffic generated from tcplib <ref> [5] </ref>. One of the application protocols available 17 in x-sim is called TRAFFIC it implements TCP Internet traffic based on tcplib. The configuration in Figure 8 was configured to have 1 sender doing a bulk data transfer (as in the Bulk Transfer experiments).
Reference: [6] <author> N. C. Hutchinson and L. L. Peterson. </author> <title> The x-kernel: An Architecture for Implementing Network Protocols. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1) </volume> <pages> 64-76, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: The simulations were done using the x-sim network simulator [2], which is based on the x-kernel <ref> [6] </ref>. x-sim is an execution-driven network simulator, where the actions of network protocols are simulated by executing its actual protocol implementation code, rather than an abstract behavioral model of the protocol. x-sim supports multiple hosts, each running a full protocol stack, and several abstract link behaviours (point-to-point and ethernet links). <p> Our results show that the process of segmentation and reassembly in ATM networks can also impose a high overhead on operations involved with maintaining such data structures. x-sim uses the x-kernel messages 5 <ref> [6] </ref> as the abstract data type to represent the PDU at each level in the protocol stack. In particular, the IP packet and the ATM cell are also represented by the x-kernel messages.
Reference: [7] <institution> MIPS Computer Systems, Sunnyvale, CA. </institution> <note> UMIPS-V Reference Manual (pixie and pixstats), </note> <year> 1990. </year>
Reference-contexts: We then describe the network configuration that we used for performing our simulations. Next, we present results showing the impact of our improvements on the simulator. This is followed by a discussion of the results. 5.1 Profiling Tools We used the pixie <ref> [7] </ref> and prof [10] tools available in Digital Unix to profile the simulator. The pixie utility instruments an executable with additional code that performs basic-block counting when the program is executed.
Reference: [8] <author> T. J. Ott and N. Aggarwal. </author> <title> TCP over ATM: </title> <booktitle> ABR or UBR? In Proceedings of the ACM SIGMETRICS '97 Conference, </booktitle> <address> Seattle, WA, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: In ATM, this type of service is called UBR 1 The AAL5 layer sits below IP in the protocol stack and is responsible for segmenting IP packets into ATM cells and reassembling them back into IP packets. 3 (unspecified bit rate) <ref> [8] </ref>. Figures 1 and 2 show typical protocol stacks for two hosts using a TCP connection over packet and ATM 2 networks respectively. TCP collects the data handed down to it by the application in a TCP segment (which also includes the protocol header).
Reference: [9] <author> M. Perloff and K. Reiss. </author> <title> Improvements to TCP Performance in High-Speed ATM Networks. </title> <journal> Communications of the ACM, </journal> <volume> 38(2) </volume> <pages> 90-100, </pages> <month> Feb. </month> <year> 1995. </year> <title> [10] prof. Digital Unix man page. </title>
Reference-contexts: The parameter n, 14 the link bandwidth and the applications running on individual hosts were varied with different experiments and shall be described in Section 5.3. This network configuration resembles the ones used in other studies on TCP dynamics <ref> [11, 9] </ref> but incorporates a high-delay link typical of wide-area networks. The switches used in the simulations are FCFS switches with output buffering and the drop-tail discipline for dropping packets/cells. Commercial ATM switches now support enhancements like Early Packet Discard [11] and per-VC queuing to enhance TCP performance.
Reference: [11] <author> A. Romanow and S. Floyd. </author> <title> The Dynamics of TCP Traffic over ATM Networks. </title> <journal> IEEE Journal on Selected Areas in Communication, </journal> <volume> 13(4), </volume> <month> May </month> <year> 1995. </year>
Reference-contexts: The parameter n, 14 the link bandwidth and the applications running on individual hosts were varied with different experiments and shall be described in Section 5.3. This network configuration resembles the ones used in other studies on TCP dynamics <ref> [11, 9] </ref> but incorporates a high-delay link typical of wide-area networks. The switches used in the simulations are FCFS switches with output buffering and the drop-tail discipline for dropping packets/cells. Commercial ATM switches now support enhancements like Early Packet Discard [11] and per-VC queuing to enhance TCP performance. <p> The switches used in the simulations are FCFS switches with output buffering and the drop-tail discipline for dropping packets/cells. Commercial ATM switches now support enhancements like Early Packet Discard <ref> [11] </ref> and per-VC queuing to enhance TCP performance. Our simulator does support these enhancements, but as this paper does not focus on TCP performance aspects, all simulations reported use switches with the FCFS queuing discipline.
Reference: [12] <author> W. Stevens. </author> <title> TCP/IP Illustrated Volume 1 : The Protocols. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1994. </year> <month> 20 </month>
Reference-contexts: ATM (Asynchronous Transfer Mode) technology is expected to gain widespread use both in wide-area networks and in high-speed local area networks (LANs). Therefore, it is important to study the interactions of existing Internet protocols with ATM. Our main intent was to study the performance of the TCP/IP protocol suite <ref> [12] </ref> over ATM as compared to conventional link level technologies such as Ethernet and FDDI (in the rest of the paper, we'll refer to networks using these technologies as packet networks as opposed to ATM networks which use fixed sized cells).
Reference: [13] <author> G. Varghese and A. Lauck. </author> <title> Hashed and hierarchical timing wheels: Data structures for the efficient implementation of a timer facility. </title> <booktitle> In Proceedings of the Eleventh ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 171-180, </pages> <month> Nov. </month> <year> 1987. </year>
Reference-contexts: The operations schedule event and get next event are modelled by the INSERT and EXTRACT MIN operations in the priority queue respectively. The priority queue implementation being originally used in the simulator was Timing Wheels based on Scheme 5 described in <ref> [13] </ref>. In Section 4.3.1 we describe this implementation and the problems associated with it. In Section 4.3.2, we describe a Calendar Queue [3] based priority queue.
References-found: 12

