URL: http://http.cs.berkeley.edu/~hari/papers/efs.ps
Refering-URL: http://http.cs.berkeley.edu/~hari/papers/efs_stuff.html
Root-URL: 
Title: File System Logging Versus Clustering: A Performance Comparison small files, both systems provide comparable read
Author: Margo Seltzer, Keith A. Smith Hari Balakrishnan, Jacqueline Chang, Sara McMains, Venkata Padmanabhan 
Note: For  
Address: Berkeley  
Affiliation: Harvard University  University of California,  
Abstract: The Log-structured File System (LFS), introduced in 1991 [8], has received much attention for its potential order-of-magnitude improvement in file system performance. Early research results [9] showed that small file performance could scale with processor speed and that cleaning costs could be kept low, allowing LFS to write at an effective bandwidth of 62 to 83% of the maximum. Later work showed that the presence of synchronous disk operations could degrade performance by as much as 62% and that cleaning overhead could become prohibitive in transaction processing workloads, reducing performance by as much as 40% [10]. The same work showed that the addition of clustered reads and writes in the Berkeley Fast File System [6] (FFS) made it competitive with LFS in large-file handling and software development environments as approximated by the Andrew benchmark [4]. These seemingly inconsistent results have caused confusion in the file system research community. This paper presents a detailed performance comparison of the 4.4BSD Log-structured File System and the 4.4BSD Fast File System. Ignoring cleaner overhead, our results show that the order-of-magnitude improvement in performance claimed for LFS applies only to meta-data intensive activities, specifically the creation of files one-kilobyte or less and deletion of files 64 kilobytes or less. Both LFS and FFS can suffer performance degradation, due to cleaning and disk fragmentation respectively. We find that active FFS file systems function at approximately 85-95% of their maximum performance after two to three years. We examine LFS cleaner performance in a transaction processing environment and find that cleaner overhead reduces LFS performance by more than 33% when the disk is 50% full. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Baker, M., Hartman, J., Kupfer, M., Shirriff, K., Ousterhout, J., </author> <title> Measurements of a Distributed File System, </title> <booktitle> Proceedings of the 13th Symposium on Operating System Principles, </booktitle> <address> Pacific Grove, CA, </address> <month> October </month> <year> 1991, </year> <pages> 192-212. </pages>
Reference-contexts: Such concerns should be mitigated, however, by many of the results of these tests. Although the greatest performance differences between a real file system and an empty one were almost 30%, most file systems showed far smaller differences, especially for large files. According to Baker et al. <ref> [1] </ref>, over half the bytes transferred to/from disk come from files over 1 MB in size. The measured performance differences for such files is less than 15% and in 70% of our tests, large files performed at 95% or better on the copied file systems.
Reference: [2] <author> Blackwell, T., Harris, J., Seltzer, M., </author> <title> Heuristic Cleaning Algorithms in LFS, </title> <booktitle> Proceedings of the 1995 Usenix Technical Conference, </booktitle> <address> New Orleans, LA, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: There is more work to be done. The effects of cleaning on LFS in other environments are still not fully understood. Trace analysis indicates that in a network of workstations environment, there may be sufficient idle time that cleaning can be accomplished without I/O penalty <ref> [2] </ref>. 7 Availability The file system and benchmark source code and trace data are available and freely redistributable. Send electronic mail to margo@das.harvard.edu. 8 Acknowledgments Many people helped make this paper possible.
Reference: [3] <author> Ganger, G., Patt, Y., </author> <title> Metadata Update Performance in File Systems, </title> <booktitle> Proceedings of the First Usenix Symposium on Operating System Design and Implementation, </booktitle> <address> Monterey, CA, </address> <month> November, </month> <year> 1994, </year> <pages> 49-60. </pages>
Reference-contexts: There are several alternatives for providing asynchronous meta-data operations, including journaling file systems [4] and ordering updates. (Using an ordered update approach, Ganger reports a factor of five to six improvement in FFS meta-data update performance <ref> [3] </ref>.) When LFS cleaner overhead is ignored, and FFS runs on a new, unfragmented file system, each file system has regions of performance dominance. LFS is an order of magnitude faster on small file creates and deletes. The systems are comparable on creates of large files (one-half megabyte or more).
Reference: [4] <author> Howard, J., Kazar, Menees, S., Nichols, D., Saty-anarayanan, M., Sidebotham, N., West, M., </author> <title> Scale and Performance in a Distributed File System, </title> <booktitle> ACM Transaction on Computer Systems 6, </booktitle> <month> 1 (February </month> <year> 1988), </year> <pages> 51-81. </pages>
Reference-contexts: In contrast, each time a creat system call returns, FFS guarantees that the file has been created and will exist after a system crash. The journaling file systems <ref> [4] </ref> avoid the synchronous writes of FFS by logging all meta-data operations to an auxiliary data structure, replacing multiple, random, synchronous I/Os with a single sequential one. <p> This improvement comes in part from LFSs disk layout, and in part from the asynchronous implementation of these operations. There are several alternatives for providing asynchronous meta-data operations, including journaling file systems <ref> [4] </ref> and ordering updates. (Using an ordered update approach, Ganger reports a factor of five to six improvement in FFS meta-data update performance [3].) When LFS cleaner overhead is ignored, and FFS runs on a new, unfragmented file system, each file system has regions of performance dominance.
Reference: [5] <author> Lieberman, H., Hewitt, C., </author> <title> A real-time garbage collector based on the lifetimes of objects, </title> <journal> Communications of the ACM, </journal> <volume> 26, 6, </volume> <year> 1983, </year> <pages> 419-429. </pages>
Reference-contexts: However, it must maintain large contiguous regions of free space on disk, called segments. LFS uses a generational garbage collector <ref> [5] </ref>, called the cleaner, to regenerate large free extents. If there is available disk space, the cleaner can always coalesce that free space to produce clean segments.
Reference: [6] <author> McKusick, M.Joy, W., Lefer, S., Fabry, R. </author> <title> A Fast File System for UNIX, </title> <journal> ACM Transactions on Computer Systems 2, </journal> <month> 3 (August </month> <year> 1984), </year> <pages> 181-197. </pages>
Reference-contexts: To understand the performance for this test, we must deduce the rotdelay setting for the Rosenblum benchmarks. Typically, rotdelay is set to optimize write performance so that a disk revolution is not lost between successive contiguous blocks <ref> [6] </ref>. Sun-FFS obtains approximately 40% of the possible disk bandwidth when writing sequentially. This is consistent with a rotdelay of one block. In BSD-FFS, our disks were sufficiently faster that we had to use a rotdelay of two blocks to avoid missing a rotation on every write.
Reference: [7] <author> McVoy, L., Kleiman, S., </author> <title> Extent-like Performance from a UNIX File System, </title> <booktitle> Proceedings of the 1990 Summer Usenix, </booktitle> <address> Anaheim, CA, </address> <month> June </month> <year> 1990, </year> <pages> 137-144. </pages>
Reference-contexts: Table 2 shows the relevant parameters and their scale factors. In the following discussion, we refer to the two file systems Rosenblum studied: the Sprite Log-Structured File System (Sprite-LFS) and the default Sun file system without clustering in effect (Sun-FFS) <ref> [7] </ref>, and the three file systems we have studied: the BSD Log-Structured File System (BSD-LFS), the BSD Fast File System with maxcontig of one so that clustering is not in effect (BSD-FFS-m1r2), and the BSD Fast File System with maxcontig of eight (BSD-FFS-m8r2). <p> Its write performance is as good as Sprites and it generally outperforms Sprite for reading due to the aggressive read-ahead and clustered I/O. Sun-FFS is a faithful implementation of the 4BSD FFS. The clustering modifications in 4.4BSD are a reimplementation of the modifications described by McVoy <ref> [7] </ref>. With these points established, we now compare the performance of BSD-LFS to that of the 4.4BSD FFS with these clustering enhancements. <p> In contrast, FFS makes no assumptions about the layout of free space on the file system. FFS uses whatever free space is available on the disk, contiguous or not. In fact, the block allocation policy of FFS remained unchanged when clustering was added <ref> [7] </ref>. FFS may not allocate contiguous blocks to a file, even when contiguous free space is available. As with the LFS cleaner, this may adversely effect performance. The fragmentation of free space in an FFS may increase with time or with file system utilization. <p> All of the file servers were SparcStations running SunOS 4.1.3. Although the operating system is substantially different than the 4.4BSD operating system used elsewhere in this study, the file systems are nearly identical. (The BSD-FFS clustering enhancements were based on those originally implemented in SunOS <ref> [7] </ref>.) Our data collection consisted of daily snapshots recorded for every file system under study.
Reference: [8] <author> Rosenblum, M., Ousterhout, J., </author> <title> The LFS Storage Manager, </title> <booktitle> Proceedings of the 1990 Summer Usenix, </booktitle> <address> Anaheim, CA, </address> <month> June </month> <year> 1990, </year> <pages> 315-324. </pages>
Reference: [9] <author> Rosenblum, M., Ousterhout, J., </author> <title> The Design and Implementation of a Log-Structured File System, </title> <journal> ACM Transactions on Computer Systems 10, </journal> <month> 1 (February </month> <year> 1992), </year> <pages> 26-52. </pages>
Reference-contexts: The early work in log-structured file systems focused on how to build such file systems. The key issues were providing efficient reads in a file system that was written sequentially and maintaining large contiguous regions on disk. The seminal work on log-structured file systems <ref> [9] </ref> showed how conventional file system structures could be implemented in an LFS and how the combination of a segmented log and a cleaner process (garbage collector) could be used to maintain large, contiguous regions of disk space. <p> This does not affect write performance, but improves read performance when files are read in their creation order. To show that BSD-LFS is a faithful implementation of a log-structured file system, we have run the benchmarks described by Rosenblum and Ousterhout <ref> [9] </ref> and compared our results to those reported for Sprite-LFS. The two hardware configurations are shown in Table 1. In each benchmark presented in this section, we scale the Sprite measurements so that the performance of the critical components match those of the BSD configuration. <p> Unfortunately, LFS cannot clean at the optimal rate described above. First, the transaction response would be unacceptably slow while the cleaner stopped for six minutes to clean. Secondly, the calculations above assumed that the disk is read sequentially. Since the selection of segments is based on Rosenblums cost-benefit algorithm <ref> [9] </ref>, there is no guarantee that collections of segments being cleaned will be contiguous on disk. Thirdly, the history file in the benchmark grows by 50 bytes per transaction, so file system utilization increases during the test run.
Reference: [10] <author> Seltzer, M., Bostic, K., McKusick., M., Staelin, C., </author> <title> The Design and Implementation of the 4.4BSD Log-Structured File System, </title> <booktitle> Proceedings of the 1993 Winter Usenix, January 1993, </booktitle> <address> San Diego, CA. </address>
Reference-contexts: The performance results reported long-term cleaning summaries (e.g. number of segments cleaned and average utilization of cleaned segments) and micro-benchmarks that demonstrated the strengths of LFS. The paper by Seltzer et al. <ref> [10] </ref> discussed design modifications necessary to incorporate LFS into a BSD framework. The performance analysis presented there focused on areas not covered by Rosenblum and Ousterhout, with an emphasis on workloads that stressed the cleaning capabilities of LFS. <p> the key issue is the cost of cleaning. 2 Validation of 4.4BSD-LFS The system under study in this work is the 4.4BSD-Lite operating system with implementations of the fast file system (FFS) and the BSD log-structured file system (BSD-LFS), both of which have been improved since the study by Seltzer <ref> [10] </ref>. Specifically, the read-ahead code used by both FFS and LFS has been largely rewritten. Fragments have been added to LFS. <p> The introduction of the cleaner changes the results substantially. At a file system utilization of 48%, LFS performance is comparable to FFS, yielding a performance degradation of 34% due to the cleaner. As the disk becomes more full, the impact increases to the approximately 41% degradation, as observed in <ref> [10] </ref>.
Reference: [11] <author> Seltzer, M., </author> <title> Transaction Support in a Log-Structured File System, </title> <booktitle> Proceedings of the Ninth International Conference on Data Engineering, </booktitle> <address> Vienna, Austria, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: The next section presents a more demanding workload, the TPC-B transaction processing benchmark. 4 Transaction Processing Performance Although LFS was designed for a UNIX time-sharing workload, there has been speculation that the ability to convert small, random I/Os into large sequential ones would make it ideal for transaction processing <ref> [11] </ref>. Seltzer et al. measured a modified TPC-B implementation and found that the cleaning overhead severely limited its performance. The disk was 80% full in the benchmark configuration.
Reference: [12] <author> Smith, K. A., Seltzer., M, </author> <title> File Layout and File System Performance, </title> <institution> Harvard Division of Applied Sciences Technical Report, </institution> <year> 1994. </year>
Reference-contexts: Subtract nine months to obtain the age at the beginning of the study. The ncg and bpg columns indicate the number of cylinder groups and the number of file system blocks per cylinder group, respectively. 5.2 Data Analysis A separate study performed extensive analysis of this snapshot data <ref> [12] </ref>. Examining this data in conjunction with the FFS block allocation algorithm provided a variety of interesting information characterizing the layout of FFS file systems. Some of the important results are summarized here.
Reference: [13] <author> Transaction Processing Performance Council, </author> <title> TPC Benchmark B Standard Specification, </title> <publisher> Waterside Associates, </publisher> <address> Fremont, CA., </address> <month> August </month> <year> 1990. </year>
Reference-contexts: The benchmark configuration is identical to that described in Section 3, except that the file systems are configured with a four-kilobyte block size to match the block size of the database indexing structures. The TPC-B benchmark simulates a check-cashing application <ref> [13] </ref>. The four files in the benchmark are described in Table 3.
References-found: 13

