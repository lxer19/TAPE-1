URL: http://www.iscs.nus.sg/~rudys/newfs.ps
Refering-URL: http://ai.iit.nrc.ca/bibliographies/feature-selection.html
Root-URL: 
Email: Email:frudys,liuhg@iscs.nus.sg  
Title: Neural-Network Feature Selector  
Author: Rudy Setiono and Huan Liu 
Address: Ridge, Singapore 119260 Republic of Singapore  
Affiliation: Department of Information Systems and Computer Science National University of Singapore Kent  
Abstract: Feature selection is an integral part of most learning algorithms. Due to the existence of irrelevant and redundant attributes, by selecting only the relevant attributes of the data, higher predictive accuracy can be expected from a machine learning method. In this paper, we propose the use of a three-layer feedforward neural network to select those input attributes that are most useful for discriminating classes in a given set of input patterns. A network pruning algorithm is the foundation of the proposed algorithm. By adding a penalty term to the error function of the network, redundant network connections can be distinguished from those relevant ones by their small weights when the network training process has been completed. A simple criterion to remove an attribute based on the accuracy rate of the network is developed. The network is retrained after removal of an attribute, and the selection process is repeated until no attribute meets the criterion for removal. Our experimental results suggest that the proposed method works very well on a wide variety of classification problems. Keywords: Feedforward neural network, backpropagation, cross entropy, penalty term, network pruning, feature selection. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T.Y. Young and T.W. Calvert, </author> <title> Classification, Estimation and Pattern Recognition. </title> <address> El-sevier, New York, </address> <year> 1974. </year>
Reference-contexts: 5 11 TABLE III ATTRIBUTES OF THE IBM DATASETS Attribute Description Value salary salary uniformly distributed from 20,000 to 150,000 commission commission if salary 75000 ! commission = 0 else uniformly distributed from 10000 to 75000. age age uniformly distributed from 20 to 80. elevel education level uniformly distributed from <ref> [0; 1; : : :; 4] </ref>. car make of the car uniformly distributed from [1; 2; : : :20]. zipcode zip code of the town uniformly chosen from 9 available zipcodes. hvalue value of the house uniformly distributed from 0.5k10000 to 1.5k1000000 where k 2 f0 : : : 9g depends <p> uniformly distributed from 20,000 to 150,000 commission commission if salary 75000 ! commission = 0 else uniformly distributed from 10000 to 75000. age age uniformly distributed from 20 to 80. elevel education level uniformly distributed from [0; 1; : : :; 4]. car make of the car uniformly distributed from <ref> [1; 2; : : :20] </ref>. zipcode zip code of the town uniformly chosen from 9 available zipcodes. hvalue value of the house uniformly distributed from 0.5k10000 to 1.5k1000000 where k 2 f0 : : : 9g depends on zipcode. hyears years house owned uniformly distributed from [1; 2; : : :; <p> car uniformly distributed from [1; 2; : : :20]. zipcode zip code of the town uniformly chosen from 9 available zipcodes. hvalue value of the house uniformly distributed from 0.5k10000 to 1.5k1000000 where k 2 f0 : : : 9g depends on zipcode. hyears years house owned uniformly distributed from <ref> [1; 2; : : :; 30] </ref>. loan total amount of loan uniformly distributed from 1 to 500000. The figures in Table II show that feature selection improves significantly the predictive accuracy of the networks. Very good results are obtained for all three problems.
Reference: [2] <author> J.R. Quinlan, C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> California, </address> <year> 1993. </year>
Reference-contexts: uniformly distributed from 20,000 to 150,000 commission commission if salary 75000 ! commission = 0 else uniformly distributed from 10000 to 75000. age age uniformly distributed from 20 to 80. elevel education level uniformly distributed from [0; 1; : : :; 4]. car make of the car uniformly distributed from <ref> [1; 2; : : :20] </ref>. zipcode zip code of the town uniformly chosen from 9 available zipcodes. hvalue value of the house uniformly distributed from 0.5k10000 to 1.5k1000000 where k 2 f0 : : : 9g depends on zipcode. hyears years house owned uniformly distributed from [1; 2; : : :; <p> car uniformly distributed from [1; 2; : : :20]. zipcode zip code of the town uniformly chosen from 9 available zipcodes. hvalue value of the house uniformly distributed from 0.5k10000 to 1.5k1000000 where k 2 f0 : : : 9g depends on zipcode. hyears years house owned uniformly distributed from <ref> [1; 2; : : :; 30] </ref>. loan total amount of loan uniformly distributed from 1 to 500000. The figures in Table II show that feature selection improves significantly the predictive accuracy of the networks. Very good results are obtained for all three problems.
Reference: [3] <author> L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone, </author> <title> "Classification and regression trees," </title> <publisher> Wadsworth & Brooks, </publisher> <address> California, </address> <year> 1984. </year>
Reference: [4] <author> K.J. Lang and M.J. Witbrock, </author> <title> "Learning to tell two spirals apart", </title> <booktitle> in Proc. of the 1988 Connectionist Model Summer School, </booktitle> <editor> D. Touretzky, G. Hinton & T. Sejnowski (Eds), </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA., </address> <year> 1988, </year> <pages> pp. 52-59. </pages>
Reference-contexts: 5 11 TABLE III ATTRIBUTES OF THE IBM DATASETS Attribute Description Value salary salary uniformly distributed from 20,000 to 150,000 commission commission if salary 75000 ! commission = 0 else uniformly distributed from 10000 to 75000. age age uniformly distributed from 20 to 80. elevel education level uniformly distributed from <ref> [0; 1; : : :; 4] </ref>. car make of the car uniformly distributed from [1; 2; : : :20]. zipcode zip code of the town uniformly chosen from 9 available zipcodes. hvalue value of the house uniformly distributed from 0.5k10000 to 1.5k1000000 where k 2 f0 : : : 9g depends
Reference: [5] <author> A. van Ooyen and B. Nienhuis, </author> <title> "Improving the convergence of the backpropagation algorithm", </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 5, </volume> <pages> pp. 465-471, </pages> <year> 1992. </year>
Reference: [6] <author> R. Setiono, </author> <title> "A penalty-function approach for pruning feedforward neural networks", Neural Computation, </title> <journal> forthcoming. </journal> <volume> 18 </volume>
Reference: [7] <editor> D.E. Rumelhart, J.L. McClelland, </editor> <booktitle> and the PDP Research Group. Parallel Distributed Processing, </booktitle> <volume> volume 1. </volume> <publisher> The MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1986. </year>
Reference: [8] <author> R.L. Watrous, </author> <title> "Learning algorithms for connectionist networks: applied gradient methods for nonlinear optimization," </title> <booktitle> in Proc. IEEE First International Conference on Neural Networks, </booktitle> <address> San Diego, </address> <year> 1987, </year> <pages> pp. 619-627. </pages>
Reference: [9] <author> R. Setiono, </author> <title> "A neural network construction algorithm which maximizes the likelihood function," </title> <journal> Connection Science, </journal> <volume> vol. 7, no. 2, </volume> <pages> pp. 147-166, </pages> <year> 1995. </year>
Reference: [10] <author> J.E. Dennis Jr. and R. B. Schnabel, </author> <title> Numerical methods for unconstrained optimization and nonlinear equations, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1983. </year>
Reference: [11] <author> L.M. Belue and K.W. Bauer, Jr., </author> <title> "Determining input features for multilayer perceptron," </title> <journal> Neurocomputing, </journal> <volume> vol. 7, no. 2, </volume> <pages> pp. 111-122, </pages> <year> 1995. </year>
Reference: [12] <editor> S.B. Thrun et al. </editor> , <title> "The MONK's Problems A performance comparison of different learning algorithms", </title> <institution> Department of Computer Science, Carnegie Mellon University, CMU-CS-91-197, </institution> <year> 1991. </year>
Reference-contexts: The problems that we selected have been widely tested by other researchers and they include both real-world problems and artificially created problems. Two sets of artificial problems were tested. They were the monks problems <ref> [12] </ref> and a subset of the data mining problems generated by Agrawal et al. [13] of IBM Almaden Research Center. Four real-world problems were also tested, they originated in diverse fields: breast cancer diagnosis, the US Congressional voting records, diabetes diagnosis, and sonar returns classification. <p> Data for these four problems as well as for the monks problems can be obtained via anonymous ftp from the University of California-Irvine repository [14]. A. The monks problems. The monks problems <ref> [12] </ref> are an artificial robot domain, in which robots are described by six different attributes (Table I): The learning tasks of the three monks problems are of binary classification, each of them is given by the following logical description of a class. 9 * Problem Monks 1: (head shape = body <p> All eleven networks with 2 input attributes have an accuracy rates of 93.44 % and 97.22 % on the training dataset and the testing dataset, respectively. The 97.22 % accuracy rate is the same as that reported by Thrun et al. <ref> [12] </ref>. It is worth noting that, despite the presence of 6 mislabeled training patterns, 14 of the 30 networks with selected attributes have a perfect 100 % accuracy rate on the testing dataset. None of the 30 networks with all input attributes has such accuracy. B. IBM datasets.
Reference: [13] <author> R. Agrawal, T. Imielinski and A. Swami. </author> <title> "Database mining: A performance perspective", </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> vol. 5, no. 6, </volume> <month> Dec. </month> <year> 1993, </year> <pages> pp. 914-925. </pages>
Reference-contexts: The problems that we selected have been widely tested by other researchers and they include both real-world problems and artificially created problems. Two sets of artificial problems were tested. They were the monks problems [12] and a subset of the data mining problems generated by Agrawal et al. <ref> [13] </ref> of IBM Almaden Research Center. Four real-world problems were also tested, they originated in diverse fields: breast cancer diagnosis, the US Congressional voting records, diabetes diagnosis, and sonar returns classification. <p> None of the 30 networks with all input attributes has such accuracy. B. IBM datasets. These datasets had been generated by Agrawal et al. <ref> [13] </ref> to test their database mining algorithm CDP. Every pattern of the datasets consists of nine attributes given in Table III. One network input unit was assigned for each of these attributes. Ten binary classification functions were developed using these attributes. <p> The number of input units was 9. The number of patterns used for training, cross validation, and testing was 800, 200 and 1000, respectively. These patterns were randomly generated according to the distributions described in Table III. Following Agrawal et al. <ref> [13] </ref>, we also included a perturbation factor as one of the parameters of the random data generator. The perturbation factor was set at 5 percent. The results of the feature selection algorithm are presented in Table V.
Reference: [14] <author> P.M. Murphy and D.W. Aha, </author> <title> UCI Repository of machine learning databases, Machine-readable data repository, </title> <address> Irvine, CA. </address> <institution> University of California, Dept. of Information and Computer Science, </institution> <year> 1992. </year>
Reference-contexts: Four real-world problems were also tested, they originated in diverse fields: breast cancer diagnosis, the US Congressional voting records, diabetes diagnosis, and sonar returns classification. Data for these four problems as well as for the monks problems can be obtained via anonymous ftp from the University of California-Irvine repository <ref> [14] </ref>. A. The monks problems.
Reference: [15] <author> W.H. Wolberg and O.L. Mangasarian, </author> <title> "Multisurface method of pattern separation for medical diagnosis applied to breast cytology," </title> <booktitle> Proc. of the National Academy of Sciences, </booktitle> <volume> vol. 87, </volume> <pages> pp. 9193-9196, </pages> <year> 1990. </year>
Reference-contexts: The Wisconsin Breast Cancer Data (WBCD) is a large data set that consists of 699 patterns of which 458 are benign samples and 241 are malignant samples. Each of these patterns consists of nine measurements taken from fine needle aspirates from a patient's breast <ref> [15] </ref>. The measurements were graded 1 to 10 at the time of sample collection, with 1 being the closest to benign and 10 the most anaplastic. A linear programming based method for pattern separation called the Multisurface Method has been proposed by Mangasarian [16].
Reference: [16] <author> O.L. Mangasarian, </author> <title> "Multisurface method of pattern separation," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 14, no. 6, </volume> <pages> pp. 801-807, </pages> <year> 1968. </year>
Reference-contexts: The measurements were graded 1 to 10 at the time of sample collection, with 1 being the closest to benign and 10 the most anaplastic. A linear programming based method for pattern separation called the Multisurface Method has been proposed by Mangasarian <ref> [16] </ref>. A computer program that implements this method for the WBCD has been in use at the University of Wisconsin Hospital since 1990 [17]. For our experiment, 315 samples were randomly selected for training, 35 samples were selected for cross-validation, and 349 for testing. 2.
Reference: [17] <author> O.L. Mangasarian, R. Setiono and W.H. Wolberg, </author> <title> "Pattern recognition via linear programming: theory and application to medical diagnosis," </title> <editor> in: T.F. Coleman and Y. Li. eds., </editor> <title> Large-scale Numerical Optimization, </title> <publisher> (SIAM, </publisher> <address> Philadelphia, </address> <year> 1990), </year> <pages> pp. 22-30. </pages>
Reference-contexts: A linear programming based method for pattern separation called the Multisurface Method has been proposed by Mangasarian [16]. A computer program that implements this method for the WBCD has been in use at the University of Wisconsin Hospital since 1990 <ref> [17] </ref>. For our experiment, 315 samples were randomly selected for training, 35 samples were selected for cross-validation, and 349 for testing. 2. United States Congressional Voting Records Dataset. The dataset consists of the voting records of 435 congressmen on 16 major issues in the 98th Congress.
Reference: [18] <author> J.C. Schlimmer, </author> <title> "Concept acquisition through representational adjustment," </title> <type> Ph.D. thesis, </type> <institution> Dept. of Information and Computer Science, University of California, </institution> <address> Irvine, CA., </address> <year> 1987. </year>
Reference-contexts: The classification problem is to predict the party affiliation of each congressman, which is either democrat or republican. We selected 197 patterns for training randomly, 21 patterns for cross-validation, and 217 patterns for testing. Schlimmer <ref> [18] </ref> reported getting an accuracy rate of 90%-95% on this dataset. 3. Pima Indians Diabetes Dataset. The dataset consists of 768 samples taken from patients who may show signs of diabetes. 15 Each sample is described by 8 attributes, 1 attribute has discrete values and the rest have continuous values.
Reference: [19] <author> J.W. Smith, J.E. Everhart, W.C. Dickson, W.C. Knowler, and R.S. Johannes, </author> <title> "Using the ADAP learning algorithm to forecast the onset of diabetes mellitus," </title> <booktitle> in Proc. of the Symposium on Computer Applications and Medical Care, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1988, </year> <pages> pp. 261-265. </pages>
Reference-contexts: The training set consists of 345 randomly selected samples, the cross-validation set consists of 39 samples, and the testing set consists of the remaining 384 samples. Applying the ADAP algorithm trained on 576 samples, Smith et al. <ref> [19] </ref> achieved an accuracy rate of 76 % on the remaining 192 samples. 4. Sonar Targets Dataset. The sonar returns classification dataset [20] consists of 208 sonar returns, each of which is represented by 60 real numbers between 0.0 and 1.0.
Reference: [20] <author> R.P. Gorman and T.J. Sejnowski, </author> <title> "Analysis of hidden units in a layered network trained to classify sonar target," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 1, </volume> <pages> pp. 75-89, </pages> <year> 1988. </year> <month> 20 </month>
Reference-contexts: Applying the ADAP algorithm trained on 576 samples, Smith et al. [19] achieved an accuracy rate of 76 % on the remaining 192 samples. 4. Sonar Targets Dataset. The sonar returns classification dataset <ref> [20] </ref> consists of 208 sonar returns, each of which is represented by 60 real numbers between 0.0 and 1.0. The task is to distinguish between returns from a metal cylinder and those from a cylindrically shaped rock. <p> The task is to distinguish between returns from a metal cylinder and those from a cylindrically shaped rock. Using 104 returns as training samples and 104 returns as testing samples, Gorman and Sejnowski trained backpropagation networks with hidden units ranging from 0 to 24 <ref> [20] </ref>. The accuracy rates reported on the testing set ranged between 73.1% and 90.4%. For our experiment, we have used 90 returns for training, 14 returns for cross-validation, and 104 returns for testing. For each of the four datasets, 30 neural networks with 12 hidden units were trained.
References-found: 20

