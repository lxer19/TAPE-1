URL: http://www.icsi.berkeley.edu/~nikki/papers/CAN_Dec95_spec.ps
Refering-URL: http://www.icsi.berkeley.edu/~nikki/papers/
Root-URL: http://www.icsi.berkeley.edu
Email: pattersong@cs.berkeley.edu  
Phone: Tel: (510) 642-4274, FAX: (510) 643-7684. fnikki, mjacoby,  
Title: Truth in SPEC Benchmarks  
Author: Nikki Mirghafori, Margret Jacoby, and David Patterson 
Keyword: SPEC benchmarks, compiler-flag tuning, optimization, reproducibility  
Address: Berkeley, Berkeley, CA 94704  
Affiliation: University of California at  
Abstract: The System Performance Evaluation Cooperative (SPEC) benchmarks are a set of integer and floating-point programs that are intended to be "effective and fair in comparing the performance of high performance computing systems" [10]. SPEC ratings are often quoted in company advertising and have been trusted as the de facto measure of comparison for computer systems. Recently, there has been some concern regarding the fairness and the value of these benchmarks for comparing computer systems [1, 2]. In this paper we investigate the following two questions regarding the SPEC92 benchmark suite: 1) How sensitive are the SPEC ratings to various tunings? 2) How reproducible are the published results? For six vendors, we compare the published SPECpeak and SPECbase ratings, and observe an 11% average improvement in the SPECpeak ratings due to changes in the compiler flags alone. In our own attempt to reproduce the published SPEC ratings, we came across various "explicit" and "hidden" tuning parameters that we consider unrealistic. We suggest a new unit called SPECsimple that requires using only the -O compiler optimization flag, shared libraries, and standard system configuration. SPECsimple is designed to better match the performance experienced by a typical user. Our measured SPECsimples are 65-86% of the advertised SPECpeak performance. We conclude this paper by citing cases of compiler optimizations specifically designed for SPEC programs, in which performance decreases drastically or the computed results are incorrect if the compiled program does not exactly match the SPEC benchmark program. These findings show that the fairness and value of the popular SPEC benchmarks are questionable. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Chan, Y., Sudarsanam, A., and Wolfe A. </author> <title> "The Effect of Compiler-Flag Tuning on SPEC Benchmark Performance," </title> <journal> Computer Architecture News, </journal> <volume> Vol. 22, No. 4, </volume> <pages> pp 60-70, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: They question the choice of the programs in the suite and claim that many are superfluous. They also argue against the use of the geometric mean, which causes distortions, and espouse the use of the harmonic mean instead. Chan et. al. <ref> [1] </ref> have also raised some questions about SPEC. In their investigation they attempt to reproduce the published results, and report that compiler-flag tuning has only a modest impact on the SPEC measurements. Our work addresses similar questions as in [1]; however, both our investigative method and, more importantly, our conclusions differ <p> Chan et. al. <ref> [1] </ref> have also raised some questions about SPEC. In their investigation they attempt to reproduce the published results, and report that compiler-flag tuning has only a modest impact on the SPEC measurements. Our work addresses similar questions as in [1]; however, both our investigative method and, more importantly, our conclusions differ significantly. The remainder of the paper is organized as follows. In Section 2 we briefly explain what the SPEC92 CPU suites are and how benchmark results are obtained. <p> Running the SPEC benchmarks turned out to be far from trivial. This sentiment is shared by other investigators <ref> [1, 3] </ref>. Our first problem was to obtain proper makefile wrappers. For all of the machines we tested, the released makefile wrappers were out of date. A MIPS wrapper file even had a misspelled filename, making us wonder whether it was ever used for any official data gathering 5 . <p> For the IBM experiments, we did not have access to local storage. Instead, we used the file server for storing the program and input/output files. Chan et. al. <ref> [1] </ref> report that the use of a central-server affects the performance of the I/O intensive integer programs; in particular, untuned CINT92 running on a central-server suffers up to a 35% loss in performance in comparison to tuned local operation. <p> In Section 3, we showed that SPEC benchmark programs are highly sensitive to compiler flag tunings. Our conclusion is different from Chan et al <ref> [1] </ref> who report only a modest impact on performance due to compiler flag tuning. They compared published SPECpeak numbers to their own controlled experiments with a set of basic optimization flags for two vendors (five platforms). <p> Running the SPEC benchmarks and attempting to reproduce the published SPEC results is not a trivial task, as discussed at length in Section 5 and noted by <ref> [1, 3] </ref>. We observed many "explicit" as well as "hidden" tuning parameters that vendors use in obtaining both SPECpeak and SPECbase ratings. We also found out that the auxiliary files for running the benchmarks were not as easily available as we had thought.
Reference: [2] <author> Giladi, R., Ahituv, N. </author> <title> "SPEC as a Performance Evaluation Measure," </title> <journal> Computer, </journal> <volume> Vol. 28, No. 8, </volume> <pages> pp 33-42, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: As a result, the benchmark suite will no longer be an accurate predictor of performance under actual usage. There has been rising concern in the computer architecture community about the value and fairness of the popular SPEC CPU benchmarks. Giladi and Ahituv <ref> [2] </ref> have run a mathematical analysis on the published SPEC ratings. They question the choice of the programs in the suite and claim that many are superfluous. They also argue against the use of the geometric mean, which causes distortions, and espouse the use of the harmonic mean instead. <p> As discussed by <ref> [2] </ref>, the geometric mean can distort the reported results. * A hidden set of tuning parameters may still be used, and SPEC95 does not require the full disclosure of these parameters. * The setup for SPECbase still does not correspond to that of a typical user.
Reference: [3] <author> Glaeser, Christopher. Nullstone Corporation. </author> <type> E-mail communication. </type> <month> April </month> <year> 1995. </year>
Reference-contexts: Running the SPEC benchmarks turned out to be far from trivial. This sentiment is shared by other investigators <ref> [1, 3] </ref>. Our first problem was to obtain proper makefile wrappers. For all of the machines we tested, the released makefile wrappers were out of date. A MIPS wrapper file even had a misspelled filename, making us wonder whether it was ever used for any official data gathering 5 . <p> Running the SPEC benchmarks and attempting to reproduce the published SPEC results is not a trivial task, as discussed at length in Section 5 and noted by <ref> [1, 3] </ref>. We observed many "explicit" as well as "hidden" tuning parameters that vendors use in obtaining both SPECpeak and SPECbase ratings. We also found out that the auxiliary files for running the benchmarks were not as easily available as we had thought.
Reference: [4] <author> Reilly, Jeff. </author> <title> SPEC CPU Subcommittee Chair. </title> <booktitle> Netnews posting. </booktitle> <month> Oc-tober </month> <year> 1994. </year>
Reference-contexts: recent findings of compiler optimizations specifically designed for SPEC programs. 2 What Is SPEC? SPEC, the Standard Performance Evaluation Corporation, is a non-profit corporation formed to "establish, maintain and endorse a standardized set of relevant benchmarks that can be applied to the newest generation of high-performance computers" (from SPEC's bylaws <ref> [4] </ref>). SPEC develops suites of benchmarks intended to measure computer performance. These suites are packaged with source code and tools.
Reference: [5] <author> Rozhin, Mark. </author> <title> HP SPEC representative. </title> <type> E-mail communication. </type> <month> November </month> <year> 1994. </year>
Reference-contexts: as the following: * A large set of compiler flag optimizations are still used. * Some companies (SGI, SUN) run SPEC92 in single user mode [6]. * Kernel reconfiguration is sometimes used [11] to get better results, and kernel reconfiguration is sometimes required to allow certain optimizations to be performed <ref> [5] </ref>. * Another method used to squeeze better performance out of a system is to change the disk partition size and place the file blocks so that the disk I/O time is reduced [11]. * Several of the companies (HP, SGI, DEC [6]) use archived libraries as opposed to shared libraries.
Reference: [6] <author> SPEC Newsletter, 6:3, </author> <month> September </month> <year> 1994. </year>
Reference-contexts: be a benchmark free from the effects of "super-tuning" [7], but SPECbase is affected by many tunings that do not reflect typical system usage, such as the following: * A large set of compiler flag optimizations are still used. * Some companies (SGI, SUN) run SPEC92 in single user mode <ref> [6] </ref>. * Kernel reconfiguration is sometimes used [11] to get better results, and kernel reconfiguration is sometimes required to allow certain optimizations to be performed [5]. * Another method used to squeeze better performance out of a system is to change the disk partition size and place the file blocks so <p> required to allow certain optimizations to be performed [5]. * Another method used to squeeze better performance out of a system is to change the disk partition size and place the file blocks so that the disk I/O time is reduced [11]. * Several of the companies (HP, SGI, DEC <ref> [6] </ref>) use archived libraries as opposed to shared libraries. * Some companies shut down the windowing system (e.g., Xwindows interface) and disconnect from the network [12]. * Many companies (IBM, SGI, DEC, Sun) use the KAP preprocessor, which is not a bundled product and must be purchased separately. 4 For these <p> Our tested systems were very similar to the systems for which official results were published <ref> [6] </ref>; the few differences are listed in Table 6 in Appendix A. The CINT92 programs need about 16MB of memory to avoid swapping [12], so it is unlikely that our extra memory has affected performance. The difference in disk drives may have had some negligible effect on the measurements. <p> In Tables 4 and 5, "NA" appears for cases where official results were not reported and a comparison was not possible. We compared our SPECpeak and SPECbase measurements to the results published in the March and June 94 issues of the SPECNewsletter <ref> [7, 6] </ref>. We were not able to run the sc benchmark without the archived library for SPECsimple measurements; so the reported sc run uses -O as well as the archived ucblib.a library. Tables 4 and 5 show how closely we matched the published results.
Reference: [7] <author> SPEC Newsletter, 6:2, </author> <month> June </month> <year> 1994. </year>
Reference-contexts: Their efforts have produced rather incredible increases in the speed of benchmark execution. As proof of their prowess at Hewlett-Packard, software changes alone doubled the SPECmark89 rating of the HP model 720 workstation." <ref> [7] </ref> How useful are computer system benchmarks? For the benchmarks to be meaningful, one must trust the source of the benchmark results and have confidence that the benchmark actually predicts performance for the intended use of the system. <p> There are almost no restrictions 2 on the usage of compiler flags for SPECpeak measurements. In contrast, there are several restrictions on the usage of compiler flags for SPECbase measurements. The relevant SPECbase rules are <ref> [7] </ref>: 1. The optimization options must be safe: i.e., they must not change the behavior of the program in subtle ways, for instance, due to the reordering of instructions. 2. <p> We remind the reader that although the SPECbase setup (supposedly) better matches the setup of a typical user, often only SPECpeak results are reported in company advertising. 4 Is SPECbase A "Basic" Measure? SPECbase is intended to be a benchmark free from the effects of "super-tuning" <ref> [7] </ref>, but SPECbase is affected by many tunings that do not reflect typical system usage, such as the following: * A large set of compiler flag optimizations are still used. * Some companies (SGI, SUN) run SPEC92 in single user mode [6]. * Kernel reconfiguration is sometimes used [11] to get <p> Our experiments show that a user may see 69% of the claimed SPECint92 and 75% of the published SPECfp92. 6.3 SUN Our Sun SPARCstation 10 Model 51 matched the configuration of the machine listed in the SPECNewsletter in June 94 <ref> [7] </ref>, with the exception that the vendor's system was run in single-user mode, while our machine was run in multi-user HP 9000/712 SPECint SPECint92 Percent of published peak peak (published) 84.3 100.0% peak (no GUI or LAN) 83.1 98.6% peak (with GUI and LAN) 82.5 97.9% base (published) 76.6 90.9% base <p> In Tables 4 and 5, "NA" appears for cases where official results were not reported and a comparison was not possible. We compared our SPECpeak and SPECbase measurements to the results published in the March and June 94 issues of the SPECNewsletter <ref> [7, 6] </ref>. We were not able to run the sc benchmark without the archived library for SPECsimple measurements; so the reported sc run uses -O as well as the archived ucblib.a library. Tables 4 and 5 show how closely we matched the published results. <p> Independent verification of the published results will only become common when the hurdles are removed. Although the SPECbase measure was introduced to "reflect how compilers are most commonly used today" <ref> [7] </ref>, our investigation showed that this measure is not fulfilling its role. We therefore proposed a new measure called SPECsimple.
Reference: [8] <author> SPEC Newsletter, 6:1, </author> <month> March </month> <year> 1994. </year>
Reference: [9] <author> SPEC Newsletter, 5:2, </author> <month> June </month> <year> 1993. </year>
Reference-contexts: This performance is similar to what a user may experience who uses -O and standard libraries. 6.2 IBM We chose the IBM 6000/370 for our IBM tests. The architecture and configuration of our machine was very similar to the one listed in the SPECNewsletter in June 93 <ref> [9] </ref>; the few configuration differences are shown in Table 7 in Appendix A. The results in Table 3 reflect how closely we matched the published results. There are various explanations why our peak number is only 75-81% of the published results.
Reference: [10] <institution> SPEC Open Systems Steering Committee Policy Document, </institution> <month> Jan-uary </month> <year> 1994. </year>
Reference: [11] <author> Trent, Eugene. </author> <title> SGI SPEC representative. </title> <type> Phone communication. </type> <month> November </month> <year> 1994. </year>
Reference-contexts: of "super-tuning" [7], but SPECbase is affected by many tunings that do not reflect typical system usage, such as the following: * A large set of compiler flag optimizations are still used. * Some companies (SGI, SUN) run SPEC92 in single user mode [6]. * Kernel reconfiguration is sometimes used <ref> [11] </ref> to get better results, and kernel reconfiguration is sometimes required to allow certain optimizations to be performed [5]. * Another method used to squeeze better performance out of a system is to change the disk partition size and place the file blocks so that the disk I/O time is reduced <p> to get better results, and kernel reconfiguration is sometimes required to allow certain optimizations to be performed [5]. * Another method used to squeeze better performance out of a system is to change the disk partition size and place the file blocks so that the disk I/O time is reduced <ref> [11] </ref>. * Several of the companies (HP, SGI, DEC [6]) use archived libraries as opposed to shared libraries. * Some companies shut down the windowing system (e.g., Xwindows interface) and disconnect from the network [12]. * Many companies (IBM, SGI, DEC, Sun) use the KAP preprocessor, which is not a bundled
Reference: [12] <author> Vitale, Phil. </author> <title> HP SPEC representative. </title> <journal> E-mail communications. </journal> <month> November </month> <year> 1994 </year> <month> and July </month> <year> 1995. </year>
Reference-contexts: disk partition size and place the file blocks so that the disk I/O time is reduced [11]. * Several of the companies (HP, SGI, DEC [6]) use archived libraries as opposed to shared libraries. * Some companies shut down the windowing system (e.g., Xwindows interface) and disconnect from the network <ref> [12] </ref>. * Many companies (IBM, SGI, DEC, Sun) use the KAP preprocessor, which is not a bundled product and must be purchased separately. 4 For these reasons, we consider SPECbase not to be a "basic" measure. Instead, we propose a new measure called SPECsimple. <p> We obtained most of the missing software upon contacting HP <ref> [12] </ref>. As discussed earlier, some vendors run the SPEC programs with the machine in single-user mode and disconnected from the network, without a windowing system running, and in some cases (e.g., SGI and HP) with changed kernel parameters. <p> specific reasons for the mismatch; however, the following are some general explanations for the discrepancy between our results and the published ratings: * The location of the relevant file blocks may have affected disk I/O performance. * The vendor may have reported the best of a large number of runs <ref> [12] </ref>, while we report the results of a single run. 6.1 HP We used an HP 9000 Model 712 for SPECint92 and an HP 9000 Model 715 for SPECfp92 measurements. <p> Our tested systems were very similar to the systems for which official results were published [6]; the few differences are listed in Table 6 in Appendix A. The CINT92 programs need about 16MB of memory to avoid swapping <ref> [12] </ref>, so it is unlikely that our extra memory has affected performance. The difference in disk drives may have had some negligible effect on the measurements. Tables 1 and 2 respectively show the results of our attempts to match the published SPECint92 and SPECfp92 ratings.
References-found: 12

