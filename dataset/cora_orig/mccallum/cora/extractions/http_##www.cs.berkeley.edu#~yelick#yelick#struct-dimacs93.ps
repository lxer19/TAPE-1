URL: http://www.cs.berkeley.edu/~yelick/yelick/struct-dimacs93.ps
Refering-URL: http://www.cs.berkeley.edu/~yelick/papers.html
Root-URL: 
Title: Data Structures for Irregular Applications  
Author: Katherine Yelick, Soumen Chakrabarti, Etienne Deprit, Jeff Jones, Arvind Krishnamurthy and Chih-Po Wen 
Address: Berkeley  
Affiliation: Computer Science Division University of California at  
Abstract: Parallelization of any large application can be a difficult task, but when the application contains irregular patterns of communication and control, the parallelization effort is higher and the likelihood of producing an efficient implementation is lower. The kinds of data structures that appear in irregular applications, for example, trees, graphs, and sets, do not have simple mappings onto distributed memory machines. We are building a library of such distributed data structures that use a combination of replication and partitioning to achieve high performance. Operations on these structures cannot be efficiently implemented as atomic operations, because of the latency of inter-processor communication. We propose a relaxed consistency model for these data structures, which is analogous to a weak consistency model on shared memory. This allows for clean, simple interfaces on the objects, but admits low latency, high throughput implementations. We demonstrate these ideas using a few data structure examples, each of which is being used in at least one application.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak ordering-a new definition. </title> <booktitle> In 17th International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1990. </year>
Reference-contexts: The analogy is to weak memory models, in which read and write operations may be reordered in the network, and guarantees about completion of the memory operations are only made at synchronization points <ref> [1, 2] </ref>. Extending this idea to arbitrary data structures, we assume that each object has an associated set of normal operations, and optionally, a set of synchronization operations. The synchronization operations are not locks, which would prevent operations from being performed, but they force outstanding normal operations to take effect. <p> Some replicated data structures use consistency protocols that are similar to those found in shared memory implementations, either in hardware <ref> [1, 5] </ref> or in software [7, 2]. Programs written using shared 3 distributed data structures have some of the programmability advantages found with shared memory.
Reference: [2] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of munin. </title> <booktitle> Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <volume> 7(4) </volume> <pages> 152-164, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: The analogy is to weak memory models, in which read and write operations may be reordered in the network, and guarantees about completion of the memory operations are only made at synchronization points <ref> [1, 2] </ref>. Extending this idea to arbitrary data structures, we assume that each object has an associated set of normal operations, and optionally, a set of synchronization operations. The synchronization operations are not locks, which would prevent operations from being performed, but they force outstanding normal operations to take effect. <p> Some replicated data structures use consistency protocols that are similar to those found in shared memory implementations, either in hardware [1, 5] or in software <ref> [7, 2] </ref>. Programs written using shared 3 distributed data structures have some of the programmability advantages found with shared memory.
Reference: [3] <author> Soumen Chakrabarti. </author> <title> A distributed memory grobner basis algorithm. </title> <type> Master's thesis, </type> <institution> University of California, Berkeley, Berkeley, </institution> <address> CA, </address> <year> 1992. </year>
Reference-contexts: Multiset The first example is a multiset: a container of unordered objects in which duplicates are allowed. This use used in the Grobner basis problem, in which a key data structure is a multiset of polynomials <ref> [3, 4] </ref>.
Reference: [4] <author> Soumen Chakrabarti and Katherine Yelick. </author> <title> Implementing an irregular application on a distributed memory multiprocessor. </title> <booktitle> In Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <year> 1993. </year> <note> to appear. </note>
Reference-contexts: Multiset The first example is a multiset: a container of unordered objects in which duplicates are allowed. This use used in the Grobner basis problem, in which a key data structure is a multiset of polynomials <ref> [3, 4] </ref>.
Reference: [5] <author> Kaourosh Gharachorloo, Daniel Lenoski, James Laudon, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <year> 1990. </year>
Reference-contexts: Some replicated data structures use consistency protocols that are similar to those found in shared memory implementations, either in hardware <ref> [1, 5] </ref> or in software [7, 2]. Programs written using shared 3 distributed data structures have some of the programmability advantages found with shared memory.
Reference: [6] <author> Maurice P. Herlihy and Jeannette M. Wing. </author> <title> Linearizability: A correctness condition for concurrent objects. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <pages> pages 463-492, </pages> <month> July </month> <year> 1990. </year> <note> 6 A preliminary version appeared in the proceedings of the 14th ACM Symposium on Principles of Programming Languages, </note> <year> 1987, </year> <title> under the title: Axioms for concurrent objects. </title>
Reference-contexts: In this abstract we do give only this informal definition of relaxed object, along with a number of examples. The notion can be made more precise by extending the definition of linearizability, which requires that operations appear to take effect atomically, sometime during the invocation of the operations <ref> [6] </ref>.
Reference: [7] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Some replicated data structures use consistency protocols that are similar to those found in shared memory implementations, either in hardware [1, 5] or in software <ref> [7, 2] </ref>. Programs written using shared 3 distributed data structures have some of the programmability advantages found with shared memory.
Reference: [8] <author> Steven Lucco. </author> <title> A dynamic scheduling method for irregular parallel programs. </title> <booktitle> In Proceedings of the Conference on Programming Language Design and Implementation. ACM Sigplan, </booktitle> <year> 1992. </year>
Reference-contexts: Each processor repeatedly executes a scheduling loop, which looks for work in one or more scheduling queues. This style, which is commonly used on shared as well as distributed memory machine, works well because the number and kind of tasks is not known until run-time <ref> [10, 8] </ref>. It adapts to input dependent load requirements, and uses fewer resources than if a new thread (complete with its own context, which means a stack) were created which each unit of parallel work.
Reference: [9] <author> Peter Reiher and David Jefferson. </author> <title> Dynamic load management in the time warp operating system. </title> <journal> Transactions of the Society for Computer Simulation, </journal> <volume> 7(2) </volume> <pages> 91-120, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Time Warp State The notion of a time warp system for doing speculative simulation was introduced by Reiher and Jefferson <ref> [9] </ref>. The essential data structure is the simulation state, which is partitioned and distributed across processors. For example, in circuit simulator, the circuit is partitioned into subcircuits based on connectivity defined by direct voltage connections [11].
Reference: [10] <author> Eric Roberts and Mark Vandevoorde. </author> <title> Work crews: An abstraction for controlling parallelism. </title> <type> Technical Report 42, </type> <institution> Digital Equipment Corporation Systems Research Center, Palo Alto, California, </institution> <year> 1989. </year>
Reference-contexts: Each processor repeatedly executes a scheduling loop, which looks for work in one or more scheduling queues. This style, which is commonly used on shared as well as distributed memory machine, works well because the number and kind of tasks is not known until run-time <ref> [10, 8] </ref>. It adapts to input dependent load requirements, and uses fewer resources than if a new thread (complete with its own context, which means a stack) were created which each unit of parallel work.
Reference: [11] <author> Chih-Po Wen. </author> <title> Parallel timing simulation on a distributed memory multiprocessor. </title> <type> Master's thesis, </type> <institution> University of California, Berkeley, Berkeley, </institution> <address> CA, </address> <year> 1992. </year> <type> Master's Report. 7 </type>
Reference-contexts: The essential data structure is the simulation state, which is partitioned and distributed across processors. For example, in circuit simulator, the circuit is partitioned into subcircuits based on connectivity defined by direct voltage connections <ref> [11] </ref>. Although the contents of the state, its partitioning, and the algorithms for simulating a time step of a partition are specific to the simulation problem, the basic state operations are the same for any speculative simulation.
References-found: 11

