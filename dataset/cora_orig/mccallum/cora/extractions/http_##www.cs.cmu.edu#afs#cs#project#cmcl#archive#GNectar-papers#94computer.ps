URL: http://www.cs.cmu.edu/afs/cs/project/cmcl/archive/GNectar-papers/94computer.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs/usr/prs/WWW/papers.html
Root-URL: 
Title: A Systematic Approach to Host Interface Design for High-Speed Networks  
Author: Peter Steenkiste 
Keyword: network interfaces, high-speed networks, buffer management, memory hierarchy  
Note: This research was sponsored by the Defense Advanced Research Projects Agency (DOD) under contract number MDA972-90-C-0035, in part by the National Science Foundation and the Defense Advanced Research Projects Agency under Cooperative Agreement NCR-8919038 with the Corporation for National Research Initiatives.  
Address: 5000 Forbes Avenue Pittsburgh, Pennsylvania 15213-3891  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: In recent years, networks with media rates of 100 Mbit/second or more have become widely available (FDDI, ATM, HIPPI, ..). However, many computer systems cannot make use of the available bandwidth because of the high overhead associated with network communication. In this paper we review the operations involved in communication over high-speed networks, and we describe optimizations of the network interface that improve network throughput. We also discuss how the payoff of the optimizations is influenced by features of the host software and architecture. This paper is based on our experience with the interfaces for the Nectar and Gigabit Nectar networks. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> David D. Clark, Van Jacobson, John Romkey, and Howard Salwen. </author> <title> "An Analysis of TCP Processing Overhead". </title> <journal> IEEE Communications Magazine 27, </journal> <month> 6 (June </month> <year> 1989), </year> <pages> 23-29. </pages>
Reference-contexts: Second, increasing the buffer size potentially reduces the frequency with which the host has to interact with the adaptor; this will in general improve performance since interactions often cause host interrupts, which creates overhead. 3. Communication overhead The overhead associated with network communication has been analyzed in several papers <ref> [1, 10] </ref>. Although it is difficult to compare the results because the measurements are made for different architectures, protocols, programming interfaces, and benchmarks, there is a common pattern: there is no single dominating source of overhead. <p> Transport protocol processing (excluding the checksum) is relatively inexpensive. Measurements show that the overhead for optimized implementations of reliable protocols can be as low as 200 instructions <ref> [1] </ref>. Moving this function outboard is unlikely to be worthwhile, unless several other costs are also reduced as a result, or under special conditions, for example when connecting a dumb device to a network. However, providing support for specific operations such as checksumming can be beneficial.
Reference: 2. <author> Eric Cooper, Peter Steenkiste, Robert Sansom, and Brian Zill. </author> <title> Protocol Implementation on the Nectar Communication Processor. </title> <booktitle> Proceedings of the SIGCOMM '90 Symposium on Communications Architectures and Protocols, ACM, </booktitle> <address> Philadelphia, </address> <month> September, </month> <year> 1990, </year> <pages> pp. 135-143. </pages>
Reference-contexts: On receive, the application is given a pointer to the location of the data, so there is no requirement to copy the message to a specific location in the user's address space. Buffered primitives are used in a number of systems, including Nectar <ref> [2] </ref> and the Firefly [10]. For protocols that store the checksum in the header, such at TCP, buffered sends will require a separate pass over the data to calculate the checksum, as shown in Figure 5. <p> However, it does not reduce the number of bus transfers for applications that use buffered primitives, since the shared buffers already automatically provide retransmit and receive buffering. The exception is when the application is allowed to access the buffers on the adaptor directly, as is the case in Nectar <ref> [2] </ref>. The Nectar system supports outboard buffering and makes these buffers available through a shared buffer interface. This allows data to be sent and received with only a single transfer over the bus (Figure 12), which is the absolute minimum. <p> The most aggressive option is to perform protocol processing on the network adaptor. The network adaptor can be very flexible and support a wide range of protocols, typically in software <ref> [2] </ref>, or it can be hardwired for one specific protocol [6]. Outboard protocol processing, i.e. the most aggressive option, has a number of disadvantages. First, the network adaptor becomes more complex and expensive, especially if multiple protocols have to be supported. <p> First, the network adaptor becomes more complex and expensive, especially if multiple protocols have to be supported. One of the reasons is that the engine performing protocol processing should be fast, preferably as fast as the host <ref> [2, 9] </ref>. Second, interactions between the host and the network adaptor can become complicated. For example, there is a need for flow control between the host and the adaptor, which can be complicated if the host and adaptor do not share memory. <p> Outboard DMA Protocol Transport Application Bus Network Buffering Processing Checksum Interface Platform Mbit/sec Protocol Engines Yes Yes XTP/IP Yes Immediate multiple up to 200 NAB [6] No Yes VMTP Yes Buffered VMP 100 Nectar <ref> [2] </ref> Yes Both Yes Yes Buffered VME 100 UltraNet No Yes TP4 TCP Yes Immediate VME/HIPPI 250-800 Bellcore ATM [5] No Yes No No - TurboChannel 622 U of Penn ATM [5] No Yes No No - MicroChannel 155 Fore Systems ATM [3] No No No No - SBus/TC 155 Medusa
Reference: 3. <author> Eric Cooper, Onat Menzilcioglu, Robert Sansom, and Francois Bitz. </author> <title> Host Interface Design for ATM LANs. </title> <booktitle> Proceedings of the 16th Conference on Local Computer Networks, IEEE, </booktitle> <month> October, </month> <year> 1991, </year> <pages> pp. 247-258. </pages>
Reference-contexts: No Yes VMTP Yes Buffered VMP 100 Nectar [2] Yes Both Yes Yes Buffered VME 100 UltraNet No Yes TP4 TCP Yes Immediate VME/HIPPI 250-800 Bellcore ATM [5] No Yes No No - TurboChannel 622 U of Penn ATM [5] No Yes No No - MicroChannel 155 Fore Systems ATM <ref> [3] </ref> No No No No - SBus/TC 155 Medusa [5] Yes Yes No Yes Immediate HP Snake 100 Afterburner [8] Yes Yes No Yes Immediate HP Snake 100-800 LANL CBI Yes Yes TCP/IP Yes - HIPPI 800 Gigabit Nectar [12] Yes Yes No Yes Immediate TurboChannel 800 DEC FDDI [9] No
Reference: 4. <author> Peter Druschel, Mark B. Abbott, Michael A. Pagels, and Larry L. Peterson. </author> <title> "Network Subsystem Design". </title> <journal> IEEE Network Magazine 7, </journal> <month> 4 (July </month> <year> 1993), </year> <pages> 8-17. </pages> <month> 5. </month> -. <journal> "Special Issue on Host Interfacing". IEEE Journal on Selected Areas in Communication 11, </journal> <month> 2 (February </month> <year> 1993). </year>
Reference-contexts: Moreover, the data accesses performed during network operations have very low locality, so memory system optimizations such as caching are not very effective <ref> [4] </ref>. 5 We showed earlier how data crosses the memory bus five times in the network interface (Figure 2). These transfers will limit network throughput if the network bandwidth is a non-negligible fraction of the memory bandwidth.
Reference: 6. <author> Hemant Kanakia and David R. Cheriton. </author> <title> The VMP Network Adaptor Board (NAB): High-Performance Network Communication for Multiprocessors. </title> <booktitle> Proceedings of the SIGCOMM '88 Symposium on Communications Architectures and Protocols, ACM, </booktitle> <month> August, </month> <year> 1988, </year> <pages> pp. 175-187. </pages>
Reference-contexts: The most aggressive option is to perform protocol processing on the network adaptor. The network adaptor can be very flexible and support a wide range of protocols, typically in software [2], or it can be hardwired for one specific protocol <ref> [6] </ref>. Outboard protocol processing, i.e. the most aggressive option, has a number of disadvantages. First, the network adaptor becomes more complex and expensive, especially if multiple protocols have to be supported. <p> A meaningful comparison of the throughput is difficult because of differences in conditions that affect performance (packet size, protocol used, ..). Outboard DMA Protocol Transport Application Bus Network Buffering Processing Checksum Interface Platform Mbit/sec Protocol Engines Yes Yes XTP/IP Yes Immediate multiple up to 200 NAB <ref> [6] </ref> No Yes VMTP Yes Buffered VMP 100 Nectar [2] Yes Both Yes Yes Buffered VME 100 UltraNet No Yes TP4 TCP Yes Immediate VME/HIPPI 250-800 Bellcore ATM [5] No Yes No No - TurboChannel 622 U of Penn ATM [5] No Yes No No - MicroChannel 155 Fore Systems ATM
Reference: 7. <author> H.T. Kung, Robert Sansom, Steven Schlick, Peter Steenkiste, Matthieu Arnould, Francois J. Bitz, Fred Chris-tianson, Eric C. Cooper, Onat Menzilcioglu, Denise Ombres, and Brian Zill. </author> <title> Network-Based Multicomputers: An Emerging Parallel Architecture. </title> <booktitle> Proceedings of Supercomputing '91, IEEE, </booktitle> <address> Albequerque, </address> <month> November, </month> <year> 1991, </year> <pages> pp. 664-673. </pages> <month> 8. </month> -. <title> "Special Issue on End-System Support for High-Speed Networks". </title> <journal> IEEE Network Magazine 7, </journal> <month> 4 (July </month> <year> 1993). </year>
Reference-contexts: For small packets, per-packet and per-send/per-receive overheads dominate the total communication cost since the cost of copying the data is small. For small packets it is actually often advantageous to copy the data, since sharing buffers between different parts of the protocol stack increases the buffer management cost <ref> [7] </ref>. Latency is reduced by optimizing the per-packet operations, as is discussed in a number of papers in the literature [10, 11]. Because of space constraints we will not further discuss the network interface design issues that affect latency.
Reference: 9. <author> K.K. Ramakrishnan. </author> <title> "Performance Considerations in Designing Network Interfaces". </title> <journal> IEEE Journal on Selected Areas in Communication 11, </journal> <month> 2 (February </month> <year> 1993), </year> <pages> 203-219. </pages>
Reference-contexts: A detailed case study on the tradeoffs between the use DMA and PIO for 10 an FDDI interface is presented in <ref> [9] </ref>. 7. Outboard buffering A second form of hardware support on the adaptor is buffer space that is larger and more flexible than the speed matching FIFO in the minimal adaptor described in Section 2. As a first step one can allocate space for a few packets. <p> First, the network adaptor becomes more complex and expensive, especially if multiple protocols have to be supported. One of the reasons is that the engine performing protocol processing should be fast, preferably as fast as the host <ref> [2, 9] </ref>. Second, interactions between the host and the network adaptor can become complicated. For example, there is a need for flow control between the host and the adaptor, which can be complicated if the host and adaptor do not share memory. <p> ATM [3] No No No No - SBus/TC 155 Medusa [5] Yes Yes No Yes Immediate HP Snake 100 Afterburner [8] Yes Yes No Yes Immediate HP Snake 100-800 LANL CBI Yes Yes TCP/IP Yes - HIPPI 800 Gigabit Nectar [12] Yes Yes No Yes Immediate TurboChannel 800 DEC FDDI <ref> [9] </ref> No Yes No No Immediate TurboChannel 100 Table 4: Overview of high-speed network interfaces 12. Conclusion The application-to-application throughput over high-speed networks is often limited by communication overhead on the sending and receiving host, so it is critical to make communication processing as efficient as possible.
Reference: 10. <author> Michael Schroeder and Michael Burrows. </author> <title> "Performance of Firefly RPC". </title> <journal> ACM Transactions on Computer Systems 8, </journal> <month> 1 (February </month> <year> 1990), </year> <pages> 1-17. </pages>
Reference-contexts: Second, increasing the buffer size potentially reduces the frequency with which the host has to interact with the adaptor; this will in general improve performance since interactions often cause host interrupts, which creates overhead. 3. Communication overhead The overhead associated with network communication has been analyzed in several papers <ref> [1, 10] </ref>. Although it is difficult to compare the results because the measurements are made for different architectures, protocols, programming interfaces, and benchmarks, there is a common pattern: there is no single dominating source of overhead. <p> For small packets it is actually often advantageous to copy the data, since sharing buffers between different parts of the protocol stack increases the buffer management cost [7]. Latency is reduced by optimizing the per-packet operations, as is discussed in a number of papers in the literature <ref> [10, 11] </ref>. Because of space constraints we will not further discuss the network interface design issues that affect latency. In the remainder of this paper we look at techniques that make the network interface more efficient for large packets, i.e. we focus on increasing throughput. <p> On receive, the application is given a pointer to the location of the data, so there is no requirement to copy the message to a specific location in the user's address space. Buffered primitives are used in a number of systems, including Nectar [2] and the Firefly <ref> [10] </ref>. For protocols that store the checksum in the header, such at TCP, buffered sends will require a separate pass over the data to calculate the checksum, as shown in Figure 5.
Reference: 11. <author> Peter Steenkiste. </author> <title> Analyzing Communication Latency using the Nectar Communication Processor. </title> <booktitle> Proceedings of the SIGCOMM '92 Symposium on Communications Architectures and Protocols, ACM, </booktitle> <address> Baltimore, </address> <month> August, </month> <year> 1992, </year> <pages> pp. 199-209. </pages>
Reference-contexts: The overheads associated with both types of packets are quite different. The operations associated with sending and receiving small packets falls in 4 classes <ref> [11] </ref>: transport protocol processing, context switching (thread/process switching and interrupt handling), datalink protocol processing (dealing with the network adaptor), and buffer management. Studies have shown that each class contributes significantly to the communication cost. <p> Studies have shown that each class contributes significantly to the communication cost. As an example, Table 2 breaks up the time it takes to send and receive a one-word message (4 bytes) using different communication protocols over the Nectar network <ref> [11] </ref>. For large packets, the same set of operations has to be performed, but the cost of copying the data starts to dominate. Figure 3 shows how the communication cost grows with the message size. <p> For small packets it is actually often advantageous to copy the data, since sharing buffers between different parts of the protocol stack increases the buffer management cost [7]. Latency is reduced by optimizing the per-packet operations, as is discussed in a number of papers in the literature <ref> [10, 11] </ref>. Because of space constraints we will not further discuss the network interface design issues that affect latency. In the remainder of this paper we look at techniques that make the network interface more efficient for large packets, i.e. we focus on increasing throughput. <p> Protocol processing Protocol processing is often blamed for the high cost of network communication, even though it is only one of several sources of overhead associated with communication <ref> [11] </ref>. Moving protocol processing to the adaptor can reduce communication overhead on the host and improve overall communication performance. Outboard support for protocol processing can be done at a number of levels: 1. Protocol processing is performed on the host. This is the most common implementation. 2.
Reference: 12. <author> Peter A. Steenkiste, Brian D. Zill, H.T. Kung, Steven J. Schlick, Jim Hughes, Bob Kowalski, and John Mul-laney. </author> <title> A Host Interface Architecture for High-Speed Networks. </title> <booktitle> Proceedings of the 4th IFIP Conference on High Performance Networks, IFIP, Liege, </booktitle> <address> Belgium, </address> <month> December, </month> <year> 1992, </year> <pages> pp. A3 1-16. 19 </pages>
Reference-contexts: Although it is still necessary to notify the host on end-of-DMA so that the application can 12 be rescheduled, it is not necessary to notify the host that packets have been transmitted, since the acknowledgement of the data by the receiver will implicitly indicate successful transmission <ref> [12] </ref>. Outboard buffering (with DMA) is a very effective optimization for applications that use immediate communication primitives. However, it does not reduce the number of bus transfers for applications that use buffered primitives, since the shared buffers already automatically provide retransmit and receive buffering. <p> A similar technique can be used when the checksum is calculated in hardware during the DMA transfer <ref> [12] </ref>. Another host software consideration is that in practice, a single host will run applications that use different programming interfaces and possibly different protocols, so the architectural features we presented will not always pay off. In general, the extraneous features will not have a significant impact on the performance. <p> [5] No Yes No No - MicroChannel 155 Fore Systems ATM [3] No No No No - SBus/TC 155 Medusa [5] Yes Yes No Yes Immediate HP Snake 100 Afterburner [8] Yes Yes No Yes Immediate HP Snake 100-800 LANL CBI Yes Yes TCP/IP Yes - HIPPI 800 Gigabit Nectar <ref> [12] </ref> Yes Yes No Yes Immediate TurboChannel 800 DEC FDDI [9] No Yes No No Immediate TurboChannel 100 Table 4: Overview of high-speed network interfaces 12.
References-found: 10

