URL: ftp://cse.ogi.edu/pub/tech-reports/1994/94-013.ps.gz
Refering-URL: ftp://cse.ogi.edu/pub/tech-reports/README.html
Root-URL: http://www.cse.ogi.edu
Title: MPI: A Message Passing Interface The MPI Forum  
Note: Copyright 1993 ACM 0-8186-4340-4/93/0011 Permission to copy for non-commercial use granted by the Association for Computing Machinery.  
Abstract: This paper presents an overview of mpi, a proposed standard message passing interface for MIMD distributed memory concurrent computers. The design of mpi has been a collective effort involving researchers in the United States and Europe from many organizations and institutions. mpi includes point-to-point and collective communication routines, as well as support for process groups, communication contexts, and application topologies. While making use of new ideas where appropriate, the mpi standard is based largely on current practice. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V. Bala, J. Bruck, R. Cypher, P. Elustondo, A. Ho, C.-T. Ho, S. Kipnis, and Marc Snir. </author> <title> Ccl: A portable and tunable collective communication library for scalable parallel computers. </title> <type> Technical report, </type> <institution> IBM T. J. Watson Research Center, </institution> <year> 1993. </year> <type> Preprint. </type>
Reference-contexts: all send and receive routines specify numbers of data items of a particular type, whether built-in or user-defined, implementations have enough information to provide transaltions that allow an mpi program to run on heterogeneous networks. 3.4 Collective Communication Collective communication routines provide for coordinated communication among a group of processes <ref> [1, 3] </ref>. The process group and context is given by the intra-communicator object that is input to the routine. The mpi collective communication routines have been designed so that their syntax and semantics are consistent with those of the point-to-point routines.
Reference: [2] <author> A. Beguelin, J. J. Dongarra, G. A. Geist, R. Manchek, and V. S. Sunderam. </author> <title> A users' guide Copyright 1993 ACM 0-8186-4340-4/93/0011 Permission to copy for non-commercial use granted by the Association for Computing Machinery. to PVM parallel virtual machine. </title> <type> Technical Re--port TM-11826, </type> <institution> Oak Ridge National Laboratory, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: The functionality that mpi is designed to provide is based on current common practice, and is similar to that provided by widely-used message passing systems such as Express [15], PVM <ref> [2] </ref>, NX/2 [16], Vertex, [14], parmacs [10, 11], and P4 [4, 13]. In addition, the flexibility and usefulness of mpi has been broadened by incorporating ideas from more recent and innovative message passing systems such as chimp [6, 7], Zipcode [17, 18], and the IBM External User Interface [8].
Reference: [3] <author> J. Bruck, R. Cypher, P. Elustondo, A. Ho, C.-T. Ho, S. Kipnis, and Marc Snir. </author> <title> Ccl: A portable and tunable collective communication library for scalable parallel computers. </title> <type> Technical report, </type> <institution> IBM Almaden Research Center, </institution> <year> 1993. </year> <type> Preprint. </type>
Reference-contexts: all send and receive routines specify numbers of data items of a particular type, whether built-in or user-defined, implementations have enough information to provide transaltions that allow an mpi program to run on heterogeneous networks. 3.4 Collective Communication Collective communication routines provide for coordinated communication among a group of processes <ref> [1, 3] </ref>. The process group and context is given by the intra-communicator object that is input to the routine. The mpi collective communication routines have been designed so that their syntax and semantics are consistent with those of the point-to-point routines.
Reference: [4] <author> Ralph Butler and Ewing Lusk. </author> <title> User's guide to the p4 parallel programming system. </title> <type> Technical Report ANL-92/17, </type> <institution> Argonne National Laboratory, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: The functionality that mpi is designed to provide is based on current common practice, and is similar to that provided by widely-used message passing systems such as Express [15], PVM [2], NX/2 [16], Vertex, [14], parmacs [10, 11], and P4 <ref> [4, 13] </ref>. In addition, the flexibility and usefulness of mpi has been broadened by incorporating ideas from more recent and innovative message passing systems such as chimp [6, 7], Zipcode [17, 18], and the IBM External User Interface [8].
Reference: [5] <author> J. J. Dongarra, R. Hempel, A. J. G. Hey, and D. W. Walker. </author> <title> A proposal for a user-level, message passing interface in a distributed memory environment. </title> <type> Technical Report TM-12231, </type> <institution> Oak Ridge National Laboratory, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: At this workshop the basic features essential to a standard message passing interface were discussed, and a working group established to continue the standardization process. Following this a preliminary draft proposal, known as MPI1, was put forward by Dongarra, Hempel, Hey, and Walker <ref> [5] </ref>. This proposal was intended as a discussion document, and embodies the main features that were identified in the earlier workshop as being necessary in a message passing standard.
Reference: [6] <institution> Edinburgh Parallel Computing Centre, University of Edinburgh. CHIMP Concepts, </institution> <month> June </month> <year> 1991. </year>
Reference-contexts: In addition, the flexibility and usefulness of mpi has been broadened by incorporating ideas from more recent and innovative message passing systems such as chimp <ref> [6, 7] </ref>, Zipcode [17, 18], and the IBM External User Interface [8].
Reference: [7] <institution> Edinburgh Parallel Computing Centre, University of Edinburgh. </institution> <note> CHIMP Version 1.0 Interface, </note> <month> May </month> <year> 1992. </year>
Reference-contexts: In addition, the flexibility and usefulness of mpi has been broadened by incorporating ideas from more recent and innovative message passing systems such as chimp <ref> [6, 7] </ref>, Zipcode [17, 18], and the IBM External User Interface [8].
Reference: [8] <author> D. Frye, R. Bryant, H. Ho, R. Lawrence, and M. Snir. </author> <title> An external user interface for scalable parallel systems. </title> <type> Technical report, </type> <institution> IBM, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: In addition, the flexibility and usefulness of mpi has been broadened by incorporating ideas from more recent and innovative message passing systems such as chimp [6, 7], Zipcode [17, 18], and the IBM External User Interface <ref> [8] </ref>.
Reference: [9] <author> Heath, M. T. and J. A. Etheridge. </author> <year> 1991. </year> <title> "Visualizing the performance of parallel programs." </title> <type> Technical Report ORNL TM-11813. </type> <institution> Oak Ridge National Laboratory. </institution>
Reference-contexts: Thus, issues such as parallel I/O, parallel program composition, and debugging are not addressed by mpi. (Though mpi does provide a portable mechanism which will allow its intrumenta-tion and the collection of tracefiles for tools such as ParaGraph <ref> [9] </ref> or Upshot [12]). mpi does not provide support for active messages. mpi was designed easily to allow heterogeneous implementations and virtual communication channels.
Reference: [10] <author> R. Hempel. </author> <title> The ANL/GMD macros (PAR-MACS) in fortran for portable parallel programming using the message passing programming model users' guide and reference manual. </title> <type> Technical report, </type> <institution> GMD, </institution> <address> Postfach 1316, D-5205 Sankt Augustin 1, Germany, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: The functionality that mpi is designed to provide is based on current common practice, and is similar to that provided by widely-used message passing systems such as Express [15], PVM [2], NX/2 [16], Vertex, [14], parmacs <ref> [10, 11] </ref>, and P4 [4, 13]. In addition, the flexibility and usefulness of mpi has been broadened by incorporating ideas from more recent and innovative message passing systems such as chimp [6, 7], Zipcode [17, 18], and the IBM External User Interface [8].
Reference: [11] <author> R. Hempel, H.-C. Hoppe, and A. Supalov. </author> <title> A proposal for a PARMACS library interface. </title> <type> Technical report, </type> <institution> GMD, </institution> <address> Postfach 1316, D-5205 Sankt Augustin 1, Germany, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: The functionality that mpi is designed to provide is based on current common practice, and is similar to that provided by widely-used message passing systems such as Express [15], PVM [2], NX/2 [16], Vertex, [14], parmacs <ref> [10, 11] </ref>, and P4 [4, 13]. In addition, the flexibility and usefulness of mpi has been broadened by incorporating ideas from more recent and innovative message passing systems such as chimp [6, 7], Zipcode [17, 18], and the IBM External User Interface [8].
Reference: [12] <author> Virginia Herrarte and Ewing Lusk. </author> <title> Studying parallel program behavior with upshot. </title> <type> Technical Report ANL-91/15, </type> <institution> Argonne National Laboratory, </institution> <year> 1991. </year>
Reference-contexts: Thus, issues such as parallel I/O, parallel program composition, and debugging are not addressed by mpi. (Though mpi does provide a portable mechanism which will allow its intrumenta-tion and the collection of tracefiles for tools such as ParaGraph [9] or Upshot <ref> [12] </ref>). mpi does not provide support for active messages. mpi was designed easily to allow heterogeneous implementations and virtual communication channels.
Reference: [13] <editor> Ewing Lusk, Ross Overbeek, et al. </editor> <title> Portable Programs for Parallel Processors. </title> <publisher> Holt, Rinehart and Winston, Inc., </publisher> <year> 1987. </year> <title> [14] nCUBE Corporation. nCUBE 2 Programmers Guide, </title> <address> r2.0, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: The functionality that mpi is designed to provide is based on current common practice, and is similar to that provided by widely-used message passing systems such as Express [15], PVM [2], NX/2 [16], Vertex, [14], parmacs [10, 11], and P4 <ref> [4, 13] </ref>. In addition, the flexibility and usefulness of mpi has been broadened by incorporating ideas from more recent and innovative message passing systems such as chimp [6, 7], Zipcode [17, 18], and the IBM External User Interface [8].
Reference: [15] <author> Parasoft Corporation. </author> <title> Express Version 1.0: A Communication Environment for Parallel Computers, </title> <year> 1988. </year>
Reference-contexts: The functionality that mpi is designed to provide is based on current common practice, and is similar to that provided by widely-used message passing systems such as Express <ref> [15] </ref>, PVM [2], NX/2 [16], Vertex, [14], parmacs [10, 11], and P4 [4, 13].
Reference: [16] <author> Paul Pierce. </author> <title> The NX/2 operating system. </title> <booktitle> In Proceedings of the Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 384-390. </pages> <publisher> ACM Press, </publisher> <year> 1988. </year>
Reference-contexts: The functionality that mpi is designed to provide is based on current common practice, and is similar to that provided by widely-used message passing systems such as Express [15], PVM [2], NX/2 <ref> [16] </ref>, Vertex, [14], parmacs [10, 11], and P4 [4, 13]. In addition, the flexibility and usefulness of mpi has been broadened by incorporating ideas from more recent and innovative message passing systems such as chimp [6, 7], Zipcode [17, 18], and the IBM External User Interface [8].
Reference: [17] <author> A. Skjellum and A. Leung. </author> <title> Zipcode: a portable multicomputer communication library atop the reactive kernel. </title> <editor> In D. W. Walker and Q. F. Stout, editors, </editor> <booktitle> Proceedings of the Fifth Distributed Memory Concurrent Computing Conference, </booktitle> <pages> pages 767-776. </pages> <publisher> IEEE Press, </publisher> <year> 1990. </year>
Reference-contexts: In addition, the flexibility and usefulness of mpi has been broadened by incorporating ideas from more recent and innovative message passing systems such as chimp [6, 7], Zipcode <ref> [17, 18] </ref>, and the IBM External User Interface [8].
Reference: [18] <author> A. Skjellum, S. Smith, C. Still, A. Leung, and M. Morari. </author> <title> The Zipcode message passing system. </title> <type> Technical report, </type> <institution> Lawrence Livermore National Laboratory, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: In addition, the flexibility and usefulness of mpi has been broadened by incorporating ideas from more recent and innovative message passing systems such as chimp [6, 7], Zipcode <ref> [17, 18] </ref>, and the IBM External User Interface [8].
Reference: [19] <author> D. Walker. </author> <title> Standards for message passing in a distributed memory environment. </title> <type> Technical Report TM-12147, </type> <institution> Oak Ridge National Laboratory, </institution> <month> August </month> <year> 1992. </year> <title> Copyright 1993 ACM 0-8186-4340-4/93/0011 Permission to copy for non-commercial use granted by the Association for Computing Machinery. </title>
Reference-contexts: The standardization process began with the Workshop on Standards for Message Passing in a Distributed Memory Environment, sponsored by the Center for Research on Parallel Computing, held April 29-30, 1992, in Williamsburg, Virginia <ref> [19] </ref>. At this workshop the basic features essential to a standard message passing interface were discussed, and a working group established to continue the standardization process. Following this a preliminary draft proposal, known as MPI1, was put forward by Dongarra, Hempel, Hey, and Walker [5].
References-found: 18

