URL: http://www.cs.iastate.edu/~honavar/Papers/ieeetnn.ps
Refering-URL: http://www.cs.iastate.edu/~honavar/homepage.html
Root-URL: http://www.cs.iastate.edu
Email: rpare@allstate.com  yang@cs.iastate.edu  honavar@cs.iastate.edu  
Title: Constructive Neural Network Learning Algorithms for Pattern Classification  
Author: Rajesh Parekh Jihoon Yang Vasant Honavar 
Note: This research was partially supported by the National Science Foundation grants IRI-9409580 and IRI 9643299 to Vasant Honavar.  
Date: October 20, 1998  
Web: URL: http://www.cs.iastate.edu/~honavar/aigroup.html  
Address: 321 Middlefield Road, Menlo Park CA 94025, USA  Ames IA 50011, USA  Ames IA 50011, USA  
Affiliation: Allstate Research and Planning Center  Department of Computer Science, Iowa State University  Department of Computer Science, Iowa State University  
Abstract: Constructive learning algorithms offer an attractive approach for the incremental construction of near-minimal neural network architectures for pattern classification. They help overcome the need for ad hoc and often inappropriate choices of network topology in the use of algorithms that search for suitable weights in a priori fixed network architectures. Several such algorithms have been proposed in the literature and are shown to converge to zero classification errors (under certain assumptions) on tasks that involve learning a binary to binary mapping (i.e., classification problems involving binary valued input attributes and two output categories). We present two constructive learning algorithms MPyramid-real and MTiling-real that extend the pyramid [19] and tiling [32] algorithms respectively for learning real to M-ary mappings (i.e., classification problems involving real valued input attributes and multiple output classes). We prove the convergence of these algorithms and empirically demonstrate their applicability on practical pattern classification problems. Additionally, we show how the incorporation of a local pruning step can eliminate several redundant neurons from MTiling-real networks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Alpaydin. Gal: </author> <title> Networks that grow when they learn and shrink when they forget. </title> <type> Technical Report TR91-032, </type> <institution> International Computer Science Institute, Berkeley, </institution> <year> 1991. </year>
Reference-contexts: Another class of constructive learning algorithms that use a one-shot learning strategy deserves mention. These algorithms exploit the geometric properties of the training patterns to directly (i.e., in one-shot) determine appropriate weights for the neurons added to the network. The Grow and Learn (GAL) algorithm <ref> [1] </ref> and the DistAl algorithm [54] construct a single hidden layer network that implements a kind of nearest neighbor classification scheme.
Reference: [2] <author> G. Bakiri and T. Dietterich. </author> <title> Solving multiclass learning problems via error-correcting output codes. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 263-286, </pages> <year> 1995. </year>
Reference-contexts: The effectiveness of genetic algorithms for feature subset selection in conjunction with the DistAl algorithm has been demonstrated in [52]. * Using Boosting and Error-Correcting Output Codes for Improved Generalization Recent advances in machine learning have resulted in the development of techniques such as boosting [17] and error-correcting output codes <ref> [2] </ref> for improving the generalization capability of learning algorithms. Boosting involves training an ensemble of models and using a voting scheme to predict the classification of a formerly unseen pattern.
Reference: [3] <author> E. Baum. </author> <title> A proposal for more powerful learning algorithms. </title> <journal> Neural Computation, </journal> <volume> 1(2) </volume> <pages> 201-207, </pages> <year> 1989. </year>
Reference-contexts: Further, it has been shown that at least in principle, algorithms that are allowed to add neurons and weights represent a class of universal learners <ref> [3] </ref>. * Potential for matching the intrinsic complexity of the learning task It is desirable that a learning algorithm construct networks whose complexity (in terms of relevant criteria such as number of nodes, number of links, and connectivity) is commensurate with the intrinsic complexity of the underlying learning task (implicitly specified
Reference: [4] <author> K. Bennett and O. Mangasarian. </author> <title> Neural network training via linear programming. </title> <type> Technical Report Technical Report 948, </type> <institution> Department of Computer Science, University of Wisconsin-Madison, </institution> <year> 1990. </year>
Reference-contexts: Each hidden neuron is an exemplar representing a group of patterns that belong to the same class and are close to each other in terms of some suitably chosen distance metric. The minimizing resources method [44], the multisurface method <ref> [4] </ref>, and the voronoi diagram approach [5] are based on the idea of partitioning the input space by constructing linear hyperplanes. The partition identifies regions within the input space where each region represents patterns belonging to one particular output class.
Reference: [5] <author> N. Bose and A. Garga. </author> <title> Neural network design using voronoi diagrams. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(5) </volume> <pages> 778-787, </pages> <year> 1993. </year>
Reference-contexts: Each hidden neuron is an exemplar representing a group of patterns that belong to the same class and are close to each other in terms of some suitably chosen distance metric. The minimizing resources method [44], the multisurface method [4], and the voronoi diagram approach <ref> [5] </ref> are based on the idea of partitioning the input space by constructing linear hyperplanes. The partition identifies regions within the input space where each region represents patterns belonging to one particular output class.
Reference: [6] <author> N. Burgess. </author> <title> A constructive algorithm that converges for real-valued input patterns. </title> <journal> International Journal of Neural Systems, </journal> <volume> 5(1) </volume> <pages> 59-66, </pages> <year> 1994. </year>
Reference-contexts: Thus, it is of interest to design algorithms that can directly accept real-valued attributes. The perceptron cascade learning algorithm <ref> [6] </ref> uses a projection based idea for handling real valued attributes in two category pattern classification. <p> An extension of the upstart algorithm [46] and the perceptron cascade algorithm <ref> [6] </ref> proposed a preprocessing technique to handle patterns with real valued attributes wherein the patterns are projected on to a parabolic surface by appending to each pattern (X p =&lt; X p p additional attribute X p P N p i ) 2 . <p> The initial temperature T 0 was set to 1.0 and was dynamically updated at the end of each epoch to match the average net input of the neuron (s) during the entire epoch <ref> [6] </ref>. Table 8 summarizes the results of experiments designed to test the convergence properties of the constructive learning algorithms.
Reference: [7] <author> C-H. Chen, R. Parekh, J. Yang, K. Balakrishnan, and V. Honavar. </author> <title> Analysis of decision boundaries generated by constructive neural network learning algorithms. </title> <booktitle> In Proceedings of WCNN'95, </booktitle> <address> July 17-21, Washington D.C., </address> <booktitle> volume 1, </booktitle> <pages> pages 628-635, </pages> <year> 1995. </year>
Reference-contexts: A gemoetrical analysis of the decision boundaries of some of these algorithms is presented in <ref> [7] </ref>. Practical pattern classification often requires assigning patterns to M (where M &gt; 2) categories.
Reference: [8] <author> T. Cormen, C. Leiserson, and Rivest R. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1991. </year>
Reference-contexts: The interested reader is referred to <ref> [8] </ref> for a detailed description of convex hulls and related topics in computational geometry. 16 From equation 4, the net input of neuron L j in response to the prototype p is: n L j = W L j ;0 + k=1 p = 2C j + jL 1j j +
Reference: [9] <author> M. Craven. </author> <title> Extracting Comprehensible Models from Trained Neural Networks. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Wisconsin, Madison, WI, </institution> <year> 1996. </year>
Reference-contexts: The question now is whether we can use some of the existing strategies (see for example <ref> [9] </ref>) or design suitable new methods for extracting the learned knowledge from a trained constructive network. 26
Reference: [10] <author> J. Dayhoff. </author> <title> Neural Network Architectures: An Introduction. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction Artificial neural networks have been successfully applied to solve problems in pattern classification, function approximation, optimization, pattern matching and associative memories <ref> [10, 20, 31] </ref>. Multilayer feedforward networks trained using the backpropagation learning algorithm [45] are limited to searching for a suitable set of weights in an a priori fixed network topology. This makes it important to select an appropriate network topology for the learning problem on hand.
Reference: [11] <author> J. Dougherty, R. Kohavi, and M. Sahami. </author> <title> Supervised and unsupervised discretization of continuous features. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 194-202, </pages> <address> San Fransisco, CA, </address> <year> 1995. </year>
Reference-contexts: Real-valued attributes are almost invariably encountered in practical classification tasks. One work around for this problem is to discretize the real-valued attributes prior to training. Although a variety of discretization techniques have been proposed in the literature (see <ref> [11] </ref> for a survey), these can result in a loss of information and also vastly increase the number of input attributes. Thus, it is of interest to design algorithms that can directly accept real-valued attributes.
Reference: [12] <author> R. Duda and P. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: However, there are no known efficient methods for determining the optimal network topology for a given problem. Too small networks are unable to adequately learn the problem well while overly large networks tend to overfit the training data and consequently result in poor generalization performance (see <ref> [12] </ref> for an analogy to the curve fitting problem). In practice, a variety of architectures are tried out and the one that appears best suited to the given problem is picked.
Reference: [13] <author> S. Fahlman and C. Lebiere. </author> <title> The cascade correlation learning algorithm. </title> <editor> In D. Touretzky, editor, </editor> <booktitle> Neural Information Systems 2, </booktitle> <pages> pages 524-532. </pages> <address> Morgan-Kauffman, </address> <year> 1990. </year>
Reference-contexts: These algorithms start with a small network (usually a single neuron) and dynamically grow the network by adding and training neurons as needed until a satisfactory solution is found <ref> [13, 15, 19, 23, 29, 32] </ref>. <p> Alternatively, algorithms such as the cascade correlation family construct multilayer networks wherein the structural 3 inter-connections among the hidden neurons allow the network to approximate complex func-tions using relatively simple neuron transfer functions like the sigmoid <ref> [13, 40, 51] </ref>. Pattern classification is a special case of function approximation where the function's output y is restricted to one of M (M 2) discrete values (or classes) i.e., it involves a real to M-ary mapping.
Reference: [14] <author> J. Fletcher and Z. Obradovic. </author> <title> Combining prior symbolic knowledge and constructive neural netwo rk learning. </title> <booktitle> Connection Science, </booktitle> <address> 5(3,4):365-375, </address> <year> 1993. </year>
Reference-contexts: Different constructive learning algorithms allow trading off certain performance measures (e.g., learning time) for others (e.g., network size and generalization accuracy) [48]. * Incorporation of prior knowledge Constructive algorithms provide a natural framework for incorporating problem-specific knowledge into initial network configurations and for modifying this knowledge using additional training examples <ref> [14, 35, 36] </ref>. * Lifelong Learning Recent research in lifelong learning [49] has proposed training networks to learn to solve multiple related problems by building on the knowledge acquired from the simpler problems in learning the more difficult ones. Constructive learning algorithms lend themselves well to the lifelong learning framework. <p> Thirdly, threshold functions can be clearly described in terms of simple "if-then-else" rules. This makes it easier to incorporate domain expertise (which is usually available in the form of if-then-else rules) into a network of threshold neurons <ref> [14] </ref> 2 . Similar argument suggests that the task of extracting learned knowledge from a network of threshold neurons would be considerably simpler. <p> An application of these techniques in the constructive learning framework is clearly of interest. * Knowledge Extraction from Trained Constructive Neural Networks The incorporation of domain specific knowledge into an initial network topology and its subsequent refinement using constructive learning has been studied in <ref> [14, 35, 36] </ref>. The question now is whether we can use some of the existing strategies (see for example [9]) or design suitable new methods for extracting the learned knowledge from a trained constructive network. 26
Reference: [15] <author> M. Frean. </author> <title> The upstart algorithm: A method for constructing and training feedforward neural networks. </title> <journal> Neural Computation, </journal> <volume> 2 </volume> <pages> 198-209, </pages> <year> 1990. </year>
Reference-contexts: These algorithms start with a small network (usually a single neuron) and dynamically grow the network by adding and training neurons as needed until a satisfactory solution is found <ref> [13, 15, 19, 23, 29, 32] </ref>. <p> learning of networks of threshold neurons for pattern classification. 1.3.1 Constructive Learning using Iterative Weight Update A number of algorithms that incrementally construct networks of threshold neurons for learning the binary to binary mapping have been proposed in the literature (for example, the tower , pyramid [19], tiling [32], upstart <ref> [15] </ref>, oil-spot [30], and sequential [29] algorithms).
Reference: [16] <author> M. Frean. </author> <title> A thermal perceptron learning rule. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 946-957, </pages> <year> 1992. </year>
Reference-contexts: Several modifications to the perceptron algorithm (e.g., the pocket algorithm with ratchet modification [19], the thermal perceptron algorithm <ref> [16] </ref>, the loss minimization algorithm [25], and the barycentric correction procedure [39]) are proposed to find a reasonably good weight vector that correctly classifies a large fraction of the training set S when S is not linearly separable and to converge to zero classification errors when S is linearly separable.
Reference: [17] <author> Y. Freund and R. Schapire. </author> <title> A decision-theoretic generalization of on-line learning algorithms and an application to boosting. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 55(1) </volume> <pages> 119-139, </pages> <year> 1997. </year>
Reference-contexts: The effectiveness of genetic algorithms for feature subset selection in conjunction with the DistAl algorithm has been demonstrated in [52]. * Using Boosting and Error-Correcting Output Codes for Improved Generalization Recent advances in machine learning have resulted in the development of techniques such as boosting <ref> [17] </ref> and error-correcting output codes [2] for improving the generalization capability of learning algorithms. Boosting involves training an ensemble of models and using a voting scheme to predict the classification of a formerly unseen pattern.
Reference: [18] <author> J. Friedman and W. Stuetzle. </author> <title> Projection pursuit regression. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 76(376) </volume> <pages> 817-823, </pages> <year> 1981. </year>
Reference-contexts: To overcome this difficulty, some constructive algorithms use the gaussian [21] or some more sophisticated transfer function while others such as the projection pursuit regression <ref> [18] </ref> use a summation of several nonlinear transfer functions. Alternatively, algorithms such as the cascade correlation family construct multilayer networks wherein the structural 3 inter-connections among the hidden neurons allow the network to approximate complex func-tions using relatively simple neuron transfer functions like the sigmoid [13, 40, 51].
Reference: [19] <author> S. Gallant. </author> <title> Perceptron based learning algorithms. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(2) </volume> <pages> 179-191, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: These algorithms start with a small network (usually a single neuron) and dynamically grow the network by adding and training neurons as needed until a satisfactory solution is found <ref> [13, 15, 19, 23, 29, 32] </ref>. <p> will focus on constructive learning of networks of threshold neurons for pattern classification. 1.3.1 Constructive Learning using Iterative Weight Update A number of algorithms that incrementally construct networks of threshold neurons for learning the binary to binary mapping have been proposed in the literature (for example, the tower , pyramid <ref> [19] </ref>, tiling [32], upstart [15], oil-spot [30], and sequential [29] algorithms). <p> Several modifications to the perceptron algorithm (e.g., the pocket algorithm with ratchet modification <ref> [19] </ref>, the thermal perceptron algorithm [16], the loss minimization algorithm [25], and the barycentric correction procedure [39]) are proposed to find a reasonably good weight vector that correctly classifies a large fraction of the training set S when S is not linearly separable and to converge to zero classification errors when <p> Note that bipolar TLUs implement the sgn function of their net input. The input layer neurons are designed to allow the patterns to be input to the network and thus simply copy their input to their output. 3 The MPyramid-real Algorithm The pyramid algorithm <ref> [19] </ref> constructs a layered network of TLUs by successively placing each new TLU above the existing ones. The first neuron receives inputs from the N input neurons. Each succeeding neuron receives inputs from the N input neurons and from each of the neurons below itself.
Reference: [20] <author> S. Gallant. </author> <title> Neural Network Learning and Expert Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction Artificial neural networks have been successfully applied to solve problems in pattern classification, function approximation, optimization, pattern matching and associative memories <ref> [10, 20, 31] </ref>. Multilayer feedforward networks trained using the backpropagation learning algorithm [45] are limited to searching for a suitable set of weights in an a priori fixed network topology. This makes it important to select an appropriate network topology for the learning problem on hand. <p> These algorithms start with a small network (usually a single neuron) and dynamically grow the network by adding and training neurons as needed until a satisfactory solution is found [13, 15, 19, 23, 29, 32]. Some key motivations <ref> [20, 24] </ref> for studying constructive neural network learning algorithms are: * Flexibility of exploring the space of neural network topologies Constructive algorithms overcome the limitation of searching for a solution in the weight space of an a priori fixed network architecture by extending the search, in a controlled fashion, to the <p> The damping factor 4 Although bipolar TLUs whose outputs are 1 and 1 is functionally equivalent to binary TLUs whose outputs are 1 and 0 empirical evidence suggests that networks constructed using bipolar TLUs are often smaller than networks constructed using binary neurons for the same task <ref> [20] </ref>. 6 (e jn p j=T ) prevents any large weight changes towards the end of the training thereby avoiding any irreversible deterioration in the TLU's classification accuracy. 2.2 Multiclass Discrimination Classification problems involving M (M &gt; 2) output classes require a layer of M TLUs. <p> WTA training offers an advantage over independent training in that pattern classes that are only pairwise separable from each other can be correctly classified using WTA training while in independent training only pattern classes that are independently separable from all the other classes can be correctly classified <ref> [20] </ref>. 2.3 Preprocessing Most constructive learning algorithms are designed for binary (or bipolar) valued inputs.
Reference: [21] <author> S. Geva and J. Sitte. </author> <title> A constructive method for multivariate function approximation by multilayer perceptrons. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3(4) </volume> <pages> 621-624, </pages> <year> 1992. </year> <month> 27 </month>
Reference-contexts: Often the unknown target function (f ) is inherently complex and cannot be closely approximated by a network comprising of a single hidden layer of neurons implementing simple transfer functions (e.g., sigmoid). To overcome this difficulty, some constructive algorithms use the gaussian <ref> [21] </ref> or some more sophisticated transfer function while others such as the projection pursuit regression [18] use a summation of several nonlinear transfer functions.
Reference: [22] <author> C. Giles, D. Chen, Guo-Zheng Sun, Hsing-Hen Chen, Yee-Chung Lee, and M Goudreau. </author> <title> Construc--tive learning of recurrent neural networks: Limitations of recurrent cascade correlation and a simple solution. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 6(4) </volume> <pages> 829-836, </pages> <year> 1997. </year>
Reference-contexts: The interested reader is referred to <ref> [22, 26] </ref> for a discussion on constructive learning of RNN. 25 gorithm first (during the preliminary analysis) as it tends to have better convergence properties than the MPyramid-real algorithm in practice.
Reference: [23] <author> V. Honavar. </author> <title> Generative Learning Structures and Processes for Generalized Connectionist Networks. </title> <type> PhD thesis, </type> <institution> University of Wisconsin, Madison, </institution> <year> 1990. </year>
Reference-contexts: These algorithms start with a small network (usually a single neuron) and dynamically grow the network by adding and training neurons as needed until a satisfactory solution is found <ref> [13, 15, 19, 23, 29, 32] </ref>.
Reference: [24] <author> V. Honavar and L Uhr. </author> <title> Generative learning structures for generalized connectionist networks. </title> <journal> Information Sciences, </journal> <volume> 70(1-2):75-108, </volume> <year> 1993. </year>
Reference-contexts: These algorithms start with a small network (usually a single neuron) and dynamically grow the network by adding and training neurons as needed until a satisfactory solution is found [13, 15, 19, 23, 29, 32]. Some key motivations <ref> [20, 24] </ref> for studying constructive neural network learning algorithms are: * Flexibility of exploring the space of neural network topologies Constructive algorithms overcome the limitation of searching for a solution in the weight space of an a priori fixed network architecture by extending the search, in a controlled fashion, to the
Reference: [25] <author> T. Hrycej. </author> <title> Modular Learning in Neural Networks. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: Several modifications to the perceptron algorithm (e.g., the pocket algorithm with ratchet modification [19], the thermal perceptron algorithm [16], the loss minimization algorithm <ref> [25] </ref>, and the barycentric correction procedure [39]) are proposed to find a reasonably good weight vector that correctly classifies a large fraction of the training set S when S is not linearly separable and to converge to zero classification errors when S is linearly separable.
Reference: [26] <author> S. Kremer. </author> <title> Comments on "constructive learning of recurrent neural networks: Limitations of recurrent cascade correlation and a simple solution. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 7(4) </volume> <pages> 1047-1049, </pages> <year> 1996. </year>
Reference-contexts: The interested reader is referred to <ref> [22, 26] </ref> for a discussion on constructive learning of RNN. 25 gorithm first (during the preliminary analysis) as it tends to have better convergence properties than the MPyramid-real algorithm in practice.
Reference: [27] <author> Tin-Yau Kwok and Dit-Yan Yeung. </author> <title> Objective functions for training new hidden units in constructive neural networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 8(5) </volume> <pages> 1131-1148, </pages> <year> 1997. </year>
Reference-contexts: the intrinsic complexity of the learning task It is desirable that a learning algorithm construct networks whose complexity (in terms of relevant criteria such as number of nodes, number of links, and connectivity) is commensurate with the intrinsic complexity of the underlying learning task (implicitly specified by the training data) <ref> [27] </ref>. Constructive algorithms search for small solutions first and thus offer a potential for discovering a near-minimal network that suitably matches the complexity of the learning task.
Reference: [28] <author> Tin-Yau Kwok and Dit-Yan Yeung. </author> <title> Constructive algorithms for structure learning in feedforward neural networks for regression problems. </title> <journal> IEEE Transactions on Neural Networks, </journal> <note> 1999. (To appear). </note>
Reference-contexts: Constructive algorithms offer several significant advantages over pruning based algorithms including the ease of specification of the initial network topology, better economy in terms of training time and number of training examples, and potential for converging to a smaller network with superior generalization <ref> [28] </ref>. In this paper we will focus primarily on constructive learning algorithms. <p> A variety of constructive neural network learning algorithms have been proposed for solving the general function approximation problem (see <ref> [28] </ref> for a survey). These algorithms typically use a greedy strategy wherein each new neuron added to the network is trained to minimize the residual error as much as possible.
Reference: [29] <author> M. Marchand, M. Golea, and P. Rujan. </author> <title> A convergence theorem for sequential learning in two-layer perceptrons. </title> <journal> Europhysics Letters, </journal> <volume> 11(6) </volume> <pages> 487-492, </pages> <year> 1990. </year>
Reference-contexts: These algorithms start with a small network (usually a single neuron) and dynamically grow the network by adding and training neurons as needed until a satisfactory solution is found <ref> [13, 15, 19, 23, 29, 32] </ref>. <p> neurons for pattern classification. 1.3.1 Constructive Learning using Iterative Weight Update A number of algorithms that incrementally construct networks of threshold neurons for learning the binary to binary mapping have been proposed in the literature (for example, the tower , pyramid [19], tiling [32], upstart [15], oil-spot [30], and sequential <ref> [29] </ref> algorithms).
Reference: [30] <author> F. Mascioli and G. Martinelli. </author> <title> A constructive algorithm for binary neural networks: The oil-spot algorithm. </title> <journal> IEEE Transactionson Neural Networks, </journal> <volume> 6(3) </volume> <pages> 794-797, </pages> <year> 1995. </year>
Reference-contexts: networks of threshold neurons for pattern classification. 1.3.1 Constructive Learning using Iterative Weight Update A number of algorithms that incrementally construct networks of threshold neurons for learning the binary to binary mapping have been proposed in the literature (for example, the tower , pyramid [19], tiling [32], upstart [15], oil-spot <ref> [30] </ref>, and sequential [29] algorithms).
Reference: [31] <author> K. Mehrotra, C. Mohan, and S. Ranka. </author> <title> Elements of Artificial Neural Networks. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1997. </year>
Reference-contexts: 1 Introduction Artificial neural networks have been successfully applied to solve problems in pattern classification, function approximation, optimization, pattern matching and associative memories <ref> [10, 20, 31] </ref>. Multilayer feedforward networks trained using the backpropagation learning algorithm [45] are limited to searching for a suitable set of weights in an a priori fixed network topology. This makes it important to select an appropriate network topology for the learning problem on hand.
Reference: [32] <author> M. Mezard and J. Nadal. </author> <title> Learning feed-forward networks: The tiling algorithm. </title> <journal> J. Phys. A: Math. Gen., </journal> <volume> 22 </volume> <pages> 2191-2203, </pages> <year> 1989. </year>
Reference-contexts: These algorithms start with a small network (usually a single neuron) and dynamically grow the network by adding and training neurons as needed until a satisfactory solution is found <ref> [13, 15, 19, 23, 29, 32] </ref>. <p> on constructive learning of networks of threshold neurons for pattern classification. 1.3.1 Constructive Learning using Iterative Weight Update A number of algorithms that incrementally construct networks of threshold neurons for learning the binary to binary mapping have been proposed in the literature (for example, the tower , pyramid [19], tiling <ref> [32] </ref>, upstart [15], oil-spot [30], and sequential [29] algorithms). <p> Number Net Input Output n L 2 n L 2 n L 2 o L 2 o L 2 o L 2 1 -2.25 -2.25 2.25 -1 -1 1 3 -3.25 0.75 -0.75 -1 1 -1 5 0.25 -3.75 -0.25 1 -1 -1 4 The MTiling-real Algorithm The tiling algorithm <ref> [32] </ref> constructs a strictly layered network of threshold neurons. The bottom-most layer receives inputs from each of the N input neurons. The neurons in each subsequent layer receive inputs from those in the layer immediately below itself. <p> The faithfulness criterion states that no two training examples belonging to different classes should produce identical output at any given layer. Faithfulness is clearly a necessary condition for convergence in strictly layered networks <ref> [32] </ref>. The proposed extension to multiple output classes involves constructing layers with M master neurons (one for each of the output classes) 6 . Unlike the MPyramid-real algorithm, it is not necessary to preprocess the dataset using projection.
Reference: [33] <author> R. Mooney, J. Shavlik, G. Towell, and Alan Gove. </author> <title> An experimental comparison of symbolic and connectionist learning algorithms. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 775-780. </pages> <publisher> Morgan Kauffman, </publisher> <year> 1989. </year>
Reference-contexts: It is likely that these datasets contain a set of carefully engineered features that were selected by the experts to work well with the algorithms existing at that time <ref> [33] </ref>. This might result in a scenario where it is not possible to improve the generalization on a particular dataset beyond what is achieved by a single layer network. Additionally, it is possible that these datasets contain irrelevant or noisy attributes that complicate the learning task.
Reference: [34] <author> P. Murphy and D. Aha. </author> <title> Repository of machine learning databases. </title> <institution> Department of Information and Computer Science, University of California, </institution> <address> Irvine, CA, </address> <year> 1994. </year>
Reference-contexts: The real-world datasets ionosphere, pima, segmentation, and vehicle are available at the UCI Machine Learning Repository <ref> [34] </ref> while the 3-circles dataset was artificially generated. The 3-circles dataset comprises of 1800 randomly drawn points from the two dimensional Euclidean space.
Reference: [35] <author> D. W. Opitz and J. W. Shavlik. </author> <title> Dynamically adding symbolically meaningful nodes to knowledge-based neural networks. </title> <journal> Knowledge-Based Systems, </journal> <volume> 8(6) </volume> <pages> 301-311, </pages> <year> 1995. </year>
Reference-contexts: Different constructive learning algorithms allow trading off certain performance measures (e.g., learning time) for others (e.g., network size and generalization accuracy) [48]. * Incorporation of prior knowledge Constructive algorithms provide a natural framework for incorporating problem-specific knowledge into initial network configurations and for modifying this knowledge using additional training examples <ref> [14, 35, 36] </ref>. * Lifelong Learning Recent research in lifelong learning [49] has proposed training networks to learn to solve multiple related problems by building on the knowledge acquired from the simpler problems in learning the more difficult ones. Constructive learning algorithms lend themselves well to the lifelong learning framework. <p> An application of these techniques in the constructive learning framework is clearly of interest. * Knowledge Extraction from Trained Constructive Neural Networks The incorporation of domain specific knowledge into an initial network topology and its subsequent refinement using constructive learning has been studied in <ref> [14, 35, 36] </ref>. The question now is whether we can use some of the existing strategies (see for example [9]) or design suitable new methods for extracting the learned knowledge from a trained constructive network. 26
Reference: [36] <author> R. Parekh and V. Honavar. </author> <title> Constructive theory refinement in knowledge based neural networks. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks (IJCNN'98), </booktitle> <pages> pages 2318-2323, </pages> <address> Anchorage, AK, </address> <year> 1998. </year>
Reference-contexts: Different constructive learning algorithms allow trading off certain performance measures (e.g., learning time) for others (e.g., network size and generalization accuracy) [48]. * Incorporation of prior knowledge Constructive algorithms provide a natural framework for incorporating problem-specific knowledge into initial network configurations and for modifying this knowledge using additional training examples <ref> [14, 35, 36] </ref>. * Lifelong Learning Recent research in lifelong learning [49] has proposed training networks to learn to solve multiple related problems by building on the knowledge acquired from the simpler problems in learning the more difficult ones. Constructive learning algorithms lend themselves well to the lifelong learning framework. <p> An application of these techniques in the constructive learning framework is clearly of interest. * Knowledge Extraction from Trained Constructive Neural Networks The incorporation of domain specific knowledge into an initial network topology and its subsequent refinement using constructive learning has been studied in <ref> [14, 35, 36] </ref>. The question now is whether we can use some of the existing strategies (see for example [9]) or design suitable new methods for extracting the learned knowledge from a trained constructive network. 26
Reference: [37] <author> R. Parekh, J. Yang, and V. Honavar. </author> <title> Constructive neural network learning algorithms for multi-category real-valued pattern classification. </title> <type> Technical Report ISU-CS-TR97-06, </type> <institution> Department of Computer Science, Iowa State University, </institution> <year> 1997. </year>
Reference-contexts: The interested reader is referred to <ref> [37] </ref> for an application of this framework to the tower , upstart , perceptron cascade , and sequential learning algorithms. 5 2 Preliminaries 2.1 Threshold Logic Units A N -input threshold logic unit (TLU, also known as a perceptron) is an elementary processing unit that computes the threshold (hard-limiting) function of <p> For each of these algorithms we have provided rigorous proofs of convergence to zero classification errors on finite, non-contradictory training sets. This proof technique is sufficiently general (see <ref> [37] </ref> for an application of this technique to several other constructive learning algorithms).
Reference: [38] <author> R. Parekh, J. Yang, and V. Honavar. </author> <title> Pruning strategies for constructive neural network learning algorithms. </title> <booktitle> In Proceedings of the IEEE/INNS International Conference on Neural Networks, ICNN'97, </booktitle> <pages> pages 1960-1965, </pages> <year> 1997. </year>
Reference-contexts: Further, since each neuron only outputs 1 or 1 and the faithfulness test only requires an equality check, the search for redundant neurons can be performed very efficiently. We conducted an experimental study of pruning in MTiling-real networks (see <ref> [38] </ref> for details) and found that the total time spent in searching for and pruning redundant neurons is less than 10% of the total training time. Further, pruning resulted in a moderate to substantial (at times as high as 50%) reduction in network size.
Reference: [39] <author> H. Poulard. </author> <title> Barycentric correction procedure: A fast method of learning threshold units. </title> <booktitle> In Proceedings of WCNN'95, </booktitle> <address> July 17-21, Washington D.C., </address> <booktitle> volume 1, </booktitle> <pages> pages 710-713, </pages> <year> 1995. </year>
Reference-contexts: Several modifications to the perceptron algorithm (e.g., the pocket algorithm with ratchet modification [19], the thermal perceptron algorithm [16], the loss minimization algorithm [25], and the barycentric correction procedure <ref> [39] </ref>) are proposed to find a reasonably good weight vector that correctly classifies a large fraction of the training set S when S is not linearly separable and to converge to zero classification errors when S is linearly separable.
Reference: [40] <author> L. Prechelt. </author> <title> Investigating the cascor family of learning algorithms. </title> <booktitle> Neural Networks, </booktitle> <volume> 10(5) </volume> <pages> 885-896, </pages> <year> 1997. </year>
Reference-contexts: Alternatively, algorithms such as the cascade correlation family construct multilayer networks wherein the structural 3 inter-connections among the hidden neurons allow the network to approximate complex func-tions using relatively simple neuron transfer functions like the sigmoid <ref> [13, 40, 51] </ref>. Pattern classification is a special case of function approximation where the function's output y is restricted to one of M (M 2) discrete values (or classes) i.e., it involves a real to M-ary mapping.
Reference: [41] <author> R. Reed. </author> <title> Pruning algorithms | a survey. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(5) </volume> <pages> 740-747, </pages> <year> 1993. </year>
Reference-contexts: Pruning techniques (see <ref> [41] </ref> for an excellent survey) begin by training a larger than necessary network and then eliminate weights and neurons that are deemed redundant.
Reference: [42] <author> B. Ripley. </author> <title> Pattern Recognition and Neural Networks. </title> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: Several feature subset selection algorithms have been proposed in the pattern recognition literature <ref> [42] </ref>.
Reference: [43] <author> F. Rosenblatt. </author> <title> The perceptron: A probabilistic model for information storage and organization in the brain. </title> <journal> Psychological Review, </journal> <volume> 65 </volume> <pages> 386-408, </pages> <year> 1958. </year>
Reference-contexts: Threshold neurons offer the following advantages over their continuous counterparts: Firstly, they are potentially easier to implement in hardware. Secondly, the perceptron learning rule <ref> [43] </ref> is a simple iterative procedure for training threshold neurons. The learning rules for sigmoid neurons and the like are more complicated and thus computation-ally more expensive. Thirdly, threshold functions can be clearly described in terms of simple "if-then-else" rules. <p> A TLU can be trained using the perceptron learning rule <ref> [43] </ref> (W W + (C p O p )X p where &gt; 0 is the learning rate) to attempt to find a weight vector ^ W such that 8X p 2 S + , ^ W X p 0 and 8X q 2 S , ^ W X q &lt; 0.
Reference: [44] <author> P. Rujan and M. Marchand. </author> <title> Learning by minimizing resources in neural networks. </title> <journal> Complex Systems, </journal> <volume> 3 </volume> <pages> 229-241, </pages> <year> 1989. </year>
Reference-contexts: Each hidden neuron is an exemplar representing a group of patterns that belong to the same class and are close to each other in terms of some suitably chosen distance metric. The minimizing resources method <ref> [44] </ref>, the multisurface method [4], and the voronoi diagram approach [5] are based on the idea of partitioning the input space by constructing linear hyperplanes. The partition identifies regions within the input space where each region represents patterns belonging to one particular output class.
Reference: [45] <author> D. Rumelhart, G. Hinton, and R. Williams. </author> <title> Learning internal representations by error propagation. In Parallel Distributed Processing: Explorations into the Microstructure of Cognition, </title> <booktitle> volume 1 (Foundations). </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: 1 Introduction Artificial neural networks have been successfully applied to solve problems in pattern classification, function approximation, optimization, pattern matching and associative memories [10, 20, 31]. Multilayer feedforward networks trained using the backpropagation learning algorithm <ref> [45] </ref> are limited to searching for a suitable set of weights in an a priori fixed network topology. This makes it important to select an appropriate network topology for the learning problem on hand.
Reference: [46] <author> J. Saffery and C. Thornton. </author> <title> Using stereographic projection as a preprocessing technique for upstart. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <pages> pages II 441-446. </pages> <publisher> IEEE Press, </publisher> <month> July </month> <year> 1991. </year>
Reference-contexts: An extension of the upstart algorithm <ref> [46] </ref> and the perceptron cascade algorithm [6] proposed a preprocessing technique to handle patterns with real valued attributes wherein the patterns are projected on to a parabolic surface by appending to each pattern (X p =&lt; X p p additional attribute X p P N p i ) 2 .
Reference: [47] <author> K-Y. Siu, V. Roychowdhury, and T. Kailath. </author> <title> Discrete Neural Computation ATheoretical Foundation. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1995. </year>
Reference-contexts: Siu et al have established the necessary and sufficient conditions for a training set S to be non-linearly separable <ref> [47] </ref>. They have also shown that the problem of identifying a largest linearly separable subset of S is NP-complete. <p> Finding an optimal weight setting for each added neuron such that the classification error is maximally reduced when the the data is non-separable is a NP-hard problem <ref> [47] </ref>. Further, in practice the heuristic algorithms such as the thermal perceptron algorithm that are used in constructive learning are only allowed limited training time (say 500 or 1000 epochs). This makes it important to study the convergence of the proposed constructive algorithms in practice. 21 2.
Reference: [48] <author> F. Smieja. </author> <title> Neural network constructive algorithms: Trading generalization for learning efficiency? Circuits, </title> <journal> Systems, and Signal Processing, </journal> <volume> 12(2) </volume> <pages> 331-374, </pages> <year> 1993. </year>
Reference-contexts: Constructive algorithms, if successful, can 2 provide useful empirical estimates of the expected case complexity of practical learning problems. * Trade-offs among performance measures Different constructive learning algorithms allow trading off certain performance measures (e.g., learning time) for others (e.g., network size and generalization accuracy) <ref> [48] </ref>. * Incorporation of prior knowledge Constructive algorithms provide a natural framework for incorporating problem-specific knowledge into initial network configurations and for modifying this knowledge using additional training examples [14, 35, 36]. * Lifelong Learning Recent research in lifelong learning [49] has proposed training networks to learn to solve multiple related <p> They can be shown to converge to networks which yield zero classification errors on any non-contradictory training set involving two output classes (see <ref> [48] </ref> for a unifying framework that explains the convergence properties of these constructive algorithms). A gemoetrical analysis of the decision boundaries of some of these algorithms is presented in [7]. Practical pattern classification often requires assigning patterns to M (where M &gt; 2) categories. <p> The geometric approach to constructive learning can be applied successfully in solving small to medium scale problems. However, the global search strategy employed by these algorithms can pose a limitation when learning from very large training sets <ref> [48] </ref>. Further, the reliance on a suitably chosen distance metric (in the case of GAL and DistAl) makes it imperative for the user to try out a variety of distance metrics for each learning problem.
Reference: [49] <author> S. Thrun. </author> <title> Lifelong learning: A case study. </title> <type> Technical Report CMU-CS-95-208, </type> <institution> Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: learning time) for others (e.g., network size and generalization accuracy) [48]. * Incorporation of prior knowledge Constructive algorithms provide a natural framework for incorporating problem-specific knowledge into initial network configurations and for modifying this knowledge using additional training examples [14, 35, 36]. * Lifelong Learning Recent research in lifelong learning <ref> [49] </ref> has proposed training networks to learn to solve multiple related problems by building on the knowledge acquired from the simpler problems in learning the more difficult ones. Constructive learning algorithms lend themselves well to the lifelong learning framework.
Reference: [50] <author> G. G. Towell, J. W. Shavlik, and M. O. Noordwier. </author> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 861-866, </pages> <address> Boston, MA, </address> <year> 1990. </year>
Reference-contexts: Although in principle, the M category classification task can be decomposed into M 2-category classification tasks, this approach does not take 1 A single output neuron suffices in the case of problems that involve two category classification. 2 Note that even the KBANN algorithm <ref> [50] </ref> that uses the backpropagation algorithm for connectionist theory refinement treats the individual sigmoid neurons as threshold neurons for the purpose of incorporating the domain theory into an initial neural network. 4 into account the inter-relationships between the M output categories.
Reference: [51] <author> J. Yang and V. Honavar. </author> <title> Experiments with the cascade-correlation algorithm. Microcomputer Applications, </title> <note> 1998. (To appear). </note>
Reference-contexts: Alternatively, algorithms such as the cascade correlation family construct multilayer networks wherein the structural 3 inter-connections among the hidden neurons allow the network to approximate complex func-tions using relatively simple neuron transfer functions like the sigmoid <ref> [13, 40, 51] </ref>. Pattern classification is a special case of function approximation where the function's output y is restricted to one of M (M 2) discrete values (or classes) i.e., it involves a real to M-ary mapping.
Reference: [52] <author> J. Yang and V. Honavar. </author> <title> Feature subset selection using a genetic algorithm. IEEE Intelligent Systems (Sepcial Issue on Feature Transformation and Subset Selection), </title> <booktitle> 13(2) </booktitle> <pages> 44-49, </pages> <year> 1998. </year>
Reference-contexts: Additionally, it is possible that these datasets contain irrelevant or noisy attributes that complicate the learning task. Experimental results have shown that using 24 a genetic algorithm based feature feature selection scheme significantly boosts the generalization performance of the DistAl learning algorithm <ref> [52] </ref>. 6 Conclusions Constructive algorithms offer an attractive approach to the automated design of neural networks for pattern classification. <p> Several feature subset selection algorithms have been proposed in the pattern recognition literature [42]. The effectiveness of genetic algorithms for feature subset selection in conjunction with the DistAl algorithm has been demonstrated in <ref> [52] </ref>. * Using Boosting and Error-Correcting Output Codes for Improved Generalization Recent advances in machine learning have resulted in the development of techniques such as boosting [17] and error-correcting output codes [2] for improving the generalization capability of learning algorithms.
Reference: [53] <author> J. Yang, R. Parekh, and V. Honavar. </author> <title> MTiling a constructive neural network learning algorithm for multi-category pattern classification. </title> <booktitle> In Proceedings of the World Congress on Neural Networks'96, </booktitle> <pages> pages 182-187, </pages> <address> San Diego, </address> <year> 1996. </year>
Reference-contexts: Groups of one or more ancillary neurons are trained at a time in an attempt to make the current layer faithful. The algorithm is described in Fig. 6 and the resulting network is shown in Fig. 7. 6 An earlier version of this algorithm appeared in <ref> [53] </ref>. 13 Algorithm MTiling-real Input: A training set S Output: A trained tiling network begin 1) Train a single layer network with M output neurons using the algorithm A (Note that these M neurons are designated as the master neurons) 2) Let L = 1 denote the number of layers in
Reference: [54] <author> J. Yang, R. Parekh, and V. Honavar. </author> <title> DistAl: An inter-pattern distance-based constructive learning algorithm. Intelligent Data Analysis, </title> <note> 1999. (To appear). </note>
Reference-contexts: These algorithms exploit the geometric properties of the training patterns to directly (i.e., in one-shot) determine appropriate weights for the neurons added to the network. The Grow and Learn (GAL) algorithm [1] and the DistAl algorithm <ref> [54] </ref> construct a single hidden layer network that implements a kind of nearest neighbor classification scheme. Each hidden neuron is an exemplar representing a group of patterns that belong to the same class and are close to each other in terms of some suitably chosen distance metric.
Reference: [55] <author> D. Yeung. </author> <title> Constructive neural networks as estimators of bayesian discriminant functions. </title> <journal> Pattern Recognition, </journal> <volume> 26(1) </volume> <pages> 189-204, </pages> <year> 1993. </year> <month> 29 </month>
Reference-contexts: Clearly, the class of constructive algorithms discussed above (which implement the more general real to real mapping) can be adapted for application to pattern classification For example, the use of a cascade correlation type network construction strategy for learning Bayesian discriminant functions is described in <ref> [55] </ref>. However, a special class of constructive learning algorithms can be designed to closely match the unique demands of pattern classification.
References-found: 55

