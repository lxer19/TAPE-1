URL: ftp://ftp.cs.dartmouth.edu/TR/TR95-265.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/reports/abstracts/TR95-265/
Root-URL: http://www.cs.dartmouth.edu
Title: Structured Permuting in Place on Parallel Disk Systems  rank  
Author: Leonard F. Wisniewski lg(M=B) 
Note: multiply/complement) permutations in place that requires at most 2N BD 2  
Date: September 11, 1995  
Address: College PCS-TR95-265  
Affiliation: Department of Computer Science Dartmouth College Dartmouth  
Pubnum: 7  
Abstract: The ability to perform permutations of large data sets in place reduces the amount of necessary available disk storage. The simplest way to perform a permutation often is to read the records of a data set from a source portion of data storage, permute them in memory, and write them to a separate target portion of the same size. It can be quite expensive, however, to provide disk storage that is twice the size of very large data sets. Permuting in place reduces the expense by using only a small amount of extra disk storage beyond the size of the data set. This paper features in-place algorithms for commonly used structured permutations. We have developed an asymptotically optimal algorithm for performing BMMC (bit-matrix parallel disk accesses, as long as M 2BD, where N is the number of records in the data set, M is the number of records that can fit in memory, D is the number of disks, B is the number of records in a block, and fl is the lower left lg(N=B) fi lg B submatrix of the characteristic matrix for the permutation. This algorithm uses N + M records of disk storage and requires only a constant factor more parallel disk accesses and insignificant additional computation than a previously published asymptotically optimal algorithm that uses 2N records of disk storage. We also give algorithms to perform mesh and torus permutations on a d-dimensional mesh. The in-place algorithm for mesh permutations requires at most 3 dN=BDe parallel I/Os and the in-place algorithm for torus permutations uses at most 4dN=BD parallel I/Os. The algorithms for mesh and torus permutations require no extra disk space as long as the memory size M is at least 3BD. The torus algorithm improves upon the previous best algorithm in terms of both time and space.
Abstract-found: 1
Intro-found: 1
Reference: [AP94] <author> Alok Aggarwal and C. Greg Plaxton. </author> <title> Optimal parallel sorting in multi-level storage. </title> <booktitle> In Proceedings of the 5th Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 659-668, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Although this cost model does not account for the variation in disk access times caused by head movement and rotational latency, programmers often have no control over these factors. The number of disk accesses, however, can be minimized by carefully designed algorithms such as those in <ref> [AP94, Arg95, CGG + 95, Cor92, Cor93, CSW94, GTVV93, NV93, VS94] </ref> and this paper. There are two restrictions implied by the Vitter-Shriver model. In order for the memory to accomodate the records transferred in a parallel I/O operation to all D disks, we require that BD M .
Reference: [Arg95] <author> Lars Arge. </author> <title> The buffer tree: A new technique for optimal I/O-algorithms. </title> <booktitle> In 4th International Workshop on Algorithms and Data Structures (Proceedings), Lecture Notes in Computer Science, </booktitle> <volume> number 955, </volume> <pages> pages 334-345. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1995. </year>
Reference-contexts: Although this cost model does not account for the variation in disk access times caused by head movement and rotational latency, programmers often have no control over these factors. The number of disk accesses, however, can be minimized by carefully designed algorithms such as those in <ref> [AP94, Arg95, CGG + 95, Cor92, Cor93, CSW94, GTVV93, NV93, VS94] </ref> and this paper. There are two restrictions implied by the Vitter-Shriver model. In order for the memory to accomodate the records transferred in a parallel I/O operation to all D disks, we require that BD M . <p> The first term comes into play when the block size B is small, and the second term is the sorting bound fi BD lg (M=B) , which was shown by Vitter and Shriver for randomized sorting and by Nodine and Vitter [NV93] and by Arge <ref> [Arg95] </ref> for deterministic sorting. These bounds are asymptotically tight, for they match the lower bounds proven earlier by Aggarwal and Vitter [AV88] using a model with one disk and D independent read/write heads, which is at least as powerful as the Vitter-Shriver model.
Reference: [AV88] <author> Alok Aggarwal and Jeffrey Scott Vitter. </author> <title> The input/output complexity of sorting and related problems. </title> <journal> Communications of the ACM, </journal> <volume> 31(9) </volume> <pages> 1116-1127, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: These bounds are asymptotically tight, for they match the lower bounds proven earlier by Aggarwal and Vitter <ref> [AV88] </ref> using a model with one disk and D independent read/write heads, which is at least as powerful as the Vitter-Shriver model. Specific classes of structured permutations sometimes require fewer parallel I/Os than general permutations.
Reference: [CGG + 95] <author> Yi-Jen Chiang, Michael T. Goodrich, Edward F. Grove, Roberto Tamassia, Dar-ren Erik Vengroff, and Jeffrey Scott Vitter. </author> <title> External-memory graph algorithms. </title> <booktitle> In Proceedings of the Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 139-149, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: Although this cost model does not account for the variation in disk access times caused by head movement and rotational latency, programmers often have no control over these factors. The number of disk accesses, however, can be minimized by carefully designed algorithms such as those in <ref> [AP94, Arg95, CGG + 95, Cor92, Cor93, CSW94, GTVV93, NV93, VS94] </ref> and this paper. There are two restrictions implied by the Vitter-Shriver model. In order for the memory to accomodate the records transferred in a parallel I/O operation to all D disks, we require that BD M .
Reference: [Cor92] <author> Thomas H. Cormen. </author> <title> Virtual Memory for Data-Parallel Computing. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <year> 1992. </year> <note> Available as Technical Report MIT/LCS/TR-559. </note>
Reference-contexts: Although this cost model does not account for the variation in disk access times caused by head movement and rotational latency, programmers often have no control over these factors. The number of disk accesses, however, can be minimized by carefully designed algorithms such as those in <ref> [AP94, Arg95, CGG + 95, Cor92, Cor93, CSW94, GTVV93, NV93, VS94] </ref> and this paper. There are two restrictions implied by the Vitter-Shriver model. In order for the memory to accomodate the records transferred in a parallel I/O operation to all D disks, we require that BD M . <p> The indexing function maps each grid position p = (p 0 ; p 1 ; : : : ; p d1 ) to a unique index in row-major order: (p; m) = i=0 4 @ j=i+1 1 3 The following lemma from <ref> [Cor92] </ref> states that the difference between the source and target index of each record in row-major order is the same for every grid location mapped by a mesh permutation. <p> Torus permutations Our algorithm for torus permutations improves on the algorithm of <ref> [Cor92] </ref>, which performs a torus permutation as a 2 d -monotonic route, i.e., a superposition of 2 d disjoint monotonic routes. This algorithm uses at most (2 d+1 + 1) dN=BDe parallel I/Os and requires 2N records of disk space and 2 d +1 stripes of memory.
Reference: [Cor93] <author> Thomas H. Cormen. </author> <title> Fast permuting in disk arrays. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):41-57, January and February 1993. </note>
Reference-contexts: Although this cost model does not account for the variation in disk access times caused by head movement and rotational latency, programmers often have no control over these factors. The number of disk accesses, however, can be minimized by carefully designed algorithms such as those in <ref> [AP94, Arg95, CGG + 95, Cor92, Cor93, CSW94, GTVV93, NV93, VS94] </ref> and this paper. There are two restrictions implied by the Vitter-Shriver model. In order for the memory to accomodate the records transferred in a parallel I/O operation to all D disks, we require that BD M . <p> Specific classes of structured permutations sometimes require fewer parallel I/Os than general permutations. Vitter and Shriver showed how to transpose an R fi S matrix (N = RS) with only fi BD 1 + lg (M=B) parallel I/Os. Subsequently, Cormen <ref> [Cor93] </ref> and Cormen, Sundquist, and Wisniewski [CSW94] also developed algorithms to perform several classes of bit-defined permutations using fewer parallel I/Os than general permutations. The bit-defined classes include matrix transposition with power-of-2 dimensions as a special case. Cormen [Cor93] also shows how to efficiently perform permutations on data with a mesh <p> Subsequently, Cormen <ref> [Cor93] </ref> and Cormen, Sundquist, and Wisniewski [CSW94] also developed algorithms to perform several classes of bit-defined permutations using fewer parallel I/Os than general permutations. The bit-defined classes include matrix transposition with power-of-2 dimensions as a special case. Cormen [Cor93] also shows how to efficiently perform permutations on data with a mesh layout. Several in-place algorithms have been developed with certain limitations. Fich, Munro and Poblete [FMP95] provide in-place algorithms to perform general permutations in memory. <p> All the one-pass permutations in the factorization of the matrix A have characteristic matrix forms included in the the BMMC subclasses of MRC (memory-rearrangement/complement) and MLD (memoryload-dispersal) permutations. The algorithms to perform MRC and MLD permutations in <ref> [Cor93] </ref> and [CSW94], respectively, read the data from a source portion of N records of disk storage, permute the data in memory, and write the data to a separate target portion of N records of disk storage. <p> m) fi (n m) submatrices of the characteristic matrix are nonsingular, the upper right m fi (n m) submatrix can contain any 0-1 values at all, and the lower left (n m) fi m submatrix is all 0: m n m " nonsingular arbitrary 0 nonsingular # n m Cormen <ref> [Cor93] </ref> shows that any MRC permutation requires only one pass of N=BD parallel reads and N=BD parallel writes. We partition the N records into N=M disjoint memoryloads of M consecutive records each.
Reference: [CSW94] <author> Thomas H. Cormen, Thomas Sundquist, and Leonard F. Wisniewski. </author> <title> Asymptotically tight bounds for performing BMMC permutations on parallel disk systems. </title> <type> Technical Report PCS-TR94-223, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> July </month> <year> 1994. </year> <booktitle> Preliminary version appeared in Proceedings of the 5th Annual ACM Symposium on Parallel Algorithms and Architectures. </booktitle> <pages> 20 </pages>
Reference-contexts: Section 2 provides the I/O complexity model and a description of previous work on out-of-core permuting and in-place permuting. In Section 3, we present the BMMC algorithm of <ref> [CSW94] </ref>, adapt it to be performed in place, and present algorithms to perform in place the one-pass permutation subclasses used by the BMMC algorithm. <p> Although this cost model does not account for the variation in disk access times caused by head movement and rotational latency, programmers often have no control over these factors. The number of disk accesses, however, can be minimized by carefully designed algorithms such as those in <ref> [AP94, Arg95, CGG + 95, Cor92, Cor93, CSW94, GTVV93, NV93, VS94] </ref> and this paper. There are two restrictions implied by the Vitter-Shriver model. In order for the memory to accomodate the records transferred in a parallel I/O operation to all D disks, we require that BD M . <p> Specific classes of structured permutations sometimes require fewer parallel I/Os than general permutations. Vitter and Shriver showed how to transpose an R fi S matrix (N = RS) with only fi BD 1 + lg (M=B) parallel I/Os. Subsequently, Cormen [Cor93] and Cormen, Sundquist, and Wisniewski <ref> [CSW94] </ref> also developed algorithms to perform several classes of bit-defined permutations using fewer parallel I/Os than general permutations. The bit-defined classes include matrix transposition with power-of-2 dimensions as a special case. Cormen [Cor93] also shows how to efficiently perform permutations on data with a mesh layout. <p> in-place algorithms require at most N + M records of disk space, a significant savings over the 2N records of disk space required by the previous algorithms when N &gt;> M . 3 Performing BMMC permutations in place In this section, we give a brief overview of the algorithm in <ref> [CSW94] </ref> to perform BMMC permu tations on parallel disk systems. We shall then adapt that algorithm to be performed in place 4 b = 3, d = 4, m = 8, and s = 6. <p> We shall generally focus on the matrix-multiplication portion of BMMC permutations rather than on the complement vector. The permutation A characterized by a matrix A is the permutation for which A (x) = A x for all source addresses x. The following lemma from <ref> [CSW94] </ref> shows the equivalence of multiplying characteristic matrices and composing permutations when the complement vectors are zero. For permutations Y and Z , the composition Z ffi Y is defined by ( Z ffi Y )(x) = Z ( Y (x)) for all x in the domain of Y . <p> Then the matrix product Z Y characterizes the composition Z ffi Y . After the factorization of a characteristic matrix A into the product of several nonsingular matrices, each factor characterizes a BMMC permutation. The following corollary from <ref> [CSW94] </ref> describes the order in which we perform these permutations to effect the permutation characterized by A. <p> That is, we perform the permutations characterized by the factors of a matrix from right to left. 2 Edelman, Heller, and Johnsson [EHJ94] call BMMC permutations affine transformations or, if there is no complementing, linear transformations. 6 BMMC algorithm The BMMC algorithm presented in <ref> [CSW94] </ref> uses a matrix decomposition/composition method which factors the characteristic matrix A of the BMMC permutation into at most l rank fl lg (M=B) + 2 matrix factors, where fl is the submatrix A b::n1;0::b1 , i.e., the lower left (n b) fi b submatrix of A. <p> This upper bound on the number of parallel I/Os asymptotically matches the lower bound also shown in <ref> [CSW94] </ref> and is, in fact, almost equal to the best known exact lower bound. All the one-pass permutations in the factorization of the matrix A have characteristic matrix forms included in the the BMMC subclasses of MRC (memory-rearrangement/complement) and MLD (memoryload-dispersal) permutations. <p> All the one-pass permutations in the factorization of the matrix A have characteristic matrix forms included in the the BMMC subclasses of MRC (memory-rearrangement/complement) and MLD (memoryload-dispersal) permutations. The algorithms to perform MRC and MLD permutations in [Cor93] and <ref> [CSW94] </ref>, respectively, read the data from a source portion of N records of disk storage, permute the data in memory, and write the data to a separate target portion of N records of disk storage. <p> Fortunately, we can easily perform erasure permutations with no extra disk space. The rest of this section shows how to perform MRC permutations and erasure permutations in place using O (N=BD) parallel I/Os. After the matrix-decomposition phase of the BMMC algorithm of <ref> [CSW94] </ref>, the factorization is A = F E 1 g E 1 g1 E 1 1 R 1 T 1 ; (1) where the factors F , S 1 i , R 1 and T 1 characterize MRC permutations, the factors E 1 i charac terize erasure permutations, and g l <p> Each of the matrices in equation (1) belong to subclasses of BMMC permutations with special matrix forms. The matrices T 1 , R 1 , and S 1 i have the trailer, reducer, and swapper matrix forms, respectively, defined in <ref> [CSW94] </ref>. Figure 4 shows these matrix forms. Each of these matrix forms has an inverse of the same form. Since the factors S 1 1 , R 1 , and T 1 from equation (1) all characterize MRC permutations, by the composition property shown in [CSW94], their product S 1 1 <p> matrix forms, respectively, defined in <ref> [CSW94] </ref>. Figure 4 shows these matrix forms. Each of these matrix forms has an inverse of the same form. Since the factors S 1 1 , R 1 , and T 1 from equation (1) all characterize MRC permutations, by the composition property shown in [CSW94], their product S 1 1 R 1 T 1 also characterizes an MRC permutation.
Reference: [EHJ94] <author> Alan Edelman, Steve Heller, and S. Lennart Johnsson. </author> <title> Index transformation algo-rithms in a linear algebra framework. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(12) </volume> <pages> 1302-1309, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: That is, we perform the permutations characterized by the factors of a matrix from right to left. 2 Edelman, Heller, and Johnsson <ref> [EHJ94] </ref> call BMMC permutations affine transformations or, if there is no complementing, linear transformations. 6 BMMC algorithm The BMMC algorithm presented in [CSW94] uses a matrix decomposition/composition method which factors the characteristic matrix A of the BMMC permutation into at most l rank fl lg (M=B) + 2 matrix factors, where
Reference: [FMP95] <author> Faith E. Fich, J. Ian Munro, and Patricio V. Poblete. </author> <title> Permuting in place. </title> <journal> SIAM Journal on Computing, </journal> <volume> 24(2) </volume> <pages> 266-278, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: The bit-defined classes include matrix transposition with power-of-2 dimensions as a special case. Cormen [Cor93] also shows how to efficiently perform permutations on data with a mesh layout. Several in-place algorithms have been developed with certain limitations. Fich, Munro and Poblete <ref> [FMP95] </ref> provide in-place algorithms to perform general permutations in memory. These algorithms, however, do not apply to permuting data sets that exceed the size of memory. Vitter and Shriver did not design their out-of-core algorithm for general permuting to be performed in place. <p> Figure 3 shows the rotation of a 3-cycle of memoryload numbers. The only unresolved problem is to find the disjoint cycles without rotating any disjoint cycle more than once. Fich et al. <ref> [FMP95] </ref> provide several algorithms to perform an arbitrary permutation in place when the entire data set fits into memory. The following theorem from [FMP95] reflects the tradeoff between time and additional space when permuting an array of length p in memory when an extra q bits of storage are available. <p> The only unresolved problem is to find the disjoint cycles without rotating any disjoint cycle more than once. Fich et al. <ref> [FMP95] </ref> provide several algorithms to perform an arbitrary permutation in place when the entire data set fits into memory. The following theorem from [FMP95] reflects the tradeoff between time and additional space when permuting an array of length p in memory when an extra q bits of storage are available. <p> In this context, we show how to perform a permutation on the p memoryload numbers, where p = N=M . We can adapt the simplest algorithm from <ref> [FMP95] </ref> which uses q = N=M bits of extra space to rotate each memoryload exactly once. With current disk prices approximately 100 times less per megabyte than memory prices, current parallel disk systems do not typically have a capacity of more than 1000 memoryloads of data storage. <p> By Theorem 3, the additional computation time is O (N=M ), which is a constant number of operations per memoryload. Since we can easily generate the inverse permutation from the characteristic matrix, we can also adapt the more complex algorithms in <ref> [FMP95] </ref> to rotate the memoryloads when N &gt;> M using O (log (N=M )) extra bits of storage and O M log (N=M ) time. For simplicity of presentation, we adapt the algorithm that uses N=M extra bits of storage.
Reference: [GTVV93] <author> Michael T. Goodrich, Jyh-Jong Tsay, Darren E. Vengroff, and Jeffrey Scott Vitter. </author> <title> External-memory computational geometry. </title> <booktitle> In Proceedings of the 34th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 714-723, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Although this cost model does not account for the variation in disk access times caused by head movement and rotational latency, programmers often have no control over these factors. The number of disk accesses, however, can be minimized by carefully designed algorithms such as those in <ref> [AP94, Arg95, CGG + 95, Cor92, Cor93, CSW94, GTVV93, NV93, VS94] </ref> and this paper. There are two restrictions implied by the Vitter-Shriver model. In order for the memory to accomodate the records transferred in a parallel I/O operation to all D disks, we require that BD M .
Reference: [NV93] <author> Mark H. Nodine and Jeffrey Scott Vitter. </author> <title> Deterministic distribution sort in shared and distributed memory multiprocessors. </title> <booktitle> In Proceedings of the 5th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 120-129, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Although this cost model does not account for the variation in disk access times caused by head movement and rotational latency, programmers often have no control over these factors. The number of disk accesses, however, can be minimized by carefully designed algorithms such as those in <ref> [AP94, Arg95, CGG + 95, Cor92, Cor93, CSW94, GTVV93, NV93, VS94] </ref> and this paper. There are two restrictions implied by the Vitter-Shriver model. In order for the memory to accomodate the records transferred in a parallel I/O operation to all D disks, we require that BD M . <p> The first term comes into play when the block size B is small, and the second term is the sorting bound fi BD lg (M=B) , which was shown by Vitter and Shriver for randomized sorting and by Nodine and Vitter <ref> [NV93] </ref> and by Arge [Arg95] for deterministic sorting. These bounds are asymptotically tight, for they match the lower bounds proven earlier by Aggarwal and Vitter [AV88] using a model with one disk and D independent read/write heads, which is at least as powerful as the Vitter-Shriver model.
Reference: [VS90] <author> Jeffrey Scott Vitter and Elizabeth A. M. Shriver. </author> <title> Optimal disk I/O with parallel block transfer. </title> <booktitle> In Proceedings of the Twenty Second Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 159-169, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: When appropriate, we show that any extra computation necessary to permute in place is insignificant. 2 Model and previous results We use the parallel-disk model first proposed by Vitter and Shriver <ref> [VS90, VS94] </ref>, who also gave asymptotically optimal algorithms for several problems including sorting and general permutations. In the Vitter-Shriver model, N records are stored on D disks D 0 ; D 1 ; : : : ; D D1 , with N=D records stored on each disk.
Reference: [VS94] <author> Jeffrey Scott Vitter and Elizabeth A. M. Shriver. </author> <title> Algorithms for parallel memory I: Two-level memories. </title> <journal> Algorithmica, </journal> 12(2/3):110-147, August and September 1994. 
Reference-contexts: When appropriate, we show that any extra computation necessary to permute in place is insignificant. 2 Model and previous results We use the parallel-disk model first proposed by Vitter and Shriver <ref> [VS90, VS94] </ref>, who also gave asymptotically optimal algorithms for several problems including sorting and general permutations. In the Vitter-Shriver model, N records are stored on D disks D 0 ; D 1 ; : : : ; D D1 , with N=D records stored on each disk. <p> Although this cost model does not account for the variation in disk access times caused by head movement and rotational latency, programmers often have no control over these factors. The number of disk accesses, however, can be minimized by carefully designed algorithms such as those in <ref> [AP94, Arg95, CGG + 95, Cor92, Cor93, CSW94, GTVV93, NV93, VS94] </ref> and this paper. There are two restrictions implied by the Vitter-Shriver model. In order for the memory to accomodate the records transferred in a parallel I/O operation to all D disks, we require that BD M .
Reference: [VV95] <author> Darren Erik Vengroff and Jeffrey Scott Vitter. </author> <title> I/O-efficient scientific computation using TPIE. </title> <booktitle> In Seventh IEEE Symposium on Parallel and Distributed Processing, </booktitle> <month> October </month> <year> 1995. </year> <note> To appear. 21 </note>
Reference-contexts: Vitter and Shriver did not design their out-of-core algorithm for general permuting to be performed in place. Furthermore, the constants before the upper bound for this algorithm are rather high compared to the constants in the worst cases for the structured permutation algorithms mentioned above <ref> [VV95] </ref>. The next two sections present out-of-core in-place algorithms to perform several commonly used structured permutations. We supply exact constants for the I/O complexity of both the previous best algorithms and the in-place algorithms.
References-found: 14

