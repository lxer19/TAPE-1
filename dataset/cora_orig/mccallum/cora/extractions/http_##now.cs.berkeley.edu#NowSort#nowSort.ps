URL: http://now.cs.berkeley.edu/NowSort/nowSort.ps
Refering-URL: http://now.cs.berkeley.edu/NowSort/abstract.html
Root-URL: 
Email: dusseau@cs.berkeley.edu  remzi@cs.berkeley.edu  culler@cs.berkeley.edu  jmh@cs.berkeley.edu  patterson@cs.berkeley.edu  
Title: High-Performance Sorting on Networks of Workstations  
Author: Andrea C. Arpaci-Dusseau Remzi H. Arpaci-Dusseau David E. Culler Joseph M. Hellerstein David A. Patterson 
Address: Berkeley  Berkeley  Berkeley  Berkeley  Berkeley  
Affiliation: Computer Science Division University of California,  Computer Science Division University of California,  Computer Science Division University of California,  Computer Science Division University of California,  Computer Science Division University of California,  
Abstract: We report the performance of NOW-Sort, a collection of sorting implementations on a Network of Workstations (NOW). We find that parallel sorting on a NOW is competitive to sorting on the large-scale SMPs that have traditionally held the performance records. On a 64-node cluster, we sort 6.0 GB in just under one minute, while a 32-node cluster finishes the Datamation benchmark in 2.41 seconds. Our implementations can be applied to a variety of disk, memory, and processor configurations; we highlight salient issues for tuning each component of the system. We evaluate the use of commodity operating systems and hardware for parallel sorting. We find existing OS primitives for memory management and file access adequate. Due to aggregate communication and disk bandwidth requirements, the bottleneck of our system is the workstation I/O bus. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. C. Agarwal. </author> <title> A Super Scalar Sort Algorithm for RISC Processors. </title> <booktitle> In Proceedings of the 1996 ACM SIGMOD Conference, </booktitle> <pages> pages 240-246, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: processor SGI Challenge with 96 disks and 2.25 GB of main memory [30] at 3.52 seconds; a single processor IBM RS/6000 with 8 disks and 256 MB of memory has an impressive time of 5.1 seconds and better price/performance, but uses raw disk, which is not allowed in the benchmark <ref> [1] </ref>. Recognizing that the Datamation benchmark is outdated and is more a test of startup and shutdown time than I/O performance, the authors of AlphaSort introduced MinuteSort in 1994 [26]. <p> The SGI system was also the previous record-holder on the MinuteSort benchmark, sorting 1.6 GB. AlphaSort achieved 1.1 GB on only three processors, 36 disks, and 1.25 GB of memory, for better price/performance [26]. Over the years, numerous authors have reported the performance of their sorting algorithms and implementations <ref> [1, 6, 7, 15, 16, 21, 25, 26, 27, 30, 35] </ref>, and we try to leverage many of the implementation and algorithmic lessons that they describe. <p> If performed correctly, the in-core sort within the disk-to-disk single-node sort comprises only a small portion of the total execution time. For example, the in-core sort consumes only 0.6 of the 5.1 seconds required to sort one million records on an IBM RS/6000 with 8 disks <ref> [1] </ref>, i.e., 12% of the total time. In this section we investigate the programming complexity needed to achieve this range of performance by measuring three different in-core sorting algorithms. Quicksort: The first in-core sort is a simple quicksort over all of the keys in memory [23]. <p> Bucket + Partial-Radix: The third in-core sort performs a partial-radix sort with clean-up on the keys within each bucket, as suggested in <ref> [1] </ref>. Once again, the most-significant 32-bits of the key (after removing the top b-bits) and a pointer to the full record are kept in the bucket. Radix sort relies on the representation of keys as n-bit numbers. <p> Clearly, the combination of distributing keys into buckets and performing a radix sort on the keys within each bucket is worth the added complexity. We should note that we implemented some of the optimizations found in <ref> [1] </ref>, including restricting the number of buckets to be less than the number of TLB entries, overlapping of sorting and writing, and a small optimization on the clean-up phase; however, we did not see a significant gain from these methods. 4.3 Discussion To summarize, Figure 4 shows the total time as
Reference: [2] <author> T. E. Anderson, D. E. Culler, and D. A. Patterson. </author> <title> A Case for NOW (Networks of Workstations). </title> <booktitle> IEEE Micro, </booktitle> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: In this paper we note some lessons for the NOW research community, which is made up largely of researchers in operating systems and computer architecture <ref> [2, 3, 5, 9, 22] </ref>. In principle NOWs are similar to "shared-nothing" architectures for databases [4, 11, 14, 18, 20, 29, 31, 32] but they have typically been analyzed and tuned in the context of compute-intensive applications.
Reference: [3] <author> R. H. Arpaci, A. C. Dusseau, A. M. Vahdat, L. T. Liu, T. E. Anderson, and D. A. Patterson. </author> <title> The Interaction of Parallel and Sequential Workloads on a Network of Workstations. </title> <booktitle> In Proceedings of SIGMETRICS/Performance '95, </booktitle> <pages> pages 267-78, </pages> <year> 1995. </year>
Reference-contexts: In this paper we note some lessons for the NOW research community, which is made up largely of researchers in operating systems and computer architecture <ref> [2, 3, 5, 9, 22] </ref>. In principle NOWs are similar to "shared-nothing" architectures for databases [4, 11, 14, 18, 20, 29, 31, 32] but they have typically been analyzed and tuned in the context of compute-intensive applications.
Reference: [4] <author> C. Baru, G. Fecteau, A. Goyal, H. Hsiao, A. Jhnigran, S. Padman-abhan, and W. Wilson. </author> <title> An Overview of DB2 Parallel Edition. </title> <booktitle> In Proceedings of 1995 SIGMOD International Conference on Management of Data, </booktitle> <address> San Jose, CA, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: In this paper we note some lessons for the NOW research community, which is made up largely of researchers in operating systems and computer architecture [2, 3, 5, 9, 22]. In principle NOWs are similar to "shared-nothing" architectures for databases <ref> [4, 11, 14, 18, 20, 29, 31, 32] </ref> but they have typically been analyzed and tuned in the context of compute-intensive applications. We demonstrate that NOWs can be a state-of-the-art platform for data-intensive applications as well.
Reference: [5] <author> A. Basu, V. Buch, W. Vogels, and T. von Eicken. U-Net: </author> <title> A User-Level Network Interface for Parallel and Distributed Computing. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <address> Copper Mountain, Colorado, </address> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: In this paper we note some lessons for the NOW research community, which is made up largely of researchers in operating systems and computer architecture <ref> [2, 3, 5, 9, 22] </ref>. In principle NOWs are similar to "shared-nothing" architectures for databases [4, 11, 14, 18, 20, 29, 31, 32] but they have typically been analyzed and tuned in the context of compute-intensive applications.
Reference: [6] <author> B. Baugsto, J. Greipsland, and J. Kamerbeek. </author> <title> Sorting Large Data Files on POMA. </title> <booktitle> In Proceedings of COMPAR-90 VAPPIV, </booktitle> <pages> pages 536-547, </pages> <address> Sept. 1990. </address> <publisher> Springer Verlag Lecture Notes No. </publisher> <pages> 357. </pages>
Reference-contexts: The SGI system was also the previous record-holder on the MinuteSort benchmark, sorting 1.6 GB. AlphaSort achieved 1.1 GB on only three processors, 36 disks, and 1.25 GB of memory, for better price/performance [26]. Over the years, numerous authors have reported the performance of their sorting algorithms and implementations <ref> [1, 6, 7, 15, 16, 21, 25, 26, 27, 30, 35] </ref>, and we try to leverage many of the implementation and algorithmic lessons that they describe.
Reference: [7] <author> M. Beck, D. Bitton, and W. K. Wilkinson. </author> <title> Sorting Large Files on a Backend Multiprocessor. </title> <institution> Technical Report 86-741,Department of Computer Science, Cornell University, </institution> <month> Mar. </month> <year> 1986. </year>
Reference-contexts: The SGI system was also the previous record-holder on the MinuteSort benchmark, sorting 1.6 GB. AlphaSort achieved 1.1 GB on only three processors, 36 disks, and 1.25 GB of memory, for better price/performance [26]. Over the years, numerous authors have reported the performance of their sorting algorithms and implementations <ref> [1, 6, 7, 15, 16, 21, 25, 26, 27, 30, 35] </ref>, and we try to leverage many of the implementation and algorithmic lessons that they describe.
Reference: [8] <author> G. Blelloch, C. Leiserson, and B. Maggs. </author> <title> A Comparison of Sorting Algorithms for the Connection Machine CM-2. </title> <booktitle> In Symposium on Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1991. </year>
Reference-contexts: This assumption has implications for our method of distributing keys into local buckets and across processing nodes. With a non-uniform distribution, we would need to modify our implementations to perform a sample sort <ref> [8, 15] </ref>. By adding an early phase where we sample the data to determine the range of keys targeted for each processor, we could ensure that each processor receives a similar amount of records; we plan on investigating this further in the future.
Reference: [9] <author> M. A. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. W. Felten, and J. Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: In this paper we note some lessons for the NOW research community, which is made up largely of researchers in operating systems and computer architecture <ref> [2, 3, 5, 9, 22] </ref>. In principle NOWs are similar to "shared-nothing" architectures for databases [4, 11, 14, 18, 20, 29, 31, 32] but they have typically been analyzed and tuned in the context of compute-intensive applications.
Reference: [10] <author> N. Boden, D. Cohen, R. E. Felderman, A. Kulawik, and C. Seitz. Myrinet: </author> <title> A Gigabit-per-second Local Area Network. </title> <booktitle> IEEE Micro, </booktitle> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: In addition to the usual connection to the outside world via 10 Mb/s Ethernet, every workstation contains a single Myrinet network card. Myrinet is a switch-based, high-speed, local-area network, with links capable of bi-directional transfer rates of 160 MB/s <ref> [10] </ref>. Each Myrinet switch has eight ports, and the 64-node cluster is constructed by connecting 26 of these switches in a 3-ary tree. 3.2 Software Each machine in our cluster runs Solaris 2.5.1, a modern, multi-threaded version of UNIX [24].
Reference: [11] <author> H. Boral, W. Alexander, L. Clay, G. Copeland, et al. </author> <title> Prototyping Bubba, a Highly Parallel Database System. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 2(1) </volume> <pages> 4-24, </pages> <month> Mar. </month> <year> 1990. </year>
Reference-contexts: In this paper we note some lessons for the NOW research community, which is made up largely of researchers in operating systems and computer architecture [2, 3, 5, 9, 22]. In principle NOWs are similar to "shared-nothing" architectures for databases <ref> [4, 11, 14, 18, 20, 29, 31, 32] </ref> but they have typically been analyzed and tuned in the context of compute-intensive applications. We demonstrate that NOWs can be a state-of-the-art platform for data-intensive applications as well.
Reference: [12] <author> D. Culler, A. Dusseau, S. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Supercomputing '93, </booktitle> <year> 1993. </year>
Reference-contexts: GLUnix monitors nodes in the system for load-balancing, can co-schedule parallel programs, and provides full job control and I/O redirection. In our experiments, we primarily use GLUnix as a parallel program launcher. The parallel versions of NOW-Sort are written in Split-C <ref> [12] </ref>. Split-C is a parallel extension to C that supports efficient access to a global address space on distributed memory machines. Split-C is built on top of Active Messages [33], a communication layer designed to take advantage of the low latency and high bandwidth of switch-based networks.
Reference: [13] <author> D. Culler, L. T. Liu, R. Martin, and C. Yoshikawa. </author> <title> LogP Performance Assessment of Fast Network Interfaces. </title> <booktitle> IEEE Micro, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: When the message is received, the handler executes atomically with respect to other message arrivals. Active Messages over the Myrinet has the following performance characteristics: the round-trip latency is roughly 20 s, and the layer can sustain a uni-directional bandwidth (one node sending, another receiving) of 35 MB/s <ref> [13] </ref>. 3.3 Input Key Characterization In this study, we make a number of simplifying assumptions about the distribution of key values and the layout of records across processors and disks, allowing us to focus on the OS and architectural issues involved in sorting. <p> This work was enabled by two key pieces of software on the NOW cluster. The first of these is Active Messages, a high-speed communication layer providing low latency and high throughput to parallel programs <ref> [13, 33] </ref>. The second key software component is GLUnix, a distributed operating system for NOWs [19]. By studying disk-to-disk sorting, we qualitatively and quantitatively assessed the workstation operating system and machine architecture.
Reference: [14] <author> D. DeWitt, S. Ghandeharizadeh, D. Schneider, A. Bricker, et al. </author> <title> The Gamma Database Machine Project. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 2(1) </volume> <pages> 44-62, </pages> <month> Mar. </month> <year> 1990. </year>
Reference-contexts: In this paper we note some lessons for the NOW research community, which is made up largely of researchers in operating systems and computer architecture [2, 3, 5, 9, 22]. In principle NOWs are similar to "shared-nothing" architectures for databases <ref> [4, 11, 14, 18, 20, 29, 31, 32] </ref> but they have typically been analyzed and tuned in the context of compute-intensive applications. We demonstrate that NOWs can be a state-of-the-art platform for data-intensive applications as well.
Reference: [15] <author> D. Dewitt, J. Naughton, and D. Schneider. </author> <title> Parallel Sorting on a Shared-Nothing Architecture using Probabilistic Splitting. </title> <booktitle> In Proceedings of the International Conference on Parallel and Distributed Information Systmes, </booktitle> <year> 1991. </year>
Reference-contexts: The SGI system was also the previous record-holder on the MinuteSort benchmark, sorting 1.6 GB. AlphaSort achieved 1.1 GB on only three processors, 36 disks, and 1.25 GB of memory, for better price/performance [26]. Over the years, numerous authors have reported the performance of their sorting algorithms and implementations <ref> [1, 6, 7, 15, 16, 21, 25, 26, 27, 30, 35] </ref>, and we try to leverage many of the implementation and algorithmic lessons that they describe. <p> This assumption has implications for our method of distributing keys into local buckets and across processing nodes. With a non-uniform distribution, we would need to modify our implementations to perform a sample sort <ref> [8, 15] </ref>. By adding an early phase where we sample the data to determine the range of keys targeted for each processor, we could ensure that each processor receives a similar amount of records; we plan on investigating this further in the future.
Reference: [16] <author> A. C. Dusseau, D. E. Culler, K. E. Schauser, and R. P. Martin. </author> <title> Fast Parallel Sorting Under LogP: Experience with the CM-5. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 7(8) </volume> <pages> 791-805, </pages> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: The SGI system was also the previous record-holder on the MinuteSort benchmark, sorting 1.6 GB. AlphaSort achieved 1.1 GB on only three processors, 36 disks, and 1.25 GB of memory, for better price/performance [26]. Over the years, numerous authors have reported the performance of their sorting algorithms and implementations <ref> [1, 6, 7, 15, 16, 21, 25, 26, 27, 30, 35] </ref>, and we try to leverage many of the implementation and algorithmic lessons that they describe.
Reference: [17] <author> A. et. al. </author> <title> A Measure of Transaction Processing Power. </title> <journal> Data-mation, </journal> <volume> 31(7) </volume> <pages> 112-118, </pages> <year> 1985. </year> <note> Also in Readings in Database Systems, </note> <editor> M.H. Stonebraker ed., </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1989. </year>
Reference-contexts: We present our conclusions in Section 8. 2 Related Work The Datamation sorting benchmark was introduced in 1985 by a group of database experts as a test of a processor's I/O subsystem and operating system <ref> [17] </ref>. The performance metric of this benchmark is the elapsed time to sort one million records from disk to disk. The records begin on disk and are each 100-bytes, where the first 10-bytes are the key.
Reference: [18] <author> B. Gerber. </author> <title> Informix Online XPS. </title> <booktitle> In Proceedings of 1995 SIG-MOD International Conference on Management of Data, </booktitle> <address> San Jose, CA, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: In this paper we note some lessons for the NOW research community, which is made up largely of researchers in operating systems and computer architecture [2, 3, 5, 9, 22]. In principle NOWs are similar to "shared-nothing" architectures for databases <ref> [4, 11, 14, 18, 20, 29, 31, 32] </ref> but they have typically been analyzed and tuned in the context of compute-intensive applications. We demonstrate that NOWs can be a state-of-the-art platform for data-intensive applications as well.
Reference: [19] <author> D. P. Ghormley, D. Petrou, A. M. Vahdat, and T. E. Anderson. GLUnix: </author> <title> A Global Layer Unix for NOW. </title> <address> http://now.cs.berkeley.edu/Glunix/glunix.html. </address>
Reference-contexts: The disparate resources in the cluster are unified under GLUnix, the prototype distributed operating system for NOW <ref> [19] </ref>. GLUnix monitors nodes in the system for load-balancing, can co-schedule parallel programs, and provides full job control and I/O redirection. In our experiments, we primarily use GLUnix as a parallel program launcher. The parallel versions of NOW-Sort are written in Split-C [12]. <p> This work was enabled by two key pieces of software on the NOW cluster. The first of these is Active Messages, a high-speed communication layer providing low latency and high throughput to parallel programs [13, 33]. The second key software component is GLUnix, a distributed operating system for NOWs <ref> [19] </ref>. By studying disk-to-disk sorting, we qualitatively and quantitatively assessed the workstation operating system and machine architecture.
Reference: [20] <author> G. Graefe. Volcano: </author> <title> An Extensible and Parallel Dataflow Query Processing System. </title> <type> Technical report, </type> <institution> Oregon Graudate Center, </institution> <month> June </month> <year> 1989. </year>
Reference-contexts: In this paper we note some lessons for the NOW research community, which is made up largely of researchers in operating systems and computer architecture [2, 3, 5, 9, 22]. In principle NOWs are similar to "shared-nothing" architectures for databases <ref> [4, 11, 14, 18, 20, 29, 31, 32] </ref> but they have typically been analyzed and tuned in the context of compute-intensive applications. We demonstrate that NOWs can be a state-of-the-art platform for data-intensive applications as well.
Reference: [21] <author> G. Graefe. </author> <title> Parallel External Sorting in Volcano. </title> <type> Technical Report CU-CS-459, </type> <institution> Computer Science, University of Colorado at Boulder, </institution> <month> June </month> <year> 1990. </year>
Reference-contexts: The SGI system was also the previous record-holder on the MinuteSort benchmark, sorting 1.6 GB. AlphaSort achieved 1.1 GB on only three processors, 36 disks, and 1.25 GB of memory, for better price/performance [26]. Over the years, numerous authors have reported the performance of their sorting algorithms and implementations <ref> [1, 6, 7, 15, 16, 21, 25, 26, 27, 30, 35] </ref>, and we try to leverage many of the implementation and algorithmic lessons that they describe.
Reference: [22] <author> M. D. Hill, J. R. Larus, S. Reinhardt, and D. A. Wood. </author> <title> Cooperative-Shared Memory: Software and Hardware for Scalable Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4) </volume> <pages> 300-318, </pages> <year> 1993. </year>
Reference-contexts: In this paper we note some lessons for the NOW research community, which is made up largely of researchers in operating systems and computer architecture <ref> [2, 3, 5, 9, 22] </ref>. In principle NOWs are similar to "shared-nothing" architectures for databases [4, 11, 14, 18, 20, 29, 31, 32] but they have typically been analyzed and tuned in the context of compute-intensive applications.
Reference: [23] <author> C. A. R. Hoare. </author> <title> Quicksort. </title> <journal> Computer Journal, </journal> <volume> 5(1) </volume> <pages> 10-15, </pages> <year> 1962. </year>
Reference-contexts: In this section we investigate the programming complexity needed to achieve this range of performance by measuring three different in-core sorting algorithms. Quicksort: The first in-core sort is a simple quicksort over all of the keys in memory <ref> [23] </ref>. Previous work has indicated that swapping only the key and a pointer to the full record is faster than swapping the entire 100-byte record, even though extra memory and work is required to set up the pointers [26].
Reference: [24] <author> S. Kleiman, J. Voll, J. Eykholt, A. Shivalingiah, D. Williams, M. Smith, S. Barton, and G. Skinner. </author> <title> Symmetric Multiprocessing in Solaris 2.0. </title> <booktitle> In Proceedings of COMPCON Spring '92, </booktitle> <year> 1992. </year>
Reference-contexts: Each Myrinet switch has eight ports, and the 64-node cluster is constructed by connecting 26 of these switches in a 3-ary tree. 3.2 Software Each machine in our cluster runs Solaris 2.5.1, a modern, multi-threaded version of UNIX <ref> [24] </ref>. The disparate resources in the cluster are unified under GLUnix, the prototype distributed operating system for NOW [19]. GLUnix monitors nodes in the system for load-balancing, can co-schedule parallel programs, and provides full job control and I/O redirection.
Reference: [25] <author> X. Li, G. Linoff, S. Smith, C. Stanfill, and K. Thearling. </author> <title> A Practical External Sort for Shared Disk MPPs. </title> <booktitle> In Proceedings of SUPERCOMPUTING '93, </booktitle> <pages> pages 666-675, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: The SGI system was also the previous record-holder on the MinuteSort benchmark, sorting 1.6 GB. AlphaSort achieved 1.1 GB on only three processors, 36 disks, and 1.25 GB of memory, for better price/performance [26]. Over the years, numerous authors have reported the performance of their sorting algorithms and implementations <ref> [1, 6, 7, 15, 16, 21, 25, 26, 27, 30, 35] </ref>, and we try to leverage many of the implementation and algorithmic lessons that they describe.
Reference: [26] <author> C. Nyberg, T. Barclay, Z. Cvetanovic, J. Gray, and D. Lomet. AlphaSort: </author> <title> A RISC Machine Sort. </title> <booktitle> In Proceedings of 1994 ACM SIGMOD Conference, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: However, the best sorting results to date have been produced by industrial researchers working on expensive, well-endowed versions of shared-memory parallel computers (SMPs) produced by their parent companies <ref> [26, 30] </ref>. In this paper we describe how we achieved new records in sorting performance using a relatively modest "shared-nothing" network of general-purpose UNIX workstations. <p> Recognizing that the Datamation benchmark is outdated and is more a test of startup and shutdown time than I/O performance, the authors of AlphaSort introduced MinuteSort in 1994 <ref> [26] </ref>. The key and record specifications are identical to that of Datamation; the performance metric is now the amount of data that can be sorted in one minute of elapsed time. Price-performance is calculated from the list price of the hardware and operating system depreciated over three years. <p> The SGI system was also the previous record-holder on the MinuteSort benchmark, sorting 1.6 GB. AlphaSort achieved 1.1 GB on only three processors, 36 disks, and 1.25 GB of memory, for better price/performance <ref> [26] </ref>. Over the years, numerous authors have reported the performance of their sorting algorithms and implementations [1, 6, 7, 15, 16, 21, 25, 26, 27, 30, 35], and we try to leverage many of the implementation and algorithmic lessons that they describe. <p> The SGI system was also the previous record-holder on the MinuteSort benchmark, sorting 1.6 GB. AlphaSort achieved 1.1 GB on only three processors, 36 disks, and 1.25 GB of memory, for better price/performance [26]. Over the years, numerous authors have reported the performance of their sorting algorithms and implementations <ref> [1, 6, 7, 15, 16, 21, 25, 26, 27, 30, 35] </ref>, and we try to leverage many of the implementation and algorithmic lessons that they describe. <p> Thus, while this cluster only contains one eighth of the processors in the other configuration, it contains one quarter of the number of disks and amount of memory. The main lesson taught by the authors of AlphaSort <ref> [26] </ref> is that even large sorting problems should be performed in a single pass, since only half the amount of disk I/O is performed and the price of memory is relatively low. <p> To fully utilize the aggregate bandwidth of multiple disks per machine, we implemented a user-level library for file striping on top of each local Solaris file system (i.e., we are not building on top of raw disk). Similar to the approach described in <ref> [26] </ref>, each striped file is characterized by a stripe definition file, which specifies the size of the base stripe in bytes, the names of the files (on different disks) in the stripe, and a multiplicative factor associated with each file/disk. <p> Previous work has indicated that swapping only the key and a pointer to the full record is faster than swapping the entire 100-byte record, even though extra memory and work is required to set up the pointers <ref> [26] </ref>. Comparisons between keys begin with the most-significant word, and only examine the remaining words if the previous ones are identical. The top line in the left-side graph of Figure 3 shows the time for the incore quicksort as a function of the number of keys. <p> However, this is probably more a function of the lack of maturity of our own cluster OS, than the fundamental costs of a distributed operating system, and is currently the focus of optimization. Interestingly, process start-up can also be time-consuming on SMPs: both <ref> [26] </ref> and [30] indicate that bzero'ing the address-space is a significant, if not dominant, cost for one-pass external sorts, most likely because the SMP OS does not parallelize the process. On NOWs, this aspect of process creation is not a problem, since each local address space is initialized in parallel.
Reference: [27] <author> B. Salzberg, A. Tsukerman, J. Gray, M. Stewart, S. Uren, and B. Vaughna. </author> <title> FastSort; A Distributed Single-Input Single-Output External Sort. </title> <booktitle> SIGMOD Record, </booktitle> <volume> 19(2) </volume> <pages> 94-101, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The SGI system was also the previous record-holder on the MinuteSort benchmark, sorting 1.6 GB. AlphaSort achieved 1.1 GB on only three processors, 36 disks, and 1.25 GB of memory, for better price/performance [26]. Over the years, numerous authors have reported the performance of their sorting algorithms and implementations <ref> [1, 6, 7, 15, 16, 21, 25, 26, 27, 30, 35] </ref>, and we try to leverage many of the implementation and algorithmic lessons that they describe.
Reference: [28] <author> M. Stonebraker. </author> <title> Operating System Support for Database Management. </title> <journal> Communications of the ACM, </journal> <volume> 24(7) </volume> <pages> 412-418, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: We find that threads and the current interface for memory management in modern UNIX operating systems do indeed help in developing efficient implementations; this is a cheering update on some of the well-known criticisms of earlier versions of UNIX for database applications <ref> [28] </ref>. On the other hand, we demonstrate that some important facilities are missing in modern workstations, such as handling data striped across heterogeneous disks and determining available memory.
Reference: [29] <author> M. Stonebraker. </author> <title> The Case for Shared Nothing. </title> <journal> Database Engineering, </journal> <volume> 9(1), </volume> <year> 1986. </year>
Reference-contexts: In this paper we note some lessons for the NOW research community, which is made up largely of researchers in operating systems and computer architecture [2, 3, 5, 9, 22]. In principle NOWs are similar to "shared-nothing" architectures for databases <ref> [4, 11, 14, 18, 20, 29, 31, 32] </ref> but they have typically been analyzed and tuned in the context of compute-intensive applications. We demonstrate that NOWs can be a state-of-the-art platform for data-intensive applications as well.
Reference: [30] <author> A. Sweeney, D. Doucette, W. Hu, C. Anderson, M. Nishimoto, and G. Peck. </author> <title> Scalability in the XFS File System. </title> <booktitle> In Proceedings of the USENIX 1996 Annual Technical Conference, </booktitle> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: However, the best sorting results to date have been produced by industrial researchers working on expensive, well-endowed versions of shared-memory parallel computers (SMPs) produced by their parent companies <ref> [26, 30] </ref>. In this paper we describe how we achieved new records in sorting performance using a relatively modest "shared-nothing" network of general-purpose UNIX workstations. <p> Price-performance of the hardware and software is computed by pro-rating the five-year cost over the time of the sort. The previous record-holder on this benchmark was a 12 processor SGI Challenge with 96 disks and 2.25 GB of main memory <ref> [30] </ref> at 3.52 seconds; a single processor IBM RS/6000 with 8 disks and 256 MB of memory has an impressive time of 5.1 seconds and better price/performance, but uses raw disk, which is not allowed in the benchmark [1]. <p> The SGI system was also the previous record-holder on the MinuteSort benchmark, sorting 1.6 GB. AlphaSort achieved 1.1 GB on only three processors, 36 disks, and 1.25 GB of memory, for better price/performance [26]. Over the years, numerous authors have reported the performance of their sorting algorithms and implementations <ref> [1, 6, 7, 15, 16, 21, 25, 26, 27, 30, 35] </ref>, and we try to leverage many of the implementation and algorithmic lessons that they describe. <p> However, this is probably more a function of the lack of maturity of our own cluster OS, than the fundamental costs of a distributed operating system, and is currently the focus of optimization. Interestingly, process start-up can also be time-consuming on SMPs: both [26] and <ref> [30] </ref> indicate that bzero'ing the address-space is a significant, if not dominant, cost for one-pass external sorts, most likely because the SMP OS does not parallelize the process. On NOWs, this aspect of process creation is not a problem, since each local address space is initialized in parallel.
Reference: [31] <author> Tandem Performance Group. </author> <title> A Benchmark of NonStop SQL on Debit-Credit Transactions. </title> <booktitle> In Proceedings of SIGMOD International Conference on Managament of Data, </booktitle> <address> Chicago, IL, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: In this paper we note some lessons for the NOW research community, which is made up largely of researchers in operating systems and computer architecture [2, 3, 5, 9, 22]. In principle NOWs are similar to "shared-nothing" architectures for databases <ref> [4, 11, 14, 18, 20, 29, 31, 32] </ref> but they have typically been analyzed and tuned in the context of compute-intensive applications. We demonstrate that NOWs can be a state-of-the-art platform for data-intensive applications as well.
Reference: [32] <author> Teradata Corporation. </author> <title> DBC/1012 Data Base Computer System Manual, </title> <note> release 2.0 edition, </note> <month> Nov. </month> <year> 1985. </year> <title> Document Number c10-0001-02. </title>
Reference-contexts: In this paper we note some lessons for the NOW research community, which is made up largely of researchers in operating systems and computer architecture [2, 3, 5, 9, 22]. In principle NOWs are similar to "shared-nothing" architectures for databases <ref> [4, 11, 14, 18, 20, 29, 31, 32] </ref> but they have typically been analyzed and tuned in the context of compute-intensive applications. We demonstrate that NOWs can be a state-of-the-art platform for data-intensive applications as well.
Reference: [33] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: The parallel versions of NOW-Sort are written in Split-C [12]. Split-C is a parallel extension to C that supports efficient access to a global address space on distributed memory machines. Split-C is built on top of Active Messages <ref> [33] </ref>, a communication layer designed to take advantage of the low latency and high bandwidth of switch-based networks. An Active Message is essentially a restricted, lightweight version of a remote procedure call. When a process sends an Active Message, it specifies a handler to be executed on the remote node. <p> This work was enabled by two key pieces of software on the NOW cluster. The first of these is Active Messages, a high-speed communication layer providing low latency and high throughput to parallel programs <ref> [13, 33] </ref>. The second key software component is GLUnix, a distributed operating system for NOWs [19]. By studying disk-to-disk sorting, we qualitatively and quantitatively assessed the workstation operating system and machine architecture.
Reference: [34] <author> H. Young and A. Swami. </author> <title> The Parameterized Round-Robin Partitioned Algorithm for Parallel External Sort. </title> <booktitle> In Proceedings 9th International Parallel Processing Symposium, </booktitle> <pages> pages 213-219, </pages> <address> Santa Barbara, CA, </address> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: Some previous work has addressed the issue of adapting sorting algorithms to memory constraints at run-time <ref> [34, 36] </ref>; however, we must first know the amount of free memory available to the sorting application. Because there is no existing Solaris interface that gives an accurate estimate, we developed memconf.
Reference: [35] <author> M. Zagha and G. Blelloch. </author> <title> Radix Sort for Vector Multiprocessors. </title> <booktitle> In Supercomputing, </booktitle> <year> 1991. </year>
Reference-contexts: The SGI system was also the previous record-holder on the MinuteSort benchmark, sorting 1.6 GB. AlphaSort achieved 1.1 GB on only three processors, 36 disks, and 1.25 GB of memory, for better price/performance [26]. Over the years, numerous authors have reported the performance of their sorting algorithms and implementations <ref> [1, 6, 7, 15, 16, 21, 25, 26, 27, 30, 35] </ref>, and we try to leverage many of the implementation and algorithmic lessons that they describe.
Reference: [36] <author> W. Zhang and P. Larson. </author> <title> A Memory-Adaptive Sort (MASORT) for Database Systems. </title> <booktitle> In Proceedings of CASCON '96, </booktitle> <address> Toronto, </address> <month> Nov. </month> <year> 1996. </year>
Reference-contexts: Some previous work has addressed the issue of adapting sorting algorithms to memory constraints at run-time <ref> [34, 36] </ref>; however, we must first know the amount of free memory available to the sorting application. Because there is no existing Solaris interface that gives an accurate estimate, we developed memconf.
References-found: 36

