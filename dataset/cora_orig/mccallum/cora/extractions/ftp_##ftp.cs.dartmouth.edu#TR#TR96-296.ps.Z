URL: ftp://ftp.cs.dartmouth.edu/TR/TR96-296.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/reports/abstracts/TR96-296/
Root-URL: http://www.cs.dartmouth.edu
Title: Tuning STARFISH  
Author: David Kotz 
Address: Hanover, NH 03755-3510  
Affiliation: Department of Computer Science Dartmouth College  
Note: Available at  
Pubnum: Technical Report PCS-TR96-296  
Email: dfk@cs.dartmouth.edu  
Date: October 14, 1996  
Web: URL ftp://ftp.cs.dartmouth.edu/TR/TR96-296.ps.Z  
Abstract: STARFISH is a parallel file-system simulator we built for our research into the concept of disk-directed I/O. In this report, we detail steps taken to tune the file systems supported by STARFISH, which include a traditional parallel file system (with caching) and a disk-directed I/O system. In particular, we now support two-phase I/O, use smarter disk scheduling, increased the maximum number of outstanding requests that a compute processor may make to each disk, and added gather/scatter block transfer. We also present results of the experiments driving the tuning effort.
Abstract-found: 1
Intro-found: 1
Reference: [BDCW91] <author> Eric A. Brewer, Chrysanthos N. Dellarocas, Adrian Colbrook, and William E. Weihl. Proteus: </author> <title> A high-performance parallel-architecture simulator. </title> <type> Technical Report MIT/LCS/TR-516, </type> <institution> MIT, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: 1 Introduction STARFISH is a parallel file-system simulator, based on the Proteus simulator <ref> [BDCW91] </ref>. It was originally developed by the author for research into the concept of disk-directed I/O [Kot94a, Kot94b, PEK96, Kot95b, KC95, Kot95c, Kot95a].
Reference: [KC95] <author> David Kotz and Ting Cai. </author> <title> Exploring the use of I/O nodes for computation in a MIMD multiprocessor. </title> <booktitle> In IPPS '95 Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 78-89, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: 1 Introduction STARFISH is a parallel file-system simulator, based on the Proteus simulator [BDCW91]. It was originally developed by the author for research into the concept of disk-directed I/O <ref> [Kot94a, Kot94b, PEK96, Kot95b, KC95, Kot95c, Kot95a] </ref>. In the course of preparing a more complete paper about disk-directed I/O [Kot96], we made several modifications to both of the parallel file systems supported by STARFISH ("traditional caching," which we now call the "traditional parallel file system," and "disk-directed I/O").
Reference: [Kot94a] <author> David Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <booktitle> In Proceedings of the 1994 Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 61-74, </pages> <month> November </month> <year> 1994. </year> <note> Updated as Dartmouth TR PCS-TR94-226 on November 8, </note> <year> 1994. </year>
Reference-contexts: 1 Introduction STARFISH is a parallel file-system simulator, based on the Proteus simulator [BDCW91]. It was originally developed by the author for research into the concept of disk-directed I/O <ref> [Kot94a, Kot94b, PEK96, Kot95b, KC95, Kot95c, Kot95a] </ref>. In the course of preparing a more complete paper about disk-directed I/O [Kot96], we made several modifications to both of the parallel file systems supported by STARFISH ("traditional caching," which we now call the "traditional parallel file system," and "disk-directed I/O").
Reference: [Kot94b] <author> David Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <type> Technical Report PCS-TR94-226, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> July </month> <year> 1994. </year> <note> Revised Novem-ber 8, </note> <year> 1994. </year>
Reference-contexts: 1 Introduction STARFISH is a parallel file-system simulator, based on the Proteus simulator [BDCW91]. It was originally developed by the author for research into the concept of disk-directed I/O <ref> [Kot94a, Kot94b, PEK96, Kot95b, KC95, Kot95c, Kot95a] </ref>. In the course of preparing a more complete paper about disk-directed I/O [Kot96], we made several modifications to both of the parallel file systems supported by STARFISH ("traditional caching," which we now call the "traditional parallel file system," and "disk-directed I/O").
Reference: [Kot95a] <author> David Kotz. </author> <title> Disk-directed I/O for an out-of-core computation. </title> <booktitle> In Proceedings of the Fourth IEEE International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 159-166, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: 1 Introduction STARFISH is a parallel file-system simulator, based on the Proteus simulator [BDCW91]. It was originally developed by the author for research into the concept of disk-directed I/O <ref> [Kot94a, Kot94b, PEK96, Kot95b, KC95, Kot95c, Kot95a] </ref>. In the course of preparing a more complete paper about disk-directed I/O [Kot96], we made several modifications to both of the parallel file systems supported by STARFISH ("traditional caching," which we now call the "traditional parallel file system," and "disk-directed I/O").
Reference: [Kot95b] <author> David Kotz. </author> <title> Expanding the potential for disk-directed I/O. </title> <booktitle> In Proceedings of the 1995 IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 490-495, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: 1 Introduction STARFISH is a parallel file-system simulator, based on the Proteus simulator [BDCW91]. It was originally developed by the author for research into the concept of disk-directed I/O <ref> [Kot94a, Kot94b, PEK96, Kot95b, KC95, Kot95c, Kot95a] </ref>. In the course of preparing a more complete paper about disk-directed I/O [Kot96], we made several modifications to both of the parallel file systems supported by STARFISH ("traditional caching," which we now call the "traditional parallel file system," and "disk-directed I/O").
Reference: [Kot95c] <author> David Kotz. </author> <title> Interfaces for disk-directed I/O. </title> <type> Technical Report PCS-TR95-270, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: 1 Introduction STARFISH is a parallel file-system simulator, based on the Proteus simulator [BDCW91]. It was originally developed by the author for research into the concept of disk-directed I/O <ref> [Kot94a, Kot94b, PEK96, Kot95b, KC95, Kot95c, Kot95a] </ref>. In the course of preparing a more complete paper about disk-directed I/O [Kot96], we made several modifications to both of the parallel file systems supported by STARFISH ("traditional caching," which we now call the "traditional parallel file system," and "disk-directed I/O").
Reference: [Kot96] <author> David Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <note> Submitted to TOCS, Oc-tober 1996. </note>
Reference-contexts: 1 Introduction STARFISH is a parallel file-system simulator, based on the Proteus simulator [BDCW91]. It was originally developed by the author for research into the concept of disk-directed I/O [Kot94a, Kot94b, PEK96, Kot95b, KC95, Kot95c, Kot95a]. In the course of preparing a more complete paper about disk-directed I/O <ref> [Kot96] </ref>, we made several modifications to both of the parallel file systems supported by STARFISH ("traditional caching," which we now call the "traditional parallel file system," and "disk-directed I/O"). This report details those changes, and the results of the experiments driving the tuning effort. <p> The throughput appears to be lower due to the increased computational overhead of the new disk-scheduling algorithm. In any case, we chose to use the new disk-scheduling algorithm in all of the other experiments in this paper, and in our revised disk-directed I/O experiments <ref> [Kot96] </ref>. 8 Table 2: Throughput of the traditional parallel file system under all access patterns, with both 8- and 8192-byte records, and with both the FCFS and the Cyclic Scan (CS) disk-scheduling algorithm, on the contiguous file layout. <p> Figure 4 seems dramatic, but the y-axis scale shows that the variations here are largely in the noise. Overall, four outstanding requests seemed to be a reasonable compromise. We used four outstanding requests in all of other experiments in this paper, and in our revised disk-directed I/O experiments <ref> [Kot96] </ref>. 13 0 10 20 30 40 MB/s Maximum number of outstanding IOP requests Contiguous disk layout, read patterns Max bandwidth ra 3 3 rn + rb 2 2 2 2 2 2 fi fi fi fi fi fi 4 4 4 4 4 4 ? rcb with 8192-byte records, with <p> So we chose to use queued Memput and Memget in all 8-byte patterns, but not in any 8192-byte patterns, in all of the other disk-directed I/O experiments in this paper, and in our revised disk-directed I/O experiments <ref> [Kot96] </ref>. Note that this choice only adversely affected three 8-byte patterns in two-phase I/O, which were slower by 2-5%. 16 Table 6: Throughput of disk-directed I/O under all access patterns, with both 8- and 8192-byte records, with both the original (Or) and queued Memputs/Memgets (QM), on the contiguous file layout. <p> Table 12 presents the results on the random disk layout, and is no more optimistic. As a result, we chose to retain the original scheme, rather than using Memget writes, in the other experiments in this paper and in our revised disk-directed I/O experiments <ref> [Kot96] </ref>. 22 Table 11: Throughput of the traditional parallel file system under all access patterns, with both 8- and 8192-byte records, and with both the original (Or) and the "Memget writes" (MW), on the contiguous file layout. <p> There are some larger improvements (36%), but one case (wc with 8-byte records) is 7% slower. As a result we chose to use queued requests in all of the other experiments in this paper, and for our revised disk-directed I/O experiments <ref> [Kot96] </ref>. 25 Table 13: Throughput of the traditional parallel file system under all access patterns, with both 8- and 8192-byte records, and with both Thread Requests (TR) and Queued Requests (QR) on the contiguous file layout. <p> We used that algorithm for all other experiments in this paper and in our revised disk-directed I/O experiments <ref> [Kot96] </ref>. Increased number of outstanding requests: Allowing more outstanding requests increased the throughput of the traditional parallel file system on many access patterns with large chunks, because the resulting deep disk queues permitted better disk scheduling. <p> Four outstanding requests appeared to be a good maximum; few patterns improved beyond this point. We used four outstanding requests for other disk-directed I/O experiments in this paper and in our revised disk-directed I/O experiments <ref> [Kot96] </ref>. Queued Memput and Memget: The queued Memput and Memget functions were helpful in both two-phase I/O and disk-directed I/O, but only for 8-byte, not 8192-byte, access patterns. <p> Thus, we chose to use queued Memput and Memget for 8-byte access patterns only, and we did so in all other experiments in this paper and in our revised disk-directed I/O experiments <ref> [Kot96] </ref>. Memget writes: Memget writes rarely increased throughput, the increases were small, and the decreases were often dramatic. We chose not to use them in any other experiments. <p> The alternate implementation, queued requests, which kept a pool of ready and reusable threads, was much faster for those patterns, and no slower for the other patterns. We chose to use queued requests in all other experiments in this paper and in our revised disk-directed I/O experiments <ref> [Kot96] </ref>. 28 5.1 Comparing file systems Based on the above conclusions, we revised our earlier experiments, in which we compare the traditional parallel file system, two-phase I/O, and disk-directed I/O. All of these experiments use the parameters from Table 1, and the features described in the conclusions above. <p> All of these experiments use the parameters from Table 1, and the features described in the conclusions above. The results are graphed and examined in detail in the full paper <ref> [Kot96] </ref>, but the raw data is presented below in Tables 15-21. 29 Table 15: A comparison of the throughput of disk-directed I/O (DDIO) and the traditional parallel file system (TPFS), on a contiguous disk layout. ra throughput has been normalized by the number of CPs.
Reference: [PEK96] <author> Apratim Purakayastha, Carla Schlatter Ellis, and David Kotz. </author> <title> ENWRICH: a compute-processor write caching scheme for parallel file systems. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 55-68, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: 1 Introduction STARFISH is a parallel file-system simulator, based on the Proteus simulator [BDCW91]. It was originally developed by the author for research into the concept of disk-directed I/O <ref> [Kot94a, Kot94b, PEK96, Kot95b, KC95, Kot95c, Kot95a] </ref>. In the course of preparing a more complete paper about disk-directed I/O [Kot96], we made several modifications to both of the parallel file systems supported by STARFISH ("traditional caching," which we now call the "traditional parallel file system," and "disk-directed I/O").
Reference: [SCO90] <author> Margo Seltzer, Peter Chen, and John Ousterhout. </author> <title> Disk scheduling revisited. </title> <booktitle> In Proceedings of the 1990 Winter USENIX Conference, </booktitle> <pages> pages 313-324, </pages> <year> 1990. </year> <month> 37 </month>
Reference-contexts: In STARFISH 3.0 we added a new disk-queuing module. Each disk has a single queue as before, but it is a priority queue rather than a simple FCFS queue. New disk requests were placed into the per-disk priority queue using the Cyclical Scan algorithm <ref> [SCO90] </ref>.
References-found: 10

