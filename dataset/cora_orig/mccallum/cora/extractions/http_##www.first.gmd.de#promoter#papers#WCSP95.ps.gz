URL: http://www.first.gmd.de/promoter/papers/WCSP95.ps.gz
Refering-URL: http://www.first.gmd.de/promoter/papers/index.html
Root-URL: 
Title: feasibility of the language to deal with nested parallelism and dynamic distributed data. even with
Note: elegant way, too. A special interest was the  But  
Abstract: With PROMOTER the focus of intention lies on what are the sources of parallelism within an application, which of these are to exploit, what are the resulting data types (topologies and classes), and how do they interact (methods, communication topologies). Mapping, aggregation, and efficient use of concrete communication structures are left to the compiler. To carry it too far one could say that the key task of the compiler is to exploit the art of sequentialization as the main source of speed. It is to investigate how far the compiler can extract all necessary information for an efficient mapping out of the provided constructs or what kind of additional input is to be provided by the application programmer. [1] D. Ackley, G. Hinton, T. Sejnowski: A Learning Algorithm for Boltzmann Machines, Cognitive Science 9, pp. 147-169, 1985 [2] M. Besch, H. W. Pohl: PROMOTER Application Study - How to Simulate Artificial Neural Networks on Large Scale Parallel Computers Exploiting Data Parallelism and Object-Orientation, RWC Technical Report TR-94022, Japan, 1994 [3] M. Besch, H. W. Pohl: Flexible Data Parallel Training of Neural Networks Using MIMD-Computers, Third Euromicro Workshop on Parallel and Distributed Processing, Sanremo, Italy, January 25-27th, 1995 [4] G. E. Blelloch: Vector Models for Data-Parallel Computing, MIT Press, Cambridge, MA, 1990 [5] B. M. Forrest et al.: Implementing Neural Network Models on Parallel Computers, The computer Journal, vol. 30, no. 5, 1987 [6] W. K. Giloi, A. Schramm: PROMOTER, an application-oriented programming model for massive parallelism, Proc. of the Massively Parallel Programming Model Working Conference, pp. 198-205, IEEE, Sept. 1993 [7] J. Hertz, A. Krogh, R. Palmer: Introduction To The Theory Of Neural Computation, Addison Wesley, 1991 [8] T. Nordstrm, B. Svensson: Using And Designing Massively Parallel Computers for Artificial Neural Networks, Journal Of Parallel And Distributed Computing, vol. 14, pp. 260-285, 1992 [9] D. E. Rumelhart, D. E. Hinton, R. J. Williams: Learning internal representations by error propagation, Rumelhart & McClelland (eds.), Parallel Distributed Processing: Explorations in the Microstructure of Cognition, vol. I, pp. 318-362, Bradford Books/MIT Press, Cambridge, MA, 1986 [10] W. Schiffmann, M. Joost, R. Werner: Comparison of optimized backpropagation algorithms, Proc. of the European Symposium on Artificial Neural Networks, ESANN '93, Brussels, pp. 97-104, 1993 [11] W. Tichy, M. Philippsen, P. Hatcher: A Critique of the Programming Language C*, Communications of the ACM, no. 35, vol. 6, pp. 21-24, June 1992 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Ackley, G. Hinton, T. Sejnowski: </author> <title> A Learning Algorithm for Boltzmann Machines, </title> <booktitle> Cognitive Science 9, </booktitle> <pages> pp. 147-169, </pages> <year> 1985 </year>
Reference: [2] <author> M. Besch, H. W. Pohl: </author> <title> PROMOTER Application Study - How to Simulate Artificial Neural Networks on Large Scale Parallel Computers Exploiting Data Parallelism and Object-Orientation, </title> <type> RWC Technical Report TR-94022, </type> <address> Japan, </address> <year> 1994 </year>
Reference: [3] <author> M. Besch, H. W. Pohl: </author> <title> Flexible Data Parallel Training of Neural Networks Using MIMD-Computers, </title> <booktitle> Third Euromicro Workshop on Parallel and Distributed Processing, </booktitle> <address> Sanremo, Italy, </address> <month> January 25-27th, </month> <year> 1995 </year>
Reference: [4] <author> G. E. Blelloch: </author> <title> Vector Models for Data-Parallel Computing, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990 </year>
Reference: [5] <editor> B. M. Forrest et al.: </editor> <title> Implementing Neural Network Models on Parallel Computers, </title> <journal> The computer Journal, </journal> <volume> vol. 30, no. 5, </volume> <year> 1987 </year>
Reference: [6] <author> W. K. Giloi, A. Schramm: </author> <title> PROMOTER, an application-oriented programming model for massive parallelism, </title> <booktitle> Proc. of the Massively Parallel Programming Model Working Conference, </booktitle> <pages> pp. 198-205, </pages> <publisher> IEEE, </publisher> <month> Sept. </month> <year> 1993 </year>
Reference: [7] <author> J. Hertz, A. Krogh, R. Palmer: </author> <title> Introduction To The Theory Of Neural Computation, </title> <publisher> Addison Wesley, </publisher> <year> 1991 </year>
Reference: [8] <author> T. Nordstrm, B. Svensson: </author> <title> Using And Designing Massively Parallel Computers for Artificial Neural Networks, </title> <journal> Journal Of Parallel And Distributed Computing, </journal> <volume> vol. 14, </volume> <pages> pp. 260-285, </pages> <year> 1992 </year>
Reference: [9] <author> D. E. Rumelhart, D. E. Hinton, R. J. Williams: </author> <title> Learning internal representations by error propagation, </title> <editor> Rumelhart & McClelland (eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> vol. I, </volume> <pages> pp. 318-362, </pages> <publisher> Bradford Books/MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986 </year>
Reference: [10] <author> W. Schiffmann, M. Joost, R. Werner: </author> <title> Comparison of optimized backpropagation algorithms, </title> <booktitle> Proc. of the European Symposium on Artificial Neural Networks, </booktitle> <address> ESANN '93, Brussels, </address> <pages> pp. 97-104, </pages> <year> 1993 </year>

References-found: 10

