URL: http://theory.lcs.mit.edu/~mslevine/mincut/exp-cut.ps.gz
Refering-URL: http://theory.lcs.mit.edu/~mslevine/mincut/index.html
Root-URL: 
Email: chekuri@theory.stanford.edu  avg@research.nj.nec.com  karger@theory.lcs.mit.edu  mslevine@theory.lcs.mit.edu  cliff@cs.dartmouth.edu.  
Title: Experimental Study of Minimum Cut Algorithms  
Author: Chandra S. Chekuri Andrew V. Goldberg David R. Karger Matthew S. Levine Cliff Stein 
Note: Supported by NSF Award CCR-9357849, with matching funds from IBM, Mitsubishi, Schlumberger Foun dation, Shell Foundation, and Xerox Corporation. Research partly supported by ARPA contract N00014-95-1-1246 and NSF CAREER award CCR-9624239. Research partly supported by NSF CAREER award CCR-9624239. Some of this work was done while visiting the second author at NEC. Research partly supported by NSF Award CCR-9308701, a Walter Burke Research Initiation Award and a  Research Initiation Award. Some of this work was done while visiting the second author at NEC, and while visiting Stanford University.  
Date: October 1996  
Address: Stanford, CA 94305  Princeton, NJ 08540  Cambridge, MA 02139  Cambridge, MA 02139  Hanover, NH, 03755  
Affiliation: Computer Science Department Stanford University  NEC Research Institute  Laboratory for Computer Science MIT  Laboratory for Computer Science MIT  Department of Computer Science Dartmouth College  Dartmouth College  
Abstract: Recently, several new algorithms have been developed for the minimum cut problem. These algorithms are very different from the earlier ones and from each other and substantially improve worst-case time bounds for the problem. We conduct experimental evaluation the relative performance of these algorithms. In the process, we develop heuristics and data structures that substantially improve practical performance of the algorithms. We also develop problem families for testing minimum cut algorithms. Our work leads to a better understanding of practical performance of the minimum cut algorithms and produces very efficient codes for the problem. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. K. Ahuja, J. B. Orlin, and R. E. Tarjan. </author> <title> Improved Time Bounds for the Maximum Flow Problem. </title> <journal> SIAM J. Comput., </journal> <volume> 18 </volume> <pages> 939-954, </pages> <year> 1989. </year>
Reference-contexts: The classical Gomory-Hu algorithm [21] solves the minimum cut problem using n 1 minimum s-t cut computations. The fastest current algorithms for the s-t cut problem <ref> [1, 6, 7, 19, 30] </ref> use flow techniques, in particular the push-relabel method [19], and run in !(nm) time. <p> We first compute m from n and d. Next the vertices are randomly colored with k different colors. We then add m edges to graph, at random. If the two endpoints of an edge have the different colors, the edge weight is chosen at random from the range <ref> [1; 100] </ref>, while if the two endpoints have the same color, the edge weight is chosen at random from the range [1; 100 P ]. <p> If the two endpoints of an edge have the different colors, the edge weight is chosen at random from the range [1; 100], while if the two endpoints have the same color, the edge weight is chosen at random from the range <ref> [1; 100 P ] </ref>. The desired effect is that, for large enough P , the graph will have k components and the minimum cut will consist solely of edges with endpoints in different components. Following [35], we generated 6 different problem families. Our families are similar to those of [35]. <p> The generator takes three parameters * n thenumber of vertices, * d the density (as a percentage), * c the type of graph to generate (1 or 2). If c = 1, for each pair of vertices, with probability d, we include an edge with weight uniformly distributed in <ref> [1; 100] </ref>. If c = 2, we split the graph into two components, one containing vertices 1 through n=2 and the other containting vertices n=2 + 1 through n. Again, for each pair of vertices, we include an edge with probability d. <p> Again, for each pair of vertices, we include an edge with probability d. If the two vertices are in the same component, the edge weight is chosen uniformly from <ref> [1; 100n] </ref>, but if the vertices are in different components, the edge weight is chosen uniformly from [1; 100]. We used eight families, PR1-PR8, with parameter values given in Table 3. These values are precisely those used by Padberg and Rinaldi in their paper [37]. <p> Again, for each pair of vertices, we include an edge with probability d. If the two vertices are in the same component, the edge weight is chosen uniformly from [1; 100n], but if the vertices are in different components, the edge weight is chosen uniformly from <ref> [1; 100] </ref>. We used eight families, PR1-PR8, with parameter values given in Table 3. These values are precisely those used by Padberg and Rinaldi in their paper [37]. We use these problem families to compare heuristics in ho with those in the Padber-Rinaldi code.
Reference: [2] <author> R. J. Anderson and J. C. Setubal. </author> <title> Goldberg's Algorithm for the Maximum Flow in Perspective: a Computational Study. </title> <editor> In D. S. Johnson and C. C. McGeoch, editors, </editor> <title> Network Flows and Matching: </title> <booktitle> First DIMACS Implementation Challenge, </booktitle> <pages> pages 1-18. </pages> <publisher> AMS, </publisher> <year> 1993. </year>
Reference-contexts: As a result, our code is always competitive with hybrid, and sometimes outperforms it by a wide margin. Implementations of the push-relabel method for the maximum flow problem have been well-studied, e.g. <ref> [2, 10, 13, 14, 36] </ref>. A maximum flow code of Cherkassky and Goldberg [10] was the starting point of our implementation, ho. The implementation uses the heuristics global update and gap relabeling heuristics that are used in the maximum flow code. <p> If the answer is yes, then v is relabeled. Otherwise, the heuristic deletes all vertices with distance labels d (v) or greater from the graph, as the sink is not reachable from these vertices. This heuristic often speeds up push-relabel algorithms for the maximum flow problem <ref> [2, 10, 13, 36] </ref> and is essential for the analysis of the Hao-Orlin algorithm. 21 We use a standard implementation of gap relabeling that maintains an array of buckets, B [0 : : : 2n 1], with bucket B [i] containing a doubly linked list of all vertices with distance labels
Reference: [3] <author> D. L. Applegate and W. J. Cook. </author> <type> Personal communication. </type> <year> 1996. </year>
Reference-contexts: This problem has many applications, including network reliability theory [26, 39], information retrieval [4], compilers for parallel languages [5], and as a subroutine in cutting-plane algorithms for the Traveling Salesman problem (TSP) <ref> [3] </ref>. The problem of finding a minimum capacity cut between two specified vertices, s and t, is called the minimum s-t cut problem, and is closely related to the minimum cut problem. The classical Gomory-Hu algorithm [21] solves the minimum cut problem using n 1 minimum s-t cut computations. <p> In order to perform meaningful comparisons, we develop problem gener 3 ators and test families for evaluating and comparing performance of the minimum cut codes. We also run and analyze our algorithms on data that arises during the TSP algorithm of Ap-plegate and Cook <ref> [3] </ref>. Our problem families are carefully selected and proved very useful for comparing and tuning minimum cut codes. Our codes use heuristics that, on some problems, significantly reduce the number of operations performed by the underlying algorithms. <p> We describe each family individually below. We also did experiments on minimum cut problems that arise in the solution of large TSP problems via cutting plane algorithms <ref> [3] </ref>. A brief summary of our problem families appears in Table 1. A detailed description of generators and families follows. <p> uniform structure, these problems are hard for all codes in our study. 43 The values used are: Family n BIKEWHE 2 10 ; 2 11 ; 2 12 ; 2 13 8.5 TSP and PRETSP instances The TSP instances are subproblems generated by the TSP solver of Applegate and Cook <ref> [3] </ref>. This is the state-of-the-art code for solving TSP problems exactly, and is based on the technique of cutting planes. The set of feasible traveling salesman tours in a given graph induces a convex polytope in a high-dimensional vector space.
Reference: [4] <author> R. A. Botafogo. </author> <title> Cluster Analysis for Hypertext Systems. </title> <booktitle> In Proc. of the 16-th Annual ACM SIGIR Conference of Res. and Dev. in Info. Retrieval, </booktitle> <pages> pages 116-125, </pages> <year> 1993. </year>
Reference-contexts: This problem has many applications, including network reliability theory [26, 39], information retrieval <ref> [4] </ref>, compilers for parallel languages [5], and as a subroutine in cutting-plane algorithms for the Traveling Salesman problem (TSP) [3].
Reference: [5] <author> S. Chatterjee, J. R. Gilbert, R. Schreiber, and T. J. She*er. </author> <title> Array Distribution in Data-Parallel Programs. </title> <booktitle> In Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 76-91. </pages> <booktitle> Lecture Notes in Computer Science series, </booktitle> <volume> vol. 896, </volume> <publisher> Springer-Verlag, </publisher> <year> 1996. </year>
Reference-contexts: This problem has many applications, including network reliability theory [26, 39], information retrieval [4], compilers for parallel languages <ref> [5] </ref>, and as a subroutine in cutting-plane algorithms for the Traveling Salesman problem (TSP) [3]. The problem of finding a minimum capacity cut between two specified vertices, s and t, is called the minimum s-t cut problem, and is closely related to the minimum cut problem.
Reference: [6] <author> J. Cheriyan and T. Hagerup. </author> <title> A randomized maximum flow algorithm. </title> <booktitle> In Proc. 30th IEEE Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 118-123, </pages> <year> 1989. </year>
Reference-contexts: The classical Gomory-Hu algorithm [21] solves the minimum cut problem using n 1 minimum s-t cut computations. The fastest current algorithms for the s-t cut problem <ref> [1, 6, 7, 19, 30] </ref> use flow techniques, in particular the push-relabel method [19], and run in !(nm) time.
Reference: [7] <author> J. Cheriyan, T. Hagerup, and K. Mehlhorn. </author> <title> Can a Maximum Flow be Computed in o(nm) Time? In Proc. </title> <booktitle> ICALP, </booktitle> <year> 1990. </year>
Reference-contexts: The classical Gomory-Hu algorithm [21] solves the minimum cut problem using n 1 minimum s-t cut computations. The fastest current algorithms for the s-t cut problem <ref> [1, 6, 7, 19, 30] </ref> use flow techniques, in particular the push-relabel method [19], and run in !(nm) time.
Reference: [8] <author> J. Cheriyan and S. N. Maheshwari. </author> <title> Analysis of Preflow Push Algorithms for Maximum Netwrok Flow. </title> <journal> SIAM J. Comput., </journal> <volume> 18 </volume> <pages> 1057-1086, </pages> <year> 1989. </year>
Reference-contexts: We used the highest label (HL) strategy: discharge an active vertex with the highest distance label. This strategy, in combination with appropriate heuristics, seems to give the best results in practice [10]. This strategy also reduces the number of push operations. Theorem 5.4 <ref> [8] </ref> The push-relabel algorithm with the highest label selection performs O (n 2 p m) push operations and runs in O (n 2 p m) time. Sophisticated data structures, such as dynamic trees, improve running times of the push-relabel algorithm in the worst case [20]. <p> The running time of the algorithm is related to the total number of relabelings done. The total number of relabels over all s-t cut computations can be bound by O (n 2 ), the same as the bound for one s-t cut computation done in isolation. Results of <ref> [8, 25] </ref> then imply the following theorem. Theorem 5.5 [8] The Hao-Orlin algorithm with the highest label selection runs in O (n 2 p m) time. 5.3 Implementation Details In this section we describe implementation details of the ho code. We describe general imple mentation issues and heuristics used. <p> The total number of relabels over all s-t cut computations can be bound by O (n 2 ), the same as the bound for one s-t cut computation done in isolation. Results of [8, 25] then imply the following theorem. Theorem 5.5 <ref> [8] </ref> The Hao-Orlin algorithm with the highest label selection runs in O (n 2 p m) time. 5.3 Implementation Details In this section we describe implementation details of the ho code. We describe general imple mentation issues and heuristics used. The internal PR heuristic for ho is described in 5.4.1.
Reference: [9] <author> B. V. Cherkassky. </author> <title> A Fast Algorithm for Computing Maximum Flow in a Network. </title> <editor> In A. V. Karzanov, editor, </editor> <booktitle> Collected Papers, </booktitle> <volume> Vol. 3: </volume> <booktitle> Combinatorial Methods for Flow Problems, </booktitle> <pages> pages 90-96. </pages> <institution> The Institute for Systems Studies, Moscow, </institution> <year> 1979. </year> <title> In Russian. </title> <journal> English translation appears in AMS Trans., </journal> <volume> Vol. 158, </volume> <pages> pp. 23-30, </pages> <year> 1994. </year>
Reference-contexts: Lemma 5.1 [20] The relabel operation increases d (v). We assume that a relabel operation always uses the gap relabeling heuristic <ref> [9, 13] </ref>. Just before relabeling v, the heuristic checks if any other vertex has a label of d (v). If the answer is yes, then v is relabeled.
Reference: [10] <author> B. V. Cherkassky and A. V. Goldberg. </author> <title> On Implementing Push-Relabel Method for the Maximum Flow Problem. </title> <type> Technical Report STAN-CS-94-1523, </type> <institution> Department of Computer Science, Stanford University, </institution> <year> 1994. </year>
Reference-contexts: As a result, our code is always competitive with hybrid, and sometimes outperforms it by a wide margin. Implementations of the push-relabel method for the maximum flow problem have been well-studied, e.g. <ref> [2, 10, 13, 14, 36] </ref>. A maximum flow code of Cherkassky and Goldberg [10] was the starting point of our implementation, ho. The implementation uses the heuristics global update and gap relabeling heuristics that are used in the maximum flow code. <p> As a result, our code is always competitive with hybrid, and sometimes outperforms it by a wide margin. Implementations of the push-relabel method for the maximum flow problem have been well-studied, e.g. [2, 10, 13, 14, 36]. A maximum flow code of Cherkassky and Goldberg <ref> [10] </ref> was the starting point of our implementation, ho. The implementation uses the heuristics global update and gap relabeling heuristics that are used in the maximum flow code. In addition, we use the graph contraction data structures mentioned above, the PR heuristics, and several new heuristics. <p> For a detailed description of the push-relabel method, see [20], and for details of an efficient implementation 20 of the push-relabel algorithm for the maximum flow problem, see <ref> [10] </ref>. Our code ho is in many respects similar to that maximum flow code. We begin with some additional definitions. Our algorithm maintains a preflow, which is a relaxed version of a flow. <p> If the answer is yes, then v is relabeled. Otherwise, the heuristic deletes all vertices with distance labels d (v) or greater from the graph, as the sink is not reachable from these vertices. This heuristic often speeds up push-relabel algorithms for the maximum flow problem <ref> [2, 10, 13, 36] </ref> and is essential for the analysis of the Hao-Orlin algorithm. 21 We use a standard implementation of gap relabeling that maintains an array of buckets, B [0 : : : 2n 1], with bucket B [i] containing a doubly linked list of all vertices with distance labels <p> At the high level, push-relabel algorithms differ by the strategy for selecting the next active vertex to discharge. We used the highest label (HL) strategy: discharge an active vertex with the highest distance label. This strategy, in combination with appropriate heuristics, seems to give the best results in practice <ref> [10] </ref>. This strategy also reduces the number of push operations. Theorem 5.4 [8] The push-relabel algorithm with the highest label selection performs O (n 2 p m) push operations and runs in O (n 2 p m) time. <p> Our times are hundreds of times smaller. However, we think that the pr code would be much faster if its maximum flow subroutine (which is an implementation of the Sleator-Tarjan algorithm [40]) were replaced by a good implementation if the push-relabel method, such as that of <ref> [10] </ref>. Also, our hardware is faster than that used by Padberg and Rinaldi. Tables 44|51 give data for the PR1-PR8 families.
Reference: [11] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year> <month> 134 </month>
Reference-contexts: In this implementation, each node in the current graph corresponds to a set of vertices of the input graph. The sets are represented using the disjoint-set union data structure implemented by disjoint-set forests with path compression; see e.g. <ref> [11] </ref>. Each set has a distinct representative. Each vertex has a pointer towards (but not necessarily directly to) the representative of its set. Representatives point to themselves. We assume that the reader is familiar with the set-union data structure. 9 Suppose u and v are currently different nodes.
Reference: [12] <author> G. B. Dantzig, D. R. Fulkerson, and S. M. Johnson. </author> <title> Solution of a Large-Scale Traveling Salesman Problem. </title> <journal> Oper. Res., </journal> <volume> 2 </volume> <pages> 393-410, </pages> <year> 1954. </year>
Reference-contexts: One set of inequalities that has been very useful is subtour elimination constraints, first introduced by Dantzig, Fulkerson and Johnson <ref> [12] </ref>. The problem of identifying a subtour elimination constraint can be rephrased as the problem of finding a minimum cut in a graph with real-valued edge weights.
Reference: [13] <author> U. Derigs and W. Meier. </author> <title> Implementing Goldberg's Max-Flow Algorithm | A Computational Investigation. </title> <journal> ZOR | Methods and Models of Operations Research, </journal> <volume> 33 </volume> <pages> 383-403, </pages> <year> 1989. </year>
Reference-contexts: As a result, our code is always competitive with hybrid, and sometimes outperforms it by a wide margin. Implementations of the push-relabel method for the maximum flow problem have been well-studied, e.g. <ref> [2, 10, 13, 14, 36] </ref>. A maximum flow code of Cherkassky and Goldberg [10] was the starting point of our implementation, ho. The implementation uses the heuristics global update and gap relabeling heuristics that are used in the maximum flow code. <p> Lemma 5.1 [20] The relabel operation increases d (v). We assume that a relabel operation always uses the gap relabeling heuristic <ref> [9, 13] </ref>. Just before relabeling v, the heuristic checks if any other vertex has a label of d (v). If the answer is yes, then v is relabeled. <p> If the answer is yes, then v is relabeled. Otherwise, the heuristic deletes all vertices with distance labels d (v) or greater from the graph, as the sink is not reachable from these vertices. This heuristic often speeds up push-relabel algorithms for the maximum flow problem <ref> [2, 10, 13, 36] </ref> and is essential for the analysis of the Hao-Orlin algorithm. 21 We use a standard implementation of gap relabeling that maintains an array of buckets, B [0 : : : 2n 1], with bucket B [i] containing a doubly linked list of all vertices with distance labels
Reference: [14] <author> U. Derigs and W. Meier. </author> <title> An Evaluation of Algorithmic Refinements and Proper Data-Structures for the Preflow-Push Approach for Maximum Flow. </title> <booktitle> In ASI Series on Computer and System Sciences, </booktitle> <volume> volume 8, </volume> <pages> pages 209-223. NATO, </pages> <year> 1992. </year>
Reference-contexts: As a result, our code is always competitive with hybrid, and sometimes outperforms it by a wide margin. Implementations of the push-relabel method for the maximum flow problem have been well-studied, e.g. <ref> [2, 10, 13, 14, 36] </ref>. A maximum flow code of Cherkassky and Goldberg [10] was the starting point of our implementation, ho. The implementation uses the heuristics global update and gap relabeling heuristics that are used in the maximum flow code.
Reference: [15] <author> P. Elias, A. Feinstein, and C. E. Shannon. </author> <title> Note on Maximum Flow Through a Network. </title> <journal> IRE Transactions on Information Theory, </journal> <volume> IT-2:117-199, </volume> <year> 1956. </year>
Reference-contexts: The well-known maxflow-mincut theorem <ref> [16, 15] </ref> states that the value of the maximum s-t flow is equal to the value of the minimum s-t cut, i.e., jf j = s;t (G).
Reference: [16] <author> L. R. Ford, Jr. and D. R. Fulkerson. </author> <title> Maximal Flow Through a Network. </title> <journal> Canadian Journal of Math., </journal> <volume> 8 </volume> <pages> 399-404, </pages> <year> 1956. </year>
Reference-contexts: The well-known maxflow-mincut theorem <ref> [16, 15] </ref> states that the value of the maximum s-t flow is equal to the value of the minimum s-t cut, i.e., jf j = s;t (G).
Reference: [17] <author> M. L. Fredman and R. E. Tarjan. </author> <title> Fibonacci Heaps and Their Uses in Improved Network Optimization Algorithms. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 34 </volume> <pages> 596-615, </pages> <year> 1987. </year>
Reference-contexts: The heap works well for most problem instances in our study. Note that for dense graphs, the number of increase key operations of the algorithm is relatively large. A brief study showed that for very large dense graphs, Fibonacci heaps <ref> [17] </ref> sometimes outperform 4-heaps. Fibonacci heaps or other priority queues with cheap increase key operations should be considered in applications involving such 19 graphs. 4.3 Incorporating the Padberg-Rinaldi Heuristic Nagamochi et al. [35] show that the Nagamochi-Ibaraki algorithm can benefit from the PR heuristics.
Reference: [18] <author> H. N. Gabow. </author> <title> A Matroid Approach to Finding Edge Connectivity and Packing Arbores-cences. </title> <journal> J. Comp. and Syst. Sci., </journal> <volume> 50 </volume> <pages> 259-273, </pages> <year> 1995. </year>
Reference-contexts: then switch to the simpler and more compact data structure described above after the preprocessing. 7 Karger's Algorithm Karger's algorithm [27] is based on the following two observations: * Any undirected graph with minimum cut c has a packing of c spanning trees that uses each edge at most twice <ref> [18] </ref>. * Given any packing of c spanning trees that uses each edge at most twice, at least some of the trees only cross the minimum cut twice. <p> We do so by finding a tree packing and considering the trees in it; some of them appropriately constrain the minimum cut. To find the packing of spanning trees, we use Gabow's Algorithm <ref> [18] </ref>. On an unweighted graph, this algorithm finds a packing of c trees in O (mc log m=n) time. This is fine for small values of c but expensive for large values. <p> We believe that the constant factor hidden in the O of the latter is large, so we only implemented the former. Packing spanning trees. Karger suggests two entirely different ways to pack spanning trees. We chose to use Gabow's algorithm <ref> [18] </ref>. It would be interesting to try the alternative ([38]) too. We tried checking to see if the tree packing contained multiple copies of the same tree, but this did not appear to help unless our sampling probability was too high, so we disabled it.
Reference: [19] <author> A. V. Goldberg and R. E. Tarjan. </author> <title> A New Approach to the Maximum Flow Problem. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 35 </volume> <pages> 921-940, </pages> <year> 1988. </year>
Reference-contexts: The classical Gomory-Hu algorithm [21] solves the minimum cut problem using n 1 minimum s-t cut computations. The fastest current algorithms for the s-t cut problem <ref> [1, 6, 7, 19, 30] </ref> use flow techniques, in particular the push-relabel method [19], and run in !(nm) time. <p> The classical Gomory-Hu algorithm [21] solves the minimum cut problem using n 1 minimum s-t cut computations. The fastest current algorithms for the s-t cut problem [1, 6, 7, 19, 30] use flow techniques, in particular the push-relabel method <ref> [19] </ref>, and run in !(nm) time.
Reference: [20] <author> A. V. Goldberg and R. E. Tarjan. </author> <title> Finding Minimum-Cost Circulations by Canceling Negative Cycles. </title> <booktitle> In Proc. 20th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 388-397, </pages> <year> 1988. </year>
Reference-contexts: Hao and Orlin showed that by using a push-relabel maximum flow algorithm to find the s-t min-cuts, and by properly pipelining these max-flow computations, they could find a min-cut in O (mn log n) time. 5.1 Push-Relabel Method First we review the push-relabel method <ref> [20] </ref> for finding minimum s-t cuts (as well as maximum flows) in directed graphs. We omit many details of the push-relabel methods, as they are not directly related to our implementation of the Hao-Orlin algorithm, ho. For a detailed description of the push-relabel method, see [20], and for details of an <p> we review the push-relabel method <ref> [20] </ref> for finding minimum s-t cuts (as well as maximum flows) in directed graphs. We omit many details of the push-relabel methods, as they are not directly related to our implementation of the Hao-Orlin algorithm, ho. For a detailed description of the push-relabel method, see [20], and for details of an efficient implementation 20 of the push-relabel algorithm for the maximum flow problem, see [10]. Our code ho is in many respects similar to that maximum flow code. We begin with some additional definitions. <p> The relabel operation applies to an active vertex v with no outgoing admissible arcs, and sets d (v) to the highest value allowed by the distance labeling constraints: one plus the smallest distance label of a vertex reachable from v via a residual arc. Lemma 5.1 <ref> [20] </ref> The relabel operation increases d (v). We assume that a relabel operation always uses the gap relabeling heuristic [9, 13]. Just before relabeling v, the heuristic checks if any other vertex has a label of d (v). If the answer is yes, then v is relabeled. <p> Then the algorithm sets d (s) = 2n 1 1 and saturates all arcs out of s. This gives the initial preflow and distance labeling. The algorithm applies push and relabel operations in an arbitrary order. When no operation applies, the algorithm terminates. Theorem 5.2 <ref> [20] </ref> When the generic algorithm terminates, the set of vertices that can reach t in G f defines a minimum cut, and e f (t) is the minimum s-t cut value. Theorem 5.3 [20] The number of relabel operations in the generic algorithm is O (n 2 ). <p> When no operation applies, the algorithm terminates. Theorem 5.2 <ref> [20] </ref> When the generic algorithm terminates, the set of vertices that can reach t in G f defines a minimum cut, and e f (t) is the minimum s-t cut value. Theorem 5.3 [20] The number of relabel operations in the generic algorithm is O (n 2 ). The number of push operations is O (n 2 m). The discharge operation combines the push and relabel operations at a low level. The discharge operation applies to an active vertex v. <p> Theorem 5.4 [8] The push-relabel algorithm with the highest label selection performs O (n 2 p m) push operations and runs in O (n 2 p m) time. Sophisticated data structures, such as dynamic trees, improve running times of the push-relabel algorithm in the worst case <ref> [20] </ref>. However, these data structures do not seem to help in practice, at least in the maximum flow context. <p> It would be interesting to find a more effective implementation of these internal tests. 5.4.2 Excess contractions. We introduce a simple heuristic that often allows us to contract a vertex in the middle of a flow computation. The general results on the push-relabel method <ref> [20] </ref> imply that the excess at a vertex v is a lower bound on the capacity of the minimum s-v cut.
Reference: [21] <author> R. E. Gomory and T. C. Hu. </author> <title> Multi-terminal network flows. </title> <journal> J. SIAM, </journal> <volume> 9 </volume> <pages> 551-570, </pages> <year> 1961. </year>
Reference-contexts: The problem of finding a minimum capacity cut between two specified vertices, s and t, is called the minimum s-t cut problem, and is closely related to the minimum cut problem. The classical Gomory-Hu algorithm <ref> [21] </ref> solves the minimum cut problem using n 1 minimum s-t cut computations. The fastest current algorithms for the s-t cut problem [1, 6, 7, 19, 30] use flow techniques, in particular the push-relabel method [19], and run in !(nm) time. <p> An s-t maximum flow algorithm can thus be used to find an s-t minimum cut, and minimizing over all n possible choices of s and t yields a minimum cut. 10 2.5 Gomory-Hu Algorithm In 1961, Gomory and Hu <ref> [21] </ref> showed that s;t (G) for all n pairs of s and t could actually be computed using only n 1 maximum flow computations. This immediately yields an algorithm for computing minimum cuts using only O (n) maximum flow computations.
Reference: [22] <author> D. Gusfield. </author> <title> Very Simple Methods for All Pairs Network Flow Analysis. </title> <journal> SIAM Journal on Computing, </journal> <volume> 19 </volume> <pages> 143-155, </pages> <year> 1990. </year>
Reference-contexts: We introduce algorithm-specific ways of using PR heuristics during execution of main subroutines of the algorithms. We also develop several new heuristics that significantly improve performance of our implementations. Implementations using graph contraction are usually difficult to code (see e.g. <ref> [22] </ref>) and may be inefficient. Our fastest implementations of all the algorithms we study use contraction. Although indeed difficult to code, these implementations are efficient because of the graph data structures we use. We now briefly detail the highlights of the implementations of our four codes.
Reference: [23] <author> J. Hao. </author> <title> A Faster Algorithm for Finding the Minimum Cut of a Graph. </title> <type> Unpublished manuscript, </type> <year> 1991. </year>
Reference-contexts: The fastest current algorithms for the s-t cut problem [1, 6, 7, 19, 30] use flow techniques, in particular the push-relabel method [19], and run in !(nm) time. For the minimum cut problem, Hao and Orlin <ref> [23, 25] </ref> have given an algorithm (ho), based on the push-relabel method, that shows how to perform all n 1 minimum s-t cuts in time asymptotically equal to that needed to perform one s-t minimum cut computation. This algorithm runs in O (nm log (n 2 =m)) time.
Reference: [24] <author> J. Hao and J. B. Orlin. </author> <title> A Faster Algorithm for Finding the Minimum Cut of a Graph. </title> <booktitle> In Proc. 3rd ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 165-174, </pages> <year> 1992. </year>
Reference-contexts: Preprocessing takes O (m log n) time. Since the cost of each PR pass is linear, it is amortized by the cost of the preceding phase. As we shall see, the PR heuristic is crucial to ni's efficiency. 5 Hao-Orlin Algorithm The Hao-Orlin algorithm <ref> [24] </ref> is a particular instance of the Gomory-Hu approach to finding minimum cuts. The basic startegy is to find the minimum cut by performing n separate s-t min-cut computation.
Reference: [25] <author> J. Hao and J. B. Orlin. </author> <title> A Faster Algorithm for Finding the Minimum Cut in a Directed Graph. </title> <journal> J. Algorithms, </journal> <volume> 17 </volume> <pages> 424-446, </pages> <year> 1994. </year>
Reference-contexts: The fastest current algorithms for the s-t cut problem [1, 6, 7, 19, 30] use flow techniques, in particular the push-relabel method [19], and run in !(nm) time. For the minimum cut problem, Hao and Orlin <ref> [23, 25] </ref> have given an algorithm (ho), based on the push-relabel method, that shows how to perform all n 1 minimum s-t cuts in time asymptotically equal to that needed to perform one s-t minimum cut computation. This algorithm runs in O (nm log (n 2 =m)) time. <p> This allows us to amortize work of the (n 1) s-t cut computations, and to obtain worst-case time bounds that are the same as those to perform one maximum flow computation. We give a brief description of this algorithm below. See <ref> [25] </ref> for details. A key concept of the Hao-Orlin algorithm is that of a frozen layer of vertices. A frozen layer is a set of vertices; different layers are distinct. A vertex is frozen if it belongs to a frozen layer and alive otherwise. <p> The running time of the algorithm is related to the total number of relabelings done. The total number of relabels over all s-t cut computations can be bound by O (n 2 ), the same as the bound for one s-t cut computation done in isolation. Results of <ref> [8, 25] </ref> then imply the following theorem. Theorem 5.5 [8] The Hao-Orlin algorithm with the highest label selection runs in O (n 2 p m) time. 5.3 Implementation Details In this section we describe implementation details of the ho code. We describe general imple mentation issues and heuristics used. <p> Second, we introduced a new heuristic, which we call excess contractions. 25 5.4.1 Padberg-Rinaldi Heuristics As in all our algorithms, we apply PR preprocessing at the beginning. Our internal PR heuristic is based on the following fact that is easy to prove using <ref> [25] </ref>: If s is the source and the edge fs; wg passes one of the tests PR1, : : :, PR4, then we can contract the edge, saturate all arcs out of w, and continue. We use amortization to decide when to apply PR1 and PR2 to the source.
Reference: [26] <author> D. R. Karger. </author> <title> A randomized fully polynomial approximation scheme for the all terminal network reliability problem. </title> <booktitle> In Proc. 27th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 11-17, </pages> <year> 1995. </year> <month> 135 </month>
Reference-contexts: 1 Introduction The minimum cut problem is the problem of partitioning the vertices of an n-node, m-edge weighted undirected graph into two sets so that the total weight of the set of edges with endpoints in different sets is minimized. This problem has many applications, including network reliability theory <ref> [26, 39] </ref>, information retrieval [4], compilers for parallel languages [5], and as a subroutine in cutting-plane algorithms for the Traveling Salesman problem (TSP) [3]. <p> At the end of this section, we justify our assumption. For simplicity, wee assume that our graph is unweighted; our analysis extends to weighted graphs by treating a weight w edge as w parallel unweighted edges. Our analysis is based on a network reliability analysis from <ref> [26] </ref>. That paper considers a graph in which each edge fails with probability p, and determines the probability that the graph remains connected. This is related to our objective as follows. <p> We ask whether deleting the uncontracted edges leaves us with a single component. In other words: we consider deleting every edge of G with probability 2 d=c , and ask whether the remaining (contracted) edges connected G. The following is proven in [32] (see also <ref> [26] </ref>), using the fact that among all graph with minimum cut c, the graph most likely to become disconnected under random edge failures is a cycle: Lemma 6.2 Let G have n edges and minimum cut c.
Reference: [27] <author> D. R. Karger. </author> <title> Minimum Cuts in Near-Linear Time. </title> <booktitle> In Proc. 28th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 56-63, </pages> <year> 1996. </year>
Reference-contexts: The algorithm of Nagamochi and Ibaraki [34] (ni) runs in O (n (m + n log n)) time. The algorithm of Karger and Stein [28] (ks) runs in O (n 2 log 3 n) expected time. Two closely related algorithms of Karger <ref> [27] </ref> (k) run in O (m log 3 n) and O (n 2 log n) expected time. These algorithms are based on new techniques which do not use flows. These algorithms do not extend to directed graphs, while the flow based algorithms, including ho, do. <p> Efficient use of these heuristics often requires a good understanding of the underlying algorithm and data structures. The Karger-Stein and Karger's algorithms are randomized. For these algorithms, we needed to develop somewhat different strategies for random edge selection than those that appeared in the original papers <ref> [27, 28] </ref>. These codes also benefit from the PR heuristics as well as new heuristics. We make significant progress in understanding practical performance of minimum cut algorithms and in establishing testing standards for future codes. Our study shows that no single algorithm dominates the others. <p> could not think of an efficient implementation of these tests without using adjacency lists, we use the the data structures and code of ni to do the PR preprocessing, and then switch to the simpler and more compact data structure described above after the preprocessing. 7 Karger's Algorithm Karger's algorithm <ref> [27] </ref> is based on the following two observations: * Any undirected graph with minimum cut c has a packing of c spanning trees that uses each edge at most twice [18]. * Given any packing of c spanning trees that uses each edge at most twice, at least some of the <p> We say that T k-constrains the cut in G. Karger's algorithm works as follows. First it finds a packing of spanning trees; then it examines the trees in the packing and for each tree finds the smallest cut that 1- or 2-respects the tree. The following is shown in <ref> [27] </ref> Lemma 7.2 The smallest cut that 1-respects a given tree can be found in fi (m + n) time. The smallest cut that 2-respects a given tree can be found in fi (n 2 ) time. <p> On the other hand, the 2-respects construction involves explicitly computing a set of n values (one for each pair of tree edges). Therefore, unlike the other algorithms we examine, this one is guaranteed to take (n 2 ) time regardless of the input. A related algorithm of <ref> [27] </ref> has a better time bound of O (m log 3 n). This algorithm, however, is more complicated and the constant factors of its implementations are likely to be large.
Reference: [28] <author> D. R. Karger and C. Stein. </author> <title> An ~ O(n 2 ) Algorithm for Minimum Cuts. </title> <booktitle> In Proc. 25th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 757-765, </pages> <year> 1993. </year>
Reference-contexts: The algorithm of Nagamochi and Ibaraki [34] (ni) runs in O (n (m + n log n)) time. The algorithm of Karger and Stein <ref> [28] </ref> (ks) runs in O (n 2 log 3 n) expected time. Two closely related algorithms of Karger [27] (k) run in O (m log 3 n) and O (n 2 log n) expected time. These algorithms are based on new techniques which do not use flows. <p> Efficient use of these heuristics often requires a good understanding of the underlying algorithm and data structures. The Karger-Stein and Karger's algorithms are randomized. For these algorithms, we needed to develop somewhat different strategies for random edge selection than those that appeared in the original papers <ref> [27, 28] </ref>. These codes also benefit from the PR heuristics as well as new heuristics. We make significant progress in understanding practical performance of minimum cut algorithms and in establishing testing standards for future codes. Our study shows that no single algorithm dominates the others. <p> complicates the code as we now need code to compute a maximum flow on a graph that is changing during the course of the flow computation. " 6 Karger-Stein Algorithm 6.1 Review of algorithm We begin by reviewing, at a high level, the recursive contraction algorithm of Karger and Stein <ref> [28] </ref>. Similar to the Nagamochi-Ibaraki algorithm, this algorithm repeatedly contracts edges. In contrast to NI, this algorithm will sometimes contract an edge that is in the minimum cut. In particular, the algorithm repeatedly chooses edges at random to contract. <p> This differs slightly from the Recursive Contraction Algorithm described in <ref> [28] </ref>. In that algorithm, rather than implementing the line marked (*), we repeatedly select and contract one edge at a time until the number of graph nodes is reduced to n= p 2. <p> In order to implement the algorithm most efficiently we need to address several issues. These include the graph data structure, random edge selection, and the use of PR tests. We discuss these issues below. 6.2 Depth Analysis Our new implementation requires an analysis somewhat different from that of <ref> [28] </ref>. The analysis of the Recursive Contraction Algorithm involved two parts. The first was a recurrence for the time taken to run a single iteration of the Recursive Contraction Algorithm. <p> Our new implementation no longer satisfies this recurrence, since we do not guarantee a specific reduction in the number of nodes in a recursive call. However, we have observed that this implementation is faster in practice. The second important part of <ref> [28] </ref> was an analysis of the success probability of a single iteration, which determined the number of iterations needed to give a high probability of success. <p> We found that it is much wiser to randomly choose and contract sets of edges, as described above, rather than choosing and contracting the edges one by one, as is done in <ref> [28] </ref>. We needed to use an exponential distribution to sample each edge with probability exponential in its weight.
Reference: [29] <author> D.R. Karger. </author> <title> Random Sampling in Cut, Flow, and Network Design Problems. </title> <booktitle> In Proc. 26th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 648-657, </pages> <year> 1994. </year> <note> Submitted to Math. </note> <institution> of Oper. Res. </institution>
Reference-contexts: This could potentially run faster and return fewer distinct trees on some graphs. Estimating the Minimum Cut In order to perform the sampling step correctly, the algorithm needs to know the value of the minimum cut. In <ref> [29] </ref> Karger gives two ways to resolve this problem. The first is to run Matula's ([33]) linear time 3-approximation algorithm; using that value divided by 3 gives a probability that doesn't affect correctness and at worst multiplies the number of edges in the sample by 3.
Reference: [30] <author> V. King, S. Rao, and R. Tarjan. </author> <title> A Faster Deterministic Maximum Flow Algorithm. </title> <journal> J. Algorithms, </journal> <volume> 17 </volume> <pages> 447-474, </pages> <year> 1994. </year>
Reference-contexts: The classical Gomory-Hu algorithm [21] solves the minimum cut problem using n 1 minimum s-t cut computations. The fastest current algorithms for the s-t cut problem <ref> [1, 6, 7, 19, 30] </ref> use flow techniques, in particular the push-relabel method [19], and run in !(nm) time.
Reference: [31] <author> E. L. Lawler, J. K. Lenstra, A. H. G. Rinnoy Kan, and D. B. Shmoys. </author> <title> The Traveling Salesman Problem. </title> <publisher> Wiley & Sons, </publisher> <year> 1985. </year>
Reference-contexts: The problem of identifying a subtour elimination constraint can be rephrased as the problem of finding a minimum cut in a graph with real-valued edge weights. Thus, cutting plane algorithms for the traveling salesman problem must solve a large number of minimum cut problems (see <ref> [31] </ref> for a survey of the area). We obtained some of the minimum cut instances that were solved by Applegate and Cook. Table 2 gives a summary of these instances, including their "names" which correspond to the original TSP problems.
Reference: [32] <author> M.V. Lomonosov and V.P. Poleskii. </author> <title> Lower bound of network reliability. </title> <journal> Problems of Information Transmission, </journal> <volume> 7 </volume> <pages> 118-123, </pages> <year> 1971. </year>
Reference-contexts: We ask whether deleting the uncontracted edges leaves us with a single component. In other words: we consider deleting every edge of G with probability 2 d=c , and ask whether the remaining (contracted) edges connected G. The following is proven in <ref> [32] </ref> (see also [26]), using the fact that among all graph with minimum cut c, the graph most likely to become disconnected under random edge failures is a cycle: Lemma 6.2 Let G have n edges and minimum cut c. <p> Lemma 6.4 Conditioned on the fact that a minimum cut has failed, the cycle is the most likely graph to partition into more than two pieces under random edge failures. Proof. A straightforward modification of <ref> [32] </ref>. Corollary 6.5 Conditioned on the fact that a minimum cut has failed, the probability a graph partitions into 3 or more pieces is at most np c=2 The following lemma is an immediate corollary.
Reference: [33] <author> D. W. Matula. </author> <title> A Linear Time 2 + * Approximation Algorithm for Edge Connectivity. </title> <booktitle> In Proc. 4th ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 500-504, </pages> <year> 1993. </year>
Reference: [34] <author> H. Nagamochi and T. Ibaraki. </author> <title> Computing Edge-Connectivity in Multigraphs and Capac-itated Graphs. </title> <journal> SIAM J. Disc. Meth., </journal> <volume> 5 </volume> <pages> 54-66, </pages> <year> 1992. </year>
Reference-contexts: This algorithm runs in O (nm log (n 2 =m)) time. Several new algorithms discovered recently are theoretically more efficient time bounds for these algorithms are competitive with or better than the best time bounds for the minimum s-t cut problem. The algorithm of Nagamochi and Ibaraki <ref> [34] </ref> (ni) runs in O (n (m + n log n)) time. The algorithm of Karger and Stein [28] (ks) runs in O (n 2 log 3 n) expected time. <p> contracted graph along with a new minimum cut estimate, we get the following high-level pseudocode: Algorithm ni input: graph G output: minimum cut value ^ = minfc (v) : v 2 V g while jV j 2 (G; ^ ) ContractSafe (G; ^ ) return ^ The original Nagamochi-Ibaraki algorithm <ref> [34] </ref> implements ContractSafe via a graph search called scan-first search. Initially, all edges are unscanned and all nodes are unvisited. <p> The search always chooses the unvisited node v with maximum r (v) and scans all its outgoing edges. Let x; y be the next-to-last and last node visited during the search, respectively. As is shown in <ref> [34] </ref>, the trivial cut fyg is a minimum x-y cut, so one can update ^ updated to be the minimum of ^ and c (y), and contract x and y.
Reference: [35] <author> H. Nagamochi, T. Ono, and T. Ibaraki. </author> <title> Implementing an Efficient Minimum Capacity Cut Algorithm. </title> <journal> Math. Prog., </journal> <volume> 67 </volume> <pages> 297-324, </pages> <year> 1994. </year>
Reference-contexts: These algorithms are based on new techniques which do not use flows. These algorithms do not extend to directed graphs, while the flow based algorithms, including ho, do. Theoretical considerations suggest that some of these new algorithms should be practical, and an experimental study of Nagamochi et al. <ref> [35] </ref> confirms this for ni. The question of practical performance of the other new algorithms, however, has not been addressed. In this paper we study the practical performance of recent minimum cut algorithms. <p> These contractions, which we call the PR heuristics, apply in the context of other algorithms and often lead to a big improvement in performance. Nagamochi et al. <ref> [35] </ref> describe an efficient implementation of ni that uses the PR heuristics. The data of [35] suggests that the hybrid code of Nagamochi et al. is more efficient than the Padberg-Rinaldi code. <p> These contractions, which we call the PR heuristics, apply in the context of other algorithms and often lead to a big improvement in performance. Nagamochi et al. <ref> [35] </ref> describe an efficient implementation of ni that uses the PR heuristics. The data of [35] suggests that the hybrid code of Nagamochi et al. is more efficient than the Padberg-Rinaldi code. The former code seems to be the fastest minimum cut code described in a paper published prior to our work. <p> Although indeed difficult to code, these implementations are efficient because of the graph data structures we use. We now briefly detail the highlights of the implementations of our four codes. Greater 4 details will be given in subsequent sections. Our implementation of ni builds on hybrid <ref> [35] </ref>. However, we use different strategy for applying the PR heuristics and develop a new technique for graph contraction. As a result, our code is always competitive with hybrid, and sometimes outperforms it by a wide margin. <p> Thus, on these classes, they can run faster. The push-relabel method has been extensively studied, and our implementations of ho take 5 advantage what has been learned in the maximum flow context. Our implementation of ni takes advantage of performance-improving ideas of <ref> [35] </ref>. Our implementations of ks and k, however, were developed from scratch. Our study shows how important data structures and heuristics are for the minimum cut algorithm performance. New ideas may improve performance. This is more likely for ks and k, which have been studied less. <p> The first way, which we call the exhaustive PR heuristic, is described in [37]. The second way, which we call the source PR heuristic, is described in <ref> [35] </ref> and is similar to another heuristic of Padberg and Rinaldi. The exhaustive PR heuristic applies all four tests PR1 - PR4 until no edge can be contracted using these tests. The exhaustive heuristic takes O (nm) time [37]. <p> If a test succeeds, x and y are contracted using compact contraction, and edges of the newly formed node are scanned again from the beginning of the edge list. If the test fails, the heuristic terminates. In <ref> [35] </ref>, an edge fx; yg if is considered to have large capacity c (x; y) 0:7c (x)=n 0 , where n 0 is the current number of nodes. The above source PR heuristic is from [35]. <p> If the test fails, the heuristic terminates. In <ref> [35] </ref>, an edge fx; yg if is considered to have large capacity c (x; y) 0:7c (x)=n 0 , where n 0 is the current number of nodes. The above source PR heuristic is from [35]. The hybrid code we obtained from the authors, however, implements a somewhat different version of this heuristic. 14 3.2 PR Passes We introduce the idea of PR passes. All our codes that use the PR heuristics apply these passes as preprocessing; ni also applies the passes internally. <p> Thus each phase contracts at least one edge, and there are at most n 3 phases to contract down to two nodes. The implementation of <ref> [35] </ref> incorporates two additional heuristics which we use as well. The first may directly contract additional edges, while the second may decrease the estimate on ^ . For complete justification of these heuristics, see [35]. <p> The implementation of <ref> [35] </ref> incorporates two additional heuristics which we use as well. The first may directly contract additional edges, while the second may decrease the estimate on ^ . For complete justification of these heuristics, see [35]. The first heuristic is to contract any edge e = fv; wg with q (e) ^ , as q (e) is actually a lower bound on v;w (G). This observation greatly speeds up the algorithm, as every additional edge contracted reduces the number of phases by one. <p> A brief study showed that for very large dense graphs, Fibonacci heaps [17] sometimes outperform 4-heaps. Fibonacci heaps or other priority queues with cheap increase key operations should be considered in applications involving such 19 graphs. 4.3 Incorporating the Padberg-Rinaldi Heuristic Nagamochi et al. <ref> [35] </ref> show that the Nagamochi-Ibaraki algorithm can benefit from the PR heuristics. They incorporate a variant of the source version of the heuristic (see Section 3): At the end of every phase, they apply the heuristic to a node created by the last contraction of the phase. <p> In our final experiment, we used five different problem generators, 39 and for each one we generated several different problem families, for a total of 21 different families. One of our generators, noigen, is an implementation of the generator of <ref> [35] </ref>, and another one, prgen, is an implementation of the generator from [37]. The remaining generators are original. One of the contributions of our work is to introduce this useful collection of generators than can be used by others to test minimum cut algorithms. We describe each family individually below. <p> For the remainder of this section, when we say something is chosen at random from a given range, we mean an integer chosen uniformly at random in the given range. 8.1 noigen generator Our noigen generator is an implementation of a generator of Nagamochi et al. <ref> [35] </ref>. The generator produces a graph consisting of several heavily connected components. With properly chosen parameters, vertices of each component are on the same side of any minimum cut. <p> The desired effect is that, for large enough P , the graph will have k components and the minimum cut will consist solely of edges with endpoints in different components. Following <ref> [35] </ref>, we generated 6 different problem families. Our families are similar to those of [35]. However, we use larger problem sizes because our algorithms and hardware are faster. 40 Based on experimental feedback, we placed additional data points at "interesting" places. <p> The desired effect is that, for large enough P , the graph will have k components and the minimum cut will consist solely of edges with endpoints in different components. Following <ref> [35] </ref>, we generated 6 different problem families. Our families are similar to those of [35]. However, we use larger problem sizes because our algorithms and hardware are faster. 40 Based on experimental feedback, we placed additional data points at "interesting" places. <p> We study the following codes: * NI implementations. hybrid, the Nagamochi et al. <ref> [35] </ref> implementation of the Nagamochi-Ibaraki algo rithm with a PR heuristic, - ni, our implementation of the Nagamochi-Ibaraki algorithm with all heuristics turned on. - ni-nopr, same as ni but with the PR heuristic turned off. * HO implementations. ho, our implementation of the Hao-Orlin algorithm with all heuristics turned on.
Reference: [36] <author> Q. C. Nguyen and V. Venkateswaran. </author> <title> Implementations of Goldberg-Tarjan Maximum Flow Algorithm. </title> <editor> In D. S. Johnson and C. C. McGeoch, editors, </editor> <title> Network Flows and Matching: </title> <booktitle> First DIMACS Implementation Challenge, </booktitle> <pages> pages 19-42. </pages> <publisher> AMS, </publisher> <year> 1993. </year>
Reference-contexts: As a result, our code is always competitive with hybrid, and sometimes outperforms it by a wide margin. Implementations of the push-relabel method for the maximum flow problem have been well-studied, e.g. <ref> [2, 10, 13, 14, 36] </ref>. A maximum flow code of Cherkassky and Goldberg [10] was the starting point of our implementation, ho. The implementation uses the heuristics global update and gap relabeling heuristics that are used in the maximum flow code. <p> If the answer is yes, then v is relabeled. Otherwise, the heuristic deletes all vertices with distance labels d (v) or greater from the graph, as the sink is not reachable from these vertices. This heuristic often speeds up push-relabel algorithms for the maximum flow problem <ref> [2, 10, 13, 36] </ref> and is essential for the analysis of the Hao-Orlin algorithm. 21 We use a standard implementation of gap relabeling that maintains an array of buckets, B [0 : : : 2n 1], with bucket B [i] containing a doubly linked list of all vertices with distance labels
Reference: [37] <author> M. Padberg and G. Rinaldi. </author> <title> An Efficient Algorithm for the Minimum Capacity Cut Problem. </title> <journal> Math. Prog., </journal> <volume> 47 </volume> <pages> 19-36, </pages> <year> 1990. </year>
Reference-contexts: We try to take maximum advantage of the heuristics while making sure that the heuristics do not significantly increase the running time when they fail. The most efficient implementation of the Gomory-Hu algorithm that we are aware of is due to Padberg and Rinaldi <ref> [37] </ref>. In order to reduce the number of maximum flow computations needed, which is n 1 for the Gomory-Hu algorithm, they developed a set of heuristics which contract certain edges during the computation and, if successful, reduce the number of maximum flow computations. <p> These bounds, however, can be very pessimistic. If discovery time of a randomized algorithm is always a tiny fraction of the total time, one may be justified in making a smaller number of "trials." 3 Padberg-Rinaldi Heuristics In 1990, Padberg and Rinaldi <ref> [37] </ref> introduced heuristic improvements to the Gomory-Hu algorithm. Recall that the Gomory-Hu algorithm performs a series of maximum flow computations; each maximum flow computation identifies one edge which can be contracted. <p> The first way, which we call the exhaustive PR heuristic, is described in <ref> [37] </ref>. The second way, which we call the source PR heuristic, is described in [35] and is similar to another heuristic of Padberg and Rinaldi. The exhaustive PR heuristic applies all four tests PR1 - PR4 until no edge can be contracted using these tests. <p> The exhaustive PR heuristic applies all four tests PR1 - PR4 until no edge can be contracted using these tests. The exhaustive heuristic takes O (nm) time <ref> [37] </ref>. The heuristic was developed in the context of the Gomory-Hu algorithm, which had a worst case running time of !(n 2 m). Since, we study more efficient algorithms, this heuristic is too expensive for our purposes. The source PR heuristic works as follows. <p> One of our generators, noigen, is an implementation of the generator of [35], and another one, prgen, is an implementation of the generator from <ref> [37] </ref>. The remaining generators are original. One of the contributions of our work is to introduce this useful collection of generators than can be used by others to test minimum cut algorithms. We describe each family individually below. <p> The Traveling Salesman Problem is much harder than the minimum cut problem, and the size of problems that can be solved by the current cutting plane algorithms is limited. 8.6 prgen generator Our prgen generator is an implementation of the generator used by Padberg and Rinaldi <ref> [37] </ref>. This generator produces two different types of graphs. The first type is a random graph with an expected density d. <p> We used eight families, PR1-PR8, with parameter values given in Table 3. These values are precisely those used by Padberg and Rinaldi in their paper <ref> [37] </ref>. We use these problem families to compare heuristics in ho with those in the Padber-Rinaldi code. See Section 10.3. 46 Family n d c PR1 100,200,300,400 2 1 PR3 100,200,300,400 50 1 PR5 100,200,300,400,500,1000,1500,2000 2 2 PR7 100,200,300,400 50 2 Table 3: PR families. <p> We use running times and operation counts to explain algorithm performance and to study performance effects of heuristics and data structures. To get a better perspective on effectiveness of ho's internal PR heuristic, we use the data from <ref> [37] </ref> to indirectly compare ho and ho-noxs with the Padberg-Rinaldi code pr [37]. See the end of Section 10.3 for details. 10.1 Relative Performance The most robust code in our study is ho, but it does not dominate all other algorithms. The second best code is ni. <p> To get a better perspective on effectiveness of ho's internal PR heuristic, we use the data from <ref> [37] </ref> to indirectly compare ho and ho-noxs with the Padberg-Rinaldi code pr [37]. See the end of Section 10.3 for details. 10.1 Relative Performance The most robust code in our study is ho, but it does not dominate all other algorithms. The second best code is ni. <p> A more effective strategy for internal PR tests may improve ho's performance. To better understand these tests, we compare ho and ho-noxs with the Padberg-Rinaldi code pr, which implements the Gomory-Hu algorithm and performs exhaustive PR tests after every maximum flow computation. We use the data from <ref> [37] </ref> and compare the number of heuristic contraction 53 operations only. We would like to make a few remarks on this experiment. First, the comparison is indirect. We compare the data from [37] with the results we obtained by implementing the generator described in that paper and using the same parameter <p> We use the data from <ref> [37] </ref> and compare the number of heuristic contraction 53 operations only. We would like to make a few remarks on this experiment. First, the comparison is indirect. We compare the data from [37] with the results we obtained by implementing the generator described in that paper and using the same parameter values as in the paper. Our instances are probably not the same as [37], but the variances are low, making the operation count comparison valid. <p> First, the comparison is indirect. We compare the data from <ref> [37] </ref> with the results we obtained by implementing the generator described in that paper and using the same parameter values as in the paper. Our instances are probably not the same as [37], but the variances are low, making the operation count comparison valid. Second, comparing the running times in this context is meaningless. Our times are hundreds of times smaller.
Reference: [38] <author> S. A. Plotkin, D. Shmoys, and E. Tardos. </author> <title> Fast Approximation Algorithms for Fractional Packing and Covering. </title> <booktitle> In Proc. 32nd IEEE Annual Symposium on Foundations of Computer Science, </booktitle> <year> 1991. </year>
Reference-contexts: Even on moderate density graphs it may help just because it doesn't use (n 2 ) space. It may also be profitable to replace Gabow's algorithm with the packing algorithm of <ref> [38] </ref>. There are two reasons for this. First, as implemented Gabow's algorithm doesn't have good constants. ho beats Gabow's algorithm until the largest of the REG3 problems, even though they are a great case for the latter.
Reference: [39] <author> A. Ramanathan and C. Colbourn. </author> <title> Counting Almost Minimum Cutsets with Reliability Applications. </title> <journal> Math. Prog., </journal> <volume> 39 </volume> <pages> 253-261, </pages> <year> 1987. </year>
Reference-contexts: 1 Introduction The minimum cut problem is the problem of partitioning the vertices of an n-node, m-edge weighted undirected graph into two sets so that the total weight of the set of edges with endpoints in different sets is minimized. This problem has many applications, including network reliability theory <ref> [26, 39] </ref>, information retrieval [4], compilers for parallel languages [5], and as a subroutine in cutting-plane algorithms for the Traveling Salesman problem (TSP) [3].
Reference: [40] <author> D. D. Sleator and R. E. Tarjan. </author> <title> A Data Structure for Dynamic Trees. </title> <journal> J. Comput. System Sci., </journal> <volume> 26 </volume> <pages> 362-391, </pages> <year> 1983. </year> <month> 136 </month>
Reference-contexts: Second, comparing the running times in this context is meaningless. Our times are hundreds of times smaller. However, we think that the pr code would be much faster if its maximum flow subroutine (which is an implementation of the Sleator-Tarjan algorithm <ref> [40] </ref>) were replaced by a good implementation if the push-relabel method, such as that of [10]. Also, our hardware is faster than that used by Padberg and Rinaldi. Tables 44|51 give data for the PR1-PR8 families.
References-found: 40

