URL: http://www-unix.mcs.anl.gov/prism/lib/techsrc/wn10.ps.Z
Refering-URL: http://www-unix.mcs.anl.gov/prism/lib/tech.html
Root-URL: http://www.mcs.anl.gov
Title: ON THE IMPACT OF HPF DATA LAYOUT ON THE DESIGN OF EFFICIENT AND MAINTAINABLE PARALLEL
Author: Christian H. Bischof Steven Huss-Lederman Elaine M. Jacobson Xiaobai Sun and Anna Tsao 
Abstract: In this document, we are concerned with the effects of data layouts for nonsquare processor meshes on the implementation of common dense linear algebra kernels such as matrix-matrix multiplication, LU factorizations, or eigenvalue solvers. In particular, we address ease of programming and tunability of the resulting software. We introduce a generalization of the torus wrap data layout, that results in a decoupling of "local" and "global" data layout view. As a result, it allows for intuitive programming of linear algebra algorithms, and tuning of the algorithm for a particular mesh aspect ratio or machine characteristics. This layout is as simple as the proposed HPF layout, but, in our opinion, enhances ease of programming as well as ease of performance tuning. We emphasize that we do not advocate that all users need be concerned with these issues. We do, however, believe, that for the foreseeable future "assembler coding" (as message-passing code is likely to be viewed from a HPF programmers' perspective) will be needed to deliver high performance for computationally intensive kernels. As a result, we believe that the adoption of this approach would not only accelerate the generation of efficient linear algebra software libraries, but would also accelerate the adoption of HPF as a result. We point out, however, that the adoption of this new layout would necessitate that an HPF compiler insure that data objects are operated on in a consistent fashion across subroutine and function calls. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Anderson, E., A. Benzoni, J. Dongarra, S. Moulton, S. Ostrouchov, B. Tourancheau, & R. van de Geijn, </author> <title> LAPACK for distributed memory architectures: Progress report, </title> <booktitle> Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Houston, </address> <publisher> SIAM, </publisher> <year> 1991. </year>
Reference-contexts: In both figures, the shaded blocks represent the diagonal blocks of the matrix when r = s. This special case is used in algorithms such as the solution of triangular systems [5] and the reduction of a symmetric matrix to tridiagonal form in <ref> [1, 9, 3] </ref>.
Reference: 2. <author> Ashcraft, C. C., </author> <title> The distributed solution of linear systems using the torus wrap data mapping, </title> <institution> Engineering Computing and Analysis Technical Report ECA-TR-147, Boeing Computer Services (1990). </institution>
Reference-contexts: Virtual 2-D torus wrap was motivated by our desire to exploit the advantageous features of block torus wrap (see, for example, <ref> [2, 6] </ref>), while at the same time maintaining the simplicity and cost effectiveness of the communication patterns found in many algorithms for square meshes.
Reference: 3. <author> Bischof, C., M. Marques, and X. Sun, </author> <title> Parallel bandreduction and tridiagonalization, </title> <booktitle> Proceedings, Sixth SIAM Conference on Parallel Processing for Scientific Computing (Norfolk, </booktitle> <address> Virginia, </address> <month> March 22-24, </month> <editor> 1993) (R. F. Sincovec, eds.), </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1993. </year>
Reference-contexts: In both figures, the shaded blocks represent the diagonal blocks of the matrix when r = s. This special case is used in algorithms such as the solution of triangular systems [5] and the reduction of a symmetric matrix to tridiagonal form in <ref> [1, 9, 3] </ref>.
Reference: 4. <author> Choi, J., J. J. Dongarra, R. Pozo, & D. W. Walker, </author> <title> ScaLAPACK: A scalable linear algebra library for distributed memory concurrent computers, </title> <booktitle> Proceedings, Fourth Symposium on the Frontiers of Massively Parallel Computation (McLean, </booktitle> <address> Virginia, </address> <month> October 19-21, </month> <title> 1992), </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1992, </year> <pages> pp. 120-127. </pages>
Reference-contexts: This data layout is a natural extension of the virtual contiguous block data layout described in papers such as [8] and subsumes the block scattered decomposition (e.g., <ref> [4] </ref>) currently being suggested for the HPF standard.
Reference: 5. <author> Choi, J., J. J. Dongarra, & D. W. Walker, </author> <title> Level 3 BLAS for distributed memory concurrent computers, </title> <booktitle> CNRS-NSF Workshop on Environments and Tools for Parallel Scientific Computing (Saint Hilaire du Touvet, </booktitle> <address> France, September 7-8, 1992), </address> <publisher> Elsevier Science Publishers, </publisher> <year> 1992. </year> <note> 8 PRISM NOTES </note>
Reference-contexts: Figure 2 (b) also shows the actual local arrangement of blocks within each processor. In both figures, the shaded blocks represent the diagonal blocks of the matrix when r = s. This special case is used in algorithms such as the solution of triangular systems <ref> [5] </ref> and the reduction of a symmetric matrix to tridiagonal form in [1, 9, 3].
Reference: 6. <author> Hendrikson, B. and D. Womble, </author> <title> The torus-wrap mapping for dense matrix calculations on massively parallel computers, </title> <institution> SAND92-0792, Sandia National Laboratories (1992). </institution>
Reference-contexts: Virtual 2-D torus wrap was motivated by our desire to exploit the advantageous features of block torus wrap (see, for example, <ref> [2, 6] </ref>), while at the same time maintaining the simplicity and cost effectiveness of the communication patterns found in many algorithms for square meshes.
Reference: 7. <author> Huss-Lederman, S., E. M. Jacobson, A. Tsao, & G. Zhang, </author> <title> Matrix Multiplication on the Intel Touchstone Delta, </title> <booktitle> Proceedings, Sixth SIAM Conference on Parallel Processing for Scientific Computing (Norfolk, </booktitle> <address> Virginia, </address> <month> March 22-24, </month> <editor> 1993) (R. F. Sincovec, eds.), </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1993, </year> <note> also to appear as Technical Report, </note> <institution> Supercomputing Research Center, </institution> <year> 1993. </year>
Reference-contexts: This data layout is currently being investigated as part of the PRISM (Parallel Research on Invariant Subspace Methods) project and has been effectively utilized in the development of a highly efficient general code for matrix multiplication on the Intel Touchstone Delta <ref> [7] </ref>. We propose that this data layout, which we call virtual 2-D torus wrap, be considered for inclusion into the High Performance Fortran standard, because we believe it to have great potential in the design and development of dense linear algebra algorithms. <p> However, symmetric operations such as transposition and tridiagonalization are greatly simplified. * Virtual 2-D torus wrap provides data layouts which maximize the granularity of local computations by ensuring proper maximal alignment in algorithms such the distributed matrix multiplication algorithms found in <ref> [8, 7] </ref>. Hence, this layout can make full use of, for example, assembler-coded node BLAS, or LAPACK routines. In the remainder of this document, we first review torus wrap and the block scattered decomposition and then describe the virtual 2-D torus wrap data layout. <p> the "node code". * Symmetric operations such as transposition and tridiagonalization are straightforward, since each virtual processor behaves as if it were a physical processor on a square mesh. * Row and column indices are naturally aligned for matrix operands in algorithms such as the distributed matrix multiplication algorithms in <ref> [8, 7] </ref>.
Reference: 8. <author> Mathur, K. K. & S. L. Johnsson, </author> <title> Multiplication of matrices of arbitrary shape on a data parallel computer (1992), Thinking Machines Corporation, </title> <type> preprint, </type> <note> also released as Technical Report TR-216. </note>
Reference-contexts: This data layout is a natural extension of the virtual contiguous block data layout described in papers such as <ref> [8] </ref> and subsumes the block scattered decomposition (e.g., [4]) currently being suggested for the HPF standard. <p> However, symmetric operations such as transposition and tridiagonalization are greatly simplified. * Virtual 2-D torus wrap provides data layouts which maximize the granularity of local computations by ensuring proper maximal alignment in algorithms such the distributed matrix multiplication algorithms found in <ref> [8, 7] </ref>. Hence, this layout can make full use of, for example, assembler-coded node BLAS, or LAPACK routines. In the remainder of this document, we first review torus wrap and the block scattered decomposition and then describe the virtual 2-D torus wrap data layout. <p> This allows the user to achieve maximal granularity in block algorithms. Notice that if r = (# rows of A)=6 and s = (# columns of A)=6, we have the contiguous block data layout <ref> [8] </ref>. An important point to realize is that successive panels in each dimension can be assigned to virtual processors arbitrarily and a very useful case occurs when panels are assigned with a "virtual panel spacing" in each dimension, s r and s c . <p> the "node code". * Symmetric operations such as transposition and tridiagonalization are straightforward, since each virtual processor behaves as if it were a physical processor on a square mesh. * Row and column indices are naturally aligned for matrix operands in algorithms such as the distributed matrix multiplication algorithms in <ref> [8, 7] </ref>.
Reference: 9. <author> Van de Geijn, R. A., </author> <title> Massively parallel LINPACK benchmark on the Intel Touchstone Delta and iPSC/860 systems: </title> <type> Progress Report, </type> <institution> Computer Science Technical Report TR-91-28, University of Texas (1991). </institution>
Reference-contexts: In both figures, the shaded blocks represent the diagonal blocks of the matrix when r = s. This special case is used in algorithms such as the solution of triangular systems [5] and the reduction of a symmetric matrix to tridiagonal form in <ref> [1, 9, 3] </ref>.
References-found: 9

