URL: http://www.research.att.com/~lewis/papers/lewis96c.ps
Refering-URL: http://www.research.att.com/~lewis/chronobib.html
Root-URL: 
Email: (lewis@research.att.com)  
Title: Information Retrieval and the Statistics of Large Data Sets  
Author: David D. Lewis 
Note: To appear in Proceedings of the Massive Data Sets Workshop. National Academy Press, 1996. (Bibliographic information tentative.)  
Address: 600 Mountain Avenue Murray Hill, NJ 07974  
Affiliation: AT&T Bell Laboratories  
Abstract: Providing content-based access to large quantities of text is a difficult task, given our poor understanding of the formal semantics of human language. The most successful approaches to retrieval, routing, and categorization of documents have relied heavily on statistical techniques. We briefly review some of those techniques and point out where better statistical insight could lead to further advances.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. </author> <title> Indexing by latent semantic indexing. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 41(6) </volume> <pages> 391-407, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: A variety of unsupervised learning methods have been applied to IR, with the hope of finding structure in large bodies of text that would improve on straightforward representations. These include clustering of words or documents [10, 20], factor analytic decompositions of term by document matrices <ref> [1] </ref>, and various term weighting methods [16]. Similarly, the retrieval query, routing profile, or category description provided by an IR system user is often far from ideal as well.
Reference: [2] <editor> D. K. Harman, editor. </editor> <booktitle> The First Text REtrieval Conference (TREC-1), </booktitle> <address> Gaithersburg, MD 20899, </address> <year> 1993. </year> <institution> National Institute of Standards and Technology. </institution> <note> Special Publication 500-207. </note>
Reference-contexts: Comparisons of over 30 IR methods in the recent NIST/ARPA Text Retrieval Conferences (TREC), have resulted in a number of modifications to these methods to deal with large (one million documents or more) collections of diverse full text documents <ref> [2, 3, 4] </ref>. Much of this tuning has been ad hoc and heavily empirical. Little is known about the relationship between properties of a text base and the best IR methods to use with it.
Reference: [3] <editor> D. K. Harman, editor. </editor> <booktitle> The Second Text REtrieval Conference (TREC-2), </booktitle> <address> Gaithersburg, MD 20899, </address> <year> 1994. </year> <institution> National Institute of Standards and Technology. </institution> <note> Special Publication 500-215. 4 </note>
Reference-contexts: Comparisons of over 30 IR methods in the recent NIST/ARPA Text Retrieval Conferences (TREC), have resulted in a number of modifications to these methods to deal with large (one million documents or more) collections of diverse full text documents <ref> [2, 3, 4] </ref>. Much of this tuning has been ad hoc and heavily empirical. Little is known about the relationship between properties of a text base and the best IR methods to use with it.
Reference: [4] <editor> D. K. Harman, editor. </editor> <booktitle> Overview of the Third Text REtrieval Conference (TREC-3), </booktitle> <address> Gaithers-burg, MD 20899-0001, </address> <year> 1995. </year> <institution> National Institute of Standards and Technology. </institution> <note> Special Publication 500-225. </note>
Reference-contexts: Comparisons of over 30 IR methods in the recent NIST/ARPA Text Retrieval Conferences (TREC), have resulted in a number of modifications to these methods to deal with large (one million documents or more) collections of diverse full text documents <ref> [2, 3, 4] </ref>. Much of this tuning has been ad hoc and heavily empirical. Little is known about the relationship between properties of a text base and the best IR methods to use with it.
Reference: [5] <author> Donna Harman. </author> <title> Ranking algorithms. </title> <editor> In William B. Frakes and Ricardo Baeza-Yates, editors, </editor> <booktitle> Information Retrieval: Data Structures and Algorithms, </booktitle> <pages> pages 363-392. </pages> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: A score is computed for each document based on the words it contains, and the highest scoring documents are retrieved, routed, categorized, etc. There are several variations on this approach <ref> [5, 17, 18, 19] </ref>. Vector space models treat the words suggested by the user as specifying an ideal relevant document in a high dimensional space. The distance of actual documents to this point is used as a measure of relevance.
Reference: [6] <author> Donna Harman. </author> <title> Relevance feedback and other query modification techniques. </title> <editor> In William B. Frakes and Ricardo Baeza-Yates, editors, </editor> <booktitle> Information Retrieval: Data Structures and Algorithms, </booktitle> <pages> pages 241-263. </pages> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: Similarly, the retrieval query, routing profile, or category description provided by an IR system user is often far from ideal as well. Supervised learning techniques, where user feedback on relevant documents is used to improve the original user input, have been widely used <ref> [6, 15] </ref>. Both parametric and nonparametric (e.g. neural nets, decision trees, nearest neighbor classifiers) have been used.
Reference: [7] <author> Donna Harman. </author> <title> Overview of the third Text REtrieval Conference (TREC-3). </title> <editor> In D. K. Harman, editor, </editor> <booktitle> Overview of the Third Text REtrieval Conference (TREC-3), </booktitle> <pages> pages 1-27, </pages> <address> Gaithersburg, MD, </address> <year> 1995. </year> <institution> U. S. Dept. of Commerce, National Institute of Standards and Technology. </institution>
Reference-contexts: Both parametric and nonparametric (e.g. neural nets, decision trees, nearest neighbor classifiers) have been used. Supervised learning is particularly effective in routing (where a user can supply ongoing feedback as the system is used) <ref> [7] </ref> and in text categorization (where a large body of manually indexed text may be available) [12, 14]. 2 The Future These are exciting times for IR.
Reference: [8] <author> David D. Lewis. </author> <booktitle> Learning in intelligent information retrieval. In Eighth International Workshop on Machine Learning, </booktitle> <pages> pages 235-239, </pages> <year> 1991. </year>
Reference-contexts: Statistical approaches have been widely applied to these systems because of the poor fit of text to data models based on formal logics (e.g. relational databases) <ref> [8] </ref>. Rather than requiring that users anticipate exactly the words and combinations of words that will appear in documents of interest, statistical IR approaches let users simply list words that are likely to appear in relevant documents.
Reference: [9] <author> David D. Lewis and Jason Catlett. </author> <title> Heterogeneous uncertainty sampling for supervised learning. </title> <editor> In William W. Cohen and Haym Hirsh, editors, </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 148-156, </pages> <address> San Francisco, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Curiously, highly computational methods have seen particularly little use. The author has been particularly interested in methods for actively selecting training data (ala statistical design of experiments) for supervised learning <ref> [9, 11] </ref>. Since vast quantities of text are now cheap, while human time is expensive, these methods are of considerable interest. In summary, the opportunities for and need of more statistical work in IR is as vast as the flood of online text engulfing the world!
Reference: [10] <author> David D. Lewis and W. Bruce Croft. </author> <title> Term clustering of syntactic phrases. </title> <booktitle> In Thirteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 385-404, </pages> <year> 1990. </year>
Reference-contexts: A variety of unsupervised learning methods have been applied to IR, with the hope of finding structure in large bodies of text that would improve on straightforward representations. These include clustering of words or documents <ref> [10, 20] </ref>, factor analytic decompositions of term by document matrices [1], and various term weighting methods [16]. Similarly, the retrieval query, routing profile, or category description provided by an IR system user is often far from ideal as well.
Reference: [11] <author> David D. Lewis and William A. Gale. </author> <title> A sequential algorithm for training text classifiers. </title> <editor> In W. Bruce Croft and C. J. van Rijsbergen, editors, </editor> <booktitle> SIGIR 94: Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 3-12, </pages> <address> London, 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Curiously, highly computational methods have seen particularly little use. The author has been particularly interested in methods for actively selecting training data (ala statistical design of experiments) for supervised learning <ref> [9, 11] </ref>. Since vast quantities of text are now cheap, while human time is expensive, these methods are of considerable interest. In summary, the opportunities for and need of more statistical work in IR is as vast as the flood of online text engulfing the world!
Reference: [12] <author> David D. Lewis and Philip J. Hayes. </author> <title> Guest editorial. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 12(3):231, </volume> <month> July </month> <year> 1994. </year>
Reference-contexts: Supervised learning is particularly effective in routing (where a user can supply ongoing feedback as the system is used) [7] and in text categorization (where a large body of manually indexed text may be available) <ref> [12, 14] </ref>. 2 The Future These are exciting times for IR.
Reference: [13] <author> David D. Lewis and Karen Sparck Jones. </author> <title> Natural language processing for information retrieval. </title> <journal> Communications of the ACM, </journal> <note> 1996. To appear. </note>
Reference-contexts: In IR systems documents are often represented as vectors of binary or numeric values corresponding directly or indirectly to the words of the document. Several properties of language, such as synonymy, ambiguity, and sheer variety make these representation far from ideal (but also hard to improve on <ref> [13] </ref>). A variety of unsupervised learning methods have been applied to IR, with the hope of finding structure in large bodies of text that would improve on straightforward representations.
Reference: [14] <author> David D. Lewis and Marc Ringuette. </author> <title> A comparison of two learning algorithms for text categorization. </title> <booktitle> In Third Annual Symposium on Document Analysis and Information Retrieval, </booktitle> <pages> pages 81-93, </pages> <address> Las Vegas, NV, </address> <month> April 11-13 </month> <year> 1994. </year> <institution> ISRI; Univ. of Nevada, </institution> <address> Las Vegas. </address>
Reference-contexts: Supervised learning is particularly effective in routing (where a user can supply ongoing feedback as the system is used) [7] and in text categorization (where a large body of manually indexed text may be available) <ref> [12, 14] </ref>. 2 The Future These are exciting times for IR.
Reference: [15] <author> Gerard Salton and Chris Buckley. </author> <title> Improving retrieval performance by relevance feedback. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 41(4) </volume> <pages> 288-297, </pages> <year> 1990. </year> <month> 5 </month>
Reference-contexts: Similarly, the retrieval query, routing profile, or category description provided by an IR system user is often far from ideal as well. Supervised learning techniques, where user feedback on relevant documents is used to improve the original user input, have been widely used <ref> [6, 15] </ref>. Both parametric and nonparametric (e.g. neural nets, decision trees, nearest neighbor classifiers) have been used.
Reference: [16] <author> Gerard Salton and Christopher Buckley. </author> <title> Term-weighting approaches in automatic text retrieval. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 24(5) </volume> <pages> 513-523, </pages> <year> 1988. </year>
Reference-contexts: These include clustering of words or documents [10, 20], factor analytic decompositions of term by document matrices [1], and various term weighting methods <ref> [16] </ref>. Similarly, the retrieval query, routing profile, or category description provided by an IR system user is often far from ideal as well. Supervised learning techniques, where user feedback on relevant documents is used to improve the original user input, have been widely used [6, 15].
Reference: [17] <author> Gerard Salton and Michael J. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill Book Company, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: A score is computed for each document based on the words it contains, and the highest scoring documents are retrieved, routed, categorized, etc. There are several variations on this approach <ref> [5, 17, 18, 19] </ref>. Vector space models treat the words suggested by the user as specifying an ideal relevant document in a high dimensional space. The distance of actual documents to this point is used as a measure of relevance.
Reference: [18] <author> H. R. Turtle and W. B. Croft. </author> <title> A comparison of text retrieval models. </title> <journal> The Computer Journal, </journal> <volume> 35(3) </volume> <pages> 279-290, </pages> <year> 1992. </year>
Reference-contexts: A score is computed for each document based on the words it contains, and the highest scoring documents are retrieved, routed, categorized, etc. There are several variations on this approach <ref> [5, 17, 18, 19] </ref>. Vector space models treat the words suggested by the user as specifying an ideal relevant document in a high dimensional space. The distance of actual documents to this point is used as a measure of relevance.
Reference: [19] <author> C. J. van Rijsbergen. </author> <title> Information Retrieval. </title> <publisher> Butterworths, </publisher> <address> London, </address> <note> second edition, </note> <year> 1979. </year>
Reference-contexts: A score is computed for each document based on the words it contains, and the highest scoring documents are retrieved, routed, categorized, etc. There are several variations on this approach <ref> [5, 17, 18, 19] </ref>. Vector space models treat the words suggested by the user as specifying an ideal relevant document in a high dimensional space. The distance of actual documents to this point is used as a measure of relevance.
Reference: [20] <author> Peter Willett. </author> <title> Recent trends in hierarchic document clustering: A critical review. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 24(5) </volume> <pages> 577-598, </pages> <year> 1988. </year>
Reference-contexts: A variety of unsupervised learning methods have been applied to IR, with the hope of finding structure in large bodies of text that would improve on straightforward representations. These include clustering of words or documents <ref> [10, 20] </ref>, factor analytic decompositions of term by document matrices [1], and various term weighting methods [16]. Similarly, the retrieval query, routing profile, or category description provided by an IR system user is often far from ideal as well.
References-found: 20

