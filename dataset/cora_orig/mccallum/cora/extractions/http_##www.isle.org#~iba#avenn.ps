URL: http://www.isle.org/~iba/avenn.ps
Refering-URL: 
Root-URL: 
Title: Average-Case Analysis of a Nearest Neighbor Algorithm  
Author: Pat Langley Wayne Iba 
Address: 755 College Road East Princeton, NJ 08540 USA  Moffett Field, CA 94035 USA  
Affiliation: Learning Systems Department Siemens Corporate Research  Recom Technologies AI Research Branch, Mail Stop 269-2 NASA Ames Research Center  
Note: To appear in Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (1993). Chambery, France.  
Abstract: In this paper we present an average-case analysis of the nearest neighbor algorithm, a simple induction method that has been studied by many researchers. Our analysis assumes a conjunctive target concept, noise-free Boolean attributes, and a uniform distribution over the instance space. We calculate the probability that the algorithm will encounter a test instance that is distance d from the prototype of the concept, along with the probability that the nearest stored training case is distance e from this test instance. From this we compute the probability of correct classification as a function of the number of observed training cases, the number of relevant attributes, and the number of irrelevant attributes. We also explore the behavioral implications of the analysis by presenting predicted learning curves for artificial domains, and give experimental results on these domains as a check on our reasoning. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W. </author> <year> (1990). </year> <title> A study of instance-based algorithms for supervised learning tasks: Mathematical, empirical, and psychological evaluations. </title> <type> Doctoral dissertation, </type> <institution> Department of Information & Computer Science, University of California, Irvine. </institution>
Reference-contexts: On the Cleveland data, the two algorithms' performance was indistinguishable, and IB1's behavior on the tumor and voting records nearly reached C4's level. Although the basic nearest neighbor algorithm fared much worse on the Hungarian data set, simple modifications produce accuracy comparable to that for C4 <ref> (Aha, 1990) </ref>, and its performance on the other domains argues that it deserves closer inspection in any case. In the remainder of this paper, we report the initial results of our average-case analysis of the simple nearest neighbor method.
Reference: <author> Aha, D. W., Kibler, D., & Albert, M. K. </author> <year> (1991). </year> <note> Instance-based learning algorithms. Machine Learning , 6 , 37-66. </note>
Reference: <author> Cover, T. M., & Hart, P. E. </author> <year> (1967). </year> <title> Nearest neighbor pattern classification. </title> <journal> IEEE Transactions on Information Theory, </journal> <pages> 13 , 21-27. </pages>
Reference: <author> Dasarathy, B. V. (Ed.). </author> <year> (1991). </year> <title> Nearest neighbor (NN) norms: NN pattern classification techniques. </title> <address> Los Ala-mitos, CA: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Fisher, D. H. </author> <year> (1987). </year> <title> Knowledge acquisition via incremental conceptual clustering. </title> <note> Machine Learning , 2 , 139-172. </note>
Reference-contexts: 1. Nearest Neighbor Algorithms Most learning methods form some abstraction from experience and store this structure in memory. The field has explored a wide range of such structures, including decision trees (Quinlan, 1986), multilayer networks (Rumelhart & McClelland, 1986), and probabilistic summaries <ref> (Fisher, 1987) </ref>. However, in recent years there has been growing interest in methods that store instances or cases in memory, and that apply this specific knowledge directly to new situations.
Reference: <author> Hirschberg, D. S., & Pazzani, M. J. </author> <year> (1991). </year> <title> Average-case analysis of a k-CNF learning algorithm (Technical Report 91-50). </title> <institution> Irvine: University of California, Department of Information & Computer Science. </institution>
Reference: <author> Iba, W., & Langley, P. </author> <year> (1992). </year> <title> Induction of one-level decision trees. </title> <booktitle> Proceedings of the Ninth International Conference on Machine Learning (pp. </booktitle> <pages> 233-240). </pages> <address> Ab-erdeen, Scotland: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Langley, P., Iba, W., & Thompson, K. </author> <year> (1992). </year> <title> An analysis of Bayesian classifiers. </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 223-228). </pages> <address> San Jose, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Pazzani, M. J., & Sarrett, W. </author> <year> (1992). </year> <title> A framework for the average case analysis of conjunctive learning algorithms. </title> <booktitle> Machine Learning, </booktitle> <pages> 9 , 349-372. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning, </booktitle> <pages> 1 , 81-106. </pages>
Reference-contexts: 1. Nearest Neighbor Algorithms Most learning methods form some abstraction from experience and store this structure in memory. The field has explored a wide range of such structures, including decision trees <ref> (Quinlan, 1986) </ref>, multilayer networks (Rumelhart & McClelland, 1986), and probabilistic summaries (Fisher, 1987). However, in recent years there has been growing interest in methods that store instances or cases in memory, and that apply this specific knowledge directly to new situations.
Reference: <author> Rumelhart, D. E., & McClelland, J. L. </author> <year> (1986). </year> <title> On learning the past tenses of English verbs. </title> <editor> In J. L. McClel-land & D. E. Rumelhart (Eds.), </editor> <booktitle> Parallel distributed processing: Explorations in the microstructure of cognition (Vol. 2). </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: 1. Nearest Neighbor Algorithms Most learning methods form some abstraction from experience and store this structure in memory. The field has explored a wide range of such structures, including decision trees (Quinlan, 1986), multilayer networks <ref> (Rumelhart & McClelland, 1986) </ref>, and probabilistic summaries (Fisher, 1987). However, in recent years there has been growing interest in methods that store instances or cases in memory, and that apply this specific knowledge directly to new situations.
Reference: <author> Stanfill, C., & Waltz, D. </author> <year> (1986). </year> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM , 29 , 1213-1228. </journal>
Reference: <author> Turney, P. </author> <title> (in press). Voting in instance-based learning: A theoretical analysis. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence. </journal>
References-found: 13

