URL: http://www.cs.umn.edu/Users/dept/users/riedl/CS-8103/Papers/Sharing/munin.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/riedl/CS-8103/Papers/Sharing/
Root-URL: http://www.cs.umn.edu
Title: Munin: Distributed Shared Memory Based on Type-Specific Memory Coherence  
Author: John K. Bennett John B. Carter flfl Willy Zwaenepoel flfl 
Address: Houston, Texas  
Affiliation: Department of Electrical and Computer Engineering flfl Department of Computer Science Rice University  
Abstract: We are developing Munin y , a system that allows programs written for shared memory multiprocessors to be executed efficiently on distributed memory machines. Thus, Munin overcomes the architectural limitations of shared memory machines, while maintaining their advantages in terms of ease of programming. A unique characteristic of Munin is the mechanism by which the shared memory programming model is translated to the distributed memory hardware. This translation is performed by runtime software, with the aid of semantic hints provided by the user. Each shared data object is supported by a memory coherence mechanism appropriate to the manner in which the object is accessed. This paper focuses on Munin's memory coherence mechanisms, and compares our approach to previous work in this area. This research was supported in part by the National Science Foundation under Grants CCR-8716914 and DCA-8619893 and by a National Science Foundation Fellowship. y In Norse mythology, the ravens Munin (Memory) and Hugin (Thought) perched on Odin's shoulder, and each evening they flew across the world to bring Odin knowledge of man's memories and thoughts. Thus, the raven Munin can be considered to have been the first distributed shared memory mechanism. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> John K. Bennett. </author> <title> The design and implementation of Distributed Smalltalk. </title> <booktitle> In Proceedings of the Second ACM Conference on Object-Oriented Programming Systems, Languages and Applications, </booktitle> <pages> pages 318-330, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: When multiple threads access a single synchronization object, these accesses must be ordered while allowing threads to get fair access. Munin supports distributed synchronization with distributed locks. More elaborate synchronization objects, such as monitors and atomic integers, are built on top of this. Our distributed locks employs proxy objects <ref> [1] </ref> to reduce network overhead. When a thread wants to acquire or test a global lock, it per-forms the lock operation on a local proxy for the distributed lock. Proxy objects are maintained by a collection of distributed lock servers, one per processor.
Reference: [2] <author> John K. Bennett, John B. Carter, and Willy Zwaenepoel. </author> <title> Shared memory access characteristics. </title> <type> Technical Report COMP TR89-99, </type> <institution> Rice University, </institution> <month> September </month> <year> 1989. </year> <note> Submitted to 1990 International Symposium on Computer Architecture. </note>
Reference-contexts: For this approach to work, a large percentage of shared data accesses must fall into a relatively small number of access type categories, that can be supported efficiently. A detailed study of the sharing behavior of parallel programs <ref> [2] </ref> supports this claim.
Reference: [3] <author> Brian N. Bershad, Edward D. Lazowska, and Henry M. Levy. </author> <title> PRESTO: A system for object-oriented parallel programming. </title> <journal> Software - Practice and Experience, </journal> <volume> 18(8) </volume> <pages> 713-732, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: We studied six shared memory parallel programs written in the C++ language [15] using the Presto program ming system <ref> [3] </ref> on the Sequent Symmetry shared memory multiprocessor [?]. <p> We are using the V kernel [6] to provide high-speed communi cation between the different processors, and we have chosen to support the Presto <ref> [3] </ref> parallel programming environment to develop our shared memory parallel programs. Presto is a shared memory parallel programming environment that provides parallelism (lightweight processes) and synchronization (locks and Mesa-style monitors) for the object-oriented language C++ [15].
Reference: [4] <author> Jeffrey S. Chase, Franz G. Amador, Edward D. Lazowska, Henry M. Levy, and Richard J. Lit-tlefield. </author> <title> The Amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: We contend that this approach provides a an abstraction of shared memory on a distributed memory machine, that is more efficient than can be achieved with a static coherence method. This use of type-specific coherence mechanisms is the primary distinction between Munin and Ivy [12], Clouds [14], and Amber <ref> [4] </ref>. For this approach to work, a large percentage of shared data accesses must fall into a relatively small number of access type categories, that can be supported efficiently. A detailed study of the sharing behavior of parallel programs [2] supports this claim. <p> Munin uses loose coherence to efficiently support multiple independent threads updating a single object, and also provides a general-purpose synchronization mechanism. Amber <ref> [4] </ref> uses an object model as a basis for providing a shared address space spanning multiple processors. It enforces strict coherence by always migrating threads to the objects that they access.
Reference: [5] <author> David R. Cheriton, Gert A. Slavenburg, and Patrick D. Boyle. </author> <title> Software-controlled caches in the VMP multiprocessor. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <month> December </month> <year> 1986. </year>
Reference-contexts: Cheriton et al. show that a software-controlled cache using a very large cache page size (an entire physical page) can provide the high performance needed to support fast multiprocessors <ref> [5] </ref>. This supports our claim that Munin, which is essentially a distributed caching mechanism provided in software, can efficiently provide a shared memory abstraction on a distributed system.
Reference: [6] <author> David R. Cheriton and Willy Zwaenepoel. </author> <title> The distributed V kernel and its performance for diskless workstations. </title> <booktitle> In Proceedings of the Ninth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 129-140, </pages> <month> October </month> <year> 1983. </year>
Reference-contexts: This implementation will allow us to assess the runtime costs of the delayed update queue and the other type-specific coherence mechanisms, as well as their benefits relative to standard static coherence mechanisms. We are using the V kernel <ref> [6] </ref> to provide high-speed communi cation between the different processors, and we have chosen to support the Presto [3] parallel programming environment to develop our shared memory parallel programs.
Reference: [7] <author> Ron Cytron, Steve Karlovsky, and Kevin P. McAuliffe. </author> <title> Automatic management of programmable caches. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Programming, </booktitle> <pages> pages 229-238, </pages> <month> August </month> <year> 1988. </year>
Reference: [8] <author> Michel Dubois, Christoph Scheurich, and Faye A. Briggs. </author> <title> Synchronization, coherence, and event ordering in multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 21(2) </volume> <pages> 9-21, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Essentially, strict coherence describes the implicit synchronization usually associated with message passing, and loose coherence describes the explicit synchronization normally associated with shared memory multiprocessors. Strict and loose coherence are closely related to the concepts of strong and weak ordering of events as described by Dubois et al. <ref> [8] </ref>. Programmers using Munin specify only a partial order on the reads and writes of shared data objects. As a result of their strict definition of coherence, Ivy and Clouds allow only one thread at a time to have write access to an object.
Reference: [9] <author> Susan J. Eggers and Randy H. Katz. </author> <title> A characterization of sharing in parallel programs and its application to coherency protocol evaluation. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 373-383, </pages> <month> May </month> <year> 1988. </year>
Reference: [10] <author> Susan J. Eggers and Randy H. Katz. </author> <title> The effect of sharing on the cache and bus performance of parallel programs. </title> <booktitle> In Proceedings of the 3rd International Conference on Architectural Support for Programming Languages and Systems, </booktitle> <pages> pages 257-270, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: If the remote copies are not going to be used, or if several updates are going to occur between uses, invalidation iss better. Previous distributed shared memory systems have assumed that only invalidation is appropriate, but again, each approach is preferable under different circumstances. Eggers and Katz <ref> [10] </ref> have shown that invalidation is preferable when the program exhibits a high degree of per-processor locality. Conversely, refresh is preferable when there is a high degree of fine-grained sharing. 4 Status and Directions We are currently implementing Munin on an Ethernet network of SUN workstations. <p> The designers of the Berkeley cache consistency protocol [11] found that their protocol can perform significantly better with a limited amount of information about how different data objects are accessed. Furthermore, Eggers and Katz <ref> [10] </ref> found that no single cache coherence protocol performed best for all types of shared data objects. 6 Conclusions We have described the motivation and memory coherence mechanisms of Munin, a distributed shared memory system that selects a memory coherence mechanism for each data object based on the manner in which
Reference: [11] <author> R. Katz, S. Eggers, D. Wood, C.L. Perkins, and R. Sheldon. </author> <title> Implementing a cache consistency protocol. </title> <booktitle> In Proceedings of the 12th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 276-283, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: It occurs when multiple threads are reading from and writing to the same data objects, and there is no particular pattern to the sharing that can be exploited. Munin handles general read-write objects using a mechanism based on the Berkeley Ownership cache consistency protocol <ref> [11] </ref>. By default, objects that are not recognized as some other specific type will be treated as general read-write. <p> Our use of type-specific cache coherence mechanisms is further supported by earlier studies of the performance of snooping caches for parallel programs on shared memory multiprocessors. The designers of the Berkeley cache consistency protocol <ref> [11] </ref> found that their protocol can perform significantly better with a limited amount of information about how different data objects are accessed.
Reference: [12] <author> Kai Li. </author> <title> Shared Virtual Memory on Loosely Coupled Multiprocessors. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> September </month> <year> 1986. </year>
Reference-contexts: We contend that this approach provides a an abstraction of shared memory on a distributed memory machine, that is more efficient than can be achieved with a static coherence method. This use of type-specific coherence mechanisms is the primary distinction between Munin and Ivy <ref> [12] </ref>, Clouds [14], and Amber [4]. For this approach to work, a large percentage of shared data accesses must fall into a relatively small number of access type categories, that can be supported efficiently. A detailed study of the sharing behavior of parallel programs [2] supports this claim. <p> This contrasts with the more common definition used in Ivy <ref> [12] </ref> and Clouds [14]: Memory is strictly coherent if the value returned by a read operation is the value written by the most recent write operation to the same object. two definitions of coherence. <p> Performance on hardware with different performance characteristics, such as higher network bandwidth or increased processor speed, retains our active interest. Finally, the provision of fault tolerance and support for heterogeneity might be required in an operational system. 5 Related Work The Ivy system <ref> [12] </ref> provides shared memory on a collection of Apollo workstations using a distributed memory manager. Ivy's shared virtual memory provides a virtual address space that is shared among all the processors in the system. Global virtual memory is divided into pages corresponding to physical pages.
Reference: [13] <author> Susan Owicki and Anant Agarwal. </author> <title> Evaluating the performance of software cache coherence. </title> <booktitle> In Proceedings of the 3rd International Conference on Architectural Support for Programming Languages and Systems, </booktitle> <pages> pages 230-242, </pages> <month> May </month> <year> 1989. </year>
Reference: [14] <author> Umakishore Ramachandran, Mustaque Ahamad, and M. Yousef A. Khalidi. </author> <title> Unifying synchronization and data transfer in maintaining coherence of distributed shared memory. </title> <type> Technical Report GIT-CS-88/23, </type> <institution> Georgia Institute of Technology, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: We contend that this approach provides a an abstraction of shared memory on a distributed memory machine, that is more efficient than can be achieved with a static coherence method. This use of type-specific coherence mechanisms is the primary distinction between Munin and Ivy [12], Clouds <ref> [14] </ref>, and Amber [4]. For this approach to work, a large percentage of shared data accesses must fall into a relatively small number of access type categories, that can be supported efficiently. A detailed study of the sharing behavior of parallel programs [2] supports this claim. <p> This contrasts with the more common definition used in Ivy [12] and Clouds <ref> [14] </ref>: Memory is strictly coherent if the value returned by a read operation is the value written by the most recent write operation to the same object. two definitions of coherence. <p> The Clouds distributed operating system was extended to provide a form of shared memory <ref> [14] </ref>. The distributed shared memory controller allows objects to be mapped into the address space of any thread (process). Shared memory is divided into logical segments corresponding to Clouds objects, reducing the potential for false sharing.
Reference: [15] <author> Bjarne Stroustrup. </author> <title> The C++ Programming Language. </title> <publisher> Addison-Wesley, </publisher> <year> 1987. </year>
Reference-contexts: We studied six shared memory parallel programs written in the C++ language <ref> [15] </ref> using the Presto program ming system [3] on the Sequent Symmetry shared memory multiprocessor [?]. <p> Presto is a shared memory parallel programming environment that provides parallelism (lightweight processes) and synchronization (locks and Mesa-style monitors) for the object-oriented language C++ <ref> [15] </ref>. Programmers write their programs using a shared memory model, inserting declarations to provide object-specific information to the Munin runtime system. These declarations are processed by the compiler, and allow the runtime system to select the appropriate coherence mechanism for each object.
Reference: [16] <author> Wolf-Dietrich Weber and Anoop Gupta. </author> <title> Analysis of cache invalidation patterns in multiprocessors. </title> <booktitle> In Proceedings of the 3rd International Conference on Architectural Support for Programming Languages and Systems, </booktitle> <pages> pages 243-256, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: We use delayed updates to combine data motion and synchronization in Munin. 3.3.3 Migratory Objects Migratory objects <ref> [16] </ref> are accessed by a single processor at a time, as would be the case with an object accessed within a critical section of code. Migratory objects can be handled efficiently by integrating their movement with that of the lock associated with their critical section.
References-found: 16

