URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1994/umsi-94-101.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1994/
Root-URL: http://www.cs.umn.edu
Title: Approximate Inverse Preconditioners for General Sparse Matrices  
Author: Edmond Chow and Yousef Saad 
Keyword: Key words. approximate inverse, preconditioning, Krylov subspace methods, threshold dropping strategies  
Note: AMS(MOS) subject classifications. 65F10, 65F35, 65F50, 65Y05 Work supported in part by the National Science Foundation under grant NSF/CCR-9214116 and in part by a grant from NASA.  
Date: May 1994  
Address: Minneapolis, MN 55455  
Affiliation: Department of Computer Science, and Minnesota Supercomputer Institute University of Minnesota  
Abstract: The standard Incomplete LU (ILU) preconditioners often fail for general sparse indefinite matrices because they give rise to `unstable' factors L and U. In such cases, it may be attractive to approximate the inverse of the matrix directly. This paper focuses on approximate inverse preconditioners based on minimizing kIAM k F , where AM is the preconditioned matrix. An iterative descent-type method is used to approximate each column of the inverse. For this approach to be efficient, the iteration must be done in sparse mode, i.e., with `sparse-matrix by sparse-vector' operations. Numerical dropping is applied to each column to maintain sparsity in the approximate inverse. Compared to previous methods, this is a natural way to determine the sparsity pattern of the approximate inverse. This paper discusses options such as Newton and `global' iteration, self-preconditioning, dropping strategies, and factorized forms. The performance of the options are compared on standard problems from the Harwell-Boeing collection. Theoretical results on general approximate inverses and the convergence behavior of the algorithms are derived. Finally, some ideas and experiments with practical variations and applications are presented. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> O. Axelsson. </author> <title> Incomplete block matrix factorization preconditioning methods. </title> <journal> The ultimate answer? J. Comput. Appl. Math., </journal> <volume> 12 & 13 (1985), </volume> <pages> pp. 3-18. </pages>
Reference-contexts: Approximate Inverse Preconditioners 27 5.1 Incomplete block tridiagonal factorizations There has been much work in the area of two-level implicit-explicit preconditioners, especially incomplete block factorizations <ref> [1, 2, 3, 4, 7, 17, 20] </ref> for matrices arising from the discretization of PDE's. These methods utilize approximate inverses when diagonal blocks need to be inverted. This application of approximate inverses, however, is much more difficult for general sparse matrices. <p> Thus, the largest singular values are mapped close to unity whereas the smallest ones are roughly divided by ff. For example, we can say that all i 's that are greater than or equal to ff will be mapped into the interval <ref> [1=2; 1] </ref>. Similarly, all i 's satisfying i 1 ff Approximate Inverse Preconditioners 31 will be mapped into the interval [ff; 1]. We show the results of a few numerical experiments with the WEST0067 matrix. <p> For example, we can say that all i 's that are greater than or equal to ff will be mapped into the interval [1=2; 1]. Similarly, all i 's satisfying i 1 ff Approximate Inverse Preconditioners 31 will be mapped into the interval <ref> [ff; 1] </ref>. We show the results of a few numerical experiments with the WEST0067 matrix. One inner iteration was used, with a scaled identity initial guess to approximate the inverse of A T A + ffI. Table 8 shows the results with no dropping.
Reference: [2] <author> O. Axelsson. </author> <title> Iterative Solution Methods. </title> <address> Cambridge, Cambridge, </address> <year> 1994. </year>
Reference-contexts: Approximate Inverse Preconditioners 27 5.1 Incomplete block tridiagonal factorizations There has been much work in the area of two-level implicit-explicit preconditioners, especially incomplete block factorizations <ref> [1, 2, 3, 4, 7, 17, 20] </ref> for matrices arising from the discretization of PDE's. These methods utilize approximate inverses when diagonal blocks need to be inverted. This application of approximate inverses, however, is much more difficult for general sparse matrices.
Reference: [3] <author> O. Axelsson, S. Brinkkemper and V. P. Il'in. </author> <title> On some versions of incomplete block-matrix factorization iterative methods. </title> <journal> Lin. Alg. Appl., </journal> <volume> 58 (1984), </volume> <pages> pp. 3-15. </pages>
Reference-contexts: Approximate Inverse Preconditioners 27 5.1 Incomplete block tridiagonal factorizations There has been much work in the area of two-level implicit-explicit preconditioners, especially incomplete block factorizations <ref> [1, 2, 3, 4, 7, 17, 20] </ref> for matrices arising from the discretization of PDE's. These methods utilize approximate inverses when diagonal blocks need to be inverted. This application of approximate inverses, however, is much more difficult for general sparse matrices.
Reference: [4] <author> O. Axelsson and B. Polman. </author> <title> On approximate factorization methods for block matrices suitable for vector and parallel processors. </title> <journal> Lin. Alg. Appl., </journal> <volume> 77 (1986), </volume> <pages> pp. 3-26. </pages>
Reference-contexts: Approximate Inverse Preconditioners 27 5.1 Incomplete block tridiagonal factorizations There has been much work in the area of two-level implicit-explicit preconditioners, especially incomplete block factorizations <ref> [1, 2, 3, 4, 7, 17, 20] </ref> for matrices arising from the discretization of PDE's. These methods utilize approximate inverses when diagonal blocks need to be inverted. This application of approximate inverses, however, is much more difficult for general sparse matrices.
Reference: [5] <author> M. W. Benson. </author> <title> Iterative solution of large scale linear systems. M.Sc. </title> <type> Thesis (1973), </type> <institution> Lakehead University, Thunder Bay, </institution> <address> Ontario. </address>
Reference-contexts: In this paper, we focus on methods of finding approximate inverses based on minimizing the Frobenius norm of the residual matrix I AM , first suggested by Benson and Frederickson <ref> [5, 6] </ref>. We look for a right approximate inverse so that practical variations that use flexible right preconditioning may be used, although left approximate inverses may be better for certain matrices depending on the structure of the inverse.
Reference: [6] <author> M. W. Benson and P. O. Frederickson. </author> <title> Iterative solution of large sparse linear systems arising in certain multidimensional approximation problems. </title> <journal> Utilitas Math., </journal> <volume> 22 (1982), </volume> <pages> pp. 127-140. </pages>
Reference-contexts: In this paper, we focus on methods of finding approximate inverses based on minimizing the Frobenius norm of the residual matrix I AM , first suggested by Benson and Frederickson <ref> [5, 6] </ref>. We look for a right approximate inverse so that practical variations that use flexible right preconditioning may be used, although left approximate inverses may be better for certain matrices depending on the structure of the inverse.
Reference: [7] <author> P. Concus, G. H. Golub and G. Meurant. </author> <title> Block preconditioning for the conjugate gradient method. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 6 (1985), </volume> <pages> pp. 309-332. </pages>
Reference-contexts: Approximate Inverse Preconditioners 27 5.1 Incomplete block tridiagonal factorizations There has been much work in the area of two-level implicit-explicit preconditioners, especially incomplete block factorizations <ref> [1, 2, 3, 4, 7, 17, 20] </ref> for matrices arising from the discretization of PDE's. These methods utilize approximate inverses when diagonal blocks need to be inverted. This application of approximate inverses, however, is much more difficult for general sparse matrices.
Reference: [8] <author> J. D. F. Cosgrove, J. C. Daz and A. Griewank. </author> <title> Approximate inverse preconditioning for sparse linear systems. </title> <journal> Intl. J. Comp. Math., </journal> <volume> 44 (1992), </volume> <pages> pp. 91-110. </pages>
Reference-contexts: Grote and Simon [15] choose M to be a banded matrix with 2p+1 diagonals, p 0, emphasizing the importance of the fast application of the preconditioner in a CM-2 implementation. This choice of structure is particularly suitable for banded matrices. Cosgrove, Daz and Griewank <ref> [8] </ref> select the initial structure of M to be diagonal and then use a procedure to improve the minimum by updating the sparsity pattern of M . <p> It may well be the case that in order to guarantee nonsingularity, we must have an M that is dense, or nearly dense. In fact, in the particular case where the norm in the proposition is the 1-norm, it has been proved by Cosgrove, Daz and Griewank <ref> [8] </ref> that the approximate inverse may be structurally dense, in that it is always possible to find a sparse matrix A for which M will be dense if kI AM k 1 &lt; 1.
Reference: [9] <author> I. S. Duff, A. M. Erisman and J. K. Reid. </author> <title> Direct Methods for Sparse Matrices. </title> <publisher> Oxford, </publisher> <address> London, </address> <year> 1989. </year>
Reference-contexts: The matrix-vector product is much more efficient if the sparse matrix is stored by columns since all the entries do not need to be traversed. Efficient codes for all these kernels may be constructed which utilize a full n-length work vector <ref> [9] </ref>. Columns from an initial guess M 0 for the approximate inverse are used as the initial guesses for the iterative solution of the linear subproblems. There are two obvious choices: M 0 = ffI and M 0 = ffA T .
Reference: [10] <author> I. S. Duff, R. G. Grimes and J. G. Lewis. </author> <title> Users' guide for the Harwell-Boeing sparse matrix collection. </title> <institution> TR/PA/92/86, CERFACS, Toulouse, </institution> <year> 1992. </year>
Reference-contexts: In the presence of numerical dropping, the proposition does not hold. 4 Numerical experiments and observations Experiments with the algorithms and options described in Section 2 were performed with matrices from the Harwell-Boeing sparse matrix collection <ref> [10] </ref>, and a few matrices from computational fluid dynamics extracted from solving the Navier-Stokes equations with the FIDAP [13] package. The matrices were pre-scaled so that the 2-norm of each column is unity, as described in Section 2.5.
Reference: [11] <author> T. Huckle and M. Grote. </author> <title> A new approach to parallel preconditioning with sparse approximate inverses. </title> <type> Manuscript SCCM-94-03, </type> <institution> Scientific Computing and Computational Mathematics Program, Stanford University, Stanford, California, </institution> <year> 1994. </year>
Reference-contexts: New fill-in elements are chosen so that the fill-in contributes a certain improvement while minimizing the number of new rows in the least squares subproblem. In similar and recent work by Huckle and Grote <ref> [11] </ref>, the reduction in the residual norm is tested for each candidate fill-in element, but fill-in may be introduced more than one at a time. <p> We can exploit the fact that since we are computing a sparse approximation, the number p of nonzero elements in each column is small, and thus we replace the scalar n in the above inequalities by p <ref> [11] </ref>. We should point out that the result does not tell us anything about the degree of sparsity of the resulting approximate inverse M . It may well be the case that in order to guarantee nonsingularity, we must have an M that is dense, or nearly dense.
Reference: [12] <author> H. C. Elman. </author> <title> A stability analysis of incomplete LU factorizations. </title> <journal> Math. Comp., </journal> <volume> 47 (1986), </volume> <pages> pp. 191-217. </pages>
Reference-contexts: This form of instability has been studied by Elman <ref> [12] </ref> who proposed a detailed analysis of ILU and MILU preconditioners. It can be observed experimentally that ILU preconditioners can be very poor when L 1 or U 1 are large, and that this situation often occurs for indefinite problems, or problems with large nonsymmetric parts.
Reference: [13] <author> M. Engleman. FIDAP: </author> <title> Examples Manual, Revision 6.0. </title> <booktitle> Fluid Dynamics International, </booktitle> <address> Evanston, Illinois, </address> <year> 1991. </year>
Reference-contexts: numerical dropping, the proposition does not hold. 4 Numerical experiments and observations Experiments with the algorithms and options described in Section 2 were performed with matrices from the Harwell-Boeing sparse matrix collection [10], and a few matrices from computational fluid dynamics extracted from solving the Navier-Stokes equations with the FIDAP <ref> [13] </ref> package. The matrices were pre-scaled so that the 2-norm of each column is unity, as described in Section 2.5. In each experiment, we report the number of GM-RES (20) steps to reduce the initial residual of the right-preconditioned linear system by 10 5 .
Reference: [14] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations, 2nd edition. </title> <publisher> John Hopkins, </publisher> <address> Baltimore, </address> <year> 1989. </year> <title> Approximate Inverse Preconditioners 34 </title>
Reference-contexts: Proof. To prove the first property we invoke Gershgorin's theorem on the matrix AM = I R each column of R is the residual vector r :;j = e j Am j . The column version of Gershgorin's theorem, see e.g., <ref> [26, 14] </ref>, asserts that all the eigenvalues of the matrix I R are located in the union of the disks centered at the diagonal elements 1 r jj and with radius n X jr ij j: In other words, each eigenvalue must satisfy at least one inequality of the form j
Reference: [15] <author> M. Grote and H. D. Simon. </author> <title> Parallel preconditioning and approximate inverses on the Connection Machine. </title> <editor> In R. F. Sincovec, D. E. Keyes, L. R. Petzold and D. A. Reed, eds., </editor> <booktitle> Parallel Processing for Scientific Computing, </booktitle> <volume> vol. 2, </volume> <pages> pp. 519-523. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Pennsylvania, </address> <year> 1993. </year>
Reference-contexts: It also gives rise to a number of different options to choose from. Up to now, the minimization in (5) has been performed directly by prescribing a sparsity pattern for M and solving the resulting least squares problems. Grote and Simon <ref> [15] </ref> choose M to be a banded matrix with 2p+1 diagonals, p 0, emphasizing the importance of the fast application of the preconditioner in a CM-2 implementation. This choice of structure is particularly suitable for banded matrices.
Reference: [16] <author> A. S. </author> <title> Householder. The Theory of Matrices in Numerical Analysis. </title> <publisher> Dover, </publisher> <address> New York, </address> <year> 1964. </year>
Reference-contexts: In the remaining sections, we consider this latter approach and the various options that are available. 2.1 Newton iteration As an alternative to directly minimizing the objective function (3), an approximate inverse may also be computed using an iterative process known as the method of Hotelling and Bodewig <ref> [16] </ref>. This method, which is modeled after Newton's method for solving f (x) 1=x a = 0, has many similarities to our descent methods which we describe later.
Reference: [17] <author> L. Yu. Kolotilina and A. Yu. Yeremin. </author> <title> On a family of two-level preconditionings of the incomplete block factorization type, </title> <journal> Soviet J. Numer. Anal. Math. Model., </journal> <volume> 1 (1986), </volume> <pages> pp. 293-320. </pages>
Reference-contexts: Approximate Inverse Preconditioners 27 5.1 Incomplete block tridiagonal factorizations There has been much work in the area of two-level implicit-explicit preconditioners, especially incomplete block factorizations <ref> [1, 2, 3, 4, 7, 17, 20] </ref> for matrices arising from the discretization of PDE's. These methods utilize approximate inverses when diagonal blocks need to be inverted. This application of approximate inverses, however, is much more difficult for general sparse matrices.
Reference: [18] <author> L. Yu. Kolotilina and A. Yu. Yeremin. </author> <title> Factorized sparse approximate inverse precon-ditionings I. </title> <journal> Theory. SIAM J. Matrix Anal. Appl., </journal> <volume> 14 (1993), </volume> <pages> pp. 45-58. </pages>
Reference-contexts: In similar and recent work by Huckle and Grote [11], the reduction in the residual norm is tested for each candidate fill-in element, but fill-in may be introduced more than one at a time. In other related work, Kolotilina and Yeremin <ref> [18] </ref> consider symmetric, positive definite systems and construct factorized sparse approximate inverse preconditioners which are also symmetric, positive definite. The preconditioner has the form M = G T L G L where G L is lower triangular, and thus A is preconditioned to become G L AG T L . <p> This latter form is reminiscent of the symmetric form of Kolotilina and Yeremin <ref> [18] </ref>. However, we only did experiments with the first factorized form (12), which we consider from here on.
Reference: [19] <author> L. Yu. Kolotilina and A. Yu. Yeremin. </author> <title> Factorized sparse approximate inverse precon-ditionings II. Solution of 3D FE systems on massively parallel computers. </title> <note> Research Report EM-RR 3/92, </note> <institution> Elegant Mathematics, Inc. </institution> <address> Bothell, Washington, </address> <year> 1992. </year>
Reference-contexts: The structure of G L is chosen to be the same as the structure of the lower triangular part of A. In their more recent work <ref> [19] </ref>, fill-in elements may be added, but are chosen in a way so that constructing G L is not much more expensive, with the simple hope that the additional degrees of freedom will improve the preconditioner quality.
Reference: [20] <author> L. Yu. Kolotilina and A. Yu. Yeremin. </author> <title> Incomplete block factorizations as precondi-tioners for sparse SPD matrices. </title> <note> Research Report EM-RR 6/92, </note> <institution> Elegant Mathematics, Inc. </institution> <address> Bothell, Washington, </address> <year> 1992. </year>
Reference-contexts: Approximate Inverse Preconditioners 27 5.1 Incomplete block tridiagonal factorizations There has been much work in the area of two-level implicit-explicit preconditioners, especially incomplete block factorizations <ref> [1, 2, 3, 4, 7, 17, 20] </ref> for matrices arising from the discretization of PDE's. These methods utilize approximate inverses when diagonal blocks need to be inverted. This application of approximate inverses, however, is much more difficult for general sparse matrices.
Reference: [21] <author> H. Manouzi. </author> <type> Personal communication, </type> <institution> University of Laval, Quebec, </institution> <year> 1993. </year>
Reference-contexts: As the iterations progress, M becomes denser and denser, and a natural idea here is to perform the above iteration in sparse mode <ref> [21] </ref>, i.e., drop some elements in M or else the iterations become too expensive. In this case, however, the convergence properties of the Newton iteration are lost.
Reference: [22] <author> V. Pan and J. Reif. </author> <title> Efficient parallel solution of linear systems. </title> <booktitle> Proc. 17th Annual ACM Symposium on Theory of Computing (1985), </booktitle> <pages> pp. 143-152. </pages>
Reference-contexts: convergence, we require that the spectral radius of I AM 0 be less than one, and if we choose an initial guess of the form M 0 = ffA T then convergence is achieved if 0 &lt; ff &lt; (AA T ) In practice, we can follow Pan and Reif <ref> [22] </ref> and use ff = kAA T k 1 for the right approximate inverse.
Reference: [23] <author> V. Pan and R. Schreiber. </author> <title> An improved Newton iteration for the generalized inverse of a matrix, with applications. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 12 (1991), </volume> <pages> pp. 1109-1130. </pages>
Reference-contexts: In both Newton and global iterations, two sparse matrix-matrix products are required in each iteration, although the convergence rate of Newton iteration may be doubled with a form of Chebyshev acceleration <ref> [23] </ref>. The cost is thus comparable to the cost of the column-oriented, self-preconditioned MR algorithm, where 2n sparse matrix-sparse vector products are used per iteration, or simply n products if self-preconditioning is not used.
Reference: [24] <author> Y. Saad. ILUT: </author> <title> A dual threshold incomplete LU factorization. </title> <note> SIAM J. Sci. Comput., to appear. Also, Technical Report UMSI 92/38, </note> <institution> Minnesota Supercomputer Institute, University of Minnesota, Minneapolis, Minnesota, </institution> <year> 1992. </year>
Reference-contexts: To address issue 3, we utilize a dual threshold strategy based a drop tolerance, drop-tol, and the maximum number of elements per column, lfil, similar to that used in the ILUT preconditioner <ref> [24] </ref>. By limiting the maximum number of elements per column, the maximum storage for the preconditioner is known beforehand. The drop tolerance may be applied directly to the elements to be dropped: i.e., elements are dropped if their magnitude is smaller than droptol. <p> Next we show some results on matrices from computational fluid dynamics extracted using the FIDAP package. These are relatively dense matrices that are nonsymmetric, but have symmetric structure. We chose matrices that we could not solve using an incomplete LU preconditioner called ILUT <ref> [24] </ref> using an equivalent lfil of about 100. Our experience also showed that ILU preconditioners on these matrices produce unstable L and U factors in (2). The matrices are from 2-dimensional finite element discretizations using quadrilateral elements with 9 nodes.
Reference: [25] <author> Y. Saad. </author> <title> A flexible inner-outer preconditioned GMRES algorithm. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 14 (1993), </volume> <pages> pp. 461-469. </pages>
Reference-contexts: Algorithm 2.3 implements the Minimal Residual iteration with self-preconditioning. In the algorithm, n o outer iterations and n i inner iterations are used. Again, M = M 0 initially. In the GMRES version of the algorithm, a variant called FGMRES <ref> [25] </ref> that allows a flexible preconditioner is actually used. Algorithm 2.3 Self-preconditioned Minimal Residual iteration 1. Start: M = M 0 2. For outer = 1; 2; : : : ; n o do 3. For each column j = 1; : : : ; n do 4.
Reference: [26] <author> Y. Saad. </author> <title> Numerical Methods for Large Eigenvalue Problems. </title> <publisher> Halstead Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: Proof. To prove the first property we invoke Gershgorin's theorem on the matrix AM = I R each column of R is the residual vector r :;j = e j Am j . The column version of Gershgorin's theorem, see e.g., <ref> [26, 14] </ref>, asserts that all the eigenvalues of the matrix I R are located in the union of the disks centered at the diagonal elements 1 r jj and with radius n X jr ij j: In other words, each eigenvalue must satisfy at least one inequality of the form j
References-found: 26

