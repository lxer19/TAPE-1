URL: ftp://ftp.cnl.salk.edu/pub/wiskott/publications/Wis99b-SegmFromMotion-PattRec.ps.gz
Refering-URL: http://www.cnl.salk.edu/~wiskott/Abstracts/Wis99b.html
Root-URL: http://www.cnl.salk.edu/~wiskott/Abstracts/Wis99b.html
Title: Pattern Recognition  Segmentation from Motion: Combining Gabor- and Mallat-Wavelets to Overcome Aperture and Correspondence Problem.  
Author: Laurenz Wiskott 
Keyword: segmentation from motion, Gabor-wavelet transform, Mallat-transform, integration, motion hypotheses.  
Web: http://www.neuroinformatik.ruhr-uni-bochum.de  
Address: Ruhr-Universitat Bochum D-44780 Bochum, Germany  
Affiliation: Institut fur Neuroinformatik  
Note: (to appear). This is a draft version! The final version will be available on the web one year after publication.  
Abstract: A segmentation-from-motion algorithm is presented, which is designed to be part of a general object recognition system. The key idea is to integrate information from Gabor- and Mallat-wavelet transform to overcome the aperture and the correspondence problem. The assumption is made that objects move fronto-parallel. Gabor-wavelet responses allow precise estimation of image flow vectors with low spatial resolution. A histogram over this image flow field is evaluated, its local maxima providing motion hypotheses. These serve to reduce the correspondence problem on the Mallat-wavelet transform, which provides the required high resolution. The segmentation reliability is improved by integration over time. The system can segment several small, disconnected, and openworked objects, such as dot patterns. Several examples demonstrate the performance of the system and show, that the algorithm behaves reasonably even if the assumption of fronto-parallel motion is strongly violated. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anandan, P. </author> <year> (1989). </year> <title> A computational framework and an algorithm for the measurement of visual motion. </title>
Reference-contexts: This can be done by simple averaging or by more sophisticated methods, such as robust estimation with outlier detection (e.g. Darrell & Pentland, 1995; Huang et al., 1995). Integration has been applied to space (e.g. Horn & Schunck, 1981), scale <ref> (e.g. Anandan, 1989) </ref>, and time (e.g. Morikawa & Harashima, 1993). Secondly, there exist some examples where different segmentation cues, such as color and motion, are integrated (e.g. Poggio et al., 1988; Dubuisson & Jain, 1995) or where segmentation cues are combined with object recognition (e.g.
Reference: <editor> Int'l J. </editor> <booktitle> of Computer Vision, </booktitle> <volume> 2(3) </volume> <pages> 283-310. </pages>
Reference: <author> Bergen, J. R., Burt, P. J., Hingorani, R., and Peleg, S. </author> <year> (1992). </year> <title> A three-frame algorithm for estimating two-component image motion. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 14(9) </volume> <pages> 886-895. </pages>
Reference-contexts: Black & Anandan, 1996; Chang et al., 1997). These systems provide a full field segmentation, possibly suppressing regions with insufficient evidence for motion. An interesting variation is <ref> (Bergen et al., 1992) </ref>; this system does not segment the image into distinct regions but separates two overlaying components as it may occur for transparent objects. Another common class of segmentation algorithms is based on matching feature points, such as corners or interest points (e.g.
Reference: <author> Black, M. J. and Anandan, P. </author> <year> (1996). </year> <title> The robust estimation of multiple motions: Parametric and piecewise-smooth flow fields. </title> <booktitle> Computer Vision and Image Understanding, </booktitle> <volume> 63(1) </volume> <pages> 75-7104. </pages>
Reference: <author> B orjesson, E. and Ahlstr om, U. </author> <year> (1993). </year> <title> Motion structure in five-dot patterns as a determinant of perceptual grouping. </title> <journal> Perception and Psychophysics, </journal> <volume> 53(1) </volume> <pages> 2-12. </pages>
Reference-contexts: Simple motion model: While most systems use affine, 3D-rigid, or similar motion models, I adopt here the simplest motion model, the fronto-parallel translation. Psychophysical experiments provide evidence that this is the dominating motion model in human subjects <ref> (B orjesson & Ahlstr om, 1993) </ref> and that humans perceive moving patterns not necessarily according to a 3D-rigid motion model (Ra machandran et al., 1988). No restrictive object model: In contrast to many other approaches, no spatial regularization is applied.
Reference: <author> Bouthemy, P. and Francois, E. </author> <year> (1993). </year> <title> Motion segmentation and qualitative dynamic scene analysis from an image sequence. </title> <journal> Int'l J. of Computer Vision, </journal> <volume> 10(2) </volume> <pages> 157-182. </pages>
Reference: <author> Burr, D. C., Morrone, M. C., and Spinelli, D. </author> <year> (1989). </year> <title> Evidence for edge and bar detectors in human vision. </title> <journal> Vision Research, </journal> <volume> 29(4) </volume> <pages> 419-431. </pages>
Reference-contexts: Biologically plausible preprocessing: Beside their technical advantages, Gabor- and Mallat-wavelet transform are also biological plausible, as shown by physiological (Jones & Palmer, 1987; DeValois & DeValois, 1988) as well as psychophysical experiments <ref> (Burr et al., 1989) </ref>. Simple motion model: While most systems use affine, 3D-rigid, or similar motion models, I adopt here the simplest motion model, the fronto-parallel translation.
Reference: <author> Cappellini, V., Mecocci, A., and Bimbo, A. D. </author> <year> (1993). </year> <title> Motion analysis and representation in computer vision. </title> <journal> J. of Circuits, Systems and Computers, </journal> <volume> 3(4) </volume> <pages> 797-831. </pages>
Reference: <author> Chang, M. M., Tekalp, A. M., and Sezan, M. I. </author> <year> (1997). </year> <title> Simultaneous motion estimation and segmentation. </title> <journal> IEEE Trans. on Image Processing, </journal> <volume> 6(9) </volume> <pages> 1326-1333. </pages>
Reference: <author> Chen, H.-J., Shirai, Y., and Asada, M. </author> <year> (1993). </year> <title> Detecting multiple rigid image motions from an optical flow field obtained with multi-scale, multi-orientation filters. </title> <journal> IEICE Trans. on Information and Systems, E76-D(10):1253-1262. </journal>
Reference-contexts: However, due to the averaging in the motion histogram no such elaborated procedure seems necessary. See <ref> (Chen et al., 1993) </ref> for a more explicit treatment of these kinds of special cases. 2.1.2 Motion Hypotheses The second stage of the algorithm is concerned with the extraction of motion hypotheses from the image flow field.
Reference: <author> Cumani, A., Guiducci, A., and Grattoni, P. </author> <year> (1991). </year> <title> Image description of dynamic scenes. </title> <journal> Pattern Recognition, </journal> <volume> 24(7) </volume> <pages> 661-673. </pages>
Reference: <author> Darrell, T. and Pentland, A. P. </author> <year> (1995). </year> <title> Cooperative robust estimation using layers of support. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 17(5) </volume> <pages> 474-487. </pages>
Reference-contexts: Matching of edges, however, seems to be used rarely only (e.g. Lawton, 1983; Cumani et al., 1991). The accordance maps and the representation of the segmentation results are somewhat analogous to the layers used in (Wang & Adelson, 1994) and <ref> (Darrell & Pentland, 1995) </ref>, though there are significant differences. Temporal integration has also been used before, though it is usually done by assuming continuous motion (e.g. Bouthemy & Francois, 1993; Smith & Brady, 1995).
Reference: <author> Daugman, J. G. </author> <year> (1988). </year> <title> Complete discrete 2-D Gabor transform by neural networks for image analysis and compression. </title> <journal> IEEE Trans. on Acoustics, Speech and Signal Processing, </journal> <volume> 36(7) </volume> <pages> 1169-1179. </pages>
Reference-contexts: Gabor wavelets are technically interesting for image representation <ref> (Daugman, 1988) </ref> and object recognition (Lades et al., 1993). They are also biologically plausible, because they have similar shape as the receptive fields of simple cells found in the visual cortex of vertebrate animals (Jones & Palmer, 1987; DeValois & DeValois, 1988).
Reference: <author> DeValois, R. L. and DeValois, K. K. </author> <year> (1988). </year> <title> Spatial Vision. </title> <publisher> Oxford Press. </publisher>
Reference: <author> Dubuisson, M.-P. and Jain, A. K. </author> <year> (1995). </year> <title> Contour extraction of moving objects in complex outdoor scenes. </title> <journal> Int'l J. of Computer Vision, </journal> <volume> 14(1) </volume> <pages> 83-105. </pages>
Reference: <author> Eckes, C. and Vorbr uggen, J. C. </author> <year> (1996). </year> <title> Combining data-driven and model-based cues for segmentation of video sequences. </title> <booktitle> In Proc. World Congress on Neural Networks, </booktitle> <pages> pages 868-875, </pages> <address> San Diego, CA. </address> <booktitle> Int'l Neural Network Soc., </booktitle> <publisher> Lawrence Erlbaum Assoc. Inc. </publisher> <address> Mahwah, NJ. </address> <note> 18 Fennema, </note> <author> C. L. and Thompson, W. B. </author> <year> (1979). </year> <title> Velocity determination in scenes containing several moving objects. </title> <journal> Computer Graphics and Image Processing, </journal> <volume> 9 </volume> <pages> 301-315. </pages>
Reference-contexts: Horn & Schunck, 1981), scale (e.g. Anandan, 1989), and time (e.g. Morikawa & Harashima, 1993). Secondly, there exist some examples where different segmentation cues, such as color and motion, are integrated (e.g. Poggio et al., 1988; Dubuisson & Jain, 1995) or where segmentation cues are combined with object recognition <ref> (e.g. Eckes & Vorbr uggen, 1996) </ref>, but these systems are the exception rather than the rule. These two types of integration are of low and high level, respectively.
Reference: <author> Fleet, D. J. and Jepson, A. D. </author> <year> (1990). </year> <title> Computation of component image velocity from local phase information. </title> <journal> Int'l J. of Computer Vision, </journal> <volume> 5(1) </volume> <pages> 77-104. </pages>
Reference-contexts: The system presented here differs significantly from these two dominant classes of systems. However, most of the components used in this system are known techniques in the literature. The image flow estimation is adopted from (Theimer & Mallot, 1994) based on <ref> (Fleet & Jepson, 1990) </ref>. A similar system has been presented in (Valentinotti et al., 1996). Histogram techniques for finding the dominant motions can be found in (e.g. Lawton, 1983; Schunck, 1989; Cumani et al., 1991). Matching feature points for image flow estimation has been used often (e.g.
Reference: <author> Healey, G. </author> <year> (1993). </year> <title> Hierarchical segmentation-based approach to motion analysis. </title> <journal> Image and Vision Computing, </journal> <volume> 11(9) </volume> <pages> 570-576. </pages>
Reference-contexts: This method again results in a rather crude segmentation with a resolution given by the block-size. However, the purpose of video coding is compression rather than segmentation in any case. An interesting variant of this matching method has been presented in <ref> (Healey, 1993) </ref>. This system first performs a segmentation on each single frame based on grey value differences and matches then between the resulting regions. This method provides full field segmentation with high resolution. These matching based methods do not suffer from the aperture problem.
Reference: <author> Heitz, F. and Bouthemy, P. </author> <year> (1993). </year> <title> Multimodal motion estimation of discontinuous optical flow using Markov random fields. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 15(12) </volume> <pages> 1217-1232. </pages>
Reference: <author> Horn, B. K. P. and Schunck, B. G. </author> <year> (1981). </year> <title> Determining optical flow. </title> <journal> Artificial Intelligence, </journal> <volume> 17 </volume> <pages> 185-203. </pages>
Reference-contexts: This can be done by simple averaging or by more sophisticated methods, such as robust estimation with outlier detection (e.g. Darrell & Pentland, 1995; Huang et al., 1995). Integration has been applied to space <ref> (e.g. Horn & Schunck, 1981) </ref>, scale (e.g. Anandan, 1989), and time (e.g. Morikawa & Harashima, 1993). Secondly, there exist some examples where different segmentation cues, such as color and motion, are integrated (e.g.
Reference: <author> Huang, Y., Palaniappan, K., Zhuang, X., and Cavanaugh, J. E. </author> <year> (1995). </year> <title> Optic flow field segmentation and motion estimation using a robust genetic partitioning algorithm. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 17(12) </volume> <pages> 1177-1190. </pages>
Reference: <author> Huttenlocher, D. P., Noh, J. J., and Rucklidge, W. J. </author> <year> (1993). </year> <title> Tracking non-rigid objects in complex scenes. </title> <booktitle> In Proc. Fourth Int'l Conf. on Computer Vision, </booktitle> <address> Berlin, Germany, </address> <pages> pages 93-101, </pages> <address> Los Alamitos, CA, USA. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Hwang, S. H. and Lee, S. U. </author> <year> (1993). </year> <title> A hierarchical optical flow estimation algorithm based on the interlevel motion smoothness constraint. </title> <journal> Pattern Recognition, </journal> <volume> 26(6) </volume> <pages> 939-952. </pages>
Reference: <author> Irani, M., Rousso, B., and Peleg, S. </author> <year> (1994). </year> <title> Computing occluding and transparent motions. </title> <journal> Int'l J. of Computer Vision, </journal> <volume> 12(1) </volume> <pages> 5-16. </pages>
Reference-contexts: Temporal integration has also been used before, though it is usually done by assuming continuous motion (e.g. Bouthemy & Francois, 1993; Smith & Brady, 1995). But there are also some examples of temporal integration without continuity assumptions, for instance <ref> (Irani et al., 1994) </ref>. However, this system integrates the gray value images over several frames while the system presented here integrates segmentation evidence.
Reference: <author> Jones, J. P. and Palmer, L. A. </author> <year> (1987). </year> <title> An evaluation of the two-dimensional Gabor filter model of simple receptive fields in cat striate cortex. </title> <journal> J. of Neurophysiology, </journal> <volume> 58 </volume> <pages> 1233-1258. </pages>
Reference: <author> Konrad, J. and Dubois, E. </author> <year> (1992). </year> <title> Bayesian estimation of vector motion fields. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 14(9) </volume> <pages> 910-927. </pages>
Reference: <author> Lades, M., Vorbr uggen, J. C., Buhmann, J., Lange, J., von der Malsburg, C., W urtz, R. P., and Konen, W. </author> <year> (1993). </year> <title> Distortion invariant object recognition in the dynamic link architecture. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 42(3) </volume> <pages> 300-311. </pages>
Reference-contexts: Gabor wavelets are technically interesting for image representation (Daugman, 1988) and object recognition <ref> (Lades et al., 1993) </ref>. They are also biologically plausible, because they have similar shape as the receptive fields of simple cells found in the visual cortex of vertebrate animals (Jones & Palmer, 1987; DeValois & DeValois, 1988).
Reference: <author> Lawton, D. T. </author> <year> (1983). </year> <title> Processing translational motion sequences. Computer Vision, </title> <journal> Graphics and Image Processing, </journal> <volume> 22(1) </volume> <pages> 116-144. </pages>
Reference: <author> Lee, S.-W., Choi, J. G., and Kim, S.-D. </author> <year> (1997). </year> <title> Scene segmentation using a combined criterion of motion and intensity. </title> <journal> Optical Engineering, </journal> <volume> 36(8) </volume> <pages> 2346-2352. </pages>
Reference: <author> Luettgen, M. R., Karl, W. C., and Willsky, A. S. </author> <year> (1994). </year> <title> Efficient multiscale regularization with applications to the computation of optical flow. </title> <journal> IEEE Trans. on Image Processing, </journal> <volume> 3(1) </volume> <pages> 41-64. </pages>
Reference: <author> Mallat, S. and Zhong, S. </author> <year> (1992). </year> <title> Characterization of signals from multiscale edges. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 14(7) </volume> <pages> 710-732. </pages>
Reference-contexts: Instead of testing all possible displacements within a certain distance range, only a few of them suggested by the motion hypotheses need to be taken into consideration; see Fig. 4. 2.1.3 Edge Valuation The third stage of the algorithm uses the Mallat-wavelet transform <ref> (Mallat & Zhong, 1992) </ref>, which can be thought of as the grey-value gradient at different levels of resolution. This stage is similar to a matching algorithm and is therefore faced with the correspondence problem, which is particularly severe for 6 15 of the moving-animals sequence. <p> With motion hypotheses, the regions to consider reduce to few small spots. 7 such a simple feature as the local gradient. Two methods are employed here to bypass the correspondence problem. Firstly, the evaluation of the Mallat-wavelet transform is restricted to edges, as defined by the modulus maxima <ref> (Mallat & Zhong, 1992) </ref>. Edges have a particularly high information content and are less ambiguous than gradients in general. Secondly, I do not match the edges between two frames, but ask for their accordance with the different motion hypotheses obtained in stage two. <p> However, as will be discussed later, it is important from a conceptional point of view that the modulus maxima of the wavelet transform provide enough information to reconstruct images <ref> (Mallat & Zhong, 1992) </ref>. Given the image I (x) the gradient is defined as g (x) = (@I=@x; @I=@y) = (g cos ; g sin ); (9) with g = g (x) 0 denoting the magnitude and = (x) the direction angle of g.
Reference: <author> Mitiche, A. and Aggarwal, J. K. </author> <year> (1985). </year> <title> Image segmentation by conventional and information-integrating techniques: a synopsis. </title> <journal> Image and Vision Computing, </journal> <volume> 3(2) </volume> <pages> 50-62. </pages>
Reference: <author> Morikawa, H. and Harashima, H. </author> <year> (1993). </year> <title> Incremental segmentation of moving pictures: An analysis by synthesis approach. </title> <journal> IEICE Trans. Inf. & Syst., E76-D(4):446-453. </journal>
Reference-contexts: This can be done by simple averaging or by more sophisticated methods, such as robust estimation with outlier detection (e.g. Darrell & Pentland, 1995; Huang et al., 1995). Integration has been applied to space (e.g. Horn & Schunck, 1981), scale (e.g. Anandan, 1989), and time <ref> (e.g. Morikawa & Harashima, 1993) </ref>. Secondly, there exist some examples where different segmentation cues, such as color and motion, are integrated (e.g. Poggio et al., 1988; Dubuisson & Jain, 1995) or where segmentation cues are combined with object recognition (e.g. <p> Image flow discontinuities typically run through homogeneous regions, e.g. Fig. 12 in (Wang & Adelson, 1994). These systems therefore have the strong tendency not to segment along object boundaries. An exception are systems which do an edge based segmentation on single frames before image flow estimation <ref> (e.g. Morikawa & Harashima, 1993) </ref>. The two-frames segmentation algorithm as well as the sequence algorithm include no object model constraints. Each pixel is segmented independently of the neighboring ones. This makes it possible to segment several small, disconnected, or openworked objects.
Reference: <author> Orchard, M. T. </author> <year> (1993). </year> <title> Predictive motion-field segmentation for image sequence coding. </title> <journal> IEEE Trans. on Circuits and Systems for Video Technology, </journal> <volume> 3(1) </volume> <pages> 54-70. </pages>
Reference: <author> Poggio, T., Gamble, E. B., and Little, J. J. </author> <year> (1988). </year> <title> Parallel integration of vision modules. </title> <journal> Science, </journal> <volume> 242 </volume> <pages> 436-440. </pages>
Reference: <author> Ramachandran, V. S., Cobb, S., and Rogers-Ramachandran, D. </author> <year> (1988). </year> <title> Perception of 3-D structure from motion: The role of velocity gradients and segmentation boundaries. </title> <journal> Perception and Psychophysics, </journal> <volume> 44(4) </volume> <pages> 390-393. </pages>
Reference: <author> Schn orr, C. </author> <year> (1992). </year> <title> Computation of discontinuous optical flow by domain decomposition and shape optimization. </title> <journal> Int. J. of Computer Vision, </journal> <volume> 8(2) </volume> <pages> 153-165. </pages>
Reference: <author> Schunck, B. G. </author> <year> (1989). </year> <title> Image flow segmentation and estimation by constraint line clustering. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 11(10) </volume> <pages> 1010-1027. </pages>
Reference: <author> Shah, M., Rangarajan, K., and Tsai, P.-S. </author> <year> (1993). </year> <title> Motion trajectories. </title> <journal> IEEE Trans. on Systems, Man and Cybernetics, </journal> <volume> 23(4) </volume> <pages> 1138-1150. </pages>
Reference: <author> Smith, S. M. and Brady, J. M. </author> <year> (1995). </year> <title> ASSET-2: Real-time motion segmentation and shape tracking. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 17(8) </volume> <pages> 814-820. </pages>
Reference: <author> Theimer, W. M. and Mallot, H. A. </author> <year> (1994). </year> <title> Phase-based binocular vergence control and depth reconstruction using active vision. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 60(3) </volume> <pages> 343-358. </pages>
Reference-contexts: Estimating Flow Vectors Estimating image flow vectors for successive frames of a sequence, disparities between pairs of stereo images, or small positional displacements in a matching procedure are closely related tasks. The algorithm presented here was adopted from a stereo algorithm <ref> (Theimer & Mallot, 1994) </ref> and has also been used for matching purposes (Wiskott et al., 1997). Consider two jets J , J 0 taken from the same pixel position in two successive frames. The underlying object in the scene may have moved by a vector d. <p> The system presented here differs significantly from these two dominant classes of systems. However, most of the components used in this system are known techniques in the literature. The image flow estimation is adopted from <ref> (Theimer & Mallot, 1994) </ref> based on (Fleet & Jepson, 1990). A similar system has been presented in (Valentinotti et al., 1996). Histogram techniques for finding the dominant motions can be found in (e.g. Lawton, 1983; Schunck, 1989; Cumani et al., 1991).
Reference: <author> Thompson, W. B. </author> <year> (1980). </year> <title> Combining motion and contrast for segmentation. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 2(6) </volume> <pages> 543-549. </pages>
Reference: <author> Thompson, W. B., Lechleider, P., and Stuck, E. R. </author> <year> (1993). </year> <title> Detecting moving objects using the rigidity constraint. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 15(2) </volume> <pages> 162-166. </pages>
Reference: <author> Valentinotti, F., Caro, G. D., and Crespi, B. </author> <year> (1996). </year> <title> Real-time parallel computation of disparity and optical flow using phase difference. </title> <journal> Machine Vision and Applications, </journal> <volume> 9(3) </volume> <pages> 87-96. </pages>
Reference-contexts: However, most of the components used in this system are known techniques in the literature. The image flow estimation is adopted from (Theimer & Mallot, 1994) based on (Fleet & Jepson, 1990). A similar system has been presented in <ref> (Valentinotti et al., 1996) </ref>. Histogram techniques for finding the dominant motions can be found in (e.g. Lawton, 1983; Schunck, 1989; Cumani et al., 1991). Matching feature points for image flow estimation has been used often (e.g. Shah et al., 1993; Thompson et al., 1993; Smith & Brady, 1995).
Reference: <author> Wang, J. Y. A. and Adelson, E. H. </author> <year> (1994). </year> <title> Representing moving images with layers. </title> <journal> IEEE Trans. on Image Processing, </journal> <volume> 3(5) </volume> <pages> 625-638. </pages>
Reference-contexts: Shah et al., 1993; Thompson et al., 1993; Smith & Brady, 1995). Matching of edges, however, seems to be used rarely only (e.g. Lawton, 1983; Cumani et al., 1991). The accordance maps and the representation of the segmentation results are somewhat analogous to the layers used in <ref> (Wang & Adelson, 1994) </ref> and (Darrell & Pentland, 1995), though there are significant differences. Temporal integration has also been used before, though it is usually done by assuming continuous motion (e.g. Bouthemy & Francois, 1993; Smith & Brady, 1995). <p> Even if they could, e.g. by assigning ambiguous regions to the background, typical image flow algorithms do not provide segmentation evidence along the edges. Image flow discontinuities typically run through homogeneous regions, e.g. Fig. 12 in <ref> (Wang & Adelson, 1994) </ref>. These systems therefore have the strong tendency not to segment along object boundaries. An exception are systems which do an edge based segmentation on single frames before image flow estimation (e.g. Morikawa & Harashima, 1993).
Reference: <author> Westberg, L. </author> <year> (1992). </year> <title> Hierarchical contour-based segmentation of dynamic scenes. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 14(9) </volume> <pages> 946-952. </pages>
Reference: <author> Wiskott, L., Fellous, J.-M., Kr uger, N., and von der Malsburg, C. </author> <year> (1997). </year> <title> Face recognition by elastic bunch graph matching. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 19(7) </volume> <pages> 775-779. </pages>
Reference-contexts: The algorithm presented here was adopted from a stereo algorithm (Theimer & Mallot, 1994) and has also been used for matching purposes <ref> (Wiskott et al., 1997) </ref>. Consider two jets J , J 0 taken from the same pixel position in two successive frames. The underlying object in the scene may have moved by a vector d. <p> The denominator normalizes S OE to the range of [1; :::; 1], so that it is independent of the contrast of the images. S OE can be used as a similarity function <ref> (Wiskott et al., 1997) </ref> and provides a cue as to how reliable the displacement estimation is.
Reference: <author> Wiskott, L. and von der Malsburg, C. </author> <year> (1993). </year> <title> A neural system for the recognition of partially occluded objects in cluttered scenes. </title> <journal> Int. J. of Pattern Recognition and Artificial Intelligence, </journal> <volume> 7(4) </volume> <pages> 935-948. </pages>
Reference: <author> Zheng, H. and Blostein, S. D. </author> <year> (1995). </year> <title> Motion-based object segmentation and estimation using the MDL principle. </title> <journal> IEEE Trans. on Image Processing, </journal> <volume> 4(9) </volume> <pages> 1223-1235. 20 </pages>
References-found: 49

