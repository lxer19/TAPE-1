URL: ftp://ftp.cis.ufl.edu/cis/tech-reports/tr92/tr92-014.ps
Refering-URL: http://www.cis.ufl.edu/tech-reports/tech-reports/tr92-abstracts.html
Root-URL: http://www.cis.ufl.edu
Title: Performance of an unsymmetric-pattern multifrontal method for sparse LU factorization  
Author: Timothy A. Davis 
Note: Supported in part by NSF ASC-9111263. To appear: Proceedings of the 7th IMACS Int. Conf. on Computer Methods for Partial Differential Equations,  
Address: Gainesville, FL 32611-2024 USA  New Brunswick, New Jersey, USA  
Affiliation: 301 CSE, Computer and Information Sciences Dept. University of Florida,  Rutgers Univ.,  
Pubnum: Technical Report TR-92-014  
Email: email: davis@cis.ufl.edu  
Phone: (904) 392-1481,  
Date: June 22-24, 1992,  May 26, 1992  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> P. R. Amestoy and I. S. Duff. </author> <title> Vectorization of a multiprocessor multifrontal code. </title> <journal> Int. J. Supercomputer Appl., </journal> <volume> 3(3) </volume> <pages> 41-59, </pages> <year> 1989. </year>
Reference-contexts: This will be corrected by modifying the upward-looking algorithm to update the proposed pivot columns before they are selected. No delayed pivoting will then occur. However, even with this limitation, the current versions outperform the MA28 algorithm [6], sequential versions of the classical multifrontal method (Mups) <ref> [1] </ref>, and the D2 algorithm [2] for some matrices. Eighty-six matrices from the Harwell/Boeing (HB) collection [5] and our collaborators have been assembled and factorized with all of these methods. Twenty-seven of these are symmetric positive-definite. Only matrices of order 500 or larger were considered.
Reference: [2] <author> T. A. Davis and P. C. Yew. </author> <title> A nondeterministic parallel algorithm for general unsymmetric sparse LU factorization. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 11(3) </volume> <pages> 383-402, </pages> <year> 1990. </year>
Reference-contexts: However, this method is based on an assumption of a symmetric nonzero pattern, and so has a poor performance on matrices whose patterns are very unsymmetric. None of the previous parallel methods for unsymmetric-patterned matrices use dense matrix kernels <ref> [2, 10, 11] </ref>, with the exception of a multifrontal QR factorization algorithm [15] (which will be compared later on with the algorithms we develop). fl available via anonymous ftp to cis.ufl.edu as /cis/tech-reports/tr92/tr92-014.ps.Z 1 This paper presents a new unsymmetric-pattern multifrontal method that takes full advantage of dense matrix kernels, maintains <p> No delayed pivoting will then occur. However, even with this limitation, the current versions outperform the MA28 algorithm [6], sequential versions of the classical multifrontal method (Mups) [1], and the D2 algorithm <ref> [2] </ref> for some matrices. Eighty-six matrices from the Harwell/Boeing (HB) collection [5] and our collaborators have been assembled and factorized with all of these methods. Twenty-seven of these are symmetric positive-definite. Only matrices of order 500 or larger were considered.
Reference: [3] <author> J. J. Dongarra, J. Du Croz, S. Hammarling, and I. S. Duff. </author> <title> A set of level-3 basic linear algebra subprograms. </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <year> 1990. </year>
Reference-contexts: These steps of LU factorization compute a submatrix of update terms that are held within the frontal matrix until they are assembled (added) into the frontal matrix of its father in the elimination tree. The elimination tree controls parallelism across multiple frontal matrices, while dense matrix operations <ref> [3] </ref> provide parallelism and vectorization within each frontal matrix. However, this method is based on an assumption of a symmetric nonzero pattern, and so has a poor performance on matrices whose patterns are very unsymmetric. <p> These steps can use dense matrix kernels (Level-3 BLAS if g k &gt; 1) <ref> [3] </ref>. These update terms are added to A [k1] to obtain the reduced matrix A [k+g k 1] . The elements E k+1 to E k+g k 1 are not explicitly represented, having been amalgamated into the single element E k .
Reference: [4] <author> I. S. Duff, A. M. Erisman, and J. K. Reid. </author> <title> Direct Methods for Sparse Matrices. </title> <publisher> London: Oxford Univ. Press, </publisher> <year> 1986. </year>
Reference-contexts: 1 Introduction Solving large, unsymmetric, sparse linear systems on a high-performance parallel supercomputer is critical to many diverse areas of scientific computing such as climate modeling, finite-element methods, structural analysis, computational fluid dynamics, economic modeling, oil reservoir simulation, circuit simulation, chemical process modeling, and electrical power network modeling <ref> [4, 16] </ref>. However, conventional sparse matrix factorization algorithms rely heavily on indirect addressing. This gives them an irregular memory access pattern that limits their performance. <p> Perform up to g k steps of numerical factorization within the front. Set g k to the number of pivots actually found. 5. Update the Lson and Uson lists. Increment k by g k and repeat until the matrix is factorized. The search is based on Markowitz' strategy <ref> [4] </ref>, which selects the pivot with minimum upper bound on fill-in. The rows and columns of the active matrix are not held explicitly, rather, they are held as a set of contribution blocks and entries from the original matrix A. <p> To avoid expensive scans only upper and lower bounds of the degree of each row and column are computed. When, and if, the true degree is calculated, the two bounds are set equal to the true degree. The pivot must also satisfy the numerical threshold partial-pivoting criterion <ref> [4] </ref>. The approximate degree update finds the upper and lower bounds of the degrees of each row i 2 L 0 k and column j 2 U 0 k by scanning their Lson and Uson lists, respectively.
Reference: [5] <author> I. S. Duff, R. G. Grimes, and J. G. Lewis. </author> <title> Sparse matrix test problems. </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 15 </volume> <pages> 1-14, </pages> <year> 1989. </year>
Reference-contexts: No delayed pivoting will then occur. However, even with this limitation, the current versions outperform the MA28 algorithm [6], sequential versions of the classical multifrontal method (Mups) [1], and the D2 algorithm [2] for some matrices. Eighty-six matrices from the Harwell/Boeing (HB) collection <ref> [5] </ref> and our collaborators have been assembled and factorized with all of these methods. Twenty-seven of these are symmetric positive-definite. Only matrices of order 500 or larger were considered. The X/* and Z/* matrices are from chemical engineering problems (from S. Zitney and others [16]) (X/m2 is from a PDE).
Reference: [6] <author> I. S. Duff and J. K. Reid. </author> <title> Some design features of a sparse matrix code. </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 5(1) </volume> <pages> 18-35, </pages> <year> 1979. </year>
Reference-contexts: Both methods exhibit excessive delayed pivoting for some matrices. This will be corrected by modifying the upward-looking algorithm to update the proposed pivot columns before they are selected. No delayed pivoting will then occur. However, even with this limitation, the current versions outperform the MA28 algorithm <ref> [6] </ref>, sequential versions of the classical multifrontal method (Mups) [1], and the D2 algorithm [2] for some matrices. Eighty-six matrices from the Harwell/Boeing (HB) collection [5] and our collaborators have been assembled and factorized with all of these methods. Twenty-seven of these are symmetric positive-definite.
Reference: [7] <author> I. S. Duff and J. K. Reid. </author> <title> The multifrontal solution of unsymmetric sets of linear equations. </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 5(3) </volume> <pages> 633-641, </pages> <year> 1984. </year>
Reference-contexts: Thus the algorithm can use dense matrix kernels in its innermost loops, potentially giving it high performance on parallel supercomputers. Parallel algorithms for sparse positive definite matrices [13] or for matrices with nearly symmetric pattern <ref> [7] </ref> are typically divided into a symbolic factorization phase that computes the pivot ordering and patterns of the LU factors, and a numerical factorization phase that computes L and U .
Reference: [8] <author> S. C. Eisenstant and J. W. H. Liu. </author> <title> Exploiting structural symmetry in unsymmetric sparse symbolic factorization. </title> <type> Technical Report CS-90-12, </type> <institution> Dept. of Computer Sci., York Univ., </institution> <address> North York, Ontario, </address> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: The disruptions are more severe in the unsymmetric-pattern multifrontal method if the pivot ordering is not known to be numerically acceptable (unsymmetric symbolic factorization when the pivot ordering is known is described in <ref> [8, 9, 12] </ref>). If a single phase is used, the numerical values are available to the pivot search heuristic. <p> Finally, node s is an LUson of its LUfather node t if (s; t) 2 E L and (s; t) 2 E U . Similar directed acyclic graphs been used for symbolic factorization of unsymmetric sparse matrices <ref> [8, 9, 12] </ref>. If the pattern of the LU factors is symmetric, the contribution block of s is always completely assembled into its single LUfather node f in the elimination tree. <p> Node amalgamation will also remove some of these edges. Edge reductions decrease memory requirements, and simplify amalgamation and degree update. These reductions are similar to those used by Eisenstat, Liu, and Gilbert, <ref> [8, 12, 9] </ref>, except that additional edges are removed during degree update. These two graphs are not necessarily minimal. Transitive reduction could be applied to obtain a minimal control flow graph, for example, but this would introduce a significant computational overhead.
Reference: [9] <author> S. C. Eisenstant and J. W. H. Liu. </author> <title> Exploiting structural symmetry in a sparse partial pivoting code. </title> <type> Technical Report CS-92-01, </type> <institution> Dept. of Computer Sci., York Univ., </institution> <address> North York, Ontario, </address> <month> Feb. </month> <year> 1992. </year>
Reference-contexts: The disruptions are more severe in the unsymmetric-pattern multifrontal method if the pivot ordering is not known to be numerically acceptable (unsymmetric symbolic factorization when the pivot ordering is known is described in <ref> [8, 9, 12] </ref>). If a single phase is used, the numerical values are available to the pivot search heuristic. <p> Finally, node s is an LUson of its LUfather node t if (s; t) 2 E L and (s; t) 2 E U . Similar directed acyclic graphs been used for symbolic factorization of unsymmetric sparse matrices <ref> [8, 9, 12] </ref>. If the pattern of the LU factors is symmetric, the contribution block of s is always completely assembled into its single LUfather node f in the elimination tree. <p> Node amalgamation will also remove some of these edges. Edge reductions decrease memory requirements, and simplify amalgamation and degree update. These reductions are similar to those used by Eisenstat, Liu, and Gilbert, <ref> [8, 12, 9] </ref>, except that additional edges are removed during degree update. These two graphs are not necessarily minimal. Transitive reduction could be applied to obtain a minimal control flow graph, for example, but this would introduce a significant computational overhead.
Reference: [10] <author> A. George and E. Ng. </author> <title> Parallel sparse Gaussian elimination with partial pivoting. </title> <type> Technical report, </type> <institution> Oak Ridge National Laboratory, </institution> <year> 1988. </year>
Reference-contexts: However, this method is based on an assumption of a symmetric nonzero pattern, and so has a poor performance on matrices whose patterns are very unsymmetric. None of the previous parallel methods for unsymmetric-patterned matrices use dense matrix kernels <ref> [2, 10, 11] </ref>, with the exception of a multifrontal QR factorization algorithm [15] (which will be compared later on with the algorithms we develop). fl available via anonymous ftp to cis.ufl.edu as /cis/tech-reports/tr92/tr92-014.ps.Z 1 This paper presents a new unsymmetric-pattern multifrontal method that takes full advantage of dense matrix kernels, maintains
Reference: [11] <author> J. R. Gilbert. </author> <title> An efficient parallel sparse partial pivoting algorithm. </title> <type> Technical Report 88/45052-1, </type> <institution> Center for Computer Science, Chr. Michelsen Institute, Bergen, Norway, </institution> <year> 1988. </year> <month> 11 </month>
Reference-contexts: However, this method is based on an assumption of a symmetric nonzero pattern, and so has a poor performance on matrices whose patterns are very unsymmetric. None of the previous parallel methods for unsymmetric-patterned matrices use dense matrix kernels <ref> [2, 10, 11] </ref>, with the exception of a multifrontal QR factorization algorithm [15] (which will be compared later on with the algorithms we develop). fl available via anonymous ftp to cis.ufl.edu as /cis/tech-reports/tr92/tr92-014.ps.Z 1 This paper presents a new unsymmetric-pattern multifrontal method that takes full advantage of dense matrix kernels, maintains
Reference: [12] <author> J. R. Gilbert and J. W. H. Liu. </author> <title> Elimination structures for unsymmetric sparse LU factors. </title> <type> Technical Report CS-90-11, </type> <institution> Dept. of Computer Sci., York Univ., </institution> <address> North York, Ontario, </address> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: The disruptions are more severe in the unsymmetric-pattern multifrontal method if the pivot ordering is not known to be numerically acceptable (unsymmetric symbolic factorization when the pivot ordering is known is described in <ref> [8, 9, 12] </ref>). If a single phase is used, the numerical values are available to the pivot search heuristic. <p> Finally, node s is an LUson of its LUfather node t if (s; t) 2 E L and (s; t) 2 E U . Similar directed acyclic graphs been used for symbolic factorization of unsymmetric sparse matrices <ref> [8, 9, 12] </ref>. If the pattern of the LU factors is symmetric, the contribution block of s is always completely assembled into its single LUfather node f in the elimination tree. <p> Node amalgamation will also remove some of these edges. Edge reductions decrease memory requirements, and simplify amalgamation and degree update. These reductions are similar to those used by Eisenstat, Liu, and Gilbert, <ref> [8, 12, 9] </ref>, except that additional edges are removed during degree update. These two graphs are not necessarily minimal. Transitive reduction could be applied to obtain a minimal control flow graph, for example, but this would introduce a significant computational overhead.
Reference: [13] <author> M. T. Heath, E. Ng, and B. W. Peyton. </author> <title> Parallel algorithms for sparse linear systems. </title> <journal> SIAM Review, </journal> <volume> 33(3) </volume> <pages> 420-460, </pages> <year> 1991. </year>
Reference-contexts: However, conventional sparse matrix factorization algorithms rely heavily on indirect addressing. This gives them an irregular memory access pattern that limits their performance. For symmetric problems, methods such as the multifrontal method and the supernodal approach have replaced irregular operations with dense matrix kernels (see <ref> [13] </ref> for a survey). The kernel of the multi-frontal method is one or more steps of LU factorization within each square, dense frontal matrix defined by the nonzero pattern of a pivot row and column. <p> Thus the algorithm can use dense matrix kernels in its innermost loops, potentially giving it high performance on parallel supercomputers. Parallel algorithms for sparse positive definite matrices <ref> [13] </ref> or for matrices with nearly symmetric pattern [7] are typically divided into a symbolic factorization phase that computes the pivot ordering and patterns of the LU factors, and a numerical factorization phase that computes L and U .
Reference: [14] <author> J. W. H. Liu. </author> <title> The role of elimination trees in sparse factorization. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 11(1) </volume> <pages> 134-172, </pages> <year> 1990. </year>
Reference-contexts: Instead, it is held as a sum of the original matrix A, and the elements created during factorization. The elimination tree forms the basis of many sparse matrix algorithms, describing either the data flow graph or the control flow graph, or both <ref> [14] </ref>. It is insufficient for our purposes.
Reference: [15] <author> P. Matstoms. </author> <title> Sparse QR factorization in MATLAB. </title> <type> Technical Report LiTH-MAT-R-1992-05, </type> <institution> Dept. of Mathematics, Linkoping Univ., Linkoping, Sweden, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: None of the previous parallel methods for unsymmetric-patterned matrices use dense matrix kernels [2, 10, 11], with the exception of a multifrontal QR factorization algorithm <ref> [15] </ref> (which will be compared later on with the algorithms we develop). fl available via anonymous ftp to cis.ufl.edu as /cis/tech-reports/tr92/tr92-014.ps.Z 1 This paper presents a new unsymmetric-pattern multifrontal method that takes full advantage of dense matrix kernels, maintains a purely unsymmetric pattern, and is suitable for both shared memory and
Reference: [16] <author> S. E. Zitney and M. A. Stadtherr. </author> <title> A frontal algorithm for equation-based chemical process flowsheeting on vector and parallel computers. </title> <booktitle> In Proc. AIChE Annual Meeting, </booktitle> <address> Washington, DC, </address> <year> 1988. </year>
Reference-contexts: 1 Introduction Solving large, unsymmetric, sparse linear systems on a high-performance parallel supercomputer is critical to many diverse areas of scientific computing such as climate modeling, finite-element methods, structural analysis, computational fluid dynamics, economic modeling, oil reservoir simulation, circuit simulation, chemical process modeling, and electrical power network modeling <ref> [4, 16] </ref>. However, conventional sparse matrix factorization algorithms rely heavily on indirect addressing. This gives them an irregular memory access pattern that limits their performance. <p> Twenty-seven of these are symmetric positive-definite. Only matrices of order 500 or larger were considered. The X/* and Z/* matrices are from chemical engineering problems (from S. Zitney and others <ref> [16] </ref>) (X/m2 is from a PDE). The Hm/* matrices are circuit simulation matrices from S. Hamm (Motorola). Table 1 lists the results for these matrices obtained on a Cray YMP/8128, sorted by asymmetry (and by order if tied).
References-found: 16

