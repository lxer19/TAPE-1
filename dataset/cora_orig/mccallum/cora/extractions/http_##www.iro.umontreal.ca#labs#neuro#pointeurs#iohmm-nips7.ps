URL: http://www.iro.umontreal.ca/labs/neuro/pointeurs/iohmm-nips7.ps
Refering-URL: http://www.iro.umontreal.ca/labs/neuro/proc.html
Root-URL: http://www.iro.umontreal.ca
Email: bengioy@IRO.UMontreal.CA  paolo@mcculloch.ing.unifi.it  
Title: An Input Output HMM Architecture  
Author: Yoshua Bengio Paolo Frasconi 
Address: Montreal, Qc H3C-3J7  (Italy)  
Affiliation: Dept. Informatique et Recherche Operationnelle Universite de  Dipartimento di Sistemi e Informatica Universita di Firenze  
Abstract: We introduce a recurrent architecture having a modular structure and we formulate a training procedure based on the EM algorithm. The resulting model has similarities to hidden Markov models, but supports recurrent networks processing style and allows to exploit the supervised learning paradigm while using maximum likelihood estimation. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Angluin, D. and Smith, C. </author> <year> (1983). </year> <title> Inductive inference: Theory and methods. </title> <journal> Computing Surveys, </journal> <volume> 15(3) </volume> <pages> 237-269. </pages>
Reference-contexts: It can be considered as a prototype for more complex language processing problems. However, even in the "simplest" case, i.e. regular grammars, the task can be proved to be NP-complete <ref> (Angluin and Smith, 1983) </ref>. We report experimental results on a set of regular grammars introduced by Tomita (1982) and afterwards used by other researchers to measure the accuracy of inference methods based on recurrent networks (Giles et al., 1992; Pollack, 1991; Watrous and Kuhn, 1992).
Reference: <author> Bengio, Y. and Frasconi, P. </author> <year> (1994a). </year> <title> Credit assignment through time: Alternatives to backpropagation. </title> <editor> In Cowan, J., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Thus, a potential solution is to propagate, backward in time, targets in a discrete state space rather than differential error information. Extending previous work <ref> (Bengio & Frasconi, 1994a) </ref>, in this paper we propose a statistical approach to target propagation, based on the EM algorithm. We consider a parametric dynamical system with discrete states and we introduce a modular architecture, with subnetworks associated to discrete states. <p> This model can also be seen as a recurrent version of the Mixture of Experts architecture (Jacobs et al., 1991), related to the model already proposed in (Cacciatore and Nowlan, 1994). Experiments on artificial tasks <ref> (Bengio & Frasconi, 1994a) </ref> have shown that EM recurrent learning can deal with long term dependencies more effectively than backpropagation through time and other alternative algorithms. However, the model used in (Bengio & Frasconi, 1994a) has very limited representational capabilities and can only map an input sequence to a final discrete <p> Experiments on artificial tasks <ref> (Bengio & Frasconi, 1994a) </ref> have shown that EM recurrent learning can deal with long term dependencies more effectively than backpropagation through time and other alternative algorithms. However, the model used in (Bengio & Frasconi, 1994a) has very limited representational capabilities and can only map an input sequence to a final discrete state. In the present paper we describe an extended architecture that allows to fully exploit both input and output portions of the data, as required by the supervised learning paradigm.
Reference: <author> Bengio, Y. and Frasconi, P. </author> <year> (1994b). </year> <title> An EM Approach to Learning Sequential Behavior. </title> <type> Tech. Rep. </type> <institution> RT-DSI/11-94, University of Florence. </institution>
Reference-contexts: For example, applications of HMMs to speech recognition always rely on structured topologies. 6 CONCLUSIONS There are still a number of open questions. In particular, the effectiveness of the model on tasks involving large or very large state spaces needs to be carefully evaluated. In <ref> (Bengio & Frasconi 1994b) </ref> we show that learning long term dependencies in these models becomes more difficult as we increase the connectivity of the state transition graph. However, because transition probabilities of IOHMMs change at each t, they deal better with this problem of long-term dependencies than standard HMMs.
Reference: <author> Bengio, Y., De Mori, R., Flammia, G., and Kompe, R. </author> <year> (1992). </year> <title> Global optimization of a neural network-hidden markov model hybrid. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3(2) </volume> <pages> 252-259. </pages>
Reference-contexts: Finally, it is worth mentioning that a number of hybrid approaches have been proposed to integrate connectionist approaches into the HMM framework. For example in <ref> (Bengio et al., 1992) </ref> the observations used by the HMM are generated by a feedforward neural network. In (Bourlard and Wellekens, 1990) a feedforward network is used to estimate state probabilities, conditional to the acoustic sequence.
Reference: <author> Bengio, Y., Simard, P., and Frasconi, P. </author> <year> (1994). </year> <title> Learning long-term dependencies with gradient descent is difficult. </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> 5(2). </volume>
Reference-contexts: Although effective for learning short term memories, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals (Bengio et al., 1994; Mozer, 1992). Previous work on alternative training algorithms <ref> (Bengio et al., 1994) </ref> could suggest that the root of the problem lies in the essentially discrete nature of the process of storing information for an indefinite amount of time.
Reference: <author> Bourlard, H. and Wellekens, C. </author> <year> (1990). </year> <title> Links between hidden markov models and multilayer perceptrons. </title> <journal> IEEE Trans. Pattern An. Mach. Intell., </journal> <volume> 12 </volume> <pages> 1167-1178. </pages>
Reference-contexts: Finally, it is worth mentioning that a number of hybrid approaches have been proposed to integrate connectionist approaches into the HMM framework. For example in (Bengio et al., 1992) the observations used by the HMM are generated by a feedforward neural network. In <ref> (Bourlard and Wellekens, 1990) </ref> a feedforward network is used to estimate state probabilities, conditional to the acoustic sequence. A common feature of these algorithms and the one proposed in this paper is that neural networks are used to extract temporally local information whereas a Markovian system integrates long-term constraints.
Reference: <author> Bridle, J. S. </author> <year> (1989). </year> <title> Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. </title> <editor> In D.S.Touretzky, ed., </editor> <booktitle> NIPS2, </booktitle> <pages> pages 211-217. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: An approach that has been found useful to improve discrimination in HMMs is based on maximum mutual information (MMI) training. It has been pointed out that supervised learning and discriminant learning criteria like MMI are actually strictly related <ref> (Bridle, 1989) </ref>. Although the parameter adjusting procedure we have defined is based on MLE, y T 1 is used as desired output in response to the input u T 1 , resulting in discriminant supervised learning.
Reference: <author> Cacciatore, T. W. and Nowlan, S. J. </author> <year> (1994). </year> <title> Mixtures of controllers for jump linear and non-linear plants. </title> <editor> In Cowan, J. et. al., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This model can also be seen as a recurrent version of the Mixture of Experts architecture (Jacobs et al., 1991), related to the model already proposed in <ref> (Cacciatore and Nowlan, 1994) </ref>. Experiments on artificial tasks (Bengio & Frasconi, 1994a) have shown that EM recurrent learning can deal with long term dependencies more effectively than backpropagation through time and other alternative algorithms.
Reference: <author> Dempster, A. P., Laird, N. M., and Rubin, D. B. </author> <year> (1977). </year> <title> Maximum-likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Stat. Soc. B, </journal> <volume> 39 </volume> <pages> 1-38. </pages>
Reference-contexts: We consider a parametric dynamical system with discrete states and we introduce a modular architecture, with subnetworks associated to discrete states. The architecture can be interpreted as a statistical model and can be trained by the EM or generalized EM (GEM) algorithms <ref> (Dempster et al., 1977) </ref>, considering the internal state trajectories as missing data.
Reference: <author> Giles, C. L., Miller, C. B., Chen, D., Sun, G. Z., Chen, H. H., and Lee, Y. C. </author> <year> (1992). </year> <title> Learning and extracting finite state automata with second-order recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 393-405. </pages>
Reference: <author> Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. </author> <year> (1991). </year> <title> Adaptive mixture of local experts. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 79-87. </pages>
Reference-contexts: This model can also be seen as a recurrent version of the Mixture of Experts architecture <ref> (Jacobs et al., 1991) </ref>, related to the model already proposed in (Cacciatore and Nowlan, 1994). Experiments on artificial tasks (Bengio & Frasconi, 1994a) have shown that EM recurrent learning can deal with long term dependencies more effectively than backpropagation through time and other alternative algorithms. <p> A common feature of these algorithms and the one proposed in this paper is that neural networks are used to extract temporally local information whereas a Markovian system integrates long-term constraints. We can also establish a link between IOHMMs and adaptive mixtures of experts (ME) <ref> (Jacobs et al., 1991) </ref>. Recently, Cacciatore & Nowlan (1994) have proposed a recurrent extension to the ME architecture, called mixture of controllers (MC), in which the gating network has feedback connections, thus allowing to take temporal context into account.
Reference: <author> Levinson, S. E., Rabiner, L. R., and Sondhi, M. M. </author> <year> (1983). </year> <title> An introduction to the application of the theory of probabilistic functions of a markov process to automatic speech recognition. </title> <journal> Bell System Technical Journal, </journal> <volume> 64(4) </volume> <pages> 1035-1074. </pages>
Reference-contexts: In order to iteratively tune parameters with the EM or GEM algorithms, the system propagates forward and backward a discrete distribution over the n states, resulting in a procedure similar to the Baum-Welch algorithm used to train standard hidden Markov models (HMMs) <ref> (Levinson et al., 1983) </ref>. HMMs however adjust their parameters using unsupervised learning, whereas we use EM in a supervised fashion. <p> In several experiments we noticed that convergence can be accelerated with stochastic gradient ascent. 4 COMPARISONS It appears natural to find similarities between the recurrent architecture described so far and standard HMMs <ref> (Levinson et al., 1983) </ref>. The architecture proposed in this paper differs from standard HMMs in two respects: computing style and learning. With IOHMMs, sequences are processed similarly to recurrent networks, e.g., an input sequence can be synchronously transformed into an output sequence.
Reference: <author> Mozer, M. C. </author> <year> (1992). </year> <title> The induction of multiscale temporal structure. </title> <editor> In Moody, J. et. al., eds, </editor> <booktitle> NIPS 4 pages 275-282. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Pollack, J. B. </author> <year> (1991). </year> <title> The induction of dynamical recognizers. </title> <journal> Machine Learning, </journal> <volume> 7(2) </volume> <pages> 196-227. </pages>
Reference: <author> Tomita, M. </author> <year> (1982). </year> <title> Dynamic construction of finite-state automata from examples using hill-climbing. </title> <booktitle> Proc. 4th Cog. Science Conf., </booktitle> <pages> pp. 105-108, </pages> <address> Ann Arbor MI. </address>
Reference-contexts: The model size was chosen using a cross-validation criterion based on performance on 20 randomly generated strings of length T 12. For comparison, in Table 1 we also report for each grammar the number of states of the minimal recognizing FSA <ref> (Tomita, 1982) </ref>. We tested the trained networks on a corpus of 2 13 1 binary strings of length T 12. The final results are summarized in Table 1. The column "Convergence" reports the fraction of trials that succeeded to separate the training set.
Reference: <author> Watrous, R. L. and Kuhn, G. M. </author> <year> (1992). </year> <title> Induction of finite-state languages using second-order recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 406-414. </pages>
References-found: 16

