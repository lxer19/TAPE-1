URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-94-1212/CS-TR-94-1212.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-94-1212/
Root-URL: http://www.cs.wisc.edu
Title: Asynchronous Analysis of Parallel Dynamic Programming Algorithms  
Author: Gary Lewandowski Anne Condon and Eric Bach 
Keyword: Dynamic programming; asynchronous parallel algorithms; parallel models of computation; queueing theory analysis.  
Note: It is likely that the techniques used here can be useful in the analysis of other algorithms that use barriers or pipelining techniques.  Work support by WARF grant 135-3094,  Work supported by NSF grant number CCR-9100886,  Work supported by NSF grant number DCR-855-2596,  
Address: 1210 W. Dayton St. Madison WI 53706  
Affiliation: Computer Sciences Department University of Wisconsin at Madison  
Email: email gary@cs.wisc.edu  email condon@cs.wisc.edu  email bach@cs.wisc.edu  
Date: January 1994  
Abstract: We examine a very simple asynchronous model of parallel computation that assumes the time to compute a task is random, following some probability distribution. The goal of this model is to capture the effects of unpredictable delays on processors, due to communication delays or cache misses, for example. Using techniques from queueing theory and occupancy problems, we use this model to analyze two parallel dynamic programming algorithms. We show that this model is both simple to analyze and accurately predicts which algorithm will perform better in practice. The algorithms we consider are a pipeline algorithm, where each processor i computes in order the entries of rows i, i + p and so on, where p is the number of processors; and a diagonal algorithm, where entries along each diagonal extending from the left to the top of the table are computed in turn. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Almquist, K., R. J. Anderson and E. D. Lazowska. </author> <title> The measured performance of parallel dynamic programming implementations, </title> <booktitle> Proceedings of the International Conference on Parallel Processing, III, </booktitle> <pages> pages 76-79, </pages> <year> 1989. </year>
Reference-contexts: 1) and (i; j 1); 1 i n; 1 j m. (The entries in the first row and column have only 1 predecessor.) Dynamic programming is a classic algorithmic technique (it is used, for example, to solve the Knapsack problem [9], the Longest Common Substring problem [22], queueing network models <ref> [1] </ref>, and DNA sequence alignment [16]). There are many possible parallel algorithms to implement it. The two dynamic programming algorithms we will examine are the pipeline and diagonal algorithms. <p> A processor can compute an entry as soon as its predecessors are computed. (We assume that processors can test whether the predecessors of an entry are 1 already computed, using locks, for example.) Almquist et al. <ref> [1] </ref> used this algorithm in solving the longest common substring problem and a problem on queueing network models. In the diagonal algorithm, entries along each diagonal extending from the left side to the top of the table, are computed in turn.
Reference: [2] <author> Anderson, R. J., P. Beame and W. L. Ruzzo. </author> <title> Low overhead parallel schedules for task graphs, </title> <booktitle> Proceedings of the 2nd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 11-21, </pages> <year> 1990. </year>
Reference-contexts: Our experimental results are reported in Section 5. 1.4 Related Work Various models have been proposed to analytically predict the running time of algorithms on asynchronous models of parallel computation, in which unpredictable delays contribute to the running time. One model, used by Anderson et al. <ref> [2] </ref> in analyzing dynamic programming algorithms on small, asynchronous parallel machines, allows an adversary to control the delays of the processors. Such a model may be useful for predicting the worst case performance of an algorithm.
Reference: [3] <author> Arjas, E. and T. Lehtonen. </author> <title> Approximating many server queues by single server queues, </title> <journal> Mathematics of Operations Research 3, </journal> <pages> pages 205-233, </pages> <year> 1978. </year>
Reference-contexts: Lemma 3.2 For x m, C (k; j) S x (k; j), for any j 1 and 1 k p. The proof of this lemma is a straightforward but tedious application of previous results on stochastic ordering of variables (Arjas and Lehtonen <ref> [3] </ref>); details can be found in the Appendix. Using Lemma 3.2 we get the following upper bound on the expected running time of the pipeline algorithm.
Reference: [4] <author> Cole, R. and O. Zajicek. </author> <title> The expected advantage of asynchrony, </title> <booktitle> Proceedings of the 2nd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 85-94, </pages> <year> 1990. </year>
Reference-contexts: Such a model may be useful for predicting the worst case performance of an algorithm. Another approach is to make the running time of a task in the computation a random variable. Nishimura [18], Cole and Zajicek <ref> [4] </ref> and Martel et al. [15] described general models of asynchronous parallel computation with such random delays. This approach appears to be a promising one, when one wants to estimate the performance of an algorithm on an average run, rather than in the worst case.
Reference: [5] <author> Fortune, S., J. Wylie. </author> <title> Parallelism in random access machines, </title> <booktitle> Proceedings of the 10th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 114-118, </pages> <year> 1978. </year>
Reference-contexts: While experiments show that their performance is generally different on the same input data, synchronous models of parallel computation, such as the PRAM <ref> [5] </ref> [8] [20], give the same time complexity for both algorithms. A useful asynchronous model must capture the difference. It is not obvious which of the two algorithms will perform better.
Reference: [6] <author> Fromm, H., U. Hercksen, U. Herzog, K. John, R. Klar and W. Kleinoder. </author> <title> Experiences with performance measurement and modeling of a processor array, </title> <journal> IEEE Transactions on Computers, </journal> <volume> 32(1), </volume> <pages> pages 15-31, </pages> <year> 1983. </year>
Reference-contexts: Our simple model follows this approach. Our work extends the above results both by providing a sharp analysis of two commonly used algorithms, and by providing empirical evidence that the analysis is realistic. Fromm et al. <ref> [6] </ref> used a stochastic model to analyze the performance of a parallel system, the Erlangen General Processor Array. The time needed to execute an instruction in this system depends on delays due to memory conflicts. Thus, the time for an instruction is modeled as a random variable.
Reference: [7] <author> Glynn, P. W. and Whitt, W. </author> <title> Departures from many queues in series, </title> <type> Technical Report Number 60, </type> <institution> Department of Operations Research, Stanford University. </institution>
Reference-contexts: Our experimental work supports our analysis. In future work, we would like to extend the analysis for the pipeline case for some non-exponential distributions, as in the diagonal algorithm. A theorem by Glynn and Whitt <ref> [7] </ref> can be used to get a very weak upper bound of (mdn=pe)= + O (nm 1a=2 ), when p = fi (m a ), for 0 &lt; a 1.
Reference: [8] <author> Goldschlager, L.M. </author> <title> A unified approach to models of synchronous parallel machines, </title> <booktitle> Proceedings of the 10th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 89-94, </pages> <year> 1978. </year>
Reference-contexts: While experiments show that their performance is generally different on the same input data, synchronous models of parallel computation, such as the PRAM [5] <ref> [8] </ref> [20], give the same time complexity for both algorithms. A useful asynchronous model must capture the difference. It is not obvious which of the two algorithms will perform better.
Reference: [9] <author> Horowitz, E. and S. Sahni. </author> <title> Computing partitions with applications to the knapsack problem, </title> <journal> Journal of the ACM 21, </journal> <pages> pages 277-292, </pages> <year> 1974. </year>
Reference-contexts: predecessors, which are entries (i 1; j); (i 1; j 1) and (i; j 1); 1 i n; 1 j m. (The entries in the first row and column have only 1 predecessor.) Dynamic programming is a classic algorithmic technique (it is used, for example, to solve the Knapsack problem <ref> [9] </ref>, the Longest Common Substring problem [22], queueing network models [1], and DNA sequence alignment [16]). There are many possible parallel algorithms to implement it. The two dynamic programming algorithms we will examine are the pipeline and diagonal algorithms.
Reference: [10] <author> Kruskal, C.P. and A. Weiss. </author> <title> Allocating independent subtasks on parallel processors, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol SE-11, No. 10, </volume> <pages> pages 1001-1016, </pages> <year> 1985. </year>
Reference-contexts: Kruskal and Weiss <ref> [10] </ref> analyzed the expected running time of p processors working on a pool of n subtasks. Each subtask could be done independently. <p> This assumption is sufficient to extend the analysis in the cases where the number of processors used is more than a constant but less than the number of rows in the table. The analysis itself is straightforward, following from results by Kruskal and Weiss <ref> [10] </ref>. They prove a theorem that can be used to asymptotically determine the maximum expected value of the time for p processors to compute K subtasks each, as long as K grows faster than p. <p> The characteristic maximum m p def = inffx : 1 G (x) 1=pg. Lai and Robbins [11] studied the relationship between the characteristic maximum and the expected maximum. Kruskal and Weiss <ref> [10] </ref> proved the following lemma relating the Lai and Robbins results to distributions with an increasing failure rate. 13 Lemma 4.3 If G (x) is IFR then no matter how Y 1 ; : : :; Y p are dependent, lim E (max (Y 1 ; : : :; Y p
Reference: [11] <author> Lai, T.L. and H. Robbins, </author> <title> Maximally dependent random variables, </title> <booktitle> Proceedings of the National Academy Sciences, </booktitle> <volume> vol 73, no. 2, </volume> <pages> pages 286-288, </pages> <year> 1976. </year>
Reference-contexts: Definition 4.2 Let Y 1 ; : : : ; Y p be random variables with the common distribution function G (x) = P (Y x). The characteristic maximum m p def = inffx : 1 G (x) 1=pg. Lai and Robbins <ref> [11] </ref> studied the relationship between the characteristic maximum and the expected maximum.
Reference: [12] <author> Lander, E., J. P. Mesirov and W. Taylor IV. </author> <title> Study of protein sequence comparison metrics on the Connection Machine CM-2, </title> <journal> The Journal of Supercomputing, </journal> <volume> 3, </volume> <pages> pages 255-269, </pages> <year> 1989. </year> <month> 18 </month>
Reference-contexts: Within each diagonal, each processor computes approximately 1=p of the entries. The computation of the entries along a diagonal is not started until all entries along the previous diagonal are computed. This can be accomplished using barriers, for example. Lander et al. <ref> [12] </ref> proposed this parallel algorithm for protein sequence alignment. The difference between the algorithms is that in the diagonal algorithm, all processors are forced to wait at a barrier, whereas in the pipeline algorithm, a processor can compute an entry once its predecessors are computed.
Reference: [13] <author> Lavenberg, S. S. and M. Reiser. </author> <title> Stationary state probabilities at arrival instants for closed queueing networks with multiple types of customers, </title> <journal> Journal of Applied Probability, </journal> <pages> pages 1048-1061, </pages> <year> 1980. </year>
Reference-contexts: cyclic p-customer problem is to determine the expected time for the pth customer to be served s times in a cyclic queueing system with 6 m servers. (See Figure 1.) If the system is in steady state, this problem has a known solution: (s=)(1 + (p 1)=m) (Lavenberg & Reiser <ref> [13] </ref>). To relate the pipeline algorithm to the cyclic p-customer problem, think of the p processors as customers, and the m columns in the n fi m table as servers. <p> We assume that the system is in steady-state and that service times are all exponentially distributed with mean 1=. The following lemma, which follows from a result of Lavenberg and Reiser <ref> [13] </ref>, gives the expected time for a customer to be served s times.
Reference: [14] <author> Mak, V. W. and S. F. Lundstrom. </author> <title> Predicting performance of parallel computations, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(3), </volume> <pages> pages 257-270, </pages> <year> 1990. </year>
Reference-contexts: Different distributions were considered, including constant, exponential and "phase-type" distributions, and the results of the analysis were compared with experimental results. The authors concluded that the analysis using the exponential distribution compared favorably with experimental results. More recently, Mak and Lundstrom <ref> [14] </ref>, described analytic models for predicting the performance of a parallel program represented as a task graph with series-parallel structure, where the time to execute a task is exponentially distributed.
Reference: [15] <author> Martel, C., R. Subramonian and A. Park. </author> <title> Asynchronous PRAMS are (almost) as good as synchronous PRAMS, </title> <booktitle> Proceedings of the 31st Annual Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 590-599, </pages> <year> 1990. </year>
Reference-contexts: Such a model may be useful for predicting the worst case performance of an algorithm. Another approach is to make the running time of a task in the computation a random variable. Nishimura [18], Cole and Zajicek [4] and Martel et al. <ref> [15] </ref> described general models of asynchronous parallel computation with such random delays. This approach appears to be a promising one, when one wants to estimate the performance of an algorithm on an average run, rather than in the worst case. Our simple model follows this approach.
Reference: [16] <author> Needleman, S. B., </author> <title> C.D. Wunsch. A general method applicable to the search for similarity in the amino acid sequence of two proteins, </title> <journal> Journal of Molecular Biology 48, </journal> <pages> pages 443-454, </pages> <year> 1970. </year>
Reference-contexts: 1 i n; 1 j m. (The entries in the first row and column have only 1 predecessor.) Dynamic programming is a classic algorithmic technique (it is used, for example, to solve the Knapsack problem [9], the Longest Common Substring problem [22], queueing network models [1], and DNA sequence alignment <ref> [16] </ref>). There are many possible parallel algorithms to implement it. The two dynamic programming algorithms we will examine are the pipeline and diagonal algorithms. In the pipeline algorithm, when the number of processors is p, the ith processor computes the entries in rows i; i+p; : : :, in order. <p> Diagonal Pipeline 500 0.017 0.026 0.037 0.012 0.056 0.009 0.074 0.006 1500 0.036 0.226 0.069 0.113 0.103 0.077 0.135 0.058 3000 0.076 0.906 0.145 0.453 0.213 0.303 0.280 0.226 Table 3: Synchronization time, in seconds 5.1 Timing Details The application implemented was the Needleman-Wunsch algorithm for aligning two DNA sequences <ref> [16] </ref>. In aligning the two sequences, gaps may be inserted in one or the other of the sequences in order to make the overall alignment better. The best alignment is determined by scoring all possible alignments using a dynamic programming algorithm.
Reference: [17] <author> Newman, D. J. and L. Shepp. </author> <title> The double dixie cup problem. </title> <journal> The American Mathematical Monthly, </journal> <volume> 67, </volume> <pages> pages 58-61, </pages> <year> 1960. </year>
Reference-contexts: If k = ffp and p ! 1 then E [Occ (p; k)] pk (1 + O ( p Proof: 1 and 2 follow directly from results of Newman and Shepp <ref> [17] </ref>. We present the proof of 3. Suppose k = ffp. We first estimate the number, N , of balls needed to ensure that with probability at least 1 1=p, all bins have at least k balls. Suppose that N balls are thrown in the p boxes.
Reference: [18] <author> Nishimura, N. </author> <title> Asynchronous shared memory parallel computation. </title> <booktitle> Proceedings of the 2nd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 76-84, </pages> <year> 1990. </year>
Reference-contexts: Such a model may be useful for predicting the worst case performance of an algorithm. Another approach is to make the running time of a task in the computation a random variable. Nishimura <ref> [18] </ref>, Cole and Zajicek [4] and Martel et al. [15] described general models of asynchronous parallel computation with such random delays. This approach appears to be a promising one, when one wants to estimate the performance of an algorithm on an average run, rather than in the worst case.
Reference: [19] <author> Purdom, P. W. and C. A. Brown. </author> <title> The Analysis of Algorithms, </title> <publisher> Holt, Rinehart and Winston, </publisher> <year> 1985. </year>
Reference: [20] <author> Savitch, W.J., M. Stimson. </author> <title> Time bounded random access machines with parallel processing, </title> <journal> Journal of the ACM, </journal> <volume> 26, </volume> <pages> pages 103-118, </pages> <year> 1979. </year>
Reference-contexts: While experiments show that their performance is generally different on the same input data, synchronous models of parallel computation, such as the PRAM [5] [8] <ref> [20] </ref>, give the same time complexity for both algorithms. A useful asynchronous model must capture the difference. It is not obvious which of the two algorithms will perform better.
Reference: [21] <author> Solomon, F. </author> <title> Residual lifetimes in random parallel systems, </title> <journal> Mathematics Magazine, </journal> <volume> 63(1), </volume> <pages> pages 37-48, </pages> <year> 1990. </year>
Reference-contexts: This is known to be (1=)H j , where H j is the jth harmonic number (Solomon <ref> [21] </ref>). 4.1 Lower Bound We now obtain a lower bound on the expected time of the diagonal algorithm. We first obtain a lower bound on T (p; j).
Reference: [22] <author> Wagner, R.A., M.J. Fischer, </author> <title> The string-to-string correction problem, </title> <journal> Journal of the ACM 21, </journal> <pages> pages 168-173, </pages> <year> 1974. </year>
Reference-contexts: j); (i 1; j 1) and (i; j 1); 1 i n; 1 j m. (The entries in the first row and column have only 1 predecessor.) Dynamic programming is a classic algorithmic technique (it is used, for example, to solve the Knapsack problem [9], the Longest Common Substring problem <ref> [22] </ref>, queueing network models [1], and DNA sequence alignment [16]). There are many possible parallel algorithms to implement it. The two dynamic programming algorithms we will examine are the pipeline and diagonal algorithms.
Reference: [23] <author> Young, D. H. </author> <title> Moment relations for order statistics of the standardized gamma distribution and the inverse multinomial distribution, </title> <journal> Biometrika, </journal> <volume> 58(3), </volume> <pages> pages 637-640, </pages> <year> 1971. </year> <month> 19 </month>
Reference-contexts: Then we obtain the following relationship between the expected values of M (p; k) and Occ (p; k). Lemma 4.1 E [M (p; k)] = 1 p E [Occ (p; k)]. Proof: This result was proved by Young <ref> [23] </ref>. We give a simple proof here. Imagine that the processors compute entries forever; then M (p; k) is the time at which all processors have computed at least k entries. Define an event to be an instant when some processor finishes computing an entry.
References-found: 23

