URL: http://vismod.www.media.mit.edu/~jdavis/OldPapers/visp.ps.Z
Refering-URL: http://vismod.www.media.mit.edu/~jdavis/
Root-URL: http://www.media.mit.edu
Title: To appear in VISION, IMAGE AND SIGNAL PROCESSING. Visual Gesture Recognition  
Author: James Davis and Mubarak Shah 
Keyword: Motion Based Recognition, Motion Tracking, Gesture Interpretation.  
Address: Orlando, FL 32816  
Affiliation: Computer Vision Laboratory University of Central Florida  
Abstract: This paper presents a method for recognizing human-hand gestures using a model-based approach. A finite state machine is used to model four qualitatively distinct phases of a generic gesture. Fingertips are tracked in multiple frames to compute motion trajectories. The trajectories are then used for finding the start and stop position of the gesture. Gestures are represented as a list of vectors and are then matched to stored gesture vector models using table lookup based on vector displacements. Results are presented showing recognition of seven gestures using images sampled at 4Hz on a SPARC-1 without any special hardware. The seven gestures are representatives for actions of Left, Right, Up, Down, Grab, Rotate, and Stop. fl The research reported here was supported by the National Science Foundation grants CDA-9200369 and IRI-9220768. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Cipolla R., Okamoto Y., and Kuno Y. </author> <title> Robust structure from motion using motion parallax. </title> <booktitle> In ICCV, </booktitle> <pages> pages 374-382. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: There are several advantages of this system over other methods. First, it uses inexpensive black-and-white video. Incorporating color markers on a glove as interest points <ref> [1] </ref> requires costly color imaging, whereas a binary marked glove, as used in this research, can be detected in low-cost black-and-white imaging. Second, a simple vision glove is employed, i.e., no mechanical glove with LEDs or bulky wires. <p> With trained users, the recognition rate was 90- to 98%. This system does not use vision to recognize gestures, but instead uses a linked hardware system to track the hand and arm movements, which makes movement less natural for the user. Cipolla, Okamoto, and Kuno <ref> [1] </ref> present a real-time structure-from-motion (SFM) method in which the 3-D visual interpretation of hand gestures is used in a man-machine interface. A glove with colored markers attached is used as input to the vision system. Movement of the hand results in motion between the images of the colored markers.
Reference: [2] <author> Darrell T., and Pentland A. </author> <title> Space-time gestures. </title> <booktitle> In CVPR, </booktitle> <pages> pages 335-340. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: Applications of this system can be similar to the gesture controlled computer-aided presentations of Baudel and Beaudouin-Lafon [6] and also can be used in a video browser with a VCR. Darrell and Pentland <ref> [2] </ref> have also proposed a glove-free environment approach for gesture recognition. Objects are represented using sets of view models, and then are matched to stored gesture patterns using dynamic time warping. Each gesture is dynamically time-warped to make it of the same length as the longest model.
Reference: [3] <author> Costello E. </author> <title> Signing: How to Speak With Your Hands. </title> <publisher> Bantam Books, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: The best example of communication through gestures is given by sign language. American Sign Language (ASL) incorporates the entire English alphabet along with many gestures representing words and phrases <ref> [3] </ref>, which permits people to exchange information in a non-verbal manner. Currently, the human-computer interface is through a keyboard and/or mouse. Physically challenged people may have difficulties with such input devices and may require a new means of entering commands or data into the computer.
Reference: [4] <author> Fukumoto, M., Mase, K., and Suenaga, Y. </author> <title> Real-time detection of pointing actions for a glove-free interface. </title> <booktitle> In IAPR Workshop on Machine Vision Applications, </booktitle> <pages> pages 473-476, </pages> <month> Dec. </month> <pages> 7-9, </pages> <year> 1992. </year>
Reference-contexts: The structure-from-motion method used here assumes rigid objects, which is not true in the case of hand gestures. Fukumoto, Mase, and Suenaga <ref> [4] </ref> present a system called Finger-Pointer which recognizes pointing actions and simple hand forms in real-time. The system uses stereo image sequences and does not require the operator to wear any special glove. It also requires no special image processing hardware.
Reference: [5] <author> Rangarajan, K., and Shah, M. </author> <title> Establishing motion correspondence. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 54 </volume> <pages> 56-73, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Motion correspondence maps points in one image to points in the next image such that no two points are mapped onto the same point. Rangarajan and Shah's <ref> [5] </ref> 8 (c) (d) image with threshold set at dashed line. <p> Using this algorithm, a path, known as a trajectory, is generated for each of the m points, starting with the points in the first image and ending with the points in the nth image. The resultant disjoint paths for each finger together is called the trajectory set <ref> [5] </ref>. <p> image k and image k + 1, 1 p; q; r m; 2 k m 1; q = k1 (p); X k r is the vector from point q in image k to point r in image k + 1, and k X k denotes the magnitude of vector X <ref> [5] </ref>. The first term in the equation represents the smoothness constraint and the second represents the proximity constraint. The algorithm uses three frames to determine the correspondence. The authors assume the correspondence between frame 1 and frame 2 is known. <p> Therefore the initial correspondence can be derived from the location of the points. Rangarajan and Shah's correspondence algorithm is a non-iterative greedy algorithm which keeps the overall proximal smoothness function minimized as much as possible in addition to being fair to each individual assignment <ref> [5] </ref>. 10 6 Gesture Modeling In general, human finger movements are linear, with extrema moving from an extended position down to palm/wrist area, e.g., from the hand in the "hello" position to the hand making a fist.
Reference: [6] <author> Baudel T. and Beaudouin-Lafon M. Charade: </author> <title> Remote control of objects using free-hand gestures. </title> <journal> CACM, </journal> <pages> pages 28-35, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Second, a simple vision glove is employed, i.e., no mechanical glove with LEDs or bulky wires. Current gesture input devices require the user to be linked to the computer, reducing autonomy <ref> [6] </ref>. Vision input overcomes this problem. Third, a duration parameter for gestures is incorporated. <p> Therefore the user can control the execution duration of the robotic arm. Finally, due to finite state machine (FSM) implementation of a generic gesture, no warping of the image sequences is necessary. 2 Related Work Baudel and Beaudouin-Lafon <ref> [6] </ref> implemented a system for the remote control of computer-aided presentations using hand gestures. In this system, the user wears a VPL DataGlove which is linked to the computer. The glove can measure the bending of fingers and the position and orientation of the hand in 3-D space. <p> The system is robust in that it is able to detect the pointing regardless of the operator's pointing style. Applications of this system can be similar to the gesture controlled computer-aided presentations of Baudel and Beaudouin-Lafon <ref> [6] </ref> and also can be used in a video browser with a VCR. Darrell and Pentland [2] have also proposed a glove-free environment approach for gesture recognition. Objects are represented using sets of view models, and then are matched to stored gesture patterns using dynamic time warping.
References-found: 6

