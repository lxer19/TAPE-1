URL: http://www.cs.ucsb.edu/~cfu/papers/irregular95.ps
Refering-URL: http://www.cs.ucsb.edu/Research/rapid_sweb/RAPID.html
Root-URL: http://www.cs.ucsb.edu
Email: fcfu,tyangg@cs.ucsb.edu gerasoulis@cs.rutgers.edu  
Title: Integrating Software Pipelining and Graph Scheduling for Iterative Scientific Computations  
Author: Cong Fu Tao Yang Apostolos Gerasoulis 
Address: Santa Barbara, CA 93106 New Brunswick, NJ 08903  
Affiliation: Dept. of Computer Science Dept. of Computer Science University of California Rutgers University  
Abstract: Graph scheduling has been shown effective for solving irregular problems represented as directed acyclic graphs(DAGs) on distributed memory systems. Many scientific applications can also be modeled as iterative task graphs(ITGs). In this paper, we model the SOR computation for solving sparse matrix systems in terms of ITGs and address the optimization issues for scheduling ITGs when communication overhead is not zero. We present an approach that incorporates techniques of software pipelining and graph scheduling. We demonstrate the effectiveness of our approach in mapping SOR computation and compare it with the multi-coloring method.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> L. Adams, and H. Jordan, </author> <title> Is SOR color-blind?, </title> <journal> SIAM J. Sci. Stat. Comp, </journal> <volume> 7 (1986), </volume> <pages> pp 490-506. </pages>
Reference-contexts: Section 3 addresses some difficulties in the ITG scheduling problem and describes how we can overcome them. Sections 4 discusses our approach and the performance analysis. Section 5 presents the experimental results. 2 Representation of Iterative Task Computation 2.1 The transformed SOR method The Successive-Over-Relaxation (SOR) method <ref> [1] </ref> is an iterative method for solving a matrix system Ax = b where A is n fi n coefficient matrix. <p> While the parallelism in a sparse matrix computation is limited, this algorithm is able to explore a decent amount of parallelism. Our approach for mapping SOR computation is essentially to overlap computation and communication of several SOR iterations. Another approach for par-allelizing SOR-based iterative computation is the multi-coloring method <ref> [1] </ref>. This Fig. 9. The scheduling performance for BCSSTK14 and BCSSTK15. scheme performs variable relabeling to increase the amount of exploitable parallelism since nodes with the same color can be computed concurrently. Adams and Jordan [1] have shown that the multi-coloring method is numerically equivalent to performing several SOR iterations simultaneously <p> Another approach for par-allelizing SOR-based iterative computation is the multi-coloring method <ref> [1] </ref>. This Fig. 9. The scheduling performance for BCSSTK14 and BCSSTK15. scheme performs variable relabeling to increase the amount of exploitable parallelism since nodes with the same color can be computed concurrently. Adams and Jordan [1] have shown that the multi-coloring method is numerically equivalent to performing several SOR iterations simultaneously in the absence of convergence tests. However, in order to preserve inter-color data dependencies, processors must communicate between each computation phase associated with each color.
Reference: 2. <author> A. Aiken and A. Nicolau, </author> <title> Optimal Loop Parallelization, </title> <booktitle> SIGPLAN 88 Conf. on Programming Language Design and Implementation. </booktitle> <address> pp.308-317. </address>
Reference-contexts: Using DAG scheduling algorithms (e.g. [20, 23, 28]) for iterative computation is not feasible since the number of iterations may be too large or may not even be known at compile-time. Software pipelining <ref> [2, 12, 19, 22] </ref> is an important technique proposed for instruction-level loop scheduling on VLIW and superscalar architectures. Loop unrolling or graph unfolding techniques [21] are proposed to explore more parallelism. <p> Compared to the schedule of Fig. 4 (b), this schedule is much longer. The problem with this approach is that the DAG scheduling algorithm does not consider the overlapping of two or more iterations. The idea of exploring parallelism within and across iterations has been proposed in software pipelining <ref> [2, 19] </ref>. In [12], a scheduling algorithm for software pipelining with no communication delay is proposed. We need to extend this result to incorporate communication optimization with load balancing since communication is a major overhead in a message-passing machine.
Reference: 3. <author> F. T. Chong and R. Schreiber, </author> <title> Parallel sparse triangular solution with partitioned inverses and prescheduled DAGs, </title> <type> Tech Report,MIT, </type> <year> 1994. </year>
Reference: 4. <author> P. Chretienne, </author> <title> Task Scheduling over Distributed Memory Machines, </title> <booktitle> Proc. of Inter. Workshop on Parallel and Distributed Algorithms, </booktitle> <publisher> (North Holland, Ed.), </publisher> <year> 1989. </year>
Reference-contexts: The DSC algorithm is able to produce optimal solutions for fork DAGs, join DAGs, coarse grain trees and a class of fine grain trees. Notice that the problem of scheduling general fine grain trees or fork-join DAGs is NP-complete <ref> [4] </ref>. Mapping clusters to processors: If the number of clusters is larger than p, we need to merge clusters. A merging algorithm which considers load balance but also preserves parallelism has been proposed in [23].
Reference: 5. <author> P. Chretienne, </author> <title> Cyclic scheduling with communication delays: a polynomial special case. </title> <month> Dec </month> <year> 1993. </year> <type> Tech Report, </type> <institution> LITP. </institution>
Reference-contexts: We show how software pipelining techniques can be used for mapping iterative task computation on message-passing architectures. The optimal solution for a special class of ITGs has been studied in <ref> [5] </ref>. We are developing heuristic algorithms for general ITGs. Load balancing and communication minimization are the important aspects of optimization. It is known that data locality must be explored for reducing unnecessary communication.
Reference: 6. <author> M. Cosnard and M. Loi, </author> <title> Automatic Task Graph Generation Techniques, </title> <booktitle> Proc. of the Hawaii International Conference on System Sciences, IEEE, </booktitle> <volume> Vol II. </volume> <year> 1995. </year>
Reference: 7. <author> V. Donaldson and J. Ferrante, </author> <title> Determining asynchronous pipeline execution times. </title> <type> Tech. Report, UCSD, </type> <year> 1995. </year>
Reference-contexts: The parallel time is approximately equal to N fl fi p . The goal of the optimization is to minimize fi p . Notice that fi p is uniform for all processors. A non-uniform model is used in <ref> [7] </ref>. We briefly present an algorithm for scheduling ITGs with dependence cycles on p processors. A detailed description of this algorithm and the algorithm for acyclic ITGs are in [29].
Reference: 8. <author> P. Diniz and T. Yang. </author> <title> Efficient Parallelization of Relaxation Iterative Methods for Solving Banded Linear Systems on Multiprocessors, </title> <publisher> TRCS94-15, UCSB. </publisher>
Reference-contexts: However, in order to preserve inter-color data dependencies, processors must communicate between each computation phase associated with each color. This constitutes a drawback in the implementation of the multi-coloring scheme for message-passing machines as communication and computation cannot be overlapped. In <ref> [8] </ref>, we conduct the experiments to compare the multi-coloring approach with our approach in solving banded matrix systems. Table 1 shows the megaflops obtained in nCUBE-2 and Intel Paragon machines (vector units are not used).
Reference: 9. <author> T. H. Dunigan, </author> <title> Performance of the INTEL iPSC/860 and nCUBE 6400 Hypercube, </title> <institution> ORNL/TM-11790, Oak Ridge National Lab., TN, </institution> <year> 1991. </year>
Reference-contexts: Each task T i in an ITG has a computation cost t i and there is a communication cost c i;j for sending a message from task T i at one processor to task T j at another processor <ref> [9, 25] </ref>.
Reference: 10. <author> I. S. Duff, R. G. Grimes and J. G. Lewis, </author> <title> Users' Guide for the Harwell-Boeing Sparse Matrix Collection, </title> <publisher> TR-PA-92-86. </publisher>
Reference-contexts: This is consistent with the previous results [16]. 5 Experiments We have tested the performance of sparse SOR ITGs based on submatrix partitioning. The test matrices are the part of the Harwell-Boeing Test Suites <ref> [10] </ref>. We use matrix BCSSTK14 arising from structural analysis for the roof of Omni Coliseum at Atlanta and matrix BCSSTK15 for Module of an offshore platform. The simulated performance on a nCUBE-2 machine is shown in Fig.9.
Reference: 11. <author> H. Gabow and R. Tarjan, </author> <title> Faster scaling algorithms for network problems, </title> <journal> SIAM J. Computing, </journal> <month> Oct </month> <year> 1989. </year>
Reference-contexts: These inequalities can be solved in a complexity of O ( p log 2 Seq (G) * ) using the shortest path algorithm <ref> [11] </ref> where constant * is the desired accuracy in finding the minimum value fi such that fi fl (G) fi fi fl (G) + *.
Reference: 12. <author> F. Gasperoni and U. </author> <title> Schweigelshohn Scheduling Loops on Parallel Processors: A simple algorithm with close to optimum performance. </title> <booktitle> Proc. of CONPAR 92 , pp. </booktitle> <pages> 613-624. </pages>
Reference-contexts: Using DAG scheduling algorithms (e.g. [20, 23, 28]) for iterative computation is not feasible since the number of iterations may be too large or may not even be known at compile-time. Software pipelining <ref> [2, 12, 19, 22] </ref> is an important technique proposed for instruction-level loop scheduling on VLIW and superscalar architectures. Loop unrolling or graph unfolding techniques [21] are proposed to explore more parallelism. <p> The problem with this approach is that the DAG scheduling algorithm does not consider the overlapping of two or more iterations. The idea of exploring parallelism within and across iterations has been proposed in software pipelining [2, 19]. In <ref> [12] </ref>, a scheduling algorithm for software pipelining with no communication delay is proposed. We need to extend this result to incorporate communication optimization with load balancing since communication is a major overhead in a message-passing machine. <p> ; fi) M OD (ff i ; fi) &lt; t i : For example, edge (C; E) of Fig. 4 (a) is deleted because its edge weight satisfies M OD (ff E ; fi) M OD (ff C ; fi) &lt; t i : This transformation was first proposed in <ref> [12] </ref> for mapping graphs when communication is zero. We can show such a transformation is still valid for our case by carefully designing the ITG schedule in Section 4.5. The dependencies represented by these deleted edges will be satisfied in the final schedule constructed in Section 4.5.
Reference: 13. <author> A. George, M.T. Heath, and J. Liu, </author> <title> Parallel Cholesky Factorization on a Shared Memory Processor, </title> <journal> Lin. Algebra Appl., </journal> <volume> Vol. 77, </volume> <year> 1986, </year> <pages> pp. 165-187. </pages>
Reference-contexts: Mapping clusters to processors: If the number of clusters is larger than p, we need to merge clusters. A merging algorithm which considers load balance but also preserves parallelism has been proposed in [23]. Currently we have used a simple heuristic based on the work profiling method <ref> [13] </ref> which works well in practice. We first compute the total computational weights for each processor and then use this information to map clusters to processors such that computational load is evenly distributed among processors. This method has a complexity of O (v log v).
Reference: 14. <author> A. Gerasoulis, J. Jiao, and T. Yang, </author> <title> A multistage approach to scheduling task graphs. To appear in DIMACS Book Series on Parallel Processing of Discrete Optimization Problems. AMS publisher. Edited by P.M. </title> <editor> Pardalos, K.G. Ramakrishnan, and M.G.C. </editor> <publisher> Resende. </publisher>
Reference-contexts: We have proposed a heuristic algorithm RCP based on the critical-task-first principle and compared its performance with others for randomly generated graphs, and RCP is optimal for fork and join DAGs [27]. This multi-stage approach has been shown practical in mapping large graphs <ref> [14, 28] </ref>. [14] conducted a comparison with a higher complexity ETF method [18] and 1 The size of the graph increases by f after unfolding. <p> We have proposed a heuristic algorithm RCP based on the critical-task-first principle and compared its performance with others for randomly generated graphs, and RCP is optimal for fork and join DAGs [27]. This multi-stage approach has been shown practical in mapping large graphs [14, 28]. <ref> [14] </ref> conducted a comparison with a higher complexity ETF method [18] and 1 The size of the graph increases by f after unfolding.
Reference: 15. <author> A. Gerasoulis, J. Jiao and T. Yang, </author> <title> Scheduling of structured and unstructured computation, To appear in DIMACS Book Series, Workshop on Interconnections Networks and Mappings and Scheduling Parallel Computation, 1994, </title> <editor> Editors: D. Hsu, A. Rosenberg, D. </editor> <publisher> Sotteau. </publisher>
Reference: 16. <author> A. Gerasoulis and T. Yang, </author> <title> On the Granularity and Clustering of Directed Acyclic Task Graphs, </title> <journal> IEEE Trans. on Parallel and Distributed Systems., </journal> <volume> Vol. 4, no. 6, </volume> <month> June </month> <year> 1993, </year> <pages> pp 686-701. </pages>
Reference-contexts: On the other hand, the theorem also indicates that the program partitioning we choose to produce ITGs should not make g (G) too small. This is consistent with the previous results <ref> [16] </ref>. 5 Experiments We have tested the performance of sparse SOR ITGs based on submatrix partitioning. The test matrices are the part of the Harwell-Boeing Test Suites [10].
Reference: 17. <author> G. Huang and W. Ongsakol, </author> <title> An Efficient Task Allocation Algorithm and its use to Parallelize Irregular Gauss-Seidel Type Algorithms, </title> <booktitle> In Proc. of the Eighth International Parallel Processing Symposium, </booktitle> <address> Cancun, Mexico, </address> <year> (1994), </year> <pages> pp. 497-501. </pages>
Reference-contexts: The experiments indicate that our approach that overlaps several SOR iterations outperforms the multi-coloring method when many colors are needed. Banded matrices are one type of sparse matrices. For sparse matrices with irregular distribution of nonzero elements, many colors are usually needed <ref> [17] </ref>. We expect our method performs well but we need to conduct more experiments. 6 Conclusions Our experiments show that the automatic scheduling algorithms for ITGs delivers good performance on the iterative SOR method.
Reference: 18. <author> J. J. Hwang, Y. C. Chow, F. D. Anger, and C. Y. Lee, </author> <title> Scheduling precedence graphs in systems with interprocessor communication times, </title> <journal> SIAM J. Comput., </journal> <pages> pp. 244-257, </pages> <year> 1989. </year>
Reference-contexts: This multi-stage approach has been shown practical in mapping large graphs [14, 28]. [14] conducted a comparison with a higher complexity ETF method <ref> [18] </ref> and 1 The size of the graph increases by f after unfolding. However in practice, we find that f is small, thus f is not included in the complexity term. found that this approach has a much lower complexity but a competitive per-formance. <p> The ETF algorithm <ref> [18] </ref> has a similar performance bound with a complexity O (pv 2 ) while our algorithm has a lower complexity O ((v + e) log v). 4.5 Constructing an ITG schedule The DAG scheduling produces processor assignment P roc (i) and the starting time fl i for each task T i
Reference: 19. <author> M. Lam, </author> <title> Software pipelining: an effective scheduling technique for VLIW machines, </title> <booktitle> ACM Conf. on Programming Language Design and Implementation, </booktitle> <year> 1988, </year> <pages> 318-328. </pages>
Reference-contexts: Using DAG scheduling algorithms (e.g. [20, 23, 28]) for iterative computation is not feasible since the number of iterations may be too large or may not even be known at compile-time. Software pipelining <ref> [2, 12, 19, 22] </ref> is an important technique proposed for instruction-level loop scheduling on VLIW and superscalar architectures. Loop unrolling or graph unfolding techniques [21] are proposed to explore more parallelism. <p> Compared to the schedule of Fig. 4 (b), this schedule is much longer. The problem with this approach is that the DAG scheduling algorithm does not consider the overlapping of two or more iterations. The idea of exploring parallelism within and across iterations has been proposed in software pipelining <ref> [2, 19] </ref>. In [12], a scheduling algorithm for software pipelining with no communication delay is proposed. We need to extend this result to incorporate communication optimization with load balancing since communication is a major overhead in a message-passing machine.
Reference: 20. <author> S.J. Kim and J.C. Browne, </author> <title> A General Approach to Mapping of Parallel Computation upon Multiprocessor Architectures, </title> <booktitle> Proc. of ICPP, 1988, V3, </booktitle> <pages> 1-8. </pages>
Reference-contexts: Mapping weighted iterative task graphs on message-passing architectures requires the exploration of task and loop parallelism by considering both load balancing and communication optimizations. Using DAG scheduling algorithms (e.g. <ref> [20, 23, 28] </ref>) for iterative computation is not feasible since the number of iterations may be too large or may not even be known at compile-time. Software pipelining [2, 12, 19, 22] is an important technique proposed for instruction-level loop scheduling on VLIW and superscalar architectures. <p> We are developing heuristic algorithms for general ITGs. Load balancing and communication minimization are the important aspects of optimization. It is known that data locality must be explored for reducing unnecessary communication. In task computation model <ref> [23, 20] </ref>, exploring data locality means to localize data communication between tasks by assigning them in the same processor so that tasks could exchange data through local memory to avoid high-cost inter-processor communication. We will demonstrate how such a strategy is used in our algorithm. <p> Tasks in the same cluster will be executed in the same processor. 2) Map the clusters into p processors. 3) Order the execution of tasks within each processor. Multi-stage approaches to scheduling have been advocated in the past works, e.g. <ref> [20, 23] </ref> due to its lower complexity and competitive performance. Clustering: The goal of clustering is to identify communication-intensive tasks and analyze the impact of communication on global parallel time to decide if sequentialization or parallelization should be used.
Reference: 21. <author> K. K. Parhi and D. G. Messerschmitt, </author> <title> Static rate-optimal scheduling of iterative dataflow programs via optimum unfolding, </title> <journal> IEEE Trans. on Computers, </journal> <volume> 40:2, </volume> <year> 1991, </year> <pages> pp. 178-195. </pages>
Reference-contexts: Software pipelining [2, 12, 19, 22] is an important technique proposed for instruction-level loop scheduling on VLIW and superscalar architectures. Loop unrolling or graph unfolding techniques <ref> [21] </ref> are proposed to explore more parallelism. Our work has been motivated by the above research work but takes into consideration the characteristics of asynchronous parallelism and the impact of communication. We show how software pipelining techniques can be used for mapping iterative task computation on message-passing architectures. <p> Thus placing two tasks together in one processor could eliminate some communication overhead however it might reduce parallelism. A trade-off between parallelization and communication locality must be addressed. Loop unrolling or graph unfolding techniques <ref> [21] </ref> are proposed to increase the number of tasks within each iteration so that more parallelism can be explored. Let f be the unfolding factor. <p> This new graph is called G f which contains f fl v nodes. And it needs to be executed bN=f c iterations. We use the unfolding algorithm proposed in <ref> [21] </ref> to construct G f .
Reference: 22. <author> R. Reiter, </author> <title> Scheduling parallel computations, </title> <journal> J. of ACM, </journal> <month> Oct </month> <year> 1968, </year> <pages> pp. 590-599. </pages>
Reference-contexts: Using DAG scheduling algorithms (e.g. [20, 23, 28]) for iterative computation is not feasible since the number of iterations may be too large or may not even be known at compile-time. Software pipelining <ref> [2, 12, 19, 22] </ref> is an important technique proposed for instruction-level loop scheduling on VLIW and superscalar architectures. Loop unrolling or graph unfolding techniques [21] are proposed to explore more parallelism. <p> This is the well-known optimal rate (the smallest iteration interval) of the pipelining when the communication cost is zero and there are a sufficient number of processors <ref> [22] </ref>.
Reference: 23. <author> V. Sarkar, </author> <title> Partitioning and Scheduling Parallel Programs for Execution on Multiprocessors, </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: Mapping weighted iterative task graphs on message-passing architectures requires the exploration of task and loop parallelism by considering both load balancing and communication optimizations. Using DAG scheduling algorithms (e.g. <ref> [20, 23, 28] </ref>) for iterative computation is not feasible since the number of iterations may be too large or may not even be known at compile-time. Software pipelining [2, 12, 19, 22] is an important technique proposed for instruction-level loop scheduling on VLIW and superscalar architectures. <p> We are developing heuristic algorithms for general ITGs. Load balancing and communication minimization are the important aspects of optimization. It is known that data locality must be explored for reducing unnecessary communication. In task computation model <ref> [23, 20] </ref>, exploring data locality means to localize data communication between tasks by assigning them in the same processor so that tasks could exchange data through local memory to avoid high-cost inter-processor communication. We will demonstrate how such a strategy is used in our algorithm. <p> Tasks in the same cluster will be executed in the same processor. 2) Map the clusters into p processors. 3) Order the execution of tasks within each processor. Multi-stage approaches to scheduling have been advocated in the past works, e.g. <ref> [20, 23] </ref> due to its lower complexity and competitive performance. Clustering: The goal of clustering is to identify communication-intensive tasks and analyze the impact of communication on global parallel time to decide if sequentialization or parallelization should be used. <p> Notice that the problem of scheduling general fine grain trees or fork-join DAGs is NP-complete [4]. Mapping clusters to processors: If the number of clusters is larger than p, we need to merge clusters. A merging algorithm which considers load balance but also preserves parallelism has been proposed in <ref> [23] </ref>. Currently we have used a simple heuristic based on the work profiling method [13] which works well in practice. We first compute the total computational weights for each processor and then use this information to map clusters to processors such that computational load is evenly distributed among processors.
Reference: 24. <author> V. H. Van Dongen, G. R. Gao and Q. </author> <title> Ning A polynomial time method for optimal software pipelining. </title> <booktitle> Proc. of CONPAR 92, </booktitle> <pages> pp. 613-624. </pages>
Reference: 25. <author> T. Von Eicken, D.E. Culler, S.C. Goldstein, and K. E. Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation, </title> <booktitle> Proc of 19th Int. Sym. on Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: Each task T i in an ITG has a computation cost t i and there is a communication cost c i;j for sending a message from task T i at one processor to task T j at another processor <ref> [9, 25] </ref>.
Reference: 26. <author> T. Yang and A. Gerasoulis. </author> <title> DSC: Scheduling parallel tasks on an unbounded number of processors, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Vol. 5, No. 9, </volume> <pages> 951-967, </pages> <year> 1994. </year>
Reference-contexts: Clustering: The goal of clustering is to identify communication-intensive tasks and analyze the impact of communication on global parallel time to decide if sequentialization or parallelization should be used. We use the Dominant Sequence Clustering Algorithm (DSC) <ref> [26] </ref> for this phase. This algorithm has a complexity of O ((v + e) log v) and it performs a sequence of clustering refinement steps. <p> Then we will try to schedule these two tasks separately through the minimization procedure described in DSC ( <ref> [26] </ref>, page 958). And we will select one of these two tasks that has earlier starting time. If a tie occurs, we will pick the latter one.
Reference: 27. <author> T. Yang and A. Gerasoulis. </author> <title> List scheduling with and without communication. </title> <booktitle> Parallel Computing, V. 19 (1993) pp. </booktitle> <pages> 1321-1344. </pages>
Reference-contexts: Finding a task ordering that minimizes the parallel time is NP-hard even for chains of tasks. We have proposed a heuristic algorithm RCP based on the critical-task-first principle and compared its performance with others for randomly generated graphs, and RCP is optimal for fork and join DAGs <ref> [27] </ref>. This multi-stage approach has been shown practical in mapping large graphs [14, 28]. [14] conducted a comparison with a higher complexity ETF method [18] and 1 The size of the graph increases by f after unfolding.
Reference: 28. <author> T. Yang and A. Gerasoulis, </author> <title> PYRROS: Static Task Scheduling and Code Generation for Message-Passing Multiprocessors, </title> <booktitle> Proc. of 6th ACM Inter. Confer. on Supercomputing, </booktitle> <address> Washington D.C., </address> <year> 1992, </year> <pages> pp. 428-437. </pages>
Reference-contexts: Mapping weighted iterative task graphs on message-passing architectures requires the exploration of task and loop parallelism by considering both load balancing and communication optimizations. Using DAG scheduling algorithms (e.g. <ref> [20, 23, 28] </ref>) for iterative computation is not feasible since the number of iterations may be too large or may not even be known at compile-time. Software pipelining [2, 12, 19, 22] is an important technique proposed for instruction-level loop scheduling on VLIW and superscalar architectures. <p> The starting time fl i and the processor assignment P roc (i) of each task T i are provided. In mapping the kernel DAG, we use two algorithms: one is a multi-stage algorithm designed for the PYRROS system <ref> [28] </ref>, another is a one-stage algorithm. We will choose the smaller one between two solutions produced by these two algorithms. The result of the DAG scheduling is used for constructing the ITG schedule in Sec--tion 4.5. <p> We have proposed a heuristic algorithm RCP based on the critical-task-first principle and compared its performance with others for randomly generated graphs, and RCP is optimal for fork and join DAGs [27]. This multi-stage approach has been shown practical in mapping large graphs <ref> [14, 28] </ref>. [14] conducted a comparison with a higher complexity ETF method [18] and 1 The size of the graph increases by f after unfolding.
Reference: 29. <author> T. Yang, C. Fu, A. Gerasoulis and V. Sarkar, </author> <title> Mapping iterative task graphs on distributed-memory machines, </title> <type> Tech. </type> <note> Report 1995. Part of this report will appear in Proc. of Inter. Conference on Parallel Processing, </note> <year> 1995. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: We will demonstrate how such a strategy is used in our algorithm. In <ref> [29] </ref>, we have also considered a method for executing ITG schedules and provided analytic and experimental results on run-time performance of static schedules when static weights are not estimated accurately. This paper is organized as follows. <p> Notice that fi p is uniform for all processors. A non-uniform model is used in [7]. We briefly present an algorithm for scheduling ITGs with dependence cycles on p processors. A detailed description of this algorithm and the algorithm for acyclic ITGs are in <ref> [29] </ref>. This algorithm first unfolds the graph to increase the number of tasks so that the algorithm has more flexibility in exploring parallelism. Then it transforms the unfolded ITG to a DAG and applies the DAG scheduling techniques. Finally it constructs the ITG schedule based on the DAG schedule. <p> In this way the unnecessary communication could be saved. The complexity of this algorithm is O ((v + e) log v). When there is a sufficient number of processors, the algorithm reaches the optimum for fork, join, coarse grained tree DAGs. Also in <ref> [29] </ref> we show that given a DAG R and p processors, the parallel time produced by this algorithm is bounded by P T (R) (2 1=p + 1=g (R))P T opt (R) where P T opt (R) is the optimal solution for this graph R. <p> For sparse matrices with irregular distribution of nonzero elements, many colors are usually needed [17]. We expect our method performs well but we need to conduct more experiments. 6 Conclusions Our experiments show that the automatic scheduling algorithms for ITGs delivers good performance on the iterative SOR method. In <ref> [29] </ref>, we have presented a method for executing ITG schedules and provided analytic and experimental results to show that run-time performance of static schedules is stable if variations 64 procs n:12,800 nCUBE-2 Paragon width color overlap color overlap 21 24.5 37.3 149 309 81 26.1 43.9 167 537 Width 161, Paragon
References-found: 29

