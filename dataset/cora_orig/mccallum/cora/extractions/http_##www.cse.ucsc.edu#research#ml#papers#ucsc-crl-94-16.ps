URL: http://www.cse.ucsc.edu/research/ml/papers/ucsc-crl-94-16.ps
Refering-URL: http://www.cse.ucsc.edu/research/ml/publications.html
Root-URL: http://www.cse.ucsc.edu
Title: Exponentiated Gradient Versus Gradient Descent for Linear Predictors  
Author: Jyrki Kivinen Manfred K. Warmuth 
Address: Santa Cruz, CA 95064 USA  
Affiliation: Baskin Center for Computer Engineering Information Sciences University of California, Santa Cruz  
Date: June 21, 1994 Revised December 7, 1995  
Pubnum: UCSC-CRL-94-16  
Abstract: We consider two algorithm for on-line prediction based on a linear model. The algorithms are the well-known gradient descent (GD) algorithm and a new algorithm, which we call EG . They both maintain a weight vector using simple updates. For the GD algorithm, the update is based on subtracting the gradient of the squared error made on a prediction. The EG algorithm uses the components of the gradient in the exponents of factors that are used in updating the weight vector multiplicatively. We present worst-case loss bounds for EG and compare them to previously known bounds for the GD algorithm. The bounds suggest that the losses of the algorithms are in general incomparable, but EG has a much smaller loss if only few components of the input are relevant for the predictions. We have performed experiments, which show that our worst-case upper bounds are quite tight already on simple artificial data. 
Abstract-found: 1
Intro-found: 1
Reference: [Ama94] <author> S. Amari. </author> <title> Information geometry of the EM and em algorithms for neural networks. </title> <type> Technical Report METR 94-4, </type> <institution> University of Tokyo, </institution> <address> Tokyo, </address> <year> 1994. </year>
Reference-contexts: This use of a distance measure for obtaining worst-case loss bounds was pioneered by Littlestone's analysis of Winnow [Lit89b], which also employs a variant of the relative entropy. Amari's <ref> [Ama94, Ama95] </ref> approach in using the relative entropy for deriving neural network learning algorithms is similar to the first use we have here for the distance measure. The distance term in the minimized function is also somewhat analogous to regularization terms used in neural network algorithms to avoid overfitting [Hay93]. <p> For small values of , the value p is slightly less than L (y; s x), so the weight vector is made to make only a small corrective movement. This approach to updating a weight vector is similar to the methods introduced by Amari <ref> [Ama94, Ama95] </ref> for more general neural network learning problems. In the next subsections we derive the updates of this paper using the method described above with the distance measures d sq , d re , d reu , and d 2.
Reference: [Ama95] <author> S. Amari. </author> <title> The EM algorithm and information geometry in neural network learning. </title> <journal> Neural Computation, </journal> <volume> 7(1) </volume> <pages> 13-18, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: This use of a distance measure for obtaining worst-case loss bounds was pioneered by Littlestone's analysis of Winnow [Lit89b], which also employs a variant of the relative entropy. Amari's <ref> [Ama94, Ama95] </ref> approach in using the relative entropy for deriving neural network learning algorithms is similar to the first use we have here for the distance measure. The distance term in the minimized function is also somewhat analogous to regularization terms used in neural network algorithms to avoid overfitting [Hay93]. <p> For small values of , the value p is slightly less than L (y; s x), so the weight vector is made to make only a small corrective movement. This approach to updating a weight vector is similar to the methods introduced by Amari <ref> [Ama94, Ama95] </ref> for more general neural network learning problems. In the next subsections we derive the updates of this paper using the method described above with the distance measures d sq , d re , d reu , and d 2.
Reference: [BGV92] <author> B. E. Boser, I. M. Guyon, and V. N. Vapnik. </author> <title> A training algorithm for optimal margin classifiers. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 144-152. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: Then a linear prediction algorithm could actually use a linear combination of the basis functions as its predictor. As an example, we might include all the O (N q ) products of up to q original input variables <ref> [BGV92] </ref>. Assuming that the input variables are in the range [1; 1], this does not increase the L 1 norms of the instances. <p> of the input variables are negative. 9.6 Expanding the instances Our next experiment illustrates the use of linear function learning to learn nonlinear target functions by the means of expanding the instances in such a way that the target function becomes linear for the expanded inputs (see Boser et al. <ref> [BGV92] </ref>). For example, let B (x; q), for q = 1; 2; 3; : : :, be a vector that has as its components all monomials over the variables x i , up to degree k.
Reference: [CBFH + 94] <author> N. Cesa-Bianchi, Y. Freund ,D. Haussler, D. P. Helmbold, R. E. Schapire, and M. K. Warmuth. </author> <title> How to use expert advice. </title> <type> Technical Report UCSC-CRL-94-33, </type> <institution> Univ. of Calif. Computer Research Lab, </institution> <address> Santa Cruz, CA, </address> <year> 1994. </year> <note> An extended abstract appeared in STOC '93. </note>
Reference-contexts: More recently, there has been some work on using an arbitrary finite comparison class P = f p 1 ; : : : ; p N g. Predictors from a finite class are often called experts <ref> [CBFH + 94] </ref>. Note that a finite comparison class can be considered as a comparison class with a very restricted set of linear predictors. <p> Such bounds are even tighter than those of the form (1.2). However, for the absolute loss such bounds were not obtained. Vovk [Vov90] and Littlestone and Warmuth [LW94] had bounds of the form (1.1) for the absolute loss. Later Cesa-Bianchi et al. <ref> [CBFH + 94] </ref> showed how these bounds could be improved to the form (1.2) by a careful choice of certain parameters in Vovk's algorithm. <p> However, if such estimates are not known before the trial sequence begins, it is in some situations possible to use an iterative method, commonly known as the doubling technique <ref> [CBFH + 94, CBLW95] </ref>, for obtaining increasingly accurate estimates as the trial sequence proceeds and modifying the learning rate accordingly. This leads to bounds of the form (5.3), but with slightly worse constant coefficients. <p> More sophisticated conversion methods are given by Cesa-Bianchi et al. <ref> [CBFH + 94] </ref> and Littlestone [Lit89a]. 9 Experimental and theoretical comparison of the algorithms 9.1 Comparison of the worst-case upper bounds In this subsection we compare the worst-case upper bounds given for GD and EG in Theorems 5.3 and 5.11. <p> In practice, these quantities are usually not known, and some other methods must be used to obtain a good learning rate. If only one of the parameters is unknown, there are strategies for guessing its value with increasing accuracy <ref> [CBFH + 94, CBLW95] </ref>. These strategies sometimes lead to loss bounds of the form (5.3), but with the coefficient c 1 and c 2 somewhat larger than the ones obtained in Theorems 5.3 and 5.11 when good values of the parameters are known.
Reference: [CBLW95] <author> N. Cesa-Bianchi, P. Long, and M.K. Warmuth. </author> <title> Worst-case quadratic loss bounds for on-line prediction of linear functions by gradient descent. </title> <journal> IEEE Transactions on Neural Networks, </journal> <note> 1995. To appear. An extended abstract appeared in COLT '93. </note>
Reference-contexts: We have succeeded in the proofs only for the square loss (y t ^y t ) 2 , although the basic ideas of this paper can be phrased for general loss functions. The immediate predecessors of this work are the papers by Cesa-Bianchi et al. <ref> [CBLW95] </ref> and Littlestone et al. [LLW95]. Cesa-Bianchi et al. consider the gradient descent algorithm, or the GD algorithm, for linear predictions. This algorithm is also known as the Widrow-Hoff algorithm and the Least Mean Squares algorithm. It is also one of the main algorithms used in this paper. <p> We now discuss the actual worst-case bounds we can obtain for the GD and EG algorithms. For the GD algorithm, the bounds we cite were already given by Cesa-Bianchi et al. <ref> [CBLW95] </ref>. These include bounds of both the forms (1.1) and (1.2). For the EG algorithm, we give new bounds that are strictly better than those obtained by Littlestone et al. [LLW95] for their algorithm. <p> For the GD algorithm, setting the learning rate suitably results in the bound Loss (GD; S) 2 Loss (u; S) + jjujj 2 2 X 2 that holds for all vectors u 2 R N <ref> [CBLW95] </ref>. To make the coefficient in front of Loss (u; S) equal to 1 and thus obtain a bound of the form (1.2), the algorithm needs before the first trial reasonably good estimates of some characteristics of the whole trial sequence. <p> This leads to a bound that is similar to (1.3) but has slightly larger constant coefficients <ref> [CBLW95] </ref>. For the EG algorithm, it is necessary to give as a parameter an upper bound U for the L 1 norm of the vectors u of the comparison class. <p> The algorithm GD (s; ) has many names, including the Widrow-Hoff algorithm and the Least Mean Square (LMS) algorithm <ref> [CBLW95, WS85] </ref>. The update for GD (s; ) is simply w t+1 = w t 2 (^y t y t )x t : (3:1) The start vector s of the algorithm can be arbitrary. Typically one would choose s = 0. <p> The method is an abstraction of the proof method employed by Littlestone [Lit89b, Lit91] and others <ref> [LLW95, CBLW95] </ref>. In the subsequent subsections, we show how this basic idea can be applied to the specific algorithms introduced in Section 3. We have succeeded in this application only for the square loss function, but we hope it could also be applicable to other loss functions. <p> However, if such estimates are not known before the trial sequence begins, it is in some situations possible to use an iterative method, commonly known as the doubling technique <ref> [CBFH + 94, CBLW95] </ref>, for obtaining increasingly accurate estimates as the trial sequence proceeds and modifying the learning rate accordingly. This leads to bounds of the form (5.3), but with slightly worse constant coefficients. <p> The weaker bounds can be achieved without any additional knowledge. 5.2 Worst-case loss bounds for GD In this subsection we give a streamlined version of the worst-case analysis of the GD algorithm. The analysis was originally presented by Cesa-Bianchi et al. <ref> [CBLW95] </ref>. We start by bounding the loss of the algorithm at a single trial in terms of the loss of a comparison vector u at that trial and the progress of the algorithm towards u. <p> Then either Loss (u + ; S) = K or Loss (u ; S) = K. Since Loss L (A; S) y 2 , we get the stated bound. 2 The special case p = q = 2 of Theorem 6.1 was noted already by Cesa-Bianchi et al. <ref> [CBLW95] </ref>. The lower bound given in Theorem 6.1 for this case coincides with the upper bound given in Theorem 5.3 for the GD algorithm. Hence, the GD algorithm has the best obtainable worst case loss bound. <p> Hence, the GD algorithm has the best obtainable worst case loss bound. Note that in Theorem 6.1, K cannot be made arbitrarily large without also making the absolute value of the outcome arbitrarily large. The following lower bound, also from Cesa-Bianchi et al. <ref> [CBLW95] </ref>, shows that if the number N of dimensions can be arbitrarily large, then again the loss bound for GD is the best possible, even if range of the outcomes is restricted. <p> the EG algorithm, which we call the EGM algorithm, has the update rule w t+1;i = w t;i r t;i =( P j w t;j r t;j ) where r t;i = exp (2 ( ^ y y t ) M t;i ) : (7:3) It has been previously shown <ref> [CBLW95, SW94] </ref> that the GDM algorithm has a loss bound similar to that of GD. Recall that the norm jjAjj 2 for a matrix A is defined as jjAjj 2 = max f jjAxjj 2 j jjxjj 2 = 1 g. <p> In practice, these quantities are usually not known, and some other methods must be used to obtain a good learning rate. If only one of the parameters is unknown, there are strategies for guessing its value with increasing accuracy <ref> [CBFH + 94, CBLW95] </ref>. These strategies sometimes lead to loss bounds of the form (5.3), but with the coefficient c 1 and c 2 somewhat larger than the ones obtained in Theorems 5.3 and 5.11 when good values of the parameters are known.
Reference: [CLR90] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: As noted in Section 4, the algorithms can be motivated by applying a distance measure to the weight vectors maintained by the algorithms. These distance measures will also be useful in proving worst-case loss bounds. They have a role similar to that of potential functions in amortized algorithm analysis <ref> [CLR90] </ref>. <p> Different distance functions lead to radically different algorithms. This framework has been adapted recently [HSSW95] to an unsupervised setting. 3. The distance function also serves in a second role as a potential function in proving worst-case loss bounds by amortized analysis <ref> [CLR90] </ref>. The bounds are first expressed as a function of the learning rate and various norms of the instances and target vectors, as well as the loss of the target vector. Good loss bounds are then obtained by carefully tuning the learning rate.
Reference: [DLR77] <author> A. P. Dempster, N. M. Laird, and D. B Rubin. </author> <title> Maximum-likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> B39:1-38, </volume> <year> 1977. </year>
Reference-contexts: It has also been noticed that in applying the exponentiated gradient update to a certain unsupervised learning problem [HSSW95] the approximation given here leads to a generalization of the Expectation Maximization algorithm <ref> [DLR77] </ref>. Note that the update rule (3.5) maintains the invariant P N i=1 w t;j = 1. However, it may make some of the weights w t+1;i zero or negative.
Reference: [HKW94] <author> D. Haussler, J. Kivinen, and M. K. Warmuth. </author> <title> Tight worst-case loss bounds for predicting with expert advice. </title> <type> Technical Report UCSC-CRL-94-36, </type> <institution> University of California, Santa Cruz, Computer Research Laboratory, </institution> <month> November </month> <year> 1994. </year> <note> Partial results appeared in EuroCOLT '93 and EuroCOLT '95. 53 </note>
Reference-contexts: Some of these results assumed that the outcomes y t must be in f 0; 1 g and were generalized for continuous-valued outcomes y t 2 [0; 1] by Haussler, Kivinen, and Warmuth <ref> [HKW94] </ref>. In this paper, we consider proving bounds of the form (1.2) for a comparison class of general linear predictors, rather than only predictors that choose one of the N components of the instance.
Reference: [Hay91] <author> S. Haykin. </author> <title> Adaptive Filter Theory. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1991. </year> <note> Second edition. </note>
Reference-contexts: However, the bounds always hold. This is in contrast to more common approaches where statistical assumptions about the distribution of the instances and the dependence of the outcomes on the instances are used in order to derive probabilistic loss bounds for the prediction algorithm <ref> [WS85, Hay91] </ref>. The research reported in this paper was inspired by Littlestone [Lit89b, Lit88], who proved worst-case bounds for the case when the comparison class consists of Boolean monomials, or more generally linear threshold functions.
Reference: [Hay93] <author> S. Haykin. </author> <title> Neural Networks: a Comprehensive Foundation. </title> <publisher> Macmillan, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: The distance term in the minimized function is also somewhat analogous to regularization terms used in neural network algorithms to avoid overfitting <ref> [Hay93] </ref>. We now discuss the actual worst-case bounds we can obtain for the GD and EG algorithms. For the GD algorithm, the bounds we cite were already given by Cesa-Bianchi et al. [CBLW95]. These include bounds of both the forms (1.1) and (1.2).
Reference: [HKW95] <author> D. P. Helmbold, J. Kivinen, and M. K. Warmuth. </author> <title> Worst-case loss bounds for sigmoided linear neurons. </title> <booktitle> In Advances in Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press, </publisher> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: For single sigmoided neurons, worst-case bounds have been obtained recently <ref> [HKW95] </ref>. We define the basic notation in Section 2. Our main algorithms are introduced in Section 3, and their derivations using the various distance measures are given in Section 4. In Section 5 we prove our worst-case upper bounds for the losses of the algorithms. <p> We have observed that in the case of linear regression, this leads to improved performance in high dimensional problems if the target weight vector is sparse. We also expect to see similar behavior in more general settings. Recently Helmbold et al. <ref> [HKW95] </ref> were able to prove worst-case loss bounds for single sigmoided linear neurons when the tanh function is used as the sigmoid function and the loss function is the relative entropy loss.
Reference: [HSSW95] <author> D. Helmbold, R. E. Schapire, Y. Singer, and M. K. Warmuth. </author> <title> A comparison of new and old algorithms for a mixture estimation problem. </title> <type> Technical Report UCSC-CRL-95-50, </type> <institution> University of California, Santa Cruz, Computer Research Laboratory, </institution> <month> October </month> <year> 1995. </year> <note> An extended abstract appeared in COLT '95. </note>
Reference-contexts: These properties also hold for the distance measures d reu and d 2 if the vectors w and u are restricted to have only nonnegative components, and for d re if u and w are restricted to be probability vectors. See Helmbold et al. <ref> [HSSW95] </ref> for some plots that visualize the distance measures for probability vectors in the three-dimensional case. 8 Algorithm GD L (s; ) Parameters: L: a loss function from R fi R to [0; 1), s: a start vector in R N , and : a learning rate in [0; 1). <p> However, we shall see in Subsection 4.4 that the update rule (3.5) also has another motivation that is not based on approximating (3.3). It has also been noticed that in applying the exponentiated gradient update to a certain unsupervised learning problem <ref> [HSSW95] </ref> the approximation given here leads to a generalization of the Expectation Maximization algorithm [DLR77]. Note that the update rule (3.5) maintains the invariant P N i=1 w t;j = 1. However, it may make some of the weights w t+1;i zero or negative. <p> Thus, we apply in our algorithms update rules that result from solving (4.3) for w i with various distance measures d. Helmbold et al. <ref> [HSSW95] </ref> give an alternative motivation for (4.3). Recall that our goal is to minimize U (w), and that solving this minimization problem exactly is difficult because both d (w; s) and L (y; w x) depend on w. <p> In a simple unsupervised learning problem for learning mixture coefficients it has been noticed that the distance measure d 2 can also be used to motivate a generalization of the Expectation Maximization (EM) optimization method <ref> [HSSW95] </ref>. In summary, we have seen that there are two different ways of arriving at the same approxi mated EG algorithm. First, one can approximate the exponential function in the update rule of EG. <p> We apply the bound ln (1 q (1 e p )) pq + p 2 =8, which holds for 0 q 1 and p 2 R <ref> [HSSW95, Lemma 1] </ref>. <p> We introduce a common framework for deriving learning algorithms based on the tradeoff between the distance traveled from the current weight vector and a loss function. Different distance functions lead to radically different algorithms. This framework has been adapted recently <ref> [HSSW95] </ref> to an unsupervised setting. 3. The distance function also serves in a second role as a potential function in proving worst-case loss bounds by amortized analysis [CLR90].
Reference: [HW95] <author> D. Helmbold and M. K. Warmuth. </author> <title> On weak learning. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 50(3) </volume> <pages> 551-573, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: If the start vector of the algorithm is equal to u and the learning rate is positive, then the expected loss of the second hypothesis w 2 obviously is higher than that of the initial hypothesis w 1 = u. We conclude this section by presenting a simple method <ref> [HW95] </ref> that can be used for proving expected instantaneous loss bounds for all algorithms and distributions.
Reference: [Hin86] <author> G. E. Hinton. </author> <title> Learning distributed representations of concepts. </title> <booktitle> In Proc. 8th Annual Conf. of the Cognitive Science Society, </booktitle> <pages> pages 1-12, </pages> <address> Hillsdale, 1986. </address> <publisher> Erlbaum. </publisher>
Reference-contexts: This class of algorithms also includes a basic variant of weight decay, where an additional jjw t jj 2 2 error term is used as a penalty for large weights <ref> [Hin86] </ref>. According to a commonly accepted heuristic, the number of examples needed to learn linear functions is roughly proportional to the number of dimensions in the instances. The results presented here do not contradict this in any way.
Reference: [Jum90] <author> G. Jumarie. </author> <title> Relative information. </title> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: The use of the relative entropy as a distance measure is motivated by the Maximum Entropy Principle of Jaynes and the more general Minimum Relative Entropy Principle of Kullback. These fundamental principles have many applications in Information Theory, Physics and Economics (See Kapur and Kesavan [KK92] and Jumarie <ref> [Jum90] </ref> for an overview). 3 For our work it is central that the distance measure is used in two different ways: first, it motivates the update rule, and second, it is applied as a tool in the analysis of the algorithm thus obtained. <p> Good loss bounds are then obtained by carefully tuning the learning rate. In this paper we are clearly championing the EG algorithm derived from the relative entropy distance measure. The use of this distance measure is motivated by the Minimum Relative Entropy Principle of Kullback <ref> [KK92, Jum90] </ref>. The resulting new algorithm EG learns very well when the target is sparse and the components of the instances are in a small range.
Reference: [KK92] <author> J. N. Kapur and H. </author> <title> K Kesavan. Entropy Optimization Principles with Applications. </title> <publisher> Academic Press, Inc., </publisher> <year> 1992. </year>
Reference-contexts: The use of the relative entropy as a distance measure is motivated by the Maximum Entropy Principle of Jaynes and the more general Minimum Relative Entropy Principle of Kullback. These fundamental principles have many applications in Information Theory, Physics and Economics (See Kapur and Kesavan <ref> [KK92] </ref> and Jumarie [Jum90] for an overview). 3 For our work it is central that the distance measure is used in two different ways: first, it motivates the update rule, and second, it is applied as a tool in the analysis of the algorithm thus obtained. <p> Good loss bounds are then obtained by carefully tuning the learning rate. In this paper we are clearly championing the EG algorithm derived from the relative entropy distance measure. The use of this distance measure is motivated by the Minimum Relative Entropy Principle of Kullback <ref> [KK92, Jum90] </ref>. The resulting new algorithm EG learns very well when the target is sparse and the components of the instances are in a small range.
Reference: [KSS94] <author> Michael J. Kearns, Robert E. Schapire, and Linda M. Sellie. </author> <title> Toward efficient agnostic learning. </title> <journal> Machine Learning, </journal> 17(2/3):115-142, 1994. 
Reference-contexts: To set a reasonable goal, we measure the performance of the algorithm against the performances of predictors from some fixed comparison class P . (The comparison class is analogous to the touchstone class of the agnostic PAC model of learning <ref> [KSS94] </ref>.) The algorithm is required to perform well if at least one predictor from the comparison class performs well.
Reference: [Lit88] <author> N. Littlestone. </author> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: The research reported in this paper was inspired by Littlestone <ref> [Lit89b, Lit88] </ref>, who proved worst-case bounds for the case when the comparison class consists of Boolean monomials, or more generally linear threshold functions. <p> This family includes the Perceptron algorithm for thresholded linear functions, the GD algorithm 52 for linear functions, the standard back-propagation algorithm for multilayer sigmoid networks, and the Linear Least Squares algorithm for fitting a line to data points. The new family includes, respectively, the Winnow algorithm <ref> [Lit88] </ref>, the EG algorithm, the exponentiated back-propagation algorithm, and an algorithm for fitting a line to data points so that the relative entropy of the coefficient vector is minimized. The new family uses a new bias, which favors sparse weight vectors.
Reference: [Lit89a] <author> N. Littlestone. </author> <title> From on-line to batch learning. </title> <booktitle> In Proc. 2nd Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 269-284, </pages> <address> San Mateo, CA, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: More sophisticated conversion methods are given by Cesa-Bianchi et al. [CBFH + 94] and Littlestone <ref> [Lit89a] </ref>. 9 Experimental and theoretical comparison of the algorithms 9.1 Comparison of the worst-case upper bounds In this subsection we compare the worst-case upper bounds given for GD and EG in Theorems 5.3 and 5.11.
Reference: [Lit89b] <author> N. Littlestone. </author> <title> Mistake Bounds and Logarithmic Linear-threshold Learning Algorithms. </title> <type> PhD thesis, Technical Report UCSC-CRL-89-11, </type> <institution> University of California Santa Cruz, </institution> <year> 1989. </year>
Reference-contexts: The research reported in this paper was inspired by Littlestone <ref> [Lit89b, Lit88] </ref>, who proved worst-case bounds for the case when the comparison class consists of Boolean monomials, or more generally linear threshold functions. <p> This use of a distance measure for obtaining worst-case loss bounds was pioneered by Littlestone's analysis of Winnow <ref> [Lit89b] </ref>, which also employs a variant of the relative entropy. Amari's [Ama94, Ama95] approach in using the relative entropy for deriving neural network learning algorithms is similar to the first use we have here for the distance measure. <p> The method is an abstraction of the proof method employed by Littlestone <ref> [Lit89b, Lit91] </ref> and others [LLW95, CBLW95]. In the subsequent subsections, we show how this basic idea can be applied to the specific algorithms introduced in Section 3.
Reference: [Lit91] <author> N. Littlestone. </author> <title> Redundant noisy attributes, attribute errors, and linear threshold learning using Winnow. </title> <booktitle> In Proc. 4th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 147-156, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The method is an abstraction of the proof method employed by Littlestone <ref> [Lit89b, Lit91] </ref> and others [LLW95, CBLW95]. In the subsequent subsections, we show how this basic idea can be applied to the specific algorithms introduced in Section 3.
Reference: [LLW95] <author> N. Littlestone, P. M. Long, and M. K. Warmuth. </author> <title> On-line learning of linear functions. </title> <journal> Journal of Computational Complexity, </journal> <volume> 5 </volume> <pages> 1-23, </pages> <year> 1995. </year>
Reference-contexts: The immediate predecessors of this work are the papers by Cesa-Bianchi et al. [CBLW95] and Littlestone et al. <ref> [LLW95] </ref>. Cesa-Bianchi et al. consider the gradient descent algorithm, or the GD algorithm, for linear predictions. This algorithm is also known as the Widrow-Hoff algorithm and the Least Mean Squares algorithm. It is also one of the main algorithms used in this paper. <p> Choosing the learning rate is nontrivial and can significantly affect the performance of the algorithm. We also introduce a new on-line prediction algorithm, which we call the exponentiated gradient algorithm, or the EG algorithm. The EG algorithm is closely related to the algorithm given by Littlestone et al. <ref> [LLW95] </ref>. The EG algorithm also has a weight vector w t and predicts with ^y t = w t x t . The update rule is w t+1;i = P N ; where r t;j = e 2 (^y t y t )x t;j for some positive learning rate . <p> For the GD algorithm, the bounds we cite were already given by Cesa-Bianchi et al. [CBLW95]. These include bounds of both the forms (1.1) and (1.2). For the EG algorithm, we give new bounds that are strictly better than those obtained by Littlestone et al. <ref> [LLW95] </ref> for their algorithm. In particular, we also have bounds of the form (1.2), whereas Littlestone et al. [LLW95] had only bounds of the form (1.1). <p> These include bounds of both the forms (1.1) and (1.2). For the EG algorithm, we give new bounds that are strictly better than those obtained by Littlestone et al. <ref> [LLW95] </ref> for their algorithm. In particular, we also have bounds of the form (1.2), whereas Littlestone et al. [LLW95] had only bounds of the form (1.1). The importance of considering both the algorithms GD and EG comes from the fact that for these algorithms, the constants hidden by the notation in (1.1) and (1.2) are quite different. <p> For other examples of reductions of this type, see Littlestone et al. <ref> [LLW95] </ref>. The parameters of EG are a loss function L, a scaling factor U , a pair (s + ; s ) of start vectors in [0; 1] N with P N i + s i ) = 1, and a learning rate . <p> The method is an abstraction of the proof method employed by Littlestone [Lit89b, Lit91] and others <ref> [LLW95, CBLW95] </ref>. In the subsequent subsections, we show how this basic idea can be applied to the specific algorithms introduced in Section 3. We have succeeded in this application only for the square loss function, but we hope it could also be applicable to other loss functions. <p> Similar bounds were earlier proven by Littlestone et al. <ref> [LLW95] </ref> for their algorithm, which is related to ours but does not have an analogous derivation. Our bounds are lower than those of Littlestone et al. In particular, we have bounds of the form (5.3), which seem unobtainable for the algorithm of Littlestone et al. <p> We consider the upper bounds for the simpler EG algorithm, from which the bounds for EG are obtained via a reduction. If we were able to improve the bounds for EG, then an improvement for EG would automatically follow. The following result of Littlestone et al. <ref> [LLW95] </ref> shows that in the case Loss (u; S) = 0, a factor ln N in the loss of the algorithm cannot be avoided. For simplicity, we consider only the case x t 2 [0; 1] N . <p> We could easily generalize result for the more general algorithm EG when it is applied to matrices. In the noise-free case K = 0, similar results were given by Littlestone et al. <ref> [LLW95] </ref>. The reduction could also be applied to the GD algorithm to obtain Theorem 7.1. <p> Hence, applying linear least squares prediction in an on-line manner in this situation results in the same large loss as shown for GD in the form w t+1 = P t i=1 a i x i can have smaller loss in this situation <ref> [LLW95] </ref>. This class of algorithms also includes a basic variant of weight decay, where an additional jjw t jj 2 2 error term is used as a penalty for large weights [Hin86].
Reference: [LW94] <author> N. Littlestone and M. K. Warmuth. </author> <title> The weighted majority algorithm. </title> <journal> Information and Computation, </journal> <volume> 108(2) </volume> <pages> 212-261, </pages> <year> 1994. </year>
Reference-contexts: Such bounds are even tighter than those of the form (1.2). However, for the absolute loss such bounds were not obtained. Vovk [Vov90] and Littlestone and Warmuth <ref> [LW94] </ref> had bounds of the form (1.1) for the absolute loss. Later Cesa-Bianchi et al. [CBFH + 94] showed how these bounds could be improved to the form (1.2) by a careful choice of certain parameters in Vovk's algorithm.
Reference: [Lue84] <author> D. G. Luenberger. </author> <title> Linear and Nonlinear Programming. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1984. </year>
Reference-contexts: If we use the squared Euclidean distance together with the constraint that the weights w i must sum to one, we obtain an algorithm GP L that is known as the gradient projection algorithm <ref> [Lue84] </ref>.
Reference: [Roy63] <author> H. Royden. </author> <title> Real Analysis. </title> <publisher> Macmillan, </publisher> <address> New York, NY, </address> <year> 1963. </year>
Reference-contexts: Note that L p and L q are dual norms if 1=p + 1=q = 1 <ref> [Roy63] </ref>. Hence, the L 1 norm used for the comparison vectors and the L 1 norm used for the instances in the bounds for the EG algorithm are dual. <p> If the norms L p and L q are dual, then the Cauchy-Schwartz Inequality can be generalized to show that jjujj p U and jjxjj q X together imply ju xj U X <ref> [Roy63] </ref>. 34 Theorem 6.1: Let p; q 2 R + [ f 1 g. Let A be an arbitrary on-line prediction algorithm, and let K, U , and X be arbitrary positive reals.
Reference: [SW94] <author> R. E. Schapire and M. K. Warmuth. </author> <title> On the worst-case analysis of temporal-difference learing algorithms. </title> <booktitle> In Proc. 11th International Conf. on Machine Learning, </booktitle> <pages> pages 266-274, </pages> <address> San Francisco, CA, </address> <month> July </month> <year> 1994. </year> <note> Morgan Kaufmann. To appear in Machine Learning. </note>
Reference-contexts: the EG algorithm, which we call the EGM algorithm, has the update rule w t+1;i = w t;i r t;i =( P j w t;j r t;j ) where r t;i = exp (2 ( ^ y y t ) M t;i ) : (7:3) It has been previously shown <ref> [CBLW95, SW94] </ref> that the GDM algorithm has a loss bound similar to that of GD. Recall that the norm jjAjj 2 for a matrix A is defined as jjAjj 2 = max f jjAxjj 2 j jjxjj 2 = 1 g. <p> The upper bound of Theorem 7.1 can be shown to be tight <ref> [SW94] </ref>. We now give a similar result for the EGM algorithm. The proof is based on a reduction that allows us to apply directly the upper bound given in Theorem 5.10 for the EG algorithm.
Reference: [Vov90] <author> V. Vovk. </author> <title> Aggregating strategies. </title> <booktitle> In Proc. 3rd Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 371-383. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: The number of dimensions is now the number of experts, i.e., the size N of the original comparison class. Vovk <ref> [Vov90] </ref> proved that for a large class of loss functions, a simple algorithm achieves bounds of the form Loss L (A; S) inf u2U Loss L (u; S) + c log N , where the constant c depends only on the loss function. <p> Such bounds are even tighter than those of the form (1.2). However, for the absolute loss such bounds were not obtained. Vovk <ref> [Vov90] </ref> and Littlestone and Warmuth [LW94] had bounds of the form (1.1) for the absolute loss. Later Cesa-Bianchi et al. [CBFH + 94] showed how these bounds could be improved to the form (1.2) by a careful choice of certain parameters in Vovk's algorithm. <p> a simpler situation, where the learner is not trying to learn a linear function but merely to pick out the best single component of the instances for predicting the outcomes, it has been possible to use a similar approach to prove bounds for a very general class of loss functions <ref> [Vov90] </ref>. As noted in Section 4, the algorithms can be motivated by applying a distance measure to the weight vectors maintained by the algorithms. These distance measures will also be useful in proving worst-case loss bounds.
Reference: [WS85] <author> B. Widrow and S. Stearns. </author> <title> Adaptive Signal Processing. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1985. </year> <month> 54 </month>
Reference-contexts: However, the bounds always hold. This is in contrast to more common approaches where statistical assumptions about the distribution of the instances and the dependence of the outcomes on the instances are used in order to derive probabilistic loss bounds for the prediction algorithm <ref> [WS85, Hay91] </ref>. The research reported in this paper was inspired by Littlestone [Lit89b, Lit88], who proved worst-case bounds for the case when the comparison class consists of Boolean monomials, or more generally linear threshold functions. <p> The algorithm GD (s; ) has many names, including the Widrow-Hoff algorithm and the Least Mean Square (LMS) algorithm <ref> [CBLW95, WS85] </ref>. The update for GD (s; ) is simply w t+1 = w t 2 (^y t y t )x t : (3:1) The start vector s of the algorithm can be arbitrary. Typically one would choose s = 0.
References-found: 28

