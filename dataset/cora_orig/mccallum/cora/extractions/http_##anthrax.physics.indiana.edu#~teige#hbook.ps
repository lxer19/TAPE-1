URL: http://anthrax.physics.indiana.edu/~teige/hbook.ps
Refering-URL: http://anthrax.physics.indiana.edu/~teige/Manuals.html
Root-URL: http://www.cs.indiana.edu
Title: CERN Program Library Long Writeup Y250 Reference Manual Version 4.20 Application Software Group  
Affiliation: Computing and Networks Division CERN Geneva, Switzerland  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> L. Lamport. </author> <title> L A T E X A Document Preparation System. </title> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: If not they are taken to be equal to the square-root of the contents 1 weights are equal to 1 T This switch controls the system-defined elementary functions set to use 0 Monomials will be selected as the standard elementary functions 1 Chebyshev polynomials with a definition region: <ref> [1; 1] </ref> 2 Legendre polynomials with a definition region: [1; 1] 3 shifted Chebyshev polynomials with a definition region: [0; 1] 4 Laguerre polynomials with a definition region: [0; +1] 5 Hermite polynomials with a definition region: [1; +1] B This switch controls the kind of regressor used in the fit <p> equal to the square-root of the contents 1 weights are equal to 1 T This switch controls the system-defined elementary functions set to use 0 Monomials will be selected as the standard elementary functions 1 Chebyshev polynomials with a definition region: <ref> [1; 1] </ref> 2 Legendre polynomials with a definition region: [1; 1] 3 shifted Chebyshev polynomials with a definition region: [0; 1] 4 Laguerre polynomials with a definition region: [0; +1] 5 Hermite polynomials with a definition region: [1; +1] B This switch controls the kind of regressor used in the fit 0 Regressors are products of standard polynomials (see preceeding <p> equal to 1 T This switch controls the system-defined elementary functions set to use 0 Monomials will be selected as the standard elementary functions 1 Chebyshev polynomials with a definition region: [1; 1] 2 Legendre polynomials with a definition region: [1; 1] 3 shifted Chebyshev polynomials with a definition region: <ref> [0; 1] </ref> 4 Laguerre polynomials with a definition region: [0; +1] 5 Hermite polynomials with a definition region: [1; +1] B This switch controls the kind of regressor used in the fit 0 Regressors are products of standard polynomials (see preceeding switch) 1 Regressors are products of user-defined elementary functions. <p> selected as the standard elementary functions 1 Chebyshev polynomials with a definition region: [1; 1] 2 Legendre polynomials with a definition region: [1; 1] 3 shifted Chebyshev polynomials with a definition region: [0; 1] 4 Laguerre polynomials with a definition region: [0; +1] 5 Hermite polynomials with a definition region: <ref> [1; +1] </ref> B This switch controls the kind of regressor used in the fit 0 Regressors are products of standard polynomials (see preceeding switch) 1 Regressors are products of user-defined elementary functions. <p> a regressor included at one time will always remain in the regression later on 2 Fixed-term regression: all selected regressors will appear in the final expression N This switch controls the normalization of the X range during the computation. 0 No normalization of X-range 1 X scaled in the range <ref> [1; 1] </ref> 2 X scaled in the range [0; 1] 3 X scaled in the range [0; +1] The value of R2MIN is used to determine the satisfactory end of the fitting process. <p> remain in the regression later on 2 Fixed-term regression: all selected regressors will appear in the final expression N This switch controls the normalization of the X range during the computation. 0 No normalization of X-range 1 X scaled in the range [1; 1] 2 X scaled in the range <ref> [0; 1] </ref> 3 X scaled in the range [0; +1] The value of R2MIN is used to determine the satisfactory end of the fitting process.
Reference: [2] <author> CNAS Group. </author> <title> HIGZ/HPLOT Users Guide, Program Library Q120 and Y251. </title> <publisher> CERN, </publisher> <year> 1993. </year>
Reference-contexts: The similarities in experimental devices and problems, and the close collaboration, favour the adoption of common software methodologies that sometimes develop into widely used standard packages. Examples are the histograming, fitting and data presentation package HBOOK, its graphic interface HPLOT <ref> [2] </ref> and the Physics Analysis Workstation (PAW) system [3], which have been developed at CERN. HBOOK is a subroutine package to handle statistical distributions (histograms and Ntuples) in a Fortran scientific computation environment. <p> The call to HPLINT initialises HPLOT and HPLCAP redirects the metafile output to unit 10. The parameters given to HPLOT instruct the program to output all histograms in the current working directory to the metafile using standard option, while HPLEND closes the metafile. See the HPLOT user's guide <ref> [2] </ref> for more details. The result of the job and the resulting PostScript file can be compared to the lineprinter output in Figure 1.4.
Reference: [3] <author> R. Brun, O. Couet, C. Vandoni, and P. Zanarini. </author> <title> PAW users guide, Program Library Q121. </title> <publisher> CERN, </publisher> <year> 1991. </year> <title> [4] various authors. SUMX User's Manual Program Library Y200. </title> <publisher> CERN, </publisher> <year> 1976. </year> <note> 1973 (Rev. </note> <year> 1976). </year>
Reference-contexts: The similarities in experimental devices and problems, and the close collaboration, favour the adoption of common software methodologies that sometimes develop into widely used standard packages. Examples are the histograming, fitting and data presentation package HBOOK, its graphic interface HPLOT [2] and the Physics Analysis Workstation (PAW) system <ref> [3] </ref>, which have been developed at CERN. HBOOK is a subroutine package to handle statistical distributions (histograms and Ntuples) in a Fortran scientific computation environment. It presents results graphically on the line printer, and can optionally draw them on graphic output devices via the HPLOT package. <p> One possibility is to create and fill 200 histograms on an event-by-event basis while reading the DST. An alternative solution, particularly interesting during interactive data analysis with the data presentation system PAW <ref> [3] </ref>, is to create one Ntuple.
Reference: [5] <author> R. Brun, M. Hansroul, and P. Palazzi. </author> <title> HBOOK users guide (Version 1.2), Program Library Y250. </title> <publisher> CERN, </publisher> <year> 1973. </year>
Reference-contexts: Therefore it was not always very practical to run several times through the data and a more lightweight system HBOOK <ref> [5, 6] </ref>, easier to learn and use, was soon developed. It was in the middle seventies, when larger proton and electron accelerators became available, that counter experiments definitively superseded bubble chambers and with them the amount of data to be treated was now in the multi megabyte range.
Reference: [6] <author> R. Brun, M. Hansroul, P. Palazzi, and B. Peuchot. </author> <title> HBOOK users guide (Version 2), Program Library Y250 and DD/75/11. </title> <publisher> CERN, </publisher> <year> 1975. </year>
Reference-contexts: Therefore it was not always very practical to run several times through the data and a more lightweight system HBOOK <ref> [5, 6] </ref>, easier to learn and use, was soon developed. It was in the middle seventies, when larger proton and electron accelerators became available, that counter experiments definitively superseded bubble chambers and with them the amount of data to be treated was now in the multi megabyte range.
Reference: [7] <author> R. Brun, I. Ivanchencko, and P. Palazzi. </author> <title> HBOOK users guide (Version 3), Program Library Y250 and DD/77/9. </title> <publisher> CERN, </publisher> <year> 1977. </year>
Reference-contexts: Then, to make the analysis more manageable, various physicists would write their own mini-DST, with a reduced fraction of the information from the DST. They would run these (m,)DSTs through HBOOK, whose functionality had increased substantially in the meantime <ref> [7, 8] </ref>. Hence various tens of one- or two-dimensional histograms would be booked in the initialization phase and the interesting parameters would be read sequentially from the DST and be binned in the histograms or scatter plots.
Reference: [8] <author> R. Brun, I. Ivanchencko, D. Lienart, and P. Palazzi. </author> <note> HBOOK users guide (Version 3 Revised), </note> <institution> Program Library Y250 and DD/EE/81-1. CERN, </institution> <year> 1984. </year>
Reference-contexts: Then, to make the analysis more manageable, various physicists would write their own mini-DST, with a reduced fraction of the information from the DST. They would run these (m,)DSTs through HBOOK, whose functionality had increased substantially in the meantime <ref> [7, 8] </ref>. Hence various tens of one- or two-dimensional histograms would be booked in the initialization phase and the interesting parameters would be read sequentially from the DST and be binned in the histograms or scatter plots.
Reference: [9] <author> R. Brun and D. Lienart. </author> <title> HBOOK users guide (Version 4), Program Library Y250. </title> <publisher> CERN, </publisher> <year> 1987. </year>
Reference-contexts: Doing this was very efficient memory wise (although 2-dim. histograms could still be very costly), but of course all correlations, not explicitly plotted, were lost. 1 2 Chapter 1. Introduction HBOOK in those days still had its own memory management, but with version 4 <ref> [9] </ref>, which became available in 1984, the ZEBRA data memory manager was introduced. This not only allowed the use of all memory managament facilities of ZEBRA, but at the same time it became possible to use the sequential FZ and random access RZ [10] input-output possiblities of that system.
Reference: [10] <author> R. Brun, M. Goossens, and J. Zoll. </author> <title> ZEBRA Users Guide, Program Library Q100. </title> <publisher> CERN, </publisher> <year> 1993. </year>
Reference-contexts: This not only allowed the use of all memory managament facilities of ZEBRA, but at the same time it became possible to use the sequential FZ and random access RZ <ref> [10] </ref> input-output possiblities of that system. This allows histograms to be saved and transferred to other systems in an easy way. At about the same time Ntuples, somewhat similar in functionality to events as written on a miniDST were implemented. <p> Unfortunately, given the way Fortran works and although the package is structured as much as possible in the sense of selective loading, some unused subroutines will usually be present. 4 Chapter 1. Introduction HBOOK uses the ZEBRA <ref> [10] </ref> data structure management package to manage its memory (see chapter 8). The working space of HBOOK is an array, allocated to the labelled common /PAWC/. In ZEBRA terms this is a ZEBRA store. <p> In fact the first task of a HBOOK user is to declare the length of this common to ZEBRA by a call to HLIMIT, as is seen in figures 1.2 and 1.4 In the /PAWC/ data store, the HBOOK, HIGZ and KUIP packages have all their own division (see <ref> [10] </ref> for more details on the notion of divisions) as follows (see figure 8.1): LINKS Some locations at the beginning of /PAWC/ for ZEBRA pointers. WORKS Working space (or division 1) used by the various packages storing information in /PAWC/ HBOOK Division 2 of the store. Reserved to HBOOK. <p> Memory Management and input/output Routines 8.4 Input/Output Routines HBOOK files are in fact ZEBRA RZ files <ref> [10] </ref>. Input/output error return codes are available vin the ZEBRA communication vector IQUEST, and the user should consult the RZ manual for their meaning. Both disk and memory resident files are supported, the latter being particularly useful in online applications, where histogram data have to be shared between different processes.
Reference: [11] <author> W. T. Eadie, D. Drijard, F. James, M. Roos, and B. Sadoulet. </author> <title> Statistical Methods in Experimental Physics. </title> <publisher> North-Holland, </publisher> <year> 1971. </year>
Reference-contexts: HDIFF will also refuse to compare 2-dimensional histograms if there is saturation, since it does not have enough information in this case. 6.2.2 Statistical Considerations The Kolmogorov Test The calculations in routine HDIFF are based on the Kolmogorov Test (See, e.g. <ref> [11] </ref>, pages 269-270). It is usually superior to the better-known Chisquare Test for the following reasons: It does not require a minimum number of events per bin, and in fact it is intended for unbinned data (this is discussed below). <p> Statistical interpretation: For discussuion of basic concepts, such as the meaning of the elements of the error matrix, or setting of exact confidence levels (see <ref> [11] </ref>). Reliability of MINUIT error estimates. MINUIT always carries around its own current estimates of the parameter errors, which it will print out on request, no matter how accurate they are at any given point in the execution.
Reference: [12] <author> F. James and M. Roos. </author> <title> MINUIT Users Guide, Program Library D506. </title> <publisher> CERN, </publisher> <year> 1981. </year>
Reference-contexts: Fewer that 25 or so events will result in DIFFS values which are too large. The result is that HDIFFB rejects too many event in these low statistic cases. Chapter 7: Fitting, parameterization and smoothing 7.1 Fitting The fitting routines in HBOOK are based on the Minuit package <ref> [12] </ref>. Minuit is conceived as a tool to find the minimum value of a multi-parameter function and analyze the shape of the function around the minimum.
Reference: [13] <author> J. H. Friedman. </author> <title> Data Analysis Techniques for High Energy Particle Physics. </title> <booktitle> In CERN, editor, Proceedings of the 1974 CERN School of Computing, </booktitle> <address> Godoysund (Norway), </address> <year> 1974. </year> <type> CERN Report 74-23. </type>
Reference-contexts: To filter out the statistical fluctuations, smoothing algorithms can be applied. Two such techniques are implemented in HBOOK, the so called 353QH (HSMOOF) and the method of B-splines (HSPLI1, HSPLI2, HSPFUN). Before trying them out references <ref> [13, 14, 15] </ref> should be consulted, and results taken with care. CALL HSMOOF (ID,ICASE,CHI2*) Action: Routine to smooth a 1-dimensional histogram according to algorithm 353QH, TWICE (see [13]). Input parameters: ID Histogram identifier ICASE 0 and 1 replace original histogram by smoothed version; 2 superimposes as a function when editing. <p> Before trying them out references [13, 14, 15] should be consulted, and results taken with care. CALL HSMOOF (ID,ICASE,CHI2*) Action: Routine to smooth a 1-dimensional histogram according to algorithm 353QH, TWICE (see <ref> [13] </ref>). Input parameters: ID Histogram identifier ICASE 0 and 1 replace original histogram by smoothed version; 2 superimposes as a function when editing. Output Parameter: CHI2 chisquare 2 between original and smoothed histogram.
Reference: [14] <author> B. Schorr. </author> <title> Spline estimation of Distributions and Density Functions. </title> <type> Technical Report DD/75/13, </type> <institution> CERN, </institution> <year> 1975. </year>
Reference-contexts: To filter out the statistical fluctuations, smoothing algorithms can be applied. Two such techniques are implemented in HBOOK, the so called 353QH (HSMOOF) and the method of B-splines (HSPLI1, HSPLI2, HSPFUN). Before trying them out references <ref> [13, 14, 15] </ref> should be consulted, and results taken with care. CALL HSMOOF (ID,ICASE,CHI2*) Action: Routine to smooth a 1-dimensional histogram according to algorithm 353QH, TWICE (see [13]). Input parameters: ID Histogram identifier ICASE 0 and 1 replace original histogram by smoothed version; 2 superimposes as a function when editing.
Reference: [15] <author> B. Schorr. </author> <title> Etude du lissage d'histogramme par la methode des B-splines et implementation dans HBOOK. </title> <type> Technical Report DD/EE/78-3, </type> <institution> CERN, </institution> <year> 1978. </year>
Reference-contexts: To filter out the statistical fluctuations, smoothing algorithms can be applied. Two such techniques are implemented in HBOOK, the so called 353QH (HSMOOF) and the method of B-splines (HSPLI1, HSPLI2, HSPFUN). Before trying them out references <ref> [13, 14, 15] </ref> should be consulted, and results taken with care. CALL HSMOOF (ID,ICASE,CHI2*) Action: Routine to smooth a 1-dimensional histogram according to algorithm 353QH, TWICE (see [13]). Input parameters: ID Histogram identifier ICASE 0 and 1 replace original histogram by smoothed version; 2 superimposes as a function when editing.
Reference: [16] <author> Roger Barlow and Christine Beeston. </author> <title> Fitting using finite Monte Carlo Samples. </title> <type> Technical Report MAN/HEP/93/1, </type> <institution> Dept. of Physics, University of Manchester, </institution> <year> 1993. </year>
Reference-contexts: The maximum likelihood method is used, the likelihood function being calculated including the effect of both data and Monte Carlo statistics, as described in <ref> [16] </ref>. Subroutine HMCMLL uses MINUIT to perform the log-likelihood maximisation and return a set of fractions. Functions HMCINI and HMCLNL are provided for those who wish to perform the fit themselves. Note that all floating point parameters and functions are of type REAL*8 (double precision on most machines). 7.8.

References-found: 15

