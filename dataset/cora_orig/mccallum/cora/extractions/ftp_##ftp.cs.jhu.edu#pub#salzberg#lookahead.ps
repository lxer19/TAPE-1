URL: ftp://ftp.cs.jhu.edu/pub/salzberg/lookahead.ps
Refering-URL: http://www.cs.jhu.edu/~salzberg/cs661.html
Root-URL: 
Title: Lookahead and Pathology in Decision Tree Induction  
Author: Sreerama K. Murthy Steven Salzberg 
Address: Baltimore, MD 21218 USA  
Affiliation: Department of Computer Science Johns Hopkins University,  
Abstract: The standard approach to decision tree induction is a top-down, greedy algorithm that makes locally optimal, irrevocable decisions at each node of a tree. In this paper, we study an alternative approach, in which the algorithms use limited lookahead to decide what test to use at a node. We systematically compare, using a very large number of decision trees, the quality of decision trees induced by the greedy approach to that of trees induced using lookahead. The main results of our experiments are: (i) the greedy approach produces trees that are just as accurate as trees produced with the much more expensive lookahead step; and (ii) decision tree induction exhibits pathology, in the sense that lookahead can produce trees that are both larger and less accurate than trees produced without it.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. P. Bennett. </author> <title> Global tree optimization: A non-greedy decision tree algorithm. </title> <booktitle> In Proceedings of Interface 94: The 26th Symposium on the Interface, </booktitle> <institution> Research Triangle, North Carolina, </institution> <year> 1994. </year>
Reference-contexts: Interesting approaches to slightly different problems include Raghavan and Rendell's lookahead Feature Construction (LFC) algorithm [21]. This method is more efficient than methods like IDX because it caches the features found while looking ahead. Bennett's Global Tree Optimization <ref> [1] </ref> is a non-greedy 2 tree induction method that does not use lookahead. This method uses iterative linear programming to update decision nodes in a greedily induced tree. In this paper, we describe experiments that aim to precisely quantify the benefits of limited lookahead search for tree induction.
Reference: [2] <author> A. Blum and R. Rivest. </author> <title> Training a 3-node neural network is NP-complete. </title> <booktitle> In Proceedings of the 1988 Workshop on COLT, </booktitle> <pages> pages 9-18, </pages> <address> Boston, MA, 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Fixed-depth lookahead search is one such technique, and in fact Sarkar et al. [22] have shown that lookahead search can guarantee bounded-error solutions for the 0/1-knapsack problem. Several versions of the optimal decision tree induction problem are known to be NP-Complete <ref> [11, 2, 9] </ref>. As a result, virtually all implemented decision tree systems use a greedy, top-down approach. There have been, however, some exceptions to this rule. Moret [12] surveys early induction systems that used dynamic programming and branch-and-bound methods to produce optimal trees.
Reference: [3] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <year> 1984. </year>
Reference-contexts: Compute the goodness of splitting S into S11, S12, S21, 3 and S22, using the same goodness measure as GREEDY. This is the goodness of T. 3,4. Same as steps 3,4 of GREEDY. We experimented with two pre-defined goodness measures, namely, the gini index of diversity (Gini) <ref> [3] </ref> and information gain (Info) [20]. This gave us four algorithms for our experiments, which we named Greedy-Gini, Greedy-Info, Look-Gini, and Look-Info. Note that Greedy-Gini is essentially identical to the CART algorithm [3] and Greedy-Info to the ID3 algorithm [20]. <p> We experimented with two pre-defined goodness measures, namely, the gini index of diversity (Gini) <ref> [3] </ref> and information gain (Info) [20]. This gave us four algorithms for our experiments, which we named Greedy-Gini, Greedy-Info, Look-Gini, and Look-Info. Note that Greedy-Gini is essentially identical to the CART algorithm [3] and Greedy-Info to the ID3 algorithm [20]. Our first set of experiments systematically evaluates the benefits of looka-head by comparing the trees induced with limited lookahead to those induced with greedy search over entire classes of decision trees. <p> We augmented our algorithms (Greedy-Gini, Look-Gini, Greedy-Info and Look-Info) with pruning for these experiments, using cost complexity pruning with the one standard error rule <ref> [3] </ref>, reserving 10% of the training data as the pruning set. All results for real world data are averages of ten 5-fold cross validation experiments.
Reference: [4] <author> W. Buntine and T. Niblett. </author> <title> A further comparison of splitting rules for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 75-85, </pages> <year> 1992. </year>
Reference-contexts: The V1 data <ref> [4, 10] </ref> is identical to the VO data, except that the "best" attribute, physician-fee freeze, is removed. All the experimental results reported in this section are obtained with information gain. Figures 8 and 9 summarize the results for accuracy and tree size respectively.
Reference: [5] <author> Lymphography data. </author> <note> Available in UCI ML Repository. Provided by M. </note> <institution> Zwitter and M. Soklic of University Medical Centre, Institute of Oncology, Ljubljana, </institution> <address> Yugoslavia. </address>
Reference-contexts: The datasets used here were taken from the UCI Machine Learning repository [13], and include the following: a breast cancer database (BC), the Cleve-land heart disease data (CL) [6], glass identification data (GL), hepatitis diagnosis (HE), Canadian labor negotiations data (LA), lymphography diagnosis (LY) <ref> [5] </ref> and Congressional voting records (VO and V1). The V1 data [4, 10] is identical to the VO data, except that the "best" attribute, physician-fee freeze, is removed. All the experimental results reported in this section are obtained with information gain.
Reference: [6] <institution> Cleveland Heart Disease database. </institution> <note> Available in UCI ML Repository. Col lected by Roberto Detrano, </note> <institution> V.A. Medical center, Long Beach and Cleve-land Clinic Foundation. </institution>
Reference-contexts: All results for real world data are averages of ten 5-fold cross validation experiments. The datasets used here were taken from the UCI Machine Learning repository [13], and include the following: a breast cancer database (BC), the Cleve-land heart disease data (CL) <ref> [6] </ref>, glass identification data (GL), hepatitis diagnosis (HE), Canadian labor negotiations data (LA), lymphography diagnosis (LY) [5] and Congressional voting records (VO and V1). The V1 data [4, 10] is identical to the VO data, except that the "best" attribute, physician-fee freeze, is removed.
Reference: [7] <author> R. M. Goodman and P. Smyth. </author> <title> Decision tree design from a communication theory standpoint. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 34(5) </volume> <pages> 979-994, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: Our pathology results indicate that each such optimization is not necessarily improving the tree globally. It has been asserted that maximizing information gain at each node should result in shallow trees <ref> [7] </ref>. Although our experimental results are consistent with this belief, pathologically deep trees (which we also found) indicate that a optimizing information gain locally can in fact make a tree deeper. An interesting question for further study is whether there exist effective goodness measures that guarantee no pathology.
Reference: [8] <author> C. R. P. Hartmann, P. K. Varshney, K. G. Mehrotra, and C. L. Gerberich. </author> <title> Application of information theory to the construction of efficient decision trees. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-28(4):565-577, </volume> <month> July </month> <year> 1982. </year>
Reference-contexts: As a result, virtually all implemented decision tree systems use a greedy, top-down approach. There have been, however, some exceptions to this rule. Moret [12] surveys early induction systems that used dynamic programming and branch-and-bound methods to produce optimal trees. Hartmann et al. <ref> [8] </ref> describe Generalized Optimum Testing Algorithm (GOTA), an algorithm based on an information theoretic criterion between branching levels in a tree. With the appropriate parameter settings, GOTA can do limited lookahead or even exhaustive search.
Reference: [9] <author> D. Heath, S. Kasif, and S. Salzberg. </author> <title> Learning oblique decision trees. </title> <booktitle> In Proceedings of the 13th IJCAI, </booktitle> <pages> pages 1002-1007, </pages> <address> Chambery, France, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Fixed-depth lookahead search is one such technique, and in fact Sarkar et al. [22] have shown that lookahead search can guarantee bounded-error solutions for the 0/1-knapsack problem. Several versions of the optimal decision tree induction problem are known to be NP-Complete <ref> [11, 2, 9] </ref>. As a result, virtually all implemented decision tree systems use a greedy, top-down approach. There have been, however, some exceptions to this rule. Moret [12] surveys early induction systems that used dynamic programming and branch-and-bound methods to produce optimal trees.
Reference: [10] <author> R. Holte. </author> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11(1) </volume> <pages> 63-90, </pages> <year> 1993. </year>
Reference-contexts: The leaf nodes L1, L2 etc. in this figure can be replaced with arbitrary subtrees. not likely to produce significant benefits. We used a survey of results <ref> [10] </ref> to choose seven "difficult" domains for our experiments domains for which the best known accuracy is at most 90%. 1 In addition to these domains, we experimented with the congressional voting records data used by Norton [19] for his lookahead experiments. <p> The V1 data <ref> [4, 10] </ref> is identical to the VO data, except that the "best" attribute, physician-fee freeze, is removed. All the experimental results reported in this section are obtained with information gain. Figures 8 and 9 summarize the results for accuracy and tree size respectively.
Reference: [11] <author> L. Hyafil and R. L. Rivest. </author> <title> Constructing optimal binary decision trees is NP-complete. </title> <journal> Information Processing Letters, </journal> <volume> 5(1) </volume> <pages> 15-17, </pages> <year> 1976. </year>
Reference-contexts: Fixed-depth lookahead search is one such technique, and in fact Sarkar et al. [22] have shown that lookahead search can guarantee bounded-error solutions for the 0/1-knapsack problem. Several versions of the optimal decision tree induction problem are known to be NP-Complete <ref> [11, 2, 9] </ref>. As a result, virtually all implemented decision tree systems use a greedy, top-down approach. There have been, however, some exceptions to this rule. Moret [12] surveys early induction systems that used dynamic programming and branch-and-bound methods to produce optimal trees.
Reference: [12] <author> B. </author> <title> M.E. Moret. Decision trees and diagrams. </title> <journal> Computing Surveys, </journal> <volume> 14(4) </volume> <pages> 593-623, </pages> <month> December </month> <year> 1982. </year>
Reference-contexts: Several versions of the optimal decision tree induction problem are known to be NP-Complete [11, 2, 9]. As a result, virtually all implemented decision tree systems use a greedy, top-down approach. There have been, however, some exceptions to this rule. Moret <ref> [12] </ref> surveys early induction systems that used dynamic programming and branch-and-bound methods to produce optimal trees. Hartmann et al. [8] describe Generalized Optimum Testing Algorithm (GOTA), an algorithm based on an information theoretic criterion between branching levels in a tree.
Reference: [13] <author> P.M. Murphy and D. Aha. </author> <title> UCI repository of machine learning databases a machine-readable data repository. </title> <note> Anonymous FTP from ics.uci.edu in the directory pub/machine-learning-databases, 1994. 14 </note>
Reference-contexts: All results for real world data are averages of ten 5-fold cross validation experiments. The datasets used here were taken from the UCI Machine Learning repository <ref> [13] </ref>, and include the following: a breast cancer database (BC), the Cleve-land heart disease data (CL) [6], glass identification data (GL), hepatitis diagnosis (HE), Canadian labor negotiations data (LA), lymphography diagnosis (LY) [5] and Congressional voting records (VO and V1).
Reference: [14] <author> S. K. Murthy, S. Kasif, and S. Salzberg. </author> <title> A system for induction of oblique decision trees. </title> <journal> Journal of AI Research, </journal> <volume> 2 </volume> <pages> 1-33, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: We are particularly interested in techniques to refine the nodes and the structure of greedily induced trees. The tree rotation operators defined in Section 3.3 are just one such example. Randomization in the context of decision trees <ref> [14] </ref> offers another alternative to enhance greedy induction.
Reference: [15] <author> S. K. Murthy and S. Salzberg. </author> <title> On lookahead for decision tree induction. </title> <type> Technical report, </type> <institution> Dept. of Computer Science, Johns Hopkins University, </institution> <year> 1994. </year> <note> http://www.cs.jhu.edu/grad/murthy/home.html. </note>
Reference-contexts: We looked at all such large trees, and found that they always had several "minimally useful" splits, splits that were separating very few points. Such splits can be easily avoided with a simple stop-splitting rule, narrowing the gap between lookahead and greedy induction further <ref> [15] </ref>. 3.2 C S : A class of larger trees This section extends class C to a class C S , which contains slightly larger trees. C S is obtained as follows: C S = ; For each tree T in C, do 1. Remove T from C. 2. <p> These operators are illustrated in Figure 7. We found that a heuristic tree balancing procedure, using rotation operators recursively on the tree nodes, reduces the difference in maximum depth between trees induced with and without lookahead. For details, see <ref> [15] </ref>. 4. Experiments with Real World Data We also experimented with seven real-world databases, for which the underlying concepts are unknown.
Reference: [16] <author> D. Mutchler. </author> <title> The multi-player version of minimax displays game pathol ogy. </title> <journal> AI, </journal> <volume> 64(2) </volume> <pages> 323-336, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Intuitively, doing more search (lookahead) should produce better decision trees, just as deeper search in game trees (e.g., for chess) produces better game-playing programs. However, it has been observed that for some games, deeper search can actually produce an inferior program, both with two players and with multiple players <ref> [18, 16] </ref>. Decision trees, one can argue, are analogous to a one-player game tree. Our discovery that deeper search can lead to inferior decision trees thus extends the earlier pathology results to a new domain. Decision tree induction is fundamentally an optimization problem. <p> Improvements in accuracy, size and maximum depth are shown, along with the number of trees in which these improvements occur. Negative values on the X-axis mean that lookahead produced inferior trees. context of game trees <ref> [18, 16] </ref>. We discuss pathology for decision trees further in Section 3.2, where this trend is exhibited more prominently. Pathology cannot occur for tree size or depth for class C, because one-level lookahead is equivalent to exhaustive search.
Reference: [17] <author> Y. Nakamura, S. Abe, Y. Ohsawa, and M. Sakauchi. </author> <title> A balanced hierarchi cal data structure for multidimensional data with highly efficient dynamic characteristics. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(4) </volume> <pages> 682-694, </pages> <month> august </month> <year> 1993. </year>
Reference-contexts: Although little work has been done on balancing decision trees, a great deal of research has considered balanced search trees <ref> [23, 17] </ref>. Roughly speaking, this literature deals with techniques to restructure search trees when elements are inserted or deleted, in order to restrict the depth of these trees to a logarithmic function of the number of search keys.
Reference: [18] <author> D. S. Nau. </author> <title> Decision quality as a function of search depth on game trees. </title> <journal> Journal of the ACM, </journal> <volume> 30(4) </volume> <pages> 687-708, </pages> <month> October </month> <year> 1983. </year>
Reference-contexts: On average, it produces slightly shallower trees with approximately the same classification accuracy and size as greedy induction. * Limited lookahead search produces inferior decision trees on a significant number of cases; i.e., decision tree induction exhibits the same pathology that has been observed in game trees <ref> [18] </ref>. 1 * Tree post-processing techniques such as pruning are at least as beneficial as limited lookahead for a variety of real-world datasets. In this context, we introduce a new post-processing technique, decision tree balancing. <p> Intuitively, doing more search (lookahead) should produce better decision trees, just as deeper search in game trees (e.g., for chess) produces better game-playing programs. However, it has been observed that for some games, deeper search can actually produce an inferior program, both with two players and with multiple players <ref> [18, 16] </ref>. Decision trees, one can argue, are analogous to a one-player game tree. Our discovery that deeper search can lead to inferior decision trees thus extends the earlier pathology results to a new domain. Decision tree induction is fundamentally an optimization problem. <p> Improvements in accuracy, size and maximum depth are shown, along with the number of trees in which these improvements occur. Negative values on the X-axis mean that lookahead produced inferior trees. context of game trees <ref> [18, 16] </ref>. We discuss pathology for decision trees further in Section 3.2, where this trend is exhibited more prominently. Pathology cannot occur for tree size or depth for class C, because one-level lookahead is equivalent to exhaustive search.
Reference: [19] <author> S. W. Norton. </author> <title> Generating better decision trees. </title> <booktitle> In Proceedings of the Eleventh IJCAI, </booktitle> <volume> volume 1, </volume> <pages> pages 800-805, </pages> <address> 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Hartmann et al. [8] describe Generalized Optimum Testing Algorithm (GOTA), an algorithm based on an information theoretic criterion between branching levels in a tree. With the appropriate parameter settings, GOTA can do limited lookahead or even exhaustive search. The ideas in GOTA motivated Norton's IDX system <ref> [19] </ref>, which is a variant of ID3 [20] that performs lookahead. Norton conducted experiments on the congressional voting records database (see Section 4), and found that lookahead reduced average decision tree depth. With a few exceptions, though, the advantages of lookahead were very small in Norton's experiments. <p> We used a survey of results [10] to choose seven "difficult" domains for our experiments domains for which the best known accuracy is at most 90%. 1 In addition to these domains, we experimented with the congressional voting records data used by Norton <ref> [19] </ref> for his lookahead experiments. We augmented our algorithms (Greedy-Gini, Look-Gini, Greedy-Info and Look-Info) with pruning for these experiments, using cost complexity pruning with the one standard error rule [3], reserving 10% of the training data as the pruning set.
Reference: [20] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: With the appropriate parameter settings, GOTA can do limited lookahead or even exhaustive search. The ideas in GOTA motivated Norton's IDX system [19], which is a variant of ID3 <ref> [20] </ref> that performs lookahead. Norton conducted experiments on the congressional voting records database (see Section 4), and found that lookahead reduced average decision tree depth. With a few exceptions, though, the advantages of lookahead were very small in Norton's experiments. <p> This is the goodness of T. 3,4. Same as steps 3,4 of GREEDY. We experimented with two pre-defined goodness measures, namely, the gini index of diversity (Gini) [3] and information gain (Info) <ref> [20] </ref>. This gave us four algorithms for our experiments, which we named Greedy-Gini, Greedy-Info, Look-Gini, and Look-Info. Note that Greedy-Gini is essentially identical to the CART algorithm [3] and Greedy-Info to the ID3 algorithm [20]. <p> two pre-defined goodness measures, namely, the gini index of diversity (Gini) [3] and information gain (Info) <ref> [20] </ref>. This gave us four algorithms for our experiments, which we named Greedy-Gini, Greedy-Info, Look-Gini, and Look-Info. Note that Greedy-Gini is essentially identical to the CART algorithm [3] and Greedy-Info to the ID3 algorithm [20]. Our first set of experiments systematically evaluates the benefits of looka-head by comparing the trees induced with limited lookahead to those induced with greedy search over entire classes of decision trees. This style of empirical investigation has only been made possible by the existence of extremely fast, inexpensive computers.
Reference: [21] <author> H. Raghavan and L. Rendell. </author> <title> Lookahead feature construction for learn ing hard concepts. </title> <booktitle> In Proceedings of the Tenth Intl Conf. on Machine Learning, </booktitle> <pages> pages 252-259, </pages> <address> 1993. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: With a few exceptions, though, the advantages of lookahead were very small in Norton's experiments. Interesting approaches to slightly different problems include Raghavan and Rendell's lookahead Feature Construction (LFC) algorithm <ref> [21] </ref>. This method is more efficient than methods like IDX because it caches the features found while looking ahead. Bennett's Global Tree Optimization [1] is a non-greedy 2 tree induction method that does not use lookahead.
Reference: [22] <author> U. K. Sarkar, P. P. Chakrabarti, S. Ghose, and S. C. DeSarkar. </author> <title> Improving greedy algorithms by lookahead-search. </title> <journal> Journal of Algorithms, </journal> <volume> 16(1) </volume> <pages> 1-23, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Assuming P 6= N P, it is natural in such domains to look for techniques to systematically bridge the gap between the approximate solutions provided by greedy search and the optimal solutions. Fixed-depth lookahead search is one such technique, and in fact Sarkar et al. <ref> [22] </ref> have shown that lookahead search can guarantee bounded-error solutions for the 0/1-knapsack problem. Several versions of the optimal decision tree induction problem are known to be NP-Complete [11, 2, 9]. As a result, virtually all implemented decision tree systems use a greedy, top-down approach.
Reference: [23] <author> Q. F. Stout and B. L. Warren. </author> <title> Tree rebalancing in optimal time and space. </title> <journal> Communications of the ACM, </journal> <volume> 29(9) </volume> <pages> 902-908, </pages> <month> September </month> <year> 1986. </year> <month> 15 </month>
Reference-contexts: Although little work has been done on balancing decision trees, a great deal of research has considered balanced search trees <ref> [23, 17] </ref>. Roughly speaking, this literature deals with techniques to restructure search trees when elements are inserted or deleted, in order to restrict the depth of these trees to a logarithmic function of the number of search keys.
References-found: 23

