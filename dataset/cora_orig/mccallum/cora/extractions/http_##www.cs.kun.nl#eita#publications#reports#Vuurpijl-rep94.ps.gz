URL: http://www.cs.kun.nl/eita/publications/reports/Vuurpijl-rep94.ps.gz
Refering-URL: http://www.cs.kun.nl/eita/publications/
Root-URL: 
Email: e-mail: louis@cs.kun.nl  
Title: CONVIS: Action Oriented Control and Visualization of Neural Networks Introduction and Technical Description  
Author: Louis Vuurpijl, Theo Schouten and Jan Vytopil 
Date: November 1994  
Address: Toernooiveld 1 6525 ED Nijmegen  
Affiliation: University of Nijmegen Faculty of Mathematics and Informatics Department of Informatics for Technical Applications  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> M. Azema-Barac, M. Hewetson, M. Recce, J. Taylor, P. Treleavan, and M. Vel-lasco. </author> <title> Pygmalion Neural Network Programming Environment. </title> <editor> In B. Angeniol and B. Widrow, editors, </editor> <booktitle> International Neural Network Conference, </booktitle> <pages> pages 1237-1244, </pages> <address> Paris, July 1990. </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Introduction. A general purpose neurosimulator is a set of software and/or hardware components dedicated to constructing, monitoring, debugging, visualizing or (fast) executing neural network simulations. Currently, many neurosimulators exist or are being developed, such as Pygmalion <ref> [1] </ref>, Galatea [11], RCS [5], Planet [12], Aspirin-Migraines [9] and Genesis [27]. Each of these has its own characteristics, but many share the same features, like having a graphical user-interface, an algorithm library, support for building new models, application specific tools, dedicated hardware accelerators, etc. <p> organizes his data, provided that it can be described following the data description described below. typedef struct f char name [50]; =fl name of the data fl= int id; =fl identification of the data fl= int type; =fl type of the data fl= int dimension; =fl dimension of the data, <ref> [1; 2; 3; 4] </ref> fl= int depth [4]; =fl for each dimension, nr of elements fl= int length; =fl the number of features per element fl= int flat; =fl flat array or n-dimensional fl= double time; =fl current time value of the data fl= double previous update; =fl previous update of <p> In his program, he probably would like to program the computa tion of both arrays as: 38 int *confusion matrix [2]; confusion matrix [0] = (int *) malloc (npatterns*sizeof (int)); confusion matrix <ref> [1] </ref> = (int *) malloc (npatterns*sizeof (int)); for (p=0;p&lt;npatterns;p++) f confusion matrix [0][p] = class [p]; confusion matrix [1][p] = ComputeClass (p); g Now furthermore, assume a tool exists that is capable of monitoring the data contained in confusion matrix (see chapter 4).
Reference: [2] <author> M. Duranton, F. Aglan, and N. Mauduit. </author> <title> Hardware Accelerators for Neural Networks: Simulations in Parallel Machines. </title> <editor> In D. Heidrich and J.C. Grossetie, editors, </editor> <booktitle> Computing with T-Node Parallel Architecture, </booktitle> <pages> pages 235-264. </pages> <address> ECSC, EEC, EAEC, Brussels and Luxembourg, </address> <year> 1991. </year>
Reference-contexts: Some neuro-simulators use parallel processor systems (Tollenaere [21], Richards [17], Recce [16], Goddard [5]), some are targeted on super computers (Ghosh and Wang [4]), others on dedicated neuro asics (Theeten [20], Han [6]) or use heterogeneous architectures (Duranton <ref> [2] </ref>). Application specific tools and devices For most experiments using neural networks, the information produced by the tools mentioned above is adequate for making general statements about how the network is performing during training. <p> organizes his data, provided that it can be described following the data description described below. typedef struct f char name [50]; =fl name of the data fl= int id; =fl identification of the data fl= int type; =fl type of the data fl= int dimension; =fl dimension of the data, <ref> [1; 2; 3; 4] </ref> fl= int depth [4]; =fl for each dimension, nr of elements fl= int length; =fl the number of features per element fl= int flat; =fl flat array or n-dimensional fl= double time; =fl current time value of the data fl= double previous update; =fl previous update of <p> In his program, he probably would like to program the computa tion of both arrays as: 38 int *confusion matrix <ref> [2] </ref>; confusion matrix [0] = (int *) malloc (npatterns*sizeof (int)); confusion matrix [1] = (int *) malloc (npatterns*sizeof (int)); for (p=0;p&lt;npatterns;p++) f confusion matrix [0][p] = class [p]; confusion matrix [1][p] = ComputeClass (p); g Now furthermore, assume a tool exists that is capable of monitoring the data contained in confusion <p> The following piece of code indicates how a tool that handles variables could be implemented: #include &lt;preens convis.h&gt; int main (int argc, char flargv []) f fd = setup client connection (argv [1],atoi (argv <ref> [2] </ref>)); ReceiveVariable (&variable,0,fd); InitializeTool (variable); while (ReceiveVariable (&variable,0,fd)==OK) HandleVariable (variable); g Algorithm 4.1: Skeleton of an output tool handling variables. The routines InitializeTool () and HandleVariable () are determined by the tool. The code depicted above repeatedly receives a variable and handles it. <p> If the tool uses any X convenience routines defined in the PREENS distribution, it must be linked with libmenus.a. 45 #include &lt;preens convis.h&gt; callback () f ReceiveVariable (&variable,0,fd); HandleVariable (variable); g int main (int argc, char flargv []) f fd = setup client connection (argv [1],atoi (argv <ref> [2] </ref>)); ReceiveVariable (&variable,0,fd); InitializeX (variable,argc,argv); XtAppAddInput (Context,fd,XtInputReadMask,callback,NULL); XtAppMainLoop (MyContext); g Algorithm 4.2: Skeleton of an output tool using the Xtoolkit. Observing the skeletons given above, it appears that any new or existing tool can be integrated with convis relatively easy. <p> The following piece of code indicates how a tool that handles data could be implemented using the Xtoolkit: #include &lt;preens convis.h&gt; callback () f ReceiveData (0,fd,data); HandleData (data); g int main (int argc, char flargv []) f fd = setup client connection (argv [1],atoi (argv <ref> [2] </ref>)); ReceiveDataHeader (0,fd,&header); InitializeX (header,argc,argv); XtAppAddInput (Context,fd,XtInputReadMask,callback,NULL); XtAppMainLoop (MyContext); g Algorithm 4.3: Skeleton of a data output tool using the Xtoolkit. As an example of a tool that handles data, consider again the tool that plots a confusion matrix (see section 3.5.5). <p> Using n or w and h, the transformation routine checks whether the PREENS data can be matched to these parameters. Transforming PREENS data to a datastructure containing two rows of integers can be done via: int *classification <ref> [2] </ref>; DataToIntMatrix (&data,classification,2,npatterns); Another way of accessing the PREENS data can be used if the programmer knows how the data is structured (which can be determined via its description field). In this case, the programmer knows that the data contains two arrays of integers. <p> The next piece of code depicts the skeleton for an input tool. int main (int argc, char flargv []) f fd = setup client connection (argv [1],atoi (argv <ref> [2] </ref>)); ReceiveInt (0,fd,&synchr); =fl receive variable or data and initialize tool fl= while (: : :) f =fl generate new variable or data fl= =fl send it using SendVariable or SendData fl= if (synchr) ReceiveInt (0,fd,&acknowledge); g Algorithm 4.4: Skeleton for an input tool.
Reference: [3] <author> B.W. Char et al. </author> <title> Maple V Language Reference Manual. </title> <publisher> Springer Verlag, </publisher> <year> 1991. </year>
Reference-contexts: organizes his data, provided that it can be described following the data description described below. typedef struct f char name [50]; =fl name of the data fl= int id; =fl identification of the data fl= int type; =fl type of the data fl= int dimension; =fl dimension of the data, <ref> [1; 2; 3; 4] </ref> fl= int depth [4]; =fl for each dimension, nr of elements fl= int length; =fl the number of features per element fl= int flat; =fl flat array or n-dimensional fl= double time; =fl current time value of the data fl= double previous update; =fl previous update of <p> This same method can be used to connect a tool to any other program that interacts via a command line interface with a user, like e.g. maple <ref> [3] </ref> or gnuplot [8]. The tool convis matlab is started like any other tool from the PREENS tool library.
Reference: [4] <author> J. Ghosh and K. Hwang. </author> <title> Mapping Neural Networks onto Message-Passing Multi-computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 6 </volume> <pages> 291-330, </pages> <year> 1989. </year>
Reference-contexts: Some neuro-simulators use parallel processor systems (Tollenaere [21], Richards [17], Recce [16], Goddard [5]), some are targeted on super computers (Ghosh and Wang <ref> [4] </ref>), others on dedicated neuro asics (Theeten [20], Han [6]) or use heterogeneous architectures (Duranton [2]). Application specific tools and devices For most experiments using neural networks, the information produced by the tools mentioned above is adequate for making general statements about how the network is performing during training. <p> organizes his data, provided that it can be described following the data description described below. typedef struct f char name [50]; =fl name of the data fl= int id; =fl identification of the data fl= int type; =fl type of the data fl= int dimension; =fl dimension of the data, <ref> [1; 2; 3; 4] </ref> fl= int depth [4]; =fl for each dimension, nr of elements fl= int length; =fl the number of features per element fl= int flat; =fl flat array or n-dimensional fl= double time; =fl current time value of the data fl= double previous update; =fl previous update of <p> be described following the data description described below. typedef struct f char name [50]; =fl name of the data fl= int id; =fl identification of the data fl= int type; =fl type of the data fl= int dimension; =fl dimension of the data, [1; 2; 3; 4] fl= int depth <ref> [4] </ref>; =fl for each dimension, nr of elements fl= int length; =fl the number of features per element fl= int flat; =fl flat array or n-dimensional fl= double time; =fl current time value of the data fl= double previous update; =fl previous update of the data fl= double delta time; =fl
Reference: [5] <author> N.H. Goddard, K.J. Lynne, T. Mintz, and L. Bukys. </author> <title> Rochester Connectionist Simulator. </title> <type> Technical Report 233 (revised), </type> <institution> University of Rochester, Computer Science Department, Rochester, </institution> <address> New York 14627, </address> <month> October </month> <year> 1989. </year>
Reference-contexts: Introduction. A general purpose neurosimulator is a set of software and/or hardware components dedicated to constructing, monitoring, debugging, visualizing or (fast) executing neural network simulations. Currently, many neurosimulators exist or are being developed, such as Pygmalion [1], Galatea [11], RCS <ref> [5] </ref>, Planet [12], Aspirin-Migraines [9] and Genesis [27]. Each of these has its own characteristics, but many share the same features, like having a graphical user-interface, an algorithm library, support for building new models, application specific tools, dedicated hardware accelerators, etc. <p> Dedicated target platforms In order to cope with the enormous processing and memory requirements required by todays neural network applications, a wide range of high performance target platforms are proposed and being used. Some neuro-simulators use parallel processor systems (Tollenaere [21], Richards [17], Recce [16], Goddard <ref> [5] </ref>), some are targeted on super computers (Ghosh and Wang [4]), others on dedicated neuro asics (Theeten [20], Han [6]) or use heterogeneous architectures (Duranton [2]).
Reference: [6] <author> I.S. Han, K.H. Ahn, T.H. Park, and K.H. </author> <month> Jun. </month> <title> Adaptable VLSI Neural Network of Tens of Thousand Connections. </title> <editor> In I. Aleksander and J. Taylor, editors, </editor> <booktitle> Artificial Neural Networks 2, </booktitle> <pages> pages 1423-1426. </pages> <publisher> Elsevier Science Publishers (North-Holland), </publisher> <year> 1992. </year>
Reference-contexts: Some neuro-simulators use parallel processor systems (Tollenaere [21], Richards [17], Recce [16], Goddard [5]), some are targeted on super computers (Ghosh and Wang [4]), others on dedicated neuro asics (Theeten [20], Han <ref> [6] </ref>) or use heterogeneous architectures (Duranton [2]). Application specific tools and devices For most experiments using neural networks, the information produced by the tools mentioned above is adequate for making general statements about how the network is performing during training.
Reference: [7] <author> Mathworks Inc. </author> <title> Matlab external interface guide. </title> <type> Technical report, </type> <institution> Mathworks Inc., Cochituate Place, </institution> <address> 24 Prime Park Way, Natick, Mass. 01760, </address> <month> Januari </month> <year> 1993. </year>
Reference-contexts: Upon receiving any data from convis, the tool transforms the data into a suitable array and writes it in matlab format <ref> [7] </ref> on file. By issuing the command to load the file over the pipe, matlab will load the data. Once it has loaded the data, via the intermediary, any matlab commands can be sent to matlab.
Reference: [8] <author> C. Kelley and T. Williams. </author> <title> Gnuplot Unix version 3.5, online manual. </title> <booktitle> info-gnuplot dartmouth.edu, </booktitle> <pages> 1986-1993. </pages>
Reference-contexts: This same method can be used to connect a tool to any other program that interacts via a command line interface with a user, like e.g. maple [3] or gnuplot <ref> [8] </ref>. The tool convis matlab is started like any other tool from the PREENS tool library.
Reference: [9] <author> R. Leighton and A. Wieland. </author> <title> The aspirin/migraines software tools. </title> <type> Technical Report MTR90W00044, </type> <institution> MITRE Washington Neural Network Group, The MITRE Corporation Washington C 3 I Division 7525 Colshire Drive McLean, </institution> <address> Vir-ginia 22102, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: Introduction. A general purpose neurosimulator is a set of software and/or hardware components dedicated to constructing, monitoring, debugging, visualizing or (fast) executing neural network simulations. Currently, many neurosimulators exist or are being developed, such as Pygmalion [1], Galatea [11], RCS [5], Planet [12], Aspirin-Migraines <ref> [9] </ref> and Genesis [27]. Each of these has its own characteristics, but many share the same features, like having a graphical user-interface, an algorithm library, support for building new models, application specific tools, dedicated hardware accelerators, etc.
Reference: [10] <author> Mathworks, </author> <title> Cochituate Place 24, Prime Pathway, Natic, Mas 01760. Matlab, High Performance Numeric Computation and Visualization Software, </title> <note> 1992. Version 4.0 User's Guide. </note>
Reference-contexts: As a final example, consider how PREENS data can be coupled to a tool that forms an intermediary between convis and matlab <ref> [10] </ref>, a powerful data visualization and processing toolkit. Matlab offers a command line interface to a user, via which he can enter any plot or dataprocessing commands. <p> The third is to extend the routine LoadPatterns () such that is capable of loading the files with the new format. For example, one of the users of PREENS used to model his input and target patterns and produce artificial data via matlab <ref> [10] </ref>. As the data changed frequently and the amount of data was significant (tens of megabytes), the first method did not seem useful.
Reference: [11] <author> C. Mejia, L. Bottou, and F. Fogelman Soulie. Galatea: </author> <title> a C-Library for Connectionist Applications. </title> <editor> In B. Angeniol and B. Widrow, editors, </editor> <booktitle> Proceedings of the International Neural Network Conference, </booktitle> <pages> pages 1062-1065, </pages> <address> Paris, July 1990. </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Introduction. A general purpose neurosimulator is a set of software and/or hardware components dedicated to constructing, monitoring, debugging, visualizing or (fast) executing neural network simulations. Currently, many neurosimulators exist or are being developed, such as Pygmalion [1], Galatea <ref> [11] </ref>, RCS [5], Planet [12], Aspirin-Migraines [9] and Genesis [27]. Each of these has its own characteristics, but many share the same features, like having a graphical user-interface, an algorithm library, support for building new models, application specific tools, dedicated hardware accelerators, etc.
Reference: [12] <author> Y. Miyata. </author> <title> A User's Guide to PlaNet Version 5.6, A Tool for Constructing, Running and Looking into a PDP Network. </title> <institution> University of Boulder, Computer Science Department, Boulder, </institution> <month> January </month> <year> 1991. </year> <month> 69 </month>
Reference-contexts: Introduction. A general purpose neurosimulator is a set of software and/or hardware components dedicated to constructing, monitoring, debugging, visualizing or (fast) executing neural network simulations. Currently, many neurosimulators exist or are being developed, such as Pygmalion [1], Galatea [11], RCS [5], Planet <ref> [12] </ref>, Aspirin-Migraines [9] and Genesis [27]. Each of these has its own characteristics, but many share the same features, like having a graphical user-interface, an algorithm library, support for building new models, application specific tools, dedicated hardware accelerators, etc.
Reference: [13] <author> Adrian Nye. </author> <title> The Xlib Programming Manual, volume 1 of The X Window System Series. </title> <publisher> O'Reilly & Associates, Inc., </publisher> <year> 1988. </year>
Reference: [14] <author> Adrian Nye. </author> <title> Xlib Reference Manual, volume 2 of The X Window System Series. </title> <publisher> O'Reilly & Associates, Inc., </publisher> <year> 1990. </year>
Reference: [15] <author> C.D. Peterson. </author> <title> Athena widget set - c language interface. </title> <type> Technical report, </type> <institution> MIT X Consortium, Massachusetts Institue of Technology, Cambridge, Massachusetts, </institution> <year> 1985. </year>
Reference: [16] <author> M.L. Recce, P.V. Rocha, </author> <title> and P.C. Treleaven. Neural Network Programming Environments. </title> <editor> In I. Aleksander and J. Taylor, editors, </editor> <booktitle> Artificial Neural Networks 2, </booktitle> <pages> pages 1237-1244. </pages> <publisher> Elsevier Science Publishers, </publisher> <month> September </month> <year> 1992. </year>
Reference-contexts: Dedicated target platforms In order to cope with the enormous processing and memory requirements required by todays neural network applications, a wide range of high performance target platforms are proposed and being used. Some neuro-simulators use parallel processor systems (Tollenaere [21], Richards [17], Recce <ref> [16] </ref>, Goddard [5]), some are targeted on super computers (Ghosh and Wang [4]), others on dedicated neuro asics (Theeten [20], Han [6]) or use heterogeneous architectures (Duranton [2]). <p> A number of approaches are possible when examining, specifying or developing neurosimulators. One possible approach is to identify the possible users, their applications and requirements. Another approach is to identify the support that neurosimulators offer or that is wanted (Recce <ref> [16] </ref>). In section 1.4, a third approach will be introduced, which we call the action oriented approach. This approach differs from existing approaches as these are either application specific, or require simulations to be specified according to some network description language or network datastructures. <p> The second approach is followed by Recce <ref> [16] </ref>, in which a taxonomy is given for neurosimulators. Three classes are distinguished based on the support that they offer: application-oriented, algorithm-oriented and general programming systems. The first class contains users which belong to the user group of ap-pliers of neural networks, which is mentioned above.
Reference: [17] <author> G.D. Richards and T. Tollenaere. </author> <title> Documentation for Rhwydwaith Version 2.1. </title> <type> Technical Report ECSP-UG-7, </type> <institution> University of Edinburgh, </institution> <month> July </month> <year> 1989. </year>
Reference-contexts: Dedicated target platforms In order to cope with the enormous processing and memory requirements required by todays neural network applications, a wide range of high performance target platforms are proposed and being used. Some neuro-simulators use parallel processor systems (Tollenaere [21], Richards <ref> [17] </ref>, Recce [16], Goddard [5]), some are targeted on super computers (Ghosh and Wang [4]), others on dedicated neuro asics (Theeten [20], Han [6]) or use heterogeneous architectures (Duranton [2]).
Reference: [18] <author> A.J.M. Russel. </author> <title> Simulating Neural Networks on a Multi-Transputer System. </title> <type> Master's thesis, Nr. 175, </type> <institution> University of Nijmegen, Faculty of Mathematics and Infor-matics, </institution> <address> Toernooiveld 1, 6525 ED Nijmegen, The Netherlands, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: What is also observed is that the implementation of a simulator for a given type of neural network has appeared to be a relatively simple task. This also holds for parallel implementations of certain classes of neural networks <ref> [23, 19, 26, 18] </ref>. Such a neurosimulator is parameterizable in its structural dimensions (number of layers, number of neurons per layer), in its environment settings (filenames) and in its parameters and variables belonging to specific actions (training, recall).
Reference: [19] <author> R. Steenbergen. </author> <title> Parallel Implementation of Modular Non-Homogeneous Neural Networks with Dynamic Architecture. </title> <type> Master's thesis, Nr. 252, </type> <institution> University of Nijmegen, Faculty of Mathematics and Informatics, </institution> <address> Toernooiveld 1, 6525 ED Ni-jmegen, The Netherlands, </address> <month> December </month> <year> 1992. </year>
Reference-contexts: What is also observed is that the implementation of a simulator for a given type of neural network has appeared to be a relatively simple task. This also holds for parallel implementations of certain classes of neural networks <ref> [23, 19, 26, 18] </ref>. Such a neurosimulator is parameterizable in its structural dimensions (number of layers, number of neurons per layer), in its environment settings (filenames) and in its parameters and variables belonging to specific actions (training, recall).
Reference: [20] <author> J.B. Theeten, M. Duranton, N. Maudit, and J.A. Sirat. </author> <title> The LNeuro-Chip: A Digital VLSI with On-Chip Learning Mechanism. </title> <editor> In B. Angeniol and B. Widrow, editors, </editor> <booktitle> Proceedings of the International Neural Network Conference, </booktitle> <pages> pages 593-596. </pages> <publisher> Kluwer Academic Publishers, </publisher> <month> July </month> <year> 1990. </year>
Reference-contexts: Some neuro-simulators use parallel processor systems (Tollenaere [21], Richards [17], Recce [16], Goddard [5]), some are targeted on super computers (Ghosh and Wang [4]), others on dedicated neuro asics (Theeten <ref> [20] </ref>, Han [6]) or use heterogeneous architectures (Duranton [2]). Application specific tools and devices For most experiments using neural networks, the information produced by the tools mentioned above is adequate for making general statements about how the network is performing during training.
Reference: [21] <author> T. Tollenaere and G.A. Orban. </author> <title> Simulating Modular Neural Networks on Message-Passing Multiprocessors. </title> <journal> Parallel Computing, </journal> <volume> 17(1) </volume> <pages> 361-379, </pages> <year> 1991. </year>
Reference-contexts: Dedicated target platforms In order to cope with the enormous processing and memory requirements required by todays neural network applications, a wide range of high performance target platforms are proposed and being used. Some neuro-simulators use parallel processor systems (Tollenaere <ref> [21] </ref>, Richards [17], Recce [16], Goddard [5]), some are targeted on super computers (Ghosh and Wang [4]), others on dedicated neuro asics (Theeten [20], Han [6]) or use heterogeneous architectures (Duranton [2]).
Reference: [22] <author> L.G. </author> <title> Vuurpijl and Th.E. Schouten. Control and Visualization of Neural Networks in the PREENS Project. </title> <booktitle> In Proceedings of the third workshop of the Esprit Parallel Computing Action, </booktitle> <address> Bonn, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: In stead of adjusting neural network simulation programs to the neurosimulator, using the action oriented approach it adapts itself to the simulation program. The first approach is followed by Vuurpijl et al <ref> [22, 24] </ref>. Two classes of possible users of neurosimulators are distinguished. The first user group consists of appliers of neural networks for a specific application. This group has not much experience with neural networks but merely wants to use them to solve their specific problems. <p> Considering any of the neurosimulators mentioned above, or any neural network simulation program, it is observed that more than 90 percent of the code consists of user-interface and (file) I/O <ref> [22] </ref>. Besides that, they are all based on a general neural network datastructure or some neural network description language (NDL). Using a hierarchical description of its architecture, the neu-rosimulator is able to access the data associated with the neural network.
Reference: [23] <author> L.G. </author> <title> Vuurpijl and Th.E. Schouten. Suitability of Transputers for Neural Network Simulations. </title> <editor> In W. Joosen and E. Milgrom, editors, </editor> <booktitle> Parallel Computing: From Theory to Sound Practice, </booktitle> <pages> pages 528-537. </pages> <publisher> IOS Press, </publisher> <year> 1992. </year>
Reference-contexts: What is also observed is that the implementation of a simulator for a given type of neural network has appeared to be a relatively simple task. This also holds for parallel implementations of certain classes of neural networks <ref> [23, 19, 26, 18] </ref>. Such a neurosimulator is parameterizable in its structural dimensions (number of layers, number of neurons per layer), in its environment settings (filenames) and in its parameters and variables belonging to specific actions (training, recall).
Reference: [24] <author> L.G. Vuurpijl, Th.E. Schouten, and J. </author> <title> Vytopil. Design Issues Towards PREENS, a Parallel Research Execution Environment for Neural Systems. </title> <booktitle> In Proceedings of the second workshop of the Esprit Parallel Computing Action, </booktitle> <address> Ispra, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: In stead of adjusting neural network simulation programs to the neurosimulator, using the action oriented approach it adapts itself to the simulation program. The first approach is followed by Vuurpijl et al <ref> [22, 24] </ref>. Two classes of possible users of neurosimulators are distinguished. The first user group consists of appliers of neural networks for a specific application. This group has not much experience with neural networks but merely wants to use them to solve their specific problems.
Reference: [25] <author> Louis G. Vuurpijl. </author> <title> Neurosimulators and Parallel Neural Network Simulations. </title> <type> PhD thesis, </type> <institution> University of Nijmegen, Faculty of Mathematics and Informatics Dep. of Technical Applications Toernooiveld 1, 6525 ED Nijmegen The Netherlands (email louis cs.kun.nl), </institution> <year> 1994. </year> <note> In Progress. </note>
Reference-contexts: Similarly, any data that is received is distributed over a processor network. Based on the work described in <ref> [25] </ref>, several distributed communication routines like broadcast (), gather () and gather accumulate () can be distinguished. For each of these, its respective equivalent will be implemented in the PREENS distributed transformation functions.
Reference: [26] <author> P. Willems. </author> <title> The ART Neural Networks Enlightened: Implementation on Sequential and Parallel Computer Systems. </title> <type> Master's thesis, </type> <institution> University of Nijmegen, Faculty of Mathematics and Informatics, </institution> <address> Toernooiveld 1, 6525 ED Nijmegen, The Netherlands, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: What is also observed is that the implementation of a simulator for a given type of neural network has appeared to be a relatively simple task. This also holds for parallel implementations of certain classes of neural networks <ref> [23, 19, 26, 18] </ref>. Such a neurosimulator is parameterizable in its structural dimensions (number of layers, number of neurons per layer), in its environment settings (filenames) and in its parameters and variables belonging to specific actions (training, recall).
Reference: [27] <author> M.A. Wilson, U.S. Bhalla, J.D. Uhley, and J.M. Bower. </author> <title> Genesis: A System for Simulating Neural Networks. </title> <editor> In D.S. Touretzky, editor, </editor> <booktitle> Advances in Neural and Information Processing Systems, </booktitle> <pages> pages 485-492. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year> <month> 70 </month>
Reference-contexts: Introduction. A general purpose neurosimulator is a set of software and/or hardware components dedicated to constructing, monitoring, debugging, visualizing or (fast) executing neural network simulations. Currently, many neurosimulators exist or are being developed, such as Pygmalion [1], Galatea [11], RCS [5], Planet [12], Aspirin-Migraines [9] and Genesis <ref> [27] </ref>. Each of these has its own characteristics, but many share the same features, like having a graphical user-interface, an algorithm library, support for building new models, application specific tools, dedicated hardware accelerators, etc. A brief overview of these is given in the next section. 1.1 Characteristics for neurosimulators.
References-found: 27

