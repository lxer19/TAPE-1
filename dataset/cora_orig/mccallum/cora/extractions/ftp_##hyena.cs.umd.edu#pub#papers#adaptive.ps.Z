URL: ftp://hyena.cs.umd.edu/pub/papers/adaptive.ps.Z
Refering-URL: http://www.cs.umd.edu/projects/hpsl/appl/chem2.html
Root-URL: 
Email: dybbuk@cs.umd.edu  
Title: Run-time and Compile-time Support for Adaptive Irregular Problems  
Author: Shamik D. Sharma Ravi Ponnusamy Bongki Moon Yuan-shin Hwang Raja Das Joel Saltz 
Address: College Park, MD 20742  
Affiliation: UMIACS and Dept. of Computer Science, University of Maryland,  
Abstract: In adaptive irregular problems, data arrays are accessed via indirection arrays, and data access patterns change during computation. Parallelizing such problems on distributed memory machines requires support for dynamic data partitioning, efficient preprocessing and fast data migration. This paper describes CHAOS, a library of efficient runtime primitives that provides such support. To demonstrate the effectiveness of the runtime support, two adaptive irregular applications have been parallelized using CHAOS primitives: a molecular dynamics code (CHARMM) and a code for simulating gas flows (DSMC). We have also proposed minor extensions to Fortran D which would enable compilers to parallelize irregular forall loops in such adaptive applications by embedding calls to primitives provided by a runtime library. We have implemented our proposed extensions in the Syracuse Fortran 90D/HPF prototype compiler, and have used the compiler to parallelize kernels from two adaptive applications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M.J. Berger and S. H. Bokhari. </author> <title> A partitioning strategy for nonuniform problems on multiprocessors. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-36(5):570-580, </volume> <month> May </month> <year> 1987. </year>
Reference-contexts: Static partitioning of cells across processors does not work well for DSMC. As molecules move across cells, the computational load balance deteriorates over time. Performance can be substantially improved by periodically redistributing the cells with the help of parallel partitioners such as recursive coordinate bisection (RCB) <ref> [1] </ref> and recursive inertial bisection (RIB) [21]. While these partitioners are parallelized, they are still expensive and are affordable only when the load imbalance becomes too severe. <p> S7 DISTRIBUTE irreg (map) S8 ALIGN x,y with irreg L2: do i = 1, n step L2: forall i = 1, sizeof indirection arrays S1 REDUCE (SUM, x (ia (i)), y (ib (i))) end forall end do researchers have developed a variety of heuristic methods to obtain irregular data distributions <ref> [25, 1] </ref>. The distributions produced by these methods typically result in a data structure called a maparray or translation table, that lists the processor assignment for each array element. Fortran D provides an interface that allows users to define irregular distributions and align arrays with such distributions.
Reference: [2] <author> Zeki Bozkus, Alok Choudhary, Geoffrey Fox, Tomasz Haupt, Sanjay Ranka, and Min-You Wu. </author> <title> Compiling Fortran 90D/HPF for distributed memory MIMD computers. </title> <note> To appear in the Journal of Parallel and Distributed Computing, </note> <year> 1994. </year>
Reference-contexts: The runtime support has been incorporated in the Fortran 90D compiler that is being developed at Syracuse University <ref> [2] </ref>. The Fortran 90D compiler transformations generate translated codes which embed calls to CHAOS procedures. The performance of the compiler generated code is compared with that of the hand parallelized versions. <p> Kohn and Baden [14] have developed a programming environment that is targeted towards particle computations. This programming environment supports irregular block distributions and provides facilities for dynamic load balancing. There are several compiler projects targeting distributed memory multiprocessors. These include the Fortran D compiler projects at Rice and Syracuse <ref> [9, 2] </ref> and the Vienna Fortran compiler project [6, 3] at the University of Vi-enna, among others.
Reference: [3] <author> P. Brezany, M. Gerndt, V. Sipkova, and H.P. Zima. </author> <title> SUPERB support for irregular scientific computations. </title> <booktitle> In Proceedings of the Scalable High Performance Com puting Conference (SHPCC-92), </booktitle> <pages> pages 314-321. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> April </month> <year> 1992. </year>
Reference-contexts: This programming environment supports irregular block distributions and provides facilities for dynamic load balancing. There are several compiler projects targeting distributed memory multiprocessors. These include the Fortran D compiler projects at Rice and Syracuse [9, 2] and the Vienna Fortran compiler project <ref> [6, 3] </ref> at the University of Vi-enna, among others. The Jade project at Stanford [16], the DINO project at Colorado [23], the CODE project at UT, Austin, and the Split-C project [15] at Berkeley provide parallel programming environments coupled with varying degrees of compiler support.
Reference: [4] <author> B. R. Brooks, R. E. Bruccoleri, B. D. Olafson, D. J. States, S. Swaminathan, and M. Karplus. Charmm: </author> <title> A program for macromolecular energy, minimization, and dynamics calculations. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 4:187, </volume> <year> 1983. </year>
Reference-contexts: Specifically, CHAOS introduces two new features | lightweight schedules and hashed schedule generation, which are useful in certain adaptive problems. We have used CHAOS to parallelize two challenging adaptive applications | CHARMM, a molecular dynamics code <ref> [4] </ref> and DSMC, a code that simulates gas flows [27]. We also present language support that can enable compilers to generate efficient code for adaptive applications. The Syracuse Fortran 90D/HPF compiler was used as a test-bed for the ideas presented in this paper.
Reference: [5] <author> B. R. Brooks and M. Hodoscek. </author> <title> Parallelization of charmm for mimd machines. Chemical Design Automation News, </title> <address> 7:16, </address> <year> 1992. </year>
Reference-contexts: comp: time of proc: i As can be seen from Table 1, CHARMM scaled well and good load balance was maintained up to 128 processors 1 . 1 Brooks and Hodoscek estimate that it would take 74595.5 seconds to execute the same benchmark on a single node of the iPSC/860 <ref> [5] </ref>.
Reference: [6] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 31-50, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: The chain partitioner, which has much smaller communication requirement than the recursive inertial bisection partitioner, did not show such slowdowns. 5 Compiling Adaptive Irregular Problems There are a wide range of languages such as Vienna Fortran <ref> [6] </ref>, pC++ [10], Fortran-D [9] and High Performance Fortran (HPF) [12], which provide a rich set of directives allowing users to specify desired data decompositions. With these decomposition directives, compilers can partition loop iterations and generate communication required to parallelize programs. <p> This programming environment supports irregular block distributions and provides facilities for dynamic load balancing. There are several compiler projects targeting distributed memory multiprocessors. These include the Fortran D compiler projects at Rice and Syracuse [9, 2] and the Vienna Fortran compiler project <ref> [6, 3] </ref> at the University of Vi-enna, among others. The Jade project at Stanford [16], the DINO project at Colorado [23], the CODE project at UT, Austin, and the Split-C project [15] at Berkeley provide parallel programming environments coupled with varying degrees of compiler support.
Reference: [7] <author> Raja Das, Mustafa Uysal, Joel Saltz, and Yuan-Shin Hwang. </author> <title> Communication optimizations for irregular scientific computations on distributed memory architectures. </title> <institution> Technical Report CS-TR-3163 and UMIACS-TR-93-109, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> October </month> <year> 1993. </year> <note> To appear in Journal of Parallel and Distributed Computing. </note>
Reference-contexts: These optimizations are called software caching and communication vec-torization respectively. Such optimizations have been successfully used earlier to parallelize static irregular problems <ref> [7] </ref>, in which array access patterns do not change during computation. For the static irregular problems considered in Das et al. [7], it is enough to perform preprocessing only once to optimize communication. Adaptive irregular problems are more complex. <p> These optimizations are called software caching and communication vec-torization respectively. Such optimizations have been successfully used earlier to parallelize static irregular problems <ref> [7] </ref>, in which array access patterns do not change during computation. For the static irregular problems considered in Das et al. [7], it is enough to perform preprocessing only once to optimize communication. Adaptive irregular problems are more complex. In these problems, the data access patterns may change during computation each such change results in new communication requirements. <p> First, a brief overview of the runtime support is presented; the framework is the same as that of PARTI, and has been described in earlier papers <ref> [7, 22] </ref>. We then focus on the inspector, which is a preprocessing stage that must be repeated frequently in adaptive problems. We introduce light-weight schedules which are used for fast data migration and describe how we have optimized the inspector to generate schedules efficiently. <p> Index translation involves converting the global array indices in indirection arrays into local indices. The purpose of index translation has been discussed in greater detail elsewhere <ref> [7] </ref>. Communication schedule generation involves analyzing data access patterns and performing optimizations such as software caching and communication vectorization. <p> Instead of building two separate communication schedules, loop L3 can reuse many of the off-processor elements of array y brought in by the schedule for loop L2. Thus, only an incremental schedule for loop L2 needs to be built <ref> [7] </ref>. The incremental schedule gathers only those elements of y which were not brought in by earlier schedules. Another optimization that can be applied in this example is schedule merging. <p> Generally, it is possible with existing compiler techniques to compile irregular loops where data access patterns are known only at runtime due to indirections <ref> [7, 22] </ref>. The compiler generates preprocessing code for such a loop that, at runtime, carries out the appropriate communication and places off-processor data in a pre-determined order. However, this technique does not detect reductions.
Reference: [8] <editor> J. Saltz et. al. </editor> <title> A manual for the CHAOS runtime library. </title> <type> Technical report, </type> <institution> University of Maryland, </institution> <year> 1993. </year>
Reference-contexts: A brief description of these phases follows. A: Data Partitioning : Phase A determines how data arrays are to be partitioned across processors. CHAOS supports a number of parallel partitioners that partition data arrays using heuristics based on spatial positions, computational load, connectivity, etc. <ref> [8] </ref>. These partitioners return an irregular assignment of array elements to processors, which is stored as a CHAOS construct called the translation table. A translation table is a globally accessible data structure which lists the home processor and offset address of each data array element.
Reference: [9] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <institution> Department of Computer Science Rice COMP TR90-141, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: The chain partitioner, which has much smaller communication requirement than the recursive inertial bisection partitioner, did not show such slowdowns. 5 Compiling Adaptive Irregular Problems There are a wide range of languages such as Vienna Fortran [6], pC++ [10], Fortran-D <ref> [9] </ref> and High Performance Fortran (HPF) [12], which provide a rich set of directives allowing users to specify desired data decompositions. With these decomposition directives, compilers can partition loop iterations and generate communication required to parallelize programs. <p> Kohn and Baden [14] have developed a programming environment that is targeted towards particle computations. This programming environment supports irregular block distributions and provides facilities for dynamic load balancing. There are several compiler projects targeting distributed memory multiprocessors. These include the Fortran D compiler projects at Rice and Syracuse <ref> [9, 2] </ref> and the Vienna Fortran compiler project [6, 3] at the University of Vi-enna, among others.
Reference: [10] <author> Dennis Gannon, Shelby Yang, and Peter Beckman. </author> <title> User Guide for a Portable Parallel C++ Programming System, pC++. </title> <institution> Department of Computer Science and CICA, Indiana University, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: The chain partitioner, which has much smaller communication requirement than the recursive inertial bisection partitioner, did not show such slowdowns. 5 Compiling Adaptive Irregular Problems There are a wide range of languages such as Vienna Fortran [6], pC++ <ref> [10] </ref>, Fortran-D [9] and High Performance Fortran (HPF) [12], which provide a rich set of directives allowing users to specify desired data decompositions. With these decomposition directives, compilers can partition loop iterations and generate communication required to parallelize programs.
Reference: [11] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. In Compilers and Runtime Software for Scalable Multiprocessors, </title> <editor> J. Saltz and P. Mehrotra Editors, </editor> <address> Amsterdam, The Netherlands, </address> <note> To appear 1991. Elsevier. </note>
Reference-contexts: Many compiler projects that have used runtime compilation techniques | the Fortran D project <ref> [11] </ref>, the Kali project [13], Marina Chen's work at Yale [17], the PARTI project [18, 24], the PYRROS project at Rutgers [29], the Split-C project at Berkeley [15], the Jade project at Stanford [16] and the DINO project at Col-orado [23]. 7 Conclusions The CHAOS procedures described in this paper can
Reference: [12] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: The chain partitioner, which has much smaller communication requirement than the recursive inertial bisection partitioner, did not show such slowdowns. 5 Compiling Adaptive Irregular Problems There are a wide range of languages such as Vienna Fortran [6], pC++ [10], Fortran-D [9] and High Performance Fortran (HPF) <ref> [12] </ref>, which provide a rich set of directives allowing users to specify desired data decompositions. With these decomposition directives, compilers can partition loop iterations and generate communication required to parallelize programs.
Reference: [13] <author> C. Koelbel, P. Mehrotra, and J. Van Rosendale. </author> <title> Supporting shared data structures on distributed memory architectures. </title> <booktitle> In 2nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 177-186. </pages> <publisher> ACM, </publisher> <month> March </month> <year> 1990. </year>
Reference-contexts: Many compiler projects that have used runtime compilation techniques | the Fortran D project [11], the Kali project <ref> [13] </ref>, Marina Chen's work at Yale [17], the PARTI project [18, 24], the PYRROS project at Rutgers [29], the Split-C project at Berkeley [15], the Jade project at Stanford [16] and the DINO project at Col-orado [23]. 7 Conclusions The CHAOS procedures described in this paper can be viewed as forming
Reference: [14] <author> S.R. Kohn and S.B. Baden. </author> <title> A robust parallel programming model for dynamic non-uniform scientific computations. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-94), </booktitle> <pages> pages 509-517. </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Williams [26] describes a programming environment (DIME) for parallelizing calculations with unstructured triangular meshes. Kohn and Baden <ref> [14] </ref> have developed a programming environment that is targeted towards particle computations. This programming environment supports irregular block distributions and provides facilities for dynamic load balancing. There are several compiler projects targeting distributed memory multiprocessors.
Reference: [15] <author> A. Krishnamurthy, D.E. Culler, A. Dusseau, S.C. Goldstein, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel programming in Split-C. </title> <booktitle> In Proc. of Supercomputing '93, </booktitle> <pages> pages 262-273. </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: The Jade project at Stanford [16], the DINO project at Colorado [23], the CODE project at UT, Austin, and the Split-C project <ref> [15] </ref> at Berkeley provide parallel programming environments coupled with varying degrees of compiler support. <p> Many compiler projects that have used runtime compilation techniques | the Fortran D project [11], the Kali project [13], Marina Chen's work at Yale [17], the PARTI project [18, 24], the PYRROS project at Rutgers [29], the Split-C project at Berkeley <ref> [15] </ref>, the Jade project at Stanford [16] and the DINO project at Col-orado [23]. 7 Conclusions The CHAOS procedures described in this paper can be viewed as forming part of a portable, compiler independent, runtime support library.
Reference: [16] <author> Monica Lam, Edward E. Rothberg, and Michael E. Wolf. </author> <title> The cache performance and optimizations of block algorithms. </title> <booktitle> In Proc. of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASP-LOS IV), </booktitle> <pages> pages 63-74. </pages> <publisher> ACM Press, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: There are several compiler projects targeting distributed memory multiprocessors. These include the Fortran D compiler projects at Rice and Syracuse [9, 2] and the Vienna Fortran compiler project [6, 3] at the University of Vi-enna, among others. The Jade project at Stanford <ref> [16] </ref>, the DINO project at Colorado [23], the CODE project at UT, Austin, and the Split-C project [15] at Berkeley provide parallel programming environments coupled with varying degrees of compiler support. <p> Many compiler projects that have used runtime compilation techniques | the Fortran D project [11], the Kali project [13], Marina Chen's work at Yale [17], the PARTI project [18, 24], the PYRROS project at Rutgers [29], the Split-C project at Berkeley [15], the Jade project at Stanford <ref> [16] </ref> and the DINO project at Col-orado [23]. 7 Conclusions The CHAOS procedures described in this paper can be viewed as forming part of a portable, compiler independent, runtime support library.
Reference: [17] <author> L. C. Lu and M.C. Chen. </author> <title> Parallelizing loops with indirect array references or pointers. </title> <booktitle> In Proc. of the 4th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Many compiler projects that have used runtime compilation techniques | the Fortran D project [11], the Kali project [13], Marina Chen's work at Yale <ref> [17] </ref>, the PARTI project [18, 24], the PYRROS project at Rutgers [29], the Split-C project at Berkeley [15], the Jade project at Stanford [16] and the DINO project at Col-orado [23]. 7 Conclusions The CHAOS procedures described in this paper can be viewed as forming part of a portable, compiler independent,
Reference: [18] <author> R. Mirchandaney, J. H. Saltz, R. M. Smith, D. M. Nicol, and Kay Crowley. </author> <title> Principles of runtime support for parallel processors. </title> <booktitle> In Proc. of the 1988 ACM International Conference on Supercomputing, </booktitle> <pages> pages 140-152, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: been used to parallelize the real code; however, for testing our compiler implementation, we have used a computational template which is similar to the code fragment shown here. 3 Runtime Support This section describes the principles and functionality of the CHAOS runtime support library, a superset of the PARTI library <ref> [18, 28, 24] </ref>. First, a brief overview of the runtime support is presented; the framework is the same as that of PARTI, and has been described in earlier papers [7, 22]. We then focus on the inspector, which is a preprocessing stage that must be repeated frequently in adaptive problems. <p> Many compiler projects that have used runtime compilation techniques | the Fortran D project [11], the Kali project [13], Marina Chen's work at Yale [17], the PARTI project <ref> [18, 24] </ref>, the PYRROS project at Rutgers [29], the Split-C project at Berkeley [15], the Jade project at Stanford [16] and the DINO project at Col-orado [23]. 7 Conclusions The CHAOS procedures described in this paper can be viewed as forming part of a portable, compiler independent, runtime support library.
Reference: [19] <author> B. Moon and J. Saltz. </author> <title> Adaptive runtime support for direct simulation Monte Carlo methods on distributed memory architectures. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC 94), </booktitle> <pages> pages 176-183. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1994. </year>
Reference-contexts: Partitioning cells along the direction of flow gives good load balance in such a case. For most DSMC simulations, experiments show that the chain partitioner reduced partitioning cost dramatically, while achieving nearly the same quality of load balance as RCB and RIB <ref> [19] </ref>. Information about the mapping of cells to processors was maintained in a replicated translation table. 4.3.2 Performance Results Table 4 compares the execution time of the 2-dimensional DSMC code using lightweight schedules with the execution time obtained using regular communication schedules.
Reference: [20] <author> David M. Nicol and David R. O'Hallaron. </author> <title> Improved algorithms for mapping pipelined and parallel computations. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(3) </volume> <pages> 295-306, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: While these partitioners are parallelized, they are still expensive and are affordable only when the load imbalance becomes too severe. We have also used a fast one dimensional partitioner, called the chain partitioner <ref> [20] </ref>, which takes advantage of the highly directional nature of particle flow that is observed in most DSMC simulations. For instance, in the experiments reported here, more than 70 percent of the molecules were found moving along the positive x-axis.
Reference: [21] <author> B. Nour-Omid, A. Raefsky, and G. Lyzenga. </author> <title> Solving finite element equations on concurrent computers. </title> <booktitle> In Proc. of Symposium on Parallel Computations and theis Impact on Mechanics, </booktitle> <address> Boston, </address> <month> December </month> <year> 1987. </year>
Reference-contexts: As molecules move across cells, the computational load balance deteriorates over time. Performance can be substantially improved by periodically redistributing the cells with the help of parallel partitioners such as recursive coordinate bisection (RCB) [1] and recursive inertial bisection (RIB) <ref> [21] </ref>. While these partitioners are parallelized, they are still expensive and are affordable only when the load imbalance becomes too severe.
Reference: [22] <author> Ravi Ponnusamy, Joel Saltz, and Alok Choudhary. </author> <title> Runtime-compilation techniques for data partitioning and communication schedule reuse. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 361-370. </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: We also present language support that can enable compilers to generate efficient code for adaptive applications. The Syracuse Fortran 90D/HPF compiler was used as a test-bed for the ideas presented in this paper. The compiler work described here is based on <ref> [22] </ref> a seminal paper describing compiler-embedded partitioners. The paper is structured as follows. In Section 2, we motivate our work by describing two adaptive irregular applications, CHARMM and DSMC. Section 3 describes our runtime support. Section 4 demonstrates the performance of the applications parallelized using CHAOS. <p> First, a brief overview of the runtime support is presented; the framework is the same as that of PARTI, and has been described in earlier papers <ref> [7, 22] </ref>. We then focus on the inspector, which is a preprocessing stage that must be repeated frequently in adaptive problems. We introduce light-weight schedules which are used for fast data migration and describe how we have optimized the inspector to generate schedules efficiently. <p> Arrays are associated with distributions using the Fortran D statement ALIGN. Figure 6 illustrates the use of these statements. We have used this interface to make CHAOS partitioners visible to the user. A detailed description of this interface can be found in <ref> [22] </ref>. 5.2 Computational Loop Structures The implementation of the Forall construct in Fortran D follows copy-in-copy-out semantics - Forall constructs with general loop-carried dependencies are not defined. However, a limited class of loop-carried dependencies can be specified using the intrinsic REDUCE function, inside a Forall construct. <p> Generally, it is possible with existing compiler techniques to compile irregular loops where data access patterns are known only at runtime due to indirections <ref> [7, 22] </ref>. The compiler generates preprocessing code for such a loop that, at runtime, carries out the appropriate communication and places off-processor data in a pre-determined order. However, this technique does not detect reductions. <p> However, if data access patterns do not change, the results from preprocessing can be reused. Therefore, it is important that the compiler-generated code be able to detect when preprocessing can be reused. An implementation of reusing results of preprocessing in compiler-generated code is described in <ref> [22] </ref>. In this approach, the compiler-generated code maintains a record of when statements or array intrinsics in loops may have modified indirection arrays.
Reference: [23] <author> Matthew Rosing, Robert B. Schnabel, and Robert P. Weaver. </author> <title> The DINO parallel programming language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(1) </volume> <pages> 30-42, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: There are several compiler projects targeting distributed memory multiprocessors. These include the Fortran D compiler projects at Rice and Syracuse [9, 2] and the Vienna Fortran compiler project [6, 3] at the University of Vi-enna, among others. The Jade project at Stanford [16], the DINO project at Colorado <ref> [23] </ref>, the CODE project at UT, Austin, and the Split-C project [15] at Berkeley provide parallel programming environments coupled with varying degrees of compiler support. <p> have used runtime compilation techniques | the Fortran D project [11], the Kali project [13], Marina Chen's work at Yale [17], the PARTI project [18, 24], the PYRROS project at Rutgers [29], the Split-C project at Berkeley [15], the Jade project at Stanford [16] and the DINO project at Col-orado <ref> [23] </ref>. 7 Conclusions The CHAOS procedures described in this paper can be viewed as forming part of a portable, compiler independent, runtime support library.
Reference: [24] <author> Joel Saltz, Harry Berryman, and Janet Wu. </author> <title> Multiprocessors and run-time compilation. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 3(6) </volume> <pages> 573-592, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: This paper presents a library of runtime procedures designed to efficiently implement adaptive irregular programs on distributed memory machines. This runtime library is called CHAOS; it subsumes PARTI, a library aimed at static irregular problems <ref> [24] </ref>. Specifically, CHAOS introduces two new features | lightweight schedules and hashed schedule generation, which are useful in certain adaptive problems. We have used CHAOS to parallelize two challenging adaptive applications | CHARMM, a molecular dynamics code [4] and DSMC, a code that simulates gas flows [27]. <p> been used to parallelize the real code; however, for testing our compiler implementation, we have used a computational template which is similar to the code fragment shown here. 3 Runtime Support This section describes the principles and functionality of the CHAOS runtime support library, a superset of the PARTI library <ref> [18, 28, 24] </ref>. First, a brief overview of the runtime support is presented; the framework is the same as that of PARTI, and has been described in earlier papers [7, 22]. We then focus on the inspector, which is a preprocessing stage that must be repeated frequently in adaptive problems. <p> Many compiler projects that have used runtime compilation techniques | the Fortran D project [11], the Kali project [13], Marina Chen's work at Yale [17], the PARTI project <ref> [18, 24] </ref>, the PYRROS project at Rutgers [29], the Split-C project at Berkeley [15], the Jade project at Stanford [16] and the DINO project at Col-orado [23]. 7 Conclusions The CHAOS procedures described in this paper can be viewed as forming part of a portable, compiler independent, runtime support library.
Reference: [25] <author> H. Simon. </author> <title> Partitioning of unstructured mesh problems for parallel processing. </title> <booktitle> In Proceedings of the Conference on Parallel Methods on Large Scale Structural Analysis and Physics Applications, </booktitle> <year> 1991. </year>
Reference-contexts: Since most irregular problems do not run efficiently with standard BLOCK and CYCLIC data distributions <ref> [25] </ref>, S1 REAL*8 x (N),y (N) S2 INTEGER map (N) S3 DECOMPOSITION reg (N),irreg (N) S4 DISTRIBUTE reg (block) S5 ALIGN map with reg S6 ... set map array using some partitioner S7 DISTRIBUTE irreg (map) S8 ALIGN x,y with irreg L2: do i = 1, n step L2: forall i <p> S7 DISTRIBUTE irreg (map) S8 ALIGN x,y with irreg L2: do i = 1, n step L2: forall i = 1, sizeof indirection arrays S1 REDUCE (SUM, x (ia (i)), y (ib (i))) end forall end do researchers have developed a variety of heuristic methods to obtain irregular data distributions <ref> [25, 1] </ref>. The distributions produced by these methods typically result in a data structure called a maparray or translation table, that lists the processor assignment for each array element. Fortran D provides an interface that allows users to define irregular distributions and align arrays with such distributions.
Reference: [26] <author> R. Williams. </author> <title> Performance of dynamic load balancing algorithms for unstructured mesh calculations. </title> <journal> Concurrency, Practice and Experience, </journal> <volume> 3(5) </volume> <pages> 457-482, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: The table presents the time for executing the DSMC loop 50 times on the Intel iPSC/860. 6 Related Work Several researchers have developed programming environments that target particular classes of irregular or adaptive problems. Williams <ref> [26] </ref> describes a programming environment (DIME) for parallelizing calculations with unstructured triangular meshes. Kohn and Baden [14] have developed a programming environment that is targeted towards particle computations. This programming environment supports irregular block distributions and provides facilities for dynamic load balancing.
Reference: [27] <author> Richard G. Wilmoth. </author> <title> Direct simulation Monte Carlo analysis of rarefied flows on paralle processors. </title> <journal> AIAA Journal of Thermophysics and Heat Transfer, </journal> <volume> 5(3) </volume> <pages> 292-300, </pages> <month> July-Sept. </month> <year> 1991. </year>
Reference-contexts: Specifically, CHAOS introduces two new features | lightweight schedules and hashed schedule generation, which are useful in certain adaptive problems. We have used CHAOS to parallelize two challenging adaptive applications | CHARMM, a molecular dynamics code [4] and DSMC, a code that simulates gas flows <ref> [27] </ref>. We also present language support that can enable compilers to generate efficient code for adaptive applications. The Syracuse Fortran 90D/HPF compiler was used as a test-bed for the ideas presented in this paper. The compiler work described here is based on [22] a seminal paper describing compiler-embedded partitioners.
Reference: [28] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berryman. </author> <title> Runtime compilation methods for multicomputers. </title> <booktitle> In Proceedings of the 1991 ICPP , volume 2, </booktitle> <pages> pages 26-30, </pages> <year> 1991. </year>
Reference-contexts: been used to parallelize the real code; however, for testing our compiler implementation, we have used a computational template which is similar to the code fragment shown here. 3 Runtime Support This section describes the principles and functionality of the CHAOS runtime support library, a superset of the PARTI library <ref> [18, 28, 24] </ref>. First, a brief overview of the runtime support is presented; the framework is the same as that of PARTI, and has been described in earlier papers [7, 22]. We then focus on the inspector, which is a preprocessing stage that must be repeated frequently in adaptive problems.
Reference: [29] <author> Tao Yang and Apostolos Gerasoulis. </author> <title> Pyrros: Static scheduling and code generation for message passing multiprocessors. </title> <booktitle> In Proc. of 6th ACM International conference on Supercomputing, </booktitle> <pages> pages 428-437, </pages> <address> Wash-ington, D.C., </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Many compiler projects that have used runtime compilation techniques | the Fortran D project [11], the Kali project [13], Marina Chen's work at Yale [17], the PARTI project [18, 24], the PYRROS project at Rutgers <ref> [29] </ref>, the Split-C project at Berkeley [15], the Jade project at Stanford [16] and the DINO project at Col-orado [23]. 7 Conclusions The CHAOS procedures described in this paper can be viewed as forming part of a portable, compiler independent, runtime support library.
References-found: 29

