URL: http://www.cs.berkeley.edu/~rfromm/Courses/FA96/cs262/ctx.ps
Refering-URL: http://www.cs.berkeley.edu/~rfromm/Courses/FA96/cs262/
Root-URL: 
Email: frfromm,treuhaftg@cs.berkeley.edu  
Title: Revisiting the Cache Interference Costs of Context Switching  
Author: Richard Fromm and Noah Treuhaft 
Address: California-Berkeley  
Affiliation: Computer Science Division, University of  
Abstract: The high cost of context switching is one reason that operating system performance is not keeping pace with hardware improvements. Besides the cost of saving and restoring registers, another component of context switch cost is the cache interference between multiple processes sharing the same cache. We measured kernel on user, user on kernel, and user on user cache interference from context switching using a complete machine simulator. While the resulting cache interference is noticeable, the time between process switches is generally sufficient to amortize this cost over a large number of instructions. Completely eliminating all contextswitch cache interference would therefore have a minimal impact on total execution time. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> AGARWAL, A., JOHN, H., AND HOROWITZ, M. </author> <title> Cache performance of operating system and multiprogramming workloads. </title> <journal> In ACM Transactions on Computer Systems (Nov. </journal> <year> 1988), </year> <pages> pp. 393-431. </pages>
Reference-contexts: Code and data from the kernel can interfere with those of user processes and vice-versa, and user processes can interfere with each other. Previous work suggests that the cost of cache interference between processes can dwarf all other costs associated with context switches <ref> [1] </ref> [9] [11]. Cache misses are sometimes classified by the three C's model, which decomposes cache behavior into compulsory, capacity, and conflict misses. In a multiprocessor, there is also a fourth C, coherency misses. <p> While useful for comparing the performance of one system to another (memory latency becomes another determining factor of context switch performance), it does not yield insight into how much cache interference occurs in realistic workloads. Agarwal et al. <ref> [1] </ref> investigated the effects of operating systems and multiprogramming on cache performance. They compared the behavior of uniprocess caches and multiprocess caches that either flush on context switches or store process-identifiers (PIDs) for every cache block. <p> (as well as the huge numbers of transistors available in the future) may make various implementations to reduce the cache interference costs of context switching feasible, the decrease in latency may counteract such desires by decreasing the overall benefits of doing so. 5 Implications Given the assertions of other researchers <ref> [1] </ref> [9] [11] that cache effects account for a significant portion of the cost of context switching, it is useful examine how our results relate to these assertions. Our results are in fact not as contradictory to these previous works as one might expect. <p> An IRAM has tremendous on-chip bandwidth, and we are seeking novel ways of exploiting this. We initially intended to investigate this from the viewpoint of saving and restoring register state. After reading papers such as <ref> [1] </ref>, [9], and [11], we decided to switch direction and concentrate on the cache effects of context switching. Given our conclusion that the cache effects do not appear to be as important as we had originally thought, it would be worthwhile to return to investigating the register costs.
Reference: [2] <author> ANDERSON, T. E., ET AL. </author> <title> The interaction of architecture and operating system design. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (Santa Clara, </booktitle> <address> CA, </address> <month> Apr. </month> <year> 1991), </year> <pages> pp. 108-20. </pages>
Reference-contexts: 1 Introduction Microprocessor performance has been improving steadily at the remarkable rate of 60% per year [5]. Operating system performance, however, is not keeping pace [10] <ref> [2] </ref>. One reason for this disparity between OS and hardware performance is the high cost of context switching. There are numerous components that contribute to the cost of context switching among different processes, each in their own address space. <p> The results are contained in Section 4, and their implications for future microprocessor designs are discussed in Section 5. Finally, Section 6 describes future work, and Section 7 presents our conclusions. 2 Related Work Both Ousterhaut [10] and Anderson et al. <ref> [2] </ref> measured context switch performance on various machines using microbench-marks to illustrate that the relative performance of primitive OS functions was not scaling with the base speed of the underlying hardware. <p> They computed the cache cost of context switching on a DECstation-5000 to be in the range of 10 to 400 s. They compared this to a measured OS context switch time of 70 s [10] and an estimated minimum cost of 7.4 s <ref> [2] </ref>, which shows that the cost of refilling the cache can be quite large compared to the cost of saving and restoring registers. They also hypothesized that the cache performance costs of context switching will increase as the performance gap between processors and memory continues to grow.
Reference: [3] <author> CHEN, J. B., AND BERSHAD, B. N. </author> <title> The impact of operating system structure on memory system performance. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating Systems Principle (Aschville, </booktitle> <address> NC, </address> <month> Dec. </month> <year> 1993), </year> <pages> pp. 120-33. </pages>
Reference-contexts: Furthermore, the authors admit that their address tracing implementation runs on an obsolete architecture with excessively long OS code paths, although neither the architecture nor the OS is identified. Chen and Bershad <ref> [3] </ref> evaluated the impact of operating system structure on memory system behavior by comparing a monolithic OS (Ultrix) and a microkernel OS (Mach) on the same hardware.
Reference: [4] <author> CVETANOVIC, Z., AND BHANDARKAR, D. </author> <title> Performance characterization of the Alpha 21164 microprocessor using TP and SPEC workloads. </title> <booktitle> In Proceedings, Second International Symposium on High-Performance Computer Architecture (San Jose, </booktitle> <address> CA, </address> <month> February </month> <year> 1996), </year> <pages> pp. 270-280. </pages>
Reference-contexts: This growing gap between processor and memory performance places a much greater emphasis on the design of the memory system; for some applications, a microprocessor can spend up to 75% of its time <ref> [4] </ref> waiting for memory accesses. As the speed gap between processors and memory continues to grow, the importance of the cache interference caused by context switching will increase.
Reference: [5] <author> HENNESSY, J., AND PATTERSON, D. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc, </publisher> <address> San Francisco, CA, </address> <year> 1996. </year>
Reference-contexts: 1 Introduction Microprocessor performance has been improving steadily at the remarkable rate of 60% per year <ref> [5] </ref>. Operating system performance, however, is not keeping pace [10] [2]. One reason for this disparity between OS and hardware performance is the high cost of context switching. There are numerous components that contribute to the cost of context switching among different processes, each in their own address space. <p> One way to view the interference resulting from contention amongst multiple processes is as a fifth C, context switch misses. While processors keep getting faster, memory latency is improving at a much lower rate, decreasing by only about 7% per year <ref> [5] </ref>. This growing gap between processor and memory performance places a much greater emphasis on the design of the memory system; for some applications, a microprocessor can spend up to 75% of its time [4] waiting for memory accesses. <p> Alternatively, the operating system can guarantee similar behavior by assigning its pages so that the index bits do not change during address translation, a technique known as page coloring. Correct operation could also be ensured by storing PIDs along with the address tags. See pages 442-425 of <ref> [5] </ref> for more details. 5.4.2 Resource Allocation Another practical difficulty with implementing multiple per-process caches is the amount of hardware resources required. One question to be asked is how many caches are enough? One possibility would be to have two: one cache for user processes and one for kernel processes.
Reference: [6] <author> HERROD, S., ET AL. </author> <title> The SimOS simulation environment. </title> <address> http://www-flash.stanford.edu/SimOS, Sept. </address> <year> 1996. </year>
Reference-contexts: The high memory bandwidth available in IRAM (processor-memory integration) architectures [12] [13] makes possible some of these ideas which previously may not have been seriously considered. In this paper we quantify the cache interference effects of context switching in multiprogrammed environments. We used SimOS, a complete machine simulator [15] <ref> [6] </ref>, to run various multiprogrammed workloads. We modified the simulator to determine both interference and cooperation between contexts and to characterize the run-lengths of processes and determine the causes of context switches. <p> SimOS is a complete software simulation of a MIPS R4000-based computer system, including an accurate memory system model (with two levels of caches) and an accurate disk model. It is capable of booting a slightly-modified IRIX 5.3 kernel and running unmodified IRIX binaries [15] <ref> [6] </ref>. SimOS provides a great deal of information about various hardware events, including those at the OS level.
Reference: [7] <author> MAYNARD, A. M. G., ET AL. </author> <title> Contrasting characteristics and cache performance of technical and multiuser commercial workloads. </title> <booktitle> In Proceedings of the Sixth Symposium on Architectural Support for Programming Languages and Operating Systems (San Jose, </booktitle> <address> CA, </address> <month> Oct. </month> <year> 1994), </year> <pages> pp. 145-155. </pages>
Reference-contexts: It is therefore questionable whether one would want to use an IRAM to minimize the cache interference effects, even if other benchmarks produced results showing greater interference than our study indicates. 6 Future Work It has been noted by researchers at IBM <ref> [7] </ref> that commercial applications, such as databases and software development, have very different behavior than technical applications, such as simulations and scientific computing.
Reference: [8] <author> MCVOY, L., AND STAELIN, C. lmbench: </author> <title> Portable tools for performance analysis. </title> <booktitle> In The Proceedings of the USENIX 1996 Annual Technical Conference (San Diego, </booktitle> <address> CA, </address> <month> Jan. </month> <year> 1996), </year> <pages> pp. 279-294. </pages>
Reference-contexts: Neither of these studies addressed the cache effects of context switching. Lmbench is a microbenchmark suite that measures the latency and bandwidth of common operating system services, including context switching <ref> [8] </ref>. In addition to varying the number of processes, the lmbench context switch microbenchmark also attempts to account for the cache interference costs of context switching by varying the cache footprint of each process by having each process allocate and access an array of a specified size.
Reference: [9] <author> MOGUL, J. C., AND BORG, A. </author> <title> The effect of context switches on cache performance. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (Santa Clara, </booktitle> <address> CA, </address> <month> Apr. </month> <year> 1991), </year> <pages> pp. 75-84. </pages>
Reference-contexts: Code and data from the kernel can interfere with those of user processes and vice-versa, and user processes can interfere with each other. Previous work suggests that the cost of cache interference between processes can dwarf all other costs associated with context switches [1] <ref> [9] </ref> [11]. Cache misses are sometimes classified by the three C's model, which decomposes cache behavior into compulsory, capacity, and conflict misses. In a multiprocessor, there is also a fourth C, coherency misses. <p> Furthermore, the VAX architecture examined in the study, while popular at the time, does not reflect contemporary system designs. Mogul and Borg <ref> [9] </ref> measured the effect of context switches on cache performance. They computed the cache cost of context switching on a DECstation-5000 to be in the range of 10 to 400 s. <p> well as the huge numbers of transistors available in the future) may make various implementations to reduce the cache interference costs of context switching feasible, the decrease in latency may counteract such desires by decreasing the overall benefits of doing so. 5 Implications Given the assertions of other researchers [1] <ref> [9] </ref> [11] that cache effects account for a significant portion of the cost of context switching, it is useful examine how our results relate to these assertions. Our results are in fact not as contradictory to these previous works as one might expect. <p> Both <ref> [9] </ref> and [11] summarized their results by estimating the time lost on a context switch due to cache interference. They compared this time to the cost to execute the kernel code to perform a context switch. <p> Mogul and Borg <ref> [9] </ref> concluded that the context switch cache effects were 10 to 400 s on a 25 MHz DECstation-5000. <p> An IRAM has tremendous on-chip bandwidth, and we are seeking novel ways of exploiting this. We initially intended to investigate this from the viewpoint of saving and restoring register state. After reading papers such as [1], <ref> [9] </ref>, and [11], we decided to switch direction and concentrate on the cache effects of context switching. Given our conclusion that the cache effects do not appear to be as important as we had originally thought, it would be worthwhile to return to investigating the register costs.
Reference: [10] <author> OUSTERHOUT, J. K. </author> <title> Why aren't operating systems getting faster as fast as hardware? In Proceedings of the USENIX Summer Conference (Anaheim, </title> <address> CA, </address> <month> June </month> <year> 1990). </year>
Reference-contexts: 1 Introduction Microprocessor performance has been improving steadily at the remarkable rate of 60% per year [5]. Operating system performance, however, is not keeping pace <ref> [10] </ref> [2]. One reason for this disparity between OS and hardware performance is the high cost of context switching. There are numerous components that contribute to the cost of context switching among different processes, each in their own address space. <p> Section 3 describes the methodology used for our simulations. The results are contained in Section 4, and their implications for future microprocessor designs are discussed in Section 5. Finally, Section 6 describes future work, and Section 7 presents our conclusions. 2 Related Work Both Ousterhaut <ref> [10] </ref> and Anderson et al. [2] measured context switch performance on various machines using microbench-marks to illustrate that the relative performance of primitive OS functions was not scaling with the base speed of the underlying hardware. <p> Mogul and Borg [9] measured the effect of context switches on cache performance. They computed the cache cost of context switching on a DECstation-5000 to be in the range of 10 to 400 s. They compared this to a measured OS context switch time of 70 s <ref> [10] </ref> and an estimated minimum cost of 7.4 s [2], which shows that the cost of refilling the cache can be quite large compared to the cost of saving and restoring registers.
Reference: [11] <author> PADMANABHAN, V., AND ROSELLI, D. </author> <title> The real cost of context switching, </title> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Code and data from the kernel can interfere with those of user processes and vice-versa, and user processes can interfere with each other. Previous work suggests that the cost of cache interference between processes can dwarf all other costs associated with context switches [1] [9] <ref> [11] </ref>. Cache misses are sometimes classified by the three C's model, which decomposes cache behavior into compulsory, capacity, and conflict misses. In a multiprocessor, there is also a fourth C, coherency misses. <p> They concluded that, while measure-able (between 0.003 and 0.074 memory cycles per instruction), the effect of user and kernel interference did not significantly influence overall performance. A limitation of their approach, however, is that they examined the effect of user-kernel interference only on uniprogramming workloads. Padmanabhan and Roselli <ref> [11] </ref> compared the cache effects of multiprogramming to the time to execute the kernel code that performs a context switch. They concluded that the cache effects (80-160 s) dominated the kernel code execution time (20-30 s) on a SPARC 10. Their methodology contained a number of shortcomings. <p> as the huge numbers of transistors available in the future) may make various implementations to reduce the cache interference costs of context switching feasible, the decrease in latency may counteract such desires by decreasing the overall benefits of doing so. 5 Implications Given the assertions of other researchers [1] [9] <ref> [11] </ref> that cache effects account for a significant portion of the cost of context switching, it is useful examine how our results relate to these assertions. Our results are in fact not as contradictory to these previous works as one might expect. <p> Both [9] and <ref> [11] </ref> summarized their results by estimating the time lost on a context switch due to cache interference. They compared this time to the cost to execute the kernel code to perform a context switch. <p> The cache effects are, in general, larger, so they both concluded that the cache effects of context switching are a significant issue. However, neither study took the next important step of translating the cache interference cost into its final impact on execution time. Padmanabhan and Roselli <ref> [11] </ref> concluded that the context switch cache effects were, on average, 80-160 s on a 50 MHz SPARC 10. The time slice on their system was 40 ms. <p> Even for the workloads with a wider range of times between user context switches, the results are skewed toward the higher end of the range. Suppose we assume (pessimistically) that the average time between context switches in <ref> [11] </ref> was half of the time slice, 40 ms, and that the cache interference cost was at the high end of the range, 160 s. <p> While this is an order of magnitude higher than the calculation based on <ref> [11] </ref>, it is still not much cause for celebration. This simple analysis was based on pessimistic costs, and a more realistic evaluation would likely yield lower benefits. <p> An IRAM has tremendous on-chip bandwidth, and we are seeking novel ways of exploiting this. We initially intended to investigate this from the viewpoint of saving and restoring register state. After reading papers such as [1], [9], and <ref> [11] </ref>, we decided to switch direction and concentrate on the cache effects of context switching. Given our conclusion that the cache effects do not appear to be as important as we had originally thought, it would be worthwhile to return to investigating the register costs.
Reference: [12] <author> PATTERSON, D., ANDERSON, T., AND YELICK, K. </author> <title> A case for intelligent DRAM: </title> <booktitle> IRAM. In HotChips VIII (Stan-ford, </booktitle> <address> CA, </address> <year> 1996), </year> <pages> pp. 75-93. </pages>
Reference-contexts: Possible ideas for reducing the cache interference effects of context switching including saving and restoring cache state, maintaining private caches for user processes, providing separate user and kernel caches, and prefetching of cache state on context switches. The high memory bandwidth available in IRAM (processor-memory integration) architectures <ref> [12] </ref> [13] makes possible some of these ideas which previously may not have been seriously considered. In this paper we quantify the cache interference effects of context switching in multiprogrammed environments. We used SimOS, a complete machine simulator [15] [6], to run various multiprogrammed workloads. <p> Finally, a promising technique for future microprocessor design involves the integration of a processor along with DRAM main memory on a single die, known as an Intelligent DRAM (IRAM) <ref> [12] </ref> [13]. An IRAM design may offer 100 times the bandwidth to main memory at 1/10 of the latency of a traditional system.
Reference: [13] <author> PATTERSON, D., ET AL. </author> <title> Intelligent ram (IRAM): Chips that remember and compute. </title> <booktitle> In To appear in 1997 IEEE International Solid-State Circuits Conference (San Fran-cisco, </booktitle> <address> CA, </address> <month> Feb. </month> <year> 1997). </year>
Reference-contexts: Possible ideas for reducing the cache interference effects of context switching including saving and restoring cache state, maintaining private caches for user processes, providing separate user and kernel caches, and prefetching of cache state on context switches. The high memory bandwidth available in IRAM (processor-memory integration) architectures [12] <ref> [13] </ref> makes possible some of these ideas which previously may not have been seriously considered. In this paper we quantify the cache interference effects of context switching in multiprogrammed environments. We used SimOS, a complete machine simulator [15] [6], to run various multiprogrammed workloads. <p> Finally, a promising technique for future microprocessor design involves the integration of a processor along with DRAM main memory on a single die, known as an Intelligent DRAM (IRAM) [12] <ref> [13] </ref>. An IRAM design may offer 100 times the bandwidth to main memory at 1/10 of the latency of a traditional system.
Reference: [14] <author> PERL, S. E., AND SITES, R. L. </author> <title> Studies of windows nt performance using dynamic execution traces. </title> <booktitle> In Proceedings of the USENIX 2nd Symposium on Operating System Design and Implementation (Seattle, </booktitle> <address> WA, </address> <month> Oct. </month> <year> 1996). </year>
Reference-contexts: Kernel time in the fp benchmark comes in occasional bursts, and is practically nonexistent in the int benchmark. 4.2 Miss Rates Previous researchers have noted that kernel code is generally less well behaved than user code and has higher miss rates <ref> [14] </ref>. This behavior is readily apparent in gmake. In pipe-ugw, the absolute kernel stall time is less than the user stall time, but the ratio of kernel stall time to kernel execution time appears to be greater than that of user stall time to user execution time.
Reference: [15] <author> ROSENBLUM, M., ET AL. </author> <title> Complete computer system simulation: The SimOS approach. </title> <booktitle> In IEEE Parallel and Distributed Technology: Systems and Applications (Winter 1995), </booktitle> <volume> vol. 3, </volume> <pages> pp. 34-43. </pages>
Reference-contexts: The high memory bandwidth available in IRAM (processor-memory integration) architectures [12] [13] makes possible some of these ideas which previously may not have been seriously considered. In this paper we quantify the cache interference effects of context switching in multiprogrammed environments. We used SimOS, a complete machine simulator <ref> [15] </ref> [6], to run various multiprogrammed workloads. We modified the simulator to determine both interference and cooperation between contexts and to characterize the run-lengths of processes and determine the causes of context switches. <p> SimOS is a complete software simulation of a MIPS R4000-based computer system, including an accurate memory system model (with two levels of caches) and an accurate disk model. It is capable of booting a slightly-modified IRIX 5.3 kernel and running unmodified IRIX binaries <ref> [15] </ref> [6]. SimOS provides a great deal of information about various hardware events, including those at the OS level.
References-found: 15

