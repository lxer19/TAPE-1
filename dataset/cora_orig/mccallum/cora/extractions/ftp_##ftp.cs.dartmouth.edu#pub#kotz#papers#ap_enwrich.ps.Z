URL: ftp://ftp.cs.dartmouth.edu/pub/kotz/papers/ap:enwrich.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/research/starfish/papers.html
Root-URL: http://www.cs.dartmouth.edu
Email: ap@cs.duke.edu carla@cs.duke.edu dfk@cs.dartmouth.edu  
Title: From the Fourth Workshop on I/O in Parallel and Distributed Systems, ENWRICH: A Compute-Processor Write
Author: Apratim Purakayastha Carla Schlatter Ellis David Kotz 
Address: Durham NC 27708-0129 Durham NC 27708-0129 Hanover NH 03755-3510  
Affiliation: Department of Computer Science Department of Computer Science Department of Computer Science Duke University Duke University Dartmouth College  
Date: 55-68, May 1996.  
Note: pp.  Available at ftp://ftp.cs.dartmouth.edu/pub/kotz/papers/ap:enwrich.ps.Z Copyright 1996 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that new copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abtracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Publications Dept., ACM Inc., fax +1(212)869-0481, or &lt;permissions@acm.org&gt;.  
Abstract: Many parallel scientific applications need high-performance I/O. Unfortunately, end-to-end parallel-I/O performance has not been able to keep up with substantial improvements in parallel-I/O hardware because of poor parallel file-system software. Many radical changes, both at the interface level and the implementation level, have recently been proposed. One such proposed interface is collective I/O, which allows parallel jobs to request transfer of large contiguous objects in a single request, thereby preserving useful semantic information that would otherwise be lost if the transfer were expressed as per-processor non-contiguous requests. Kotz has proposed disk-directed I/O as an efficient implementation technique for collective-I/O operations, where the compute processors make a single collective data-transfer request, and the I/O processors thereafter take full control of the actual data transfer, exploiting their detailed knowledge of the disk layout to attain substantially improved performance. Recent parallel file-system usage studies show that writes to write-only files are a dominant part of the workload. Therefore, optimizing writes could have a significant impact on overall performance. In this paper, we propose ENWRICH, a compute-processor write-caching scheme for write-only files in parallel file systems. ENWRICH combines low-overhead write caching at the compute processors with high performance disk-directed I/O at the I/O processors to achieve both low latency and high bandwidth. This combination facilitates the use of the powerful disk-directed I/O technique independent of any particular choice of interface. By collecting writes over many files and applications, ENWRICH lets the I/O processors optimize disk I/O over a large pool of requests. We evaluate our design via simulated implementation and show that EN-WRICH achieves high performance for various configurations and workloads. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. E. Anderson, M. D. Dahlin, J. M. Neefe, D. A. Patterson, D. S. Roselli, and R. Y. Wang. </author> <title> Serverless network file systems. </title> <booktitle> In Proceedings of 15th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 109-126. </pages> <institution> Association for Computing Machinery SIGOPS, </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: In the Scotch Parallel File System, targeted for a network-of-workstations supercomputing environment, client write caches provide weakly consistent file-sharing with the help of propagate and expunge mechanisms and explicit barriers to avoid read/write data hazards [13]. The xFS <ref> [1] </ref> exploits high speeds offered by ATM networks by using a technique called cooperative-caching. This technique allows a client to access file data directly from another client's memory. Cache consistency in xFS is maintained by a token-based scheme.
Reference: [2] <author> Mary Baker, Satoshi Asami, Etienne Deprit, John Ousterhout, and Margo Seltzer. </author> <title> Non-Volatile memory for fast, reliable file systems. </title> <booktitle> In Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 10-22, </pages> <year> 1992. </year>
Reference-contexts: Biswas et al. show that a small amount of NV-RAM (about 1 MB) is sufficient to provide large performance gains [4]. Baker et al. show similar gains using a persistent write cache with a log-structured file system <ref> [2] </ref>. In ENWRICH, we draw on some of these results in the uniprocessor domain and apply them to multiprocessors. 2.3 Recent Advances in Parallel I/O Earlier generations of parallel file systems like Intel CFS extended the Unix interface with file-pointer modes for parallel access [5, 28].
Reference: [3] <author> Michael L. Best, Adam Greenberg, Craig Stanfill, and Lewis W. Tucker. </author> <title> CMMD I/O: A parallel Unix I/O. </title> <booktitle> In Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <pages> pages 489-495, </pages> <year> 1993. </year>
Reference-contexts: For example, many exhibit forward-jumping sequential patterns [24, 31], many of which are actually complex strided patterns [26, 27]. Clearly, parallel file systems must be redesigned to fit these common access patterns. Several recent works have proposed changes to the file-system interface <ref> [3, 7, 8, 10, 12, 18, 19, 26] </ref>. One such proposed interface is collective I/O. <p> The Scalable File System (SFS) on the CM-5 also provides a Unix-like interface, but has an additional collective-I/O interface <ref> [3] </ref>. Among more recent parallel file systems, Vesta [8] allows users to specify both the logical partitioning of file data among the CPs and the physical partitioning of file blocks across the disks. Some recent proposals focus on devising and implementing new and meaningful interfaces.
Reference: [4] <author> Prabuddha Biswas, K.K. Ramakrishnan, and Don Towsley. </author> <title> Trace driven analysis of write caching policies for disks. </title> <booktitle> In Proceedings of the ACM SIGMETRICS, </booktitle> <pages> pages 13-23, </pages> <year> 1993. </year>
Reference-contexts: NV-RAMs allow write caches to use write-behind strategies, thereby reducing the number of disk-write operations. Biswas et al. show that a small amount of NV-RAM (about 1 MB) is sufficient to provide large performance gains <ref> [4] </ref>. Baker et al. show similar gains using a persistent write cache with a log-structured file system [2].
Reference: [5] <author> Rajesh Bordawekar, Alok Choudhary, and Juan Miguel Del Rosario. </author> <title> An experimental performance evaluation of Touchstone Delta Concurrent File System. </title> <booktitle> In Proceedings of the 7th ACM International Conference on Supercomputing, </booktitle> <pages> pages 367-376, </pages> <year> 1993. </year>
Reference-contexts: In ENWRICH, we draw on some of these results in the uniprocessor domain and apply them to multiprocessors. 2.3 Recent Advances in Parallel I/O Earlier generations of parallel file systems like Intel CFS extended the Unix interface with file-pointer modes for parallel access <ref> [5, 28] </ref>. The Scalable File System (SFS) on the CM-5 also provides a Unix-like interface, but has an additional collective-I/O interface [3].
Reference: [6] <author> Eric A. Brewer, Chrysanthos N. Dellarocas, Adrian Colbrook, and William E. Weihl. Proteus: </author> <title> A high-performance parallel-architecture simulator. </title> <type> Technical Report MIT/LCS/TR-516, </type> <institution> MIT, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: high throughput via DDIO, this approach combines low latency and high throughput for writes, a desired but so far unreached goal in existing parallel file systems. 4 Evaluation Method We have implemented ENWRICH on top of the STARFISH 2 simulator, which is further based on the Proteus parallel architecture simulator <ref> [6] </ref>. Proteus is an execution-driven simulator that provides a 2 More information about STARFISH, including source code, can be found at the URL http://www.cs.dartmouth.edu/research/starfish. generic framework with basic message-passing capabilities, inter-processor interrupts, thread operations, and shared memory. Pro-teus runs as a single multithreaded process on a uniprocessor.
Reference: [7] <author> Peter Corbett, Dror Feitelson, Sam Fineberg, Yarsun Hsu, Bill Nitzberg, Jean-Pierre Prost, Marc Snir, Bernard Traversat, and Parkson Wong. </author> <title> Overview of the MPI-IO parallel I/O interface. </title> <booktitle> In IPPS '95 Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 1-15, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: For example, many exhibit forward-jumping sequential patterns [24, 31], many of which are actually complex strided patterns [26, 27]. Clearly, parallel file systems must be redesigned to fit these common access patterns. Several recent works have proposed changes to the file-system interface <ref> [3, 7, 8, 10, 12, 18, 19, 26] </ref>. One such proposed interface is collective I/O. <p> Some recent proposals focus on devising and implementing new and meaningful interfaces. Corbett et al. propose MPI-IO, which models I/O as message passing and allows programmers to express I/O with program datatypes rather than byte offsets within a file <ref> [7] </ref>. Nieuwejaar and Kotz propose a nested-batched interface for complex access patterns [26].
Reference: [8] <author> Peter F. Corbett, Dror G. Feitelson, Jean-Pierre Prost, and San-dra Johnson Baylor. </author> <title> Parallel access to files in the Vesta file system. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 472-481, </pages> <year> 1993. </year>
Reference-contexts: For example, many exhibit forward-jumping sequential patterns [24, 31], many of which are actually complex strided patterns [26, 27]. Clearly, parallel file systems must be redesigned to fit these common access patterns. Several recent works have proposed changes to the file-system interface <ref> [3, 7, 8, 10, 12, 18, 19, 26] </ref>. One such proposed interface is collective I/O. <p> The Scalable File System (SFS) on the CM-5 also provides a Unix-like interface, but has an additional collective-I/O interface [3]. Among more recent parallel file systems, Vesta <ref> [8] </ref> allows users to specify both the logical partitioning of file data among the CPs and the physical partitioning of file blocks across the disks. Some recent proposals focus on devising and implementing new and meaningful interfaces.
Reference: [9] <author> Thomas H. Cormen and David Kotz. </author> <title> Integrating theory and practice in parallel file systems. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 64-74, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year> <institution> Dartmouth Institute for Advanced Graduate Studies. </institution> <note> Revised from Dartmouth PCS-TR93-188. </note>
Reference-contexts: In this paper we propose one technique that can dramatically improve file-system performance. Most current multiprocessor file systems are derivatives of Unix file systems. Typical Unix workloads [29, 11], however, differ significantly from scientific multiprocessor workloads [24, 31]. Scientific programs use files for checkpointing, application-controlled virtual memory <ref> [9, 12] </ref>, and visualization output, which are not common in Unix workloads. Furthermore, parallel scientific programs exhibit patterns that are more complicated than simple sequential patterns observed in vector scientific or Unix work-loads.
Reference: [10] <author> Juan Miguel del Rosario, Rajesh Bordawekar, and Alok Choudhary. </author> <title> Improved parallel I/O via a two-phase run-time access strategy. </title> <booktitle> In IPPS '93 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <pages> pages 56-70, </pages> <year> 1993. </year> <note> Also published in Computer Architecture News 21(5), </note> <month> December </month> <year> 1993, </year> <pages> pages 31-38. </pages>
Reference-contexts: For example, many exhibit forward-jumping sequential patterns [24, 31], many of which are actually complex strided patterns [26, 27]. Clearly, parallel file systems must be redesigned to fit these common access patterns. Several recent works have proposed changes to the file-system interface <ref> [3, 7, 8, 10, 12, 18, 19, 26] </ref>. One such proposed interface is collective I/O. <p> Two-phase I/O, proposed by Del Rosario et al., is an efficient implementation of a large transfer operation where data is permuted in CP memory before a collective I/O operation so that the I/O operation conforms to the actual file layout for better I/O performance <ref> [10] </ref>. <p> Workload. Most of our experiments used synthetic access patterns that were derived from different modes of distributing two-dimensional matrices in CP memories, as suggested by those available in High Performance Fortran <ref> [15, 10] </ref> (Figure 3, adapted from [19]). Elements in each dimension of a multidimensional array could be mapped fully in one CP (denoted as NONE), distributed in contiguous segments among the CPs (BLOCK 5 ), or distributed 4 ENWRICH cache size was larger than that of TC.
Reference: [11] <author> R. Floyd. </author> <title> Short-term file reference patterns in a UNIX environment. </title> <type> Technical Report 177, </type> <institution> Dept. of Computer Science, Univ. of Rochester, </institution> <month> March </month> <year> 1986. </year>
Reference-contexts: This shortfall is largely due to poor performance of the file-system software. In this paper we propose one technique that can dramatically improve file-system performance. Most current multiprocessor file systems are derivatives of Unix file systems. Typical Unix workloads <ref> [29, 11] </ref>, however, differ significantly from scientific multiprocessor workloads [24, 31]. Scientific programs use files for checkpointing, application-controlled virtual memory [9, 12], and visualization output, which are not common in Unix workloads.
Reference: [12] <author> N. Galbreath, W. Gropp, and D. Levine. </author> <title> Applications-driven parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 462-471, </pages> <year> 1993. </year> <title> a contiguous layout. There were 16 CPs and 8 IOPs in all the experiments. The ENWRICH cache size was fixed at 1 MB per CP. </title>
Reference-contexts: In this paper we propose one technique that can dramatically improve file-system performance. Most current multiprocessor file systems are derivatives of Unix file systems. Typical Unix workloads [29, 11], however, differ significantly from scientific multiprocessor workloads [24, 31]. Scientific programs use files for checkpointing, application-controlled virtual memory <ref> [9, 12] </ref>, and visualization output, which are not common in Unix workloads. Furthermore, parallel scientific programs exhibit patterns that are more complicated than simple sequential patterns observed in vector scientific or Unix work-loads. <p> For example, many exhibit forward-jumping sequential patterns [24, 31], many of which are actually complex strided patterns [26, 27]. Clearly, parallel file systems must be redesigned to fit these common access patterns. Several recent works have proposed changes to the file-system interface <ref> [3, 7, 8, 10, 12, 18, 19, 26] </ref>. One such proposed interface is collective I/O.
Reference: [13] <author> Garth A. Gibson, Daniel Stodolsky, Pay W. Chang, William V. Courtwright II, Chris G. Demetriou, Eka Ginting, Mark Hol-land, Qingming Ma, LeAnn Neal, R. Hugo Patterson, Jiawen Su, Rachad Youssef, and Jim Zelenka. </author> <title> The Scotch parallel storage systems. </title> <booktitle> In Proceedings of 40th IEEE Computer Society International Conference (COMPCON 95), </booktitle> <pages> pages 403-410, </pages> <address> San Francisco, </address> <month> Spring </month> <year> 1995. </year>
Reference-contexts: In the Scotch Parallel File System, targeted for a network-of-workstations supercomputing environment, client write caches provide weakly consistent file-sharing with the help of propagate and expunge mechanisms and explicit barriers to avoid read/write data hazards <ref> [13] </ref>. The xFS [1] exploits high speeds offered by ATM networks by using a technique called cooperative-caching. This technique allows a client to access file data directly from another client's memory. Cache consistency in xFS is maintained by a token-based scheme.
Reference: [14] <author> John H. Hartman and John K. Ousterhout. </author> <title> The Zebra striped network file system. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 29-43, </pages> <year> 1993. </year>
Reference-contexts: For uniprocessors, however, there have been numerous studies on various kinds of write-caching. In the log-structured file system [32], for example, file system changes are buffered and periodically written to disk in a single, big disk-write operation. In Zebra <ref> [14] </ref>, the log-structured file system is combined with a distributed striped file system. In the Scotch Parallel File System, targeted for a network-of-workstations supercomputing environment, client write caches provide weakly consistent file-sharing with the help of propagate and expunge mechanisms and explicit barriers to avoid read/write data hazards [13]. <p> This technique of batching writes, although for somewhat different reasons, has been used successfully in the log-structured file system [32] and in the Zebra file system <ref> [14] </ref>. Note that ENWRICH only caches writes to files opened only for writing (writes to write-only files were dominant in CHARISMA studies; the number of files that were both read and written was tiny).
Reference: [15] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification, </title> <address> 1.0 edition, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Workload. Most of our experiments used synthetic access patterns that were derived from different modes of distributing two-dimensional matrices in CP memories, as suggested by those available in High Performance Fortran <ref> [15, 10] </ref> (Figure 3, adapted from [19]). Elements in each dimension of a multidimensional array could be mapped fully in one CP (denoted as NONE), distributed in contiguous segments among the CPs (BLOCK 5 ), or distributed 4 ENWRICH cache size was larger than that of TC.
Reference: [16] <author> Jay Huber, Christopher L. Elford, Daniel A. Reed, Andrew A. Chien, and David S. Blumenthal. </author> <title> PPFS: A high performance portable parallel file system. </title> <booktitle> In Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <pages> pages 385-394, </pages> <address> Barcelona, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: He concluded that a strategy in which a disk write was issued only after n bytes were written to an n-byte buffer (the WriteFull strategy) performed the best for his test workload [23]. Huber et al. propose centralized caching agents for write-shared files <ref> [16] </ref>, a technique which still suffers network latency on every write request. Compute-processor write caching in multiprocessors has been unpopular because of anticipated consistency overhead. Indeed, the amount of block sharing between compute nodes clearly predicts high consistency overhead for a block-level cache at the compute nodes [24, 31].
Reference: [17] <institution> Concurrent I/O application examples. Intel Corporation Background Information, </institution> <year> 1989. </year>
Reference-contexts: 1 Introduction Many parallel scientific applications need high-performance I/O <ref> [17, 30] </ref>. The computational performance of multiprocessors has leaped far ahead of their I/O performance, making I/O the fl This work was supported in part by the National Science Foundation under grant number CCR-9113170 and CCR-9404919. bottleneck for many of these applications.
Reference: [18] <author> David Kotz. </author> <title> Multiprocessor file system interfaces. </title> <booktitle> In Proceedings of the Second International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 194-201, </pages> <year> 1993. </year>
Reference-contexts: For example, many exhibit forward-jumping sequential patterns [24, 31], many of which are actually complex strided patterns [26, 27]. Clearly, parallel file systems must be redesigned to fit these common access patterns. Several recent works have proposed changes to the file-system interface <ref> [3, 7, 8, 10, 12, 18, 19, 26] </ref>. One such proposed interface is collective I/O.
Reference: [19] <author> David Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <booktitle> In Proceedings of the 1994 Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 61-74, </pages> <month> November </month> <year> 1994. </year> <note> Updated as Dartmouth TR PCS-TR94-226 on November 8, </note> <year> 1994. </year>
Reference-contexts: For example, many exhibit forward-jumping sequential patterns [24, 31], many of which are actually complex strided patterns [26, 27]. Clearly, parallel file systems must be redesigned to fit these common access patterns. Several recent works have proposed changes to the file-system interface <ref> [3, 7, 8, 10, 12, 18, 19, 26] </ref>. One such proposed interface is collective I/O. <p> A collective interface allows all processes to make a single large request, preserving semantic information and thereby making it possible for the I/O subsystem to better coordinate the actual data transfer. Disk-directed I/O (DDIO) is one efficient implementation technique for collective I/O <ref> [19] </ref>. In DDIO, the I/O processor (IOP) directs the order in which disk I/O operations should occur, after the job as a whole makes a collective I/O request. The IOP uses its intimate knowledge of the disk subsystem to optimize performance. <p> Each IOP manages a cache that has two buffers per CP per local disk, large enough to double-buffer an independent stream of requests from each CP to each disk. The IOP write cache uses a WriteFull policy (Section 2). More details about the traditional-caching implementation can be found in <ref> [19] </ref>. 4.1 Experimental Design Most of our experiments had 32 processors (16 CPs and 16 IOPs). Each CPU was a generic RISC with a 50 MHz clock. Each IOP had one SCSI bus with 10 MB/s peak bandwidth. <p> Workload. Most of our experiments used synthetic access patterns that were derived from different modes of distributing two-dimensional matrices in CP memories, as suggested by those available in High Performance Fortran [15, 10] (Figure 3, adapted from <ref> [19] </ref>). Elements in each dimension of a multidimensional array could be mapped fully in one CP (denoted as NONE), distributed in contiguous segments among the CPs (BLOCK 5 ), or distributed 4 ENWRICH cache size was larger than that of TC. <p> We experimented with two extreme disk layout policies: contiguous, where logically consecutive file blocks were actually mapped on the disk in a physically consecutive manner; and random, where files blocks were placed on random physical locations on the disk. Kotz in <ref> [19] </ref> asserts that a real system would have a layout intermediate between the two and would have intermediate performance. 5 Results In this section we first study ENWRICH and Traditional Caching (TC) performance on the base configuration described in section 4.1. <p> For patterns like wnc, wcc, and wpnncc, I/O took place in small 8-byte chunks. In such cases, while TC suffered badly due to intense buffer contention problems, ENWRICH inherited DDIO overhead for transferring 8-byte chunks from CPs <ref> [19] </ref> and also caused excessive directory flushes. A directory flush was an undesirable situation where a flush occurred because there was no space to write meta-data. With 8-byte I/O, the CP stored 24 bytes of directory metadata for each 8 bytes of real data. <p> Hence, each flush actually transferred little real data to disk, requiring many directory flushes. Even with such disadvantages, ENWRICH almost always performed at par or better than TC, although not quite as well as pure DDIO <ref> [19] </ref> wherever DDIO could be applied. sizes. The 8-byte patterns starting with an underscore used a file size of 8 MB instead of 64 MB. Each bar represents the average of five independent trials (maximum coefficient of variation was 0.16). <p> For reasons discussed in the random-blocks layout case, for some patterns such as wcc, wpbccb, with 8-byte record sizes, ENWRICH lost performance significantly. It still performed at par with TC but worse than DDIO <ref> [19] </ref>. Except for the wnn pattern, in the few other cases where ENWRICH performed worse than TC, it was never worse by more than 5%. TC lost performance for a few important reasons. For patterns like wcn, there was little interprocess spatial locality.
Reference: [20] <author> David Kotz. </author> <title> Disk-directed I/O for an out-of-core computation. </title> <booktitle> In Proceedings of the Fourth IEEE International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 159-166, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: Disk-directed I/O (discussed in Section 1), proposed by Kotz, efficiently implements collective I/O, out-of-core computations, data-dependent distributions, and both regular and irregular requests <ref> [20, 21] </ref>. 3 Design and Operation of ENWRICH Write caches are normally used to delay writes, avoiding extraneous disk I/O for blocks that are overwritten in pieces. Although ENWRICH achieves that, its primary motivation is different.
Reference: [21] <author> David Kotz. </author> <title> Expanding the potential for disk-directed I/O. </title> <booktitle> In Proceedings of the 1995 IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 490-495, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: Disk-directed I/O (discussed in Section 1), proposed by Kotz, efficiently implements collective I/O, out-of-core computations, data-dependent distributions, and both regular and irregular requests <ref> [20, 21] </ref>. 3 Design and Operation of ENWRICH Write caches are normally used to delay writes, avoiding extraneous disk I/O for blocks that are overwritten in pieces. Although ENWRICH achieves that, its primary motivation is different.
Reference: [22] <author> David Kotz. </author> <title> Interfaces for disk-directed I/O. </title> <type> Technical Report PCS-TR95-270, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: In this paper we introduce ENWRICH (Efficient compute-Node WRIte caCHes) a system that combines compute-node write caches for write-only files with disk-directed I/O for high-performance writes. Kotz explores ways to adapt many of the existing interfaces to DDIO <ref> [22] </ref>, but, as proposed so far, DDIO enhances performance over one collective request from one job on one file. Moreover, to accommodate a set of generic patterns, the IOP has to maintain a sizable library of those pattern mappings.
Reference: [23] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Caching and writeback policies in parallel file systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):140-145, January and February 1993. </note>
Reference-contexts: Kotz compared several IOP write-caching policies on a shared-memory multiprocessor. He concluded that a strategy in which a disk write was issued only after n bytes were written to an n-byte buffer (the WriteFull strategy) performed the best for his test workload <ref> [23] </ref>. Huber et al. propose centralized caching agents for write-shared files [16], a technique which still suffers network latency on every write request. Compute-processor write caching in multiprocessors has been unpopular because of anticipated consistency overhead.
Reference: [24] <author> David Kotz and Nils Nieuwejaar. </author> <title> Dynamic file-access characteristics of a production parallel scientific workload. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 640-649, </pages> <month> Nov </month> <year> 1994. </year>
Reference-contexts: This shortfall is largely due to poor performance of the file-system software. In this paper we propose one technique that can dramatically improve file-system performance. Most current multiprocessor file systems are derivatives of Unix file systems. Typical Unix workloads [29, 11], however, differ significantly from scientific multiprocessor workloads <ref> [24, 31] </ref>. Scientific programs use files for checkpointing, application-controlled virtual memory [9, 12], and visualization output, which are not common in Unix workloads. Furthermore, parallel scientific programs exhibit patterns that are more complicated than simple sequential patterns observed in vector scientific or Unix work-loads. <p> Scientific programs use files for checkpointing, application-controlled virtual memory [9, 12], and visualization output, which are not common in Unix workloads. Furthermore, parallel scientific programs exhibit patterns that are more complicated than simple sequential patterns observed in vector scientific or Unix work-loads. For example, many exhibit forward-jumping sequential patterns <ref> [24, 31] </ref>, many of which are actually complex strided patterns [26, 27]. Clearly, parallel file systems must be redesigned to fit these common access patterns. Several recent works have proposed changes to the file-system interface [3, 7, 8, 10, 12, 18, 19, 26]. One such proposed interface is collective I/O. <p> These patterns and the difficulty of rewriting code to fit a collective interface can dilute the benefits of DDIO. The CHARISMA workload studies which spanned two systems, two sites, and two programming models <ref> [24, 31, 27] </ref> exposed certain trends in multiprocessor file access. First, write traffic was high; the number of bytes written was almost double the number read, and the number of files written was almost double the number read. <p> Compute-processor write caching in multiprocessors has been unpopular because of anticipated consistency overhead. Indeed, the amount of block sharing between compute nodes clearly predicts high consistency overhead for a block-level cache at the compute nodes <ref> [24, 31] </ref>. For uniprocessors, however, there have been numerous studies on various kinds of write-caching. In the log-structured file system [32], for example, file system changes are buffered and periodically written to disk in a single, big disk-write operation.
Reference: [25] <author> David Kotz, Song Bac Toh, and Sriram Radhakrishnan. </author> <title> A detailed simulation model of the HP 97560 disk drive. </title> <type> Technical Report PCS-TR94-220, </type> <institution> Dept. of Computer Science, Dart-mouth College, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: Pro-teus runs as a single multithreaded process on a uniprocessor. The STARFISH simulator is essentially a Proteus application, but it provides various higher level abstractions to facilitate parallel-I/O simulation. It includes a validated disk model <ref> [25] </ref> based on Ruemmler and Wilkes' HP97560 model [33]. It provides the framework of a multiprocessor system with CPs and IOPs, and IOPs having one or more disks and one I/O bus attached to each IOP.
Reference: [26] <author> Nils Nieuwejaar and David Kotz. </author> <title> Low-level interfaces for high-level parallel I/O. </title> <booktitle> In IPPS '95 Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 47-62, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Furthermore, parallel scientific programs exhibit patterns that are more complicated than simple sequential patterns observed in vector scientific or Unix work-loads. For example, many exhibit forward-jumping sequential patterns [24, 31], many of which are actually complex strided patterns <ref> [26, 27] </ref>. Clearly, parallel file systems must be redesigned to fit these common access patterns. Several recent works have proposed changes to the file-system interface [3, 7, 8, 10, 12, 18, 19, 26]. One such proposed interface is collective I/O. <p> For example, many exhibit forward-jumping sequential patterns [24, 31], many of which are actually complex strided patterns [26, 27]. Clearly, parallel file systems must be redesigned to fit these common access patterns. Several recent works have proposed changes to the file-system interface <ref> [3, 7, 8, 10, 12, 18, 19, 26] </ref>. One such proposed interface is collective I/O. <p> Corbett et al. propose MPI-IO, which models I/O as message passing and allows programmers to express I/O with program datatypes rather than byte offsets within a file [7]. Nieuwejaar and Kotz propose a nested-batched interface for complex access patterns <ref> [26] </ref>. Two-phase I/O, proposed by Del Rosario et al., is an efficient implementation of a large transfer operation where data is permuted in CP memory before a collective I/O operation so that the I/O operation conforms to the actual file layout for better I/O performance [10].
Reference: [27] <author> Nils Nieuwejaar, David Kotz, Apratim Purakayastha, Carla Schlatter Ellis, and Michael Best. </author> <title> File-access characteristics of parallel scientific workloads. </title> <type> Technical Report PCS-TR95-263, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> August </month> <year> 1995. </year> <note> Submitted to IEEE TPDS. </note> <editor> a random-blocks layout. </editor> <title> There were 16 CPs and 8 IOPs in all the experiments. The ENWRICH cache size was fixed at 1 MB per CP. </title>
Reference-contexts: Furthermore, parallel scientific programs exhibit patterns that are more complicated than simple sequential patterns observed in vector scientific or Unix work-loads. For example, many exhibit forward-jumping sequential patterns [24, 31], many of which are actually complex strided patterns <ref> [26, 27] </ref>. Clearly, parallel file systems must be redesigned to fit these common access patterns. Several recent works have proposed changes to the file-system interface [3, 7, 8, 10, 12, 18, 19, 26]. One such proposed interface is collective I/O. <p> These patterns and the difficulty of rewriting code to fit a collective interface can dilute the benefits of DDIO. The CHARISMA workload studies which spanned two systems, two sites, and two programming models <ref> [24, 31, 27] </ref> exposed certain trends in multiprocessor file access. First, write traffic was high; the number of bytes written was almost double the number read, and the number of files written was almost double the number read.
Reference: [28] <author> Bill Nitzberg. </author> <title> Performance of the iPSC/860 Concurrent File System. </title> <type> Technical Report RND-92-020, </type> <institution> NAS Systems Division, NASA Ames, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: In ENWRICH, we draw on some of these results in the uniprocessor domain and apply them to multiprocessors. 2.3 Recent Advances in Parallel I/O Earlier generations of parallel file systems like Intel CFS extended the Unix interface with file-pointer modes for parallel access <ref> [5, 28] </ref>. The Scalable File System (SFS) on the CM-5 also provides a Unix-like interface, but has an additional collective-I/O interface [3].
Reference: [29] <author> J. Ousterhout, H. DaCosta, D. Harrison, J. Kunze, M. Kupfer, and J. Thompson. </author> <title> A trace driven analysis of the UNIX 4.2 BSD file system. </title> <booktitle> In Proceedings of 10th Symposium on Operating System Principles, </booktitle> <pages> pages 15-24, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: This shortfall is largely due to poor performance of the file-system software. In this paper we propose one technique that can dramatically improve file-system performance. Most current multiprocessor file systems are derivatives of Unix file systems. Typical Unix workloads <ref> [29, 11] </ref>, however, differ significantly from scientific multiprocessor workloads [24, 31]. Scientific programs use files for checkpointing, application-controlled virtual memory [9, 12], and visualization output, which are not common in Unix workloads.
Reference: [30] <author> James Pool. </author> <title> Preliminary survey of I/O intensive applications. </title> <type> Technical Report CCSF-38, </type> <institution> Caltech Concurrent Supercomputing Facilities, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Many parallel scientific applications need high-performance I/O <ref> [17, 30] </ref>. The computational performance of multiprocessors has leaped far ahead of their I/O performance, making I/O the fl This work was supported in part by the National Science Foundation under grant number CCR-9113170 and CCR-9404919. bottleneck for many of these applications.
Reference: [31] <author> A. Purakayastha, Carla S. Ellis, David Kotz, Nils Nieuwejaar, and Michael Best. </author> <title> Characterizing parallel file-access patterns on a large-scale multiprocessor. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium, </booktitle> <pages> pages 165-172, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: This shortfall is largely due to poor performance of the file-system software. In this paper we propose one technique that can dramatically improve file-system performance. Most current multiprocessor file systems are derivatives of Unix file systems. Typical Unix workloads [29, 11], however, differ significantly from scientific multiprocessor workloads <ref> [24, 31] </ref>. Scientific programs use files for checkpointing, application-controlled virtual memory [9, 12], and visualization output, which are not common in Unix workloads. Furthermore, parallel scientific programs exhibit patterns that are more complicated than simple sequential patterns observed in vector scientific or Unix work-loads. <p> Scientific programs use files for checkpointing, application-controlled virtual memory [9, 12], and visualization output, which are not common in Unix workloads. Furthermore, parallel scientific programs exhibit patterns that are more complicated than simple sequential patterns observed in vector scientific or Unix work-loads. For example, many exhibit forward-jumping sequential patterns <ref> [24, 31] </ref>, many of which are actually complex strided patterns [26, 27]. Clearly, parallel file systems must be redesigned to fit these common access patterns. Several recent works have proposed changes to the file-system interface [3, 7, 8, 10, 12, 18, 19, 26]. One such proposed interface is collective I/O. <p> It is not often easy, however, for a program to form its I/O requests as large, collective requests. Even in data-parallel programs, where all I/O was collective, we saw a surprising abundance of small requests <ref> [31] </ref>. These patterns and the difficulty of rewriting code to fit a collective interface can dilute the benefits of DDIO. The CHARISMA workload studies which spanned two systems, two sites, and two programming models [24, 31, 27] exposed certain trends in multiprocessor file access. <p> These patterns and the difficulty of rewriting code to fit a collective interface can dilute the benefits of DDIO. The CHARISMA workload studies which spanned two systems, two sites, and two programming models <ref> [24, 31, 27] </ref> exposed certain trends in multiprocessor file access. First, write traffic was high; the number of bytes written was almost double the number read, and the number of files written was almost double the number read. <p> Compute-processor write caching in multiprocessors has been unpopular because of anticipated consistency overhead. Indeed, the amount of block sharing between compute nodes clearly predicts high consistency overhead for a block-level cache at the compute nodes <ref> [24, 31] </ref>. For uniprocessors, however, there have been numerous studies on various kinds of write-caching. In the log-structured file system [32], for example, file system changes are buffered and periodically written to disk in a single, big disk-write operation. <p> For some experiments we used another pattern named wss (for Write Synchronous Sequential). For this pattern, the array layout was NONE-CYCLIC, but the CPs requested corresponding chunks synchronously. This pattern was abundant in CMMD programs <ref> [31] </ref>. Our studies show that some jobs write to two files at the same time [31]. To capture the flavor of such a scenario, we devised patterns for experiments where CPs simultaneously wrote two different arrays with different mappings that were output to two different files on disk. <p> For this pattern, the array layout was NONE-CYCLIC, but the CPs requested corresponding chunks synchronously. This pattern was abundant in CMMD programs <ref> [31] </ref>. Our studies show that some jobs write to two files at the same time [31]. To capture the flavor of such a scenario, we devised patterns for experiments where CPs simultaneously wrote two different arrays with different mappings that were output to two different files on disk. <p> Experiments with an 8192-byte record size (size of a file-system block) exercised the system at another extreme where there was minimal contention and inter-process locality. Experiments with a 512-byte record size reflect a popular range of request sizes <ref> [31] </ref>. The synthetic patterns described above helped us explicitly control the experiments and answer questions such as why performance of both TC and ENWRICH varied from pattern to pattern in different configurations. <p> For a more realistic comparison we also experimented with a workload derived from some of the traces we collected in <ref> [31] </ref>. Section 5.3 describes the trace-driven experiments in more detail. File and Disk Layout. We used an 8-KB file-system block size in all the experiments. Each file was striped, at block granu Patterns are named by the distribution method (NONE, BLOCK, or CYCLIC) in each dimension. <p> All experiments used contiguous disk layout, 8192-byte records, 64 MB files, and 16 each of CP, IOP, and disks. 5.3 Trace-driven Experiments In this section we describe results obtained by driving both EN-WRICH and TC with traces obtained from a workload study on the CM-5 at the NCSA <ref> [31] </ref>. We had to make some significant compromises in using the traces. <p> The traces included several self-selecting CMMD jobs, and most CMF jobs. For logistical convenience, traces were pre processed and coalesced from selected CMMD jobs into one trace called cmmd, and from selected CMF jobs into three traces called cmf1, cmf2, and cmf3. These jobs mirrored certain observations made in <ref> [31] </ref>: the average write-request size was small (they varied between 500 and 1200 bytes for the four files), and CMMD files were, on average, an order of magnitude larger than CMF files. We used 32 CPs, 16 IOPs, 16 disks, and a 7-by-7 bidirectional torus interconnect in all the experiments.
Reference: [32] <author> Mendel Rosenblum and John K. Ousterhout. </author> <title> The design and implementation of a log-structured file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1) </volume> <pages> 26-52, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Indeed, the amount of block sharing between compute nodes clearly predicts high consistency overhead for a block-level cache at the compute nodes [24, 31]. For uniprocessors, however, there have been numerous studies on various kinds of write-caching. In the log-structured file system <ref> [32] </ref>, for example, file system changes are buffered and periodically written to disk in a single, big disk-write operation. In Zebra [14], the log-structured file system is combined with a distributed striped file system. <p> This technique of batching writes, although for somewhat different reasons, has been used successfully in the log-structured file system <ref> [32] </ref> and in the Zebra file system [14]. Note that ENWRICH only caches writes to files opened only for writing (writes to write-only files were dominant in CHARISMA studies; the number of files that were both read and written was tiny).
Reference: [33] <author> Chris Ruemmler and John Wilkes. </author> <title> An introduction to disk drive modeling. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 17-28, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Pro-teus runs as a single multithreaded process on a uniprocessor. The STARFISH simulator is essentially a Proteus application, but it provides various higher level abstractions to facilitate parallel-I/O simulation. It includes a validated disk model [25] based on Ruemmler and Wilkes' HP97560 model <ref> [33] </ref>. It provides the framework of a multiprocessor system with CPs and IOPs, and IOPs having one or more disks and one I/O bus attached to each IOP. Some file systems, like the traditional-caching system and the disk-directed system, are also implemented as part of STARFISH. ENWRICH Implementation.
References-found: 33

