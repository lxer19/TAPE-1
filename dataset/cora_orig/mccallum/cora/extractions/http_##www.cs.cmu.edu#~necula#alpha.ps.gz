URL: http://www.cs.cmu.edu/~necula/alpha.ps.gz
Refering-URL: http://www.cs.cmu.edu/~necula/cv.html
Root-URL: 
Title: Accounting for the performance of Standard ML on the DEC Alpha  
Author: George C. Necula Lal George 
Date: September 20, 1994  
Address: 600 Mountain Ave. Pittsburgh, PA 15213 Murray Hill, NJ 07974  
Affiliation: School of Computer Science Rm.2A426 AT&T Bell Labs Carnegie Mellon University  
Abstract: The performance of several Standard ML (SML) programs on the DEC Alpha 3000/600 is accounted for using the built-in hardware performance counters. The counters provide detailed information of the processor state during execution such as: total instructions, multiple-issue, stalls, cache behavior, and classification of instructions executed. The purpose of this paper is to determine how well the current Standard ML of New Jersey (SML/NJ) compiler utlilizes the DEC Alpha processor. Surprisingly, the processor is stalled for 60-70% of the total cycles executed for most benchmarks | resource conflicts and data cache misses accounting for a large fraction of these. The paper also provides the experimental data needed for an informed choice of which future optimizations to implement. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Appel, A. W. </author> <title> Emulating write-allocate on a no-write-allocate cache. </title> <type> Tech. Rep. </type> <institution> CS-TR-459-94, Princeton University, </institution> <month> June 15 </month> <year> 1994. </year>
Reference-contexts: Back end optimizations emphasize better usage of processor pipelines and superscalar capabilities over generating the fewest number of instructions possible. Also the usage patterns of the memory subsystem have a significant impact on performance, and optimizations like prefetching, <ref> [1, 6] </ref> speculative execution,[2] or maintaining data alignment are crucial. The motivation for this work was to give compiler writers an insight into the processor level behavior of Standard ML (SML) programs on Alpha machines. We provide detailed experimental information about pipeline and cache events during execution.
Reference: [2] <author> Bernstein, D., and Rodeh, M. </author> <title> Global instruction scheduling for superscalar machines. </title> <booktitle> In Proc. of the ACM SIGPLAN`91 conf. on programming language design and implementation (June 1991), ACM, </booktitle> <pages> pp. 241-255. </pages>
Reference: [3] <author> Cvetanovic, Z., and Bhandarkar, D. </author> <title> Characterization of Alpha AXP performance using TP and SPEC workloads. </title> <booktitle> In Proc. of the 21st annual Int. Symp. on Computer Architecture (April 18 1994), ACM, </booktitle> <pages> pp. 60-70. </pages> <note> Also Computer Arch. </note> <editor> News, </editor> <volume> Vol 22, No. 2, </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: Stores account for 15% of all instructions executed and branches are about 12%. The average of just loads, stores and branches across all benchmarks is 52%. The sum of loads and stores is at least 15% higher than that reported for C and Fortran benchmarks <ref> [3] </ref>. This was to be expected because SML/NJ uses boxed representations to compile polymorphic functions. What did come as a surprise were the floating point benchmarks. Simple that was regarded as a floating point intensive program had less than 1% of all instructions executed being floating point. <p> Simple that was regarded as a floating point intensive program had less than 1% of all instructions executed being floating point. The SML/NJ floating point benchmarks, have a much smaller ratio of floating point instructions than their C counterparts that may have anywhere between 40-50% <ref> [3] </ref>. This is mainly attributed to the current compiler that does a lot of unnecessary boxing/unboxing of floating point values, and subscript bounds checks associated with arrays. 5 6 Cache Misses significantly higher than the Icache misses. <p> The CPI ranges from 1.8 to 3.4 with an average of 2.5. This is only about 25% worse than C benchmarks that have an average between 1.5-2.5 for both integer and floating point benchmarks <ref> [3] </ref>. The SML/NJ floating point benchmarks have a higher average CPI of 2.9 compared to 2.0 for integer benchmarks. 11 Conclusions Using the performance counters we have accounted for the performance of SML/NJ programs on the DEC Alpha.
Reference: [4] <institution> DEC. DECchip T M 21064-AA Microprocessor. Digital Electronics Corporation, </institution> <year> 1992. </year> <title> Order No: </title> <publisher> EC-N0079-72. </publisher>
Reference-contexts: Without this kind of information the process of fine tuning the compiler by implementing new optimizations is merely based on guesswork. For complex architectures like the DEC Alpha, optimizations chosen without precise experimental data are unlikely to perform as anticipated. The DEC 21064-AA <ref> [4] </ref> microprocessor provides a set of non-intrusive hardware counters that can be used to measure a variety of execution characteristics. In the past, similar execution profiles were obtained from detailed pipeline simulations | a process that was both tedious and time consuming. <p> The read latency of the 64 Mbyte main memory, determined experimentally, is 48 cycles. Table 1 summarizes these numbers. Clock Frequency 150 MHz On-chip Cache direct mapped size 8 Kbyte data 8 Kbyte instruction 32 byte lines latency 3 cycle (Figure 2.5, page 2-21 <ref> [4] </ref>) write policy write-through, write-around Secondary Cache direct mapped size 2 Mbyte latency 10 cycles write policy write-back, write-around Memory 64 Mbyte latency 48 cycles Table 1: DEC 3000/600 AXP Diwan, et.al.,[5] show that the write-no-allocate miss policy is not well suited for the SML/NJ programs, and (correctly) predict poor memory
Reference: [5] <author> Diwan, A., Tarditi, D., and Moss, E. </author> <title> Memory subsystem performance of programs using copying garbage collection. </title> <booktitle> In 21st ACM SIGPLAN-SIGACT Symp. on Principles of Programming Languages (January 1994), ACM, </booktitle> <pages> pp. 1-14. </pages>
Reference: [6] <author> Mowry, T. C., Lam, M. S., and Gupta, A. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Fifth intn. conf. on architectural support for programming languages and operating systems (Sept. </booktitle> <year> 1992), </year> <pages> pp. 62-75. </pages> <note> Also SIGPLAN Notices vol.27 num.9. 10 </note>
Reference-contexts: Back end optimizations emphasize better usage of processor pipelines and superscalar capabilities over generating the fewest number of instructions possible. Also the usage patterns of the memory subsystem have a significant impact on performance, and optimizations like prefetching, <ref> [1, 6] </ref> speculative execution,[2] or maintaining data alignment are crucial. The motivation for this work was to give compiler writers an insight into the processor level behavior of Standard ML (SML) programs on Alpha machines. We provide detailed experimental information about pipeline and cache events during execution.
References-found: 6

