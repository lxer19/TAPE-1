URL: http://www.cs.jhu.edu/~sheppard/cs.605.755/papers/paper7-1.ps
Refering-URL: http://www.cs.jhu.edu/~sheppard/cs.605.755/sched.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: 11 Genetic Algorithms and Neural Networks major application is the use of genetic algorithms to
Author: D. WHITLEY 
Note: A third  Genetic Algorithms in Engineering and Computer Science Editor J. Periaux and G. Winter c fl1995 John Wiley Sons Ltd. cbook 16/8/1995 13:52|PAGE PROOFS for John Wiley Sons Ltd (jwbook.sty v3.0, 12-1-1995)  
Date: 11.1 INTRODUCTION  
Abstract: Genetic algorithms and neural networks are both inspired by computation in biological systems. A good deal of biological neural architecture is determined genetically. It is therefore not surprising that as some neural network researchers explored how neural systems are organized that the idea of evolving neural architectures should arise. Genetic algorithms have been used in conjunction with neural networks in three major ways. First, they have been used to set the weights in fixed architectures. This includes both supervised learning applications and reinforcement learning applications. In related work, a genetic algorithm has been used to set the learning rates which in turn are used by other types of learning algorithms. Genetic algorithms have also been combined with more traditional forms of gradient based search. Second, genetic algorithms have been used to learn neural network topologies. When evolving neural networks topologies for function approximation, this includes the problem of specifying how many hidden units a neural network should have and how the nodes are connected. Schaffer, Whitley and Eshelman (1992) survey these various areas in an introduction to the proceeding of a 1992 workshop on Combinations of Genetic algorithms and Neural Networks. The current paper is tutorial in nature and highlights select cases and briefly references some of the work that has been introduced in the last 3 years. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ackley D.H. and Littman M. </author> <title> (1991) Interactions between learning and evolution. </title> <booktitle> In Proc. of the 2nd Conf. on Artificial Life, </booktitle> <editor> C.G. Langton, ed., </editor> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference: <author> Anderson C. W. </author> <title> (1989) Learning to Control an Inverted Pendulum Using Neural Networks. </title> <journal> IEEE Control Systems Magazine, </journal> <volume> 9, </volume> <pages> 31-37. </pages>
Reference: <author> Angeline P.J., Saunders G. M. and Pollack J.B. </author> <title> (1994) An evolutionary algorithm that constructs recurrent neural networks. </title> <journal> IEEE Transactions on Neural Networks 5(1) </journal> <pages> 54-64. </pages>
Reference: <author> Baldwin J.M. </author> <title> (1896) A new factor in evolution. </title> <journal> American Naturalist, </journal> <volume> 30 </volume> <pages> 441-451, 1896. </pages>
Reference: <author> Belew R. </author> <title> (1989) When both individuals and populations search: Adding simple learning to the genetic algorithm. </title> <editor> In J.D. Schaffer (Ed.), </editor> <booktitle> Third international conference on genetic algorithms (pp. </booktitle> <pages> 34-41). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Belew R., McInerney J. and Schraudolph N. </author> <title> (1990) Evolving Networks: Using the Genetic Algorithms with Connectionist Learning. </title> <type> CSE Technical Report CS90-174, </type> <institution> Computer Science, UCSD. </institution>
Reference: <author> Brill F.Z., Brown D.E. and Martin W.N. </author> <title> (1992) Fast genetic selection of features for neural network classifiers. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3 (2), </volume> <pages> 324-328. </pages>
Reference: <author> Chang E.J. and Lippmann R.P. </author> <title> (1991) Using genetic algorithms to improve pattern classification performance. </title> <editor> In R.P. Lippmann, J.E. Moody and D.S. Touretsky (Eds.), </editor> <booktitle> Advances in neural information processing 3 (pp. </booktitle> <pages> 797-803). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Das R. and Whitley D. </author> <title> (1992) Genetic Sparse Distributed Memories. Combinations of Genetic Algorithms and Neural Networks. </title> <editor> D. Whitley and J.D. Schaffer (eds.) </editor> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Eberhart R.C. and Dobbins R.W. </author> <title> (1991) Designing neural network explanation facilities using genetic algorithms. </title> <booktitle> IEEE international joint conference on neural networks (pp. </booktitle> <pages> 1758-1763). </pages> <address> Singapore: </address> <publisher> IEEE. </publisher>
Reference: <author> Eberhart R.C. </author> <title> (1992) The role of genetic algorithms in neural network query-based learning and explanation facilities. In Combinations of Genetic Algorithms and Neural Networks. </title> <address> D. </address>
Reference: <editor> Whitley and J.D. Schaffer (eds.) </editor> <publisher> IEEE Computer Society Press. </publisher> <editor> Fahlman S. and Lebiere C. </editor> <year> (1990). </year> <title> The Cascade Correlation Learning Architecture. </title> <editor> In D. Touretzky (Ed), </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Gruau F. </author> <title> (1992) Genetic synthesis of Boolean neural networks with a cell rewriting developmental process. In, Combination of Genetic Algorithms and Neural Networks, </title> <address> D. </address>
Reference: <editor> Whitley and J.D. Schaffer, eds, </editor> <publisher> IEEE Computer Society Press, </publisher> <year> 1992. </year>
Reference: <author> Gruau F. and Whitley D. </author> <title> (1993) Adding Learning to the Cellular Development of Neural Networks: Evolution and the Baldwin Effect. </title> <booktitle> Evolutionary Computation 1(3): </booktitle> <pages> 213-233. </pages>
Reference-contexts: In step 6 a parallel divide is executed. In step 7 the "-" symbol has been executed and a negative weight is introduced feeding into the output node. In step 8 the black cell has changed its threshold. In the final steps, the remaining cells just read termination symbols. <ref> (This figure is taken from Gruau and Whitley, 1993) </ref>. cbook 16/8/1995 13:52|PAGE PROOFS for John Wiley & Sons Ltd (jwbook.sty v3.0, 12-1-1995) GENETIC ALGORITHMS AND NEURAL NETS 11 Another way to reuse development code is to use a form of Automatic Function Definition like that used in Genetic Programming.
Reference: <author> Gruau F. </author> <year> (1995). </year> <title> Automatic Definition of Modular Neural Networks, Adaptive Behavior,3(2):151-183. </title>
Reference-contexts: Subtrees thus function like program subroutines. Gruau has used Automatic Function Definition to evolve a mechanism to control the gait of a 6-legged robot. The use of Automatic Function Definition results in simpler, more modular and well structured neural network <ref> (Gruau 1995) </ref>. 11.5 Evolution, Learning and the Baldwin Effect There has been considerable interest recently in the idea that learning can impact evolution even if learned behaviors are not coded back on the chromosome, as in Lamarckian evolution.
Reference: <author> Hancock P.J.B. </author> <title> (1992) Genetic algorithms and permutation problems: a comparison of recombination operators for neural structure specification. In Combinations of Genetic Algorithms and Neural Networks. </title> <editor> D. Whitley and J.D. Schaffer (eds.) </editor> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Harp S.A., Samad T. and Guha A. </author> <title> (1989) Towards the genetic synthesis of neural networks. </title>
Reference: <editor> In J.D. Schaffer (Ed.), </editor> <booktitle> Third international conference on genetic algorithms (pp. </booktitle> <pages> 360-369). </pages> <booktitle> Genetic Algorithms in Engineering and Computer Science Editor J. </booktitle> <editor> Periaux and G. </editor> <publisher> Winter c fl1995 John Wiley & Sons Ltd. cbook 16/8/1995 13:52|PAGE PROOFS for John Wiley & Sons Ltd (jwbook.sty v3.0, </publisher> <address> 12-1-1995) 14 D. WHITLEY San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Harp S.A., Samad T. and Guha A. </author> <title> (1990) Designing application-specific neural networks using the genetic algorithm. </title> <editor> In D.S. Touretsky (Ed.), </editor> <booktitle> Advances in neural information processing 2 (pp. </booktitle> <pages> 447-454). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Hinton G.E. </author> <title> and Nowlan S.J.(1987) How learning can guide evolution. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 495-502. </pages>
Reference: <author> Holland J. </author> <booktitle> (1975) Adaptation in Natural and Artificial Systems. </booktitle> <address> Ann Arbor, </address> <publisher> Univ. of Michigan Press. </publisher> <editor> Kanerva Pentti (1988). </editor> <title> Sparse Distributed Memory. </title> <address> Cambridge, Mass: </address> <publisher> MIT Press. </publisher>
Reference: <author> Kitano H. </author> <title> (1990) Designing neural network using genetic algorithm with graph generation system. </title> <journal> Complex Systems, </journal> <volume> 4 </volume> <pages> 461-476. </pages>
Reference: <author> Kitano H. </author> <title> (1995) A simple model of neurogenesis and cell differentiation based on evolutionary large-scale chaos. </title> <journal> Artificial Life, </journal> <volume> 2 </volume> <pages> 79-99. </pages>
Reference: <author> Korning P.G. </author> <title> (1994) Training of neural networks by means of genetic algorithm working on very long chromosomes. </title> <type> Technical Report, </type> <institution> Computer Science Department, Aarhus C, Denmark. </institution>
Reference: <author> Koza J.R. and Rice J.P. </author> <title> (1991) Genetic generation of both the weights and architecture for a neural network. </title> <booktitle> In, Intern. Joint Conf. on Neural Networks, </booktitle> <address> Seattle 92. </address>
Reference-contexts: Weights and architectures are often developed together. This can include systems such as GNARL. Other researchers have also looked at genetic programming as a way of developing architectures and weights together <ref> (Koza and Rice 1991) </ref>. Grammar based architecture descriptions have been explored by Kitano (1990), Mjolsness et al. (1988) and by Gruau (1992). Nolfi et al. (1990) have also looked at grammar based systems that retain may of the characteristics of L-systems.
Reference: <author> Miller G., Todd P. and Hedge S. </author> <title> (1989) Designing Neural Networks using Genetic Algorithm, </title> <booktitle> In, 3rd Intern. Conf. on Genetic Algorithms, </booktitle> <editor> D.J. Schaffer, ed., </editor> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Mjolsness E., Sharp D.H. and Alpert B.K. </author> <title> (1989) Scaling, </title> <booktitle> machine learning, and genetic neural nets. Advances in Applied Mathematics, </booktitle> <volume> 10, </volume> <pages> 137-163. </pages>
Reference: <author> Montana D.J. and Davis L. </author> <title> (1989) Training feedforward neural networks using genetic algorithms. </title> <booktitle> In Proceedings of eleventh international joint conference on artificial intelligence (pp. </booktitle> <pages> 762-767). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Muhlenbein H. </author> <title> (1990) Limitations of multi-layer perceptrons networks steps towards genetic neural networks. </title> <journal> Parallel Computing, </journal> <volume> 14 </volume> <pages> 249-260. </pages>
Reference: <author> Muhlenbein H. & Kindermann J. </author> <year> (1989). </year> <title> The dynamics of evolution and learning Towards genetic neural networks. </title> <editor> In R. Pfeifer, Z. Schreter, F. Fogelman-Soulie & L. Steels (Eds.), </editor> <booktitle> Connectionism in perspective (pp. </booktitle> <pages> 173-197). </pages> <address> Amsterdam: </address> <publisher> Elsevier Science Publishers B.V. (North-Holland). </publisher>
Reference: <author> Nolfi S., Elman J.L. and Parisi D. </author> <title> (1990) Learning and evolution in neural networks. </title> <type> CRL Technical Report 9019, </type> <institution> La Jolla, CA: University of California at San Diego. </institution>
Reference: <editor> Porto V.W. and Fogel D.B. </editor> <booktitle> (1990) Neural network techniques for navigation of AUVs. Proceedings of the IEEE Symposium on Autonomous Underwater Vehicle Technology (pp. </booktitle> <pages> 137-141). </pages> <address> Washington, DC: </address> <publisher> IEEE. </publisher>
Reference-contexts: Second, the problem of training a feed forward Artificial Neural Network (ANN) may represent an application that is inherently not a good match for genetic algorithms that rely heavily on recombination. Some researchers do not use recombination <ref> (e.g. Porto and Fogel, 1990) </ref> while other have used small populations and high mutation rates in conjunction with recombination.
Reference: <author> Radcliffe N.J. </author> <title> (1990) Genetic neural networks on MIMD computers. </title> <type> Doctoral dissertation, </type> <institution> University of Edinburgh, Edinburgh, </institution> <address> Scotland. </address>
Reference: <author> Radcliffe N.J. </author> <title> (1991) Genetic set recombination and its application to neural network topology optimization. </title> <type> Technical report EPCC-TR-91-21, </type> <institution> University of Edinburgh, Edinburgh, </institution> <address> Scotland. </address>
Reference: <author> Rogers D. </author> <title> (1990) Predicting Weather Using a Genetic Memory: a Combination of Kanerva's Sparse Distributed Memory with Holland's Genetic Algorithm; Advances in Neural Information Processing 2. </title>
Reference: <author> Sutton R. </author> <title> (1988) Learning to Predict by the Methods of Temporal Differences, </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44. </pages>
Reference-contexts: These algorithms often use smaller population sizes and higher mutation rates to cope with the "Competing Conventions Problems." Whitley et al. (1991; 1993) compared a genetic hill-climber to the well known work of Anderson (1989) which uses the "temporal difference method" <ref> (Sutton 1988) </ref> to train an "Adaptive Heuristic Critic" (AHC) which in turn is used to generate target outputs for doing reinforcement backpropagation. The results suggests that that the genetic algorithms produced training times comparable to the AHC with reinforcement backpropagation, while generalization was better for the genetic algorithm. .
Reference: <author> Skinner A. and Broughton J.Q. </author> <booktitle> (1995) Neural Networks in Computational Materials Science: Training Algorithms Modelling and Simulation in Materials Science and Engineering, </booktitle> <address> 3:371|390. </address>
Reference: <editor> Schaffer J.D., Whitley D. and Eshelman L. </editor> <title> (1992) Combination of Genetic Algorithms and Neural Networks: The state of the art. Combination of Genetic Algorithms and Neural Networks, </title> <publisher> IEEE Computer Society, </publisher> <year> 1992. </year>
Reference: <editor> Schaffer J.D., Caruana R.A. and Eshelman L.J. </editor> <title> (1990) Using genetic search to exploit the emergent behavior of neural networks. </title> <editor> In S. Forrest (Ed.), </editor> <booktitle> Emergent computation (pp. </booktitle> <pages> 244-248). </pages> <address> Amsterdam: </address> <publisher> North Holland. cbook 16/8/1995 13:52|PAGE PROOFS for John Wiley & Sons Ltd (jwbook.sty v3.0, </publisher> <address> 12-1-1995) REFERENCES 15 Weiland A.P. </address> <note> (1990) Evolving controls for unstable systems. In D.S. </note> <editor> Touretsky, J.L. Elman, T.J Sejnowski & G.E. Hinton (Eds.) </editor> <booktitle> Proceedings of the 1990 connectionist models summer school (pp. </booktitle> <pages> 91-102). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Weiland A.P. </author> <title> (1991) Evolving neural network controllers for unstable systems. </title> <booktitle> IEEE international joint conference on neural networks (pp. </booktitle> <address> II-667 II-673). Seattle, WA: </address> <publisher> IEEE. </publisher>
Reference: <author> Wilson S.W. </author> <title> (1990) Perceptron redux: Emergence of structure. </title> <editor> In S. Forrest (Ed.), </editor> <booktitle> Emergent Computation (pp. </booktitle> <pages> 249-256). </pages> <address> Amsterdam: </address> <publisher> North Holland. </publisher>
Reference: <author> Whitley D. and Hanson T. </author> <title> (1989) Optimizing neural networks using faster, more accurate genetic search. </title> <editor> In J.D. Schaffer (Ed.), </editor> <booktitle> Third international conference on genetic algorithms (pp. </booktitle> <pages> 391-396). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Whitley D., Starkweather T. and Bogart C. </author> <title> (1990) Genetic Algorithms and Neural Networks: Optimizing Connections and Connectivity. </title> <journal> Parallel Computing. </journal> <volume> 14 </volume> <pages> 347-361. </pages>
Reference: <author> Whitley, D., Dominic, S. & Das, R. </author> <year> (1991). </year> <title> Genetic Reinforcement Learning with Multilayered Neural Networks. </title> <booktitle> Proc. 4th International Conf. on Genetic Algorithms, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Whitley D., Dominic S., Das R. and Anderson C. </author> <title> (1993) Genetic Reinforcement Learning for Neurocontrol Problems. </title> <booktitle> Machine Learning 13 </booktitle> <pages> 259-284. </pages>
Reference-contexts: In step 6 a parallel divide is executed. In step 7 the "-" symbol has been executed and a negative weight is introduced feeding into the output node. In step 8 the black cell has changed its threshold. In the final steps, the remaining cells just read termination symbols. <ref> (This figure is taken from Gruau and Whitley, 1993) </ref>. cbook 16/8/1995 13:52|PAGE PROOFS for John Wiley & Sons Ltd (jwbook.sty v3.0, 12-1-1995) GENETIC ALGORITHMS AND NEURAL NETS 11 Another way to reuse development code is to use a form of Automatic Function Definition like that used in Genetic Programming.
Reference: <author> Whitley D., Gruau F. and Pyeatt L. </author> <title> (1995) Cellular Encoding Applied to Neurocontrol. </title> <booktitle> In, 5th Intern. Conf. on Genetic Algorithms, </booktitle> <editor> L. Eshelman, ed., </editor> <publisher> Morgan Kaufmann. cbook 16/8/1995 13:52|PAGE PROOFS for John Wiley & Sons Ltd (jwbook.sty v3.0, 12-1-1995) </publisher>
References-found: 47

