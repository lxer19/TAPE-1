URL: http://www.cs.dartmouth.edu/~jasonliu/courses/sim188/biblio/muel94.ps.gz
Refering-URL: http://www.cs.dartmouth.edu/~jasonliu/courses/sim188/notes-08.html
Root-URL: http://www.cs.dartmouth.edu
Title: Static Cache Simulation and its Applications  
Author: by Frank Mueller Frank Mueller 
Note: Copyright c  
Address: Tallahassee, FL 32306-4019  
Affiliation: Dept. of Computer Science Florida State University  
Email: e-mail: mueller@cs.fsu.edu  
Phone: phone: (904) 644-3441  
Degree: A Dissertation submitted to the Department of Computer Science in partial fulfillment of the requirements for the degree of Doctor of Philosophy Degree Awarded:  All Rights Reserved  
Web: fl1994  
Date: July 12, 1994  Summer Semester, 1994  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Adl-Tabatabai and Thomas Gross. </author> <title> Detection and recovery of endangered variables caused by instruction scheduling. </title> <booktitle> In ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 13-25, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: The program being debugged has been compiled with full optimizations to avoid time distortion. The compiler was modified to emit debugging information for unoptimized code as well as optimized code. Emitting accurate debugging information for optimized code is a non-trivial task and subject to ongoing research <ref> [1, 11, 32] </ref>. Contrary to debugging unoptimized code, debugging optimized code typically restricts the scope of breakpoints and the displaying of data structures.
Reference: [2] <author> A. Agrawal, R. L Sites, and M. Horowitz. ATUM: </author> <title> A new technique for capturing address traces using microcode. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <pages> pages 119-127, </pages> <year> 1986. </year>
Reference-contexts: The technique is about as fast as inline tracing (about 20x slow down) due to the fact that the execution is still slowed down moderately by the additional microcode instructions <ref> [2] </ref>. Furthermore, the technique is generally not portable, and modern architectures, such as RISC processors, do not have microcode or do not provide the ability to reprogram microcode anymore.
Reference: [3] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: Such an example will be illustrated later in the context of instruction cache analysis. A program may be composed of a number of functions. The possible sequence of calls between these functions is depicted in a call graph <ref> [3] </ref>. Functions can be further distinguished by function instances. An instance depends on the call sequence, i.e. on the immediate call site of its caller, the caller's call site, etc. The function instances of a call graph are defined below. <p> The correctness of the algorithm for data-flow analysis is discussed in <ref> [3] </ref>. The calculation can be performed for an arbitrary control-flow graph, even if it is irreducible. In addition, the order of processing basic blocks is irrelevant for the correctness of the algorithm. <p> This information for basic blocks is commonly used in optimizing compilers. A more detailed discussion of post dominators can be found elsewhere <ref> [3] </ref>. In addition, a new category of first-hits is introduced. In analogy to a first miss, a first hit occurs when the first reference to an instruction in a loop results in a cache hit but all subsequent references in the loop result in misses. <p> " u) first-miss if worst (i k ; child ()) =first-miss^k = first ^ l 2 s^ 9 m 2 s ^ 8 m 62 (s " u) always-miss otherwise 2 The common notion of "natural loops" defines a single loop header preceded by a single preheader outside the loop <ref> [3] </ref>. This work extends this notion to handle more general control flow with unstructured loops. Multiple loop headers occur only for unstructured loops, which are handled by the simulator. <p> In addition, the value of variables assigned 71 to a register will only be displayed if all live ranges of the variable are assigned to the same register <ref> [3] </ref>. Register-mapped values may still be inconsistent at times due to global optimizations, such as common subexpression elimination, which is a common problem when debugging optimized code. The fact that optimized code is executed during debugging speeds up the execution over conventional debugging of unoptimized code.
Reference: [4] <author> R. Arnold. </author> <title> Bounding instruction cache performance. </title> <type> Master's thesis, </type> <institution> Dept. of CS, Florida State University, </institution> <month> December </month> <year> 1994. </year> <note> (to appear). </note>
Reference-contexts: A detailed description can be found elsewhere <ref> [5, 4] </ref>. But a short outline and some preliminary results shall be presented to illustrate the benefit of static cache simulation for timing analysis. The timing tool constructs a timing analysis tree as discussed earlier. The instruction categorizations are then used to calculate the timings for each node.
Reference: [5] <author> R. Arnold, F. Mueller, D. B. Whalley, and M. Harmon. </author> <title> Bounding worst-case instruction cache performance. </title> <booktitle> In IEEE Symposium on Real-Time Systems, </booktitle> <month> December </month> <year> 1994. </year> <note> (accepted). </note>
Reference-contexts: However, more recent work (combining the static simulator with a timing tool) shows that the instruction categorization of the static simulator may be used by a more sophisticated timing tool to provide tight worst-case execution time predictions with a 4-9 times speedup over uncached system using a conventional instruction cache <ref> [5] </ref>. This will be discussed in more detail in the next chapter. 7.6 Future Work Further work focuses on integrating the method of static cache simulation with a tool that estimates a program's best-case execution time (BET) and worst-case execution time (WET) [28, 5]. <p> This will be discussed in more detail in the next chapter. 7.6 Future Work Further work focuses on integrating the method of static cache simulation with a tool that estimates a program's best-case execution time (BET) and worst-case execution time (WET) <ref> [28, 5] </ref>. Using the information provided by static cache simulation, the BET and WET can be based on the categorization of instructions. This relieves the time-estimation tool from having to simulate all possible cache states. <p> This chapter describes an approach for bounding the worst-case instruction cache performance of large code segments. Excerpts of this chapter can be found in <ref> [5] </ref>. 8.1 Introduction Caches present a dilemma for architects of real-time systems. The use of cache memory in the context of real-time systems introduces a potentially high level of unpredictability. An instruction's execution time can vary greatly depending on whether the instruction causes a cache hit or miss. <p> A detailed description can be found elsewhere <ref> [5, 4] </ref>. But a short outline and some preliminary results shall be presented to illustrate the benefit of static cache simulation for timing analysis. The timing tool constructs a timing analysis tree as discussed earlier. The instruction categorizations are then used to calculate the timings for each node.
Reference: [6] <author> T. P. Baker, F. Mueller, and Viresh Rustagi. </author> <title> Experience with a prototype of the POSIX "minimal realtime system profile". </title> <booktitle> In IEEE Workshop on Real-Time Operating Systems and Software, </booktitle> <pages> pages 12-16, </pages> <year> 1994. </year>
Reference-contexts: Traditional trace-driven cache simulation (for each basic block) is reported to slow down the execution time by over one to three orders of a magnitude [68]. 2 A multi-threaded real-time kernel has been designed for such an embedded system based on a SPARC VME bus board <ref> [6, 51] </ref>. 74 9.7 Conclusion This work discusses some challenges of real-time debugging that have not yet been addressed adequately. A debugging environment is proposed that addresses the problem of time distortion during debugging.
Reference: [7] <author> T. Ball and J. R. Larus. </author> <title> Optimally profiling and tracing programs. </title> <booktitle> In ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 59-70, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: This technique is faster than the above techniques (about 10x slow down) and is currently regarded as the preferred method to collect trace data. A minimal set of instrumentation points can be determined by analyzing the control-flow graph <ref> [7] </ref>. A number of different inline tracing techniques have been used to instrument the code and to process the trace data. Stunkel and Fuchs [62] instrumented the code during compilation and analyzed the trace on-the-fly as part of the program execution. <p> The counters are typically associated with basic blocks and are incremented each time the basic block executes. The number of measurement points can be reduced from all basic blocks to a minimal set of control-flow transitions, which guarantees optimal profiling for most programs as explained by Ball and Larus <ref> [7] </ref>. Their reported overhead is a factor of 1.3-3.0 for basic block frequency accounting and 1.1-1.5 for optimal frequency accounting. <p> In such cases, the trace action can be simplified for each subsequent iteration. But the performance of these techniques suggests that they do not scale well for small caches. Unfortunately, on-the-fly analysis cannot be performed on the minimal set of measurement points used by inline tracing and frequency counting <ref> [7] </ref>. The minimal set of measurement points does not immediately provide a full set of events and their execution order. It is one of the objectives of this work to determine a small set of measurement points that still covers all events and preserves their execution order. <p> A near-optimal (often even optimal) solution to the problem for a control-flow graph G can be found by determining a maximum spanning tree max (G) for the control-flow graph and inserting code on the edges of G max (G) <ref> [38, 7] </ref>. Recently, tracing and analyzing programs has been combined using inline tracing [10] and on-the-fly analysis [68, 69]. Both techniques require that events are analyzed as they occur. Traditional inline tracing performs the analysis separate from the generation of trace information. <p> The first aspect, the strategy of instrumenting edges (or vertices where possible), is also fundamental to the aforementioned work on optimal profiling and tracing by Ball and Larus <ref> [7] </ref>. It is the second aspect that distinguishes this new approach from their work. The option of performing static analysis on the control flow to determine and optimize the instrumentation code for order-dependent on-the-fly analysis requires the definition of ranges for the analysis. <p> It has been well established that a small set of measurement points for this traditional approach can be provided by the edges of G max (G), where G is the control-flow graph and max (G) is its maximum spanning tree <ref> [38, 7] </ref>. The resulting placement is optimal for a large class of control-flow graphs, in particular reducible graphs resulting from structured programming constructs, and it is near-optimal for most other cases. <p> The postprocessing pass, which generates the complete trace from the subset of addresses stored, was much slower and produced about 3,000 addresses per second. No information was given on the overhead required to actually analyze the cache performance. Ball and Larus <ref> [7, 41] </ref> also reduced the overhead of the trace generation by storing a portion of the trace from which the complete trace can be generated. They optimized the placement of the instrumentation code to produce the reduced trace with respect to a weighting of the control-flow graph.
Reference: [8] <author> M. E. Benitez and J. W. Davidson. </author> <title> A portable global optimizer and linker. </title> <booktitle> In ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 329-338, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Table 3.1 summarizes the performance tests for user programs, benchmarks, and UNIX utilities. The numbers were produced by modifying the back-end of an optimizing compiler VPO (Very Portable Optimizer) <ref> [8] </ref> to determine measurement points by partitioning the control flow and by creating the function-instance graph. 25 Table 3.1: Results for Measurement Overhead Size Instructions Measure Pts. <p> The measurements in the next section are based on this number. All measurements were produced by modifying the back-end of an optimizing compiler VPO (Very Portable Optimizer) <ref> [8] </ref> and by performing static cache simulation. The simula tion was performed for the Sun SPARC instruction set, a RISC architecture with a uniform instruction size of one word (four bytes). The parameters for cache simulation included direct-mapped caches with sizes of 1kB, 2kB, 4kB, and 8kB. <p> Cache measurements were obtained for user programs, benchmarks, and UNIX utilities listed in Table 3.1. The measurements were produced by modifying the back-end of an optimizing compiler VPO (Very Portable Optimizer) <ref> [8] </ref> and by performing static cache simulation. The compiler back-end provided the control-flow information for the static simulator. It also produced assembly code with instrumentation points for instruction cache simulation. 3 Architectural features may present an exception to this general rule. <p> It includes a modified compiler front-end of VPCC (very portable C compiler) [18] and a modified back-end of VPO (very portable optimizer) <ref> [8] </ref>, the static simulator for direct-mapped caches [50], and the regular system linker and source level debugger dbx under SunOS 4.1.3. Calling a library routine to query the elapsed time takes a negligible amount of time in the order of one millisecond.
Reference: [9] <author> D. Bhatt, A. Ghonami, and R. </author> <title> Ramanujan. An instrumented testbed for real-time distributed systems development. </title> <booktitle> In IEEE Symposium on Real-Time Systems, </booktitle> <pages> pages 241-250, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: The amount of work in the area of 69 real-time debugging has been limited with a few exceptions. The Remedy debugging tool [57] addresses the customization of the debugging interface for real-time purposes and synchronizes on breakpoints by suspending the execution on all processors. DCT <ref> [9] </ref> is a tool that allows practically non-intrusive monitoring but requires special hardware for bus access and does not extend to non-intrusive debugging. Both RED [30] and ART [67] provide monitoring and debugging facilities at the price of software instrumentation.
Reference: [10] <author> A. Borg, R. E. Kessler, and D. W. Wall. </author> <title> Generation and analysis of very long address traces. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <pages> pages 270-279, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Stunkel and Fuchs [62] instrumented the code during compilation and analyzed the trace on-the-fly as part of the program execution. Eggers et. al. [22] instrumented code during compilation and saved the trace data in secondary storage for later analysis. Borg et. al. <ref> [10] </ref> instrumented the program at link time and processed the trace data in parallel to the program execution by using buffers shared between the program and the analysis tool. The pros and cons of these approaches can be summarized as follows. <p> Recently, tracing and analyzing programs has been combined using inline tracing <ref> [10] </ref> and on-the-fly analysis [68, 69]. Both techniques require that events are analyzed as they occur. Traditional inline tracing performs the analysis separate from the generation of trace information. On-the-fly analysis integrates the program analysis into its execution. <p> Measurement instructions are inserted in the 40 program to record the addresses that are referenced during the execution in a file or buffer. The program analysis may be performed concurrently on a buffered address trace to reduce the storage requirements to temporary trace buffers. Borg et. al <ref> [10] </ref> and Eggers et. al. [22] used this technique to obtain accurate measurements for the simulation of instruction and data caches. Whalley [68, 69] used on-the-fly analysis where a cache simulation function was called during program execution for each basic block. <p> The program analysis is performed either concurrently on the buffered address trace to reduce the storage requirements to temporary trace buffers or it is performed after program execution on trace file data. Borg, Kessler, and Wall <ref> [10] </ref> modified programs at link time to write addresses to a trace buffer and these addresses were analyzed by a separate process. The time required to generate the trace of addresses was reduced by reserving five of the general purpose registers to avoid memory references in the trace generation code. <p> Example: The approach for bounding instruction cache performance is illustrated in Figure 8.3. Part (a) contains the C code for a simple toy program that finds the largest value in 1 extern int a <ref> [10] </ref>; 2 3 int value (index) 4 int index; 5 - 6 return a [index]; 7 - 9 main () 10 - 12 14 if (max &lt; value (i)) 15 max = value (i); 16 return max; 17 - (a) C Program to find MAX (Array) (b) Timing Analysis Tree (c) <p> Yet, it is possible to analyze the control flow of an arbitrary executable and to modify the binary by inserting instrumentation code (similar to the work in <ref> [42, 10] </ref>).
Reference: [11] <author> G. Brooks, G. Hansen, and S. Simmons. </author> <title> A new approach to debugging optimized code. </title> <booktitle> In ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 1-11, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: The program being debugged has been compiled with full optimizations to avoid time distortion. The compiler was modified to emit debugging information for unoptimized code as well as optimized code. Emitting accurate debugging information for optimized code is a non-trivial task and subject to ongoing research <ref> [1, 11, 32] </ref>. Contrary to debugging unoptimized code, debugging optimized code typically restricts the scope of breakpoints and the displaying of data structures.
Reference: [12] <author> B. Burgess, N. Ullah, P. Van Overen, and D. Ogden. </author> <title> The PowerPC 603 microprocessor. </title> <journal> Communications of the ACM, </journal> <volume> 37(6) </volume> <pages> 34-42, </pages> <month> June </month> <year> 1994. </year> <month> 89 </month>
Reference-contexts: Recent results have shown that direct-mapped caches have a faster access time for hits, which outweighs the benefit of a higher hit ratio in set-associative caches for large cache sizes [31]. Yet, current micro-processors are still designed with set-associative caches <ref> [12] </ref>. A modified algorithm and data structure could be designed to handle set-associative caches within the framework of static cache simulation. The implementation of the static cache simulator currently rejects the analysis of recursive functions.
Reference: [13] <editor> G. Chartrand and L. Lesniak. </editor> <title> Graphs & Digraphs. </title> <publisher> Wadsworth & Brooks, </publisher> <address> 2nd edition, </address> <year> 1986. </year>
Reference-contexts: 1 ; -1 ; :::; * n ; -n with the ordered set of edges * p = f* 1 ; :::; * n g E and the ordered set of vertices -p = f-0 ; :::; -n g V , i.e., a sequence of distinct vertices connected by edges <ref> [13] </ref>. The edge * i may also be denoted as i1 ! i. Vertex -0 is called an head vertex and vertex -n a tail vertex, while all other i are internal vertices.
Reference: [14] <author> C.-H. Chi and H. Dietz. </author> <title> Unified management of register and cache using liveness and cache bypass. </title> <booktitle> In ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 344-355, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: McFarling [45] used such an approach to exclude instructions from cache that were not likely to be in cache on subsequent references. Chi and Dietz <ref> [14] </ref> introduced a data cache bypass bit on load and store instructions, which, when set, indicates that the processor should go directly to memory (without caching the value as a side-effect) or goes to the cache when clear.
Reference: [15] <author> D. W. Clark. </author> <title> Cache performance in the VAX-11/780. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 1(1) </volume> <pages> 24-37, </pages> <month> February </month> <year> 1983. </year>
Reference-contexts: The probes can be stored in a trace file. This technique requires additional, expensive hardware and some expertise to use this hardware. The technique is very fast since the program executes at its original speed and does not need to be modified <ref> [16, 15] </ref>. Yet, the method hides on-chip activities such as instruction or data references accessing primary caches. * Frequency Counting: Similar to inline tracing, the program is modified to include instrumentation code. But rather than generating a program trace, the execution frequency of code portions is recorded for later analysis.
Reference: [16] <author> D. W. Clark and H. M. Levy. </author> <title> Measurement and analysis of instruction use in the VAX-11/780. </title> <booktitle> In Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 9-17, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: The probes can be stored in a trace file. This technique requires additional, expensive hardware and some expertise to use this hardware. The technique is very fast since the program executes at its original speed and does not need to be modified <ref> [16, 15] </ref>. Yet, the method hides on-chip activities such as instruction or data references accessing primary caches. * Frequency Counting: Similar to inline tracing, the program is modified to include instrumentation code. But rather than generating a program trace, the execution frequency of code portions is recorded for later analysis.
Reference: [17] <author> B. Cogswell and Z. Segall. </author> <title> MACS: a predictable architecture for real time systems. </title> <booktitle> In IEEE Symposium on Real-Time Systems, </booktitle> <pages> pages 296-305, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: One attempt by Kirk via 59 cache partitioning has already been discussed in the last chapter, including some of the problems, such as lower hit ratios and increased complexity of scheduling analysis [35]. Cogswell and Segall suggest a different approach for the MACS architecture <ref> [17] </ref>, which uses no cache memory. Instead, their pipelined processor performs a context swap between threads in a round-robin ordering on each instruction. No thread may have more than one instruction in the pipeline at one time.
Reference: [18] <author> J. W. Davidson and D. B. Whalley. </author> <title> Quick compilers using peephole optimizations. </title> <journal> Software Practice & Experience, </journal> <volume> 19(1) </volume> <pages> 195-203, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: Another option would be to redesign the task set and the schedule, for example by further partitioning of the tasks [24]. 9.5 Measurements The environment discussed above was implemented for the SPARC architecture. It includes a modified compiler front-end of VPCC (very portable C compiler) <ref> [18] </ref> and a modified back-end of VPO (very portable optimizer) [8], the static simulator for direct-mapped caches [50], and the regular system linker and source level debugger dbx under SunOS 4.1.3.
Reference: [19] <author> J. W. Davidson and D. B. Whalley. </author> <title> Ease: An environment for architecture study and experimentation. </title> <booktitle> In SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 259-260, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The measurements are accurate and the order of events is preserved for the analysis. The program execution has to be repeated if simulation parameters change. With some additional effort, this method can even be used for prototyped architectures <ref> [19] </ref>. In this dissertation, inline on-the-fly analysis will be simply referred to as "on-the-fly analysis". 2.2 Cache Simulation The task of cache simulation is to ascertain the number of cache hits and misses for a program execution. <p> Lately, on-the-fly analysis has been performed for collecting all measurements for a certain analysis during program execution and generally results in a lower overall overhead than traditional tracing methods. In the past, on-the-fly analysis was performed at the level of basic blocks <ref> [19] </ref>. Independent research by Emami et. al. [23] defines an invocation graph that has properties similar to the function-instance graph. Their intention lies in interprocedural data-flow and alias analysis.
Reference: [20] <author> J. W. Davidson and D. B. Whalley. </author> <title> A design environment for addressing architecture and compiler interactions. </title> <journal> Microprocessors and Microsystems, </journal> <volume> 15(9) </volume> <pages> 459-472, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: This was due to the program spending most of its cycles in 3 tightly nested loops containing no calls to perform the actual multiplication. Column 8 shows the time in cycles for an execution with worst-case input data. The number of cycles was measured using a traditional cache simulator <ref> [20] </ref>, where a hit required one cycle and a miss required ten cycles (a miss penalty of nine cycles). These assumptions were described as realistic by other researchers [31, 29].
Reference: [21] <author> S. J. Eggers and R. H. Katz. </author> <title> A characterization of sharing in parallel programs and its application to coherency protocol evaluation. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <pages> pages 373-382, </pages> <year> 1988. </year>
Reference-contexts: The interrupt handler can be used to gather the trace data. This technique is just slightly faster the hardware simulation (100x - 1000x slow down) and works only for existing architectures <ref> [70, 21] </ref>, though sometimes traces from an existing architecture are used to project the speed of prototyped architectures [56]. Inline Tracing is a technique where the program is instrumented before execution such that the trace data is generated by the instrumentation code as a side effect of the program execution.
Reference: [22] <author> S. J. Eggers, D. R. Keppel, E. J. Koldinge, and H. M. Levy. </author> <title> Techniques for efficient inline tracing on a shared-memory multiprocessor. </title> <booktitle> In SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 37-47, </pages> <year> 1990. </year>
Reference-contexts: A number of different inline tracing techniques have been used to instrument the code and to process the trace data. Stunkel and Fuchs [62] instrumented the code during compilation and analyzed the trace on-the-fly as part of the program execution. Eggers et. al. <ref> [22] </ref> instrumented code during compilation and saved the trace data in secondary storage for later analysis. Borg et. al. [10] instrumented the program at link time and processed the trace data in parallel to the program execution by using buffers shared between the program and the analysis tool. <p> The program analysis may be performed concurrently on a buffered address trace to reduce the storage requirements to temporary trace buffers. Borg et. al [10] and Eggers et. al. <ref> [22] </ref> used this technique to obtain accurate measurements for the simulation of instruction and data caches. Whalley [68, 69] used on-the-fly analysis where a cache simulation function was called during program execution for each basic block. <p> Analysis of the trace was stated to require at least 10 times of the overhead of the generation of the trace (or about 100 times slower than normal execution time). 42 Eggers et. al. <ref> [22] </ref> also used the technique of inline tracing to generate a trace of addresses to a trace buffer, which was copied to disk by a separate process. They used several strategies for minimizing the overhead of generating the trace.
Reference: [23] <author> M. Emami, R. Ghiya, and L. J. Hendren. </author> <title> Context-sensitive interprocedural points-to analysis in the presence of function pointers. </title> <booktitle> In ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 242-256, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Lately, on-the-fly analysis has been performed for collecting all measurements for a certain analysis during program execution and generally results in a lower overall overhead than traditional tracing methods. In the past, on-the-fly analysis was performed at the level of basic blocks [19]. Independent research by Emami et. al. <ref> [23] </ref> defines an invocation graph that has properties similar to the function-instance graph. Their intention lies in interprocedural data-flow and alias analysis.
Reference: [24] <author> R. Gerber and S. Hong. </author> <title> Semantics-based compiler transformations for enhanced schedu-lability. </title> <booktitle> In IEEE Symposium on Real-Time Systems, </booktitle> <pages> pages 232-242, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: The debugger will help to find the culprit in such situations. Another option would be to redesign the task set and the schedule, for example by further partitioning of the tasks <ref> [24] </ref>. 9.5 Measurements The environment discussed above was implemented for the SPARC architecture.
Reference: [25] <author> S. L. Graham, P. B. Kessler, and M. K. McKusick. </author> <title> gprof: A call graph execution profiler. </title> <booktitle> In Symposium on Compiler Construction, </booktitle> <pages> pages 276-283, </pages> <month> June </month> <year> 1982. </year>
Reference-contexts: This approach provides a statistical sampling method that yields approximate measurements indicating the estimated portion of the total execution time spent in certain portions of the program 11 (typically at the scope of functions). To perform the sampling, common tools, such as prof [65] and gprof <ref> [25, 26] </ref>, rely on the availability of hardware timers and the corresponding operating system interface to activate these timers, catch the timer interrupt, and sample the program counter at the interrupt point. <p> Excerpts of this chapter can be found in [47]. 5.1 Introduction Statistical sampling methods are often employed by profiling tools such as prof [65] or gprof <ref> [25, 26] </ref>. Yet, these tools only provide approximate measurements. On the other hand, code instrumentation results in accurate profiling measurements. For example, instruction frequency measurements can be obtained by inserting instructions that increment frequency counters into a program.
Reference: [26] <author> S. L. Graham, P. B. Kessler, and M. K. McKusick. </author> <title> An execution profiler for modular programs. </title> <journal> Software Practice & Experience, </journal> <volume> 21(11) </volume> <pages> 25-40, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: This approach provides a statistical sampling method that yields approximate measurements indicating the estimated portion of the total execution time spent in certain portions of the program 11 (typically at the scope of functions). To perform the sampling, common tools, such as prof [65] and gprof <ref> [25, 26] </ref>, rely on the availability of hardware timers and the corresponding operating system interface to activate these timers, catch the timer interrupt, and sample the program counter at the interrupt point. <p> Excerpts of this chapter can be found in [47]. 5.1 Introduction Statistical sampling methods are often employed by profiling tools such as prof [65] or gprof <ref> [25, 26] </ref>. Yet, these tools only provide approximate measurements. On the other hand, code instrumentation results in accurate profiling measurements. For example, instruction frequency measurements can be obtained by inserting instructions that increment frequency counters into a program.
Reference: [27] <author> T. </author> <title> Hand. Real-time systems need predictability. </title> <booktitle> Computer Design (RISC Supplement), </booktitle> <pages> pages 57-59, </pages> <month> August </month> <year> 1989. </year> <month> 90 </month>
Reference-contexts: cache analysis via static cache simulation is a general method to quickly obtain accurate measurements. 48 Chapter 7 Predicting Instruction Cache Behavior It has been claimed that the execution time of a program can often be predicted more accurately on an uncached system than on a system with cache memory <ref> [27, 60, 43] </ref>. Thus, caches are often disabled for critical real-time tasks to ensure the predictability required for scheduling analysis. This work shows that instruction caching can be exploited to gain execution speed without sacrificing predictability.
Reference: [28] <author> M. Harmon, T. P. Baker, and D. B. Whalley. </author> <title> A retargetable technique for predicting execution time. </title> <booktitle> In IEEE Symposium on Real-Time Systems, </booktitle> <pages> pages 68-77, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: For real-time systems, several tools to predict the execution time of programs have been designed. The analysis has been performed at the level of source code [54], at the level of intermediate code [52], and at the level of machine code <ref> [28] </ref>. Only Harmon's tool took the impact of instruction caches into account for restrictive circumstances, i.e. only for small code segments that entirely fit into cache. Niehaus outlined how the effects of caching can be taken into account in the prediction of execution time [53]. <p> This will be discussed in more detail in the next chapter. 7.6 Future Work Further work focuses on integrating the method of static cache simulation with a tool that estimates a program's best-case execution time (BET) and worst-case execution time (WET) <ref> [28, 5] </ref>. Using the information provided by static cache simulation, the BET and WET can be based on the categorization of instructions. This relieves the time-estimation tool from having to simulate all possible cache states. <p> Finally, future work and conclusions are presented. 8.2 Related Work As already mentioned in Chapter 7, several tools to predict the execution time of programs have been designed for real-time systems. The analysis has been performed at the level of source code [54], intermediate code [52], and machine code <ref> [28] </ref>. Only the last tool attempted to estimate the effect of instruction caching and was only able to analyze small code segments that contained no function calls and entirely fit into cache. Thus, this tool was able to assume that at most one miss will occur for each reference. <p> Current work also includes an attempt to predict the execution time of code segments on a MicroSPARC I processor. In order to provide realistic timing predictions, the effect of other architectural features besides instruction caching (e.g. pipelining) must be analyzed. A technique called micro-analysis <ref> [28] </ref> was developed to detect the potential overlap between operations on various CISC processors.
Reference: [29] <author> J. Hennessy and D. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Mor-gan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Primary caches are a major contributor to the speed-up of memory access. Unified caches have been designed where instructions and data are not separated (Von-Neumann Architecture). For primary caches separate instruction caches and data caches are more popular (Harvard Architecture) <ref> [31, 29] </ref>. This is due to the trend of modern processors to pipeline the instruction execution. A pipelined architecture overlaps the different stages of instruction execution to effectively achieve a throughput of one instruction per clock cycle. Typical pipeline stages are instruction fetch, decode, load operands, execute, and store result. <p> Assume a cache look-up penalty of one cycle <ref> [31, 29] </ref>. <p> These assumptions are described as realistic by other researchers <ref> [31, 29] </ref>. A memory fetch in an uncached system fetches exactly one instruction while a memory fetch in a cached system fetches a line of 4 instructions. <p> Bounding instruction cache performance for real-time applications may be quite beneficial. The use of instruction caches has a greater impact on performance than the use of data caches. Code generated for a RISC machine typically results in four times more instruction references than data references <ref> [29] </ref>. In addition, there tends to be a greater locality for instruction references than data references, typically resulting in higher hit ratios for instruction cache performance. Also, unlike many data references, the address of each instruction remains the same during a program's execution. <p> The number of cycles was measured using a traditional cache simulator [20], where a hit required one cycle and a miss required ten cycles (a miss penalty of nine cycles). These assumptions were described as realistic by other researchers <ref> [31, 29] </ref>. Column 9 shows the ratio of the predicted worst-case instruction cache performance using the timing analyzer to the observed worst-case performance in column 8. Column 10 shows a similar ratio assuming a disabled cache.
Reference: [30] <author> C. R. Hill. </author> <title> A real-time microprocessor debugging technique. </title> <booktitle> In ACM SIG-SOFT/SIGPLAN Software Engineering Symposium on High-Level Debugging, </booktitle> <pages> pages 145-148, </pages> <year> 1983. </year>
Reference-contexts: DCT [9] is a tool that allows practically non-intrusive monitoring but requires special hardware for bus access and does not extend to non-intrusive debugging. Both RED <ref> [30] </ref> and ART [67] provide monitoring and debugging facilities at the price of software instrumentation. RED dedicates a co-processor to collect trace data and send it to the host system. The instrumentation is removed for production code.
Reference: [31] <author> M. Hill. </author> <title> A case for direct-mapped caches. </title> <journal> IEEE Computer, </journal> <volume> 21(11) </volume> <pages> 25-40, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: Primary caches are a major contributor to the speed-up of memory access. Unified caches have been designed where instructions and data are not separated (Von-Neumann Architecture). For primary caches separate instruction caches and data caches are more popular (Harvard Architecture) <ref> [31, 29] </ref>. This is due to the trend of modern processors to pipeline the instruction execution. A pipelined architecture overlaps the different stages of instruction execution to effectively achieve a throughput of one instruction per clock cycle. Typical pipeline stages are instruction fetch, decode, load operands, execute, and store result. <p> This would not be possible if a primary cache was a unified cache. The organization of caches varies from fully-associative caches to direct-mapped caches. Recent results have shown that direct-mapped caches tend to match, if not exceed, the speed of associative caches for large cache sizes <ref> [31] </ref>. Due to the slightly more complex design, the access time for hits of associative caches is generally slower (by about 10%) than the access time for direct-mapped caches. <p> The current implementation of the static simulator imposes the restriction that only direct-mapped cache configurations are allowed. Recent results have shown that direct-mapped caches have a faster access time for hits, which outweighs the benefit of a higher hit ratio in set-associative caches for large cache sizes <ref> [31] </ref>. Yet, current micro-processors are still designed with set-associative caches [12]. A modified algorithm and data structure could be designed to handle set-associative caches within the framework of static cache simulation. The implementation of the static cache simulator currently rejects the analysis of recursive functions. <p> Assume a cache look-up penalty of one cycle <ref> [31, 29] </ref>. <p> These assumptions are described as realistic by other researchers <ref> [31, 29] </ref>. A memory fetch in an uncached system fetches exactly one instruction while a memory fetch in a cached system fetches a line of 4 instructions. <p> The number of cycles was measured using a traditional cache simulator [20], where a hit required one cycle and a miss required ten cycles (a miss penalty of nine cycles). These assumptions were described as realistic by other researchers <ref> [31, 29] </ref>. Column 9 shows the ratio of the predicted worst-case instruction cache performance using the timing analyzer to the observed worst-case performance in column 8. Column 10 shows a similar ratio assuming a disabled cache. <p> The elapsed time is then calculated as t elapsed = hits fl hit penalty + misses fl miss penalty [cycles] where the hit penalty is typically one cycle while the miss penalty is ten cycles <ref> [31] </ref> or even more, depending on the clock rate and the access time of main memory. This time estimate can be converted into seconds by multiplying it by the cycle time.
Reference: [32] <author> U. Hoelzle, C. Chambers, and D. Ungar. </author> <title> Debugging optimized code with dynamic deoptimization. </title> <booktitle> In ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 32-43, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: The program being debugged has been compiled with full optimizations to avoid time distortion. The compiler was modified to emit debugging information for unoptimized code as well as optimized code. Emitting accurate debugging information for optimized code is a non-trivial task and subject to ongoing research <ref> [1, 11, 32] </ref>. Contrary to debugging unoptimized code, debugging optimized code typically restricts the scope of breakpoints and the displaying of data structures.
Reference: [33] <author> M. Huguet, T. Lang, and Y. Tamir. </author> <title> A block-and-actions generator as an alternative to a simulator for collecting architecture measurement. </title> <booktitle> In ACM SIGPLAN Symposium on Interpreters and Interpretive Techniques, </booktitle> <pages> pages 14-25, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: Such an approach can easily slow the execution by a factor of a 1000 or more <ref> [55, 70, 33] </ref>. A technique called inline tracing can be used to generate the trace of addresses with much less overhead than trapping or simulation. Measurement instructions are inserted in the program to record the addresses that are referenced during the execution in a buffer.
Reference: [34] <author> D. Kerns and S. Eggers. </author> <title> Balanced scheduling: Instruction scheduling when memory latency is uncertain. </title> <booktitle> In ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 278-289, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Compiler flow analysis can be used to detect the pattern of many calculated references, such as indexing through an array. Previous work has shown improvements by balancing the number of instructions placed behind loads where the memory latency was uncertain <ref> [34] </ref>. By predicting the memory latency of a large portion of loads, instruction scheduling could be performed more effectively.
Reference: [35] <author> D. B. Kirk. </author> <title> SMART (strategic memory allocation for real-time) cache design. </title> <booktitle> In IEEE Symposium on Real-Time Systems, </booktitle> <pages> pages 229-237, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: No general method to analyze the call graph of a task and control flow for each function was given. A few attempts have been made to improve the predictability of caches by architectural modifications to meet the needs of real-time systems. Kirk <ref> [35] </ref> outlined such a system that relied on the ability to segment the cache memory into a number of dedicated partitions, each of which can only be accessed by a dedicated task. <p> One attempt by Kirk via 59 cache partitioning has already been discussed in the last chapter, including some of the problems, such as lower hit ratios and increased complexity of scheduling analysis <ref> [35] </ref>. Cogswell and Segall suggest a different approach for the MACS architecture [17], which uses no cache memory. Instead, their pipelined processor performs a context swap between threads in a round-robin ordering on each instruction. No thread may have more than one instruction in the pipeline at one time.
Reference: [36] <author> E. Kligerman and A. Stoyenko. </author> <title> Real-time euclid: A language for reliable real-time systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-12(9):941-949, </volume> <month> September </month> <year> 1986. </year>
Reference-contexts: The following problems have to be addressed to predict the execution time of a task or program: * The number of loop iterations needs to be known prior to execution. It is often required that the maximum number of iterations be provided by the programmer <ref> [36] </ref>. * The possible execution paths in the control flow have to be analyzed to predict both BET and WET. * Architectural features have to be taken into account (e.g. pipeline stalls).
Reference: [37] <author> D. E. Knuth. </author> <title> An empirical study of FORTRAN programs. </title> <journal> Software Practice & Experience, </journal> <volume> 1 </volume> <pages> 105-133, </pages> <year> 1971. </year>
Reference-contexts: In the following, a summary of different measurement techniques is given. 2.1 Measurement Techniques Program analysis through profiling and tracing has long been used to evaluate new hardware and software designs. For instance, an early reference to profiling by Knuth can be found in <ref> [37, 39] </ref>. Measurement techniques can be distinguished by the provided level of detail of the program analysis.
Reference: [38] <author> D. E. Knuth. </author> <booktitle> The Art of Computer Programming: Fundamental Algorithms, </booktitle> <volume> volume 2. </volume> <publisher> Addison Wesley, </publisher> <address> 2 edition, </address> <year> 1973. </year>
Reference-contexts: A near-optimal (often even optimal) solution to the problem for a control-flow graph G can be found by determining a maximum spanning tree max (G) for the control-flow graph and inserting code on the edges of G max (G) <ref> [38, 7] </ref>. Recently, tracing and analyzing programs has been combined using inline tracing [10] and on-the-fly analysis [68, 69]. Both techniques require that events are analyzed as they occur. Traditional inline tracing performs the analysis separate from the generation of trace information. <p> It has been well established that a small set of measurement points for this traditional approach can be provided by the edges of G max (G), where G is the control-flow graph and max (G) is its maximum spanning tree <ref> [38, 7] </ref>. The resulting placement is optimal for a large class of control-flow graphs, in particular reducible graphs resulting from structured programming constructs, and it is near-optimal for most other cases.
Reference: [39] <author> D. E. Knuth and F. R. Steverson. </author> <title> Optimal measurement points for program frequency counts. </title> <journal> BIT, </journal> <volume> 13 </volume> <pages> 313-322, </pages> <year> 1973. </year>
Reference-contexts: In the following, a summary of different measurement techniques is given. 2.1 Measurement Techniques Program analysis through profiling and tracing has long been used to evaluate new hardware and software designs. For instance, an early reference to profiling by Knuth can be found in <ref> [37, 39] </ref>. Measurement techniques can be distinguished by the provided level of detail of the program analysis.
Reference: [40] <author> S. Laha, J. H. Patel, and R. K. Iyer. </author> <title> Accurate low-cost methods for performance evaluation of cache memory systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(11) </volume> <pages> 1325-1336, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: It is neither possible to collect accurate measurements with this method, nor is it possible to deduce the order of events from the sample after program execution <ref> [40] </ref>. Yet, if the interrupt handler is used to collect and record trace data (see below), the order of events can be reconstructed [56]. * Tracing: This method involves the generation of a partial or full sequence of the instruction and data references encountered during program execution.
Reference: [41] <author> J. R. Larus. </author> <title> Abstract execution: A technique for efficiently tracing programs. </title> <journal> Software Practice & Experience, </journal> <volume> 13(8) </volume> <pages> 671-685, </pages> <month> August </month> <year> 1983. </year>
Reference-contexts: Immediate processing of the trace data allows the analysis of longer traces but can only be performed for one cache configuration per program execution. Abstract Execution is a variation of inline tracing <ref> [41] </ref>. This technique relies on a modified compiler that instruments the original program to generate a trace of "significant events" for a subset of basic blocks and a given program input. <p> The postprocessing pass, which generates the complete trace from the subset of addresses stored, was much slower and produced about 3,000 addresses per second. No information was given on the overhead required to actually analyze the cache performance. Ball and Larus <ref> [7, 41] </ref> also reduced the overhead of the trace generation by storing a portion of the trace from which the complete trace can be generated. They optimized the placement of the instrumentation code to produce the reduced trace with respect to a weighting of the control-flow graph.
Reference: [42] <author> J. R. Larus and T. Ball. </author> <title> Rewriting executable files to measure program behavior. </title> <type> TR 1083, </type> <institution> University of Wisconsin, </institution> <month> March </month> <year> 1992. </year> <month> 91 </month>
Reference-contexts: Yet, it is possible to analyze the control flow of an arbitrary executable and to modify the binary by inserting instrumentation code (similar to the work in <ref> [42, 10] </ref>).
Reference: [43] <author> M. Lee, S. Min, C. Park, Y. Bae, H. Shin, and C. Kim. </author> <title> A dual-mode instruction prefetch scheme for improved worst case and average program execution times. </title> <booktitle> In IEEE Symposium on Real-Time Systems, </booktitle> <pages> pages 98-105, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: cache analysis via static cache simulation is a general method to quickly obtain accurate measurements. 48 Chapter 7 Predicting Instruction Cache Behavior It has been claimed that the execution time of a program can often be predicted more accurately on an uncached system than on a system with cache memory <ref> [27, 60, 43] </ref>. Thus, caches are often disabled for critical real-time tasks to ensure the predictability required for scheduling analysis. This work shows that instruction caching can be exploited to gain execution speed without sacrificing predictability. <p> Thus, this approach requires that a task be broken up into a number of independent threads, which implies restructuring of conventional real-time programs. Lee et. al. suggested an architecture that prefetches instructions in the direction of the worst-case execution path <ref> [43] </ref>. The justification for using their approach was that "it is very difficult, if not impossible, to determine the worst-case execution path and, therefore, the worst-case execution time of a task" when instruction caching is employed. <p> This technique is being extended to model the MicroSPARC I processor. 8.7 Conclusion Predicting the worst-case execution time of a program on a processor that uses cache memory has long been considered an intractable problem <ref> [60, 44, 43] </ref>. However, this work shows that tight estimations in the presence of instruction caches are feasible, using the fact that the addresses of the instructions within a program and the possible control-flow paths between these instructions are known statically. <p> This speedup is a considerable improvement over prior work, such as requiring special architectural modifications for prefetching, which only results in a speedup factor of 2 <ref> [43] </ref>.
Reference: [44] <author> T. H. Lin and W. S. Liou. </author> <title> Using cache to improve task scheduling in hard real-time systems. </title> <booktitle> In IEEE Workshop on Architecture Supports for Real-Time Systems, </booktitle> <pages> pages 81-85, </pages> <year> 1991. </year>
Reference-contexts: However, no general method was provided to analyze the call graph of a program and the control flow within each function. Lin and Liou suggested that more frequently executed tasks be placed entirely in cache and other tasks be denied any cache access <ref> [44] </ref>. While this approach may have some benefit for a few tasks, the performance of the remaining tasks will be significantly decreased. <p> This technique is being extended to model the MicroSPARC I processor. 8.7 Conclusion Predicting the worst-case execution time of a program on a processor that uses cache memory has long been considered an intractable problem <ref> [60, 44, 43] </ref>. However, this work shows that tight estimations in the presence of instruction caches are feasible, using the fact that the addresses of the instructions within a program and the possible control-flow paths between these instructions are known statically.
Reference: [45] <author> S. McFarling. </author> <title> Program optimization for instruction caches. </title> <booktitle> In Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 183-191, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: This work shows that better results for the timing prediction can be achieved by using instruction caches, with or without architectural modifications. 50 Other suggested architectural modifications often dedicate a bit in the instruction en-coding that is used by the compiler to affect the cache behavior. McFarling <ref> [45] </ref> used such an approach to exclude instructions from cache that were not likely to be in cache on subsequent references.
Reference: [46] <author> F. Mueller and D. B. Whalley. </author> <title> Efficient on-the-fly analysis of program behavior and static cache simulation. </title> <booktitle> In Static Analysis Symposium, </booktitle> <month> September </month> <year> 1994. </year> <note> (accepted). </note>
Reference-contexts: The analysis is performed to find a small set of measurement points suitable for on-the-fly analysis. The analysis provides a general framework to reduce the overhead of event-ordered profiling and tracing during program execution. Excerpts of this chapter can be found in <ref> [46] </ref>. This chapter precedes the central part of the dissertation, static cache simulation. In this chapter, the terms of a unique path, a unique path partitioning, and the function-instance graph are defined. These terms are used throughout this dissertation. <p> It utilizes control-flow partitioning and function-instance graphs for predicting the caching behavior of each instruction. No prior work on predicting caching behavior statically could be found in the literature. Excerpts of this chapter can be found in <ref> [46, 50] </ref>. 4.1 Introduction In the last chapter, a framework for efficient on-the-fly analysis was developed. One application for on-the-fly program analysis is cache performance evaluation. Different cache configurations can be evaluated by determining the number of cache hits and misses for a set of programs.
Reference: [47] <author> F. Mueller and D. B. Whalley. </author> <title> Fast instruction cache analysis via static cache simulation. </title> <type> TR 94-042, </type> <institution> Dept. of CS, Florida State University, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: It will be shown in later chapters that this new method speeds up cache analysis over conventional trace-driven methods by an order of a magnitude. Excerpts of this chapter can be found in <ref> [47] </ref>. 5.1 Introduction Statistical sampling methods are often employed by profiling tools such as prof [65] or gprof [25, 26]. Yet, these tools only provide approximate measurements. On the other hand, code instrumentation results in accurate profiling measurements. <p> Thus, cache analysis with static cache simulation makes it possible to analyze the instruction cache behavior of longer and more realistic program executions. Excerpts of this chapter can be found in <ref> [47] </ref>. 6.1 Introduction The method for instruction cache analysis discussed in this chapter uses static cache simulation to statically predict the cache behavior of a large number of instruction references. The method also uses the techniques for code instrumentation described in the last chapter.
Reference: [48] <author> F. Mueller and D. B. Whalley. </author> <title> On debugging real-time applications. </title> <booktitle> In ACM SIGPLAN Workshop on Language, Compiler, and Tool Support for Real-Time Systems, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: This facilitates hand-tuning of selected tasks to make a schedule feasible. Excerpts of this chapter can be found in <ref> [48] </ref>. 9.1 Introduction The issue of debugging real-time applications has received little attention in the past. Yet, in the process of building real-time applications, debugging is commonly performed just as often as in the development of non-real-time software and may account for up to 50% of the development time [66].
Reference: [49] <author> F. Mueller and D. B. Whalley. </author> <title> Real-time debugging by minimal hardware simulation. </title> <booktitle> In PEARL Workshop uber Realzeitsysteme, </booktitle> <month> December </month> <year> 1994. </year> <note> (accepted). </note>
Reference-contexts: This reduces the overhead considerably. 9.6 Future Work The work is currently being extended to take the effect of pipeline stalls and other machine-specific characteristics into account. The goal is to provide a debugging framework via minimal hardware simulation for the MicroSPARC I processor <ref> [49] </ref>. The work could be extended to take external events into account. The user will be required to specify the occurrence of events in a time table. The events are then simulated by the debugging environment based on the elapsed time.
Reference: [50] <author> F. Mueller, D. B. Whalley, and M. Harmon. </author> <title> Predicting instruction cache behavior. </title> <booktitle> In ACM SIGPLAN Workshop on Language, Compiler, and Tool Support for Real-Time Systems, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: It utilizes control-flow partitioning and function-instance graphs for predicting the caching behavior of each instruction. No prior work on predicting caching behavior statically could be found in the literature. Excerpts of this chapter can be found in <ref> [46, 50] </ref>. 4.1 Introduction In the last chapter, a framework for efficient on-the-fly analysis was developed. One application for on-the-fly program analysis is cache performance evaluation. Different cache configurations can be evaluated by determining the number of cache hits and misses for a set of programs. <p> Excerpts of this chapter can be found in <ref> [50] </ref>. 7.1 Introduction Predicting the execution time of programs or code segments is a difficult task. Yet, in the context of hard real-time systems, it is essential to provide a schedule for tasks with known deadlines. <p> It can be quite a challenge to perform order-dependent events efficiently. This chapter describes the design and implementation of such an environment within the framework of a compiler, a static cache simulator <ref> [50] </ref>, and an arbitrary source-level debugger. The compiler translates a program into assembly code and provides control-flow information to the static cache simulator. The static cache simulator analyzes the caching behavior of the program and produces instrumentation code that is merged into the assembly code. <p> It includes a modified compiler front-end of VPCC (very portable C compiler) [18] and a modified back-end of VPO (very portable optimizer) [8], the static simulator for direct-mapped caches <ref> [50] </ref>, and the regular system linker and source level debugger dbx under SunOS 4.1.3. Calling a library routine to query the elapsed time takes a negligible amount of time in the order of one millisecond. Thus, this section focuses on measuring the overhead of cache simulation during program execution.
Reference: [51] <author> Frank Mueller. </author> <title> A library implementation of POSIX threads under UNIX. </title> <booktitle> In Proceedings of the USENIX Conference, </booktitle> <pages> pages 29-41, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Traditional trace-driven cache simulation (for each basic block) is reported to slow down the execution time by over one to three orders of a magnitude [68]. 2 A multi-threaded real-time kernel has been designed for such an embedded system based on a SPARC VME bus board <ref> [6, 51] </ref>. 74 9.7 Conclusion This work discusses some challenges of real-time debugging that have not yet been addressed adequately. A debugging environment is proposed that addresses the problem of time distortion during debugging.
Reference: [52] <author> D. Niehaus. </author> <title> Program representation and translation for predictable real-time systems. </title> <booktitle> In IEEE Symposium on Real-Time Systems, </booktitle> <pages> pages 53-63, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: Thus, his method cannot capture the impact of caching on execution time. For real-time systems, several tools to predict the execution time of programs have been designed. The analysis has been performed at the level of source code [54], at the level of intermediate code <ref> [52] </ref>, and at the level of machine code [28]. Only Harmon's tool took the impact of instruction caches into account for restrictive circumstances, i.e. only for small code segments that entirely fit into cache. <p> Finally, future work and conclusions are presented. 8.2 Related Work As already mentioned in Chapter 7, several tools to predict the execution time of programs have been designed for real-time systems. The analysis has been performed at the level of source code [54], intermediate code <ref> [52] </ref>, and machine code [28]. Only the last tool attempted to estimate the effect of instruction caching and was only able to analyze small code segments that contained no function calls and entirely fit into cache.
Reference: [53] <author> D. Niehaus, E. Nahum, and J. A. Stankovic. </author> <title> Predictable real-time caching in the spring system. </title> <booktitle> In IEEE Workshop on Real-Time Operating Systems and Software, </booktitle> <pages> pages 80-87, </pages> <year> 1991. </year>
Reference-contexts: Only Harmon's tool took the impact of instruction caches into account for restrictive circumstances, i.e. only for small code segments that entirely fit into cache. Niehaus outlined how the effects of caching can be taken into account in the prediction of execution time <ref> [53] </ref>. He suggested that caches be flushed on context switches to provide a consistent cache state at the beginning of each task execution. <p> Thus, this tool was able to assume that at most one miss will occur for each reference. Niehaus outlined how the effects of caching on execution time can be estimated <ref> [53] </ref>, as discussed in the last chapter. However, no general method was provided to analyze the call graph of a program and the control flow within each function.
Reference: [54] <author> C. Y. Park. </author> <title> Predicting program execution times by analyzing static and dynamic program paths. </title> <booktitle> Real-Time Systems, </booktitle> <volume> 5(1) </volume> <pages> 31-61, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: Thus, his method cannot capture the impact of caching on execution time. For real-time systems, several tools to predict the execution time of programs have been designed. The analysis has been performed at the level of source code <ref> [54] </ref>, at the level of intermediate code [52], and at the level of machine code [28]. Only Harmon's tool took the impact of instruction caches into account for restrictive circumstances, i.e. only for small code segments that entirely fit into cache. <p> Finally, future work and conclusions are presented. 8.2 Related Work As already mentioned in Chapter 7, several tools to predict the execution time of programs have been designed for real-time systems. The analysis has been performed at the level of source code <ref> [54] </ref>, intermediate code [52], and machine code [28]. Only the last tool attempted to estimate the effect of instruction caching and was only able to analyze small code segments that contained no function calls and entirely fit into cache.
Reference: [55] <author> B. L. Peuto and L. J. Shustek. </author> <title> An instruction timing model of CPU performance. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <pages> pages 165-178, </pages> <month> March </month> <year> 1977. </year>
Reference-contexts: Such an approach can easily slow the execution by a factor of a 1000 or more <ref> [55, 70, 33] </ref>. A technique called inline tracing can be used to generate the trace of addresses with much less overhead than trapping or simulation. Measurement instructions are inserted in the program to record the addresses that are referenced during the execution in a buffer.
Reference: [56] <author> A. Poursepanj. </author> <title> The PowerPC performance modeling methodology. </title> <journal> Communications of the ACM, </journal> <volume> 37(6) </volume> <pages> 47-55, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Yet, if the interrupt handler is used to collect and record trace data (see below), the order of events can be reconstructed <ref> [56] </ref>. * Tracing: This method involves the generation of a partial or full sequence of the instruction and data references encountered during program execution. Trace data is generated during program execution but analyzed at a later point in time. Thus, the trace data is commonly stored in a file. <p> The interrupt handler can be used to gather the trace data. This technique is just slightly faster the hardware simulation (100x - 1000x slow down) and works only for existing architectures [70, 21], though sometimes traces from an existing architecture are used to project the speed of prototyped architectures <ref> [56] </ref>. Inline Tracing is a technique where the program is instrumented before execution such that the trace data is generated by the instrumentation code as a side effect of the program execution.
Reference: [57] <author> P. Rowe and B. Pagurek. Remedy: </author> <title> A real-time, multiprocessor, system level debugger. </title> <booktitle> In IEEE Symposium on Real-Time Systems, </booktitle> <pages> pages 230-239, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: The amount of work in the area of 69 real-time debugging has been limited with a few exceptions. The Remedy debugging tool <ref> [57] </ref> addresses the customization of the debugging interface for real-time purposes and synchronizes on breakpoints by suspending the execution on all processors. DCT [9] is a tool that allows practically non-intrusive monitoring but requires special hardware for bus access and does not extend to non-intrusive debugging.
Reference: [58] <author> A. D. </author> <title> Samples. Profile-Driven Compilation. </title> <type> PhD thesis, </type> <institution> University of California at Berkley, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: It shall be noted that placing measurement code on an edge may involve the creation of new basic blocks and unconditional jumps. Samples <ref> [58] </ref> challenges the claim that the maximum spanning tree approach is optimal. He argues that the overhead of control-flow transformations should be taken into account.
Reference: [59] <author> V. Sarkar. </author> <title> Determining average program execution times and their variance. </title> <booktitle> In ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 298-312, </pages> <month> June </month> <year> 1989. </year> <month> 92 </month>
Reference-contexts: Furthermore, a quantitative analysis of both static cache simulation and the bit-encoding approach is provided. Finally, future work and conclusions are presented. 7.2 Related Work The problem of determining the execution time of programs has been the subject of some research in the past. Sarkar <ref> [59] </ref> suggested a framework to determine both average execution time and its variance. His work was based on the analysis of a program's interval structure and its forward control flow.
Reference: [60] <author> D. Simpson. </author> <title> Real-time RISCS. </title> <booktitle> Systems Integration, </booktitle> <pages> pages 35-38, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: cache analysis via static cache simulation is a general method to quickly obtain accurate measurements. 48 Chapter 7 Predicting Instruction Cache Behavior It has been claimed that the execution time of a program can often be predicted more accurately on an uncached system than on a system with cache memory <ref> [27, 60, 43] </ref>. Thus, caches are often disabled for critical real-time tasks to ensure the predictability required for scheduling analysis. This work shows that instruction caching can be exploited to gain execution speed without sacrificing predictability. <p> In the context of real-time systems, caches have been regarded as a source of unpredictability, which conflicts with the goal of making the execution of tasks deterministic <ref> [60] </ref>. For a system with an instruction cache as a primary (on-chip) cache, the execution time of an instruction can vary greatly depending on whether the given instruction is in cache or not. <p> In addition, context switches and interrupts may replace the instructions cached by one task with instructions from another task or an interrupt handler. As a result, it has been common practice to simply disable the cache for sections of code when predictability was required <ref> [60] </ref>. This work shows that it is possible to predict some cache behavior with certain restrictions. Let a task be the portion of code executed between two scheduling points (context 49 switches). When a task starts execution, the cache memory is assumed to be invalidated. <p> Whether or not a particular reference is in cache depends on the program's previous dynamic behavior (i.e. the history of its previous references to the cache). As a result, it has been common practice to simply disable the cache for sections of code where predictability is required <ref> [60] </ref>. Unfortunately, even the use of other architectural features, such as a prefetch buffer, cannot approach the effectiveness of using a cache. Furthermore, as processor speeds continues to increase faster than the speed of accessing memory, the performance advantage of using cache memory becomes more significant. <p> This technique is being extended to model the MicroSPARC I processor. 8.7 Conclusion Predicting the worst-case execution time of a program on a processor that uses cache memory has long been considered an intractable problem <ref> [60, 44, 43] </ref>. However, this work shows that tight estimations in the presence of instruction caches are feasible, using the fact that the addresses of the instructions within a program and the possible control-flow paths between these instructions are known statically.
Reference: [61] <author> K. So, F. Darema, D. A. George, V. A. Norton, and G. F. Pfister. </author> <title> PSIMUL a system for parallel execution of parallel programs. </title> <booktitle> Performance Evaluation of Supercomputers, </booktitle> <pages> pages 187-213, </pages> <year> 1988. </year>
Reference-contexts: Hardware Simulation of the execution of a program can be used to generate the trace for prototyped architectures. This technique is known to be very slow (100x to over 1000x slower than the original execution) but provides accurate and very detailed measurements <ref> [61] </ref>. - Single-Stepping is a processor mode that interrupts the execution of a program after each instruction. The interrupt handler can be used to gather the trace data.
Reference: [62] <author> C. Stunkel and W. Fuchs. Trapeds: </author> <title> Producing traces for multicomputers via execution driven simulation. </title> <booktitle> In SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 70-78, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: A minimal set of instrumentation points can be determined by analyzing the control-flow graph [7]. A number of different inline tracing techniques have been used to instrument the code and to process the trace data. Stunkel and Fuchs <ref> [62] </ref> instrumented the code during compilation and analyzed the trace on-the-fly as part of the program execution. Eggers et. al. [22] instrumented code during compilation and saved the trace data in secondary storage for later analysis. <p> This static analysis depends on the measurements requested by the user, i.e. cache performance analysis requires a specific static analysis and instrumentation code for this purpose. Several variations on the generation of trace data are possible. Stunkel and Fuchs <ref> [62] </ref> generated a full trace, Whalley [68, 69] only generated a partial trace for some highly 13 tuned methods, and this dissertation discusses a method that does not generate any address trace at all during program execution. <p> On-the-fly analysis requires that the program be instrumented with code, which performs the analysis. Many applications, including cache simulation, require that all events are simulated in the order in which they occur. In the past, each basic block was instrumented with code to support event-ordered analysis <ref> [62] </ref>. Inserting code based on the maximum 15 spanning tree (or, to be more precise, on its complement) does not cover all events since instrumentation points are placed on a subset of the control-flow graph. It is therefore not applicable to on-the-fly analysis.
Reference: [63] <author> C. B. Stunkel. </author> <title> Address tracing for parallel machines. </title> <journal> IEEE Computer, </journal> <volume> 24(1) </volume> <pages> 31-38, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: It is therefore important to include only the (partial) information that is essential to reconstruct a full trace after program execution. In the following, different tracing techniques are described and their overhead is reported based on previous work <ref> [63] </ref>. Hardware Simulation of the execution of a program can be used to generate the trace for prototyped architectures.
Reference: [64] <author> Sun Microsystems, Inc. </author> <title> Programmer's Language Guide, </title> <month> March </month> <year> 1990. </year> <title> Part No. </title> <publisher> 800-3844-10. </publisher>
Reference-contexts: A quantitative analysis of the effect of these issues will be given in the measurement section. 9.4 Application of the Debugging Tool The output shown in Figure 9.2 illustrates a short debugging session of a program perform ing fast Fourier transformations within the environment using the unmodified source-level debugger dbx <ref> [64] </ref>. &gt; dbx fft Reading symbolic information...
Reference: [65] <author> SunSoft, Inc. </author> <title> SunOS 5.1 User Commands, </title> <month> March </month> <year> 1992. </year>
Reference-contexts: This approach provides a statistical sampling method that yields approximate measurements indicating the estimated portion of the total execution time spent in certain portions of the program 11 (typically at the scope of functions). To perform the sampling, common tools, such as prof <ref> [65] </ref> and gprof [25, 26], rely on the availability of hardware timers and the corresponding operating system interface to activate these timers, catch the timer interrupt, and sample the program counter at the interrupt point. <p> It will be shown in later chapters that this new method speeds up cache analysis over conventional trace-driven methods by an order of a magnitude. Excerpts of this chapter can be found in [47]. 5.1 Introduction Statistical sampling methods are often employed by profiling tools such as prof <ref> [65] </ref> or gprof [25, 26]. Yet, these tools only provide approximate measurements. On the other hand, code instrumentation results in accurate profiling measurements. For example, instruction frequency measurements can be obtained by inserting instructions that increment frequency counters into a program.
Reference: [66] <author> M. Timmerman, F. Gielen, and P. Lambix. </author> <title> A knowledge-based approach for the debugging of real-time multiprocessor systems. </title> <booktitle> In IEEE Workshop on Real-Time Applications, </booktitle> <pages> pages 23-28, </pages> <year> 1993. </year>
Reference-contexts: Yet, in the process of building real-time applications, debugging is commonly performed just as often as in the development of non-real-time software and may account for up to 50% of the development time <ref> [66] </ref>. The debugging tools used for real-time applications are often ordinary debuggers that do not cater to specific needs of real-time systems listed below. Time distortion: The notion of real time is central to real-time applications. <p> The instrumentation code is a permanent part of the application. It will never be removed to prevent alteration of the timing. Debugging is limited to forced suspension and resumption of entities, viewing and alteration of variables, and monitoring of communication messages. The DARTS system <ref> [66] </ref> approaches the debugging problem in two stages. It first generates a program trace and then allows for debugging based on the trace data that is time-stamped to address the time distortion problem. The debugging is limited to a restricted set of events that is extracted from the control flow.
Reference: [67] <author> H. Tokuda, M. Kotera, and C. W. Mercer. </author> <title> A real-time monitor for a distributed real-time operating system. </title> <booktitle> In ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pages 68-77, </pages> <year> 1988. </year>
Reference-contexts: DCT [9] is a tool that allows practically non-intrusive monitoring but requires special hardware for bus access and does not extend to non-intrusive debugging. Both RED [30] and ART <ref> [67] </ref> provide monitoring and debugging facilities at the price of software instrumentation. RED dedicates a co-processor to collect trace data and send it to the host system. The instrumentation is removed for production code. In ART, a special reporting task sends trace data to a host system for further processing.
Reference: [68] <author> D. B. Whalley. </author> <title> Fast instruction cache performance evaluation using compile-time analysis. </title> <booktitle> In SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 13-22, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: This static analysis depends on the measurements requested by the user, i.e. cache performance analysis requires a specific static analysis and instrumentation code for this purpose. Several variations on the generation of trace data are possible. Stunkel and Fuchs [62] generated a full trace, Whalley <ref> [68, 69] </ref> only generated a partial trace for some highly 13 tuned methods, and this dissertation discusses a method that does not generate any address trace at all during program execution. <p> Some of the references have to be reconstructed to determine the full trace when only a partial trace is generated for a minimal set of instrumentation points. The on-the-fly analysis techniques described by Whalley <ref> [68, 69] </ref> do not require the interpretation of each reference. Consecutive references are passed to the simulator as one block and only the first reference of a program line is simulated. <p> Recently, tracing and analyzing programs has been combined using inline tracing [10] and on-the-fly analysis <ref> [68, 69] </ref>. Both techniques require that events are analyzed as they occur. Traditional inline tracing performs the analysis separate from the generation of trace information. On-the-fly analysis integrates the program analysis into its execution. <p> The program analysis may be performed concurrently on a buffered address trace to reduce the storage requirements to temporary trace buffers. Borg et. al [10] and Eggers et. al. [22] used this technique to obtain accurate measurements for the simulation of instruction and data caches. Whalley <ref> [68, 69] </ref> used on-the-fly analysis where a cache simulation function was called during program execution for each basic block. <p> They showed that the placements are optimal for a large class of graphs. The overhead for the trace generation was less than a factor of 5. However, the postprocessing pass to regenerate the full trace required 19-60 times of the normal execution time. Whalley <ref> [68, 69] </ref> evaluated a set of on-the-fly analysis techniques to reduce the time required to evaluate instruction cache performance. He linked a cache simulator to the programs, which were instrumented with measurement code to evaluate the instruction cache performance during the program's execution. <p> Traditional trace-driven cache simulation (for each basic block) is reported to slow down the execution time by over one to three orders of a magnitude <ref> [68] </ref>. 2 A multi-threaded real-time kernel has been designed for such an embedded system based on a SPARC VME bus board [6, 51]. 74 9.7 Conclusion This work discusses some challenges of real-time debugging that have not yet been addressed adequately.
Reference: [69] <author> D. B. Whalley. </author> <title> Techniques for fast instruction cache performance evaluation. </title> <journal> Software Practice & Experience, </journal> <volume> 19(1) </volume> <pages> 195-203, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: This static analysis depends on the measurements requested by the user, i.e. cache performance analysis requires a specific static analysis and instrumentation code for this purpose. Several variations on the generation of trace data are possible. Stunkel and Fuchs [62] generated a full trace, Whalley <ref> [68, 69] </ref> only generated a partial trace for some highly 13 tuned methods, and this dissertation discusses a method that does not generate any address trace at all during program execution. <p> Some of the references have to be reconstructed to determine the full trace when only a partial trace is generated for a minimal set of instrumentation points. The on-the-fly analysis techniques described by Whalley <ref> [68, 69] </ref> do not require the interpretation of each reference. Consecutive references are passed to the simulator as one block and only the first reference of a program line is simulated. <p> Recently, tracing and analyzing programs has been combined using inline tracing [10] and on-the-fly analysis <ref> [68, 69] </ref>. Both techniques require that events are analyzed as they occur. Traditional inline tracing performs the analysis separate from the generation of trace information. On-the-fly analysis integrates the program analysis into its execution. <p> Different cache configurations can be evaluated by determining the number of cache hits and misses for a set of programs. Cache analysis can be performed on-the-fly or by analyzing stored trace data, though faster results have been reported for the former approach <ref> [69] </ref>. This chapter introduces the method of static cache simulation, which predicts the caching behavior of a large number of instruction references prior to execution time 1 . The method employs a novel view of cache memories that seems to be unprecedented. <p> The program analysis may be performed concurrently on a buffered address trace to reduce the storage requirements to temporary trace buffers. Borg et. al [10] and Eggers et. al. [22] used this technique to obtain accurate measurements for the simulation of instruction and data caches. Whalley <ref> [68, 69] </ref> used on-the-fly analysis where a cache simulation function was called during program execution for each basic block. <p> They showed that the placements are optimal for a large class of graphs. The overhead for the trace generation was less than a factor of 5. However, the postprocessing pass to regenerate the full trace required 19-60 times of the normal execution time. Whalley <ref> [68, 69] </ref> evaluated a set of on-the-fly analysis techniques to reduce the time required to evaluate instruction cache performance. He linked a cache simulator to the programs, which were instrumented with measurement code to evaluate the instruction cache performance during the program's execution. <p> Consequently, less instrumentation code to update conflicting SPSs is needed. Finally, for large caches, hardly any conflicts remain. Thus, the cache simulation at execution time can be reduced to simple frequency counting, which imposes a much lower 1 We used a traditional trace-driven method similar to "Technique B" in <ref> [69] </ref> but the new method was probably finer tuned. 2 Sometimes, the instrumented code ran faster than the uninstrumented program, i.e. the ratio was smaller than 1. These results were reproducible. <p> Even the best results published in <ref> [69] </ref> required an overhead factor of 4-15 over uninstrumented code for hit ratios between 96% and 99%. This highly tuned traditional method required a recompilation pass for better instrumentation.
Reference: [70] <author> C. A. Wiecek. </author> <title> A case study of VAX-11 instruction set usage for compiler execution. </title> <booktitle> In Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 177-184, </pages> <month> March </month> <year> 1982. </year> <month> 93 </month>
Reference-contexts: The interrupt handler can be used to gather the trace data. This technique is just slightly faster the hardware simulation (100x - 1000x slow down) and works only for existing architectures <ref> [70, 21] </ref>, though sometimes traces from an existing architecture are used to project the speed of prototyped architectures [56]. Inline Tracing is a technique where the program is instrumented before execution such that the trace data is generated by the instrumentation code as a side effect of the program execution. <p> Such an approach can easily slow the execution by a factor of a 1000 or more <ref> [55, 70, 33] </ref>. A technique called inline tracing can be used to generate the trace of addresses with much less overhead than trapping or simulation. Measurement instructions are inserted in the program to record the addresses that are referenced during the execution in a buffer.
References-found: 70

