URL: http://www.cs.princeton.edu/~dubnicki/papers/lncs.ps
Refering-URL: http://www.cs.princeton.edu/~dubnicki/pub.html
Root-URL: http://www.cs.princeton.edu
Title: Network Interface Support for User-Level Buffer Management  
Author: Cezary Dubnicki, Kai Li, Malena Mesarina 
Address: Princeton NJ 08544  
Affiliation: Department of Computer Science, Princeton University,  
Abstract: The network interfaces of existing multicomputers and workstations require a significant amount of software overhead to provide protection and buffer management in order to implement message-passing protocols. This paper advocates a physical memory mapping method in a network interface design that supports user-level buffer management. The method requires only a minimal addition to the traditional DMA-based network interface design and eliminates the need for memory buffer management in the operating system kernel.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Matthias A. Blumrich, Kai Li, Richard Alpert, Cezary Dubnicki, Edward W. Felten, and Jonathan S. Sandberg. </author> <title> Virtual memory mapped network interface for the shrimp multicomputer. </title> <booktitle> In Proceedings of the 21st ISCA, </booktitle> <pages> pages 142-153, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Felten found out that using compiled, application-tailored runtime library for message passing, the latency can be improved by 30% [4]. With a non-traditional design of network interface, the software overhead of message passing primitives for common cases can be reduced to less than 10 instructions <ref> [1] </ref> without sacrificing protection. An interesting question is whether it is possible to support user-level buffer management with very minimal modifications to the traditional DMA-based network interface designs. <p> This simple addition makes our network interface very flexible. It supports both the traditional DMA-based message passing and virtual memory-mapped communication with or without interrupting the receiving processor. Table 1 shows the overhead components of message passing on three kinds of network interfaces: traditional, SHRIMP-I, and SHRIMP-II <ref> [1] </ref>. The SHRIMP-II network interface implements virtual memory mapping translation in hardware so that the send operation can be performed at user level. SHRIMP-I network interface requires a system call to send a message, but it requires very little addition to the traditional network interface design. <p> The cost of mapping in the SHRIMP-I case is similar to that of passing a small message using csend and crecv of NX/2. For applications that have static communication patterns <ref> [1] </ref>, the amortized overhead of creating a mapping can be negligible. We would like to point out that the semantics of csend/crecv primitives of NX/2 is richer than the virtual memory-mapped communication supported by the SHRIMP-I interface. Our comparison shows that rich semantics comes with the substantial overhead. <p> We have built a simulator for software development. We expect the SHRIMP-I network interface to be operational in the summer of 1994. We are also designing a virtual memory-mapped network interface (SHRIMP-II) that allows send operations to perform entirely in user mode <ref> [1] </ref>. The design of SHRIMP-II network interface is however more complicated than SHRIMP-I. Acknowledgements We would like to thank Douglas W. Clark for his help in the evaluation of SHRIMP-I design. We are also grateful to Ed Felten for his helpful comments on the organization of this paper.
Reference: 2. <author> Shekhar Borkar, Robert Cohn, George Cox, Thomas Gross, H.T.Kung, Monica Lam, Margie Levine, Brian Moore, Wire Moore, Craig Peterson, Jim Susman, Jim Sutton, John Urbanski, and Jon Webb. </author> <title> Supporting systolic and memory communication in iWarp. </title> <booktitle> In Proceedings of the 17th ISCA, </booktitle> <pages> pages 70-81, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In addition, the node is complex and expensive to build. Several projects have taken the approach of lowering communication latency by bringing the network all the way into the processor and mapping the network interface FIFOs to special processor registers <ref> [2, 6, 3] </ref>. Writing and reading these registers queues and dequeues data from the FIFOs respectively. While this is efficient for fine-grain, low-latency communication, it requires the use of a nonstandard CPU, and it does not support the protection of multiple contexts in a multiprogramming environment.
Reference: 3. <author> William J. Dally, Roy Davison, J. A. Stuart Fiske, Greg Fyler, John S. Keen, Richard A. Lethin, Michael Noakes, and Peter R. Nuth. </author> <title> The message-driven processor: A multicomputer processing node with efficient mechanisms. </title> <journal> IEEE Micro, </journal> <volume> 12(2) </volume> <pages> 23-39, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: In addition, the node is complex and expensive to build. Several projects have taken the approach of lowering communication latency by bringing the network all the way into the processor and mapping the network interface FIFOs to special processor registers <ref> [2, 6, 3] </ref>. Writing and reading these registers queues and dequeues data from the FIFOs respectively. While this is efficient for fine-grain, low-latency communication, it requires the use of a nonstandard CPU, and it does not support the protection of multiple contexts in a multiprogramming environment.
Reference: 4. <author> Edward W. Felten. </author> <title> Protocol Compilation: High-Performance Communication for Parallel Programs. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science and Engineering, University of Washington, </institution> <month> August </month> <year> 1993. </year> <note> Available as technical report 93-09-09. </note>
Reference-contexts: Recent studies and analysis indicate that moving communication buffer management out of the kernel to the user level can greatly reduce the software overhead of message passing. Felten found out that using compiled, application-tailored runtime library for message passing, the latency can be improved by 30% <ref> [4] </ref>. With a non-traditional design of network interface, the software overhead of message passing primitives for common cases can be reduced to less than 10 instructions [1] without sacrificing protection.
Reference: 5. <institution> FORE Systems. TCA-100 TURBOchannel ATM Computer Interface, </institution> <note> User's Manual, </note> <year> 1992. </year>
Reference-contexts: While this is efficient for fine-grain, low-latency communication, it requires the use of a nonstandard CPU, and it does not support the protection of multiple contexts in a multiprogramming environment. An alternative network interface design approach employs memory-mapped network interface FIFOs <ref> [10, 5] </ref>. In this scheme, the controller has no DMA capability, instead the host processor communicates with the network interface by reading or writing special memory locations that correspond to the FIFOs. This approach results in good latency for short messages.
Reference: 6. <author> Dana S. Henry and Christopher F. Joerg. </author> <title> A tightly-coupled processor-network interface. </title> <booktitle> In Proceedings of 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 111-122, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: In addition, the node is complex and expensive to build. Several projects have taken the approach of lowering communication latency by bringing the network all the way into the processor and mapping the network interface FIFOs to special processor registers <ref> [2, 6, 3] </ref>. Writing and reading these registers queues and dequeues data from the FIFOs respectively. While this is efficient for fine-grain, low-latency communication, it requires the use of a nonstandard CPU, and it does not support the protection of multiple contexts in a multiprogramming environment.
Reference: 7. <author> Mark Homewood and Moray McLaren. </author> <title> Meiko CS-2 interconnect elan elite design. </title> <booktitle> In Proceedings of Hot Interconnects '93 Symposium, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: One solution to the problem of software overhead is to add a separate processor on every node just for message passing [12, 8]. Recent examples of this approach are the Intel Paragon [9] and Meiko CS-2 <ref> [7] </ref>. The basic idea is for the "compute" processor to communicate with the "message" processor through either mailboxes in shared memory or closely-coupled datapaths. The compute and message processors can then work in parallel, to overlap communication and computation.
Reference: 8. <author> Jiun-Ming Hsu and Prithviraj Banerjee. </author> <title> A message passing coprocessor for distributed memory multicomputers. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <pages> pages 720-729, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: The main disadvantage of traditional network interface is that message passing costs are usually thousands of CPU cycles. One solution to the problem of software overhead is to add a separate processor on every node just for message passing <ref> [12, 8] </ref>. Recent examples of this approach are the Intel Paragon [9] and Meiko CS-2 [7]. The basic idea is for the "compute" processor to communicate with the "message" processor through either mailboxes in shared memory or closely-coupled datapaths.
Reference: 9. <author> Intel Corporation. </author> <title> Paragon XP/S Product Overview, </title> <booktitle> 1991. </booktitle> <pages> 10 </pages>
Reference-contexts: One solution to the problem of software overhead is to add a separate processor on every node just for message passing [12, 8]. Recent examples of this approach are the Intel Paragon <ref> [9] </ref> and Meiko CS-2 [7]. The basic idea is for the "compute" processor to communicate with the "message" processor through either mailboxes in shared memory or closely-coupled datapaths. The compute and message processors can then work in parallel, to overlap communication and computation.
Reference: 10. <author> Charles E. Leiserson, Zahi S. Abuhamdeh, David C. Douglas, Carl R. Feynman, Mahesh N. Ganmukhi, Jeffrey V. Hill, Daniel Hillis, Bradley C. Kuszmaul, Mar-garet A. St. Pierre, David S. Wells, Monica C. Wong, Shaw-Wen Yang, and Robert Zak. </author> <title> The network architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of 4th ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 272-285, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: While this is efficient for fine-grain, low-latency communication, it requires the use of a nonstandard CPU, and it does not support the protection of multiple contexts in a multiprogramming environment. An alternative network interface design approach employs memory-mapped network interface FIFOs <ref> [10, 5] </ref>. In this scheme, the controller has no DMA capability, instead the host processor communicates with the network interface by reading or writing special memory locations that correspond to the FIFOs. This approach results in good latency for short messages.
Reference: 11. <author> Richard J. Littlefield. </author> <title> Characterizing and tuning communications performance for real applications. </title> <booktitle> In Proceedings of the First Intel DELTA Applications Workshop, </booktitle> <pages> pages 179-190, </pages> <month> February </month> <year> 1992. </year> <note> Proceedings also available as Caltech Technical Report CCSF-14-92. </note>
Reference-contexts: For example, on the Intel DELTA multicomputer, sending and receiving a message requires 67 sec, of which less than 1 sec is due to time on the wire <ref> [11] </ref>. Recent studies and analysis indicate that moving communication buffer management out of the kernel to the user level can greatly reduce the software overhead of message passing. Felten found out that using compiled, application-tailored runtime library for message passing, the latency can be improved by 30% [4].
Reference: 12. <author> R.S. Nikhil, G.M. Papadopoulos, and Arvind. </author> <title> *T: A multithreaded massively parallel architecture. </title> <booktitle> In Proceedings of the 19th ISCA, </booktitle> <pages> pages 156-167, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The main disadvantage of traditional network interface is that message passing costs are usually thousands of CPU cycles. One solution to the problem of software overhead is to add a separate processor on every node just for message passing <ref> [12, 8] </ref>. Recent examples of this approach are the Intel Paragon [9] and Meiko CS-2 [7]. The basic idea is for the "compute" processor to communicate with the "message" processor through either mailboxes in shared memory or closely-coupled datapaths.
Reference: 13. <author> Steven Nugent. </author> <title> The iPSC/2 direct-connect communication technology. </title> <booktitle> In Proceedings of 3rd Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 51-60, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Since the SHRIMP-I network interface supports both traditional message passing and virtual memory-mapped communication, it allows user programs to optimize their common cases. 6 Related Work The traditional network interface design is based on DMA data transfer. Recent examples include the NCUBE [14], iPSC/2 <ref> [13] </ref> and iPSC/860. In this scheme an application sends messages by making operating system calls to initiate DMA data transfers.
Reference: 14. <author> John Palmer. </author> <title> The NCUBE family of high-performance parallel computer systems. </title> <booktitle> In Proceedings of 3rd Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 845-851, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Since the SHRIMP-I network interface supports both traditional message passing and virtual memory-mapped communication, it allows user programs to optimize their common cases. 6 Related Work The traditional network interface design is based on DMA data transfer. Recent examples include the NCUBE <ref> [14] </ref>, iPSC/2 [13] and iPSC/860. In this scheme an application sends messages by making operating system calls to initiate DMA data transfers.
Reference: 15. <author> Paul Pierce. </author> <title> The NX/2 operating system. </title> <booktitle> In Proceedings of 3rd Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 384-390, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: 1 Introduction The network interfaces of existing multicomputers and workstations require a significant amount of software overhead to implement message-passing protocols. In fact, message-passing primitives such as the NX/2 <ref> [15] </ref> csend/crecv on many multicomputers often execute more than a thousand instructions to send and receive a message; the hardware overhead of data transfer is negligible.
Reference: 16. <author> Roger Traylor and Dave Dunning. </author> <title> Routing chip set for intel paragon parallel supercomputer. </title> <booktitle> In Proceedings of Hot Chips '92 Symposium, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: The network interface uses a NIC (Network Interface Chip) to interface with the processor channels of an iMRC on the routing backplane <ref> [16] </ref>. use memory-mapped registers to hold packet headers and to control send and receive operations. Global registers implement standard EISA interface (id and control registers). There is also registers' base global register which holds the base physical address of memory-mapped registers.
References-found: 16

