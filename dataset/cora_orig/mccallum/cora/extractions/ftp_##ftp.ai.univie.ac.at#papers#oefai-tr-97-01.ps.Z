URL: ftp://ftp.ai.univie.ac.at/papers/oefai-tr-97-01.ps.Z
Refering-URL: http://www.ai.univie.ac.at/cgi-bin/tr-online/?number+97-01
Root-URL: 
Email: E-mail: juffi@ai.univie.ac.at  
Title: More Efficient Windowing  
Author: Johannes Furnkranz 
Web: OEFAI-TR-97-01  
Address: Schottengasse 3, A-1010 Wien, Austria  
Affiliation: Austrian Research Institute for Artificial Intelligence  
Abstract: Windowing has been proposed as a procedure for efficient memory use in the ID3 decision tree learning algorithm. However, previous work has shown that windowing may often lead to a decrease in performance. In this work, we try to argue that separate-and-conquer rule learning algorithms are more appropriate for windowing than divide-and-conquer algorithms, because they learn rules independently and are less susceptible to changes in class distributions. In particular, we will present a new windowing algorithm that achieves additional gains in efficiency by exploiting this property of separate-and-conquer algorithms. While the presented algorithm is only suitable for redundant, noise-free data sets, we will also briefly discuss the problem of noisy data in windowing and present some preliminary ideas how it might be solved with an extension of the algorithm introduced in this paper. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Catlett, J. </author> <year> (1991). </year> <title> Megainduction: A test flight. </title> <editor> In L. Birnbaum and G. Collins (Eds.), </editor> <booktitle> Proceedings of the 8th International Workshop on Machine Learning (ML-91), </booktitle> <address> Evanston, IL, </address> <pages> pp. 596-599. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In noisy domains it can be considerably slower. There has been some evidence that slight variations of the basic windowing procedure like the one employed in C4.5 (Quinlan 1993) can improve the performance of windowing, in particular in noise-free domains <ref> (Catlett 1991) </ref>, but no further empirical study has been devoted to this subject. Furthermore, windowing has been connected to decision tree learning and has not been studied with other learning algorithms. <p> It can be assumed that many more examples have to be added to the window in order to recover the structure that is inherent in the data. This hypothesis is consistent with the results of (Wirth and Catlett 1988) and <ref> (Catlett 1991) </ref>, where it was shown that windowing is highly sensitive to noise.
Reference: <author> Cohen, W. W. </author> <year> (1995). </year> <title> Fast effective rule induction. </title> <editor> In A. Prieditis and S. Russell (Eds.), </editor> <booktitle> Proceedings of the 12th International Conference on Machine Learning (ML-95), </booktitle> <address> Lake Tahoe, CA, </address> <pages> pp. 115-123. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Currently we are thinking about ways to integrate the work presented in this paper with ideas discussed in (Furnkranz and Widmer 1994) and <ref> (Cohen 1995) </ref>, where it was shown that pruning individual rules instead of pruning complete theories is the more adequate procedure for separate-and-conquer rule learning algorithms.
Reference: <author> Fayyad, U., G. Piatetsky-Shapiro, and P. </author> <title> Smyth (1996, Fall). From data mining to knowledge discovery in databases. </title> <journal> AI Magazine 17 (3), </journal> <pages> 37-54. </pages>
Reference-contexts: One reason for this certainly is the rapid development of computer hardware, which made the motivation for windowing seem less compelling. However, recent developments in the areas of Knowledge Discovery in Databases <ref> (Fayyad, Piatetsky-Shapiro, and Smyth 1996) </ref> and Intelligent Information Retrieval (Hearst and Hirsh 1996) have again shown the limits of conventional machine 1 learning algorithms. Dimensionality reduction through subsampling procedures has been recognized as a promising field of research (Lewis and Catlett 1994; Yang 1996).
Reference: <author> Furnkranz, J. </author> <year> (1996). </year> <title> Separate-and-conquer rule learning. </title> <type> Technical Report OEFAI-TR-96-25, </type> <institution> Austrian Research Institute for Artificial Intelligence. </institution>
Reference-contexts: windowing algorithms and present some very vague ideas how the modularity in the separate-and-conquer rule learning algorithm could be exploited for a noise-tolerant version of our algorithm. 2 Separate-and-Conquer Rule Learning We have conducted our study in the framework of separate-and-conquer rule learning algorithms that has recently gained in popularity <ref> (Furnkranz 1996) </ref>. Our basic learning algorithm, DOS, 1 is a simple propositional version of Foil (Quinlan 1990). It employs a top-down hill-climbing search on the information gain heuristic.
Reference: <author> Furnkranz, J. and G. </author> <title> Widmer (1994). Incremental Reduced Error Pruning. </title> <editor> In W. Cohen and H. Hirsh (Eds.), </editor> <booktitle> Proceedings of the 11th International Conference on Machine Learning, </booktitle> <address> New Brunswick, NJ, </address> <pages> pp. 70-77. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Currently we are thinking about ways to integrate the work presented in this paper with ideas discussed in <ref> (Furnkranz and Widmer 1994) </ref> and (Cohen 1995), where it was shown that pruning individual rules instead of pruning complete theories is the more adequate procedure for separate-and-conquer rule learning algorithms.
Reference: <author> Hearst, M. A. and H. Hirsh (Eds.) </author> <year> (1996). </year> <booktitle> Proceedings of the AAAI Spring Symposium on Machine Learning in Information Access. </booktitle> <publisher> AAAI Press. Technical Report SS-96-05. </publisher>
Reference-contexts: One reason for this certainly is the rapid development of computer hardware, which made the motivation for windowing seem less compelling. However, recent developments in the areas of Knowledge Discovery in Databases (Fayyad, Piatetsky-Shapiro, and Smyth 1996) and Intelligent Information Retrieval <ref> (Hearst and Hirsh 1996) </ref> have again shown the limits of conventional machine 1 learning algorithms. Dimensionality reduction through subsampling procedures has been recognized as a promising field of research (Lewis and Catlett 1994; Yang 1996).
Reference: <author> Lewis, D. D. and J. </author> <title> Catlett (1994). Heterogeneous uncertainty sampling for supervised learning. </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning (ML-94). </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Adding all examples that have been misclassified to the current window will again result in training sets containing too high a noise level. Another approach for subsampling in noisy domains might be to use variants of uncertainty sampling <ref> (Lewis and Catlett 1994) </ref>, which do not select the new window on the basis 9 of misclassified examples, but on the basis of the learner's confidence in the learned theory. The examples that are classified with the least confidence will be added to the training set in the next iteration.
Reference: <author> Muggleton, S., M. Bain, J. Hayes-Michie, and D. </author> <title> Michie (1989). An experimental comparison of human and machine learning formalisms. </title> <booktitle> In Proceedings of the 6th International Workshop on Machine Learning (ML-89), </booktitle> <pages> pp. 113-118. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The graphs for the total number of examples processed by DOS, which we had to omit because of space limitations, were quite similar to the run-time graphs in all important aspects. The first domain is a propositional version of the well-known king-rook-king classification task <ref> (Muggleton, Bain, Hayes-Michie, and Michie 1989) </ref>, which is commonly used as a benchmark for relational learning algorithms.
Reference: <author> Quinlan, J. R. </author> <year> (1983). </year> <title> Learning efficient classification procedures and their application to chess end games. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell (Eds.), </editor> <booktitle> Machine Learning. An Artificial Intelligence Approach, </booktitle> <pages> pp. 463-482. </pages> <address> Palo Alto, CA: </address> <publisher> Tioga. </publisher>
Reference-contexts: The gain in efficiency is obtained by identifying an appropriate subset of the given training examples, from which a theory of sufficient quality can be induced. Such procedures are also known as subsampling. Windowing has been proposed as a supplement to the inductive decision tree learner ID3 <ref> (Quinlan 1983) </ref> in order to allow it to tackle tasks which would otherwise have exceeded the memory capacity of the computers of those days. Despite first successful experiments in the KRKN domain (Quinlan 1983) windowing has not played a major role in machine learning research. <p> Windowing has been proposed as a supplement to the inductive decision tree learner ID3 <ref> (Quinlan 1983) </ref> in order to allow it to tackle tasks which would otherwise have exceeded the memory capacity of the computers of those days. Despite first successful experiments in the KRKN domain (Quinlan 1983) windowing has not played a major role in machine learning research. One reason for this certainly is the rapid development of computer hardware, which made the motivation for windowing seem less compelling. <p> However, for reasons discussed later in this paper we think that noise is a fundamental problem in windowing, which cannot be solved by merely using a noise-tolerant learning algorithm. 3 Windowing Windowing has been first introduced in the ID3 decision tree learning algorithm <ref> (Quinlan 1983) </ref> as a procedure for making efficient use of memory limitations, which were much more 1 Dull Old Separate-and-conquer 2 procedure Win-DOS-3.1 (Examples,InitSize,MaxIncSize) Train = RandomSample (Examples,InitSize) Test = Examples n Train repeat Theory = DOS (Train) NewTrain = ; OldTest = ; for Example 2 Test Test = Test
Reference: <author> Quinlan, J. R. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <booktitle> Machine Learning 5, </booktitle> <pages> 239-266. </pages>
Reference-contexts: Our basic learning algorithm, DOS, 1 is a simple propositional version of Foil <ref> (Quinlan 1990) </ref>. It employs a top-down hill-climbing search on the information gain heuristic. The only stopping criteria are completeness and consistency, i.e., rules are specialized until they do not cover any negative examples, and more rules are added to the theory until all positive examples are covered.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In noisy domains it can be considerably slower. There has been some evidence that slight variations of the basic windowing procedure like the one employed in C4.5 <ref> (Quinlan 1993) </ref> can improve the performance of windowing, in particular in noise-free domains (Catlett 1991), but no further empirical study has been devoted to this subject. Furthermore, windowing has been connected to decision tree learning and has not been studied with other learning algorithms. <p> The sensitivity of ID3 to such changes is also confirmed by <ref> (Quinlan 1993) </ref> where it is reported that changing windowing in a way such that the class distribution in the initial window is as uniform as possible (this is the standard procedure in C4.5) produces better results. <p> This finding contradicts the heuristic that is currently employed in C4.5, namely to add at least half of the total misclassified examples. However, this heuristic was formed in order to make windowing more effective in noisy domains <ref> (Quinlan 1993) </ref>, a goal that in our opinion cannot be achieved with merely using a noise-tolerant learner inside the windowing loop for reasons discussed in the next section. 7 The Problem of Noise in Windowing A major effort in future research has to go into incorporating noise handling capabilities into windowing
Reference: <author> Wirth, J. and J. </author> <title> Catlett (1988). Experiments on the costs and benefits of windowing in ID3. </title> <editor> In J. Laird (Ed.), </editor> <booktitle> Proceedings of the 5th International Conference on Machine Learning (ML-88), </booktitle> <address> Ann Arbor, MI, </address> <pages> pp. 87-99. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Dimensionality reduction through subsampling procedures has been recognized as a promising field of research (Lewis and Catlett 1994; Yang 1996). A good deal of the lack of interest in windowing can also be attributed to an empirical study <ref> (Wirth and Catlett 1988) </ref> that showed that windowing is unlikely to gain any efficiency. The authors studied windowing with ID3 in various domains and concluded that windowing cannot be recommended as a procedure for improving efficiency. <p> Win-DOS-95 needs about the same number of iterations, but it rarely keeps more than 150 examples in its window. This domain is particularly interesting, because windowing with the decision tree learner ID3 could not achieve significant run-time gains over pure ID3 in a previous study (figure 2 of <ref> (Wirth and Catlett 1988) </ref>), while the slightly modified version of windowing used in C4.5 is able to achieve a run-time improvement of only about 15% (p. 59 of (Quin-lan 1993)). <p> Different variations of the InitSize parameter have been investigated in <ref> (Wirth and Catlett 1988) </ref> and the results indicate that the algorithm is quite insensitive to this parameter. We consider the parameter MaxIncSize more important, which specifies the maximum number of examples that can be added to a window. <p> different settings of this parameter showed that in the KRK domain the performance of the algorithms in terms of run-time and total number of examples needed for learning is best 3 Note that this example set is only a subset of the Tic-Tac-Toe data set that has been studied in <ref> (Wirth and Catlett 1988) </ref>. We did not have access to the full data set. 8 if this parameter is kept comparably low (10 to 50 examples). In this range, the parameter is relatively insensitive to its exact setting. If more examples are added to the window size, performance degrades. <p> It can be assumed that many more examples have to be added to the window in order to recover the structure that is inherent in the data. This hypothesis is consistent with the results of <ref> (Wirth and Catlett 1988) </ref> and (Catlett 1991), where it was shown that windowing is highly sensitive to noise.
Reference: <author> Yang, Y. </author> <year> (1996). </year> <title> Sampling strategies and learning efficiency in text categorization. </title> <editor> In M. Hearst and H. Hirsh (Eds.), </editor> <booktitle> Proceedings of the AAAI Spring Symposium on Machine Learning in Information Access, </booktitle> <pages> pp. 88-95. </pages> <note> AAAI Press. Technical Report SS-96-05. </note>
References-found: 13

