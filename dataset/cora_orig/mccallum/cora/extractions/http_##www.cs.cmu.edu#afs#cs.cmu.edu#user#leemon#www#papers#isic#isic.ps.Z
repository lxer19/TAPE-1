URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/leemon/www/papers/isic/isic.ps.Z
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/leemon/www/papers/index.html
Root-URL: 
Email: Email: scott.weaver@uc.edu  
Title: An Analytical Framework for Local Feedforward Networks 1  
Author: Scott Weaver ; Leemon Baird Marios Polycarpou 
Note: 2 Wright-Patterson Air Force Base WL/AAAT Bldg 635 2185 Avionics  3 United States Air Force  
Address: 45221-0030  WPAFB, OH 45433-7301  2354 Fairchild Dr. Suite 6K41 USAFA, CO 80840-6234  
Affiliation: 1 Department of Electrical and Computer Engineering University of Cincinnati Cincinnati, Ohio  Circle  Academy HQ USAFA/DFCS  
Abstract: Although feedforward neural networks are well suited to function approximation, in some applications networks experience problems when learning a desired function. One problem is interference which occurs when learning in one area of the input space causes unlearning in another area. Networks that are less susceptible to interference are referred to as spatially local networks. To understand these properties, a theoretical framework, consisting of a measure of interference and a measure of network localization, is developed that incorporates not only the network weights and architecture but also the learning algorithm. Using this framework to analyze sigmoidal multi-layer perceptron (MLP) networks that employ the back-prop learning algorithm, we address a familiar misconception that sigmoidal networks are inherently non-local by demonstrating that given a sufficiently large number of adjustable parameters, sigmoidal MLPs can be made arbitrarily local while retaining the ability to represent any continuous function on a compact domain. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K.-I. Funahashi, </author> <title> "On the approximate realization of continuous mappings by neural networks," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 2, </volume> <pages> pp. 183-192, </pages> <year> 1989. </year>
Reference-contexts: 1. Introduction In order for the universal-approximation ability of feed-forward neural networks to be useful or even meaningful for function approximation, there needs to be an underlying goodness measure describing how well the network is approximating a desired function <ref> [1] </ref>. Goodness measures, such as mean squared error, exist and are well used and understood. After a network has been trained, its generalization ability can be measured using Vapnik-Chervonenkis (VC) Dimension to predict the network's accuracy of responses to novel stimuli [2]. <p> b i (5) where w = [a 1 a 8 b 1 b 8 c 1 c 8 ] T , and a i = 1, b i = 0:8, c i = i=8, for i 2 f1; 8g: The centers are equally spaced along the input domain X = <ref> [0; 1] </ref>. Assuming the input x is chosen from a uniform distribution over X , the measure of localization for this network can be numerically computed as L f;w;H;X = 0:67.
Reference: [2] <author> E. B. Baum and D. Haussler, </author> <title> "What size net gives valid generalization?," </title> <journal> Neural Computation, </journal> <volume> vol. 1, </volume> <pages> pp. 81-90, </pages> <year> 1989. </year>
Reference-contexts: Goodness measures, such as mean squared error, exist and are well used and understood. After a network has been trained, its generalization ability can be measured using Vapnik-Chervonenkis (VC) Dimension to predict the network's accuracy of responses to novel stimuli <ref> [2] </ref>. Neither ability, however, measures what happens during the learning process; neither the approximation ability nor generalization ability depends upon the learning algorithm. Even though much research is aimed at developing efficient learning algorithms, there are few analytical tools to help us understand the learning process.
Reference: [3] <author> R. </author> <title> French, "Dynamically constraining connectionist networks to produce distributed, orthogonal representations to reduce catastrophic interference," </title> <booktitle> in Proceedings of the 16th Annual Cognitive Science Society Conference, </booktitle> <volume> vol. 5, </volume> <pages> pp. 207-220, </pages> <year> 1994. </year>
Reference-contexts: Analytical tools for investigating, understanding, and perhaps mitigating the problem of interference would be useful because interference problems have been uncovered in various forms by researchers in many areas <ref> [3, 4] </ref>. For example, consider a dynamical system after it settles into a desired trajectory (where only a small portion of the input/output (i/o) map is used). Without noise, a network function approxima-tor learns the trajectory, reduces the approximation error and then ceases learning. <p> Another variant of the interference problem is in the classification literature; "Catastrophic Interference" occurs when the training of a new pattern causes the unlearning of originally trained patterns <ref> [3] </ref>. These and other interference problems may appear different when embedded in their particular applications but the root of these problems is the same; learning tends to interfere with previous learning elsewhere in the input space.
Reference: [4] <author> D. Sofge and D. White, </author> <title> "Applied learning: optimal control for manufacturing," in Handbook of Intelligent Control Neural, Fuzzy, and Adaptive Approaches (D. </title> <editor> White and D. Sofge, eds.), </editor> <address> (New York, NY), </address> <pages> pp. 259-281, </pages> <publisher> Van Nostrand Reinhold, </publisher> <year> 1992. </year>
Reference-contexts: Analytical tools for investigating, understanding, and perhaps mitigating the problem of interference would be useful because interference problems have been uncovered in various forms by researchers in many areas <ref> [3, 4] </ref>. For example, consider a dynamical system after it settles into a desired trajectory (where only a small portion of the input/output (i/o) map is used). Without noise, a network function approxima-tor learns the trajectory, reduces the approximation error and then ceases learning. <p> With noise, however, the learning algorithm stays active and continually memorizes the trajectory (because the error never goes to zero) even though there is no need to do so. "Global Network Collapse" results, as other areas of the mapping gradually unlearn due to interference <ref> [4] </ref>. Another variant of the interference problem is in the classification literature; "Catastrophic Interference" occurs when the training of a new pattern causes the unlearning of originally trained patterns [3].
Reference: [5] <author> W. Baker and J. Farrell, </author> <title> "An introduction to connectionist learning control systems," in Handbook of Intelligent Control Neural, Fuzzy, and Adaptive Approaches (D. </title> <editor> White and D. Sofge, eds.), </editor> <address> (New York, NY), </address> <pages> pp. 35-63, </pages> <publisher> Van Nostrand Reinhold, </publisher> <year> 1992. </year>
Reference-contexts: Of course, taken to this extreme the memory requirement would become prohibitive. When using neural networks, however, learning at x typically does affect the output at x 0 6= x, leading to the potential of interference <ref> [5] </ref>. Networks that are less prone to interference are called local networks because learning in one region, of the input space causes changes in the i/o map in only a region local to the point of training. <p> The mapping in one region of the input space is less likely to be unlearned when learning migrates to another area of the input space. With the exception of Baker and Farrell <ref> [5] </ref> who describe a rule-of-thumb for characterizing useful local networks, the majority of the neural-network literature provides only a cursory look at what it means to be local. We offer a rigorous definition of interference between learning at two points.
Reference: [6] <author> J. Sjoberg, Q. Zhang, L. Ljung, A. Benveniste, B. Deylon, P.-Y. Glorennec, H. Hjalmarsson, and A. Ju-ditsky, </author> <title> "Nonlinear black-box modeling in system identification: A unified overview," </title> <year> 1995. </year>
Reference-contexts: The functions shown in Figure 3 (b) and 3 (c), however, vanish rapidly at infinity and indicate local properties (Sjoberg et al. also characterize local networks in this way <ref> [6] </ref>). From this analysis we see that the network architecture and weights help determine the interference that occurs and hence plays an important role in determining network localization. 4. Localization and Approximation In the preceding section only the local properties of a network was discussed.
Reference: [7] <author> S. Weaver, L. Baird, and M. Polycarpou, </author> <title> "An analytical framework for local feedforward networks," </title> <type> Tech. Rep. TR 195/07/96/ECECS, </type> <institution> University of Cincinnati, </institution> <year> 1996. </year>
Reference-contexts: The proof of the theorem has been omitted due to space limitations, but can be found in Weaver et al. <ref> [7] </ref>. This theorem corrects the generally held misconception that MLPs are inherently non-local. 5. Conclusion The formal definitions of interference and localization provided in this paper comprise an analytical framework for investigating local properties of neural networks.
Reference: [8] <author> J. Albus, </author> <title> "A new approach to manipulator control: The cerebellar model articulation controller," Journal of Dynamic Systems, </title> <booktitle> Measurement, and Control, </booktitle> <pages> pp. 220-227, </pages> <year> 1975. </year>
Reference-contexts: This framework is consistent with the neural-network literature, which loosely describes local networks as having an association between a region of the input space and a weight. This characteristic is easy to see in certain networks such as CMAC <ref> [8] </ref> in which the input space is partitioned and each partition is tied to n weights.
Reference: [9] <author> A. G. Barto, </author> <title> "Connectionist learning for control," in Neural Networks for Control (I. </title> <editor> W. Thomas Miller, R. S. Sutton, and P. J. </editor> <title> Werbos, </title> <type> eds.), </type> <institution> (Massachusetts Institute of Technology, </institution> <address> MA), </address> <pages> pp. 5-58, </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: A char acteristic of such a non-local network, however, is that learning at each point in the input domain affects the entire i/o map. Neural networks are versatile because they can be either local or non-local <ref> [9] </ref>. To measure interference, which is a result of learning, one requires the "state" of the network in the form of the architecture and the weights, and one requires the "action" that is occuring, i.e., the learning algorithm.
References-found: 9

