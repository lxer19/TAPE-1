URL: http://robotics.stanford.edu/~sbenson/ML95.ps
Refering-URL: http://robotics.stanford.edu/~sbenson/
Root-URL: http://www.cs.stanford.edu
Email: sbenson@cs.stanford.edu  
Title: Inductive Learning of Reactive Action Models  
Author: Scott Benson 
Note: In Armand Prieditis Stuart Russell, eds., Machine Learning: Proceedings of the Twelfth International Conference, Morgan Kaufmann Publishers,  
Web: http://robotics.stanford.edu/users/sbenson/bio.html  
Address: Stanford, CA 94305  San Francisco, CA.  
Affiliation: Computer Science Department Stanford University  
Abstract: An important area of learning in autonomous agents is the ability to learn domain-specific models of actions to be used by planning systems. In this paper, we present methods by which an agent learns action models from its own experience and from its observation of a domain expert. These methods differ from previous work in the area in two ways: the use of an action model formalism which is better suited to the needs of a reactive agent, and successful implementation of noise-handling mechanisms. Training instances are generated from experience and observation, and a variant of GOLEM is used to learn action models from these instances. The integrated learning system has been experimentally validated in simulated construc tion and office domains.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bates, J. </author> <year> (1992), </year> <title> "Virtual reality, art, and entertainment", PRESENCE: </title> <booktitle> Teleoperators and Virtual Environments 1(1), </booktitle> <pages> 133-138. </pages>
Reference-contexts: Experimentally, TRAIL will be applied in other domains that are more complicated than the two simple domains discussed above. We are considering as possibilities the SGI Flight Simulator, the Woggleworld system <ref> (Bates 1992) </ref>, and teacher-based training of factory robot manipulators. Acknowledgements I thank Nils Nilsson for his supervision of my research, and thank George John, Ronny Kohavi, Pat Lang-ley, and the two anonymous reviewers for helpful comments on this paper.
Reference: <author> Benson, S. </author> <year> (1995), </year> <title> Action model learning and action execution in a reactive agent, </title> <note> research note, available at http://robotics.stanford.edu/ users/sbenson/bio.html. </note>
Reference: <author> Benson, S. & Nilsson, N. </author> <year> (1995), </year> <title> Reacting, planning, and learning in an autonomous agent, </title> <editor> in K. Fu-rukawa, D. Michie & S. Muggleton, eds, </editor> <booktitle> "Machine Intelligence 14", </booktitle> <publisher> Oxford: the Clarendon Press. in press. </publisher>
Reference: <author> Christiansen, A. D. </author> <year> (1991), </year> <title> Manipulation planning from empirical backprojection, </title> <booktitle> in "Proceedings. 1991 IEEE International Conference on Robotics and Automation", </booktitle> <pages> pp. 762-768. </pages>
Reference: <author> Connell, J. H. & Mahadevan, S., </author> <title> eds (1993), Robot Learning, </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Dzeroski, S. & Lavrac, N. </author> <year> (1994), </year> <title> Inductive Logic Programming: Techniques and Applications, </title> <address> Chich-ester, England: </address> <publisher> Ellis Horwood. </publisher>
Reference: <author> Dzeroski, S., Muggleton, S. & Russell, S. </author> <year> (1993), </year> <title> Learnability of constrained logic programs, </title> <booktitle> in "Proceedings of the European Conference on Machine Learning", </booktitle> <pages> pp. 342-347. </pages>
Reference: <author> Fikes, R. E. & Nilsson, N. J. </author> <year> (1971), </year> <title> "STRIPS: A new approach to the application of theorem proving to problem solving", </title> <booktitle> Artificial Intelligence 2, </booktitle> <pages> 189-208. </pages>
Reference-contexts: Our work differs from these systems in two main ways: the use of an action model formalism which is better suited to the needs of a reactive agent than the state-action next state model of traditional STRIPS operators <ref> (Fikes & Nilsson 1971) </ref>, and successful implementation of noise-handling mechanisms. First, our system uses the teleo-operator model (Ben-son & Nilsson 1995) to represent both durative and discrete actions in a compact manner. <p> Traditional reasoning systems for autonomous agents have used discrete action representations, beginning with GPS (Newell & Simon 1963), the situation calculus (Mc-Carthy & Hayes 1970), and STRIPS <ref> (Fikes & Nils-son 1971) </ref>. However, the need for execution monitoring and reactive behavior in dynamic and uncertain environments has led various authors (Schoppers 1992, Nilsson 1994) to reject the view of actions as monolithic processes which are uninterruptible once invoked.
Reference: <author> Gil, Y. </author> <year> (1992), </year> <title> Acquiring Domain Knowledge for Planning by Experimentation, </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University. </institution>
Reference: <author> Lozano-Perez, T., Mason, M. T. & Taylor, R. </author> <year> (1984), </year> <title> "Automatic synthesis of fine-motion strategies for robots", </title> <journal> International Journal of Robotics Research 3(1), </journal> <volume> 3 - 24. </volume>
Reference: <author> Maes, P. </author> <year> (1991), </year> <title> Adaptive action selection, </title> <booktitle> in "Proceedings of the Thirteenth Annual Converence of the Cognitive Science Society". </booktitle>
Reference: <author> Mahadevan, S. </author> <year> (1992), </year> <title> Enhancing transfer in reinforcement learning by building stochastic models of robot actions, </title> <booktitle> in Sleeman & Edwards (1992), </booktitle> <pages> pp. 290 - 299. </pages>
Reference: <author> McCarthy, J. & Hayes, P. </author> <year> (1970), </year> <title> Some philosophical problems from the standpoint of artificial intelligence, </title> <editor> in B. Meltzer & D. Michie, eds, </editor> <booktitle> "Machine Intelligence 4", </booktitle> <publisher> Edinburgh: Edinburgh University Press, </publisher> <pages> pp. 463-502. </pages>
Reference-contexts: The first step in restating the TOP preimage learning problem in ILP terms is to reify the state for each instance, thus adding a state variable to each literal in the state description, as in the situation calculus <ref> (McCarthy & Hayes 1970) </ref>. Next, a new predicate InPreimage is created which forms the target relation which the ILP learner will induce.
Reference: <author> Muggleton, S. & Feng, C. </author> <year> (1990), </year> <title> Efficient induction of logic programs, </title> <booktitle> in "Proceedings of the First Conference on Algorithm Learning Theory", </booktitle> <pages> pp. 368-381. </pages>
Reference-contexts: The algorithm used for learning preimages in TRAIL is based on the bottom-up inductive logic programming system GOLEM <ref> (Muggleton & Feng 1990) </ref>. <p> As in GOLEM, several restrictions are imposed on the LGGs computed by TRAIL, both to make the computation tractable and to avoid overfitting. First, all literals in each LGG must be determinate <ref> (Muggleton & Feng 1990) </ref>, meaning that the binding of each variable that appears must be uniquely determined given both the bindings of the TOP-bound variables in the head and the bindings of the variables which appear in preceding literals.
Reference: <author> Muggleton, S., ed. </author> <year> (1992), </year> <title> Inductive Logic Programming, </title> <publisher> Academic Press. </publisher>
Reference-contexts: This representation simplifies learning considerably by avoiding unnecessary computation associated with anticipating every state of the world which will be visited and with predicting the value of every condition in each of these states. Second, the field of inductive logic programming, or ILP <ref> (Muggleton 1992, Dzeroski & Lavrac 1994) </ref> provides methods for concept induction in first-order predicate logic.
Reference: <author> Newell, A. & Simon, H. A. </author> <year> (1963), </year> <title> GPS, a program that simulates human thought, </title> <editor> in E. </editor> <publisher> A. </publisher>
Reference-contexts: Traditional reasoning systems for autonomous agents have used discrete action representations, beginning with GPS <ref> (Newell & Simon 1963) </ref>, the situation calculus (Mc-Carthy & Hayes 1970), and STRIPS (Fikes & Nils-son 1971).
Reference: <editor> Feigenbaum & J. Feldman, eds, </editor> <booktitle> "Computers and Thought", R. Oldenbourg KG., </booktitle> <pages> pp. 279-293. </pages>
Reference: <author> Nilsson, N. J. </author> <year> (1994), </year> <title> "Teleo-reactive programs for agent control", </title> <journal> Journal of Artificial Intelligence Research 1, </journal> <volume> 139 - 158. </volume> <booktitle> Pazzani & Kibler (1992), "The utility of knowledge in inductive learning", Machine Learning 9(1), </booktitle> <pages> 57-94. </pages>
Reference-contexts: In short, TRAIL plans and executes teleo-reactive trees <ref> (Nilsson 1994) </ref> while maintaining a set of action models based on its experience. The general policy of TRAIL is to continue planning, executing, learning, and replanning for as long as possible, calling on an external teacher when one of a variety of impasses is reached.
Reference: <author> Sablon, G. </author> <year> (1994), </year> <type> Personal Communication. </type>
Reference-contexts: Sablon and Bruynooghe (1994) propose us ing Inductive Logic Programming to learn action mod-els, as TRAIL does, but have done little work on the subject beyond their proposal <ref> (Sablon 1994) </ref>. Wang's (1994) system learns the preconditions of STRIPS-like operators, by always maintaining the most specific condition consistent with all observed examples. It is assumed that all preconditions are conjunctive and that there is no domain noise.
Reference: <author> Sablon, G. & Bruynooghe, M. </author> <year> (1994), </year> <title> Using the event calculus to integrate planning and learning in an intelligent autonomous agent, </title> <editor> in C. Backstrom & E. Sandewall, eds, </editor> <booktitle> "Current Trends in AI Planning", </booktitle> <publisher> IOS Press, </publisher> <pages> pp. 254 - 265. </pages>
Reference-contexts: Sablon and Bruynooghe (1994) propose us ing Inductive Logic Programming to learn action mod-els, as TRAIL does, but have done little work on the subject beyond their proposal <ref> (Sablon 1994) </ref>. Wang's (1994) system learns the preconditions of STRIPS-like operators, by always maintaining the most specific condition consistent with all observed examples. It is assumed that all preconditions are conjunctive and that there is no domain noise.
Reference: <author> Sammut, C., Hurst, S., Kedzier, D. & Michie, D. </author> <year> (1992), </year> <note> Learning to fly, in Sleeman & Edwards (1992), pp. 385 - 393. </note>
Reference-contexts: These two domains have very distinct properties, such that few if any existing learning systems can deal with both. Policy learning systems, e.g. those using reinforcement learning (Sutton 1988) or behavioral cloning <ref> (Sammut et al. 1992) </ref>, should learn successfully in the bar-grabbing task, but would have difficulty with the variety of tasks which occur in the office domain.
Reference: <author> Schoppers, M. J. </author> <year> (1992), </year> <title> Building plans to monitor and exploit open-loop and closed-loop dynamics, </title> <booktitle> in "Proceedings of the First International Conference on AI Planning Systems", </booktitle> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Traditional reasoning systems for autonomous agents have used discrete action representations, beginning with GPS (Newell & Simon 1963), the situation calculus (Mc-Carthy & Hayes 1970), and STRIPS (Fikes & Nils-son 1971). However, the need for execution monitoring and reactive behavior in dynamic and uncertain environments has led various authors <ref> (Schoppers 1992, Nilsson 1994) </ref> to reject the view of actions as monolithic processes which are uninterruptible once invoked.
Reference: <author> Shen, W.-M. </author> <year> (1994), </year> <title> Autonomous Learning from the Environment, </title> <publisher> Computer Science Press, W.H. Freeman and Company. </publisher>
Reference-contexts: Wang's (1994) system learns the preconditions of STRIPS-like operators, by always maintaining the most specific condition consistent with all observed examples. It is assumed that all preconditions are conjunctive and that there is no domain noise. Shen <ref> (Shen 1994) </ref> has developed an algorithm called CDL3 which learns decision lists from structured instances in the context of learning operator preconditions. Due to the differences in concept description languages, it is difficult to compare CDL3 to more traditional ILP systems.
Reference: <editor> Sleeman, D. & Edwards, P., eds (1992), </editor> <booktitle> Machine Learning: Proceedings of the Ninth International Workshop, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sutton, R. </author> <year> (1988), </year> <title> "Learning to predict by methods of temporal difference", </title> <booktitle> Machine Learning 3(1), </booktitle> <pages> 9-44. </pages>
Reference-contexts: These two domains have very distinct properties, such that few if any existing learning systems can deal with both. Policy learning systems, e.g. those using reinforcement learning <ref> (Sutton 1988) </ref> or behavioral cloning (Sammut et al. 1992), should learn successfully in the bar-grabbing task, but would have difficulty with the variety of tasks which occur in the office domain.
Reference: <author> Vere, S. A. </author> <year> (1980), </year> <title> "Multilevel counterfactuals for generalizations of relational concepts and productions", </title> <booktitle> Artificial Intelligence 14(2), </booktitle> <pages> 139-164. </pages>
Reference: <author> Wang, X. </author> <year> (1994), </year> <title> Learning planning operators by observation and practice, </title> <editor> in K. Hammond, ed., </editor> <booktitle> "Proceedings of the Second International Conference on AI Planning Systems", </booktitle> <publisher> AAAI Press, </publisher> <pages> pp. 335-341. </pages>
Reference: <author> Watkins, C. </author> <year> (1989), </year> <title> Learning from Delayed Rewards, </title> <type> PhD thesis, </type> <institution> Cambridge University. Psychology Department. </institution>
Reference-contexts: However, the need for execution monitoring and reactive behavior in dynamic and uncertain environments has led various authors (Schoppers 1992, Nilsson 1994) to reject the view of actions as monolithic processes which are uninterruptible once invoked. One possible solution, as in reinforcement learning <ref> (Watkins 1989, Mahadevan 1992) </ref>, is to subdivide actions into discrete units corresponding to the maximum rate at which the agent senses its environment, thus allowing an execution system to control the agent's behavior reactively.
Reference: <author> Whitehead, S. D. & Ballard, D. H. </author> <year> (1990), </year> <title> Active perception and reinforcement learning, </title> <booktitle> in "Machine Learning: Proceedings of the Seventh International Conference", </booktitle> <address> Austin, TX., </address> <pages> pp. 179-188. </pages>
Reference-contexts: definition of only specifies that the TOP normally succeeds when ever is true.) * The inverse case is that in which serendipitous circumstances or the actions of other agents allow a TOP to succeed in a condition which does not satisfy , producing a false positive instance. * Perceptual aliasing <ref> (Whitehead & Ballard 1990) </ref> may result in states in which a TOP sometimes succeeds and sometimes fails. TRAIL does not currently do any experimentation to relieve noise difficulties.
References-found: 29

