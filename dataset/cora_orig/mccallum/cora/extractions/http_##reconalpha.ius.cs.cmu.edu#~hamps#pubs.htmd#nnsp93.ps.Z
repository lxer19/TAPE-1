URL: http://reconalpha.ius.cs.cmu.edu/~hamps/pubs.htmd/nnsp93.ps.Z
Refering-URL: http://reconalpha.ius.cs.cmu.edu/~hamps/pubs.htmd/index.html
Root-URL: 
Email: hamps@faraday.ece.cmu.edu kumar@previa.ece.cmu.edu  
Title: DIFFERENTIALLY GENERATED NEURAL NETWORK CLASSIFIERS ARE EFFICIENT  
Author: J. B. Hampshire II B.V.K. Vijaya Kumar Kamm, Kuhn, Yoon, Chellappa, and Kung, 
Date: 1993.  
Note: Reprinted (with minor reference changes) from Neural Networks for Signal Processing 3 IEEE Proceedings of the 1993 IEEE Workshop,  eds., New York: IEEE Press,  
Address: Pittsburgh, PA 15213-3890  
Affiliation: Department of Electrical Computer Engineering Carnegie Mellon University,  
Abstract: Differential learning for statistical pattern classification is described in [5]; it is based on the classification figure-of-merit (CFM) objective function described in [9, 5]. We prove that differential learning is asymptotically efficient, guaranteeing the best generalization allowed by the choice of hypothesis class (see below) as the training sample size grows large, while requiring the least classifier complexity necessary for Bayesian (i.e., minimum probability-of-error) discrimination. Moreover, differential learning almost always guarantees the best generalization allowed by the choice of hypothesis class for small training sample sizes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. M. </author> <title> Cover. Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition. </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> EC-14:326-- 334, </volume> <year> 1965. </year>
Reference-contexts: As an example, the set of all C-output multi-layer perceptrons with no more than 500 total connections is a hypothesis class. the best". Indeed there is a large body of work validating the maxim. We note in particular the work of Cover <ref> [1] </ref> and Vapnik (e.g., [18]), and Valiant's PAC (probably approximately correct) model of learning deterministic concepts [17].
Reference: [2] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, NY, </address> <year> 1973. </year>
Reference-contexts: INTRODUCTION The objective of all statistical pattern classifiers <ref> [2] </ref> is to implement the Bayesian discriminant function (BDF i.e., any set 1 of discriminant functions that guarantees the lowest probability of making a classification error in the pattern recognition task). A classifier that implements the BDF is said to yield Bayesian discrimination. <p> , 0 , G (fi), fl P e F Bayes DError fi fl 7 7 = E S n , 0 P e G j S , 0 , G (fi), fl P e F Bayes This bias is always non-negative, since the Bayes error rate is provably minimal (e.g., <ref> [2] </ref>).
Reference: [3] <author> B. Efron. </author> <title> The Efficiency of Logistic Regression Compared to Normal Discriminant Analysis. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 70(352):892--898, </volume> <year> 1975. </year>
Reference-contexts: is the problem with asymptotically efficient estimators in general: we know they do good things with large sample sizes, but there may be another asymptotically efficient estimator that does even better for small sample sizes a possibility that is confirmed or refuted by examining the asymptotic relative efficiency (ARE, e.g., <ref> [3] </ref>) of the alternative estimator. In almost all cases, the efficiency of differential learning holds for small sample sizes too, in that no other learning strategy generates a classifier from the arbitrarily-chosen G (fi) with lower MSDE.
Reference: [4] <author> S. Geman, E. Bienenstock, and R. Doursat. </author> <title> Neural Networks and the Bias/Variance Dilemma. </title> <booktitle> Neural Computation, </booktitle> <address> 4(1):1--58, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: Note that discriminant bias, discriminant variance, and mean-squared discriminant error are very different from the functional bias, functional variance, and mean-squared (functional) error (along with other related error measures such as the Kullback-Liebler information distance [14]) typically discussed in the neural network and machine/computational learning literature (e.g., <ref> [4] </ref>).
Reference: [5] <author> J. B. </author> <title> Hampshire II. A Differential Theory of Learning for Efficient Statistical Pattern Recognition. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, Department of Electrical & Computer Engineering, Hammerschlag Hall, </institution> <address> Pittsburgh, PA 15213-3890, </address> <month> expected July </month> <year> 1993. </year>
Reference-contexts: We propose a differential learning strategy; it is based on the classification figure-of-merit (CFM) objective function, 3 originally described in [9] and recently replicated in [16, 12]. 4 Rigorous proofs regarding the asymptotic efficiency of differential learning and the inefficiency of probabilistic learning are given in <ref> [5] </ref>. The following is a summary: * Classifiers that learn by minimizing error measure objective functions (e.g., mean-squared error, the Kullback-Liebler information distance, etc.) learn probabilistically. <p> As a result, probabilistic learning is inefficient. * Classifiers that learn by maximizing the classification figure of merit objective function (CFM <ref> [9, 5] </ref>) learn differentially. The classifier that learns differentially attempts to learn only the most likely class of the feature vector over its domain. * Learning differentially by maximizing the synthetic CFM objective function described in [5] minimizes the classifier's MSDE for large training sample sizes. <p> The classifier that learns differentially attempts to learn only the most likely class of the feature vector over its domain. * Learning differentially by maximizing the synthetic CFM objective function described in <ref> [5] </ref> minimizes the classifier's MSDE for large training sample sizes. <p> Throughout this paper we refer to an improved synthetic functional form of CFM <ref> [5] </ref>, which provably leads to higher learning speeds and lower error rates than the original form described in [9]. * Learning differentially by maximizing the synthetic CFM objective function described in [5] almost always minimizes the classifier's MSDE for small training sample sizes as well. <p> Throughout this paper we refer to an improved synthetic functional form of CFM <ref> [5] </ref>, which provably leads to higher learning speeds and lower error rates than the original form described in [9]. * Learning differentially by maximizing the synthetic CFM objective function described in [5] almost always minimizes the classifier's MSDE for small training sample sizes as well. As a result, differential learning almost always produces the best-generalizing classifier for small training sample sizes. * Learning differentially by maximizing the synthetic CFM objective function described in [5] requires a classifier with the least functional complexity <p> maximizing the synthetic CFM objective function described in <ref> [5] </ref> almost always minimizes the classifier's MSDE for small training sample sizes as well. As a result, differential learning almost always produces the best-generalizing classifier for small training sample sizes. * Learning differentially by maximizing the synthetic CFM objective function described in [5] requires a classifier with the least functional complexity (e.g., the fewest parameters) necessary for Bayesian discrimination. * Information-theoretic analysis [6] [5, ch. 6] shows why the training sample sizes necessary to guarantee a specified level of generalization (as measured by MSDE) via differential learning are typically orders of magnitude smaller <p> As a result, differential learning almost always produces the best-generalizing classifier for small training sample sizes. * Learning differentially by maximizing the synthetic CFM objective function described in [5] requires a classifier with the least functional complexity (e.g., the fewest parameters) necessary for Bayesian discrimination. * Information-theoretic analysis [6] <ref> [5, ch. 6] </ref> shows why the training sample sizes necessary to guarantee a specified level of generalization (as measured by MSDE) via differential learning are typically orders of magnitude smaller than those predicted by current probabilistic extensions of the PAC learning paradigm [17] to stochastic concepts on uncountable feature vector domains <p> An Introduction to Differential Learning and Classifier Efficiency Proofs of all the preceding assertions are beyond the scope of this paper. Owing to page limitations, we focus on the single proof that differential learning is asymptotically efficient. Readers interested in the other proofs can find them in <ref> [5] </ref> and the earlier works on which that reference is based (e.g., [9, 8, 6]). <p> For these reasons, we strongly encourage the reader to review [7] before proceeding, in order to develop the requisite background (those seeking a complete and detailed treatment of the material should refer to <ref> [5] </ref>). DISCRIMINANT ERROR, EFFICIENCY, AND GENERALIZATION Let us quantify precisely what we mean by ``efficient learning''. The post-learning error rate of the classifier depends on its initial parameterization 0 and its hypothesis class G (fi) , the training sample S n , and the learning strategy fl . <p> DIFFERENTIAL LEARNING IS ASYMPTOTICALLY EFFICIENT The CFM generated by a training sample is the sum of the synthetic objective function [ ] described in <ref> [5] </ref> over the training sample: CFM = i=1 p=1 n 6 6 4 n p 2 6 6 g i (X p j ) max g k (X p j ) ffi i (X p j ) 3 7 7 3 7 7 (4) where g i (X j ) is <p> As the training sample size n grows asymptotically large, the generalized Glivenko-Cantelli theorem [18, pg. 38] holds, and the empirical frequencies in (4) converge to their underlying probabilities. Thus [8, sec. 4] <ref> [5, ch. 2] </ref>, n!1 CFM = 6 6 6 C X ffi i (X j ) , P Wjx (! i j X) CFM (X) 7 7 7 x (X) dX (5) The synthetic form of the CFM objective function fi fl becomes a step function as its confidence parameter goes <p> n!1 CFM = 6 6 6 C X ffi i (X j ) , P Wjx (! i j X) CFM (X) 7 7 7 x (X) dX (5) The synthetic form of the CFM objective function fi fl becomes a step function as its confidence parameter goes to zero <ref> [5] </ref>: lim ffi i (X j ) , = &lt; 0 , ffi i (X j ) 0 (6) Note that a positive discriminant differential ffi i (X j ) indicates that the corre-sponding ith discriminator output is greater than all other outputs ( ffi i (X j ) &gt; 0 <p> Given the limiting form of the objective function in (6), max CFM (X) in (5) has only one non-zero term, corresponding to the most likely class, given X, ! (1) : P Wjx (! (1) j X) P Wjx (! i j X) 8 i 6= (1) [8, sec. 4] <ref> [5, ch. 2] </ref>. <p> As a result, lim n!1 1 P e F Bayes DError h i F Bayes DError fi fl (8) where fi denotes all of parameter space. Since DError fi fl is always non negative <ref> [5, ch. 3] </ref>, (8) leads to lim n!1 DError h i DError fi fl (9) Thus, maximizing the step form of CFM is equivalent to minimizing the classifier's discriminant error ( = 1 ) for asymptotically large training sample sizes. <p> There are special cases for which differential learning does not generate the minimum-MSDE classifier allowed by the choice of hypoth esis class G (fi): these are cases for which G (fi) happens to constitute a proper parametric model of the feature vector X (see <ref> [5, ch's. 3-4] </ref>). MINIMUM-COMPLEXITY REQUIREMENTS OF DIFFERENTIAL LEARNING Probabilistic learning fl P is the process by which the classifier's discriminant functions learn the a posteriori class probabilities over the domain of the feature vector. <p> In simple terms, there is a limit to the intricacy of the mapping from feature vector space to classification space implemented by a classifier with limited functional complexity. 158 References [6] and <ref> [5] </ref> prove that usually it requires less and never requires more functional complexity to satisfy (14) than it requires to satisfy (13). Indeed, learning the BDF differentially provably requires the least functional complexity; learning the BDF probabilistically usually requires significantly greater functional complexity (see [7] for an illustration).
Reference: [6] <author> J. B. Hampshire II and B. V. K. Vijaya Kumar. </author> <title> Shooting Craps in Search of an Optimal Strategy for Training Connectionist Pattern Classifiers. </title> <editor> In J. Moody, S. Hanson, and R. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> vol. 4, </volume> <pages> pages 1125--1132, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kauffman. </publisher> <pages> 159 </pages>
Reference-contexts: As a result, differential learning almost always produces the best-generalizing classifier for small training sample sizes. * Learning differentially by maximizing the synthetic CFM objective function described in [5] requires a classifier with the least functional complexity (e.g., the fewest parameters) necessary for Bayesian discrimination. * Information-theoretic analysis <ref> [6] </ref> [5, ch. 6] shows why the training sample sizes necessary to guarantee a specified level of generalization (as measured by MSDE) via differential learning are typically orders of magnitude smaller than those predicted by current probabilistic extensions of the PAC learning paradigm [17] to stochastic concepts on uncountable feature vector <p> Owing to page limitations, we focus on the single proof that differential learning is asymptotically efficient. Readers interested in the other proofs can find them in [5] and the earlier works on which that reference is based (e.g., <ref> [9, 8, 6] </ref>). All the proofs assume that the reader understands the fundamental differences between probabilistic and differential learning strategies, and that discriminant bias, discriminant variance, mean-squared discriminant error (MSDE), classifier efficiency, and efficient learning are at least intuitively meaningful notions. <p> In simple terms, there is a limit to the intricacy of the mapping from feature vector space to classification space implemented by a classifier with limited functional complexity. 158 References <ref> [6] </ref> and [5] prove that usually it requires less and never requires more functional complexity to satisfy (14) than it requires to satisfy (13). Indeed, learning the BDF differentially provably requires the least functional complexity; learning the BDF probabilistically usually requires significantly greater functional complexity (see [7] for an illustration).
Reference: [7] <author> J. B. Hampshire II and B. V. K. Vijaya Kumar. </author> <title> Differential theory of learning for efficient neural network pattern recognition. </title> <editor> In D. Ruck, editor, </editor> <booktitle> Proceedings of the 1993 SPIE International Symposium on Optical Engineering and Photonics in Aerospace and Remote Sensing, vol. 1966: Science of Artificial Neural Networks, </booktitle> <pages> pages 76--95, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Without this perspective it is doubtful that the following proof will be enlightening, given the terse notational convention. For these reasons, we strongly encourage the reader to review <ref> [7] </ref> before proceeding, in order to develop the requisite background (those seeking a complete and detailed treatment of the material should refer to [5]). DISCRIMINANT ERROR, EFFICIENCY, AND GENERALIZATION Let us quantify precisely what we mean by ``efficient learning''. <p> Indeed, learning the BDF differentially provably requires the least functional complexity; learning the BDF probabilistically usually requires significantly greater functional complexity (see <ref> [7] </ref> for an illustration). SUMMARY We have defined the efficient learning strategy as one that generates the classifier with the lowest mean-squared discriminant error (MSDE) allowed by the training sample size and the choice of hypothesis class.
Reference: [8] <author> J. B. Hampshire II and B. A. Pearlmutter. </author> <title> Equivalence Proofs for Multi-Layer Perceptron Classifiers and the Bayesian Discriminant Function. </title> <editor> In Touretzky, Elman, Sejnowski, and Hinton, editors, </editor> <booktitle> Proceedings of the 1990 Connectionist Models Summer School, pages 159--172, </booktitle> <address> San Mateo, CA, </address> <year> 1991. </year> <note> Morgan Kaufmann. Announced and published electronically in the Ohio-State University pub/neuroprose archive, September 23, 1990: available via anonymous ftp from archive.cis.ohio-state.edu, in file pub/neuroprose/hampshire.bayes90.ps.Z. </note>
Reference-contexts: Similarly, it is straightforward to prove that Juang and Katagiri's MCE objective function is equivalent to the original form of the CFM objective function in both its ``N-monotonic'' and non-monotonic forms, depending on the value of their parameter (cf. [12] and <ref> [9, 8] </ref>). <p> Owing to page limitations, we focus on the single proof that differential learning is asymptotically efficient. Readers interested in the other proofs can find them in [5] and the earlier works on which that reference is based (e.g., <ref> [9, 8, 6] </ref>). All the proofs assume that the reader understands the fundamental differences between probabilistic and differential learning strategies, and that discriminant bias, discriminant variance, mean-squared discriminant error (MSDE), classifier efficiency, and efficient learning are at least intuitively meaningful notions. <p> As the training sample size n grows asymptotically large, the generalized Glivenko-Cantelli theorem [18, pg. 38] holds, and the empirical frequencies in (4) converge to their underlying probabilities. Thus <ref> [8, sec. 4] </ref> [5, ch. 2], n!1 CFM = 6 6 6 C X ffi i (X j ) , P Wjx (! i j X) CFM (X) 7 7 7 x (X) dX (5) The synthetic form of the CFM objective function fi fl becomes a step function as its <p> Given the limiting form of the objective function in (6), max CFM (X) in (5) has only one non-zero term, corresponding to the most likely class, given X, ! (1) : P Wjx (! (1) j X) P Wjx (! i j X) 8 i 6= (1) <ref> [8, sec. 4] </ref> [5, ch. 2].
Reference: [9] <author> J. B. Hampshire II and A. H. Waibel. </author> <title> A Novel Objective Function for Improved Phoneme Recognition Using Time-Delay Neural Networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(2):216--228, </volume> <month> June </month> <year> 1990. </year> <note> A revised and extended version of work published earlier in 1) Carnegie Mellon University, </note> <institution> School of Computer Science Technical Report CMU-CS-89-118, </institution> <month> March 31, </month> <year> 1989, </year> <booktitle> and 2) the IEEE Proceedings of the 1989 International Joint Conference on Neural Networks, </booktitle> <volume> vol. I, </volume> <pages> pp. 235-241, </pages> <month> June, </month> <year> 1989. </year>
Reference-contexts: We propose a differential learning strategy; it is based on the classification figure-of-merit (CFM) objective function, 3 originally described in <ref> [9] </ref> and recently replicated in [16, 12]. 4 Rigorous proofs regarding the asymptotic efficiency of differential learning and the inefficiency of probabilistic learning are given in [5]. <p> As a result, probabilistic learning is inefficient. * Classifiers that learn by maximizing the classification figure of merit objective function (CFM <ref> [9, 5] </ref>) learn differentially. The classifier that learns differentially attempts to learn only the most likely class of the feature vector over its domain. * Learning differentially by maximizing the synthetic CFM objective function described in [5] minimizes the classifier's MSDE for large training sample sizes. <p> Optimizing the objective function via iterative search on the classifier's parameter space is therefore a mathematically justifiable approach to machine learning. 4 Telfer and Szu's MME objective function is equivalent to the original form of the ``N-monotonic'' CFM objective function (cf. [16] and <ref> [9] </ref>). Similarly, it is straightforward to prove that Juang and Katagiri's MCE objective function is equivalent to the original form of the CFM objective function in both its ``N-monotonic'' and non-monotonic forms, depending on the value of their parameter (cf. [12] and [9, 8]). <p> Similarly, it is straightforward to prove that Juang and Katagiri's MCE objective function is equivalent to the original form of the CFM objective function in both its ``N-monotonic'' and non-monotonic forms, depending on the value of their parameter (cf. [12] and <ref> [9, 8] </ref>). <p> Throughout this paper we refer to an improved synthetic functional form of CFM [5], which provably leads to higher learning speeds and lower error rates than the original form described in <ref> [9] </ref>. * Learning differentially by maximizing the synthetic CFM objective function described in [5] almost always minimizes the classifier's MSDE for small training sample sizes as well. <p> Owing to page limitations, we focus on the single proof that differential learning is asymptotically efficient. Readers interested in the other proofs can find them in [5] and the earlier works on which that reference is based (e.g., <ref> [9, 8, 6] </ref>). All the proofs assume that the reader understands the fundamental differences between probabilistic and differential learning strategies, and that discriminant bias, discriminant variance, mean-squared discriminant error (MSDE), classifier efficiency, and efficient learning are at least intuitively meaningful notions.
Reference: [10] <author> D. Haussler. </author> <title> Decision Theoretic Generalizations of the PAC Model for Neural Net and Other Learning Applications. </title> <type> Technical Report UCSC-CRL-91-02, </type> <institution> University of California, Santa Cruz, </institution> <month> January </month> <year> 1991. </year> <note> A revised and extended version of UCSC-CRL-89-30. </note>
Reference-contexts: 6] shows why the training sample sizes necessary to guarantee a specified level of generalization (as measured by MSDE) via differential learning are typically orders of magnitude smaller than those predicted by current probabilistic extensions of the PAC learning paradigm [17] to stochastic concepts on uncountable feature vector domains (e.g., <ref> [10, 11, 15, 19] </ref>). An Introduction to Differential Learning and Classifier Efficiency Proofs of all the preceding assertions are beyond the scope of this paper. Owing to page limitations, we focus on the single proof that differential learning is asymptotically efficient.
Reference: [11] <author> D. Haussler, M. Kearns, and R. Schapire. </author> <title> Bounds on the Sample Complexity of Bayesian Learning Using Information Theory and the VC Dimension. </title> <editor> In M. K. Warmuth and L. G. Valiant, editors, </editor> <booktitle> Proceedings of the Fourth Annual Workshop on Computational Learning Theory, pages 61--74, </booktitle> <address> San Mateo, CA, August 1991. </address> <publisher> Morgan Kaufmann, Inc. </publisher>
Reference-contexts: 6] shows why the training sample sizes necessary to guarantee a specified level of generalization (as measured by MSDE) via differential learning are typically orders of magnitude smaller than those predicted by current probabilistic extensions of the PAC learning paradigm [17] to stochastic concepts on uncountable feature vector domains (e.g., <ref> [10, 11, 15, 19] </ref>). An Introduction to Differential Learning and Classifier Efficiency Proofs of all the preceding assertions are beyond the scope of this paper. Owing to page limitations, we focus on the single proof that differential learning is asymptotically efficient.
Reference: [12] <author> B. H. Juang and S. Katagiri. </author> <title> Discriminative Learning for Minimum Error Classification. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 40(12):3043--3054, </volume> <month> December </month> <year> 1992. </year>
Reference-contexts: We propose a differential learning strategy; it is based on the classification figure-of-merit (CFM) objective function, 3 originally described in [9] and recently replicated in <ref> [16, 12] </ref>. 4 Rigorous proofs regarding the asymptotic efficiency of differential learning and the inefficiency of probabilistic learning are given in [5]. The following is a summary: * Classifiers that learn by minimizing error measure objective functions (e.g., mean-squared error, the Kullback-Liebler information distance, etc.) learn probabilistically. <p> Similarly, it is straightforward to prove that Juang and Katagiri's MCE objective function is equivalent to the original form of the CFM objective function in both its ``N-monotonic'' and non-monotonic forms, depending on the value of their parameter (cf. <ref> [12] </ref> and [9, 8]).
Reference: [13] <author> A. N. </author> <title> Kolmogorov. Three Approaches to the Quantitative Definition of Information. Problems of Information Transmission, </title> <address> 1(1):1--7, </address> <month> Jan. Mar. </month> <year> 1965. </year> <note> Faraday Press translation of Problemy Peredachi Informatsii. </note>
Reference-contexts: A classifier that implements the BDF is said to yield Bayesian discrimination. The challenge is to approximate the BDF efficiently, using the fewest training examples and the least complex classifier (e.g., the one with the fewest parameters) necessary for the task. By Kolmogorov's theorem <ref> [13] </ref> there is no algorithm for determining a priori the least complex classifier necessary for Bayesian discrimination. Instead we must restrict our search to a particular hypothesis class, 2 and focus on generating from it a classifier with the minimum sufficient complexity for Bayesian discrimination.
Reference: [14] <author> S. Kullback. </author> <title> Information Theory and Statistics. </title> <publisher> Wiley, </publisher> <address> New York, NY, </address> <year> 1959. </year>
Reference-contexts: Note that discriminant bias, discriminant variance, and mean-squared discriminant error are very different from the functional bias, functional variance, and mean-squared (functional) error (along with other related error measures such as the Kullback-Liebler information distance <ref> [14] </ref>) typically discussed in the neural network and machine/computational learning literature (e.g., [4]).
Reference: [15] <author> B. K. Natarajan. </author> <title> Machine Learning: A Theoretical Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: More recent extensions of the PAC model to learning stochastic concepts (see, for example, the work of Barron, Baum, Blumer, Ehrenfeucht, Haussler, Vapnik, and Yamanishi much of which is summarized in <ref> [15] </ref>) apply directly to statistical pattern recognition. Despite this large body of work in complexity theory, relatively little attention has been paid to the role that the learning strategy plays in producing a classifier that generalizes well. <p> 6] shows why the training sample sizes necessary to guarantee a specified level of generalization (as measured by MSDE) via differential learning are typically orders of magnitude smaller than those predicted by current probabilistic extensions of the PAC learning paradigm [17] to stochastic concepts on uncountable feature vector domains (e.g., <ref> [10, 11, 15, 19] </ref>). An Introduction to Differential Learning and Classifier Efficiency Proofs of all the preceding assertions are beyond the scope of this paper. Owing to page limitations, we focus on the single proof that differential learning is asymptotically efficient.
Reference: [16] <author> B. A. Telfer and H. H. Szu. </author> <title> Implementing the Minimum-Misclassification-Error Energy Function for Target Recognition. </title> <booktitle> In IEEE Proceedings of the 1992 International Joint Conference on Neural Networks, </booktitle> <volume> Vol. IV, </volume> <pages> pages 214--219, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: We propose a differential learning strategy; it is based on the classification figure-of-merit (CFM) objective function, 3 originally described in [9] and recently replicated in <ref> [16, 12] </ref>. 4 Rigorous proofs regarding the asymptotic efficiency of differential learning and the inefficiency of probabilistic learning are given in [5]. The following is a summary: * Classifiers that learn by minimizing error measure objective functions (e.g., mean-squared error, the Kullback-Liebler information distance, etc.) learn probabilistically. <p> Optimizing the objective function via iterative search on the classifier's parameter space is therefore a mathematically justifiable approach to machine learning. 4 Telfer and Szu's MME objective function is equivalent to the original form of the ``N-monotonic'' CFM objective function (cf. <ref> [16] </ref> and [9]). Similarly, it is straightforward to prove that Juang and Katagiri's MCE objective function is equivalent to the original form of the CFM objective function in both its ``N-monotonic'' and non-monotonic forms, depending on the value of their parameter (cf. [12] and [9, 8]).
Reference: [17] <author> L. G. Valiant. </author> <title> A Theory of the Learnable. </title> <booktitle> Communciations of the ACM, </booktitle> <address> 27(11):1134--1142, </address> <month> November </month> <year> 1984. </year>
Reference-contexts: Indeed there is a large body of work validating the maxim. We note in particular the work of Cover [1] and Vapnik (e.g., [18]), and Valiant's PAC (probably approximately correct) model of learning deterministic concepts <ref> [17] </ref>. More recent extensions of the PAC model to learning stochastic concepts (see, for example, the work of Barron, Baum, Blumer, Ehrenfeucht, Haussler, Vapnik, and Yamanishi much of which is summarized in [15]) apply directly to statistical pattern recognition. <p> necessary for Bayesian discrimination. * Information-theoretic analysis [6] [5, ch. 6] shows why the training sample sizes necessary to guarantee a specified level of generalization (as measured by MSDE) via differential learning are typically orders of magnitude smaller than those predicted by current probabilistic extensions of the PAC learning paradigm <ref> [17] </ref> to stochastic concepts on uncountable feature vector domains (e.g., [10, 11, 15, 19]). An Introduction to Differential Learning and Classifier Efficiency Proofs of all the preceding assertions are beyond the scope of this paper.
Reference: [18] <author> V. N. Vapnik. </author> <title> Estimation of Dependencies Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1982. </year> <title> The publisher lists the title in the following alternate form: "Estimation of Dependencies Based on Empirical Data Dependencies", </title> <note> ISBN 0-387-90733-5. </note>
Reference-contexts: As an example, the set of all C-output multi-layer perceptrons with no more than 500 total connections is a hypothesis class. the best". Indeed there is a large body of work validating the maxim. We note in particular the work of Cover [1] and Vapnik (e.g., <ref> [18] </ref>), and Valiant's PAC (probably approximately correct) model of learning deterministic concepts [17]. <p> As the training sample size n grows asymptotically large, the generalized Glivenko-Cantelli theorem <ref> [18, pg. 38] </ref> holds, and the empirical frequencies in (4) converge to their underlying probabilities.
Reference: [19] <author> K. Yamanishi. </author> <title> A learning criterion for stochastic rules. </title> <booktitle> Machine Learning, </booktitle> <address> 9:165--203, </address> <year> 1992. </year> <month> 160 </month>
Reference-contexts: 6] shows why the training sample sizes necessary to guarantee a specified level of generalization (as measured by MSDE) via differential learning are typically orders of magnitude smaller than those predicted by current probabilistic extensions of the PAC learning paradigm [17] to stochastic concepts on uncountable feature vector domains (e.g., <ref> [10, 11, 15, 19] </ref>). An Introduction to Differential Learning and Classifier Efficiency Proofs of all the preceding assertions are beyond the scope of this paper. Owing to page limitations, we focus on the single proof that differential learning is asymptotically efficient.
References-found: 19

