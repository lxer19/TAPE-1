URL: http://www.cs.nyu.edu/vijayk/papers/phd-thesis.ps
Refering-URL: http://www.cs.nyu.edu/vijayk/papers.html
Root-URL: http://www.cs.nyu.edu
Title: RUN-TIME TECHNIQUES FOR DYNAMIC MULTITHREADED COMPUTATIONS  
Author: BY VIJAY KARAMCHETI 
Degree: B.Tech., Indian Institute of Technology, Kanpur, 1988 M.S., The  THESIS Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Electrical Engineering in the Graduate College of the  
Date: 1989  
Address: Austin,  1998 Urbana, Illinois  
Affiliation: University of Texas at  University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> M. J. Berger and J. Oliger, </author> <title> Adaptive mesh refinement for hyperbolic partial differential equations, </title> <journal> Journal of Computational Physics, </journal> <volume> vol. 53, </volume> <pages> pp. 484512, </pages> <year> 1984. </year>
Reference-contexts: Given a set of species, with each species u represented as a vector of character values, u <ref> [1] </ref>; : : : ; u [c max ] (c max is the maximum number of characters to be considered), a character is compatible with a phylogenetic tree if no value for that character arises more than once in any path in the tree.
Reference: [2] <author> M. J. Berger and P. Colella, </author> <title> Local adaptive mesh refinement for shock hydrodynamics, </title> <journal> Journal of Computational Physics, </journal> <volume> vol. 82, </volume> <pages> pp. 6484, </pages> <year> 1989. </year>
Reference: [3] <author> E. Steinthorsson, D. Modiano, W. Y. Crutchfield, J. B. Bell, and P. Colella, </author> <title> Three dimensional adapative mesh refinement for hyperbolic conservation law, </title> <booktitle> in Proceedings of the 12th AIAA Computational Fluid Dynamics Conference, </booktitle> <year> 1995, </year> <pages> pp. 902912. </pages>
Reference: [4] <author> J. Barnes and P. Hut, </author> <title> A hierarchical O(N log N) force calculation algorithm, The Institute for Advanced Study, Princeton, New Jersey, </title> <type> tech. rep., </type> <year> 1986. </year>
Reference-contexts: State-of-the-art techniques for these problems rely on sophisticated adaptive and hierarchical algorithmic cores (e.g., adaptive mesh refinement [13], and tree based n-body codes <ref> [4, 5] </ref>) which reduce total computation by focusing effort where most beneficial. These algorithms, in turn, are conveniently expressed using pointer-based data structures such as linked lists, trees, and sparse matrices. <p> A synthetic microkernel allows the easy modeling of a range of application behaviors encountered in irregular and adaptive applications <ref> [4, 5] </ref>.
Reference: [5] <author> L. Greengard and V. Rokhlin, </author> <title> A fast algorithm for particle simulations, </title> <journal> Journal of Computational Physics, </journal> <volume> vol. 73, </volume> <pages> pp. 32548, </pages> <year> 1987. </year>
Reference-contexts: State-of-the-art techniques for these problems rely on sophisticated adaptive and hierarchical algorithmic cores (e.g., adaptive mesh refinement [13], and tree based n-body codes <ref> [4, 5] </ref>) which reduce total computation by focusing effort where most beneficial. These algorithms, in turn, are conveniently expressed using pointer-based data structures such as linked lists, trees, and sparse matrices. <p> A synthetic microkernel allows the easy modeling of a range of application behaviors encountered in irregular and adaptive applications <ref> [4, 5] </ref>.
Reference: [6] <author> M. Haines, D. Cronk, and P. Mehrotra, </author> <title> On the design of Chant: A talking threads package, </title> <booktitle> in Proceedings of Supercomputing'94, </booktitle> <year> 1994, </year> <pages> pp. 350359. </pages>
Reference-contexts: is comparable to the scheduling and synchronization of a Concert thread. 33 Heap-allocated contexts enable thread creation, switching, and synchronization deletion costs of less than 1s on the T3D and less than 0:2s on the Origin, an order of magnitude cheaper than corresponding primitives in multithreading systems such as Chant <ref> [6] </ref> and Nexus [13]. These latter systems require a dedicated stack per thread because they rely on a sequential compiler infrastructure that does not provide any support for saving and restoring thread state across suspensions. Communication. <p> Related work can be organized into two major categories: medium-grained and fine-grained approaches. Medium-grained approaches require programmers to explicitly manage placement of threads <ref> [6, 13, 117] </ref>, which freezes the load balancing structure into the program, or incur high overheads due to 44 exchanges of load information between processors [10, 118120], which makes them unsuitable for fine-grained computations. <p> Therefore, load balancing mechanisms must preserve data locality to achieve execution efficiency. * Priority sensitivity: Some irregular search applications require prioritization of thread execution, so load balancing mechanisms must preserve these priorities to avoid creating redundant work. Previous solutions <ref> [6, 7, 9, 10, 13, 24, 27, 117120] </ref> described in Section 3.3 are valid for systems which exhibit only a subset of these characteristics. <p> Previous solutions [6, 7, 9, 10, 13, 24, 27, 117120] described in Section 3.3 are valid for systems which exhibit only a subset of these characteristics. Most are either too heavyweight for use with fine-granularity threads <ref> [6, 10, 13, 117120] </ref>, completely ignore locality [7, 24], or work well only with regular computations [9, 10, 27]. <p> Load balancing policies. Many researchers have proposed a large number of load balancing policies based on object placement [162166], thread placement <ref> [6, 7, 9, 10, 24, 27, 82, 117120] </ref>, and thread ordering [167], which are sensitive to varying degrees to data locality. <p> With few exceptions, most policies are inappropriate for use in dynamic multithreaded computations because they are either too heavy-weight for use with fine-granularity threads <ref> [6, 10, 13, 117120] </ref>, completely ignore locality [7, 24], or work well only with regular computations [9, 10, 27]. In addition, a major philosophical difference of our approach relates to the fact that several of these policies have been proposed as a single unified solution for application wide load balancing.
Reference: [7] <author> R. D. Blumofe, C. F. Joerg, B. C. Kuszmaul, C. E. Leiserson, K. H. Randall, A. Shaw, and Y. Zhou, Cilk: </author> <title> An efficient multithreaded runtime system, </title> <booktitle> in Proceedings of Principles and Practice of Parallel Programming, </booktitle> <year> 1995, </year> <pages> pp. </pages> <year> 207216. </year>
Reference-contexts: More efficient implementations [79, 12, 33, 95, 96] contain low-overhead communication and thread mechanisms. However, these techniques are tied to specific 43 language contexts (e.g., run to completion threads in functional <ref> [7] </ref> and message-driven [12, 95] lan-guages, and static thread scheduling in dataflow languages [8]) which prevents their applicability in our language context. <p> Fine-grained approaches have predominantly controlled thread placement by controlling the location of data objects and having threads execute local to data that they access [9, 19, 27], which yields poor performance if threads access objects in an unbalanced fashion. Randomized thread scheduling approaches <ref> [7, 24] </ref> control thread placement directly and are theoretically optimal [121, 122], but these approaches suffer from the practical drawback that they are locality oblivious. 3.3.4 Summary Previous approaches for thread and communication management, data locality, and load balance do not achieve good performance on dynamic multithreaded computations. <p> Therefore, load balancing mechanisms must preserve data locality to achieve execution efficiency. * Priority sensitivity: Some irregular search applications require prioritization of thread execution, so load balancing mechanisms must preserve these priorities to avoid creating redundant work. Previous solutions <ref> [6, 7, 9, 10, 13, 24, 27, 117120] </ref> described in Section 3.3 are valid for systems which exhibit only a subset of these characteristics. <p> Previous solutions [6, 7, 9, 10, 13, 24, 27, 117120] described in Section 3.3 are valid for systems which exhibit only a subset of these characteristics. Most are either too heavyweight for use with fine-granularity threads [6, 10, 13, 117120], completely ignore locality <ref> [7, 24] </ref>, or work well only with regular computations [9, 10, 27]. <p> Load balancing policies. Many researchers have proposed a large number of load balancing policies based on object placement [162166], thread placement <ref> [6, 7, 9, 10, 24, 27, 82, 117120] </ref>, and thread ordering [167], which are sensitive to varying degrees to data locality. <p> With few exceptions, most policies are inappropriate for use in dynamic multithreaded computations because they are either too heavy-weight for use with fine-granularity threads [6, 10, 13, 117120], completely ignore locality <ref> [7, 24] </ref>, or work well only with regular computations [9, 10, 27]. In addition, a major philosophical difference of our approach relates to the fact that several of these policies have been proposed as a single unified solution for application wide load balancing. <p> Several other researchers have also developed mechanisms towards this end. Systems such as the Threaded Abstract Machine (TAM) model [8], pSather [33], Prelude [96], Olden [9], and Cilk <ref> [7] </ref> contain low-overhead communication, thread, and load balancing mechanisms, while systems such as Shasta [108] and Tempest [107] contain low-overhead data locality mechanisms. The main difference of our mechanisms is the fact that they yield robust performance in the face of computation irregularity. <p> An interesting avenue for future exploration is how the tradeoffs between different load balancing policies (e.g., work sharing and work stealing) change in the context of a cluster environment. Researchers <ref> [7] </ref> have argued that work stealing provides a more robust alternative for utilizing resources which can change dynamically, but additional work is required to validate this hypothesis. 2.
Reference: [8] <author> D. Culler, A. Sah, K. E. Schauser, T. von Eicken, and J. Wawrzynek, </author> <title> Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract machine, </title> <booktitle> in Proceedings of the Fourth International Conference on Architectural Support for Programming Languages an Operating Systems, </booktitle> <year> 1991, </year> <pages> pp. 16475. </pages>
Reference-contexts: More efficient implementations [79, 12, 33, 95, 96] contain low-overhead communication and thread mechanisms. However, these techniques are tied to specific 43 language contexts (e.g., run to completion threads in functional [7] and message-driven [12, 95] lan-guages, and static thread scheduling in dataflow languages <ref> [8] </ref>) which prevents their applicability in our language context. Additionally, the techniques do not yield good performance in the face of computation irregularity. 3.3.2 Data locality Two popular approaches for run-time management of data locality in irregular applications are separable communication and distributed shared memory. <p> Several other researchers have also developed mechanisms towards this end. Systems such as the Threaded Abstract Machine (TAM) model <ref> [8] </ref>, pSather [33], Prelude [96], Olden [9], and Cilk [7] contain low-overhead communication, thread, and load balancing mechanisms, while systems such as Shasta [108] and Tempest [107] contain low-overhead data locality mechanisms.
Reference: [9] <author> A. Rogers, M. Carlisle, J. Reppy, and L. Hendren, </author> <title> Supporting dynamic data structures on distributed memory machines, </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> vol. 17, no. 2, </volume> <pages> pp. 233263, </pages> <year> 1995. </year>
Reference-contexts: thread creation costs as low as a C procedure call * A robust communication mechanism, pull messaging, which combines the advantages of distributed queuing and lazy receiver-initiated data transfer to provide performance which is virtually insensitive to output contention and unbalanced sharing These techniques differ from comparable mechanisms in literature <ref> [9, 2327] </ref> in their explicit design for robustness and ability to exploit different run-time locality and load balance situations. 3. <p> Fine-grained approaches have predominantly controlled thread placement by controlling the location of data objects and having threads execute local to data that they access <ref> [9, 19, 27] </ref>, which yields poor performance if threads access objects in an unbalanced fashion. <p> The idea of lazily creating threads as required by run-time situations can also be found in the work on Lazy Task Creation [24] and Leapfrogging [25] in the context of shared-memory machines, and Olden <ref> [9] </ref>, Stacklets [26], and StackThreads [27] in the context of distributed-memory machines. <p> Therefore, load balancing mechanisms must preserve data locality to achieve execution efficiency. * Priority sensitivity: Some irregular search applications require prioritization of thread execution, so load balancing mechanisms must preserve these priorities to avoid creating redundant work. Previous solutions <ref> [6, 7, 9, 10, 13, 24, 27, 117120] </ref> described in Section 3.3 are valid for systems which exhibit only a subset of these characteristics. <p> Most are either too heavyweight for use with fine-granularity threads [6, 10, 13, 117120], completely ignore locality [7, 24], or work well only with regular computations <ref> [9, 10, 27] </ref>. <p> Load balancing policies. Many researchers have proposed a large number of load balancing policies based on object placement [162166], thread placement <ref> [6, 7, 9, 10, 24, 27, 82, 117120] </ref>, and thread ordering [167], which are sensitive to varying degrees to data locality. <p> With few exceptions, most policies are inappropriate for use in dynamic multithreaded computations because they are either too heavy-weight for use with fine-granularity threads [6, 10, 13, 117120], completely ignore locality [7, 24], or work well only with regular computations <ref> [9, 10, 27] </ref>. In addition, a major philosophical difference of our approach relates to the fact that several of these policies have been proposed as a single unified solution for application wide load balancing. <p> Several other researchers have also developed mechanisms towards this end. Systems such as the Threaded Abstract Machine (TAM) model [8], pSather [33], Prelude [96], Olden <ref> [9] </ref>, and Cilk [7] contain low-overhead communication, thread, and load balancing mechanisms, while systems such as Shasta [108] and Tempest [107] contain low-overhead data locality mechanisms. The main difference of our mechanisms is the fact that they yield robust performance in the face of computation irregularity.
Reference: [10] <author> R. Chandra, A. Gupta, and J. L. Hennessy, </author> <title> Data locality and load balancing in COOL, </title> <booktitle> in Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <year> 1993, </year> <pages> pp. 249259. </pages>
Reference-contexts: Related work can be organized into two major categories: medium-grained and fine-grained approaches. Medium-grained approaches require programmers to explicitly manage placement of threads [6, 13, 117], which freezes the load balancing structure into the program, or incur high overheads due to 44 exchanges of load information between processors <ref> [10, 118120] </ref>, which makes them unsuitable for fine-grained computations. Fine-grained approaches have predominantly controlled thread placement by controlling the location of data objects and having threads execute local to data that they access [9, 19, 27], which yields poor performance if threads access objects in an unbalanced fashion. <p> Therefore, load balancing mechanisms must preserve data locality to achieve execution efficiency. * Priority sensitivity: Some irregular search applications require prioritization of thread execution, so load balancing mechanisms must preserve these priorities to avoid creating redundant work. Previous solutions <ref> [6, 7, 9, 10, 13, 24, 27, 117120] </ref> described in Section 3.3 are valid for systems which exhibit only a subset of these characteristics. <p> Previous solutions [6, 7, 9, 10, 13, 24, 27, 117120] described in Section 3.3 are valid for systems which exhibit only a subset of these characteristics. Most are either too heavyweight for use with fine-granularity threads <ref> [6, 10, 13, 117120] </ref>, completely ignore locality [7, 24], or work well only with regular computations [9, 10, 27]. <p> Most are either too heavyweight for use with fine-granularity threads [6, 10, 13, 117120], completely ignore locality [7, 24], or work well only with regular computations <ref> [9, 10, 27] </ref>. <p> Load balancing policies. Many researchers have proposed a large number of load balancing policies based on object placement [162166], thread placement <ref> [6, 7, 9, 10, 24, 27, 82, 117120] </ref>, and thread ordering [167], which are sensitive to varying degrees to data locality. <p> With few exceptions, most policies are inappropriate for use in dynamic multithreaded computations because they are either too heavy-weight for use with fine-granularity threads <ref> [6, 10, 13, 117120] </ref>, completely ignore locality [7, 24], or work well only with regular computations [9, 10, 27]. In addition, a major philosophical difference of our approach relates to the fact that several of these policies have been proposed as a single unified solution for application wide load balancing. <p> With few exceptions, most policies are inappropriate for use in dynamic multithreaded computations because they are either too heavy-weight for use with fine-granularity threads [6, 10, 13, 117120], completely ignore locality [7, 24], or work well only with regular computations <ref> [9, 10, 27] </ref>. In addition, a major philosophical difference of our approach relates to the fact that several of these policies have been proposed as a single unified solution for application wide load balancing.
Reference: [11] <author> V. W. Freeh, D. K. Lowenthal, and G. R. Andrews, </author> <title> Distributed filaments: Efficient fine-grain parallelism on a cluster of workstations, </title> <booktitle> in First Symposium on Operating Systems Design and Implementation, </booktitle> <year> 1994, </year> <pages> pp. </pages> <year> 201212. </year>
Reference-contexts: Distributed shared memory (DSM) approaches provide a shared address space on distributed memory computers without hardware support for cache-coherence. Both page-based <ref> [11, 100102] </ref> and object-based approaches [7779,103108], even those relying on weak consistency models [109], either incur high overhead or achieve poor performance in the presence of computation irregularity. <p> Related work on software DSM systems can be classified into three categories: weak consistency models, hybrid hardware-software approaches, and application-specific protocol customization. Weak consistency models. Several page-based <ref> [11, 101, 102] </ref> and object-based [7779, 106108] DSM systems have been proposed which rely on weak consistency models for performance. Such systems use request-reply protocols derived from hardware cache coherence systems, requiring prompt 122 servicing of coherence requests (similar to dedicated hardware cache controllers).
Reference: [12] <author> L. V. Kale and S. Krishnan, CHARM++: </author> <title> A portable concurrent object oriented system based on C++, </title> <booktitle> in Proceedings of OOPSLA'93, </booktitle> <year> 1993, </year> <pages> pp. 91108. 298 </pages>
Reference-contexts: On the other hand, high-level programming models based on dynamic thread creation and multi-threading [613] enable convenient expression of irregular parallel computations. These models form the basis for several concurrent object-oriented languages [1417] and message-driven systems <ref> [12] </ref>. Such models permit concurrency to be expressed in terms of arbitrary user-defined computation units (hereafter referred to as logical threads), which synchronize using object-level concurrency control. In addition, the models provide a global name space that simplifies management of data placement and distribution. <p> Fine-grained multithreaded systems, particularly those which implement concurrent object-oriented languages [16, 27, 93, 94] have focused more on language expression issues with less complete compilers and few large-scale experiments. More efficient implementations <ref> [79, 12, 33, 95, 96] </ref> contain low-overhead communication and thread mechanisms. However, these techniques are tied to specific 43 language contexts (e.g., run to completion threads in functional [7] and message-driven [12, 95] lan-guages, and static thread scheduling in dataflow languages [8]) which prevents their applicability in our language context. <p> More efficient implementations [79, 12, 33, 95, 96] contain low-overhead communication and thread mechanisms. However, these techniques are tied to specific 43 language contexts (e.g., run to completion threads in functional [7] and message-driven <ref> [12, 95] </ref> lan-guages, and static thread scheduling in dataflow languages [8]) which prevents their applicability in our language context.
Reference: [13] <author> I. Foster, C. Kesselman, R. Olson, and S. Tuecke, </author> <title> Nexus: An interoperability layer for parallel and distributed computer systems, </title> <institution> Argonne National Laboratory, </institution> <type> Tech. Rep. </type> <note> Version 1.3, </note> <year> 1993. </year>
Reference-contexts: INTRODUCTION Irregular and dynamic computations have seen widespread use in various applications such as molecular dynamics, particle simulations, and radiosity calculations that span a number of engineering and scientific domains. State-of-the-art techniques for these problems rely on sophisticated adaptive and hierarchical algorithmic cores (e.g., adaptive mesh refinement <ref> [13] </ref>, and tree based n-body codes [4, 5]) which reduce total computation by focusing effort where most beneficial. These algorithms, in turn, are conveniently expressed using pointer-based data structures such as linked lists, trees, and sparse matrices. <p> the scheduling and synchronization of a Concert thread. 33 Heap-allocated contexts enable thread creation, switching, and synchronization deletion costs of less than 1s on the T3D and less than 0:2s on the Origin, an order of magnitude cheaper than corresponding primitives in multithreading systems such as Chant [6] and Nexus <ref> [13] </ref>. These latter systems require a dedicated stack per thread because they rely on a sequential compiler infrastructure that does not provide any support for saving and restoring thread state across suspensions. Communication. Communication operations arise whenever a thread accesses a remote data object or interacts with a remote thread. <p> Related work can be organized into two major categories: medium-grained and fine-grained approaches. Medium-grained approaches require programmers to explicitly manage placement of threads <ref> [6, 13, 117] </ref>, which freezes the load balancing structure into the program, or incur high overheads due to 44 exchanges of load information between processors [10, 118120], which makes them unsuitable for fine-grained computations. <p> Therefore, load balancing mechanisms must preserve data locality to achieve execution efficiency. * Priority sensitivity: Some irregular search applications require prioritization of thread execution, so load balancing mechanisms must preserve these priorities to avoid creating redundant work. Previous solutions <ref> [6, 7, 9, 10, 13, 24, 27, 117120] </ref> described in Section 3.3 are valid for systems which exhibit only a subset of these characteristics. <p> Previous solutions [6, 7, 9, 10, 13, 24, 27, 117120] described in Section 3.3 are valid for systems which exhibit only a subset of these characteristics. Most are either too heavyweight for use with fine-granularity threads <ref> [6, 10, 13, 117120] </ref>, completely ignore locality [7, 24], or work well only with regular computations [9, 10, 27]. <p> With few exceptions, most policies are inappropriate for use in dynamic multithreaded computations because they are either too heavy-weight for use with fine-granularity threads <ref> [6, 10, 13, 117120] </ref>, completely ignore locality [7, 24], or work well only with regular computations [9, 10, 27]. In addition, a major philosophical difference of our approach relates to the fact that several of these policies have been proposed as a single unified solution for application wide load balancing.
Reference: [14] <author> A. Grimshaw, </author> <title> Easy-to-use object-oriented parallel processing with Mentat, </title> <journal> IEEE Computer, </journal> <volume> vol. 5, no. 26, </volume> <pages> pp. 3951, </pages> <year> 1993. </year>
Reference: [15] <author> A. A. Chien, </author> <title> Concurrent Aggregates: Supporting Modularity in Massively-Parallel Programs. </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: The Concert system provides a high-performance implementation platform for concurrent object-oriented languages on scalable parallel platforms. It currently supports two languages: Concurrent Aggregates <ref> [15] </ref> and Illinois Concert C++ [32] (the latter is described in more detail in Section 2.3). The system consists of an optimizing compiler and a high-performance run-time system that includes the mechanisms described in this thesis. <p> Explicit continuation passing can improve the composability of concurrent programs <ref> [15, 16] </ref>. However, when continuation passing occurs, invocations on the stack are complicated because the callee may want its continuation. If the call is being executed on the stack, the callee's continuation is implicit.
Reference: [16] <editor> A. Yonezawa, ed., </editor> <title> ABCL: An Object-Oriented Concurrent System. </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: These various kinds of placement control are illustrated in the code fragment below: Foo *obj1 = new ( LOCAL ) Foo (4); // object placement Foo *obj2 = new ( location of (obj1) ) Foo (5); // object collocation Accum *coll1 = new ( BLOCK ) Accum <ref> [16] </ref>; // collection distribution Data consistency. The data-consistency interface allows the language implementation to use relaxed consistency models for higher performance. This interface is expressed using a with local annotation to blocks which specifies a set of objects that need to be localized and provides information about their access semantics. <p> With few exceptions [88], communication operations are poorly integrated with ongoing computation and incur high overheads for frequent, small-sized communication operations. Fine-grained multithreaded systems, particularly those which implement concurrent object-oriented languages <ref> [16, 27, 93, 94] </ref> have focused more on language expression issues with less complete compilers and few large-scale experiments. More efficient implementations [79, 12, 33, 95, 96] contain low-overhead communication and thread mechanisms. <p> Explicit continuation passing can improve the composability of concurrent programs <ref> [15, 16] </ref>. However, when continuation passing occurs, invocations on the stack are complicated because the callee may want its continuation. If the call is being executed on the stack, the callee's continuation is implicit. <p> Various kinds of placement control are illustrated in the code fragment below: Foo *obj1 = new ( LOCAL ) Foo (4); // object placement Foo *obj2 = new ( location of (obj1) ) Foo (5); // object collocation Accum *coll1 = new ( BLOCK ) Accum <ref> [16] </ref>; // collection distribution Note that the above support can also be used to control data locality. Here, our interest is in using object placement as a wire frame that controls the processor where a thread executes. Thread placement. These mechanisms directly control the load balancing of the thread subset.
Reference: [17] <author> A. Chien and U. Reddy, </author> <title> ICC++ language definition. Concurrent Systems Architecture Group Memo, </title> <note> Also available from http://www-csag.cs.uiuc.edu, 1995. </note>
Reference-contexts: We then describe the expression of an example application program in ICC++ (Section 2.3.3) to demonstrate use of the language constructs and to highlight the programmability advantages of the concurrent object-oriented model. The reader is referred to <ref> [17, 32, 46] </ref> for a complete language definition. 2.3.1 Basic programming interface Concurrent statements.
Reference: [18] <author> J. Plevyak, </author> <title> Optimization of object-oriented and concurrent programs, </title> <type> Ph.D. dissertation, </type> <institution> University of Illinois, Urbana, IL. </institution> <year> 1996. </year>
Reference-contexts: Although the above problem can be addressed entirely at either the compiler or the run-time system level, a good solution must include both components in complementary roles. Static compiler techniques have been successful in achieving local efficiency at the single thread level <ref> [18] </ref> but, in general, have been insufficient for managing global resources (e.g., processors, memories, and caches) because of the dynamic nature of the computation model. Thus, compiler techniques enable, but do not achieve, parallel efficiency. <p> The Concert system is available for public use, along with accompanying documentation and reference programs from the following web site: http://www-csag.cs.uiuc.edu. The Concert system has been a vehicle for extensive research on compiler optimization and run-time techniques over the past five years <ref> [18, 6271] </ref>. These techniques represent a range of aggressive optimizations that have been used to demonstrate high performance in absolute terms on a wide range of applications [21, 70, 72, 73]. <p> techniques described in the rest of the thesis. 28 2.5.1 The Concert compiler The Concert compiler employs aggressive static global program analyses and transformations, exploiting information about the high-level semantics of the concurrent object-oriented programming model, to produce code which, on uniprocessor platforms, executes with efficiency comparable to C programs <ref> [18, 66] </ref>. The discussion here provides only an overview of the compiler structure and its state of the art optimization capabilities. The interested reader is referred to the web site listed above and the following papers and theses for additional details [18, 6269]. <p> The discussion here provides only an overview of the compiler structure and its state of the art optimization capabilities. The interested reader is referred to the web site listed above and the following papers and theses for additional details <ref> [18, 6269] </ref>. The global program analyses [18, 65] use a unified flow analysis framework to obtain a variety of information: types of variables to resolve dynamic dispatches prevalent in the object-oriented model, relative locality of objects (if specified at the language level), and container object relationships used in storage optimizations. <p> The discussion here provides only an overview of the compiler structure and its state of the art optimization capabilities. The interested reader is referred to the web site listed above and the following papers and theses for additional details [18, 6269]. The global program analyses <ref> [18, 65] </ref> use a unified flow analysis framework to obtain a variety of information: types of variables to resolve dynamic dispatches prevalent in the object-oriented model, relative locality of objects (if specified at the language level), and container object relationships used in storage optimizations. <p> In this case, we create both the callee's context as well as the continuation lazily. Our compiler selects the appropriate schema for each thread interaction based on a global flow analysis <ref> [18, 65] </ref> which conservatively determines the blocking and continuation requirements of each thread body [131]. A novel aspect of our hybrid stack-heap execution model is that it is implemented entirely in C, and consequently, is portable across a variety of parallel platforms.
Reference: [19] <author> A. Chien, V. Karamcheti, and J. Plevyak, </author> <title> The Concert systemcompiler and runtime support for efficient fine-grained concurrent object-oriented programs, </title> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <type> Tech. Rep. </type> <institution> UIUCDCS-R-93-1815, </institution> <year> 1993. </year>
Reference-contexts: Fine-grained approaches have predominantly controlled thread placement by controlling the location of data objects and having threads execute local to data that they access <ref> [9, 19, 27] </ref>, which yields poor performance if threads access objects in an unbalanced fashion.
Reference: [20] <author> A. Chien, V. Karamcheti, J. Plevyak, and D. Sahrawat, </author> <title> The Concert system: Compiler and runtime technology for efficient concurrent object-oriented programming, </title> <booktitle> in Proceedings of the Computing in Aerospace 9 Conference, </booktitle> <year> 1993, </year> <pages> pp. 709720. </pages>
Reference: [21] <author> A. Chien, J. Dolby, B. Ganguly, V. Karamcheti, and X. Zhang, </author> <title> Evaluating high level parallel programming support for irregular applications in ICC++, </title> <booktitle> in Proceedings of the International Scientific Computing in Object-oriented Parallel Environments Conference, </booktitle> <year> 1997, </year> <pages> pp. 121128. </pages>
Reference-contexts: The Concert system has been a vehicle for extensive research on compiler optimization and run-time techniques over the past five years [18, 6271]. These techniques represent a range of aggressive optimizations that have been used to demonstrate high performance in absolute terms on a wide range of applications <ref> [21, 70, 72, 73] </ref>. <p> For all the applications used in the study, the optimized parallel application versions exhibit identical basic program structure to their sequential counterparts, requiring changes to fewer than 5% of the source code lines (see Section 2.3.3 for an example). The reader is referred to <ref> [21] </ref> for a detailed discussion of the programming effort involved in optimizing data locality and load balancing for the application programs listed below.
Reference: [22] <author> A. Chien, J. Dolby, B. Ganguly, V. Karamcheti, and X. Zhang, </author> <title> Supporting high level programming with high performance: The Illinois Concert system, </title> <booktitle> in Proceedings of the Second International Workshop on High-level Parallel Programming Models and Supportive Environments, </booktitle> <year> 1997, </year> <pages> pp. 1524. </pages>
Reference: [23] <author> T. von Eicken, D. Culler, S. Goldstein, and K. Schauser, </author> <title> Active Messages: a mechanism for integrated communication and computation, </title> <booktitle> in Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1992, </year> <pages> pp. 256266. </pages>
Reference-contexts: primitive: typedef void function ptr (...); void send 4 (int rnode, function ptr *fptr, arg1, arg2, arg3, arg4); void send (int rnode, function ptr *fptr, void *buf, int size); int extract (); Each message send is associated with execution of a handler at the destination node (similar to Active Messages <ref> [23] </ref>). The send 4 is optimized for transferring a small number of register arguments (up to 4) to the destination node, while the send primitive is more general, accepting an arbitrary sized buffer and the message length as arguments. <p> In fact, we have found it effectively impossible to generate traffic loads which cause any input or output contention using pull-based messaging. 4.3.3 Related work Although several researchers have focused on improving the performance of point-to-point communication, both from a hardware [134, 141] and a software <ref> [23, 142, 143] </ref> perspective, we restrict our attention here to performance optimizations for multiparty communication. Decoupling activities on different processors is only an issue in the latter. Among hardware approaches, the Ultracomputer [144] provides a combining network which reduces output contention by combining fetch-and-op requests.
Reference: [24] <author> E. Mohr, D. Kranz, and R. Halstead Jr., </author> <title> Lazy task creation: A technique for increasing the granularity of parallel programs, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 2, no. 3, </volume> <pages> pp. 264280, </pages> <year> 1991. </year>
Reference-contexts: Fine-grained approaches have predominantly controlled thread placement by controlling the location of data objects and having threads execute local to data that they access [9, 19, 27], which yields poor performance if threads access objects in an unbalanced fashion. Randomized thread scheduling approaches <ref> [7, 24] </ref> control thread placement directly and are theoretically optimal [121, 122], but these approaches suffer from the practical drawback that they are locality oblivious. 3.3.4 Summary Previous approaches for thread and communication management, data locality, and load balance do not achieve good performance on dynamic multithreaded computations. <p> The idea of lazily creating threads as required by run-time situations can also be found in the work on Lazy Task Creation <ref> [24] </ref> and Leapfrogging [25] in the context of shared-memory machines, and Olden [9], Stacklets [26], and StackThreads [27] in the context of distributed-memory machines. <p> Therefore, load balancing mechanisms must preserve data locality to achieve execution efficiency. * Priority sensitivity: Some irregular search applications require prioritization of thread execution, so load balancing mechanisms must preserve these priorities to avoid creating redundant work. Previous solutions <ref> [6, 7, 9, 10, 13, 24, 27, 117120] </ref> described in Section 3.3 are valid for systems which exhibit only a subset of these characteristics. <p> Previous solutions [6, 7, 9, 10, 13, 24, 27, 117120] described in Section 3.3 are valid for systems which exhibit only a subset of these characteristics. Most are either too heavyweight for use with fine-granularity threads [6, 10, 13, 117120], completely ignore locality <ref> [7, 24] </ref>, or work well only with regular computations [9, 10, 27]. <p> Load balancing policies. Many researchers have proposed a large number of load balancing policies based on object placement [162166], thread placement <ref> [6, 7, 9, 10, 24, 27, 82, 117120] </ref>, and thread ordering [167], which are sensitive to varying degrees to data locality. <p> With few exceptions, most policies are inappropriate for use in dynamic multithreaded computations because they are either too heavy-weight for use with fine-granularity threads [6, 10, 13, 117120], completely ignore locality <ref> [7, 24] </ref>, or work well only with regular computations [9, 10, 27]. In addition, a major philosophical difference of our approach relates to the fact that several of these policies have been proposed as a single unified solution for application wide load balancing.
Reference: [25] <author> D. B. Wagner and B. G. Calder, </author> <title> Leapfrogging: A portable technique for implementing efficient futures, </title> <booktitle> in Proceedings of the Fifth ACM SIGPLAN Symposium on the Principles and Practice of Parallel Programming, </booktitle> <year> 1993, </year> <pages> pp. </pages> <year> 208217. </year>
Reference-contexts: The idea of lazily creating threads as required by run-time situations can also be found in the work on Lazy Task Creation [24] and Leapfrogging <ref> [25] </ref> in the context of shared-memory machines, and Olden [9], Stacklets [26], and StackThreads [27] in the context of distributed-memory machines.
Reference: [26] <author> S. C. Goldstein, K. E. Schauser, and D. Culler, </author> <title> Lazy threads: Implementing a fast parallel call, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 37, no. 1, </volume> <pages> pp. 520, </pages> <year> 1996. </year> <month> 299 </month>
Reference-contexts: The idea of lazily creating threads as required by run-time situations can also be found in the work on Lazy Task Creation [24] and Leapfrogging [25] in the context of shared-memory machines, and Olden [9], Stacklets <ref> [26] </ref>, and StackThreads [27] in the context of distributed-memory machines.
Reference: [27] <author> K. Taura, S. Matsuoka, and A. Yonezawa, StackThreads: </author> <title> An abstract machine for scheduling fine-grain threads on stock CPUs, </title> <booktitle> in Proceedings of Workshop on Theory and Practice of Parallel Programming, </booktitle> <year> 1994, </year> <pages> pp. 121136. </pages>
Reference-contexts: With few exceptions [88], communication operations are poorly integrated with ongoing computation and incur high overheads for frequent, small-sized communication operations. Fine-grained multithreaded systems, particularly those which implement concurrent object-oriented languages <ref> [16, 27, 93, 94] </ref> have focused more on language expression issues with less complete compilers and few large-scale experiments. More efficient implementations [79, 12, 33, 95, 96] contain low-overhead communication and thread mechanisms. <p> Fine-grained approaches have predominantly controlled thread placement by controlling the location of data objects and having threads execute local to data that they access <ref> [9, 19, 27] </ref>, which yields poor performance if threads access objects in an unbalanced fashion. <p> The idea of lazily creating threads as required by run-time situations can also be found in the work on Lazy Task Creation [24] and Leapfrogging [25] in the context of shared-memory machines, and Olden [9], Stacklets [26], and StackThreads <ref> [27] </ref> in the context of distributed-memory machines. <p> Therefore, load balancing mechanisms must preserve data locality to achieve execution efficiency. * Priority sensitivity: Some irregular search applications require prioritization of thread execution, so load balancing mechanisms must preserve these priorities to avoid creating redundant work. Previous solutions <ref> [6, 7, 9, 10, 13, 24, 27, 117120] </ref> described in Section 3.3 are valid for systems which exhibit only a subset of these characteristics. <p> Most are either too heavyweight for use with fine-granularity threads [6, 10, 13, 117120], completely ignore locality [7, 24], or work well only with regular computations <ref> [9, 10, 27] </ref>. <p> Load balancing policies. Many researchers have proposed a large number of load balancing policies based on object placement [162166], thread placement <ref> [6, 7, 9, 10, 24, 27, 82, 117120] </ref>, and thread ordering [167], which are sensitive to varying degrees to data locality. <p> With few exceptions, most policies are inappropriate for use in dynamic multithreaded computations because they are either too heavy-weight for use with fine-granularity threads [6, 10, 13, 117120], completely ignore locality [7, 24], or work well only with regular computations <ref> [9, 10, 27] </ref>. In addition, a major philosophical difference of our approach relates to the fact that several of these policies have been proposed as a single unified solution for application wide load balancing.
Reference: [28] <author> J. Carter, J. Bennett, and W. Zwaenepoel, </author> <title> Implementation and performance of Munin, </title> <booktitle> in Proceedings of the ACM Symposium on Operating Systems Principles, </booktitle> <year> 1991, </year> <pages> pp. 152164. </pages>
Reference-contexts: Both page-based [11, 100102] and object-based approaches [7779,103108], even those relying on weak consistency models [109], either incur high overhead or achieve poor performance in the presence of computation irregularity. Systems supporting protocol customization <ref> [28, 29, 107, 110, 111] </ref> suffer from the drawback that they are not prescriptive, providing little insight into how to optimize protocols for performance in the presence of irregularity. <p> Among other DSM approaches, techniques [148, 149] which optimize synchronous protocols by predicting object access patterns are less effective given a dynamic context. Recent systems <ref> [28, 30] </ref> which allow application customization of protocols provide mechanisms for improving performance, but offer little insight into how to build protocols for tolerating unresponsive processors. <p> Second, we have chosen to work within the constraints imposed by a general fine-grained software DSM consistency model (object consistency) and examined what it takes to get good performance for a range of application scenarios. Application-specific protocol customization. Systems such as Munin <ref> [28] </ref>, Poly-C [29], and Tempest [30], which allow application-specific customization of coherence protocols, are most similar to our approach. Munin and Poly-C allows custom coherence actions for shared regions by selecting from among a set of predefined coherence protocols that reflect common application sharing scenarios.
Reference: [29] <author> B. K. Totty, </author> <title> Tunable shared memory abstractions for distributed memory systems, </title> <type> Ph.D. dissertation, </type> <institution> University of Illinois, Urbana-Champaign, IL, </institution> <year> 1994. </year>
Reference-contexts: Both page-based [11, 100102] and object-based approaches [7779,103108], even those relying on weak consistency models [109], either incur high overhead or achieve poor performance in the presence of computation irregularity. Systems supporting protocol customization <ref> [28, 29, 107, 110, 111] </ref> suffer from the drawback that they are not prescriptive, providing little insight into how to optimize protocols for performance in the presence of irregularity. <p> Second, we have chosen to work within the constraints imposed by a general fine-grained software DSM consistency model (object consistency) and examined what it takes to get good performance for a range of application scenarios. Application-specific protocol customization. Systems such as Munin [28], Poly-C <ref> [29] </ref>, and Tempest [30], which allow application-specific customization of coherence protocols, are most similar to our approach. Munin and Poly-C allows custom coherence actions for shared regions by selecting from among a set of predefined coherence protocols that reflect common application sharing scenarios.
Reference: [30] <author> M. D. Hill, J. R. Larus, and D. A. Wood, </author> <title> Tempest: A substrate for portable parallel programs, </title> <booktitle> in Compcon, </booktitle> <year> 1995, </year> <pages> pp. 321324. </pages>
Reference-contexts: Among other DSM approaches, techniques [148, 149] which optimize synchronous protocols by predicting object access patterns are less effective given a dynamic context. Recent systems <ref> [28, 30] </ref> which allow application customization of protocols provide mechanisms for improving performance, but offer little insight into how to build protocols for tolerating unresponsive processors. <p> Second, we have chosen to work within the constraints imposed by a general fine-grained software DSM consistency model (object consistency) and examined what it takes to get good performance for a range of application scenarios. Application-specific protocol customization. Systems such as Munin [28], Poly-C [29], and Tempest <ref> [30] </ref>, which allow application-specific customization of coherence protocols, are most similar to our approach. Munin and Poly-C allows custom coherence actions for shared regions by selecting from among a set of predefined coherence protocols that reflect common application sharing scenarios. <p> Thus, an interesting future direction would be to look at designing an appropriate set of hardware primitives which would enable efficient, robust implementation of shared memory using view caching protocols. This is similar in spirit to the work on Pica [198] and Tempest <ref> [30, 185] </ref>, both of which define a set of primitives for parallel processing. Based on the experiments reported in Chapter 7, an example set of primitives might include support for fast access control, put/get operations, and asynchronous and synchronous updates of global directory state.
Reference: [31] <author> R. H. Halstead Jr., </author> <title> Multilisp: A language for concurrent symbolic computation, </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> vol. 7, no. 4, </volume> <pages> pp. 501538, </pages> <year> 1985. </year>
Reference-contexts: Concurrent statements specify non-binding (or elective) concurrency by specifying a partial order among program statements (typically, method invocations). Each method invocation corresponds to a logical thread of execution. Invocations need not block the caller thread: Synchronization between caller and callee threads is achieved via futures <ref> [31] </ref>. Concurrent objects are abstract data types which provide a consistent interface in a concurrent environment. Object-level concurrency control is enforced by selective processing of method invocations. Multiaccess object collections are object groups which are indexable using a single name. <p> The former two schemes were developed in the context of parallel languages with explicit futures <ref> [31] </ref>, and 4 The CP-MB fallback corresponds to context creations and scheduling for both caller and callee threads. 61 allow stealing previously deferred stack frames to adaptively control execution granularity and work dis-tribution. Hybrid stack-heap execution differs in that it allows eager work distribution.
Reference: [32] <author> A. A. Chien, U. S. Reddy, J. Plevyak, and J. Dolby, </author> <title> ICC++ a C++ dialect for high-performance parallel computation, </title> <booktitle> in Proceedings of the 2nd International Symposium on Object Technologies for Advanced Software, </booktitle> <year> 1996, </year> <pages> pp. 7695. </pages>
Reference-contexts: The single name allows a single-object abstraction to be replaced by a multiaccess abstraction (where concurrent operations can scale with the number of group objects), without changing its interface for the rest of the program. This general programming model forms the basis for several object-based parallel languages <ref> [12,15, 32, 33] </ref>. In Section 2.3, we describe a specific language, Illinois Concert C++ (ICC++), in more detail. <p> We then describe the expression of an example application program in ICC++ (Section 2.3.3) to demonstrate use of the language constructs and to highlight the programmability advantages of the concurrent object-oriented model. The reader is referred to <ref> [17, 32, 46] </ref> for a complete language definition. 2.3.1 Basic programming interface Concurrent statements. <p> The Concert system provides a high-performance implementation platform for concurrent object-oriented languages on scalable parallel platforms. It currently supports two languages: Concurrent Aggregates [15] and Illinois Concert C++ <ref> [32] </ref> (the latter is described in more detail in Section 2.3). The system consists of an optimizing compiler and a high-performance run-time system that includes the mechanisms described in this thesis.
Reference: [33] <author> S. Murer, J. A. Feldman, C.-C. Lim, and M.-M. Seidel, pSather: </author> <title> Layered extensions to an object-oriented language for efficient parallel computation, </title> <institution> International Computer Science Institute, Berkeley, CA, </institution> <type> Tech. Rep. </type> <institution> TR-93-028, </institution> <year> 1993. </year>
Reference-contexts: The single name allows a single-object abstraction to be replaced by a multiaccess abstraction (where concurrent operations can scale with the number of group objects), without changing its interface for the rest of the program. This general programming model forms the basis for several object-based parallel languages <ref> [12,15, 32, 33] </ref>. In Section 2.3, we describe a specific language, Illinois Concert C++ (ICC++), in more detail. <p> Fine-grained multithreaded systems, particularly those which implement concurrent object-oriented languages [16, 27, 93, 94] have focused more on language expression issues with less complete compilers and few large-scale experiments. More efficient implementations <ref> [79, 12, 33, 95, 96] </ref> contain low-overhead communication and thread mechanisms. However, these techniques are tied to specific 43 language contexts (e.g., run to completion threads in functional [7] and message-driven [12, 95] lan-guages, and static thread scheduling in dataflow languages [8]) which prevents their applicability in our language context. <p> Several other researchers have also developed mechanisms towards this end. Systems such as the Threaded Abstract Machine (TAM) model [8], pSather <ref> [33] </ref>, Prelude [96], Olden [9], and Cilk [7] contain low-overhead communication, thread, and load balancing mechanisms, while systems such as Shasta [108] and Tempest [107] contain low-overhead data locality mechanisms. The main difference of our mechanisms is the fact that they yield robust performance in the face of computation irregularity.
Reference: [34] <author> C. A. R. Hoare, </author> <title> Communicating sequential processes, </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> vol. 21, no. 8, </volume> <pages> pp. 666677, </pages> <year> 1978. </year>
Reference-contexts: In Section 2.3, we describe a specific language, Illinois Concert C++ (ICC++), in more detail. A theoretical basis for reasoning about the concurrent object-oriented model can be found in the work on Communicating Sequential Processes (CSP) <ref> [34] </ref> and Actors [35]. 2.1.2 High-level programming features The concurrent object-oriented model provides three high-level features which simplify the expression of dynamic, irregular applications. Object-oriented programming improves organization and modularity of programs, reducing the complexity of large parallel programs.
Reference: [35] <author> G. Agha, </author> <title> Actors: A Model of Concurrent Computation in Distributed Systems. </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: In Section 2.3, we describe a specific language, Illinois Concert C++ (ICC++), in more detail. A theoretical basis for reasoning about the concurrent object-oriented model can be found in the work on Communicating Sequential Processes (CSP) [34] and Actors <ref> [35] </ref>. 2.1.2 High-level programming features The concurrent object-oriented model provides three high-level features which simplify the expression of dynamic, irregular applications. Object-oriented programming improves organization and modularity of programs, reducing the complexity of large parallel programs.
Reference: [36] <author> B. J. Smith, </author> <title> A pipelined, shared resource MIMD computer, </title> <booktitle> in IEEE Proceeding of the International Conference on Parallel Processing, </booktitle> <year> 1978, </year> <pages> pp. 68. </pages>
Reference: [37] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith, </author> <title> The Tera computer system, </title> <booktitle> in International Conference on Supercomputing, </booktitle> <year> 1990, </year> <pages> pp. 16. </pages>
Reference: [38] <author> D. M. Tullsen, S. J. Eggers, and H. M. Levy, </author> <title> Simultaneous multithreading: Maximizing on-chip parallelism, </title> <booktitle> in 22nd Annual International Symposium on Computer Architecture, </booktitle> <year> 1995, </year> <pages> pp. 392 403. </pages>
Reference: [39] <institution> The Connection Machine CM-5 Technical Summary, 1991, Available from Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address>
Reference-contexts: Processor-network interface. Our parallel machine model encompasses a variety of communication architectures for the processor-network interface. Specifically, we consider three kinds of commu nication architectures: 1. Two-sided: A send-receive interface (e.g., the CM-5 <ref> [39] </ref>) where processor participation on both the sender and the receiver is required for the completion of a communication operation, 2.
Reference: [40] <institution> Cray T3D System Architecture Overview, </institution> <year> 1993, </year> <note> Available from Cray Research, </note> <institution> Inc., Eagan, MN. </institution>
Reference-contexts: Specifically, we consider three kinds of commu nication architectures: 1. Two-sided: A send-receive interface (e.g., the CM-5 [39]) where processor participation on both the sender and the receiver is required for the completion of a communication operation, 2. One-sided: A remote memory access interface (e.g., the Cray T3D <ref> [40] </ref>) where a processor can directly access memory on a remote node, but such accesses are not cache-coherent, and 3. <p> However, this ease of programming comes at the cost of requiring a more sophisticated implementation. 2.4 Hardware Platforms This research uses two hardware platformsthe Cray T3D <ref> [40] </ref> and the SGI Cray Origin 2000 [41] which possess the characteristics of the generic target architecture defined in Section 2.2.2. <p> Such support is available on the Cray T3D <ref> [40] </ref> and T3E [42]. On such organizations, both data transfer and atomic operations can be accomplished without involving the remote processor whose responsiveness is less of an issue. * CC-LD/ST is a zero-sided communication architecture that differs from LD/ST in two ways.
Reference: [41] <author> Origin Servers: </author> <title> Technical Overview of the Origin Family, 1996, Available from Silicon Graphics, </title> <publisher> Inc., </publisher> <address> Mountain View, CA. </address>
Reference-contexts: One-sided: A remote memory access interface (e.g., the Cray T3D [40]) where a processor can directly access memory on a remote node, but such accesses are not cache-coherent, and 3. Zero-sided: An interface supporting cache-coherent loads and stores (e.g., the SGI Cray Origin 2000 <ref> [41] </ref>) where data transfer between two memories can be achieved without requiring the explicit participation of either processor. 14 While the specific semantics of the communication architecture is flexible, we assume that access to some memory in the system is significantly faster than accessing other memory (e.g., there is a local <p> However, this ease of programming comes at the cost of requiring a more sophisticated implementation. 2.4 Hardware Platforms This research uses two hardware platformsthe Cray T3D [40] and the SGI Cray Origin 2000 <ref> [41] </ref> which possess the characteristics of the generic target architecture defined in Section 2.2.2. The choice of these two platforms was motivated mainly by the desire to evaluate the run-time mechanisms developed in this research on a range of communication architectures, specifically the two-sided, one-sided, and zero-sided architectures discussed earlier.
Reference: [42] <author> S. L. Scott, </author> <title> Synchronization and communication in the T3E multiprocessor, </title> <booktitle> in Architectural Support for Programming Languages and Operating Systems (ASPLOS-VII), </booktitle> <year> 1996, </year> <pages> pp. 2636. 300 </pages>
Reference-contexts: Such support is available on the Cray T3D [40] and T3E <ref> [42] </ref>. On such organizations, both data transfer and atomic operations can be accomplished without involving the remote processor whose responsiveness is less of an issue. * CC-LD/ST is a zero-sided communication architecture that differs from LD/ST in two ways.
Reference: [43] <institution> Convex Exemplar Architecture, 1993, Available from Convex Computer Corporation, Richard--son, TX. </institution>
Reference: [44] <institution> Scalable POWERparallel System, 1995, Available from IBM Corporation, </institution> <address> White Plains, NY. </address>
Reference-contexts: Table 2.2 Costs of basic Concert thread management primitives on the T3D and the Origin. Costs of comparable operations in the Nexus multithreading system [82] on the IBM SP-2 <ref> [44] </ref> are shown for comparison.
Reference: [45] <author> B. Stroustrup, </author> <title> The C++ Programming Language, 2nd ed. </title> <address> Reading, MA: </address> <publisher> Addison Wesley, </publisher> <year> 1991. </year>
Reference-contexts: ICC++ is an extension of the sequential programming language C++ <ref> [45] </ref>, augmenting the basic object-oriented programming system with concurrent statements, concurrent objects, and multiaccess object collections. We first describe the basic programming interface of ICC++ (Section 2.3.1), and its performance optimization interfaces (Section 2.3.2). <p> A description of these interfaces has been included here for completeness. Data placement. Data placement and collocation for individual objects can be specified using the standard C++ placement syntax to overload the new operator <ref> [45] </ref>; an implementation-specific function location of is assumed available that provides the run-time location of its argument. Data placement and alignment for object groups can also be managed through object collectionsthe user can specify either standard layouts (e.g., block, cyclic) or custom distributions at creation time.
Reference: [46] <author> A. Chien, J. Dolby, V. Karamcheti, J. Plevyak, and X. Zhang, </author> <title> The ICC++ reference manual. Concurrent Systems Architecture Group Memo. </title> <note> Available from http://www-csag.cs.uiuc.edu/, 1996. </note>
Reference-contexts: We then describe the expression of an example application program in ICC++ (Section 2.3.3) to demonstrate use of the language constructs and to highlight the programmability advantages of the concurrent object-oriented model. The reader is referred to <ref> [17, 32, 46] </ref> for a complete language definition. 2.3.1 Basic programming interface Concurrent statements.
Reference: [47] <author> P. Hanrahan, D. Salzman, and L. Aupperle, </author> <title> A rapid hierarchical radiosity algorithm, </title> <booktitle> Computer Graphics (Proc Siggraph), </booktitle> <volume> vol. 25, no. 4, </volume> <pages> pp. </pages> <address> 197206, </address> <year> 1991. </year>
Reference-contexts: The radiosity method computes the global illumination in a scene containing diffusely reflecting surfaces by expressing the radiosity of an elemental surface patch as a weighted linear function of the radiosities of all other patches. We base our implementation upon a hierarchical algorithm due to Han-rahan et al. <ref> [47] </ref> which starts with the initial patches comprising the scene and computes light transport between pairs of patches, hierarchically subdividing each patch as needed to ensure accuracy. Each patch maintains interaction lists of potentially visible neighbors. <p> Our application uses a sequential algorithm due to Hanrahan et al. <ref> [47] </ref>, modeled after hierarchical N-body methods.
Reference: [48] <author> R. H. Arpaci, D. E. Culler, A. Krishnamurthy, S. G. Steinberg, and K. Yelick, </author> <title> Empirical evaluation of the CRAY-T3D: A compiler perspective, </title> <booktitle> in Proceeedings of the International Symposium on Computer Architecture, </booktitle> <year> 1995, </year> <pages> pp. 320331. </pages>
Reference: [49] <author> R. Numrich, P. Springer, and J. Peterson, </author> <title> Measurement of communication rates on the cray T3D interconnection network, </title> <booktitle> in Proceedings of HPCN Europe'94, </booktitle> <volume> Vol. II, </volume> <year> 1994, </year> <pages> pp. 150157. </pages>
Reference: [50] <author> J. Laudon and D. Lenoski, </author> <title> The SGI Origin: A ccNUMA highly scalable server, </title> <journal> Computer Architecture News, </journal> <volume> vol. 25, no. 2, </volume> <pages> pp. 241251, </pages> <year> 1997. </year>
Reference-contexts: The Hub chip encapsulates the cache-coherence logic, as well as the processor and network interface logic <ref> [50, 57] </ref>. It provides data paths between the processors and the memory, as well as between the memory and the network. The cache coherence protocol works with 128-byte cache lines, and is similar to the Stanford DASH invalidation-based protocol [58]. <p> Message extracts require the destination processor to delink the buffer, and invoke the corresponding handler. Our implementation utilizes hardware support for uncached fetch-and-op operations <ref> [50] </ref> to reduce the locking overhead associated with enqueuing a buffer into the destination queue. 2 To reduce secondary cache misses associated with storing data into and retrieving data from message buffers, our implementation splits up the logically shared buffer pool into per-processor pools; a processor deallocates a recently read buffer <p> Such implementations explicitly manage the creation and distribution of radiosity, visibility, and error calculation tasks, relying on the underlying hardware support to coherently access the patch objects. Previously published speedup numbers (obtained from [172] and <ref> [50] </ref>) on the same data set show that the application achieves a speedup of 26 on 32 processors of the DASH [128] and a speedup of 11 on 16 processors of the Origin. 7.3.2 Data locality and load-balancing optimizations In contrast to the approach mentioned above, our focus is on achieving
Reference: [51] <author> S. Scott and G. Thorson, </author> <title> Optimized routing in the Cray T3D, </title> <booktitle> in Proceedings of the Parallel Computer Routing and Communication Workshop (PCRCW'94), </booktitle> <year> 1994, </year> <pages> pp. 281294. </pages>
Reference-contexts: the network into the node card, which for the purposes of this research, is the limiting factor. 24 2.4.1 Cray T3D The Cray T3D is a physically distributed memory machine with support for a shared address space, consisting of up to 512 nodes interconnected with a scalable three-dimensional torus network <ref> [51] </ref>. Each processing node consists of a 150 MHz DEC 21064 [52] processor (with an 8 KB direct-mapped L1 cache, and no L2 cache) and up to 64 MB of memory, extended with a shell of Cray custom circuitry, called the DTB Annex.
Reference: [52] <author> DECchip 21064-AA Microprocessor Hardware Reference Manual, </author> <year> 1992, </year> <institution> Available from Digital Equipment Corporation, Maynard, </institution> <address> MA. </address>
Reference-contexts: Each processing node consists of a 150 MHz DEC 21064 <ref> [52] </ref> processor (with an 8 KB direct-mapped L1 cache, and no L2 cache) and up to 64 MB of memory, extended with a shell of Cray custom circuitry, called the DTB Annex.
Reference: [53] <author> R. W. </author> <title> Numrich, The Cray T3D address space and how to use it, Cray Research, Inc., </title> <type> Tech. Rep., </type> <year> 1994. </year>
Reference-contexts: The T3D processing element structure is shown in queues, but because of the associated overheads, that hardware is relevant only for large messages. T3D nodes also support three special memory operations for synchronization and latency hiding: fetch-and-increment, atomic swap, and prefetch <ref> [53] </ref>. First, each processor has a fetch-and-increment register that can be read or written by other processors. Reading the fetch-and-increment register atomically increments the register's contents and returns the original contents to the requesting processor. <p> Message sends require the source processor to reserve one of the buffers at the destination, and transfer data into it. Message extracts require the destination processor to detect filled buffers 34 and invoke the corresponding handler. Our implementation makes use of hardware support for fetch--and-increment and remote memory access <ref> [53] </ref> to perform buffer management and data transfer without involving the destination processor. This decouples the sending processor from destination processor activity, improving communication performance. The fetch-and-increment register is used to index the preallocated buffers.
Reference: [54] <institution> CRAY T3D Software Overview Technical Note, </institution> <year> 1992, </year> <note> Available from Cray Research, </note> <institution> Inc., Eagan, MN. </institution>
Reference-contexts: This is most useful for masking remote 25 memory latency. These special operations all involve off-chip access, and thus are expensive (greater than 10 processor cycles). Each node of the Cray T3D runs a microkernel based upon the Cray UNICOS operating system <ref> [54] </ref>. The machine is operated in a space-shared fashion with each job getting a dedicated view of the machine. Thus, the architecture of the T3D has the following implications for performance.
Reference: [55] <author> M. Galles, Spider: </author> <title> A high-speed network interconnect, </title> <journal> IEEE Micro, </journal> <volume> vol. 17, no. 1, </volume> <pages> pp. 3439, </pages> <year> 1997. </year>
Reference-contexts: a very flexible interface for implementing the run-time communication and object-caching mechanisms. 2.4.2 SGI Cray Origin 2000 The SGI Cray Origin 2000 is a cache-coherent nonuniform memory access multiprocessor (CC-NUMA), consisting of up to 512 nodes interconnected by a scalable bristled hypercube network based on the SGI Spider router chip <ref> [55] </ref>. Although memory modules are physically distributed (being part of the processing node), the Origin provides global addressability, employing a distributed shared memory (DSM) architecture with cache-coherence maintained via a directory-based protocol. Figure 2.6 shows relevant portions of the Origin processing element structure.
Reference: [56] <author> K. C. Yeager, </author> <title> The MIPS R10000 superscalar microprocessor, </title> <journal> IEEE Micro, </journal> <volume> vol. 16, no. 2, </volume> <pages> pp. 2840, </pages> <year> 1996. </year>
Reference-contexts: Figure 2.6 shows relevant portions of the Origin processing element structure. Each node consists of one or two 195 MHz MIPS R10000 <ref> [56] </ref> processors (with a 32 KB two-way set associative L1 cache, and a 4 MB three-way set associative L2 cache) and up to 4 GB of coherent memory (along with its corresponding directory memory).
Reference: [57] <author> J. Laudon and D. Lenoski, </author> <title> System overview of the SGI Origin 200/2000 product line, </title> <booktitle> in Proceedings of COMPCON 97, </booktitle> <year> 1997, </year> <pages> pp. 150156. </pages>
Reference-contexts: The Hub chip encapsulates the cache-coherence logic, as well as the processor and network interface logic <ref> [50, 57] </ref>. It provides data paths between the processors and the memory, as well as between the memory and the network. The cache coherence protocol works with 128-byte cache lines, and is similar to the Stanford DASH invalidation-based protocol [58]. <p> Depending on the specific architectural parameters, this interface can model an I/O-mapped network interface processor as in the Myrinet network [127], or a hardware cache-coherent DSM multiprocessor with a custom protocol processor <ref> [57, 128130] </ref>. The above interfaces represent discrete design points on the continuum between distributed-memory machines and a completely hardware-assisted cache-coherent shared-memory machine.
Reference: [58] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy, </author> <title> The directory-based cache coherence protocol for the dash multiprocessor, </title> <booktitle> in Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <year> 1990, </year> <pages> pp. 148159. </pages>
Reference-contexts: It provides data paths between the processors and the memory, as well as between the memory and the network. The cache coherence protocol works with 128-byte cache lines, and is similar to the Stanford DASH invalidation-based protocol <ref> [58] </ref>. Enhancements include request forwarding to reduce the latency of interprocessor communication, a clean-exclusive state to minimize latency on read-modify-write operations, and cache dropping of clean-exclusive or shared data without notifying the directory to reduce use of memory/directory bandwidth.
Reference: [59] <author> W.-D. Weber, </author> <title> Scalable directories for cache-coherent shared-memory multiprocessors, </title> <type> Ph.D. dissertation, </type> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <year> 1993. </year> <month> 301 </month>
Reference-contexts: Origin supports a full bit-vector directory up to 64 nodes (128 processors), and beyond that chooses dynamically between a full bit vector and a coarse bit vector <ref> [59] </ref> scheme. The processor interface contains logic for implementing the request tracking for both processors, as well 26 as for generating interrupts to the processors. The network interface takes messages to and from the network and other parts of the Hub and is responsible for generating multiple unicast invalidation messages.
Reference: [60] <author> J. Dolby, </author> <title> ParaSight: A debugger for concurrent object-oriented programs, M.S. </title> <type> thesis, </type> <institution> Univer--sity of Illinois, Urbana, IL, </institution> <year> 1995. </year>
Reference-contexts: The system consists of an optimizing compiler and a high-performance run-time system that includes the mechanisms described in this thesis. In addition, it facilitates program development by including an emulator, a symbolic debugger (ParaSight <ref> [60] </ref>), and tools for performance monitoring [61]. The Concert system is available for public use, along with accompanying documentation and reference programs from the following web site: http://www-csag.cs.uiuc.edu.
Reference: [61] <author> A. A. Chien and J. Dolby, </author> <title> The Illinois Concert system: A problem-solving environment for irregular applications, </title> <booktitle> in Proceedings of DAGS'94, The Symposium on Parallel Computation and Problem Solving Environments., </booktitle> <year> 1994, </year> <pages> pp. 2129. </pages>
Reference-contexts: The system consists of an optimizing compiler and a high-performance run-time system that includes the mechanisms described in this thesis. In addition, it facilitates program development by including an emulator, a symbolic debugger (ParaSight [60]), and tools for performance monitoring <ref> [61] </ref>. The Concert system is available for public use, along with accompanying documentation and reference programs from the following web site: http://www-csag.cs.uiuc.edu. The Concert system has been a vehicle for extensive research on compiler optimization and run-time techniques over the past five years [18, 6271].
Reference: [62] <author> A. A. Chien, W. Feng, V. Karamcheti, and J. Plevyak, </author> <title> Techniques for efficient execution of fine-grained concurrent programs, </title> <booktitle> in Proceedings of the Fifth Workshop on Compilers and Languages for Parallel Computing, </booktitle> <year> 1992, </year> <pages> pp. 103113. </pages>
Reference: [63] <author> J. Plevyak, V. Karamcheti, and A. Chien, </author> <title> Analysis of dynamic structures for efficient parallel execution, </title> <booktitle> in Proceedings of the Sixth Workshop for Languages and Compilers for Parallel Machines, </booktitle> <year> 1993, </year> <pages> pp. 3756. </pages>
Reference-contexts: Finally, the above choices may need to be revised based upon execution behavior. The revision could happen while the program is executing or for additional program runs. Good starting points for the compiler-based approach are work on aliasing and structure analysis <ref> [63, 186189] </ref>, nonconcurrency analysis [190], commutativity analysis [191], and dynamic pointer alignment [69]. These describe compiler analyses for extracting information about object structure, thread interactions, their effect on object state, and the nature of object access respectively.
Reference: [64] <author> J. Plevyak and A. Chien, </author> <title> Incremental inference of concrete types, </title> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <type> Tech. Rep. </type> <institution> UIUCDCS-R-93-1829, </institution> <year> 1993. </year>
Reference: [65] <author> J. Plevyak and A. A. Chien, </author> <title> Precise concrete type inference of object-oriented programs, </title> <booktitle> in Proceedings of OOPSLA'94, Object-Oriented Programming Systems, Languages and Architectures, </booktitle> <year> 1994, </year> <pages> pp. 324340. </pages>
Reference-contexts: The discussion here provides only an overview of the compiler structure and its state of the art optimization capabilities. The interested reader is referred to the web site listed above and the following papers and theses for additional details [18, 6269]. The global program analyses <ref> [18, 65] </ref> use a unified flow analysis framework to obtain a variety of information: types of variables to resolve dynamic dispatches prevalent in the object-oriented model, relative locality of objects (if specified at the language level), and container object relationships used in storage optimizations. <p> In this case, we create both the callee's context as well as the continuation lazily. Our compiler selects the appropriate schema for each thread interaction based on a global flow analysis <ref> [18, 65] </ref> which conservatively determines the blocking and continuation requirements of each thread body [131]. A novel aspect of our hybrid stack-heap execution model is that it is implemented entirely in C, and consequently, is portable across a variety of parallel platforms.
Reference: [66] <author> J. Plevyak, X. Zhang, and A. A. Chien, </author> <title> Obtaining sequential efficiency in concurrent object-oriented programs, </title> <booktitle> in Proceedings of the ACM Symposium on the Principles of Programming Languages, </booktitle> <year> 1995, </year> <pages> pp. 311321. </pages>
Reference-contexts: techniques described in the rest of the thesis. 28 2.5.1 The Concert compiler The Concert compiler employs aggressive static global program analyses and transformations, exploiting information about the high-level semantics of the concurrent object-oriented programming model, to produce code which, on uniprocessor platforms, executes with efficiency comparable to C programs <ref> [18, 66] </ref>. The discussion here provides only an overview of the compiler structure and its state of the art optimization capabilities. The interested reader is referred to the web site listed above and the following papers and theses for additional details [18, 6269].
Reference: [67] <author> J. Plevyak and A. A. Chien, </author> <title> Type directed cloning for object-oriented programs, </title> <booktitle> in Proceedings of the Workshop for Languages and Compilers for Parallel Computing, </booktitle> <year> 1995, </year> <pages> pp. 566580. </pages>
Reference: [68] <author> J. Dolby, </author> <title> Automatic inline allocation of objects, </title> <booktitle> in Proceedings of the 1997 ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1997, </year> <pages> pp. 717. </pages>
Reference: [69] <author> X. Zhang and A. A. Chien, </author> <title> Dynamic pointer alignment: Tiling and communication optimizations for parallel pointer-based computations, </title> <booktitle> in Proceedings of ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <year> 1997, </year> <pages> pp. 3747. </pages>
Reference-contexts: The revision could happen while the program is executing or for additional program runs. Good starting points for the compiler-based approach are work on aliasing and structure analysis [63, 186189], nonconcurrency analysis [190], commutativity analysis [191], and dynamic pointer alignment <ref> [69] </ref>. These describe compiler analyses for extracting information about object structure, thread interactions, their effect on object state, and the nature of object access respectively.
Reference: [70] <author> V. Karamcheti, J. Plevyak, and A. A. Chien, </author> <title> Runtime mechanisms for efficient dynamic multi-threading, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 37, no. 1, </volume> <pages> pp. 2140, </pages> <year> 1996. </year>
Reference-contexts: The Concert system has been a vehicle for extensive research on compiler optimization and run-time techniques over the past five years [18, 6271]. These techniques represent a range of aggressive optimizations that have been used to demonstrate high performance in absolute terms on a wide range of applications <ref> [21, 70, 72, 73] </ref>.
Reference: [71] <author> V. Karamcheti and A. A. Chien, </author> <title> View caching: Efficient software shared memory for dynamic computations, </title> <booktitle> in Proceedings of the International Parallel Processing Symposium, </booktitle> <year> 1997, </year> <pages> pp. 483489. </pages>
Reference: [72] <author> X. Zhang, V. Karamcheti, T. Ng, and A. Chien, </author> <title> Optimizing COOP languages: Study of a protein dynamics program, </title> <booktitle> in IPPS'96, </booktitle> <year> 1996, </year> <pages> pp. 235240. </pages>
Reference-contexts: The Concert system has been a vehicle for extensive research on compiler optimization and run-time techniques over the past five years [18, 6271]. These techniques represent a range of aggressive optimizations that have been used to demonstrate high performance in absolute terms on a wide range of applications <ref> [21, 70, 72, 73] </ref>. <p> Table 7.2 lists some key statistics about the thread and object structure which characterize the application behavior qualitatively. We briefly discuss these characteristics below. 171 Table 7.1 The irregular applications suite. APPLICATION DESCRIPTION INPUT IC-Cedar protein dynamics <ref> [72] </ref> simulates protein macromolecular dynamics. myoglobin Radiosity computer graphics [168] computes illumination using the hierarchical radiosity method. room Grobner computational algebra [169] computes Gr obner basis. ae-4 Phylogeny evolutionary history [170] determines the evolutionary history of a set of species. prim.40 Table 7.2 Sequential execution profile of the ICC++ applications on <p> This separation has several advantages which contribute to efficiency, such as permitting the grouping of multiple communication requests, eliminating per-access synchronization, and reducing cache interference between computation and communication. In previous work <ref> [72] </ref>, we have shown that applying the inspector-executor approach to IC-Cedar results in a speedup of 32 on 64 nodes of the T3D on the same data set.
Reference: [73] <author> A. A. Chien, M. Straka, J. Dolby, V. Karamcheti, J. Plevyak, and X. Zhang, </author> <title> A case study in irregular parallel programming, </title> <booktitle> in Proceedings of the DIMACS Workshop on Specification of Parallel Algorithms, </booktitle> <year> 1994, </year> <pages> pp. 120. </pages>
Reference-contexts: The Concert system has been a vehicle for extensive research on compiler optimization and run-time techniques over the past five years [18, 6271]. These techniques represent a range of aggressive optimizations that have been used to demonstrate high performance in absolute terms on a wide range of applications <ref> [21, 70, 72, 73] </ref>.
Reference: [74] <author> S. Pakin, V. Karamcheti, and A. A. Chien, </author> <title> Fast Messages: Efficient, portable communication for workstation clusters and MPPs, </title> <journal> IEEE Concurrency, </journal> <volume> vol. 5, no. 2, </volume> <pages> pp. 6073, </pages> <year> 1997. </year> <month> 302 </month>
Reference-contexts: The run-time system for ICC++ consists of language-independent and language-dependent portions. The language-independent portion consists of four core modules: communication, thread management, memory management, and object caching. The communication module supports the Fast Messages interface <ref> [74] </ref>. The thread management module provides support for thread scheduling, quiescence detection, and custom user schedulers (see Chapter 6). The quiescence detection support employs a message-efficient algorithm by Sinha, Kale, and Ramkumar [75]. <p> Communication. Communication operations arise whenever a thread accesses a remote data object or interacts with a remote thread. We describe in turn the interface, implementation, and performance of Concert communication primitives. Interface. The Concert communication primitives use a subset of the low-level Fast Messages <ref> [74, 83] </ref> Version 1.0 interface.
Reference: [75] <author> A. B. Sinha, L. V. Kale, and B. Ramkumar, </author> <title> A dynamic and adaptive quiescence detection al-gorithm, Parallel Programming Laboratory, </title> <institution> Department of Computer Science , University of Illinois, Urbana, IL, </institution> <type> Tech. Rep. </type> <pages> 93-11, </pages> <year> 1993. </year>
Reference-contexts: The communication module supports the Fast Messages interface [74]. The thread management module provides support for thread scheduling, quiescence detection, and custom user schedulers (see Chapter 6). The quiescence detection support employs a message-efficient algorithm by Sinha, Kale, and Ramkumar <ref> [75] </ref>. The memory-management module supports allocation of memory blocks in a single address space, as well as aligned stripes across address spaces. In addition, it implements a distributed garbage-collection algorithm, based on a generalization of the Baker's Treadmill Algorithm [76], to reclaim unreferenced memory.
Reference: [76] <author> H. Baker, </author> <title> The Treadmill: Real-time garbage collection without motion sickness, </title> <journal> ACM SIG-PLAN Notices, </journal> <volume> vol. 27, no. 3, </volume> <pages> pp. 6670, </pages> <year> 1992. </year>
Reference-contexts: The memory-management module supports allocation of memory blocks in a single address space, as well as aligned stripes across address spaces. In addition, it implements a distributed garbage-collection algorithm, based on a generalization of the Baker's Treadmill Algorithm <ref> [76] </ref>, to reclaim unreferenced memory. The object caching module supports software distributed object memory based on object consistency [7779] and view caching protocols (see Chapter 5). All modules use a hierarchical design that reduces contention when used with multiple processors per address space.
Reference: [77] <author> B. N. Bershad, M. J. Zekauskas, and W. A. Sawdon, </author> <title> The Midway distributed shared memory system, </title> <booktitle> in Proceedings of COMPCON 1993, </booktitle> <year> 1993, </year> <pages> pp. 528537. </pages>
Reference-contexts: This allows the underlying implementation to delay propagating updates performed by a thread until another thread accesses the same object. Examples of such systems in literature include Shared Regions [150], Midway <ref> [77] </ref>, SAM [78], and CRL [79]. All these systems demarcate the region in which a shared data object is accessed with user annotations. In the context of the concurrent object-oriented programming model described in Section 2.1, methods naturally define the extent of access to a shared object. <p> Exploiting application knowledge. A central theme in our parallel efficiency mechanisms is the use of knowledge of application semantics to offset performance degradation due to computation irregularity. While other researchers have eschewed the use of such information, relying instead of application oblivious data locality <ref> [77, 79, 107, 108] </ref> and load balancing techniques [121, 122], none of these systems have published results showing good performance with irregular applications. We believe that knowledge of application semantics is a requirement in the latter context and must be exploited by the language implementation to improve efficiency.
Reference: [78] <author> D. J. Scales and M. S. Lam, </author> <title> The design and evaluation of a shared object system for distributed memory machines, </title> <booktitle> in First Symposium on Operating Systems Design and Implementation, </booktitle> <year> 1994, </year> <pages> pp. 101114. </pages>
Reference-contexts: This allows the underlying implementation to delay propagating updates performed by a thread until another thread accesses the same object. Examples of such systems in literature include Shared Regions [150], Midway [77], SAM <ref> [78] </ref>, and CRL [79]. All these systems demarcate the region in which a shared data object is accessed with user annotations. In the context of the concurrent object-oriented programming model described in Section 2.1, methods naturally define the extent of access to a shared object. <p> As with the overhead, occupancy costs are also split into requester, directory, and remote node components. The data in the tables show that overall, our implementations of software object shared memory on the four communication architecture interfaces are competitive with the best reported in literature <ref> [78, 79, 108] </ref>. The following specific observations can be made about the different organizations. MSG. This implementation incurs the largest latency, overhead, and occupancy, all of which are attributable to the least amount of hardware support.
Reference: [79] <author> K. L. Johnson, M. F. Kaashoek, and D. A. Wallach, </author> <title> CRL: High-performance all-software distributed shared memory, </title> <booktitle> in Proceedings of the Symposium on Operating Systems Principles, </booktitle> <year> 1995, </year> <pages> pp. 213228. </pages>
Reference-contexts: Fine-grained multithreaded systems, particularly those which implement concurrent object-oriented languages [16, 27, 93, 94] have focused more on language expression issues with less complete compilers and few large-scale experiments. More efficient implementations <ref> [79, 12, 33, 95, 96] </ref> contain low-overhead communication and thread mechanisms. However, these techniques are tied to specific 43 language contexts (e.g., run to completion threads in functional [7] and message-driven [12, 95] lan-guages, and static thread scheduling in dataflow languages [8]) which prevents their applicability in our language context. <p> We first describe a state-of-the-art object shared memory system, the C Region Library, which provides the implementation context for this work. We then describe a synthetic sharing microkernel which provides the evaluation basis. 5.2.1 Object-consistent DSM: The C Region Library The C Region Library (CRL) <ref> [79] </ref> is an efficient implementation of a state of the art object shared memory system. Although the source code of CRL Version 1.0 is available, we have chosen to reimple-ment the library. <p> This allows the underlying implementation to delay propagating updates performed by a thread until another thread accesses the same object. Examples of such systems in literature include Shared Regions [150], Midway [77], SAM [78], and CRL <ref> [79] </ref>. All these systems demarcate the region in which a shared data object is accessed with user annotations. In the context of the concurrent object-oriented programming model described in Section 2.1, methods naturally define the extent of access to a shared object. <p> Messages from the home node to the remote node include invalidation request messages (MsgRInvalidate, MsgWInvalidate), and responses to the access requests (MsgSharedAckData, MsgExclusiveAckData, MsgModifyAck, and MsgModifyAckData). The simple interface and coherence protocol together contribute to an efficient implementation <ref> [79] </ref>. Specifically, the protocol is implemented using indirect calls associated with local call events and message events. The event identifier is used to index through a function table which contains an entry for each possible kind of event. <p> As with the overhead, occupancy costs are also split into requester, directory, and remote node components. The data in the tables show that overall, our implementations of software object shared memory on the four communication architecture interfaces are competitive with the best reported in literature <ref> [78, 79, 108] </ref>. The following specific observations can be made about the different organizations. MSG. This implementation incurs the largest latency, overhead, and occupancy, all of which are attributable to the least amount of hardware support. <p> Exploiting application knowledge. A central theme in our parallel efficiency mechanisms is the use of knowledge of application semantics to offset performance degradation due to computation irregularity. While other researchers have eschewed the use of such information, relying instead of application oblivious data locality <ref> [77, 79, 107, 108] </ref> and load balancing techniques [121, 122], none of these systems have published results showing good performance with irregular applications. We believe that knowledge of application semantics is a requirement in the latter context and must be exploited by the language implementation to improve efficiency.
Reference: [80] <author> Message Passing Interface Forum, </author> <title> MPI: A message passing interface standard, </title> <institution> University of Tennessee, Knoxville, </institution> <type> Tech. Rep. </type> <note> Version 1.1, </note> <year> 1995. </year>
Reference-contexts: These primitives represent a high-performance implementation of typical operations found in standard communication and thread libraries <ref> [80, 81] </ref>, and provide a baseline for evaluating the performance advantages of techniques described in the rest of the thesis. <p> All such systems provide support for thread management and communication and can be broadly classified into two groups: coarse-grained multithreaded systems, and fine-grained multithreaded systems. Coarse-grained multithreaded systems, which include standard communication <ref> [80, 87] </ref> and thread libraries [81], unified run-time systems [6,82,88, 89], or complete programming systems [10,14,9092] all suffer from the drawback that their thread overheads are high, in the range of tens of microseconds, limiting their applicability to only coarse-grained irregular applications.
Reference: [81] <institution> Thread extensions for portable operating systems. IEEE POSIX 1003.1.c Standard, </institution> <year> 1995. </year>
Reference-contexts: These primitives represent a high-performance implementation of typical operations found in standard communication and thread libraries <ref> [80, 81] </ref>, and provide a baseline for evaluating the performance advantages of techniques described in the rest of the thesis. <p> All such systems provide support for thread management and communication and can be broadly classified into two groups: coarse-grained multithreaded systems, and fine-grained multithreaded systems. Coarse-grained multithreaded systems, which include standard communication [80, 87] and thread libraries <ref> [81] </ref>, unified run-time systems [6,82,88, 89], or complete programming systems [10,14,9092] all suffer from the drawback that their thread overheads are high, in the range of tens of microseconds, limiting their applicability to only coarse-grained irregular applications.
Reference: [82] <author> I. Foster, C. Kesselman, and S. Tuecke, </author> <title> The Nexus approach to integrating multithreading and communication, </title> <journal> J. Parallel and Distributed Computing, </journal> <volume> vol. 37, no. 1, </volume> <pages> pp. 7082, </pages> <year> 1996. </year>
Reference-contexts: Table 2.2 Costs of basic Concert thread management primitives on the T3D and the Origin. Costs of comparable operations in the Nexus multithreading system <ref> [82] </ref> on the IBM SP-2 [44] are shown for comparison. <p> Load balancing policies. Many researchers have proposed a large number of load balancing policies based on object placement [162166], thread placement <ref> [6, 7, 9, 10, 24, 27, 82, 117120] </ref>, and thread ordering [167], which are sensitive to varying degrees to data locality.
Reference: [83] <author> S. Pakin, M. Lauria, and A. Chien, </author> <title> High performance messaging on workstations: Illinois Fast Messages (FM) for Myrinet, </title> <booktitle> in Supercomputing '95, </booktitle> <year> 1995, </year> <pages> pp. 501511. </pages>
Reference-contexts: Communication. Communication operations arise whenever a thread accesses a remote data object or interacts with a remote thread. We describe in turn the interface, implementation, and performance of Concert communication primitives. Interface. The Concert communication primitives use a subset of the low-level Fast Messages <ref> [74, 83] </ref> Version 1.0 interface.
Reference: [84] <author> V. Karamcheti and A. A. Chien, </author> <title> A comparison of architectural support for messaging on the TMC CM-5 and the Cray T3D, </title> <booktitle> in Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1995, </year> <pages> pp. 298307. </pages>
Reference-contexts: These implementations represent traditional techniques for implementing low-overhead messaging layers on message-passing and shared-memory architectures respectively, and are described in more detail below. The T3D implementation of the Fast Messages interface <ref> [84] </ref> preallocates a set of message buffers on each processor. Message sends require the source processor to reserve one of the buffers at the destination, and transfer data into it. Message extracts require the destination processor to detect filled buffers 34 and invoke the corresponding handler. <p> This overhead difference is largely due to the high cost of interaction with the T3D prefetch queue: 0.1s (15 cycles) to issue a single word fetch, and 0.15s (20 cycles) to extract a word from the prefetch queue. Note however, that simple architectural improvements of the prefetch queue <ref> [84] </ref> can make the costs of pull messaging competitive with push messaging for all message sizes. 65 Table 4.3 Base source and destination overheads of the push and pull messaging schemes on the T3D. (all times in s).
Reference: [85] <author> V. Karamcheti and A. Chien, </author> <title> Concertefficient runtime support for concurrent object-oriented programming languages on stock hardware, </title> <booktitle> in Proceedings of Supercomputing '93, </booktitle> <year> 1993, </year> <pages> pp. 598607. </pages>
Reference-contexts: In addition, the compiler can customize the bundling of the run-time mechanisms affecting procedure call boundary crossings (e.g., the compiler can inline some of the run-time primitive calls), further reducing cost in situations where it has static information about thread interactions <ref> [85] </ref>.
Reference: [86] <author> L. V. Kale, B. Ramkumar, V. Saletore, and A. B. Sinha, </author> <booktitle> Prioritization in parallel symbolic computing, Lecture Notes in Computer Science, </booktitle> <volume> vol. 748, </volume> <pages> pp. 1241, </pages> <year> 1993. </year>
Reference-contexts: First, irregular applications exhibit an uneven distribution of work across the processors. Second, these threads exhibit wide variations in granularity. And third, in some search-based symbolic applica tions, thread execution must be prioritized to minimize redundant work <ref> [86] </ref>. * Unpredictable data access: Because shared object access is intertwined with the computation and may evolve as a result of it, irregular applications exhibit unpredictable and unbalanced patterns of shared data object access.
Reference: [87] <institution> PVM 3 User's Guide and Reference Manual, </institution> <note> 1994, </note> <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. </author> <type> Sunderam. </type> <institution> Oak Ridge National Laboratory, Oak Ridge, TN 37831. </institution>
Reference-contexts: All such systems provide support for thread management and communication and can be broadly classified into two groups: coarse-grained multithreaded systems, and fine-grained multithreaded systems. Coarse-grained multithreaded systems, which include standard communication <ref> [80, 87] </ref> and thread libraries [81], unified run-time systems [6,82,88, 89], or complete programming systems [10,14,9092] all suffer from the drawback that their thread overheads are high, in the range of tens of microseconds, limiting their applicability to only coarse-grained irregular applications.
Reference: [88] <author> L. V. Kale, M. Bhandarkar, N. Jagathesan, S. Krishnan, and J. M. Yelon, </author> <title> Converse: an in-teroperable framework for parallel programming, </title> <booktitle> in Proceedings of the International Parallel Processing Symposium, </booktitle> <year> 1996, </year> <pages> pp. 212217. </pages>
Reference-contexts: With few exceptions <ref> [88] </ref>, communication operations are poorly integrated with ongoing computation and incur high overheads for frequent, small-sized communication operations. Fine-grained multithreaded systems, particularly those which implement concurrent object-oriented languages [16, 27, 93, 94] have focused more on language expression issues with less complete compilers and few large-scale experiments. <p> The primary difference of our approach with respect to real-time and multimedia operating systems is that the integration between different schedulers is considerably more efficient, relying on non-preemptive scheduling of cooperating application threads. On the other hand, only a handful of programming systems, such as Presto [90] and Converse <ref> [88] </ref>, possess similar capabilities. While support for attaching custom schedulers in both Presto and Converse share similar objectives to our approach, there are important differences. First, the thread scheduler 166 interface in both systems is more suitable for medium-grained threads, requiring multiple indirect func-tion calls per scheduling operation.
Reference: [89] <institution> PORTS Consortium, The PORTS0 interface, Argonne National Laboratory, </institution> <type> Tech. Rep. </type> <note> Version 0.3, </note> <year> 1995. </year>
Reference-contexts: All such systems provide support for thread management and communication and can be broadly classified into two groups: coarse-grained multithreaded systems, and fine-grained multithreaded systems. Coarse-grained multithreaded systems, which include standard communication [80, 87] and thread libraries [81], unified run-time systems <ref> [6,82,88, 89] </ref>, or complete programming systems [10,14,9092] all suffer from the drawback that their thread overheads are high, in the range of tens of microseconds, limiting their applicability to only coarse-grained irregular applications.
Reference: [90] <author> B. Bershad, E. Lazowska, and H. Levy, </author> <title> Presto: A system for object-oriented parallel programming, </title> <journal> SoftwarePractice and Experience, </journal> <volume> vol. 18, no. 8, </volume> <pages> pp. 713732, </pages> <year> 1988. </year> <month> 303 </month>
Reference-contexts: The primary difference of our approach with respect to real-time and multimedia operating systems is that the integration between different schedulers is considerably more efficient, relying on non-preemptive scheduling of cooperating application threads. On the other hand, only a handful of programming systems, such as Presto <ref> [90] </ref> and Converse [88], possess similar capabilities. While support for attaching custom schedulers in both Presto and Converse share similar objectives to our approach, there are important differences.
Reference: [91] <author> A. Chatterjee, A. Khanna, and Y. Hung, ES-Kit: </author> <title> An object-oriented distributed system, </title> <journal> Con--currency Practice and Experience, </journal> <volume> vol. 3, no. 6, </volume> <pages> pp. 525540, </pages> <year> 1991. </year>
Reference: [92] <author> K. M. Chandy and C. Kesselman, </author> <title> Compositional C++: Compositional parallel programming, </title> <booktitle> in Proceedings of the Fifth Workshop on Compilers and Languages for Parallel Computing, </booktitle> <year> 1992, </year> <pages> pp. 114123. </pages>
Reference: [93] <author> A. Yonezawa, E. Shibayama, T. Takada, and Y. Honda, </author> <title> Object-oriented concurrent programming modelling and programming in an object-oriented concurrent language ABCL/1, in Object-Oriented Concurrent Programming, </title> <editor> A. Yonezawa and M. Tokoro, </editor> <booktitle> Eds., </booktitle> <pages> pp. 5589, </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: With few exceptions [88], communication operations are poorly integrated with ongoing computation and incur high overheads for frequent, small-sized communication operations. Fine-grained multithreaded systems, particularly those which implement concurrent object-oriented languages <ref> [16, 27, 93, 94] </ref> have focused more on language expression issues with less complete compilers and few large-scale experiments. More efficient implementations [79, 12, 33, 95, 96] contain low-overhead communication and thread mechanisms.
Reference: [94] <author> C. Houck and G. Agha, HAL: </author> <title> A high-level actor language and its distributed implementation, </title> <booktitle> in Proceedings of the 21st International Conference on Parallel Processing, </booktitle> <year> 1992, </year> <pages> pp. 158165. </pages>
Reference-contexts: With few exceptions [88], communication operations are poorly integrated with ongoing computation and incur high overheads for frequent, small-sized communication operations. Fine-grained multithreaded systems, particularly those which implement concurrent object-oriented languages <ref> [16, 27, 93, 94] </ref> have focused more on language expression issues with less complete compilers and few large-scale experiments. More efficient implementations [79, 12, 33, 95, 96] contain low-overhead communication and thread mechanisms.
Reference: [95] <author> L. V. Kale, </author> <booktitle> The Chare-Kernel parallel programming language, in Proceedings of the International Conference on Parallel Processing, </booktitle> <year> 1990, </year> <pages> pp. 1725. </pages>
Reference-contexts: Fine-grained multithreaded systems, particularly those which implement concurrent object-oriented languages [16, 27, 93, 94] have focused more on language expression issues with less complete compilers and few large-scale experiments. More efficient implementations <ref> [79, 12, 33, 95, 96] </ref> contain low-overhead communication and thread mechanisms. However, these techniques are tied to specific 43 language contexts (e.g., run to completion threads in functional [7] and message-driven [12, 95] lan-guages, and static thread scheduling in dataflow languages [8]) which prevents their applicability in our language context. <p> More efficient implementations [79, 12, 33, 95, 96] contain low-overhead communication and thread mechanisms. However, these techniques are tied to specific 43 language contexts (e.g., run to completion threads in functional [7] and message-driven <ref> [12, 95] </ref> lan-guages, and static thread scheduling in dataflow languages [8]) which prevents their applicability in our language context. <p> Munin and Poly-C allows custom coherence actions for shared regions by selecting from among a set of predefined coherence protocols that reflect common application sharing scenarios. A related approach is also employed in the Charm system <ref> [95] </ref>, which provides a predefined set of information-sharing mechanisms. The Tempest approach is more general, permitting an arbitrary user-defined coherence handler to be invoked to service an access request. View caching is very similar to the above systems, yet there are important differences.
Reference: [96] <author> W. C. Hsieh, P. Wang, and W. E. Weihl, </author> <title> Computation migration: Enhancing locality for distributed-memory parallel systems, </title> <booktitle> in Proceedings of the Fifth ACM SIGPLAN Symposium on the Principles and Practice of Parallel Programming, </booktitle> <year> 1993, </year> <pages> pp. 239248. </pages>
Reference-contexts: Fine-grained multithreaded systems, particularly those which implement concurrent object-oriented languages [16, 27, 93, 94] have focused more on language expression issues with less complete compilers and few large-scale experiments. More efficient implementations <ref> [79, 12, 33, 95, 96] </ref> contain low-overhead communication and thread mechanisms. However, these techniques are tied to specific 43 language contexts (e.g., run to completion threads in functional [7] and message-driven [12, 95] lan-guages, and static thread scheduling in dataflow languages [8]) which prevents their applicability in our language context. <p> Several other researchers have also developed mechanisms towards this end. Systems such as the Threaded Abstract Machine (TAM) model [8], pSather [33], Prelude <ref> [96] </ref>, Olden [9], and Cilk [7] contain low-overhead communication, thread, and load balancing mechanisms, while systems such as Shasta [108] and Tempest [107] contain low-overhead data locality mechanisms. The main difference of our mechanisms is the fact that they yield robust performance in the face of computation irregularity.
Reference: [97] <author> J. Saltz, K. Crowley, R. Mirchandaney, and H. Berryman, </author> <title> Run-time scheduling and execution of loops on message passing machines, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 8, </volume> <pages> pp. 303312, </pages> <year> 1990. </year>
Reference: [98] <author> S. Haranandani, J. Saltz, P. Mehrotra, and H. Berryman, </author> <title> Performance of hashed cache data migration schemes on multicomputers, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 12, </volume> <pages> pp. 415422, </pages> <year> 1991. </year>
Reference: [99] <author> S. D. Sharma, R. Ponnusamy, B. Moon, Y. shin Hwang, R. Das, and J. Saltz, </author> <title> Run-time and compile-time support for adaptive irregular problems, </title> <booktitle> in Proceedings of Supercomputing '94, </booktitle> <year> 1994, </year> <pages> pp. 97106. </pages>
Reference: [100] <author> K. Li and P. Hudak, </author> <title> Memory coherence in shared virtual memory systems, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 7, no. 4, </volume> <pages> pp. 321359, </pages> <year> 1989. </year>
Reference: [101] <author> P. Keleher, A. L. Cox, S. Dwarkadas, and W. Zwaenopol, Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems, </title> <booktitle> in Proceedings of the 1994 Winter Usenix Conference, </booktitle> <year> 1994, </year> <pages> pp. 115132. </pages>
Reference-contexts: Most DSM implementations are based upon synchronous (request-reply) operations, modeled after hardware cache-coherence protocols. While such protocols yield good performance with responsive message processing (as with dedicated cache controllers), they result in large idle times when that is not the case. Even state-of-the-art DSM implementations <ref> [7779, 101] </ref>, that rely on weak consistency models to reduce coherence traffic, retain the synchronous (request-reply) nature of hardware cache-coherence protocols, requiring responsive processing for performance. The requesting processor (requester) sends the request to the processor which keeps track of object copies (directory). <p> Related work on software DSM systems can be classified into three categories: weak consistency models, hybrid hardware-software approaches, and application-specific protocol customization. Weak consistency models. Several page-based <ref> [11, 101, 102] </ref> and object-based [7779, 106108] DSM systems have been proposed which rely on weak consistency models for performance. Such systems use request-reply protocols derived from hardware cache coherence systems, requiring prompt 122 servicing of coherence requests (similar to dedicated hardware cache controllers).
Reference: [102] <author> L. Iftode, C. Dubnicki, E. W. Felten, and K. Li, </author> <title> Improving release-consistent shared virtual memory using automatic update, </title> <booktitle> in Proceedings of the 2nd International Symposium on High Performance Computer Architecture, </booktitle> <year> 1996, </year> <pages> pp. 1425. </pages>
Reference-contexts: Systems supporting protocol customization [28, 29, 107, 110, 111] suffer from the drawback that they are not prescriptive, providing little insight into how to optimize protocols for performance in the presence of irregularity. Recently, researchers <ref> [102, 112, 113] </ref> have examined the use of additional hardware support, such as the communication architectures described in Section 2.2.2 to mitigate the problems of pure software DSM approaches. However, most of these efforts have focused on page-based systems. <p> Related work on software DSM systems can be classified into three categories: weak consistency models, hybrid hardware-software approaches, and application-specific protocol customization. Weak consistency models. Several page-based <ref> [11, 101, 102] </ref> and object-based [7779, 106108] DSM systems have been proposed which rely on weak consistency models for performance. Such systems use request-reply protocols derived from hardware cache coherence systems, requiring prompt 122 servicing of coherence requests (similar to dedicated hardware cache controllers). <p> These solutions eliminate coherence operations from the critical path and replace synchronous protocols with asynchronous ones. However, such optimizations are less effective in a dynamic context where predictive information about future object accesses is hard to obtain. Hybrid hardware-software approaches. Recent work by several researchers <ref> [102, 112, 113] </ref> has examined the use of hardware support for improving the performance of page-based software DSM systems. Specifically, these researchers have explored how coherence protocols (and often new consistency models) can be developed to take advantage of hardware features such as remote stores and reflective memory.
Reference: [103] <author> R. Raj, E. Tempero, H. Levy, A. Black, N. Hutchinson, and E. </author> <month> Jul, </month> <title> Emerald: A general-purpose programming language, </title> <journal> Software Practice and Experience, </journal> <volume> vol. 21, no. 1, </volume> <pages> pp. 91118, </pages> <year> 1991. </year>
Reference: [104] <author> J. S. Chase, F. G. Amador, E. Lazowska, H. Levy, and R. J. Littlefield, </author> <title> The Amber system: Parallel programming on a network of multiprocessors, </title> <booktitle> in Proceedings of Twelfth Symposium on Operating Systems Principles, </booktitle> <year> 1989, </year> <pages> pp. 14758. 304 </pages>
Reference: [105] <author> H. E. Bal, M. F. Kaashoek, and A. S. Tanenbaum, Orca: </author> <title> A language for parallel programming of distributed systems, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. 18, no. 3, </volume> <pages> pp. </pages> <address> 190205, </address> <year> 1992. </year>
Reference: [106] <author> M. J. Feeley and H. M. Levy, </author> <title> Distributed shared memory with versioned objects, </title> <booktitle> in Proceedings of OOPSLA, </booktitle> <year> 1992, </year> <pages> pp. 247262. </pages>
Reference: [107] <author> I. Schoinas, B. Falsafi, A. R. Lebeck, S. K. Reinhardt, J. R. Larus, and D. Wood, </author> <title> Fine-grain access control for distributed shared memory, </title> <booktitle> in ASPLOS-VI, </booktitle> <year> 1994, </year> <pages> pp. 297306. </pages>
Reference-contexts: Both page-based [11, 100102] and object-based approaches [7779,103108], even those relying on weak consistency models [109], either incur high overhead or achieve poor performance in the presence of computation irregularity. Systems supporting protocol customization <ref> [28, 29, 107, 110, 111] </ref> suffer from the drawback that they are not prescriptive, providing little insight into how to optimize protocols for performance in the presence of irregularity. <p> For IC-Cedar and phylogeny, performance on non-cache-coherent architectures is limited by the software overheads of access control, a situation that can be alleviated using finer grained access control methods <ref> [107, 108] </ref>. Individual impact. Figure 7.26 shows the individual performance impact of the view-caching mechanisms for each application on the Cray T3D and the SGI Origin 2000. The performance loss resulting from disabling the data locality mechanisms are shown by the shaded portion of the bar. <p> Several other researchers have also developed mechanisms towards this end. Systems such as the Threaded Abstract Machine (TAM) model [8], pSather [33], Prelude [96], Olden [9], and Cilk [7] contain low-overhead communication, thread, and load balancing mechanisms, while systems such as Shasta [108] and Tempest <ref> [107] </ref> contain low-overhead data locality mechanisms. The main difference of our mechanisms is the fact that they yield robust performance in the face of computation irregularity. As we saw in Chapters 4 and 5, mechanisms which are otherwise low-overhead can suffer significant performance degradation due to computation irregularity. <p> Exploiting application knowledge. A central theme in our parallel efficiency mechanisms is the use of knowledge of application semantics to offset performance degradation due to computation irregularity. While other researchers have eschewed the use of such information, relying instead of application oblivious data locality <ref> [77, 79, 107, 108] </ref> and load balancing techniques [121, 122], none of these systems have published results showing good performance with irregular applications. We believe that knowledge of application semantics is a requirement in the latter context and must be exploited by the language implementation to improve efficiency.
Reference: [108] <author> D. Scales, K. Gharachorloo, and C. Thekkath, </author> <title> Shasta: A low overhead, software-only approach for supporting fine-grain shared memory, </title> <booktitle> in Proceedings of ASPLOS-VII, </booktitle> <year> 1996, </year> <pages> pp. 174185. </pages>
Reference-contexts: The performance issues highlighted by microkernel experiments will be corroborated later for whole application programs in Chapter 7. 2 The CRL protocol does not exhibit the three-party optimization, common to several hardware coherence protocols [50,58] and some recent software protocols <ref> [108] </ref>, where a modified copy is directly forwarded from the previous (nonhome) owner to the current requester. However, this optimization still retains the synchronous nature of the coherence protocol and is subject to a similar degradation due to unresponsive processors. 83 Table 5.2 Summary of base CRL protocol states. <p> As with the overhead, occupancy costs are also split into requester, directory, and remote node components. The data in the tables show that overall, our implementations of software object shared memory on the four communication architecture interfaces are competitive with the best reported in literature <ref> [78, 79, 108] </ref>. The following specific observations can be made about the different organizations. MSG. This implementation incurs the largest latency, overhead, and occupancy, all of which are attributable to the least amount of hardware support. <p> For IC-Cedar and phylogeny, performance on non-cache-coherent architectures is limited by the software overheads of access control, a situation that can be alleviated using finer grained access control methods <ref> [107, 108] </ref>. Individual impact. Figure 7.26 shows the individual performance impact of the view-caching mechanisms for each application on the Cray T3D and the SGI Origin 2000. The performance loss resulting from disabling the data locality mechanisms are shown by the shaded portion of the bar. <p> Several other researchers have also developed mechanisms towards this end. Systems such as the Threaded Abstract Machine (TAM) model [8], pSather [33], Prelude [96], Olden [9], and Cilk [7] contain low-overhead communication, thread, and load balancing mechanisms, while systems such as Shasta <ref> [108] </ref> and Tempest [107] contain low-overhead data locality mechanisms. The main difference of our mechanisms is the fact that they yield robust performance in the face of computation irregularity. <p> Exploiting application knowledge. A central theme in our parallel efficiency mechanisms is the use of knowledge of application semantics to offset performance degradation due to computation irregularity. While other researchers have eschewed the use of such information, relying instead of application oblivious data locality <ref> [77, 79, 107, 108] </ref> and load balancing techniques [121, 122], none of these systems have published results showing good performance with irregular applications. We believe that knowledge of application semantics is a requirement in the latter context and must be exploited by the language implementation to improve efficiency.
Reference: [109] <author> K. Gharacharloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy, </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors, </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 1526, </pages> <year> 1990. </year>
Reference-contexts: Distributed shared memory (DSM) approaches provide a shared address space on distributed memory computers without hardware support for cache-coherence. Both page-based [11, 100102] and object-based approaches [7779,103108], even those relying on weak consistency models <ref> [109] </ref>, either incur high overhead or achieve poor performance in the presence of computation irregularity. Systems supporting protocol customization [28, 29, 107, 110, 111] suffer from the drawback that they are not prescriptive, providing little insight into how to optimize protocols for performance in the presence of irregularity.
Reference: [110] <author> J. Bennett, J. B. Carter, and W. Zwaenepoel, Munin: </author> <title> Distributed shared memory based on type-specific memory coherence, </title> <booktitle> in Proceedings of the Second ACM SIGPLAN Symposium on the Principles and Practice of Parallel Programming, </booktitle> <year> 1990, </year> <pages> pp. 168176. </pages>
Reference-contexts: Both page-based [11, 100102] and object-based approaches [7779,103108], even those relying on weak consistency models [109], either incur high overhead or achieve poor performance in the presence of computation irregularity. Systems supporting protocol customization <ref> [28, 29, 107, 110, 111] </ref> suffer from the drawback that they are not prescriptive, providing little insight into how to optimize protocols for performance in the presence of irregularity.
Reference: [111] <author> S. K. Reinhardt, </author> <title> Tempest interface specification. Revision 1.2.1, </title> <note> Available from http://www.cs.wisc.edu, 1995. </note>
Reference-contexts: Both page-based [11, 100102] and object-based approaches [7779,103108], even those relying on weak consistency models [109], either incur high overhead or achieve poor performance in the presence of computation irregularity. Systems supporting protocol customization <ref> [28, 29, 107, 110, 111] </ref> suffer from the drawback that they are not prescriptive, providing little insight into how to optimize protocols for performance in the presence of irregularity.
Reference: [112] <author> K. Petersen and K. Li, </author> <title> Multiprocessor cache coherence based on virtual memory support, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 29, no. 2, </volume> <pages> pp. 158178, </pages> <year> 1995. </year>
Reference-contexts: Systems supporting protocol customization [28, 29, 107, 110, 111] suffer from the drawback that they are not prescriptive, providing little insight into how to optimize protocols for performance in the presence of irregularity. Recently, researchers <ref> [102, 112, 113] </ref> have examined the use of additional hardware support, such as the communication architectures described in Section 2.2.2 to mitigate the problems of pure software DSM approaches. However, most of these efforts have focused on page-based systems. <p> These solutions eliminate coherence operations from the critical path and replace synchronous protocols with asynchronous ones. However, such optimizations are less effective in a dynamic context where predictive information about future object accesses is hard to obtain. Hybrid hardware-software approaches. Recent work by several researchers <ref> [102, 112, 113] </ref> has examined the use of hardware support for improving the performance of page-based software DSM systems. Specifically, these researchers have explored how coherence protocols (and often new consistency models) can be developed to take advantage of hardware features such as remote stores and reflective memory.
Reference: [113] <author> L. I. Kontothanassis and M. L. Scott, </author> <title> Using memory-mapped network interfaces to improve the performance of distributed shared memory, </title> <booktitle> in Proceedings of the 2nd International Symposium on High Performance Computer Architecture, </booktitle> <year> 1996, </year> <pages> pp. 166177. </pages>
Reference-contexts: Systems supporting protocol customization [28, 29, 107, 110, 111] suffer from the drawback that they are not prescriptive, providing little insight into how to optimize protocols for performance in the presence of irregularity. Recently, researchers <ref> [102, 112, 113] </ref> have examined the use of additional hardware support, such as the communication architectures described in Section 2.2.2 to mitigate the problems of pure software DSM approaches. However, most of these efforts have focused on page-based systems. <p> These solutions eliminate coherence operations from the critical path and replace synchronous protocols with asynchronous ones. However, such optimizations are less effective in a dynamic context where predictive information about future object accesses is hard to obtain. Hybrid hardware-software approaches. Recent work by several researchers <ref> [102, 112, 113] </ref> has examined the use of hardware support for improving the performance of page-based software DSM systems. Specifically, these researchers have explored how coherence protocols (and often new consistency models) can be developed to take advantage of hardware features such as remote stores and reflective memory.
Reference: [114] <author> J. A. Stankovic, </author> <title> Simulations of three adaptive, decentralized controlled, job scheduling algorithms, </title> <journal> Computer Networks, </journal> <volume> vol. 8, no. 3, </volume> <pages> pp. </pages> <address> 199217, </address> <year> 1984. </year>
Reference: [115] <author> Y. Wang and R. J. T. Morris, </author> <title> Load sharing in distributed systems, </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-34, no. 3, </volume> <pages> pp. </pages> <address> 204217, </address> <year> 1985. </year>
Reference: [116] <author> D. L. Eager, E. D. Lazowska, and J. Zahorjan, </author> <title> Adaptive load sharing in homogeneous distributed systems, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. SE-12, no. 5, </volume> <pages> pp. 662674, </pages> <year> 1986. </year>
Reference: [117] <author> C.-P. Wen, S. Chakrabarti, E. Deprit, A. Krishnamurthy, and K. Yelick, </author> <title> Run-time support for portable distributed data structures, </title> <booktitle> in Proceedins of Third Workshop on Languages, Compilers, and Run-Time Systems for Scalable Computers, </booktitle> <year> 1995, </year> <pages> pp. 111120. </pages>
Reference-contexts: Related work can be organized into two major categories: medium-grained and fine-grained approaches. Medium-grained approaches require programmers to explicitly manage placement of threads <ref> [6, 13, 117] </ref>, which freezes the load balancing structure into the program, or incur high overheads due to 44 exchanges of load information between processors [10, 118120], which makes them unsuitable for fine-grained computations. <p> For example, the inspector-executor approach uses a library to support iterated irregular communication (such as in finite element simulations) [174]. The Multipol library provides a set of tools to build irregular applications, but no uniform high level interface <ref> [117] </ref>. Libraries such as Sparspak, Kelp, and A++/P++ support particular classes of algorithms on sparse matrices and adaptive meshes [180,181]. These systems have demonstrated good performance on large-scale irregular problems by exploiting specific properties of the restricted problem subset. In contrast, our solution reflects a major philosophical difference in approach.
Reference: [118] <author> R. Keller and F. C. H. Lin, </author> <title> Simulated performance of a reduction based multiprocessor, </title> <journal> IEEE Computer, </journal> <volume> vol. 17, no. 7, </volume> <year> 1984. </year>
Reference: [119] <author> L. V. Kale, </author> <title> Comparing the performance of two dynamic load distribution methods, </title> <booktitle> in International Conference on Parallel Processing, </booktitle> <year> 1988, </year> <pages> pp. 812. </pages>
Reference: [120] <author> W. Shu and L. V. Kale, </author> <title> A dynamic scheduling strategy for the Chare Kernel system, </title> <booktitle> in Proceedings of Supercomputing'89, </booktitle> <year> 1989, </year> <pages> pp. 389398. 305 </pages>
Reference: [121] <author> R. M. Karp and Y. Zhang, </author> <title> Randomized parallel algorithms for backtrack search and branch--and-bound computation, </title> <journal> Journal of the Association of Computing Machinery, </journal> <volume> vol. 40, no. 3, </volume> <pages> pp. 765789, </pages> <year> 1993. </year>
Reference-contexts: Randomized thread scheduling approaches [7, 24] control thread placement directly and are theoretically optimal <ref> [121, 122] </ref>, but these approaches suffer from the practical drawback that they are locality oblivious. 3.3.4 Summary Previous approaches for thread and communication management, data locality, and load balance do not achieve good performance on dynamic multithreaded computations. <p> Table 6.1 Library of load balancing policies. CLASS POLICY DESCRIPTION Object Placement ObjectLocation Placement and collocation of individual data objects. GroupDistribution Distribution and alignment of an object collection. Thread Placement WorkSharing Thread scheduling using a sender-initiated randomized work sharing <ref> [121] </ref> scheduler. WorkSharingLocal Locality preserving variant of work sharing, which uses a user-supplied function to determine task distribution targets. WorkStealing Thread scheduling using a receiver-initiated work stealing [122] scheduler. Thread Ordering ObjectLocationPriority ObjectLocation augmented with a per-processor priority scheduler. GroupDistributionPriority GroupDistribution augmented with a per-processor priority scheduler. <p> All three policies explicitly control thread placement, with the latter two providing various levels of locality sensitivity. Semantics. The WorkSharing mechanism provides a sender-initiated randomized work sharing scheduler <ref> [121] </ref>: on creation, threads are distributed to a randomly selected processor from the entire set. WorkSharingLocal provides a locality-sensitive variant, where the choice of the target 145 processor is made according to a user-supplied hashing function applied to the arguments. <p> Related work on load-balancing techniques can be classified into three categories: theoretical results, load balancing policies, and extensible scheduling and load balancing frameworks. 165 Theoretical results. Both randomized work sharing <ref> [121] </ref> and work stealing [122] strategies have been shown to be optimal to within a constant factor in a probabilistic sense (i.e., the worst-case parallel time using such an algorithm is, with high probability, at most a constant times the optimal parallel time for a given task graph). <p> A central theme in our parallel efficiency mechanisms is the use of knowledge of application semantics to offset performance degradation due to computation irregularity. While other researchers have eschewed the use of such information, relying instead of application oblivious data locality [77, 79, 107, 108] and load balancing techniques <ref> [121, 122] </ref>, none of these systems have published results showing good performance with irregular applications. We believe that knowledge of application semantics is a requirement in the latter context and must be exploited by the language implementation to improve efficiency. <p> Designing online algorithms: Since our load balancing strategies deal with dynamic thread creation, it would be desirable to analyze the competitive ratio of these strategies to understand how far they are from the optimal. While the analysis of randomized work-sharing <ref> [121] </ref> and work-stealing [122] strategies are good starting points, they do not model data locality in the computation. The work on dag-consistency [161] is a promising step in the latter direction, but is still far from modeling general locality. 3.
Reference: [122] <author> R. D. Blumofe and C. E. Leiserson, </author> <title> Scheduling multithreaded computations by work stealing, </title> <booktitle> in Proceedings of the 35th Annual Symposium on Foundations of Computer Science (FOCS '94), </booktitle> <year> 1994, </year> <pages> pp. 356368. </pages>
Reference-contexts: Randomized thread scheduling approaches [7, 24] control thread placement directly and are theoretically optimal <ref> [121, 122] </ref>, but these approaches suffer from the practical drawback that they are locality oblivious. 3.3.4 Summary Previous approaches for thread and communication management, data locality, and load balance do not achieve good performance on dynamic multithreaded computations. <p> GroupDistribution Distribution and alignment of an object collection. Thread Placement WorkSharing Thread scheduling using a sender-initiated randomized work sharing [121] scheduler. WorkSharingLocal Locality preserving variant of work sharing, which uses a user-supplied function to determine task distribution targets. WorkStealing Thread scheduling using a receiver-initiated work stealing <ref> [122] </ref> scheduler. Thread Ordering ObjectLocationPriority ObjectLocation augmented with a per-processor priority scheduler. GroupDistributionPriority GroupDistribution augmented with a per-processor priority scheduler. WorkSharingPriority WorkSharing augmented with a per-processor priority scheduler. WorkSharingLocalPriority WorkSharingLocal augmented with a per-processor priority scheduler. WorkStealingPriority WorkStealing augmented with a per-processor priority scheduler. <p> Thus, WorkSharingLocal enhances data reuse by mapping all threads accessing the same set of objects to the same processor. WorkStealing provides a receiver-initiated load-balancing mechanism <ref> [122] </ref>. Threads are enqueued locally upon creation. When a scheduler becomes idle, it steals threads from schedulers on neighboring processors. Implementation. Implementations of WorkSharing and WorkSharingLocal first evaluate the target processor where to distribute the thread, and then send the thread arguments there using an asynchronous message send. <p> Related work on load-balancing techniques can be classified into three categories: theoretical results, load balancing policies, and extensible scheduling and load balancing frameworks. 165 Theoretical results. Both randomized work sharing [121] and work stealing <ref> [122] </ref> strategies have been shown to be optimal to within a constant factor in a probabilistic sense (i.e., the worst-case parallel time using such an algorithm is, with high probability, at most a constant times the optimal parallel time for a given task graph). <p> A central theme in our parallel efficiency mechanisms is the use of knowledge of application semantics to offset performance degradation due to computation irregularity. While other researchers have eschewed the use of such information, relying instead of application oblivious data locality [77, 79, 107, 108] and load balancing techniques <ref> [121, 122] </ref>, none of these systems have published results showing good performance with irregular applications. We believe that knowledge of application semantics is a requirement in the latter context and must be exploited by the language implementation to improve efficiency. <p> Designing online algorithms: Since our load balancing strategies deal with dynamic thread creation, it would be desirable to analyze the competitive ratio of these strategies to understand how far they are from the optimal. While the analysis of randomized work-sharing [121] and work-stealing <ref> [122] </ref> strategies are good starting points, they do not model data locality in the computation. The work on dag-consistency [161] is a promising step in the latter direction, but is still far from modeling general locality. 3.
Reference: [123] <author> M. A. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. W. Felten, and J. Sandberg, </author> <title> Virtual memory mapped network interface for the SHRIMP multicomputer, </title> <booktitle> in Proceeding of the International Symposium on Computer Architecture, </booktitle> <year> 1994, </year> <pages> pp. 142153. </pages>
Reference-contexts: Such support is present on the Princeton SHRIMP <ref> [123] </ref>, the DEC Memory Channel [124], the HP Hamlyn [125], and versions of the IBM SP-x [126]. <p> Similar hardware support is present in several existing parallel machines (including the T3D, the Shrimp multicomputer <ref> [123, 138] </ref>, and clusters based upon the DEC Memory Channel [124] and the HP Hamlyn [125,139] network interfaces), and a likely feature of future parallel machines.
Reference: [124] <author> R. B. Gillett, </author> <title> Memory Channel network for PCI, </title> <journal> IEEE Micro, </journal> <volume> vol. 16, no. 1, </volume> <pages> pp. 1218, </pages> <year> 1996. </year>
Reference-contexts: Such support is present on the Princeton SHRIMP [123], the DEC Memory Channel <ref> [124] </ref>, the HP Hamlyn [125], and versions of the IBM SP-x [126]. <p> Similar hardware support is present in several existing parallel machines (including the T3D, the Shrimp multicomputer [123, 138], and clusters based upon the DEC Memory Channel <ref> [124] </ref> and the HP Hamlyn [125,139] network interfaces), and a likely feature of future parallel machines. Our implementation of pull-based messaging on the T3D utilizes hardware support for atomic-swap to link in messages into the distributed message queue, allowing send operations to proceed at the maximum swap rate.
Reference: [125] <author> G. Buzzard, D. Jacobson, M. Mackey, S. Marovich, and J. Wilkes, </author> <title> An implementation of the Hamlyn sender-managed interface architecture, </title> <booktitle> in Proceedings of the 2nd Symposium on Operating Systems Design and Implementation (OSDI '96), </booktitle> <year> 1996, </year> <pages> pp. 245259. </pages>
Reference-contexts: Such support is present on the Princeton SHRIMP [123], the DEC Memory Channel [124], the HP Hamlyn <ref> [125] </ref>, and versions of the IBM SP-x [126].
Reference: [126] <institution> IBM POWERparallel System - SP2, Performance Measurements, </institution> <year> 1995, </year> <editor> J. M. Kuzela. </editor> <title> Power Parallel Division, IBM. </title>
Reference-contexts: Such support is present on the Princeton SHRIMP [123], the DEC Memory Channel [124], the HP Hamlyn [125], and versions of the IBM SP-x <ref> [126] </ref>. On such organizations, data transfer alone can be achieved without remote processor involvement, which is still required for atomic operations (shown by dashed lines). * LD/ST is a one-sided communication architecture that differs from PUT/GET in supporting non-cache-coherent stores, loads and atomic operations against remote memory.
Reference: [127] <author> N. J. Boden, D. Cohen, R. E. Felderman, A. E. Kulawik, C. L. Seitz, J. N. Seizovic, and W.-K. </author> <title> Su, </title> <journal> Myrineta gigabit-per-second local-area network, IEEE Micro, </journal> <volume> vol. 15, no. 1, </volume> <pages> pp. 2936, </pages> <year> 1995. </year>
Reference-contexts: Second, data transfer does not require active participation from either processor: the network interface processor can directly access memory. Depending on the specific architectural parameters, this interface can model an I/O-mapped network interface processor as in the Myrinet network <ref> [127] </ref>, or a hardware cache-coherent DSM multiprocessor with a custom protocol processor [57, 128130]. The above interfaces represent discrete design points on the continuum between distributed-memory machines and a completely hardware-assisted cache-coherent shared-memory machine.
Reference: [128] <author> D. Lenoski et al., </author> <title> The Stanford DASH Multiprocessor, </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 6379, </pages> <year> 1992. </year>
Reference-contexts: Previously published speedup numbers (obtained from [172] and [50]) on the same data set show that the application achieves a speedup of 26 on 32 processors of the DASH <ref> [128] </ref> and a speedup of 11 on 16 processors of the Origin. 7.3.2 Data locality and load-balancing optimizations In contrast to the approach mentioned above, our focus is on achieving good performance on a variety of communication architectures without altering the dynamic multithreaded expression of the program. <p> Hardware solutions for fine-grained computations. Several researchers have argued for including custom hardware capabilities such as multiple hardware contexts [133, 134], tightly integrated communication [134, 141, 182, 183], integrating thread creation with message reception [135, 136], and support for cache coherence <ref> [128, 129, 184,185] </ref> on the grounds that such support is essential for obtaining efficiency on fine-grained problems. While it cannot be denied that some problems require low-overhead hardware support, a large class of applications can be efficiently executed without any specialized support using an approach similar to 232 ours.
Reference: [129] <author> J. Kuskin et al., </author> <title> The Stanford FLASH Multiprocessor, </title> <booktitle> in Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <year> 1994, </year> <pages> pp. 302313. </pages>
Reference-contexts: Hardware solutions for fine-grained computations. Several researchers have argued for including custom hardware capabilities such as multiple hardware contexts [133, 134], tightly integrated communication [134, 141, 182, 183], integrating thread creation with message reception [135, 136], and support for cache coherence <ref> [128, 129, 184,185] </ref> on the grounds that such support is essential for obtaining efficiency on fine-grained problems. While it cannot be denied that some problems require low-overhead hardware support, a large class of applications can be efficiently executed without any specialized support using an approach similar to 232 ours.
Reference: [130] <author> J. Kubiatowicz and A. Agarwal, </author> <title> The anatomy of a message in the Alewife multiprocessor, </title> <booktitle> in Proceedings of the International Conference on Supercomputing, </booktitle> <year> 1993, </year> <pages> pp. </pages> <year> 195206. </year>
Reference: [131] <author> J. Plevyak, V. Karamcheti, X. Zhang, and A. Chien, </author> <title> A hybrid execution model for fine-grained languages on distributed memory multicomputers, </title> <booktitle> in Proceedings of Supercomputing'95, </booktitle> <year> 1995. </year>
Reference-contexts: In this case, we create both the callee's context as well as the continuation lazily. Our compiler selects the appropriate schema for each thread interaction based on a global flow analysis [18, 65] which conservatively determines the blocking and continuation requirements of each thread body <ref> [131] </ref>. A novel aspect of our hybrid stack-heap execution model is that it is implemented entirely in C, and consequently, is portable across a variety of parallel platforms. <p> Figure 4.7 shows the stack unwinding and linkages which need to be set up as part of the fallback. The reader is referred to <ref> [131] </ref> for additional details about the implementation. blocking, whereas the right figure shows the fallback when the callee requires the caller's continuation. 4.2.2 Primitive costs Table 4.2 presents the cost of the hybrid stack-heap invocation mechanisms for various caller-callee scenarios on the Cray T3D.
Reference: [132] <author> W. Horwat, A. Chien, and W. Dally, </author> <title> Experience with CST: </title> <booktitle> Programming and implementation, in Proceedings of the SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1989, </year> <pages> pp. 101109. </pages>
Reference-contexts: However, when continuation passing occurs, invocations on the stack are complicated because the callee may want its continuation. If the call is being executed on the stack, the callee's continuation is implicit. Since one of our goals is to execute forwarded invocations <ref> [132] </ref> on the stack, lazy allocation of the continuation is essential. As we shall see, allocation of a continuation also implies creation of the context in which the returned value will be stored.
Reference: [133] <author> A. Agarwal, J. Kubiatowicz, D. Kranz, B.-H. Lim, D. Yeung, G. D'Souza, and M. Parkin, Spar-cle: </author> <title> An evolutionary processor design for large-scale multiprocessors, </title> <journal> IEEE Micro, </journal> <volume> vol. 13, no. 3, </volume> <pages> pp. 4861, </pages> <year> 1993. </year>
Reference-contexts: incur substantial overhead if it blocks repeatedly incurring multiple fallbacks; thus, reverting to the parallel method after the first fallback is a good strategy, especially if several synchronizations are likely. 4.2.3 Related work Although there have been several hardware-based approaches for reducing thread management costs (e.g., providing multiple hardware contexts <ref> [133, 134] </ref> and integrating thread creation with message reception [135, 136]), we restrict our attention here to the more relevant software approaches which work with commodity hardware. <p> Hardware solutions for fine-grained computations. Several researchers have argued for including custom hardware capabilities such as multiple hardware contexts <ref> [133, 134] </ref>, tightly integrated communication [134, 141, 182, 183], integrating thread creation with message reception [135, 136], and support for cache coherence [128, 129, 184,185] on the grounds that such support is essential for obtaining efficiency on fine-grained problems.
Reference: [134] <author> W. J. Dally, A. Chien, S. Fiske, W. Horwat, J. Keen, M. Larivee, R. Lethin, P. Nuth, S. Wills, P. Carrick, and G. Fyler, </author> <title> The J-Machine: A fine-grain concurrent computer, </title> <booktitle> in Information Processing 89, Proceedings of the IFIP Congress, </booktitle> <year> 1989, </year> <pages> pp. 11471153. </pages>
Reference-contexts: incur substantial overhead if it blocks repeatedly incurring multiple fallbacks; thus, reverting to the parallel method after the first fallback is a good strategy, especially if several synchronizations are likely. 4.2.3 Related work Although there have been several hardware-based approaches for reducing thread management costs (e.g., providing multiple hardware contexts <ref> [133, 134] </ref> and integrating thread creation with message reception [135, 136]), we restrict our attention here to the more relevant software approaches which work with commodity hardware. <p> In fact, we have found it effectively impossible to generate traffic loads which cause any input or output contention using pull-based messaging. 4.3.3 Related work Although several researchers have focused on improving the performance of point-to-point communication, both from a hardware <ref> [134, 141] </ref> and a software [23, 142, 143] perspective, we restrict our attention here to performance optimizations for multiparty communication. Decoupling activities on different processors is only an issue in the latter. Among hardware approaches, the Ultracomputer [144] provides a combining network which reduces output contention by combining fetch-and-op requests. <p> Hardware solutions for fine-grained computations. Several researchers have argued for including custom hardware capabilities such as multiple hardware contexts <ref> [133, 134] </ref>, tightly integrated communication [134, 141, 182, 183], integrating thread creation with message reception [135, 136], and support for cache coherence [128, 129, 184,185] on the grounds that such support is essential for obtaining efficiency on fine-grained problems. <p> Hardware solutions for fine-grained computations. Several researchers have argued for including custom hardware capabilities such as multiple hardware contexts [133, 134], tightly integrated communication <ref> [134, 141, 182, 183] </ref>, integrating thread creation with message reception [135, 136], and support for cache coherence [128, 129, 184,185] on the grounds that such support is essential for obtaining efficiency on fine-grained problems.
Reference: [135] <author> S. Sakai, Y. Yamaguchi, K. Hiraki, Y. Kodama, and T. Yuba, </author> <title> An architecture of a dataflow single chip processor, </title> <booktitle> in International Symposium on Computer Architecture, </booktitle> <year> 1989, </year> <pages> pp. 4653. 306 </pages>
Reference-contexts: fallbacks; thus, reverting to the parallel method after the first fallback is a good strategy, especially if several synchronizations are likely. 4.2.3 Related work Although there have been several hardware-based approaches for reducing thread management costs (e.g., providing multiple hardware contexts [133, 134] and integrating thread creation with message reception <ref> [135, 136] </ref>), we restrict our attention here to the more relevant software approaches which work with commodity hardware. <p> Hardware solutions for fine-grained computations. Several researchers have argued for including custom hardware capabilities such as multiple hardware contexts [133, 134], tightly integrated communication [134, 141, 182, 183], integrating thread creation with message reception <ref> [135, 136] </ref>, and support for cache coherence [128, 129, 184,185] on the grounds that such support is essential for obtaining efficiency on fine-grained problems.
Reference: [136] <author> R. S. Nikhil, G. M. Papadopoulos, and Arvind, </author> <title> *T: A multithreaded massively parallel architec-ture, </title> <booktitle> in International Symposium on Computer Architecture, </booktitle> <year> 1992, </year> <pages> pp. 156167. </pages>
Reference-contexts: fallbacks; thus, reverting to the parallel method after the first fallback is a good strategy, especially if several synchronizations are likely. 4.2.3 Related work Although there have been several hardware-based approaches for reducing thread management costs (e.g., providing multiple hardware contexts [133, 134] and integrating thread creation with message reception <ref> [135, 136] </ref>), we restrict our attention here to the more relevant software approaches which work with commodity hardware. <p> Hardware solutions for fine-grained computations. Several researchers have argued for including custom hardware capabilities such as multiple hardware contexts [133, 134], tightly integrated communication [134, 141, 182, 183], integrating thread creation with message reception <ref> [135, 136] </ref>, and support for cache coherence [128, 129, 184,185] on the grounds that such support is essential for obtaining efficiency on fine-grained problems.
Reference: [137] <author> K. Taura and A. Yonezawa, </author> <title> Fine-grain multithreading with minimal compiler support a cost effective approach to implementing efficient multithreading languages, </title> <booktitle> in Proceedings of the 1997 ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1997, </year> <pages> pp. 320333. </pages>
Reference-contexts: The latter three schemes all allocate new threads whenever the current one blocks due to a remote operation. Thus, unlike hybrid stack-heap execution, they use the same mechanism to support both work distribution and latency-hiding and cannot optimize their code for either situation. A more recent version of Stack-Threads <ref> [137] </ref> borrows heavily from the work reported here, incorporating several of the same ideas in a standalone C library that does not require specialized compiler support. 4.2.4 Summary The hybrid stack-heap run-time execution mechanisms permit the compiler to generate code which dynamically coalesces logical threads into larger-grained physical threads based on
Reference: [138] <author> M. A. Blumrich, C. Dubnicki, E. W. Felten, K. Li, and M. R. Mesarina, </author> <title> Virtual-memory-mapped network interfaces, </title> <booktitle> IEEE Micro, </booktitle> <pages> pp. 2128, </pages> <year> 1995. </year>
Reference-contexts: Similar hardware support is present in several existing parallel machines (including the T3D, the Shrimp multicomputer <ref> [123, 138] </ref>, and clusters based upon the DEC Memory Channel [124] and the HP Hamlyn [125,139] network interfaces), and a likely feature of future parallel machines.
Reference: [139] <author> G. Buzzard, D. Jacobson, S. Marovich, and J. Wilkes, Hamlyn: </author> <title> A high-performance network interface with sender-based memory management, </title> <booktitle> in Proceedings of the IEEE Hot Interconnects Symposium, </booktitle> <year> 1995. </year>
Reference: [140] <author> G. F. Pfister and V. A. Norton, </author> <title> Hot spot contention and combining in multistage interconnection networks, </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-34, no. 10, </volume> <pages> pp. 943948, </pages> <year> 1985. </year>
Reference-contexts: For 1024 bytes, the source overheads for the point-to-point case are 13.5 s, while they are 95 s for the hotspotfi16 pattern (3:1 degradation), and 455 s for the all-to-one pattern (15:1 degradation). This performance loss is due to the classic hot spot contention <ref> [140] </ref>, and will occur in any messaging layer which eagerly pushes messages to their destination. In contrast, pull messaging delivers performance robust to spatial irregularity, as seen by the close clustering of the curves for different traffic patterns in Figure 4.9.
Reference: [141] <author> S. Borkar, R. Cohn, G. Cox, T. Gross, H. T. Kung, M. Lam, M. Levine, B. Moore, W. Moore, C. Peterson, J. Susman, J. Sutton, J. Urbanski, and J. Webb, </author> <title> Supporting systolic and memory communication in iWarp, </title> <booktitle> in Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <year> 1990, </year> <pages> pp. 7081. </pages>
Reference-contexts: In fact, we have found it effectively impossible to generate traffic loads which cause any input or output contention using pull-based messaging. 4.3.3 Related work Although several researchers have focused on improving the performance of point-to-point communication, both from a hardware <ref> [134, 141] </ref> and a software [23, 142, 143] perspective, we restrict our attention here to performance optimizations for multiparty communication. Decoupling activities on different processors is only an issue in the latter. Among hardware approaches, the Ultracomputer [144] provides a combining network which reduces output contention by combining fetch-and-op requests. <p> Hardware solutions for fine-grained computations. Several researchers have argued for including custom hardware capabilities such as multiple hardware contexts [133, 134], tightly integrated communication <ref> [134, 141, 182, 183] </ref>, integrating thread creation with message reception [135, 136], and support for cache coherence [128, 129, 184,185] on the grounds that such support is essential for obtaining efficiency on fine-grained problems.
Reference: [142] <author> P. Druschel and L. L. Peterson, Fbufs: </author> <title> A high-bandwidth cross-domain transfer facility, </title> <booktitle> in Proceedings of Fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <year> 1993, </year> <pages> pp. 189 202. </pages>
Reference-contexts: In fact, we have found it effectively impossible to generate traffic loads which cause any input or output contention using pull-based messaging. 4.3.3 Related work Although several researchers have focused on improving the performance of point-to-point communication, both from a hardware [134, 141] and a software <ref> [23, 142, 143] </ref> perspective, we restrict our attention here to performance optimizations for multiparty communication. Decoupling activities on different processors is only an issue in the latter. Among hardware approaches, the Ultracomputer [144] provides a combining network which reduces output contention by combining fetch-and-op requests.
Reference: [143] <author> H. L. C. A. Thekkath and E. D. Lazowska, </author> <title> Separating data and control transfer in distributed operating systems, </title> <booktitle> in Proceedings of the Sixth Symposium on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <year> 1994, </year> <pages> pp. 211. </pages>
Reference-contexts: In fact, we have found it effectively impossible to generate traffic loads which cause any input or output contention using pull-based messaging. 4.3.3 Related work Although several researchers have focused on improving the performance of point-to-point communication, both from a hardware [134, 141] and a software <ref> [23, 142, 143] </ref> perspective, we restrict our attention here to performance optimizations for multiparty communication. Decoupling activities on different processors is only an issue in the latter. Among hardware approaches, the Ultracomputer [144] provides a combining network which reduces output contention by combining fetch-and-op requests.
Reference: [144] <author> A. Gottlieb et al., </author> <title> The NYU Ultracomputerdesigning an mimd shared memory parallel computer, </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-32, no. 2, </volume> <pages> pp. 175189, </pages> <year> 1983. </year>
Reference-contexts: Decoupling activities on different processors is only an issue in the latter. Among hardware approaches, the Ultracomputer <ref> [144] </ref> provides a combining network which reduces output contention by combining fetch-and-op requests. NIFDY [145] is a proposed network interface which limits the number of outstanding messages between pairs of processors to improve network performance even for unbalanced traffic.
Reference: [145] <author> T. Callahan and S. Goldstein, NIFDY: </author> <title> A low overhead, high throughput network interface, </title> <booktitle> in Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1995, </year> <pages> pp. 230241. </pages>
Reference-contexts: Decoupling activities on different processors is only an issue in the latter. Among hardware approaches, the Ultracomputer [144] provides a combining network which reduces output contention by combining fetch-and-op requests. NIFDY <ref> [145] </ref> is a proposed network interface which limits the number of outstanding messages between pairs of processors to improve network performance even for unbalanced traffic.
Reference: [146] <author> E. Anderson, A. Benzoni, J. Dongarra, S. Moulton, S. Ostrouchov, B. Tourancheau, and R. V. de Geijn, </author> <title> Basic linear algebra communication subprograms, </title> <booktitle> in Sixth Distributed Memory Computing Conference Proceedings, </booktitle> <year> 1991, </year> <pages> pp. 287290. </pages>
Reference-contexts: Pull messaging achieves the same effect as these systems, but with general-purpose hardware: remote memory access and synchronization hardware is available in several current parallel machines. Software approaches have focused for the most part on optimizing regular synchronous communication patterns, as evidenced by the work on collective operations <ref> [146] </ref>, and all-pairs messaging [147].
Reference: [147] <author> E. A. Brewer and B. C. Kuszmaul, </author> <title> How to get good performance from the CM-5 data network, </title> <booktitle> in Proceedings of the International Parallel Processing Symposium, </booktitle> <year> 1994, </year> <pages> pp. 858867. </pages>
Reference-contexts: Software approaches have focused for the most part on optimizing regular synchronous communication patterns, as evidenced by the work on collective operations [146], and all-pairs messaging <ref> [147] </ref>.
Reference: [148] <author> A. L. Cox and R. J. Fowler, </author> <title> Adaptive cache coherency for detecting migratory shared data, </title> <booktitle> in Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <year> 1993, </year> <pages> pp. 109118. </pages>
Reference-contexts: Among other DSM approaches, techniques <ref> [148, 149] </ref> which optimize synchronous protocols by predicting object access patterns are less effective given a dynamic context. Recent systems [28, 30] which allow application customization of protocols provide mechanisms for improving performance, but offer little insight into how to build protocols for tolerating unresponsive processors. <p> This sensitivity results in performance degradation on communication architectures such as MSG and PUT/GET where it increases the time processors idle awaiting completion of remote coherence actions. A class of DSM systems <ref> [148, 149, 152] </ref> predicts access requests and eagerly creates an object copy on the requesting processor. These solutions eliminate coherence operations from the critical path and replace synchronous protocols with asynchronous ones.
Reference: [149] <author> T. Mowry and A. Gupta, </author> <title> Tolerating latency through software-controlled prefetching in shareed-memory multiprocessors, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 12, </volume> <pages> pp. 87106, </pages> <year> 1991. </year>
Reference-contexts: Among other DSM approaches, techniques <ref> [148, 149] </ref> which optimize synchronous protocols by predicting object access patterns are less effective given a dynamic context. Recent systems [28, 30] which allow application customization of protocols provide mechanisms for improving performance, but offer little insight into how to build protocols for tolerating unresponsive processors. <p> This sensitivity results in performance degradation on communication architectures such as MSG and PUT/GET where it increases the time processors idle awaiting completion of remote coherence actions. A class of DSM systems <ref> [148, 149, 152] </ref> predicts access requests and eagerly creates an object copy on the requesting processor. These solutions eliminate coherence operations from the critical path and replace synchronous protocols with asynchronous ones.
Reference: [150] <author> H. S. Sandhu, B. Gamsa, and S. Zhou, </author> <title> The shared region approach to software cache coherence on multiprocessors, </title> <booktitle> in Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPOPP'93), </booktitle> <year> 1993, </year> <pages> pp. 229238. </pages>
Reference-contexts: This allows the underlying implementation to delay propagating updates performed by a thread until another thread accesses the same object. Examples of such systems in literature include Shared Regions <ref> [150] </ref>, Midway [77], SAM [78], and CRL [79]. All these systems demarcate the region in which a shared data object is accessed with user annotations. In the context of the concurrent object-oriented programming model described in Section 2.1, methods naturally define the extent of access to a shared object.
Reference: [151] <author> K. L. Johnson, </author> <title> High-performance all-software distributed shared memory, </title> <type> Ph.D. dissertation, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: The CRL coherence protocol can be described by state machines at the home and remote nodes. The interested reader is referred to <ref> [151] </ref> for a detailed description of the state machines. Here, we focus only on providing sufficient context to understand the different implementations discussed in the rest of this chapter. The states and events in the state machines are shown in Tables 5.2 and 5.3, respectively.
Reference: [152] <author> H. Cheong and A. V. Veidenbaum, </author> <title> Compiler-directed cache management in multiprocessors, </title> <journal> IEEE Computer, </journal> <volume> vol. 23, no. 6, </volume> <pages> pp. 3948, </pages> <year> 1990. </year>
Reference-contexts: This sensitivity results in performance degradation on communication architectures such as MSG and PUT/GET where it increases the time processors idle awaiting completion of remote coherence actions. A class of DSM systems <ref> [148, 149, 152] </ref> predicts access requests and eagerly creates an object copy on the requesting processor. These solutions eliminate coherence operations from the critical path and replace synchronous protocols with asynchronous ones.
Reference: [153] <author> M. R. Garey and D. S. Johnson, </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <address> San Francisco: </address> <publisher> Freeman, </publisher> <year> 1979. </year>
Reference-contexts: An execution schedule specifies both the processor on which a thread executes, and the order of its execution. 6.2.2 Intractability results We show that the general load-balancing (LB) problem as stated above is intractable by proving that two strictly simpler static formulations of the problem are NP-complete <ref> [153] </ref>: 1. <p> Since we were not able to find a formal proof of the intractability of LB-CACHE, we describe its proof in greater detail. Theorem 6.1 LB-PARALLEL is NP-complete. Proof: The NP-completeness of the decision problem follows by reduction from the SET-PARTITION problem <ref> [153] </ref>: An instance of the above decision problem can be constructed from an instance of the SET-PARTITION problem by considering the numbers to be task times and setting U to be P t i =2. Note that this reduction does not rely on the data objects at all.
Reference: [154] <author> James Philbin et. al., </author> <title> Thread scheduling for cache locality, </title> <booktitle> in Proceedings of the Seventh Symposium on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VII), </booktitle> <year> 1996, </year> <pages> pp. 6071. </pages>
Reference-contexts: LB-PARALLEL models the parallelism challenge, and is a classical scheduling 130 formulation. The locality aspect of the load-balancing problem is modeled by the LB-CACHE formu-lation which is less well known and has only been informally described in literature <ref> [154] </ref>. To show that the general load balancing problem is intractable, we prove that the above formulations are NP-complete. Given the ubiquity of the LB-PARALLEL formulation, we only sketch the proof of its NP-completeness.
Reference: [155] <author> P. Goyal, X. Guo, and H. M. Vin, </author> <title> A hierarchical CPU scheduler for multimedia operating systems, </title> <booktitle> in Proceedings of the 2nd Symposium on Operating Systems Design and Implementation (OSDI'96), </booktitle> <year> 1996, </year> <pages> pp. 107121. </pages>
Reference: [156] <author> C. A. Waldspurger, </author> <title> Lottery and stride scheduling: Flexible proportional-share resource management, </title> <type> Ph.D. dissertation, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference: [157] <author> C. Waldspurger and W. Weihl, </author> <title> Lottery scheduling: Flexible, proportional share resource management, </title> <booktitle> in Proceedings of the Symposium on Operating Systems Design and Implementation, </booktitle> <year> 1994, </year> <pages> pp. 232245. </pages>
Reference: [158] <author> C. L. Liu and J. W. Layland, </author> <title> Scheduling algorithms for multiprogramming in a hard real-time environment, </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> vol. 20, no. 1, </volume> <pages> pp. 4661, </pages> <year> 1973. </year>
Reference: [159] <author> J. Hyman, A. Lazar, and G. Pacifici, </author> <title> Real-time scheduling with quality of service constraints, </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 9, no. 7, </volume> <pages> pp. 10521063, </pages> <year> 1991. </year>
Reference: [160] <author> I. Stoica, H. Abdel-Wahab, and K. Jeffay, </author> <title> A proportional share resource allocation algorithm for real-time, time-shared systems, </title> <booktitle> in Proceedings of IEEE Real Time Systems Symposium, </booktitle> <year> 1996, </year> <pages> pp. 288299. </pages>
Reference: [161] <author> R. D. Blumofe, M. Frigo, C. F. Joerg, C. E. Leiserson, and K. H. Randall, </author> <title> Dag-consistent distributed shared memory, </title> <booktitle> in 10th International Parallel Processing Symposium, </booktitle> <year> 1996, </year> <pages> pp. 132 141. </pages>
Reference-contexts: A major shortcoming of these theoretical results is that, unlike our locality-aware load-balancing formulation, they do not model locality costs in the model, assuming that all data accessed by the threads is available on all processors. This shortcoming is alleviated in more recent work <ref> [161] </ref>; however, the assumed sharing model (dag-consistency) is very restrictive and not representative of general sharing behavior encountered in multithreaded computations. Load balancing policies. <p> While the analysis of randomized work-sharing [121] and work-stealing [122] strategies are good starting points, they do not model data locality in the computation. The work on dag-consistency <ref> [161] </ref> is a promising step in the latter direction, but is still far from modeling general locality. 3. Modeling realistic cache behavior: The specific cache model did not affect the NP-completeness proof in Section 6.2, however it is likely to affect the design of online and approximation algorithms.
Reference: [162] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng, </author> <title> Compiling FORTRAN D for MIMD distributed-memory machines, </title> <journal> Communications of the ACM, </journal> <volume> vol. 35, no. 8, </volume> <pages> pp. 6680, </pages> <year> 1992. </year>
Reference: [163] <author> G. C. Fox, </author> <title> Numerical Algorithms for Modern Parallel Computer Architectures. </title> <address> New York: </address> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference: [164] <author> H. Samet, </author> <title> The Design and Analysis of Spatial Data Structures. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: Our load-balancing framework enables a custom distribution for the collection of atom objects which ensures load balance while preserving locality. We use 178 the GroupDistribution policy to specify a spatial decomposition of the atom objects based on a Peano-Hilbert encoding <ref> [164, 165] </ref> traversal of spatial cells, a technique borrowed from astrophysics applications [176, 177]. As described in Section 6.5, the GroupDistribution policy is implemented using a distribution map supplied at startup.
Reference: [165] <author> J. P. Singh, </author> <title> Parallel hierarchical N-body methods and their implications for multiprocessors, </title> <type> Ph.D. dissertation, </type> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <year> 1993. </year>
Reference-contexts: Our load-balancing framework enables a custom distribution for the collection of atom objects which ensures load balance while preserving locality. We use 178 the GroupDistribution policy to specify a spatial decomposition of the atom objects based on a Peano-Hilbert encoding <ref> [164, 165] </ref> traversal of spatial cells, a technique borrowed from astrophysics applications [176, 177]. As described in Section 6.5, the GroupDistribution policy is implemented using a distribution map supplied at startup.
Reference: [166] <author> S. Krishnan and L. V. Kale, </author> <title> A parallel array abstraction for data-driven objects, </title> <booktitle> in Proceedings of Parallel Object-Oriented Methods and Applications Conference, </booktitle> <year> 1996, </year> <pages> pp. 1519. </pages>
Reference: [167] <author> A. B. Sinha and L. V. Kale, </author> <title> A load balancing strategy for prioritized execution of tasks, </title> <booktitle> in Proceedings of International Symposium on Parallel Processing, </booktitle> <year> 1993, </year> <pages> pp. 230237. </pages>
Reference-contexts: Load balancing policies. Many researchers have proposed a large number of load balancing policies based on object placement [162166], thread placement [6, 7, 9, 10, 24, 27, 82, 117120], and thread ordering <ref> [167] </ref>, which are sensitive to varying degrees to data locality.
Reference: [168] <author> S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta, </author> <title> The SPLASH-2 programs: Characterization and methodological considerations, </title> <booktitle> in Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1995, </year> <pages> pp. 2436. </pages>
Reference-contexts: Table 7.2 lists some key statistics about the thread and object structure which characterize the application behavior qualitatively. We briefly discuss these characteristics below. 171 Table 7.1 The irregular applications suite. APPLICATION DESCRIPTION INPUT IC-Cedar protein dynamics [72] simulates protein macromolecular dynamics. myoglobin Radiosity computer graphics <ref> [168] </ref> computes illumination using the hierarchical radiosity method. room Grobner computational algebra [169] computes Gr obner basis. ae-4 Phylogeny evolutionary history [170] determines the evolutionary history of a set of species. prim.40 Table 7.2 Sequential execution profile of the ICC++ applications on the Cray T3D. <p> a threshold. get refined on demand as the computation progresses. 1 The formfactor determines the contribution of a patch's radiosity to other patches and depends upon the shape, distance, and relative orientation of the two patches. 185 Our parallel algorithm is derived from the radiosity application in the SPLASH-2 suite <ref> [168] </ref>. There are three levels of parallelism in each iteration: across all input patches (radiosity calculation), across child patches of a subdivided patch (radiosity calculation), and across neighbor patches stored in the interaction list (visibility, error, and radiosity calculation). Each of these tasks (radiosity calculation, visibility calculation, and error calculation).
Reference: [169] <author> S. Chakrabarti and K. Yelick, </author> <title> Implementing an irregular application on a distributed memory multiprocessor, </title> <booktitle> in Proceedings of the Fourth ACM/SIGPLAN Symposium on Principles and Practices of Parallel Programming, </booktitle> <year> 1993, </year> <pages> pp. 169179. </pages>
Reference-contexts: We briefly discuss these characteristics below. 171 Table 7.1 The irregular applications suite. APPLICATION DESCRIPTION INPUT IC-Cedar protein dynamics [72] simulates protein macromolecular dynamics. myoglobin Radiosity computer graphics [168] computes illumination using the hierarchical radiosity method. room Grobner computational algebra <ref> [169] </ref> computes Gr obner basis. ae-4 Phylogeny evolutionary history [170] determines the evolutionary history of a set of species. prim.40 Table 7.2 Sequential execution profile of the ICC++ applications on the Cray T3D. <p> Additionally, threads require prioritized execution to minimize redundant work. Threads concurrently read and write the basis object, so any cached copies must be kept coherent. 197 new polynomials to the basis as the computation progresses. Performance of low-level approaches. Previous efforts <ref> [169, 179] </ref> to efficiently execute the Gr obner application have either been restricted to small scale shared memory implementations [179] or have adopted a low-level approach with explicit locality management on distributed memory machines [169]. <p> Performance of low-level approaches. Previous efforts [169, 179] to efficiently execute the Gr obner application have either been restricted to small scale shared memory implementations [179] or have adopted a low-level approach with explicit locality management on distributed memory machines <ref> [169] </ref>. The latter approach explicitly replicates the polynomial and basis objects; all updates to the basis are explicitly propagated to its copies. Previously published speedup numbers (obtained from [169]) on similar data sets show that the application achieves a speedup of 610 times on 16 processors of the CM-5. 3 7.4.2 <p> restricted to small scale shared memory implementations [179] or have adopted a low-level approach with explicit locality management on distributed memory machines <ref> [169] </ref>. The latter approach explicitly replicates the polynomial and basis objects; all updates to the basis are explicitly propagated to its copies. Previously published speedup numbers (obtained from [169]) on similar data sets show that the application achieves a speedup of 610 times on 16 processors of the CM-5. 3 7.4.2 Data locality and load-balancing optimizations In contrast to the explicit replication approach described above, our focus is on achieving good performance without modifying the dynamic multithreaded expression of
Reference: [170] <author> J. A. Jones, </author> <title> Parallelizing the phylogeny problem, M.S. </title> <type> thesis, </type> <institution> University of California, </institution> <address> Berke-ley, CA, </address> <year> 1994. </year>
Reference-contexts: We briefly discuss these characteristics below. 171 Table 7.1 The irregular applications suite. APPLICATION DESCRIPTION INPUT IC-Cedar protein dynamics [72] simulates protein macromolecular dynamics. myoglobin Radiosity computer graphics [168] computes illumination using the hierarchical radiosity method. room Grobner computational algebra [169] computes Gr obner basis. ae-4 Phylogeny evolutionary history <ref> [170] </ref> determines the evolutionary history of a set of species. prim.40 Table 7.2 Sequential execution profile of the ICC++ applications on the Cray T3D. All applications involve dynamic thread and object creation, and span a range of thread granularities, object sizes, and priority sensitivities. <p> Our algorithm is based on a sequential algorithm described by Jones <ref> [170] </ref>. We describe in turn the application structure, its use of view-caching and load-balancing frameworks, and the resulting performance. 204 T3D Origin 2000 MSG LD/ST 205 T3D Origin 2000 MSG LD/ST application. 206 7.5.1 Application structure Phylogenetic trees are constructed by considering the characters exhibited by the species. <p> Our algorithm for the general phylogeny problem is based on a method known as character compatibility and uses a solution to the perfect phylogeny problem (determining whether or not a perfect phylogenetic tree exists) (see <ref> [170] </ref> for details) as a subroutine. The essence of the character compatibility method is a search for the largest compatible subset of characters, the rationale being that if the subset is large enough, the corresponding perfect phylogeny will be a good estimate of the evolutionary history of the species. <p> Each solution node maintains a character subset, and child and parent pointers of the trie. Given the sophisticated nature of the data structures, and the pruning-based search procedure, the algorithm is complicated to express even on sequential platforms. Our parallel algorithm follows the strategy of <ref> [170] </ref> and evaluates multiple character subsets in parallel. Given that adequate parallelism exists at this level, we have chosen to utilize a bundled solution to the perfect phylogeny problem as a (sequential) leaf subroutine. <p> Additionally, threads require prioritized execution which corresponds to a depth-first and right-to-left traversal of the search space. Threads read and write the failure set which must be kept consistent against concurrent access. Performance of low-level approaches. Previous efforts <ref> [170] </ref> to efficiently execute the phylogeny application have relied on a low-level approach which explicitly manages locality on distributed memory machines. The failure set is replicated across the processors with processors periodically synchronizing to make it consistent. Published speedup numbers (obtained from [170]) on similar data sets show that using this <p> Performance of low-level approaches. Previous efforts <ref> [170] </ref> to efficiently execute the phylogeny application have relied on a low-level approach which explicitly manages locality on distributed memory machines. The failure set is replicated across the processors with processors periodically synchronizing to make it consistent. Published speedup numbers (obtained from [170]) on similar data sets show that using this approach, the application achieves a speedup of 20 on 32 nodes of the CM-5. 7.5.2 Data locality and load-balancing optimizations In contrast to the explicit replication approach described above, our focus is on achieving good performance without modifying the dynamic multithreaded expression
Reference: [171] <author> Y.-S. Hwang, R. Das, J. Saltz, B. Brooks, and M. Hodoffsffcek, </author> <title> Parallelizing molecular dynamics programs for distributed memory machines, </title> <journal> IEEE Computational Science and Engineering, </journal> <volume> vol. 2, no. 2, </volume> <pages> pp. 1829, </pages> <year> 1995. </year>
Reference-contexts: In previous work [72], we have shown that applying the inspector-executor approach to IC-Cedar results in a speedup of 32 on 64 nodes of the T3D on the same data set. In comparison, CHARMM <ref> [171] </ref>, a highly-tuned SPMD Fortran program with calls to the CHAOS [174] run-time library, achieves a speedup of 42 on 64 nodes of the T3D for a comparable data set.
Reference: [172] <author> J. P. Singh, A. Gupta, and M. Levoy, </author> <title> Parallel visualization algorithms: Performance and architectural implications, </title> <journal> IEEE Computer, </journal> <volume> vol. 27, no. 7, </volume> <pages> pp. 4556, </pages> <year> 1994. </year>
Reference-contexts: This results in an irregular and unpredictable thread structure. Additionally, threads exhibit wide granularity variations and access data objects in an unstructured fashion. Patch fields are updated every iteration, so they cannot be cached across iterations. Performance of low-level approaches. Previous efforts <ref> [172] </ref> for efficiently executing the hierarchical radiosity application have primarily been restricted to hand-tuned implementations on cache-coherent shared memory machines. In fact, the irregular thread and data access structure of the application have often been used to motivate the need for such machines. <p> Such implementations explicitly manage the creation and distribution of radiosity, visibility, and error calculation tasks, relying on the underlying hardware support to coherently access the patch objects. Previously published speedup numbers (obtained from <ref> [172] </ref> and [50]) on the same data set show that the application achieves a speedup of 26 on 32 processors of the DASH [128] and a speedup of 11 on 16 processors of the Origin. 7.3.2 Data locality and load-balancing optimizations In contrast to the approach mentioned above, our focus is
Reference: [173] <author> J. Hermans and M. Carson, </author> <title> CEDAR documentation. Unpublished manual for CEDAR, </title> <year> 1985. </year>
Reference-contexts: As we shall see later, the run-time techniques described in Chapters 5 and 6 enable each application to achieve performance comparable to the above low-level approaches. 7.2 IC-Cedar IC-Cedar is a parallel version of CEDAR <ref> [173] </ref>, a sequential protein molecular dynamics program that models the motion of individual protein and surrounding solvent atoms using Newton's equations of motion.
Reference: [174] <editor> J.H. Saltz et al., </editor> <title> A manual for the CHAOS runtime library, </title> <institution> Department of Computer Science, University of Maryland, </institution> <type> Tech. Rep. </type> <institution> CS-TK-3437, </institution> <year> 1995. </year>
Reference-contexts: In previous work [72], we have shown that applying the inspector-executor approach to IC-Cedar results in a speedup of 32 on 64 nodes of the T3D on the same data set. In comparison, CHARMM [171], a highly-tuned SPMD Fortran program with calls to the CHAOS <ref> [174] </ref> run-time library, achieves a speedup of 42 on 64 nodes of the T3D for a comparable data set. <p> Research efforts have largely focused on building special libraries which provide run-time support for one of several dimensions of irregularity. For example, the inspector-executor approach uses a library to support iterated irregular communication (such as in finite element simulations) <ref> [174] </ref>. The Multipol library provides a set of tools to build irregular applications, but no uniform high level interface [117]. Libraries such as Sparspak, Kelp, and A++/P++ support particular classes of algorithms on sparse matrices and adaptive meshes [180,181].
Reference: [175] <author> L. V. Kale, M. Bhandarkar, R. Brunner, N. Krawetz, J. Phillips, and A. Shinozaki, NAMD: </author> <title> A case study in multilingual parallel programming, </title> <booktitle> in Proceedings of the 10th International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1997, </year> <pages> pp. 3751. </pages>
Reference-contexts: In comparison, CHARMM [171], a highly-tuned SPMD Fortran program with calls to the CHAOS [174] run-time library, achieves a speedup of 42 on 64 nodes of the T3D for a comparable data set. Similar performance levels have also been reported on the SGI Origin 2000 for NAMD <ref> [175] </ref>, a hand-tuned multilingual parallel program which combines three different paradigms: parallel message-driven objects, message passing, and coarse-grained multithreading. 177 7.2.2 Data locality and load-balancing optimizations In contrast to the low-level approaches described above, our focus is on achieving good performance without changing the dynamic multithreaded expression of the program.
Reference: [176] <author> A. Y. Grama, V. Kumar, and A. Sameh, </author> <title> Scalable parallel formulations of the Barnes-Hut method for N-body simulations, </title> <booktitle> in Proceedings of Supercomputing Conference, </booktitle> <year> 1994, </year> <pages> pp. 439448. </pages>
Reference-contexts: We use 178 the GroupDistribution policy to specify a spatial decomposition of the atom objects based on a Peano-Hilbert encoding [164, 165] traversal of spatial cells, a technique borrowed from astrophysics applications <ref> [176, 177] </ref>. As described in Section 6.5, the GroupDistribution policy is implemented using a distribution map supplied at startup. By controlling object placement and executing threads local to target objects, we achieve load balance while ensuring that most of the data accesses are to collocated objects.
Reference: [177] <author> M. Warren and J. Salmon, </author> <title> A parallel hashed oct-tree N-body algorithm, </title> <booktitle> in Proceedings of Supercomputing Conference, </booktitle> <year> 1993, </year> <pages> pp. 1221. </pages>
Reference-contexts: We use 178 the GroupDistribution policy to specify a spatial decomposition of the atom objects based on a Peano-Hilbert encoding [164, 165] traversal of spatial cells, a technique borrowed from astrophysics applications <ref> [176, 177] </ref>. As described in Section 6.5, the GroupDistribution policy is implemented using a distribution map supplied at startup. By controlling object placement and executing threads local to target objects, we achieve load balance while ensuring that most of the data accesses are to collocated objects.
Reference: [178] <author> B. </author> <title> Buchberger, Multidimensional Systems Theory, ch. Grobner basis: an algorithmic method in polynomial ideal theory, </title> <journal> pp. </journal> <volume> 184232. </volume> <publisher> Dordrecht: </publisher> <address> D. </address> <publisher> Reidel Publishing Company, </publisher> <year> 1985. </year>
Reference-contexts: Our algorithm is based on a sequential algorithm due to Buchberger <ref> [178] </ref>. We describe, in turn, the application structure, its use of view caching and load-balancing frameworks, and the resulting performance. 7.4.1 Application structure The algorithm starts off with an initial basis set of polynomials equal to the input set.
Reference: [179] <author> J.-P. Vidal, </author> <title> The computation of Grobner bases on a shared memory multiprocessor, </title> <institution> School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, </institution> <type> Tech. Rep. </type> <institution> CMU-CS-90-163, </institution> <year> 1990. </year>
Reference-contexts: Additionally, threads require prioritized execution to minimize redundant work. Threads concurrently read and write the basis object, so any cached copies must be kept coherent. 197 new polynomials to the basis as the computation progresses. Performance of low-level approaches. Previous efforts <ref> [169, 179] </ref> to efficiently execute the Gr obner application have either been restricted to small scale shared memory implementations [179] or have adopted a low-level approach with explicit locality management on distributed memory machines [169]. <p> Performance of low-level approaches. Previous efforts [169, 179] to efficiently execute the Gr obner application have either been restricted to small scale shared memory implementations <ref> [179] </ref> or have adopted a low-level approach with explicit locality management on distributed memory machines [169]. The latter approach explicitly replicates the polynomial and basis objects; all updates to the basis are explicitly propagated to its copies.
Reference: [180] <author> E. Chu, A. George, J. Liu, and E. Ng, Sparspak: </author> <title> Waterloo sparse matrix package user's guide for Sparspak-A, </title> <institution> Department of Computer Science, University of Waterloo, Waterloo, Ontario, Canada, </institution> <type> Tech. Rep. </type> <institution> CS-84-36, </institution> <year> 1984. </year> <month> 309 </month>
Reference: [181] <author> R. Parsons and D. Quinlan, </author> <title> A++/P++ array classes for architecture independent finite differ-ence computations, </title> <booktitle> in Proceedings of the Second Annual Object-Oriented Numerics Conference, </booktitle> <year> 1994, </year> <pages> pp. 408418. </pages>
Reference: [182] <author> D. S. Henry and C. F. Joerg, </author> <title> A tightly-coupled processor-network interface, </title> <booktitle> in Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <year> 1992, </year> <pages> pp. 111122. </pages>
Reference-contexts: Hardware solutions for fine-grained computations. Several researchers have argued for including custom hardware capabilities such as multiple hardware contexts [133, 134], tightly integrated communication <ref> [134, 141, 182, 183] </ref>, integrating thread creation with message reception [135, 136], and support for cache coherence [128, 129, 184,185] on the grounds that such support is essential for obtaining efficiency on fine-grained problems.
Reference: [183] <author> C. Whitby-Strevens, </author> <title> The Transputer, </title> <booktitle> in Proceedings of 12th International Symposium on Computer Architecture, </booktitle> <year> 1985, </year> <pages> pp. 292300. </pages>
Reference-contexts: Hardware solutions for fine-grained computations. Several researchers have argued for including custom hardware capabilities such as multiple hardware contexts [133, 134], tightly integrated communication <ref> [134, 141, 182, 183] </ref>, integrating thread creation with message reception [135, 136], and support for cache coherence [128, 129, 184,185] on the grounds that such support is essential for obtaining efficiency on fine-grained problems.
Reference: [184] <author> Anant Agarwal et al., </author> <title> The MIT Alewife Machine: Architecture and Performance, </title> <booktitle> in Proceedings of the 22nd International Symposium on Computer Architecture, </booktitle> <year> 1995, </year> <pages> pp. 213. </pages>
Reference: [185] <author> S. Reinhardt, J. Larus, and D. Wood, Tempest and Typhoon: </author> <title> User-level shared memory, </title> <booktitle> in Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1994, </year> <pages> pp. 325336. </pages>
Reference-contexts: Thus, an interesting future direction would be to look at designing an appropriate set of hardware primitives which would enable efficient, robust implementation of shared memory using view caching protocols. This is similar in spirit to the work on Pica [198] and Tempest <ref> [30, 185] </ref>, both of which define a set of primitives for parallel processing. Based on the experiments reported in Chapter 7, an example set of primitives might include support for fast access control, put/get operations, and asynchronous and synchronous updates of global directory state.
Reference: [186] <author> D. Chase, M. Wegman, and F. Zadeck, </author> <title> Analysis of pointers and structures, </title> <booktitle> in Proceedings of SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1990, </year> <pages> pp. 296 310. </pages>
Reference: [187] <author> J.-D. Choi, M. Burke, and P. Carini, </author> <title> Efficient flow-sensative interprocedural computation of pointer-induced aliases and side effects, </title> <booktitle> in Twentieth Symposium on Principles of Programming Languages, </booktitle> <year> 1993, </year> <pages> pp. 232245. </pages>
Reference: [188] <author> W. Landi and B. Ryder, </author> <title> Pointer-induced aliasing: A problem classification, </title> <booktitle> in Symposium on Principles of Programming Languages, </booktitle> <year> 1991, </year> <pages> pp. 93103. </pages>
Reference: [189] <author> L. Hendren, A. Nicolau, and J. Hummel, </author> <title> Abstractions for recursive pointer data structures: Improving the analysis and transformation of imperative programs, </title> <booktitle> in Proceedings of the SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1992, </year> <pages> pp. 249260. </pages>
Reference: [190] <author> S. P. Masticola and B. G. Ryder, </author> <title> Non-concurrency analysis, </title> <booktitle> in Proceedings of Fourth Symposium on Principles and Practice of Parallel Programming, </booktitle> <year> 1993, </year> <pages> pp. 129138. </pages>
Reference-contexts: Finally, the above choices may need to be revised based upon execution behavior. The revision could happen while the program is executing or for additional program runs. Good starting points for the compiler-based approach are work on aliasing and structure analysis [63, 186189], nonconcurrency analysis <ref> [190] </ref>, commutativity analysis [191], and dynamic pointer alignment [69]. These describe compiler analyses for extracting information about object structure, thread interactions, their effect on object state, and the nature of object access respectively.
Reference: [191] <author> M. Rinard and P. C. Diniz, </author> <title> Commutativity analysis: A new analysis framework for parallelizing compilers, </title> <booktitle> in Proceedings of the 1996 ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1996, </year> <pages> pp. 5467. </pages>
Reference-contexts: Finally, the above choices may need to be revised based upon execution behavior. The revision could happen while the program is executing or for additional program runs. Good starting points for the compiler-based approach are work on aliasing and structure analysis [63, 186189], nonconcurrency analysis [190], commutativity analysis <ref> [191] </ref>, and dynamic pointer alignment [69]. These describe compiler analyses for extracting information about object structure, thread interactions, their effect on object state, and the nature of object access respectively.
Reference: [192] <author> S. Krishnan and L. V. Kale, </author> <title> Automating parallel runtime optimizations using post-mortem analysis, </title> <booktitle> in Proceedings of the 10th ACM International Conference on Supercomputing, </booktitle> <year> 1996, </year> <pages> pp. 221228. </pages>
Reference-contexts: These describe compiler analyses for extracting information about object structure, thread interactions, their effect on object state, and the nature of object access respectively. Postmortem analysis has been explored in the context of optimizing object placement <ref> [192] </ref> and for providing performance feedback [193] in the Charm system. 235 Run-time mechanisms for diverse environments.
Reference: [193] <author> A. B. Sinha, </author> <title> Analysis of object based and message driven programs, </title> <type> Ph.D. dissertation, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1995. </year>
Reference-contexts: These describe compiler analyses for extracting information about object structure, thread interactions, their effect on object state, and the nature of object access respectively. Postmortem analysis has been explored in the context of optimizing object placement [192] and for providing performance feedback <ref> [193] </ref> in the Charm system. 235 Run-time mechanisms for diverse environments.
Reference: [194] <author> A. C. Dusseau, R. H. Arpaci, and D. E. Culler, </author> <title> Effective distributed scheduling of parallel work-loads, </title> <booktitle> in ACM SIGMETRICS '96 Conference on the Measurement and Modeling of Computer Systems, </booktitle> <year> 1996, </year> <pages> pp. 2536. 310 </pages>
Reference: [195] <author> A. Dusseau-Arpaci and D. Culler, </author> <title> Extending proportional-share scheduling to a network of workstations, </title> <booktitle> in Proceedings of the International Conference on Parallel and Distributed Processing Techniques and Applications, </booktitle> <year> 1997, </year> <pages> pp. </pages> <year> 191200. </year>
Reference: [196] <author> P. Sobalvarro, </author> <title> Dynamic coscheduling (DCS), </title> <type> Ph.D. dissertation, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference: [197] <author> M. Buchanan and A. A. Chien, </author> <title> Coordinated thread scheduling for workstation clusters under Windows NT, </title> <booktitle> in Proceedings of the USENIX Windows NT Workshop, </booktitle> <year> 1997, </year> <pages> pp. 4754. </pages>

References-found: 197

