URL: http://www.cs.cornell.edu/Info/People/ronitt/PAP/cover.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/ronitt/papers.html
Root-URL: 
Email: danar@theory.lcs.mit.edu  ronitt@cs.cornell.edu  
Title: Exactly Learning Automata with Small Cover Time  
Author: Dana Ron Ronitt Rubinfeld 
Note: Supported by a National Science Foundation Postdoctoral Research Fellowship, Grant No. DMS-9508963 Supported by ONR Young Investigator Award N00014-93-1-0590 and grant No. 92-00226 from the United States Israel Binational Science Foundation (BSF), Jerusalem, Israel.  
Address: Cambridge, MA 02139  Ithaca, NY 14853  
Affiliation: Laboratory of Computer Science MIT  Computer Science Department Cornell University  
Abstract: We present algorithms for exactly learning unknown environments that can be described by deterministic finite automata. The learner performs a walk on the target automaton, where at each step it observes the output of the state it is at, and chooses a labeled edge to traverse to the next state. We assume that the learner has no means of a reset, and we also assume that the learner does not have access to a teacher that answers equivalence queries and gives the learner counterexamples to its hypotheses. We present two algorithms, one assumes that the outputs observed by the learner are always correct and the other assumes that the outputs might be erroneous. The running times of both algorithms are polynomial in the cover time of the underlying graph of the target automaton. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Angluin. </author> <title> Negative results for equivalence queries. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 121-150, </pages> <year> 1990. </year>
Reference-contexts: Rivest and Schapire [19] show how permutation automata can be exactly learned efficiently without means of a reset and without making equivalence queries. Angluin proves <ref> [1] </ref> that the problem of exactly learning DFAs from equivalence queries alone is hard. Ibarra and Jiang [15] show that the subclass of k-bounded regular languages can be exactly learned from a polynomial number of equivalence queries.
Reference: [2] <author> D. Angluin and C. H. Smith. </author> <title> Inductive inference: Theory and methods. </title> <journal> Computing Surveys, </journal> <volume> 15(3) </volume> <pages> 237-269, </pages> <month> September </month> <year> 1983. </year>
Reference-contexts: We refer the reader to a survey by Angluin and Smith <ref> [2] </ref>. Here we briefly survey the known efficient learning algorithms for DFAs.
Reference: [3] <author> Dana Angluin. </author> <title> A note on the number of queries needed to identify regular languages. </title> <journal> Information and Control, </journal> <volume> 51 </volume> <pages> 76-87, </pages> <year> 1981. </year>
Reference-contexts: Note that having a teacher which answers membership queries is equivalent to having means of a reset. We use as a subroutine of our algorithm a variant of Angluin's algorithm which is similar to the one described in <ref> [3] </ref>. <p> We refer the reader to a survey by Angluin and Smith [2]. Here we briefly survey the known efficient learning algorithms for DFAs. We start with the problem of exactly learning DFAs: Angluin <ref> [3] </ref> proves that it is hard to exactly learn DFA when the learner has access to a membership oracle (which, as noted previously, is equivalent to having means of a reset) but not to an equivalence oracle. <p> The algorithm works in the setting where the learner has means of a reset. The analysis is similar to that in <ref> [3] </ref> and shows that if the target automaton M has cover time C (M ) then with high probability, the algorithm exactly learns the target automaton by performing O (nC (M )) walks, each of length O (C (M )).
Reference: [4] <author> Dana Angluin. </author> <title> Learning regular sets from queries and counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 87-106, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: Our results are easily extendible to larger alphabets. In our algorithms we apply ideas from the no-reset learning algorithm of Rivest and Schapire [19], which in turn uses Angluin's algorithm <ref> [4] </ref> as a subroutine. Angluin's algorithm is an algorithm for exactly learning automata from a teacher that can answer both membership queries and equivalence queries. Note that having a teacher which answers membership queries is equivalent to having means of a reset. <p> This model is equivalent to PAC learning with membership queries. Since Angluin's algorithm <ref> [4] </ref> can be modified to a PAC learning algorithm with membership queries, DFAs are efficiently learnable in this model. However, when the learner does not have means of a reset, and thus performs a single walk on M , we know of no natural notion of approximately correct learning. <p> Since the task of learning becomes harder as approaches 1=2, and ff approaches 0, we allow the running algorithm to depend polynomially on 1=ff, as well as on n and log (1=ffi). 3 Exact Learning with Reset In this section we describe a simple variant of Angluin's algorithm <ref> [4] </ref> for learning deterministic finite automata. The algorithm works in the setting where the learner has means of a reset. <p> Q T def = frow T (r i ) j r i 2 R; 8 2 f0; 1g; r i 2 Rg; * t T (row T (r i ); ) = row T (r i ); 0 = row T (); def It is not hard to verify (see <ref> [4] </ref>) that M T is consistent with T in the sense that for every r i 2 R, and for every s j 2 S, M T (r i s j ) = T (r i ; s j ).
Reference: [5] <author> Dana Angluin and Philip Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: number of walks, each of polynomial length, it output a hypothesis c M , which is equivalent to M , i.e., for every string s, c M (s) = M (s). 2.2.2 The noisy model Our assumptions on the noise follow the classification noise model introduced by Angluin and Laird <ref> [5] </ref>. We assume that for some fixed noise rate &lt; 1=2, at each step, with probability 1 the algorithm observes the (correct) output of the state it has reached, and with probability it 4 observes an incorrect output.
Reference: [6] <author> Michael Bender and Donna Slonim. </author> <title> The power of team exploration: Two robots can learn unlabeled directed graphs. </title> <booktitle> In Proceedings of the Thirty Sixth Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 75-85, </pages> <year> 1994. </year>
Reference-contexts: Angluin proves [1] that the problem of exactly learning DFAs from equivalence queries alone is hard. Ibarra and Jiang [15] show that the subclass of k-bounded regular languages can be exactly learned from a polynomial number of equivalence queries. Bender and Slonim <ref> [6] </ref> study the related problem of exactly learning directed graphs (which do no have any outputs associated with their nodes). They show that this task can be performed efficiently by two cooperating robots where each robot performs a single walk on the target graph.
Reference: [7] <author> Thomas Dean, Dana Angluin, Kenneth Basye, Sean Engelson, Leslie Kaelbling, Evangelos Kokkevis, and Oded Maron. </author> <title> Inferring finite automata with stochastic output functions and an application to map learning. </title> <journal> Machine Learning, </journal> <volume> 18(1) </volume> <pages> 81-108, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: One major difficulty is that it is not clear how the learner can orient itself since when executing a homing sequence, with high probability it does not observe the correct output sequence. In order to overcome this difficulty, we adapt a "looping" idea presented by Dean et. al. <ref> [7] </ref>. Dean et. al. study a similar setting in which the noise rate is not fixed but is a function of the current state, and present a learning algorithm for this problem. <p> Next we assume that the algorithm has no means of a reset, but instead, has a homing sequence, h. Clearly, in a single execution of h, with high probability, the output sequence will be erroneous. We thus adapt a technique that was used in <ref> [7] </ref>. Assume we execute the homing sequence m consecutive times, where m &gt;> n and is set subsequently. The last m n executions of the homing sequence must be following a cycle (though not a simple cycle).
Reference: [8] <author> F. Ergun, S. Ravikumar, and R. Rubinfeld. </author> <title> On learning bounded-width branching programs. </title> <booktitle> In Proceedings of the Eighth Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 361-368, </pages> <year> 1995. </year>
Reference-contexts: Learning algorithms for several special classes of automata have been studied in this setting: Li and Vazirani [18] give several examples of regular languages that can be learned efficiently, including 1-letter languages. In <ref> [8] </ref> a learning algorithm for languages accepted by width 2 branching programs is given. It is also shown that the problem of learning width 3 branching programs is as hard as learning DNF, and it is observed that learning width 5 branching programs is hard under certain number theoretical assumptions.
Reference: [9] <author> L. Fortnow and D. Whang. </author> <title> Optimality and domination in repeated games with bounded players. </title> <booktitle> In The 25th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 741-749, </pages> <year> 1994. </year>
Reference-contexts: If M is known to the player, then it is not hard to prove that the player can find an optimal cycle strategy efficiently using dynamic programming. However, if M is not known to the player, then Fortnow and Wang <ref> [9] </ref> show that there exists a subclass of automata (sometimes referred to as combination-lock automata) for which it is hard to find an optimal strategy in the case of a general game 2 . <p> When the underlying game is penny matching, Fortnow and Wang <ref> [9] </ref> describe an algorithm that finds an optimal strategy efficiently, using ideas from Rivest and Schapire's [19] learning algorithm (but without actually learning the automaton). 1 cover time of M is defined to be the smallest integer t such that for every state q in M , a random walk of
Reference: [10] <author> Michael Frazier, Sally Goldman, Nina Mishra, and Leonard Pitt. </author> <title> Learning from a consistently ignorant teacher. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 328-339, </pages> <year> 1994. </year> <month> 18 </month>
Reference-contexts: The following works consider the case when the labels of the examples are assumed to be noisy. In [21], an algorithm is given for PAC-learning DFAs with membership queries in the presence of persistent noise. In <ref> [10] </ref>, an algorithm is given for learning DFAs by blurry concepts. 3 2 Preliminaries 2.1 Basic Definitions Let M be the deterministic finite state automaton (DFA) we would like to learn.
Reference: [11] <author> Yoav Freund, Michael J. Kearns, Yishay Mansour, Dana Ron, Ronitt Rubinfeld, and Robert E. Schapire. </author> <title> Efficient algorithms for learning to play repeated games against computationally bounded adversaries. </title> <booktitle> To appear in Proceedings of the Thirty Sixth Annual Symposium on Foundations of Computer Science, </booktitle> <year> 1995. </year>
Reference-contexts: In particular, it is not clear what type of approximation suffices for the game theoretical scenario. In recent work of Freund et. al. <ref> [11] </ref> our results have been improved as follows. Freund et. al. consider the problem of learning probabilistic output automata. These are finite automata whose transition function is deterministic, but whose output function is probabilistic. <p> In the case when the biases at each state are either or 1 for some 0 &lt; 1=2, this is essentially the problem of learning deterministic automata in the presence of noise, for which we give an algorithm in this paper. In <ref> [11] </ref>, a learning algorithm is given that runs in time polynomial in the cover time of the target automaton, with no restrictions on the biases at each state. Other Related Work Several researchers have considered the problem of learning DFAs in the limit.
Reference: [12] <author> Yoav Freund, Michael J. Kearns, Dana Ron, Ronitt Rubinfeld, Robert E. Schapire, and Linda Sellie. </author> <title> Efficient learning of typical finite automata from random walks. </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1993. </year>
Reference-contexts: It is also shown that the problem of learning width 3 branching programs is as hard as learning DNF, and it is observed that learning width 5 branching programs is hard under certain number theoretical assumptions. In <ref> [12] </ref> it is shown how to learn typical automata (automata in which the underlying graph is arbitrary, but the accept/reject labels on the states are chosen randomly) by passive learning (the edge traversed by the robot is chosen randomly) in a type of mistake bound model.
Reference: [13] <author> I. Gilboa and D. Samet. </author> <title> Bounded versus unbounded rationality: The tyranny of the weak. </title> <journal> Games and Economic Behavior, </journal> <volume> 1(3) </volume> <pages> 213-221, </pages> <year> 1989. </year>
Reference-contexts: Namely, starting from the starting state, the opponent outputs the action labeling the state it is at, and the action played by the player determines the opponent's next state 1 . It is known <ref> [13] </ref> that there exist optimal strategies in which the player simply forces the opponent DFA M to follow a cycle along the nodes of M 's underlying graph.
Reference: [14] <author> Wassily Hoeffding. </author> <title> Probability inequalities for sums of bounded random variables. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 58(301) </volume> <pages> 13-30, </pages> <month> March </month> <year> 1963. </year>
Reference-contexts: Since we have less than n 2 pairs, if L = 13 ((1=) 2 (1=ff) 2 log (n=ffi 0 )), then by Hoeffding's inequality <ref> [14] </ref>, with probability at least 1 ffi 0 , for every pair i; j, jd ij E [d ij ]j ff, and hence jd min 2 (1 )j 2ff. It directly follows (see [21]) that j^ j .
Reference: [15] <author> O. Ibarra and T. Jiang. </author> <title> Learning regular languaages from counterexamples. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pages 371-385, </pages> <year> 1988. </year>
Reference-contexts: Rivest and Schapire [19] show how permutation automata can be exactly learned efficiently without means of a reset and without making equivalence queries. Angluin proves [1] that the problem of exactly learning DFAs from equivalence queries alone is hard. Ibarra and Jiang <ref> [15] </ref> show that the subclass of k-bounded regular languages can be exactly learned from a polynomial number of equivalence queries. Bender and Slonim [6] study the related problem of exactly learning directed graphs (which do no have any outputs associated with their nodes).
Reference: [16] <author> Michael Kearns and Leslie G. Valiant. </author> <title> Cryptographic limitations on learning Boolean formulae and finite automata. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 41 </volume> <pages> 67-95, </pages> <year> 1994. </year> <note> An extended abstract of this paper appeared in STOC89. </note>
Reference-contexts: They also show how their algorithm can be modified and made more efficient if the graph has high conductance [23], where conductance is a measure of the expansion properties of the graph. As for non-exact (approximate) learning, without the aid of queries, Kearns and Valiant <ref> [16] </ref> show that under certain number theoretical assumptions, the problem of PAC learning DFAs is hard when only given access to random examples.
Reference: [17] <author> Zvi Kohavi. </author> <title> Switching and Finite Automata Theory. </title> <publisher> McGraw-Hill, </publisher> <address> second edition, </address> <year> 1978. </year>
Reference-contexts: It is not hard to verify (cf. <ref> [17] </ref>) that every DFA has a homing sequence of length at most quadratic in its size.
Reference: [18] <author> M. Li and U. Vazirani. </author> <title> On the learnability of finite automata. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pages 359-370, </pages> <year> 1988. </year>
Reference-contexts: Learning algorithms for several special classes of automata have been studied in this setting: Li and Vazirani <ref> [18] </ref> give several examples of regular languages that can be learned efficiently, including 1-letter languages. In [8] a learning algorithm for languages accepted by width 2 branching programs is given.
Reference: [19] <author> Ronald. L. Rivest and Robert. E. Schapire. </author> <title> Inference of finite automata using homing sequences. </title> <journal> Information and Computation, </journal> <volume> 103(2) </volume> <pages> 299-347, </pages> <year> 1993. </year>
Reference-contexts: When the underlying game is penny matching, Fortnow and Wang [9] describe an algorithm that finds an optimal strategy efficiently, using ideas from Rivest and Schapire's <ref> [19] </ref> learning algorithm (but without actually learning the automaton). 1 cover time of M is defined to be the smallest integer t such that for every state q in M , a random walk of length t starting from q visits every state in M with probability at least 1=2. <p> Our results are easily extendible to larger alphabets. In our algorithms we apply ideas from the no-reset learning algorithm of Rivest and Schapire <ref> [19] </ref>, which in turn uses Angluin's algorithm [4] as a subroutine. Angluin's algorithm is an algorithm for exactly learning automata from a teacher that can answer both membership queries and equivalence queries. Note that having a teacher which answers membership queries is equivalent to having means of a reset. <p> Note that having a teacher which answers membership queries is equivalent to having means of a reset. We use as a subroutine of our algorithm a variant of Angluin's algorithm which is similar to the one described in [3]. As in <ref> [19] </ref>, we use a homing sequence to overcome the absence of a reset, only we are able to construct such a sequence without the aid of a teacher, while Rivest and Schapire's learner needs a teacher to answer its equivalence queries and supply it with counterexamples for its incorrect hypotheses. <p> In fact, Angluin shows that if the learner has a method of efficiently reaching all states of the automaton (which is true for graphs with polynomial cover time), then it can exactly learn the automaton using reset. Rivest and Schapire <ref> [19] </ref> show how permutation automata can be exactly learned efficiently without means of a reset and without making equivalence queries. Angluin proves [1] that the problem of exactly learning DFAs from equivalence queries alone is hard. <p> This algorithm closely follows Rivest and Schapire's learning algorithm <ref> [19] </ref>. However, we use new techniques that exploit the small cover time of the automaton in place of relying on a teacher who supplies us with counterexamples to incorrect hypotheses. We name the algorithm Exact-Learn, and its pseudo-code appears in Figure 3. <p> The main problem encountered when the learner does not have means of a reset is that it cannot simply orient itself whenever needed by returning to the starting state. We thus need an alternative way by which the learner can orient itself. As in <ref> [19] </ref>, we overcome the absence of a reset by the use of a homing sequence, defined below. A homing sequence is a sequence such that whenever it is executed, the corresponding output sequence observed uniquely determines the final state reached. <p> We continue in this way until one copy terminates. In order to ensure that we never fill in a distinguishing entry 4 As in <ref> [19] </ref>, we actually need not discard all copies and restart the algorithm, but we may only discard the copy in which the disagreement was found, and construct an adaptive homing sequence which results in a more efficient algorithm. <p> As mentioned previously, Rivest and Schapire <ref> [19] </ref> give an exact learning algorithm that runs in time polynomial in n and log (1=ffi) and does not depend on any other parameter related to the target automaton. However, they rely on a teacher that gives the learner counterexamples to the incorrect hypotheses output by the learner.
Reference: [20] <author> Ronald L. Rivest and David Zuckermann. </author> <title> Private communication. </title> <year> 1992. </year>
Reference-contexts: Rivest and Zuckerman <ref> [20] </ref> construct a pair of automata which both have small cover time, but for which the probability of randomly guessing a sequence which distinguishes between the automata is exponentially small.
Reference: [21] <author> Dana Ron and Ronitt Rubinfeld. </author> <title> Learning fallible finite state automata. </title> <journal> Machine Learning, </journal> <volume> 18 </volume> <pages> 149-185, </pages> <year> 1995. </year>
Reference-contexts: The following works consider the case when the labels of the examples are assumed to be noisy. In <ref> [21] </ref>, an algorithm is given for PAC-learning DFAs with membership queries in the presence of persistent noise. In [10], an algorithm is given for learning DFAs by blurry concepts. 3 2 Preliminaries 2.1 Basic Definitions Let M be the deterministic finite state automaton (DFA) we would like to learn. <p> This is done by running 12 Procedure Estimate-Noise-Rate whose pseudo-code appears in Figure 4, and which is analyzed in the following lemma. A very similar procedure was described in <ref> [21] </ref>. Lemma 5.1 For any given ffi 0 &gt; 0, and &gt; 0, after time polynomial in n, 1=ff, log (1=ffi 0 ) and 1=, Procedure Estimate-Noise-Rate outputs an approximation ^ of , such that with probability at least 1 ffi 0 , j^ j . <p> It directly follows (see <ref> [21] </ref>) that j^ j . We thus assume from here on that we have a good approximation, ^, of .
Reference: [22] <author> Yasubumi Sakakibara. </author> <title> On learning from queries and couterexamples in the presence of noise. </title> <journal> Information Processing Letters, </journal> <volume> 37 </volume> <pages> 279-284, </pages> <year> 1991. </year>
Reference-contexts: In the noisy setting the learning problem becomes harder since the outputs observed may be erroneous. If the learner has means of a reset then the problem can easily be solved <ref> [22] </ref> by running the noise-free algorithm and repeating each walk a large enough number of times so that the majority output observed is the correct output. However, when the learner does not have means of a reset then we encounter several difficulties. <p> In particular we assume that ^ is at most ff=n away from . 5.2 Learning When a Homing Sequence is Known As in the noise free case, we first assume that the algorithm has means of a reset. With this assumption, we use the technique of <ref> [22] </ref> and define a slight modification of Exact-Learn-with-Reset, named Exact-Noisy-Learn-with-Reset, which given a large enough integer N simply repeats each walk to fill in an entry in the table N times, and fills the corresponding entry with the majority observed label.
Reference: [23] <author> Alistair Sinclair and Mark Jerrum. </author> <title> Approximate counting, uniform generation, and rapidly mixing Markov chains. </title> <journal> Information and Computation, </journal> <volume> 82 </volume> <pages> 93-13, </pages> <year> 1989. </year>
Reference-contexts: They also show how their algorithm can be modified and made more efficient if the graph has high conductance <ref> [23] </ref>, where conductance is a measure of the expansion properties of the graph. As for non-exact (approximate) learning, without the aid of queries, Kearns and Valiant [16] show that under certain number theoretical assumptions, the problem of PAC learning DFAs is hard when only given access to random examples.
References-found: 23

