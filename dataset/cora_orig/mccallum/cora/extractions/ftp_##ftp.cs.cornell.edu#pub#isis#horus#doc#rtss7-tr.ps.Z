URL: ftp://ftp.cs.cornell.edu/pub/isis/horus/doc/rtss7-tr.ps.Z
Refering-URL: http://www.cs.cornell.edu/Info/Projects/HORUS/Papers.html
Root-URL: http://www.cs.brown.edu/
Email: e-mail:froy,keng@cs.cornell.edu  
Title: Using Group Communication Technology to Implement a Reliable and Scalable Distributed IN Coprocessor  
Author: Roy Friedman Ken Birman 
Date: March 15, 1996  
Address: Ithaca, NY 14853 USA  
Affiliation: Department of Computer Science Cornell University  
Note: DRAFT Do Not Distribute  This work was supported by ARPA/ONR grant N00014-92-J-1866  
Abstract-found: 0
Intro-found: 1
Reference: [1] <institution> General Recommendations on Telephone Switching and Signalling Intelligent Network. ITU-T Recommendation Q.12xx. </institution>
Reference-contexts: 1 Introduction The SS7 switching network architecture specifies a hierarchical structure for telecommunication switching nodes <ref> [1] </ref>. According to this specification, each switching node is composed of a switch and an Intelligent Networking (IN) coprocessor. The switch must be able to handle a well known set of tasks on its own: call routing for regular calls, hang-up processing, and other simple functions.
Reference: [2] <author> P. A. Alsberg and J. D. Day. </author> <title> A Principle for Resilient Sharing of Distributed Resources. </title> <booktitle> In Proc. of the 2nd International Conference on Software Engineering, </booktitle> <pages> pages 562-570, </pages> <address> San Francisco, CA, </address> <year> 1976. </year>
Reference-contexts: Our approach is slightly different than typical primary-backup protocols, e.g., <ref> [2, 3, 5] </ref>, since in our implementation both the primary and the backup relay the answer even when all other messages arrive on time. This is done in order to overcome a failure of one of the links between the EAs and the switch.
Reference: [3] <author> J. F. Bartlett. </author> <title> A Non-Stop Kernel. </title> <booktitle> In Proc. of the Eighth Symposium on Operating Systems Principles, 1981. In ACM Operating Systems Review 15(5). </booktitle>
Reference-contexts: Our approach is slightly different than typical primary-backup protocols, e.g., <ref> [2, 3, 5] </ref>, since in our implementation both the primary and the backup relay the answer even when all other messages arrive on time. This is done in order to overcome a failure of one of the links between the EAs and the switch.
Reference: [4] <author> A. Basu, V. Buch, W. Vogels, and T. von Eiken. U-Net: </author> <title> A User-Level Network Interface for Parallel and Distributed Computing. </title> <booktitle> In Proc. of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 40-53, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: This improvement also indicates that the efficiency of the network interface being used is extremely important for the ability of the system to scale to large number of QEs. 2 User space interfaces for other high-speed networks, like ATM, have also been developed <ref> [4] </ref>. 8 DRAFT - Do Not Distribute We have measured the performance of the system in failure free runs, during a failure of a QE or an EA, and when a QE or an EA was added to the system.
Reference: [5] <author> A. Bhide, E. Elnozahy, and S. Morgan. </author> <title> A Highly Available Network File Server. </title> <booktitle> In Proc. of the USENIX Conference, </booktitle> <pages> pages 199-205, </pages> <year> 1991. </year>
Reference-contexts: Our approach is slightly different than typical primary-backup protocols, e.g., <ref> [2, 3, 5] </ref>, since in our implementation both the primary and the backup relay the answer even when all other messages arrive on time. This is done in order to overcome a failure of one of the links between the EAs and the switch.
Reference: [6] <author> C. Chang, G. Czajkowski, and T. von Eicken. </author> <title> Design and Performance of Active Messages on the SP-2. </title> <type> Technical Report TR96-1572, </type> <institution> Department of Computer Science, Cornell University, </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: These nodes operate at 66.7 MHz, each equipped with 128 MBytes of memory. The communication was done, through Horus, over the High-Performance Switch of the SP2, using an active messages interface <ref> [6] </ref>. The round-trip latency of active messages on the SP2 for 4 KByte messages, which is roughly the largest message we were sending in our experiments, is 200 microseconds. The maximum achievable throughput using active messages is 34.5 Mbyte/sec, which is well above what we needed for our experiments.
Reference: [7] <author> R. Friedman and R. van Renesse. </author> <title> Packing Messages as a Tool for Boosting the Performance of Total Ordering Protocols. </title> <type> Technical Report TR95-1527, </type> <institution> Department of Computer Science, Cornell University, </institution> <month> July </month> <year> 1995. </year> <note> Submitted for publication. </note>
Reference-contexts: To overcome this problem, we buffer queries at the EAs, and replies at the QEs, such that every 10 milliseconds a sweeper thread sends all buffered queries, or replies, as the case may be, in one Horus message <ref> [7] </ref>. In our initial design, each request and each reply were allocated a new Horus message, and we used a layer of Horus which buffers messages and then on a timely basis packs the buffered messages and sends them as one packed message.
Reference: [8] <author> H. Kopetz and P. Verissimo. </author> <title> Real-Time and Dependability Concepts. </title> <editor> In S. Mullender, editor, </editor> <booktitle> Distributed Systems, chapter 16. </booktitle> <publisher> Addison Wesley, </publisher> <year> 1993. </year>
Reference-contexts: Similar to other (hard and soft) real-time systems, we use the rule that it is better to drop several requests if it seems that there is no chance of servicing them in time, than to service every request but to miss all deadlines <ref> [8] </ref>. A situation where this rule must be applied is likely to happen whenever the system becomes overloaded. That is, if for some reason, the switch suddenly receives more calls than it was designed to handle.
Reference: [9] <author> F. Schneider. </author> <title> Synchronization in Distributed Programs. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 4(2) </volume> <pages> 125-148, </pages> <year> 1982. </year>
Reference-contexts: Note that our approach does not fall into the category of active replication <ref> [9] </ref> either. Active replication means that the primary as well as all the backups act immediately and simultaneously.
Reference: [10] <author> R. van Renesse, K. Birman, R. Friedman, M. Hayden, and D. Karr. </author> <title> A Framework for Protocol Composition in Horus. </title> <booktitle> In Proc. of the 14th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 80-89, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: Assuming that the upgraded software is backward compatible, such an upgrade can take place without interfering with the services provided by the switching node. In this paper we explore the use of group communication technology, developed in the Horus project <ref> [11, 10] </ref>, to implement a distributed IN coprocessor. Horus provides management tools which greatly simplify the task of developing such distributed applications, as well as failure detection and automatic reconfiguration which allows the solution to achieve the necessary fault-tolerant requirements.
Reference: [11] <author> R. van Renesse, K. Birman, and S. Maffeis. Horus: </author> <title> A Flexible Group Communications System. </title> <journal> Communications of the ACM, </journal> <volume> 39(4), </volume> <month> April </month> <year> 1996. </year> <title> 12 DRAFT - Do Not Distribute </title>
Reference-contexts: Assuming that the upgraded software is backward compatible, such an upgrade can take place without interfering with the services provided by the switching node. In this paper we explore the use of group communication technology, developed in the Horus project <ref> [11, 10] </ref>, to implement a distributed IN coprocessor. Horus provides management tools which greatly simplify the task of developing such distributed applications, as well as failure detection and automatic reconfiguration which allows the solution to achieve the necessary fault-tolerant requirements.
Reference: [12] <author> G. Varghese and T. Lauck. </author> <title> Hashed and Hierarchical Timing Wheels: Data Structures for the Efficient Implementation of a Timer Facility. </title> <booktitle> In Proc. of the 11th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 25-38, </pages> <address> Austin, Texas, </address> <month> November </month> <year> 1987. </year> <month> 13 </month>
Reference-contexts: Instead, we decided to aggregate requests into sets of requests that arrive at a proximity of 10 millisecond of each other. These requests are kept in a cyclic set of queues, such that every request is inserted into the current queue, similarly to what is done in <ref> [12] </ref>. (See illustration in Figure 3.) Then, every 10 milliseconds, a sweeper task scans the last queue, i.e., the one that follows the current queue, reissues all the requests which are held in this queue and for which a reply has not arrived yet, and then assigns the last queue to
References-found: 12

