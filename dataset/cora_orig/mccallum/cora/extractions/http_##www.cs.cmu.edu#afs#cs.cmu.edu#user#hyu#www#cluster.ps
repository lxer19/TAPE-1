URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/hyu/www/cluster.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/hyu/www/papers.html
Root-URL: 
Email: (hyu@cs.cmu.edu)  
Title: Automatically Determining Number of Clusters  
Author: Hua Yu 
Abstract: Automatically determining number of clusters in the data is an unsolved/unexplored problem. First I'll show why we need to do this, and whether this is a reasonable problem in text clustering in particular. Then starting from simple 1-d/2-d study, I find BIC (Bayesian Information Criterion) is a useful measure, which by penalizing model fitness by model complexity, usually tells the right number of clusters. Experiments on EM clustering of 1-d/2-d data are presented. In text clustering, I find inter-document similarity matrix, when correctly organized & visualized, a very good representation of the collection. Based on that I tried a Similarity-matrix based clustering algorithm, which gives visually appealing results. However, BIC measure can't be easily extended to text clustering case.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Lei Xu, </author> <title> On Convergence Properties of the EM algorithm for Gaussian Mixtures, </title> <journal> Neural Computation, </journal> <volume> 8, </volume> <pages> 129-151, </pages> <year> 1996 </year>
Reference-contexts: Note to get the likelihood measurement for k-mixture modelling, we need to actually find the optimal k-mixture model that fits the data. This is of course an EM task. EM formular for mixture Gaussian estimation can be found in <ref> [1] </ref>. <p> I tried annealed EM, an EM variant that's said to be robust w.r.t. local maximum, but there's no guarantee and in fact, it didn't work well in my problem. EM convergence problem is well discussed in <ref> [1] </ref>. Another non-parametric method for choosing the right model size is cross-validation, i.e. we keep an eye on the performance on an independent validaton set while fitting the training set with different models. I also examined its usefulness in the simulated study.
Reference: [2] <author> Kenneth Rose, et al., </author> <title> Statistical Mechanics and Phase Transitions in Clustering, </title> <journal> Physical Review Letters, Vol.65, </journal> <volume> 8, </volume> <year> 1990 </year>
Reference-contexts: Experiment on SWB corpus shows similar results. 1.3 Literature Survey There're few paper dedicated to this topic. The only one I found is in <ref> [2] </ref>, a paper on clustering from a physics point of view. <p> There's another way we can go, as described in <ref> [2] </ref>. They call it "simulated annealing" or "stachastic relaxation". Translated into my terms, what they're doing is that they fix the number of clusters to be a number large enough, and each time they set a different likelihood value as target, adjust model parameters to achieve that likelihood. <p> Also how to gradually increase the likelihood, how to set the stepsize is also not obvious. But the idea is clear: we're examining model goodness vs. model complexity, and we can go along either dimension, while keeping an eye on the other. <ref> [2] </ref> is monitoring model complexity while requiring higher and higher model accuracy, the way I'm going is to monitor model accuracy while gradually increasing model complexity.
Reference: [3] <author> S.S.Chen et al., </author> <title> Speaker, Environment and Channel Change Detection and Clustering via the Bayesian Information Criterion, </title> <booktitle> Hub4 Workshop, </booktitle> <year> 1998 </year> <month> 4 </month>
Reference-contexts: The only one I found is in [2], a paper on clustering from a physics point of view. This paper gives a clean theoretical treatment of model complexity vs. model fitness 1 relationship, though it doesn't come up with any experiment or workable solution. <ref> [3] </ref> introduced the BIC (Bayesian Information Criterion), and discussed how it can be used in audio segmentation and audio segments clustering (with a different focus).
Reference: [4] <author> P.Willett, </author> <title> Recent Trends in Hierarchic Document Clustering: a Critical Review </title>
Reference-contexts: Since he only cares about discriminating between single Gaussian and mixture Gaussian with 2 components, he avoids much headaches which general purpose clustering has to face. <ref> [4] </ref> is not directly relevant to my purpose, though he gives a rather comprehensive review of popular clustering methods used in text clustering, and some ideas on testing cluster validity As there's little work done before, I decide to start from some simulated study of 1-d/2-d clustering, get some experience before
Reference: [5] <author> T.Hofmann, et al., </author> <title> Statistical Models for Co-occurance Data, </title> <publisher> M.I.T. A.I. </publisher> <address> Memo No.1625, </address> <year> 1998 </year>
Reference: [6] <author> T.Hofmann, </author> <title> Structuring Document Databases by Automated Clustering and Abstraction 5 </title>
References-found: 6

