URL: ftp://ftp.cs.indiana.edu/pub/techreports/TR352.ps.Z
Refering-URL: http://www.cs.indiana.edu/trindex.html
Root-URL: 
Email: bradford@cs.indiana.edu  
Phone: (812) 855-3609  
Title: Efficient Parallel Dynamic Programming (Revised)  
Author: Phillip G. Bradford 
Date: October 27, 1994  
Address: 215 Lindley Hall Bloomington, IN 47405  
Affiliation: Indiana University Department of Computer Science  
Abstract: 1 An extended abstract of this paper is in the Proceedings of the 30 th Annual Allerton Conference on Communication, Control and Computing, University of Illinois, 185-194, 1992. This is Technical Report # 352 (Revised), Indiana University. Hardcopy is available or a postscript file is obtainable via internet: cs.indiana.edu:/pub/techreports/TR352.ps.Z. Tech-Report revision posted: Oct. 1994. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Apostolico, M. J. Atallah, L. L. Larmore and S. H. McFaddin: </author> <title> "Efficient Parallel Algorithms for String Editing and Related Problems", </title> <journal> SIAM Journal on Computing, </journal> <volume> Vol. 19, No. 5, </volume> <pages> 968-988, </pages> <month> Oct. </month> <year> 1990. </year>
Reference-contexts: Finally, exploiting special properties of the digraphs that model the semigroupoids, a O (log 3 n) time algorithm using n 3 =log n processors is given. In some sense this work extends the results of <ref> [1, 2, 26] </ref>. The work of Hu and Shing [19, 21, 22] supplies a basis for these algorithms, although they are built in a different framework. This paper is an update of [6] and the full version of [7]. <p> It has been conjectured that Hu and Shing's algorithm is optimal [28]. Finally, variations of the string editing problem are often solved with dynamic programming algorithms [11], and parallel polylog time solutions of these problems use O (n 2 ) processors <ref> [1, 2, 26] </ref>. However, these string edit problems differ from the three in Subsection 1.1.1 and have elementary O (n 2 ) sequential dynamic programming solutions. <p> However, these string edit problems differ from the three in Subsection 1.1.1 and have elementary O (n 2 ) sequential dynamic programming solutions. In particular, Apostolico et al. <ref> [1] </ref> use divide and conquer techniques to find shortest paths in special planar digraphs in O (lg n) time using n 2 =lg n processors. Ibarra, Pong and Sohn [26] also use a graph characterization to solve such problems achieving similar results. <p> Since G n has O (n 2 ) vertices, computing a minimum path in O (lg 2 n) time by a parallel matrix multiplication based minimum path algorithm takes n 6 =lg n processors. With specialized minimum path algorithms the processor complexity improves dramatically <ref> [1, 2, 26] </ref>. These methods can compute a shortest path in O (lg 2 n) time with n 2 =lg n processors on a CREW PRAM. In [1, 2, 26] dynamic programming problems are also transformed into graph search problems; these variations of the BPP are used to solve string edit <p> With specialized minimum path algorithms the processor complexity improves dramatically <ref> [1, 2, 26] </ref>. These methods can compute a shortest path in O (lg 2 n) time with n 2 =lg n processors on a CREW PRAM. In [1, 2, 26] dynamic programming problems are also transformed into graph search problems; these variations of the BPP are used to solve string edit problems. 4.1 The Minimum Cost Parenthesization Problem To represent these split parenthesizations we add edges to G n called jumpers. <p> But this is not a well-formed subproduct of the optimal matrix product of all four matrices, that is (M 1 * M 2 ) * (M 3 * M 4 ). This apparent lack of greediness seems to make techniques such as those of <ref> [1, 2, 26] </ref>, fail to work for the MCOP. Note the similarity of a D n graph and a classical dynamic programming table, T , for the matrix chain ordering problem. <p> The converse of this theorem also holds. That is, for any optimal parenthesization of a weighted semigroupoid there is a shortest path in a corresponding D n graph. It is not clear how to generalize the shortest path algorithms in <ref> [1, 2, 26] </ref> to D n graphs. <p> We derive the intuition of this section from the MCOP. From here on D n graphs are the central focus since the balanced version of this problem has been solved efficiently in <ref> [1, 2, 26] </ref>. <p> Proof: The array A contains values from the set f0; 1g, so if A [j] = 0 then A [j] does not have an ANSV match. On the other hand, if A [j] = 1 then A [j] must have an ANSV match since A <ref> [1] </ref> = 0 and A [n + 1] = 0. Now consider the case where A [j] has match [A [i]; A [k]]. This means that for all t such that i &lt; t &lt; k, A [t] also has match [A [i]; A [k]].
Reference: [2] <author> A. Aggarwal and J. Park: </author> <title> "Notes on Searching Multidimensional Monotone Arrays", </title> <booktitle> Proceedings of the 29 th Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> 497-512, </pages> <year> 1988. </year>
Reference-contexts: Finally, exploiting special properties of the digraphs that model the semigroupoids, a O (log 3 n) time algorithm using n 3 =log n processors is given. In some sense this work extends the results of <ref> [1, 2, 26] </ref>. The work of Hu and Shing [19, 21, 22] supplies a basis for these algorithms, although they are built in a different framework. This paper is an update of [6] and the full version of [7]. <p> It has been conjectured that Hu and Shing's algorithm is optimal [28]. Finally, variations of the string editing problem are often solved with dynamic programming algorithms [11], and parallel polylog time solutions of these problems use O (n 2 ) processors <ref> [1, 2, 26] </ref>. However, these string edit problems differ from the three in Subsection 1.1.1 and have elementary O (n 2 ) sequential dynamic programming solutions. <p> Ibarra, Pong and Sohn [26] also use a graph characterization to solve such problems achieving similar results. Aggarwal and Park <ref> [2] </ref> also employ graph characterizations of these problems, but in addition they use properties of certain monotone arrays to find shortest paths. 1.1.3 Structure of this Paper Section 2 gives the computational assumptions of this paper. Section 3 defines the balanced minimum cost parenthesization problem (BPP). <p> Since G n has O (n 2 ) vertices, computing a minimum path in O (lg 2 n) time by a parallel matrix multiplication based minimum path algorithm takes n 6 =lg n processors. With specialized minimum path algorithms the processor complexity improves dramatically <ref> [1, 2, 26] </ref>. These methods can compute a shortest path in O (lg 2 n) time with n 2 =lg n processors on a CREW PRAM. In [1, 2, 26] dynamic programming problems are also transformed into graph search problems; these variations of the BPP are used to solve string edit <p> With specialized minimum path algorithms the processor complexity improves dramatically <ref> [1, 2, 26] </ref>. These methods can compute a shortest path in O (lg 2 n) time with n 2 =lg n processors on a CREW PRAM. In [1, 2, 26] dynamic programming problems are also transformed into graph search problems; these variations of the BPP are used to solve string edit problems. 4.1 The Minimum Cost Parenthesization Problem To represent these split parenthesizations we add edges to G n called jumpers. <p> But this is not a well-formed subproduct of the optimal matrix product of all four matrices, that is (M 1 * M 2 ) * (M 3 * M 4 ). This apparent lack of greediness seems to make techniques such as those of <ref> [1, 2, 26] </ref>, fail to work for the MCOP. Note the similarity of a D n graph and a classical dynamic programming table, T , for the matrix chain ordering problem. <p> The converse of this theorem also holds. That is, for any optimal parenthesization of a weighted semigroupoid there is a shortest path in a corresponding D n graph. It is not clear how to generalize the shortest path algorithms in <ref> [1, 2, 26] </ref> to D n graphs. <p> We derive the intuition of this section from the MCOP. From here on D n graphs are the central focus since the balanced version of this problem has been solved efficiently in <ref> [1, 2, 26] </ref>.
Reference: [3] <author> S. Baase: </author> <title> Computer Algorithms, Second Edition, </title> <publisher> Addison-Wesley, </publisher> <year> 1988. </year>
Reference-contexts: From this foundation, several algorithms that solve a variety of problems follow. The following problems are representative of those to which these methods can be applied (for formal definitions of these problems see <ref> [3, 11] </ref>): * optimal matrix chain ordering: Find an optimal ordering to multiply a matrix chain together, where the matrices are pairwise compatible but of varying dimensions. * optimal convex polygon triangulation: Find an optimal triangularization of a convex polygon given a triangle cost metric. * optimal binary search tree construction:
Reference: [4] <author> O. Berkman, D. Breslauer, Z. Galil, B. Schieber and U. Vishkin: </author> <title> "Highly Parallelizable Problems", </title> <booktitle> Symposium on the Theory on Computing, </booktitle> <pages> 309-319, </pages> <year> 1989. </year>
Reference-contexts: Next follows a subpoly--logarithmic time and sublinear processor algorithm that provides an approximate solution that is guaranteed to be within 15.5% of optimality. This algorithm follows from the work of Chin [10], and Hu and Shing [20] combined with that of Berkman et al. <ref> [4] </ref>. Finally, exploiting special properties of the digraphs that model the semigroupoids, a O (log 3 n) time algorithm using n 3 =log n processors is given. In some sense this work extends the results of [1, 2, 26]. <p> Throughout this paper we focus on the MCOP since solutions to the MCOP are often given as standard examples of the dynamic programming paradigm. We model these weights and related graph nodes by the nesting levels of parentheses, using the all nearest smaller value problem of Berkman et al. <ref> [4, 5] </ref>. Section 6 contains a parallel approximation algorithm for the MCOP. This algorithm does a linear amount of work. Intuitively, this algorithm works by removing relatively heavy weights so that the remaining weight list closely approximates an optimal solution. <p> Intuitively, this technique allows matrix dimensions to approximate the nesting level of matched parentheses. Given an associative product where the level of each parenthesis in an optimal product is known, we can compute the parenthesization of this associative product by solving the following all nearest smaller value (ANSV) problem <ref> [4, 5] </ref>: Given w 1 ; w 2 ; : : : ; w n drawn from a totally ordered set, for each w i find the largest j, where 1 j &lt; i, and smallest k where i &lt; k n, so that w j &lt; w i and w <p> Although, Berkman et al. proved the next theorem for ANSV matches, it follows as a direct result of the relationship of matches in the weight list and critical nodes in D n graphs. Theorem 7 (Berkman et al. <ref> [4] </ref>) All critical nodes can be computed in O (lg lg n) and O (lg n) time using n=lg lg n and n=lg n processors, respectively. <p> graph, then both w i &lt; w i+1 &lt; &lt; w j and w k &gt; w k+1 &gt; &gt; w t+1 follow from Theorem 12. 6.1 A Parallel Approximation Algorithm for the MCOP Combining results of Chin [10], and Hu and Shing [20] with those of Berkman et al. <ref> [4] </ref> this section develops an O (lg lg n) time and n=lg lg n processor approximation algorithm for the MCOP. This algorithm approximates the MCOP to within 15:5% of optimality. In addition, the processor time product of this algorithm is linear. <p> Theorem 24 also applies to the optimal convex triangulation problem with the standard triangle cost metrics [11, 21]. 7.2 Acknowledgments Greg Shannon has been very supportive of this work; in particular he made me aware of reference <ref> [4] </ref>. Gregory J. E. Rawlins has been extremely helpful with the presentation, as was Sudhir Rao and Andrea Rafael. Alok Aggarwal has been an inspiration for this work. Artur Czumaj was also helpful. One of the anonymous referee's comments contributed greatly to this paper. 30
Reference: [5] <author> O. Berkman, B. Schieber and U. Vishkin: </author> <title> "Optimal Doubly Logarithmic Parallel Algorithms Based on Finding All Nearest Smaller Values," </title> <journal> J. of Algorithms, </journal> <volume> Vol. 14, </volume> <pages> 344-370, </pages> <year> 1993. </year>
Reference-contexts: Throughout this paper we focus on the MCOP since solutions to the MCOP are often given as standard examples of the dynamic programming paradigm. We model these weights and related graph nodes by the nesting levels of parentheses, using the all nearest smaller value problem of Berkman et al. <ref> [4, 5] </ref>. Section 6 contains a parallel approximation algorithm for the MCOP. This algorithm does a linear amount of work. Intuitively, this algorithm works by removing relatively heavy weights so that the remaining weight list closely approximates an optimal solution. <p> Intuitively, this technique allows matrix dimensions to approximate the nesting level of matched parentheses. Given an associative product where the level of each parenthesis in an optimal product is known, we can compute the parenthesization of this associative product by solving the following all nearest smaller value (ANSV) problem <ref> [4, 5] </ref>: Given w 1 ; w 2 ; : : : ; w n drawn from a totally ordered set, for each w i find the largest j, where 1 j &lt; i, and smallest k where i &lt; k n, so that w j &lt; w i and w
Reference: [6] <author> P. G. Bradford: </author> <title> "Efficient Parallel Dynamic Programming," </title> <type> Technical Report # 352, </type> <institution> Indiana University, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: In some sense this work extends the results of [1, 2, 26]. The work of Hu and Shing [19, 21, 22] supplies a basis for these algorithms, although they are built in a different framework. This paper is an update of <ref> [6] </ref> and the full version of [7]. Much subsequent work has appeared, take for example [13, 29, 8, 30]. In addition, [12] reports very similar results to those in section 6 of this paper.
Reference: [7] <author> P. G. Bradford: </author> <title> "Efficient Parallel Dynamic Programming," </title> <booktitle> Extended Abstract in the Proceed ings of the 30 th Allerton Conference on Communication, Control and Computation, </booktitle> <institution> University of Illinois at Urbana-Champaign, </institution> <month> 185-194, </month> <year> 1992. </year>
Reference-contexts: In some sense this work extends the results of [1, 2, 26]. The work of Hu and Shing [19, 21, 22] supplies a basis for these algorithms, although they are built in a different framework. This paper is an update of [6] and the full version of <ref> [7] </ref>. Much subsequent work has appeared, take for example [13, 29, 8, 30]. In addition, [12] reports very similar results to those in section 6 of this paper.
Reference: [8] <author> P. G. Bradford, G. J. E. Rawlins and G. E. Shannon: </author> <title> "Matrix Chain Ordering in Polylog Time with n=lg n Processors," </title> <type> Technical Report # 360, </type> <institution> Indiana University, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: The work of Hu and Shing [19, 21, 22] supplies a basis for these algorithms, although they are built in a different framework. This paper is an update of [6] and the full version of [7]. Much subsequent work has appeared, take for example <ref> [13, 29, 8, 30] </ref>. In addition, [12] reports very similar results to those in section 6 of this paper.
Reference: [9] <author> A. K. Chandra: </author> <title> "Computing Matrix Chain Products in Near Optimal Time", </title> <institution> IBM Research Report RC-5625, </institution> <month> Oct. </month> <year> 1975. </year>
Reference-contexts: The second step of the approximation algorithm requires that we now form the appropriate linear product with the remaining matrices. The depth of the parentheses provides an approximation to within 15:5% of optimal for the MCOP. This is due to Chandra <ref> [9] </ref>, Chin [10], and Hu and Shing [20]. Theorem 15 (Hu and Shing [20]) If a weight list w 1 ; w 2 ; ; w r+1 is reduced, then the MCOP can be solved to within a multiplicative factor of 1:155 from optimal in constant time using n processors.
Reference: [10] <author> F. Y. Chin: </author> <title> "An O(n) Algorithm for Determining Near-Optimal Computation Order of Matrix Chain Products", </title> <journal> Communications of the ACM, </journal> <volume> Vol. 21, No. 7, </volume> <pages> 544-549, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: Next follows a subpoly--logarithmic time and sublinear processor algorithm that provides an approximate solution that is guaranteed to be within 15.5% of optimality. This algorithm follows from the work of Chin <ref> [10] </ref>, and Hu and Shing [20] combined with that of Berkman et al. [4]. Finally, exploiting special properties of the digraphs that model the semigroupoids, a O (log 3 n) time algorithm using n 3 =log n processors is given. <p> m &gt; w m+1 . 2 Take a D (i;t) (j;k) canonical graph, then both w i &lt; w i+1 &lt; &lt; w j and w k &gt; w k+1 &gt; &gt; w t+1 follow from Theorem 12. 6.1 A Parallel Approximation Algorithm for the MCOP Combining results of Chin <ref> [10] </ref>, and Hu and Shing [20] with those of Berkman et al. [4] this section develops an O (lg lg n) time and n=lg lg n processor approximation algorithm for the MCOP. This algorithm approximates the MCOP to within 15:5% of optimality. <p> Proof of this Theorem is left to the literature, see <ref> [10] </ref> and [20] for different proofs. When w i+1 &gt; maxfw i ; w i+2 g fails to hold Equation 6.1 cannot hold, so there is no gain in assuming w i+1 &gt; maxfw i ; w i+2 g. <p> Therefore, using the ANSV problem, the values in the weight list approximate the optimal level of parentheses. 16 A list of weights is reduced iff for all weights, say w i+1 , with ANSV match [w i ; w i+2 ] Equation 6.1 fails to hold, <ref> [10] </ref>. A reduced weight list may be non-monotonic. Now we generalize Equation 6.1. Suppose by Theorem 13 that (a i * a i+1 ) is in an optimal parenthesization. <p> Given the weight list l 1 = w 1 ; w 2 ; : : : ; w n+1 the approximation algorithm is <ref> [10, 18, 20] </ref>: 1. Reduce the weight list l 1 giving the weight list l 2 , renumbering l 2 to be l 2 = w 1 ; w 2 ; : : : ; w r+1 where w 1 = min 1ir+1 fw i g. 2. <p> The second step of the approximation algorithm requires that we now form the appropriate linear product with the remaining matrices. The depth of the parentheses provides an approximation to within 15:5% of optimal for the MCOP. This is due to Chandra [9], Chin <ref> [10] </ref>, and Hu and Shing [20]. Theorem 15 (Hu and Shing [20]) If a weight list w 1 ; w 2 ; ; w r+1 is reduced, then the MCOP can be solved to within a multiplicative factor of 1:155 from optimal in constant time using n processors.
Reference: [11] <author> T. H. Cormen, C. E. Leiserson and R. L. Rivest: </author> <title> Introduction to Algorithms, </title> <publisher> McGraw Hill, </publisher> <year> 1990. </year>
Reference-contexts: From this foundation, several algorithms that solve a variety of problems follow. The following problems are representative of those to which these methods can be applied (for formal definitions of these problems see <ref> [3, 11] </ref>): * optimal matrix chain ordering: Find an optimal ordering to multiply a matrix chain together, where the matrices are pairwise compatible but of varying dimensions. * optimal convex polygon triangulation: Find an optimal triangularization of a convex polygon given a triangle cost metric. * optimal binary search tree construction: <p> It has been conjectured that Hu and Shing's algorithm is optimal [28]. Finally, variations of the string editing problem are often solved with dynamic programming algorithms <ref> [11] </ref>, and parallel polylog time solutions of these problems use O (n 2 ) processors [1, 2, 26]. However, these string edit problems differ from the three in Subsection 1.1.1 and have elementary O (n 2 ) sequential dynamic programming solutions. <p> This is at least partially due to the jumper's weights; computing these weights seems difficult. 4.1.1 Constructing a D n Graph Constructing a D n graph can be done by starting with a weighted digraph G n and then performing incremental path relaxation <ref> [11] </ref> while adding new jumpers. A shortest path in D n is found simultaneously. We accomplish these two goals by using a variation of a matrix multiplication based all pairs shortest path algorithm. <p> Also, the minimum distances 7 between all pairs of nodes in G n up to 2 nodes apart are now available. With this, construct all jumpers of length 2. For length 2 horizontal jumpers, this is done by relaxing <ref> [11] </ref> them with the two horizontal edges they are directly above. In particular, having more than one path from a source to a destination relaxing is the process of finding the minimum of these paths. In Figure 4.5, the min operations are path relaxations. <p> This algorithm uses O (n) nodes in a D n graph to solve the MCOP. Thus we can solve the MCOP by using only O (n) elements of a classical dynamic programming table. Theorem 24 also applies to the optimal convex triangulation problem with the standard triangle cost metrics <ref> [11, 21] </ref>. 7.2 Acknowledgments Greg Shannon has been very supportive of this work; in particular he made me aware of reference [4]. Gregory J. E. Rawlins has been extremely helpful with the presentation, as was Sudhir Rao and Andrea Rafael. Alok Aggarwal has been an inspiration for this work.
Reference: [12] <author> A. Czumaj: </author> <title> "An Optimal Parallel Algorithm for Computing a Near-Optimal Order of Matrix Multiplications," </title> <publisher> SWAT, Springer Verlag, </publisher> <pages> LNCS # 621 , 62-72, </pages> <year> 1992. </year>
Reference-contexts: This paper is an update of [6] and the full version of [7]. Much subsequent work has appeared, take for example [13, 29, 8, 30]. In addition, <ref> [12] </ref> reports very similar results to those in section 6 of this paper.
Reference: [13] <author> A. Czumaj: </author> <title> "Parallel algorithm for the matrix chain product and the optimal triangulation problem (Extended Abstract)," </title> <booktitle> STACS 93, </booktitle> <publisher> Springer Verlag, LNCS # 665, </publisher> <pages> 294-305, </pages> <year> 1993. </year> <month> 31 </month>
Reference-contexts: The work of Hu and Shing [19, 21, 22] supplies a basis for these algorithms, although they are built in a different framework. This paper is an update of [6] and the full version of [7]. Much subsequent work has appeared, take for example <ref> [13, 29, 8, 30] </ref>. In addition, [12] reports very similar results to those in section 6 of this paper.
Reference: [14] <author> L. E. Deimel, Jr. and T. A. Lampe: </author> <title> "An Invariance Theorem Concerning Optimal Computa--tion of Matrix Chain Products," </title> <institution> North Carolina State Univ. </institution> <note> Tech Report # TR79-14. </note>
Reference-contexts: We derive the intuition of this section from the MCOP. From here on D n graphs are the central focus since the balanced version of this problem has been solved efficiently in [1, 2, 26]. Letting fl be a cyclic rotation function, Theorem 5 (Deimel and Lampe <ref> [14] </ref>; Hu and Shing [21]) Given an instance of the MCOP with the weight list l 1 = w 1 ; w 2 ; : : : ; w n+1 and cyclically rotating it getting l 2 = w fl (1) ; w fl (2) ; : : : ; w
Reference: [15] <author> Z. Galil and K. Park: </author> <title> "Parallel Dynamic Programming," </title> <note> 1991, Submitted. </note>
Reference-contexts: In 1988, Rytter used pebbling games to show that these same problems can be solved on a CREW PRAM in O (lg 2 n) time with n 6 =lg n processors. Recently, Huang, Liu and Viswanathan [23] and Galil and Park <ref> [15] </ref> give algorithms that improve this processor complexity by polylog factors. Using a graph structure that is analogous to the classical dynamic programming table, this paper improves these results. First, this graph characterization leads to a polylog time and n 6 =lg n processor algorithm that solves these problems. <p> He solves the three problems mentioned in the previous subsection in O (log 2 n) time with n 6 =log n processors on a CRCW PRAM. Huang, Liu and Viswanathan [23] and Galil and Park <ref> [15] </ref> give algorithms that solve this problem in O (lg 2 n) time using n 6 =! lg 5 n and n 6 =! lg 6 n processors, respectively.
Reference: [16] <author> A. Gibbons and W. Rytter: </author> <title> Efficient Parallel Algorithms, </title> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference: [17] <author> D. S. Hirschberg and L. L. Larmore, </author> <title> "The Least Weight Subsequence Problem", </title> <journal> SIAM J. on Computing, </journal> <volume> Vol. 16, No. 4, </volume> <pages> 628-638, </pages> <year> 1987. </year>
Reference: [18] <author> T. C. Hu: </author> <title> Combinatorial Algorithms, </title> <publisher> Addison-Wesley, </publisher> <year> 1982. </year>
Reference-contexts: Given the weight list l 1 = w 1 ; w 2 ; : : : ; w n+1 the approximation algorithm is <ref> [10, 18, 20] </ref>: 1. Reduce the weight list l 1 giving the weight list l 2 , renumbering l 2 to be l 2 = w 1 ; w 2 ; : : : ; w r+1 where w 1 = min 1ir+1 fw i g. 2.
Reference: [19] <author> T. C. Hu and M. T. Shing: </author> <title> "Some Theorems about Matrix Multiplication", </title> <booktitle> Proceedings of the 21 st Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> 28-35, </pages> <year> 1980. </year>
Reference-contexts: Finally, exploiting special properties of the digraphs that model the semigroupoids, a O (log 3 n) time algorithm using n 3 =log n processors is given. In some sense this work extends the results of [1, 2, 26]. The work of Hu and Shing <ref> [19, 21, 22] </ref> supplies a basis for these algorithms, although they are built in a different framework. This paper is an update of [6] and the full version of [7]. Much subsequent work has appeared, take for example [13, 29, 8, 30]. <p> But the best serial solution of the matrix chain ordering problem known is Hu and Shing's O (n lg n) algorithm <ref> [19, 21, 22] </ref>. It has been conjectured that Hu and Shing's algorithm is optimal [28]. Finally, variations of the string editing problem are often solved with dynamic programming algorithms [11], and parallel polylog time solutions of these problems use O (n 2 ) processors [1, 2, 26]. <p> For example, in a monotone weight list there are no ANSV matches for any weight. Now we investigate the effect monotonicity has on weight lists. This is interesting because we can solve any instance of the MCOP very efficiently if its weight list is monotonic. As in <ref> [19] </ref> let kw i : w k k = j=i w j w j+1 , where all such pair-wise sums can be computed by performing one parallel partial prefix. Theorem 6 (Hu and Shing [19]) The vector F [i] = kw 1 : w i k can be computed by a <p> As in <ref> [19] </ref> let kw i : w k k = j=i w j w j+1 , where all such pair-wise sums can be computed by performing one parallel partial prefix. Theorem 6 (Hu and Shing [19]) The vector F [i] = kw 1 : w i k can be computed by a parallel partial prefix. As suggested by this last theorem, we can compute kw i : w k k by performing one subtraction kw i : w k k = F [k] F [i]. <p> By solving the ANSV problem it is possible to isolate certain nodes that are central to finding shortest paths in D n graphs. The next definition is originally due to Hu and Shing <ref> [19] </ref>, although they present it in a different framework: In D n , a critical node is a node (i; k) such that w j &gt; maxfw i ; w k+1 g for i &lt; j k. <p> In a D n graph the structure joining all of the critical nodes is a canonical tree, see also Hu and Shing <ref> [19, 21, 22] </ref>. Define the leaves, edges, and internal nodes of a canonical tree as follows. Initially, in every canonical subgraph D (1;m) the critical node (1; m) is a leaf and is denoted by (1; m) to distinguish it from other critical nodes.
Reference: [20] <author> T. C. Hu and M. T. Shing: </author> <title> "An O(n) Algorithm to Find a Near-Optimum Partition of a Convex Polygon", </title> <journal> J. of Algorithms, </journal> <volume> Vol. 2, </volume> <pages> 122-138, </pages> <year> 1981. </year>
Reference-contexts: Next follows a subpoly--logarithmic time and sublinear processor algorithm that provides an approximate solution that is guaranteed to be within 15.5% of optimality. This algorithm follows from the work of Chin [10], and Hu and Shing <ref> [20] </ref> combined with that of Berkman et al. [4]. Finally, exploiting special properties of the digraphs that model the semigroupoids, a O (log 3 n) time algorithm using n 3 =log n processors is given. In some sense this work extends the results of [1, 2, 26]. <p> 2 Take a D (i;t) (j;k) canonical graph, then both w i &lt; w i+1 &lt; &lt; w j and w k &gt; w k+1 &gt; &gt; w t+1 follow from Theorem 12. 6.1 A Parallel Approximation Algorithm for the MCOP Combining results of Chin [10], and Hu and Shing <ref> [20] </ref> with those of Berkman et al. [4] this section develops an O (lg lg n) time and n=lg lg n processor approximation algorithm for the MCOP. This algorithm approximates the MCOP to within 15:5% of optimality. In addition, the processor time product of this algorithm is linear. <p> Theorem 13 (Hu and Shing <ref> [20] </ref>) If w 1 w i w i+2 + w i w i+1 w i+2 &lt; w 1 w i w i+1 + w 1 w i+1 w i+2 (6.1) then the product (a i * a i+1 ) is in an optimal parenthesization. <p> Proof of this Theorem is left to the literature, see [10] and <ref> [20] </ref> for different proofs. When w i+1 &gt; maxfw i ; w i+2 g fails to hold Equation 6.1 cannot hold, so there is no gain in assuming w i+1 &gt; maxfw i ; w i+2 g. <p> Given the weight list l 1 = w 1 ; w 2 ; : : : ; w n+1 the approximation algorithm is <ref> [10, 18, 20] </ref>: 1. Reduce the weight list l 1 giving the weight list l 2 , renumbering l 2 to be l 2 = w 1 ; w 2 ; : : : ; w r+1 where w 1 = min 1ir+1 fw i g. 2. <p> The second step of the approximation algorithm requires that we now form the appropriate linear product with the remaining matrices. The depth of the parentheses provides an approximation to within 15:5% of optimal for the MCOP. This is due to Chandra [9], Chin [10], and Hu and Shing <ref> [20] </ref>. Theorem 15 (Hu and Shing [20]) If a weight list w 1 ; w 2 ; ; w r+1 is reduced, then the MCOP can be solved to within a multiplicative factor of 1:155 from optimal in constant time using n processors. <p> The depth of the parentheses provides an approximation to within 15:5% of optimal for the MCOP. This is due to Chandra [9], Chin [10], and Hu and Shing <ref> [20] </ref>. Theorem 15 (Hu and Shing [20]) If a weight list w 1 ; w 2 ; ; w r+1 is reduced, then the MCOP can be solved to within a multiplicative factor of 1:155 from optimal in constant time using n processors.
Reference: [21] <author> T. C. Hu and M. T. Shing: </author> <title> "Computation of Matrix Product Chains. Part I", </title> <journal> SIAM J. on Computing, </journal> <volume> Vol. 11, No. 3, </volume> <pages> 362-373, </pages> <year> 1982. </year>
Reference-contexts: Finally, exploiting special properties of the digraphs that model the semigroupoids, a O (log 3 n) time algorithm using n 3 =log n processors is given. In some sense this work extends the results of [1, 2, 26]. The work of Hu and Shing <ref> [19, 21, 22] </ref> supplies a basis for these algorithms, although they are built in a different framework. This paper is an update of [6] and the full version of [7]. Much subsequent work has appeared, take for example [13, 29, 8, 30]. <p> But the best serial solution of the matrix chain ordering problem known is Hu and Shing's O (n lg n) algorithm <ref> [19, 21, 22] </ref>. It has been conjectured that Hu and Shing's algorithm is optimal [28]. Finally, variations of the string editing problem are often solved with dynamic programming algorithms [11], and parallel polylog time solutions of these problems use O (n 2 ) processors [1, 2, 26]. <p> From here on D n graphs are the central focus since the balanced version of this problem has been solved efficiently in [1, 2, 26]. Letting fl be a cyclic rotation function, Theorem 5 (Deimel and Lampe [14]; Hu and Shing <ref> [21] </ref>) Given an instance of the MCOP with the weight list l 1 = w 1 ; w 2 ; : : : ; w n+1 and cyclically rotating it getting l 2 = w fl (1) ; w fl (2) ; : : : ; w fl (n+1) , then <p> The next theorem was originally proved by Hu and Shing in a different framework and follows from the properties of ANSV matches. Theorem 8 (Hu and Shing <ref> [21] </ref>) In any instance of the matrix chain ordering problem all critical nodes are compatible. Proof: Suppose D n represents an instance of the matrix chain ordering problem with two noncompatible critical nodes (i; s) and (j; t). <p> For example, a monotone weight list does not have any critical nodes. The next lemma follows from the ANSV characterization of critical nodes. Lemma 5 (Hu and Shing <ref> [21] </ref>) Any D n graph has at most n 1 critical nodes. Proof: Take a list of n + 1 weights w 1 ; w 2 ; : : :; w n+1 making up the edge weights of some D n graph. <p> This generalizes to paths with more than one jumper. In our terminology we have the following result of Hu and Shing <ref> [21, Corollary 3] </ref>, Theorem 16 (Hu and Shing [21]) In any canonical graph, the sum of the two products w i w j+1 w k+1 + w j+1 w j+2 w k+1 where i &lt; j &lt; k, cannot contribute to the weight of any shortest path iff w k+1 &gt; <p> This generalizes to paths with more than one jumper. In our terminology we have the following result of Hu and Shing [21, Corollary 3], Theorem 16 (Hu and Shing <ref> [21] </ref>) In any canonical graph, the sum of the two products w i w j+1 w k+1 + w j+1 w j+2 w k+1 where i &lt; j &lt; k, cannot contribute to the weight of any shortest path iff w k+1 &gt; w j+1 &gt; w i . <p> Theorem 18 (Hu and Shing <ref> [21] </ref>) A shortest path to a critical node (i; j) in a D (1;m) graph is either along a straight unit path to (i; j), along the path of critical nodes to (i; j), or along subpaths of critical nodes connected together by angular paths and finally to (i; j). <p> In essence, this result gives some of the power of the greedy principle together with the principle of optimality. That is, with this result we can isolate some substructures that are necessarily in an optimal superstructure. Theorem 19 (Hu and Shing <ref> [21] </ref>) Given a weight list w 1 ; : : : ; w n+1 with the three smallest weights w 1 &lt; w i+1 &lt; w v+1 , the products w 1 w i+1 and w 1 w v+1 are in some associative product (s) in an optimal parenthesization. <p> In a D n graph the structure joining all of the critical nodes is a canonical tree, see also Hu and Shing <ref> [19, 21, 22] </ref>. Define the leaves, edges, and internal nodes of a canonical tree as follows. Initially, in every canonical subgraph D (1;m) the critical node (1; m) is a leaf and is denoted by (1; m) to distinguish it from other critical nodes. <p> This algorithm uses O (n) nodes in a D n graph to solve the MCOP. Thus we can solve the MCOP by using only O (n) elements of a classical dynamic programming table. Theorem 24 also applies to the optimal convex triangulation problem with the standard triangle cost metrics <ref> [11, 21] </ref>. 7.2 Acknowledgments Greg Shannon has been very supportive of this work; in particular he made me aware of reference [4]. Gregory J. E. Rawlins has been extremely helpful with the presentation, as was Sudhir Rao and Andrea Rafael. Alok Aggarwal has been an inspiration for this work.
Reference: [22] <author> T. C. Hu and M. T. Shing: </author> <title> "Computation of Matrix Product Chains. Part II", </title> <journal> SIAM J. on Computing, </journal> <volume> Vol. 13, No. 2, </volume> <pages> 228-251, </pages> <year> 1984. </year>
Reference-contexts: Finally, exploiting special properties of the digraphs that model the semigroupoids, a O (log 3 n) time algorithm using n 3 =log n processors is given. In some sense this work extends the results of [1, 2, 26]. The work of Hu and Shing <ref> [19, 21, 22] </ref> supplies a basis for these algorithms, although they are built in a different framework. This paper is an update of [6] and the full version of [7]. Much subsequent work has appeared, take for example [13, 29, 8, 30]. <p> But the best serial solution of the matrix chain ordering problem known is Hu and Shing's O (n lg n) algorithm <ref> [19, 21, 22] </ref>. It has been conjectured that Hu and Shing's algorithm is optimal [28]. Finally, variations of the string editing problem are often solved with dynamic programming algorithms [11], and parallel polylog time solutions of these problems use O (n 2 ) processors [1, 2, 26]. <p> In a D n graph the structure joining all of the critical nodes is a canonical tree, see also Hu and Shing <ref> [19, 21, 22] </ref>. Define the leaves, edges, and internal nodes of a canonical tree as follows. Initially, in every canonical subgraph D (1;m) the critical node (1; m) is a leaf and is denoted by (1; m) to distinguish it from other critical nodes.
Reference: [23] <author> S.-H. S. Huang, H. Liu, V. Viswanathan: </author> <title> "Parallel Dynamic Programming," </title> <booktitle> Proceedings of the 2 nd IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> 497-500, </pages> <year> 1990. </year>
Reference-contexts: In 1988, Rytter used pebbling games to show that these same problems can be solved on a CREW PRAM in O (lg 2 n) time with n 6 =lg n processors. Recently, Huang, Liu and Viswanathan <ref> [23] </ref> and Galil and Park [15] give algorithms that improve this processor complexity by polylog factors. Using a graph structure that is analogous to the classical dynamic programming table, this paper improves these results. <p> He solves the three problems mentioned in the previous subsection in O (log 2 n) time with n 6 =log n processors on a CRCW PRAM. Huang, Liu and Viswanathan <ref> [23] </ref> and Galil and Park [15] give algorithms that solve this problem in O (lg 2 n) time using n 6 =! lg 5 n and n 6 =! lg 6 n processors, respectively.
Reference: [24] <author> J. JaJa: </author> <title> An Introduction to Parallel Algorithms, </title> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference-contexts: After the appropriate preprocessing, by Theorem 6, we can compute the initial cost of each of these O (n) tree edges in constant time with one processor each. Assume the standard tree contraction algorithm <ref> [24] </ref> except for the prune operation. Along with the new prune operation, there is an ordering of the leaves that prevents the simultaneous pruning of two adjacent leaves. <p> Otherwise, say w k+1 &lt; w i &lt; w j+1 , then prune leaf (i; j). While, standard tree contraction algorithms use the Euler Tour Technique <ref> [24, 27] </ref> to number the leaves appropriately, as we have just seen the canonical nodes often provide a natural prune ordering. The viability of this natural leaf numbering follows by induction.
Reference: [25] <author> L. L. Larmore and W. Rytter: </author> <title> "Efficient Sublinear Time Parallel Algorithms for the Recognition of Context-Free Languages", </title> <booktitle> Proceedings of 2 nd Scandinavian Workshop on Algorithm Theory 1992, </booktitle> <publisher> Springer Verlag, LNCS #577, </publisher> <year> 1992. </year>
Reference: [26] <author> O. H. Ibarra, T.-C. Pong and S. M. Sohn: </author> <title> "Hypercube Algorithms for Some String Comparison Problems", </title> <booktitle> Proceedings of the IEEE International Conference on Parallel Processing, </booktitle> <pages> 190-193, </pages> <year> 1988. </year>
Reference-contexts: Finally, exploiting special properties of the digraphs that model the semigroupoids, a O (log 3 n) time algorithm using n 3 =log n processors is given. In some sense this work extends the results of <ref> [1, 2, 26] </ref>. The work of Hu and Shing [19, 21, 22] supplies a basis for these algorithms, although they are built in a different framework. This paper is an update of [6] and the full version of [7]. <p> It has been conjectured that Hu and Shing's algorithm is optimal [28]. Finally, variations of the string editing problem are often solved with dynamic programming algorithms [11], and parallel polylog time solutions of these problems use O (n 2 ) processors <ref> [1, 2, 26] </ref>. However, these string edit problems differ from the three in Subsection 1.1.1 and have elementary O (n 2 ) sequential dynamic programming solutions. <p> In particular, Apostolico et al. [1] use divide and conquer techniques to find shortest paths in special planar digraphs in O (lg n) time using n 2 =lg n processors. Ibarra, Pong and Sohn <ref> [26] </ref> also use a graph characterization to solve such problems achieving similar results. <p> Since G n has O (n 2 ) vertices, computing a minimum path in O (lg 2 n) time by a parallel matrix multiplication based minimum path algorithm takes n 6 =lg n processors. With specialized minimum path algorithms the processor complexity improves dramatically <ref> [1, 2, 26] </ref>. These methods can compute a shortest path in O (lg 2 n) time with n 2 =lg n processors on a CREW PRAM. In [1, 2, 26] dynamic programming problems are also transformed into graph search problems; these variations of the BPP are used to solve string edit <p> With specialized minimum path algorithms the processor complexity improves dramatically <ref> [1, 2, 26] </ref>. These methods can compute a shortest path in O (lg 2 n) time with n 2 =lg n processors on a CREW PRAM. In [1, 2, 26] dynamic programming problems are also transformed into graph search problems; these variations of the BPP are used to solve string edit problems. 4.1 The Minimum Cost Parenthesization Problem To represent these split parenthesizations we add edges to G n called jumpers. <p> But this is not a well-formed subproduct of the optimal matrix product of all four matrices, that is (M 1 * M 2 ) * (M 3 * M 4 ). This apparent lack of greediness seems to make techniques such as those of <ref> [1, 2, 26] </ref>, fail to work for the MCOP. Note the similarity of a D n graph and a classical dynamic programming table, T , for the matrix chain ordering problem. <p> The converse of this theorem also holds. That is, for any optimal parenthesization of a weighted semigroupoid there is a shortest path in a corresponding D n graph. It is not clear how to generalize the shortest path algorithms in <ref> [1, 2, 26] </ref> to D n graphs. <p> We derive the intuition of this section from the MCOP. From here on D n graphs are the central focus since the balanced version of this problem has been solved efficiently in <ref> [1, 2, 26] </ref>.
Reference: [27] <author> R. M. Karp and V. Ramachandran: </author> <title> "Parallel Algorithms for Shared Memory Machines", Chapter 17 in Handbook of Theoretical Computer Science, Vol. A, Algorithms and Complexity, </title> <editor> V. Van Leeuwen|editor, </editor> <publisher> Elsevier, </publisher> <year> 1990. </year>
Reference-contexts: Otherwise, say w k+1 &lt; w i &lt; w j+1 , then prune leaf (i; j). While, standard tree contraction algorithms use the Euler Tour Technique <ref> [24, 27] </ref> to number the leaves appropriately, as we have just seen the canonical nodes often provide a natural prune ordering. The viability of this natural leaf numbering follows by induction. <p> Therefore, choosing to do them simultaneously makes most sense. But, any number of nested bands must be merged in a way to avoid conflicts. This is easily done using the Euler Tour Technique for numbering appropriately for tree contraction, see <ref> [27] </ref>. The next lemma shows the correctness of pruning canonical subgraphs of the form D (1;m) . This is necessary for canonical subtrees as in Figure 7.12.
Reference: [28] <author> P. Ramanan: </author> <title> "A New Lower Bound Technique and its Application: Tight Lower Bounds for a Polygon Triangularization Problem", </title> <booktitle> Proceedings of the Second Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> 281-290, </pages> <year> 1991. </year>
Reference-contexts: But the best serial solution of the matrix chain ordering problem known is Hu and Shing's O (n lg n) algorithm [19, 21, 22]. It has been conjectured that Hu and Shing's algorithm is optimal <ref> [28] </ref>. Finally, variations of the string editing problem are often solved with dynamic programming algorithms [11], and parallel polylog time solutions of these problems use O (n 2 ) processors [1, 2, 26].
Reference: [29] <author> P. Ramanan: </author> <title> "An Efficient Parallel Algorithm for Finding an Optimal Order of Computing a Matrix Chain Product," </title> <type> Technical Report, </type> <institution> WSUCS-92-2, Wichita State University, </institution> <month> June, </month> <year> 1992. </year>
Reference-contexts: The work of Hu and Shing [19, 21, 22] supplies a basis for these algorithms, although they are built in a different framework. This paper is an update of [6] and the full version of [7]. Much subsequent work has appeared, take for example <ref> [13, 29, 8, 30] </ref>. In addition, [12] reports very similar results to those in section 6 of this paper.
Reference: [30] <author> P. Ramanan: </author> <title> "An Efficient Parallel Algorithm for the Matrix Chain Product Problem," </title> <type> Technical Report, </type> <institution> WSUCS-93-1, Wichita State University, </institution> <month> January, </month> <year> 1993. </year>
Reference-contexts: The work of Hu and Shing [19, 21, 22] supplies a basis for these algorithms, although they are built in a different framework. This paper is an update of [6] and the full version of [7]. Much subsequent work has appeared, take for example <ref> [13, 29, 8, 30] </ref>. In addition, [12] reports very similar results to those in section 6 of this paper.
Reference: [31] <author> W. Rytter: </author> <title> "On Efficient Parallel Computation for Some Dynamic Programming Problems", </title> <journal> Theoretical Computer Science, </journal> <volume> Vol. 59, </volume> <pages> 297-307, </pages> <year> 1988. </year> <month> 32 </month>
Reference-contexts: However, the algorithms they suggest appear to require fi (log 2 n) time and n 9 processors. Using pebbling games, Rytter <ref> [31] </ref> describes a general method to generate more efficient parallel algorithms for a class of optimization problems with dynamic programming solutions. He solves the three problems mentioned in the previous subsection in O (log 2 n) time with n 6 =log n processors on a CRCW PRAM. <p> It is modified in that straight minimum paths are dynamically relaxed with new jumpers. This algorithm is basically the same as Rytter's algorithm <ref> [31] </ref>. for all 1 i; j; u; v n in parallel do W [(i; j); (u; v)] 1 for all 1 i; j n in parallel do if j + 1 n then W [(i; j); (i; j + 1)] f (i; j; j + 1) if i 1 1 then
Reference: [32] <author> L. G. Valiant, S. Skyum, S. Berkowitz and C. Rackoff: </author> <title> "Fast Parallel Computation of Polyno--mials Using Few Processors", </title> <journal> SIAM J. on Computing, </journal> <volume> Vol. 12, No. 4, </volume> <pages> 641-644, </pages> <month> Nov. </month> <year> 1983. </year>
Reference-contexts: In addition, [12] reports very similar results to those in section 6 of this paper. This paper takes all logarithms as base 2 and assumes the common-CRCW PRAM parallel model. 1.1.2 Previous Results Using straight line arithmetic programs Valiant et al. <ref> [32] </ref> showed that many classical optimization problems with efficient sequential dynamic programming solutions are N C. However, the algorithms they suggest appear to require fi (log 2 n) time and n 9 processors.
Reference: [33] <author> F. F. Yao: </author> <title> "Speed-Up in Dynamic Programming", </title> <journal> SIAM J. on Algebraic and Discrete Methods, </journal> <volume> Vol. 3, No. 4, </volume> <pages> 532-540, </pages> <year> 1982. </year> <month> 33 </month>
Reference-contexts: The three problems in Subsection 1.1.1 can be solved sequentially in O (n 3 ) time with elementary dynamic programming algorithms, and in O (n 2 ) time by more complex algorithms <ref> [33] </ref>. But the best serial solution of the matrix chain ordering problem known is Hu and Shing's O (n lg n) algorithm [19, 21, 22]. It has been conjectured that Hu and Shing's algorithm is optimal [28].
References-found: 33

