URL: http://www.bmc.riken.go.jp/sensor/Allan/ICA/files/BELLnips94.ps.Z
Refering-URL: http://www.bmc.riken.go.jp/sensor/Allan/ICA/index.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: tony@salk.edu  terry@salk.edu  
Title: A Non-linear Information Maximisation Algorithm that Performs Blind Separation.  
Author: Anthony J. Bell Terrence J. Sejnowski and 
Address: 10010 N. Torrey Pines Road La Jolla, California 92037-1099  La Jolla CA 92093  
Affiliation: Computational Neurobiology Laboratory The Salk Institute  Department of Biology University of California at San Diego  
Abstract: A new learning algorithm is derived which performs online stochastic gradient ascent in the mutual information between outputs and inputs of a network. In the absence of a priori knowledge about the `signal' and `noise' components of the input, propagation of information depends on calibrating network non-linearities to the detailed higher-order moments of the input density functions. By incidentally minimising mutual information between outputs, as well as maximising their individual entropies, the network `fac-torises' the input into independent components. As an example application, we have achieved near-perfect separation of ten digitally mixed speech signals. Our simulations lead us to believe that our network performs better at blind separation than the Herault-Jutten network, reflecting the fact that it is derived rigorously from the mutual information objective. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Atick J.J. </author> <year> 1992. </year> <title> Could information theory provide an ecological theory of sensory processing, </title> <booktitle> Network 3, </booktitle> <pages> 213-251 </pages>
Reference: [2] <author> Barlow H.B. </author> <year> 1961. </year> <title> Possible principles underlying the transformation of sensory messages, in Sensory Communication, </title> <address> Rosenblith W.A. </address> <publisher> (ed), MIT press </publisher>
Reference: [3] <author> Barlow H.B. & Foldiak P. </author> <year> 1989. </year> <title> Adaptation and decorrelation in the cortex, </title> <editor> in Durbin R. et al (eds) The Computing Neuron, </editor> <publisher> Addison-Wesley </publisher>
Reference: [4] <author> Becker S. </author> <year> 1992. </year> <title> An information-theoretic unsupervised learning algorithm for neural networks, </title> <type> Ph.D. thesis, </type> <institution> Dept. of Comp. Sci., Univ. of Toronto </institution>
Reference: [5] <author> Bell A.J. & Sejnowski T.J. </author> <year> 1995. </year> <title> An information-maximisation approach to blind separation and blind deconvolution, Neural Computation, </title> <publisher> in press </publisher>
Reference-contexts: Despite this, we may still differentiate eq.1 as follows (see <ref> [5] </ref>): @w @ H (Y ) (2) Thus in the noiseless case, the mutual information can be maximised by maximising the entropy alone. 2.1 One input, one output. Consider an input variable, x, passed through a transforming function, g (x), to produce an output variable, y, as in Fig.2.1 (a). <p> This one resulted from separating five speech signals with our algorithm. entropy by representing some combination of independent components than by representing just one. When this condition is satisfied for all output units, the residual goal, of minimising the mutual information between the outputs, will dominate the learning. See <ref> [5] </ref> for further discussion of this. With this caveat in mind, we turn to the problem of blind separation, (Jutten & Herault 1991), illustrated in Fig.2.
Reference: [6] <author> Comon P., Jutten C. & Herault J. </author> <year> 1991. </year> <title> Blind separation of sources, part II: problems statement, </title> <booktitle> Signal processing, </booktitle> <volume> 24, </volume> <pages> 11-21 </pages>
Reference: [7] <author> Comon P. </author> <year> 1994. </year> <title> Independent component analysis, a new concept? Signal processing, </title> <booktitle> 36, </booktitle> <pages> 287-314 </pages>
Reference: [8] <author> Hopfield J.J. </author> <year> 1991. </year> <title> Olfactory computation and object perception, </title> <booktitle> Proc. </booktitle> <institution> Natl. Acad. Sci. USA, </institution> <note> vol. 88, pp.6462-6466 </note>
Reference: [9] <author> Jutten C. & Herault J. </author> <year> 1991. </year> <title> Blind separation of sources, part I: an adaptive algorithm based on neuromimetic architecture, </title> <booktitle> Signal processing, </booktitle> <volume> 24, </volume> <pages> 1-10 </pages>
Reference: [10] <author> Linsker R. </author> <year> 1992. </year> <title> Local synaptic learning rules suffice to maximise mutual information in a linear network, </title> <journal> Neural Computation, </journal> <volume> 4, </volume> <pages> 691-702 </pages>
Reference: [11] <author> Papoulis A. </author> <year> 1984. </year> <title> Probability, random variables and stochastic processes, 2nd edition, </title> <publisher> McGraw-Hill, </publisher> <address> New York </address>
Reference: [12] <author> Platt J.C. & Faggin F. </author> <year> 1992. </year> <title> Networks for the separation of sources that are superimposed and delayed, </title> <editor> in Moody J.E et al (eds) Adv. Neur. Inf. </editor> <booktitle> Proc. Sys. </booktitle> <volume> 4, </volume> <publisher> Morgan-Kaufmann </publisher>
Reference: [13] <author> Schraudolph N.N., Hart W.E. & Belew R.K. </author> <year> 1992. </year> <title> Optimal information flow in sigmoidal neurons, </title> <type> unpublished manuscript </type>
Reference-contexts: Applications to blind deconvolution (removing the effects of unknown causal filtering) are also described, and the limitations of the approach are discussed. Acknowledgements We are much indebted to Nici Schraudolph, who not only supplied the original idea in Fig.1 and shared his unpublished calculations <ref> [13] </ref>, but also provided detailed criticism at every stage of the work. Much constructive advice also came from Paul Viola and Alexandre Pouget.
References-found: 13

