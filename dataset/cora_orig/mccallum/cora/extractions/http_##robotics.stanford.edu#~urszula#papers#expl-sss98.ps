URL: http://robotics.stanford.edu/~urszula/papers/expl-sss98.ps
Refering-URL: http://robotics.stanford.edu/~urszula/
Root-URL: http://www.cs.stanford.edu
Email: urszula@cs.stanford.edu  draper@rpal.rockwell.com  
Title: Explaining Predictions in Bayesian Networks and Influence Diagrams  
Author: Urszula Chajewska Denise L. Draper 
Address: Stanford, CA 94305-9010  444 High Street, Suite 400 Palo Alto, CA 94301  
Affiliation: Stanford University Computer Science Department  Rockwell Palo Alto Laboratory  
Abstract: As Bayesian Networks and Influence Diagrams are being used more and more widely, the importance of an efficient explanation mechanism becomes more apparent. We focus on predictive explanations, the ones designed to explain predictions and recommendations of probabilistic systems. We analyze the issues involved in defining, computing and evaluating such explanations and present an algorithm to compute them. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Boutilier, C. and V. </author> <title> Becher (1995, August). Abduction as belief revision. </title> <booktitle> Artificial intelligence 77, </booktitle> <pages> 43-94. </pages>
Reference: <author> Chajewska, U. and J. Y. </author> <title> Halpern (1997). Defining explanation in probabilistic systems. </title> <booktitle> In Proceedings Uncertainty in Artificial Intelligence 13 (UAI 97), </booktitle> <pages> pp. 62-71. </pages>
Reference-contexts: By asking why the observed facts changed the 1 There is no reason to exclude the information about the causal structure of the domain in diagnostic explanation, however, as pointed out in <ref> (Chajewska and Halpern, 1997) </ref> most of the work in this area does so. prediction, the user informs us that although he knows of these facts, he doesnt understand all of their ramifications. We are faced with the fact that users are not perfect reasoners.
Reference: <author> Cooper, G. F.(1990). </author> <title> The computational complexity of probabilistic inference using Bayesian belief networks. </title> <booktitle> Artificial Intelligence 42, </booktitle> <pages> pp 393-405. </pages>
Reference-contexts: Our goal is to improve on Suermondts work in several ways: Better computational complexity. The algorithm works by instantiating subsets of evidence and evaluating the network for each one of them. Each evaluation of the network is exponential in the number of nodes in the network <ref> (Cooper, 1990) </ref>. To get the full answer, we would need an exponential number of instantiations and evaluations (exponential in the number of instantiated nodes). Suermondt suggests some heuristics which let him reduce this number to linear in some cases, with some loss of accuracy. Relax assumptions about conflicts between findings.
Reference: <author> Grdenfors, P. </author> <year> (1988). </year> <title> Knowledge in Flux: Modeling the Dynamics of Epistemic States. </title> <address> Cambridge, Mass: </address> <publisher> MIT Press. </publisher>
Reference: <author> Hempel, C. G. </author> <year> (1965). </year> <title> Aspects of Scientific Explanation. </title> <publisher> Free Press. </publisher>
Reference: <author> Hempel, C. G. and P. </author> <title> Oppenheim (1948). Studies in the logic of explanation. </title> <booktitle> Philosophy of Science 15. </booktitle>
Reference-contexts: They also exhibit biases which often lead to serious errors. It is not surprising that the results of the systems computations often leave them baffled. The notion of explanation, once investigated only by philosophers <ref> (Hempel and Oppenheim 1948, Hempel 1965, Salmon 1984, Grdenfors 1988) </ref>, found its way into Artificial Intelligence in recent years (Spiegelhalter and Knill-Jones, 1984, Pearl 1988, Henrion and Druzdzel 1990, Shimony 1991, 1993, Suermondt 1992, Boutilier and Becher 1995, Chajewska and Halpern 1997).
Reference: <author> Henrion, M. and M. J. </author> <month> Druzdzel </month> <year> (1991). </year> <title> Qualitative propagation and scenario-based schemes for explaining probabilistic reasoning. </title> <booktitle> In Uncertainty in Artificial Intelligence 6 (UAI 90), </booktitle> <pages> pp 17-32. </pages>
Reference: <author> Howard, R. A. </author> <year> (1976). </year> <title> The used car buyer. </title> <editor> In Howard, </editor> <publisher> R. </publisher>
Reference-contexts: The need for diagnostic explanations arises from the uncertainty about the current state of the world. Predictive explanations are required when the uncertainty concerns the causal structure and dynamics of the domain. Example 1 Consider the well-known car-buyers example <ref> (Howard 1976, Pearl 1988) </ref>. The buyer of a used car has a choice between buying car one (C1), car two (C2) or no car at all. There are several tests available, with different costs and accuracy guarantees to be run on one or both cars.
Reference: <author> A., J. E. Matheson and K. L. Miller (Eds.), </author> <title> Readings in Decision Analysis, </title> <address> Menlo Park, CA: </address> <institution> Stanford Research Institute. </institution>
Reference: <author> Howard, R. A. and J. E. </author> <title> Matheson (1984). Influence Diagrams. </title> <editor> In Howard, R. A. and J. E. Matheson (Eds.), </editor> <booktitle> The Principles and Applications of Decision Analysis, </booktitle> <address> Menlo Park, CA: </address> <institution> Strategic Decisions Group. </institution>
Reference-contexts: In addition, the systems ever increasing size makes their computations more and more difficult to follow even for their creators. This situation makes an explanation mechanism critical for making these systems useful and widely accepted. Probabilistic systems, such as Bayesian Networks (Pearl 1988) and Influence Diagrams <ref> (Howard and Matheson 1984) </ref>, need such a mechanism even more than others. Human judgment under uncertainty differs considerably from the idealized rationality of probability and decision theories.
Reference: <author> Littman, M., J. Goldsmith and M. </author> <month> Mundhenk </month> <year> (1998). </year> <title> The Computational Complexity of Probabilistic Planning. </title> <journal> In Journal of Artificial Intelligence Research, forthcoming. </journal>
Reference: <author> Littman, M. </author> <type> (1998) personal communication. </type>
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems. </title> <address> San Francisco, Calif: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In addition, the systems ever increasing size makes their computations more and more difficult to follow even for their creators. This situation makes an explanation mechanism critical for making these systems useful and widely accepted. Probabilistic systems, such as Bayesian Networks <ref> (Pearl 1988) </ref> and Influence Diagrams (Howard and Matheson 1984), need such a mechanism even more than others. Human judgment under uncertainty differs considerably from the idealized rationality of probability and decision theories. <p> Most of the AI research dealing with explanation in probabilistic systems restricted the explanandum to a subset of observations and the explanation to factual information about the state of the world <ref> (Pearl 1988, Henrion and Druzdzel 1990, Shimony 1991 and 1993) </ref> 1 . We call the explanations provided under these restrictions diagnostic explanations. The intuition behind this name comes from medical applicationswe observe symptoms and make a diagnosis.
Reference: <author> Peot, M. A. and R. D. </author> <title> Shachter (1991). Fusion and propagation with multiple observations in belief networks (research note). </title> <journal> Artificial Intelligence, </journal> <volume> 48, </volume> <month> pp.299-318. </month>
Reference: <author> Salmon, W. C. </author> <year> (1984). </year> <title> Scientific Explanation and the Causal Structure of the World. </title> <publisher> Princeton, </publisher> <address> NJ: </address> <publisher> Princeton University Press. </publisher>
Reference: <author> Shachter, R. D. </author> <year> (1986). </year> <title> Evaluating influence diagrams. </title> <editor> In Glen Shafer and Judea Pearl, editors, </editor> <booktitle> Readings in Uncertain Reasoning, </booktitle> <pages> pages 259-273, </pages> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The only solution to this problem, which would work in every case, would be to combine the two neighbors into one node (using arc reversal <ref> (Shachter 1986) </ref> to avoid creating directed cycles if necessary).
Reference: <author> Shimony, S. E. </author> <year> (1991). </year> <title> Explanation, irrelevance and statistical independence. </title> <booktitle> In Proc. National Conference on Artificial Intelligence (AAAI 91), </booktitle> <pages> pp. 482-487. </pages>
Reference: <author> Shimony, S. E. </author> <year> (1993). </year> <title> Relevant explanations: Allowing disjunctive assignments. </title> <booktitle> In Proc. Ninth Conference on Uncertainty in Artificial Intelligence (UAI 93), </booktitle> <pages> pp. 200-207. </pages>
Reference: <author> Spiegelhalter, D. J. and R. P. </author> <month> Knill-Jones </month> <year> (1984). </year> <title> Statistical and knowledge-based approaches to clinical decision-support systems, with an application in gastroenterology. </title> <journal> In Journal Royal Statistical Society A, </journal> <volume> vol. 147, </volume> <pages> pp. 35-77. </pages>
Reference: <author> Suermondt, H. J. </author> <year> (1991). </year> <title> Explanation of Probabilistic Inference in Bayesian Belief Networks, </title> <institution> Report KSL-91-39 (Knowledge Systems Laboratory, Medical Computer Science, Stanford University) Suermondt, H. J. </institution> <year> (1992). </year> <title> Explanation in Bayesian Belief Networks. </title> <type> Ph. D. thesis, </type> <institution> Stanford University. </institution>
Reference-contexts: Conclusion The algorithm presented here has several advantages compared to those found in the literature <ref> (Suermondt 1991, 1992) </ref>. Although the worst-case running time is the same, our algorithm works in a general case without making any assumptions about the structure of the conditional probability tables. It finds both the explanation set and the influential paths through the network in a single pass.
Reference: <author> Tversky, A. and D. </author> <month> Kahneman </month> <year> (1974). </year> <title> Judgement under uncertainty: Heuristics and biases. </title> <editor> In Glen Shafer and Judea Pearl, editors, </editor> <booktitle> Readings in Uncertain Reasoning, </booktitle> <pages> pages 32-39, </pages> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
References-found: 21

