URL: http://ptolemy.eecs.berkeley.edu/papers/97/now/report.ps
Refering-URL: http://ptolemy.eecs.berkeley.edu/papers/97/now/
Root-URL: 
Title: Network of Workstations Active Messages Target for Ptolemy C Code Generation  
Author: by Patrick Warner 
Degree: Submitted to the Department of Electrical Engineering and Computer Science, University of Cal-ifornia at Berkeley, in partial satisfaction of the requirements for the degree of Master of Science,  
Note: Plan II.  
Date: January 24, 1997  
Pubnum: Memorandum No. UCB/ERL M97/8  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Joseph Buck, Soonhoi Ha, Edward A. Lee, David G. Messerschmitt, Ptolemy: </author> <title> A Framework for Simulating and Prototyping Heterogeneous Systems, </title> <journal> International Journal of Computer Simulation, </journal> <year> 1992. </year>
Reference-contexts: 1. Introduction Ptolemy is a software environment for simulating, prototyping, and synthesizing heterogeneous systems <ref> [1] </ref>. Examples of such systems include an ATM network for transmitting video signals, a Unix workstation and Digital Signal Processing (DSP) target board used for music tone generation, and a VHDL (VHSIC Hardware Description Language) model of a computer Central Processing Unit interacting with serial data. <p> A C union type was created to transport a double oating point number as two integers. The type definition is: typedef union ints_or_double - int asInt [2]; double asDouble; - convert; The data is sent as follows: convert myData; myData.asDouble = double_float; AM_Request4 (endpoint, reply_index, handler_index,myData.asInt [0], myData.asInt <ref> [1] </ref>, 0, 0); The data is retrieved as follows: void handler (void *token, int arg0, int arg1, int arg2, int arg3) - convert temp; temp.asInt [0] = arg0; temp.asInt [1] = arg1; Receive_double = temp.asDouble; - As UDPAM is the only AM implementation available at the time of this research, a <p> double asDouble; - convert; The data is sent as follows: convert myData; myData.asDouble = double_float; AM_Request4 (endpoint, reply_index, handler_index,myData.asInt [0], myData.asInt <ref> [1] </ref>, 0, 0); The data is retrieved as follows: void handler (void *token, int arg0, int arg1, int arg2, int arg3) - convert temp; temp.asInt [0] = arg0; temp.asInt [1] = arg1; Receive_double = temp.asDouble; - As UDPAM is the only AM implementation available at the time of this research, a bulk transfer 11 method for sending data proves to be more efficient.
Reference: [2] <author> Alan M. Mainwaring, </author> <title> Active Message Applications Programming Interface and Communication Subsystem Organization, </title> <type> Draft Technical Report, </type> <institution> University of California at Berkeley, Computer Science Department, </institution> <year> 1996. </year>
Reference-contexts: A communication protocol matching that of a MPP seems reasonable for a NOW, and AM is a proven MPP communication method <ref> [2] </ref>. AM was originally developed strictly for MPP platforms as the Generic Active Messages (GAM) specification [3]. The GAM library was restricted to SPMD (Single Process Multiple Data) programs [4] that required the same code image to execute on each computing node. <p> As network hardware continues to advance into the area of multi-gigabit bandwidths and sub-microsecond switch latencies, the existing protocol software overheads will dominate communication costs. AM provide a simple, portable, and general-purpose communications interface with direct application-network interactions that bypass the operating system on high-performance implementations <ref> [2] </ref>. AM have been shown to be a useful means for building high-performance communication protocols, run-time environments, and message passing libraries on MPPs. In the original implementation of AM, GAM, all communication occurred within individual parallel processes with one network port per process. <p> AM implementations seek to take advantage of the embedded processor to provide the direct application to network device channel. The AM interface allows arbitrary serial and parallel processes to create multiple communication endpoints <ref> [2] </ref>. The communication endpoints are completely independent and secure network ports that resemble conventional Berkeley Unix sockets. Messages can be sent from any endpoint to any other endpoint, with the restriction that they use a common tag for protection purposes. <p> The AM library is accessed through an ANSI C application programming interface (API), which is detailed in <ref> [2] </ref>. <p> A C union type was created to transport a double oating point number as two integers. The type definition is: typedef union ints_or_double - int asInt <ref> [2] </ref>; double asDouble; - convert; The data is sent as follows: convert myData; myData.asDouble = double_float; AM_Request4 (endpoint, reply_index, handler_index,myData.asInt [0], myData.asInt [1], 0, 0); The data is retrieved as follows: void handler (void *token, int arg0, int arg1, int arg2, int arg3) - convert temp; temp.asInt [0] = arg0; temp.asInt
Reference: [3] <author> David Culler, Kim Keeton, Cedric Krumbein, Lok Tin Liu, Alan Mainwaring, Rich Martin, Steve Rodrigues, Kristin Wright, Chad Yoshikawa, </author> <title> Generic Active Message Interface Specification, </title> <type> Version 1.1, </type> <institution> University of California at Berkeley, Computer Science Department, </institution> <year> 1995. </year>
Reference-contexts: A communication protocol matching that of a MPP seems reasonable for a NOW, and AM is a proven MPP communication method [2]. AM was originally developed strictly for MPP platforms as the Generic Active Messages (GAM) specification <ref> [3] </ref>. The GAM library was restricted to SPMD (Single Process Multiple Data) programs [4] that required the same code image to execute on each computing node. As the uses for GAM extended into more distributed, client-server computing (operating systems, file systems, network RAM), the SPMD model became too restrictive.
Reference: [4] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, George Karypis, </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms, </title> <publisher> The Benjamin/Cummings Publishing Company, Inc., </publisher> <address> Redwood City, California, </address> <year> 1994. </year> <month> 18 </month>
Reference-contexts: AM was originally developed strictly for MPP platforms as the Generic Active Messages (GAM) specification [3]. The GAM library was restricted to SPMD (Single Process Multiple Data) programs <ref> [4] </ref> that required the same code image to execute on each computing node. As the uses for GAM extended into more distributed, client-server computing (operating systems, file systems, network RAM), the SPMD model became too restrictive. <p> In order to determine the maximum speedup possible for the simulation, information from the Ptolemy schedule can be used. Speedup is defined as the time it takes the program to execute on a single processor divided by the total elapsed time to execute the program in parallel <ref> [4] </ref>. A graph with node computation costs and communication costs is pictured in Figure 5. Each node in the graph is assigned to a processor, taking into account communication delays and delays resulting from scheduling. <p> If the time waiting for the receive data could be eliminated, the maximum speedup of 1.68 would be approached. Efficiency in parallel computing is the speedup divided by the number of processors <ref> [4] </ref>. For the two processor simulation, the efficiency is 80%. The four processor simulation was carried out using three SparcStation 20s with 128 MB of RAM, and one SparcStation 10 with 80 MB of RAM, all connected by a 100 Mbps switched Ethernet network.
Reference: [5] <author> Remzi H. Arpaci, Andrea Dusseau, Amin M. Vahdat, Lok T. Liu, Thomas E. Anderson, and David A. Patterson, </author> <title> The Interaction of Parallel and Sequential Workloads on a Network of Workstations, </title> <type> Technical Report CS-94-838, </type> <institution> University of California at Berkeley, Computer Science Department, </institution> <year> 1994. </year>
Reference-contexts: Active Messages AM seek to minimize the latency associated with communication protocol overhead and maximize user-level application bandwidth. Research at the University of California at Berkeley observed communication protocols such as TCP/IP add unnecessary overhead to the base hardware cost <ref> [5] </ref>. As network hardware continues to advance into the area of multi-gigabit bandwidths and sub-microsecond switch latencies, the existing protocol software overheads will dominate communication costs. AM provide a simple, portable, and general-purpose communications interface with direct application-network interactions that bypass the operating system on high-performance implementations [2].
Reference: [6] <author> Edward A. Lee and David G. Messerschmitt, </author> <title> Synchronous Data Flow, </title> <booktitle> Proceedings of the IEEE, </booktitle> <month> September, </month> <year> 1987. </year>
Reference-contexts: The model of computation for which Ptolemy code synthesis has been best defined is synchronous dataow (SDF), a special case of the dataow model of computation developed by Dennis <ref> [6] </ref>. SDF Stars produce and consume a constant number of data tokens at each invocation. Because of this, the execution order and resource requirements of SDF Stars can be determined at compile time. The SDF paradigm is used extensively in the definitions of Ptolemys scheduling and code generation facilities.
Reference: [7] <author> Gilbert C. Sih, </author> <title> Multiprocessor Scheduling To Account For Interprocessor Communication, </title> <type> Ph.D. Thesis, </type> <institution> University of California at Berkeley, Department of Electrical Engineering and Computer Science, </institution> <year> 1991. </year>
Reference-contexts: The SDF model produces a graph that exposes the functional parallelism available in an algorithm. The next step in multiprocessor scheduling is to construct an Acyclic Precedence Expansion Graph (APEG) from the original SDF graph <ref> [7] </ref>. The APEG adds additional information such as communication costs between graph nodes and node computation costs. <p> This method is known a Highest Levels First with Estimated Times list scheduling, and assigns nodes to processors when they are ready to be executed and a processor is available, not taking communication cost into account. Using IPC communication costs and allowing overlapping communication, Sihs dynamic level (DL) scheduling <ref> [7] </ref> results in less communication because a node must have a high enough execution time to warrant the IPC overhead costs of transferring data between processors. This technique allows communication and computation to overlap, assuming dedicated communication hardware is available. The final Scheduler, Sihs declustering scheduling [7], makes multiple iterations over <p> dynamic level (DL) scheduling <ref> [7] </ref> results in less communication because a node must have a high enough execution time to warrant the IPC overhead costs of transferring data between processors. This technique allows communication and computation to overlap, assuming dedicated communication hardware is available. The final Scheduler, Sihs declustering scheduling [7], makes multiple iterations over the graph, whereas Sihs DL scheduling makes a single pass, grouping nodes into clusters and analyzing trade-offs between parallel execution and IPC overhead. The base CG star class (CGStar) contains the methods shared by all code generation Stars.
Reference: [8] <author> Jose L. Pino, </author> <title> Software Synthesis for Single-Processor DSP Systems Using Ptolemy, </title> <type> Masters Report, </type> <institution> University of California at Berkeley, Department of Electrical Engineering and Computer Science, </institution> <year> 1994. </year>
Reference-contexts: States represent parameters that can be set by the user or internal memory states needed in the generated code. Code blocks contain the target language and Star macro functions. Macro functions include parameter value substitution, unique symbol generation with multiple scopes, and state reference substitution <ref> [8] </ref>. The start () method is invoked prior to any scheduling or memory allocation because it initializes any information that will affect these actions. The init-Code () method is called before scheduling but after memory allocation.
Reference: [9] <author> Jose L. Pino, Soonhoi Ha, Edward A. Lee, Joseph T. Buck, </author> <title> Software Synthesis for DSP Using Ptolemy, </title> <journal> Journal of VLSI Signal Processing, </journal> <volume> 9, </volume> <pages> 7-21, </pages> <year> 1995. </year>
Reference-contexts: The .h and .cc source files for the CGCNOWamTarget can be found in Appendix A. Because CGCNOWamTarget is a multiprocessor target, several additional design issues in Ptolemy are raised. To support multiprocessor targets, a concept of parent-child target relationships is used <ref> [9] </ref>. The parent target defines the IPC mechanism and resources to be shared by the children. A hierarchy of child targets, which may themselves be complex heterogeneous multiprocessors or a single processor, completes the multiprocessor target definition. The child targets manage resources local to themselves.
Reference: [10] <author> Jose L. Pino and Edward A. Lee, </author> <title> Hierarchical Static Scheduling of Dataow Graphs onto Multiple Processors, </title> <booktitle> Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <month> May </month> <year> 1995, </year> <pages> pp. 2643-2646. 19 </pages>
Reference-contexts: This code explosion is what caused the compiler to run out of memory when compiling all four stages of the up-sample simulation on two processors. A solution to this problem is to use Ptolemys hierarchical scheduling <ref> [10] </ref>. This scheduler allows one of the three multiprocessor schedulers discussed in Section 3 to be used as a top-level scheduler. For each child of the multiprocessor target, a single processor looping scheduler is used to reduce code explosion.
References-found: 10

