URL: http://www.cs.princeton.edu/prism/papers-ps/bulk-transfer-conf.ps
Refering-URL: http://www.cs.princeton.edu/prism/html/all-papers.html
Root-URL: http://www.cs.princeton.edu
Title: The Performance Advantages of Integrating Block Data Transfer in Cache-Coherent Multiprocessors  
Author: Steven Cameron Woo, Jaswinder Pal Singh, and John L. Hennessy 
Address: Stanford, CA 94305  
Affiliation: Computer Systems Laboratory Stanford University  
Abstract: Integrating 1 support for block data transfer has become an important emphasis in recent cache-coherent shared address space multiprocessors. This paper examines the potential performance benefits of adding this support. A set of ambitious hardware mechanisms is used to study performance gains in five important scientific computations that appear to be good candidates for using block transfer. Our conclusion is that the benefits of block transfer are not substantial for hardware cache-coherent multiprocessors. The main reasons for this are (i) the relatively modest fraction of time applications spend in communication amenable to block transfer, (ii) the difficulty of finding enough independent computation to overlap with the communication latency that remains after block transfer, and (iii) long cache lines often capture many of the benefits of block transfer in efficient cache-coherent machines. In the cases where block transfer improves performance, prefetching can often provide comparable, if not superior, performance benefits. We also examine the impact of varying important communication parameters and processor speed on the effectiveness of block transfer, and comment on useful features that a block transfer facility should support for real applications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, Beng-Hong Lim, David Kranz, and John Kubiatowicz. </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: address space are clearly not mutually exclusive, and since the core mechanisms for moving data with low latency and high bandwidth are very similar for cache coherence and block transfer, architects have begun designing machines with hardware support for both a cache-coherent shared address space and block transfer of data <ref> [1, 9, 12, 14] </ref>. Structuring communication in large blocks is crucial on current message-passing machines and workstation networks, since the overheads and latencies of sending messages are very high. However, it is not clear what the role of coarse-grained messages is on tightly coupled, hardware cache-coherent shared address space architectures. <p> Support for this type of copying mechanism is provided in the Cray T3D [12], and is planned for other machines currently being developed <ref> [1, 9, 14] </ref>. Within the load-store or block transfer models, communication can be distinguished by whether it is initiated by the consuming processor (receiver-initiated) or by the producing processor (sender-initiated). In the load-store model, these types of communication occur through read and write operations to remote data, respectively.
Reference: [2] <author> David H. Bailey. </author> <title> FFTs in External or Hierarchical Memory. </title> <journal> Journal of Supercomputing, </journal> <volume> 4(1) </volume> <pages> 23-35, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Results For each application and kernel, we briefly describe the algorithm and the load-store (LS) and block transfer (BT) implementations for which we present results. 5.1 The Fast Fourier Transform Algorithm: The FFT we use is a complex 1-D version of the radix p n six-step FFT algorithm described in <ref> [2] </ref>, which is optimized to minimize interprocessor communication. The data set for the FFT consists of the n complex data points to be transformed, and another n complex data points referred to as the roots of unity.
Reference: [3] <author> Brian N. Bershad, Matthew J. Zekauskas, and Wayne A. Sawdon. </author> <title> The Midway Distributed Shared Memory System. </title> <booktitle> In Proceedings of COMPCON'93, </booktitle> <month> February </month> <year> 1993. </year>
Reference-contexts: For this reason, block transfer has enormous advantages in such systems <ref> [3, 5] </ref>. 6.3 Increasing Processor Speed We next examine the effect of having processor speeds increase much faster than memory and network speeds, a continuing technology trend. In particular, we show results for processors that have a peak performance twice that of our base processor.
Reference: [4] <author> Achi Brandt. </author> <title> Multi-Level Adaptive Solutions to Boundary-Value Problems. </title> <journal> Mathematics of Computation, </journal> <volume> 31(138) </volume> <pages> 333-390, </pages> <month> April </month> <year> 1977. </year>
Reference-contexts: grids into square-like subgrids rather than groups of columns to improve the communication to computation ratio, (iii) as in LU, it uses dynamically allocated 4-D arrays designed to allow appropriate data distribution and reduce false sharing, and (iv) it uses a red-black Gauss-Seidel multigrid technique based on that presented in <ref> [4] </ref>, whereas the SPLASH version uses a relaxed Gauss-Seidel SOR solver. Results: The bulk of the application's execution time is spent in the multigrid solver. The solver performs nearest-neighbor sweeps on a hierarchy of grids rather than on a single grid as in SOR.
Reference: [5] <author> Sandhya Dwarkadas, Pete Keleher, Alan Cox, and Willy Zwaenepoel. </author> <title> An Evaluation of Software Distributed Shared Memory for Next-Generation Processors and Networks. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 144-155, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: For this reason, block transfer has enormous advantages in such systems <ref> [3, 5] </ref>. 6.3 Increasing Processor Speed We next examine the effect of having processor speeds increase much faster than memory and network speeds, a continuing technology trend. In particular, we show results for processors that have a peak performance twice that of our base processor.
Reference: [6] <author> David H. Bailey et al. </author> <title> The NAS Parallel Benchmarks. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 5(3) </volume> <pages> 63-73, </pages> <month> Fall </month> <year> 1991. </year>
Reference-contexts: is quite natural to block transfer, but which do not benefit much from block transfer in a tightly-coupled multiprocessor. 5.5 Radix Sort Algorithm: The integer radix sort kernel also has a wide range of applications, including database management systems and aerospace applications (it is one of the NAS parallel benchmarks <ref> [6] </ref>). It is based on the method described in [8], and requires the movement of bulk data (the keys being sorted) from one processor to another during each phase of the computation. Each processor is assigned an equal fraction of the n keys to be sorted.
Reference: [7] <author> David Kranz et al. </author> <title> Integrating Message-Passing and Shared-Memory: Early Experience. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 54-63, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: As in other proposed block transfer protocols, we assume that data can only be sent from the node on which its storage is allocated, and we ensure only that the sender and receiver's caches stay coherent with their respective local memories <ref> [7] </ref>. These assumptions suffice for all our applications. More ambitious protocols that maintain global coherence on block transfer data would add implementation complexity, and we comment on their importance in Section 5.2. The second issue is deciding how the node controller should prioritize load-store and block transfer transactions.
Reference: [8] <author> Guy E. Blelloch et al. </author> <title> A Comparison of Sorting Algorithms for the Connection Machine CM-2. </title> <booktitle> In Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 3-16, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: It is based on the method described in <ref> [8] </ref>, and requires the movement of bulk data (the keys being sorted) from one processor to another during each phase of the computation. Each processor is assigned an equal fraction of the n keys to be sorted.
Reference: [9] <author> Jeffrey Kuskin et al. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: address space are clearly not mutually exclusive, and since the core mechanisms for moving data with low latency and high bandwidth are very similar for cache coherence and block transfer, architects have begun designing machines with hardware support for both a cache-coherent shared address space and block transfer of data <ref> [1, 9, 12, 14] </ref>. Structuring communication in large blocks is crucial on current message-passing machines and workstation networks, since the overheads and latencies of sending messages are very high. However, it is not clear what the role of coarse-grained messages is on tightly coupled, hardware cache-coherent shared address space architectures. <p> Support for this type of copying mechanism is provided in the Cray T3D [12], and is planned for other machines currently being developed <ref> [1, 9, 14] </ref>. Within the load-store or block transfer models, communication can be distinguished by whether it is initiated by the consuming processor (receiver-initiated) or by the producing processor (sender-initiated). In the load-store model, these types of communication occur through read and write operations to remote data, respectively. <p> The node controller contains a programmable processor which processes local memory accesses, standard directory-based cache-coherence protocol transactions and block transfers. The network interface connects the node to a 2-dimensional, bidirectional mesh network. The architecture is based closely on the Stanford FLASH multiprocessor that is currently being developed <ref> [9] </ref>. Node Controller Processor Cache Main Memory Network Interface We assume a set of ambitious hardware mechanisms for performing block transfer. The node controller is assumed to be quite sophisticated and allows different strides to be specified at the source and destination.
Reference: [10] <author> John Heinlein et al. </author> <title> Integration of Message Passing and Shared Memory in the Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: To perform a block transfer, the initiating processor describes the transfer to the node controller in its node. Data are then communicated by the node controller in a pipelined fashion at the granularity of cache lines <ref> [10] </ref>. The pipeline stage time for block transfer is the greater of the time for the node controller to retrieve a line from memory and the time to push a line and its associated header into the network.
Reference: [11] <author> Stephen Goldschmidt. </author> <title> Simulation of Multiprocessors: Accuracy and Performance. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: We shall explore transfers into a large second level cache in the future. The architecture is simulated using the Tango-Lite event-driven reference generator <ref> [11] </ref>. Our detailed, variable-latency memory system simulator models contention at the node controller and memory system, but not in the network itself. An invalidation-based cache-coherence protocol similar to the one used in the Stanford DASH multiprocessor [13] is simulated.
Reference: [12] <author> Cray Research Inc. </author> <title> Cray T3D System Architecture and Overview. Revision 1.c. </title> <type> Technical report, </type> <institution> Cray Research Inc., </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: address space are clearly not mutually exclusive, and since the core mechanisms for moving data with low latency and high bandwidth are very similar for cache coherence and block transfer, architects have begun designing machines with hardware support for both a cache-coherent shared address space and block transfer of data <ref> [1, 9, 12, 14] </ref>. Structuring communication in large blocks is crucial on current message-passing machines and workstation networks, since the overheads and latencies of sending messages are very high. However, it is not clear what the role of coarse-grained messages is on tightly coupled, hardware cache-coherent shared address space architectures. <p> Support for this type of copying mechanism is provided in the Cray T3D <ref> [12] </ref>, and is planned for other machines currently being developed [1, 9, 14]. Within the load-store or block transfer models, communication can be distinguished by whether it is initiated by the consuming processor (receiver-initiated) or by the producing processor (sender-initiated).
Reference: [13] <author> Dan Lenoski, James Laudon, Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The architecture is simulated using the Tango-Lite event-driven reference generator [11]. Our detailed, variable-latency memory system simulator models contention at the node controller and memory system, but not in the network itself. An invalidation-based cache-coherence protocol similar to the one used in the Stanford DASH multiprocessor <ref> [13] </ref> is simulated. Processors are forced to block on read misses, but infinite write buffering hardware is included to eliminate processor stalls on write misses. To reduce miss latencies, speculative memory reads are performed at the home node at the same time that the state of a line is checked.
Reference: [14] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-level Shared Memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: address space are clearly not mutually exclusive, and since the core mechanisms for moving data with low latency and high bandwidth are very similar for cache coherence and block transfer, architects have begun designing machines with hardware support for both a cache-coherent shared address space and block transfer of data <ref> [1, 9, 12, 14] </ref>. Structuring communication in large blocks is crucial on current message-passing machines and workstation networks, since the overheads and latencies of sending messages are very high. However, it is not clear what the role of coarse-grained messages is on tightly coupled, hardware cache-coherent shared address space architectures. <p> Support for this type of copying mechanism is provided in the Cray T3D [12], and is planned for other machines currently being developed <ref> [1, 9, 14] </ref>. Within the load-store or block transfer models, communication can be distinguished by whether it is initiated by the consuming processor (receiver-initiated) or by the producing processor (sender-initiated). In the load-store model, these types of communication occur through read and write operations to remote data, respectively.
Reference: [15] <author> Jaswinder Pal Singh, John L. Hennessy, and Anoop Gupta. </author> <title> Scaling Parallel Programs for Multiprocessors: Methodology and Examples. </title> <journal> IEEE Computer, </journal> <volume> 26(7) </volume> <pages> 42-50, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: is time-constrained scaling, in which the data set size does not grow as quickly as the number 4 Replication in main memory can be useful for data structures that are not block transferred (particularly in the presence of conflict misses), and when data distribution is not done appropriately. of processors <ref> [15] </ref>. The communication to computation ratio grows, but the message size becomes smaller, so that beyond a point the effectiveness of block transfer usually diminishes under time-constrained scaling. Effect of architectural variations: Moderate changes in network bandwidth did not affect the benefits of block transfer very much.
Reference: [16] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year> <note> Also Stanford University Technical Report No. CSL-TR-92-526, </note> <month> June </month> <year> 1992. </year>
Reference-contexts: The applications and kernels we study are listed in Table 2. Our ocean simulation (which is a complete application) is a scalable version of the Ocean application in the SPLASH suite <ref> [16] </ref>, and uses a different solver and partitioning scheme. The blocked Cholesky factorization is also a more scalable alternative to the panel Cholesky kernel in SPLASH. 4.2 Experimental Methodology For each application, we examine the performance gains obtained by block transfer over the load-store model. <p> blocked sparse Cholesky factorization also does not benefit much from block transfer, particularly with long cache lines. 5.4 Ocean Simulation Application description: The ocean simulation studies large scale ocean movements based on eddy and boundary currents, and is an enhanced version of the Ocean application in the SPLASH application suite <ref> [16] </ref>.
Reference: [17] <author> Steven Cameron Woo, Jaswinder Pal Singh, and John L. Hennessy. </author> <title> The Performance Advantages of Integrating Message Passing in Cache-Coherent Multiprocessors. </title> <type> Technical Report CSL-TR-93-593, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: The performance advantages of sender-initiated load-store communication depend intimately on details of the architecture and application and are discussed in <ref> [17] </ref>. <p> While we have experimented with many intermediate block transfer versions, we do not discuss them here for reasons of space. A more complete discussion can be found in <ref> [17] </ref>. We start with highly optimized load-store versions of the applications, discuss the most effective ways to incorporate block transfer, and examine the performance benefits, trying to isolate their sources as much as possible. We also examine how the effectiveness of block transfer changes with the number of processors used. <p> versions: Instead of performing a matrix transpose by reading an entire column at a time of the source matrix and writing it into a row at a time in the destination matrix, the LS version utilizes a blocked transpose algorithm to exploit the spatial locality afforded by long cache lines <ref> [17] </ref>. Patches are communicated in a staggered fashion (processor i first transposes a patch from processor i + 1, then one from processor i + 2, etc.) in order to avoid hot-spotting. <p> Overlapping communication with computation: The BT version with a blocked transpose obtains the benefit of fast data transfer, but not overlap of communication with computation at a coarse level. Through a technique we call subpatching <ref> [17] </ref>, overlap can be obtained by combining steps in the six-step algorithm. The patches to be transposed are first broken into smaller subpatches. <p> In our LS version, however, we found that the benefits of explicit replication in main memory are small and that the overheads often outweigh them <ref> [17] </ref>. Therefore, our LS version has no explicit replication. In the BT version, the processor that updates the diagonal block sends a copy of it to all processors that own perimeter blocks. <p> We use a 16KB cache, which holds the data required for a block-block update with our block size 32-by-32. As in LU, we found that main memory replication was not useful in the LS versions, and use a version with no explicit replication as the base <ref> [17] </ref>. In receiver-initiated block transfer, the receiver sends request messages to the node controller of the block's owner, which then transfers the block to it. <p> discarded after it is used (and may need to be requested again later), (ii) the receiver has two block-sized receive buffers, one of which can be used for computation while data is transferred concurrently into the other (a double-buffering approach that achieves overlap of communication and computation through block prefetching <ref> [17] </ref>), and (iii) the receiver utilizes user-level memory management (without system calls) to locally replicate all remote blocks that it requires (a full replication approach). <p> It is therefore difficult to find a point where block transfer is very useful. Also, other parts of the application than the multigrid solver do not have nearly as much communication. Results for this application show negligible BT performance benefits and may be found in <ref> [17] </ref>. <p> We empirically determine the optimal radix separately for the LS and BT versions. Results: Detailed results for radix sort may be found in <ref> [17] </ref>. The BT version tends to have nearly the same optimal radix as the LS version in our experiments, but does not perform as well for three reasons. The first is the problem associated with small messages mentioned earlier.
References-found: 17

