URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR93365-S.ps
Refering-URL: http://www.cs.rice.edu/~ken/kennedy-vita.html
Root-URL: 
Title: Fortran Tools Newsletter #12 Value-Based Distributions and Alignments in Fortran D Languages, Special Issue on
Author: Reinhard v. Hanxleden Ken Kennedy Joel Saltz 
Address: P.O. Box 1892 Houston, TX 77251-1892  
Affiliation: Rice University  
Note: Center for Research on Parallel Computation  From the Journal of Programming  to appear.  
Date: December 1993  Revised May 1994.  
Pubnum: CRPC-TR93365-S  
Abstract-found: 0
Intro-found: 1
Reference: [BB87] <author> M. J. Berger and S. Bokhari. </author> <title> A partitioning strategy for non-uniform problems on multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(5):570-580, </volume> <year> 1987. </year>
Reference-contexts: They have developed a wealth of different mapping strategies, both value-based <ref> [BB87, HS91, DHU + 93] </ref>, and connectivity based [Sim91], which can be used as mapping strategies for irregular distributions. Run-time iteration graphs can assist in improving load balance and access locality when distributing loop iterations across processors [PSC93a].
Reference: [BBO + 83] <author> B. R. Brooks, R. E. Bruccoleri, B. D. Olafson, D. J. States, S. Swaminathan, and M. Karplus. CHARMM: </author> <title> A program for macromolecular energy, minimization and dynamics calculations. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 4(2) </volume> <pages> 187-217, </pages> <year> 1983. </year>
Reference-contexts: Another example where data are not related by indices, but values, are molecular dynamics programs such as GROMOS [GB88], CHARMM <ref> [BBO + 83] </ref>, or ARGOS [SM90] that are used to simulate biomolecular systems. One important routine common to these programs is the nonbonded force (NBF) routine, which typically accounts for the bulk of the computational work (around 90%). Figure 1 shows an abstracted version of a sequential NBF calculation.
Reference: [BCF + 93] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, and S. Ranka. </author> <title> A compilation approach for Fortran 90D/HPF compilers on distributed memory MIMD computers. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year> <month> 20 </month>
Reference-contexts: A prototype Fortran 77D compiler targeting MIMD distributed memory machines has been developed at Rice University [HKK + 91, Tse93] as part of the Para-Scope programming environment [CCH + 88]. A Fortran 90D compiler has been written at Syra-cuse University <ref> [BCF + 93] </ref>. For regular problems, i.e., applications with relatively simple array subscript functions and fixed communication requirements, these compilers have had considerable successes [Tse93].
Reference: [Bia91] <author> E. S. Biagioni. </author> <title> Scan Directed Load Balancing. </title> <type> PhD thesis, </type> <institution> University of North Carolina at Chapel Hill, </institution> <year> 1991. </year>
Reference-contexts: If an application has good index-based locality, then one might still want to distribute data irregularly to improve load balance, but some or all of the mapping computation can be done on the fly <ref> [BK93, Han92, Bia91, CHMS94] </ref>.) 5 PROGRAM nbf INTEGER i, j, p, t, n$proc, atomMax, pMax, stepMax PARAMETER (n$proc = 8) 5 PARAMETER (atomMax = 8000, pMax = 250, stepMax = 30) INTEGER inb (atomMax), partners (atomMax, pMax) REAL x (atomMax), f (atomMax), force C Fortran D directives 10 DECOMPOSITION atomD (atomMax)
Reference: [BK93] <author> S. B. Baden and S. R. Kohn. </author> <title> Portable parallel programming of numerical problems under the LPAR system. </title> <type> Technical Report CS93-330, </type> <institution> Department of Computer Science and Engineering, University of California, </institution> <address> San Diego, </address> <year> 1993. </year>
Reference-contexts: If an application has good index-based locality, then one might still want to distribute data irregularly to improve load balance, but some or all of the mapping computation can be done on the fly <ref> [BK93, Han92, Bia91, CHMS94] </ref>.) 5 PROGRAM nbf INTEGER i, j, p, t, n$proc, atomMax, pMax, stepMax PARAMETER (n$proc = 8) 5 PARAMETER (atomMax = 8000, pMax = 250, stepMax = 30) INTEGER inb (atomMax), partners (atomMax, pMax) REAL x (atomMax), f (atomMax), force C Fortran D directives 10 DECOMPOSITION atomD (atomMax) <p> LPAR is a programming model for implementing numerical algorithms with a local structure, such as Particle-in-Cell or Multigrid, on distributed memory MIMD multiprocessors <ref> [BK93] </ref>. Given a data structure which already reflects the physical locality characteristics of the underlying problem, it distributes data and computation in a load balanced fashion. To specify irregular mappings in a data-parallel context, index-based mapping arrays [WSBH91] or mapping functions [CMZ92] have been proposed.
Reference: [CCH + 88] <author> D. Callahan, K. Cooper, R. Hood, K. Kennedy, and L. Torczon. </author> <title> ParaScope: A parallel programming environment. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 2(4) </volume> <pages> 84-99, </pages> <month> Winter </month> <year> 1988. </year>
Reference-contexts: Fax: (713) 285-5136. 1 annotated program, data-parallel compilers will generate codes in different native Fortran dialects for different parallel architectures. A prototype Fortran 77D compiler targeting MIMD distributed memory machines has been developed at Rice University [HKK + 91, Tse93] as part of the Para-Scope programming environment <ref> [CCH + 88] </ref>. A Fortran 90D compiler has been written at Syra-cuse University [BCF + 93]. For regular problems, i.e., applications with relatively simple array subscript functions and fixed communication requirements, these compilers have had considerable successes [Tse93].
Reference: [CHMS94] <author> T. W. Clark, R. v. Hanxleden, J. A. McCammon, and L. R. Scott. </author> <title> Parallelization using spatial decomposition for molecular dynamics. </title> <booktitle> In Scalable High Performance Computing Conference, </booktitle> <address> Knoxville, TN, </address> <month> May </month> <year> 1994. </year> <note> Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR93356-S. </note>
Reference-contexts: If an application has good index-based locality, then one might still want to distribute data irregularly to improve load balance, but some or all of the mapping computation can be done on the fly <ref> [BK93, Han92, Bia91, CHMS94] </ref>.) 5 PROGRAM nbf INTEGER i, j, p, t, n$proc, atomMax, pMax, stepMax PARAMETER (n$proc = 8) 5 PARAMETER (atomMax = 8000, pMax = 250, stepMax = 30) INTEGER inb (atomMax), partners (atomMax, pMax) REAL x (atomMax), f (atomMax), force C Fortran D directives 10 DECOMPOSITION atomD (atomMax)
Reference: [CMZ92] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 31-50, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: Several research projects have aimed at providing a "machine-independent parallel program ming style" as a more user-friendly and investment-preserving alternative. Here the applications programmer uses a dialect of sequential Fortran and annotates it with high-level mapping infor mation. Examples include High Performance Fortran (HPF) [KLS + 94], Vienna Fortran <ref> [CMZ92] </ref>, and Fortran D [FHK + 90], which comes in two flavors, Fortran 77D and Fortran 90D. From this fl Corresponding author. E-mail: reinhard@rice.edu. Phone: (713) 527-8750x2740. Fax: (713) 285-5136. 1 annotated program, data-parallel compilers will generate codes in different native Fortran dialects for different parallel architectures. <p> Given a data structure which already reflects the physical locality characteristics of the underlying problem, it distributes data and computation in a load balanced fashion. To specify irregular mappings in a data-parallel context, index-based mapping arrays [WSBH91] or mapping functions <ref> [CMZ92] </ref> have been proposed. For these, however, the programmer has to provide such arrays or functions that explicitly map indices to processors, even though the programmer may not be interested in what exactly these mappings look like. Value-based distributions were initially proposed as an enhancement of Fortran 77D [Han92].
Reference: [DHU + 93] <author> R. Das, Y.-S. Hwang, M. Uysal, J. Saltz, and A. Sussman. </author> <title> Applying the CHAOS/PARTI library to irregular problems in computational chemistry and computational aerodynamics. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <institution> Mississippi State University, Starkville, MS, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: Therefore, not only storing but also accessing the information adds complexity to the program. As mentioned in Section 2, run-time libraries such as CHAOS can take most of the complexity of this task from the programmer <ref> [DHU + 93] </ref>, but their use still requires explicit managing of the data structures associated with irregular distributions and communications. 3.2 Storing Value-Based Distributed Data Again assuming that x is distributed according to its value, the number of elements of x assigned to each processor typically varies and is not known <p> They have developed a wealth of different mapping strategies, both value-based <ref> [BB87, HS91, DHU + 93] </ref>, and connectivity based [Sim91], which can be used as mapping strategies for irregular distributions. Run-time iteration graphs can assist in improving load balance and access locality when distributing loop iterations across processors [PSC93a]. <p> Run-time iteration graphs can assist in improving load balance and access locality when distributing loop iterations across processors [PSC93a]. High-level library routines, such as CHAOS <ref> [DMS + 92, DHU + 93] </ref>, can assist in tasks like global-to-local name space mappings, communication schedule generation, and schedule based communication.
Reference: [DMS + 92] <author> R. Das, D. Mavriplis, J. Saltz, S. Gupta, and R. Ponnusamy. </author> <title> The design and implementation of a parallel unstructured Euler solver using software primitives, </title> <booktitle> AIAA-92-0562. In Proceedings of the 30th Aerospace Sciences Meeting. AIAA, </booktitle> <month> January </month> <year> 1992. </year>
Reference-contexts: Run-time iteration graphs can assist in improving load balance and access locality when distributing loop iterations across processors [PSC93a]. High-level library routines, such as CHAOS <ref> [DMS + 92, DHU + 93] </ref>, can assist in tasks like global-to-local name space mappings, communication schedule generation, and schedule based communication.
Reference: [DSvH93] <author> R. Das, J. Saltz, and R. v. Hanxleden. </author> <title> Slicing analysis and indirect accesses to distributed arrays. </title> <editor> In U. Banerjee et al., editor, </editor> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> volume 769, </volume> <pages> pages 152-168. </pages> <publisher> Springer, </publisher> <address> Berlin, </address> <month> August </month> <year> 1993. </year> <booktitle> From the Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR. </address> <note> Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR93319-S. </note>
Reference-contexts: Here we will focus on the compiler technology specific to handling such mappings and its relationship to other aspects of the compiler. For a discussion of other, equally important aspects of compiling irregular applications, such as communication analysis and preprocessing, we refer the reader to other publications <ref> [DSvH93, Han93] </ref>. The rest of this paper is organized as follows. Section 2 introduces value-based locality and illustrates the use of value-based mappings with kernels taken from a Molecular Dynamics code and an Unstructured Mesh application. Section 3 lists the implications of value-based mappings for message-passing node programs. <p> This schedule is used in the communication statements that gather coordinates (line 69) and add forces across processors (line 81). The inspector is relatively similar to irregular references to regularly distributed arrays, and we refer to other publications for a more detailed discussion <ref> [DSvH93, Han93] </ref>. One additional complication, however, is that a translation table has to be computed based on D:loc2glob (line 55). This implies the need for solving the reaching decompositions problem both intra- and inter-procedurally [HHKT92]. <p> However, the traces may still become too space consuming, in which case more space saving alternatives, such as a hash table combined with name-space translations on the fly, may be used <ref> [DSvH93] </ref>. 4.5 The Actual Computation After distributing data and prefetching off-processor references, the actual computation can be performed (lines 62 : : : 87).
Reference: [FHK + 90] <author> G. C. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year> <note> Revised April, </note> <year> 1991. </year>
Reference-contexts: Here the applications programmer uses a dialect of sequential Fortran and annotates it with high-level mapping infor mation. Examples include High Performance Fortran (HPF) [KLS + 94], Vienna Fortran [CMZ92], and Fortran D <ref> [FHK + 90] </ref>, which comes in two flavors, Fortran 77D and Fortran 90D. From this fl Corresponding author. E-mail: reinhard@rice.edu. Phone: (713) 527-8750x2740. Fax: (713) 285-5136. 1 annotated program, data-parallel compilers will generate codes in different native Fortran dialects for different parallel architectures.
Reference: [GB88] <author> W. F. van Gunsteren and H. J. C. Berendsen. GROMOS: </author> <title> GROningen MOlecular Simulation software. </title> <type> Technical report, </type> <institution> Laboratory of Physical Chemistry, University of Groningen, </institution> <address> Nijenborgh, The Netherlands, </address> <year> 1988. </year>
Reference-contexts: Another example where data are not related by indices, but values, are molecular dynamics programs such as GROMOS <ref> [GB88] </ref>, CHARMM [BBO + 83], or ARGOS [SM90] that are used to simulate biomolecular systems. One important routine common to these programs is the nonbonded force (NBF) routine, which typically accounts for the bulk of the computational work (around 90%).
Reference: [Han92] <author> R. v. Hanxleden. </author> <title> Compiler support for machine independent parallelization of irregular problems. </title> <type> Technical Report CRPC-TR92301-S, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> November </month> <year> 1992. </year> <note> Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR92301-S. </note>
Reference-contexts: If an application has good index-based locality, then one might still want to distribute data irregularly to improve load balance, but some or all of the mapping computation can be done on the fly <ref> [BK93, Han92, Bia91, CHMS94] </ref>.) 5 PROGRAM nbf INTEGER i, j, p, t, n$proc, atomMax, pMax, stepMax PARAMETER (n$proc = 8) 5 PARAMETER (atomMax = 8000, pMax = 250, stepMax = 30) INTEGER inb (atomMax), partners (atomMax, pMax) REAL x (atomMax), f (atomMax), force C Fortran D directives 10 DECOMPOSITION atomD (atomMax) <p> For these, however, the programmer has to provide such arrays or functions that explicitly map indices to processors, even though the programmer may not be interested in what exactly these mappings look like. Value-based distributions were initially proposed as an enhancement of Fortran 77D <ref> [Han92] </ref>. A variant of it, based on a GeoCoL (Geometrical, Connectivity and/or Load) data structure, has since then been implemented in a Fortran 90D prototype compiler by Ponnusamy et al. [PSC93a, PSC + 93b].
Reference: [Han93] <author> R. v. Hanxleden. </author> <title> Handling irregular problems with Fortran D | A preliminary report. </title> <booktitle> In Proceedings of the Fourth Workshop on Compilers for Parallel Computers, </booktitle> <pages> pages 353-364, </pages> <address> Delft, The Nether-lands, </address> <month> December </month> <year> 1993. </year> <note> D Newsletter #9, available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR93339-S. </note>
Reference-contexts: Here we will focus on the compiler technology specific to handling such mappings and its relationship to other aspects of the compiler. For a discussion of other, equally important aspects of compiling irregular applications, such as communication analysis and preprocessing, we refer the reader to other publications <ref> [DSvH93, Han93] </ref>. The rest of this paper is organized as follows. Section 2 introduces value-based locality and illustrates the use of value-based mappings with kernels taken from a Molecular Dynamics code and an Unstructured Mesh application. Section 3 lists the implications of value-based mappings for message-passing node programs. <p> Note that the same problem occurs when regularly distributed arrays are accessed irregularly and we wish to append buffer space for off-processor data at the end of the array <ref> [Han93] </ref>. 3.3 Translating Name Spaces Translating between the global name space of a Fortran D program and the local name space of the node program is an important component of the parallelization process. <p> This schedule is used in the communication statements that gather coordinates (line 69) and add forces across processors (line 81). The inspector is relatively similar to irregular references to regularly distributed arrays, and we refer to other publications for a more detailed discussion <ref> [DSvH93, Han93] </ref>. One additional complication, however, is that a translation table has to be computed based on D:loc2glob (line 55). This implies the need for solving the reaching decompositions problem both intra- and inter-procedurally [HHKT92].
Reference: [HHKT92] <author> M. W. Hall, S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Interprocedural compilation of Fortran D for MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <address> Minneapolis, MN, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: One additional complication, however, is that a translation table has to be computed based on D:loc2glob (line 55). This implies the need for solving the reaching decompositions problem both intra- and inter-procedurally <ref> [HHKT92] </ref>. Note that the current strategy for collecting off-processor references is to record each individual subscript in a trace array. This trace array is created in global name space (j$glob, see lines 47 : : : 54) and then converted into local name space (j$loc, line 56).
Reference: [HKK + 91] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> An overview of the Fortran D programming system. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: From this fl Corresponding author. E-mail: reinhard@rice.edu. Phone: (713) 527-8750x2740. Fax: (713) 285-5136. 1 annotated program, data-parallel compilers will generate codes in different native Fortran dialects for different parallel architectures. A prototype Fortran 77D compiler targeting MIMD distributed memory machines has been developed at Rice University <ref> [HKK + 91, Tse93] </ref> as part of the Para-Scope programming environment [CCH + 88]. A Fortran 90D compiler has been written at Syra-cuse University [BCF + 93]. For regular problems, i.e., applications with relatively simple array subscript functions and fixed communication requirements, these compilers have had considerable successes [Tse93].
Reference: [HS91] <author> R. v. Hanxleden and L. R. Scott. </author> <title> Load balancing on message passing architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13 </volume> <pages> 312-324, </pages> <year> 1991. </year>
Reference-contexts: They have developed a wealth of different mapping strategies, both value-based <ref> [BB87, HS91, DHU + 93] </ref>, and connectivity based [Sim91], which can be used as mapping strategies for irregular distributions. Run-time iteration graphs can assist in improving load balance and access locality when distributing loop iterations across processors [PSC93a].
Reference: [KLS + 94] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: Several research projects have aimed at providing a "machine-independent parallel program ming style" as a more user-friendly and investment-preserving alternative. Here the applications programmer uses a dialect of sequential Fortran and annotates it with high-level mapping infor mation. Examples include High Performance Fortran (HPF) <ref> [KLS + 94] </ref>, Vienna Fortran [CMZ92], and Fortran D [FHK + 90], which comes in two flavors, Fortran 77D and Fortran 90D. From this fl Corresponding author. E-mail: reinhard@rice.edu. Phone: (713) 527-8750x2740. <p> These extensions could also be applied directly to the HPF standard <ref> [KLS + 94] </ref>. We were able to limit ourselves to a simple extension of the already existing DISTRIBUTE and ALIGN directives, as was also illustrated by the code in Figure 3.
Reference: [KMCKC93] <author> U. Kremer, J. Mellor-Crummey, K. Kennedy, and A. Carle. </author> <title> Automatic data layout for distributed-memory machines in the D programming environment. </title> <editor> In Christoph W. Kessler, editor, </editor> <title> Automatic Parallelization | New Approaches to Code Generation, Data Distribution, </title> <booktitle> and Performance Prediction, </booktitle> <pages> pages 136-152. </pages> <booktitle> Vieweg Advanced Studies in Computer Science, </booktitle> <publisher> Verlag Vieweg, Wiesbaden, </publisher> <address> Germany, </address> <year> 1993. </year> <note> Also available as technical report CRPC-TR93-298-S, </note> <institution> Rice University. </institution> <month> 21 </month>
Reference-contexts: We will refer to this characteristic as index-based locality. Exactly how arrays should be mapped in the presence of index-based locality is by no means trivial and still an active field of research <ref> [KMCKC93] </ref>. However, one can generally assume that only regular mappings (like BLOCK, CYCLIC, or BLOCK-CYCLIC) and remappings have to be considered. For irregular problems, this assumption cannot be made.
Reference: [KMV90] <author> C. Koelbel, P. Mehrotra, and J. Van Rosendale. </author> <title> Supporting shared data structures on distributed memory machines. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle, WA, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: To still generate efficient code, one should precompute and reuse as much of this information as possible. For example, the inspector-executor paradigm allows us to message-vectorize low-locality data accesses, even in the absence of compile-time knowledge <ref> [MSS + 88, KMV90] </ref>. First, an "inspection phase" determines what data have to be communicated within a certain loop and generates a communication schedule.
Reference: [MSS + 88] <author> R. Mirchandaney, J. Saltz, R. Smith, D. Nicol, and K. Crowley. </author> <title> Principles of runtime support for parallel processors. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <pages> pages 140-152, </pages> <address> St. Malo, France, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: To still generate efficient code, one should precompute and reuse as much of this information as possible. For example, the inspector-executor paradigm allows us to message-vectorize low-locality data accesses, even in the absence of compile-time knowledge <ref> [MSS + 88, KMV90] </ref>. First, an "inspection phase" determines what data have to be communicated within a certain loop and generates a communication schedule.
Reference: [PSC93a] <author> R. Ponnusamy, J. Saltz, and A. Choudhary. </author> <title> Runtime compilation techniques for data partitioning and communication schedule reuse. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 361-370. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year> <note> Technical Report CS-TR-3055 and UMIACS-TR-93-32, University of Maryland, April `93. Available via anonymous ftp from hyena.cs.umd.edu. </note>
Reference-contexts: an "execution phase" (repeatedly) performs the actual computation and uses the communication schedule to exchange data. 3.5 A Bootstrapping Problem One characteristic of value-based mappings is that they may pose a certain bootstrapping problem to both the user and the compiler, as has already been identified by Ponnussamy et al. <ref> [PSC93a] </ref>. This problem occurs when an array is distributed based on its own values, which is considered perfectly legal and actually the case in the code shown in Figure 3. <p> In fact, the strategy specified by the user might be passed through verbatim to the run-time library. However, one might still require a certain minimal set of strategies to be always available; see <ref> [PSC93a] </ref> for a comparison. Note also that the user may not select a specific strategy for distributing data, as shown in since the default number of dimensions is one and some key words are optional, the DISTRIBUTE atomD (VALUE (DIM=1,VALS=x,WEIGHT=inb)) could also be written as DISTRIBUTE atomD (VALUE (x, inb)). <p> They have developed a wealth of different mapping strategies, both value-based [BB87, HS91, DHU + 93], and connectivity based [Sim91], which can be used as mapping strategies for irregular distributions. Run-time iteration graphs can assist in improving load balance and access locality when distributing loop iterations across processors <ref> [PSC93a] </ref>. High-level library routines, such as CHAOS [DMS + 92, DHU + 93], can assist in tasks like global-to-local name space mappings, communication schedule generation, and schedule based communication. <p> Value-based distributions were initially proposed as an enhancement of Fortran 77D [Han92]. A variant of it, based on a GeoCoL (Geometrical, Connectivity and/or Load) data structure, has since then been implemented in a Fortran 90D prototype compiler by Ponnusamy et al. <ref> [PSC93a, PSC + 93b] </ref>. However, the GeoCoL structure still has to be managed explicitly by the programmer. 7 Summary and Conclusions Data-parallel languages, such as HPF or Fortran D, promise a user-friendly, efficient programming environment for parallel machines.
Reference: [PSC + 93b] <author> R. Ponnusamy, J. Saltz, A. Choudhary, Y.-S. Hwang, and G. Fox. </author> <title> Runtime support and compilation methods for user-specified data distributions. </title> <institution> Technical Report CS-TR-3194 and UMIACS-TR-93-135, University of Maryland, </institution> <month> November </month> <year> 1993. </year> <note> Available via anonymous ftp from hyena.cs.umd.edu. </note>
Reference-contexts: Value-based distributions were initially proposed as an enhancement of Fortran 77D [Han92]. A variant of it, based on a GeoCoL (Geometrical, Connectivity and/or Load) data structure, has since then been implemented in a Fortran 90D prototype compiler by Ponnusamy et al. <ref> [PSC93a, PSC + 93b] </ref>. However, the GeoCoL structure still has to be managed explicitly by the programmer. 7 Summary and Conclusions Data-parallel languages, such as HPF or Fortran D, promise a user-friendly, efficient programming environment for parallel machines.
Reference: [SH91] <author> J. P. Singh and J. L. Hennessy. </author> <title> An empirical investigation of the effectiveness and limitations of automatic parallelization. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessing, </booktitle> <address> Tokyo, Japan, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: For regular problems, i.e., applications with relatively simple array subscript functions and fixed communication requirements, these compilers have had considerable successes [Tse93]. However, irregular problems, such as unstructured mesh codes, molecular dynamics, galaxy simulations, or computational fluid dynamics, have turned out to be significantly harder to paral-lelize <ref> [SH91] </ref>. Common problems are low access locality, both within and across data structures, and poor load balance. For example, assume that the value assigned to some array element a (i) depends on some other array element b (j).
Reference: [Sim91] <author> H. Simon. </author> <title> Partitioning of unstructured mesh problems for parallel processing. </title> <booktitle> In Proceedings of the Conference on Parallel Methods on Large Scale Structural Analysis and Physics Applications. </booktitle> <publisher> Pergamon Press, </publisher> <year> 1991. </year>
Reference-contexts: They have developed a wealth of different mapping strategies, both value-based [BB87, HS91, DHU + 93], and connectivity based <ref> [Sim91] </ref>, which can be used as mapping strategies for irregular distributions. Run-time iteration graphs can assist in improving load balance and access locality when distributing loop iterations across processors [PSC93a].
Reference: [SM90] <author> T. P. Straatsma and J. Andrew McCammon. ARGOS, </author> <title> a vectorized general molecular dynamics program. </title> <journal> Journal of Computational Chemistry, </journal> <volume> II(8):943-951, </volume> <year> 1990. </year>
Reference-contexts: Another example where data are not related by indices, but values, are molecular dynamics programs such as GROMOS [GB88], CHARMM [BBO + 83], or ARGOS <ref> [SM90] </ref> that are used to simulate biomolecular systems. One important routine common to these programs is the nonbonded force (NBF) routine, which typically accounts for the bulk of the computational work (around 90%). Figure 1 shows an abstracted version of a sequential NBF calculation.
Reference: [Tse93] <author> C. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: From this fl Corresponding author. E-mail: reinhard@rice.edu. Phone: (713) 527-8750x2740. Fax: (713) 285-5136. 1 annotated program, data-parallel compilers will generate codes in different native Fortran dialects for different parallel architectures. A prototype Fortran 77D compiler targeting MIMD distributed memory machines has been developed at Rice University <ref> [HKK + 91, Tse93] </ref> as part of the Para-Scope programming environment [CCH + 88]. A Fortran 90D compiler has been written at Syra-cuse University [BCF + 93]. For regular problems, i.e., applications with relatively simple array subscript functions and fixed communication requirements, these compilers have had considerable successes [Tse93]. <p> A Fortran 90D compiler has been written at Syra-cuse University [BCF + 93]. For regular problems, i.e., applications with relatively simple array subscript functions and fixed communication requirements, these compilers have had considerable successes <ref> [Tse93] </ref>. However, irregular problems, such as unstructured mesh codes, molecular dynamics, galaxy simulations, or computational fluid dynamics, have turned out to be significantly harder to paral-lelize [SH91]. Common problems are low access locality, both within and across data structures, and poor load balance.
Reference: [WCSM93] <author> Y.-T. Wong, T. W. Clark, J. Shen, and J. A. McCammon. </author> <title> Molecular dynamics simulation of substrate-enzyme interactions in the active site channel of superoxide dismutase. </title> <journal> Journal of Molecular Simulation, </journal> <volume> 10(2-6):277-289, </volume> <year> 1993. </year>
Reference-contexts: of the syntax proposed for value-based mappings.) In other words, if the values of two elements of 1 SOD (superoxide dismutase) is a catalytic enzyme that converts the toxic free-radical, O 4 2 , a byproduct of aerobic respiration, to the neutral molecules O 2 and H 2 O 2 <ref> [WCSM93] </ref>. 4 x are close together, they are likely to be on the same processor. The actual use of a value-based distribution can be seen in Figure 3, which shows a Fortran D implementation of the NBF kernel outlined in Figure 1.
Reference: [WSBH91] <author> J. Wu, J. Saltz, H. Berryman, and S. Hiranandani. </author> <title> Distributed memory compiler design for sparse problems. </title> <type> ICASE Report 91-13, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> January </month> <year> 1991. </year> <month> 22 </month>
Reference-contexts: A translation table maps global indices i glob into pairs (i loc ; p) of local indices and processor numbers. Often the translation table itself is too large to be fully replicated and is distributed instead <ref> [WSBH91] </ref>. Therefore, not only storing but also accessing the information adds complexity to the program. <p> Given a data structure which already reflects the physical locality characteristics of the underlying problem, it distributes data and computation in a load balanced fashion. To specify irregular mappings in a data-parallel context, index-based mapping arrays <ref> [WSBH91] </ref> or mapping functions [CMZ92] have been proposed. For these, however, the programmer has to provide such arrays or functions that explicitly map indices to processors, even though the programmer may not be interested in what exactly these mappings look like.
References-found: 30

