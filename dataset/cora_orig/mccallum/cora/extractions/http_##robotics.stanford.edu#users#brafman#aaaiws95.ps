URL: http://robotics.stanford.edu/users/brafman/aaaiws95.ps
Refering-URL: http://www.robotics.stanford.edu/users/brafman/bio.html
Root-URL: http://www.robotics.stanford.edu
Email: brafman@cs.stanford.edu  moshet@ie.technion.ac.il  
Title: Action Prediction Using a Mental-Level Model  
Author: Ronen I. Brafman Moshe Tennenholtz 
Address: Stanford, CA 94305-2140  Haifa 32000, Israel  
Affiliation: Stanford University Dept. of Computer Science  Faculty of Industrial Engineering and Management Technion  
Abstract: We propose a formal approach to the problem of prediction based on the following steps: First, a mental-level model is constructed based on the agent's previous actions; Consequently, the model is updated to account for any new observations by the agent, and finally, we predict the optimal action w.r.t. the agent's mental state as its next action. This paper formalizes this prediction process. In order to carry out this process we need to understand how a mental state can be ascribed to an agent, and how this mental state should be updated. In [ Brafman and Tennenholtz, 1994b ] we examined the first stage. Here we investigate a particular update operator, whose use requires making only weak modeling assumptions, and show that this operator has a number of desirable properties. Finally, we provide an algorithm for ascribing the agent's mental state under this operator. 
Abstract-found: 1
Intro-found: 1
Reference: [ Alchourron et al., 1985 ] <author> Alchourron, C. E.; Gardenfors, P.; and Makinson, D. </author> <year> 1985. </year> <title> On the logic of theory change: partial meet functions for contraction and revision. </title> <journal> Journal of Symbolic Logic 50 </journal> <pages> 510-530. </pages>
Reference: [ Brafman and Tennenholtz, 1994a ] <author> Brafman, R. I. and Tennenholtz, M. </author> <year> 1994a. </year> <title> Belief ascription. </title> <type> Technical report, </type> <institution> Stanford University. </institution>
Reference-contexts: Two examples of decision criteria are maximin, which chooses the tuples in which the worst case outcome is maximal, and the principle of indifference which prefers tuples whose average outcome is maximal.(A fuller discussion of decision criteria appears in <ref> [ Luce and Raiffa, 1957, Brafman and Tennenholtz, 1994a ] </ref> .) We come to a key definition that ties all of the components we have discussed so far. <p> That is, it would have to generate for a given local state be-liefs that would have made the agent act as it did. We assume such an algorithm as a subroutine (see <ref> [ Brafman and Tennenholtz, 1994a ] </ref> ). What we offer here is an algorithm for generating an admissible belief assignment given this subroutine. We show that the complexity of obtaining global consistency is a low polynomial in the complexity of computing a locally consistent assignment in a given local state.
Reference: [ Brafman and Tennenholtz, 1994b ] <author> Brafman, R. I. and Tennenholtz, M. </author> <year> 1994b. </year> <title> Belief ascription and mental-level modelling. </title> <editor> In Doyle, J.; Sandewall, E.; and Torasso, P., editors 1994b, </editor> <booktitle> Proc. of Fourth Intl. Conf. on Principles of Knowledge Representation and Reasoning. </booktitle> <pages> 87-98. </pages>
Reference-contexts: Motivated by work in decision-theory [ Luce and Raiffa, 1957 ] and work on knowledge ascription [ Halpern and Moses, 1990, Rosenschein, 1985 ] , we suggested in <ref> [ Brafman and Tennenholtz, 1994b ] </ref> a specific structure for mental-level models, consisting of beliefs, desires and a decision criterion. <p> In order to perform this prediction process we must understand how beliefs can be ascribed, how they should be updated, and how they should be used to determine the best perceived action. We have examined the first and the last question in <ref> [ Brafman and Tennenholtz, 1994b ] </ref> (although not in the context of prediction). In this paper we wish to concentrate on the second question, that of modeling the agent's belief change. <p> Our discussion of the problem of prediction will be in the context of the framework of mental-level modeling and belief ascription investigated in <ref> [ Brafman and Tennenholtz, 1994b ] </ref> . This framework is reviewed in Section 2. In Section 3 we discuss the problem of prediction. We suggest a three-step process for prediction and highlight the importance of the ascription of a belief change operator to this process. <p> Moreover, we show that under minimal assumptions, this belief change operator can always be ascribed to an agent. Finally, in Section 5 we discuss the algorithmic construction of beliefs obeying this belief change operator. The Framework We start by establishing a structure for mental-level models. Our framework, discussed in <ref> [ Brafman and Tennenholtz, 1994b ] </ref> , is motivated by the work of Halpern and Moses [ Halpern and Moses, 1990 ] and Rosenschein [ Rosenschein, 1985 ] on knowledge ascription, and by ideas from decision-theory [ Savage, 1972, Luce and Raiffa, 1957 ] . <p> If the robot believes fA; B; Cg, a better path would be along the middle, rather than the left-hand side. Our ability to ascribe belief in the framework of the mental-level model just presented is thoroughly discussed in <ref> [ Brafman and Tennenholtz, 1994b ] </ref> . Predictions We wish to explore the application of mental-level models in a particular form of prediction: We observed an agent taking part in some activity; we know its goals; and we wish to predict its next actions. <p> We discussed the first among these issues in our previous work <ref> [ Brafman and Tennenholtz, 1994b ] </ref> . In particular, we have shown a class of agents that can be ascribed the mental-level model discussed in Section 2. We devote the rest of this paper to the second issue. <p> Our work brings to this task a special bias in the form of the agency-hypothesis: Machines are agents of their designers; they are usually designed with a purpose in mind and with some underlying assumptions; therefore, they should be modeled accordingly. With this motivation in mind, this work and <ref> [ Brafman and Tennenholtz, 1994b ] </ref> attempt to understand the basis for modeling entities as if they have a mental state. <p> The central issues are: what elements should such a model contain? How should we use observable information to construct it? And, under what assumptions is our modeling "bias" justified? This paper complements our previous work on belief ascription <ref> [ Brafman and Tennenholtz, 1994b ] </ref> and helps supply initial answers to the above-mentioned questions. In this paper, we reviewed our proposal for the structure of mental-level models and their construction, and explained how they can be used to predict an agent's future behavior. <p> Putting these ingredients together, we get a theory of action prediction using a mental-level model, which consists of the three-step process, a theory of belief ascription (discussed in <ref> [ Brafman and Tennenholtz, 1994b ] </ref> ), and a study of belief change modelling. Acknowledgement We are grateful to Yoav Shoham and Nir Friedman for their valuable comments on this work. Ronen Brafman was supported in part by AFOSR grant AF F49620-94-1-0090.
Reference: [ Brafman et al., 1994 ] <author> Brafman, R. I.; Latombe, J. C.; Moses, Y.; and Shoham, Y. </author> <year> 1994. </year> <title> Knowledge as a tool in motion planning under uncertainty. </title> <editor> In Fagin, R., editor 1994, </editor> <booktitle> Proc. 5th Conf. on Theor. Asp. of Reas. about Know., </booktitle> <address> San Francisco. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 208-224. </pages>
Reference-contexts: This state will relate three key notions: beliefs, goals and decision criteria. This state will manifest itself in the agent's behavior. 3 We will assume runs are finite. The extension to infinite runs is straightforward. 4 A continuous model of time may be preferred here. This is possible, e.g., <ref> [ Brafman et al., 1994 ] </ref> . We start the description of the mental level model by defining the notion of belief. Belief is part of an abstract description of the agent's state. It sums up the agent's view of the world, and is a basis for decision making.
Reference: [ Cohen et al., 1995 ] <author> Cohen, P.; Atkin, M.; Oates, T.; and Gregory, D. </author> <year> 1995. </year> <title> A representation and learning mechanism for mental states. </title> <booktitle> In Proc. of the AAAI Spring Symposium on Representing Mental States and Mechanisms. </booktitle> [ <institution> del Val and Shoham, </institution> <note> 1993 ] del Val, </note> <author> A. and Shoham, Y. </author> <year> 1993. </year> <title> Deriving properties of belief update from theories of action. </title> <booktitle> In Proc. Eleventh Intl. Joint Conf. on Artificial Intelligence (IJCAI '89). </booktitle> <pages> 584-589. </pages>
Reference-contexts: Modelling data is a central task of machine-learning. Much like our work, these models are constructed to help make predictions, e.g., a decision tree helps us predict what class an instance belongs to, or the detection algorithm of <ref> [ Cohen et al., 1995 ] </ref> . Our work brings to this task a special bias in the form of the agency-hypothesis: Machines are agents of their designers; they are usually designed with a purpose in mind and with some underlying assumptions; therefore, they should be modeled accordingly.
Reference: [ Fagin et al., 1994 ] <author> Fagin, R.; Halpern, J. Y.; Moses, Y.; and Vardi, M. Y. </author> <year> 1994. </year> <title> Reasoning about Knowledge. </title> <publisher> MIT Press. to appear. </publisher>
Reference-contexts: the results of actions are nondeterministic and in which the environment may take actions as well can be mapped into this framework using richer state descriptions and larger sets of states, a common practice in game theory. 2 Though context is an overloaded term, its use here seems appropriate, following <ref> [ Fagin et al., 1994 ] </ref> . part of its local state and r can only be obtained in worlds in which the actual position of the robot is r.
Reference: [ Friedman and Halpern, 1994 ] <author> Friedman, N. and Halpern, J. Y. </author> <year> 1994. </year> <title> A knowledge-based framework for belief change. Part I: </title> <booktitle> Foundations. In Proc. of the Fifth Conf. on Theoretical Aspects of Reasoning About Knowledge, </booktitle> <address> San Francisco, California. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Goldzmidt and Pearl, 1992 ] <author> Goldzmidt, M. and Pearl, J. </author> <year> 1992. </year> <title> Rank-based systems: A simple approach to belief revision, belief update and reasoning about evidence and actions. </title> <booktitle> In Principles of Knowledge Representation and Reasoning: Proc. Third Intl. Conf. (KR '92). </booktitle> <pages> 661-672. </pages>
Reference: [ Halpern and Moses, 1990 ] <author> Halpern, J. Y. and Moses, Y. </author> <year> 1990. </year> <title> Knowledge and common knowledge in a distributed environment. </title> <editor> J. </editor> <booktitle> ACM 37(3) </booktitle> <pages> 549-587. </pages>
Reference-contexts: We present a formalism that attempts to make these ideas more concrete, and will hopefully lead to a better understanding of how the ascription of mental state could be mechanized. Motivated by work in decision-theory [ Luce and Raiffa, 1957 ] and work on knowledge ascription <ref> [ Halpern and Moses, 1990, Rosenschein, 1985 ] </ref> , we suggested in [ Brafman and Tennenholtz, 1994b ] a specific structure for mental-level models, consisting of beliefs, desires and a decision criterion. <p> Finally, in Section 5 we discuss the algorithmic construction of beliefs obeying this belief change operator. The Framework We start by establishing a structure for mental-level models. Our framework, discussed in [ Brafman and Tennenholtz, 1994b ] , is motivated by the work of Halpern and Moses <ref> [ Halpern and Moses, 1990 ] </ref> and Rosenschein [ Rosenschein, 1985 ] on knowledge ascription, and by ideas from decision-theory [ Savage, 1972, Luce and Raiffa, 1957 ] . To clarify the concepts used we will refer to the following example.
Reference: [ Katsuno and Mendelzon, 1991 ] <author> Katsuno, H. and Mendelzon, A. </author> <year> 1991. </year> <title> On the difference between updating a knowledge base and revising it. </title> <booktitle> In Principles of Knowledge Representation and Reasoning: Proc. Second Intl. Conf. (KR '91). </booktitle> <pages> 387-394. </pages>
Reference: [ Levesque, 1984 ] <author> Levesque, H. J. </author> <year> 1984. </year> <title> A logic of implicit and explicit belief. </title> <booktitle> In Proc. National Conf. on Artificial Intelligence (AAAI '84). </booktitle> <pages> 198-202. </pages>
Reference: [ Levesque, 1986 ] <author> Levesque, H. J. </author> <year> 1986. </year> <title> Making believers out of computers. </title> <booktitle> Artificial Intelligence 30 </booktitle> <pages> 81-108. </pages>
Reference-contexts: Thus, we are more concerned with modelling agent's ascribed beliefs, than with designing them. An important related work that shares some of our perspective is Levesque's work <ref> [ Levesque, 1986 ] </ref> , which is concerned with treating computers as believers. However, this work describes the beliefs of one particular class of agents whose actions are answering queries. Our work attempts to address a more general class of agents, whose actions are arbitrary.
Reference: [ Luce and Raiffa, 1957 ] <author> Luce, R. D and Raiffa, H. </author> <year> 1957. </year> <title> Games and Decisions. </title> <publisher> John Wiley & Sons, </publisher> <address> New York. </address>
Reference-contexts: We present a formalism that attempts to make these ideas more concrete, and will hopefully lead to a better understanding of how the ascription of mental state could be mechanized. Motivated by work in decision-theory <ref> [ Luce and Raiffa, 1957 ] </ref> and work on knowledge ascription [ Halpern and Moses, 1990, Rosenschein, 1985 ] , we suggested in [ Brafman and Tennenholtz, 1994b ] a specific structure for mental-level models, consisting of beliefs, desires and a decision criterion. <p> Our framework, discussed in [ Brafman and Tennenholtz, 1994b ] , is motivated by the work of Halpern and Moses [ Halpern and Moses, 1990 ] and Rosenschein [ Rosenschein, 1985 ] on knowledge ascription, and by ideas from decision-theory <ref> [ Savage, 1972, Luce and Raiffa, 1957 ] </ref> . To clarify the concepts used we will refer to the following example. Example 1 We start with a robot located at an initial position. <p> Two examples of decision criteria are maximin, which chooses the tuples in which the worst case outcome is maximal, and the principle of indifference which prefers tuples whose average outcome is maximal.(A fuller discussion of decision criteria appears in <ref> [ Luce and Raiffa, 1957, Brafman and Tennenholtz, 1994a ] </ref> .) We come to a key definition that ties all of the components we have discussed so far.
Reference: [ McCarthy, 1979 ] <author> McCarthy, J. </author> <year> 1979. </year> <title> Ascribing mental qualities to machines. </title> <editor> In Ringle, M., editor 1979, </editor> <booktitle> Philosophical Perspectives in Artificial Intelligence, </booktitle> <address> Atlantic Highlands, NJ. </address> <publisher> Humanities Press. </publisher>
Reference-contexts: The goal of this paper is to advance our understanding of basic questions related to the construction of a mental-level model, and in particular its application to prediction. The idea of ascribing mental qualities for the purpose of prediction is not new. John McCarthy talks about it in <ref> [ McCarthy, 1979 ] </ref> . One important aspect of McCarthy's ideas is that the agent's mental state is ascribed.
Reference: [ Newell, 1980 ] <author> Newell, A. </author> <year> 1980. </year> <title> The knowledge level. </title> <journal> AI Magazine 1-20. </journal>
Reference-contexts: Thus, McCarthy views mental qualities as abstractions. This view is shared by another well-known author, Allen Newell <ref> [ Newell, 1980 ] </ref> , who contemplates the possibility of viewing computer programs at a level more abstract than that of the programming language, which he calls the knowledge-level. The notion of a mental state is useful because it is abstract.
Reference: [ Rosenschein, 1985 ] <author> Rosenschein, S. J. </author> <year> 1985. </year> <title> Formal theories of knowledge in AI and robotics. </title> <journal> New Generation Comp. </journal> <volume> 3 </volume> <pages> 345-357. </pages>
Reference-contexts: We present a formalism that attempts to make these ideas more concrete, and will hopefully lead to a better understanding of how the ascription of mental state could be mechanized. Motivated by work in decision-theory [ Luce and Raiffa, 1957 ] and work on knowledge ascription <ref> [ Halpern and Moses, 1990, Rosenschein, 1985 ] </ref> , we suggested in [ Brafman and Tennenholtz, 1994b ] a specific structure for mental-level models, consisting of beliefs, desires and a decision criterion. <p> The Framework We start by establishing a structure for mental-level models. Our framework, discussed in [ Brafman and Tennenholtz, 1994b ] , is motivated by the work of Halpern and Moses [ Halpern and Moses, 1990 ] and Rosenschein <ref> [ Rosenschein, 1985 ] </ref> on knowledge ascription, and by ideas from decision-theory [ Savage, 1972, Luce and Raiffa, 1957 ] . To clarify the concepts used we will refer to the following example. Example 1 We start with a robot located at an initial position.
Reference: [ Savage, 1972 ] <author> Savage, L. J. </author> <year> 1972. </year> <title> The Foundations of Statistics. </title> <publisher> Dover Publications, </publisher> <address> New York. </address>
Reference-contexts: Our framework, discussed in [ Brafman and Tennenholtz, 1994b ] , is motivated by the work of Halpern and Moses [ Halpern and Moses, 1990 ] and Rosenschein [ Rosenschein, 1985 ] on knowledge ascription, and by ideas from decision-theory <ref> [ Savage, 1972, Luce and Raiffa, 1957 ] </ref> . To clarify the concepts used we will refer to the following example. Example 1 We start with a robot located at an initial position.
Reference: [ von Neumann and Morgenstern, 1944 ] <author> Neumann, J.von and Morgenstern, O. </author> <year> 1944. </year> <title> Theory of Games and Economic Behavior. </title> <publisher> Princeton University Press, Princeton. </publisher>
Reference-contexts: We start with the agent's preference order over the set of run suffixes, represented by a utility function. This preference order embodies the relative desirability of different futures. Definition 6 A utility function u is a real-valued function on the set of run-suffixes. It is well known <ref> [ von Neumann and Morgenstern, 1944 ] </ref> that a utility function can represent preference orders satisfying certain assumptions, which in this paper we will accept.
References-found: 18

