URL: ftp://ftp.cis.upenn.edu/pub/pelachaud/cognitive_science94/cognitive_science94.ps.Z
Refering-URL: http://www.cis.upenn.edu/~hms/publications.html
Root-URL: 
Title: Generating Facial Expressions for Speech  
Abstract-found: 0
Intro-found: 1
Reference: [Argyle and Cook, 1976] <author> Argyle, M. and Cook, M. </author> <year> (1976). </year> <title> Gaze and Mutual gaze. </title> <publisher> Cambridge University Press. </publisher>
Reference-contexts: Facial signals also express affectual signals, which may be used communicatively, to influence the other participant's behavior <ref> [Argyle and Cook, 1976] </ref>, [Collier, 1985]. <p> exposition we illustrate our algorithms with examples. 2 Background We present here the background and some definitions relevant to our study. 2.1 The Different Determinants of Facial Expressions Among their other functions, facial movements are used to delineate items in a sequence as punctuation marks do in a written text <ref> [Argyle and Cook, 1976] </ref>. For example raising the eyebrows can punctuate a discourse. We consider the following determinants as defined in Section 1.1: conversational signals : correspond to facial actions occurring on accented items or on emphatic segments; these actions clarify and support what is being said. <p> Eye movements can be defined by the gaze direction, the point or points of fixation, the percentage of eye contact over gaze avoidance (with respect to another conversant), and the duration of eye contact <ref> [Argyle and Cook, 1976] </ref>. A common variable of eye behavior is interest. Eyes scan the objects of interest with longer glances. <p> When looking at a picture of a person, viewers are found to look by saccade mainly at the eyes (58% of the time), then at the mouth (13%); the remaining regions of the face are each scanned just 1% of the time <ref> [Argyle and Cook, 1976] </ref>. 10 2.2.3 Eye Contact Eye contact is an important non-verbal process to establish relationship as well as to communicate with others: 1. Depending on the situation, eye contact or its avoidance can be variously interpreted [Argyle and Cook, 1976]. 2. <p> of the face are each scanned just 1% of the time <ref> [Argyle and Cook, 1976] </ref>. 10 2.2.3 Eye Contact Eye contact is an important non-verbal process to establish relationship as well as to communicate with others: 1. Depending on the situation, eye contact or its avoidance can be variously interpreted [Argyle and Cook, 1976]. 2. It plays an important role during social encounters to process information, to seek or send it, and to establish and synchronize the conversation [Argyle and Cook, 1976]. 3. It is also linked with intonation. <p> Depending on the situation, eye contact or its avoidance can be variously interpreted <ref> [Argyle and Cook, 1976] </ref>. 2. It plays an important role during social encounters to process information, to seek or send it, and to establish and synchronize the conversation [Argyle and Cook, 1976]. 3. It is also linked with intonation. It is used to keep control of the communication process [Duncan, 1974]. 4. It follows the same rules as head movements for speaking turns. <p> In this study we consider two types of blinks: * Periodic blinks keep the eyes wet. On average, they appear every 4.8 sec. and last about 1/4 of a sec., with 1/8 sec. of closure time, 1/24 sec. of closed eyes, and 1/12 sec. of opening time <ref> [Argyle and Cook, 1976] </ref>. Nevertheless, their period of occurrence is affect-dependent [Collier, 1985] (Table 13). * Voluntary blinks serve to emphasize speech, to accentuate a word, or to mark a pause [Ekman, 1979]. They are either synchronized to the word or to the syllable level [Condon and Osgton, 1971]. <p> Gaze can be used to establish power relationships or it can act as a signal of liking <ref> [Argyle and Cook, 1976] </ref>. Personality and context are also parameters of the visual pattern. A submissive person more frequently breaks eye contact than a dominant one. Moreover, during a dialog situation the listener moves in synchrony with the speaker [Condon and Osgton, 1971]. <p> Like speech acts, they can be used indirectly. We leave the specification of these parameters to the animator. B.1 Choice of the Rules for Affects From the definition of affects presented in a previous section, and information stated in <ref> [Argyle and Cook, 1976] </ref>, [Collier, 1985], [Ekman, 1979], we established the corresponding facial expression of affects in term of a set of AUs, eye openness, pupil size and types of movements (Fig. 15).
Reference: [Badler et al., 1993] <author> Badler, N.I., Phillips, C., and Webber, B. </author> <year> (1993). </year> <title> Simulating Humans: Computer Graphics Animation and Control. </title> <publisher> Oxford University Press. </publisher>
Reference-contexts: The script files describing the animation 4 Visual accuracy is defined by the lighting conditions and visibility conditions of the speaker's lip [Jeffers and Barley, 1971]. 17 are saved and the animation is played through Jack R fl <ref> [Badler et al., 1993] </ref>, a 3-D human animation software system developed at the University of Pennsylvania. 3.5 Example Before outlining each procedure of our algorithm, we present an example to clarify the process. <p> In our case, none are needed since already computed blinks occur at a sufficient rate. * The facial expressions of each item of the utterance are computing using Platt's program [Platt, 1985]. Script files are output. * The animation is done using the Jack software <ref> [Badler et al., 1993] </ref>. 19 4 Details for each Determinant So far, we have presented the main steps of the algorithm.
Reference: [Benoit et al., 1990] <author> Benoit, C., Lallouache, T., Mohamedi, T., Tseva, A., and Abry, C. </author> <year> (1990). </year> <title> Nineteen (+- two) french visemes for visual speech synthesis. </title> <booktitle> In Proceedings of the ESCA Workshop on Speech Synthesis, Autrans. ESCA. </booktitle>
Reference-contexts: Clustering visible speech by facial expression enhances speech perception. In speechreading techniques, phonemes are grouped in the way that one lip shape corresponds to one cluster (called visemes <ref> [Benoit et al., 1990] </ref>). These viseme groups may be ranked from the least deformable group (such as f`f',`v'g cluster) to the most deformable one (i.e., very context dependent such as f`l', `n'g cluster) [Brooke, 1990], [Jeffers and Barley, 1971]. This clustering is speech-rate dependent (see Table 7).
Reference: [Bolinger, 1986] <author> Bolinger, D. </author> <year> (1986). </year> <title> Intonation and its Part. </title> <publisher> Stanford University Press. </publisher>
Reference-contexts: While formal theories of the discourse semantics of such signals are generally lacking, in the area of spoken intonation, at least, there are more-or-less formalisable accounts of the way spoken intonation conveys discourse information associated with a speaker's messages [Halliday, 1967], <ref> [Bolinger, 1986] </ref>, [Pierrehumbert and Hirschberg, 1990], [Steedman, 1991]. Similarly, some psychologists have claimed to find universal facial expressions linked to affects and attitudes [Ekman, 1979]. Animating the face by specifying every action manually is a very tedious task and often does not yield entirely appropriate facial expression. <p> On the other hand, rules expressing affect alter the entire utterance. Moreover, affect is expressed vocally through the variation of paralinguistic parameters but does not modify the type or the placement of the accents relative to words in the utterance <ref> [Bolinger, 1986] </ref>. This is an important property that allows decomposition into AUs of the various facial patterns corresponding to either affect, accents, or other vocal parameters as well as simultaneous additive combination of all these facial actions.
Reference: [Bourne, 1973] <author> Bourne, G. </author> <year> (1973). </year> <title> Structure and Function of Muscle, volume III, Physiology and Biochemistry. </title> <publisher> Academic Press, </publisher> <address> second edition edition. </address>
Reference-contexts: After the first computation, we check that the current speech posture has time to contract after the previous speech posture (or, respectively, to relax before the next one). If the time between two consecutive articulatory configurations is smaller than the contraction time of a muscle <ref> [Bourne, 1973] </ref>, the previous speech posture is influenced by the contraction of the current one. In a similar manner, if the time between two consecutive speech postures is smaller than the relaxation time, the current segment will influence the next segment when relaxing.
Reference: [Brooke, 1990] <author> Brooke, N. </author> <year> (1990). </year> <title> Computer graphics synthesis of talking faces. </title> <booktitle> In Proceedings of the ESCA Workshop on Speech Synthesis, Autrans. ESCA. </booktitle>
Reference-contexts: These viseme groups may be ranked from the least deformable group (such as f`f',`v'g cluster) to the most deformable one (i.e., very context dependent such as f`l', `n'g cluster) <ref> [Brooke, 1990] </ref>, [Jeffers and Barley, 1971]. This clustering is speech-rate dependent (see Table 7). A person speaking fast moves lips much less than a person talking slowly. We decided to use this technique because of its reliability in describing visible lip movements. <p> But in some cases this is not enough since the correct position can depend on up to five segments before or after the current one [Kent and Minifie, 1977]. Some rules look at the context of phoneme production to compute adequate lip positions <ref> [Brooke, 1990] </ref>, [Kent and Minifie, 1977], [Cohen and Massaro, 1993]. Nevertheless no completely satisfactory set of rules solving every coarticulation problem exist.
Reference: [Cahn, 1989] <author> Cahn, J. </author> <year> (1989). </year> <title> Generating expression in synthesized speech. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, Cambridge, Massachusetts. </institution>
Reference-contexts: The latter reference claims that suprasegmental features are systematically related to discourse information units corresponding to topic or theme (that is, what the discourse segment is about) and comment or rheme (that is, what novel information the utterance supplies). Listeners may also detect the speaker's affect from prosodic features <ref> [Cahn, 1989] </ref>. Affects seem to be differentiated mainly by pitch (while frequency is a physical property of a sound, pitch is a subjective one), loudness (the perceived intensity of a sound), pitch contour (the global envelope of the pitch), tempo (rate of speech), and pause [Cahn, 1989]. 2.3.2 Notational System The <p> speaker's affect from prosodic features <ref> [Cahn, 1989] </ref>. Affects seem to be differentiated mainly by pitch (while frequency is a physical property of a sound, pitch is a subjective one), loudness (the perceived intensity of a sound), pitch contour (the global envelope of the pitch), tempo (rate of speech), and pause [Cahn, 1989]. 2.3.2 Notational System The notation for intonation contours that we use is derived from J. Pierrehumbert [Pierrehumbert, 1980]. We follow [Pierrehumbert and Hirschberg, 1990] [Prevost and Steedman, to appear] in assuming that different intonational tunes are used to convey various discourse-related distinctions of focus, contrast and propositional attitude.
Reference: [Calvert, 1990] <author> Calvert, T. </author> <year> (1990). </year> <title> Composition of realistic animation sequences for multiple human figures. </title> <editor> In Badler, N.I., Barsky, B., and Zeltzer, D., editors, </editor> <title> Making Them Move: Mechanics, Control, and Animation of Articulated Figures. </title> <publisher> Morgan Kaufmann Publishers Inc. </publisher>
Reference-contexts: The coordination of these various facial motions with the intonation is done completely automatically, by rule. Moreover, this method allows us to define various and individualized speaker characteristics by specifying particular sets of type and timing parameters for the facial actions <ref> [Calvert, 1990] </ref>, [Ekman, 1979], [Moravetz, 1989], [Unuma and Takeuchi, 1991].
Reference: [Cassell et al., 1994] <author> Cassell, J., Pelachaud, C., Badler, N.I., Steedman, M., Achorn, B., Becket, T., Douville, B., Prevost, S., and Stone, M. </author> <year> (1994). </year> <title> Animated conversation: Rule-based generation of facial expression, gesture and spoken intonation for multiple conversational agents. </title> <booktitle> Computer Graghics Annual Conferences Series, </booktitle> <pages> pages 413-420. </pages>
Reference-contexts: This is the basic principle which regulates the computations in our facial animation system. In recent work with Justine Cassell this system and related assumptions about synchrony have been extended to manual gesture <ref> [Cassell et al., 1994] </ref>. 3 Description of our System The following Sections describe the system in more detail. 3.1 Input Assumptions The input to the program is a file containing an utterance already decomposed and written in its phonological representation with its accents marked in its bracketed elements. <p> In this phase, after recording a sentence, the timing of each phoneme and pause was extracted from a spectrogram. In more recent work we use a query answering program including a sentence generation and a Bell Labs speech synthesizer to automate the determination of paralinguistic parameters and phoneme timing <ref> [Cassell et al., 1994] </ref>. At the beginning of the file, the user specifies the desired affectual parameters and their intensity (a number between 0 for minimum intensity and 1 for maximum intensity) (see Appendix B). Three levels of description are represented in the input. <p> To obtain a refinement of the computation of eye movements we add in the regulator group the auditor feedback determinant [Duncan, 1974] and consider various parameters (such as maximum gaze length, percentage of mutual gaze) <ref> [Cassell et al., 1994] </ref>. Another extension to the system is the integration of emblems and emotional emblems [Ekman, 1979]. The last ones are expressed by employing parts of the corresponding affect they refer to, while the first ones are used to replace and repeat verbal elements. <p> While the examples here are determined by measurement from real speech, we have reported a somewhat narrower range of examples generated entirely by rule from machine-generated semantic representations via a speech synthesizer <ref> [Cassell et al., 1994] </ref>, [Prevost and Steedman, 1994], [Prevost and Steedman, to appear]. Our model can be expected to help further research of human communicative faculties via automatically synthesized animation.
Reference: [Cathiard et al., 1991] <author> Cathiard, M., Tiberghien, G., Cirot-Tseva, A., Lallouache, M., and Escudier, P. </author> <year> (1991). </year> <title> Visual perception of anticipatory rounding during acoustic pauses: A cross-language study. </title> <booktitle> In Proceedings of the XIIth International Congress of Phonetic Sciences, </booktitle> <pages> pages 50-53, </pages> <address> Aix-en-Provence, France. </address>
Reference-contexts: Moreover, articulatory adjustments continue on the pause existing just after the considered word or is foreseen on the pause just before the word <ref> [Cathiard et al., 1991] </ref>. These influences are computed by simulating this muscular contraction and relaxation properties by two third-degree polynomial curves [Pelachaud, 1991]. Finally, we take into account the geometric relationship between successive actions. Lip closure is more easily performed from a slightly parted position than from a puckered position.
Reference: [Cohen and Massaro, 1993] <author> Cohen, M. and Massaro, D. </author> <year> (1993). </year> <title> Modeling coarticulation in synthetic visual speech. </title> <editor> In Magnenat-Thalmann, N. and Thalmann, D., editors, </editor> <title> Computer Animation '93. </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: A correspondence between each speech unit and a basic lip shape is established. Of particular relevance is the model of coarticulation proposed by <ref> [Cohen and Massaro, 1993] </ref>. It uses overlapping dominance functions. These functions specify for each viseme how close the lips reach their target value. Greater realism at the expense of synthetic control comes from techniques which extract information from live-animation [Williams, 1990], [Terzopoulos and Waters, 1991], [Essa and Pentland, 1994]. <p> But in some cases this is not enough since the correct position can depend on up to five segments before or after the current one [Kent and Minifie, 1977]. Some rules look at the context of phoneme production to compute adequate lip positions [Brooke, 1990], [Kent and Minifie, 1977], <ref> [Cohen and Massaro, 1993] </ref>. Nevertheless no completely satisfactory set of rules solving every coarticulation problem exist. We view lip movements corresponding to speech as a sequence of key positions (corresponding to phonemes belonging to non-deformable clusters) and transition positions (corresponding to phonemes belonging to deformable clusters).
Reference: [Collier, 1985] <author> Collier, G. </author> <year> (1985). </year> <title> Emotional Expression. </title> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: Facial signals also express affectual signals, which may be used communicatively, to influence the other participant's behavior [Argyle and Cook, 1976], <ref> [Collier, 1985] </ref>. <p> movements appear frequently as conversational signals [Ekman, 1979], though rapid head movements, gaze direction and eyeblinks may also be involved. 8 punctuators : correspond to facial actions occurring on pauses; these actions can reduce the ambiguity of the speech by grouping or separating sequences of words into discrete unit phrases <ref> [Collier, 1985] </ref>. Specific head motions, a blink, or eyebrow actions may highlight a pause. manipulators : correspond to the biological needs of the face (such as blinking to wet the eyes). regulators : help the interaction between speaker-listener as they control the flow of speech. <p> On average, they appear every 4.8 sec. and last about 1/4 of a sec., with 1/8 sec. of closure time, 1/24 sec. of closed eyes, and 1/12 sec. of opening time [Argyle and Cook, 1976]. Nevertheless, their period of occurrence is affect-dependent <ref> [Collier, 1985] </ref> (Table 13). * Voluntary blinks serve to emphasize speech, to accentuate a word, or to mark a pause [Ekman, 1979]. They are either synchronized to the word or to the syllable level [Condon and Osgton, 1971]. <p> Pupil dilation is followed by pupil constriction during hap piness and anger and remains dilated during fear and sadness [Hess, 1975]. Depending on the affect, eye openness also varies. For surprise and fear the eyes are wide open; they are partially closed during sadness, disgust and happiness <ref> [Collier, 1985] </ref> (Table 10). Many gaze patterns, such as mutual gaze or gaze avoidance, imply the existence of a listener/partner. This is beyond the scope of the present study. <p> Identically, AU17 appears for /AH/ due to the contraction time involved in the pronounciation of /FF/. B Affect Our modelling of affect is not meaning-based and is confined to those affects characteristically displayed on the face and through the voice. Body postures indicate essentially the intensity of affect <ref> [Collier, 1985] </ref>. Happiness is recognized by smile (corners of the mouth are drawn back and up) and raised cheeks creating wrinkles around the eyes. Disgust is characterized by nose wrinkling and raised upper lip. <p> Like speech acts, they can be used indirectly. We leave the specification of these parameters to the animator. B.1 Choice of the Rules for Affects From the definition of affects presented in a previous section, and information stated in [Argyle and Cook, 1976], <ref> [Collier, 1985] </ref>, [Ekman, 1979], we established the corresponding facial expression of affects in term of a set of AUs, eye openness, pupil size and types of movements (Fig. 15). Sadness is the least active affect and shows a few actions appearing and disappearing smoothly on the face. <p> On the other hand an angry person moves in an aggressive 42 and rapid manner punctuating his/her speech with brusque facial actions. The amount and type of movements vary with the level of arousal <ref> [Collier, 1985] </ref>. For example, fear has varying types of action depending on whether the intensity of affect is low or high. Fear is expressed only with the eyes for low intensity, while for intense affect, the entire face is involved.
Reference: [Condon and Osgton, 1971] <author> Condon, W. and Osgton, W. </author> <year> (1971). </year> <title> Speech and body motion synchrony of the speaker-hearer. </title> <editor> In Horton, D. and Jenkins, J., editors, </editor> <booktitle> The perception of Language, </booktitle> <pages> pages 150-184. </pages> <publisher> Academic Press. </publisher>
Reference-contexts: Nevertheless, their period of occurrence is affect-dependent [Collier, 1985] (Table 13). * Voluntary blinks serve to emphasize speech, to accentuate a word, or to mark a pause [Ekman, 1979]. They are either synchronized to the word or to the syllable level <ref> [Condon and Osgton, 1971] </ref>. A blink is considered to be a conversational signal when it occurs on an accented word, a punctuator when it occurs on a pause. 2.3 Intonation The intonational melody of an utterance can be viewed as conveying partial information of three kinds. <p> to syntax and semantics based on Categorial Grammar to produce apparently appropriate intonation contours for spoken responses to database queries. 2.4 Underlying Property An important property linking intonation and facial expression (in fact it extends to gesture and body movement in general) lies in the existence of synchrony between them <ref> [Condon and Osgton, 1971] </ref>, [Unuma and Takeuchi, 1991], [Magnenat-Thalmann and Thalmann, 1987]. The face and the body do not move at random but in concert with the flow of speech. Thus, changes of body posture and orientation occurs at the beginning of a new topic of conversation. <p> Thus, changes of body posture and orientation occurs at the beginning of a new topic of conversation. Similarly, a facial movement might be synchronized at the phoneme level such as blink, or at the word level such as eyebrow movement <ref> [Condon and Osgton, 1971] </ref>. Synchrony implies that changes occurring in speech and in body movements should appear at the same time. Thus facial synchrony is integrated in this body synchrony scheme as an 13 extension of this property. <p> find the two other parameters pitch accent phrase tone boundary tone emphasis RM (Rapid Movement) OM (Ordinary Movement) SM (Slow Movement) RM (Rapid Movement) Table 4: Head movements on accented segments ture of an eye blink (closure time, time it remains closed, time of aperture) is synchronized with the articulation <ref> [Condon and Osgton, 1971] </ref>. To find such a timing, the program parses the utterance and looks at the phonemes which have the closest timing to the average speed of an eye blink. <p> Personality and context are also parameters of the visual pattern. A submissive person more frequently breaks eye contact than a dominant one. Moreover, during a dialog situation the listener moves in synchrony with the speaker <ref> [Condon and Osgton, 1971] </ref>. To obtain a refinement of the computation of eye movements we add in the regulator group the auditor feedback determinant [Duncan, 1974] and consider various parameters (such as maximum gaze length, percentage of mutual gaze) [Cassell et al., 1994].
Reference: [DEC, 1985] <institution> Dectalk (1985). Dectalk and DTC03 Text-To-Speech System Owner's. Digital Equipment Corporation. </institution>
Reference-contexts: Three levels of description are represented in the input. At the segmental level the sentence is specified (either by hand or by the generation program) as a list of strings corresponding to the phonetic representation of the utterance and whose notation is compatible with ascii-keyboard notation <ref> [DEC, 1985] </ref>. Pauses acting either as silence or as syntactic markers (such as comma or period) are included. Each segment and pause is followed by its duration expressed in seconds.
Reference: [Dittmann, 1974] <author> Dittmann, A. </author> <year> (1974). </year> <title> The body movement-speech rhythm relationship as a cue to speech encoding. In Weitz, editor, Nonverbal Communication. </title> <publisher> Oxford University Press. </publisher>
Reference-contexts: As an example, for facial actions occurring in some accents, we define the term first accent as corresponding to the first accent of the utterances; such a choice follows Dittman's results <ref> [Dittmann, 1974] </ref> which found that most movements happen at the beginning of the utterance. <p> less first si+fl less fl+si nose wrinkling less fear yes more every si+he more he+si brow fear less happiness yes more every si+fl more fl+he smile more sadness no less first si+fl+he less less surprise no more first si+fl more fl raised brow less Pauses are classified by their function <ref> [Dittmann, 1974] </ref>: fl: fluent pause occurs at boundary points he: hesitation pause corresponds to false start, word finding problem si: specified silence marked by the speaker POS, RM, OM, SM: head movements (see explanation in Table 1) every: occurrence on every accented segment first: occurrence on the first accented segment of
Reference: [Duncan, 1974] <author> Duncan, S. </author> <year> (1974). </year> <title> Some signals and rules for taking speaking turns in conversations. In Weitz, editor, Nonverbal Communication. </title> <publisher> Oxford University Press. </publisher>
Reference-contexts: Breaking or looking for eye contact with the listener, and turning the head away or toward the listener are part of the elaborated interaction during a conversation <ref> [Duncan, 1974] </ref>. They are decomposed as a Speaker-State-Signal (displayed at the beginning of a speaking turn), a Speaker-Within-Turn (the speaker wants to keep the floor), and a Speaker-Continuation-Signal (frequently follows a Speaker-Within-Turn). A more complete facial animation system is obtained if we include all these determinants. <p> These speeds are affect-dependent (Table 12). Distinct patterns accompany linguistic features [Hadar et al., 1983] (Tables 4, 5, and 6). The occurrence of POS at the beginning of speech between Speaking-Turns <ref> [Duncan, 1974] </ref> and at grammatical pauses [Hadar et al., 1983] imply its involvement in speech production, regulation of turn-taking and finally, in marking syntactic boundaries inside clauses. 2.2.2 Eye Behavior The eyes are always moving. <p> It plays an important role during social encounters to process information, to seek or send it, and to establish and synchronize the conversation [Argyle and Cook, 1976]. 3. It is also linked with intonation. It is used to keep control of the communication process <ref> [Duncan, 1974] </ref>. 4. It follows the same rules as head movements for speaking turns. Indeed, before ceasing to speak, eye contact is temporarily broken, then re-established, to signal the other's turn to speak [Duncan, 1974]. 2.2.4 Eye Blinks Eye blinks occur quite frequently. <p> It is also linked with intonation. It is used to keep control of the communication process <ref> [Duncan, 1974] </ref>. 4. It follows the same rules as head movements for speaking turns. Indeed, before ceasing to speak, eye contact is temporarily broken, then re-established, to signal the other's turn to speak [Duncan, 1974]. 2.2.4 Eye Blinks Eye blinks occur quite frequently. They serve not only to accentuate speech but also to wet the eye. Normally there is at least one eye blink per utterance. In this study we consider two types of blinks: * Periodic blinks keep the eyes wet. <p> A submissive person more frequently breaks eye contact than a dominant one. Moreover, during a dialog situation the listener moves in synchrony with the speaker [Condon and Osgton, 1971]. To obtain a refinement of the computation of eye movements we add in the regulator group the auditor feedback determinant <ref> [Duncan, 1974] </ref> and consider various parameters (such as maximum gaze length, percentage of mutual gaze) [Cassell et al., 1994]. Another extension to the system is the integration of emblems and emotional emblems [Ekman, 1979].
Reference: [Efron, 1972] <author> Efron, D. </author> <year> (1972). </year> <title> Gesture, Race, and Culture. The Hague, </title> <publisher> Mouton. </publisher> <pages> 47 </pages>
Reference-contexts: They are conventionalized. Their encoding and decoding share a lot of appearances and meanings [Ekman, 1976]. Since they are discourse-driven their appearance is entered by the user. This is done by creating a library of possible emblems; Efron gives a large list of them <ref> [Efron, 1972] </ref> and Ekman proposes a set of words which have a corresponding emblem. Nevertheless the user can build his/her own emblems and add them to the library [Pelachaud, 1993]. When lying, the timing of an expression changes.
Reference: [Ekman, 1976] <author> Ekman, P. </author> <year> (1976). </year> <title> Movements with precise meanings. </title> <journal> The Journal of Communication, </journal> <volume> 26. </volume>
Reference-contexts: Most of the time both are intentional, deliberate actions used to communicate. In general 36 they are produced consciously and are driven by the semantics of the utterance. They are conventionalized. Their encoding and decoding share a lot of appearances and meanings <ref> [Ekman, 1976] </ref>. Since they are discourse-driven their appearance is entered by the user. This is done by creating a library of possible emblems; Efron gives a large list of them [Efron, 1972] and Ekman proposes a set of words which have a corresponding emblem.
Reference: [Ekman, 1979] <author> Ekman, P. </author> <year> (1979). </year> <title> About brows: emotional and conversational signals. </title> <editor> In von Cranach, M., Foppa, K., Lepenies, W., and Ploog, D., editors, </editor> <title> Human ethology: claims and limits of a new disipline: </title> <booktitle> contributions to the Colloquium, </booktitle> <pages> pages 169-248. </pages> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England; New-York. </address>
Reference-contexts: While talking, people's faces are rarely still. They not only use their lips to talk, but raise their eyebrows, move or blink their eyes, or nod and turn their head <ref> [Ekman, 1979] </ref>. Facial signals seem to help to regulate the flow of conversation in much the same way as intonation does, signalling emphasis and contrast, as well as information related to turn-taking and control of the floor during an interaction. <p> Similarly, some psychologists have claimed to find universal facial expressions linked to affects and attitudes <ref> [Ekman, 1979] </ref>. Animating the face by specifying every action manually is a very tedious task and often does not yield entirely appropriate facial expression. In order to improve facial animation systems, understanding linguistic semantics and its interaction with intonation is an important priority. <p> We consider the following determinants as defined in Section 1.1: conversational signals : correspond to facial actions occurring on accented items or on emphatic segments; these actions clarify and support what is being said. Eyebrow movements appear frequently as conversational signals <ref> [Ekman, 1979] </ref>, though rapid head movements, gaze direction and eyeblinks may also be involved. 8 punctuators : correspond to facial actions occurring on pauses; these actions can reduce the ambiguity of the speech by grouping or separating sequences of words into discrete unit phrases [Collier, 1985]. <p> Indeed, a speaker may replace common verbal expressions by a specific facial expression, or may display part of the facial expression of an affect to mention it even though it is not actually being felt at the present moment <ref> [Ekman, 1979] </ref>. The appearance of such expressions is voluntary and depend on what is being said. 2.2 Specification of Head and Eye Movements Each facial expression is expressed as a set of AUs. <p> Nevertheless, their period of occurrence is affect-dependent [Collier, 1985] (Table 13). * Voluntary blinks serve to emphasize speech, to accentuate a word, or to mark a pause <ref> [Ekman, 1979] </ref>. They are either synchronized to the word or to the syllable level [Condon and Osgton, 1971]. <p> This is important, since the actions performed by a person while talking may vary. Most people show eyebrow movements to accentuate a word but other facial actions may be chosen such as nose wrinkling or eye flashes <ref> [Ekman, 1979] </ref>. The user just needs to modify the rules which describe the type of facial action and need not alter the rules of occurrence of facial action. Another unknown parameter is the frequency of occurrence of an action [Ekman, 1979]. <p> actions may be chosen such as nose wrinkling or eye flashes <ref> [Ekman, 1979] </ref>. The user just needs to modify the rules which describe the type of facial action and need not alter the rules of occurrence of facial action. Another unknown parameter is the frequency of occurrence of an action [Ekman, 1979]. A paralinguistic feature is not always accompanied by a facial movement (not every accented word 3 Adults and children do not have the same systems of facial expression [Ekman, 1979]. Our focus is on an adult model. 15 is accompanied by an eyebrow movement, for example). <p> Another unknown parameter is the frequency of occurrence of an action <ref> [Ekman, 1979] </ref>. A paralinguistic feature is not always accompanied by a facial movement (not every accented word 3 Adults and children do not have the same systems of facial expression [Ekman, 1979]. Our focus is on an adult model. 15 is accompanied by an eyebrow movement, for example). Thus we need to have access to the timing of the occurrence of an action. Not every rule involves the same level. <p> The list of AUs for the affect is computed and is added to the list of AUs for each item of the utterance (Tables 10 and 11). * Conversational signals appear in this example, on pitch accents under various forms <ref> [Ekman, 1979] </ref>. Eyebrow movements coincide, for both actions, with the stressed syllables. Rapid movements around the actual position of the head characterize the head motion on the pitch accent. <p> Brow actions are frequently used as conversational signals <ref> [Ekman, 1979] </ref>. They can be used to accentuate a word or to emphasize a sequence of words. On accented words, actions may vary with the type of the pitch accent. The user has the possibility to choose the type of parameter defining the facial action. <p> Given a context, an affect is associated with a particular facial expression. But there may be some cultural variability some cultures forbid direct gaze while others find gaze aversion an offense; mourning in some cultures is over-acted while in others it should be masked by a smile. Display Rules <ref> [Ekman, 1979] </ref> refer to this problem of who can show which affect to whom and when. We have not taken them into consideration for automatic procedures since they are very difficult to handle and very little information is available. They may affect expression in various ways. <p> We have not taken them into consideration for automatic procedures since they are very difficult to handle and very little information is available. They may affect expression in various ways. They can amplify, de-amplify, or neutralize an expression <ref> [Ekman, 1979] </ref>; they may blend with other expressions or may even be masked by other facial expressions. The user can simulate these effects through the use of different functions. Amplify, de-amplify and neutralize affect the intensity of the facial changes. The blend is done by summing the effects together. <p> Another extension to the system is the integration of emblems and emotional emblems <ref> [Ekman, 1979] </ref>. The last ones are expressed by employing parts of the corresponding affect they refer to, while the first ones are used to replace and repeat verbal elements. Most of the time both are intentional, deliberate actions used to communicate. <p> The coordination of these various facial motions with the intonation is done completely automatically, by rule. Moreover, this method allows us to define various and individualized speaker characteristics by specifying particular sets of type and timing parameters for the facial actions [Calvert, 1990], <ref> [Ekman, 1979] </ref>, [Moravetz, 1989], [Unuma and Takeuchi, 1991]. <p> Disgust is characterized by nose wrinkling and raised upper lip. Six affects (anger, disgust, fear, happiness, sadness 40 of coarticulation. 41 and surprise) have been claimed to have universal facial expressions <ref> [Ekman, 1979] </ref> corresponding to prototypes (Fig. 15). We have chosen to study these. A person may feel an affect with different strength. <p> Like speech acts, they can be used indirectly. We leave the specification of these parameters to the animator. B.1 Choice of the Rules for Affects From the definition of affects presented in a previous section, and information stated in [Argyle and Cook, 1976], [Collier, 1985], <ref> [Ekman, 1979] </ref>, we established the corresponding facial expression of affects in term of a set of AUs, eye openness, pupil size and types of movements (Fig. 15). Sadness is the least active affect and shows a few actions appearing and disappearing smoothly on the face.
Reference: [Ekman and Friesen, 1978] <author> Ekman, P. and Friesen, W. </author> <year> (1978). </year> <title> Facial Action Coding System. </title> <publisher> Consulting Psychologists Press, Inc. </publisher>
Reference-contexts: To do so, we need to understand the link between spoken intonation, the information transmitted in the given context, and facial movement. We use the FACS notation (Facial Action Coding System) created by P. Ekman and W. Friesen <ref> [Ekman and Friesen, 1978] </ref> to describe visible facial expressions 1 . This system is based on anatomical studies. Every facial action is due to muscular activity, relaxation or contraction. <p> Each AU describes the direct effect of a muscle plus eventual secondary motion due to the propagation of movement, and possible appearance of wrinkles or bulges. We refer the reader to <ref> [Ekman and Friesen, 1978] </ref> for a detailed description of each AU.
Reference: [Emmett, 1985] <author> Emmett, A. </author> <year> (1985). </year> <title> Digital portfolio: Tony de peltrie. </title> <journal> Computer Graphics World, </journal> <volume> 8(10) </volume> <pages> 72-77. </pages>
Reference-contexts: We now show each of the determinants in more detail. 4.1 Lip Shape Conventional cel animation resolves the problem of lip synchronization by defining a set of mouth shapes and timing for speech. Such techniques consider a small number of stereotyped speech postures to produce animation, including computer graphics <ref> [Emmett, 1985] </ref>, [Kleiser-Walczak, 1988]. Even though they produce realistic animations, this technique requires a skilled animator and a considerable investment in time and manual effort. Other systems [Parke, 1982], [Lewis and Parke, 1987], [Magnenat-Thalmann and Thalmann, 1987], [Hill et al., 1988] offer a higher level of parameterization to their model.
Reference: [Essa and Pentland, 1994] <author> Essa, I. and Pentland, A. </author> <year> (1994). </year> <title> A vision system for observing and extracting facial action parameters. </title> <booktitle> Proceedings of Computer Vision and Pattern Recognition (CVPR 94), </booktitle> <pages> pages 76-83. </pages>
Reference-contexts: It uses overlapping dominance functions. These functions specify for each viseme how close the lips reach their target value. Greater realism at the expense of synthetic control comes from techniques which extract information from live-animation [Williams, 1990], [Terzopoulos and Waters, 1991], <ref> [Essa and Pentland, 1994] </ref>. The computed movement information is interpreted as muscle contractions and is given as input to the animation system. Texture mapping is used to enhance the realism of features and skin tone of the model [Nahas et al., 1988], [Williams, 1990], [Kurihara and Arai, 1991].
Reference: [Hadar et al., 1983] <author> Hadar, U., Steiner, T., Grant, E., and Rose, F. C. </author> <year> (1983). </year> <title> Kinematics of head movements accompanying speech during conversation. </title> <booktitle> Human Movement Science, </booktitle> <volume> 2 </volume> <pages> 35-46. </pages>
Reference-contexts: Head movements may be associated with emblems (nodding or shaking for agreement/disagreement), or with maintaining the flow of conversation (turn taking 9 value deg=sec POS RM OM SM max 125.0 77.0 59.2 28.8 Head movements are classified by their frequency and amplitude <ref> [Hadar et al., 1983] </ref>. POS: Postural Shift (high frequency, wide amplitude) RM, OM, SM: small amplitude and various frequencies. Table 1: Velocity values for each class of head movements system). <p> Head direction may depend upon affect (sadness is marked with a downward direction) (Table 10) or used to point at something. Four classes of head motions are distinguished by their amplitude and frequency: slow movements (SM), ordinary movements (OM), and rapid movements (RM) <ref> [Hadar et al., 1983] </ref>. Postural Shifts (POS) are defined as linear movements of wide amplitude (i.e., they change the axis of motion) [Hadar et al., 1983] (Table 1). These speeds are affect-dependent (Table 12). Distinct patterns accompany linguistic features [Hadar et al., 1983] (Tables 4, 5, and 6). <p> Four classes of head motions are distinguished by their amplitude and frequency: slow movements (SM), ordinary movements (OM), and rapid movements (RM) <ref> [Hadar et al., 1983] </ref>. Postural Shifts (POS) are defined as linear movements of wide amplitude (i.e., they change the axis of motion) [Hadar et al., 1983] (Table 1). These speeds are affect-dependent (Table 12). Distinct patterns accompany linguistic features [Hadar et al., 1983] (Tables 4, 5, and 6). The occurrence of POS at the beginning of speech between Speaking-Turns [Duncan, 1974] and at grammatical pauses [Hadar et al., 1983] imply its involvement in <p> frequency: slow movements (SM), ordinary movements (OM), and rapid movements (RM) <ref> [Hadar et al., 1983] </ref>. Postural Shifts (POS) are defined as linear movements of wide amplitude (i.e., they change the axis of motion) [Hadar et al., 1983] (Table 1). These speeds are affect-dependent (Table 12). Distinct patterns accompany linguistic features [Hadar et al., 1983] (Tables 4, 5, and 6). The occurrence of POS at the beginning of speech between Speaking-Turns [Duncan, 1974] and at grammatical pauses [Hadar et al., 1983] imply its involvement in speech production, regulation of turn-taking and finally, in marking syntactic boundaries inside clauses. 2.2.2 Eye Behavior The <p> (i.e., they change the axis of motion) <ref> [Hadar et al., 1983] </ref> (Table 1). These speeds are affect-dependent (Table 12). Distinct patterns accompany linguistic features [Hadar et al., 1983] (Tables 4, 5, and 6). The occurrence of POS at the beginning of speech between Speaking-Turns [Duncan, 1974] and at grammatical pauses [Hadar et al., 1983] imply its involvement in speech production, regulation of turn-taking and finally, in marking syntactic boundaries inside clauses. 2.2.2 Eye Behavior The eyes are always moving. <p> Head movements are computed depending on the type of accents. The program scans the utterance and assigns the corresponding head movements to the considered segments (Table 4). 4.4 Punctuators A boundary point (such as a comma) is underlined by slow movement and a final pause coincides with stillness <ref> [Hadar et al., 1983] </ref>. When occurring on an hesitation pause the type of accent varies with the affect (Table 15). But the type varies also with the type of pause. <p> At the end of the computation a specific and unique pattern is found for each clause. But we note in the case of successive utterances having in common certain features of intonational tunes, or part of the same topic, that they share the same type of head motion <ref> [Hadar et al., 1983] </ref>. The second part of the computation of regulators is the eye motion. For a first approach we decided to implement a simple simulation of eye movements (Fig. 12).
Reference: [Halliday, 1967] <author> Halliday, M. </author> <year> (1967). </year> <title> Intonation and Grammar in British English. Mouton, The Hague. </title>
Reference-contexts: While formal theories of the discourse semantics of such signals are generally lacking, in the area of spoken intonation, at least, there are more-or-less formalisable accounts of the way spoken intonation conveys discourse information associated with a speaker's messages <ref> [Halliday, 1967] </ref>, [Bolinger, 1986], [Pierrehumbert and Hirschberg, 1990], [Steedman, 1991]. Similarly, some psychologists have claimed to find universal facial expressions linked to affects and attitudes [Ekman, 1979]. Animating the face by specifying every action manually is a very tedious task and often does not yield entirely appropriate facial expression. <p> The first is information about the syntax (and therefore the semantics) of an utterance [Selkirk, 1984], [Hirschberg and Pierrehumbert, 1986], [Pierrehumbert and Hirschberg, 1990]. We claim, following <ref> [Halliday, 1967] </ref>, [Isard and Pearson, 1988], [Prevost and Steedman, to appear] that this information includes markers of questioning, 11 stating, and other speech acts, and markers of discourse information including topic or theme, comment or rheme, focus or new information, and background or given information.
Reference: [Harper et al., 1978] <author> Harper, R., Wiens, A., and Matarazzo, J. </author> <year> (1978). </year> <title> Nonverbal Communication: The State of the Art. </title> <editor> J. </editor> <publisher> Wiley and Sons, </publisher> <address> New York. </address>
Reference-contexts: As we have seen, gaze behavior plays a great role in communication settings but some cultural differences are found in the amount of gaze allowed during a social encounter <ref> [Harper et al., 1978] </ref>. Gaze can be used to establish power relationships or it can act as a signal of liking [Argyle and Cook, 1976]. Personality and context are also parameters of the visual pattern. A submissive person more frequently breaks eye contact than a dominant one.
Reference: [Hess, 1975] <author> Hess, E. </author> <year> (1975). </year> <title> The role of the pupil size in communication. </title> <publisher> Scientific American, </publisher> <pages> pages 113-119. </pages>
Reference-contexts: Nevertheless, the eyes are forced to follow head motion when the head position has passed a certain angle. Pupil changes occur during affectual experiences. Pupil dilation is followed by pupil constriction during hap piness and anger and remains dilated during fear and sadness <ref> [Hess, 1975] </ref>. Depending on the affect, eye openness also varies. For surprise and fear the eyes are wide open; they are partially closed during sadness, disgust and happiness [Collier, 1985] (Table 10). Many gaze patterns, such as mutual gaze or gaze avoidance, imply the existence of a listener/partner.
Reference: [Hill et al., 1988] <author> Hill, D., Pearce, A., and Wyvill, B. </author> <year> (1988). </year> <title> Animating speech: an automated approach using speech synthesised by rules. </title> <journal> The Visual Computer, </journal> <volume> 3 </volume> <pages> 277-289. </pages>
Reference-contexts: Automatic lip synchronization is included into animation systems by a multi-layer approach [Kalra et al., 1991] or by adding speech parameters [Nahas et al., 1988], <ref> [Hill et al., 1988] </ref>. A correspondence between each speech unit and a basic lip shape is established. Of particular relevance is the model of coarticulation proposed by [Cohen and Massaro, 1993]. It uses overlapping dominance functions. These functions specify for each viseme how close the lips reach their target value. <p> Even though they produce realistic animations, this technique requires a skilled animator and a considerable investment in time and manual effort. Other systems [Parke, 1982], [Lewis and Parke, 1987], [Magnenat-Thalmann and Thalmann, 1987], <ref> [Hill et al., 1988] </ref> offer a higher level of parameterization to their model. Parameters are grouped to represent the mouth shape of each phoneme. The user only needs to work at the phoneme level and not at the low level of facial parameters.
Reference: [Hirschberg and Pierrehumbert, 1986] <author> Hirschberg, J. and Pierrehumbert, J. </author> <year> (1986). </year> <title> The intonational structuring of discourse. </title> <booktitle> In 24th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 136-144. </pages>
Reference-contexts: The first is information about the syntax (and therefore the semantics) of an utterance [Selkirk, 1984], <ref> [Hirschberg and Pierrehumbert, 1986] </ref>, [Pierrehumbert and Hirschberg, 1990].
Reference: [Isard and Pearson, 1988] <author> Isard, S. and Pearson, M. </author> <year> (1988). </year> <title> A repertoire of British English intonation contours for synthetic speech. </title> <booktitle> In Proceedings of Speech '88, 7th FASE Symposium, </booktitle> <pages> pages 1233-1240, </pages> <address> Edinburgh. </address>
Reference-contexts: The first is information about the syntax (and therefore the semantics) of an utterance [Selkirk, 1984], [Hirschberg and Pierrehumbert, 1986], [Pierrehumbert and Hirschberg, 1990]. We claim, following [Halliday, 1967], <ref> [Isard and Pearson, 1988] </ref>, [Prevost and Steedman, to appear] that this information includes markers of questioning, 11 stating, and other speech acts, and markers of discourse information including topic or theme, comment or rheme, focus or new information, and background or given information.
Reference: [Jeffers and Barley, 1971] <author> Jeffers, J. and Barley, M. </author> <year> (1971). </year> <title> Speechreading (lipreading). </title> <type> C.C. </type> <institution> Thomas. </institution>
Reference-contexts: The script files describing the animation 4 Visual accuracy is defined by the lighting conditions and visibility conditions of the speaker's lip <ref> [Jeffers and Barley, 1971] </ref>. 17 are saved and the animation is played through Jack R fl [Badler et al., 1993], a 3-D human animation software system developed at the University of Pennsylvania. 3.5 Example Before outlining each procedure of our algorithm, we present an example to clarify the process. <p> prev. sgmt != `wh', `ww' AU11+20+25 prec. sgmt = `wh', `ww' AU12+25 prec. sgmt = `wh', `ww' AU12+25 AU11: nasolabial furrow deepener AU12: lip corner puller AU17: chin raiser AU20: lip stretcher AU25: lips part AU28: lip suck Table 2: An example of rules to compute lip shapes Speechreading techniques <ref> [Jeffers and Barley, 1971] </ref> offer a tool to interpret lip and facial movements to help the hearing-impaired understand speech. Clustering visible speech by facial expression enhances speech perception. <p> These viseme groups may be ranked from the least deformable group (such as f`f',`v'g cluster) to the most deformable one (i.e., very context dependent such as f`l', `n'g cluster) [Brooke, 1990], <ref> [Jeffers and Barley, 1971] </ref>. This clustering is speech-rate dependent (see Table 7). A person speaking fast moves lips much less than a person talking slowly. We decided to use this technique because of its reliability in describing visible lip movements. An example of the rules is given in Table 2. <p> Considering tongue movement when lip shapes are computed helps to make unambiguous obscure movements of some phonemic segments (since some speech postures are only differentiated by their tongue motions <ref> [Jeffers and Barley, 1971] </ref>, [Kent and Minifie, 1977]) [Pelachaud et al., 1994]. 7 Conclusion We have proposed a method of characterizing any facial movements by separating them into phonemic, intonational, informational and affectual determinants. We believe that no previous computational model has taken into account all of these factors.
Reference: [Kalra et al., 1991] <author> Kalra, P., Mangili, A., Magnenat-Thalmann, N., and Thalmann, D. </author> <year> (1991). </year> <title> Smile: A multilayered facial animation system. </title> <editor> In Kunii, T., editor, </editor> <booktitle> Modeling in Computer Graphics. </booktitle> <publisher> Springer-Verlag. </publisher>
Reference-contexts: By integrating models of several layers of facial tissue with dynamic simulation of muscle movements, considerable realism in creating subtle facial actions may be achieved [Terzopoulos and Waters, 1991]. Automatic lip synchronization is included into animation systems by a multi-layer approach <ref> [Kalra et al., 1991] </ref> or by adding speech parameters [Nahas et al., 1988], [Hill et al., 1988]. A correspondence between each speech unit and a basic lip shape is established. Of particular relevance is the model of coarticulation proposed by [Cohen and Massaro, 1993]. It uses overlapping dominance functions.
Reference: [Kent and Minifie, 1977] <author> Kent, R. and Minifie, F. </author> <year> (1977). </year> <title> Coarticulation in recent speech production models. </title> <journal> Journal of Phonetics, </journal> <volume> 5 </volume> <pages> 115-135. </pages>
Reference-contexts: But in some cases this is not enough since the correct position can depend on up to five segments before or after the current one <ref> [Kent and Minifie, 1977] </ref>. Some rules look at the context of phoneme production to compute adequate lip positions [Brooke, 1990], [Kent and Minifie, 1977], [Cohen and Massaro, 1993]. Nevertheless no completely satisfactory set of rules solving every coarticulation problem exist. <p> But in some cases this is not enough since the correct position can depend on up to five segments before or after the current one <ref> [Kent and Minifie, 1977] </ref>. Some rules look at the context of phoneme production to compute adequate lip positions [Brooke, 1990], [Kent and Minifie, 1977], [Cohen and Massaro, 1993]. Nevertheless no completely satisfactory set of rules solving every coarticulation problem exist. We view lip movements corresponding to speech as a sequence of key positions (corresponding to phonemes belonging to non-deformable clusters) and transition positions (corresponding to phonemes belonging to deformable clusters). <p> The transition position receives the same shape as the `strongest' key-position (`strongest' meaning lip shapes belonging to the least deformable clusters). Two rules are considered: the forward and backward rules. They consider articulatory adjustment on a sequence of consonants followed or preceded by a vowel <ref> [Kent and Minifie, 1977] </ref>. Forward coarticulation arises in a sequence of consonants (not belonging to the low deformable clusters such as f`f', `v'g cluster) followed by a vowel, showing an articulatory adjustment. <p> Indeed, the lips show the influence of the vowel on the first consonant of the sequence. In the sequence `istrstry' (French example taken from sinistre structure cited in <ref> [Kent and Minifie, 1977] </ref>) the influence of the `y' is shown on the first 23 `s' (forward rule). To solve particular problems (certain visual transitions between segments) which cannot be solved by these two rules, we consider a three-step algorithm. <p> For fast speech-rate, there are fewer different cluster, in fact, most of them gather into one type: moderate opening of the mouth performed by the AU25. * From Table 7, we can notice that the forward rule is applied across word boundaries <ref> [Kent and Minifie, 1977] </ref>; the segment /AX/, even though it is another word, receives the same list of AUs as the segment /YU/ of the previous word. 32 context Fred /WW/ /AA/ /NN/ /TT/ /IH/ /DD/ 1 onset apex offset 2 onset apex context to try to produce a &lt;silence&gt; play <p> Considering tongue movement when lip shapes are computed helps to make unambiguous obscure movements of some phonemic segments (since some speech postures are only differentiated by their tongue motions [Jeffers and Barley, 1971], <ref> [Kent and Minifie, 1977] </ref>) [Pelachaud et al., 1994]. 7 Conclusion We have proposed a method of characterizing any facial movements by separating them into phonemic, intonational, informational and affectual determinants. We believe that no previous computational model has taken into account all of these factors.
Reference: [Kleiser-Walczak, 1988] <institution> Kleiser-Walczak (1988). Sextone for president. </institution> <note> ACM SIGGRAPH '88 Film and Video Show, issue 38/39. Kleiser Walczak Construction Co. </note>
Reference-contexts: Such techniques consider a small number of stereotyped speech postures to produce animation, including computer graphics [Emmett, 1985], <ref> [Kleiser-Walczak, 1988] </ref>. Even though they produce realistic animations, this technique requires a skilled animator and a considerable investment in time and manual effort. Other systems [Parke, 1982], [Lewis and Parke, 1987], [Magnenat-Thalmann and Thalmann, 1987], [Hill et al., 1988] offer a higher level of parameterization to their model.
Reference: [Kurihara and Arai, 1991] <author> Kurihara, T. and Arai, K. </author> <year> (1991). </year> <title> A transformation method for modeling and animation of the human face from photographs. </title> <editor> In Magnenat-Thalmann, N. and Thalmann, D., editors, </editor> <booktitle> Computer Animation '91, </booktitle> <pages> pages 45-58. </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The computed movement information is interpreted as muscle contractions and is given as input to the animation system. Texture mapping is used to enhance the realism of features and skin tone of the model [Nahas et al., 1988], [Williams, 1990], <ref> [Kurihara and Arai, 1991] </ref>. The consideration of wrinkles and of aging effects [Viaud and Yahia, 1992] adds much to the rendering of facial skin and expressions. 1.1 The Approach Our main goal is to look at coordinated motion during a conversation.
Reference: [Lewis and Parke, 1987] <author> Lewis, J. and Parke, F. </author> <year> (1987). </year> <title> Automated lip-synch and speech synthesis for character animation. </title> <booktitle> CHI + GI, </booktitle> <pages> pages 143-147. </pages>
Reference-contexts: Such techniques consider a small number of stereotyped speech postures to produce animation, including computer graphics [Emmett, 1985], [Kleiser-Walczak, 1988]. Even though they produce realistic animations, this technique requires a skilled animator and a considerable investment in time and manual effort. Other systems [Parke, 1982], <ref> [Lewis and Parke, 1987] </ref>, [Magnenat-Thalmann and Thalmann, 1987], [Hill et al., 1988] offer a higher level of parameterization to their model. Parameters are grouped to represent the mouth shape of each phoneme.
Reference: [Magnenat-Thalmann and Thalmann, 1987] <author> Magnenat-Thalmann, N. and Thalmann, D. </author> <year> (1987). </year> <title> The direction of synthetic actors in the film rendez-vous a montreal. </title> <journal> IEEE Computer Graphics and Applications, </journal> <pages> pages 9-19. </pages>
Reference-contexts: Earlier systems for facial animation [Parke, 1982], <ref> [Magnenat-Thalmann and Thalmann, 1987] </ref> were based on a set 4 of parameters which affect not only the structure of the model (long nose, short forehead) but also its expressions (opening of the mouth, raising eyebrows). <p> to produce apparently appropriate intonation contours for spoken responses to database queries. 2.4 Underlying Property An important property linking intonation and facial expression (in fact it extends to gesture and body movement in general) lies in the existence of synchrony between them [Condon and Osgton, 1971], [Unuma and Takeuchi, 1991], <ref> [Magnenat-Thalmann and Thalmann, 1987] </ref>. The face and the body do not move at random but in concert with the flow of speech. Thus, changes of body posture and orientation occurs at the beginning of a new topic of conversation. <p> Such techniques consider a small number of stereotyped speech postures to produce animation, including computer graphics [Emmett, 1985], [Kleiser-Walczak, 1988]. Even though they produce realistic animations, this technique requires a skilled animator and a considerable investment in time and manual effort. Other systems [Parke, 1982], [Lewis and Parke, 1987], <ref> [Magnenat-Thalmann and Thalmann, 1987] </ref>, [Hill et al., 1988] offer a higher level of parameterization to their model. Parameters are grouped to represent the mouth shape of each phoneme. The user only needs to work at the phoneme level and not at the low level of facial parameters.
Reference: [Moravetz, 1989] <author> Moravetz, C. </author> <year> (1989). </year> <title> A high level approach to animating secondary human movement. </title> <type> Master's thesis, </type> <institution> School of Computing Science, Simon Fraser University. </institution>
Reference-contexts: The user can therefore modify one parameter for one action without touching any other variable in the system. This process allows the user to independently vary the manifestation of the speaker's attitude (what is to be conveyed) and individuality in the computation of facial expressions <ref> [Moravetz, 1989] </ref>. * One important enhancement to the lip synchronization technique is the consideration of coarticulated effects where we examine how the action of a muscle is affected by temporal and spatial context. <p> The coordination of these various facial motions with the intonation is done completely automatically, by rule. Moreover, this method allows us to define various and individualized speaker characteristics by specifying particular sets of type and timing parameters for the facial actions [Calvert, 1990], [Ekman, 1979], <ref> [Moravetz, 1989] </ref>, [Unuma and Takeuchi, 1991].
Reference: [Nahas et al., 1988] <author> Nahas, M., Huitric, H., and Saintourens, M. </author> <year> (1988). </year> <title> Animation of b-spline figures. </title> <journal> The Visual Computer, </journal> <volume> 3 </volume> <pages> 272-276. </pages>
Reference-contexts: Automatic lip synchronization is included into animation systems by a multi-layer approach [Kalra et al., 1991] or by adding speech parameters <ref> [Nahas et al., 1988] </ref>, [Hill et al., 1988]. A correspondence between each speech unit and a basic lip shape is established. Of particular relevance is the model of coarticulation proposed by [Cohen and Massaro, 1993]. It uses overlapping dominance functions. <p> The computed movement information is interpreted as muscle contractions and is given as input to the animation system. Texture mapping is used to enhance the realism of features and skin tone of the model <ref> [Nahas et al., 1988] </ref>, [Williams, 1990], [Kurihara and Arai, 1991]. The consideration of wrinkles and of aging effects [Viaud and Yahia, 1992] adds much to the rendering of facial skin and expressions. 1.1 The Approach Our main goal is to look at coordinated motion during a conversation.
Reference: [Parke, 1982] <author> Parke, F. </author> <year> (1982). </year> <title> Parametrized models for facial animation. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 2(9) </volume> <pages> 61-68. </pages>
Reference-contexts: Earlier systems for facial animation <ref> [Parke, 1982] </ref>, [Magnenat-Thalmann and Thalmann, 1987] were based on a set 4 of parameters which affect not only the structure of the model (long nose, short forehead) but also its expressions (opening of the mouth, raising eyebrows). <p> Such techniques consider a small number of stereotyped speech postures to produce animation, including computer graphics [Emmett, 1985], [Kleiser-Walczak, 1988]. Even though they produce realistic animations, this technique requires a skilled animator and a considerable investment in time and manual effort. Other systems <ref> [Parke, 1982] </ref>, [Lewis and Parke, 1987], [Magnenat-Thalmann and Thalmann, 1987], [Hill et al., 1988] offer a higher level of parameterization to their model. Parameters are grouped to represent the mouth shape of each phoneme.
Reference: [Pelachaud, 1991] <author> Pelachaud, C. </author> <year> (1991). </year> <title> Communication and Coarticulation in Facial Animation. </title> <type> PhD thesis, </type> <institution> Computer and Information Science Department, University of Pennsylvania, Philadelphia, Pennsylvania. </institution>
Reference-contexts: That is to say that we sidestep the entire issue of recognition, leaving the integration of speech input for future work. Automatically finding the bracketing and intonational structure of a sentence is far from being a simple problem [Silverman, 1987]. The original work reported in <ref> [Pelachaud, 1991] </ref> used recorded natural speech to guide the animation. In this phase, after recording a sentence, the timing of each phoneme and pause was extracted from a spectrogram. <p> Moreover, articulatory adjustments continue on the pause existing just after the considered word or is foreseen on the pause just before the word [Cathiard et al., 1991]. These influences are computed by simulating this muscular contraction and relaxation properties by two third-degree polynomial curves <ref> [Pelachaud, 1991] </ref>. Finally, we take into account the geometric relationship between successive actions. Lip closure is more easily performed from a slightly parted position than from a puckered position. The intensity of an action is rescaled depending on its surrounding context and on the cluster it belongs to (Table 3).
Reference: [Pelachaud, 1993] <author> Pelachaud, C. </author> <year> (1993). </year> <title> Consideration of facial and audio channels for a facial animation system. In SCAN '93, </title> <address> Philadelphia. </address>
Reference-contexts: Amplify, de-amplify and neutralize affect the intensity of the facial changes. The blend is done by summing the effects together. Masking an expression A by another one B affects the timing of the parameters of B while some features of A remain <ref> [Pelachaud, 1993] </ref>. As we have seen, gaze behavior plays a great role in communication settings but some cultural differences are found in the amount of gaze allowed during a social encounter [Harper et al., 1978]. <p> This is done by creating a library of possible emblems; Efron gives a large list of them [Efron, 1972] and Ekman proposes a set of words which have a corresponding emblem. Nevertheless the user can build his/her own emblems and add them to the library <ref> [Pelachaud, 1993] </ref>. When lying, the timing of an expression changes. An expression may appear too early or too late, too fast or too slow. Thus by having access to the value of the onset, apex and offset of an action, one can modify them in order to simulate lies.
Reference: [Pelachaud et al., 1991] <author> Pelachaud, C., Badler, N.I., and Steedman, M. </author> <year> (1991). </year> <title> Linguistic issues in facial animation. </title> <editor> In Magnenat-Thalmann, N. and Thalmann, D., editors, </editor> <booktitle> Computer Animation '91, </booktitle> <pages> pages 15-30. </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: We have elaborated a repertory of such movements <ref> [Pelachaud et al., 1991] </ref>. We have included eye gestures as well as head gestures as part of the animation of the utterance.
Reference: [Pelachaud et al., 1994] <author> Pelachaud, C., Badler, N.I., and Viaud, M. </author> <year> (1994). </year> <title> Final report to NSF of the standards for facial animation workshop. </title> <type> Technical report, </type> <institution> NSF, University of Pennsylvania. </institution>
Reference-contexts: FACS describes temporary changes in facial appearance, how a feature is affected by specifying its new location, and the intensity of changes. An action unit AU corresponds to an action produced 1 There has been some discussion lately (e.g., at a recent NSF facial workshop <ref> [Pelachaud et al., 1994] </ref>) that it may not suffice for fine description of the mouth region. 5 by one or a group of related muscles. <p> Considering tongue movement when lip shapes are computed helps to make unambiguous obscure movements of some phonemic segments (since some speech postures are only differentiated by their tongue motions [Jeffers and Barley, 1971], [Kent and Minifie, 1977]) <ref> [Pelachaud et al., 1994] </ref>. 7 Conclusion We have proposed a method of characterizing any facial movements by separating them into phonemic, intonational, informational and affectual determinants. We believe that no previous computational model has taken into account all of these factors.
Reference: [Pelachaud et al., 1994] <author> Pelachaud, C., van Overveld, C., and Seah, C. </author> <year> (1994). </year> <title> Modeling and animating the human tongue during speech production. </title> <editor> In Magnenat-Thalmann, N. and Thalmann, D., editors, </editor> <title> Computer Animation '94. </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: FACS describes temporary changes in facial appearance, how a feature is affected by specifying its new location, and the intensity of changes. An action unit AU corresponds to an action produced 1 There has been some discussion lately (e.g., at a recent NSF facial workshop <ref> [Pelachaud et al., 1994] </ref>) that it may not suffice for fine description of the mouth region. 5 by one or a group of related muscles. <p> Considering tongue movement when lip shapes are computed helps to make unambiguous obscure movements of some phonemic segments (since some speech postures are only differentiated by their tongue motions [Jeffers and Barley, 1971], [Kent and Minifie, 1977]) <ref> [Pelachaud et al., 1994] </ref>. 7 Conclusion We have proposed a method of characterizing any facial movements by separating them into phonemic, intonational, informational and affectual determinants. We believe that no previous computational model has taken into account all of these factors.
Reference: [Pelachaud et al., 1993] <author> Pelachaud, C., Viaud, M., and Yahia, H. </author> <year> (1993). </year> <title> Rule-structured facial animation system. </title> <booktitle> In IJCAI' 93. </booktitle> <pages> 48 </pages>
Reference-contexts: There is a lack of empirical information on when an accent or other intonational components are accompanied by a facial action. The user can specify type and relative time of occurrence of a facial action as well as apex, onset and offset values <ref> [Pelachaud et al., 1993] </ref>. Given a context, an affect is associated with a particular facial expression.
Reference: [Pierrehumbert, 1980] <author> Pierrehumbert, J. </author> <year> (1980). </year> <title> The Phonology and Phonetics of English Intonation. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology. Distributed by Indiana University Linguistics Club, Bloomington, </institution> <note> IN. </note>
Reference-contexts: Pierrehumbert <ref> [Pierrehumbert, 1980] </ref>. We follow [Pierrehumbert and Hirschberg, 1990] [Prevost and Steedman, to appear] in assuming that different intonational tunes are used to convey various discourse-related distinctions of focus, contrast and propositional attitude. We use the categories defined in [Prevost and Steedman, to appear].
Reference: [Pierrehumbert and Hirschberg, 1990] <author> Pierrehumbert, J. and Hirschberg, J. </author> <year> (1990). </year> <title> The meaning of intonational contours in the interpretation of discourse. </title> <editor> In Cohen, P., Morgan, J., and Pollack, M., editors, </editor> <booktitle> Intentions in Communication, </booktitle> <pages> pages 271-312. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: While formal theories of the discourse semantics of such signals are generally lacking, in the area of spoken intonation, at least, there are more-or-less formalisable accounts of the way spoken intonation conveys discourse information associated with a speaker's messages [Halliday, 1967], [Bolinger, 1986], <ref> [Pierrehumbert and Hirschberg, 1990] </ref>, [Steedman, 1991]. Similarly, some psychologists have claimed to find universal facial expressions linked to affects and attitudes [Ekman, 1979]. Animating the face by specifying every action manually is a very tedious task and often does not yield entirely appropriate facial expression. <p> The first is information about the syntax (and therefore the semantics) of an utterance [Selkirk, 1984], [Hirschberg and Pierrehumbert, 1986], <ref> [Pierrehumbert and Hirschberg, 1990] </ref>. <p> Pierrehumbert [Pierrehumbert, 1980]. We follow <ref> [Pierrehumbert and Hirschberg, 1990] </ref> [Prevost and Steedman, to appear] in assuming that different intonational tunes are used to convey various discourse-related distinctions of focus, contrast and propositional attitude. We use the categories defined in [Prevost and Steedman, to appear].
Reference: [Platt, 1985] <author> Platt, S. </author> <year> (1985). </year> <title> A Structural Model of the Human Face. </title> <type> PhD thesis, </type> <institution> Computer and Information Science Department, University of Pennsylvania, Philadelphia, Pennsylvania. </institution>
Reference-contexts: The separation between conformation parameters and expression parameters provides independence between facial features and the production of an expression. More accurate facial motions can be obtained by simulating muscle actions <ref> [Platt, 1985] </ref>, [Waters, 1987]. By integrating models of several layers of facial tissue with dynamic simulation of muscle movements, considerable realism in creating subtle facial actions may be achieved [Terzopoulos and Waters, 1991]. <p> The facial expressions may be applied to any other facial model which uses FACS to drive its animation. * The facial model we are presently using integrates the action of each muscle or group of muscles and propagates their movements though the skin <ref> [Platt, 1985] </ref>. It is programmed through FACS AUs. 1.2 Organization In Section 2 we present the background of our system. We characterize the various determinants of facial expressions, describe head and eye movements computation, and present the intonational system we are using. <p> Regulators are mainly characterized by head and eye motions. The final list of AUs for each phoneme and pause is then obtained and the computation of the corresponding facial expressions can be performed using Steve Platt's program <ref> [Platt, 1985] </ref>. <p> In our case, none are needed since already computed blinks occur at a sufficient rate. * The facial expressions of each item of the utterance are computing using Platt's program <ref> [Platt, 1985] </ref>. Script files are output. * The animation is done using the Jack software [Badler et al., 1993]. 19 4 Details for each Determinant So far, we have presented the main steps of the algorithm.
Reference: [Prevost and Steedman, 1994] <author> Prevost, S. and Steedman, M. </author> <year> (1994). </year> <title> Information based intonation synthesis. </title> <booktitle> In Proceedings of the ARPA Workshop on Human Language Technology, </booktitle> <address> Princeton. </address>
Reference-contexts: Consequently the facial conversational signals and punctuators associated to each utterance differ also. This system provides a case where intonation structure apparently departs from traditional surface structure. The speech generation component of our system is based on the Information Based Intonation Structure (IBIS) System of Prevost and Steedman <ref> [Prevost and Steedman, 1994] </ref>, [Prevost and Steedman, to appear], which exploits a novel flexible approach to syntax and semantics based on Categorial Grammar to produce apparently appropriate intonation contours for spoken responses to database queries. 2.4 Underlying Property An important property linking intonation and facial expression (in fact it extends to <p> LL&lt;0.04&gt; YY&lt;0.078&gt; AA&lt;0.03&gt; C: PP&lt;0.106&gt; RR&lt;0.026&gt; AH&lt;0.03&gt; FF&lt;0.098&gt; (phrasal-tone-L (boundary-tone-H% ER &lt;0.254&gt;)) ZZ&lt;0.088&gt; end-intonational-phrase begin-intonational-phrase C: PP&lt;0.12&gt; (pitch-accent-H* AO&lt;0.118&gt;) PP&lt;0.096&gt; KK&lt;0.092&gt; OXR&lt;0.182&gt; (phrasal-tone-L (boundary-tone-L% NN&lt;0.164&gt;)) end-intonational-phrase .&lt;0.2&gt; In the recent phase of the work, this representation is derived entirely by rules from the output of the generation program and synthesizer <ref> [Prevost and Steedman, 1994] </ref>, [Prevost and Steedman, to appear]. 3.2 Organization of the Rules The computation of facial expressions corresponding to each determinant (conversational signal, punctuator, regulator and manipulator) is entirely by rule 3 . Two parameters are used to define an action: its type and its time of occurrence. <p> While the examples here are determined by measurement from real speech, we have reported a somewhat narrower range of examples generated entirely by rule from machine-generated semantic representations via a speech synthesizer [Cassell et al., 1994], <ref> [Prevost and Steedman, 1994] </ref>, [Prevost and Steedman, to appear]. Our model can be expected to help further research of human communicative faculties via automatically synthesized animation. In particular, it offers to linguists and 37 cognitive scientists a tool to analyze, manipulate and integrate several different determinants of communication.
Reference: [Prevost and Steedman, to appear] <author> Prevost, S. and Steedman, M. </author> <title> ((to appear)). Specifying intonation from context for speech synthesis. Speech Communication. </title>
Reference-contexts: The first is information about the syntax (and therefore the semantics) of an utterance [Selkirk, 1984], [Hirschberg and Pierrehumbert, 1986], [Pierrehumbert and Hirschberg, 1990]. We claim, following [Halliday, 1967], [Isard and Pearson, 1988], <ref> [Prevost and Steedman, to appear] </ref> that this information includes markers of questioning, 11 stating, and other speech acts, and markers of discourse information including topic or theme, comment or rheme, focus or new information, and background or given information. <p> Pierrehumbert [Pierrehumbert, 1980]. We follow [Pierrehumbert and Hirschberg, 1990] <ref> [Prevost and Steedman, to appear] </ref> in assuming that different intonational tunes are used to convey various discourse-related distinctions of focus, contrast and propositional attitude. We use the categories defined in [Prevost and Steedman, to appear]. <p> Pierrehumbert [Pierrehumbert, 1980]. We follow [Pierrehumbert and Hirschberg, 1990] <ref> [Prevost and Steedman, to appear] </ref> in assuming that different intonational tunes are used to convey various discourse-related distinctions of focus, contrast and propositional attitude. We use the categories defined in [Prevost and Steedman, to appear]. Intonation contours serve to indicate the way that the current utterance relates to the context established by previous ones for example, they may mark continuation of the same topic or theme, or the introduction of a new one. <p> This system provides a case where intonation structure apparently departs from traditional surface structure. The speech generation component of our system is based on the Information Based Intonation Structure (IBIS) System of Prevost and Steedman [Prevost and Steedman, 1994], <ref> [Prevost and Steedman, to appear] </ref>, which exploits a novel flexible approach to syntax and semantics based on Categorial Grammar to produce apparently appropriate intonation contours for spoken responses to database queries. 2.4 Underlying Property An important property linking intonation and facial expression (in fact it extends to gesture and body movement <p> PP&lt;0.106&gt; RR&lt;0.026&gt; AH&lt;0.03&gt; FF&lt;0.098&gt; (phrasal-tone-L (boundary-tone-H% ER &lt;0.254&gt;)) ZZ&lt;0.088&gt; end-intonational-phrase begin-intonational-phrase C: PP&lt;0.12&gt; (pitch-accent-H* AO&lt;0.118&gt;) PP&lt;0.096&gt; KK&lt;0.092&gt; OXR&lt;0.182&gt; (phrasal-tone-L (boundary-tone-L% NN&lt;0.164&gt;)) end-intonational-phrase .&lt;0.2&gt; In the recent phase of the work, this representation is derived entirely by rules from the output of the generation program and synthesizer [Prevost and Steedman, 1994], <ref> [Prevost and Steedman, to appear] </ref>. 3.2 Organization of the Rules The computation of facial expressions corresponding to each determinant (conversational signal, punctuator, regulator and manipulator) is entirely by rule 3 . Two parameters are used to define an action: its type and its time of occurrence. <p> While the examples here are determined by measurement from real speech, we have reported a somewhat narrower range of examples generated entirely by rule from machine-generated semantic representations via a speech synthesizer [Cassell et al., 1994], [Prevost and Steedman, 1994], <ref> [Prevost and Steedman, to appear] </ref>. Our model can be expected to help further research of human communicative faculties via automatically synthesized animation. In particular, it offers to linguists and 37 cognitive scientists a tool to analyze, manipulate and integrate several different determinants of communication.
Reference: [Scherer et al., 1984] <author> Scherer, K., Ladd, D., and Silverman, K. </author> <year> (1984). </year> <title> Vocal cues to speaker affect: testing two models. </title> <journal> Journal of Acoustical Society of America, </journal> <volume> 76 </volume> <pages> 1346-1356. </pages>
Reference-contexts: The second kind of information affecting intonation and prosody is affect or affectual attitude: involuntary aspects of the speaker's speech <ref> [Scherer et al., 1984] </ref>. A third of information concerns the conversational attitudes of the speaker: what stand the speaker takes towards the listener (e.g., politeness or irony which may be directly signaled or conversationally implied).
Reference: [Selkirk, 1984] <author> Selkirk, E. </author> <year> (1984). </year> <title> Phonology and Syntax. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The first is information about the syntax (and therefore the semantics) of an utterance <ref> [Selkirk, 1984] </ref>, [Hirschberg and Pierrehumbert, 1986], [Pierrehumbert and Hirschberg, 1990]. <p> Conversational attitudes may also include the conscious manipulation of affectual markers such as calm or anger, but in our current research we are not considering such manipulations. 2.3.1 Vocal Parameters A close relation between the syntax and the semantics of sentences and suprasegmental features has been suggested <ref> [Selkirk, 1984] </ref>, [Steedman, 1991]. The latter reference claims that suprasegmental features are systematically related to discourse information units corresponding to topic or theme (that is, what the discourse segment is about) and comment or rheme (that is, what novel information the utterance supplies).
Reference: [Silverman, 1987] <author> Silverman, K. </author> <year> (1987). </year> <title> The structure and processing of fundamental frequency contours. </title> <type> PhD thesis, </type> <institution> University of Cambridge. </institution>
Reference-contexts: That is to say that we sidestep the entire issue of recognition, leaving the integration of speech input for future work. Automatically finding the bracketing and intonational structure of a sentence is far from being a simple problem <ref> [Silverman, 1987] </ref>. The original work reported in [Pelachaud, 1991] used recorded natural speech to guide the animation. In this phase, after recording a sentence, the timing of each phoneme and pause was extracted from a spectrogram.
Reference: [Steedman, 1991] <author> Steedman, M. </author> <year> (1991). </year> <title> Structure and intonation. </title> <booktitle> Language, </booktitle> <volume> 67 </volume> <pages> 260-296. </pages>
Reference-contexts: While formal theories of the discourse semantics of such signals are generally lacking, in the area of spoken intonation, at least, there are more-or-less formalisable accounts of the way spoken intonation conveys discourse information associated with a speaker's messages [Halliday, 1967], [Bolinger, 1986], [Pierrehumbert and Hirschberg, 1990], <ref> [Steedman, 1991] </ref>. Similarly, some psychologists have claimed to find universal facial expressions linked to affects and attitudes [Ekman, 1979]. Animating the face by specifying every action manually is a very tedious task and often does not yield entirely appropriate facial expression. <p> Conversational attitudes may also include the conscious manipulation of affectual markers such as calm or anger, but in our current research we are not considering such manipulations. 2.3.1 Vocal Parameters A close relation between the syntax and the semantics of sentences and suprasegmental features has been suggested [Selkirk, 1984], <ref> [Steedman, 1991] </ref>. The latter reference claims that suprasegmental features are systematically related to discourse information units corresponding to topic or theme (that is, what the discourse segment is about) and comment or rheme (that is, what novel information the utterance supplies). <p> This bracketing is (partially) reflected in intonation. Consider the sentence Julia prefers popcorn (the example is related to one discussed in <ref> [Steedman, 1991] </ref>). context : I know that Harry prefers POTATO chips, but what does JULIA prefer? bracketing: (JULIA PREFERS)(POPCORN). accent : L+H* LH% H* LL% The tune is also annotated more formally using Pierrehumbert's notation, in which (H and L denote high and low tones which combine in the various pitch
Reference: [Terzopoulos and Waters, 1991] <author> Terzopoulos, D. and Waters, K. </author> <year> (1991). </year> <title> Techniques for realistic facial modelling and animation. </title> <editor> In Magnenat-Thalmann, N. and Thalmann, D., editors, </editor> <booktitle> Computer Animation '91, </booktitle> <pages> pages 45-58. </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: More accurate facial motions can be obtained by simulating muscle actions [Platt, 1985], [Waters, 1987]. By integrating models of several layers of facial tissue with dynamic simulation of muscle movements, considerable realism in creating subtle facial actions may be achieved <ref> [Terzopoulos and Waters, 1991] </ref>. Automatic lip synchronization is included into animation systems by a multi-layer approach [Kalra et al., 1991] or by adding speech parameters [Nahas et al., 1988], [Hill et al., 1988]. A correspondence between each speech unit and a basic lip shape is established. <p> It uses overlapping dominance functions. These functions specify for each viseme how close the lips reach their target value. Greater realism at the expense of synthetic control comes from techniques which extract information from live-animation [Williams, 1990], <ref> [Terzopoulos and Waters, 1991] </ref>, [Essa and Pentland, 1994]. The computed movement information is interpreted as muscle contractions and is given as input to the animation system.
Reference: [Unuma and Takeuchi, 1991] <author> Unuma, M. and Takeuchi, R. </author> <year> (1991). </year> <title> Generation of human motion with emotion. </title> <editor> In Magnenat-Thalmann, N. and Thalmann, D., editors, </editor> <booktitle> Computer Animation '91, </booktitle> <pages> pages 45-58. </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: based on Categorial Grammar to produce apparently appropriate intonation contours for spoken responses to database queries. 2.4 Underlying Property An important property linking intonation and facial expression (in fact it extends to gesture and body movement in general) lies in the existence of synchrony between them [Condon and Osgton, 1971], <ref> [Unuma and Takeuchi, 1991] </ref>, [Magnenat-Thalmann and Thalmann, 1987]. The face and the body do not move at random but in concert with the flow of speech. Thus, changes of body posture and orientation occurs at the beginning of a new topic of conversation. <p> The coordination of these various facial motions with the intonation is done completely automatically, by rule. Moreover, this method allows us to define various and individualized speaker characteristics by specifying particular sets of type and timing parameters for the facial actions [Calvert, 1990], [Ekman, 1979], [Moravetz, 1989], <ref> [Unuma and Takeuchi, 1991] </ref>. While the examples here are determined by measurement from real speech, we have reported a somewhat narrower range of examples generated entirely by rule from machine-generated semantic representations via a speech synthesizer [Cassell et al., 1994], [Prevost and Steedman, 1994], [Prevost and Steedman, to appear].
Reference: [Viaud and Yahia, 1992] <author> Viaud, M. and Yahia, H. </author> <year> (1992). </year> <title> Facial animation with wrinkles. </title> <booktitle> In 3nd Workshop of animation, Eurographics' 92, </booktitle> <address> Cambridge. </address>
Reference-contexts: Texture mapping is used to enhance the realism of features and skin tone of the model [Nahas et al., 1988], [Williams, 1990], [Kurihara and Arai, 1991]. The consideration of wrinkles and of aging effects <ref> [Viaud and Yahia, 1992] </ref> adds much to the rendering of facial skin and expressions. 1.1 The Approach Our main goal is to look at coordinated motion during a conversation.
Reference: [Waters, 1987] <author> Waters, K. </author> <year> (1987). </year> <title> A muscle model for animating three-dimensional facial expression. </title> <journal> Computer Graphics, </journal> <volume> 21(4) </volume> <pages> 17-24. </pages>
Reference-contexts: The separation between conformation parameters and expression parameters provides independence between facial features and the production of an expression. More accurate facial motions can be obtained by simulating muscle actions [Platt, 1985], <ref> [Waters, 1987] </ref>. By integrating models of several layers of facial tissue with dynamic simulation of muscle movements, considerable realism in creating subtle facial actions may be achieved [Terzopoulos and Waters, 1991]. <p> A simple solution to the problem of coarticulation is to look at the previous, the current and the next segments to determine the mouth positions <ref> [Waters, 1987] </ref>. But in some cases this is not enough since the correct position can depend on up to five segments before or after the current one [Kent and Minifie, 1977].
Reference: [Williams, 1990] <author> Williams, L. </author> <year> (1990). </year> <title> Performance-driven facial animation. </title> <journal> Computer Graphics, </journal> <volume> 24(4) </volume> <pages> 235-242. 49 </pages>
Reference-contexts: Of particular relevance is the model of coarticulation proposed by [Cohen and Massaro, 1993]. It uses overlapping dominance functions. These functions specify for each viseme how close the lips reach their target value. Greater realism at the expense of synthetic control comes from techniques which extract information from live-animation <ref> [Williams, 1990] </ref>, [Terzopoulos and Waters, 1991], [Essa and Pentland, 1994]. The computed movement information is interpreted as muscle contractions and is given as input to the animation system. Texture mapping is used to enhance the realism of features and skin tone of the model [Nahas et al., 1988], [Williams, 1990], [Kurihara <p> from live-animation <ref> [Williams, 1990] </ref>, [Terzopoulos and Waters, 1991], [Essa and Pentland, 1994]. The computed movement information is interpreted as muscle contractions and is given as input to the animation system. Texture mapping is used to enhance the realism of features and skin tone of the model [Nahas et al., 1988], [Williams, 1990], [Kurihara and Arai, 1991]. The consideration of wrinkles and of aging effects [Viaud and Yahia, 1992] adds much to the rendering of facial skin and expressions. 1.1 The Approach Our main goal is to look at coordinated motion during a conversation.
References-found: 59

