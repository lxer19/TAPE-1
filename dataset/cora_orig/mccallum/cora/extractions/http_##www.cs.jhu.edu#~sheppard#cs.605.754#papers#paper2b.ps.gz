URL: http://www.cs.jhu.edu/~sheppard/cs.605.754/papers/paper2b.ps.gz
Refering-URL: http://www.cs.jhu.edu/~sheppard/cs.605.754/sched.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: tgd@cs.orst.edu  
Title: Statistical Tests for Comparing Supervised Classification Learning Algorithms  
Author: Thomas G. Dietterich 
Date: October 16, 1996  
Address: Corvallis, OR 97331  
Affiliation: Department of Computer Science Oregon State University  
Abstract: This paper reviews five statistical tests for determining whether one learning algorithm outperforms another on a particular learning task. These tests are compared experimentally to determine their probability of incorrectly detecting a difference when no difference exists (type 1 error). Two widely-used statistical tests are shown to have high probability of Type I error in certain situations and should never be used. These tests are (a) a test for the difference of two proportions and (b) a paired-differences t test based on taking several random train/test splits. A third test, a paired-differences t test based on 10-fold cross-validation, exhibits somewhat elevated probability of Type I error. A fourth test, McNemar's test, is shown to have low Type I error. The fifth test is a new test, 5x2cv, based on 5 iterations of 2-fold cross-validation. Experiments show that this test also has good Type I error. The paper also measures the power (ability to detect algorithm differences when they do exist) of these tests. The 5x2cv test is shown to be slightly more powerful than McNemar's test. The choice of the best test is determined by the computational cost of running the learning algorithm. For algorithms that can be executed only once, McNemar's test is the only test with acceptable Type I error. For algorithms that can be executed ten times, the 5x2cv test is recommended, because it is slightly more powerful and because it directly measures variation due to the choice of training set. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and regression trees. </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: Breiman (1994, 1996) has called this behavior "instability", and he has shown that this is a serious problem for the decision tree algorithms, such as CART <ref> (Breiman, Friedman, Olshen, & Stone, 1984) </ref>. A third source of variance can be internal randomness in the learning algorithm. Consider, for example, the widely-used backpropagation algorithm for training feed-forward neural networks. This algorithm is usually initialized with a set of random weights which it then improves.
Reference: <author> Breiman, L. </author> <year> (1994). </year> <title> Heuristics of instability and stabilization in model selection. </title> <type> Tech. rep. 416, </type> <institution> Department of Statistics, University of California, Berkeley, </institution> <address> CA. </address>
Reference: <author> Breiman, L. </author> <year> (1996). </year> <title> Bagging predictors. </title> <journal> Machine Learning, </journal> <volume> 24 (2), </volume> <pages> 123-140. </pages>
Reference: <author> Dasarathy, B. V. (Ed.). </author> <year> (1991). </year> <title> Nearest neighbor (NN) norms: NN pattern classification techniques. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA. </address>
Reference-contexts: We also needed the learning algorithms to be very efficient, so that the experiments could be replicated many times. To achieve this, we chose C4.5 Release 1 (Quinlan, 1993) and the first nearest-neighbor algorithm <ref> (Dasarathy, 1991) </ref>. We then selected three difficult problems: the EXP6 data generator developed by Kong (1995), the Letter Recognition data set (Frey & Slate, 1991), and the Pima Indians Diabetes Task (Murphy & Aha, 1994). Of course, C4.5 and NN do not have the same performance on these data sets.
Reference: <author> Efron, B., & Tibshirani, R. J. </author> <year> (1993). </year> <title> An introduction to the bootstrap. </title> <publisher> Chapman and Hall, </publisher> <address> New York, NY. </address> <note> 19 Everitt, </note> <author> B. S. </author> <year> (1977). </year> <title> The analysis of contingency tables. </title> <publisher> Chapman and Hall, London. </publisher>
Reference: <author> Frey, P. W., & Slate, D. J. </author> <year> (1991). </year> <title> Letter recognition using Holland-style adaptive classifiers. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 161-182. </pages>
Reference-contexts: To achieve this, we chose C4.5 Release 1 (Quinlan, 1993) and the first nearest-neighbor algorithm (Dasarathy, 1991). We then selected three difficult problems: the EXP6 data generator developed by Kong (1995), the Letter Recognition data set <ref> (Frey & Slate, 1991) </ref>, and the Pima Indians Diabetes Task (Murphy & Aha, 1994). Of course, C4.5 and NN do not have the same performance on these data sets. In EXP6 and Letter Recognition, NN performs much better than C4.5; the reverse is true in the Pima data set.
Reference: <author> Hinton, G. E., Neal, R. M., Tibshirani, R., Revow, M., Rasmussen, C. E., van Camp, D., Kustra, R. & Ghahramani, Z. </author> <year> (1995). </year> <title> Assessing learning procedures using DELVE. </title> <type> Tech. rep., </type> <institution> University of Toronto, Department of Computer Science, </institution> <note> http://www.cs.utoronto.ca/neuron/delve/delve.html. </note>
Reference-contexts: One approach, advocated by the DELVE project <ref> (Hinton et al., 1995) </ref>, is to subdivide S into a test set and several disjoint training sets of size m. Then A is trained on each of the training sets and the resulting classifiers are tested on the test set.
Reference: <author> Kohavi, R. </author> <year> (1995). </year> <title> Wrappers for performance enhancement and oblivious decision graphs. </title> <type> Ph.D. thesis, </type> <institution> Stanford University. </institution>
Reference: <author> Kolen, J. F., & Pollack, J. B. </author> <year> (1991). </year> <title> Back propagation is sensitive to initial conditions. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 3, </volume> <pages> pp. </pages> <address> 860-867 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Consider, for example, the widely-used backpropagation algorithm for training feed-forward neural networks. This algorithm is usually initialized with a set of random weights which it then improves. The resulting learned network depends critically on the random starting state <ref> (Kolen & Pollack, 1991) </ref>. In this case, even if the training data are not changed, the algorithm is likely to produce a different hypothesis if it is executed again from a different random starting state.
Reference: <author> Kong, E. B., & Dietterich, T. G. </author> <year> (1995). </year> <title> Error-correcting output coding corrects bias and variance. </title> <booktitle> In The XII International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 313-321 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Murphy, P., & Aha, D. </author> <year> (1994). </year> <title> UCI repository of machine learning databases [machine-readable data repository]. </title> <type> Tech. rep., </type> <institution> University of California, Irvine. </institution>
Reference-contexts: To achieve this, we chose C4.5 Release 1 (Quinlan, 1993) and the first nearest-neighbor algorithm (Dasarathy, 1991). We then selected three difficult problems: the EXP6 data generator developed by Kong (1995), the Letter Recognition data set (Frey & Slate, 1991), and the Pima Indians Diabetes Task <ref> (Murphy & Aha, 1994) </ref>. Of course, C4.5 and NN do not have the same performance on these data sets. In EXP6 and Letter Recognition, NN performs much better than C4.5; the reverse is true in the Pima data set.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Empirical Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: We also needed the learning algorithms to be very efficient, so that the experiments could be replicated many times. To achieve this, we chose C4.5 Release 1 <ref> (Quinlan, 1993) </ref> and the first nearest-neighbor algorithm (Dasarathy, 1991). We then selected three difficult problems: the EXP6 data generator developed by Kong (1995), the Letter Recognition data set (Frey & Slate, 1991), and the Pima Indians Diabetes Task (Murphy & Aha, 1994).
Reference: <author> Snedecor, G. W., & Cochran, W. G. </author> <year> (1989). </year> <title> Statistical Methods. </title> <institution> Iowa State University Press, Ames, IA. </institution> <note> Eighth Edition. 20 </note>
Reference-contexts: that the two algorithms have different performance when trained on training sets of size jRj. 6 3.2 A test for the difference of two proportions A second simple statistical test is based on measuring the difference between the error rate of algorithm A and the error rate of algorithm B <ref> (Snedecor & Cochran, 1989) </ref>. Specifically, let p A = (n 00 + n 01 )=n be the proportion of test examples incorrectly classified by algorithm A and let p B = (n 00 + n 10 )=n be the proportion of test examples incorrectly classified by algorithm B.
References-found: 13

