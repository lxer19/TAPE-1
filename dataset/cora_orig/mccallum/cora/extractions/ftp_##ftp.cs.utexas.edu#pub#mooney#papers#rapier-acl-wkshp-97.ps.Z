URL: ftp://ftp.cs.utexas.edu/pub/mooney/papers/rapier-acl-wkshp-97.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ml/abstracts.html
Root-URL: 
Email: fmecaliff,mooneyg@cs.utexas.edu  
Title: Relational Learning of Pattern-Match Rules for Information Extraction  
Author: Mary Elaine Califf and Raymond J. Mooney 
Address: Austin, TX 78712  
Affiliation: Department of Computer Sciences University of Texas at Austin  
Note: To appear in Working Papers of the ACL-97 Workshop in Natural Language Learning  
Abstract: Information extraction systems process natural language documents and locate a specific set of relevant items. Given the recent success of empirical or corpus-based approaches in other areas of natural language processing, machine learning has the potential to significantly aid the development of these knowledge-intensive systems. This paper presents a system, Rapier, that takes pairs of documents and filled templates and induces pattern-match rules that directly extract fillers for the slots in the template. The learning algorithm incorporates techniques from several inductive logic programming systems and learns unbounded patterns that include constraints on the words and part-of-speech tags surrounding the filler. Encouraging results are presented on learning to extract information from computer job postings from the newsgroup misc.jobs.offered.
Abstract-found: 1
Intro-found: 1
Reference: <editor> ARPA, editor. </editor> <booktitle> 1992. Proceedings of the Fourth DARPA Message Understanding Evaluation and Conference, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufman. </publisher> <editor> ARPA, editor. </editor> <booktitle> 1993. Proceedings of the Fifth DARPA Message Understanding Evaluation and Conference, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufman. </publisher>
Reference: <editor> Birnbaum, L. A. and G. C. Collins, editors. </editor> <booktitle> 1991. Proceedings of the Eighth International Workshop on Machine Learning: Part VI Learning Relations, </booktitle> <address> Evanston, IL, </address> <month> June. </month>
Reference-contexts: Inductive logic programming and other relational learning methods <ref> (Birnbaum and Collins, 1991) </ref> allow induction over structured examples that can include first-order logical predicates and functions and unbounded data structures such as lists, strings, and trees.
Reference: <author> Brill, Eric. </author> <year> 1994. </year> <title> Some advances in rule-based part of speech tagging. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 722-727. </pages>
Reference-contexts: This rule assumes that the document has been tagged with the POS tagger of <ref> (Brill, 1994) </ref>. 3.2 The Learning Algorithm As noted above, Rapier is inspired by ILP methods, and primarily consists of a specific to general (bottom-up) search. First, for each slot, most-specific patterns are created for each example, specifying word and tag for the filler and its complete context.
Reference: <author> Cohen, W. W. </author> <year> 1995. </year> <title> Text categorization and relational learning. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 124-132, </pages> <address> San Francisco, CA. </address> <publisher> Morgan Kauf-man. </publisher>
Reference-contexts: Detailed experimental comparisons of ILP and feature-based induction have demonstrated the advantages of relational representations in two language related tasks, text categorization <ref> (Cohen, 1995) </ref> and generating the past tense of an English verb (Mooney and Califf, 1995). While Rapier is not strictly an ILP system, its relational learning algorithm was inspired by ideas from the following ILP systems.
Reference: <author> Doorenbos, R. B., O. Etzioni, and D. S. Weld. </author> <year> 1997. </year> <title> A scalable comparison-shopping agent for the world-wide web. </title> <booktitle> In Proceedings of the First International Conference on Autonomous Agents. </booktitle>
Reference-contexts: Numerous other Internet applications are possible, such as extracting information from product web pages for a shopping agent <ref> (Doorenbos, Etzioni, and Weld, 1997) </ref>. 2.2 Relational Learning Most empirical natural-language research has employed statistical techniques that base decisions on very limited contexts, or symbolic techniques such as decision trees that require the developer to specify a manageable, finite set of features for use in making decisions.
Reference: <author> Huffman, S. B. </author> <year> 1996. </year> <title> Learning information extraction patterns from examples. </title> <editor> In S. Wermter, E. Riloff, and G. Scheler, editors, </editor> <title> Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Language Processing. </title> <publisher> Springer, Berlin, </publisher> <pages> pages 246-260. </pages>
Reference-contexts: IE can be useful in a variety of domains. The various MUC's have focused on domains such as Latin American terrorism, joint ventures, microelectronics, and company management changes. Others have used IE to track medical patient records (Soderland et al., 1995) or company mergers <ref> (Huffman, 1996) </ref>. A general task considered in this paper is extracting information from postings to USENET newsgroups, such as job announcements. <p> AutoSlog, Crystal, and Palka all rely on prior sentence analysis to identify syntactic elements and their relationships, and their output requires further processing to produce the final filled templates. Liep also learns IE patterns <ref> (Huffman, 1996) </ref>.
Reference: <author> Kim, Jun-Tae and Dan I. Moldovan. </author> <year> 1995. </year> <title> Acquisition of linguistic patterns for knowledge-based information extraction. </title> <journal> IEEE Transactions on Knowledge and DataEngineering, </journal> <volume> 7(5) </volume> <pages> 713-724, </pages> <month> October. </month>
Reference-contexts: AutoSlog creates a dictionary of extraction patterns by specializing a set of general syntactic patterns (Riloff, 1993; Riloff, 1996). It assumes that an expert will later examine the patterns it produces. Palka learns extraction patterns relying on a concept hierarchy to guide generalization and specialization <ref> (Kim and Moldovan, 1995) </ref>. AutoSlog, Crystal, and Palka all rely on prior sentence analysis to identify syntactic elements and their relationships, and their output requires further processing to produce the final filled templates. Liep also learns IE patterns (Huffman, 1996).
Reference: <author> Lehnert, Wendy and Beth Sundheim. </author> <year> 1991. </year> <title> A performance evaluation of text-analysis technologies. </title> <journal> AI Magazine, </journal> <volume> 12(3) </volume> <pages> 81-94. </pages>
Reference-contexts: The need to intelligently process such texts makes information extraction (IE), the task of locating specific pieces of data from a natural language document, a particularly useful sub-area of natural language processing (NLP). In recognition of their significance, IE systems have been the focus of DARPA's MUC program <ref> (Lehnert and Sundheim, 1991) </ref>. Unfortunately, IE systems are difficult and time-consuming to build and the resulting systems generally contain highly domain-specific components, making them difficult to port to new domains.
Reference: <author> McCarthy, J. and W. Lehnert. </author> <year> 1995. </year> <title> Using decision trees for coreference resolution. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1050-1055. </pages>
Reference-contexts: Resolve uses decision trees to handle coreference decisions for an IE system and requires annotated coref-erence examples <ref> (McCarthy and Lehnert, 1995) </ref>. Crystal uses a form of clustering to create a dictionary of extraction patterns by generalizing patterns identified in the text by an expert (Soderland et al., 1995; Soderland et al., 1996).
Reference: <author> Miller, G., R. Beckwith, C. Fellbaum, D. Gross, and K. Miller. </author> <year> 1993. </year> <title> Introduction to WordNet: An on-line lexical database. </title> <note> Available by ftp to clarity.princeton.edu. </note>
Reference-contexts: Using only a corpus of documents paired with filled templates, Rapier learns unbounded Eliza-like patterns (Weizenbaum, 1966) that utilize limited syntactic information, such as the output of a part-of-speech tagger. Induced patterns can also easily incorporate semantic class information, such as that provided by WordNet <ref> (Miller et al., 1993) </ref>. The learning algorithm was inspired by several Inductive Logic Programming (ILP) systems and primarily consists of a specific-to-general (bottom-up) search for patterns that characterize slot-fillers and their surrounding context. The remainder of the paper is organized as follows. <p> may allow the search to explode. 3 Rapier System 3.1 Rule Representation Rapier's rule representation uses patterns that make use of limited syntactic and semantic information, using freely available, robust knowledge sources such as a part-of-speech tagger and a lexicon with semantic classes, such as the hypernym links in Word-Net <ref> (Miller et al., 1993) </ref>. The initial implementation does not use a parser, primarily because of the difficulty of producing a robust parser for unrestricted text and because simpler patterns of the type we propose can represent useful extraction rules for at least some domains.
Reference: <author> Mooney, R. J. and M. E. Califf. </author> <year> 1995. </year> <title> Induction of first-order decision lists: Results on learning the past tense of English verbs. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 3 </volume> <pages> 1-24. </pages>
Reference-contexts: Detailed experimental comparisons of ILP and feature-based induction have demonstrated the advantages of relational representations in two language related tasks, text categorization (Cohen, 1995) and generating the past tense of an English verb <ref> (Mooney and Califf, 1995) </ref>. While Rapier is not strictly an ILP system, its relational learning algorithm was inspired by ideas from the following ILP systems. Golem (Muggleton and Feng, 1992) is a bottom-up (specific to general) ILP algorithm based on the construction of relative least-general generalizations, rlggs (Plotkin, 1970).
Reference: <author> Muggleton, S. and C. Feng. </author> <year> 1992. </year> <title> Efficient induction of logic programs. </title> <editor> In S. Muggleton, editor, </editor> <booktitle> Inductive Logic Programming. </booktitle> <publisher> Academic Press, </publisher> <address> New York, </address> <pages> pages 281-297. </pages>
Reference-contexts: While Rapier is not strictly an ILP system, its relational learning algorithm was inspired by ideas from the following ILP systems. Golem <ref> (Muggleton and Feng, 1992) </ref> is a bottom-up (specific to general) ILP algorithm based on the construction of relative least-general generalizations, rlggs (Plotkin, 1970). The idea of least-general generalizations (LGGs) is, given two items (in ILP, two clauses), finding the least general item that covers the original pair.
Reference: <author> Muggleton, Steve. </author> <year> 1995. </year> <title> Inverse entailment and Progol. </title> <journal> New Generation Computing Journal, </journal> <volume> 13 </volume> <pages> 245-286. </pages>
Reference-contexts: Chillin uses the notion of empirical subsumption, which means that as new, more general clauses are added, all of the clauses which are not needed to prove positive examples are removed from the definition. Progol <ref> (Muggleton, 1995) </ref> also combines bottom-up and top-down search. Using mode declarations provided for both the background predicates and the predicate being learned, it constructs a most specific clause for a random seed example.
Reference: <author> Plotkin, G. D. </author> <year> 1970. </year> <title> A note on inductive generalization. </title> <editor> In B. Meltzer and D. Michie, editors, </editor> <booktitle> Machine Intelligence (Vol. </booktitle> <volume> 5). </volume> <publisher> Elsevier North-Holland, </publisher> <address> New York. </address>
Reference-contexts: While Rapier is not strictly an ILP system, its relational learning algorithm was inspired by ideas from the following ILP systems. Golem (Muggleton and Feng, 1992) is a bottom-up (specific to general) ILP algorithm based on the construction of relative least-general generalizations, rlggs <ref> (Plotkin, 1970) </ref>. The idea of least-general generalizations (LGGs) is, given two items (in ILP, two clauses), finding the least general item that covers the original pair. This is usually a fairly simple computation. Rlggs are the LGGs relative to a set of background relations.
Reference: <author> Quinlan, J.R. </author> <year> 1990. </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5(3) </volume> <pages> 239-266. </pages>
Reference-contexts: We maintain a list of the best n rules created and specialize the rules under consid eration by adding pieces of the generalizations of the pre- and post-filler patterns of the two seed rules, working outward from the fillers. The rules are or dered using an information value metric <ref> (Quinlan, 1990) </ref> weighted by the size of the rule (preferring smaller rules). When the best rule under consider ation produces no negative examples, specialization ceases; that rule is added to the rule base, and all rules empirically subsumed by it are removed.
Reference: <author> Riloff, E. </author> <year> 1993. </year> <title> Automatically constructing a dictionary for information extraction tasks. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 811-816. </pages>
Reference: <author> Riloff, Ellen. </author> <year> 1996. </year> <title> Automatically generating ex-traction patterns from untagged text. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1044-1049. </pages>
Reference-contexts: This performance is comparable to that of Crystal on a medical domain task (Soderland et al., 1996), and better than that of AutoSlog and AutoSlog-TS on part of the MUC4 terrorism task <ref> (Riloff, 1996) </ref>. It also compares favorably with the typical system performance on the MUC tasks (ARPA, 1992; ARPA, 1993). All of these comparisons are only general, since the tasks are different, but they do indicate that Rapier is doing relatively well.
Reference: <author> Soderland, Stephen, D. Fisher, J. Aseltine, and W. Lehnert. </author> <year> 1995. </year> <title> Crystal: Inducing a conceptual dictionary. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1314-1319. </pages>
Reference-contexts: IE can be useful in a variety of domains. The various MUC's have focused on domains such as Latin American terrorism, joint ventures, microelectronics, and company management changes. Others have used IE to track medical patient records <ref> (Soderland et al., 1995) </ref> or company mergers (Huffman, 1996). A general task considered in this paper is extracting information from postings to USENET newsgroups, such as job announcements.
Reference: <author> Soderland, Stephen, David Fisher, Jonathan Asel-tine, and Wendy Lehnert. </author> <year> 1996. </year> <title> Issues in inductive learning of domain-specific text extraction rules. </title> <editor> In Stefan Wermter, Ellen Riloff, and Gabriele Scheller, editors, </editor> <title> Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Language Processing, </title> <booktitle> Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer, </publisher> <pages> pages 290-301. </pages>
Reference-contexts: At 90 training examples, the average precision was 83.7% and the average recall was 53.1%. These numbers look quite promising when compared to the measured performance of other information extraction systems on various domains. This performance is comparable to that of Crystal on a medical domain task <ref> (Soderland et al., 1996) </ref>, and better than that of AutoSlog and AutoSlog-TS on part of the MUC4 terrorism task (Riloff, 1996). It also compares favorably with the typical system performance on the MUC tasks (ARPA, 1992; ARPA, 1993).
Reference: <author> Weizenbaum, J. </author> <year> 1966. </year> <title> ELIZA A computer program for the study of natural language communications between men and machines. </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> 9 </volume> <pages> 36-45. </pages>
Reference-contexts: Our system, Rapier (Robust Automated Production of Information Extraction Rules), learns rules for the complete IE task. The resulting rules extract the desired items directly from documents without prior parsing or subsequent processing. Using only a corpus of documents paired with filled templates, Rapier learns unbounded Eliza-like patterns <ref> (Weizenbaum, 1966) </ref> that utilize limited syntactic information, such as the output of a part-of-speech tagger. Induced patterns can also easily incorporate semantic class information, such as that provided by WordNet (Miller et al., 1993).
Reference: <author> Zelle, J. M. and R. J. Mooney. </author> <year> 1994. </year> <title> Combining top-down and bottom-up methods in inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 343-351, </pages> <address> New Brunswick, NJ, </address> <month> July. </month>
Reference-contexts: The resulting clause with the greatest coverage of positive examples is selected, and that clause is further generalized by computing the rlggs of the selected clause with new randomly chosen positive examples. The generalization process stops when the coverage of the best clause no longer increases. The Chillin <ref> (Zelle and Mooney, 1994) </ref> system combines top-down (general to specific) and bottom-up ILP techniques. The algorithm starts with a most specific definition (the set of positive examples) and introduces generalizations which make the definition more compact. Generalizations are created by selecting pairs of clauses in the definition and computing LGGs.
Reference: <author> Zelle, J. M. and R. J. Mooney. </author> <year> 1996. </year> <title> Learning to parse database queries using inductive logic programming. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <address> Port-land, OR, </address> <month> August. </month>
Reference-contexts: A general task considered in this paper is extracting information from postings to USENET newsgroups, such as job announcements. Our overall goal is to extract a database from all the messages in a newsgroup and then use learned query parsers <ref> (Zelle and Mooney, 1996) </ref> to answer natural language questions such as "What jobs are available in Austin for C++ programmers with only one year of experience?".
References-found: 22

