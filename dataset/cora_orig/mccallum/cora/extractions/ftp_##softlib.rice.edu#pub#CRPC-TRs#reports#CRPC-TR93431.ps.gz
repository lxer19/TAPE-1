URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR93431.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Operator Overloading as an Enabling Technology for Automatic Differentiation  
Author: George F. Corliss Andreas Griewank 
Date: May 1993.  
Address: Argonne, Ill.,  
Note: Preprint MCS-P358-0493,  
Affiliation: Marquette University and Argonne National Laboratory  Argonne National Laboratory  Mathematics and Computer Science Division, Argonne National Laboratory,  
Abstract: We present an example of the science that is enabled by object-oriented programming techniques. Scientific computation often needs derivatives for solving nonlinear systems such as those arising in many PDE algorithms, optimization, parameter identification, stiff ordinary differential equations, or sensitivity analysis. Automatic differentiation computes derivatives accurately and efficiently by applying the chain rule to each arithmetic operation or elementary function. Operator overloading enables the techniques of either the forward or the reverse mode of automatic differentiation to be applied to real-world scientific problems. We illustrate automatic differentiation with an example drawn from a model of unsaturated flow in a porous medium. The problem arises from planning for the long-term storage of radioactive waste.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. Baur and V. Strassen. </author> <title> The complexity of partial derivatives. </title> <journal> Theoretical Computer Science, </journal> <volume> 22 </volume> <pages> 317-330, </pages> <year> 1983. </year>
Reference-contexts: Here, the vector x contains the independent variables, and the vector y contains the dependent variables. The function described by this program is defined except at x [2] = 0 and is differentiable except at x <ref> [1] </ref> = 2. We can transform the program in Figure 1 into one for computing derivatives by associating a derivative object rt with every variable t. Assume that rt contains the derivatives of t with respect to the independent variables x, rt = @x [1] ; @t T . <p> and is differentiable except at x <ref> [1] </ref> = 2. We can transform the program in Figure 1 into one for computing derivatives by associating a derivative object rt with every variable t. Assume that rt contains the derivatives of t with respect to the independent variables x, rt = @x [1] ; @t T . We propagate these derivatives by using elementary differentiation arithmetic based on the chain rule [13, 23] for computing the derivatives of y [1] and y [2], as shown in Figure 2. <p> Assume that rt contains the derivatives of t with respect to the independent variables x, rt = @x <ref> [1] </ref> ; @t T . We propagate these derivatives by using elementary differentiation arithmetic based on the chain rule [13, 23] for computing the derivatives of y [1] and y [2], as shown in Figure 2. In this example, each assignment to a derivative is actually a vector assignment of length two. This example contains a branch and a loop to illustrate that the forward mode handles control structures. <p> This example contains a branch and a loop to illustrate that the forward mode handles control structures. The only difficulty is that functions with branches may fail to be differentiable. Work is in progress to warn the programmer when this has occurred. if x <ref> [1] </ref> &gt; 2 f g else f a = x [1] * x [2]; for (i = 1; i &lt;= 2; i ++) f g y [2] = sin (x [2]); if x [1] &gt; 2.0 f ra = rx [1] + rx [2]; g else f a = x [1] <p> The only difficulty is that functions with branches may fail to be differentiable. Work is in progress to warn the programmer when this has occurred. if x <ref> [1] </ref> &gt; 2 f g else f a = x [1] * x [2]; for (i = 1; i &lt;= 2; i ++) f g y [2] = sin (x [2]); if x [1] &gt; 2.0 f ra = rx [1] + rx [2]; g else f a = x [1] * x [2]; for (i = 1; i &lt;= 2; <p> Work is in progress to warn the programmer when this has occurred. if x <ref> [1] </ref> &gt; 2 f g else f a = x [1] * x [2]; for (i = 1; i &lt;= 2; i ++) f g y [2] = sin (x [2]); if x [1] &gt; 2.0 f ra = rx [1] + rx [2]; g else f a = x [1] * x [2]; for (i = 1; i &lt;= 2; i ++) f temp = a; a = a * x (i); ra = x (i) * ra + temp * rx (i); <p> to warn the programmer when this has occurred. if x <ref> [1] </ref> &gt; 2 f g else f a = x [1] * x [2]; for (i = 1; i &lt;= 2; i ++) f g y [2] = sin (x [2]); if x [1] &gt; 2.0 f ra = rx [1] + rx [2]; g else f a = x [1] * x [2]; for (i = 1; i &lt;= 2; i ++) f temp = a; a = a * x (i); ra = x (i) * ra + temp * rx (i); g y [1] = a / x <p> <ref> [1] </ref> &gt; 2 f g else f a = x [1] * x [2]; for (i = 1; i &lt;= 2; i ++) f g y [2] = sin (x [2]); if x [1] &gt; 2.0 f ra = rx [1] + rx [2]; g else f a = x [1] * x [2]; for (i = 1; i &lt;= 2; i ++) f temp = a; a = a * x (i); ra = x (i) * ra + temp * rx (i); g y [1] = a / x [2]; a/ (x [2] * x [2]) * rx [2]; <p> f ra = rx <ref> [1] </ref> + rx [2]; g else f a = x [1] * x [2]; for (i = 1; i &lt;= 2; i ++) f temp = a; a = a * x (i); ra = x (i) * ra + temp * rx (i); g y [1] = a / x [2]; a/ (x [2] * x [2]) * rx [2]; ry [2] = cos (x [2]) * rx [2]; This mode of automatic differentiation, where we maintain the derivatives with respect to the independent variables, is called the forward mode of automatic differentiation. <p> Instead, the complexity of the forward and the reverse modes can be bounded in terms of the chromatic numbers of the column- and row-incidence graphs, respectively. This issue is discussed in more detail in [17, 16]. Wolfe observed [25], and Baur and Strassen confirmed <ref> [1] </ref>, that if care is taken in handling quantities that are common to the (rational) function and its derivatives, then the cost of evaluating a gradient with n components is a small multiple of the cost of evaluating the underlying scalar function. <p> Formula (1) is applied to each entry from the code list. At the end of the computation, the desired derivatives are in xbar and ybar. 5 if x <ref> [1] </ref> &gt; 2 f g else f a = x [1] * x [2g a = a * x (i); y [1] = a / x [2]; Yields the code list: // Assuming x [1] 2 a = x [1] * x [2]; t2 = t1 * x [2]; y [2] <p> Formula (1) is applied to each entry from the code list. At the end of the computation, the desired derivatives are in xbar and ybar. 5 if x <ref> [1] </ref> &gt; 2 f g else f a = x [1] * x [2g a = a * x (i); y [1] = a / x [2]; Yields the code list: // Assuming x [1] 2 a = x [1] * x [2]; t2 = t1 * x [2]; y [2] = sin (x [2]); // Initialize adjoint objects: ybar [1] <p> Formula (1) is applied to each entry from the code list. At the end of the computation, the desired derivatives are in xbar and ybar. 5 if x <ref> [1] </ref> &gt; 2 f g else f a = x [1] * x [2g a = a * x (i); y [1] = a / x [2]; Yields the code list: // Assuming x [1] 2 a = x [1] * x [2]; t2 = t1 * x [2]; y [2] = sin (x [2]); // Initialize adjoint objects: ybar [1] = (1, 0); ybar [2] = (0, 1); xbar [1] = <p> the end of the computation, the desired derivatives are in xbar and ybar. 5 if x <ref> [1] </ref> &gt; 2 f g else f a = x [1] * x [2g a = a * x (i); y [1] = a / x [2]; Yields the code list: // Assuming x [1] 2 a = x [1] * x [2]; t2 = t1 * x [2]; y [2] = sin (x [2]); // Initialize adjoint objects: ybar [1] = (1, 0); ybar [2] = (0, 1); xbar [1] = xbar [2] = abar = t1bar = t2bar = (0, 0); xbar [2] <p> the desired derivatives are in xbar and ybar. 5 if x <ref> [1] </ref> &gt; 2 f g else f a = x [1] * x [2g a = a * x (i); y [1] = a / x [2]; Yields the code list: // Assuming x [1] 2 a = x [1] * x [2]; t2 = t1 * x [2]; y [2] = sin (x [2]); // Initialize adjoint objects: ybar [1] = (1, 0); ybar [2] = (0, 1); xbar [1] = xbar [2] = abar = t1bar = t2bar = (0, 0); xbar [2] += ybar [2] * cos <p> <ref> [1] </ref> * x [2g a = a * x (i); y [1] = a / x [2]; Yields the code list: // Assuming x [1] 2 a = x [1] * x [2]; t2 = t1 * x [2]; y [2] = sin (x [2]); // Initialize adjoint objects: ybar [1] = (1, 0); ybar [2] = (0, 1); xbar [1] = xbar [2] = abar = t1bar = t2bar = (0, 0); xbar [2] += ybar [2] * cos (x [2]); // y [1] = t2 / x [2]; t2bar += ybar [1] / x [2]; xbar [2] += ybar <p> y <ref> [1] </ref> = a / x [2]; Yields the code list: // Assuming x [1] 2 a = x [1] * x [2]; t2 = t1 * x [2]; y [2] = sin (x [2]); // Initialize adjoint objects: ybar [1] = (1, 0); ybar [2] = (0, 1); xbar [1] = xbar [2] = abar = t1bar = t2bar = (0, 0); xbar [2] += ybar [2] * cos (x [2]); // y [1] = t2 / x [2]; t2bar += ybar [1] / x [2]; xbar [2] += ybar [1] * (-t2 / (x [2] * x [2]); // <p> = t1 * x [2]; y [2] = sin (x [2]); // Initialize adjoint objects: ybar <ref> [1] </ref> = (1, 0); ybar [2] = (0, 1); xbar [1] = xbar [2] = abar = t1bar = t2bar = (0, 0); xbar [2] += ybar [2] * cos (x [2]); // y [1] = t2 / x [2]; t2bar += ybar [1] / x [2]; xbar [2] += ybar [1] * (-t2 / (x [2] * x [2]); // t2 = t1 * x [2]; xbar [2] += t2bar * t1; // t1 = a * x [1]; abar += t1bar * x <p> (x [2]); // Initialize adjoint objects: ybar <ref> [1] </ref> = (1, 0); ybar [2] = (0, 1); xbar [1] = xbar [2] = abar = t1bar = t2bar = (0, 0); xbar [2] += ybar [2] * cos (x [2]); // y [1] = t2 / x [2]; t2bar += ybar [1] / x [2]; xbar [2] += ybar [1] * (-t2 / (x [2] * x [2]); // t2 = t1 * x [2]; xbar [2] += t2bar * t1; // t1 = a * x [1]; abar += t1bar * x [1]; xbar [1] += t1bar * a; // a <p> = (1, 0); ybar [2] = (0, 1); xbar <ref> [1] </ref> = xbar [2] = abar = t1bar = t2bar = (0, 0); xbar [2] += ybar [2] * cos (x [2]); // y [1] = t2 / x [2]; t2bar += ybar [1] / x [2]; xbar [2] += ybar [1] * (-t2 / (x [2] * x [2]); // t2 = t1 * x [2]; xbar [2] += t2bar * t1; // t1 = a * x [1]; abar += t1bar * x [1]; xbar [1] += t1bar * a; // a = x [1] * x [2]; xbar [1] <p> cos (x [2]); // y <ref> [1] </ref> = t2 / x [2]; t2bar += ybar [1] / x [2]; xbar [2] += ybar [1] * (-t2 / (x [2] * x [2]); // t2 = t1 * x [2]; xbar [2] += t2bar * t1; // t1 = a * x [1]; abar += t1bar * x [1]; xbar [1] += t1bar * a; // a = x [1] * x [2]; xbar [1] += abar * x [2]; xbar [2] += abar * x [1]; // ry [1] = (xbar [1][1], xbar [2][1]); // ry [2] = (xbar [1][2], xbar [2][2]); <p> = t2 / x [2]; t2bar += ybar <ref> [1] </ref> / x [2]; xbar [2] += ybar [1] * (-t2 / (x [2] * x [2]); // t2 = t1 * x [2]; xbar [2] += t2bar * t1; // t1 = a * x [1]; abar += t1bar * x [1]; xbar [1] += t1bar * a; // a = x [1] * x [2]; xbar [1] += abar * x [2]; xbar [2] += abar * x [1]; // ry [1] = (xbar [1][1], xbar [2][1]); // ry [2] = (xbar [1][2], xbar [2][2]); This discussion shows that the principles <p> / x [2]; t2bar += ybar <ref> [1] </ref> / x [2]; xbar [2] += ybar [1] * (-t2 / (x [2] * x [2]); // t2 = t1 * x [2]; xbar [2] += t2bar * t1; // t1 = a * x [1]; abar += t1bar * x [1]; xbar [1] += t1bar * a; // a = x [1] * x [2]; xbar [1] += abar * x [2]; xbar [2] += abar * x [1]; // ry [1] = (xbar [1][1], xbar [2][1]); // ry [2] = (xbar [1][2], xbar [2][2]); This discussion shows that the principles underlying automatic <p> [2]; xbar [2] += ybar <ref> [1] </ref> * (-t2 / (x [2] * x [2]); // t2 = t1 * x [2]; xbar [2] += t2bar * t1; // t1 = a * x [1]; abar += t1bar * x [1]; xbar [1] += t1bar * a; // a = x [1] * x [2]; xbar [1] += abar * x [2]; xbar [2] += abar * x [1]; // ry [1] = (xbar [1][1], xbar [2][1]); // ry [2] = (xbar [1][2], xbar [2][2]); This discussion shows that the principles underlying automatic differentiation are not complicated. <p> <ref> [1] </ref> * (-t2 / (x [2] * x [2]); // t2 = t1 * x [2]; xbar [2] += t2bar * t1; // t1 = a * x [1]; abar += t1bar * x [1]; xbar [1] += t1bar * a; // a = x [1] * x [2]; xbar [1] += abar * x [2]; xbar [2] += abar * x [1]; // ry [1] = (xbar [1][1], xbar [2][1]); // ry [2] = (xbar [1][2], xbar [2][2]); This discussion shows that the principles underlying automatic differentiation are not complicated. <p> t1 * x [2]; xbar [2] += t2bar * t1; // t1 = a * x <ref> [1] </ref>; abar += t1bar * x [1]; xbar [1] += t1bar * a; // a = x [1] * x [2]; xbar [1] += abar * x [2]; xbar [2] += abar * x [1]; // ry [1] = (xbar [1][1], xbar [2][1]); // ry [2] = (xbar [1][2], xbar [2][2]); This discussion shows that the principles underlying automatic differentiation are not complicated. <p> [2]; xbar [2] += t2bar * t1; // t1 = a * x <ref> [1] </ref>; abar += t1bar * x [1]; xbar [1] += t1bar * a; // a = x [1] * x [2]; xbar [1] += abar * x [2]; xbar [2] += abar * x [1]; // ry [1] = (xbar [1][1], xbar [2][1]); // ry [2] = (xbar [1][2], xbar [2][2]); This discussion shows that the principles underlying automatic differentiation are not complicated. <p> significant improvements in efficiency, including * run time code optimizations such as removal of common subexpressions, * parallel scheduling [2, 4], or 8 * Markowitz elimination of nodes from the computational graph [19]. 4.2 Reverse Mode increased complexity of the implementation is justified by the result of Baur and Strassen <ref> [1] </ref> that a gradient can be evaluated at a cost of about five times the cost of a function evaluation, independent of the number of independent variables. As discussed in Section 3.6, the reverse mode requires the ability to reverse the order of execution of the code list.
Reference: [2] <author> Christian Bischof. </author> <title> Issues in parallel automatic differentiation. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 100-113. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: Here, the vector x contains the independent variables, and the vector y contains the dependent variables. The function described by this program is defined except at x <ref> [2] </ref> = 0 and is differentiable except at x [1] = 2. We can transform the program in Figure 1 into one for computing derivatives by associating a derivative object rt with every variable t. <p> Assume that rt contains the derivatives of t with respect to the independent variables x, rt = @x [1] ; @t T . We propagate these derivatives by using elementary differentiation arithmetic based on the chain rule [13, 23] for computing the derivatives of y [1] and y <ref> [2] </ref>, as shown in Figure 2. In this example, each assignment to a derivative is actually a vector assignment of length two. This example contains a branch and a loop to illustrate that the forward mode handles control structures. <p> The only difficulty is that functions with branches may fail to be differentiable. Work is in progress to warn the programmer when this has occurred. if x [1] &gt; 2 f g else f a = x [1] * x <ref> [2] </ref>; for (i = 1; i &lt;= 2; i ++) f g y [2] = sin (x [2]); if x [1] &gt; 2.0 f ra = rx [1] + rx [2]; g else f a = x [1] * x [2]; for (i = 1; i &lt;= 2; i ++) f <p> Work is in progress to warn the programmer when this has occurred. if x [1] &gt; 2 f g else f a = x [1] * x <ref> [2] </ref>; for (i = 1; i &lt;= 2; i ++) f g y [2] = sin (x [2]); if x [1] &gt; 2.0 f ra = rx [1] + rx [2]; g else f a = x [1] * x [2]; for (i = 1; i &lt;= 2; i ++) f temp = a; a = a * x (i); ra = x (i) <p> Work is in progress to warn the programmer when this has occurred. if x [1] &gt; 2 f g else f a = x [1] * x <ref> [2] </ref>; for (i = 1; i &lt;= 2; i ++) f g y [2] = sin (x [2]); if x [1] &gt; 2.0 f ra = rx [1] + rx [2]; g else f a = x [1] * x [2]; for (i = 1; i &lt;= 2; i ++) f temp = a; a = a * x (i); ra = x (i) * ra + temp <p> programmer when this has occurred. if x [1] &gt; 2 f g else f a = x [1] * x <ref> [2] </ref>; for (i = 1; i &lt;= 2; i ++) f g y [2] = sin (x [2]); if x [1] &gt; 2.0 f ra = rx [1] + rx [2]; g else f a = x [1] * x [2]; for (i = 1; i &lt;= 2; i ++) f temp = a; a = a * x (i); ra = x (i) * ra + temp * rx (i); g y [1] = a / x [2]; a/ (x <p> f g else f a = x [1] * x <ref> [2] </ref>; for (i = 1; i &lt;= 2; i ++) f g y [2] = sin (x [2]); if x [1] &gt; 2.0 f ra = rx [1] + rx [2]; g else f a = x [1] * x [2]; for (i = 1; i &lt;= 2; i ++) f temp = a; a = a * x (i); ra = x (i) * ra + temp * rx (i); g y [1] = a / x [2]; a/ (x [2] * x [2]) * rx [2]; ry [2] = <p> + rx <ref> [2] </ref>; g else f a = x [1] * x [2]; for (i = 1; i &lt;= 2; i ++) f temp = a; a = a * x (i); ra = x (i) * ra + temp * rx (i); g y [1] = a / x [2]; a/ (x [2] * x [2]) * rx [2]; ry [2] = cos (x [2]) * rx [2]; This mode of automatic differentiation, where we maintain the derivatives with respect to the independent variables, is called the forward mode of automatic differentiation. <p> g else f a = x [1] * x <ref> [2] </ref>; for (i = 1; i &lt;= 2; i ++) f temp = a; a = a * x (i); ra = x (i) * ra + temp * rx (i); g y [1] = a / x [2]; a/ (x [2] * x [2]) * rx [2]; ry [2] = cos (x [2]) * rx [2]; This mode of automatic differentiation, where we maintain the derivatives with respect to the independent variables, is called the forward mode of automatic differentiation. <p> a = x [1] * x <ref> [2] </ref>; for (i = 1; i &lt;= 2; i ++) f temp = a; a = a * x (i); ra = x (i) * ra + temp * rx (i); g y [1] = a / x [2]; a/ (x [2] * x [2]) * rx [2]; ry [2] = cos (x [2]) * rx [2]; This mode of automatic differentiation, where we maintain the derivatives with respect to the independent variables, is called the forward mode of automatic differentiation. <p> [1] * x <ref> [2] </ref>; for (i = 1; i &lt;= 2; i ++) f temp = a; a = a * x (i); ra = x (i) * ra + temp * rx (i); g y [1] = a / x [2]; a/ (x [2] * x [2]) * rx [2]; ry [2] = cos (x [2]) * rx [2]; This mode of automatic differentiation, where we maintain the derivatives with respect to the independent variables, is called the forward mode of automatic differentiation. <p> x <ref> [2] </ref>; for (i = 1; i &lt;= 2; i ++) f temp = a; a = a * x (i); ra = x (i) * ra + temp * rx (i); g y [1] = a / x [2]; a/ (x [2] * x [2]) * rx [2]; ry [2] = cos (x [2]) * rx [2]; This mode of automatic differentiation, where we maintain the derivatives with respect to the independent variables, is called the forward mode of automatic differentiation. <p> = 1; i &lt;= 2; i ++) f temp = a; a = a * x (i); ra = x (i) * ra + temp * rx (i); g y [1] = a / x <ref> [2] </ref>; a/ (x [2] * x [2]) * rx [2]; ry [2] = cos (x [2]) * rx [2]; This mode of automatic differentiation, where we maintain the derivatives with respect to the independent variables, is called the forward mode of automatic differentiation. <p> &lt;= 2; i ++) f temp = a; a = a * x (i); ra = x (i) * ra + temp * rx (i); g y [1] = a / x <ref> [2] </ref>; a/ (x [2] * x [2]) * rx [2]; ry [2] = cos (x [2]) * rx [2]; This mode of automatic differentiation, where we maintain the derivatives with respect to the independent variables, is called the forward mode of automatic differentiation. <p> At the end of the computation, the desired derivatives are in xbar and ybar. 5 if x [1] &gt; 2 f g else f a = x [1] * x [2g a = a * x (i); y [1] = a / x <ref> [2] </ref>; Yields the code list: // Assuming x [1] 2 a = x [1] * x [2]; t2 = t1 * x [2]; y [2] = sin (x [2]); // Initialize adjoint objects: ybar [1] = (1, 0); ybar [2] = (0, 1); xbar [1] = xbar [2] = abar = <p> are in xbar and ybar. 5 if x [1] &gt; 2 f g else f a = x [1] * x [2g a = a * x (i); y [1] = a / x <ref> [2] </ref>; Yields the code list: // Assuming x [1] 2 a = x [1] * x [2]; t2 = t1 * x [2]; y [2] = sin (x [2]); // Initialize adjoint objects: ybar [1] = (1, 0); ybar [2] = (0, 1); xbar [1] = xbar [2] = abar = t1bar = t2bar = (0, 0); xbar [2] += ybar [2] * cos (x [2]); // <p> if x [1] &gt; 2 f g else f a = x [1] * x [2g a = a * x (i); y [1] = a / x <ref> [2] </ref>; Yields the code list: // Assuming x [1] 2 a = x [1] * x [2]; t2 = t1 * x [2]; y [2] = sin (x [2]); // Initialize adjoint objects: ybar [1] = (1, 0); ybar [2] = (0, 1); xbar [1] = xbar [2] = abar = t1bar = t2bar = (0, 0); xbar [2] += ybar [2] * cos (x [2]); // y [1] = t2 / x <p> [1] &gt; 2 f g else f a = x [1] * x [2g a = a * x (i); y [1] = a / x <ref> [2] </ref>; Yields the code list: // Assuming x [1] 2 a = x [1] * x [2]; t2 = t1 * x [2]; y [2] = sin (x [2]); // Initialize adjoint objects: ybar [1] = (1, 0); ybar [2] = (0, 1); xbar [1] = xbar [2] = abar = t1bar = t2bar = (0, 0); xbar [2] += ybar [2] * cos (x [2]); // y [1] = t2 / x [2]; t2bar <p> g else f a = x [1] * x [2g a = a * x (i); y [1] = a / x <ref> [2] </ref>; Yields the code list: // Assuming x [1] 2 a = x [1] * x [2]; t2 = t1 * x [2]; y [2] = sin (x [2]); // Initialize adjoint objects: ybar [1] = (1, 0); ybar [2] = (0, 1); xbar [1] = xbar [2] = abar = t1bar = t2bar = (0, 0); xbar [2] += ybar [2] * cos (x [2]); // y [1] = t2 / x [2]; t2bar += ybar [1] / <p> = a * x (i); y [1] = a / x <ref> [2] </ref>; Yields the code list: // Assuming x [1] 2 a = x [1] * x [2]; t2 = t1 * x [2]; y [2] = sin (x [2]); // Initialize adjoint objects: ybar [1] = (1, 0); ybar [2] = (0, 1); xbar [1] = xbar [2] = abar = t1bar = t2bar = (0, 0); xbar [2] += ybar [2] * cos (x [2]); // y [1] = t2 / x [2]; t2bar += ybar [1] / x [2]; xbar [2] += ybar [1] * (-t2 / (x <p> a / x <ref> [2] </ref>; Yields the code list: // Assuming x [1] 2 a = x [1] * x [2]; t2 = t1 * x [2]; y [2] = sin (x [2]); // Initialize adjoint objects: ybar [1] = (1, 0); ybar [2] = (0, 1); xbar [1] = xbar [2] = abar = t1bar = t2bar = (0, 0); xbar [2] += ybar [2] * cos (x [2]); // y [1] = t2 / x [2]; t2bar += ybar [1] / x [2]; xbar [2] += ybar [1] * (-t2 / (x [2] * x [2]); // t2 = t1 <p> [1] 2 a = x [1] * x <ref> [2] </ref>; t2 = t1 * x [2]; y [2] = sin (x [2]); // Initialize adjoint objects: ybar [1] = (1, 0); ybar [2] = (0, 1); xbar [1] = xbar [2] = abar = t1bar = t2bar = (0, 0); xbar [2] += ybar [2] * cos (x [2]); // y [1] = t2 / x [2]; t2bar += ybar [1] / x [2]; xbar [2] += ybar [1] * (-t2 / (x [2] * x [2]); // t2 = t1 * x [2]; xbar [2] += t2bar * t1; // t1 <p> = x [1] * x <ref> [2] </ref>; t2 = t1 * x [2]; y [2] = sin (x [2]); // Initialize adjoint objects: ybar [1] = (1, 0); ybar [2] = (0, 1); xbar [1] = xbar [2] = abar = t1bar = t2bar = (0, 0); xbar [2] += ybar [2] * cos (x [2]); // y [1] = t2 / x [2]; t2bar += ybar [1] / x [2]; xbar [2] += ybar [1] * (-t2 / (x [2] * x [2]); // t2 = t1 * x [2]; xbar [2] += t2bar * t1; // t1 = a * <p> x <ref> [2] </ref>; t2 = t1 * x [2]; y [2] = sin (x [2]); // Initialize adjoint objects: ybar [1] = (1, 0); ybar [2] = (0, 1); xbar [1] = xbar [2] = abar = t1bar = t2bar = (0, 0); xbar [2] += ybar [2] * cos (x [2]); // y [1] = t2 / x [2]; t2bar += ybar [1] / x [2]; xbar [2] += ybar [1] * (-t2 / (x [2] * x [2]); // t2 = t1 * x [2]; xbar [2] += t2bar * t1; // t1 = a * x [1]; abar += <p> y <ref> [2] </ref> = sin (x [2]); // Initialize adjoint objects: ybar [1] = (1, 0); ybar [2] = (0, 1); xbar [1] = xbar [2] = abar = t1bar = t2bar = (0, 0); xbar [2] += ybar [2] * cos (x [2]); // y [1] = t2 / x [2]; t2bar += ybar [1] / x [2]; xbar [2] += ybar [1] * (-t2 / (x [2] * x [2]); // t2 = t1 * x [2]; xbar [2] += t2bar * t1; // t1 = a * x [1]; abar += t1bar * x [1]; xbar [1] += t1bar <p> Initialize adjoint objects: ybar [1] = (1, 0); ybar <ref> [2] </ref> = (0, 1); xbar [1] = xbar [2] = abar = t1bar = t2bar = (0, 0); xbar [2] += ybar [2] * cos (x [2]); // y [1] = t2 / x [2]; t2bar += ybar [1] / x [2]; xbar [2] += ybar [1] * (-t2 / (x [2] * x [2]); // t2 = t1 * x [2]; xbar [2] += t2bar * t1; // t1 = a * x [1]; abar += t1bar * x [1]; xbar [1] += t1bar * a; // a = x [1] <p> objects: ybar [1] = (1, 0); ybar <ref> [2] </ref> = (0, 1); xbar [1] = xbar [2] = abar = t1bar = t2bar = (0, 0); xbar [2] += ybar [2] * cos (x [2]); // y [1] = t2 / x [2]; t2bar += ybar [1] / x [2]; xbar [2] += ybar [1] * (-t2 / (x [2] * x [2]); // t2 = t1 * x [2]; xbar [2] += t2bar * t1; // t1 = a * x [1]; abar += t1bar * x [1]; xbar [1] += t1bar * a; // a = x [1] * x <p> = (0, 1); xbar [1] = xbar <ref> [2] </ref> = abar = t1bar = t2bar = (0, 0); xbar [2] += ybar [2] * cos (x [2]); // y [1] = t2 / x [2]; t2bar += ybar [1] / x [2]; xbar [2] += ybar [1] * (-t2 / (x [2] * x [2]); // t2 = t1 * x [2]; xbar [2] += t2bar * t1; // t1 = a * x [1]; abar += t1bar * x [1]; xbar [1] += t1bar * a; // a = x [1] * x [2]; xbar [1] += abar * x [2]; <p> xbar [1] = xbar <ref> [2] </ref> = abar = t1bar = t2bar = (0, 0); xbar [2] += ybar [2] * cos (x [2]); // y [1] = t2 / x [2]; t2bar += ybar [1] / x [2]; xbar [2] += ybar [1] * (-t2 / (x [2] * x [2]); // t2 = t1 * x [2]; xbar [2] += t2bar * t1; // t1 = a * x [1]; abar += t1bar * x [1]; xbar [1] += t1bar * a; // a = x [1] * x [2]; xbar [1] += abar * x [2]; xbar [2] += <p> = t1bar = t2bar = (0, 0); xbar <ref> [2] </ref> += ybar [2] * cos (x [2]); // y [1] = t2 / x [2]; t2bar += ybar [1] / x [2]; xbar [2] += ybar [1] * (-t2 / (x [2] * x [2]); // t2 = t1 * x [2]; xbar [2] += t2bar * t1; // t1 = a * x [1]; abar += t1bar * x [1]; xbar [1] += t1bar * a; // a = x [1] * x [2]; xbar [1] += abar * x [2]; xbar [2] += abar * x [1]; // ry [1] <p> = t2bar = (0, 0); xbar <ref> [2] </ref> += ybar [2] * cos (x [2]); // y [1] = t2 / x [2]; t2bar += ybar [1] / x [2]; xbar [2] += ybar [1] * (-t2 / (x [2] * x [2]); // t2 = t1 * x [2]; xbar [2] += t2bar * t1; // t1 = a * x [1]; abar += t1bar * x [1]; xbar [1] += t1bar * a; // a = x [1] * x [2]; xbar [1] += abar * x [2]; xbar [2] += abar * x [1]; // ry [1] = (xbar <p> += ybar [1] * (-t2 / (x <ref> [2] </ref> * x [2]); // t2 = t1 * x [2]; xbar [2] += t2bar * t1; // t1 = a * x [1]; abar += t1bar * x [1]; xbar [1] += t1bar * a; // a = x [1] * x [2]; xbar [1] += abar * x [2]; xbar [2] += abar * x [1]; // ry [1] = (xbar [1][1], xbar [2][1]); // ry [2] = (xbar [1][2], xbar [2][2]); This discussion shows that the principles underlying automatic differentiation are not complicated. <p> <ref> [2] </ref> * x [2]); // t2 = t1 * x [2]; xbar [2] += t2bar * t1; // t1 = a * x [1]; abar += t1bar * x [1]; xbar [1] += t1bar * a; // a = x [1] * x [2]; xbar [1] += abar * x [2]; xbar [2] += abar * x [1]; // ry [1] = (xbar [1][1], xbar [2][1]); // ry [2] = (xbar [1][2], xbar [2][2]); This discussion shows that the principles underlying automatic differentiation are not complicated. <p> x <ref> [2] </ref>); // t2 = t1 * x [2]; xbar [2] += t2bar * t1; // t1 = a * x [1]; abar += t1bar * x [1]; xbar [1] += t1bar * a; // a = x [1] * x [2]; xbar [1] += abar * x [2]; xbar [2] += abar * x [1]; // ry [1] = (xbar [1][1], xbar [2][1]); // ry [2] = (xbar [1][2], xbar [2][2]); This discussion shows that the principles underlying automatic differentiation are not complicated. <p> t1 = a * x [1]; abar += t1bar * x [1]; xbar [1] += t1bar * a; // a = x [1] * x <ref> [2] </ref>; xbar [1] += abar * x [2]; xbar [2] += abar * x [1]; // ry [1] = (xbar [1][1], xbar [2][1]); // ry [2] = (xbar [1][2], xbar [2][2]); This discussion shows that the principles underlying automatic differentiation are not complicated. We associate extra computations (which are entirely specified on a statement-by-statement basis) with the statements executed in the original code, a task for which operator overloading is well suited. <p> However, run time access to the code list provides opportunities for significant improvements in efficiency, including * run time code optimizations such as removal of common subexpressions, * parallel scheduling <ref> [2, 4] </ref>, or 8 * Markowitz elimination of nodes from the computational graph [19]. 4.2 Reverse Mode increased complexity of the implementation is justified by the result of Baur and Strassen [1] that a gradient can be evaluated at a cost of about five times the cost of a function evaluation,
Reference: [3] <author> Christian Bischof, Alan Carle, George Corliss, and Andreas Griewank. </author> <title> ADIFOR Generating derivative codes from Fortran programs. </title> <journal> Scientific Programming, </journal> <volume> 1 </volume> <pages> 11-29, </pages> <year> 1992. </year>
Reference-contexts: Speed can be improved and code growth reduced by better compilation techniques. In principle, programs that place function calls for overloaded operators in line and then do code optimization should execute as fast as conventional compiled code. Source transformation uses a preprocessor (ADIFOR <ref> [3] </ref>, for example) to generate code similar to that shown in Figure 2. <p> The source transformation tool can generate code for the reverse mode evaluation of basic blocks and parallel loops. Even the partial use of the reverse mode in the context of an overall forward-mode propagation of derivatives gives a significant speed improvement in some examples <ref> [3] </ref>. The disadvantage of source transformation techniques is that it is difficult to handle the full reverse mode in the presence of branches and data-dependent loops. The reverse mode for functions containing branches and data-dependent loops requires the run time support furnished by operator overloading.
Reference: [4] <author> Christian Bischof, Andreas Griewank, and David Juedes. </author> <title> Exploiting parallelism in automatic differentiation. </title> <editor> In E. Houstis and Y. Muraoka, editors, </editor> <booktitle> Proceedings of the 1991 International Conference on Supercomputing, </booktitle> <pages> pages 146-153. </pages> <publisher> ACM Press, </publisher> <address> Baltimore, Md., </address> <year> 1991. </year>
Reference-contexts: However, run time access to the code list provides opportunities for significant improvements in efficiency, including * run time code optimizations such as removal of common subexpressions, * parallel scheduling <ref> [2, 4] </ref>, or 8 * Markowitz elimination of nodes from the computational graph [19]. 4.2 Reverse Mode increased complexity of the implementation is justified by the result of Baur and Strassen [1] that a gradient can be evaluated at a cost of about five times the cost of a function evaluation,
Reference: [5] <author> Bruce D. Christianson. </author> <title> Automatic Hessians by reverse accumulation. </title> <type> Technical Report NOC TR228, </type> <institution> The Numerical Optimisation Center, Hatfield Polytechnic, Hatfield, U.K., </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: We also note that even though we showed the computation only of first derivatives, the automatic differentiation approach can easily be generalized to the computation of univariate Taylor series or Hessians and multivariate higher-order derivatives <ref> [5, 7, 9, 15, 17, 23] </ref>. 4 Role of Object Orientation The ADOL-C tool for automatic differentiation relies critically on overloaded operators in C++. In this section, we discuss the role of operator overloading.
Reference: [6] <author> T. F. Coleman and J. J. </author> <title> More. Estimation of sparse Jacobian matrices and graph coloring problems. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 20 </volume> <pages> 187-209, </pages> <year> 1984. </year>
Reference-contexts: Further, b D is sparse and can be computed as three columns or as eight rows using matrix-coloring techniques <ref> [6] </ref>. 3 Alternative Methods for Computing J A common paradigm for the numerical solution of two-, three-, or higher-dimensional partial differential equations is as follows: * Given a PDE and boundary conditions, * apply finite-difference or finite-element approximations on some appropriate (frequently nonuni form) grid, and * enforce an approximate solution <p> Therefore, the accuracy of divided-difference approximations is in doubt. A naive coding perturbing each component of u in turn is easy, but very expensive. Applications usually attempt to exploit any known sparsity structure. 3 3.4 Matrix Coloring and Partial Separability Matrix coloring <ref> [6] </ref> and partial separability [20] are techniques commonly used in computing divided differences efficiently for sparse Jacobians. Coloring identifies several columns of J that can be computed simultaneously with the same computational effort as a single column. <p> The nonlinear contributions are contained in a 350fi936 subblock that is sparse and can be computed as three columns (1050 values) or as eight rows (7488 values) using matrix coloring techniques <ref> [6] </ref>. The forward mode propagated three gradient vectors of length 350. The tape for the three-color forward mode evaluation was 520 Kbytes long. The reverse mode computed eight gradient vectors of length 936.
Reference: [7] <author> George Corliss. </author> <title> Overloading point and interval Taylor operators. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 139-146. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: We also note that even though we showed the computation only of first derivatives, the automatic differentiation approach can easily be generalized to the computation of univariate Taylor series or Hessians and multivariate higher-order derivatives <ref> [5, 7, 9, 15, 17, 23] </ref>. 4 Role of Object Orientation The ADOL-C tool for automatic differentiation relies critically on overloaded operators in C++. In this section, we discuss the role of operator overloading. <p> Figure 5 shows part of such a class for interval-valued functions written in C-XSC [22]. Many authors 6 have written such a package (see the survey in [21]). Corliss <ref> [7] </ref> described an implementation in Ada of real- and interval-valued Taylor series with storage management for efficient one-term-at-a-time generation. class autodiff - // Author: Andreas Wiethoff, University of Karlsruhe private: interval f, df; public: autodiff (); // Constructors autodiff (interval&, interval&); ~autodiff (); // Destructors autodiff& operator = (interval&); // Assignment
Reference: [8] <author> George Corliss, Andreas Griewank, Tom Robey, and Steve Wright. </author> <title> Automatic differentiation applied to unsaturated flow | ADOL-C case study. </title> <type> Technical Memorandum ANL/MCS-TM-162, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: We believed that automatic differentiation could improve both the accuracy and the speed by using operator overloading in C++. We modified the original code of Robey in three steps (for further details, see <ref> [8, 10] </ref>). 1. Convert to C++. Robey's original code was written in C. We converted function headers to a form acceptable to C++, removed some system-dependent calls, and added system dependent timing instrumentation. 2. Change to derivative type.
Reference: [9] <author> George Corliss and Louis B. Rall. </author> <title> Automatic generation of Taylor series in Pascal-SC: Basic operations and applications to differential equations, in Trans. </title> <booktitle> of the First Army Conference on Applied Mathematics and Computing (Washington, </booktitle> <address> D.C., </address> <year> 1983), </year> <pages> pages 177-209. </pages> <publisher> ARO Rep. </publisher> <pages> 84-1, </pages> <institution> U. S. Army Res. Office, Research Triangle Park, N.C., </institution> <year> 1984. </year>
Reference-contexts: We also note that even though we showed the computation only of first derivatives, the automatic differentiation approach can easily be generalized to the computation of univariate Taylor series or Hessians and multivariate higher-order derivatives <ref> [5, 7, 9, 15, 17, 23] </ref>. 4 Role of Object Orientation The ADOL-C tool for automatic differentiation relies critically on overloaded operators in C++. In this section, we discuss the role of operator overloading.
Reference: [10] <author> George Corliss, Thomas Robey, Christian Bischof, Andreas Griewank, and Steve Wright. </author> <title> Automatic differentiation for PDEs | Unsaturated flow case study. </title> <editor> In Robert Vichnevetski, Doyle Knight, and Gerard Richter, editors, </editor> <booktitle> Advances in Computer Methods for Partial Differential Equations - VII, </booktitle> <pages> pages 150-156. </pages> <address> IMACS, New Brunswick N.J., </address> <year> 1992. </year> <month> 10 </month>
Reference-contexts: We believed that automatic differentiation could improve both the accuracy and the speed by using operator overloading in C++. We modified the original code of Robey in three steps (for further details, see <ref> [8, 10] </ref>). 1. Convert to C++. Robey's original code was written in C. We converted function headers to a form acceptable to C++, removed some system-dependent calls, and added system dependent timing instrumentation. 2. Change to derivative type.
Reference: [11] <author> Lawrence C. W. Dixon. </author> <title> Use of automatic differentiation for calculating Hessians and Newton steps. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 114-125. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: Current tools achieve this by storing a record of every computation performed. An interpreter performs a backward pass on this "tape." Unfortunately, the resulting overhead sometimes destroys the complexity advantage of the reverse mode (see <ref> [11] </ref>). In the unsaturated flow problem, complexity analysis predict no clear advantage for either mode over the other.
Reference: [12] <author> Richard Ewing and Mary Wheeler. </author> <title> Computational aspects of mixed finite element methods. </title> <editor> In R. Stepleman et al., editors, </editor> <publisher> Scientific Computing. IMACS/North-Holland Publishing Company, </publisher> <year> 1983. </year>
Reference-contexts: we present results obtained when the different modes are used, and we draw some conclusions about the future of automatic differentiation in scientific problem-solving. 2 Unsaturated Flow Problem A mathematical model of steady-state flow in a porous medium involves an elliptic partial differential equation (PDE) that contains a conductivity coefficient <ref> [12, 24] </ref>. The coefficient is typically discontinuous across different materials and can vary greatly. For unsaturated flow, the conductivity is usually taken to be a function of pore pressure, which introduces a nonlinearity into the problem.
Reference: [13] <author> Andreas Griewank. </author> <title> On automatic differentiation. </title> <editor> In M. Iri and K. Tanabe, editors, </editor> <booktitle> Mathematical Programming: Recent Developments and Applications, </booktitle> <pages> pages 83-108. </pages> <publisher> Kluwer Academic Publishers, </publisher> <address> Dordrecht, </address> <year> 1989. </year>
Reference-contexts: The Jacobian of a partially separable function can be written as a sum of several simple, smaller Jacobians. Matrix coloring and partial separability also work well with the techniques of automatic differentiation. 3.5 Automatic Differentiation Forward Mode We illustrate automatic differentiation <ref> [13, 16, 23] </ref> with an example. Assume that we have the sample program shown in Figure 1 for the computation of a function f : R 2 7! R 2 . Here, the vector x contains the independent variables, and the vector y contains the dependent variables. <p> Assume that rt contains the derivatives of t with respect to the independent variables x, rt = @x [1] ; @t T . We propagate these derivatives by using elementary differentiation arithmetic based on the chain rule <ref> [13, 23] </ref> for computing the derivatives of y [1] and y [2], as shown in Figure 2. In this example, each assignment to a derivative is actually a vector assignment of length two. This example contains a branch and a loop to illustrate that the forward mode handles control structures.
Reference: [14] <author> Andreas Griewank. </author> <title> Achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation. Optimization Methods and Software, </title> <note> to appear. </note>
Reference-contexts: Work is in progress for a checkpointing option in ADOL-C that reduces the length of the tape required to a logarithm of the original running time at the expense of increasing the undifferentiated running time <ref> [14] </ref>. Once the tape has recorded the sequence of operations in the evaluation of f , a separate function call interprets the code list in order and propagates the desired derivatives.
Reference: [15] <author> Andreas Griewank. </author> <title> Automatic evaluation of first- and higher-derivative vectors. </title> <editor> In R. Sey-del, F. W. Schneider, T. Kupper, and H. Troger, editors, </editor> <booktitle> Proceedings of the Conference at Wurzburg, </booktitle> <month> Aug. </month> <year> 1990, </year> <title> Bifurcation and Chaos: Analysis, Algorithms, </title> <journal> Applications, </journal> <volume> volume 97, </volume> <pages> pages 135-148. </pages> <publisher> Birkhauser Verlag, </publisher> <address> Basel, Switzerland, </address> <year> 1991. </year>
Reference-contexts: We also note that even though we showed the computation only of first derivatives, the automatic differentiation approach can easily be generalized to the computation of univariate Taylor series or Hessians and multivariate higher-order derivatives <ref> [5, 7, 9, 15, 17, 23] </ref>. 4 Role of Object Orientation The ADOL-C tool for automatic differentiation relies critically on overloaded operators in C++. In this section, we discuss the role of operator overloading.
Reference: [16] <author> Andreas Griewank. </author> <title> The chain rule revisited in scientific computing. </title> <journal> SIAM News, </journal> <volume> 24, </volume> <month> May & July </month> <year> 1991. </year> <note> No. 3, </note> <editor> p. </editor> <volume> 20 & No. 4, </volume> <editor> p. </editor> <volume> 8. </volume>
Reference-contexts: The Jacobian of a partially separable function can be written as a sum of several simple, smaller Jacobians. Matrix coloring and partial separability also work well with the techniques of automatic differentiation. 3.5 Automatic Differentiation Forward Mode We illustrate automatic differentiation <ref> [13, 16, 23] </ref> with an example. Assume that we have the sample program shown in Figure 1 for the computation of a function f : R 2 7! R 2 . Here, the vector x contains the independent variables, and the vector y contains the dependent variables. <p> For sparse problems, the number of variables alone is not the determining factor. Instead, the complexity of the forward and the reverse modes can be bounded in terms of the chromatic numbers of the column- and row-incidence graphs, respectively. This issue is discussed in more detail in <ref> [17, 16] </ref>.
Reference: [17] <author> Andreas Griewank. </author> <title> Some bounds on the complexity of gradients, Jacobians, </title> <journal> and Hessians. </journal> <note> To appear in P. </note> <editor> M. Pardalos, editor, </editor> <title> Complexity in Numerical Optimization, </title> <publisher> World Scientific Publishers, </publisher> <year> 1993. </year>
Reference-contexts: For sparse problems, the number of variables alone is not the determining factor. Instead, the complexity of the forward and the reverse modes can be bounded in terms of the chromatic numbers of the column- and row-incidence graphs, respectively. This issue is discussed in more detail in <ref> [17, 16] </ref>. <p> We also note that even though we showed the computation only of first derivatives, the automatic differentiation approach can easily be generalized to the computation of univariate Taylor series or Hessians and multivariate higher-order derivatives <ref> [5, 7, 9, 15, 17, 23] </ref>. 4 Role of Object Orientation The ADOL-C tool for automatic differentiation relies critically on overloaded operators in C++. In this section, we discuss the role of operator overloading.
Reference: [18] <author> Andreas Griewank, David Juedes, Jay Srinivasan, and Charles Tyner. ADOL-C, </author> <title> a package for the automatic differentiation of algorithms written in C/C++. </title> <journal> ACM Trans. Math. Software, </journal> <note> to appear. </note>
Reference-contexts: For problems with many independent variables, however, the reverse mode of automatic differentiation is required for efficiency. fl This work was supported in part by the Office of Scientific Computing, U.S. Department of Energy, under Contract W-31-109-Eng-38 and through NSF Cooperative Agreement No. CCR-8809615. 1 The software package ADOL-C <ref> [18] </ref> offers researchers a means for applying the reverse mode efficiently and effectively. ADOL-C includes overloaded operators that record each arithmetic operation and elementary function and write the operands and the results on a "tape." The tape is subsequently interpreted to provide the derivatives required for Newton's method. <p> He calculates a very sparse 1989 fi 1989 Jacobian J by centered differences. The resulting linear equation is solved by a biconjugate gradient algorithm. We approached Robey's code hoping to show the superiority of the ADOL-C <ref> [18] </ref> implementation of automatic differentiation over centered differences. We believed that automatic differentiation could improve both the accuracy and the speed by using operator overloading in C++. We modified the original code of Robey in three steps (for further details, see [8, 10]). 1. Convert to C++. <p> In this section, we discuss the role of operator overloading. The details of the ADOL-C implementation can be found in <ref> [18] </ref>. 4.1 Forward Mode There are three alternatives for the forward-mode propagation of derivative objects: naive operator overloading, source transformation, and use of a tape. We discuss each alternative in turn.
Reference: [19] <author> Andreas Griewank and S. Reese. </author> <title> On the calculation of Jacobian matrices by the Markowitz rule. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 126-135. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: However, run time access to the code list provides opportunities for significant improvements in efficiency, including * run time code optimizations such as removal of common subexpressions, * parallel scheduling [2, 4], or 8 * Markowitz elimination of nodes from the computational graph <ref> [19] </ref>. 4.2 Reverse Mode increased complexity of the implementation is justified by the result of Baur and Strassen [1] that a gradient can be evaluated at a cost of about five times the cost of a function evaluation, independent of the number of independent variables.
Reference: [20] <author> Andreas Griewank and Ph. L. Toint. </author> <title> On the unconstrained optimization of partially separable functions. </title> <editor> In M. J. D. Powell, editor, </editor> <booktitle> Nonlinear Optimization 1981, NATO Conference Series II: Systems Science, </booktitle> <pages> pages 301-321. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: Therefore, the accuracy of divided-difference approximations is in doubt. A naive coding perturbing each component of u in turn is easy, but very expensive. Applications usually attempt to exploit any known sparsity structure. 3 3.4 Matrix Coloring and Partial Separability Matrix coloring [6] and partial separability <ref> [20] </ref> are techniques commonly used in computing divided differences efficiently for sparse Jacobians. Coloring identifies several columns of J that can be computed simultaneously with the same computational effort as a single column.
Reference: [21] <author> David Juedes. </author> <title> A taxonomy of automatic differentiation tools. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 315-329. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: The forward mode is easy to implement using overloaded operators, and many authors have done so (see the survey in <ref> [21] </ref>). 3.6 Automatic Differentiation Reverse Mode The reverse mode of automatic differentiation maintains the derivative of the result with respect to intermediate quantities, called adjoints, which measure the sensitivity of the result with respect to some intermediate quantity. <p> We associate extra computations (which are entirely specified on a statement-by-statement basis) with the statements executed in the original code, a task for which operator overloading is well suited. As a result, a variety of implementations of automatic differentiation have been developed over the years (see <ref> [21] </ref> for a survey). <p> Figure 5 shows part of such a class for interval-valued functions written in C-XSC [22]. Many authors 6 have written such a package (see the survey in <ref> [21] </ref>).
Reference: [22] <author> R. Klatte, U. Kulisch, C. Lawo, M. Rauch, and A. Wiethoff. </author> <title> C-XSC : A C++ Class Library for Extended Scientific Computing, </title> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1993. </year>
Reference-contexts: One defines a class containing the desired derivative type and overloaded operations and elementary functions according the recurrence relations given by Rall [23], for example. Figure 5 shows part of such a class for interval-valued functions written in C-XSC <ref> [22] </ref>. Many authors 6 have written such a package (see the survey in [21]).
Reference: [23] <author> Louis B. Rall. </author> <title> Automatic Differentiation: Techniques and Applications, </title> <booktitle> volume 120 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1981. </year> <month> 11 </month>
Reference-contexts: The Jacobian of a partially separable function can be written as a sum of several simple, smaller Jacobians. Matrix coloring and partial separability also work well with the techniques of automatic differentiation. 3.5 Automatic Differentiation Forward Mode We illustrate automatic differentiation <ref> [13, 16, 23] </ref> with an example. Assume that we have the sample program shown in Figure 1 for the computation of a function f : R 2 7! R 2 . Here, the vector x contains the independent variables, and the vector y contains the dependent variables. <p> Assume that rt contains the derivatives of t with respect to the independent variables x, rt = @x [1] ; @t T . We propagate these derivatives by using elementary differentiation arithmetic based on the chain rule <ref> [13, 23] </ref> for computing the derivatives of y [1] and y [2], as shown in Figure 2. In this example, each assignment to a derivative is actually a vector assignment of length two. This example contains a branch and a loop to illustrate that the forward mode handles control structures. <p> We also note that even though we showed the computation only of first derivatives, the automatic differentiation approach can easily be generalized to the computation of univariate Taylor series or Hessians and multivariate higher-order derivatives <ref> [5, 7, 9, 15, 17, 23] </ref>. 4 Role of Object Orientation The ADOL-C tool for automatic differentiation relies critically on overloaded operators in C++. In this section, we discuss the role of operator overloading. <p> It is straightforward to write overloaded operators and functions to propagate simple derivatives, gradients, Jacobians, Hessians, single-, or multi-variable Taylor series. One defines a class containing the desired derivative type and overloaded operations and elementary functions according the recurrence relations given by Rall <ref> [23] </ref>, for example. Figure 5 shows part of such a class for interval-valued functions written in C-XSC [22]. Many authors 6 have written such a package (see the survey in [21]).
Reference: [24] <author> Mary Wheeler and Ruth Gonzalez. </author> <title> Mixed finite element methods for petroleum reservoir engineering problems. </title> <editor> In R. Glowinski and J.-L. Lions, editors, </editor> <booktitle> Computing Methods in Applied Sciences and Engineering, VI. </booktitle> <publisher> Elsevier, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: we present results obtained when the different modes are used, and we draw some conclusions about the future of automatic differentiation in scientific problem-solving. 2 Unsaturated Flow Problem A mathematical model of steady-state flow in a porous medium involves an elliptic partial differential equation (PDE) that contains a conductivity coefficient <ref> [12, 24] </ref>. The coefficient is typically discontinuous across different materials and can vary greatly. For unsaturated flow, the conductivity is usually taken to be a function of pore pressure, which introduces a nonlinearity into the problem.
Reference: [25] <author> Philip Wolfe. </author> <title> Checking the calculation of gradients. </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 6(4) </volume> <pages> 337-343, </pages> <year> 1982. </year>
Reference-contexts: Instead, the complexity of the forward and the reverse modes can be bounded in terms of the chromatic numbers of the column- and row-incidence graphs, respectively. This issue is discussed in more detail in [17, 16]. Wolfe observed <ref> [25] </ref>, and Baur and Strassen confirmed [1], that if care is taken in handling quantities that are common to the (rational) function and its derivatives, then the cost of evaluating a gradient with n components is a small multiple of the cost of evaluating the underlying scalar function.
References-found: 25

