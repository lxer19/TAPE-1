URL: http://www.cis.ohio-state.edu/~kchen/nca97.ps
Refering-URL: http://www.cis.ohio-state.edu/~kchen/on-line.html
Root-URL: 
Title: Combining Linear Discriminant Functions with Neural Networks for Supervised Learning  
Author: Ke Chen a;b; Xiang Yu 
Keyword: Divide-and-conquer, linear discriminant function, multi-layered perceptron, modular and hierarchical architecture, constructive learning, supervised learning.  
Date: 19-41.  
Address: Columbus, OH 43210-1277, U.S.A.  
Affiliation: Center for Information Science Peking University, Beijing 100871, China b The Center for Cognitive Science and Department of Computer and Information Science The Ohio State University,  
Note: a and Huisheng Chi a a National Lab of Machine Perception and  Neural Computing Applications, Vol. 6, No. 1, 1997, pp.  
Abstract: A novel supervised learning method is presented by combining linear discriminant functions with neural networks. The proposed method results in a tree-structured hybrid architecture. Due to constructive learning, the binary tree hierarchical architecture is automatically generated by a controlled growing process for a specific supervised learning task. Unlike the classic decision tree, the linear discriminant functions are merely employed in the intermediate level of the tree for heuristically partitioning a large and complicated task into several smaller and simpler subtasks in the proposed method. These subtasks are dealt with by component neural networks at the leaves of the tree accordingly. For constructive learning, growing and credit-assignment algorithms are developed to serve for the hybrid architecture. The proposed architecture provides an efficient way to apply existing neural networks (e.g. multi-layered perceptron) for solving a large scale problem. We have already applied the proposed method to a universal approximation problem and several benchmark classification problems in order to evaluate its performance. Simulation results have shown that the proposed method yields better results and faster training in comparison with the multi-layered perceptron.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Bishop. </author> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford University Press, </publisher> <year> 1995. </year>
Reference-contexts: 1 Introduction Neural networks, particularly Multi-Layered Perceptrons (MLPs), have already been found to be successful for various supervised learning tasks <ref> [1, 11, 23, 24, 44, 39, 42, 45, 48, 50, 51] </ref>. Both theoretical and empirical studies have shown that the neural network is of powerful capabilities for pattern classification and universal approximation which are typical supervised learning tasks. <p> Although some statistical techniques have been recently borrowed for 1 Corresponding author. E-mail: kchen@cis.ohio-state.edu. 1 model selection <ref> [1, 46, 57] </ref>, most of them are involved in a time-consuming procedure for practical use. Thus, the network architecture must be determined by trial and error. <p> uh 1 (i) + (1 u)h 3 (i) + * i ; i = 1; ; 21: For class 3, x i = uh 2 (i) + (1 u)h 3 (i) + * i ; i = 1; ; 21: Here u is a uniform random variable on the interval <ref> [0; 1] </ref>, and * 1 ; ; * 21 are independent Gaussian random variables with zero mean and unit variance. The three classes have equal priori probabilities. Breiman et al reported that the Bayesian misclassification rate for this problem is approximately 14% [3].
Reference: [2] <author> A. Blum and R. Rivest. </author> <title> Training a 3-node neural net is NP-complete. </title> <editor> In D. S. Touretsky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <pages> pages 494-501, </pages> <address> San Mateo, CA, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Obviously, what these linear discriminant functions do is either to reduce the complexity of the original problem or to transfer the original problem into two smaller problems. As theoretically and empirically shown previously, the training time of an MLP often increases exponentially with the size of the problem <ref> [2, 32, 35] </ref>. Thus, a real world problem (e.g. image processing) will be often intractable when the MLP is directly used.
Reference: [3] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth & Brooks, </publisher> <address> Monterey, CA, </address> <year> 1984. </year>
Reference-contexts: On the other hand, supervised learning has been studied for a long time in the pattern recognition community. Decision tree is one of the most efficient tools for supervised learning in the pattern recognition community <ref> [3, 4, 19, 33, 36, 41, 53, 55] </ref>. In general, decision trees are hierarchical structures which use a sequential decision making strategy to handle a supervised learning task. <p> In decision tree approaches, linear discriminant functions or hyperplanes are commonly used for test and decision <ref> [3, 36, 41] </ref>. The traditional approach to training of a decision tree has been to first generate a set of possible hyperplanes and then exhaustively search this set to find the best hyperplanes with respect to some distortion metric [3, 36, 41]. <p> discriminant functions or hyperplanes are commonly used for test and decision <ref> [3, 36, 41] </ref>. The traditional approach to training of a decision tree has been to first generate a set of possible hyperplanes and then exhaustively search this set to find the best hyperplanes with respect to some distortion metric [3, 36, 41]. In most decision tree approaches, the hyperplanes are constrained to be perpendicular to the feature space axes. <p> This is very restrictive and can result in a large number of hyperplanes tests, even in a linearly separable problem [36], if the separating hyperplane is not perpendicular to the feature space axes. Indeed there are some decision tree approaches which allow for non-perpendicular hyperplanes <ref> [3, 4, 19, 53, 55] </ref>, but these algorithms used are computationally expensive due to the large search space of hyperplanes that need to be evaluated. <p> For generalization, a pruning procedure is also needed in these neural trees as same as in decision tree approaches such as Classification And Regression Tree (CART) <ref> [3] </ref>. The other hybrid techniques between decision trees and neural networks use elaborate methods for converting a decision tree into a neural network and then retraining it [10, 29]. <p> Although both the special MLP and the cascade correlation can also produce approximately correct resulting decision regions, the modular trees yield faster training. 4.3 Waveform Recognition Problem The synthetic waveform recognition problem has been first introduced in <ref> [3] </ref> to study the behavior of Classification And Regression Tree (CART). It is a three-class problem based on the waveforms h 1 (t); h 2 (t) and h 3 (t) depicted in Figure 9. Each class is a random convex combination of two of these waveforms. <p> The three classes have equal priori probabilities. Breiman et al reported that the Bayesian misclassification rate for this problem is approximately 14% <ref> [3] </ref>. In simulations, we randomly produced 7 independent training sets ranging in size from 500 to 2000 samples. For each training set, a modular tree with the specific architecture of subnetworks was generated. <p> It is evident from for the waveform recognition problem. For comparison, we also illustrate testing results produced by resulting modular trees with MT (21-12-3), CART with two different splitting rules <ref> [3] </ref> and an individual four-layered MLP consisting of 21 neurons in the input layer, 20 neurons in the first hidden layer, 5 neurons in the second hidden layer and 3 neurons in the output layer [22] in Figure 11. <p> As described previously, two architectures might be relevant to the proposed architecture. The decision tree and its variants are a kind of typical architectures to use the principle of divide-and-conquer for dealing with a problem <ref> [3] </ref>. The basic characteristics of the decision tree might be summarized as follows: (1) uniform apparatus are used for both dividing and conquering a problem, (2) `hard' partition way is adopted and (3) each leaf node is only associated with a class label often.
Reference: [4] <author> D. E. Brown and C. L. Pittard. </author> <title> Classification trees with optimal multivariate splits. </title> <booktitle> In Proc. IEEE Int. Conf. Systems, Man and Cybernetics, </booktitle> <volume> volume 3, </volume> <pages> pages 475-477, </pages> <address> Le Touquet, </address> <year> 1993. </year>
Reference-contexts: On the other hand, supervised learning has been studied for a long time in the pattern recognition community. Decision tree is one of the most efficient tools for supervised learning in the pattern recognition community <ref> [3, 4, 19, 33, 36, 41, 53, 55] </ref>. In general, decision trees are hierarchical structures which use a sequential decision making strategy to handle a supervised learning task. <p> This is very restrictive and can result in a large number of hyperplanes tests, even in a linearly separable problem [36], if the separating hyperplane is not perpendicular to the feature space axes. Indeed there are some decision tree approaches which allow for non-perpendicular hyperplanes <ref> [3, 4, 19, 53, 55] </ref>, but these algorithms used are computationally expensive due to the large search space of hyperplanes that need to be evaluated.
Reference: [5] <author> K. Chen, D. H. Xie, and H. S. Chi. </author> <title> A modified HME architecture for text-dependent speaker identification. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 7(5) </volume> <pages> 1309-1313, </pages> <year> 1996. </year>
Reference-contexts: The HME can be used in both regression and pattern classification tasks. However, the problem of determining an architecture prior to training is still encountered in practical use of the HME <ref> [5, 6] </ref>. Unlike the aforementioned work, an alternative tree-structured architecture is presented in this paper based upon our earlier work [7] for supervised learning. The principle of divide-and-conquer is directly employed in the proposed architecture, which results in a new binary tree-structured hybrid architecture.
Reference: [6] <author> K. Chen, D. H. Xie, and H. S. Chi. </author> <title> Speaker identification using time-delay HMEs. </title> <journal> International Journal of Neural Systems, </journal> <volume> 7(1) </volume> <pages> 29-43, </pages> <year> 1996. </year> <month> 16 </month>
Reference-contexts: The HME can be used in both regression and pattern classification tasks. However, the problem of determining an architecture prior to training is still encountered in practical use of the HME <ref> [5, 6] </ref>. Unlike the aforementioned work, an alternative tree-structured architecture is presented in this paper based upon our earlier work [7] for supervised learning. The principle of divide-and-conquer is directly employed in the proposed architecture, which results in a new binary tree-structured hybrid architecture.
Reference: [7] <author> K. Chen, L. P. Yang, X. Yu, and H. S. Chi. </author> <title> A self-generating modular neural network architec-ture for supervised learning. </title> <journal> Neurocomputing, </journal> <volume> 16(1) </volume> <pages> 33-48, </pages> <year> 1997. </year>
Reference-contexts: However, the problem of determining an architecture prior to training is still encountered in practical use of the HME [5, 6]. Unlike the aforementioned work, an alternative tree-structured architecture is presented in this paper based upon our earlier work <ref> [7] </ref> for supervised learning. The principle of divide-and-conquer is directly employed in the proposed architecture, which results in a new binary tree-structured hybrid architecture.
Reference: [8] <author> K. Chen, X. Yu, and H. S. Chi. </author> <title> Text-dependent speaker identification based on the modular tree. </title> <journal> Chinese Journal of Electronics, </journal> <volume> 5(2) </volume> <pages> 63-69, </pages> <year> 1996. </year>
Reference-contexts: In particular, the proposed method yields significantly faster training. The application of the proposed method in a real world problem called speaker recognition has been already investigated <ref> [8, 9] </ref>. It has also shown that the proposed method is a more effective way than other classic methods to solve a large scale problem [8]. The basic idea underlying the proposed method is use of the principle of divide-and-conquer. The issue is worth discussing here furthermore. <p> The application of the proposed method in a real world problem called speaker recognition has been already investigated [8, 9]. It has also shown that the proposed method is a more effective way than other classic methods to solve a large scale problem <ref> [8] </ref>. The basic idea underlying the proposed method is use of the principle of divide-and-conquer. The issue is worth discussing here furthermore.
Reference: [9] <author> K. Chen, X. Yu, and H. S. Chi. </author> <title> Text-dependent speaker identification based on the modular tree: an empirical study. </title> <editor> In S. Amari et al, editor, </editor> <booktitle> Progress in neural information processing, </booktitle> <pages> pages 294-299, </pages> <address> Singarpore, 1996. </address> <publisher> Spinger-Verlag. </publisher>
Reference-contexts: In particular, the proposed method yields significantly faster training. The application of the proposed method in a real world problem called speaker recognition has been already investigated <ref> [8, 9] </ref>. It has also shown that the proposed method is a more effective way than other classic methods to solve a large scale problem [8]. The basic idea underlying the proposed method is use of the principle of divide-and-conquer. The issue is worth discussing here furthermore.
Reference: [10] <author> K. J. Cios and N. Liu. </author> <title> A machine learning method for generation of a neural network architecture: A continuous ID3 algorithm. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3(2) </volume> <pages> 280-291, </pages> <year> 1992. </year>
Reference-contexts: As a result, many researchers in the neural network community have recently considered hybrid structures between decision trees and neural networks. Although these techniques were developed as neural networks whose structure could be automatically determined, their outcome can be interpreted as decision trees with nonlinear splits <ref> [10, 21, 22, 25, 29, 43, 49, 54] </ref>. In most of these hybrid structures, a small neural network at each node of the tree classifier is used to implement nonlinear and multivariate splits instead of a hyperplane in decision tree approaches. <p> The other hybrid techniques between decision trees and neural networks use elaborate methods for converting a decision tree into a neural network and then retraining it <ref> [10, 29] </ref>. However, these techniques can be only used in simple pattern classification problems and their performance for a real world problem remains unknown.
Reference: [11] <author> M. Cohen, H. Franco, N. Morgan, D. Rumelhart, and V. Abrash. </author> <title> Context-dependent multiple distribution phonetic modeling with MLPs. </title> <editor> In S. J. Hanson, J. D. Cowan, and C. L. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <pages> pages 649-657, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1 Introduction Neural networks, particularly Multi-Layered Perceptrons (MLPs), have already been found to be successful for various supervised learning tasks <ref> [1, 11, 23, 24, 44, 39, 42, 45, 48, 50, 51] </ref>. Both theoretical and empirical studies have shown that the neural network is of powerful capabilities for pattern classification and universal approximation which are typical supervised learning tasks.
Reference: [12] <author> S. P. Curram and J. Mingers. </author> <title> Neural networks, decision tree induction and discriminant analysis: An empirical comparison. </title> <journal> Journal of the Operational Research Society, </journal> <volume> 45(4) </volume> <pages> 440-450, </pages> <year> 1994. </year>
Reference-contexts: In comparison with neural networks, however, merits of decision trees are that there is no problem of determining an architecture prior to training in design of decision tree and the performance of a linear decision tree is basically similar to an MLP's for a specific task <ref> [12, 40] </ref>. As a result, many researchers in the neural network community have recently considered hybrid structures between decision trees and neural networks.
Reference: [13] <author> G. Cybenko. </author> <title> Approximation by superpositions of a sigmoidal function. </title> <institution> University of Illinois, Urbana, </institution> <year> 1988. </year>
Reference-contexts: simulation that modular trees yield better generalization and faster training than the MLPs. 4.6 Function Approximation It is well-known that an MLP with sigmoidal activation function can perform the universal approximation of any continuous multivariate function to any desired degree of accuracy, provided that sufficiently many hidden neurons are available <ref> [13, 20, 26] </ref>. To evaluate the universal approximation ability of the proposed method, we performed an experiment by learning a multivariate function approximation task.
Reference: [14] <author> D. H. Deterding. </author> <title> Speaker normalization for automatic speech recognition. </title> <type> Ph.D. Thesis, </type> <institution> University of Cambridge, </institution> <year> 1989. </year>
Reference-contexts: collected by Deterding, who recorded examples of the 11 steady-state vowels of English spoken by 15 speakers for a speaker normalization study. 11 words including 11 vowel sounds were recorded and each word was uttered once by each of the 15 speakers, 7 of whom were female and 8 male <ref> [14] </ref>. The speech signals were low pass filtered at 4.7 kHz and then digitized to 12 bits with a 10 kHz sampling rate. 12-order linear predictive analysis was carried out on six 512 sample Hamming windowed segments from the steady part of the vowel.
Reference: [15] <author> R. Duda and P. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John wiley & Sons, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Discussions and conclusions are presented in last section. 2 Linear Discriminant Functions In this section, we review two linear discriminant functions used in the proposed method. One is the Fisher's linear discriminant function [17] and the other is a specific linear discriminant function relevant to the normal density <ref> [15] </ref>. 2.1 Fisher's Linear Discriminant Function One of the recurring problems encountered in applying statistical techniques to pattern recognition is so-called curse of dimensionality. That is, procedures that are analytically or computationally manageable in low-dimensional spaces can become completely impractical in a space of high-dimensions. <p> That is, procedures that are analytically or computationally manageable in low-dimensional spaces can become completely impractical in a space of high-dimensions. In order to attack the problem, Fisher's suggestion <ref> [15, 17] </ref> was to look for the linear function which maximizes the ratio of the between-class scatter to within-class scatter. <p> If S W is nonsingular, the final solution <ref> [15] </ref> is w = S 1 Thus, we have obtained the Fisher's linear discriminant function, the linear function with the max imum ratio of between-class scatter to within-class scatter. 4 2.2 A Linear Discriminant Function for the Normal Density In the Bayesian decision, the minimum-error-rate classification can be achieved by use <p> 1 Thus, we have obtained the Fisher's linear discriminant function, the linear function with the max imum ratio of between-class scatter to within-class scatter. 4 2.2 A Linear Discriminant Function for the Normal Density In the Bayesian decision, the minimum-error-rate classification can be achieved by use of the discriminant functions <ref> [15] </ref> g i (x) = log p (xj! i ) + log P (! i ); (12) where ! i is the label of class i, p (xj! i ) is the conditional probability density for x and P (! i ) is a priori probability. <p> Note, however, that if the variance 2 is small relative to the squared distance jjm i m j jj 2 , then the position of the decision boundary is relatively insensitive to the exact values of the a priori probabilities <ref> [15] </ref>. 5 3 Architecture and Constructive Learning Algorithms In this section, we present a self-generating hybrid architecture for a supervised learning task by combining linear discriminant functions and neural networks (e.g. MLPs). <p> Fisher used the data set in his classic paper on discriminant analysis [17] and the data set has since become a favorite example in pattern recognition <ref> [15] </ref>. Irises are classified into three categories: setosa, versicolor and virginica. Each category consists of 50 samples. Each sample possesses four attributes: sepal length, sepal width, petal length and petal width. In experiments, a subset of data was randomly chosen for training and the remaining data were used for test.
Reference: [16] <author> S. E. Fahlman and C. Lebiere. </author> <title> The cascade-correlation learning architecture. </title> <editor> In D. S. Touretsky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <pages> pages 524-532, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Thus, the network architecture must be determined by trial and error. To overcome the difficulty in determining a neural network architecture prior to training, practical approaches for dynamic neural network architecture generation have been sought <ref> [16, 38, 52] </ref>. However, these models do not specify in what exact sequence a neuron should be added to give the maximum effect in classifying training examples and keep the slow convergence property since they still suffer from serious catastrophic interference in both spatial and temporal crosstalk during training. <p> Fahlman et al used a constructive learning algorithm to solve the problem successfully. The problem has since been popular in the neural network community and has been extensively used for evaluating both nonlinear separability and generalization ability of a neural architecture <ref> [16] </ref>. Here, we use the benchmark to evaluate the performance of the proposed tree-structured architecture. <p> To visualize the process of growing a modular tree during training, we diagrammatically show how the input space is split up in Figure 8. In addition, we have also applied the MLP with 2-5-5-5-1 and additional connections suggested by Lang et al [34] and the cascade correlation architecture <ref> [16] </ref> to the two spirals problems in the same workstation. As a result, the CPU time of training modular trees and those architectures are listed in Table 6 for comparison.
Reference: [17] <author> R. A. Fisher. </author> <title> The use of multiple measurements in taxonomic problem. </title> <journal> Annals of Eugenics, </journal> <volume> 7 </volume> <pages> 179-188, </pages> <year> 1936. </year>
Reference-contexts: Section 4 reports simulation and comparative results in great detail. Discussions and conclusions are presented in last section. 2 Linear Discriminant Functions In this section, we review two linear discriminant functions used in the proposed method. One is the Fisher's linear discriminant function <ref> [17] </ref> and the other is a specific linear discriminant function relevant to the normal density [15]. 2.1 Fisher's Linear Discriminant Function One of the recurring problems encountered in applying statistical techniques to pattern recognition is so-called curse of dimensionality. <p> That is, procedures that are analytically or computationally manageable in low-dimensional spaces can become completely impractical in a space of high-dimensions. In order to attack the problem, Fisher's suggestion <ref> [15, 17] </ref> was to look for the linear function which maximizes the ratio of the between-class scatter to within-class scatter. <p> In the sequel, we shall describe all experimental results in detail. 10 4.1 Classification of Irises The classification of irises is a famous benchmark problem in pattern recognition. Fisher used the data set in his classic paper on discriminant analysis <ref> [17] </ref> and the data set has since become a favorite example in pattern recognition [15]. Irises are classified into three categories: setosa, versicolor and virginica. Each category consists of 50 samples. Each sample possesses four attributes: sepal length, sepal width, petal length and petal width.
Reference: [18] <author> R. Fletcher. </author> <title> Practical methods of optimization. </title> <publisher> John Wiley&Sons, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: For training MLPs in the growing algorithm, the Levenberg-Marquat method, a second-order algorithm <ref> [18, 56] </ref>, was employed for parameter estimation of all subnetworks instead of the standard Back-Propagation (BP) learning algorithm. For the proposed tree-structured hybrid architecture, we denote a generated tree with N H nonterminal nodes and N MLP terminal nodes as the tree with (N H ,N MLP ).
Reference: [19] <author> J. H. Friedman. </author> <title> A recursive partitioning decision rule for nonparametric classification. </title> <journal> IEEE Transactions on Computer, </journal> <volume> 26 </volume> <pages> 404-408, </pages> <year> 1977. </year>
Reference-contexts: On the other hand, supervised learning has been studied for a long time in the pattern recognition community. Decision tree is one of the most efficient tools for supervised learning in the pattern recognition community <ref> [3, 4, 19, 33, 36, 41, 53, 55] </ref>. In general, decision trees are hierarchical structures which use a sequential decision making strategy to handle a supervised learning task. <p> This is very restrictive and can result in a large number of hyperplanes tests, even in a linearly separable problem [36], if the separating hyperplane is not perpendicular to the feature space axes. Indeed there are some decision tree approaches which allow for non-perpendicular hyperplanes <ref> [3, 4, 19, 53, 55] </ref>, but these algorithms used are computationally expensive due to the large search space of hyperplanes that need to be evaluated.
Reference: [20] <author> K. Funahashi. </author> <title> On the approximate realization of continuous mappings by neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 183-192, </pages> <year> 1989. </year>
Reference-contexts: simulation that modular trees yield better generalization and faster training than the MLPs. 4.6 Function Approximation It is well-known that an MLP with sigmoidal activation function can perform the universal approximation of any continuous multivariate function to any desired degree of accuracy, provided that sufficiently many hidden neurons are available <ref> [13, 20, 26] </ref>. To evaluate the universal approximation ability of the proposed method, we performed an experiment by learning a multivariate function approximation task.
Reference: [21] <author> M. Golea and M. Marchand. </author> <title> A growth algorithm for neural network decision trees. </title> <journal> EuroPhysics Letters, </journal> <volume> 12(3) </volume> <pages> 205-210, </pages> <year> 1990. </year>
Reference-contexts: As a result, many researchers in the neural network community have recently considered hybrid structures between decision trees and neural networks. Although these techniques were developed as neural networks whose structure could be automatically determined, their outcome can be interpreted as decision trees with nonlinear splits <ref> [10, 21, 22, 25, 29, 43, 49, 54] </ref>. In most of these hybrid structures, a small neural network at each node of the tree classifier is used to implement nonlinear and multivariate splits instead of a hyperplane in decision tree approaches.
Reference: [22] <author> H. Guo and S. B. Gelfand. </author> <title> Classification trees with neural network feature extraction. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3(6) </volume> <pages> 923-933, </pages> <year> 1992. </year>
Reference-contexts: As a result, many researchers in the neural network community have recently considered hybrid structures between decision trees and neural networks. Although these techniques were developed as neural networks whose structure could be automatically determined, their outcome can be interpreted as decision trees with nonlinear splits <ref> [10, 21, 22, 25, 29, 43, 49, 54] </ref>. In most of these hybrid structures, a small neural network at each node of the tree classifier is used to implement nonlinear and multivariate splits instead of a hyperplane in decision tree approaches. <p> Basically, such neural trees follow the principle underlying decision trees, i.e. non-overlapping (`hard') split in each nonterminal node and only one class label associated with each leaf node <ref> [22, 43, 49, 54] </ref>. For generalization, a pruning procedure is also needed in these neural trees as same as in decision tree approaches such as Classification And Regression Tree (CART) [3]. <p> results produced by resulting modular trees with MT (21-12-3), CART with two different splitting rules [3] and an individual four-layered MLP consisting of 21 neurons in the input layer, 20 neurons in the first hidden layer, 5 neurons in the second hidden layer and 3 neurons in the output layer <ref> [22] </ref> in Figure 11. According to Figure 11, it is shown that modular trees outperform CART in all cases and the four-layered MLP when the number of samples in the training set is 1000, 1250, 1500 and 1750.
Reference: [23] <author> I. Gyuyon, P. Albrecht, Y. LeCun, J. Denker, and W. Hubbard. </author> <title> Applications of neural networks to character recognition. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 5 </volume> <pages> 353-382, </pages> <year> 1991. </year> <month> 17 </month>
Reference-contexts: 1 Introduction Neural networks, particularly Multi-Layered Perceptrons (MLPs), have already been found to be successful for various supervised learning tasks <ref> [1, 11, 23, 24, 44, 39, 42, 45, 48, 50, 51] </ref>. Both theoretical and empirical studies have shown that the neural network is of powerful capabilities for pattern classification and universal approximation which are typical supervised learning tasks.
Reference: [24] <author> S. Haykin and C. Deng. </author> <title> Classification of radar clutter using neural networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2 </volume> <pages> 589-600, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction Neural networks, particularly Multi-Layered Perceptrons (MLPs), have already been found to be successful for various supervised learning tasks <ref> [1, 11, 23, 24, 44, 39, 42, 45, 48, 50, 51] </ref>. Both theoretical and empirical studies have shown that the neural network is of powerful capabilities for pattern classification and universal approximation which are typical supervised learning tasks.
Reference: [25] <author> G. T. Herman and K. T. D. Yeung. </author> <title> On piecewise-linear classification. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 14(7) </volume> <pages> 782-786, </pages> <year> 1992. </year>
Reference-contexts: As a result, many researchers in the neural network community have recently considered hybrid structures between decision trees and neural networks. Although these techniques were developed as neural networks whose structure could be automatically determined, their outcome can be interpreted as decision trees with nonlinear splits <ref> [10, 21, 22, 25, 29, 43, 49, 54] </ref>. In most of these hybrid structures, a small neural network at each node of the tree classifier is used to implement nonlinear and multivariate splits instead of a hyperplane in decision tree approaches.
Reference: [26] <author> K. Hornik, M. Stinchcombe, and H. White. </author> <title> Multilayer feedforward networks are universal approximators. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 359-366, </pages> <year> 1989. </year>
Reference-contexts: Hornik and Irie et al proved that a three-layered MLP with an infinite number of nodes in the hidden layer can also solve arbitrary mapping problems <ref> [26, 27] </ref>. However, the problem of training an MLP is NP-complete [32] and therefore all existing algorithms are heuristics; which results in that the training of an MLP often suffers from a slow convergence property though some methods have been suggested to attack the problem [30, 56]. <p> simulation that modular trees yield better generalization and faster training than the MLPs. 4.6 Function Approximation It is well-known that an MLP with sigmoidal activation function can perform the universal approximation of any continuous multivariate function to any desired degree of accuracy, provided that sufficiently many hidden neurons are available <ref> [13, 20, 26] </ref>. To evaluate the universal approximation ability of the proposed method, we performed an experiment by learning a multivariate function approximation task.
Reference: [27] <author> B. Irie and S. Miyake. </author> <title> Capabilities of three-layered perceptrons. </title> <booktitle> In Proc. IEEE Int. Conf. Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 641-648, </pages> <year> 1988. </year>
Reference-contexts: Hornik and Irie et al proved that a three-layered MLP with an infinite number of nodes in the hidden layer can also solve arbitrary mapping problems <ref> [26, 27] </ref>. However, the problem of training an MLP is NP-complete [32] and therefore all existing algorithms are heuristics; which results in that the training of an MLP often suffers from a slow convergence property though some methods have been suggested to attack the problem [30, 56].
Reference: [28] <author> M. Ishikawa. </author> <title> Structural learning with forgetting. </title> <booktitle> Neural Networks, </booktitle> <volume> 9(3) </volume> <pages> 509-521, </pages> <year> 1996. </year>
Reference-contexts: We also report results produced by the individual MLP with 4-4-3, the method of structural learning with forgetting viewed as a method which can yield better generalization than the standard BP algorithm <ref> [28] </ref> as well as the proposed method for comparison in Table 4.
Reference: [29] <author> K. Ishwar and K. Sethi. </author> <title> Entropy nets: from decision trees to neural networks. </title> <booktitle> Proceedings of IEEE, </booktitle> <volume> 78(10) </volume> <pages> 1605-1613, </pages> <year> 1990. </year>
Reference-contexts: As a result, many researchers in the neural network community have recently considered hybrid structures between decision trees and neural networks. Although these techniques were developed as neural networks whose structure could be automatically determined, their outcome can be interpreted as decision trees with nonlinear splits <ref> [10, 21, 22, 25, 29, 43, 49, 54] </ref>. In most of these hybrid structures, a small neural network at each node of the tree classifier is used to implement nonlinear and multivariate splits instead of a hyperplane in decision tree approaches. <p> The other hybrid techniques between decision trees and neural networks use elaborate methods for converting a decision tree into a neural network and then retraining it <ref> [10, 29] </ref>. However, these techniques can be only used in simple pattern classification problems and their performance for a real world problem remains unknown.
Reference: [30] <author> R. A. Jacobs. </author> <title> Increased rates of convergence through learning rate adaptation. </title> <booktitle> Neural Networks, </booktitle> <volume> 1 </volume> <pages> 295-307, </pages> <year> 1988. </year>
Reference-contexts: However, the problem of training an MLP is NP-complete [32] and therefore all existing algorithms are heuristics; which results in that the training of an MLP often suffers from a slow convergence property though some methods have been suggested to attack the problem <ref> [30, 56] </ref>. In addition, the exact number of hidden layers and neurons in a hidden layer as well as connectivity between layers must be specified before learning can begin.
Reference: [31] <author> M. I. Jordan and R. A. Jacobs. </author> <title> Hierarchical mixture of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 181-214, </pages> <year> 1994. </year>
Reference-contexts: More recently, Jordan and Jacobs presented a tree-structured modular neural network architecture called Hierarchical Mixture of Experts (HME) <ref> [31] </ref>. In the HME architecture, gating networks sit at the nonterminal nodes and expert networks sit at the leaves of the tree. The structures of gating and expert networks could be different. <p> On the other hand, the HME is a modular neural network architecture based upon the principle of divide-and-conquer <ref> [31] </ref>. In contrast with the stacked generalization [58] which makes explicit partitions of the input space, the HME preferentially 15 weights the input space by the posterior probabilities that experts generated the output from the input.
Reference: [32] <author> S. Judd. </author> <title> Learning in networks is hard. </title> <booktitle> In Proc. IEEE Int. Conf. Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 685-692, </pages> <year> 1987. </year>
Reference-contexts: Hornik and Irie et al proved that a three-layered MLP with an infinite number of nodes in the hidden layer can also solve arbitrary mapping problems [26, 27]. However, the problem of training an MLP is NP-complete <ref> [32] </ref> and therefore all existing algorithms are heuristics; which results in that the training of an MLP often suffers from a slow convergence property though some methods have been suggested to attack the problem [30, 56]. <p> Obviously, what these linear discriminant functions do is either to reduce the complexity of the original problem or to transfer the original problem into two smaller problems. As theoretically and empirically shown previously, the training time of an MLP often increases exponentially with the size of the problem <ref> [2, 32, 35] </ref>. Thus, a real world problem (e.g. image processing) will be often intractable when the MLP is directly used.
Reference: [33] <author> B. Kim and D. A. Landgrebe. </author> <title> Hierarchical classifier design in high-dimensional numerous class cases. </title> <journal> IEEE Transactions on Geoscience and Remote Sensing, </journal> <volume> 29(4) </volume> <pages> 518-528, </pages> <year> 1991. </year>
Reference-contexts: On the other hand, supervised learning has been studied for a long time in the pattern recognition community. Decision tree is one of the most efficient tools for supervised learning in the pattern recognition community <ref> [3, 4, 19, 33, 36, 41, 53, 55] </ref>. In general, decision trees are hierarchical structures which use a sequential decision making strategy to handle a supervised learning task.
Reference: [34] <author> K. J. Lang and M. J. Witbrock. </author> <title> Learning to tell two spirals apart. </title> <editor> In D. Touretzky, G. Hinton, and T. Sejnowski, editors, </editor> <booktitle> Proc. The 1988 Connectionist Models Summer School, </booktitle> <pages> pages 52-59, </pages> <address> San Mateo, CA, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Moreover, it seems to be a rather difficult task for typical feedforward neural networks (e.g. MLPs with sigmoidal activation functions). Lang et al could not solve the problem with a standard MLP and had to use additional connections to achieve convergence <ref> [34] </ref>. Fahlman et al used a constructive learning algorithm to solve the problem successfully. The problem has since been popular in the neural network community and has been extensively used for evaluating both nonlinear separability and generalization ability of a neural architecture [16]. <p> To visualize the process of growing a modular tree during training, we diagrammatically show how the input space is split up in Figure 8. In addition, we have also applied the MLP with 2-5-5-5-1 and additional connections suggested by Lang et al <ref> [34] </ref> and the cascade correlation architecture [16] to the two spirals problems in the same workstation. As a result, the CPU time of training modular trees and those architectures are listed in Table 6 for comparison.
Reference: [35] <author> M. Minsky and S. Papert. </author> <title> Perceptrons: an introductionto computational geometry. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1988. </year>
Reference-contexts: Obviously, what these linear discriminant functions do is either to reduce the complexity of the original problem or to transfer the original problem into two smaller problems. As theoretically and empirically shown previously, the training time of an MLP often increases exponentially with the size of the problem <ref> [2, 32, 35] </ref>. Thus, a real world problem (e.g. image processing) will be often intractable when the MLP is directly used. <p> From the standpoint of computational geometry, Minsky et al have shown that some problems (e.g. connectedness) cannot be computed at all in parallel by a diameter-limited or order-limited perceptron; moreover, an MLP seems encounter the same difficulty <ref> [35] </ref>. A salient reason is that such problems are solvable only if the global information is available, while an order-limited perceptron can only capture local information.
Reference: [36] <author> K. V. S. Murthy. </author> <title> On growing better decision trees from data. </title> <type> Ph.D. Thesis, </type> <institution> The Johns Hopkins University, </institution> <year> 1995. </year>
Reference-contexts: On the other hand, supervised learning has been studied for a long time in the pattern recognition community. Decision tree is one of the most efficient tools for supervised learning in the pattern recognition community <ref> [3, 4, 19, 33, 36, 41, 53, 55] </ref>. In general, decision trees are hierarchical structures which use a sequential decision making strategy to handle a supervised learning task. <p> In decision tree approaches, linear discriminant functions or hyperplanes are commonly used for test and decision <ref> [3, 36, 41] </ref>. The traditional approach to training of a decision tree has been to first generate a set of possible hyperplanes and then exhaustively search this set to find the best hyperplanes with respect to some distortion metric [3, 36, 41]. <p> discriminant functions or hyperplanes are commonly used for test and decision <ref> [3, 36, 41] </ref>. The traditional approach to training of a decision tree has been to first generate a set of possible hyperplanes and then exhaustively search this set to find the best hyperplanes with respect to some distortion metric [3, 36, 41]. In most decision tree approaches, the hyperplanes are constrained to be perpendicular to the feature space axes. <p> In most decision tree approaches, the hyperplanes are constrained to be perpendicular to the feature space axes. This is very restrictive and can result in a large number of hyperplanes tests, even in a linearly separable problem <ref> [36] </ref>, if the separating hyperplane is not perpendicular to the feature space axes. Indeed there are some decision tree approaches which allow for non-perpendicular hyperplanes [3, 4, 19, 53, 55], but these algorithms used are computationally expensive due to the large search space of hyperplanes that need to be evaluated.
Reference: [37] <author> P. M. Murthy and D. W. Aha. </author> <title> UCI Repository of machine learning database. </title> <institution> [http://www.ics.uci.edu/mlearn/MLRepository.html], Department of Information and Computer Science, Irvine, CA: University of California, </institution> <year> 1994. </year>
Reference-contexts: Most of them have been viewed as benchmarks in machine learning <ref> [37] </ref> and a function approximation problem has also been used to evaluate the performance of the proposed hybrid architecture. All of these problems were solved on a SUN Sparc II workstation and programs were written by C language. <p> the problem spent so long time that the training had to be terminated once a threshold of iterations reached [47]. 4.5 Image Segmentation The image segmentation data was collected by Brodley at university of Massachusetts and has become a benchmark for machine learning in UCI Repository of machine learning database <ref> [37] </ref>. The instances were drawn randomly from a database of 7 outdoor images. The images were manually segmented to create a classification for every pixel. Each instance is a 3 fi 3 region. The feature vector consists of 19 continuous attributes associated with region, density, contrast and intensity etc. <p> Each instance is a 3 fi 3 region. The feature vector consists of 19 continuous attributes associated with region, density, contrast and intensity etc. All data were classified into 7 categories: brickface, sky, foliage, cement, window, path and grass. In the database <ref> [37] </ref>, all data have been explicitly divided into two sets, i.e training set and test set. There are 210 samples (30 instances/class) in the training set and there are 2100 samples (300 instances/class) in the test set. According to the statement on the data [37], no result on the data has <p> In the database <ref> [37] </ref>, all data have been explicitly divided into two sets, i.e training set and test set. There are 210 samples (30 instances/class) in the training set and there are 2100 samples (300 instances/class) in the test set. According to the statement on the data [37], no result on the data has been published yet [37]. We have applied the proposed method to the image segmentation problem to evaluate the generalization ability of resulting modular trees. In our experiments, the architecture of subnetworks was chosen as the MLP with either 19-17-7 or 19-21-7. <p> There are 210 samples (30 instances/class) in the training set and there are 2100 samples (300 instances/class) in the test set. According to the statement on the data <ref> [37] </ref>, no result on the data has been published yet [37]. We have applied the proposed method to the image segmentation problem to evaluate the generalization ability of resulting modular trees. In our experiments, the architecture of subnetworks was chosen as the MLP with either 19-17-7 or 19-21-7.
Reference: [38] <author> J. P. Nadal. </author> <title> New algorithms for feedforward networks. </title> <editor> In Theumann and Kiberle, editors, </editor> <booktitle> Neural Networks and Spin Glasses, </booktitle> <pages> pages 80-88. </pages> <publisher> World Scientific, </publisher> <year> 1989. </year>
Reference-contexts: Thus, the network architecture must be determined by trial and error. To overcome the difficulty in determining a neural network architecture prior to training, practical approaches for dynamic neural network architecture generation have been sought <ref> [16, 38, 52] </ref>. However, these models do not specify in what exact sequence a neuron should be added to give the maximum effect in classifying training examples and keep the slow convergence property since they still suffer from serious catastrophic interference in both spatial and temporal crosstalk during training.
Reference: [39] <author> K. S. Narendra and K. Parthasarathy. </author> <title> Indentification and control of dynamical systems using neural networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1 </volume> <pages> 4-27, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction Neural networks, particularly Multi-Layered Perceptrons (MLPs), have already been found to be successful for various supervised learning tasks <ref> [1, 11, 23, 24, 44, 39, 42, 45, 48, 50, 51] </ref>. Both theoretical and empirical studies have shown that the neural network is of powerful capabilities for pattern classification and universal approximation which are typical supervised learning tasks.
Reference: [40] <author> Y. Park. </author> <title> A comparison of neural net classifiers and linear tree classifiers: their similarities and differences. </title> <journal> Pattern Recognition, </journal> <volume> 27(11) </volume> <pages> 1493-1503, </pages> <year> 1994. </year>
Reference-contexts: In comparison with neural networks, however, merits of decision trees are that there is no problem of determining an architecture prior to training in design of decision tree and the performance of a linear decision tree is basically similar to an MLP's for a specific task <ref> [12, 40] </ref>. As a result, many researchers in the neural network community have recently considered hybrid structures between decision trees and neural networks.
Reference: [41] <author> Y. Park and J. Sklansky. </author> <title> Automated design of linear tree classifiers. </title> <journal> Pattern Recognition, </journal> <volume> 23(12) </volume> <pages> 1393-1412, </pages> <year> 1990. </year>
Reference-contexts: On the other hand, supervised learning has been studied for a long time in the pattern recognition community. Decision tree is one of the most efficient tools for supervised learning in the pattern recognition community <ref> [3, 4, 19, 33, 36, 41, 53, 55] </ref>. In general, decision trees are hierarchical structures which use a sequential decision making strategy to handle a supervised learning task. <p> In decision tree approaches, linear discriminant functions or hyperplanes are commonly used for test and decision <ref> [3, 36, 41] </ref>. The traditional approach to training of a decision tree has been to first generate a set of possible hyperplanes and then exhaustively search this set to find the best hyperplanes with respect to some distortion metric [3, 36, 41]. <p> discriminant functions or hyperplanes are commonly used for test and decision <ref> [3, 36, 41] </ref>. The traditional approach to training of a decision tree has been to first generate a set of possible hyperplanes and then exhaustively search this set to find the best hyperplanes with respect to some distortion metric [3, 36, 41]. In most decision tree approaches, the hyperplanes are constrained to be perpendicular to the feature space axes.
Reference: [42] <author> D. A. Pomerleau. </author> <title> Nerual network perception for mobile robot guidance. </title> <type> Ph.D. Thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1992. </year>
Reference-contexts: 1 Introduction Neural networks, particularly Multi-Layered Perceptrons (MLPs), have already been found to be successful for various supervised learning tasks <ref> [1, 11, 23, 24, 44, 39, 42, 45, 48, 50, 51] </ref>. Both theoretical and empirical studies have shown that the neural network is of powerful capabilities for pattern classification and universal approximation which are typical supervised learning tasks.
Reference: [43] <author> F. DAlche-Buc, D. Zwierski, and J. P. Nadal. </author> <title> Trio learning: A new strategy for building hybrid neural trees. </title> <journal> International Journal of Neural Systems, </journal> <volume> 5(4) </volume> <pages> 259-274, </pages> <year> 1994. </year>
Reference-contexts: As a result, many researchers in the neural network community have recently considered hybrid structures between decision trees and neural networks. Although these techniques were developed as neural networks whose structure could be automatically determined, their outcome can be interpreted as decision trees with nonlinear splits <ref> [10, 21, 22, 25, 29, 43, 49, 54] </ref>. In most of these hybrid structures, a small neural network at each node of the tree classifier is used to implement nonlinear and multivariate splits instead of a hyperplane in decision tree approaches. <p> Basically, such neural trees follow the principle underlying decision trees, i.e. non-overlapping (`hard') split in each nonterminal node and only one class label associated with each leaf node <ref> [22, 43, 49, 54] </ref>. For generalization, a pruning procedure is also needed in these neural trees as same as in decision tree approaches such as Classification And Regression Tree (CART) [3].
Reference: [44] <author> Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, and W. Hubbard. </author> <title> Handwritten digit recognition with a back-propagation network. </title> <editor> In D. S. Touretsky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <pages> pages 396-404, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1 Introduction Neural networks, particularly Multi-Layered Perceptrons (MLPs), have already been found to be successful for various supervised learning tasks <ref> [1, 11, 23, 24, 44, 39, 42, 45, 48, 50, 51] </ref>. Both theoretical and empirical studies have shown that the neural network is of powerful capabilities for pattern classification and universal approximation which are typical supervised learning tasks.
Reference: [45] <author> A. Rajavelu, M. Musavi, and M. Shivaikar. </author> <title> A neural network approach to character recognition. </title> <booktitle> Neural Networks, </booktitle> <volume> 2(5) </volume> <pages> 387-394, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction Neural networks, particularly Multi-Layered Perceptrons (MLPs), have already been found to be successful for various supervised learning tasks <ref> [1, 11, 23, 24, 44, 39, 42, 45, 48, 50, 51] </ref>. Both theoretical and empirical studies have shown that the neural network is of powerful capabilities for pattern classification and universal approximation which are typical supervised learning tasks.
Reference: [46] <author> B. D. Ripley. </author> <title> Pattern Recognition and Neural Networks. </title> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: Although some statistical techniques have been recently borrowed for 1 Corresponding author. E-mail: kchen@cis.ohio-state.edu. 1 model selection <ref> [1, 46, 57] </ref>, most of them are involved in a time-consuming procedure for practical use. Thus, the network architecture must be determined by trial and error.
Reference: [47] <author> A. J. Robinson. </author> <title> Dynamic error propagation networks. </title> <type> Ph.D. Thesis, </type> <institution> University of Cambridge, </institution> <year> 1989. </year>
Reference-contexts: Each speaker thus yielded six frames of speech from 11 vowels. This gave 990 frames from the 15 speakers. Robinson used this data to investigate several types of neural network algorithms and classic classifiers <ref> [47] </ref>. He used 528 frames from 4 male and 4 female speakers to train the networks and used the remaining 462 frames from 4 male and 3 female speakers to test the performance. <p> In our experiments, the architecture of subnetworks was chosen as the MLP with either 10-18-11 or 10-20-11 and all parameters used in the growing algorithm are listed in Table 10. We generated several modular trees corresponding to different overlapping factors with the same data used by Robinson <ref> [47] </ref> and thereafter used his test data to evaluate the generalization ability of modular trees. As a result, all architectures of resulting modular trees are shown in Table 11 and error rates produced by modular trees corresponding to different overlapping factors are illustrated in Figure 12. <p> The architecture of the modular tree, MT (10-18-11), producing the best result (corresponding to the overlapping factor = 0:55) is illustrated in Figure 13. In addition, the CPU time of generating modular trees with different overlapping factors are listed in Table 12, and both results in <ref> [47] </ref> and ours are shown in Table 13 for comparison. It is evident from the simulation that the proposed method outperforms classical classifiers. <p> We could conjecture that the proposed method also yields significantly faster training than MLPs since Robinson reported that the training of an MLP for the problem spent so long time that the training had to be terminated once a threshold of iterations reached <ref> [47] </ref>. 4.5 Image Segmentation The image segmentation data was collected by Brodley at university of Massachusetts and has become a benchmark for machine learning in UCI Repository of machine learning database [37]. The instances were drawn randomly from a database of 7 outdoor images.
Reference: [48] <author> D. Rumelhart and J. McClelland. </author> <title> Parallel Distributed Processing. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: 1 Introduction Neural networks, particularly Multi-Layered Perceptrons (MLPs), have already been found to be successful for various supervised learning tasks <ref> [1, 11, 23, 24, 44, 39, 42, 45, 48, 50, 51] </ref>. Both theoretical and empirical studies have shown that the neural network is of powerful capabilities for pattern classification and universal approximation which are typical supervised learning tasks.
Reference: [49] <author> A. Sankar and R. J. Mammone. </author> <title> Growing and pruning neural tree networks. </title> <journal> IEEE Transactions on Computer, </journal> <volume> 42(3) </volume> <pages> 291-299, </pages> <year> 1993. </year>
Reference-contexts: As a result, many researchers in the neural network community have recently considered hybrid structures between decision trees and neural networks. Although these techniques were developed as neural networks whose structure could be automatically determined, their outcome can be interpreted as decision trees with nonlinear splits <ref> [10, 21, 22, 25, 29, 43, 49, 54] </ref>. In most of these hybrid structures, a small neural network at each node of the tree classifier is used to implement nonlinear and multivariate splits instead of a hyperplane in decision tree approaches. <p> Basically, such neural trees follow the principle underlying decision trees, i.e. non-overlapping (`hard') split in each nonterminal node and only one class label associated with each leaf node <ref> [22, 43, 49, 54] </ref>. For generalization, a pruning procedure is also needed in these neural trees as same as in decision tree approaches such as Classification And Regression Tree (CART) [3].
Reference: [50] <author> T. J. Sejnowski and C. R. Resenberg. </author> <title> Parallel networks that learn to pronounce English text. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 145-168, </pages> <year> 1987. </year>
Reference-contexts: 1 Introduction Neural networks, particularly Multi-Layered Perceptrons (MLPs), have already been found to be successful for various supervised learning tasks <ref> [1, 11, 23, 24, 44, 39, 42, 45, 48, 50, 51] </ref>. Both theoretical and empirical studies have shown that the neural network is of powerful capabilities for pattern classification and universal approximation which are typical supervised learning tasks.
Reference: [51] <author> T. J. Sejnowski, B. P. Yuhas, M. H. Goldstein, and R. E. Jenkins. </author> <title> Combining visual and acoustic speech signals with a neural network improves intelligibility. </title> <editor> In D. S. Touretsky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <pages> pages 232-239, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1 Introduction Neural networks, particularly Multi-Layered Perceptrons (MLPs), have already been found to be successful for various supervised learning tasks <ref> [1, 11, 23, 24, 44, 39, 42, 45, 48, 50, 51] </ref>. Both theoretical and empirical studies have shown that the neural network is of powerful capabilities for pattern classification and universal approximation which are typical supervised learning tasks.
Reference: [52] <author> R. S. Shadafan and M. Niranjan. </author> <title> A dynamic neural network architecture by sequential partitioning of the input space. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 1202-1222, </pages> <year> 1994. </year>
Reference-contexts: Thus, the network architecture must be determined by trial and error. To overcome the difficulty in determining a neural network architecture prior to training, practical approaches for dynamic neural network architecture generation have been sought <ref> [16, 38, 52] </ref>. However, these models do not specify in what exact sequence a neuron should be added to give the maximum effect in classifying training examples and keep the slow convergence property since they still suffer from serious catastrophic interference in both spatial and temporal crosstalk during training.
Reference: [53] <author> Q. Y. Shi and K. S. Fu. </author> <title> A method for the design of binary tree classifiers. </title> <journal> Pattern Recognition, </journal> <volume> 16 </volume> <pages> 593-603, </pages> <year> 1983. </year>
Reference-contexts: On the other hand, supervised learning has been studied for a long time in the pattern recognition community. Decision tree is one of the most efficient tools for supervised learning in the pattern recognition community <ref> [3, 4, 19, 33, 36, 41, 53, 55] </ref>. In general, decision trees are hierarchical structures which use a sequential decision making strategy to handle a supervised learning task. <p> This is very restrictive and can result in a large number of hyperplanes tests, even in a linearly separable problem [36], if the separating hyperplane is not perpendicular to the feature space axes. Indeed there are some decision tree approaches which allow for non-perpendicular hyperplanes <ref> [3, 4, 19, 53, 55] </ref>, but these algorithms used are computationally expensive due to the large search space of hyperplanes that need to be evaluated.
Reference: [54] <author> J. A. Sirat and J. P. Nadal. </author> <title> Neural tree: A new tool for classification. Network: </title> <booktitle> Computation in Neural Systems, </booktitle> <volume> 1(4) </volume> <pages> 423-438, </pages> <year> 1990. </year>
Reference-contexts: As a result, many researchers in the neural network community have recently considered hybrid structures between decision trees and neural networks. Although these techniques were developed as neural networks whose structure could be automatically determined, their outcome can be interpreted as decision trees with nonlinear splits <ref> [10, 21, 22, 25, 29, 43, 49, 54] </ref>. In most of these hybrid structures, a small neural network at each node of the tree classifier is used to implement nonlinear and multivariate splits instead of a hyperplane in decision tree approaches. <p> Basically, such neural trees follow the principle underlying decision trees, i.e. non-overlapping (`hard') split in each nonterminal node and only one class label associated with each leaf node <ref> [22, 43, 49, 54] </ref>. For generalization, a pruning procedure is also needed in these neural trees as same as in decision tree approaches such as Classification And Regression Tree (CART) [3].
Reference: [55] <author> J. Sklansky and G. N. Wassel. </author> <title> Pattern Classifiers and Trainable Machines. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: On the other hand, supervised learning has been studied for a long time in the pattern recognition community. Decision tree is one of the most efficient tools for supervised learning in the pattern recognition community <ref> [3, 4, 19, 33, 36, 41, 53, 55] </ref>. In general, decision trees are hierarchical structures which use a sequential decision making strategy to handle a supervised learning task. <p> This is very restrictive and can result in a large number of hyperplanes tests, even in a linearly separable problem [36], if the separating hyperplane is not perpendicular to the feature space axes. Indeed there are some decision tree approaches which allow for non-perpendicular hyperplanes <ref> [3, 4, 19, 53, 55] </ref>, but these algorithms used are computationally expensive due to the large search space of hyperplanes that need to be evaluated.
Reference: [56] <author> P. Patrick Van Der Smagt. </author> <title> Minimization methods for training feedforward neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 7(1) </volume> <pages> 1-11, </pages> <year> 1994. </year>
Reference-contexts: However, the problem of training an MLP is NP-complete [32] and therefore all existing algorithms are heuristics; which results in that the training of an MLP often suffers from a slow convergence property though some methods have been suggested to attack the problem <ref> [30, 56] </ref>. In addition, the exact number of hidden layers and neurons in a hidden layer as well as connectivity between layers must be specified before learning can begin. <p> For training MLPs in the growing algorithm, the Levenberg-Marquat method, a second-order algorithm <ref> [18, 56] </ref>, was employed for parameter estimation of all subnetworks instead of the standard Back-Propagation (BP) learning algorithm. For the proposed tree-structured hybrid architecture, we denote a generated tree with N H nonterminal nodes and N MLP terminal nodes as the tree with (N H ,N MLP ).
Reference: [57] <author> G. Wahba. </author> <title> Generalization and regularization in nonlinear learning systems. </title> <editor> In M. A. Arbib, editor, </editor> <booktitle> The Handbook of Brain Theory and Neural Networks, </booktitle> <pages> pages 426-430, </pages> <address> Cambridge, 1995. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Although some statistical techniques have been recently borrowed for 1 Corresponding author. E-mail: kchen@cis.ohio-state.edu. 1 model selection <ref> [1, 46, 57] </ref>, most of them are involved in a time-consuming procedure for practical use. Thus, the network architecture must be determined by trial and error.

References-found: 57

