URL: http://www.aic.nrl.navy.mil/papers/1996/AIC-96-014.ps.Z
Refering-URL: http://www.aic.nrl.navy.mil/~aha/pub-details.html
Root-URL: 
Title: Simplifying Decision Trees: A Survey  
Author: Leonard A. Breslow and David W. Aha 
Keyword: decision tree, survey, simplification, classification, case retrieval  
Address: Washington, DC 20375  
Affiliation: Navy Center for Applied Research in Artificial Intelligence Naval Research Laboratory  
Email: fbreslow,ahag@aic.nrl.navy.mil  
Phone: (202) f404-7736,767-9006g FAX: (202) 767-3172  
Web: http://www.aic.nrl.navy.mil/~fbreslow,ahag/  
Abstract: Induced decision trees are an extensively-researched solution to classification tasks. For many practical tasks, the trees produced by tree-generation algorithms are not comprehensible to users due to their size and complexity. Although many tree induction algorithms have been shown to produce simpler, more comprehensible trees (or data structures derived from trees) with good classification accuracy, tree simplification has usually been of secondary concern relative to accuracy and no attempt has been made to survey the literature from the perspective of simplification. We present a framework that organizes the approaches to tree simplification and summarize and critique the approaches within this framework. The purpose of this survey is to provide researchers and practitioners with a concise overview of tree-simplification approaches and insight into their relative capabilities. In our final discussion, we briefly describe some empirical findings and discuss the application of tree induction algorithms to case retrieval in case-based reasoning systems.
Abstract-found: 1
Intro-found: 1
Reference: <author> Almuallim, H. and T. G. </author> <title> Dietterich (1991). Learning with many irrelevant features. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 547-552). </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Auer, P., R. C. Holte, and W. </author> <title> Maass (1995). Theory and applications of agnostic PAC-learning with small decision trees. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning (pp. </booktitle> <pages> 21-29). </pages> <address> Tahoe City, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In experiments with 20 data sets, he found that R8 generally increases classification accuracy and reduces tree size relative to release 7 (R7). Quinlan also compared C4.5 R8 with Dougherty et al. 's (1995) global discretization method and with T2 <ref> (Auer et al. 1995) </ref>. On average, C4.5 R8 produced more accurate, but larger, trees than these methods. However, on small data sets, global discretization was superior to R8 in both accuracy and tree size. <p> + ID2-of-3 + XofN + Hypothesis-driven FRINGE + Modified Test Search Selection Measures MDL + Kolmogorov-Smirnoff C-SEP + Lubinsky's (1995) hybrid Discretization Global Discretization + T2 Lookahead LFC + Database Restrictions Case Selection RobustC4.5 Feature Selection SET-Gen Alternative Data Decision Graphs EODG Structure EDAG Rules C4.5rules + RULER + T2 <ref> (Auer et al. 1995) </ref>, LMDT (Brodley and Utgoff 1995), XofN (Zheng 1995), and SET-Gen (Cherkauer and Shavlik 1996) on eight data sets. Below we summarize highlights from our survey and comparison study according to each of the five top-level categories in our framework. <p> Promising new test selection measures include MDL, KS, and, especially, C-SEP. Also, Lubinsky's hybrid measure drastically reduced tree sizes at the price of small sacrifices in accuracy. Methods for discretizing continuous features, such as Dougherty et al. 's (1995) global and T2's local discretiza- tion <ref> (Auer et al. 1995) </ref> show great promise for reducing tree size for data sets containing many continuous variables. Finally, LFC's lookahead search method produced dramatic improvements in accuracy, as well as reductions in size.
Reference: <author> Auriol, E., W. S., M. Manago, K. D. Althoff, and Traphoner, R. </author> <year> (1995). </year> <title> Inreca: A seamlessly integrated system based on inductive inference and case-based reasoning. </title> <booktitle> In Proceedings of the First International Conference on Case-Based Reasoning (pp. </booktitle> <address> Sesimbra, Portugal. </address>
Reference-contexts: Approximately 15 organizations are currently marketing commercial CBR software. Several CBR shells use decision trees to index cases, including ReMind (Cognitive Systems Inc.), Kate (AcknoSoft), The Easy Reasoner (The Haley Enterprise), and Knowledge Builder (Service- Soft). Some CBR research software also uses decision trees (e.g., INRECA <ref> (Auriol et al. 1995) </ref>). One motivation for simplifying decision trees in this context is to provide understandable explanations of CBR solutions. CBR systems typically generate precedent explanations, derived from the retrieved cases and the similarity measure. In addition, some systems provide more general characteristic explanations.
Reference: <author> Barletta, R. </author> <year> (1994). </year> <title> A hybrid indexing and retrieval strategy for advisory CBR systems built with ReMind. </title> <booktitle> In Proceedings of the Second European Workshop on Case-Based Reasoning (pp. </booktitle> <pages> 49-58). </pages> <address> Chantilly, France: </address> <note> Unpublished. </note>
Reference-contexts: Our own interest in decision tree simplification stems from our interest in developing a practical case-based reasoning (CBR) tool. Our effort follows that of others that use decision trees to retrieve stored cases (e.g., ReMind <ref> (Barletta 1994) </ref>). We will discuss the applicability of the decision tree literature we review to the development of tree-based case retrieval in Section 4.2. In Section 2, we review decision tree induction and discuss issues concerning tree simplification. <p> These approaches retain the training cases and use the decision tree to retrieve the cases stored at the leaf node corresponding to the current test case. The cases retrieved are then subjected to a further classifier, such as k-nearest neighbor <ref> (Barletta 1994) </ref> or kernel density estimators (Smyth et al. 1995). Both approaches substantially improve accuracy over decision trees alone. This suggests the possibility of over-pruning a decision 17 tree within a hybrid algorithm without sacrificing accuracy.
Reference: <author> Bohanec, M. and I. </author> <title> Bratko (1994). Trading accuracy for simplicity in decision trees. </title> <journal> Machine Learning, </journal> <volume> 15, </volume> <pages> 223-250. </pages>
Reference-contexts: However, more conservative reductions in size can sometimes produce significant, if small, improvements in accuracy (Elomaa 1994) that can be crucial in some practical applications (Danyluk and Provost 1993). Thus, many authors describe a tradeoff between the accuracy and simplicity of decision trees <ref> (e.g., Bohanec and Bratko 1994) </ref> and some have proposed tree construction methods that favor one or the other criterion.
Reference: <author> Breiman, L., J. H. Friedman, R. A. Olshen, and C. J. </author> <title> Stone (1984). Classification and regression trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: However, for many practical applications, it is desirable that the classifier "provide insight and understanding into the predictive structure of the data" <ref> (Breiman et al. 1984) </ref>, as well as explanations of its individual predictions (Michie 1990). Decision tree induction has been extensively studied in the machine learning and statistics communities as a solution to classification tasks (Breiman et al. 1984; Quinlan 1986; 1993a). <p> This is frequently accomplished by defining eval () using an information-theoretic (Quinlan 1986) or statistical measure <ref> (Breiman et al. 1984) </ref>. In most research on classification, the database is divided into a training set and a test set. The training set is used to generate the decision tree, while the test set is used to assess its predictive accuracy. <p> Pruning Method Set Strategy Characteristics Minimal Cost-Complexity (MCCP) 0-SE: yes bottom-up Can use cross-validation (0-SE, 1-SE, CV) 1-SE: yes (any internal or standard error to <ref> (Breiman et al. 1984) </ref> CV: no node) estimate accuracy. Reduced Error Pruning (REP) yes bottom-up Finds the optimally pruned (Quinlan 1987a) tree wrt the pruning set. Minimum Error Pruning (MEP-2) yes bottom-up Based on the m-probability (Cestnik & Bratko 1991) estimate of the error rate. <p> FRINGE uses a tree induction algorithm that is a hybrid of ID3 and CART <ref> (Breiman et al. 1984) </ref> with reduced error post-pruning (Quinlan 1987a). The induction- construction cycle iterates until no new features are added or a specified maximum number of features has been reached. FRINGE was tested on nine data sets with irrelevant features and various amounts of noise added. <p> All of these methods modify the best test and partition function in the generic decision tree induction algorithm displayed (Figure 2). 24 3.3.1 Alternative Test Selection Measures Minimum Description Length. Many test selection measures have been proposed, including information gain (Quinlan 1986) and statistical measures of purity <ref> (Breiman et al. 1984) </ref>. In a comparative study, Mingers (1989b) found no significant differences in accuracy for several popular test selection measures. 12 However, some selection measures might be superior in producing smaller trees. <p> Lubinsky (1995) and Brodley (1995b) both studied resub- stitution accuracy as a test selection measure. Because Pazzani et al. (1994) found that using it as the selection measure reduces predictive accuracy, these authors integrated it with a "traditional" test selection measure; Lubinsky used the Gini criterion <ref> (Breiman et al. 1984) </ref> and Brodley used information gain (Quinlan 1986). Both authors report small gains in predictive accuracy when the traditional measure is used for nodes close to the root and an accuracy measure (Brodley) or a hybrid accuracy/traditional measure (Lubinsky) is used for nodes closer to the leaves.
Reference: <author> Breslow, L. and D. W. </author> <title> Aha (1997). Comparing tree-simplification procedures. </title> <booktitle> To appear in Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics. </booktitle> <address> Ft. Lauderdale, FL: </address> <note> Unpublished. </note>
Reference-contexts: In particular, Section 4.1 discusses promising tree-simplification strategies, both as suggested from our survey and from our recent empirical comparison <ref> (Breslow and Aha 1997) </ref>, while Section 4.2 describes our plans for enhancing a case-based reasoning shell. 4.1 Discussion Based on our literature survey, Table 4 lists those algorithms that performed particularly well with regard to simplifying trees. <p> Some of the included algorithms were noteworthy for either enhancing or diminishing accuracy; we indicate this by a + or respectively in the Accuracy column in the table. We empirically compared some of the more promising tree simplification procedures; the details are reported in <ref> (Breslow and Aha 1997) </ref>. <p> Future research will assess the advantages of combining tree simplification methods. For instance, combined-method algorithms may be less biased than single-method algorithms, performing well in a wider range of tasks. 4.2 Case-based reasoning This survey summarizes various methods available for simplifying decision trees, while our empirical comparison <ref> (Breslow and Aha 1997) </ref> evaluates their relative merits. Our ultimate objective for this research is to determine which approaches might prove useful in the design of commercial case-based reasoning (CBR) systems.
Reference: <author> Brodley, C. E. </author> <year> (1993). </year> <title> Addressing the selective superiority problem: Automatic algorithm/model class selection. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning (pp. </booktitle> <pages> 17-24). </pages> <address> Amherst, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Brodley, C. E. </author> <year> (1995a). </year> <title> Recursive automatic bias selection for classifier construction. </title> <journal> Machine Learning, </journal> <volume> 20, </volume> <pages> 63-94. </pages> <note> 39 Brodley, </note> <author> C. E. </author> <year> (1995b). </year> <title> Automatic selection of split criterion during tree growing based on node location. </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning (pp. </booktitle> <pages> 73-80). </pages> <address> Tahoe City, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Brodley, C. E. and P. E. </author> <title> Utgoff (1995). Multivariate decision trees. </title> <journal> Machine Learning, </journal> <volume> 19, </volume> <pages> 45-77. </pages>
Reference-contexts: Some of these are worth mentioning. First, while most tree generation algorithms induce univariate trees, where each test assesses the value of a single feature, several researchers have recently proposed methods for inducing multivari- ate trees <ref> (e.g., Brodley and Utgoff 1995) </ref>, where tests can be functions on multiple case features. Second, a typically-adopted constraint is that a test cannot be reused along a tree path. <p> These algorithms also differ in their representational biases. For example, many algorithms assume that the model is expressible as a disjunctive normal form expression in which the tests are univariate functions on case features, while other algorithms assume that tests can be linear combinations of feature values <ref> (e.g., Brodley and Utgoff 1995) </ref>. Naturally, some representations are more appropriate for some tasks than others. If no inferences can be made concerning what biases are best for a given database, then all possible algorithms must be compared to determine which performs best for it (Schaffer 1993). <p> Hypothesis-driven FRINGE + Modified Test Search Selection Measures MDL + Kolmogorov-Smirnoff C-SEP + Lubinsky's (1995) hybrid Discretization Global Discretization + T2 Lookahead LFC + Database Restrictions Case Selection RobustC4.5 Feature Selection SET-Gen Alternative Data Decision Graphs EODG Structure EDAG Rules C4.5rules + RULER + T2 (Auer et al. 1995), LMDT <ref> (Brodley and Utgoff 1995) </ref>, XofN (Zheng 1995), and SET-Gen (Cherkauer and Shavlik 1996) on eight data sets. Below we summarize highlights from our survey and comparison study according to each of the five top-level categories in our framework.
Reference: <author> Buntine, W. </author> <year> (1992). </year> <title> Learning classification trees. </title> <journal> Statistics and Computing, </journal> <volume> 2, </volume> <pages> 63-73. </pages>
Reference: <author> Buntine, W. and T. </author> <title> Niblett (1992). A further comparison of splitting rules for decision-tree induc-tion. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 75-86. </pages>
Reference: <author> Catlett, J. </author> <year> (1991). </year> <title> On changing continuous attributes into ordered discrete attributes. </title> <booktitle> In Proceedings of the European Working Session on Learning (pp. </booktitle> <pages> 164-178). </pages> <address> Berlin: </address> <publisher> Springer Verlag. </publisher>
Reference: <author> Cendrowska, J. </author> <year> (1987). </year> <title> PRISM: An algorithm for inducing modular rules. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27, </volume> <pages> 349-370. </pages>
Reference-contexts: Gaines proposed a common complexity measure for trees, rule sets, and EDAGs that accounts for the number of nodes, edges, and conditions. Based on this measure, Gaines found EDAGs to be less complex than decision trees (including ID3 and C4.5) and rule sets induced by Prism <ref> (Cendrowska 1987) </ref> and by C4.5rules (Section 3.5.2) when compared on three tasks (Gaines 1996; Gaines 1995). At present, EDAGs have been tested on only three small noise-free tasks; it is unknown how they perform on larger, noisy tasks.
Reference: <author> Cestnik, B. and I. </author> <title> Bratko (1991). On estimating proabilities in tree pruning. </title> <booktitle> In Proceedings of the Fifth European Working Session on Learning (pp. </booktitle> <pages> 138-150). </pages> <address> Porto, Portugal: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Reduced Error Pruning (REP) yes bottom-up Finds the optimally pruned (Quinlan 1987a) tree wrt the pruning set. Minimum Error Pruning (MEP-2) yes bottom-up Based on the m-probability <ref> (Cestnik & Bratko 1991) </ref> estimate of the error rate. Critical Value Pruning (CVP) yes bottom-up Uses a critical value (Mingers 1989a) threshold on the test selection measure Pessimistic Error Pruning (PEP) no top-down Uses the continuity (Quinlan 1987a) correction.
Reference: <author> Cheng, J., U. M. Fayyad, K. B. Irani, and Z. </author> <title> Qian (1988). Improved decision trees: A generalized version of ID3. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning (pp. </booktitle> <pages> 100-106). </pages> <address> Ann Arbor, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Finally, a minimal subset of the rules that covers the entire training sample is selected using a greedy covering algorithm. RULER can use any tree induction algorithm. In their evaluation, the authors used O-BTree (Fayyad and Irani 1992, see Section 3.3) and an improved version of GID3 <ref> (Cheng et al. 1988) </ref> named GID3*. Tested on a large astronomy data set, both RULER variants produced more accurate rule sets with lower complexity than trees produced by O-BTree, GID3*, or ID3. We did not find studies that directly compare C4.5rules with RULER.
Reference: <author> Cherkauer, K. J. and J. W. </author> <title> Shavlik (1996). Growing simpler decision trees to facilitate knowledge discovery. </title> <booktitle> In Proceedings of the Knowledge Discovery and Data Mining Conference. </booktitle> <address> Portland, OR: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: + Kolmogorov-Smirnoff C-SEP + Lubinsky's (1995) hybrid Discretization Global Discretization + T2 Lookahead LFC + Database Restrictions Case Selection RobustC4.5 Feature Selection SET-Gen Alternative Data Decision Graphs EODG Structure EDAG Rules C4.5rules + RULER + T2 (Auer et al. 1995), LMDT (Brodley and Utgoff 1995), XofN (Zheng 1995), and SET-Gen <ref> (Cherkauer and Shavlik 1996) </ref> on eight data sets. Below we summarize highlights from our survey and comparison study according to each of the five top-level categories in our framework. Among the techniques that explicitly control tree size, post-pruning methods have shown the greatest promise to date.
Reference: <author> Cockett, J. R. B. and J. A. </author> <title> Herrera (1990). Decision tree reduction. </title> <journal> Journal of the ACM, </journal> <volume> 37(4), </volume> <pages> 815-842. </pages>
Reference: <author> Cohen, P. R. and D. </author> <title> Jensen (1997). Overfitting explained. </title> <booktitle> In Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics. </booktitle> <address> Ft. Lauderdale, FL. </address>
Reference-contexts: Noise can cause some irrelevant features to be included among the selected tests. This in turn causes overfitting <ref> (Cohen and Jensen 1997) </ref>, where trees model both the target concept and the inherent noise. This is a pervasive problem given that cases recorded from most applications will contain some degree of noise. <p> In our study, the post-pruning methods used in recent releases of C4.5 (EBP) and in ITI (MDL) compared favorably to other methods for reducing tree size, although the C4.5 variants were somewhat superior to ITI in maintaining accuracy. The recently introduced post-pruning method based on MT theory <ref> (Cohen and Jensen 1997) </ref> also appears quite promising. In contrast to post-pruning, pre-pruning methods have been largely abandoned in recent years. One exception is T2, which limits tree size to a depth of two. We found that T2 output the smallest-size trees, but scored low accuracies on several data sets.
Reference: <author> Cohen, W. W. </author> <year> (1995). </year> <title> Fast effective rule induction. </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning (pp. </booktitle> <pages> 115-123). </pages> <address> Tahoe City, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We did not find studies that directly compare C4.5rules with RULER. Unfortunately, translating trees to rules can be computationally expensive. C4.5rules is slow for large databases (Daelemans et al. 1997); its complexity is O (n 3 ) <ref> (Cohen 1995) </ref>. One alternative might be to generate rules directly, without initial tree creation. On many tasks, rule learning systems are more accurate than decision tree learners (Pagallo and Haussler 1990; Weiss and Indurkhya 1991). <p> They often produce simpler data structures (Weiss and Indurkhya 1991), although Gaines (1996) found that they increase complexity for two small data sets without noise. While early rule induction algorithms were computationally expensive for large data sets (e.g., O (n 4 ) <ref> (Cohen 1995) </ref>), fast rule induction algorithms have recently been developed that scale much better than C4.5rules (Furnkranz and Widmer 1994; Cohen 1995). One of these (Cohen 1995) attains accuracies comparable to C4.5rules, which is in turn often more accurate than C4.5, and scales nearly linearly with the number of cases. 4 <p> While early rule induction algorithms were computationally expensive for large data sets (e.g., O (n 4 ) <ref> (Cohen 1995) </ref>), fast rule induction algorithms have recently been developed that scale much better than C4.5rules (Furnkranz and Widmer 1994; Cohen 1995). One of these (Cohen 1995) attains accuracies comparable to C4.5rules, which is in turn often more accurate than C4.5, and scales nearly linearly with the number of cases. 4 Discussion and Future Directions This section summarizes our survey and describes some of our future research objectives.
Reference: <author> Crawford, S. L. </author> <year> (1989). </year> <title> Extensions to the CART algorithm. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 31(2), </volume> <pages> 197-217. </pages>
Reference: <author> Daelemans, W., A. van den Bosch, and G. </author> <month> Weijters </month> <year> (1997). </year> <title> IGTree: Using trees for compression and classification in lazy learning algorithms. </title> <note> To appear in Artificial Intelligence Review. </note>
Reference-contexts: We did not find studies that directly compare C4.5rules with RULER. Unfortunately, translating trees to rules can be computationally expensive. C4.5rules is slow for large databases <ref> (Daelemans et al. 1997) </ref>; its complexity is O (n 3 ) (Cohen 1995). One alternative might be to generate rules directly, without initial tree creation. On many tasks, rule learning systems are more accurate than decision tree learners (Pagallo and Haussler 1990; Weiss and Indurkhya 1991). <p> However, EDAGs have been tested on only a few data sets. Conversion to rule sets by RULER yields improvements in both size and accuracy. Similarly, C4.5rules was the best algorithm we tested in terms of overall accuracy and size. While some have reported that it performs slowly <ref> (Daelemans et al. 1997) </ref>, we found it to be quite fast. Many of the methods from different categories can be combined.
Reference: <author> Danyluk, A. P. and F. J. </author> <title> Provost (1993). Small disjuncts in action: Learning to diagnose errors in the local loop of the telephone network. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning (pp. </booktitle> <pages> 81-88). </pages> <address> Amherst, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, more conservative reductions in size can sometimes produce significant, if small, improvements in accuracy (Elomaa 1994) that can be crucial in some practical applications <ref> (Danyluk and Provost 1993) </ref>. Thus, many authors describe a tradeoff between the accuracy and simplicity of decision trees (e.g., Bohanec and Bratko 1994) and some have proposed tree construction methods that favor one or the other criterion.
Reference: <author> Devijver, P. A. and J. </author> <title> Kittler (1982). Pattern recognition: A statistical approach. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: They describe an extension of CART that, at each node, hill- climbs in the space of linear combinations of continuous features to search for a "best" test. Because few features usually contribute significantly to the test at any given node, they use a variant of backward sequential elimination <ref> (Devijver and Kittler 1982) </ref> to discard features from each node's test. To prevent CART from overfitting training data, internal nodes are required to have an above-threshold number of cases.
Reference: <author> Dietterich, T., M. Kearns, and Y. </author> <title> Mansour (1996). Applying the weak learning framework to un-derstand and improve C4.5. </title> <booktitle> To appear in Proceedings of the Thirteenth International Conference on Machine Learning. </booktitle> <address> Bari, Italy: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Doak, J. </author> <year> (1992). </year> <title> An evaluation of feature selection methods and their application to computer security (Technical Report CSE-92-18). </title> <institution> Davis, CA: University of California, Department of Computer Science. </institution>
Reference: <author> Dougherty, J., R. Kohavi, and M. </author> <title> Sahami (1995). Supervised and unsupervied discretization of coninuous features. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning (pp. </booktitle> <pages> 194-202). </pages> <address> Tahoe City, CA: </address> <publisher> Morgan Kaufmann. </publisher> <address> 40 Elomaa, T. </address> <year> (1994). </year> <title> In defense of C4.5: Notes on learning one-level decision trees. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning (pp. </booktitle> <pages> 62-69). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Esposito, F., D. Malerba, and G. </author> <title> Semeraro (1993). Decision tree pruning as a search in the state space. </title> <booktitle> Proceedings of the European Conference on Machine Learning (pp. </booktitle> <pages> 165-184). </pages> <address> Vienna, Austria: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Esposito, F., D. Malerba, and G. </author> <title> Semeraro (1995a). A further study of pruning methods in decision tree induction. </title> <booktitle> Proceedings of the Fifth International Workshop on Artificial Intelligence and Statistics. </booktitle> <pages> (pp. 211-218). </pages> <address> Ft. Lauderdale, FL. </address>
Reference: <author> Esposito, F., D. Malerba, and G. </author> <title> Semeraro (1995b). Simplifying decision trees by pruning and grafting: new results. </title> <booktitle> Proceedings of the European Conference on Machine Learning (pp. 287290). </booktitle> <address> Heraklion, Greece: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Fayyad, U. M. and K. B. </author> <title> Irani (1992). The attribute selection problem in decision tree generation. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 104-110). </pages> <address> San Jose, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Finally, a minimal subset of the rules that covers the entire training sample is selected using a greedy covering algorithm. RULER can use any tree induction algorithm. In their evaluation, the authors used O-BTree <ref> (Fayyad and Irani 1992, see Section 3.3) </ref> and an improved version of GID3 (Cheng et al. 1988) named GID3*. Tested on a large astronomy data set, both RULER variants produced more accurate rule sets with lower complexity than trees produced by O-BTree, GID3*, or ID3.
Reference: <author> Fayyad, U. M. and K. B. </author> <title> Irani (1993). Multi-interval discretization of continuous-valued attributes for classification learning. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 1022-1029). </pages> <address> Chambery, France: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Two approaches have been proposed. The first induces a single decision tree and then prunes the rule set derived from it (Quinlan 1987a, 1987b). The second iteratively generates decision trees, selecting the best rule from each iteration <ref> (Fayyad and Irani 1993) </ref>. We examine each approach in turn. Quinlan's (1993a) C4.5 system includes an option for translating the induced tree to a set of rules. The C4.5rules option consists of the following steps: 1. Tree creation. A tree T is created, without pruning, using C4.5. 2. Translation.
Reference: <author> Feelders, A. and W. </author> <month> Verkooijen </month> <year> (1995). </year> <title> Which method learns most from the data? Methodological issues in the analysis of comparative studies. </title> <booktitle> In Proceedings of the Fifth International Workshop on Artificial Intelligence and Statistics (pp. </booktitle> <pages> 219-225). </pages> <address> Ft. Lauderdale, FL: </address> <note> Unpublished. </note>
Reference-contexts: Selecting a "best" tree: Selecting the tree that maximizes some evaluation function also max <p>- imizes the probability of overfitting. They tested their theory using a post-pruning algorithm that is consistent with MT theory: it uses a Bonferroni adjustment to test statistical significance when evaluating candidate tests <ref> (Feelders and Verkooijen 1995) </ref>. In tests on simple synthesized data sets, their algorithm produced the least overfitting and simplest decision trees among the four algorithms tested. It also had the highest accuracies for tasks where overfitting could occur.
Reference: <author> Fisher, D. H. </author> <year> (1989). </year> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 139-172. </pages>
Reference: <author> Fisher, D. H. and J. C. </author> <month> Schlimmer </month> <year> (1988). </year> <title> Concept simplification and prediction accuracy. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning (pp. </booktitle> <pages> 22-28). </pages> <address> Ann Arbor, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Forsyth, R. S., D. D. Clarke, and R. L. </author> <title> Wright (1994). Overfitting revisited: an informationtheoretic approach to simplifying discrimination trees. </title> <journal> Journal of Experimental Artificial Intelligence, </journal> <volume> 6, </volume> <pages> 289-302. </pages>
Reference: <author> Friedman, J. H. </author> <year> (1977). </year> <title> A recursive partitioning decision rule for nonparametric classification. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 26, </volume> <pages> 404-408. </pages>
Reference: <author> Furnkranz, J. and G. </author> <title> Widmer (1994). Incremental reduced error pruning. </title> <booktitle> In Proceedings of the Eleventh Conference on Machine Learning (pp. </booktitle> <pages> 70-77). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Gaines, B. R. </author> <year> (1989). </year> <title> An ounce of knowledge is worth a ton of data: quantitiatve studies of the trade-off between expertise and data based on statistically well-founded empirical induction. </title> <booktitle> Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <pages> (156-159). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Decision trees and rule sets can be converted to EDAGs by an algorithm that factors common sub-premises from the trees' rules (corresponding to its paths). Alternately, EDAGs can be induced directly using Induct <ref> (Gaines 1989) </ref>. Gaines proposed a common complexity measure for trees, rule sets, and EDAGs that accounts for the number of nodes, edges, and conditions.
Reference: <author> Gaines, B. R. </author> <year> (1995). </year> <title> Structured and unstructured induction with EDAGs. </title> <booktitle> In Proceedings of the First International Conference on Knowledge Discovery and Data Mining. </booktitle> <address> Montreal, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Gaines, B. R. </author> <year> (1996). </year> <title> Transforming rules and trees into comprehensible knowledge structures. </title>
Reference: <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smythand R. Uthurusamy (Eds.), </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <address> Cambridge, Massachusetts: </address> <publisher> MIT Press. 41 Gallant, </publisher> <editor> S. I. </editor> <year> (1986). </year> <title> Optimal linear discriminants. </title> <booktitle> In Proceedings of the International Conference on Pattern Recognition (pp. </booktitle> <pages> 849-852). </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Finally, eval () is usually a measure of a given partition's homogeneity; the most preferred partitions are those in which a large majority of the instances in each subset P j belong to the same class. This is frequently accomplished by defining eval () using an information-theoretic <ref> (Quinlan 1986) </ref> or statistical measure (Breiman et al. 1984). In most research on classification, the database is divided into a training set and a test set. The training set is used to generate the decision tree, while the test set is used to assess its predictive accuracy. <p> Another cause for large trees is noise. When cases have a large amount of feature noise (i.e., mislabeled feature values) or class noise (i.e., mislabeled class values), the induction algorithm may expand the tree too far based on irrelevant case distinctions <ref> (Quinlan 1986) </ref>. Noise can cause some irrelevant features to be included among the selected tests. This in turn causes overfitting (Cohen and Jensen 1997), where trees model both the target concept and the inherent noise. <p> Subtree replication is one cause of tree fragmentation, and suggests a mismatch of representational biases, as explained above. Whatever the cause, a large, fragmented tree is clearly difficult for users to understand. An example <ref> (from Quinlan 1986) </ref> is the tree in Figure 4, based on the small database shown in Table 1. 3 Synonymous terms include apparent error and apparent accuracy. 5 Table 1: A small database (from Quinlan 1986). No. <p> Whatever the cause, a large, fragmented tree is clearly difficult for users to understand. An example <ref> (from Quinlan 1986) </ref> is the tree in Figure 4, based on the small database shown in Table 1. 3 Synonymous terms include apparent error and apparent accuracy. 5 Table 1: A small database (from Quinlan 1986). No. <p> In their empirical evaluation, SADT is run 100 times and the smallest tree it generates is retained. Using this approach, SADT consistently generated smaller trees than ID3 <ref> (Quinlan 1986) </ref>, while usually increasing accuracy. Murthy et al. (1993; 1994) introduced OC1, which extends previous work on CART and SADT by combining both deterministic and non-deterministic strategies for finding multivariate tests. In particular, it first uses a deterministic strategy that individually perturbs the coefficients of a (numeric) multivariate test. <p> All of these methods modify the best test and partition function in the generic decision tree induction algorithm displayed (Figure 2). 24 3.3.1 Alternative Test Selection Measures Minimum Description Length. Many test selection measures have been proposed, including information gain <ref> (Quinlan 1986) </ref> and statistical measures of purity (Breiman et al. 1984). In a comparative study, Mingers (1989b) found no significant differences in accuracy for several popular test selection measures. 12 However, some selection measures might be superior in producing smaller trees. <p> Because Pazzani et al. (1994) found that using it as the selection measure reduces predictive accuracy, these authors integrated it with a "traditional" test selection measure; Lubinsky used the Gini criterion (Breiman et al. 1984) and Brodley used information gain <ref> (Quinlan 1986) </ref>. Both authors report small gains in predictive accuracy when the traditional measure is used for nodes close to the root and an accuracy measure (Brodley) or a hybrid accuracy/traditional measure (Lubinsky) is used for nodes closer to the leaves. <p> It is particularly useful for tasks with feature interactions. Unfortunately, lookahead can be computationally expensive. Feature selection at a node has complexity O (f n) <ref> (Quinlan 1986) </ref> for f features and n cases. In contrast, one-level lookahead's complexity is O (f 2 n 2 ) (Murthy and Salzberg 1995), or more generally O (f d n d ) for d levels of lookahead. <p> LFC reduces computational costs by controlling lookahead using a constrained branch-and- bound search. User-specified parameters include the beam size ff, a maximum search depth, and a window width . Included in the beam are the current best ff features, ranked by information gain <ref> (Quinlan 1986) </ref>. Feature construction conjoins one of these features (f 1 ) with another (or its negation) outside the beam but within window of f 1 . <p> A reduced decision graph is one in which no two nodes have identical branching and there are no constant nodes, whose outgoing edges all terminate at the same node. Their smog algorithm begins by generating three data structures: (1) a decision tree produced by a standard induction algorithm <ref> (Quinlan 1986) </ref> without pruning, (2) a tree generated using constructive induction (see Section 3.2.2) (Oliveira and Vin- centelli 1993), and (3) a RODG. smog then selects the structure with minimal description length. If needed, the selected structure is converted into a RODG.
Reference: <author> Gelfand, S. B., C. S. Ravishankar, and E. J. </author> <month> Delp </month> <year> (1991). </year> <title> An iterative growing and pruning algorithm for classification tree design. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(2), </volume> <pages> 163-174. </pages>
Reference: <author> Gleser, M. A. and M. F. </author> <month> Collen </month> <year> (1972). </year> <title> Towards automated medical decisions. </title> <journal> Computers and Biomedical Research, </journal> <volume> 5(2), </volume> <pages> pp. 180-189, </pages> <month> April </month> <year> 1972. </year>
Reference: <author> Hart, P. E. </author> <year> (1968). </year> <title> The condensed nearest neighbor rule. </title> <journal> Institute of Electrical and Electronics Engineers Transactions on Information Theory, </journal> <volume> 14, </volume> <pages> 515-516. </pages>
Reference-contexts: Indeed, incrementality together with case selection has been shown to reduce tree size and increase accuracy (Utogff 1989a; 1994). 14 This is similar to the distinction between edited nearest neighbor algorithms that delete misclassified cases <ref> (Hart 1968) </ref> versus those that delete correctly classified cases (Wilson 1972). 29 3.4.2 Feature Selection Since decision tree induction can be misled by the presence of irrelevant features, several researchers have investigated algorithms for removing such features prior to decision tree induction (Almuallim and Dietterich 1991; Doak 1992; Kira and Rendell
Reference: <author> Heath, D., S. Kasif, and S. </author> <title> Salzberg (1993). Induction of oblique decision trees. </title> <booktitle> Proccedings of the Thirteenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 1002-1007). </pages> <address> Chambery, France: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Holder, L. B. </author> <year> (1995). </year> <title> Intermediate decision trees. </title> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 1056-1063). </pages> <address> Montreal: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Holte, R. C. </author> <year> (1993). </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11, </volume> <pages> 63-91. </pages>
Reference: <author> Holte, R. C., L. E. Acker, and B. W. </author> <title> Porter (1989). Concept learning and the problem of small disjuncts. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 813-818). </pages> <address> Detroit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Hunt, E. B., J. Marin, and P. J. </author> <title> Stone (1966). Experiments in Induction. </title> <publisher> New York; Academic Press. </publisher>
Reference: <author> Hyafil, L. and R. </author> <title> Rivest (1976). Constructing optimal binary decision trees is NP-complete. </title> <journal> Information Processing Letters, </journal> <volume> 5(1), </volume> <pages> 15-17. </pages>
Reference-contexts: ()) FOR j := 1 TO jV j DO subtree (N; V j ) := induce tree (P j ; I; eval (); stop ()) RETURN N Tree induction algorithms, such as our generic algorithm, must employ computationally efficient heuristics because constructing an optimal decision tree is an NP-complete task <ref> (Hyafil and Rivest 1976) </ref>; search complexity grows exponentially with tree depth (i.e., path length from root to lowest leaf). Thus, our algorithm greedily selects a test at each node that maximizes eval ().
Reference: <author> John, G. </author> <year> (1995). </year> <title> Robust decision trees: Removing outliers in databases. </title> <booktitle> In Proceedings of the First International Conference on Knowledge Discovery and Data Mining. </booktitle> <address> Montreal, Canada: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Using available software 16 , we compared C4.5 release 7 (Quinlan 1993) (with and without EBP pruning, inducing trees or rules (C4.5rules)), C4.5 release 8 (Quinlan 1996) (tree induction with post-pruning), ITI (Utgoff 1994) (with the gain ratio and with the KS (Utgoff and Clouse 1996) test selection measures), RobustC4.5 <ref> (John 1995) </ref>, 16 Some of the best algorithms listed in Table 4 were not tested because the software was not available. 35 Table 4: Some promising tree-simplification algorithms.
Reference: <author> John, G., R. Kohavi, and K. </author> <month> Pfleger </month> <year> (1994). </year> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Proceedings of the Eleventh International Machine Learning Conference (pp. </booktitle> <pages> 121-129). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Jordan, M. I. </author> <year> (1994). </year> <title> A statistical approach to decision tree modeling. </title> <booktitle> In Proceedings of the Eleventh Conference on Machine Learning (pp. </booktitle> <pages> 363-370). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kalkanis, G. </author> <year> (1993). </year> <title> The application of confidence interval error analysis to the design of decision tree classifiers. </title> <journal> Pattern Recognition Letters, </journal> <volume> 14, </volume> <pages> 355-361. </pages>
Reference: <author> Kearns, M. and Y. </author> <title> Mansour (1996). On the boosting ability of top-down decision tree learning algorithms. </title> <booktitle> To appear in Proceedings of the 28th ACM Symposium on the Theory of Computing. </booktitle> <publisher> ACM Press: </publisher> <address> New York, NY. </address>
Reference-contexts: Weak Theory Learning Measure. Dietterich et al. (1996) describe theoretical motivation for an improved information gain test selection measure, G (p) = 2 p p (1 p) (where p is the percentage of positive cases in a node), based on the weak learning or boosting model <ref> (Kearns and Mansour 1996) </ref>. They prove that the number of splits required by C4.5 to yield a low error * changes from a superpolynomial to a polynomial in * when using G rather than using the usual information- theoretic function to compute information gain ratio.
Reference: <author> Kim, H. and G. J. </author> <title> Koehler (1994). An investigation on the conditions of pruning an induced decision tree. </title> <journal> European Journal of Operational Research, </journal> <volume> 77(1), </volume> <pages> 82-95. </pages>
Reference: <author> Kira, K. and L. A. </author> <title> Rendell (1992a). The feature selection problem: Traditional methods and a new algorithm. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 129-134). </pages> <address> San Jose, CA: </address> <publisher> AAAI Press. 42 Kira, </publisher> <editor> K. and L. A. </editor> <title> Rendell (1992b). A practical approach to feature selection. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning (pp. </booktitle> <pages> 249-256). </pages> <address> Aberdeen, Scotland: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kohavi, R. </author> <year> (1994a). </year> <title> Bottom-up induction of oblivious, read-once decision graphs. </title> <booktitle> Proceedings of the European Conference on Machine Learning (pp. </booktitle> <pages> 154-169). </pages> <address> Catania, Italy: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Kohavi, R. </author> <year> (1994b). </year> <title> Bottom-up induction of oblivious, read-once decision graphs: Strengths and limitations. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 613-618). </pages> <address> Seattle, WA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Kohavi, R. and C.-H. </author> <month> Li </month> <year> (1995). </year> <title> Oblivious decision trees, graphs, and top-down pruning. </title> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 1071-1077). </pages> <address> Montreal: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kononenko, I. </author> <year> (1995). </year> <title> A counter example of the stronger version of the binary tree hypothesis. </title> <booktitle> In ECML-95 Workshop on Statistics and Machine Learning in KDD. </booktitle> <address> Crete. </address>
Reference-contexts: This section describes procedures that simplify decision trees by using a modified set of tests. Fayyad and Irani (1992) report evidence that binary tests often yield smaller trees than n-ary tests (i.e., which yield one subtree for each of n feature values), although exceptions to this trend exist <ref> (Kononenko 1995) </ref>. However, most of the procedures in this category modify the set of tests via feature construction: they construct new multivariate tests by applying mathematical or logical combination operators to individual case features.
Reference: <author> Kubat, M. and D. </author> <month> Flotzinger </month> <year> (1995). </year> <title> Pruning multivariate decision trees by hyperplane merging. </title> <booktitle> Proceedings of the European Conference on Machine Learning (pp. </booktitle> <pages> 190-199). </pages> <address> Heraklion, Greece: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Most of these approaches use post-pruning techniques similar to those described in Section 3.1.2. An alternative post-processing method designed specifically for multivariate trees is hyperplane merging <ref> (Kubat and Flotzinger 1995) </ref>, in which a node and its parent are combined by merging their hyperplanes. Tested on three artificial and four real-world data sets, hyperplane merging produced smaller trees that were at least as accurate as post-pruning.
Reference: <author> Langley, P. </author> <year> (1996). </year> <title> Elements of machine learning. </title> <address> San Francisco, </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Since pruning is more beneficial with larger training samples (Fisher and Schlimmer 1988; Kim and Koehler 1994), it makes sense to revisit earlier pruning decisions after accumulating additional cases. A less purely incremental approach is dynamic pruning <ref> (Langley 1996, p. 232) </ref> or virtual pruning (Utgoff 1994). In this approach, the decision tree structure can be created statically at the outset. However, it is used in a dynamic manner: the entire tree need not be used for classification.
Reference: <author> Langley, P. and S. </author> <title> Sage (1994). Oblivious decision trees and abstract cases. </title> <editor> In D. W. Aha (Ed.), </editor> <booktitle> Case-Based Reasoning: Papers from the 1994 Workshop (Technical Report WS-94-01). </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Li, X. and R. C. </author> <title> Dubes (1986). Tree classifier design with a permutation statistic. </title> <journal> Pattern Recognition, </journal> <volume> 19, </volume> <pages> 229-235. </pages>
Reference: <author> Lubinsky, D. </author> <year> (1995). </year> <title> Increasing the performance and consistency of classification trees by using the accuracy criterion at the leaves. </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning (pp. </booktitle> <pages> 371-377). </pages> <address> Tahoe City, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Matheus, C. J. </author> <year> (1990). </year> <title> Adding domain knowledge to SBL through feature construction. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 803-808). </pages> <address> Boston, MA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: When tested on a tic-tac-toe endgame classification task in ablation studies, CITRE's constructive induction component was shown to improve trees' accuracy <ref> (Matheus 1990) </ref>. Variants of CITRE including either generalization or domain knowledge demonstrated further gains in accuracy (Matheus and Rendell 1989). Trees produced by CITRE had fewer nodes than those produced without constructive induction.
Reference: <author> Matheus, C. J. and L. A. </author> <title> Rendell (1989). Constructive induction on decision trees. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 645-650). </pages> <address> Detroit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: When tested on a tic-tac-toe endgame classification task in ablation studies, CITRE's constructive induction component was shown to improve trees' accuracy (Matheus 1990). Variants of CITRE including either generalization or domain knowledge demonstrated further gains in accuracy <ref> (Matheus and Rendell 1989) </ref>. Trees produced by CITRE had fewer nodes than those produced without constructive induction.
Reference: <author> Michalski, R. S. </author> <year> (1983). </year> <title> A theory and methodology of inductive learning. </title> <editor> In R. S. Michalski, J. G. Carbonelland T. M. Mitchell (Eds.), </editor> <booktitle> Machine learning: An artificial intelligence approach. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Michie, D. </author> <year> (1990). </year> <title> Inducing knowledge from data: </title> <booktitle> First principles. Unpublished manuscript for a talk given at the Seventh International Conference on Machine Learning. </booktitle> <address> Austin, Texas. </address>
Reference-contexts: However, for many practical applications, it is desirable that the classifier "provide insight and understanding into the predictive structure of the data" (Breiman et al. 1984), as well as explanations of its individual predictions <ref> (Michie 1990) </ref>. Decision tree induction has been extensively studied in the machine learning and statistics communities as a solution to classification tasks (Breiman et al. 1984; Quinlan 1986; 1993a). Many tree-simplification algorithms have been shown to yield simpler or smaller trees.
Reference: <author> Mingers, J. </author> <year> (1987). </year> <title> Expert systems-rule induction with statistical data. </title> <journal> Journal of the Operational Research Society, </journal> <volume> 38, </volume> <pages> 39-47. </pages>
Reference: <author> Mingers, J. </author> <year> (1989a). </year> <title> An empirical comparison of pruning methods for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 4, </volume> <pages> 227-243. </pages>
Reference-contexts: Reduced Error Pruning (REP) yes bottom-up Finds the optimally pruned (Quinlan 1987a) tree wrt the pruning set. Minimum Error Pruning (MEP-2) yes bottom-up Based on the m-probability (Cestnik & Bratko 1991) estimate of the error rate. Critical Value Pruning (CVP) yes bottom-up Uses a critical value <ref> (Mingers 1989a) </ref> threshold on the test selection measure Pessimistic Error Pruning (PEP) no top-down Uses the continuity (Quinlan 1987a) correction. Error-Based Pruning (EBP) no bottom-up Estimates confidence intervals (Quinlan 1993a) on the training set. pruned trees as well as to select among them. <p> Error-Based Pruning (EBP) no bottom-up Estimates confidence intervals (Quinlan 1993a) on the training set. pruned trees as well as to select among them. An advantage of pruning set methods, according to some authors <ref> (e.g., Mingers 1989a) </ref>, is that they produce a set of trees, rather than a single tree. This may be especially desirable when an expert is available who can compare and select among the trees, because the expert might disagree with (and override) the selection made by the algorithm.
Reference: <author> Mingers, J. </author> <year> (1989b). </year> <title> An empirical comparison of selection measures for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 319-342. </pages> <note> 43 Minsky, </note> <author> M. and S. </author> <title> Papert (1969). Perceptrons: An introduction to computational geometry. </title> <address> Cam-bridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Moore, A. W. and M. S. </author> <title> Lee (1994). Efficient algorithms for minimizing cross validation error. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning (pp. </booktitle> <pages> 190-198). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: When tested on a proprietary data set containing almost 200 features, many of which are irrelevant, SET-Gen excelled in accuracy relative to the other algorithms. This is expected, since SET-Gen was designed to handle irrelevant features. However, SET-Gen is very slow, while RobustC4.5 is fast. Racing algorithms <ref> (Moore and Lee 1994) </ref> and other fast methods for feature selection might be more appropriate than slower methods when applied to large data sets. The last category, converting trees to other data structure, shows great promise.
Reference: <author> Morgan, J. and J. A. </author> <month> Sonquist </month> <year> (1963). </year> <title> Problems in the analysis of survey data, and a proposal. </title> <journal> Journal of the American Statistical Assocation, </journal> <volume> 58, </volume> <pages> 415-435. </pages>
Reference: <author> Murphy, P. M. and M. J. </author> <title> Pazzani (1991). Constructive induction of M-of-N terms. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning (pp. </booktitle> <pages> 183-187). </pages> <address> Evanston, IL: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Murphy, P. M. and M. J. </author> <title> Pazzani (1994). Exploring the decision forest: An empirical investigation of Occam's razor in decision tree induction. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1, </volume> <pages> 257275. </pages>
Reference: <author> Murthy, S., S. Kasif, and S. </author> <title> Salzberg (1994). A system for induction of oblique decision trees. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <pages> 1-32. </pages>
Reference: <author> Murthy, S., S. Kasif, S. Salzberg, and R. </author> <title> Beigel (1993). OC1: Randomized induction of oblique decision trees. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence (pp. </booktitle> <pages> 322-327). </pages> <address> Washington, DC: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Murthy, S. K. and S. </author> <title> Salzberg (1995). Lookahead and pathology in decision tree induction. </title> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artifical Intelligence (pp. </booktitle> <pages> 1025-1031). </pages> <address> Montreal, Canada: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It is particularly useful for tasks with feature interactions. Unfortunately, lookahead can be computationally expensive. Feature selection at a node has complexity O (f n) (Quinlan 1986) for f features and n cases. In contrast, one-level lookahead's complexity is O (f 2 n 2 ) <ref> (Murthy and Salzberg 1995) </ref>, or more generally O (f d n d ) for d levels of lookahead. Thus, lookahead is typically constrained, either by limiting the set of features involved or lookahead depth. <p> Finally, LFC's lookahead search method produced dramatic improvements in accuracy, as well as reductions in size. However, lookahead methods sometimes yield larger, less accurate trees <ref> (Murthy and Salzberg 1995) </ref>, suggesting that it is not always appropriate. The few methods in the fourth category restrict the database through either case selection or feature selection. We tested one method from each subcategory: RobustC4.5 and SET-Gen.
Reference: <author> Nakamura, Y., S. Abe, Y. Ohsawa, and M. </author> <month> Sakauchi </month> <year> (1993). </year> <title> A balanced hierarchical data structure for multidimensional data with highly efficient dynamic characteristics. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(4), </volume> <pages> 682-694. </pages>
Reference-contexts: This is similar to techniques used for search trees that reduce depth by applying left and right rotation operators <ref> (e.g., Nakamura et al. 1993) </ref>. 3.1.5 Hybrid Procedures Some evidence suggests that hybrid classifiers combining decision trees and another classifier might allow for greater tree pruning with adequate accuracy.
Reference: <author> Niblett, T. and I. </author> <title> Bratko (1986). Learning decision rules in noisy domains. </title> <booktitle> In Proceedings of Expert Systems 1986 (pp. </booktitle> <pages> 25-34). </pages> <address> Cambridge, England: </address> <publisher> Cambridge University Press. </publisher>
Reference: <author> Norton, S. W. </author> <year> (1989). </year> <title> Generating better decision trees. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 800-805). </pages> <address> Detroit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Oliveira, A. L. and A. </author> <title> Sangiovanni-Vincentelli (1995). Inferring reduced ordered decision graphs of minimum description length. </title> <booktitle> Twelfth International Conference on Machine Learning, </booktitle> <pages> (pp. 421-429). </pages> <address> Tahoe City, CA. </address>
Reference: <author> Oliveira, A. L. and A. S. </author> <month> Vincentelli </month> <year> (1993). </year> <title> Learning complex boolean functions: Algorithms and applications. </title> <editor> In Hanson, S. J., et al. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 5. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Their smog algorithm begins by generating three data structures: (1) a decision tree produced by a standard induction algorithm (Quinlan 1986) without pruning, (2) a tree generated using constructive induction (see Section 3.2.2) <ref> (Oliveira and Vin- centelli 1993) </ref>, and (3) a RODG. smog then selects the structure with minimal description length. If needed, the selected structure is converted into a RODG. Then, the structure is compressed by greedily merging nodes with identical branching and by pruning constant nodes.
Reference: <author> Oliver, J. J. </author> <year> (1993). </year> <title> Decision graphs-an extension of decision trees. </title> <booktitle> Fourth International Workshop on Artifical Intelligence and Statistics (pp. </booktitle> <pages> 343-350). </pages> <address> Ft. Lauderdale, FL: </address> <note> Unpublished. </note>
Reference: <author> Oliver, J. J. and D. J. </author> <title> Hand (1995). On pruning and averaging decision trees. </title> <booktitle> Proceedings of the Twelfth International Machine Learning Conference (pp. </booktitle> <pages> 430-437). </pages> <address> Tahoe City, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Some of these methods sacrifice comprehensibility for the sake of accuracy (e.g., methods that create a set of decision trees <ref> (e.g., Oliver and Hand 1995) </ref> or methods that yield small gains in accuracy at the price of large increases in tree size (e.g., Webb 1996)), while others fine-tune the balance between accuracy and comprehensibility (e.g., Breiman et al. 1984; Bohanec and Bratko 1994; Holder 1995).
Reference: <author> Pagallo, G. </author> <year> (1989). </year> <title> Learning DNF by decision trees. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 639-644). </pages> <address> Detroit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Pagallo, G. </author> <year> (1990). </year> <title> Adaptive decision tree algorithms for learning from examples. </title> <institution> Doctoral disser-tation, Department of Computer Science, University of California at Santa Cruz, CA. </institution> <note> 44 Pagallo, </note> <author> G. and D. </author> <title> Haussler (1990). Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 71-100. </pages>
Reference: <author> Pazzani, M., C. Merz, P. Murphy, K. Ali, T. Hume, and C. </author> <month> Brunk </month> <year> (1994). </year> <title> Reducing misclassification costs: Knowledge-intensive approaches to learning from noisy data. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning (pp. </booktitle> <pages> 217-225). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Perez, E. and L. A. </author> <title> Rendell (1995). Using multidimensional projection to find relations. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning (pp. </booktitle> <pages> 447-455). </pages> <address> Tahoe City, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Pitt, L. and L. G. </author> <title> Valiant (1988). Computational limitations on learning from examples. </title> <journal> Journal of the ACM, </journal> <volume> 35, </volume> <pages> 965-984. </pages>
Reference-contexts: M-of-n concepts subsume logical conjunction, because a conjunction is equivalent to an n-of-n concept. In general, univariate decision trees for representing m-of-n concepts are quite large, and determining whether there is an m-of-n concept consistent with a set of cases is NP-hard <ref> (Pitt and Valiant 1988) </ref>. Murphy and Pazzani (1991) proposed the GS algorithm in their ID2-of-3 system as a greedy solution to the m-of-n problem. When selecting a test at a node, GS first finds the best feature with regard to information gain and converts it to a simple 1-of-1 test.
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: Finally, eval () is usually a measure of a given partition's homogeneity; the most preferred partitions are those in which a large majority of the instances in each subset P j belong to the same class. This is frequently accomplished by defining eval () using an information-theoretic <ref> (Quinlan 1986) </ref> or statistical measure (Breiman et al. 1984). In most research on classification, the database is divided into a training set and a test set. The training set is used to generate the decision tree, while the test set is used to assess its predictive accuracy. <p> Another cause for large trees is noise. When cases have a large amount of feature noise (i.e., mislabeled feature values) or class noise (i.e., mislabeled class values), the induction algorithm may expand the tree too far based on irrelevant case distinctions <ref> (Quinlan 1986) </ref>. Noise can cause some irrelevant features to be included among the selected tests. This in turn causes overfitting (Cohen and Jensen 1997), where trees model both the target concept and the inherent noise. <p> Subtree replication is one cause of tree fragmentation, and suggests a mismatch of representational biases, as explained above. Whatever the cause, a large, fragmented tree is clearly difficult for users to understand. An example <ref> (from Quinlan 1986) </ref> is the tree in Figure 4, based on the small database shown in Table 1. 3 Synonymous terms include apparent error and apparent accuracy. 5 Table 1: A small database (from Quinlan 1986). No. <p> Whatever the cause, a large, fragmented tree is clearly difficult for users to understand. An example <ref> (from Quinlan 1986) </ref> is the tree in Figure 4, based on the small database shown in Table 1. 3 Synonymous terms include apparent error and apparent accuracy. 5 Table 1: A small database (from Quinlan 1986). No. <p> In their empirical evaluation, SADT is run 100 times and the smallest tree it generates is retained. Using this approach, SADT consistently generated smaller trees than ID3 <ref> (Quinlan 1986) </ref>, while usually increasing accuracy. Murthy et al. (1993; 1994) introduced OC1, which extends previous work on CART and SADT by combining both deterministic and non-deterministic strategies for finding multivariate tests. In particular, it first uses a deterministic strategy that individually perturbs the coefficients of a (numeric) multivariate test. <p> All of these methods modify the best test and partition function in the generic decision tree induction algorithm displayed (Figure 2). 24 3.3.1 Alternative Test Selection Measures Minimum Description Length. Many test selection measures have been proposed, including information gain <ref> (Quinlan 1986) </ref> and statistical measures of purity (Breiman et al. 1984). In a comparative study, Mingers (1989b) found no significant differences in accuracy for several popular test selection measures. 12 However, some selection measures might be superior in producing smaller trees. <p> Because Pazzani et al. (1994) found that using it as the selection measure reduces predictive accuracy, these authors integrated it with a "traditional" test selection measure; Lubinsky used the Gini criterion (Breiman et al. 1984) and Brodley used information gain <ref> (Quinlan 1986) </ref>. Both authors report small gains in predictive accuracy when the traditional measure is used for nodes close to the root and an accuracy measure (Brodley) or a hybrid accuracy/traditional measure (Lubinsky) is used for nodes closer to the leaves. <p> It is particularly useful for tasks with feature interactions. Unfortunately, lookahead can be computationally expensive. Feature selection at a node has complexity O (f n) <ref> (Quinlan 1986) </ref> for f features and n cases. In contrast, one-level lookahead's complexity is O (f 2 n 2 ) (Murthy and Salzberg 1995), or more generally O (f d n d ) for d levels of lookahead. <p> LFC reduces computational costs by controlling lookahead using a constrained branch-and- bound search. User-specified parameters include the beam size ff, a maximum search depth, and a window width . Included in the beam are the current best ff features, ranked by information gain <ref> (Quinlan 1986) </ref>. Feature construction conjoins one of these features (f 1 ) with another (or its negation) outside the beam but within window of f 1 . <p> A reduced decision graph is one in which no two nodes have identical branching and there are no constant nodes, whose outgoing edges all terminate at the same node. Their smog algorithm begins by generating three data structures: (1) a decision tree produced by a standard induction algorithm <ref> (Quinlan 1986) </ref> without pruning, (2) a tree generated using constructive induction (see Section 3.2.2) (Oliveira and Vin- centelli 1993), and (3) a RODG. smog then selects the structure with minimal description length. If needed, the selected structure is converted into a RODG.
Reference: <author> Quinlan, J. R. </author> <year> (1987a). </year> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27, </volume> <pages> 221-234. </pages>
Reference-contexts: Pruning Method Set Strategy Characteristics Minimal Cost-Complexity (MCCP) 0-SE: yes bottom-up Can use cross-validation (0-SE, 1-SE, CV) 1-SE: yes (any internal or standard error to (Breiman et al. 1984) CV: no node) estimate accuracy. Reduced Error Pruning (REP) yes bottom-up Finds the optimally pruned <ref> (Quinlan 1987a) </ref> tree wrt the pruning set. Minimum Error Pruning (MEP-2) yes bottom-up Based on the m-probability (Cestnik & Bratko 1991) estimate of the error rate. <p> Minimum Error Pruning (MEP-2) yes bottom-up Based on the m-probability (Cestnik & Bratko 1991) estimate of the error rate. Critical Value Pruning (CVP) yes bottom-up Uses a critical value (Mingers 1989a) threshold on the test selection measure Pessimistic Error Pruning (PEP) no top-down Uses the continuity <ref> (Quinlan 1987a) </ref> correction. Error-Based Pruning (EBP) no bottom-up Estimates confidence intervals (Quinlan 1993a) on the training set. pruned trees as well as to select among them. <p> They also argue that the probabilistic measure is not valid, and therefore tested CVP using a pruning set. They found that CVP has a strong tendency to underprune and selects trees with comparatively low predictive accuracy. 5. Pessimistic Error Pruning (PEP). PEP <ref> (Quinlan 1987a) </ref> was developed in the context of ID3 and, like MCCP's CV variants, does not use a separate pruning set. <p> FRINGE uses a tree induction algorithm that is a hybrid of ID3 and CART (Breiman et al. 1984) with reduced error post-pruning <ref> (Quinlan 1987a) </ref>. The induction- construction cycle iterates until no new features are added or a specified maximum number of features has been reached. FRINGE was tested on nine data sets with irrelevant features and various amounts of noise added. <p> Wallace and Patrick (1993) corrected an error in the coding technique used by Quinlan and Rivest, but confirmed their findings: the trees created using the MDL principle were consistently smaller and usually more accurate than trees generated by C4 (a precursor of C4.5) with pessimistic error pruning <ref> (Quinlan 1987a) </ref>. This seems a promising approach for reducing the size of decision trees with an appealing theoretical foundation. Weak Theory Learning Measure. <p> Two approaches have been proposed. The first induces a single decision tree and then prunes the rule set derived from it <ref> (Quinlan 1987a, 1987b) </ref>. The second iteratively generates decision trees, selecting the best rule from each iteration (Fayyad and Irani 1993). We examine each approach in turn. Quinlan's (1993a) C4.5 system includes an option for translating the induced tree to a set of rules.
Reference: <author> Quinlan, J. R. </author> <year> (1987b). </year> <title> Generating production rules from decision trees. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 304-307). </pages> <address> Milan, Italy: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> (1991). </year> <title> Improved estimates for the accuracy of small disjuncts. </title> <journal> Machine Learning, </journal> <volume> 6(1), </volume> <pages> 93-98. </pages>
Reference-contexts: In addition to being difficult to understand, a complex tree often performs more poorly in classification tasks because small disjuncts are more likely to be error-prone than large disjuncts <ref> (Quinlan 1991) </ref>.
Reference: <author> Quinlan, J. R. </author> <year> (1993a). </year> <title> C4.5: Programs for machine learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Critical Value Pruning (CVP) yes bottom-up Uses a critical value (Mingers 1989a) threshold on the test selection measure Pessimistic Error Pruning (PEP) no top-down Uses the continuity (Quinlan 1987a) correction. Error-Based Pruning (EBP) no bottom-up Estimates confidence intervals <ref> (Quinlan 1993a) </ref> on the training set. pruned trees as well as to select among them. An advantage of pruning set methods, according to some authors (e.g., Mingers 1989a), is that they produce a set of trees, rather than a single tree. <p> Despite these objections, PEP did record high accuracies, and its top-down pruning strategy would likely make it more efficient than the other methods. 6. Error-Based Pruning (EBP). EBP is a more pessimistic descendant of PEP that is used in C4.5 <ref> (Quinlan 1993a) </ref>, a descendant of ID3. <p> However, these tests have higher arity than Boolean-valued tests and thus might partition a set of cases into small subsets, exacerbating the fragmentation problem (see Section 2.1). Zheng's XofN algorithm, which extends C4.5 <ref> (Quinlan 1993a) </ref>, is similar to GS; it starts at 9 However, they report that CART's post-pruning algorithm reduced the multivariate tree's size from 50 to 25 nodes while simultaneously increasing predictive accuracy. 10 Search for an m-of-n concept includes first a search for the subset n and then for the value
Reference: <author> Quinlan, J. R. </author> <year> (1993b). </year> <title> Combining instance-based learning and model-based learning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning (pp. </booktitle> <pages> 236-243). </pages> <address> Amherst, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> (1996). </year> <title> Improved use of continuous attributes in C4.5. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4, </volume> <pages> 77-90. </pages>
Reference-contexts: To reduce complexity, they restrict decision trees to two levels of internal nodes, with only the second level nodes using non-binary splits of continuous features. Their algorithm produces much smaller trees than C4.5 <ref> (Quinlan 1996) </ref>, yet with comparable accuracy for several tasks. Quinlan (1996) introduced a local, MDL-based method to penalize continuous features having many values in release 8 (R8) of C4.5. <p> We empirically compared some of the more promising tree simplification procedures; the details are reported in (Breslow and Aha 1997). Using available software 16 , we compared C4.5 release 7 (Quinlan 1993) (with and without EBP pruning, inducing trees or rules (C4.5rules)), C4.5 release 8 <ref> (Quinlan 1996) </ref> (tree induction with post-pruning), ITI (Utgoff 1994) (with the gain ratio and with the KS (Utgoff and Clouse 1996) test selection measures), RobustC4.5 (John 1995), 16 Some of the best algorithms listed in Table 4 were not tested because the software was not available. 35 Table 4: Some promising
Reference: <author> Quinlan, J. R. and R. M. </author> <title> Cameron-Jones (1995). Oversearching and layered search in empirical learning. </title> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> (pp. 1019-1024). </pages> <address> Montreal: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Quinlan, J. R. and R. L. </author> <title> Rivest (1989). Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80, </volume> <pages> 227-248. </pages>
Reference-contexts: In a comparative study, Mingers (1989b) found no significant differences in accuracy for several popular test selection measures. 12 However, some selection measures might be superior in producing smaller trees. One example is a selection measure based on the minimum description length (MDL) principle <ref> (Quinlan and Rivest 1989) </ref>, which combines considerations of tree size and accuracy.
Reference: <author> Ragavan, H. and L. </author> <title> Rendell (1993). Lookahead feature construction for learning hard concepts. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning (pp. </booktitle> <pages> 252-259). </pages> <address> Amherst, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The complexity of iterative constructive induction algorithms is even greater (see Section 3.2.2). Fortunately, the number of feature combinations to consider can be reduced by applying only some operators to only some feature 8 LFC <ref> (Ragavan et al. 1993) </ref> constructs new features using logical conjunction, but will be described in Section 3.3.3, on lookahead search methods, because it incorporates both lookahead and feature construction. 20 subsets. Test construction algorithms constrain this space by heuristically selecting which features to combine. <p> Ragavan and Rendell (1993; Ragavan et al. 1993) empirically compared LFC with several algorithms, including decision tree algorithms distinguished by whether they perform feature construction. They reported that the trees produced by LFC were more compact and informative to experts. LFC <ref> (Ragavan and Rendell 1993) </ref> also had the highest, or tied for the highest, accuracies on the selected data sets, and its accuracy degraded much more gracefully with increasing feature interactions than did the other algorithms.
Reference: <author> Ragavan, H., L. Rendell, M. Shaw, and A. </author> <month> Tessmer </month> <year> (1993). </year> <title> Complex concept acquisition through directed search and feature caching. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 946-951). </pages> <address> Chambery, France: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The complexity of iterative constructive induction algorithms is even greater (see Section 3.2.2). Fortunately, the number of feature combinations to consider can be reduced by applying only some operators to only some feature 8 LFC <ref> (Ragavan et al. 1993) </ref> constructs new features using logical conjunction, but will be described in Section 3.3.3, on lookahead search methods, because it incorporates both lookahead and feature construction. 20 subsets. Test construction algorithms constrain this space by heuristically selecting which features to combine. <p> Ragavan and Rendell (1993; Ragavan et al. 1993) empirically compared LFC with several algorithms, including decision tree algorithms distinguished by whether they perform feature construction. They reported that the trees produced by LFC were more compact and informative to experts. LFC <ref> (Ragavan and Rendell 1993) </ref> also had the highest, or tied for the highest, accuracies on the selected data sets, and its accuracy degraded much more gracefully with increasing feature interactions than did the other algorithms.
Reference: <author> Rendell, L. </author> <year> (1985). </year> <title> Substantial constructive induction using layered information compression: Tractable feature formation in search. </title> <booktitle> In Proceedings of the Ninth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 650-658). </pages> <address> Los Angeles, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Rendell, L. and H. </author> <month> Ragavan </month> <year> (1993). </year> <title> Improving the design of induction methods by analyzing algo-rithm functionality and data-based concept complexity. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 952-958). </pages> <address> Chambery, France: </address> <publisher> Morgan Kaufmann. 45 Rissanen, </publisher> <editor> J. </editor> <year> (1983). </year> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> Annals of Statistics, </journal> <volume> 11, </volume> <pages> 416-431. </pages> <month> Rounds </month> <year> (1980). </year> <title> A combined non-parametric approach to feature selection and binary decision tree design. </title> <journal> Pattern Recognition, </journal> <volume> 12 </volume> <pages> 313-317, </pages> <year> 1980. </year>
Reference-contexts: Ragavan and Rendell (1993; Ragavan et al. 1993) empirically compared LFC with several algorithms, including decision tree algorithms distinguished by whether they perform feature construction. They reported that the trees produced by LFC were more compact and informative to experts. LFC <ref> (Ragavan and Rendell 1993) </ref> also had the highest, or tied for the highest, accuracies on the selected data sets, and its accuracy degraded much more gracefully with increasing feature interactions than did the other algorithms.
Reference: <author> Schaffer, C. </author> <year> (1992a). </year> <title> Deconstructing the digit recognition problem. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning (pp. </booktitle> <pages> 394-399). </pages> <address> Aberdeen, Scotland: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schaffer, C. </author> <year> (1992b). </year> <title> Sparse data and the effect of overfitting avoidance in decision tree induction. </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 147-152). </pages> <address> San Jose, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Schaffer, C. </author> <year> (1993). </year> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 10, </volume> <pages> 113-152. </pages>
Reference-contexts: As we will show in Section 3.1, several methods now exist that prune trees in an attempt to prevent the fitting of noise, but there is no one pruning method that works best for all tasks <ref> (Schaffer 1993) </ref>, and comparisons with other tree simplification approaches are both rare and non- comprehensive. Overly large trees can become fragmented, having many leaves with only a few cases per leaf. <p> Naturally, some representations are more appropriate for some tasks than others. If no inferences can be made concerning what biases are best for a given database, then all possible algorithms must be compared to determine which performs best for it <ref> (Schaffer 1993) </ref>. Fortunately, some authors have shown that a database's characteristics can be profitably used to heuristically select inductive biases (e.g., Brodley 1993; 1995a; Ting 1994).
Reference: <author> Schlimmer, J. C. and D. </author> <title> Fisher (1986). A case study of incremental concept induction. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 496-501). </pages> <address> Philadelphia, PA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Seshu, R. </author> <year> (1989). </year> <title> Solving the parity problem. </title> <booktitle> In Proceedings of the Fourth European Working Session on Learning (pp. </booktitle> <pages> 263-271). </pages> <address> Montpellier, France: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sethi, I. K. and G. P. R. </author> <month> Sarvarayudu </month> <year> (1982). </year> <title> Hierarchical classifier design using mutual informa-tion. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 4, </volume> <pages> 441-445. </pages>
Reference-contexts: Rather than using a local test selection measure that is insensitive to the number of cases in a node (Li and Dubes 1986; Zhou and Dillon 1991; Kalkanis 1993) or adopting a global measure <ref> (Sethi and Sarvarayudu 1982) </ref>, researchers solved this problem by using stopping criteria that use a different measure than that used for selecting tests.
Reference: <author> Smyth, P., A. Gray, and U. M. </author> <title> Fayyad (1995). Retrofitting decision tree classifiers using kernel density estimation. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning (pp. </booktitle> <pages> 506-514). </pages> <address> Tahoe City, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: These approaches retain the training cases and use the decision tree to retrieve the cases stored at the leaf node corresponding to the current test case. The cases retrieved are then subjected to a further classifier, such as k-nearest neighbor (Barletta 1994) or kernel density estimators <ref> (Smyth et al. 1995) </ref>. Both approaches substantially improve accuracy over decision trees alone. This suggests the possibility of over-pruning a decision 17 tree within a hybrid algorithm without sacrificing accuracy. The reduction in accuracy that would normally result from over-pruning may be offset by the contribution of the second classifier.
Reference: <author> Thornton, C. J. </author> <year> (1990). </year> <title> The complexity of constructive induction (Technical Report 463). </title> <institution> Edinburgh, Scotland: University of Edinburgh, Department of Artificial Intelligence. </institution>
Reference-contexts: example, given m feature combination operators that can each apply to up to (one subset of) f primitive Boolean features, the space of feature combinations is O (m2 f ), and choosing which subset of these constructed features to apply yields a space of size O (2 m2 f ) <ref> (Thornton 1990) </ref>. The complexity of iterative constructive induction algorithms is even greater (see Section 3.2.2). <p> These methods construct new features in binary trees by iteratively applying simple logical operators (e.g., conjunction, disjunction, negation) to combine primitive features. They iteratively create a tree and then induce new features. Considering all available features for constructive induction would produce an enormous space of feature combinations <ref> (Thornton 1990) </ref>. The analysis in Section 3.2.1 showed that a single iteration explores O (m2 f ) feature combinations for f primitive Boolean features and m feature combination operators.
Reference: <author> Ting, K. M. </author> <year> (1994). </year> <title> The problem of small disjuncts: Its remedy in decision trees. </title> <booktitle> In Proceedings of the Tenth Canadian Conference on Artificial Intelligence (pp. </booktitle> <pages> 91-97). </pages>
Reference: <author> Utgoff, P. E. </author> <year> (1988a). </year> <title> ID5: An incremental ID3. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning (pp. </booktitle> <pages> 107-120). </pages> <address> Ann Arbor, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Thus, our algorithm greedily selects a test at each node that maximizes eval (). This limits make tree's complexity to O (jCjf 2 ), where f is the number of features <ref> (Utgoff 1988a) </ref>. While efficient, this greedy search is subject to horizon effects (i.e., it selects tests based on local rather than global measures), and so risks being trapped by local minima. In this survey, we discuss several proposed solutions for reducing this risk.
Reference: <author> Utgoff, P. E. </author> <year> (1988b). </year> <title> Perceptron trees: A case study in hybrid concept representations. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence (pp. </booktitle> <pages> 601-606). </pages> <address> St. Paul, MN: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Utgoff, P. E. </author> <year> (1989a). </year> <title> Improved training via incremental learning. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning (pp. </booktitle> <address> 362-365) Ithaca, NY: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Utgoff, P. E. </author> <year> (1989b). </year> <title> Perceptron trees: A case study in hybrid concept representations. </title> <journal> Connection Science, </journal> <volume> 1, </volume> <pages> 377-391. </pages>
Reference: <author> Utgoff, P. E. </author> <year> (1994). </year> <title> An improved algorithm for incremental induction of decision trees. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning (pp. </booktitle> <pages> 318-325). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Since pruning is more beneficial with larger training samples (Fisher and Schlimmer 1988; Kim and Koehler 1994), it makes sense to revisit earlier pruning decisions after accumulating additional cases. A less purely incremental approach is dynamic pruning (Langley 1996, p. 232) or virtual pruning <ref> (Utgoff 1994) </ref>. In this approach, the decision tree structure can be created statically at the outset. However, it is used in a dynamic manner: the entire tree need not be used for classification. <p> Utgoff and Clouse extend KS for discrete variables and missing values. The KS test is currently restricted to binary classification tasks. Using the ITI <ref> (Utgoff 1994) </ref> incremental decision tree induction algorithm, Utgoff and Clouse compared using KS versus gain ratio for selecting tests in experiments with 27 data sets. While the classification accuracies did not differ significantly, KS yielded significantly smaller trees for these data sets. <p> Using available software 16 , we compared C4.5 release 7 (Quinlan 1993) (with and without EBP pruning, inducing trees or rules (C4.5rules)), C4.5 release 8 (Quinlan 1996) (tree induction with post-pruning), ITI <ref> (Utgoff 1994) </ref> (with the gain ratio and with the KS (Utgoff and Clouse 1996) test selection measures), RobustC4.5 (John 1995), 16 Some of the best algorithms listed in Table 4 were not tested because the software was not available. 35 Table 4: Some promising tree-simplification algorithms.
Reference: <author> Utgoff, P. E. and C. E. </author> <title> Brodley (1990). An incremental method for finding multivariate splits for decision trees. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning (pp. </booktitle> <pages> 58-65). </pages> <address> Austin, TX: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Utgoff, P. E. and C. E. </author> <title> Brodley (1991). Linear machine decision trees (Technical Report 10). </title> <address> Amherst, MA: </address> <institution> University of Massachusetts, Department of Computer Science. </institution> <note> 46 Utgoff, </note> <author> P. E. and J. A. </author> <month> Clouse </month> <year> (1996). </year> <title> A Kolmogorov-Smirnoff metric for decision tree induction (Technical Report 96-3). </title> <institution> Amherst, Massachusetts: University of Massachusetts, Department of Computer Science. </institution>
Reference-contexts: OC1 produced slightly smaller and more accurate trees than SADT in their experiments, but in much less time, and its performance improved with the amount of randomization search used. In comparison with an extension of the perceptron tree algorithm, LMDT <ref> (Utgoff and Brodley 1991) </ref>, OC1 showed advantages in terms of accuracy and tree size when the cases were not linearly separable. Most of these approaches use post-pruning techniques similar to those described in Section 3.1.2.
Reference: <author> Vadera, S. and S. </author> <month> Nechab </month> <year> (1994). </year> <title> ID3, its children and their safety. </title> <journal> BCS Specialist Group on Expert Systems Newsletter, </journal> <volume> 31, </volume> <pages> 11-21. </pages>
Reference: <author> Wallace, C. S. and J. D. </author> <title> Patrick (1993). Code decision trees. </title> <journal> Machine Learning, </journal> <volume> 11, </volume> <pages> 7-22. </pages>
Reference: <author> Webb, G. I. </author> <year> (1996). </year> <title> Further experimental evidence against the utility of Occam's razor. </title> <journal> JAIR, </journal> <volume> 4, </volume> <pages> pp. 397-417. </pages>
Reference-contexts: Some of these methods sacrifice comprehensibility for the sake of accuracy (e.g., methods that create a set of decision trees (e.g., Oliver and Hand 1995) or methods that yield small gains in accuracy at the price of large increases in tree size <ref> (e.g., Webb 1996) </ref>), while others fine-tune the balance between accuracy and comprehensibility (e.g., Breiman et al. 1984; Bohanec and Bratko 1994; Holder 1995).
Reference: <author> Weiss, S. and N. </author> <title> Indurkhya (1994a). Small sample decision tree pruning. </title> <booktitle> Proceedings of the Eleventh International Machine Learning Conference (pp. </booktitle> <pages> 335-342). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Weiss, S. H. and N. </author> <title> Indurkhya (1994b). Decision tree pruning: </title> <booktitle> Biased or optimal? In Proceedings of the Twelfth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 626-632). </pages> <address> Seattle, WA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Weiss, S. M. and N. </author> <title> Indurkhya (1991). Reduced complexity rule induction. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 678-684). </pages> <address> Anaheim, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: One alternative might be to generate rules directly, without initial tree creation. On many tasks, rule learning systems are more accurate than decision tree learners (Pagallo and Haussler 1990; Weiss and Indurkhya 1991). They often produce simpler data structures <ref> (Weiss and Indurkhya 1991) </ref>, although Gaines (1996) found that they increase complexity for two small data sets without noise.
Reference: <author> Wilson, D. </author> <year> (1972). </year> <title> Asymptotic properties of nearest neighbor rules using edited data. </title> <journal> Institute of Electrical and Electronic Engineers Transactions on Systems, Man and Cybernetics, </journal> <volume> 2, </volume> <pages> 408-421. </pages>
Reference-contexts: Indeed, incrementality together with case selection has been shown to reduce tree size and increase accuracy (Utogff 1989a; 1994). 14 This is similar to the distinction between edited nearest neighbor algorithms that delete misclassified cases (Hart 1968) versus those that delete correctly classified cases <ref> (Wilson 1972) </ref>. 29 3.4.2 Feature Selection Since decision tree induction can be misled by the presence of irrelevant features, several researchers have investigated algorithms for removing such features prior to decision tree induction (Almuallim and Dietterich 1991; Doak 1992; Kira and Rendell 1992a, 1992b).
Reference: <author> Wirth, J. and J. </author> <title> Catlett (1988). Experiments on the costs and benefits of windowing in ID3. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning (pp. </booktitle> <pages> 87-99). </pages> <address> Ann Arbor, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Windowing did not produce significantly smaller trees <ref> (Wirth and Catlett 1988) </ref>. However, Utgoff's (1989a; 1994) sampling strategy, which randomly selects only a single case both during induction and when extending the training sample, was more successful in reducing tree size.
Reference: <author> Yang, D., G. Blix, and L. A. </author> <title> Rendell (1991a). The replication problem: A constructive induction approach. </title> <booktitle> In Proceedings of the European Working Session on Learning (pp. </booktitle> <pages> 44-61). </pages> <address> Porto, Portugal: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Yang, D., G. Blix, and L. A. </author> <title> Rendell (1991b). A scheme for feature construction and a comparison of empirical methods. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 699-704). </pages> <address> Sydney, Australia: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Zheng, Z. </author> <year> (1992). </year> <title> Constructing conjuntive tests for decision trees. </title> <booktitle> In Proceedings of the Fifth Australian Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 355-360). </pages> <address> Hobart, Australia: </address> <publisher> World Scientific Publisher. </publisher>
Reference-contexts: The selected test is added to the set of possible features to use at subsequent nodes; thus, this algorithm performs constructive induction. Zheng reported that XofN tends to yield higher accuracies than a conjunctive feature construction algorithm <ref> (Zheng 1992) </ref>, ID2-of-3, and C4.5, both for some artificial and benchmark data sets, and frequently also reduced tree size compared with the latter two algorithms. However, XofN is much slower than C4.5 due to its increased search effort.
Reference: <author> Zheng, Z. </author> <year> (1995). </year> <title> Constructing nominal X-of-N attributes. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 1064-1070). </pages> <address> Montreal: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Search Selection Measures MDL + Kolmogorov-Smirnoff C-SEP + Lubinsky's (1995) hybrid Discretization Global Discretization + T2 Lookahead LFC + Database Restrictions Case Selection RobustC4.5 Feature Selection SET-Gen Alternative Data Decision Graphs EODG Structure EDAG Rules C4.5rules + RULER + T2 (Auer et al. 1995), LMDT (Brodley and Utgoff 1995), XofN <ref> (Zheng 1995) </ref>, and SET-Gen (Cherkauer and Shavlik 1996) on eight data sets. Below we summarize highlights from our survey and comparison study according to each of the five top-level categories in our framework. Among the techniques that explicitly control tree size, post-pruning methods have shown the greatest promise to date.
Reference: <author> Zhou, X. J. and T. S. </author> <title> Dillon (1991). A statistical-heuristic feature selection criterion for decision tree induction. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13, </volume> <pages> 834-841. 47 </pages>
References-found: 135

