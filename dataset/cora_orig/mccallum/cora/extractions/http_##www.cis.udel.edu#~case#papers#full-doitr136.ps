URL: http://www.cis.udel.edu/~case/papers/full-doitr136.ps
Refering-URL: http://www.cis.udel.edu/~case/colt.html
Root-URL: http://www.cis.udel.edu
Email: case@cis.udel.edu  sanjay@iscs.nus.sg  slange@informatik.uni-leipzig.de  thomas@i.kyushu-u.ac.jp  
Title: Incremental Concept Learning for Bounded Data Mining  
Author: John Case Sanjay Jain Steffen Lange Thomas Zeugmann 
Address: 19716, USA  Lower Kent Ridge Road Singapore 119260, Rep. of Singapore  04109 Leipzig, Germany  Fukuoka 812-81 Japan  
Affiliation: Department of CIS University of Delaware Newark DE  Department of ISCS National University of Singapore  Universitat Leipzig Fakultat fur Mathematik und Informatik Institut fur Mathematik und Informatik  Department of Informatics Kyushu University  
Abstract: Important refinements of concept learning in the limit from positive data considerably restricting the accessibility of input data are studied. Let c be any concept; every infinite sequence of elements exhausting c is called positive presentation of c. In all learning models considered the learning machine computes a sequence of hypotheses about the target concept from a positive presentation of it. With iterative learning, the learning machine, in making a conjecture, has access to its previous conjecture and the latest data item coming in. In k-bounded example-memory inference (k is a priori fixed) the learner is allowed to access, in making a conjecture, its previous hypothesis, its memory of up to k data items it has already seen, and the next element coming in. In the case of k-feedback identification, the learning machine, in making a conjecture, has access to its previous conjecture, the latest data item coming in, and, on the basis of this information, it can compute k items and query the database of previous data to find out, for each of the k items, whether or not it is in the database (k is again a priori fixed). In all cases, the sequence of conjectures has to converge to a hypothesis correctly describing the target concept. Our results are manyfold. An infinite hierarchy of more and more powerful feedback learners in dependence on the number k of queries allowed to be asked is established. However, the hierarchy collapses to 1-feedback inference if only indexed families of infinite concepts are considered, and moreover, its learning power is then equal to learning in the limit. But it remains infinite for concept classes of only infinite r.e. concepts. Both k-feedback inference and k-bounded example-memory identification are more powerful than iterative learning but incomparable to one another. Furthermore, there are cases where redundancy in the hypothesis space is shown to be a resource increasing the learning power of iterative learners. Finally, the union of at most k pattern languages is shown to be iteratively inferable. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Angluin, </author> <title> Finding patterns common to a set of strings, </title> <journal> Journal of Computer and System Science 21 (1980), </journal> <pages> 46-62. </pages>
Reference-contexts: Theorem 8 shows that, even in fairly concrete contexts, with iterative learning, redundancy in the hypothesis space increases learning power. Angluin's <ref> [1] </ref> pattern languages are learnable from positive data, and they (and finite unions thereof) have been extensively studied and applied to molecular biology and to the learning of interesting special classes of logic programs (see the references in Section 3.4 below). <p> Instead, it either output an index for the corresponding finite language or, in case enough evidence has been presented, the index ^| k for L hk;0i . We omit the details. 3.4. The Pattern Languages The pattern languages (defined two paragraphs below) were formally introduced by An-gluin <ref> [1] </ref> and have been widely investigated (cf., e.g., Salomaa [34, 35], and Shinohara and Arikawa [39] for an overview). Moreover, Angluin [1] proved that the class of all pattern languages is learnable in the limit from positive data. <p> We omit the details. 3.4. The Pattern Languages The pattern languages (defined two paragraphs below) were formally introduced by An-gluin <ref> [1] </ref> and have been widely investigated (cf., e.g., Salomaa [34, 35], and Shinohara and Arikawa [39] for an overview). Moreover, Angluin [1] proved that the class of all pattern languages is learnable in the limit from positive data. Subsequently, Nix [30] as well as Shi-nohara and Arikawa [39] outlined interesting applications of pattern inference algorithms. <p> More recently, using similar techniques, Krishna-Rao [32] has established the learnability from only positive data of an even larger class of logic programs. These results have consequences for Inductive Logic Programming. 6 Patterns and pattern languages are defined as follows (cf. Angluin <ref> [1] </ref>). Let A = f0; 1; . . .g be any non-empty finite alphabet containing at least two elements. By A fl we denote the free monoid over A (cf. Hopcroft and Ullman [19]). <p> Club stands for consistent least upper bounds. As already mentioned above, the class PAT is Txt Ex Pat -learnable from positive data (cf. Angluin <ref> [1] </ref>). Subsequently, Lange and Wiehagen [23] showed PAT to be TxtItEx Pat - inferable. As for unions, the first result goes back to Shinohara [37] who proved the class of all unions of at most two pattern languages to be in Txt Ex Pat (2 ) .
Reference: [2] <author> D. Angluin, </author> <title> Inductive inference of formal languages from positive data, </title> <booktitle> Information and Control 45 (1980), </booktitle> <pages> 117-135. </pages>
Reference-contexts: Hence, we also refer to the index i of W i as a grammar for W i . Furthermore, we sometimes consider the scenario that indexed families of recursive languages have to be learned (cf. Angluin <ref> [2] </ref>). Let be any finite alphabet of symbols, and let X be the free monoid over , i.e., X = fl . As usual, we refer to subsets L X as to languages. <p> ; . . . of all and only the languages in L and a recursive function f such that for all j 2 IN and all strings x 2 X we have f (j; x) = 1; if x 2 L j ; 0; otherwise: Since the paper of Angluin <ref> [2] </ref> learning of indexed families of languages has attracted much attention (cf., e.g., Zeugmann and Lange [46]). Mainly, this seems due to the fact that most of the established language families such as regular languages, context-free languages, context-sensitive languages, and pattern languages are indexed families. <p> Proof. Let L 2 TxtEx . Without loss of generality, we may assume that there is an IIM M witnessing L 2 T xtEx L (cf. [24]). By Angluin's <ref> [2] </ref> characterization of TxtEx , there is a uniformly recursively generable family (T y j ) j;y2IN of finite telltale sets such that (ff) for all j; y 2 IN, T j T j L j , y j ) exists, (fl) for all j; k 2 IN, T j L <p> Therefore, the comparison of k-feedback learning and k-bounded example-memory inference deserves special interest, and future research should address the problem under what circumstances which model is preferable. Characterizations may serve as suitable tool for accomplishing this goal (cf., e.g., <ref> [2, 7, 47] </ref>). Additionally, feed-back identification and bounded example-memory inference have been considered in the general context of classes of recursively enumerable concepts rather than uniformly recursives ones as done in [25]. As our Theorem 4 shows, there are subtle differences.
Reference: [3] <author> S. Arikawa, T. Shinohara, and A. Yamamoto, </author> <title> Learning elementary formal systems, </title> <booktitle> Theoretical Computer Science 95 (1992), </booktitle> <pages> 97-113. REFERENCES 30 </pages>
Reference-contexts: For example, pattern language learning algorithms have been successfully applied for solving problems in molecular biology (cf., e.g. Shimozono et al. [36], Shinohara and Arikawa [39]). Pattern languages and finite unions of pattern languages turn out to be subclasses of Smullyan's [41]) elementary formal systems (EFS). Arikawa et al. <ref> [3] </ref> have shown that EFS can also be treated as a logic programming language over strings. Recently, the techniques for learning finite unions of pattern languages have been extended to show the learnability of various subclasses of EFS (cf. Shinohara [38]).
Reference: [4] <author> H. Arimura and T. Shinohara, </author> <title> Inductive inference of Prolog programs with linear data dependency from positive data, </title> <booktitle> In Proc. Information Modeling and Knowledge Bases V, </booktitle> <pages> pp. 365-375, </pages> <publisher> IOS Press, </publisher> <year> 1994. </year>
Reference-contexts: Shinohara [38]). From a theoretical point of view, investigations of the learnability of subclasses of EFS are important because they yield corresponding results about the learnability of subclasses of logic programs. Arimura and Shinohara <ref> [4] </ref> have used the insight gained from the learnability of EFS subclasses to show that a class of linearly covering logic programs with local variables is identifiable in the limit from only positive data.
Reference: [5] <author> M. Blum M, </author> <title> A machine independent theory of the complexity of recursive functions, </title> <journal> Journal of the ACM 14 (1967), </journal> <pages> 322-336. </pages>
Reference-contexts: The quantifiers ` 1 1 9 ,' and `9!' are interpreted as `for all but finitely many,' `there exists infinitely many,' and `there exists a unique,' respectively (cf. <ref> [5] </ref>). By h; i: IN fi IN ! IN we denote Cantor's pairing function. 3 Moreover, we let 1 and 2 denote the corresponding projection functions over IN to the first and second components, respectively. <p> Let ' 0 ; ' 1 ; ' 2 ; . . . denote any fixed acceptable programming system (cf. [33] for all (and only) the partial recursive functions over IN, and let 0 ; 1 ; 2 ; . . . be any associated complexity measure (cf. Blum <ref> [5] </ref>). Then ' k is the partial recursive function computed by program k. Furthermore, let k; x 2 IN; if ' k (x) is defined (abbr. ' k (x)#) then we also say that ' k (x) converges; otherwise ' k (x) diverges.
Reference: [6] <author> R. Brachman and T. Anand, </author> <title> The process of knowledge discovery in databases: A human centered approach, In (U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, Eds.), </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pp. 37-58, </pages> <address> Menlo Park, CA, </address> <publisher> AAAI Press, </publisher> <year> 1996. </year>
Reference-contexts: Clearly, a major problem involved concerns the choice of the right sampling size. One way proposed to solve this problem as well as other problems related to huge data sets is interaction and iteration (cf., e.g., Brachman and Anand <ref> [6] </ref> and Fayyad et al. [14]). That is, the whole data mining process is iterated a few times, thereby allowing human interaction until a satisfactory interpretation of the data is found. Looking at data mining from the perspective described above, it becomes a true limiting process.
Reference: [7] <author> M. Blum and L. Blum, </author> <title> Toward a mathematical theory of inductive inference, </title> <booktitle> Information and Control 28 (1975), </booktitle> <pages> 125-155. </pages>
Reference-contexts: Wright [44] extended this result to PAT (k) 2 Txt Ex Pat (k) for all k 1. Moreover, Theorem 4.2 in Shinohara and Arimura's [40] together with a lemma from Blum and Blum's <ref> [7] </ref> shows that S is not Txt Ex H -inferable for every hypothesis space H. However, nothing was known previous to the present paper concerning the incremental learnability of PAT (k). <p> Therefore, the comparison of k-feedback learning and k-bounded example-memory inference deserves special interest, and future research should address the problem under what circumstances which model is preferable. Characterizations may serve as suitable tool for accomplishing this goal (cf., e.g., <ref> [2, 7, 47] </ref>). Additionally, feed-back identification and bounded example-memory inference have been considered in the general context of classes of recursively enumerable concepts rather than uniformly recursives ones as done in [25]. As our Theorem 4 shows, there are subtle differences.
Reference: [8] <author> J. </author> <title> Case, Periodicity in generation of automata, </title> <booktitle> Mathematical Systems Theory 8 (1974), </booktitle> <pages> 15-32. </pages>
Reference-contexts: For any finite sequence t , let ProgSet (M ; t ) = f 1 (M fl ()) t g, and we define for any text T the set ProgSet (M ; T ) similarly. Then by implicit use of the Operator Recursion Theorem (cf. <ref> [8, 10] </ref>) there exists a recursive 1-1 increasing function p, such that W p () may be defined as follows (p (0) will be nice). Enumerate h0; p (0)i in W p (0) . Let 0 be such that content ( 0 ) = fh0; p (0)ig. <p> By the Operator Recursion Theorem <ref> [8, 10] </ref> there exists a recursive 1-1 increasing function p such that W p () may be defined as follows. Initially enumerate h0; p (0)i in W p (0) . Let 0 be such that content ( 0 ) = fh0; p (0)ig. Let avail = 1.
Reference: [9] <author> J. </author> <title> Case, The power of vacillation, </title> <editor> In (D. Haussler and L. Pitt, Eds.), </editor> <booktitle> Proceedings of the Workshop on Computational Learning Theory, </booktitle> <pages> pp. 133-142, </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1988. </year>
Reference-contexts: The following theorem clarifies the relation between Gold's [18] classical learning in the limit and TxtFex -inference. The assertion remains true even if the learner is only allowed to vacillate between up to 2 descriptions, i.e., in the case jDj 2 (cf. Case <ref> [9, 11] </ref>). Theorem 1 (Osherson et al. [31]; Case [9, 11]). T xtEx a T xtF ex a , for all a 2 IN [fflg. <p> The assertion remains true even if the learner is only allowed to vacillate between up to 2 descriptions, i.e., in the case jDj 2 (cf. Case <ref> [9, 11] </ref>). Theorem 1 (Osherson et al. [31]; Case [9, 11]). T xtEx a T xtF ex a , for all a 2 IN [fflg.
Reference: [10] <author> J. </author> <title> Case, Infinitary self-reference in learning theory, </title> <journal> Journal of Experimental & Theoretical Artificial Intelligence 6 (1994), </journal> <pages> 3-16. </pages>
Reference-contexts: For any finite sequence t , let ProgSet (M ; t ) = f 1 (M fl ()) t g, and we define for any text T the set ProgSet (M ; T ) similarly. Then by implicit use of the Operator Recursion Theorem (cf. <ref> [8, 10] </ref>) there exists a recursive 1-1 increasing function p, such that W p () may be defined as follows (p (0) will be nice). Enumerate h0; p (0)i in W p (0) . Let 0 be such that content ( 0 ) = fh0; p (0)ig. <p> By the Operator Recursion Theorem <ref> [8, 10] </ref> there exists a recursive 1-1 increasing function p such that W p () may be defined as follows. Initially enumerate h0; p (0)i in W p (0) . Let 0 be such that content ( 0 ) = fh0; p (0)ig. Let avail = 1.
Reference: [11] <author> J. </author> <title> Case, </title> <booktitle> The power of vacillation in language learning, Technical Report LP-96-08, Logic, Philosophy and Linguistics Series of the Institute for Logic, Language and Computation, </booktitle> <address> University of Amsterdam, The Netherlands, </address> <year> 1996. </year> <note> To appear revised in SIAM Journal on Computing. </note>
Reference-contexts: It is arguable that all natural languages are infinite, and 2. many language learning unsolvability results depend strongly on including the finite languages (cf. Gold [18]; Case <ref> [11] </ref>). <p> The following theorem clarifies the relation between Gold's [18] classical learning in the limit and TxtFex -inference. The assertion remains true even if the learner is only allowed to vacillate between up to 2 descriptions, i.e., in the case jDj 2 (cf. Case <ref> [9, 11] </ref>). Theorem 1 (Osherson et al. [31]; Case [9, 11]). T xtEx a T xtF ex a , for all a 2 IN [fflg. <p> The assertion remains true even if the learner is only allowed to vacillate between up to 2 descriptions, i.e., in the case jDj 2 (cf. Case <ref> [9, 11] </ref>). Theorem 1 (Osherson et al. [31]; Case [9, 11]). T xtEx a T xtF ex a , for all a 2 IN [fflg.
Reference: [12] <author> J. Case and C.H. Smith, </author> <title> Comparison of identification criteria for machine inductive inference, </title> <booktitle> Theoretical Computer Science 25 (1983), </booktitle> <pages> 193-220. </pages>
Reference-contexts: TxtEx a denotes the collection of all concept classes C for which there are an IIM M and a hypothesis space H such that M TxtEx a H -infers C. 6 The a represents the number of mistakes or anomalies allowed in the final conjectures (cf. Case and Smith <ref> [12] </ref>), with a = 0 being Gold's [18] original case where no mistakes are allowed. If a = 0, we usually omit the upper index, e.g., we write TxtEx instead of TxtEx 0 . We adopt this convention in the definitions of the learning types below. <p> Instead, we may learn a small number of equivalent grammars each of which is easier to apply than the others in quite different situations. This speculation directly suggests the following definition. Definition 2 (Case and Smith <ref> [12] </ref>). Let C be a concept class, let c be a concept, let H = (h j ) j2IN be a hypothesis space, and let a 2 IN [ fflg. <p> It is easy to see that L 2 Txt Fb 0 Ex a+1 0 . However, an easy modification of the proof of Ex a+1 n Ex a 6= ; from <ref> [12] </ref> shows that L 62 TxtFex a . 3.2.
Reference: [13] <author> U.M. Fayyad, S.G. Djorgovski, and N. Weir, </author> <title> Automating the analysis and cataloging of sky surveys, In (U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, Eds.), </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pp. 471-494, </pages> <address> Menlo Park, CA, </address> <publisher> AAAI Press, </publisher> <year> 1996. </year>
Reference-contexts: Usually, the data are available from massive data sets collected, for example, by scientific instruments (cf., e.g.,Fayyad et al. <ref> [13] </ref>), by scientists all over the world (as in the human genome project), or in databases that have been built for other purposes than a current purpose. We shall be mainly concerned with the extraction of concepts in the data mining process. <p> We shall be mainly concerned with the extraction of concepts in the data mining process. Thereby, we emphasize the aspect of working with huge data sets. For example, in Fayyad et al. <ref> [13] </ref> the SKICAT-system is described which operates on 3 terabytes of image data originating from approximately 2 billion sky objects which had to be classified.
Reference: [14] <author> U.M. Fayyad, G. Piatetsky-Shapiro, and P. Smyth, </author> <title> From data mining to knowledge discovery: An overview, In (U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthu-rusamy, Eds.), </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pp. 1-34, </pages> <address> Menlo Park, CA, </address> <publisher> AAAI Press, </publisher> <year> 1996. </year>
Reference-contexts: Thus, the additional steps such as data presentation, data selection, incorporating prior knowledge, and defining the semantics of the results obtained belong to KDD (cf., e.g., Fayyad et al. <ref> [14] </ref>). Prominent examples of KDD applications in health care and finance include Matheus et al. [27] and Kloesgen [22]. <p> Clearly, a major problem involved concerns the choice of the right sampling size. One way proposed to solve this problem as well as other problems related to huge data sets is interaction and iteration (cf., e.g., Brachman and Anand [6] and Fayyad et al. <ref> [14] </ref>). That is, the whole data mining process is iterated a few times, thereby allowing human interaction until a satisfactory interpretation of the data is found. Looking at data mining from the perspective described above, it becomes a true limiting process.
Reference: [15] <author> G. </author> <title> File, The relation of two patterns with comparable languages, </title> <editor> In (R. Cori and M. Wirsing, Eds.), </editor> <booktitle> Proceedings of the 5th Ann. Symposium on Theoretical Aspects of Computer Science, Lecture Notes in Computer Science, </booktitle> <volume> Vol. 294, </volume> <pages> pp. 184-192, </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1988. </year>
Reference-contexts: It is also possible to consider erasing substitutions where variables may be replaced by empty strings, leading to a different class of languages (cf. File <ref> [15] </ref>). 25 Note that S 0 can be effectively obtained from s 0 , since every pattern with s 0 2 L () must satisfy jj js 0 j. Thus, there are only finitely many candidate patterns with s 0 2 L () which can be effectively constructed.
Reference: [16] <author> R. Freivalds and C.H. Smith, </author> <title> On the role of procrastination for machine learning, </title> <booktitle> Information and Computation 2 (1993), </booktitle> <pages> 237-271. REFERENCES 31 </pages>
Reference-contexts: Next, we discuss possible extensions of the incremental learning models considered. A natural relaxation of the constraint to fix k a priori can be obtained by using the notion of constructive ordinals as done by Freivalds and Smith <ref> [16] </ref> for mind changes. Intuitively, the paramenter k is now specified to be a constructive ordinal, and the bounded example-memory learner as well as a feedback machine can change their mind of how many data items to store and to ask for, respectively, in dependence on k.
Reference: [17] <author> M. Fulk, S. Jain, and D.N. Osherson, </author> <title> Open problems in systems that learn, </title> <journal> Journal of Computer and System Science 49 (1994), </journal> <pages> 589-604. </pages>
Reference-contexts: k (Q k (M n (T ); x n+1 )) is 1 iff the j-th component of Q k (M n (T ); x n+1 ) appears in T n .) 4 Our definition is a variant of one found in Osherson, Stob and Weinstein [31] and Fulk et al. <ref> [17] </ref> which will be discussed later. 8 Finally, M TxtFb k Ex a H -infers C iff there is computable mapping Q k as described above such that, for each c 2 C, M TxtFb k Ex a H -identifies c. <p> Moreover this separation can be witnessed by a class consisting of only infinite languages. Proof. For every w 2 IN, we define X w = fhj; w; ii 1 j k + 2; i 2 INg, and X 0 5 It follows from Fulk et al. <ref> [17] </ref> and Lange and Zeugmann [25] that there is an infinite hierarchy of successively more powerful bounded example-memory learners in dependence on the number k of items that can be memorized. 9 A number e is said to be nice iff (a) fx 2 IN h0; x; 0i 2 W e <p> Further Comparisons Finally, we turn our attention to the differences and similarities between Definition 4 and a variant of bounded example-memory inference that has been considered in the literature. The following learning type, called k-memory bounded inference, goes back to Fulk et al. <ref> [17] </ref> and is a slight modification of k-memory limited learning defined in Osherson et al. [31], where the learner could just memorize the latest k data items received. It has been thoroughly studied 26 by Fulk et al. [17]. The main differences to Definition 4 are easily explained. <p> learning type, called k-memory bounded inference, goes back to Fulk et al. <ref> [17] </ref> and is a slight modification of k-memory limited learning defined in Osherson et al. [31], where the learner could just memorize the latest k data items received. It has been thoroughly studied 26 by Fulk et al. [17]. The main differences to Definition 4 are easily explained. <p> Moreover, the actual hypothesis computed is allowed to depend on the previous conjecture, the new data item coming in and the newly stored elements. We continue with the formal definition. Subsequently, let % denote the empty sequence. Definition 6 (Fulk et al. <ref> [17] </ref>). <p> Definition 7 (Fulk et al. <ref> [17] </ref>). Let k 2 IN; then we set TxtMb k Ex = fC -(X ) (9k - memory bounded machine M TxtEx -inferring C)g. Our next theorem shows that, for every k, 1-memory bounded inference may outperform k bounded example-memory identification. Theorem 10. <p> Furthermore, let L k be the collection of L 1 and all L m 0 ;...;m k k , m 0 ; . . . ; m k 2 IN. Now, one easily shows that L k =2 TxtBem k Ex using the same ideas as in <ref> [17] </ref>. On the other hand, L k 2 TxtMb 1 Ex . <p> Thus, the corollary follows by Theorem 10. But there is more. The following theorem nicely contrasts Theorem 7 and puts the con dition to use mem (T z ) in computing M fl (T z ) in memory bounded identification as defined in <ref> [17] </ref> into the right perspective. Theorem 12. Txt Fb 1 Ex TxtMb 1 Ex . Proof. It suffices to show that Txt Fb 1 Ex TxtMb 1 Ex , since TxtMb 1 Ex n Txt Fb 1 Ex 6= ; follows immediately from Theorem 7 and Corollary 11. <p> The study undertaken extends previous work done by Osherson et al. [31], Fulk et al. <ref> [17] </ref> and Lange and Zeugmann [25] in various directions. First, the class of all unions of at most k pattern languages has been shown to be iteratively learnable.
Reference: [18] <author> E.M. Gold, </author> <title> Language identification in the limit, </title> <booktitle> Information and Control 10 (1967), </booktitle> <pages> 447-474. </pages>
Reference-contexts: Thus, from a theoretical point of view, it is appropriate to look at the data mining process as an ongoing, incremental one. 3 In the present theoretical study, then, we focus on important refinements or restrictions of Gold's <ref> [18] </ref> model of learning in the limit grammars for concepts from positive instances. 1 Gold's [18] model itself makes the unrealistic assumption that the learner has access to samples of increasingly growing size. Therefore, we investigate refinements that considerably restrict the accessibility of input data. <p> theoretical point of view, it is appropriate to look at the data mining process as an ongoing, incremental one. 3 In the present theoretical study, then, we focus on important refinements or restrictions of Gold's <ref> [18] </ref> model of learning in the limit grammars for concepts from positive instances. 1 Gold's [18] model itself makes the unrealistic assumption that the learner has access to samples of increasingly growing size. Therefore, we investigate refinements that considerably restrict the accessibility of input data. In particular, we deal with so-called iterative learning, bounded example-memory inference, and feedback identification (cf. <p> It is arguable that all natural languages are infinite, and 2. many language learning unsolvability results depend strongly on including the finite languages (cf. Gold <ref> [18] </ref>; Case [11]). <p> Mainly, this seems due to the fact that most of the established language families such as regular languages, context-free languages, context-sensitive languages, and pattern languages are indexed families. Essentially from Gold <ref> [18] </ref> we define an inductive inference machine (abbr. IIM), or simply a learning machine, to be an algorithmic mapping from SEQ to IN [ f?g. Intuitively, we interpret the output of a learning machine with respect to a suitably chosen hypothesis space H. <p> The sequence (M (T y )) y2IN is said to converge to the number j iff in (M (T y )) y2IN all but finitely many terms are equal to j. Now we define some models of learning. We start with Gold's <ref> [18] </ref> unrestricted learning in the limit (and some variants). Then we will present the definitions of the models which more usefully restrict access to the database. Definition 1 (Gold [18]). <p> Now we define some models of learning. We start with Gold's <ref> [18] </ref> unrestricted learning in the limit (and some variants). Then we will present the definitions of the models which more usefully restrict access to the database. Definition 1 (Gold [18]). Let C be a concept class, let c be a concept, let H = (h j ) j2IN be a hypothesis space, and let a 2 IN [fflg. <p> Case and Smith [12]), with a = 0 being Gold's <ref> [18] </ref> original case where no mistakes are allowed. If a = 0, we usually omit the upper index, e.g., we write TxtEx instead of TxtEx 0 . We adopt this convention in the definitions of the learning types below. <p> TxtFex a denotes the collection of all concept classes C for which there are an IIM M and a hypothesis space H such that M TxtFex a H -infers C. The following theorem clarifies the relation between Gold's <ref> [18] </ref> classical learning in the limit and TxtFex -inference. The assertion remains true even if the learner is only allowed to vacillate between up to 2 descriptions, i.e., in the case jDj 2 (cf. Case [9, 11]). Theorem 1 (Osherson et al. [31]; Case [9, 11]). <p> Hence, M 0 converges, a contradiction. This proves Claim 4. Hence, in the case of indexed families of infinite languages, the hierarchy of Theorem 2 collapses for k 2; furthermore, again, for indexed families of infinite languages, the expansion of Gold's <ref> [18] </ref> model, which not only has unrestricted access to the data base, but which also allows finitely many correct grammars output in the limit, achieves no more learning power than feedback identification with only one query of the database. Moreover, our proof shows actually a bit more. <p> Theorem 5. Txt Fb 0 Ex a+1 n TxtFex a 6= ;, for all a 2 IN. Hence, for some concept domains, the model of iterative learning, where we tolerate a + 1 anomalies in the single final grammar, is competent, but the expanded Gold <ref> [18] </ref> model, where we allow unlimited access to the database and finitely many grammars in the limit each with no more than a anomalies, is not. A little extra anomaly tolerance nicely buys, in such cases, no need to remember any past database history or to query it! Proof.
Reference: [19] <author> J.E. Hopcroft and J.D. Ullman, </author> <title> "Formal Languages and their Relation to Automata," </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1969. </year>
Reference-contexts: There are apparent 1 The sub-focus on learning grammars, or, equivalently, recognizers (cf. Hopcroft and Ullman <ref> [19] </ref>), for concepts from positive instances nicely models the situation where the database flags or contains examples of the concept to be learned and doesn't flag or contain the non-examples. 2 That the concepts in the concept classes witnessing this hierarchy are all infinite languages is also interesting and for two <p> Angluin [1]). Let A = f0; 1; . . .g be any non-empty finite alphabet containing at least two elements. By A fl we denote the free monoid over A (cf. Hopcroft and Ullman <ref> [19] </ref>). The set of all finite non-null strings of symbols from A is denoted by A + , i.e., A + = A fl n f"g, where " denotes the empty string. By jAj we denote the cardinality of A.
Reference: [20] <author> S. Jain and A. Sharma, </author> <title> On identification by teams and probabilistic machines, </title> <editor> (In K. P. Jantke and S. Lange, Eds.), </editor> <booktitle> Algorithmic Learning for Knowledge-Based Systems, Lecture Notes in Artificial Intelligence, </booktitle> <volume> Vol. 961, </volume> <pages> pp. 108-145, </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1995. </year>
Reference-contexts: Moreover, it would also be interesting to extend this and the topics of the present paper to probabilistic learning machines. This branch of learning theory has recently seen as variety of surprising results (cf., e.g., <ref> [20, 28, 29] </ref>), and thus, one may expect further interesting insight into the power of probabilism by combining it with incremental learning.
Reference: [21] <author> T. Jiang, A. Salomaa, K. Salomaa, and S. Yu, </author> <title> Inclusion is undecidable for pattern languages, </title> <editor> In (A. Lingas, R. Karlsson, and S. Carlsson, Eds.), </editor> <booktitle> Proceedings of the 20th International Colloquium on Automata, Languages and Programming, Lecture Notes in Computer Science, </booktitle> <volume> Vol. 700, </volume> <pages> pp. 301-312, </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1993. </year>
Reference: [22] <author> W. Kloesgen, </author> <title> Efficient discovery of interesting statements in databases, </title> <journal> Journal of Intelligent Information Systems 4 (1995), </journal> <pages> 53-69. </pages>
Reference-contexts: Thus, the additional steps such as data presentation, data selection, incorporating prior knowledge, and defining the semantics of the results obtained belong to KDD (cf., e.g., Fayyad et al. [14]). Prominent examples of KDD applications in health care and finance include Matheus et al. [27] and Kloesgen <ref> [22] </ref>. The importance of KDD research finds its explanation in the fact that the data collected in various fields such as biology, finance, retail, astronomy, medicine are extremely rapidly growing, while our ability to analyze those data has not kept up proportionally.
Reference: [23] <author> S. Lange and R. Wiehagen, </author> <title> Polynomial-time inference of arbitrary pattern languages, </title> <booktitle> New Generation Computing 8 (1991), </booktitle> <pages> 361-370. </pages>
Reference-contexts: Club stands for consistent least upper bounds. As already mentioned above, the class PAT is Txt Ex Pat -learnable from positive data (cf. Angluin [1]). Subsequently, Lange and Wiehagen <ref> [23] </ref> showed PAT to be TxtItEx Pat - inferable. As for unions, the first result goes back to Shinohara [37] who proved the class of all unions of at most two pattern languages to be in Txt Ex Pat (2 ) . <p> Clearly, once the principal learnability has been established, complexity becomes a central issue. Thus, further research should address the problem of designing time efficient iterative learners for PAT (k). This problem is even unsolved for k = 1. On the one hand, Lange and Wieha-gen <ref> [23] </ref> designed an iterative pattern learner having polynomial update time. Nevertheless, the expected total learning time, i.e., the overall time needed until convergence is exponential in the number of different variables occurring in the target pattern for inputs drawn with respect to the uniform distribution (cf. Zeugmann [45]).
Reference: [24] <author> S. Lange and T. Zeugmann, </author> <title> Learning recursive languages with bounded mind changes, </title> <booktitle> International Journal of Foundations of Computer Science 4 (1993), </booktitle> <pages> 157-178. </pages>
Reference-contexts: Otherwise, output the canonical index in ~ H for T Cons." We leave it to the reader to verify that ~ M witnesses L 2 TxtEx ~ H . Finally, the TxtEx H - inferability of L directly follows from Proposition 1 in Lange and Zeugmann <ref> [24] </ref>, and thus Lemma 1 is proved. Lemma 2. Let L be an indexed family exclusively containing infinite languages such that L 2 TxtEx . <p> Proof. Let L 2 TxtEx . Without loss of generality, we may assume that there is an IIM M witnessing L 2 T xtEx L (cf. <ref> [24] </ref>). <p> Let us consider M 's behavior when fed the text T = a k+1 ; b; b 2 ; . . . for the language L hk;0i . Since L cannot be learned without outputting at least once an overgeneralization (cf. Lange and Zeugman-n <ref> [24] </ref>), there have to be indices k; x 2 IN such that ' k (k) is defined such that k (k) &gt; x and M outputs the hypothesis hk; 0i after processing T x .
Reference: [25] <author> S. Lange and T. Zeugmann, </author> <title> Incremental learning from positive data, </title> <note> Journal of Computer and System Sciences 53 (1996) 88-103. </note>
Reference-contexts: Consequently, a bounded example-memory IIM has to output a hypothesis as well as a subset of the set of examples seen so far. Definition 4 (Lange and Zeugmann <ref> [25] </ref>). Let k 2 IN, let C be a concept class, let c be a concept, let H = (h j ) j2IN be a hypothesis space, and let a 2 IN [fflg. <p> Clearly, by definition, TxtItEx a = TxtBem 0 Ex a , for all a 2 IN [fflg. Finally, we define learning by feedback IIMs. The idea of feedback learning goes back to Wiehagen [42] who considered it in the setting of inductive inference of recursive functions. Lange and Zeugmann <ref> [25] </ref> adapted the concept of feedback learning to inference from positive data. Here, we generalize this definition. Informally, a feedback IIM M is an iterative IIM that is additionally allowed to make a bounded number of a particular type of query. <p> Proof. For every w 2 IN, we define X w = fhj; w; ii 1 j k + 2; i 2 INg, and X 0 5 It follows from Fulk et al. [17] and Lange and Zeugmann <ref> [25] </ref> that there is an infinite hierarchy of successively more powerful bounded example-memory learners in dependence on the number k of items that can be memorized. 9 A number e is said to be nice iff (a) fx 2 IN h0; x; 0i 2 W e g = feg; and w <p> The study undertaken extends previous work done by Osherson et al. [31], Fulk et al. [17] and Lange and Zeugmann <ref> [25] </ref> in various directions. First, the class of all unions of at most k pattern languages has been shown to be iteratively learnable. Moreover, we proved redundancy in the hypothesis space to be a resource extending the learning power of iterative learners in fairly concrete contexts. <p> Nevertheless, the expected total learning time, i.e., the overall time needed until convergence is exponential in the number of different variables occurring in the target pattern for inputs drawn with respect to the uniform distribution (cf. Zeugmann [45]). Second, we considerably generalized the model of feedback inference introduced in <ref> [25] </ref> by allowing the feedback learner to ask simultaneously k queries. <p> Characterizations may serve as suitable tool for accomplishing this goal (cf., e.g., [2, 7, 47]). Additionally, feed-back identification and bounded example-memory inference have been considered in the general context of classes of recursively enumerable concepts rather than uniformly recursives ones as done in <ref> [25] </ref>. As our Theorem 4 shows, there are subtle differences. Furthermore, a closer look at the proof of Theorem 4 directly yields the interesting problem whether or not allowing a learner to ask simultaneously k questions instead of querying one data item per time may speed-up the learning process.
Reference: [26] <author> S. Lange and T. Zeugmann, </author> <title> Set-driven and rearrangement-independent learning of recursive languages, </title> <booktitle> Mathematical Systems Theory 29 (1996), </booktitle> <pages> 599-634. </pages>
Reference: [27] <author> C.J. Matheus, G. Piatetsky-Shapiro, and D. McNeil, </author> <title> Selecting and reporting what is interesting, In (U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, Eds.), </editor> <booktitle> Advances in Knowledge Discovery and Data Mining pp. </booktitle> <pages> 495-515, </pages> <address> Menlo Park, CA, </address> <publisher> AAAI Press, </publisher> <year> 1996. </year>
Reference-contexts: Thus, the additional steps such as data presentation, data selection, incorporating prior knowledge, and defining the semantics of the results obtained belong to KDD (cf., e.g., Fayyad et al. [14]). Prominent examples of KDD applications in health care and finance include Matheus et al. <ref> [27] </ref> and Kloesgen [22]. The importance of KDD research finds its explanation in the fact that the data collected in various fields such as biology, finance, retail, astronomy, medicine are extremely rapidly growing, while our ability to analyze those data has not kept up proportionally.
Reference: [28] <author> L. Meyer, </author> <title> Probabilistic language learning under monotonicity constraints, In (K.P. </title> <editor> Jan-tke, T. Shinohara and T. Zeugmann, Eds.), </editor> <booktitle> Proceedings 6th International Workshop on Algorithmic Learning Theory, ALT'95, Lecture Notes in Artificial Intelligence, </booktitle> <volume> Vol. 997, </volume> <pages> pp. 169-184, </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1995. </year>
Reference-contexts: Moreover, it would also be interesting to extend this and the topics of the present paper to probabilistic learning machines. This branch of learning theory has recently seen as variety of surprising results (cf., e.g., <ref> [20, 28, 29] </ref>), and thus, one may expect further interesting insight into the power of probabilism by combining it with incremental learning.
Reference: [29] <author> L. Meyer, </author> <title> Monotonic and dual monotonic probabilistic language learning of indexed families with high probability, </title> <editor> In (S. Ben-David, Ed.), </editor> <booktitle> Proceedings 3rd European Conference on Computational Learning Theory, EuroColt'97, Lecture Notes in Artificial Intelligence, </booktitle> <volume> Vol. 1208, </volume> <pages> pp. 66-78, </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1997. </year>
Reference-contexts: Moreover, it would also be interesting to extend this and the topics of the present paper to probabilistic learning machines. This branch of learning theory has recently seen as variety of surprising results (cf., e.g., <ref> [20, 28, 29] </ref>), and thus, one may expect further interesting insight into the power of probabilism by combining it with incremental learning.
Reference: [30] <author> R.P. Nix, </author> <title> Editing by examples, </title> <type> Technical Report 280, </type> <institution> Department of Computer Science, Yale University, </institution> <address> New Haven, USA, </address> <year> 1983. </year> <note> REFERENCES 32 </note>
Reference-contexts: Moreover, Angluin [1] proved that the class of all pattern languages is learnable in the limit from positive data. Subsequently, Nix <ref> [30] </ref> as well as Shi-nohara and Arikawa [39] outlined interesting applications of pattern inference algorithms. For example, pattern language learning algorithms have been successfully applied for solving problems in molecular biology (cf., e.g. Shimozono et al. [36], Shinohara and Arikawa [39]).
Reference: [31] <author> D.N. Osherson, M. Stob, and S. Weinstein, </author> <title> "Systems that Learn, An introduction to Learning Theory for Cognitive and Computer Scientists," </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: The assertion remains true even if the learner is only allowed to vacillate between up to 2 descriptions, i.e., in the case jDj 2 (cf. Case [9, 11]). Theorem 1 (Osherson et al. <ref> [31] </ref>; Case [9, 11]). T xtEx a T xtF ex a , for all a 2 IN [fflg. <p> j-th component of A n k (Q k (M n (T ); x n+1 )) is 1 iff the j-th component of Q k (M n (T ); x n+1 ) appears in T n .) 4 Our definition is a variant of one found in Osherson, Stob and Weinstein <ref> [31] </ref> and Fulk et al. [17] which will be discussed later. 8 Finally, M TxtFb k Ex a H -infers C iff there is computable mapping Q k as described above such that, for each c 2 C, M TxtFb k Ex a H -identifies c. <p> The following learning type, called k-memory bounded inference, goes back to Fulk et al. [17] and is a slight modification of k-memory limited learning defined in Osherson et al. <ref> [31] </ref>, where the learner could just memorize the latest k data items received. It has been thoroughly studied 26 by Fulk et al. [17]. The main differences to Definition 4 are easily explained. <p> Given this, a systematic study of incremental learning is important for gaining a better understanding of how different restrictions to the accessibility of input data do affect the resulting inference capabilities of the corresponding learning models. The study undertaken extends previous work done by Osherson et al. <ref> [31] </ref>, Fulk et al. [17] and Lange and Zeugmann [25] in various directions. First, the class of all unions of at most k pattern languages has been shown to be iteratively learnable.
Reference: [32] <author> K. Rao, </author> <title> A class of Prolog programs inferable from positive data, </title> <editor> In (S. Arikawa and A.K. Sharma, Eds.), </editor> <booktitle> Proceedings of the 7th International Workshop on Algorithmic Learning Theory, ALT'96, </booktitle> <pages> pp. 272-284. </pages> <booktitle> Lecture Notes in Artificial Intelligence, </booktitle> <volume> Vol. 1160, </volume> <publisher> Springer-Verlag, </publisher> <year> 1996. </year>
Reference-contexts: Arimura and Shinohara [4] have used the insight gained from the learnability of EFS subclasses to show that a class of linearly covering logic programs with local variables is identifiable in the limit from only positive data. More recently, using similar techniques, Krishna-Rao <ref> [32] </ref> has established the learnability from only positive data of an even larger class of logic programs. These results have consequences for Inductive Logic Programming. 6 Patterns and pattern languages are defined as follows (cf. Angluin [1]).
Reference: [33] <author> H. Rogers, </author> <title> "Theory of Recursive Functions and Effective Computability," </title> <publisher> McGraw Hill, </publisher> <address> New York, 1967. </address> <publisher> Reprinted, MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: Theorem 9 implies that, for each k &gt; 0, the concept class consisting of all unions of at most k pattern languages is learnable (from positive data) by an iterative machine! 2. Preliminaries Unspecified notation follows Rogers <ref> [33] </ref>. In addition to or in contrast with Rogers [33] we use the following. By IN = f0; 1; 2; . . .g we denote the set of all natural numbers. We set IN + = IN n f0g. The cardinality of a set S is denoted by jSj. <p> Theorem 9 implies that, for each k &gt; 0, the concept class consisting of all unions of at most k pattern languages is learnable (from positive data) by an iterative machine! 2. Preliminaries Unspecified notation follows Rogers <ref> [33] </ref>. In addition to or in contrast with Rogers [33] we use the following. By IN = f0; 1; 2; . . .g we denote the set of all natural numbers. We set IN + = IN n f0g. The cardinality of a set S is denoted by jSj. <p> That is, 1 (hx; yi) = x and 2 (hx; yi) = y for all x; y 2 IN. Let ' 0 ; ' 1 ; ' 2 ; . . . denote any fixed acceptable programming system (cf. <ref> [33] </ref> for all (and only) the partial recursive functions over IN, and let 0 ; 1 ; 2 ; . . . be any associated complexity measure (cf. Blum [5]). Then ' k is the partial recursive function computed by program k. <p> Furthermore, let = x 0 ; . . . ; x n1 be any finite sequence. Then we use jj to denote the length n of . Additionally, let T be a text and 3 This function is easily computable, 1-1, and onto (cf. Rogers <ref> [33] </ref>). 5 let t be a finite sequence; then we use T and t to denote the sequence obtained by concatenating onto the front of T and t , respectively. By SEQ we denote the set of all finite sequences of elements from X [ f#g. <p> Claim 2. L 62 Txt Fb k Fex fl . Suppose the converse, i.e., that there are an IIM M and an associated query asking function Q k such that M witnesses L 2 Txt Fb k Fex fl . Then by implicit use of the Recursion Theorem (cf. <ref> [33] </ref>) there exists an e such that W e may be described as follows.
Reference: [34] <author> A. Salomaa, </author> <title> Patterns (The Formal Language Theory Column), </title> <journal> EATCS Bulletin 54 (1994), </journal> <pages> 46-62. </pages>
Reference-contexts: We omit the details. 3.4. The Pattern Languages The pattern languages (defined two paragraphs below) were formally introduced by An-gluin [1] and have been widely investigated (cf., e.g., Salomaa <ref> [34, 35] </ref>, and Shinohara and Arikawa [39] for an overview). Moreover, Angluin [1] proved that the class of all pattern languages is learnable in the limit from positive data. Subsequently, Nix [30] as well as Shi-nohara and Arikawa [39] outlined interesting applications of pattern inference algorithms.
Reference: [35] <author> A. Salomaa, </author> <title> Return to patterns (The Formal Language Theory Column), </title> <journal> EATCS Bulletin 55 (1994), </journal> <pages> 144-157. </pages>
Reference-contexts: We omit the details. 3.4. The Pattern Languages The pattern languages (defined two paragraphs below) were formally introduced by An-gluin [1] and have been widely investigated (cf., e.g., Salomaa <ref> [34, 35] </ref>, and Shinohara and Arikawa [39] for an overview). Moreover, Angluin [1] proved that the class of all pattern languages is learnable in the limit from positive data. Subsequently, Nix [30] as well as Shi-nohara and Arikawa [39] outlined interesting applications of pattern inference algorithms.
Reference: [36] <author> S. Shimozono, A. Shinohara, T. Shinohara, S. Miyano, S. Kuhara, and S. </author> <title> Arikawa, Knowledge acquisition from amino acid sequences by machine learning system BONSAI, </title> <journal> Trans. Information Processing Society of Japan 35 (1994), </journal> <pages> 2009-2018. </pages>
Reference-contexts: Subsequently, Nix [30] as well as Shi-nohara and Arikawa [39] outlined interesting applications of pattern inference algorithms. For example, pattern language learning algorithms have been successfully applied for solving problems in molecular biology (cf., e.g. Shimozono et al. <ref> [36] </ref>, Shinohara and Arikawa [39]). Pattern languages and finite unions of pattern languages turn out to be subclasses of Smullyan's [41]) elementary formal systems (EFS). Arikawa et al. [3] have shown that EFS can also be treated as a logic programming language over strings.
Reference: [37] <author> T. Shinohara, </author> <title> Inferring unions of two pattern languages, </title> <journal> Bulletin of Informatics and Cybernetics 20 (1983), </journal> <pages> 83-88. </pages>
Reference-contexts: Club stands for consistent least upper bounds. As already mentioned above, the class PAT is Txt Ex Pat -learnable from positive data (cf. Angluin [1]). Subsequently, Lange and Wiehagen [23] showed PAT to be TxtItEx Pat - inferable. As for unions, the first result goes back to Shinohara <ref> [37] </ref> who proved the class of all unions of at most two pattern languages to be in Txt Ex Pat (2 ) . Wright [44] extended this result to PAT (k) 2 Txt Ex Pat (k) for all k 1.
Reference: [38] <author> T. Shinohara, </author> <title> Inductive inference of monotonic formal systems from positive data, </title> <booktitle> New Generation Computing 8 (1991), </booktitle> <pages> 371-384. </pages>
Reference-contexts: Arikawa et al. [3] have shown that EFS can also be treated as a logic programming language over strings. Recently, the techniques for learning finite unions of pattern languages have been extended to show the learnability of various subclasses of EFS (cf. Shinohara <ref> [38] </ref>). From a theoretical point of view, investigations of the learnability of subclasses of EFS are important because they yield corresponding results about the learnability of subclasses of logic programs.
Reference: [39] <author> T. Shinohara and S. </author> <title> Arikawa, Pattern inference, In (K.P. </title> <editor> Jantke and S. Lange, Eds.), </editor> <booktitle> Algorithmic Learning for Knowledge-Based Systems, Lecture Notes in Artificial Intelligence, </booktitle> <volume> Vol. 961, </volume> <pages> pp. 259-291, </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1995. </year>
Reference-contexts: We omit the details. 3.4. The Pattern Languages The pattern languages (defined two paragraphs below) were formally introduced by An-gluin [1] and have been widely investigated (cf., e.g., Salomaa [34, 35], and Shinohara and Arikawa <ref> [39] </ref> for an overview). Moreover, Angluin [1] proved that the class of all pattern languages is learnable in the limit from positive data. Subsequently, Nix [30] as well as Shi-nohara and Arikawa [39] outlined interesting applications of pattern inference algorithms. <p> introduced by An-gluin [1] and have been widely investigated (cf., e.g., Salomaa [34, 35], and Shinohara and Arikawa <ref> [39] </ref> for an overview). Moreover, Angluin [1] proved that the class of all pattern languages is learnable in the limit from positive data. Subsequently, Nix [30] as well as Shi-nohara and Arikawa [39] outlined interesting applications of pattern inference algorithms. For example, pattern language learning algorithms have been successfully applied for solving problems in molecular biology (cf., e.g. Shimozono et al. [36], Shinohara and Arikawa [39]). <p> Subsequently, Nix [30] as well as Shi-nohara and Arikawa <ref> [39] </ref> outlined interesting applications of pattern inference algorithms. For example, pattern language learning algorithms have been successfully applied for solving problems in molecular biology (cf., e.g. Shimozono et al. [36], Shinohara and Arikawa [39]). Pattern languages and finite unions of pattern languages turn out to be subclasses of Smullyan's [41]) elementary formal systems (EFS). Arikawa et al. [3] have shown that EFS can also be treated as a logic programming language over strings.
Reference: [40] <author> S. Shinohara and H. Arimura, </author> <title> Inductive inference of unbounded unions of pattern languages from positive data, </title> <editor> In (S. Arikawa and A.K. Sharma, Eds.), </editor> <booktitle> Proceedings 7th International Workshop on Algorithmic Learning Theory, ALT'96, Lecture Notes in Artificial Intelligence, </booktitle> <volume> Vol. 1160, </volume> <pages> pp. 256-271, </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1996. </year>
Reference-contexts: Wright [44] extended this result to PAT (k) 2 Txt Ex Pat (k) for all k 1. Moreover, Theorem 4.2 in Shinohara and Arimura's <ref> [40] </ref> together with a lemma from Blum and Blum's [7] shows that S is not Txt Ex H -inferable for every hypothesis space H. However, nothing was known previous to the present paper concerning the incremental learnability of PAT (k).
Reference: [41] <author> R. </author> <title> Smullyan, </title> <journal> Theory of Formal Systems, Annals of Mathematical Studies, </journal> <volume> No. </volume> <pages> 47. </pages> <address> Princeton, NJ, </address> <year> 1961. </year>
Reference-contexts: For example, pattern language learning algorithms have been successfully applied for solving problems in molecular biology (cf., e.g. Shimozono et al. [36], Shinohara and Arikawa [39]). Pattern languages and finite unions of pattern languages turn out to be subclasses of Smullyan's <ref> [41] </ref>) elementary formal systems (EFS). Arikawa et al. [3] have shown that EFS can also be treated as a logic programming language over strings. Recently, the techniques for learning finite unions of pattern languages have been extended to show the learnability of various subclasses of EFS (cf. Shinohara [38]).
Reference: [42] <author> R. Wiehagen, </author> <title> Limes-Erkennung rekursiver Funktionen durch spezielle Strategien, </title> <journal> Journal of Information Processing and Cybernetics (EIK) 12 (1976), </journal> <pages> 93-99. </pages>
Reference-contexts: Conceptionally, an iterative IIM M defines a sequence (M n ) n2N of machines each of which takes as its input the output of its predecessor. Definition 3 (Wiehagen <ref> [42] </ref>). Let C be a concept class, let c be a concept, let H = (h j ) j2IN be a hypothesis space, and let a 2 IN [ fflg. <p> Clearly, by definition, TxtItEx a = TxtBem 0 Ex a , for all a 2 IN [fflg. Finally, we define learning by feedback IIMs. The idea of feedback learning goes back to Wiehagen <ref> [42] </ref> who considered it in the setting of inductive inference of recursive functions. Lange and Zeugmann [25] adapted the concept of feedback learning to inference from positive data. Here, we generalize this definition.
Reference: [43] <author> R. Wiehagen and T. Zeugmann, </author> <title> Ignoring data may be the only way to learn efficiently, </title> <journal> Journal of Experimental & Theoretical Artificial Intelligence 6 (1994), </journal> <pages> 131-144. </pages>
Reference-contexts: Finally, while the research presented in the present paper clarified what are the strength and limitations of incremental learning, further investigations are necessary dealing with the impact of incremental inference to the complexity of the resulting learner. First results along this line are established in Wiehagen and Zeugmann <ref> [43] </ref>, and we shall see what the future brings concerning this interesting topic.
Reference: [44] <author> K. Wright, </author> <title> Identification of unions of languages drawn from an identifiable class, </title> <editor> In (R. Rivest, D. Haussler, and M. Warmuth, Eds.), </editor> <booktitle> Proceedings of the 2nd Workshop on Computational Learning Theory, </booktitle> <pages> pp. 328-333, </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1989. </year> <note> REFERENCES 33 </note>
Reference-contexts: Angluin [1]). Subsequently, Lange and Wiehagen [23] showed PAT to be TxtItEx Pat - inferable. As for unions, the first result goes back to Shinohara [37] who proved the class of all unions of at most two pattern languages to be in Txt Ex Pat (2 ) . Wright <ref> [44] </ref> extended this result to PAT (k) 2 Txt Ex Pat (k) for all k 1. Moreover, Theorem 4.2 in Shinohara and Arimura's [40] together with a lemma from Blum and Blum's [7] shows that S is not Txt Ex H -inferable for every hypothesis space H.
Reference: [45] <author> T. Zeugmann, </author> <title> Lange and Wiehagen's pattern language learning algorithm: An average-case analysis with respect to its total learning time, </title> <journal> Annals of Mathematics and Artificial Intelligence. </journal> <note> to appear 1997. </note>
Reference-contexts: Nevertheless, the expected total learning time, i.e., the overall time needed until convergence is exponential in the number of different variables occurring in the target pattern for inputs drawn with respect to the uniform distribution (cf. Zeugmann <ref> [45] </ref>). Second, we considerably generalized the model of feedback inference introduced in [25] by allowing the feedback learner to ask simultaneously k queries.
Reference: [46] <author> T. Zeugmann and S. Lange, </author> <title> A guided tour across the boundaries of learning recursive languages, (In K.P. </title> <editor> Jantke and S. Lange, Eds.), </editor> <booktitle> Algorithmic Learning for Knowledge-Based Systems, Lecture Notes in Artificial Intelligence, </booktitle> <volume> Vol. 961, </volume> <pages> pp. 190-258, </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1995. </year>
Reference-contexts: f such that for all j 2 IN and all strings x 2 X we have f (j; x) = 1; if x 2 L j ; 0; otherwise: Since the paper of Angluin [2] learning of indexed families of languages has attracted much attention (cf., e.g., Zeugmann and Lange <ref> [46] </ref>). Mainly, this seems due to the fact that most of the established language families such as regular languages, context-free languages, context-sensitive languages, and pattern languages are indexed families. Essentially from Gold [18] we define an inductive inference machine (abbr.
Reference: [47] <author> T. Zeugmann, S. Lange, and S. Kapur, </author> <title> Characterizations of monotonic and dual monotonic language learning, </title> <booktitle> Information and Computation 120 (1995), </booktitle> <pages> 155-173. </pages>
Reference-contexts: Therefore, the comparison of k-feedback learning and k-bounded example-memory inference deserves special interest, and future research should address the problem under what circumstances which model is preferable. Characterizations may serve as suitable tool for accomplishing this goal (cf., e.g., <ref> [2, 7, 47] </ref>). Additionally, feed-back identification and bounded example-memory inference have been considered in the general context of classes of recursively enumerable concepts rather than uniformly recursives ones as done in [25]. As our Theorem 4 shows, there are subtle differences.
References-found: 47

