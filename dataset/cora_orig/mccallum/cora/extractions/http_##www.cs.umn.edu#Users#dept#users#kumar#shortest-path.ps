URL: http://www.cs.umn.edu/Users/dept/users/kumar/shortest-path.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Email: Internet: kumar@cs.umn.edu  Internet: vsingh@mcc.com  
Title: Scalability of Parallel Algorithms for the All-Pairs Shortest Path Problem  
Author: Vipin Kumar Vineet Singh 
Note: This work was partially supported by Army Research Office grant 28408-MA-SDI to the University of Minnesota and by the Army High Performance Computing Research Center at the University of Minnesota. A short version of this paper appeared in the proceedings of the 1990 International Conference on Parallel Processing.  
Date: March 21, 1991  
Address: Minneapolis, MN 55455  3500 West Balcones Center Drive Austin, Texas 78759  
Affiliation: Computer Science Department University of Minnesota  MCC  
Abstract: This paper uses the isoefficiency metric to analyze the scalability of several parallel algorithms for finding shortest paths between all pairs of nodes in a densely connected graph. Parallel algorithms analyzed in this paper have either been previously presented elsewhere or are small variations of them. Scalability is analyzed with respect to mesh, hypercube and shared-memory architectures. We demonstrate that isoefficiency functions are a compact and useful predictor of performance. In fact, previous comparative predictions of some of the algorithms based on experimental results are shown to be incorrect whereas isoefficiency functions predict correctly. We find the classic tradeoffs of hardware cost vs. time and memory vs. time to be represented here as tradeoffs of hardware cost vs. scalability and memory vs. scalability. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman. </author> <title> Data Structures and Algorithms. </title> <booktitle> Computer Science and Information Processing. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1983. </year>
Reference-contexts: Results are summarized in Table 3. We may refer to this algorithm as "Floyd Striped" in the rest of this paper. 10 Sequential Algorithm by Dijkstra We will now briefly present the sequential single-source algorithm by Dijkstra <ref> [1] </ref>. This can be used to solve the all-pairs shortest path problem by repeating the algorithm with every possible source. We will refer to this as Dijkstra's all-pairs shortest path algorithm.
Reference: [2] <author> S. G. Akl. </author> <title> The Design and Analysis of Parallel Algorithms. </title> <publisher> Prentice-Hall, </publisher> <year> 1989. </year>
Reference-contexts: for pseudo-shared memory multiprocessors like BBN Butterfly 4 can be done using the same model as the Cube; however, the proportionality constants are much smaller for BBN Butterfly compared to a Cube such as Intel iPSC/2. 5 4.3 Shared-Memory Parallel Architectures There are many different models of shared-memory parallel processors <ref> [2] </ref>. The one we consider here is the CREW (concurrent read, exclusive write) PRAM model [2]. Memory latency for both reads and writes, when they are allowed, is uniform for any location in memory. <p> as the Cube; however, the proportionality constants are much smaller for BBN Butterfly compared to a Cube such as Intel iPSC/2. 5 4.3 Shared-Memory Parallel Architectures There are many different models of shared-memory parallel processors <ref> [2] </ref>. The one we consider here is the CREW (concurrent read, exclusive write) PRAM model [2]. Memory latency for both reads and writes, when they are allowed, is uniform for any location in memory. <p> But the Source-parallel version of Dijkstra's algorithm has a better scalability than the Checkerboard version of Floyd's algorithm on the Cube architecture. Another very widely used metric is the PT-product (also called cost in <ref> [2] </ref>); i.e., the product of the run time of the parallel algorithm and the number of processors used by it. A lower bound on the PT-product is the run time of the best sequential algorithm for solving the same problem.
Reference: [3] <author> D. P. Bertsekas and J. N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods, chapter 4, page 307. </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: 1 Introduction The problem of finding a shortest path in an explicit graph is an important problem encountered in the study of communication and transportation networks. Considerable attention has been devoted to solving this problem on sequential computers [7] as well as parallel computers <ref> [8, 11, 19, 3, 20] </ref>. This paper analyzes the scalability of a number of parallel algorithms for finding shortest paths between all pairs of nodes in a densely connected graph. The scalability of a parallel algorithm is determined by its capability to effectively utilize increasing number of processors. <p> These algorithms have either been previously presented elsewhere <ref> [11, 19, 3] </ref> or are small variations of them. The scalability of these parallel algorithms is analyzed for mesh, hypercube and shared-memory architectures. It is noteworthy that one of these modified algorithms has much better scalability than other algorithms over a variety of parallel architectures. <p> Therefore, the overall isoefficiency function of the algorithm is fi (p 1:5 ). 7 When there is exactly one element per processor, the pipelined-checkerboard algorithm reduces to the one described by Bertsekas and Tsitsiklis <ref> [3] </ref>. Our development of this parallel algorithm was motivated by the algorithm in [3]. The algorithm in [3] can be easily adapted to work for the case when p &lt; n 2 by using time-slicing to emulate n 2 virtual processors on the p processors available. <p> Therefore, the overall isoefficiency function of the algorithm is fi (p 1:5 ). 7 When there is exactly one element per processor, the pipelined-checkerboard algorithm reduces to the one described by Bertsekas and Tsitsiklis <ref> [3] </ref>. Our development of this parallel algorithm was motivated by the algorithm in [3]. The algorithm in [3] can be easily adapted to work for the case when p &lt; n 2 by using time-slicing to emulate n 2 virtual processors on the p processors available. <p> the overall isoefficiency function of the algorithm is fi (p 1:5 ). 7 When there is exactly one element per processor, the pipelined-checkerboard algorithm reduces to the one described by Bertsekas and Tsitsiklis <ref> [3] </ref>. Our development of this parallel algorithm was motivated by the algorithm in [3]. The algorithm in [3] can be easily adapted to work for the case when p &lt; n 2 by using time-slicing to emulate n 2 virtual processors on the p processors available.
Reference: [4] <author> Steven Brawer. </author> <title> Introduction to Parallel Programming. </title> <publisher> Academic Press, Inc., </publisher> <address> 1250 Sixth Avenue, San Diego, CA 92101, </address> <year> 1989. </year>
Reference-contexts: The equivalent of barrier synchronization <ref> [4] </ref> can be implemented in a mesh at the cost of 4 (t s + t w )( p 1) as follows. First a synchronizing message of one unit is passed along all rows left-ward starting at the processors in the right-most column.
Reference: [5] <author> Gregory T. Byrd, Nakul Saraiya, and Bruce Delagi. </author> <title> Multicast Communication in Multiprocessor Systems. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, pages I-196 to I-200, </booktitle> <month> August </month> <year> 1989. </year>
Reference-contexts: The time for barrier synchronization remains the same as for Mesh. "Mesh Multicomputer with Cut-Through Routing Hardware" is abbreviated as "Mesh-CT" in the rest of the paper. 4.1.3 Mesh Multicomputer with Cut-Through and Multicast Routing Hardware The hardware for cut-through routing can be enhanced to efficiently support multicast in hardware <ref> [5] </ref>. 3 In particular, to multicast a message along a row/column, one need only take as much time as sending a unicast message to the end of the row/column (i.e., fi (m + p p)). Copies of the message are deposited at the intermediate processors without any extra delay. <p> The time to send a message to a single destination as well as for barrier synchronization remain the same as for Mesh-CT. "Mesh Multicomputer with Cut-Through and Multicast Routing Hardware" is abbreviated as "Mesh-CT-MC" in the rest of the paper. 3 Strictly speaking, the scheme in <ref> [5] </ref> is a general scheme in which all destination addresses must be listed explicitly. Therefore, the message size of a multicast message along a row would be larger than that of a message sent to just a single destination.
Reference: [6] <author> William J. Dally. </author> <title> A VLSI Architecture for Concurrent Data Structures. </title> <type> PhD thesis, </type> <institution> California Institute of Technology, </institution> <month> March </month> <year> 1986. </year> <month> 23 </month>
Reference-contexts: This also takes 2 (t s +t w )( p time. "Simple Mesh Multicomputer" is abbreviated as "Mesh" in the rest of the paper. 4.1.2 Mesh Multicomputer with Cut-Through Routing Hardware A simple mesh may be augmented with hardware for cut-through routing <ref> [6] </ref>. This can dramatically cut down the time to deliver a message. With cut-through routing, the time to deliver a message containing m words to a processor that is d steps away is reduced from fi (m fi d) to fi (m + d).
Reference: [7] <author> N. Deo and C. Pang. </author> <title> Shortest path algorithms : Taxonomy and annotation. </title> <booktitle> Networks, </booktitle> <pages> pages 275-323, </pages> <year> 1984. </year>
Reference-contexts: 1 Introduction The problem of finding a shortest path in an explicit graph is an important problem encountered in the study of communication and transportation networks. Considerable attention has been devoted to solving this problem on sequential computers <ref> [7] </ref> as well as parallel computers [8, 11, 19, 3, 20]. This paper analyzes the scalability of a number of parallel algorithms for finding shortest paths between all pairs of nodes in a densely connected graph.
Reference: [8] <author> N. Deo, C.Y. Pang, and R.E. Lord. </author> <title> Two parallel algorithms for shortest path problems. </title> <booktitle> In Proceedings of IEEE International Conference on Parallel Processing, </booktitle> <pages> pages 244-253, </pages> <year> 1980. </year>
Reference-contexts: 1 Introduction The problem of finding a shortest path in an explicit graph is an important problem encountered in the study of communication and transportation networks. Considerable attention has been devoted to solving this problem on sequential computers [7] as well as parallel computers <ref> [8, 11, 19, 3, 20] </ref>. This paper analyzes the scalability of a number of parallel algorithms for finding shortest paths between all pairs of nodes in a densely connected graph. The scalability of a parallel algorithm is determined by its capability to effectively utilize increasing number of processors.
Reference: [9] <author> Anshul Gupta and Vipin Kumar. </author> <title> On the scalability of FFT on parallel computers. </title> <booktitle> In Proceedings of the Frontiers 90 Conference on Massively Parallel Computation, </booktitle> <month> October </month> <year> 1990. </year> <note> An extended version of the paper is available as a technical report from the Department of Computer Science, and as TR 90-20 from Army High Performance Computing Research Center, </note> <institution> University of Minnesota, </institution> <address> Minneapolis, MN 55455. </address>
Reference-contexts: The isoefficiency analysis has been found to be very useful in characterizing the scalability of a variety of parallel algorithms <ref> [15, 21, 22, 16, 9, 13] </ref>. By doing isoefficiency analysis, one can test the performance of a parallel program on a small number of processors, and then predict its performance on a larger number of processors.
Reference: [10] <author> John L. Gustafson. </author> <title> Reevaluating Amdahl's Law. </title> <journal> Communications of the ACM, </journal> <volume> 31(5) </volume> <pages> 532-533, </pages> <year> 1988. </year>
Reference-contexts: Again, the usefulness of this is limited to the case in which as many processors are available as can be used by the algorithm. A recently introduced metric that is useful for assessing the performance of parallel algorithms on practically feasible architectures is Scaled Speedup <ref> [10] </ref>. This metric is defined to be the speedup obtained when the problem size is increased linearly with the number of processors. There is a close relation between this metric and isoefficiency.
Reference: [11] <author> J. Jenq and S. Sahni. </author> <title> All Pairs Shortest Paths on a Hypercube Multiprocessor. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 713-716, </pages> <year> 1987. </year>
Reference-contexts: 1 Introduction The problem of finding a shortest path in an explicit graph is an important problem encountered in the study of communication and transportation networks. Considerable attention has been devoted to solving this problem on sequential computers [7] as well as parallel computers <ref> [8, 11, 19, 3, 20] </ref>. This paper analyzes the scalability of a number of parallel algorithms for finding shortest paths between all pairs of nodes in a densely connected graph. The scalability of a parallel algorithm is determined by its capability to effectively utilize increasing number of processors. <p> These algorithms have either been previously presented elsewhere <ref> [11, 19, 3] </ref> or are small variations of them. The scalability of these parallel algorithms is analyzed for mesh, hypercube and shared-memory architectures. It is noteworthy that one of these modified algorithms has much better scalability than other algorithms over a variety of parallel architectures. <p> This property is true of all parallel algorithms. For a large class of parallel algorithms (e.g., parallel DFS [15], parallel shortest path algorithms <ref> [11] </ref>) the following additional property is also true: * For any given number p of processors, the efficiency of the parallel algorithm goes up monoton ically (i.e., it never goes down, and approaches a constant e, s.t. 0 &lt; e 1) if it is used to solve problem instances of increasing <p> Concurrent reads are allowed but only one write to a given location can take place at one time. "Shared-Memory parallel processor" is abbreviated as "SM" in the rest of the paper. 5 Sequential Algorithm by Floyd as given in <ref> [11] </ref>. A matrix P of dimension n fi n is used to store the currently best known shortest distances between every pair of nodes. <p> For this, it needs access to the corresponding segments of the row P [k; j] and the column P [i; k]. We assume that the required initial P values are available on each processor and that the final P values stay on the processors. Jenq and Sahni <ref> [11] </ref> describe two parallel algorithms based on this generic parallel algorithm. We briefly describe and analyze these in sections 7 and 9. In Section 8, we discuss a small variation of one of these algorithms that has better scalability on a variety of architectures. <p> In Section 8, we discuss a small variation of one of these algorithms that has better scalability on a variety of architectures. Although we have tried to make the discussion in Section 7 self-contained, the reader may find it useful to read reference <ref> [11] </ref>. 7 Checkerboard Version of Parallel Floyd Algorithm In this version (given in [11]), the cost matrix P is divided into equal parts of size n p p fi n p p , and each is allocated to a different processor. <p> Although we have tried to make the discussion in Section 7 self-contained, the reader may find it useful to read reference <ref> [11] </ref>. 7 Checkerboard Version of Parallel Floyd Algorithm In this version (given in [11]), the cost matrix P is divided into equal parts of size n p p fi n p p , and each is allocated to a different processor. Each processor has the responsibility to update its allocated part of the matrix in each iteration. <p> Of course, this will be different for different architectures. Consider the case for Cube. As discussed in <ref> [11] </ref>, it is possible to map a virtual mesh of p p on a p-processor hypercube such that each row and column of this virtual mesh is a hypercube of p p processors. <p> The results from this section are summarized in Table 3. We may refer to this improved algorithm as "Floyd Pipelined" in the rest of the paper. 9 Striped Version of Parallel Floyd Algorithm In this version (given in <ref> [11] </ref>), the cost matrix P is divided into equal parts each containing p columns, and each part is allocated to a different processor. Each processor has the responsibility to update its allocated part of the matrix in each iteration. <p> This is predicted by our scalability analysis. On Cube, the isoefficiency function of Floyd Checkerboard is fi (p 1:5 (log p) 3 ) which is substantially better than fi ((p log p) 3 ) (the isoefficiency function of Floyd Striped). In the experiments reported by Jenq and Sahni <ref> [11] </ref>, the difference in the performance of these algorithms was very small. The reason is that they computed speedup after taking the input time into consideration. If the cost matrix has to be loaded serially from outside the multicomputer (which is the case in [11]) or from a single processor in <p> experiments reported by Jenq and Sahni <ref> [11] </ref>, the difference in the performance of these algorithms was very small. The reason is that they computed speedup after taking the input time into consideration. If the cost matrix has to be loaded serially from outside the multicomputer (which is the case in [11]) or from a single processor in the multicomputer, then the reader can verify that no parallel shortest path algorithm has isoefficiency better than fi (p 3 ). <p> This is again explained by scalability analysis, as the isoefficiency function of the Source-Parallel version of Dijkstra's algorithm is better than Floyd Checkerboard. In <ref> [11] </ref>, it is mentioned that Source-Partitioned version of Dijkstra's algorithm will outperform Floyd Checkerboard when p is large. &gt;From Table 3, it is clear that Floyd Checkerboard is much more scalable than the Source-Partitioned version of Dijkstra's algorithm. <p> We did not find it necessary to validate the theoretically derived efficiency figures given in this section by actual experimentation for the following reasons. The time complexity of the two sequential 10 This value is computed from the experimental results given in <ref> [11] </ref>. 16 P! 4 16 64 256 1024 4096 16384 65536 Floyd Pipeline 0.9987 0.9905 0.9420 0.7091 0.2560 0.0441 Floyd Checkerboard 0.9411 0.7272 0.3478 0.1000 0.0229 0.0050 64 Floyd Striped 0.9142 0.5714 0.1818 Dijkstra Partition 0.6250 0.6250 0.6250 Dijkstra Parallel 0.3225 0.0735 0.0135 Floyd Pipeline 0.9999 0.9995 0.9977 0.9882 0.9362 0.6989 <p> It is often incorrect to judge the merit of a parallel algorithm from speedup experiments on a sample of problem sizes and number of processors. For example, <ref> [11] </ref> concluded from a limited set of experiments that Dijkstra Source-Parallel is better than Floyd Checkerboard for large number of processors. Our results show that this is true only for moderate number of processors.
Reference: [12] <author> Clyde P. Kruskal, Larry Rudolph, and Marc Snir. </author> <title> A complexity theory of efficient parallel algorithms. </title> <type> Technical Report RC13572, </type> <institution> IBM TJ-Watson Research Center, </institution> <address> NY, </address> <year> 1988. </year>
Reference: [13] <author> Vipin Kumar and Anshul Gupta. </author> <title> Analyzing the scalability of parallel algorithms and architectures: A survey. </title> <booktitle> In Proceedings of the 1991 International Conference on Supercomputing, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: The isoefficiency analysis has been found to be very useful in characterizing the scalability of a variety of parallel algorithms <ref> [15, 21, 22, 16, 9, 13] </ref>. By doing isoefficiency analysis, one can test the performance of a parallel program on a small number of processors, and then predict its performance on a larger number of processors.
Reference: [14] <author> Vipin Kumar and V. N. Rao. </author> <title> Scalable parallel formulations of depth-first search. </title> <editor> In Vipin Kumar, P. S. Gopalakrishnan, and Laveen Kanal, editors, </editor> <booktitle> Parallel Algorithms for Machine Intelligence and Vision. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: Kumar and Rao have developed a scalability metric, called isoefficiency, which relates the problem size to the number of processors necessary for an increase in speedup in proportion to the number of processors used <ref> [14, 15] </ref>. The isoefficiency analysis has been found to be very useful in characterizing the scalability of a variety of parallel algorithms [15, 21, 22, 16, 9, 13].
Reference: [15] <author> Vipin Kumar and V. Nageshwara Rao. </author> <title> Parallel depth-first search, part II: Analysis. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16 </volume> (6):501-519, 1987. 
Reference-contexts: Kumar and Rao have developed a scalability metric, called isoefficiency, which relates the problem size to the number of processors necessary for an increase in speedup in proportion to the number of processors used <ref> [14, 15] </ref>. The isoefficiency analysis has been found to be very useful in characterizing the scalability of a variety of parallel algorithms [15, 21, 22, 16, 9, 13]. <p> The isoefficiency analysis has been found to be very useful in characterizing the scalability of a variety of parallel algorithms <ref> [15, 21, 22, 16, 9, 13] </ref>. By doing isoefficiency analysis, one can test the performance of a parallel program on a small number of processors, and then predict its performance on a larger number of processors. <p> This property is true of all parallel algorithms. For a large class of parallel algorithms (e.g., parallel DFS <ref> [15] </ref>, parallel shortest path algorithms [11]) the following additional property is also true: * For any given number p of processors, the efficiency of the parallel algorithm goes up monoton ically (i.e., it never goes down, and approaches a constant e, s.t. 0 &lt; e 1) if it is used to
Reference: [16] <author> Vipin Kumar and V. Nageshwara Rao. </author> <title> Load balancing on the hypercube architecture. </title> <booktitle> In Proceedings of the 1989 Conference on Hypercubes, Concurrent Computers and Applications, </booktitle> <pages> pages 603-608, </pages> <year> 1989. </year>
Reference-contexts: The isoefficiency analysis has been found to be very useful in characterizing the scalability of a variety of parallel algorithms <ref> [15, 21, 22, 16, 9, 13] </ref>. By doing isoefficiency analysis, one can test the performance of a parallel program on a small number of processors, and then predict its performance on a larger number of processors.
Reference: [17] <author> Vipin Kumar and Vineet Singh. </author> <title> Scalability of Parallel Algorithms for the All-Pairs Shortest Path Problem: A Summary of Results. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <year> 1990. </year> <note> Extended version available as a technical report from the department of computer science, </note> <institution> University of Minnesota, Minneapolis, </institution> <note> MN 55455 and as MCC TR ACT-OODS-058-90. </note>
Reference-contexts: Finally, Section 14 summarizes the results. A short version of this paper appeared in <ref> [17] </ref>. 2 Definitions and Assumptions In this section, we introduce some terminology used in the rest of the paper.
Reference: [18] <author> Vipin Kumar and Vineet Singh. </author> <title> Scalability of Parallel Algorithms for the All-Pairs Shortest Path Problem. </title> <note> Technical Report ACT-OODS-058-90 (revised Jan. 1991), MCC, 1990. To appear in Journal of Parallel and Distributed Computing (Special Issue on Massively Parallel Computation. A shorter version appeared in proceedings of the 1990 International Conference on Parallel Processing. 24 </note>
Reference-contexts: Recall that it takes time t c to compute the next iteration value for one element in the matrix, and time t s + t w to communicate one value from one processor to its immediate neighbor in Mesh. In <ref> [18] </ref>, we prove 6 that for this algorithm, T p = p p n p Hence, the total work done by all the processors in all the iterations is pT p = p fi ( p p n p = n 3 t c + 4p ( p 1)(t s + <p> p = p p n p Hence, the total work done by all the processors in all the iterations is pT p = p fi ( p p n p = n 3 t c + 4p ( p 1)(t s + p t w ) 6 As stated in <ref> [18] </ref>, the given expression for T p is correct only when the number of elements per processor is more than 1. <p> It is assumed that the value P (k1) [k; k] takes time a to get to the processor with element P (k1) [i; k] and that it takes time b to get to the processor with element P (k1) [k; j]. As shown in Appendix A and B of <ref> [18] </ref>, the time taken for this algorithm remains the same as the previous Pipelined Checkerboard version when there is more than one matrix element per processor. As opposed to the previous case, this expression is also valid for the one element per processor case for the current algorithm.
Reference: [19] <author> Richard C. Paige and Clyde P. Kruskal. </author> <title> Parallel Algorithms for Shortest Path Problems. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <pages> pages 14-19, </pages> <year> 1985. </year>
Reference-contexts: 1 Introduction The problem of finding a shortest path in an explicit graph is an important problem encountered in the study of communication and transportation networks. Considerable attention has been devoted to solving this problem on sequential computers [7] as well as parallel computers <ref> [8, 11, 19, 3, 20] </ref>. This paper analyzes the scalability of a number of parallel algorithms for finding shortest paths between all pairs of nodes in a densely connected graph. The scalability of a parallel algorithm is determined by its capability to effectively utilize increasing number of processors. <p> These algorithms have either been previously presented elsewhere <ref> [11, 19, 3] </ref> or are small variations of them. The scalability of these parallel algorithms is analyzed for mesh, hypercube and shared-memory architectures. It is noteworthy that one of these modified algorithms has much better scalability than other algorithms over a variety of parallel architectures. <p> Here we discuss another variant that can keep more than n processors busy. This parallel variant of Dijkstra's algorithm for all pair shortest path is similar to the previous one except that the single-source algorithm is also run on many processors as described in <ref> [19] </ref>. Thus, each of the n copies of single-source Dijkstra's algorithm is run on a disjoint subset of processors.
Reference: [20] <author> Michael J. Quinn and Narsingh Deo. </author> <title> Data structures for the efficient solution of graph theoretic problems on tightly-coupled MIMD computers. </title> <booktitle> In Proceedings of International Conf. on Parallel Processing, </booktitle> <pages> pages 431-438, </pages> <year> 1984. </year>
Reference-contexts: 1 Introduction The problem of finding a shortest path in an explicit graph is an important problem encountered in the study of communication and transportation networks. Considerable attention has been devoted to solving this problem on sequential computers [7] as well as parallel computers <ref> [8, 11, 19, 3, 20] </ref>. This paper analyzes the scalability of a number of parallel algorithms for finding shortest paths between all pairs of nodes in a densely connected graph. The scalability of a parallel algorithm is determined by its capability to effectively utilize increasing number of processors.
Reference: [21] <author> S. Ranka and S. Sahni. </author> <title> Hypercube Algorithms for Image Processing and Pattern Recognition. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: The isoefficiency analysis has been found to be very useful in characterizing the scalability of a variety of parallel algorithms <ref> [15, 21, 22, 16, 9, 13] </ref>. By doing isoefficiency analysis, one can test the performance of a parallel program on a small number of processors, and then predict its performance on a larger number of processors. <p> By doing isoefficiency analysis, one can test the performance of a parallel program on a small number of processors, and then predict its performance on a larger number of processors. As stated in <ref> [21] </ref>, ": : : with this technique we can eliminate (or at least predict) the often reported observation that while a particular parallel program performed well on a small multicomputer, it was found to perform poorly when ported to a larger multicomputer." The parallel algorithms analyzed in this paper are all
Reference: [22] <author> Vineet Singh, Vipin Kumar, Gul Agha, and Chris Tomlinson. </author> <title> Scalability of parallel sorting on mesh multicomputers. </title> <booktitle> In Proceedings of the Fifth International Parallel ProcessingSymposium, </booktitle> <month> March </month> <year> 1991. </year> <note> Extended version available as a technical report (number TR 90-45) from the department of computer science, </note> <institution> University of Minnesota, Minneapolis, MN 55455, </institution> <note> and as TR ACT-SPA-298-90 from MCC, Austin, Texas. 25 </note>
Reference-contexts: The isoefficiency analysis has been found to be very useful in characterizing the scalability of a variety of parallel algorithms <ref> [15, 21, 22, 16, 9, 13] </ref>. By doing isoefficiency analysis, one can test the performance of a parallel program on a small number of processors, and then predict its performance on a larger number of processors.
References-found: 22

