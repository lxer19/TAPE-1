URL: http://cag-www.lcs.mit.edu/~metcalf/papers/milan.ps.Z
Refering-URL: http://cag-www.lcs.mit.edu/~metcalf/papers/milan/
Root-URL: 
Title: An Exploration of Asynchronous Data-Parallelism  
Author: Michael S. Littman Christopher D. Metcalf 
Date: July 22, 1990  
Abstract: In this paper, we develop the idea of asynchronous data-parallelism; that is, data-parallel programs in which the processing elements may be performing different computations at any given time. We suggest that since synchronous machines are forced to sequentialize the clauses of conditional statements, programming is hindered. Programmers must waste time factoring and merging expensive expressions from conditional clauses. This is still not always sufficient to use the machines at their maximum efficiency. We propose a method by which a synchronous machine can be used to simulate a MIMD machine language called Milan. Milan can be used as an efficient target for existing data-parallel languages such as Crystal and Paralation Lisp. By keeping Milan's instruction set small and regular, we are able to keep the simulation overhead to a minimum and achieve impressive performance on a few simple problems. We believe that Milan could be used as a research tool for computational models which require fine-grained asynchronous machines, such as very complex neural networks, CSP or parallel symbolic processing.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. Daniel Hillis and Guy L. Steele Jr. </author> <title> "Data Parallel Algorithms," </title> <journal> IEEE Com puters, </journal> <volume> 29(12) </volume> <pages> 1170-1183, </pages> <year> 1986. </year>
Reference-contexts: Both of these configurations have distinct advantages and disadvantages. An asynchronous machine supports the parallel execution of multiple expressions, subroutines, and even programs. However, control-parallel algorithms needed to run on these machines can be difficult to express and debug due to tricky synchronization issues <ref> [1] </ref>. A data-parallel program is often a clearer description of the problem and may capture a greater fraction of the potential parallelism in a problem [1, 2]. Unfortunately, synchronous machines can not easily compute differing expressions in parallel. <p> However, control-parallel algorithms needed to run on these machines can be difficult to express and debug due to tricky synchronization issues [1]. A data-parallel program is often a clearer description of the problem and may capture a greater fraction of the potential parallelism in a problem <ref> [1, 2] </ref>. Unfortunately, synchronous machines can not easily compute differing expressions in parallel. This leads to the opinion that SIMD machines are not general purpose, that is, that they are limited to a certain class of problems. <p> to do computation. 1 Adding a general communication scheme would slow things down even more. 1 This number comes from a rough count of the number of instructions in 68000 machine language. 3 Therefore, whereas the idea of simulating a MIMD machine using SIMD hardware is not unique to us <ref> [1] </ref>, doing so practically is new. The desired tradeoff is to win back in multiprocessing ability what is lost on simulation overhead. Our approach to efficient MIMD simulation is similar to the approach used in RISC architectures, although for a different reason.
Reference: [2] <author> Douglas Baldwin, </author> <note> University of Rochester Technical Report, TR 224, </note> <month> August </month> <year> 1987 </year>
Reference-contexts: However, control-parallel algorithms needed to run on these machines can be difficult to express and debug due to tricky synchronization issues [1]. A data-parallel program is often a clearer description of the problem and may capture a greater fraction of the potential parallelism in a problem <ref> [1, 2] </ref>. Unfortunately, synchronous machines can not easily compute differing expressions in parallel. This leads to the opinion that SIMD machines are not general purpose, that is, that they are limited to a certain class of problems.
Reference: [3] <author> A.G. Ranade, </author> <title> S.N. Bhatt and S.L. Johnsson, "The Fluent Abstract Machine," </title> <booktitle> Proceedings of the 5th MIT Conference on Advanced Research in VLSI, </booktitle> <month> March </month> <year> 1988. </year>
Reference-contexts: At the moment, there are only a few machines that might be able to support such a model directly; these include the Fluent machine <ref> [3] </ref>, currently under development at Yale, the PASM (Partitionable SIMD/MIMD) machine [4], and the Ultracomputer [5], also still only on paper. Each of these is asynchronous, consists of many processors, and supports data-parallel operations.
Reference: [4] <institution> A Partitionable SIMD/MIMD machine. </institution>
Reference-contexts: At the moment, there are only a few machines that might be able to support such a model directly; these include the Fluent machine [3], currently under development at Yale, the PASM (Partitionable SIMD/MIMD) machine <ref> [4] </ref>, and the Ultracomputer [5], also still only on paper. Each of these is asynchronous, consists of many processors, and supports data-parallel operations. <p> The language as currently written 7 The w (n) function was found, when hand-factored and coded in Paris, to run about in linear time and about 200 times faster than Milan; but w (n) is, of course, a trivial problem. 8 The PASM machine <ref> [4] </ref> is an attempt to do this at the hardware level. 15 supports an arbitrary number of programs running simultaneously on the Connection Machine (up to the number of available processors).
Reference: [5] <institution> The NYU Ultracomputer Project. </institution>
Reference-contexts: At the moment, there are only a few machines that might be able to support such a model directly; these include the Fluent machine [3], currently under development at Yale, the PASM (Partitionable SIMD/MIMD) machine [4], and the Ultracomputer <ref> [5] </ref>, also still only on paper. Each of these is asynchronous, consists of many processors, and supports data-parallel operations.
Reference: [6] <author> W. D. Hillis. </author> <title> The Connection Machine, </title> <publisher> MIT Press, </publisher> <year> 1985. </year> <title> [7] "Connection Machine Model CM-2 Technical Summary," </title> <type> Thinking Machines Technical Report HA87-4, </type> <month> April </month> <year> 1987. </year>
Reference-contexts: To explore asynchronous data-parallelism we devised an asynchronous data-parallel machine language called Milan (Multiple Instruction LANguage) and implemented it on a commercially available synchronous machine, the Connection Machine-2 <ref> [6, 7] </ref>. 3 The Milan Virtual Machine 3.1 The Milan Approach In principle, using a synchronous machine to simulate an asynchronous one is not very difficult. First, the code for the entire program is loaded into the memory for each processor.
Reference: [8] <author> Kenneth E. Iverson. </author> <title> A Programming Language, </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1962. </year> <title> [9] "Connection Machine Parallel Instruction Set (Paris), The C Interface, </title> <type> Version 4.0", </type> <institution> Thinking Machines Corporation, </institution> <year> 1987. </year> <title> [10] "The Essential flLisp Manual. Release 1, Revision 7." </title> <type> Technical Report, </type> <institution> Thinking Machines Corporation, </institution> <month> July </month> <year> 1986. </year>
Reference-contexts: 10 logical or logical and copy output exponential logical xor logical or maximum set random square root logical equivalence logical xor random power maximum start clock sine minimum split clock cosine overwrite break tangent unsigned maximum unsigned minimum Table 2: A summary of the Milan interpreter cycle. 10 Neither APL <ref> [8] </ref>, nor Paris [9] (PARallel Instruction Set|the machine language of the Connection Machine) have any notion of a parallel conditional. In APL, our example function can be implemented by computing both the case for odd and the case for even and merging the two results together (using addition, for example).
Reference: [11] <author> John R. Rose and Guy L. Steele Jr. </author> <title> "Cfl: An extended C Language for Data Par allel Programming," </title> <type> Technical Report PL87-5, </type> <institution> Thinking Machines Corporation, </institution> <month> April </month> <year> 1987. </year>
Reference-contexts: Paris has a slightly more elegant method in that processors in the odd case can be turned off while processors in the even case are computing and vice versa. Nonetheless, both languages throw away the tiny amount of asynchronousness possible. The Connection Machine's high-level languages (flLisp [10], Cfl <ref> [11] </ref>, and CM-Lisp [12]) support a variety of parallel conditionals. In these languages, our example function can be expressed straight-forwardly as a standard conditional expression. However, this syntax is really only an illusion of asynchronousness.
Reference: [12] <author> Guy L. Steele Jr. and W. Daniel Hillis. </author> <title> "Connection Machine Lisp: fine-grained parallel symbolic processing," </title> <booktitle> In Proceedings of the 1986 Symposium on Lisp and Functional Programming, </booktitle> <pages> pages 279-297, </pages> <year> 1986. </year>
Reference-contexts: Nonetheless, both languages throw away the tiny amount of asynchronousness possible. The Connection Machine's high-level languages (flLisp [10], Cfl [11], and CM-Lisp <ref> [12] </ref>) support a variety of parallel conditionals. In these languages, our example function can be expressed straight-forwardly as a standard conditional expression. However, this syntax is really only an illusion of asynchronousness.
Reference: [13] <author> Gary W. Sabot. </author> <title> The Paralation Model: Architecture-Independent Parallel Pro gramming, </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1988. </year>
Reference-contexts: Due to each of these languages' dependence on side-effects, their designers chose to define the semantics of conditionals to be the same as if the clauses were computed sequentially. In other words, these languages express no more in the way of asynchronous behavior than APL or Paris. Paralation Lisp <ref> [13] </ref>, and Crystal [14] do not allow side-effects in parallel conditionals; Crystal because it is a functional language and Paralation Lisp to avoid the need for resynchronization.
Reference: [14] <author> M. C. Chen. </author> <title> "Very-high-level parallel programming in Crystal," </title> <booktitle> In The Proceed ings of the Hypercube Microprocessors Conf. </booktitle> <address> Knoxville, TN, </address> <month> September </month> <year> 1986. </year>
Reference-contexts: In other words, these languages express no more in the way of asynchronous behavior than APL or Paris. Paralation Lisp [13], and Crystal <ref> [14] </ref> do not allow side-effects in parallel conditionals; Crystal because it is a functional language and Paralation Lisp to avoid the need for resynchronization.
Reference: [15] <author> Gul Agha, </author> <title> Actors: A Model of Concurrent Computation in Distributed Systems, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, London, England, </address> <year> 1986. </year>
Reference-contexts: This would allow a dynamic form of multitasking that might make the Connection Machine easier to share. Other applications lend themselves to immediate implementation. Milan could support higher-level neural net programming with many thousands of highly sophisticated neurons. This naturally leads into an implementation of actors <ref> [15] </ref> or of interacting agents for natural language and knowledge representation problems.
Reference: [16] <author> C. A. R. Hoare. </author> <title> "Communicating sequential processes," </title> <journal> Communication of ACM, </journal> <volume> 21(8) </volume> <pages> 666-677, </pages> <year> 1978. </year> <month> 17 </month>
Reference-contexts: This naturally leads into an implementation of actors [15] or of interacting agents for natural language and knowledge representation problems. Finally, the machine provides an easy way to implement the various higher-level asynchronous languages that have been proposed, such as CSP <ref> [16] </ref>. 8 Conclusions The common configurations for multiprocessor computers fail to provide the combination of efficiency and expressibility. Asynchronous data-parallelism provides the best features of asynchronous execution and data-parallel programming, and it is easier to use and more efficient than it might at first seem.
References-found: 13

