URL: http://www.cs.huji.ac.il/course/hpc/tutorial/sparse.ps
Refering-URL: http://www.cs.huji.ac.il/course/hpc/sch.html
Root-URL: http://www.cs.huji.ac.il
Title: LA Numerical Linear Algebra  
Date: 1991, 1992, 1993, 1994, 1995  
Note: Copyright (C)  by the Computational Science Education Project  
Abstract: This electronic book is copyrighted, and protected by the copyright laws of the United States. This (and all associated documents in the system) must contain the above copyright notice. If this electronic book is used anywhere other than the project's original system, CSEP must be notified in writing (email is acceptable) and the copyright notice must remain intact. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Dongarra, J. Bunch, C. Moler, and G. W. Stewart, </author> <title> LINPACK User's Guide (SIAM, </title> <address> Philadelphia, PA, </address> <year> 1979). </year>
Reference-contexts: All the rest of the 3! permutations of i, j and k lead to valid algorithms, some of which access columns of A in the innermost loop. The next algorithm is one of these, and is used in the LINPACK routine sgefa <ref> [1] </ref>. <p> Let us see what happens when we go on to solve Ax = <ref> [1; 2] </ref> T for x using this LU factorization. The correct answer is x [1; 1] T . Instead we get the following. Solving Ly = [1; 2] T yields y 1 = fl (1=1) = 1 and y 2 = fl (2 10 4 1) = 10 4 ; note <p> Let us see what happens when we go on to solve Ax = [1; 2] T for x using this LU factorization. The correct answer is x <ref> [1; 1] </ref> T . Instead we get the following. Solving Ly = [1; 2] T yields y 1 = fl (1=1) = 1 and y 2 = fl (2 10 4 1) = 10 4 ; note that the value 2 has been "lost" by subtracting 10 4 from it. <p> Let us see what happens when we go on to solve Ax = <ref> [1; 2] </ref> T for x using this LU factorization. The correct answer is x [1; 1] T . Instead we get the following. Solving Ly = [1; 2] T yields y 1 = fl (1=1) = 1 and y 2 = fl (2 10 4 1) = 10 4 ; note that the value 2 has been "lost" by subtracting 10 4 from it.
Reference: [2] <author> E. Anderson et al., </author> <note> LAPACK Users' Guide, Release 1.0 (SIAM, Philadelphia, </note> <year> 1992), </year> <pages> 235 pages. </pages>
Reference-contexts: Since the three standard problems listed above have been studied for so long, one might hope that good numerical software libraries would be available to solve them. This is partly true, as we illustrated by LAPACK <ref> [2, 7] </ref>. However, there are several reasons one cannot depend on libraries entirely. First, it is impossible to anticipate all possible structures of A that can arise in practice and write corresponding libraries (although extensive libraries for various specialized A do exist [7]). <p> To achieve higher performance, we modify this code first to use the Level 2 and then the Level 3 BLAS in its innermost loops. Again, 3! versions of these algorithms are possible, but we just describe the ones used in the LAPACK library <ref> [2] </ref>. There is obvious parallelism in the innermost loop, since each A ij can be updated independently. <p> Other similar algorithms may be derived by conformally partitioning L, U and A, and equating partitions in A = LU . Algorithms 6.3 and 6.4 are available as, respectively, subroutines sgetf2 and sgetrf in LAPACK <ref> [2] </ref>. See exercise 9. The LAPACK routine implementing Cholesky is sposv. <p> Let us see what happens when we go on to solve Ax = <ref> [1; 2] </ref> T for x using this LU factorization. The correct answer is x [1; 1] T . Instead we get the following. Solving Ly = [1; 2] T yields y 1 = fl (1=1) = 1 and y 2 = fl (2 10 4 1) = 10 4 ; note <p> Let us see what happens when we go on to solve Ax = <ref> [1; 2] </ref> T for x using this LU factorization. The correct answer is x [1; 1] T . Instead we get the following. Solving Ly = [1; 2] T yields y 1 = fl (1=1) = 1 and y 2 = fl (2 10 4 1) = 10 4 ; note that the value 2 has been "lost" by subtracting 10 4 from it.
Reference: [3] <author> J. Demmel, M. Heath, and H. van der Vorst, </author> <booktitle> in Acta Numerica, </booktitle> <volume> volume 2, </volume> <editor> edited by A. </editor> <publisher> Iserles (Cambridge University Press, ADDRESS, </publisher> <year> 1993). </year>
Reference-contexts: Eigenvalue problems arise when analyzing vibrations. There are also many important variations on these basic problems, 1 but to keep this chapter to a reasonable length, we will concentrate on algorithms for solving systems of linear equations, and refer elsewhere for least squares and eigenvalue problems <ref> [3, 4, 5, 6] </ref>. A great many algorithms are available for all these problems. The choice of algorithm depends on three things: * The structure of A has a large influence on the algorithm. <p> This is a necessarily partial survey of a large and active field. For further reading in numerical linear algebra, see the textbooks by Golub and Van Loan [4], Watkins [5] or Demmel [6]. For more details on parallel numerical linear algebra in particular, see <ref> [3, 8, 9] </ref>, the last of which includes a bibliography of over 2000 references. 2 Memory Hierarchies To understand the speedups of LAPACK over LINPACK in Table 1, and in general to be able to understand which algorithms are fast and which are slow, we need to understand memory hierarchies. <p> Cannon's algorithm may also be easily adapted to a hypercube <ref> [3] </ref>. The simplest way is to embed a grid (or two-dimensional torus) in a hypercube, i.e. map the processors in a grid to the processors in a hypercube, and the connections in a grid to a subset of the connections in a hypercube [22, 23].
Reference: [4] <author> G. Golub and C. Van Loan, </author> <title> Matrix Computations, 2nd ed. </title> <publisher> (Johns Hopkins University Press, </publisher> <address> Baltimore, MD, </address> <year> 1989). </year>
Reference-contexts: Eigenvalue problems arise when analyzing vibrations. There are also many important variations on these basic problems, 1 but to keep this chapter to a reasonable length, we will concentrate on algorithms for solving systems of linear equations, and refer elsewhere for least squares and eigenvalue problems <ref> [3, 4, 5, 6] </ref>. A great many algorithms are available for all these problems. The choice of algorithm depends on three things: * The structure of A has a large influence on the algorithm. <p> Finally, section 12 gives a quick references guide to the BLAS. This is a necessarily partial survey of a large and active field. For further reading in numerical linear algebra, see the textbooks by Golub and Van Loan <ref> [4] </ref>, Watkins [5] or Demmel [6]. <p> Reordering the rows of A with P is called pivoting, and is necessary for numerical stability (more on this in section 10). We use the standard partial pivoting scheme <ref> [4] </ref>: this means L has ones on its diagonal and other entries bounded in absolute value by one. <p> In this case the preconditioner K = LU , where L and U have a sparsity pattern equal or close to the sparsity pattern of the corresponding parts of A (L is lower triangular, U is upper triangular). For details see <ref> [4] </ref>, [82] and [83]. Solving Kw = r leads to solving successively Lz = r and U w = z. These triangular solves lead to recurrence relations that are not easily parallelized. <p> In other short recurrence methods, other properties of A may be required or desirable, but we will not exploit these properties explicitly here. Most often, CG is used in combination with some kind of preconditioning <ref> [79, 4, 97] </ref>. This means that the matrix A is implicitly multiplied by an approximation K 1 of A 1 . Usually, K is constructed to be an approximation of A, and so that Ky = z is easier to solve than Ax = b. <p> Since we do not require A to be symmetric, we need long recurrences: each new vector must be explicitly orthogonalized against all previously generated basis vectors. In its most common form GMRES orthogonalizes using Modified Gram-Schmidt <ref> [4] </ref>. In order to limit memory requirements (since all basis vectors must be stored), GMRES is restarted after each cycle of m iteration steps; this is called GMRES (m). <p> For larger linear systems, they observed speedups close to 2:5. 10 Error Bounds for Solving Ax = b How accurately can we expect to solve Ax = b? In this section we will outline the answer to this question, and refer to standard textbooks for details <ref> [4, 5, 6] </ref>. The answer depends both on the matrix A, and on the algorithm we use. To derive a useful error bound, the property of the matrix we need to know is its condition number (A), and the property we require of the algorithm is numerical stability. <p> Since 2 n grows very quickly with n, the error bound rapidly becomes enormous (and the actual solution also becomes poor). These examples are found in numerical analysis textbooks <ref> [4, 5, 6] </ref>, but not in practice. Rather than formally proving that Gaussian elimination with partial pivoting is generally stable, let us instead illustrate how omitting pivoting can destroy stability.
Reference: [5] <author> D. Watkins, </author> <title> Fundamentals of Matrix Computations (Wiley, </title> <publisher> ADDRESS, </publisher> <year> 1991). </year>
Reference-contexts: Eigenvalue problems arise when analyzing vibrations. There are also many important variations on these basic problems, 1 but to keep this chapter to a reasonable length, we will concentrate on algorithms for solving systems of linear equations, and refer elsewhere for least squares and eigenvalue problems <ref> [3, 4, 5, 6] </ref>. A great many algorithms are available for all these problems. The choice of algorithm depends on three things: * The structure of A has a large influence on the algorithm. <p> Finally, section 12 gives a quick references guide to the BLAS. This is a necessarily partial survey of a large and active field. For further reading in numerical linear algebra, see the textbooks by Golub and Van Loan [4], Watkins <ref> [5] </ref> or Demmel [6]. <p> For larger linear systems, they observed speedups close to 2:5. 10 Error Bounds for Solving Ax = b How accurately can we expect to solve Ax = b? In this section we will outline the answer to this question, and refer to standard textbooks for details <ref> [4, 5, 6] </ref>. The answer depends both on the matrix A, and on the algorithm we use. To derive a useful error bound, the property of the matrix we need to know is its condition number (A), and the property we require of the algorithm is numerical stability. <p> Since 2 n grows very quickly with n, the error bound rapidly becomes enormous (and the actual solution also becomes poor). These examples are found in numerical analysis textbooks <ref> [4, 5, 6] </ref>, but not in practice. Rather than formally proving that Gaussian elimination with partial pivoting is generally stable, let us instead illustrate how omitting pivoting can destroy stability.
Reference: [6] <author> J. Demmel, </author> <title> Berkeley Lecture Notes in Numerical Linear Algebra, </title> <institution> Mathematics Department, University of California, </institution> <year> 1993, </year> <pages> 215 pages. </pages>
Reference-contexts: Eigenvalue problems arise when analyzing vibrations. There are also many important variations on these basic problems, 1 but to keep this chapter to a reasonable length, we will concentrate on algorithms for solving systems of linear equations, and refer elsewhere for least squares and eigenvalue problems <ref> [3, 4, 5, 6] </ref>. A great many algorithms are available for all these problems. The choice of algorithm depends on three things: * The structure of A has a large influence on the algorithm. <p> Finally, section 12 gives a quick references guide to the BLAS. This is a necessarily partial survey of a large and active field. For further reading in numerical linear algebra, see the textbooks by Golub and Van Loan [4], Watkins [5] or Demmel <ref> [6] </ref>. <p> For larger linear systems, they observed speedups close to 2:5. 10 Error Bounds for Solving Ax = b How accurately can we expect to solve Ax = b? In this section we will outline the answer to this question, and refer to standard textbooks for details <ref> [4, 5, 6] </ref>. The answer depends both on the matrix A, and on the algorithm we use. To derive a useful error bound, the property of the matrix we need to know is its condition number (A), and the property we require of the algorithm is numerical stability. <p> Since 2 n grows very quickly with n, the error bound rapidly becomes enormous (and the actual solution also becomes poor). These examples are found in numerical analysis textbooks <ref> [4, 5, 6] </ref>, but not in practice. Rather than formally proving that Gaussian elimination with partial pivoting is generally stable, let us instead illustrate how omitting pivoting can destroy stability.
Reference: [7] <author> J. Dongarra and E. </author> <title> Grosse, </title> <journal> Communications of the ACM 30, </journal> <month> 403 </month> <year> (1987). </year>
Reference-contexts: Since the three standard problems listed above have been studied for so long, one might hope that good numerical software libraries would be available to solve them. This is partly true, as we illustrated by LAPACK <ref> [2, 7] </ref>. However, there are several reasons one cannot depend on libraries entirely. First, it is impossible to anticipate all possible structures of A that can arise in practice and write corresponding libraries (although extensive libraries for various specialized A do exist [7]). <p> However, there are several reasons one cannot depend on libraries entirely. First, it is impossible to anticipate all possible structures of A that can arise in practice and write corresponding libraries (although extensive libraries for various specialized A do exist <ref> [7] </ref>). Second, high performance computer architectures, programming languages and compilers have been changing so much lately that library writers have not been able to keep up. There is no clear cut rule on when to use a library routine.
Reference: [8] <author> J. Dongarra, I. Duff, D. Sorensen, and H. van der Vorst, </author> <title> Solving linear systems on vector and shared memory computers (SIAM, </title> <address> Philadelphia, PA, </address> <year> 1991), </year> <pages> 256 pages. </pages>
Reference-contexts: This is a necessarily partial survey of a large and active field. For further reading in numerical linear algebra, see the textbooks by Golub and Van Loan [4], Watkins [5] or Demmel [6]. For more details on parallel numerical linear algebra in particular, see <ref> [3, 8, 9] </ref>, the last of which includes a bibliography of over 2000 references. 2 Memory Hierarchies To understand the speedups of LAPACK over LINPACK in Table 1, and in general to be able to understand which algorithms are fast and which are slow, we need to understand memory hierarchies. <p> From the discussions it should be clear how to combine coarse-grained and fine-grained approaches, for example when implementing a method on a parallel machine with vector processors. The implementation for such machines, in particular those with shared memory, is given much attention in <ref> [8] </ref>. <p> For vector computers this leads to a vectorizable preconditioner (see <ref> [84, 8, 85, 86] </ref>). For coarse-grained parallelism this approach is insufficient. By a similar approach more parallelism can be obtained in three-dimensional situations: the so-called hyperplane approach [87, 85, 86]. <p> Duff and Meurant [88] have carried out numerical experiments for many different orderings, which show that the numbers of iterations may increase significantly for other than lexicographical ordering. Some modest degree of parallelism can be obtained, however, with so-called incomplete twisted factorizations <ref> [8, 89, 85] </ref>. Multi-color schemes with a large number of colors (e.g., 20 to 100) may lead to little or no degradation in convergence behavior [90]. but also to less parallelism. Moreover, the ratio of computation to communication may be more unfavorable. (3) Forced parallelism.
Reference: [9] <author> K. Gallivan et al., </author> <title> Parallel algorithms for matrix computations (SIAM, </title> <address> Philadelphia, PA, </address> <year> 1990). </year>
Reference-contexts: This is a necessarily partial survey of a large and active field. For further reading in numerical linear algebra, see the textbooks by Golub and Van Loan [4], Watkins [5] or Demmel [6]. For more details on parallel numerical linear algebra in particular, see <ref> [3, 8, 9] </ref>, the last of which includes a bibliography of over 2000 references. 2 Memory Hierarchies To understand the speedups of LAPACK over LINPACK in Table 1, and in general to be able to understand which algorithms are fast and which are slow, we need to understand memory hierarchies. <p> In the limiting case of tridiagonal matrices, the parallel algorithm derived as in section 6.1 and the standard serial algorithm are nearly identical. The problem of solving banded linear systems with a narrow band has been studied by many authors, see for instance the references in <ref> [9, 38] </ref>. We will only sketch some of the main ideas and we will do so for rather simple problems. The reader should keep in mind that these ideas can easily be generalized for more complicated situations, and many have appeared in the literature.
Reference: [10] <author> C. Lawson, R. Hanson, D. Kincaid, and F. </author> <title> Krogh, </title> <journal> ACM Trans. Math. Soft. </journal> <volume> 5, </volume> <month> 308 </month> <year> (1979). </year>
Reference-contexts: Here is the most successful approach we have discovered so far. Since operations like matrix-matrix multiplication are so common, computer manufacturers have standardized them as the Basic Linear Algebra Subroutines or BLAS <ref> [10, 11, 12] </ref>, and optimized them for their machines. In other words, a library of subroutines for matrix-matrix multiplication, matrix-vector multiplication, and other routines is available as a standard Fortran (or C) callable library on most 6 high performance machines, and underneath they have been optimized for each machine. <p> Table 2 reflects a hierarchy of operations: Operations like saxpy operate on vectors and offer the worst q values; these are called Level 1 BLAS <ref> [10] </ref>, and include inner products and other simple operations.
Reference: [11] <author> J. Dongarra, J. Du Croz, S. Hammarling, and R. J. </author> <title> Hanson, </title> <journal> ACM Trans. Math. Soft. </journal> <volume> 14, </volume> <month> 1 </month> <year> (1988). </year>
Reference-contexts: Here is the most successful approach we have discovered so far. Since operations like matrix-matrix multiplication are so common, computer manufacturers have standardized them as the Basic Linear Algebra Subroutines or BLAS <ref> [10, 11, 12] </ref>, and optimized them for their machines. In other words, a library of subroutines for matrix-matrix multiplication, matrix-vector multiplication, and other routines is available as a standard Fortran (or C) callable library on most 6 high performance machines, and underneath they have been optimized for each machine. <p> Operations like matrix-vector multiplication operate on matrices and vectors, and offer slightly better q values; these are called Level 2 BLAS <ref> [11] </ref>, and include solving triangular systems of equations and rank-1 updates of matrices (A + xy T , x and y column vectors).
Reference: [12] <author> J. Dongarra, J. Du Croz, I. Duff, and S. </author> <title> Hammarling, </title> <journal> ACM Trans. Math. Soft. </journal> <volume> 16, </volume> <month> 1 </month> <year> (1990). </year> <note> Exercises 57 </note>
Reference-contexts: Here is the most successful approach we have discovered so far. Since operations like matrix-matrix multiplication are so common, computer manufacturers have standardized them as the Basic Linear Algebra Subroutines or BLAS <ref> [10, 11, 12] </ref>, and optimized them for their machines. In other words, a library of subroutines for matrix-matrix multiplication, matrix-vector multiplication, and other routines is available as a standard Fortran (or C) callable library on most 6 high performance machines, and underneath they have been optimized for each machine. <p> Operations like matrix-matrix multiplication operate on pairs of matrices, and offer the best q values; these are called Level 3 BLAS <ref> [12] </ref>, and include solving triangular systems of equations with many right hand sides.
Reference: [13] <author> B. T. Smith et al., </author> <title> Matrix Eigensystem Routines - EISPACK Guide, </title> <booktitle> Vol. 6 of Lecture Notes in Computer Science (Springer-Verlag, </booktitle> <address> Berlin, </address> <year> 1976). </year>
Reference-contexts: This was the approach taken in LAPACK: the algorithms in LINPACK (and the corresponding library for eigenvalue problems, EISPACK <ref> [13, 14] </ref>) were reorganized to call the BLAS in their innermost loops, where most of the work is done. This led to the speedups shown in Table 1. This approach was very successful on machines like the Cray, i.e. parallel vector processors using fast shared memory with relatively few processors.
Reference: [14] <author> B. S. Garbow, J. M. Boyle, J. J. Dongarra, and C. B. Moler, </author> <title> Matrix Eigensystem Routines - EISPACK Guide Extension, </title> <booktitle> Vol. 51 of Lecture Notes in Computer Science (Springer-Verlag, </booktitle> <address> Berlin, </address> <year> 1977). </year>
Reference-contexts: This was the approach taken in LAPACK: the algorithms in LINPACK (and the corresponding library for eigenvalue problems, EISPACK <ref> [13, 14] </ref>) were reorganized to call the BLAS in their innermost loops, where most of the work is done. This led to the speedups shown in Table 1. This approach was very successful on machines like the Cray, i.e. parallel vector processors using fast shared memory with relatively few processors.
Reference: [15] <author> X. Hong and H. T. Kung, </author> <booktitle> in Proceedings of the 13th Symposium on the Theory of Computing (ACM, </booktitle> <publisher> ADDRESS, </publisher> <year> 1981), </year> <pages> pp. 326-334. </pages>
Reference-contexts: This yields q q which is much better than the previous algorithms. In <ref> [15] </ref> an analysis of this problem leads to an upper bound for q near p M , so we cannot expect to improve much on this algorithm for square matrices.
Reference: [16] <author> S. L. Johnsson, </author> <title> private communication, </title> <year> 1990. </year>
Reference-contexts: Write an algorithm to return A and B to their original positions. What is the running time of your algorithm? 2 The matrix-multiplication subroutine in the CM-2 Scientific Subroutine Library took approximately 10 person-years of effort to write <ref> [16] </ref>. Exercises 55 Exercise 7 Illustrate the block scattered layout of a 50 fi 60 matrix on a 4 fi 8 processor grid with 5 fi 5 blocks.
Reference: [17] <author> E. Horowitz and S. Sahni, </author> <booktitle> Fundamentals of Computer Algorithms (Computer Science Press, </booktitle> <address> Potomac, MD, </address> <year> 1978). </year>
Reference-contexts: This is why computer manufacturers are often in the best position to write the matrix-multiplication (and other BLAS) for their machines, because they often have the best understanding of these machine specific details. 2 Another quite different algorithm is Strassen's method <ref> [17] </ref>, which multiplies matrices recursively by dividing them into 2 fi 2 block matrices, and multiplying the subblocks using 7 matrix multiplications (recursively) and 15 matrix additions of half the size. This leads to an asymptotic complexity of n log 2 7 n 2:81 instead of n 3 .
Reference: [18] <author> D. H. Bailey, K. Lee, and H. D. Simon, J. </author> <booktitle> Supercomputing 4, </booktitle> <month> 97 </month> <year> (1991). </year>
Reference-contexts: This approach has led to speedups on relatively large matrices on some machines <ref> [18] </ref>. A drawback is the need for significant workspace, and somewhat lower numerical stability, although it is adequate for many purposes [19]. Exercise 2 A comparison of blocked implementations for the basic linear algebra subroutine and the matrix-multiplication implemention, previously provided.
Reference: [19] <author> J. Demmel and N. J. </author> <title> Higham, </title> <journal> ACM Trans. Math. Soft. </journal> <volume> 18, </volume> <month> 274 </month> <year> (1992). </year>
Reference-contexts: This approach has led to speedups on relatively large matrices on some machines [18]. A drawback is the need for significant workspace, and somewhat lower numerical stability, although it is adequate for many purposes <ref> [19] </ref>. Exercise 2 A comparison of blocked implementations for the basic linear algebra subroutine and the matrix-multiplication implemention, previously provided.
Reference: [20] <author> L. Cannon, </author> <type> Ph.D. thesis, </type> <institution> Montana State University, Bozeman, MN, </institution> <year> 1969. </year>
Reference-contexts: We begin by showing how best to implement matrix multiplication without regard to the layout's suitability for other matrix operations, and return to the question of layouts in the next section. The algorithm is due to Cannon <ref> [20] </ref> and is well suited for computers laid out in a square N fi N mesh, i.e. where each processor communicates most efficiently with the four other processors immediately north, east, south and west of itself.
Reference: [21] <author> G. Fox et al., </author> <title> Solving problems on concurrent processors, </title> <publisher> v. I (Prentice Hall, ADDRESS, </publisher> <year> 1988). </year>
Reference-contexts: B after skewing A, B after shift k = 1 A, B after shift k = 2 A variation of this algorithm suitable for machines that are efficient at spreading subblocks across rows (or down columns) is to do this instead of the preshifting and rotation of A (or B) <ref> [21] </ref>. Cannon's algorithm may also be easily adapted to a hypercube [3]. <p> If lb and/or ub are large, then the techniques of the Section 6 are still applicable, and the LAPACK routines for band matrices (sgbsv and spbsv) have been optimized for this situation <ref> [37, 21] </ref>. Different techniques are needed when the bandwidth is small. The reason is that proportionately less and less parallel work is available in updating the trailing submatrix, which is where we use the Level 3 BLAS.
Reference: [22] <author> C. T. Ho, </author> <type> Ph.D. thesis, </type> <institution> Yale University, </institution> <year> 1990. </year>
Reference-contexts: The simplest way is to embed a grid (or two-dimensional torus) in a hypercube, i.e. map the processors in a grid to the processors in a hypercube, and the connections in a grid to a subset of the connections in a hypercube <ref> [22, 23] </ref>. This approach (which is useful for more than matrix multiplication) uses only a subset of the connections in a hypercube, which makes the communication slower than it need be.
Reference: [23] <author> S. L. Johnsson, J. </author> <booktitle> of Parallel and Distributed Computing 4, </booktitle> <month> 133 </month> <year> (1987). </year>
Reference-contexts: The simplest way is to embed a grid (or two-dimensional torus) in a hypercube, i.e. map the processors in a grid to the processors in a hypercube, and the connections in a grid to a subset of the connections in a hypercube <ref> [22, 23] </ref>. This approach (which is useful for more than matrix multiplication) uses only a subset of the connections in a hypercube, which makes the communication slower than it need be.
Reference: [24] <author> E. Dekel, D. Nassimi, and S. </author> <title> Sahni, </title> <journal> SIAM J. Comput. </journal> <volume> 10, </volume> <month> 657 </month> <year> (1981). </year>
Reference-contexts: This approach (which is useful for more than matrix multiplication) uses only a subset of the connections in a hypercube, which makes the communication slower than it need be. Several sophisticated improvements on this basic algorithm have been developed <ref> [24, 25] </ref>, the latter of which fully utilizes the available bandwidth of the hypercube to reduce the number of messages sent by a factor of 2 and the number of words sent by a factor of nearly 2n=N .
Reference: [25] <author> C. T. Ho, S. L. Johnsson, and A. Edelman, </author> <booktitle> in The Sixth Distributed Memory Computing Conference Proceedings (IEEE Computer Society Press, </booktitle> <publisher> ADDRESS, </publisher> <year> 1991), </year> <pages> pp. 447-451. </pages>
Reference-contexts: This approach (which is useful for more than matrix multiplication) uses only a subset of the connections in a hypercube, which makes the communication slower than it need be. Several sophisticated improvements on this basic algorithm have been developed <ref> [24, 25] </ref>, the latter of which fully utilizes the available bandwidth of the hypercube to reduce the number of messages sent by a factor of 2 and the number of words sent by a factor of nearly 2n=N .
Reference: [26] <author> G. </author> <note> Fox et al., </note> <institution> Computer Science Department Report CRPC-TR90079, Rice University, Houston, TX (unpublished). </institution>
Reference-contexts: There is an emerging consensus about data layouts for distributed memory machines. This is being implemented in several programming languages <ref> [26, 27] </ref>, that will be available to programmers in the near future. We describe these layouts here.
Reference: [27] <institution> High Performance Fortran, </institution> <note> documentation available via anonymous ftp from titan.cs.rice.edu in directory public/HPFF, </note> <year> 1991. </year>
Reference-contexts: There is an emerging consensus about data layouts for distributed memory machines. This is being implemented in several programming languages <ref> [26, 27] </ref>, that will be available to programmers in the near future. We describe these layouts here. <p> There is an emerging consensus about data layouts for distributed memory machines. This is being implemented in several programming languages [26, 27], that will be available to programmers in the near future. We describe these layouts here. High Performance Fortran (HPF) <ref> [27] </ref> permits the user to define a virtual array of processors, align actual data structures like matrices and arrays with this virtual array (and so with respect to each other), and then to layout the virtual processor array on an actual machine.
Reference: [28] <author> E. Van de Velde, </author> <title> caltech, </title> <address> Pasadena, CA (unpublished). </address>
Reference-contexts: A different approach is to write algorithms that work independently of the location of the data, and rely on the underlying language or run-time system to optimize the necessary communications. This makes code easier to write, but puts a large burden on compiler and run-time system writers <ref> [28] </ref>.
Reference: [29] <author> E. Anderson and J. </author> <type> Dongarra, </type> <institution> Computer Science Dept. </institution> <type> Technical Report CS-90-103, </type> <institution> University of Tennessee, Knoxville (unpublished), </institution> <note> (LAPACK Working Note #19). </note>
Reference-contexts: Other permutations of the nested loops lead to different algorithms, which depend on the BLAS for matrix-vector multiplication and solving a triangular system instead of rank-1 updating <ref> [29, 30] </ref>; which is faster depends on the relative speed of these on a given machine.
Reference: [30] <author> Y. Robert, </author> <title> The impact of vector and parallel architectures on the Gaussian elimination algorithm (Wiley, </title> <publisher> ADDRESS, </publisher> <year> 1990). </year> <month> 58 </month>
Reference-contexts: Other permutations of the nested loops lead to different algorithms, which depend on the BLAS for matrix-vector multiplication and solving a triangular system instead of rank-1 updating <ref> [29, 30] </ref>; which is faster depends on the relative speed of these on a given machine. <p> Given this function, it may be minimized as a function of b 1 , b 2 , p 1 and p 2 . Some theoretical analyses of this sort for special cases may be found in <ref> [30] </ref> and the references therein. See also [31] and [32].
Reference: [31] <author> J. Dongarra and S. </author> <type> Ostrouchov, </type> <institution> Computer Science Dept. </institution> <type> Technical Report CS-90-115, </type> <institution> University of Tennessee, Knoxville (unpublished), </institution> <note> (LAPACK Working Note #24). </note>
Reference-contexts: Given this function, it may be minimized as a function of b 1 , b 2 , p 1 and p 2 . Some theoretical analyses of this sort for special cases may be found in [30] and the references therein. See also <ref> [31] </ref> and [32]. As an example of the performance that can be attained in practice, on an Intel Delta with 512 processors the speed of LU ranged from a little over 1 gigaflop for n = 2000 to nearly 12 gigaflops for n = 25000.
Reference: [32] <author> J. Dongarra and R. </author> <type> van de Geijn, </type> <institution> Computer Science Dept. </institution> <type> Technical Report CS-91-138, </type> <institution> University of Tennessee, Knoxville (unpublished), </institution> <note> (LAPACK Working Note #37). </note>
Reference-contexts: Given this function, it may be minimized as a function of b 1 , b 2 , p 1 and p 2 . Some theoretical analyses of this sort for special cases may be found in [30] and the references therein. See also [31] and <ref> [32] </ref>. As an example of the performance that can be attained in practice, on an Intel Delta with 512 processors the speed of LU ranged from a little over 1 gigaflop for n = 2000 to nearly 12 gigaflops for n = 25000.
Reference: [33] <author> S. Eisenstat, M. Heath, C. Henkel, and C. </author> <title> Romine, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 9, </volume> <month> 589 </month> <year> (1988). </year>
Reference-contexts: Designing such an algorithm on a distributed memory machine is harder, because the fewer floating point operations performed (O (n 2 ) instead of O (n 3 )) make it harder to mask the communication <ref> [33, 34, 35, 36] </ref>. 18 7 Gaussian Elimination on Band Matrices Recall that a band matrix is one which is nonzero only on certain diagonals.
Reference: [34] <author> M. Heath and C. </author> <title> Romine, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 9, </volume> <month> 558 </month> <year> (1988). </year>
Reference-contexts: Designing such an algorithm on a distributed memory machine is harder, because the fewer floating point operations performed (O (n 2 ) instead of O (n 3 )) make it harder to mask the communication <ref> [33, 34, 35, 36] </ref>. 18 7 Gaussian Elimination on Band Matrices Recall that a band matrix is one which is nonzero only on certain diagonals.
Reference: [35] <author> G. Li and T. </author> <title> Coleman, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 9, </volume> <month> 485 </month> <year> (1988). </year>
Reference-contexts: Designing such an algorithm on a distributed memory machine is harder, because the fewer floating point operations performed (O (n 2 ) instead of O (n 3 )) make it harder to mask the communication <ref> [33, 34, 35, 36] </ref>. 18 7 Gaussian Elimination on Band Matrices Recall that a band matrix is one which is nonzero only on certain diagonals.
Reference: [36] <author> C. Romine and J. Ortega, </author> <booktitle> Parallel Computing 6, </booktitle> <month> 109 </month> <year> (1988). </year>
Reference-contexts: Designing such an algorithm on a distributed memory machine is harder, because the fewer floating point operations performed (O (n 2 ) instead of O (n 3 )) make it harder to mask the communication <ref> [33, 34, 35, 36] </ref>. 18 7 Gaussian Elimination on Band Matrices Recall that a band matrix is one which is nonzero only on certain diagonals.
Reference: [37] <author> J. Du Croz, P. J. D. Mayes, and G. </author> <type> Radicati di Brozolo, </type> <institution> Computer Science Dept. </institution> <type> Technical Report CS-90-109, </type> <institution> University of Tennessee, Knoxville (unpublished), </institution> <note> (LAPACK Working Note #21. </note>
Reference-contexts: If lb and/or ub are large, then the techniques of the Section 6 are still applicable, and the LAPACK routines for band matrices (sgbsv and spbsv) have been optimized for this situation <ref> [37, 21] </ref>. Different techniques are needed when the bandwidth is small. The reason is that proportionately less and less parallel work is available in updating the trailing submatrix, which is where we use the Level 3 BLAS.
Reference: [38] <author> J. Ortega, </author> <title> Introduction to Parallel and Vector Solution of Linear Systems (Plenum Press, </title> <publisher> ADDRESS, </publisher> <year> 1988). </year>
Reference-contexts: In the limiting case of tridiagonal matrices, the parallel algorithm derived as in section 6.1 and the standard serial algorithm are nearly identical. The problem of solving banded linear systems with a narrow band has been studied by many authors, see for instance the references in <ref> [9, 38] </ref>. We will only sketch some of the main ideas and we will do so for rather simple problems. The reader should keep in mind that these ideas can easily be generalized for more complicated situations, and many have appeared in the literature. <p> Since the amount of serial work is halved in each step by completely parallel (or vectorizable) operations, this approach has been successfully applied on vector supercomputers, especially when the vector speed of the machine is significantly greater than the scalar speed <ref> [38, 45, 46] </ref>. For distributed memory computers the method requires too much data 20 movement for the reduced system to be practical. However, the method is easily generalized to one with more parallelism. <p> It can also be viewed as a `demand-driven' algorithm, since the inner products that affect a given column are not accumulated until actually needed to modify and complete that column. For this reason, Ortega <ref> [38] </ref> terms column-Cholesky a `delayed-update' algorithm. It is also sometimes referred to as a `fan-in' algorithm, since the basic operation is to combine the effects of multiple previous columns on a single target column. <p> It can also be viewed as a `data-driven' algorithm, since each new column is used as soon as it is completed to make all modifications to all the subsequent columns it affects. For this reason, Ortega <ref> [38] </ref> terms submatrix-Cholesky an `immediate-update' algorithm. It is also sometimes referred to as a `fan-out' algorithm, since the basic operation is for a single column to affect multiple subsequent columns. We will see that these characterizations of the column-Cholesky and submatrix-Cholesky algorithms have important implications for parallel implementations.
Reference: [39] <author> I. </author> <title> Babuska, </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 9, </volume> <month> 53 </month> <year> (1972). </year>
Reference-contexts: Most of the parallel approaches perform more arithmetic operations than standard (se quential) Gaussian elimination (typically 2:5 times as many), twisted factorization being the only exception. In twisted factorization the Gaussian elimination process is carried out in parallel from both sides. This method was first proposed in <ref> [39] </ref> for tridiagonal systems T x = b as a means to compute a specified component of x more accurately.
Reference: [40] <author> H. A. van der Vorst, </author> <booktitle> Parallel Computing 5, </booktitle> <month> 303 </month> <year> (1987). </year>
Reference-contexts: The twisted factorization and subsequent forward and back substitutions with P and Q take as many arithmetic operations as the standard factorization, and can be carried out with twofold parallelism by working from both ends of the matrix simultaneously. For an analysis of this process for tridiagonal systems, see <ref> [40] </ref>. Twisted factorization can be combined with any of the following techniques, often doubling the parallelism.
Reference: [41] <author> H. Stone, J. </author> <title> Assoc. </title> <journal> Comput. Mach. </journal> <volume> 20, </volume> <month> 27 </month> <year> (1973). </year>
Reference-contexts: Thus, the original system splits in two independent lower bidiagonal systems of half the size, one for the odd-numbered unknowns, and one for the even-numbered unknowns. This process can be repeated recursively for both new systems, leading to an algorithm known as recursive doubling <ref> [41] </ref>. It has been analyzed and generalized for banded systems in [42]. Its significance for modern parallel computers is limited, which we illustrate with the following examples. Suppose we perform a single step of recursive doubling.
Reference: [42] <author> P. Dubois and G. Rodrigue, </author> <title> in High speed computer and algorithm organization, </title> <editor> edited by D. J. Kuck and A. H. </editor> <publisher> Sameh (Academic Press, </publisher> <address> New York, </address> <year> 1977). </year>
Reference-contexts: This process can be repeated recursively for both new systems, leading to an algorithm known as recursive doubling [41]. It has been analyzed and generalized for banded systems in <ref> [42] </ref>. Its significance for modern parallel computers is limited, which we illustrate with the following examples. Suppose we perform a single step of recursive doubling. This step can be done in parallel, but it involves slightly more arithmetic than the serial elimination process for solving Lx = b.
Reference: [43] <author> J. J. Lambiotte and R. G. Voigt, </author> <type> Technical report, </type> <institution> ICASE-NASA Langley Research Center, Hampton, VA (unpublished). </institution>
Reference-contexts: After having solved this reduced system, the odd-numbered unknowns can be computed in parallel from the odd-numbered equations. Of course, the trick can be repeated for the subsystem of half size, and this process is known as cyclic reduction <ref> [43, 44] </ref>. Since the amount of serial work is halved in each step by completely parallel (or vectorizable) operations, this approach has been successfully applied on vector supercomputers, especially when the vector speed of the machine is significantly greater than the scalar speed [38, 45, 46].
Reference: [44] <author> D. </author> <title> Heller, </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 13, </volume> <month> 484 </month> <year> (1978). </year>
Reference-contexts: After having solved this reduced system, the odd-numbered unknowns can be computed in parallel from the odd-numbered equations. Of course, the trick can be repeated for the subsystem of half size, and this process is known as cyclic reduction <ref> [43, 44] </ref>. Since the amount of serial work is halved in each step by completely parallel (or vectorizable) operations, this approach has been successfully applied on vector supercomputers, especially when the vector speed of the machine is significantly greater than the scalar speed [38, 45, 46].
Reference: [45] <author> P. P. N. </author> <title> de Groen, </title> <journal> Appl. Num. Math. </journal> <volume> 8, </volume> <month> 117 </month> <year> (1991). </year>
Reference-contexts: Since the amount of serial work is halved in each step by completely parallel (or vectorizable) operations, this approach has been successfully applied on vector supercomputers, especially when the vector speed of the machine is significantly greater than the scalar speed <ref> [38, 45, 46] </ref>. For distributed memory computers the method requires too much data 20 movement for the reduced system to be practical. However, the method is easily generalized to one with more parallelism.
Reference: [46] <author> J. J. F. M. Schlichting and H. A. van der Vorst, </author> <type> Technical Report No. </type> <institution> NM-R8725, CWI, </institution> <address> Amsterdam, the Netherlands (unpublished). </address>
Reference-contexts: Since the amount of serial work is halved in each step by completely parallel (or vectorizable) operations, this approach has been successfully applied on vector supercomputers, especially when the vector speed of the machine is significantly greater than the scalar speed <ref> [38, 45, 46] </ref>. For distributed memory computers the method requires too much data 20 movement for the reduced system to be practical. However, the method is easily generalized to one with more parallelism.
Reference: [47] <author> S. C. Chen, D. J. Kuck, and A. H. </author> <title> Sameh, </title> <journal> ACM Trans. Math. </journal> <volume> Software 4, </volume> <month> 270 </month> <year> (1978). </year>
Reference-contexts: In practical cases k is chosen so large that the process is not repeated for the resulting subsystems, as for cyclic reduction (where k = 2). This approach is referred to as a divide-and-conquer approach. For banded triangular systems it was first suggested in <ref> [47] </ref>, for tridiagonal systems it was proposed in [48]. To illustrate, let us apply one parallel elimination step to the lower bidiagonal system Lx = b to eliminate all subdiagonal elements in all diagonal blocks.
Reference: [48] <author> H. H. </author> <title> Wang, </title> <journal> ACM Trans. Math. </journal> <volume> Software 7, </volume> <month> 170 </month> <year> (1981). </year>
Reference-contexts: This approach is referred to as a divide-and-conquer approach. For banded triangular systems it was first suggested in [47], for tridiagonal systems it was proposed in <ref> [48] </ref>. To illustrate, let us apply one parallel elimination step to the lower bidiagonal system Lx = b to eliminate all subdiagonal elements in all diagonal blocks. <p> In the original approach <ref> [48] </ref>, the fill-in in the subdiagonal blocks is eliminated in parallel, or vector mode, for each subdiagonal block (note that each subdiagonal block has only one column with nonzero elements).
Reference: [49] <author> H. A. van der Vorst and K. </author> <title> Dekker, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 10, </volume> <month> 27 </month> <year> (1989). </year>
Reference-contexts: In the original approach [48], the fill-in in the subdiagonal blocks is eliminated in parallel, or vector mode, for each subdiagonal block (note that each subdiagonal block has only one column with nonzero elements). It has been shown in <ref> [49] </ref> that this leads to very efficient vectorized code for machines such as Cray, Fujitsu, etc. For parallel computers, the parallelism in eliminating these subdiagonal blocks is rela tively fine-grained compared with the more coarse-grained parallelism in the first step of the algorithm.
Reference: [50] <author> P. H. Michielse and H. A. van der Vorst, </author> <note> Parallel Computing 7, 87 (1988). Exercises 59 </note>
Reference-contexts: For parallel computers, the parallelism in eliminating these subdiagonal blocks is rela tively fine-grained compared with the more coarse-grained parallelism in the first step of the algorithm. Furthermore, on distributed memory machines the data for each subdiagonal block has to be spread over all processors. In <ref> [50] </ref> it has been shown that this limits the performance of the algorithm, the speedup being bounded by the ratio of computational speed and communication speed. This ratio is often very low [50]. <p> In <ref> [50] </ref> it has been shown that this limits the performance of the algorithm, the speedup being bounded by the ratio of computational speed and communication speed. This ratio is often very low [50]. Gaussian Elimination on Sparse Matrices 21 The other approach is first to eliminate successively the last nonzero elements in the subdiagonal blocks ~ L j;j1 . This can be done with a short recurrence of length n=k 1, after which all fill-in can be eliminated in parallel. <p> However, for k large enough with respect to n=k, one can attain speedups close to 2k=5 for this algorithm on a k processor system [51]. For a generalization of the divide-and-conquer approach for banded systems, see [52]; the data transport aspects for distributed memory machines have been discussed in <ref> [50] </ref>. There are other variants of the divide-and-conquer approach that move the fill-in into other columns of the subblocks or are more stable numerically.
Reference: [51] <author> H. A. van der Vorst, </author> <booktitle> Future Generation Computer Systems 4, </booktitle> <month> 285 </month> <year> (1989). </year>
Reference-contexts: For the recurrence we need some data communication between processors. However, for k large enough with respect to n=k, one can attain speedups close to 2k=5 for this algorithm on a k processor system <ref> [51] </ref>. For a generalization of the divide-and-conquer approach for banded systems, see [52]; the data transport aspects for distributed memory machines have been discussed in [50]. There are other variants of the divide-and-conquer approach that move the fill-in into other columns of the subblocks or are more stable numerically.
Reference: [52] <author> U. </author> <title> Meier, </title> <journal> Parallel Comput. </journal> <volume> 2, </volume> <month> 33 </month> <year> (1985). </year>
Reference-contexts: For the recurrence we need some data communication between processors. However, for k large enough with respect to n=k, one can attain speedups close to 2k=5 for this algorithm on a k processor system [51]. For a generalization of the divide-and-conquer approach for banded systems, see <ref> [52] </ref>; the data transport aspects for distributed memory machines have been discussed in [50]. There are other variants of the divide-and-conquer approach that move the fill-in into other columns of the subblocks or are more stable numerically.
Reference: [53] <author> V. Mehrmann, </author> <type> Technical Report No. Bericht Nr. 68, </type> <institution> Inst. fuer Geometrie und Prakt. Math., Aachen (unpublished). </institution>
Reference-contexts: There are other variants of the divide-and-conquer approach that move the fill-in into other columns of the subblocks or are more stable numerically. For example, in <ref> [53] </ref> the matrix is split into a block diagonal matrix and a remainder via rank-1 updates. 8 Gaussian Elimination on Sparse Matrices 8.1 Cholesky Factorization In this section we discuss parallel algorithms for solving sparse systems of linear equations by direct methods.
Reference: [54] <author> M. Heath, E. Ng, and B. Peyton, </author> <note> SIAM Review 33, 420 (1991). </note>
Reference-contexts: Most of the lessons learned are also applicable to other matrix factorizations, such as LU and QR. We do not try to give an exhaustive survey of research in this area, which is currently very active, instead referring the reader to existing surveys, such as <ref> [54] </ref>. Our main point in the current discussion is to explain how the sparse case differs from the dense case, and examine the performance implications of those differences. We begin by considering the main features of sparse Cholesky factorization that affect its performance on serial machines.
Reference: [55] <author> I. Duff, A. Erisman, and J. Reid, </author> <title> Direct Methods for Sparse Matrices (Oxford University Press, </title> <publisher> Oxford, </publisher> <address> England, </address> <year> 1986). </year>
Reference-contexts: These techniques include minimum degree, nested dissection, and various schemes for reducing the bandwidth or profile of a matrix (see, e.g., <ref> [55, 56] </ref>). for details on these and many other concepts used in sparse matrix computations). <p> We will see that these characterizations of the column-Cholesky and submatrix-Cholesky algorithms have important implications for parallel implementations. We note that many variations and hybrid implementations that lie somewhere between pure column-Cholesky and pure submatrix-Cholesky are possible. Perhaps the most important of these are the multi-frontal methods (see, e.g., <ref> [55] </ref>), in which updating operations are accumulated in and propagated through a series of front matrices until finally being incorporated into the ultimate target columns. <p> This is not a general purpose strategy for sparse matrices, but it is often used to enhance parallelism in tridiagonal and related systems, so we illustrate it for the sake of comparison with more general purpose methods. In odd-even reduction (see, e.g., <ref> [55] </ref>), odd node numbers come before even node numbers, and then this same renumbering is applied recursively within each Gaussian Elimination on Sparse Matrices 31 Cholesky factor and elimination tree (right). resulting subset, and so on until all nodes are numbered.
Reference: [56] <author> A. George and J. Liu, </author> <title> Computer Solution of Large Sparse Positive Definite Systems (Prentice-Hall Inc., </title> <address> Englewood Cliffs, New Jersey, </address> <year> 1981). </year>
Reference-contexts: These techniques include minimum degree, nested dissection, and various schemes for reducing the bandwidth or profile of a matrix (see, e.g., <ref> [55, 56] </ref>). for details on these and many other concepts used in sparse matrix computations).
Reference: [57] <author> J. Liu, </author> <note> SIAM J. Matrix Anal. Appl. 11, 134 (1990). </note>
Reference-contexts: The elimination tree, which we denote by T (A), is a spanning tree for the filled graph F (A). The many uses of the elimination tree in analyzing and organizing sparse Cholesky factorization are surveyed in <ref> [57] </ref>.
Reference: [58] <author> B. </author> <title> Irons, </title> <journal> Internat. J. Numer. Meth. Engrg. </journal> <volume> 2, </volume> <month> 5 </month> <year> (1970). </year>
Reference-contexts: Multifrontal methods have a number of attractive advantages, most of which accrue from the localization of memory references in the front matrices, thereby facilitating the effective use of memory hierarchies, including cache, virtual memory with paging, or explicit out-of-core solutions (the latter was the original motivation for these methods <ref> [58] </ref>. In addition, since the front matrices are essentially dense, the operations on them can be done using optimized kernels, such as the BLAS, to take advantage of vectorization or any other available architectural features.
Reference: [59] <author> C. Ashcraft et al., </author> <note> Internat. J. Supercomp. Appl 1, 10 (1987). </note>
Reference-contexts: For example, such techniques have been used to attain very high performance for sparse factorization on conventional vector supercomputers <ref> [59] </ref> and on RISC workstations [60]. 8.4 Parallelism in Sparse Factorization We now examine in greater detail the opportunities for parallelism in sparse Cholesky factorization and various algorithms for exploiting it.
Reference: [60] <author> E. Rothberg and A. Gupta, </author> <type> Technical Report No. </type> <institution> STAN-CS-89-1286, Stanford University, Stanford, California (unpublished). </institution>
Reference-contexts: For example, such techniques have been used to attain very high performance for sparse factorization on conventional vector supercomputers [59] and on RISC workstations <ref> [60] </ref>. 8.4 Parallelism in Sparse Factorization We now examine in greater detail the opportunities for parallelism in sparse Cholesky factorization and various algorithms for exploiting it.
Reference: [61] <author> J. Liu, </author> <booktitle> Parallel Computing 3, </booktitle> <month> 327 </month> <year> (1986). </year>
Reference-contexts: The optimal choice of task size depends on the tradeoff between communication costs and the load balance across processors. We follow Liu <ref> [61] </ref> in identifying three potential levels of granularity in a parallel implementation of Cholesky factorization: Gaussian Elimination on Sparse Matrices 27 (1) fine-grain, in which each task consists of only one or two floating point operations, such as a multiply-add pair, (2) medium-grain, in which each task is a single column
Reference: [62] <author> J. Jess and H. </author> <title> Kees, </title> <journal> IEEE Trans. Comput. </journal> <volume> C-31, </volume> <month> 231 </month> <year> (1982). </year>
Reference-contexts: And just as the fill in the Cholesky factor is very sensitive to the ordering of the matrix, so is the structure of the elimination tree. This suggests that we should choose an ordering to enhance parallelism, and indeed this is possible (see, e.g., <ref> [62, 63, 64] </ref>) but such an objective may conflict to some degree with preservation of sparsity. Roughly speaking, sparsity and parallelism are largely compatible, since the large-grain parallelism is due to sparsity in the first place.
Reference: [63] <author> J. Lewis, B. Peyton, and A. </author> <title> Pothen, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 10, </volume> <month> 1156 </month> <year> (1989). </year>
Reference-contexts: And just as the fill in the Cholesky factor is very sensitive to the ordering of the matrix, so is the structure of the elimination tree. This suggests that we should choose an ordering to enhance parallelism, and indeed this is possible (see, e.g., <ref> [62, 63, 64] </ref>) but such an objective may conflict to some degree with preservation of sparsity. Roughly speaking, sparsity and parallelism are largely compatible, since the large-grain parallelism is due to sparsity in the first place.
Reference: [64] <author> J. Liu, </author> <booktitle> Parallel Computing 11, </booktitle> <month> 73 </month> <year> (1989). </year>
Reference-contexts: And just as the fill in the Cholesky factor is very sensitive to the ordering of the matrix, so is the structure of the elimination tree. This suggests that we should choose an ordering to enhance parallelism, and indeed this is possible (see, e.g., <ref> [62, 63, 64] </ref>) but such an objective may conflict to some degree with preservation of sparsity. Roughly speaking, sparsity and parallelism are largely compatible, since the large-grain parallelism is due to sparsity in the first place. <p> Cholesky factor and elimination tree (right). 30 Cholesky factor and elimination tree (right). but which is taller and less well balanced than our example in Figure 7. Again, this is typical of minimum degree orderings. In view of this property, Liu <ref> [64] </ref> has developed an interesting strategy for further reordering of an initial minimum degree ordering that preserves fill while reducing the height of the elimination tree. with all edges incident upon nodes in S, disconnects G (A) into two remaining subgraphs.
Reference: [65] <author> A. George and J. Liu, </author> <note> SIAM Review 31, 1 (1989). </note>
Reference-contexts: The cmod operations involve only a couple of flops each, so that even the `medium-grain' tasks are actually rather small in this case.) degree algorithm. Minimum degree is the most effective general purpose heuristic known for limiting fill in sparse factorization <ref> [65] </ref>. In its simplest form, this algorithm begins by selecting a node of minimum degree (i.e. one having fewest incident edges) in G (A) and numbering it first. The selected node is then deleted and new edges are added, if necessary, to make its former neighbors into a clique.
Reference: [66] <author> A. George, </author> <note> SIAM J. Num. Anal. 10, 345 (1973). </note>
Reference: [67] <author> A. George, M. Heath, J. Liu, and E. Ng, Internat. J. </author> <booktitle> Parallel Programming 15, </booktitle> <month> 309 </month> <year> (1986). </year>
Reference-contexts: This approach has the additional advantage of providing automatic load balancing to whatever degree is permitted by the chosen task granularity. An implementation of this approach for parallel sparse factorization is given in <ref> [67] </ref>. In a distributed memory environment, communication costs often prohibit dynamic task assignment or load balancing, and thus we seek a static mapping of tasks to processors.
Reference: [68] <author> A. George, M. Heath, J. Liu, and E. Ng, J. </author> <title> Comp. </title> <journal> Appl. Math. </journal> <volume> 27, </volume> <month> 129 </month> <year> (1989). </year>
Reference-contexts: A good example of this technique is the `subtree-to-subcube' mapping often used with hypercube multicomputers <ref> [68] </ref>. Of course, the same idea applies to other network topologies, such as submeshes of a larger mesh. We will assume that some such mapping is used, and we will comment further on its implications later.
Reference: [69] <author> A. George, M. Heath, J. Liu, and E. </author> <title> Ng, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 9, </volume> <month> 327 </month> <year> (1988). </year>
Reference-contexts: One of the earliest and simplest parallel algorithms for sparse Cholesky factorization is the following version of submatrix-Cholesky <ref> [69] </ref>. Algorithm 8.4 runs on each processor, with each responsible for its own subset, mycols, of columns.
Reference: [70] <author> A. George, J. Liu, and E. Ng, </author> <booktitle> Parallel Computing 10, </booktitle> <month> 287 </month> <year> (1989). </year>
Reference-contexts: The communication requirements can be reduced by careful mapping and by aggregating updating information over subtrees (see, e.g., <ref> [70, 71, 72] </ref>) but even with this improvement, the fan-out algorithm is usually not competitive with other algorithms presented later. The shortcomings of the fan-out algorithm motivated the formulation of the following fan-in algorithm for sparse factorization, which is a parallel implementation of column-Cholesky [73].
Reference: [71] <author> M. Mu and J. </author> <title> Rice, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 13, </volume> <month> 826 </month> <year> (1992). </year>
Reference-contexts: The communication requirements can be reduced by careful mapping and by aggregating updating information over subtrees (see, e.g., <ref> [70, 71, 72] </ref>) but even with this improvement, the fan-out algorithm is usually not competitive with other algorithms presented later. The shortcomings of the fan-out algorithm motivated the formulation of the following fan-in algorithm for sparse factorization, which is a parallel implementation of column-Cholesky [73].
Reference: [72] <author> E. Zmijewski, </author> <type> Technical Report No. </type> <institution> TRCS89-18, Department of Computer Science, University of California, Santa Barbara, </institution> <note> CA (unpublished). 60 </note>
Reference-contexts: The communication requirements can be reduced by careful mapping and by aggregating updating information over subtrees (see, e.g., <ref> [70, 71, 72] </ref>) but even with this improvement, the fan-out algorithm is usually not competitive with other algorithms presented later. The shortcomings of the fan-out algorithm motivated the formulation of the following fan-in algorithm for sparse factorization, which is a parallel implementation of column-Cholesky [73].
Reference: [73] <author> C. Ashcraft, S. Eisenstat, and J. Liu, </author> <note> SIAM J. </note> <institution> Sci. Stat. Comput. </institution> <month> 11, 593 </month> <year> (1990). </year>
Reference-contexts: The shortcomings of the fan-out algorithm motivated the formulation of the following fan-in algorithm for sparse factorization, which is a parallel implementation of column-Cholesky <ref> [73] </ref>.
Reference: [74] <author> R. Benner, G. Montry, and G. </author> <title> Weigand, </title> <journal> Internat. J. Supercomp. </journal> <note> Appl 1, 26 (1987). </note>
Reference-contexts: As a consequence, multi-frontal methods are difficult to specify succinctly, so we will not attempt to do so here, but note that multi-frontal methods have been implemented for both shared-memory (e.g., <ref> [74, 75] </ref>) and distributed-memory (e.g., [76, 77]) parallel computers, and are among the most effective methods known for sparse factorization in all types of computational environments. For a unified description and comparison of parallel fan-in, fan-out and multi-frontal methods, see [78].
Reference: [75] <author> I. Duff, </author> <booktitle> Parallel Computing 3, </booktitle> <month> 193 </month> <year> (1986). </year>
Reference-contexts: As a consequence, multi-frontal methods are difficult to specify succinctly, so we will not attempt to do so here, but note that multi-frontal methods have been implemented for both shared-memory (e.g., <ref> [74, 75] </ref>) and distributed-memory (e.g., [76, 77]) parallel computers, and are among the most effective methods known for sparse factorization in all types of computational environments. For a unified description and comparison of parallel fan-in, fan-out and multi-frontal methods, see [78].
Reference: [76] <author> J. Gilbert and R. </author> <title> Schreiber, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 13, </volume> <month> 1151 </month> <year> (1992). </year>
Reference-contexts: As a consequence, multi-frontal methods are difficult to specify succinctly, so we will not attempt to do so here, but note that multi-frontal methods have been implemented for both shared-memory (e.g., [74, 75]) and distributed-memory (e.g., <ref> [76, 77] </ref>) parallel computers, and are among the most effective methods known for sparse factorization in all types of computational environments. For a unified description and comparison of parallel fan-in, fan-out and multi-frontal methods, see [78].
Reference: [77] <author> R. Lucas, W. Blank, and J. </author> <title> Tieman, </title> <journal> IEEE Trans. Computer Aided Design CAD-6, </journal> <month> 981 </month> <year> (1987). </year>
Reference-contexts: As a consequence, multi-frontal methods are difficult to specify succinctly, so we will not attempt to do so here, but note that multi-frontal methods have been implemented for both shared-memory (e.g., [74, 75]) and distributed-memory (e.g., <ref> [76, 77] </ref>) parallel computers, and are among the most effective methods known for sparse factorization in all types of computational environments. For a unified description and comparison of parallel fan-in, fan-out and multi-frontal methods, see [78].
Reference: [78] <author> C. Ashcraft, S. Eisenstat, J. Liu, and A. Sherman, </author> <type> Technical Report No. </type> <institution> YALEU/DCS/RR-810, Dept. of Computer Science, Yale University, New Haven, CT (unpublished). </institution>
Reference-contexts: For a unified description and comparison of parallel fan-in, fan-out and multi-frontal methods, see <ref> [78] </ref>. In this brief section on parallel direct methods for sparse systems, we have concentrated on numeric Cholesky factorization for SPD matrices. We have omitted many other aspects of the computation, even for the SPD case: computing the ordering in parallel, symbolic factorization, and triangular solution.
Reference: [79] <author> R. Freund, G. Golub, and N. Nachtigal, </author> <title> in Acta Numerica 1992, edited by A. </title> <publisher> Iserles (Cambridge University Press, ADDRESS, </publisher> <year> 1992), </year> <pages> pp. 57-100. </pages>
Reference-contexts: For a good mathematical introduction to a class of successful and popular methods, the so-called Krylov subspace methods, see <ref> [79] </ref>. There are many such methods and new ones are frequently proposed. Fortunately, they share enough properties that to understand how to implement them in parallel it suffices to examine carefully just a few. <p> In other short recurrence methods, other properties of A may be required or desirable, but we will not exploit these properties explicitly here. Most often, CG is used in combination with some kind of preconditioning <ref> [79, 4, 97] </ref>. This means that the matrix A is implicitly multiplied by an approximation K 1 of A 1 . Usually, K is constructed to be an approximation of A, and so that Ky = z is easier to solve than Ax = b.
Reference: [80] <author> E. de Sturler, </author> <type> Technical Report No. 91-85, </type> <institution> Delft University of Technology, Delft (unpublished). </institution>
Reference-contexts: If the number of connections to these neighboring blocks is small compared to the number of internal nodes, then the communication time can be overlapped with computational work. 36 For more detailed discussions on implementation aspects on distributed memory systems, see <ref> [80] </ref> and [81]. Preconditioning is often the most problematic part in a parallel environment. Incomplete decompositions of A form a popular class of preconditionings in the context of solving discretized PDEs. <p> Newton polynomials are suggested in [111] and and Chebychev polynomials in <ref> [80] </ref>. After generating a suitable starting set, we still have to orthogonalize it. In [80] modified Gram-Schmidt is used while avoiding communication times that cannot be overlapped. We outline this approach, since it may be of value for other orthogonalization methods. <p> Newton polynomials are suggested in [111] and and Chebychev polynomials in <ref> [80] </ref>. After generating a suitable starting set, we still have to orthogonalize it. In [80] modified Gram-Schmidt is used while avoiding communication times that cannot be overlapped. We outline this approach, since it may be of value for other orthogonalization methods. <p> For a 150 processor MEIKO system, configured as a 15 fi 10 torus, de Sturler <ref> [80] </ref> reports speedups of about 120 for typical discretized PDE systems with 60; 000 unknowns (i.e. 400 unknowns per processor). For larger systems, the speedup increases to 150 (or more if more processors are involved) as expected.
Reference: [81] <author> C. </author> <title> Pommerell, </title> <type> Ph.D. thesis, </type> <institution> Swiss Federal Institute of Technology, </institution> <address> Zurich, </address> <year> 1992. </year>
Reference-contexts: If the number of connections to these neighboring blocks is small compared to the number of internal nodes, then the communication time can be overlapped with computational work. 36 For more detailed discussions on implementation aspects on distributed memory systems, see [80] and <ref> [81] </ref>. Preconditioning is often the most problematic part in a parallel environment. Incomplete decompositions of A form a popular class of preconditionings in the context of solving discretized PDEs.
Reference: [82] <author> J. A. Meijerink and H. A. van der Vorst, Math.Comp. </author> <month> 31, 148 </month> <year> (1977). </year>
Reference-contexts: In this case the preconditioner K = LU , where L and U have a sparsity pattern equal or close to the sparsity pattern of the corresponding parts of A (L is lower triangular, U is upper triangular). For details see [4], <ref> [82] </ref> and [83]. Solving Kw = r leads to solving successively Lz = r and U w = z. These triangular solves lead to recurrence relations that are not easily parallelized. We will now discuss a number of approaches to obtain parallelism in the preconditioning part. (1) Reordering the computations.
Reference: [83] <author> J. A. Meijerink and H. A. van der Vorst, J. </author> <title> Comp. </title> <journal> Phys. </journal> <volume> 44, </volume> <month> 134 </month> <year> (1981). </year>
Reference-contexts: In this case the preconditioner K = LU , where L and U have a sparsity pattern equal or close to the sparsity pattern of the corresponding parts of A (L is lower triangular, U is upper triangular). For details see [4], [82] and <ref> [83] </ref>. Solving Kw = r leads to solving successively Lz = r and U w = z. These triangular solves lead to recurrence relations that are not easily parallelized. We will now discuss a number of approaches to obtain parallelism in the preconditioning part. (1) Reordering the computations.
Reference: [84] <author> C. Ashcraft and R. </author> <title> Grimes, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 9, </volume> <month> 122 </month> <year> (1988). </year>
Reference-contexts: For vector computers this leads to a vectorizable preconditioner (see <ref> [84, 8, 85, 86] </ref>). For coarse-grained parallelism this approach is insufficient. By a similar approach more parallelism can be obtained in three-dimensional situations: the so-called hyperplane approach [87, 85, 86].
Reference: [85] <author> H. A. </author> <title> van der Vorst, </title> <journal> SIAM J. Sci. Statist. Comput. </journal> <volume> 10, </volume> <month> 1174 </month> <year> (1989). </year>
Reference-contexts: For vector computers this leads to a vectorizable preconditioner (see <ref> [84, 8, 85, 86] </ref>). For coarse-grained parallelism this approach is insufficient. By a similar approach more parallelism can be obtained in three-dimensional situations: the so-called hyperplane approach [87, 85, 86]. <p> For vector computers this leads to a vectorizable preconditioner (see [84, 8, 85, 86]). For coarse-grained parallelism this approach is insufficient. By a similar approach more parallelism can be obtained in three-dimensional situations: the so-called hyperplane approach <ref> [87, 85, 86] </ref>. The disadvantage is that the data need to be redistributed over the processors, since the grid points, which correspond to a hyperplane in the grid, are located quite irregularly in the array. For shared memory machines this also leads to reduced performance because of indirect addressing. <p> Duff and Meurant [88] have carried out numerical experiments for many different orderings, which show that the numbers of iterations may increase significantly for other than lexicographical ordering. Some modest degree of parallelism can be obtained, however, with so-called incomplete twisted factorizations <ref> [8, 89, 85] </ref>. Multi-color schemes with a large number of colors (e.g., 20 to 100) may lead to little or no degradation in convergence behavior [90]. but also to less parallelism. Moreover, the ratio of computation to communication may be more unfavorable. (3) Forced parallelism.
Reference: [86] <author> H. A. van der Vorst, </author> <booktitle> in Proc. of the fifth Int.Symp. on Numer. Methods in Eng., </booktitle> <editor> edited by R. Gruber, J. Periaux, and R. P. Shaw (PUBLISHER, </editor> <publisher> ADDRESS, </publisher> <year> 1989), </year> <note> vol 1. </note>
Reference-contexts: For vector computers this leads to a vectorizable preconditioner (see <ref> [84, 8, 85, 86] </ref>). For coarse-grained parallelism this approach is insufficient. By a similar approach more parallelism can be obtained in three-dimensional situations: the so-called hyperplane approach [87, 85, 86]. <p> For vector computers this leads to a vectorizable preconditioner (see [84, 8, 85, 86]). For coarse-grained parallelism this approach is insufficient. By a similar approach more parallelism can be obtained in three-dimensional situations: the so-called hyperplane approach <ref> [87, 85, 86] </ref>. The disadvantage is that the data need to be redistributed over the processors, since the grid points, which correspond to a hyperplane in the grid, are located quite irregularly in the array. For shared memory machines this also leads to reduced performance because of indirect addressing.
Reference: [87] <author> J. J. F. M. Schlichting and H. A. </author> <title> van der Vorst, </title> <journal> Journal of Comp. and Appl. Math. </journal> <volume> 27, </volume> <month> 323 </month> <year> (1989). </year>
Reference-contexts: For vector computers this leads to a vectorizable preconditioner (see [84, 8, 85, 86]). For coarse-grained parallelism this approach is insufficient. By a similar approach more parallelism can be obtained in three-dimensional situations: the so-called hyperplane approach <ref> [87, 85, 86] </ref>. The disadvantage is that the data need to be redistributed over the processors, since the grid points, which correspond to a hyperplane in the grid, are located quite irregularly in the array. For shared memory machines this also leads to reduced performance because of indirect addressing.
Reference: [88] <author> I. S. Duff and G. A. Meurant, </author> <note> BIT 29, 635 (1989). </note>
Reference-contexts: This means that the triangular solves can be parallelized for each color. Of course, communication is required for couplings between groups of different colors. Simple coloring schemes, like red-black ordering for the five-point discretized Poisson operator, seem to have a negative effect on the convergence behavior. Duff and Meurant <ref> [88] </ref> have carried out numerical experiments for many different orderings, which show that the numbers of iterations may increase significantly for other than lexicographical ordering. Some modest degree of parallelism can be obtained, however, with so-called incomplete twisted factorizations [8, 89, 85].
Reference: [89] <author> H. A. van der Vorst, J. </author> <title> Comp. </title> <journal> and Appl. Math. </journal> <volume> 18, </volume> <month> 249 </month> <year> (1987). </year>
Reference-contexts: Duff and Meurant [88] have carried out numerical experiments for many different orderings, which show that the numbers of iterations may increase significantly for other than lexicographical ordering. Some modest degree of parallelism can be obtained, however, with so-called incomplete twisted factorizations <ref> [8, 89, 85] </ref>. Multi-color schemes with a large number of colors (e.g., 20 to 100) may lead to little or no degradation in convergence behavior [90]. but also to less parallelism. Moreover, the ratio of computation to communication may be more unfavorable. (3) Forced parallelism.
Reference: [90] <author> S. </author> <title> Doi, </title> <journal> Appl. Num. Math. </journal> <volume> 7, </volume> <month> 417 </month> <year> (1991). </year>
Reference-contexts: Some modest degree of parallelism can be obtained, however, with so-called incomplete twisted factorizations [8, 89, 85]. Multi-color schemes with a large number of colors (e.g., 20 to 100) may lead to little or no degradation in convergence behavior <ref> [90] </ref>. but also to less parallelism. Moreover, the ratio of computation to communication may be more unfavorable. (3) Forced parallelism. Parallelism can also be forced by simply neglecting couplings to unknowns residing in other processors.
Reference: [91] <author> M. K. </author> <title> Seager, </title> <booktitle> Parallel Computing 3, </booktitle> <month> 35 </month> <year> (1986). </year>
Reference-contexts: Moreover, the ratio of computation to communication may be more unfavorable. (3) Forced parallelism. Parallelism can also be forced by simply neglecting couplings to unknowns residing in other processors. This is like block Jacobi preconditioning, in which the blocks may be decomposed in incomplete form <ref> [91] </ref>. Again, this may not always reduce the overall solution time, since the effects of increased parallelism are more Iterative Methods for Linear Systems 37 than undone by an increased number of iteration steps.
Reference: [92] <author> G. Radicati di Brozolo and Y. Robert, </author> <note> Technical Report No. 681-M, IMAG/TIM3, Grenoble (unpublished). Exercises 61 </note>
Reference-contexts: Again, this may not always reduce the overall solution time, since the effects of increased parallelism are more Iterative Methods for Linear Systems 37 than undone by an increased number of iteration steps. In order to reduce this effect, it is suggested in <ref> [92] </ref> to construct incomplete decompositions on slightly overlapping domains. This requires communication similar to that of matrix-vector products. In [92] results are reported on a six-processor shared memory system (IBM3090), and speedups close to 6 have been observed. <p> In order to reduce this effect, it is suggested in <ref> [92] </ref> to construct incomplete decompositions on slightly overlapping domains. This requires communication similar to that of matrix-vector products. In [92] results are reported on a six-processor shared memory system (IBM3090), and speedups close to 6 have been observed. The problems with parallelism in the preconditioner have led to searches for other pre-conditioners. Often simple diagonal scaling is an adequate preconditioner and this is trivially parallelizable.
Reference: [93] <author> H. Berryman, J. Saltz, W. Gropp, and R. Mirchandaney, </author> <type> Technical Report No. 89-54, </type> <institution> NASA Langley Research Center, ICASE, Hampton, VA (unpublished). </institution>
Reference-contexts: The problems with parallelism in the preconditioner have led to searches for other pre-conditioners. Often simple diagonal scaling is an adequate preconditioner and this is trivially parallelizable. For results on a Connection Machine, see <ref> [93] </ref>. Often this approach leads to a significant increase in iteration steps. Still another approach is to use polynomial preconditioning: w = p j (A)r, i.e. K 1 = p j (A), for some suitable jth degree polynomial.
Reference: [94] <author> Y. </author> <title> Saad, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 6, </volume> <month> 865 </month> <year> (1985). </year>
Reference-contexts: Still another approach is to use polynomial preconditioning: w = p j (A)r, i.e. K 1 = p j (A), for some suitable jth degree polynomial. This preconditioner can be implemented by forming only matrix-vector products, which, depending on the structure of A, are easier to parallelize <ref> [94] </ref>. For p j one often selects a Chebychev polynomial, which requires some information on the spectrum of A. Finally we point out the possibility of using the truncated Neumann series for the approximate inverse of A, or parts of L and U . <p> Several authors [98, 99, 100, 101] have attempted to improve this ratio, and to reduce the number of synchronization points (the points at which computation must wait for communication). In Algorithm 9.1 there are two such synchronization points, namely the computation of both inner products. Meurant [100] (see also <ref> [94] </ref>) has proposed a variant in which there is only one synchronization point, however at the cost of possibly reduced numerical stability, and one additional inner product. In this scheme the ratio between computations and memory references is about 2.
Reference: [95] <author> N. K. Madsen, G. H. Rodrigue, and J. I. </author> <title> Karush, </title> <journal> Inform. Process.Lett. </journal> <volume> 5, </volume> <month> 41 </month> <year> (1976). </year>
Reference-contexts: For p j one often selects a Chebychev polynomial, which requires some information on the spectrum of A. Finally we point out the possibility of using the truncated Neumann series for the approximate inverse of A, or parts of L and U . Madsen et al. <ref> [95] </ref> discuss approximate inversion of A, which from the implementation point of view is equivalent to polynomial preconditioning. In [96] the use of truncated Neumann series for removing some of the recurrences in the triangular solves is discussed.
Reference: [96] <author> H. A. van der Vorst, </author> <note> SIAM J. </note> <institution> Sci. Stat. Comput. </institution> <month> 3, 86 </month> <year> (1982). </year>
Reference-contexts: Finally we point out the possibility of using the truncated Neumann series for the approximate inverse of A, or parts of L and U . Madsen et al. [95] discuss approximate inversion of A, which from the implementation point of view is equivalent to polynomial preconditioning. In <ref> [96] </ref> the use of truncated Neumann series for removing some of the recurrences in the triangular solves is discussed. This approach leads to only fine-grained parallelism (vectorization). 9.2 Parallelism and Data Locality in Preconditioned CG To use CG to solve Ax = b, A must be symmetric and positive definite.
Reference: [97] <author> M. R. Hestenes and E. </author> <type> Stiefel, </type> <institution> J. Res. Natl. Bur. Stand. </institution> <month> 49, 409 </month> <year> (1954). </year>
Reference-contexts: In other short recurrence methods, other properties of A may be required or desirable, but we will not exploit these properties explicitly here. Most often, CG is used in combination with some kind of preconditioning <ref> [79, 4, 97] </ref>. This means that the matrix A is implicitly multiplied by an approximation K 1 of A 1 . Usually, K is constructed to be an approximation of A, and so that Ky = z is easier to solve than Ax = b.
Reference: [98] <author> A. T. Chronopoulos and C. W. </author> <title> Gear, </title> <journal> J. on Comp. and Appl. Math. </journal> <volume> 25, </volume> <month> 153 </month> <year> (1989). </year>
Reference-contexts: This means that for this part of the computation only 10=7 floating point operation can be carried out per memory reference on average. Several authors <ref> [98, 99, 100, 101] </ref> have attempted to improve this ratio, and to reduce the number of synchronization points (the points at which computation must wait for communication). In Algorithm 9.1 there are two such synchronization points, namely the computation of both inner products. <p> In this scheme the ratio between computations and memory references is about 2. We show here yet another variant, proposed by Chronopoulos and Gear <ref> [98] </ref>. <p> However, the price is 2n extra flops per iteration step. Chronopoulos and Gear <ref> [98] </ref> claim the method is stable, based on their numerical experiments. <p> Another slight advantage is that these inner products can be computed in parallel. Chronopoulos and Gear <ref> [98] </ref> propose to improve further the data locality and parallelism in CG by combining s successive steps. Their algorithm is based upon the following property of CG.
Reference: [99] <author> G. Meurant, </author> <type> Technical Report No. </type> <institution> LBL-18023, University of California, Berkeley, CA (unpublished). </institution>
Reference-contexts: This means that for this part of the computation only 10=7 floating point operation can be carried out per memory reference on average. Several authors <ref> [98, 99, 100, 101] </ref> have attempted to improve this ratio, and to reduce the number of synchronization points (the points at which computation must wait for communication). In Algorithm 9.1 there are two such synchronization points, namely the computation of both inner products.
Reference: [100] <author> G. Meurant, </author> <note> BIT 24, 623 (1984). </note>
Reference-contexts: This means that for this part of the computation only 10=7 floating point operation can be carried out per memory reference on average. Several authors <ref> [98, 99, 100, 101] </ref> have attempted to improve this ratio, and to reduce the number of synchronization points (the points at which computation must wait for communication). In Algorithm 9.1 there are two such synchronization points, namely the computation of both inner products. <p> Several authors [98, 99, 100, 101] have attempted to improve this ratio, and to reduce the number of synchronization points (the points at which computation must wait for communication). In Algorithm 9.1 there are two such synchronization points, namely the computation of both inner products. Meurant <ref> [100] </ref> (see also [94]) has proposed a variant in which there is only one synchronization point, however at the cost of possibly reduced numerical stability, and one additional inner product. In this scheme the ratio between computations and memory references is about 2.
Reference: [101] <author> H. A. van der Vorst, </author> <booktitle> Parallel Computing 3, </booktitle> <month> 49 </month> <year> (1986). </year>
Reference-contexts: This means that for this part of the computation only 10=7 floating point operation can be carried out per memory reference on average. Several authors <ref> [98, 99, 100, 101] </ref> have attempted to improve this ratio, and to reduce the number of synchronization points (the points at which computation must wait for communication). In Algorithm 9.1 there are two such synchronization points, namely the computation of both inner products.
Reference: [102] <author> A. T. </author> <title> Chronopoulos, </title> <publisher> in Supercomputing '91 (IEEE Computer Society Press, </publisher> <address> Los Alami-tos, CA, </address> <year> 1991), </year> <pages> pp. 578-587. </pages>
Reference-contexts: Their approach leads to slightly more flops than s successive steps of standard CG, and also one additional matrix-vector product every s steps. The implementation issues for vector register computers and distributed memory machines are discussed in great detail in <ref> [102] </ref>.
Reference: [103] <author> Y. Saad, </author> <type> Technical report, </type> <institution> RIACS, Moffett Field, CA (unpublished). </institution>
Reference-contexts: The authors claim success in using this approach without serious stability problems for small values of s. Nevertheless, it seems that s-step CG still has a bad reputation <ref> [103] </ref> because of these problems. However, a similar approach, suggested by Chronopoulos and Kim [104] for other processes such as GMRES, seems to be more promising. Several authors have pursued this direction, and we will come back to this in Section 9.3. <p> In Algorithm 9.4, this is done using Level 1 BLAS, which may be quite inefficient. To incorporate Level 2 BLAS we can do either Householder orthogonalization or classical Gram-Schmidt twice (which mitigates classical Gram-Schmidt's potential instability <ref> [103] </ref>. Both approaches significantly increase the computational work and do not remove the synchronization and data-locality problems completely.
Reference: [104] <author> A. T. Chronopoulos and S. K. Kim, </author> <note> Technical Report No. 90/43R, UMSI, Minneapolis (unpublished). </note>
Reference-contexts: The authors claim success in using this approach without serious stability problems for small values of s. Nevertheless, it seems that s-step CG still has a bad reputation [103] because of these problems. However, a similar approach, suggested by Chronopoulos and Kim <ref> [104] </ref> for other processes such as GMRES, seems to be more promising. Several authors have pursued this direction, and we will come back to this in Section 9.3. We consider another variant of CG, in which we may overlap all communication time with useful computations. <p> The obvious way to extract more parallelism and data locality is to generate a basis v 1 , Av 1 , ..., A m v 1 for the Krylov subspace first, and to orthogonalize this set afterwards; this is called m-step GMRES (m) <ref> [104] </ref>. This approach does not increase the computational work and, in contrast to CG, the numerical instability due to generating a possibly near-dependent set is not necessarily a drawback. One reason is that error cannot build up as in CG, because the method is restarted every m steps.
Reference: [105] <author> E. D'Azevedo and C. Romine, </author> <type> Technical Report ORNL/TM-12192, </type> <institution> Oak Ridge National Laboratory (unpublished). </institution>
Reference-contexts: To this end, one might split the computation in (4) in two parts. The first part would be computed in parallel with (3), and the second part with i+1 . More recent work on removing synchronization points in CG while retaining numerical stability appears in <ref> [105, 106] </ref>. 9.3 Parallelism and Data Locality in GMRES GMRES, proposed by Saad and Schultz [107], is a CG-like method for solving general non-singular linear systems Ax = b.
Reference: [106] <author> V. </author> <type> Eijkhout, </type> <institution> Compute Science Dept. </institution> <type> Technical Report CS-92-170, </type> <institution> University of Ten-nessee at Knoxville (unpublished). </institution>
Reference-contexts: To this end, one might split the computation in (4) in two parts. The first part would be computed in parallel with (3), and the second part with i+1 . More recent work on removing synchronization points in CG while retaining numerical stability appears in <ref> [105, 106] </ref>. 9.3 Parallelism and Data Locality in GMRES GMRES, proposed by Saad and Schultz [107], is a CG-like method for solving general non-singular linear systems Ax = b.
Reference: [107] <author> Y. Saad and M. H. </author> <title> Schultz, </title> <journal> SIAM J. Sci. Statist. Comput. </journal> <volume> 7, </volume> <month> 856 </month> <year> (1986). </year>
Reference-contexts: The first part would be computed in parallel with (3), and the second part with i+1 . More recent work on removing synchronization points in CG while retaining numerical stability appears in [105, 106]. 9.3 Parallelism and Data Locality in GMRES GMRES, proposed by Saad and Schultz <ref> [107] </ref>, is a CG-like method for solving general non-singular linear systems Ax = b. GMRES minimizes the residual over the Krylov subspace span [r 0 ; Ar 0 ; A 2 r 0 ; :::; A i r 0 ], with r 0 = b Ax 0 .
Reference: [108] <author> Y. Saad and M. H. </author> <title> Schultz, </title> <journal> Math. of Comp. </journal> <volume> 44, </volume> <month> 417 </month> <year> (1985). </year>
Reference-contexts: In order to limit memory requirements (since all basis vectors must be stored), GMRES is restarted after each cycle of m iteration steps; this is called GMRES (m). A slightly simplified version of GMRES (m) with preconditioning K is as follows (for details, see <ref> [108] </ref>): Algorithm 9.4 GMRES (m) x 0 is an initial guess; r = b Ax 0 ; for j = 1; 2; :::: Solve for w in Kw = r; v 1 = w=kwk 2 ; Solve for w in Kw = Av i ; for k = 1; :::; i
Reference: [109] <author> H. F. </author> <title> Walker, </title> <journal> SIAM J. Sci. Stat. Comp. </journal> <volume> 9, </volume> <month> 152 </month> <year> (1988). </year>
Reference-contexts: Compute x m using the h k;i and v i ; r = b Ax m ; if residual r is small enough then quit else (x 0 := x m ;); end j; Another scheme for GMRES, based upon Householder orthogonalization instead of modified Gram-Schmidt, has been proposed in <ref> [109] </ref>. For some applications the additional computation required by Householder orthogonalization is compensated by improved numerical properties: the better orthogonality saves iteration steps. In [110] a variant of GMRES is proposed in which the preconditioner itself may be an iterative process, which may help to increase parallel efficiency.
Reference: [110] <author> H. A. van der Vorst and C. </author> <title> Vuik, </title> <type> Technical Report No. 91-80, </type> <institution> Delft University of Technology, Faculty of Tech. Math. </institution> <note> (unpublished). </note>
Reference-contexts: For some applications the additional computation required by Householder orthogonalization is compensated by improved numerical properties: the better orthogonality saves iteration steps. In <ref> [110] </ref> a variant of GMRES is proposed in which the preconditioner itself may be an iterative process, which may help to increase parallel efficiency. Similar to CG and other iterative schemes, the major computations are matrix-vector computations (with A and K), inner products and vector updates.
Reference: [111] <author> Z. Bai, D. Hu, and L. Reichel, </author> <type> Technical Report No. 91-03, </type> <institution> University of Kentucky (unpublished). </institution>
Reference-contexts: Newton polynomials are suggested in <ref> [111] </ref> and and Chebychev polynomials in [80]. After generating a suitable starting set, we still have to orthogonalize it. In [80] modified Gram-Schmidt is used while avoiding communication times that cannot be overlapped. We outline this approach, since it may be of value for other orthogonalization methods.
Reference: [112] <author> D. Calvetti, J. Petersen, and L. Reichel, </author> <type> Technical Report No. </type> <institution> ICM-9110-6, Institute for Computational Mathematics, Kent, OH (unpublished). </institution> <month> 62 </month>
Reference-contexts: For larger systems, the speedup increases to 150 (or more if more processors are involved) as expected. Calvetti et al. <ref> [112] </ref> report results for an implementation of m-step GMRES, using BLAS2 Householder orthogonalization, for a four-processor IBM 6000 distributed memory system.
Reference: [113] <author> W. W. </author> <title> Hager, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 5, </volume> <month> 311 </month> <year> (1984). </year>
Reference-contexts: Note that computing the condition number to within a factor of two is more than accurate enough for an error bound; indeed, an order-of-magnitude estimate is enough to say how many correct decimal places are in the answer. The algorithm for estimating kA 1 k 1 is described in <ref> [113, 114, 115] </ref>, and implemented in LAPACK. The routines which compute error bounds have the same names as above, but with an 'x' appended. For example, the routine that solves Ax = b for general, dense A and computes error bounds is called sgesvx.
Reference: [114] <author> N. J. Higham, </author> <note> SIAM Review 29, 575 (1987). </note>
Reference-contexts: Note that computing the condition number to within a factor of two is more than accurate enough for an error bound; indeed, an order-of-magnitude estimate is enough to say how many correct decimal places are in the answer. The algorithm for estimating kA 1 k 1 is described in <ref> [113, 114, 115] </ref>, and implemented in LAPACK. The routines which compute error bounds have the same names as above, but with an 'x' appended. For example, the routine that solves Ax = b for general, dense A and computes error bounds is called sgesvx.
Reference: [115] <author> N. J. </author> <title> Higham, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 11, </volume> <month> 804 </month> <year> (1990). </year>
Reference-contexts: Note that computing the condition number to within a factor of two is more than accurate enough for an error bound; indeed, an order-of-magnitude estimate is enough to say how many correct decimal places are in the answer. The algorithm for estimating kA 1 k 1 is described in <ref> [113, 114, 115] </ref>, and implemented in LAPACK. The routines which compute error bounds have the same names as above, but with an 'x' appended. For example, the routine that solves Ax = b for general, dense A and computes error bounds is called sgesvx.
References-found: 115

