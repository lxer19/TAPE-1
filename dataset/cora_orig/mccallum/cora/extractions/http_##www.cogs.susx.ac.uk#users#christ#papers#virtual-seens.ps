URL: http://www.cogs.susx.ac.uk/users/christ/papers/virtual-seens.ps
Refering-URL: http://www.cogs.susx.ac.uk/users/christ/index-noframes.html
Root-URL: 
Email: Email: Chris.Thornton@cogs.susx.ac.uk  
Phone: Tel: (44)1273 678856  
Title: Virtual Seens and the Frequently Used Dataset  
Author: Chris Thornton 
Date: March 28, 1997  
Web: WWW: http://www.cogs.susx.ac.uk  
Address: Brighton BN1 9QH  
Affiliation: Cognitive and Computing Sciences University of Sussex  
Abstract: The paper considers the situation in which a learner's testing set contains close approximations of cases which appear in the training set. Such cases can be considered `virtual seens' since they are approximately seen by the learner. Generalisation measures which do not take account of the frequency of virtual seens may be misleading. The paper shows that the 1-NN algorithm can be used to derive a normalising baseline for gen-eralisation statistics. The normalisation process is demonstrated though application to Holte's [1] study in which the generalisation performance of the 1R algorithm was tested against C4.5 on 16 commonly used datasets.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Holte, R. </author> <year> (1993). </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <booktitle> Machine learning, </booktitle> <pages> 3 (pp. 63-91). </pages>
Reference-contexts: The value obtained provides a measure of generalisation which discounts the possibilities for `lookup' of virtual seens. 2 Application of NN-normalisation to the Holte Study To get some insight into how useful the normalisation method might be in practice, it was applied to the results of Holte's 16-datasets study <ref> [1] </ref>.
Reference: [2] <author> Duda, R. and Hart, P. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: This criterion factors out the variability in both data distances and densities. It also permits us to detect virtual seens through application of a simple `nearest-neighbour' regime. In fact when we apply a 1-NN algorithm <ref> [2] </ref> to a training/testing set combination, the performance obtained is precisely the average frequency of virtual seens in the testing set.
Reference: [3] <author> Quinlan, J. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Holte showed that the performance of this learner was almost as good as that of C4.5 <ref> [3] </ref> even though it was restricted to the formation of simple hypotheses based on single attributes. He concluded that a `simplicity first' methodology is appropriate in machine learning.
Reference: [4] <author> Aha, D. and Kibler, D. </author> <year> (1989). </year> <title> Noise-tolerant instance-based learning algorithms. </title> <booktitle> Proceedings of the Eleventh Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 794-799). </pages> <publisher> Morgan Kaufmann. </publisher> <pages> 8 </pages>
Reference-contexts: In all cases the performance was superior to that of Holte's 1R algorithm. The measured performance of 1-NN algorithm in this study appears to be broadly compatible with its performance (or the performance of a K-NN variant) as reported in similar studies such as <ref> [4] </ref> and [5]. However, the performance obtained in this study is in general superior to that reported by Weiss and Kapouleas [6]. They recorded a mean generalisation level of 65.3 on the BC dataset whereas the figure obtained in the present study was 69.7. <p> The performance of the 1-NN method on LA and VO is also markedly better than that recorded by [7] and <ref> [4] </ref> for K-NN variants of the method.
Reference: [5] <author> Henery, R. </author> <year> (1994). </year> <title> Review of previous empirical comparisons. </title> <editor> In D. Michie, D. Speigelhalter and C. Taylor (Eds.), </editor> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Ellis Horwood. </publisher>
Reference-contexts: In all cases the performance was superior to that of Holte's 1R algorithm. The measured performance of 1-NN algorithm in this study appears to be broadly compatible with its performance (or the performance of a K-NN variant) as reported in similar studies such as [4] and <ref> [5] </ref>. However, the performance obtained in this study is in general superior to that reported by Weiss and Kapouleas [6]. They recorded a mean generalisation level of 65.3 on the BC dataset whereas the figure obtained in the present study was 69.7.
Reference: [6] <author> Weiss, S. and Kapouleas, I. </author> <year> (1989). </year> <title> An empiricial comparison of pattern recognition, neural nets and machine learning classification methods. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 781-787). </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, the performance obtained in this study is in general superior to that reported by Weiss and Kapouleas <ref> [6] </ref>. They recorded a mean generalisation level of 65.3 on the BC dataset whereas the figure obtained in the present study was 69.7. Similarly, they recorded a generalisation level of 95.3 on the HY dataset but the present figure is 96.9.
Reference: [7] <author> Bergadano, F., Kodratoff, Y. and Morik, K. </author> <year> (1992). </year> <title> Machine learning and knowledge acquisition: summary of reserach contributions presented at IJ-CAI'91. </title> <journal> AI Communications, </journal> <volume> 5, No. </volume> <pages> 1 (pp. 19-24). </pages>
Reference-contexts: On the other hand they recorded a higher level of performance on IR (96.0) although in this case they were employing a cross-validation method in addition to the basic algorithm. The performance of the 1-NN method on LA and VO is also markedly better than that recorded by <ref> [7] </ref> and [4] for K-NN variants of the method.
Reference: [8] <author> Friedman, J. </author> <year> (1994). </year> <title> Flexible Metric Nearest Neighbor Classification. </title> <journal> Unpublished MS. </journal> <volume> 9 </volume>
Reference-contexts: The implication is that with these datasets, training sets containing 2/3 of the original cases do not pose a substantive test of generalisation. This result is in agreement with Friedman's analysis <ref> [8] </ref> which explains the surprising robustness of NN methods against the so-called `curse of dimension 5 ality' in terms of the redundant distributional properties of common datasets. It is also in agreement with the general implications of Holte's study.
References-found: 8

