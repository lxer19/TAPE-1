URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-93-1145/CS-TR-93-1145.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-93-1145/
Root-URL: http://www.cs.wisc.edu
Title: Parallel Gradient Distribution in Unconstrained Optimization  
Author: O. L. Mangasarian 
Date: Revised February 1994  
Abstract: A parallel version is proposed for a fundamental theorem of serial unconstrained optimization. The parallel theorem allows each of k parallel processors to use simultaneously a different algorithm, such as a descent, Newton, quasi-Newton or a conjugate gradient algorithm. Each processor can perform one or many steps of a serial algorithm on a portion of the gradient of the objective function assigned to it, independently of the other processors. Eventually a synchronization step is performed which, for differentiable convex functions, consists of taking a strong convex combination of the k points found by the k processors. For nonconvex, as well as convex, differentiable functions, the best point found by the k processors is taken, or any better point. The fundamental result that we establish is that any accumulation point of the parallel algorithm is stationary for the nonconvex case, and is a global solution for the convex case. Computational testing on the Thinking Machines CM-5 multiprocessor indicate a speedup of the order of the number of processors employed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> K.P. Bennett and O.L. Mangasarian. </editor> <title> Serial and parallel multicategory dicrimination. </title> <institution> Computer Sciences Department Technical Report 1165, University of Wisconsin, Madison, Wis-consin, </institution> <month> July </month> <year> 1993. </year> <note> SIAM Journal on Optimization 4, 1994, to appear. </note>
Reference-contexts: However, this theorem is not as suggestive of an explicit computational scheme as the Partially Asynchronous Nonconvex PCAT1 of Corollary 3.2. In the concluding Section 4 we report briefly on computational experience with parallel gradient distribution algorithms on multicategory discrimination problems <ref> [1] </ref>, and on publicly available test problems [6] from the constrained and unconstrained testing environment CUTE [3]. Computations were carried out on the Thinking Machines CM-5 multiprocessor. Speedup efficiency depended on problem size and number of processors employed (2 to 32) and averaged between 129% and 20%. <p> Numerical implementations of parallel gradient distribution algorithms have been carried out in <ref> [1, 6] </ref> on the Thinking Machines CM-5 multiprocessor. In these implementations, inexact quasi-Newton minimization was used in each parallel processor so as to satisfy (16). Each processor was allowed to take a number of steps before synchronization. <p> Each processor was allowed to take a number of steps before synchronization. The synchronization consisted of searching the affine hull of the points generated by the parallel processors as well as the current point. The problems solved in <ref> [1] </ref> consisted of real world multicategory discrimination problems, formulated as unconstrained minimization of piecewise convex quadratic functions with Lipschitz continuous gradients. Problem size varied between 70 and 140 variables. For these multicategory discrimination problems, it is most efficient to use as many parallel processors as there are categories. <p> For the multicategory discrimination problems, speedup efficiency was between 50% and 91%. For more details see <ref> [1] </ref>. In [6], thirty unconstrained problems from the publicly available CUTE (Constrained and Unconstrained Testing Environment) [3] were tested. Among others, the parallel variable distribution algorithm version PVD0 was tested, which is equivalent to a parallel gradient distribution algorithm. Problems solved were between 100 and 1024 variables in size.
Reference: [2] <author> D.P. Bertsekas and J.N. Tsitsiklis. </author> <title> Parallel and Distributed Computation. </title> <publisher> Prentice-Hall, Inc, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1989. </year>
Reference-contexts: Other parallelization schemes are discussed extensively in <ref> [2] </ref>. We give now an outline of the paper. In Section 2 we establish two serial convergent algorithm Theorems 2.1 and 2.2 (SCAT1 and SCAT2) which cover many unconstrained direction-stepsize algorithms that are suitable for parallelization.
Reference: [3] <author> I. Bongartz, A.R. Conn, N. Gould, and Ph.L. Toint. Cute: </author> <title> Constrained and unconstrained testing environment. </title> <institution> Publications du Department de Mathematique Report 93/10, Facultes Universitaires De Namur, </institution> <year> 1993. </year>
Reference-contexts: In the concluding Section 4 we report briefly on computational experience with parallel gradient distribution algorithms on multicategory discrimination problems [1], and on publicly available test problems [6] from the constrained and unconstrained testing environment CUTE <ref> [3] </ref>. Computations were carried out on the Thinking Machines CM-5 multiprocessor. Speedup efficiency depended on problem size and number of processors employed (2 to 32) and averaged between 129% and 20%. We now briefly describe our notation. <p> For the multicategory discrimination problems, speedup efficiency was between 50% and 91%. For more details see [1]. In [6], thirty unconstrained problems from the publicly available CUTE (Constrained and Unconstrained Testing Environment) <ref> [3] </ref> were tested. Among others, the parallel variable distribution algorithm version PVD0 was tested, which is equivalent to a parallel gradient distribution algorithm. Problems solved were between 100 and 1024 variables in size.
Reference: [4] <author> J.W. Daniel. </author> <title> The approximate minimization of functionals. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1971. </year>
Reference-contexts: For nonconvex as well as convex f , the best point found by the k processors can be taken, or any other point with a lower value of f will work. The fundamental theorem we intend to parallelize is related to some classical forcing function theorems given in <ref> [7, 4, 10] </ref> that establish convergence for a wide class of algorithms. Such algorithms typically consist of a direction choice followed by a stepsize choice. The combined fl Computer Sciences Department, University of Wisconsin, 1210 West Dayton Street, Madison, WI 53706, email: olvi@cs.wisc.edu. <p> The theorem is related to some classical forcing function theorems given in <ref> [7, 4] </ref> that establish convergence for a wide class of algorithms that consist of a direction choice followed by a stepsize choice. The decrease in the objective function forces the satisfaction of an optimality condition, namely the vanishing of the gradient.
Reference: [5] <author> J.E. Dennis and R.B. Schnabel. </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1983. </year>
Reference-contexts: fjrf (x i + d i ) T d i = 0g; f 2 LC 1 K (R n ) (iii) Interval stepsize 0 &lt; " 1 5 i 5 K K (R n ) for some " 1 &gt; 0; " 2 &gt; 0 and &gt; 0 (iv) Armijo <ref> [5, pp.118-119] </ref> i = max f i ; 2 ; : : :g such that f (x i ) f (x i + i d i ) = ffi i rf (x i ) T d i f or some ffi 2 (0; 1); and i = rf (x i )
Reference: [6] <author> M.C. Ferris and O.L. Mangasarian. </author> <title> Parallel variable distribution. </title> <institution> Computer Sciences Department Technical Report 1175, University of Wisconsin, Madison, Wisconsin, </institution> <month> August </month> <year> 1993. </year> <note> SIAM Journal on Optimization 4, 1994, to appear. </note>
Reference-contexts: However, this theorem is not as suggestive of an explicit computational scheme as the Partially Asynchronous Nonconvex PCAT1 of Corollary 3.2. In the concluding Section 4 we report briefly on computational experience with parallel gradient distribution algorithms on multicategory discrimination problems [1], and on publicly available test problems <ref> [6] </ref> from the constrained and unconstrained testing environment CUTE [3]. Computations were carried out on the Thinking Machines CM-5 multiprocessor. Speedup efficiency depended on problem size and number of processors employed (2 to 32) and averaged between 129% and 20%. We now briefly describe our notation. <p> Numerical implementations of parallel gradient distribution algorithms have been carried out in <ref> [1, 6] </ref> on the Thinking Machines CM-5 multiprocessor. In these implementations, inexact quasi-Newton minimization was used in each parallel processor so as to satisfy (16). Each processor was allowed to take a number of steps before synchronization. <p> For the multicategory discrimination problems, speedup efficiency was between 50% and 91%. For more details see [1]. In <ref> [6] </ref>, thirty unconstrained problems from the publicly available CUTE (Constrained and Unconstrained Testing Environment) [3] were tested. Among others, the parallel variable distribution algorithm version PVD0 was tested, which is equivalent to a parallel gradient distribution algorithm. Problems solved were between 100 and 1024 variables in size.
Reference: [7] <author> E.S. Levitin and B. T. Polyak. </author> <title> Constrained minimization methods. </title> <journal> Computational Mathematics and Mathematical Physics, </journal> <volume> 6 </volume> <pages> 1-50, </pages> <year> 1968. </year> <note> Translated from Russian. </note>
Reference-contexts: For nonconvex as well as convex f , the best point found by the k processors can be taken, or any other point with a lower value of f will work. The fundamental theorem we intend to parallelize is related to some classical forcing function theorems given in <ref> [7, 4, 10] </ref> that establish convergence for a wide class of algorithms. Such algorithms typically consist of a direction choice followed by a stepsize choice. The combined fl Computer Sciences Department, University of Wisconsin, 1210 West Dayton Street, Madison, WI 53706, email: olvi@cs.wisc.edu. <p> The theorem is related to some classical forcing function theorems given in <ref> [7, 4] </ref> that establish convergence for a wide class of algorithms that consist of a direction choice followed by a stepsize choice. The decrease in the objective function forces the satisfaction of an optimality condition, namely the vanishing of the gradient.
Reference: [8] <author> Z.-Q. Luo and P. Tseng. </author> <title> On the convergence of the coordinate descent method for convex differentiable minimization. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 72 </volume> <pages> 7-35, </pages> <year> 1992. </year>
Reference-contexts: sequentially minimized with respect to certain variables, include the serial algorithm proposed by Warga [15] for a strictly convex function in each block of variables and in which the function is sequentially minimized for each block of variables, and the coordinate descent methods of Tseng [14] and Luo and Tseng <ref> [8] </ref>. Other parallelization schemes are discussed extensively in [2]. We give now an outline of the paper. In Section 2 we establish two serial convergent algorithm Theorems 2.1 and 2.2 (SCAT1 and SCAT2) which cover many unconstrained direction-stepsize algorithms that are suitable for parallelization.
Reference: [9] <author> J.M. Ortega and W.C. Rheinboldt. </author> <title> Iterative Solution of Nonlinear Equations in Several Variables. </title> <publisher> Academic Press, </publisher> <year> 1970. </year>
Reference-contexts: The decrease in the objective function forces the satisfaction of an optimality condition, namely the vanishing of the gradient. Before stating and proving SCAT1 we adapt the definition of a forcing function <ref> [9, p.479] </ref> for our purposes.
Reference: [10] <author> E. Polak. </author> <title> Computational methods in optimization; A unified approach. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: For nonconvex as well as convex f , the best point found by the k processors can be taken, or any other point with a lower value of f will work. The fundamental theorem we intend to parallelize is related to some classical forcing function theorems given in <ref> [7, 4, 10] </ref> that establish convergence for a wide class of algorithms. Such algorithms typically consist of a direction choice followed by a stepsize choice. The combined fl Computer Sciences Department, University of Wisconsin, 1210 West Dayton Street, Madison, WI 53706, email: olvi@cs.wisc.edu. <p> We note that the conjugate direction conditions of (iii) are satisfied by the Polyak-Polak-Ribiere <ref> [12, 11, 13, 10] </ref> coefficient ff i := krf (x i1 )k 2 (6) for f 2 C 2 (R n ) and such that fijjzk 2 z T r 2 f (x)z = ffkzk 2 8z 2 R n for some fi = ff &gt; 0: (7) We also note
Reference: [11] <author> E. Polak and G. Ribiere. </author> <title> Note sur la convergence de methodes de directions conjugees. </title> <institution> Revue Francaise Informatique et Recherche Operationelle, 16-R1:35-43, </institution> <year> 1969. </year>
Reference-contexts: We note that the conjugate direction conditions of (iii) are satisfied by the Polyak-Polak-Ribiere <ref> [12, 11, 13, 10] </ref> coefficient ff i := krf (x i1 )k 2 (6) for f 2 C 2 (R n ) and such that fijjzk 2 z T r 2 f (x)z = ffkzk 2 8z 2 R n for some fi = ff &gt; 0: (7) We also note
Reference: [12] <author> B.T. Polyak. </author> <title> The conjugate gradient method in extremal problems. </title> <journal> USSR Computational Mathematics and Mathematical Physics, </journal> <volume> 9(4) </volume> <pages> 94-112, </pages> <year> 1969. </year> <note> Translated from Russian. </note>
Reference-contexts: We note that the conjugate direction conditions of (iii) are satisfied by the Polyak-Polak-Ribiere <ref> [12, 11, 13, 10] </ref> coefficient ff i := krf (x i1 )k 2 (6) for f 2 C 2 (R n ) and such that fijjzk 2 z T r 2 f (x)z = ffkzk 2 8z 2 R n for some fi = ff &gt; 0: (7) We also note
Reference: [13] <author> B.T. Polyak. </author> <title> Introduction to Optimization. Optimization Software, </title> <publisher> Inc., Publications Division, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: We note that the conjugate direction conditions of (iii) are satisfied by the Polyak-Polak-Ribiere <ref> [12, 11, 13, 10] </ref> coefficient ff i := krf (x i1 )k 2 (6) for f 2 C 2 (R n ) and such that fijjzk 2 z T r 2 f (x)z = ffkzk 2 8z 2 R n for some fi = ff &gt; 0: (7) We also note
Reference: [14] <author> P. Tseng. </author> <title> Dual ascent methods with strictly convex costs and linear constraints: A unified approach. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 28 </volume> <pages> 214-242, </pages> <year> 1990. </year>
Reference-contexts: wherein the objective function is sequentially minimized with respect to certain variables, include the serial algorithm proposed by Warga [15] for a strictly convex function in each block of variables and in which the function is sequentially minimized for each block of variables, and the coordinate descent methods of Tseng <ref> [14] </ref> and Luo and Tseng [8]. Other parallelization schemes are discussed extensively in [2]. We give now an outline of the paper. In Section 2 we establish two serial convergent algorithm Theorems 2.1 and 2.2 (SCAT1 and SCAT2) which cover many unconstrained direction-stepsize algorithms that are suitable for parallelization.
Reference: [15] <author> J. Warga. </author> <title> Minimizing certain convex functions. </title> <journal> Journal of SIAM on Applied Mathematics, </journal> <volume> 11 </volume> <pages> 588-593, </pages> <year> 1963. </year>
Reference-contexts: Stepsize choices along the chosen direction include minimization, finding the first stationary point, interval stepsize, the Armijo stepsize and others. Related algorithms, wherein the objective function is sequentially minimized with respect to certain variables, include the serial algorithm proposed by Warga <ref> [15] </ref> for a strictly convex function in each block of variables and in which the function is sequentially minimized for each block of variables, and the coordinate descent methods of Tseng [14] and Luo and Tseng [8]. Other parallelization schemes are discussed extensively in [2].
References-found: 15

