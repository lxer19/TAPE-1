URL: http://www.icsi.berkeley.edu/ftp/global/pub/techreports/1994/tr-94-003.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/ftp/global/pub/techreports/1994/
Root-URL: http://www.icsi.berkeley.edu
Title: Best-first Model Merging for Hidden Markov Model Induction  
Author: Andreas Stolcke Stephen M. Omohundro 
Address: I 1947 Center St. Suite 600 Berkeley, California 94704-1198  1947 Center Street, Berkeley, CA 94704,  1947 Center Street, Berkeley, CA 94704,  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  Computer Science Division, University of California at Berkeley, and International Computer Science Institute,  International Computer Science Institute,  
Pubnum: TR-94-003  
Email: e-mail stolcke@icsi.berkeley.edu.  e-mail om@icsi.berkeley.edu.  
Phone: (510) 643-9153 FAX (510) 643-7684  
Date: January 1994 Revised April 1994  
Abstract: This report describes a new technique for inducing the structure of Hidden Markov Models from data which is based on the general `model merging' strategy (Omohundro 1992). The process begins with a maximum likelihood HMM that directly encodes the training data. Successively more general models are produced by merging HMM states. A Bayesian posterior probability criterion is used to determine which states to merge and when to stop generalizing. The procedure may be considered a heuristic search for the HMM structure with the highest posterior probability. We discuss a variety of possible priors for HMMs, as well as a number of approximations which improve the computational efficiency of the algorithm. We studied three applications to evaluate the procedure. The first compares the merging algorithm with the standard Baum-Welch approach in inducing simple finite-state languages from small, positive-only training samples. We found that the merging procedure is more robust and accurate, particularly with a small amount of training data. The second application uses labelled speech data from the TIMIT database to build compact, multiple-pronunciation word models that can be used in speech recognition. Finally, we describe how the algorithm was incorporated in an operational speech understanding system, where it is combined with neural network acoustic likelihood estimators to improve performance over single-pronunciation word models. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Angluin, D., & C. H. Smith. </author> <year> 1983. </year> <title> Inductive inference: Theory and methods. </title> <journal> ACM Computing Surveys 15.237-269. </journal>
Reference-contexts: At the most basic level we have the concept of state merging, which is implicit in the notion of state equivalence classes, and as such is pervasively used in much of automata theory (Hopcroft & Ullman 1979). It has also been applied to the induction of non-probabilistic automata <ref> (Angluin & Smith 1983) </ref>. Still in the field of non-probabilistic automata induction, Tomita (1982) has used a simple hill-climbing procedure combined with a goodness measure based on positive/negative samples to search the space of possible models.
Reference: <author> Baldi, Pierre, Yves Chauvin, Tim Hunkapiller, & Marcella A. McClure. </author> <year> 1993. </year> <title> Hidden Markov models in molecular biology: New algorithms and applications. </title> <booktitle> In Advances in Neural Information Processing Systems 5 , ed. </booktitle> <editor> by Stephen Jose Hanson, Jack D. Cowan, & C. Lee Giles, </editor> <address> 747-754. San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Baum, Leonard E., Ted Petrie, George Soules, & Norman Weiss. </author> <year> 1970. </year> <title> A maximization technique occuring in the statistical analysis of probabilistic functions in Markov chains. </title> <journal> The Annals of Mathematical Statistics 41.164-171. </journal>
Reference-contexts: Section 2 defines the HMM formalism and gives an overview of these standard estimation methods. In contrast to traditional HMM estimation based on the Baum-Welch technique <ref> (Baum et al. 1970) </ref>, our method uses Bayesian evidence evaluation to adjust the model topology to the data. The approach is based on the idea that models should evolve from simply storing examples to representing more complex and general relationships as increasing amounts of data become available. <p> M is computed as the sum of the probabilities of all paths that generate x: P (xjM ) = q 1 :::q ` 2Q ` 1 Where possible we try to keep the notation consistent with Bourlard & Morgan (1993). 3 2.4 HMM estimation The Baum-Welch estimation method for HMMs <ref> (Baum et al. 1970) </ref> assumes a certain topology and adjusts the parameters so as to maximize the model likelihood on the given samples.
Reference: <author> Bourlard, Herv e, & Nelson Morgan. </author> <year> 1993. </year> <title> Connectionist Speech Recognition. A Hybrid Approach. </title> <address> Boston, Mass.: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Brown, Peter F., Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, & Robert L. Mercer. </author> <year> 1992. </year> <title> Class-based n-gram models of natural language. </title> <note> Computational Linguistics 18.467-479. </note>
Reference-contexts: A class-based n-gram grammar is easily represented as an HMM, with one state per class. Transition probabilities represent the conditional probabilities between classes, whereas emission probabilities correspond to the word distributions for each class (for n &gt; 2, higher-order HMMs are required). The incremental word clustering algorithm given in <ref> (Brown et al. 1992) </ref> then becomes an instance of HMM merging, albeit one that is entirely based on likelihoods. 8 6 Evaluation We have evaluated the HMM merging algorithm experimentally in a series of applications.
Reference: <author> Buntine, W. L. </author> <year> 1991. </year> <title> Theory refinement of Bayesian networks. </title> <booktitle> In Seventh Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> Anaheim, CA. </address>
Reference: <author> Buntine, Wray. </author> <year> 1992. </year> <title> Learning classification trees. </title> <booktitle> In Artificial Intelligence Frontiers in Statistics: AI and Statistics III , ed. </booktitle> <editor> by D. J. Hand. </editor> <publisher> Chapman & Hall. </publisher>
Reference-contexts: HMMs are a special kind of parameterized graph structure. Unsurprisingly, many aspects of the priors discussed in this section can be found in Bayesian approaches to the induction of graph-based models in other domains (e.g., Bayesian networks (Cooper & Herskovits 1992; Buntine 1991) and decision trees <ref> (Buntine 1992) </ref>). 3.4.1 Structural vs. parameter priors An HMM can be described in two stages: 1. A model structure or topology is specified as a set of states, transitions and emissions.
Reference: <author> Cheeseman, Peter, James Kelly, Matthew Self, John Stutz, Will Taylor, & Don Freeman. </author> <year> 1988. </year> <title> AutoClass: A Bayesian classification system. </title> <booktitle> In Proceedings of the 5th International Conference on Machine Learning, </booktitle> <pages> 54-64, </pages> <institution> University of Michigan, Ann Arbor, Mich. </institution>
Reference-contexts: For example, HMMs with mixtures of Gaussians as emission densities are being used extensively (Gauvain & Lee 1991) for speech modeling. Our merging algorithm becomes applicable to such models provided that one has a prior for such densities, which should be straightforward <ref> (Cheeseman et al. 1988) </ref>. Efficient implementation of the merging operator may be a bigger problem|one wants to avoid having to explicitly compute a merged density for each merge under consideration.
Reference: <author> Chen, Francine R. </author> <year> 1990. </year> <title> Identification of contextual factors for pronunciation networks. </title> <booktitle> In Proceedings IEEE Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume 2, </volume> <pages> 753-756, </pages> <address> Albuquerque, NM. Cleeremans, Axel, </address> <year> 1991. </year> <title> Mechanisms of Implicit Learning. A Parallel Distributed Processing Model of Sequence Acquisition. </title> <institution> Pittsburgh, Pa.: Department of Psychology, Carnegie Mellon University dissertation. </institution>
Reference: <author> Cooper, Gregory F., & Edward Herskovits. </author> <year> 1992. </year> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <booktitle> Machine Learning 9.309-347. </booktitle>
Reference: <author> Cover, Thomas M., & Joy A. Thomas. </author> <year> 1991. </year> <title> Elements of Information Theory. </title> <address> New York: </address> <publisher> John Wiley and Sons, Inc. </publisher>
Reference: <author> Cutting, Doug, Julian Kupiec, Jan Pedersen, & Penelope Sibun. </author> <year> 1992. </year> <title> A practical part-of-speech tagger. </title> <booktitle> In Proceedings of the Third Conference on Applied Natural Language Processing, </booktitle> <address> Trento, Italy. </address> <note> ACL. Also available as Xerox PARC Technical Report SSL-92-01. 55 Dempster, </note> <author> A. P., N. M. Laird, & D. B. Rubin. </author> <year> 1977. </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B 34.1-38. </journal>
Reference-contexts: Some of their first uses were in the area of cryptanalysis and they are now the model of choice for speech recognition (Rabiner & Juang 1986). Recent applications include part-of-speech tagging <ref> (Cutting et al. 1992) </ref> and protein classification and alignment (Haussler et al. 1992; Baldi et al. 1993). Because HMMs can be seen as probabilistic generalizations of non-deterministic finite-state automata they are also of interest from the point of view of formal language induction.
Reference: <author> Garofolo, J. S., </author> <year> 1988. </year> <title> Getting Started with the DARPA TIMIT CD-ROM: an Acoustic Phonetic Continuous Speech Database. </title> <institution> National Institute of Standards and Technology (NIST), Gaithersburgh, Maryland. </institution>
Reference-contexts: The TIMIT (Texas Instruments-MIT) database is a collection of hand-labeled speech samples compiled for the purpose of training speaker-independent phonetic recognition systems <ref> (Garofolo 1988) </ref>. It contains acoustic data segmented by words and aligned with discrete labels from an alphabet of 62 phones. For our purposes, we ignored the continuous, acoustic data and viewed the database simply as a collection of string samples over a discrete alphabet.
Reference: <author> Gauvain, Jean-Luc, & Chin-Hin Lee. </author> <year> 1991. </year> <title> Bayesian learning of Gaussian mixture densities for hidden Markov models. </title> <booktitle> In Proceedings DARPA Speech and Natural Language Processing Workshop, </booktitle> <pages> 271-277. </pages> <address> Pacific Grove, CA: </address> <institution> Defence Advanced Research Projects Agency, Information Science and Technology Office. </institution>
Reference-contexts: This could, and in fact should, change with the use of more informative priors. Likewise, we haven't pursued merging of HMMs with non-discrete outputs. For example, HMMs with mixtures of Gaussians as emission densities are being used extensively <ref> (Gauvain & Lee 1991) </ref> for speech modeling. Our merging algorithm becomes applicable to such models provided that one has a prior for such densities, which should be straightforward (Cheeseman et al. 1988).
Reference: <author> Gull, S. F. </author> <year> 1988. </year> <title> Bayesian inductive inference and maximum entropy. In Maximum Entropy and Bayesian Methods in Science and Engineering, Volume 1: Foundations, </title> <editor> ed. by G. J. Erickson & C. R. Smith, </editor> <address> 53-74. Dordrecht: </address> <publisher> Kluwer. </publisher>
Reference-contexts: However, as we will see below, the state-based priors by themselves produce a tendency towards reducing the number of states as a result of Bayesian `Occam factors' <ref> (Gull 1988) </ref>. In the case of narrow parameter priors we need to specify how the prior probability mass is distributed among all possible model topologies with a given number of states. <p> This result, of course, just confirms our intuition that a prophet 18 whose predictions are specific (and true) is more credible than one whose predictions are more general. The ratio between the allowable range of a model's parameters a posterior and a priori is known as the Occam factor <ref> (Gull 1988) </ref>. In the discrete case these ranges are just the respective numbers of possible parameter settings: 1 3 versus 1 2 in the example.
Reference: <author> Haussler, David, Anders Krogh, I. Saira Mian, & Kimmen Sj olander. </author> <year> 1992. </year> <title> Protein modeling using hidden Markov models: Analysis of globins. </title> <type> Technical Report UCSC-CRL-92-23, </type> <institution> Computer and Information Sciences, University of California, Santa Cruz, Ca. </institution> <note> Revised Sept. </note> <year> 1992. </year>
Reference: <author> Hopcroft, John E., & Jeffrey D. Ullman. </author> <year> 1979. </year> <title> Introduction to Automata Theory, </title> <booktitle> Languages, and Computation. </booktitle> <address> Reading, Mass.: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: At the most basic level we have the concept of state merging, which is implicit in the notion of state equivalence classes, and as such is pervasively used in much of automata theory <ref> (Hopcroft & Ullman 1979) </ref>. It has also been applied to the induction of non-probabilistic automata (Angluin & Smith 1983).
Reference: <author> Horning, James Jay. </author> <year> 1969. </year> <title> A study of grammatical inference. </title> <type> Technical Report CS 139, </type> <institution> Computer Science Department, Stanford University, Stanford, </institution> <address> Ca. </address>
Reference: <author> Jelinek, Frederick, & Robert L. Mercer. </author> <year> 1980. </year> <title> Interpolated estimation of Markov source parameters from sparse data. </title> <booktitle> In Proceedings Workshop on Pattern Recognition in Practice, </booktitle> <pages> 381-397, </pages> <address> Amsterdam. </address>
Reference-contexts: This holding-out of training data makes the mixture model approach similar to the deleted interpolation method <ref> (Jelinek & Mercer 1980) </ref>.
Reference: <author> Jurafsky, Daniel, Chuck Wooters, Gary Tajchman, Jonathan Segal, Andreas Stolcke, & Nelson Morgan. </author> <year> 1994. </year> <title> The Berkeley Restaurant Project. </title> <type> Technical report, </type> <institution> International Computer Science Institute, Berkeley, CA. </institution> <note> To appear. </note>
Reference-contexts: BeRP is medium vocabulary, speaker-independent spontaneous continuous speech understanding system that functions as a consultant for finding restaurants in the city of Berkeley, California <ref> (Jurafsky et al. 1994) </ref>. In this application, the merging algorithm is run on strings of phone labels obtained by Viterbi-aligning previously existing word models to sample speech (using the TIMIT labels as the phone alphabet).
Reference: <author> Katz, Slava M. </author> <year> 1987. </year> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing 35.400-401. </journal>
Reference-contexts: Back-off models (where a second model is consulted if, and only if, the first one returns probability zero) do not yield consistent probabilities unless they are combined with `discounting' of probabilities to ensure that the total probability mass sums up to unity <ref> (Katz 1987) </ref>. The discounting scheme, as well as various smoothing approaches (e.g., adding a fixed number of virtual `Dirichlet' samples into parameter estimates) tend to be specific to the model used, and are therefore inherently problematic when comparing different model-building methods.
Reference: <author> Lari, K., & S. J. Young. </author> <year> 1990. </year> <title> The estimation of stochastic context-free grammars using the Inside-Outside algorithm. </title> <booktitle> Computer Speech and Language 4.35-56. </booktitle>
Reference: <author> Omohundro, Stephen M. </author> <year> 1992. </year> <title> Best-first model merging for dynamic learning and recognition. </title> <booktitle> In Advances in Neural Information Processing Systems 4 , ed. </booktitle> <publisher> by John E. </publisher>
Reference-contexts: approximation here because it also turns out to be very useful in an efficient implementation of the HMM induction algorithm described in later sections. 3 HMM Induction by Bayesian Model Merging 3.1 Model merging The approach to HMM induction presented here was motivated by earlier work by one of us <ref> (Omohundro 1992) </ref> on model merging as a fundamental, cognitively plausible induction technique that is applicable to a variety of domains. The basic idea is that a model of a domain is constructed from submodels.
Reference: <editor> Moody, Steve J. Hanson, & Richard P. Lippman, </editor> <address> 958-965. San Mateo, CA: </address> <publisher> Morgan Kaufmann. 56 Pereira, </publisher> <editor> Fernando, & Yves Schabes. </editor> <year> 1992. </year> <title> Inside-outside reestimation from partially bracketed corpora. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 128-135, </pages> <institution> University of Delaware, Newark, Delaware. </institution>
Reference: <author> Porat, Sara, & Jerome A. Feldman. </author> <year> 1991. </year> <title> Learning automata from ordered examples. </title> <booktitle> Machine Learning 7.109-138. </booktitle>
Reference: <author> Quinlan, J. Ross, & Ronald L. Rivest. </author> <year> 1989. </year> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation 80.227-248. </journal>
Reference: <author> Rabiner, L. R., & B. H. Juang. </author> <year> 1986. </year> <title> An introduction to hidden Markov models. </title> <journal> IEEE ASSP Magazine 3.4-16. </journal>
Reference-contexts: 1 Introduction and Overview Hidden Markov Models (HMMs) are a popular method for modeling stochastic sequences with an underlying finite-state structure. Some of their first uses were in the area of cryptanalysis and they are now the model of choice for speech recognition <ref> (Rabiner & Juang 1986) </ref>. Recent applications include part-of-speech tagging (Cutting et al. 1992) and protein classification and alignment (Haussler et al. 1992; Baldi et al. 1993).
Reference: <author> Reber, A. S. </author> <year> 1969. </year> <title> Implicit learning of artifical grammars. </title> <journal> Journal of Verbal Learning and Verbal Behavior 6.855-863. </journal>
Reference: <author> Redner, Richard A., & Homer F. Walker. </author> <year> 1984. </year> <title> Mixture densities, maximum likelihood and the EM algorithm. </title> <journal> SIAM Review 26.195-239. </journal>
Reference-contexts: When comparing two model induction methods, we first let each induce a structure. Each is built into a mixture model, and both the component model parameters and the mixture proportions are estimated using a variant of the EM procedure for generic mixture distributions <ref> (Redner & Walker 1984) </ref>. To get meaningful estimates for the mixture proportions, the HMM structure should be induced based on a subset of the training data, and the full training data is then used to estimate the parameters, including the mixture weights.
Reference: <author> Riley, Michael D. </author> <year> 1991. </year> <title> A statistical model for generating pronunciation networks. </title> <booktitle> In Proceedings IEEE Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume 2, </volume> <pages> 737-740, </pages> <address> Toronto. </address>
Reference: <author> Rissanen, Jorma. </author> <year> 1983. </year> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> The Annals of Statistics 11.416-431. </journal>
Reference: <author> Ron, Dana, Yoram Singer, & Naftali Tishby. </author> <year> 1994. </year> <title> The power of amnesia. </title> <booktitle> In Advances in Neural Information Processing Systems 6 , ed. </booktitle> <editor> by Jack Cowan, Gerald Tesauro, & Joshua Alspector. </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Stolcke, Andreas, & Stephen Omohundro. </author> <year> 1993. </year> <title> Hidden Markov model induction by Bayesian model merging. </title> <booktitle> In Advances in Neural Information Processing Systems 5 , ed. </booktitle> <editor> by Stephen Jose Hanson, Jack D. Cowan, & C. Lee Giles, </editor> <address> 11-18. San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Thomason, Michael G., & Erik Granum. </author> <year> 1986. </year> <title> Dynamic programming inference of Markov networks from finite set of sample strings. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 8.491-501. </journal>
Reference: <author> Tomita, Masaru. </author> <year> 1982. </year> <title> Dynamic construction of finite automata from examples using hill-climbing. </title> <booktitle> In Proceedings of the 4th Annual Conference of the Cognitive Science Society, </booktitle> <pages> 105-108, </pages> <address> Ann Arbor, Mich. </address>
Reference: <author> Viterbi, A. </author> <year> 1967. </year> <title> Error bounds for convolutional codes and an asymptotically optimum decodning algorithm. </title> <journal> IEEE Transactions on Information Theory 260-269. </journal>
Reference: <author> Wallace, C. S., & P. R. Freeman. </author> <year> 1987. </year> <title> Estimation and inference by compact coding. </title> <journal> Journal of the Royal Statistical Society, Series B 49.240-265. </journal>
Reference: <author> Wooters, Charles C., </author> <year> 1993. </year> <title> Lexical Modeling in a Speaker Independent Speech Understanding System. </title> <institution> Berkeley, CA: University of California dissertation. </institution> <month> 57 </month>
Reference-contexts: For the BeRP system, HMM merging made it possible and practical to use multiple pronunciation word models, whereas before it was confined to single pronunciation models. (Note that in this setting, even a very restricted HMM can produce any acoustic emission 51 derstanding system <ref> (Wooters 1993) </ref>. 52 with non-zero probability, due to the continuous nature of the domain, and because the emission distribution represented by the MLP is inherently non-vanishing.) To assess its effectiveness, the recognition performance of the multiple-pronunciation system was compared against that of an otherwise identical system in which only one phone
References-found: 38

