URL: ftp://ftp.cse.ucsc.edu/pub/tr/ucsc-crl-95-23.ps.Z
Refering-URL: ftp://ftp.cse.ucsc.edu/pub/tr/README.html
Root-URL: http://www.cse.ucsc.edu
Title: A Massively Parallel Architecture for Associative-Based Artificial Intelligence  
Author: James D. Roberts James D. Roberts is Richard Hughey Robert Levinson Charles McDowell Dean 
Degree: A dissertation submitted in partial satisfaction of the requirements for the degree of Doctor of Philosophy in Computer Engineering by  approved:  
Note: The dissertation of  
Date: MISC:  June 1995  
Address: Santa Cruz  
Affiliation: University of California  of Graduate Studies and Research  
Abstract-found: 0
Intro-found: 1
Reference: [A + 87] <editor> Judy M. Anderson et al. </editor> <booktitle> The architecture of FAIM-1. Computer, </booktitle> <pages> pages 55-65, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: Estimates showed a 16 K PE NON-VON to be only 10 times faster than earlier database machines [Sha88, AG89]. FAIM FAIM (1987) supports object-oriented, logic, and procedural programming (LISP and Prolog) by attaching a small amount of CAM to RISC processors <ref> [A + 87] </ref>. Each CAM has only a small number of data objects per match circuit and provides only masked-comparison. The CAM is used to match the heads of logic clauses, providing single-level indexing for clause selection and subsequent unification. <p> hypercube 64 K 4,000 MIPS AMT DAP [TW91] 1987 SIMD, 2-d mesh, orth. bus 1 K 800 MFLOPS GAPP [NCR87] 1987 SIMD, 1,2-d mesh 2 K 900 MOPS DADO [AG89] 1987 MIMD, tree 8 K PASM [MSS85] 1987 MSIMD, bus, xbar 1 K TRAC [AG89] 1987 MSIMD, banyan 4 FAIM <ref> [A + 87] </ref> 1987 MSIMD, 2-d mesh 1 K+ MasPar MP-1 [Nic90] 1989 SIMD, 2-d mesh, MIN 16 K 26,000 MIPS nCUBE2 [H + 86] 1989 MIMD, hypercube 8 K 61,440 MIPS April [A + 90] 1990 MIMD, 2,3-d mesh, dataflow 1000+ BLITZEN [B + 90a] 1990 SIMD, bit serial, 2-d <p> SEGUL includes message passing as means of activating, synchronizing, and communicating data between parallel processes. 4 This has already been proven useful for coarse-grain user-defined parallelism in knowledge processing programs <ref> [A + 87] </ref>. The syntax and methodology is similar to that used in Linda, and is based on the associative tuple matching. <p> Others have shown that 2-4 execution threads are usually sufficient to keep a processor busy in a multiprocessing environment [A + 90]. Multi-threading, in conjunction with a superscalar microprocessor would address medium-grain parallelism. Kogge and others <ref> [Kog91, Kog82, Ode91, PJK89, A + 87] </ref> have shown significant performance improvements from stack architectures for production systems.
Reference: [A + 90] <author> A. Agarwal et al. </author> <month> April: </month> <title> A processor architecture for multiprocessing. </title> <booktitle> In 17th Int. Sym. on Computer Architectures, </booktitle> <pages> pages 104-114, </pages> <year> 1990. </year>
Reference-contexts: 8 K PASM [MSS85] 1987 MSIMD, bus, xbar 1 K TRAC [AG89] 1987 MSIMD, banyan 4 FAIM [A + 87] 1987 MSIMD, 2-d mesh 1 K+ MasPar MP-1 [Nic90] 1989 SIMD, 2-d mesh, MIN 16 K 26,000 MIPS nCUBE2 [H + 86] 1989 MIMD, hypercube 8 K 61,440 MIPS April <ref> [A + 90] </ref> 1990 MIMD, 2,3-d mesh, dataflow 1000+ BLITZEN [B + 90a] 1990 SIMD, bit serial, 2-d mesh 16 K 3,000 MIPS iWarp [B + 90b] 1990 MIMD, 2-d mesh 64 1,300 MFLOPS BSYS [Hug91] 1990 systolic, 1-d mesh 470 108 MOPS NAP [KF90] 1990 SIMD, various simulated SNAP [M <p> Mennemeier [Men91] details an external co-processor for performing rapid task switching and state saving for conventional microprocessors. Others have shown that 2-4 execution threads are usually sufficient to keep a processor busy in a multiprocessing environment <ref> [A + 90] </ref>. Multi-threading, in conjunction with a superscalar microprocessor would address medium-grain parallelism. Kogge and others [Kog91, Kog82, Ode91, PJK89, A + 87] have shown significant performance improvements from stack architectures for production systems.
Reference: [A + 93a] <author> Anant Agarwal et al. Sparcle: </author> <title> An evolutionary processor design for large-scale multiprocessors. </title> <journal> IEEE Micro, </journal> <volume> 13(3) </volume> <pages> 48-61, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: These are high-performance off-the-shelf microprocessors to take advantage of large commercial design efforts and the economies of high-volume production and existing assembler, compiler, and operating system software. Although there are several excellent experimental research projects designing microprocessors specifically for parallel processing and AI applications (including <ref> [A + 93a, L + 89] </ref>), it is doubtful that their special features will provide speeds significantly greater than those of lower-cost commercial processors. 147 The simulations described in Appendix C show that a third of the execution time for SDM and semantic networks is executed by the supervisor, even with
Reference: [A + 93b] <author> Robert Allen et al. </author> <title> A parallel algorithm for graph matching and its MasPar implementation. </title> <booktitle> In Computer Architectures for Machine Perception, </booktitle> <pages> pages 13-18. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year>
Reference-contexts: The model uses nested collections, such as sets of tuples, to represent relational structures. Allen et al. define a relational description of an object as "a set of `primitives' each having its own attribute description and a set of named relations" <ref> [A + 93b] </ref>, which coincides with the definition of a labeled graph. The MISC associative model generalizes relational descriptions from graphs to nested hypergraphs.
Reference: [AAA93] <author> AAAI. </author> <title> Spring symposium series: Innovative applications of massive parallelism. </title> <institution> Stanford University, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: On the other hand, there are numerous effective codes developed specifically for massively parallel machines including neural networks and natural language processing <ref> [AAA93] </ref>. Fine-grain (massive) parallelism is highly encouraging but there has been limited work to date. Knowledge processing has several characteristics that make efficient parallelization difficult on current general-purpose machines. Instead of the statically-sized arrays of numeric operations, knowledge processing programs emphasize dynamic, variable length, and linked data structures. <p> Researchers and industry have developed a large number of algorithms and applications for the CM-2 and MasPar machines. Table 8.2 presents a small sampling taken from a single conference <ref> [AAA93] </ref>. This prior work is important in establishing state-space backtracking search and load balancing on SIMD architectures for AI applications. Backtracking search methods include state-space (tree) search and IDA*. LUCAS Similar arguments hold for LUCAS as for the CM-2.
Reference: [AB91] <author> Selim Akyokus and P. Bruce Berra. </author> <title> Optical content-addressable memories for data/knowledge base processing. </title> <editor> In V.K. Prasanna, editor, </editor> <booktitle> Proc. 5th Int. Parallel Processing Symposium, </booktitle> <pages> pages 202-207. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: The asymptotic times in the table require full data parallelism (one processing element per data item). CAM operations are highly implementation independent; several proposed optical CAMs provide virtually the same matching operations <ref> [AB91, Lou91] </ref>. Both types of CAM are restricted to static, fixed-length, ordered lists of attributes. Associative Processing CAM hardware and programming techniques can be extended to support more sophisticated matching and faster manipulation of the selected data.
Reference: [AB93] <author> Ed P. Andert Jr. and Thomas Bartolac. </author> <title> Parallel neural network training. </title> <editor> In James Geller, editor, </editor> <booktitle> Innovative Applications of Massive Parallelism, </booktitle> <pages> pages 7-13. </pages> <publisher> AAAI, </publisher> <year> 1993. </year> <booktitle> Working Notes, Spring Symposium Series, </booktitle> <publisher> Stanford University. </publisher>
Reference-contexts: were especially influential in reviving work on massively parallel AI. 42 Paper Machine PEs Category [SJ94b] search [ACP93] search [CR94] vision [Hen94] search [HJ93] constraint satisfaction [FFG94] other [SLH94] other [Pin94] neural net, logic [Ass94] theorem proving, logic [SS94a] rule-based system [Sha93] neural net [AG93] search [Kum93] SN and related <ref> [AB93] </ref> neural net [WG93] CM-5 other [LHB93] Symmetry constraint satisfaction [Rob93] custom other [HIM94] custom other [LCV94] transputer neural net [OMT94] custom 10 other [SJ94a] transputer 16 theorem proving, logic [Gel93b] CM-5 32 SN and related [CS94] CM-5 32 other [GW94] WARP 32 vision [LP94] iPSC 32 constraint satisfaction [ZM94] transputer
Reference: [ACP93] <author> Jean-Marc Andreoli, Paolo Ciancarini, and Remo Pareschi. </author> <title> Parallel searching with multisets-as-agents. </title> <editor> In James Geller, editor, </editor> <booktitle> Innovative Applications of Massive Parallelism, </booktitle> <pages> pages 14-25. </pages> <publisher> AAAI, </publisher> <year> 1993. </year> <booktitle> Working Notes, Spring Symposium Series, </booktitle> <publisher> Stanford University. </publisher>
Reference-contexts: Semantic networks (particularly Fahlman [Fah85]) and neural networks (particularly Hopfield, Rumelhart, and Hinton [Hop82, RHW86]) were especially influential in reviving work on massively parallel AI. 42 Paper Machine PEs Category [SJ94b] search <ref> [ACP93] </ref> search [CR94] vision [Hen94] search [HJ93] constraint satisfaction [FFG94] other [SLH94] other [Pin94] neural net, logic [Ass94] theorem proving, logic [SS94a] rule-based system [Sha93] neural net [AG93] search [Kum93] SN and related [AB93] neural net [WG93] CM-5 other [LHB93] Symmetry constraint satisfaction [Rob93] custom other [HIM94] custom other [LCV94] transputer
Reference: [Adv88a] <author> Advanced Micro Devices. </author> <title> Am95C85 Content-Addressable Data Manager Product Description, </title> <year> 1988. </year>
Reference-contexts: Among the numerous CAM integrated circuits developed in the last two decades, two are (unsuccessful) commercial offerings: the Am95C85, a CAM-based "articulated queue," <ref> [Adv88b, Adv88a] </ref> and the MD1210, a bit-serial fuzzy matcher [Mic89]. Two 20 K bit CAMs, one by Motomura et a. and one by Ogura et al. are among the largest research research chips [M + 90, O + 89].
Reference: [Adv88b] <institution> Advanced Micro Devices. </institution> <note> Am95C85 Content-Addressable Data Manager Technical Manual, </note> <year> 1988. </year>
Reference-contexts: Among the numerous CAM integrated circuits developed in the last two decades, two are (unsuccessful) commercial offerings: the Am95C85, a CAM-based "articulated queue," <ref> [Adv88b, Adv88a] </ref> and the MD1210, a bit-serial fuzzy matcher [Mic89]. Two 20 K bit CAMs, one by Motomura et a. and one by Ogura et al. are among the largest research research chips [M + 90, O + 89].
Reference: [AG89] <author> George S. Almasi and Allan Gottlieb. </author> <title> Highly Parallel Computing. </title> <publisher> Ben-jamin/Cummings Publishing Co. Inc., </publisher> <address> Redwood City, CA, </address> <year> 1989. </year>
Reference-contexts: Multiple binary trees of SIMD PEs are rooted at a MIMD processor, and multiple trees are interconnected by a shu*e-based network. Memory is extremely small, at most 256 bytes per PE. Estimates showed a 16 K PE NON-VON to be only 10 times faster than earlier database machines <ref> [Sha88, AG89] </ref>. FAIM FAIM (1987) supports object-oriented, logic, and procedural programming (LISP and Prolog) by attaching a small amount of CAM to RISC processors [A + 87]. Each CAM has only a small number of data objects per match circuit and provides only masked-comparison. <p> This work is not is not included in Table 2.2 as it did not progress past a general architectural concept. Machine Year Description Processors Peak Performance ASP [Koh80] 1967 SIMD, 2-d mesh, assoc. proposed ILLIAC IV <ref> [AG89] </ref> 1968 SIMD, 2-d mesh 64 500 MFLOPS cm* [SFS77] 1977 MIMD, multibus clusters 100 10 MIPS UNC Cellular [AG89] 1979 MSIMD, reduction, tree 1 M MPP [Bat82] 1983 SIMD, bit-serial, 2-d mesh 16 K 3,000 MIPS CM-1 [AG89] 1985 SIMD, bit serial, hypercube 64 K 2,000 MIPS H.A.P.S. [Stu85] 1985 <p> Machine Year Description Processors Peak Performance ASP [Koh80] 1967 SIMD, 2-d mesh, assoc. proposed ILLIAC IV <ref> [AG89] </ref> 1968 SIMD, 2-d mesh 64 500 MFLOPS cm* [SFS77] 1977 MIMD, multibus clusters 100 10 MIPS UNC Cellular [AG89] 1979 MSIMD, reduction, tree 1 M MPP [Bat82] 1983 SIMD, bit-serial, 2-d mesh 16 K 3,000 MIPS CM-1 [AG89] 1985 SIMD, bit serial, hypercube 64 K 2,000 MIPS H.A.P.S. [Stu85] 1985 SIMD, bus, assoc. proposed nCUBE [H + 86] 1985 MIMD, hypercube 1 K 1,945 MIPS NON-VON [AG89] 1985 MSIMD, <p> Peak Performance ASP [Koh80] 1967 SIMD, 2-d mesh, assoc. proposed ILLIAC IV <ref> [AG89] </ref> 1968 SIMD, 2-d mesh 64 500 MFLOPS cm* [SFS77] 1977 MIMD, multibus clusters 100 10 MIPS UNC Cellular [AG89] 1979 MSIMD, reduction, tree 1 M MPP [Bat82] 1983 SIMD, bit-serial, 2-d mesh 16 K 3,000 MIPS CM-1 [AG89] 1985 SIMD, bit serial, hypercube 64 K 2,000 MIPS H.A.P.S. [Stu85] 1985 SIMD, bus, assoc. proposed nCUBE [H + 86] 1985 MIMD, hypercube 1 K 1,945 MIPS NON-VON [AG89] 1985 MSIMD, bit-byte, tree, MIN 16 K+ AAP2 [K + 86] 1986 SIMD, 2-d mesh, assoc. 64 K 10,000 MOPS CLIP7A <p> UNC Cellular <ref> [AG89] </ref> 1979 MSIMD, reduction, tree 1 M MPP [Bat82] 1983 SIMD, bit-serial, 2-d mesh 16 K 3,000 MIPS CM-1 [AG89] 1985 SIMD, bit serial, hypercube 64 K 2,000 MIPS H.A.P.S. [Stu85] 1985 SIMD, bus, assoc. proposed nCUBE [H + 86] 1985 MIMD, hypercube 1 K 1,945 MIPS NON-VON [AG89] 1985 MSIMD, bit-byte, tree, MIN 16 K+ AAP2 [K + 86] 1986 SIMD, 2-d mesh, assoc. 64 K 10,000 MOPS CLIP7A [FMD88] 1986 SIMD, 2-d mesh 256 64 MIPS LUCAS [FKS85] 1986 SIMD, bit-serial, MIN, assoc. 128 CM-2 [TR88] 1987 SIMD, bit-serial, hypercube 64 K 4,000 MIPS AMT DAP [TW91] <p> [FMD88] 1986 SIMD, 2-d mesh 256 64 MIPS LUCAS [FKS85] 1986 SIMD, bit-serial, MIN, assoc. 128 CM-2 [TR88] 1987 SIMD, bit-serial, hypercube 64 K 4,000 MIPS AMT DAP [TW91] 1987 SIMD, 2-d mesh, orth. bus 1 K 800 MFLOPS GAPP [NCR87] 1987 SIMD, 1,2-d mesh 2 K 900 MOPS DADO <ref> [AG89] </ref> 1987 MIMD, tree 8 K PASM [MSS85] 1987 MSIMD, bus, xbar 1 K TRAC [AG89] 1987 MSIMD, banyan 4 FAIM [A + 87] 1987 MSIMD, 2-d mesh 1 K+ MasPar MP-1 [Nic90] 1989 SIMD, 2-d mesh, MIN 16 K 26,000 MIPS nCUBE2 [H + 86] 1989 MIMD, hypercube 8 K <p> 128 CM-2 [TR88] 1987 SIMD, bit-serial, hypercube 64 K 4,000 MIPS AMT DAP [TW91] 1987 SIMD, 2-d mesh, orth. bus 1 K 800 MFLOPS GAPP [NCR87] 1987 SIMD, 1,2-d mesh 2 K 900 MOPS DADO <ref> [AG89] </ref> 1987 MIMD, tree 8 K PASM [MSS85] 1987 MSIMD, bus, xbar 1 K TRAC [AG89] 1987 MSIMD, banyan 4 FAIM [A + 87] 1987 MSIMD, 2-d mesh 1 K+ MasPar MP-1 [Nic90] 1989 SIMD, 2-d mesh, MIN 16 K 26,000 MIPS nCUBE2 [H + 86] 1989 MIMD, hypercube 8 K 61,440 MIPS April [A + 90] 1990 MIMD, 2,3-d mesh, dataflow 1000+ BLITZEN [B + <p> The resolver tree is distributed across the PE chips by including one extra block per chip for use in sub-tree combining, as in Leiserson's H-tree layout <ref> [AG89] </ref>. A circuit-level analysis including gate, and signal propagation and chip-crossing delays shows that resolution time for a MISC PN is slightly less than 3 PE clock cycles.
Reference: [AG93] <author> Tito Autrey and Herbert Gelernter. </author> <title> Parallel heuristic search. </title> <editor> In James Geller, editor, </editor> <booktitle> Innovative Applications of Massive Parallelism, </booktitle> <pages> pages 26-31. </pages> <publisher> AAAI, </publisher> <year> 1993. </year> <booktitle> Working Notes, Spring Symposium Series, </booktitle> <publisher> Stanford University. </publisher>
Reference-contexts: Hopfield, Rumelhart, and Hinton [Hop82, RHW86]) were especially influential in reviving work on massively parallel AI. 42 Paper Machine PEs Category [SJ94b] search [ACP93] search [CR94] vision [Hen94] search [HJ93] constraint satisfaction [FFG94] other [SLH94] other [Pin94] neural net, logic [Ass94] theorem proving, logic [SS94a] rule-based system [Sha93] neural net <ref> [AG93] </ref> search [Kum93] SN and related [AB93] neural net [WG93] CM-5 other [LHB93] Symmetry constraint satisfaction [Rob93] custom other [HIM94] custom other [LCV94] transputer neural net [OMT94] custom 10 other [SJ94a] transputer 16 theorem proving, logic [Gel93b] CM-5 32 SN and related [CS94] CM-5 32 other [GW94] WARP 32 vision [LP94]
Reference: [AHEK94] <author> William A. Anderson, James A. Hendler, Matthew Evett, and Brian Kettler. </author> <title> Massively parallel matching of knowledge structures. </title> <editor> In Hiroaki Kitano and James A. Hendler, editors, </editor> <booktitle> Massively Parallel Artificial Intelligence, </booktitle> <pages> pages 52-73, </pages> <address> Menlo Park, CA, 1994. </address> <publisher> AAAI Press / The MIT Press. </publisher>
Reference-contexts: nCUBE 1024 search [NM93] CM-2 4096 genetic, classifier systems [Gel94a] CM-2 8192 SN and related [CG94] CM-2 8192 search [KK93] CM-2 8192 search [JSS93] MP-1 8192 theorem proving, logic [PKF93] CM-2 16384 search [PW94] CM-2 16384 natural language [WD94] CM-2 16384 neural net, learning [ND94] CM-2 16384 neural net, learning <ref> [AHEK94] </ref> CM-2 16384 SN and related [EAH94] CM-2 16384 SN and related [JB94] MP-1 16384 vision, neural net [ZMHV93] MP-1 16384 neural net, learning [Coo93] CM-2 32768 search Table 2.6: Sample of Recent Massively Parallel AI Publications Prior work emphasizes a relatively small number of algorithms, with generally disappointing results for <p> Hendler et al. report queries taking from 0.5 to 1.6 s on a 16 K processor CM-2. They also more restricted structure matching by DeMara on SNAP [EAH94] and by Higuchi on the IXM2 <ref> [AHEK94] </ref>. Memory-Based Reasoning In essence, memory-based reasoning stores a large body of knowledge and experience and deals with new situations by retrieving and processing old situations, making the knowledge-rule tradeoff clearly on the side of knowledge. Retrieval techniques and measures of similarity are important. <p> Third, the backtracking refinement search is performed. The refinement procedure is called at each branch of the backtracking search tree, dramatically pruning the number of branches explored. Hendler et al. present a structure-matching algorithm for finding instances of a query graph within a semantic network <ref> [AHEK94] </ref>. This corresponds to finding multiple subgraph isomorphisms within a single large graph. Although time can be linear in the number of nodes, one processor per node and edge in the semantic network is evidently required.
Reference: [AK + 89] <author> Hassan Ait-Kaci et al. </author> <title> Efficient implementation of lattice operations. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 11(1) </volume> <pages> 115-146, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: Figure 2.5 shows a small example; the hierarchy is essentially a semantic network with only isa relations. Both the concept-type hierarchy and the MIS partial order can be encoded using lattice codes in order to speed certain operations <ref> [AK + 89] </ref>. The simplest code is just the transitive closure of the graph edges, shown in Figure 2.5. These codes are N bits long, where N is the number of objects in the graph, for total storage O (N 2 ). <p> Each row of the RTC matrix can be used as a code for the corresponding node, as shown previously in Figure 2.5 (page 20), and the recursive application becomes a simple logical operation <ref> [AK + 89] </ref>. For example, subsumes (X,Y) becomes a masked match on the codes for X and Y. Since the transitive closure is the minimum information required to fully characterize the transitive relations, alternative codes can be viewed as compressing only the representation, not the underlying information. <p> the total number of 1 in all the node codes combined is lg N 1 X (L + 1)2 L = N lg N N + 1 = O (N ln N ): Ait-Kaci presents an extension to transitive-closure coding that recursively groups and encodes clusters in the partial order <ref> [AK + 89] </ref>. In this method, codes are O (lg N ) bits in length. The asymptotic space reduction is significant; with 64 K graphs, modulated encoding requires over 600 times less storage. <p> Operations Using Hierarchy Codes For transitive closure codes, operations such as greatest-lower-bound (GLB) and least-upper-bound (LUB) are simply logical-and or logical-or operations between the codes, provided the GLB or LUB exist. Otherwise a decoding function can return the appropriate set of nodes <ref> [AK + 89] </ref>. In Stormon's coding a parent can be "named" by replacing the last 0 or 1 in the code with don't-care, and a sibling can be named by flipping the last 0 or 1.
Reference: [AL93] <author> J. Amenta and R. A. Levinson. </author> <title> Morph, an experience-based adaptive chess system: A demonstration report. </title> <journal> International Computer Chess Association Journal, </journal> <volume> 16(1) </volume> <pages> 51-53, </pages> <month> March </month> <year> 1993. </year> <month> 242 </month>
Reference-contexts: Additionally, weights associated with graphs can form the basis of learning, as in Morph <ref> [AL93] </ref>. Semantic networks support several types of inference through marker or message propagation [Fah85, M + 92]. Semantic networks and relational structure processing can also be combined in supporting AI.
Reference: [Alb91] <author> James S. Albus. </author> <title> Outline for a theory of intelligence. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 21(3) </volume> <pages> 473-509, </pages> <month> May/June </month> <year> 1991. </year>
Reference-contexts: Kanerva's key contribution is the analysis of the associative memory's behavior in computational geometry terms, drawing directly from prior models of cerebellar function <ref> [Mar69, Alb91] </ref> to provide multi-layer functionality without backpropagation. It is both computationally efficient and biologically plausible. In SDM an m-dimensional input vector induces a f0; 1g m parameter space, which is sparsely populated by storage locations (weights).
Reference: [AS83] <author> Gregory R. Andrews and Fred B. Schneider. </author> <title> Concepts and notations for concurrent computing. </title> <journal> Computing Surveys, </journal> <volume> 15(1) </volume> <pages> 3-43, </pages> <month> March </month> <year> 1983. </year>
Reference-contexts: The discussion is from the perspective of applications programming, excluding operating system and compiler issues. The reader is referred to Andrews and Schneider for a survey of general parallel languages and additional discussion of parallel programming requirements <ref> [AS83] </ref>. Sabot and Blelloch provide concise summaries of several collection-oriented programming languages [Sab88, Ble90]. Tables 2.3 and 2.4 combine these summaries and additional information. Prolog and LISP are excluded from the scope of this dissertation, due in part to limited speedups with control-level parallelism and their difficulty in programming applications.
Reference: [ASHH88] <author> A. Agarwal, R. Simoni, J. Hennessy, and M. Horowitz. </author> <title> An evaluation of directory schemes for cache coherence. </title> <booktitle> In 15th International Symposium on Computer Architecture, </booktitle> <pages> pages 280-289, </pages> <year> 1988. </year>
Reference-contexts: In addition to the memory sharing itself, this would add the requirement of cache coherency, as the high-performance microprocessors require cache. Several studies demonstrate the practicality of multi-bus cache coherency without requiring any bus to be common to all processors <ref> [ASHH88, TD91] </ref>. In summary, a co-processor might provide the following functions. * Managing inter-PN communication independently of the supervisor.
Reference: [Ass94] <author> Ulrich Assmann. </author> <title> A model for parallel deduction. </title> <editor> In V. Kumar et al., editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <pages> pages 145-162, </pages> <address> New York, </address> <year> 1994. </year> <journal> North-Holland. </journal> <volume> volume 2. </volume>
Reference-contexts: Semantic networks (particularly Fahlman [Fah85]) and neural networks (particularly Hopfield, Rumelhart, and Hinton [Hop82, RHW86]) were especially influential in reviving work on massively parallel AI. 42 Paper Machine PEs Category [SJ94b] search [ACP93] search [CR94] vision [Hen94] search [HJ93] constraint satisfaction [FFG94] other [SLH94] other [Pin94] neural net, logic <ref> [Ass94] </ref> theorem proving, logic [SS94a] rule-based system [Sha93] neural net [AG93] search [Kum93] SN and related [AB93] neural net [WG93] CM-5 other [LHB93] Symmetry constraint satisfaction [Rob93] custom other [HIM94] custom other [LCV94] transputer neural net [OMT94] custom 10 other [SJ94a] transputer 16 theorem proving, logic [Gel93b] CM-5 32 SN and
Reference: [B + 90a] <author> D. W. Blevins et al. BLITZEN: </author> <title> A highly integrated massively parallel machine. </title> <journal> J. Parallel Distributed Computing, </journal> <volume> 8 </volume> <pages> 150-160, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Despite having one quarter the number of processors as the CM-1, the MPP is 50% faster due to the increased clock rate, additional PE functional units, and communication network. Blitzen Blitzen is a VLSI successor to the MPP, with many of the improvements suggested by Batcher <ref> [Bat86, B + 90a] </ref>. The ability to modify the SIMD address by or-ing 10 bits from the shift register is a significant enhancement. Due to pin count limitations, however, this can only be used to access a small on-chip memory of 1 K bit per PE. <p> TRAC [AG89] 1987 MSIMD, banyan 4 FAIM [A + 87] 1987 MSIMD, 2-d mesh 1 K+ MasPar MP-1 [Nic90] 1989 SIMD, 2-d mesh, MIN 16 K 26,000 MIPS nCUBE2 [H + 86] 1989 MIMD, hypercube 8 K 61,440 MIPS April [A + 90] 1990 MIMD, 2,3-d mesh, dataflow 1000+ BLITZEN <ref> [B + 90a] </ref> 1990 SIMD, bit serial, 2-d mesh 16 K 3,000 MIPS iWarp [B + 90b] 1990 MIMD, 2-d mesh 64 1,300 MFLOPS BSYS [Hug91] 1990 systolic, 1-d mesh 470 108 MOPS NAP [KF90] 1990 SIMD, various simulated SNAP [M + 92] 1990 MSIMD, modified hypercube 144 CM-5 [Thi92] 1991 <p> Suspense's accuracy was surprisingly good considering the numerous circuit `tricks' used in these experimental memories. Die validation included two SIMD processor chips (BLITZEN and the MasPar MP-2 <ref> [B + 90a, Mas92a] </ref>), a systolic array (BSYS [Hug91]), a bit-serial CAM (PCAM [Rob89b]) and a CAM with a small 66 transistor bit-serial PE per word ([WS89]).
Reference: [B + 90b] <author> S. Bokar et al. </author> <title> Supporting systolic and memory communication in iWarp. </title> <booktitle> In 17th Int. Sym. on Computer Architectures, </booktitle> <pages> pages 70-81, </pages> <year> 1990. </year>
Reference-contexts: 1 K+ MasPar MP-1 [Nic90] 1989 SIMD, 2-d mesh, MIN 16 K 26,000 MIPS nCUBE2 [H + 86] 1989 MIMD, hypercube 8 K 61,440 MIPS April [A + 90] 1990 MIMD, 2,3-d mesh, dataflow 1000+ BLITZEN [B + 90a] 1990 SIMD, bit serial, 2-d mesh 16 K 3,000 MIPS iWarp <ref> [B + 90b] </ref> 1990 MIMD, 2-d mesh 64 1,300 MFLOPS BSYS [Hug91] 1990 systolic, 1-d mesh 470 108 MOPS NAP [KF90] 1990 SIMD, various simulated SNAP [M + 92] 1990 MSIMD, modified hypercube 144 CM-5 [Thi92] 1991 MSIMD, fat-tree, vector 1 K 128,000 MFLOPS Paragon [Hwa93] 1991 MIMD, 2-d mesh 300,000
Reference: [B + 94] <editor> Wolfgang O. Budde et al. </editor> <title> An asynchronous high-speed packet switching component. </title> <booktitle> IEEE Design and Test of Computers, </booktitle> <pages> pages 33-42, </pages> <month> Summer </month> <year> 1994. </year>
Reference-contexts: The T3D emphasizes advanced technology high-speed communications. Each bidirectional link has 300 M Byte/s bandwidth. Coupled with the 3-d (as opposed to 2-d) mesh topology, this improves system scalability, although as systems become larger link congestion will eventually occur <ref> [B + 94] </ref>. An additional tree 2 One might thereby argue that the CM-5 is a MSIMD architecture as well as a vector processor. 29 network provides global-and as well as global-or for barrier and status synchronization. <p> Other algorithms and applications may also have higher communication rates. The 200 MByte/s links of the Cray T3D [Oed93] processor nodes could support as many as a thousand MISC PNs in a mesh topology, even at 20 MByte/s per node. With gigabyte/s networks emerging <ref> [B + 94] </ref>, higher bandwidth PNs or much larger MISC Machines should be feasible, with scaling limited more by available high-level parallelism than by global communication hardware capabilities. 146 The author hypothesizes that hierarchical networks, such as fat trees and hierarchical multibus topologies are preferable for MISC and AI applications. <p> The supervisor is taken as a 3 million transistor microprocessor and memories are based on commercial product literature supplemented by Suspense calculations. The communications interface is taken as 1/4 of a four-in one-out ATM packet switching component <ref> [B + 94] </ref> which includes virtual channels and a total throughput of 70 M bytes/s, giving 105,000 transistors. Lower-latency alternatives would be used in any MISC implementation; the ATM is used only as a guide to likely transistor counts.
Reference: [Bak90] <author> H. B. Bakoglu. </author> <title> Circuits, Interconnections, and Packaging for VLSI. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1990. </year>
Reference-contexts: SUSPENS models interactions between key parameters such as device properties, on-chip interconnections, off-chip interconnect, and empirical correlations between chip characteristics and system architecture and organization (such as Rent's rule and typical logic depths and gate fan-outs) <ref> [Bak90] </ref>. The off-chip models include wafer-scale integration, multi-chip modules, and printed circuit boards. Suspense (SUSPENS Extensions) adds models specific to RAM and CAM and allows `tiles' of different circuit types to be combined on a single die. Extensions permit analysis of differently-sized dies and mixtures of package types. <p> The SUSPENS equations are described in Appendix A. The chip and system models include numerous implicit and explicit assumptions in addition to those mentioned here, and the reader is referred to Bakoglu <ref> [Bak90] </ref> for a more complete description. Validation tests indicate that the assumptions are appropriate for the MISC domain. There are three basic models: die, module, and system. <p> A user-input circuit category (memory, microprocessor, gate array or random logic, supercomputer) selects effective logic-depth (including typical critical-path gate sizing, latch and setup delays, and on-chip memory access), gate fanout, and Rent's coefficients from empirical correlations, also by table lookup. Next, Rent's rule <ref> [Bak90] </ref> is applied to the user-input number of logic gates on the die to determine average on-chip interconnect length in terms of average gate size. <p> These parameters include wiring 3 The die model can be modified for other VLSI technologies, including bipolar and gallium arsenide, with relatively minor revisions <ref> [Bak90] </ref>. 52 capacitance, resistance, pitch, characteristic impedance, and typical number of wiring layers, pin pitch and capacitance, and the substrate dielectric constant. Rent's coefficients are also determined by table look-up from user input of the system type (microprocessor, gate array, or supercomputer). <p> minimum of all the modules and all the die, power is the total for all modules adjusted for frequency, and area is the total of all modules. 3.2.3 Suspense Extensions The Suspense early analysis tool developed as part of this dissertation extends the SUSPENS model. 4 SUSPENS as described in <ref> [Bak90] </ref> assumes that the total gates in a system are uniformly divided among some specified number of equal-sized chips all of the same type. Unlike SUSPENS, Suspense allows non-uniform system partitioning and introduces a "tile" into the hierarchy of models. <p> In addition to the general die models in SUSPENS, Suspense adds new models specific to RAM and CAM that exploit the regular topology and circuit characteristics to 4 Suspense was written independently in C based on the equations in <ref> [Bak90] </ref>, and is not a modification of any existing SUSPENS code. 53 improve accuracy and simplify user input. Suspense also adds models for differing packaging technologies, and permits mixtures of module technologies in a single analysis run. <p> Rent's rule empirically relates the number of devices to the number of connections. The number of external connections is cN fi where c and fi are empirical constants. Since fi is generally less than 0.5 (except for supercomputer die) <ref> [Bak90] </ref>, the O ( p N) scaling not a fundamental problem for uniprocessor systems. Practically, mechanical issues make packages with more than a few hundred pins difficult, and advanced microprocessors are pushing these packaging limits. <p> The proposed architecture is general-purpose, providing new insights into massively parallel AI and promising improved computational support for a variety of advanced applications. 214 Appendices 215 Appendix A. SUSPENS This appendix presents the equations and parameters for the SUSPENS early analysis tool as presented in Chapter 9 of <ref> [Bak90] </ref> and summarized in this dissertation's Chapter 3. Tables A.1 and A.2 list the symbols used here and in Appendix B. Bakoglu's notation is changed in several places for consistency and clarity. <p> Finally, the number of pins follows Rent's rule: P = K p N b : Table A.3 lists the -dependent coefficients and Table A.4 lists the chip-type dependent coefficients from <ref> [Bak90] </ref>. 7 m 3 m 2 m 1.3 m 0.7 m 0.35 m Parameter nMOS nMOS CMOS CMOS CMOS CMOS p w (m) 28 10 6 4 2 1 R tr (NENH) () 15000 15000 10000 8000 8000 8000 C tr (f F) 20 6 4 3 2 1 C int <p> = 1 + Z o ln 5 Module capacitance is given by C m = F c + 1 1 5 N C tr + 3C pad + RF p C int 219 and power by W m = 2 dd : Table A.5 list the module-technology dependent coefficients from <ref> [Bak90] </ref>. Parameter WSI MCM Ceram. PCB p w (m) 20 50 500 160 p p (m) 40 100 250 1000 C pad (pF) 0 0.25 0.5 10 C int (pF=cm) 1.5 1.0 1.9 0.8 Table A.5: Module-Technology Parameters A.3 System-Level Equations The SUSPENS system model is straightforward. <p> The notation and symbols used are given in Tables A.1 and A.2 in Appendix A. B.1 General Tile Model CMOS die parameters for the tile and die models are calculated as a function of , with the function and its parameters derived from regression to Table 9.6 of <ref> [Bak90] </ref>, with V dd input by the user. <p> The model adds signal propagation delay for both single-die packages and die stacks. For packaged die this is two lines at 1/2 the length of the package edge. For die stacks this is one line at the length of the stack. Interconnect parameters are taken from Table 9.7 in <ref> [Bak90] </ref>, for a substrate typical of the package type. The delay caused by pad capacitances are included in the module model. The package model does include pad capacitance in determining package power so that power density (W=cm 2 ) is appropriate for estimating cooling requirements. <p> Rents rule is applied for type supercomputer (K p = 82 and fi = 0:25 <ref> [Bak90] </ref>) and all other module types (K p = 7 and fi = 0:21 [MQF91]). Area and capacitance for inter-module connections area added based on typical pad pitch and capacitance. Inter-chip communication power is calculated from C mi , V c , and module frequency. <p> For die validation, only information on packaged microprocessor and memory die was available, combining tile, die, and package validation. Verification was limited by the few details publicly available; early low-transistor count microprocessors were nMOS or HMOS. Microprocessor results were evaluated against the 80386, 80486, 68020, and the experimental Bellmac-32A <ref> [Bak90] </ref>. Verification against commercial SRAMS and DRAMS required fitting , the number of sub-arrays, and the cell area simultaneously so the accuracy results are suspect.
Reference: [Bar88] <author> J. M. Barnard. </author> <title> Problems of substructure search and their solution. </title> <editor> In Wendy Warr, editor, </editor> <booktitle> Chemical Structures: The International Language of Chemistry. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: Fortunately, the expected case time is empirically polynomial [Ull76, WWR91]. The worst-case time results from generally pathological cases that rarely occur in practice <ref> [Bar88, CK80, Sus65] </ref> so that parallelization can provide a difference in tractable graph size. Further, graph size is bounded for most applications.
Reference: [Bat82] <author> Kenneth E. Batcher. </author> <title> Bit-serial parallel processing systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-31(5):377-384, </volume> <month> May </month> <year> 1982. </year>
Reference-contexts: These improvements (in particular the floating point chips) greatly increase performance, primarily for numeric codes. MPP The Goodyear Massively Parallel Processor (MPP) (1983), developed for NASA / God-dard, is based on the STARAN and ASPRO associative processors <ref> [Bat82] </ref>. In addition to its original purpose of Landsat image processing, applications include weather forecasting, digital system simulation, and neural networks. The full system has 16 K bit-serial processors in a 2-d mesh network. <p> Machine Year Description Processors Peak Performance ASP [Koh80] 1967 SIMD, 2-d mesh, assoc. proposed ILLIAC IV [AG89] 1968 SIMD, 2-d mesh 64 500 MFLOPS cm* [SFS77] 1977 MIMD, multibus clusters 100 10 MIPS UNC Cellular [AG89] 1979 MSIMD, reduction, tree 1 M MPP <ref> [Bat82] </ref> 1983 SIMD, bit-serial, 2-d mesh 16 K 3,000 MIPS CM-1 [AG89] 1985 SIMD, bit serial, hypercube 64 K 2,000 MIPS H.A.P.S. [Stu85] 1985 SIMD, bus, assoc. proposed nCUBE [H + 86] 1985 MIMD, hypercube 1 K 1,945 MIPS NON-VON [AG89] 1985 MSIMD, bit-byte, tree, MIN 16 K+ AAP2 [K +
Reference: [Bat86] <author> Kenneth E. </author> <title> Batcher. </title> <booktitle> The architecture of tomorrow's massively parallel computer. In Proc. 1st Symp. on the Frontiers of Massively Parallel Scientific Computation, </booktitle> <pages> pages 151-157, </pages> <year> 1986. </year>
Reference-contexts: Despite having one quarter the number of processors as the CM-1, the MPP is 50% faster due to the increased clock rate, additional PE functional units, and communication network. Blitzen Blitzen is a VLSI successor to the MPP, with many of the improvements suggested by Batcher <ref> [Bat86, B + 90a] </ref>. The ability to modify the SIMD address by or-ing 10 bits from the shift register is a significant enhancement. Due to pin count limitations, however, this can only be used to access a small on-chip memory of 1 K bit per PE.
Reference: [BCM88] <author> J. P. Banatre, A. Coutant, and D. Le Metayer. </author> <title> A parallel machine for multi-set transformation and its programming style. </title> <journal> Future Generations Computer Systems, </journal> <volume> 4 </volume> <pages> 133-144, </pages> <year> 1988. </year>
Reference-contexts: Gamma Sections 2.4.3 and 4.1.2 discuss Gamma's "react-act" programming and the MISC associative model's match-act style, showing that the Gamma constructs can be readily translated into SEGUL. By this argument, MISC can execute the programming examples given in the Gamma literature <ref> [BM93, BM90, BCM88] </ref>, listed in Table 8.3. SEGUL supplements this construct with many others, including global information and control structures which remove the Gamma restriction that prohibit its use for crucial algorithms such as backtracking.
Reference: [BF82] <author> Avron Barr and Edward A. Feigenbaum. </author> <booktitle> The Handbook of Artificial Intelligence. </booktitle> <publisher> HeurisTech Press, Stanford, </publisher> <address> CA, </address> <year> 1982. </year>
Reference-contexts: On the other hand, extensive use of matching operations and knowledge base search can be used to advantage. Knowledge processing often emphasizes pattern matching and classification (associative retrieval) as the single mechanism for accessing components of compound data structures, selecting procedures, and passing parameters <ref> [BF82] </ref>. Moldovan considers matching to be one of the "inner loops" of reasoning, and comments that it is trivial for single attributes but difficult for structured relations [M + 92]. <p> Many forms of knowledge processing can be viewed as having pattern matching as the single mechanism for accessing components of compound data structures, selecting procedures, and passing parameters <ref> [BF82] </ref>. This view is shared by Moldovan and the SNAP group (Section 2.3.4), who hold matching as the inner loop in reasoning, describing it as trivial for single keys but difficult for structured relations [M + 92].
Reference: [BHN + 92] <author> Franz Baader, Bernhard Hollunder, Bernhard Nebel, Hans-Jurgen Profitlich, and Enrico Franconi. </author> <title> An empirical analysis of optimization techniques for terminological representation systems. </title> <booktitle> In Proceedings of the 3rd International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <address> Cambridge, MA, </address> <month> Oc-tober </month> <year> 1992. </year>
Reference-contexts: It has been proven that Method III is superior to conventional single-level indexing if there is structure to be taken advantage of in the KB [Lev92a]. Baader, Hollunder, 20 and Nebel show Method III to be the fastest of all surveyed complex-object retrieval methods <ref> [BHN + 92] </ref>. In addition to significantly increasing the speed and efficiency of retrieval, Method III supports conceptual clustering, generalization, planning, parsing, rule selection, and machine learning.
Reference: [BKS93] <author> Jon Bright, Simon Kasif, and Lewis Stiller. </author> <title> Exploiting algebraic structure in parallel state-space search (extended abstract). </title> <editor> In James Geller, editor, </editor> <booktitle> Innovative Applications of Massive Parallelism, </booktitle> <pages> pages 32-39. </pages> <publisher> AAAI, </publisher> <year> 1993. </year> <booktitle> Working Notes, Spring Symposium Series, </booktitle> <publisher> Stanford University. </publisher>
Reference-contexts: SN and related [Lan93] CM-2 SN, natural language [H + 94] IXM2 SN, natural language [O + 94] IXM2 SN and related [Fah93] NETL SN and related [Twa94] custom genetic, classifier systems [LH94] custom other [YK94] custom neural net, learning [CGK94] nCUBE2 512 other [HJ94] transputer 512 theorem proving, logic <ref> [BKS93] </ref> CM-2 1024 other [S + 93c] DADO2 1024 rule-based system [CGK93] nCUBE 1024 search [NM93] CM-2 4096 genetic, classifier systems [Gel94a] CM-2 8192 SN and related [CG94] CM-2 8192 search [KK93] CM-2 8192 search [JSS93] MP-1 8192 theorem proving, logic [PKF93] CM-2 16384 search [PW94] CM-2 16384 natural language [WD94]
Reference: [Ble90] <author> Guy E. Blelloch. </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year> <month> 243 </month>
Reference-contexts: The discussion is from the perspective of applications programming, excluding operating system and compiler issues. The reader is referred to Andrews and Schneider for a survey of general parallel languages and additional discussion of parallel programming requirements [AS83]. Sabot and Blelloch provide concise summaries of several collection-oriented programming languages <ref> [Sab88, Ble90] </ref>. Tables 2.3 and 2.4 combine these summaries and additional information. Prolog and LISP are excluded from the scope of this dissertation, due in part to limited speedups with control-level parallelism and their difficulty in programming applications. The sample also excludes systolic languages. <p> Roughly half the languages allow explicit user control of communication. The remainder make communication implicit as part of the computation functions. Very 34 Base Parallel Language Year Type Language Side Garbage Matching Parallel Effects Collection Emphasized a APL <ref> [Ble90] </ref> 1962 serial * * ffi b ASC [P + 94] 1992 `all' * * * c C* [Ble90] SIMD ffi * ffi ffi d CM-LISP [Ble90] 1986 SIMD ffi * * ffi e Cray Fortran [Ble90] 1978 vector ffi * ffi ffi f Gamma [BM93] 1986 * ffi ffi * <p> The remainder make communication implicit as part of the computation functions. Very 34 Base Parallel Language Year Type Language Side Garbage Matching Parallel Effects Collection Emphasized a APL <ref> [Ble90] </ref> 1962 serial * * ffi b ASC [P + 94] 1992 `all' * * * c C* [Ble90] SIMD ffi * ffi ffi d CM-LISP [Ble90] 1986 SIMD ffi * * ffi e Cray Fortran [Ble90] 1978 vector ffi * ffi ffi f Gamma [BM93] 1986 * ffi ffi * g Linda [CG89] 1982 MIMD ffi h *LISP [Ble90] SIMD ffi * ffi ffi i MPL [Chr90] 1990 <p> Very 34 Base Parallel Language Year Type Language Side Garbage Matching Parallel Effects Collection Emphasized a APL <ref> [Ble90] </ref> 1962 serial * * ffi b ASC [P + 94] 1992 `all' * * * c C* [Ble90] SIMD ffi * ffi ffi d CM-LISP [Ble90] 1986 SIMD ffi * * ffi e Cray Fortran [Ble90] 1978 vector ffi * ffi ffi f Gamma [BM93] 1986 * ffi ffi * g Linda [CG89] 1982 MIMD ffi h *LISP [Ble90] SIMD ffi * ffi ffi i MPL [Chr90] 1990 SIMD ffi ffi ffi j MultiLISP [Ble90] ffi <p> Parallel Language Year Type Language Side Garbage Matching Parallel Effects Collection Emphasized a APL <ref> [Ble90] </ref> 1962 serial * * ffi b ASC [P + 94] 1992 `all' * * * c C* [Ble90] SIMD ffi * ffi ffi d CM-LISP [Ble90] 1986 SIMD ffi * * ffi e Cray Fortran [Ble90] 1978 vector ffi * ffi ffi f Gamma [BM93] 1986 * ffi ffi * g Linda [CG89] 1982 MIMD ffi h *LISP [Ble90] SIMD ffi * ffi ffi i MPL [Chr90] 1990 SIMD ffi ffi ffi j MultiLISP [Ble90] ffi * * ffi k NIAL [Ble90] 1982 l Occam [Bur88] <p> + 94] 1992 `all' * * * c C* <ref> [Ble90] </ref> SIMD ffi * ffi ffi d CM-LISP [Ble90] 1986 SIMD ffi * * ffi e Cray Fortran [Ble90] 1978 vector ffi * ffi ffi f Gamma [BM93] 1986 * ffi ffi * g Linda [CG89] 1982 MIMD ffi h *LISP [Ble90] SIMD ffi * ffi ffi i MPL [Chr90] 1990 SIMD ffi ffi ffi j MultiLISP [Ble90] ffi * * ffi k NIAL [Ble90] 1982 l Occam [Bur88] MIMD * ffi m Paralation [Ble90] 1987 serial, vector, ffi SIMD, MIMD n Parallel Pascal [Ble90] ffi * ffi ffi o Pascal-L [FKS85] <p> CM-LISP <ref> [Ble90] </ref> 1986 SIMD ffi * * ffi e Cray Fortran [Ble90] 1978 vector ffi * ffi ffi f Gamma [BM93] 1986 * ffi ffi * g Linda [CG89] 1982 MIMD ffi h *LISP [Ble90] SIMD ffi * ffi ffi i MPL [Chr90] 1990 SIMD ffi ffi ffi j MultiLISP [Ble90] ffi * * ffi k NIAL [Ble90] 1982 l Occam [Bur88] MIMD * ffi m Paralation [Ble90] 1987 serial, vector, ffi SIMD, MIMD n Parallel Pascal [Ble90] ffi * ffi ffi o Pascal-L [FKS85] 1983 SIMD ffi ffi p pSather, dSather [FLR91, Sch92] 1991 MIMD, SIMD ffi ffi * q <p> ffi e Cray Fortran <ref> [Ble90] </ref> 1978 vector ffi * ffi ffi f Gamma [BM93] 1986 * ffi ffi * g Linda [CG89] 1982 MIMD ffi h *LISP [Ble90] SIMD ffi * ffi ffi i MPL [Chr90] 1990 SIMD ffi ffi ffi j MultiLISP [Ble90] ffi * * ffi k NIAL [Ble90] 1982 l Occam [Bur88] MIMD * ffi m Paralation [Ble90] 1987 serial, vector, ffi SIMD, MIMD n Parallel Pascal [Ble90] ffi * ffi ffi o Pascal-L [FKS85] 1983 SIMD ffi ffi p pSather, dSather [FLR91, Sch92] 1991 MIMD, SIMD ffi ffi * q SETL [SDDS86] 1981 serial r SQL [Ble90] <p> ffi f Gamma [BM93] 1986 * ffi ffi * g Linda [CG89] 1982 MIMD ffi h *LISP <ref> [Ble90] </ref> SIMD ffi * ffi ffi i MPL [Chr90] 1990 SIMD ffi ffi ffi j MultiLISP [Ble90] ffi * * ffi k NIAL [Ble90] 1982 l Occam [Bur88] MIMD * ffi m Paralation [Ble90] 1987 serial, vector, ffi SIMD, MIMD n Parallel Pascal [Ble90] ffi * ffi ffi o Pascal-L [FKS85] 1983 SIMD ffi ffi p pSather, dSather [FLR91, Sch92] 1991 MIMD, SIMD ffi ffi * q SETL [SDDS86] 1981 serial r SQL [Ble90] serial (ffi) ffi no somewhat * yes Table 2.3: Parallel <p> Linda [CG89] 1982 MIMD ffi h *LISP <ref> [Ble90] </ref> SIMD ffi * ffi ffi i MPL [Chr90] 1990 SIMD ffi ffi ffi j MultiLISP [Ble90] ffi * * ffi k NIAL [Ble90] 1982 l Occam [Bur88] MIMD * ffi m Paralation [Ble90] 1987 serial, vector, ffi SIMD, MIMD n Parallel Pascal [Ble90] ffi * ffi ffi o Pascal-L [FKS85] 1983 SIMD ffi ffi p pSather, dSather [FLR91, Sch92] 1991 MIMD, SIMD ffi ffi * q SETL [SDDS86] 1981 serial r SQL [Ble90] serial (ffi) ffi no somewhat * yes Table 2.3: Parallel Languages Characteristic a b c d e f g h <p> <ref> [Ble90] </ref> 1982 l Occam [Bur88] MIMD * ffi m Paralation [Ble90] 1987 serial, vector, ffi SIMD, MIMD n Parallel Pascal [Ble90] ffi * ffi ffi o Pascal-L [FKS85] 1983 SIMD ffi ffi p pSather, dSather [FLR91, Sch92] 1991 MIMD, SIMD ffi ffi * q SETL [SDDS86] 1981 serial r SQL [Ble90] serial (ffi) ffi no somewhat * yes Table 2.3: Parallel Languages Characteristic a b c d e f g h i j k l m n o p q r Collections dynamic creation * * ffi * ffi * * * ffi ffi * * * dynamic change ffi * <p> Paralation adds nested parallelism to collection oriented languages; its key contribution is allowing operations on nested collections (collections whose members are themselves collections) to proceed in parallel, supporting multiple levels (grains) of parallelism. This, however, excludes iterative and other coarse-grain parallelism typical of MIMD models. Blelloch extends <ref> [Ble90] </ref> Sabot's prior work [Sab88] to include compiler techniques for compiling operations on nested collections, including conditionals, by concatenating the nestings and including descriptor information to "flattened" vector operations. In a SIMD implementation, elements of a collection are uniformly and sequentially divided over the available PEs. <p> Reductions include plus, maximum, and concatenate. The programmer can also specify a default value for destination elements not receiving any source element. In his dissertation, Blelloch demonstrates asymptotic performance similar to parallel random access machine (PRAM) computational models <ref> [Ble90] </ref>. He treats scans and permutations as constant-time operations; these in fact require computation and communication time, typically O ( p N ) or O (lg N ) for 2-d mesh and hypercube networks respectively. <p> Algorithms coded and analyzed include sorting, computational geometry, simple graph algorithms, and matrix operations <ref> [Ble90] </ref>. The graph algorithms demonstrate efficient processing on graphs represented as nested collections. As such, Paralation provides a means of expressing multiple levels of parallelism on compound objects, but within the confines of a single instruction stream (serial control). <p> Applying nested 63 parallelism to these objects as in the Paralation model maximizes parallelism, improves processing element utilization, and reduces execution time. Indeed, the term "paralation" is a contraction of "parallel relations" <ref> [Ble90] </ref>. 4.2 SEGUL Notation and Terminology This section presents the details of the MISC programming model through the pseu-docode notation SEGUL, developed as part of the MISC research. The MISC associative programming model brings together issues of programmability, algorithm development, and hardware requirements and design. <p> The programming model and notation informally draws from a number of collection oriented and parallel programming languages (the paralation model <ref> [Ble90] </ref>, SETL [SDDS86], Gamma [BM93], and Linda [CG89]) and is called SEGUL as a mutilated acronym of these. 2 The notation is primarily that of naive set theory [Dev93], thus incorporating a widely understood formalism and resulting in many similarities with the set- and tuple-oriented language SETL. <p> onetoten [int: xj1 &lt;= x &lt;= 10] nonunique paddr [bit:1024 xjx = random01 ()] nonunique myvec [int:64 xjx = 0] unique G f [x,y] j x 2 A, y 2 Bg "Matrix" Operations Numeric and logical operations involving collections are based on prior vector and data-parallel models, particularly Blelloch's Paralation <ref> [Ble90] </ref>. Most correspond to the element-wise "dot" operations of linear algebra. In the following examples the explicit collections can be replaced by the variable name of a collection, and operations can be numerical or logical. <p> This does not capture all parallelism however, nor can a compiler automatically identify and efficiently implement all available parallelisms <ref> [Ble90] </ref>. SEGUL includes message passing as means of activating, synchronizing, and communicating data between parallel processes. 4 This has already been proven useful for coarse-grain user-defined parallelism in knowledge processing programs [A + 87]. <p> Blelloch presents several graph algorithms in which graphs are represented as adjacency lists expressed as a nested tuple [[y 11 ; y 12 :::]; [y 21 ; y 22 ]; :::] with the source-node (x) identification implicit in the outer tuple's ordering <ref> [Ble90] </ref>. 4.3 Processing Requirements Blelloch has shown that the Paralation model with nested, ordered collections can be efficiently implemented on SIMD architectures. This section shows how extension to unordered 73 collections, set operations, and collection matching can also be efficiently implemented on a SIMD architecture. <p> operations. 4.3.1 Numeric and Logic Operations on Nested Collections Blelloch's dissertation demonstrated that the elementwise, scan, permute, insert, extract, expand (distribute), and length operations described above in terms of SEGUL's syntax can be efficiently executed with nested tuples on a generic SIMD array in conjunction with its controller or host <ref> [Ble90, BS90] </ref>. <p> If the number of different elements present is reasonable in proportion to the number of processing elements available, a basis-set representation will support constant-time 2 ; [; "; n; and operations on non-nested collections. In the paralation representation these operations are O (n=P + c lg n) <ref> [Ble90] </ref>, where n is the size of the smaller collection, P is the number of processing elements assigned, and c is the communication time to perform a permutation. The basis-set representation allocates a virtual processing element to each active element; each element has a flag for each active collection. <p> For log-sum and similar reductions using the bypass-mesh requires 3 (lg (N=32) + 5)(m + :5 lg N )=b cycles, assuming a 32 fi 32 mesh. Other options such as Foster's carry shower [Fos76] or Blelloch's scan-tree <ref> [Ble90] </ref> require more cycles under typical conditions. An adder tree would have f ld = 2 lg N , and including chip crossings a reduction over a full PN would require about 8 PE cycle times to do plus and minimum reductions. <p> Stack (local) With address increment/decrement each PE can push and pop at different times, provided the total numbers are equal over a given control structure (see <ref> [Ble90] </ref>). The possible data-structure access modes are multiplied by the option of selecting the resulting address as the upper or lower address bits, offering important opportunities for more complex locally-manipulated data structures. Local addressing is particularly useful in table look-up operations such as token matching distances and low-resolution functions. <p> Data-Dependent Control The associative processing model does not rely entirely on match predicates but also includes the standard declarative programming constructs (if, for, etc.) for supporting complex data-driven control structures. Blelloch's dissertation shows how conditionals can be efficiently applied to nested collections <ref> [Ble90] </ref>, and Willett's parallelization of Ullmann's algorithm demonstrates that recursive backtracking search algorithms can be run with nested parallelism on a SIMD architecture [WWR91]. SEGUL also includes a control structure for iterating through collections. <p> Blelloch's analysis treats scan operations as constant-time, when in fact they are dependent on communication time. Making this correction, his results show that most scan operations execute within the communication-time factor of the lower bound of VLSI and circuit theory models for time, and with asymptotically less area <ref> [Ble90] </ref>. He also compares several important algorithms to the EREW (exclusive-read exclusive-write) and CRCW (concurrent-read concurrent write) PRAM models of concurrent computation.
Reference: [BM90] <author> Jean-Pierre Banatre and Daniel Le Metayer. </author> <title> The Gamma model and its discipline of programming. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 15 </volume> <pages> 55-77, </pages> <year> 1990. </year>
Reference-contexts: Gamma Sections 2.4.3 and 4.1.2 discuss Gamma's "react-act" programming and the MISC associative model's match-act style, showing that the Gamma constructs can be readily translated into SEGUL. By this argument, MISC can execute the programming examples given in the Gamma literature <ref> [BM93, BM90, BCM88] </ref>, listed in Table 8.3. SEGUL supplements this construct with many others, including global information and control structures which remove the Gamma restriction that prohibit its use for crucial algorithms such as backtracking.
Reference: [BM93] <author> Jean-Pierre Banatre and Daniel Le Metayer. </author> <title> Programming by multiset transformation. </title> <journal> Communications of the ACM, </journal> <volume> 36(1) </volume> <pages> 98-111, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Effects Collection Emphasized a APL [Ble90] 1962 serial * * ffi b ASC [P + 94] 1992 `all' * * * c C* [Ble90] SIMD ffi * ffi ffi d CM-LISP [Ble90] 1986 SIMD ffi * * ffi e Cray Fortran [Ble90] 1978 vector ffi * ffi ffi f Gamma <ref> [BM93] </ref> 1986 * ffi ffi * g Linda [CG89] 1982 MIMD ffi h *LISP [Ble90] SIMD ffi * ffi ffi i MPL [Chr90] 1990 SIMD ffi ffi ffi j MultiLISP [Ble90] ffi * * ffi k NIAL [Ble90] 1982 l Occam [Bur88] MIMD * ffi m Paralation [Ble90] 1987 serial, vector, <p> Further development may benefit from consideration of ASC's features. The following languages have already influenced the MISC research. 2.4.3 Gamma The inherently parallel Gamma model is based on multiset transformation and emphasizes matching <ref> [BM93] </ref>. Its developers consider it a methodology for developing parallel programs, and not a practically implementable language [BM93]. All data is treated as a multiset of objects. Programs are specified as a matching condition (R) and action (A) to be performed on the match results. <p> The following languages have already influenced the MISC research. 2.4.3 Gamma The inherently parallel Gamma model is based on multiset transformation and emphasizes matching <ref> [BM93] </ref>. Its developers consider it a methodology for developing parallel programs, and not a practically implementable language [BM93]. All data is treated as a multiset of objects. Programs are specified as a matching condition (R) and action (A) to be performed on the match results. Wherever the R () predicate is true, its objects in the multiset are modified by the A () operator. <p> Banatre and Metayer give other examples, including generating the Fibonacci series and factorials, prime factorization, sorting, longest motonically increasing sequence, majority, graph connectivity, shortest path, minimum spanning tree, convex hull, and image edge-detection <ref> [BM93] </ref>. sieve (n) := (R,A)(2...n) R (x1,x2) = multiple (x1,x2) A (x1,x2) = x2 The Gamma model has two fundamental limitations. First, only local information is available. Problems requiring control decisions based on the overall state of computation cannot be described by the Gamma model. <p> Problems requiring control decisions based on the overall state of computation cannot be described by the Gamma model. Unfortunately, this excludes backtracking and other recursive strategies such as divide-and-conquer; additionally, algorithms such as maximum flow, and productions are extremely difficult to describe in the Gamma formalism <ref> [BM93] </ref>. Second, each object is implicitly compared to all others. <p> Match operations are used to form sets of objects followed by data-parallel operations on the sets so produced. The SDM algorithm epitomizes this style. As Banatre and Metayer point out, however, this "pure" associative model cannot support backtracking and other important algorithms <ref> [BM93] </ref>. This necessitates the ability to deal with global information and non-match-based control structures. Nested Collections and Relational Structures The associative model emphasizes collections such as sets and tuples. <p> The programming model and notation informally draws from a number of collection oriented and parallel programming languages (the paralation model [Ble90], SETL [SDDS86], Gamma <ref> [BM93] </ref>, and Linda [CG89]) and is called SEGUL as a mutilated acronym of these. 2 The notation is primarily that of naive set theory [Dev93], thus incorporating a widely understood formalism and resulting in many similarities with the set- and tuple-oriented language SETL. <p> Gamma Sections 2.4.3 and 4.1.2 discuss Gamma's "react-act" programming and the MISC associative model's match-act style, showing that the Gamma constructs can be readily translated into SEGUL. By this argument, MISC can execute the programming examples given in the Gamma literature <ref> [BM93, BM90, BCM88] </ref>, listed in Table 8.3. SEGUL supplements this construct with many others, including global information and control structures which remove the Gamma restriction that prohibit its use for crucial algorithms such as backtracking.
Reference: [Bon92] <author> David Bondurant. </author> <title> Enhanced dynamic ram. </title> <journal> IEEE Spectrum, </journal> <month> October </month> <year> 1992. </year>
Reference-contexts: Further, Rambus requires a large on-chip area per memory port. EDRAM <ref> [Bon92, Jon92, Har92, Ng92] </ref> extends DRAM with an on-chip SRAM "cache," taking advantage of the wide on-chip DRAM line size. <p> All other memory-chip control lines are global to the array. 8 Other current high-speed memory technologies either have excessive cycle time or suffer from lengthy transfer set-up times. 9 The Ramtrom EDRAM has 15 ns posted write cycle, 15 ns read hit access, and 35 ns read miss access times <ref> [Bon92] </ref>. 160 unit pipelines off-chip memory addressing, fetching the next data word while the current instruction is executing. The complications from row caching are easily handled and memory array details are straightforward. MISC-1 is not a load-store architecture.
Reference: [Bow92] <author> Bruce Bower. </author> <title> Rethinking the mind. </title> <journal> Science News, </journal> <volume> 142:264, </volume> <month> October 10 </month> <year> 1992. </year>
Reference: [Bro91] <author> Chappell Brown. </author> <title> Polyimide gains favor. </title> <journal> Electronic Engineering Times, </journal> <pages> page 110, </pages> <month> May 13 </month> <year> 1991. </year>
Reference: [BS90] <author> Guy E. Blelloch and Gary W. Sabot. </author> <title> Compiling collection-oriented languages onto massively parallel computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 119-134, </pages> <year> 1990. </year>
Reference-contexts: People often have difficulty porting a serial program to effective parallel code, and automatic translation has even more difficulty. On the other hand, recent results suggest that parallel programs can be efficiently executed on serial machines <ref> [BS90] </ref>. Although the 33 dissertation emphasizes algorithm-architecture-technology interactions in the development of the MISC architecture, this section discusses parallel programming languages to provide a background for establishing the programmability of MSIMD architectures and a context for the associative programming model. <p> Although the existing compiler is a partial implementation of the model for the CM-2 (Paralation-LISP), the model is applicable to MIMD and vector-processor architectures as well as to SIMD and serial machines <ref> [BS90] </ref>. The abstract processor model is that of a scalar computer with an attached vector processor, which can be efficiently emulated on other machine architectures. <p> operations. 4.3.1 Numeric and Logic Operations on Nested Collections Blelloch's dissertation demonstrated that the elementwise, scan, permute, insert, extract, expand (distribute), and length operations described above in terms of SEGUL's syntax can be efficiently executed with nested tuples on a generic SIMD array in conjunction with its controller or host <ref> [Ble90, BS90] </ref>. <p> A non-nested collection corresponds to the basic vector or data-parallel model of parallel programming and Blelloch has shown that he multiple levels of parallelism available at each level of nesting can be efficiently implemented on vector, MIMD, and SIMD architectures <ref> [BS90] </ref>. 1 The hierarchical retrieval pseudocode in Section 5.3.4 demonstrates the implicit expression of even more complex nested parallelism, the simultaneous execution of subgraph isomorphism comparison of a set of graphs. <p> Topological sort is important in its use in Levinson's Method III multi-level indexed sort and in "untangling" hierarchies for encoding. Paralation Sections 4.3 and 8.2.2 demonstrate MISC's ability to efficiently incorporate Blelloch and Sabot's paralation model <ref> [BS90] </ref>. Their model has been implemented on the CM-2. MISC includes hardware (the bypass-mesh network) which speeds segmented scans, not present in the CM-2. Chapter 4 also showed various additions to the paralation model, including extension from ordered to both ordered and unordered collections, associative matching, and comparison between collections.
Reference: [BS93] <author> Kenneth Baclawski and J. Elliot Smith. KEYNET: </author> <title> Fast indexing for semantically rich information retrieval. Unpublished, </title> <booktitle> from kenb@ccs.neu.edu, </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: Baclawski notes that relational structures provide semantically richer means of expressing information-retrieval queries, semantic proximity being very difficult even to approximate using attribute lists, but that they have been little used due to the high computational requirements <ref> [BS93] </ref> Relational structures are readily visualized as graphs. Indeed, a graph is formally a set of binary relations, and a hypergraph is a set of n-ary relations. A tremendous range of problems can be expressed as relational structures and thereby solved using graph processing algorithms.
Reference: [Bur88] <author> Alan Burns. </author> <title> Programming in Occam 2. </title> <publisher> Addison-Wesley, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: [Ble90] 1978 vector ffi * ffi ffi f Gamma [BM93] 1986 * ffi ffi * g Linda [CG89] 1982 MIMD ffi h *LISP [Ble90] SIMD ffi * ffi ffi i MPL [Chr90] 1990 SIMD ffi ffi ffi j MultiLISP [Ble90] ffi * * ffi k NIAL [Ble90] 1982 l Occam <ref> [Bur88] </ref> MIMD * ffi m Paralation [Ble90] 1987 serial, vector, ffi SIMD, MIMD n Parallel Pascal [Ble90] ffi * ffi ffi o Pascal-L [FKS85] 1983 SIMD ffi ffi p pSather, dSather [FLR91, Sch92] 1991 MIMD, SIMD ffi ffi * q SETL [SDDS86] 1981 serial r SQL [Ble90] serial (ffi) ffi no
Reference: [Bur92] <author> Dave Bursky. </author> <title> Memory|CPU interface speeds up data transfers. </title> <booktitle> Electronic Design, </booktitle> <pages> pages 137-142, </pages> <month> March 19 </month> <year> 1992. </year>
Reference-contexts: Fast commodity DRAMS have access times of 60-80 ns, so even with interleaving cannot meet the required cycle times. Large SRAMs have similar speeds, and smaller fast SRAMs (15 ns or less access) are over 10 times more expensive per byte than DRAM. Rambus <ref> [Bur92] </ref> has a peak transfer rate of 1 byte per 2 ns, but the 212 ns set-up time gives effective speeds no faster than DRAM except for long contiguous transfers (atypical of the above MISC macros where fetches are typically only 2-32 bytes).
Reference: [CD89] <author> Lawrence Chisvin and R. James Duckworth. </author> <title> Content-addressable and associative memory: Alternatives to the ubiquitous RAM. </title> <journal> Computer, </journal> <volume> 22(7) </volume> <pages> 51-64, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Instead, this section summarizes and compares examples of different architectural classes to provide a context for MISC. The reader is referred to several surveys of specialized database, knowledge base, neural network, CAM, LISP, Prolog, and functional programming machines <ref> [KT88, Kog91, DFM89, CD89, WLL90] </ref>. 2.3.1 Associative Processors Associative processors offer fast highly-parallel matching with low silicon area and minimal communication.
Reference: [CG89] <author> Nicholas Carriero and David Gelernter. </author> <title> Linda in context. </title> <journal> Communications of the ACM, </journal> <volume> 32(4) </volume> <pages> 444-458, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: * * ffi b ASC [P + 94] 1992 `all' * * * c C* [Ble90] SIMD ffi * ffi ffi d CM-LISP [Ble90] 1986 SIMD ffi * * ffi e Cray Fortran [Ble90] 1978 vector ffi * ffi ffi f Gamma [BM93] 1986 * ffi ffi * g Linda <ref> [CG89] </ref> 1982 MIMD ffi h *LISP [Ble90] SIMD ffi * ffi ffi i MPL [Chr90] 1990 SIMD ffi ffi ffi j MultiLISP [Ble90] ffi * * ffi k NIAL [Ble90] 1982 l Occam [Bur88] MIMD * ffi m Paralation [Ble90] 1987 serial, vector, ffi SIMD, MIMD n Parallel Pascal [Ble90] ffi <p> structure confirmed the present author's initial work on the MISC associative processing model, demonstrating the power and expressiveness of a match-set-set operation cycle for a variety of locally-controlled algorithms. 2.4.4 Linda Linda provides mechanisms for adding MIMD parallelism to existing serial languages through the addition of a few simple functions <ref> [CG89] </ref>. One if its strong features is its language and machine independence. Linda extensions have been included in C, C++, Fortran, Scheme, and Modula-2, and there are Linda implementations for a wide range of standard MIMD machines. <p> Free processors can then retrieve unevaluated tuples to begin execution. The logically shared tuple bag simplifies programming, avoids context switching (each processor executes a single process, then retrieves another), and provides scalability and dynamic load balancing <ref> [CG89] </ref>. On the negative side, an implementation must deal with either the intensive communication to a single tuple memory or the cost and time overhead of maintaining coherency in a distributed tuple memory. <p> The programming model and notation informally draws from a number of collection oriented and parallel programming languages (the paralation model [Ble90], SETL [SDDS86], Gamma [BM93], and Linda <ref> [CG89] </ref>) and is called SEGUL as a mutilated acronym of these. 2 The notation is primarily that of naive set theory [Dev93], thus incorporating a widely understood formalism and resulting in many similarities with the set- and tuple-oriented language SETL.
Reference: [CG94] <author> Van-Dat Cung and Lucien Gotte. </author> <title> A first step towards the massively parallel game-tree search: A SIMD approach. </title> <editor> In V. Kumar et al., editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <pages> pages 117-130, </pages> <address> New York, </address> <year> 1994. </year> <journal> North-Holland. </journal> <volume> volume 2. </volume>
Reference-contexts: [Twa94] custom genetic, classifier systems [LH94] custom other [YK94] custom neural net, learning [CGK94] nCUBE2 512 other [HJ94] transputer 512 theorem proving, logic [BKS93] CM-2 1024 other [S + 93c] DADO2 1024 rule-based system [CGK93] nCUBE 1024 search [NM93] CM-2 4096 genetic, classifier systems [Gel94a] CM-2 8192 SN and related <ref> [CG94] </ref> CM-2 8192 search [KK93] CM-2 8192 search [JSS93] MP-1 8192 theorem proving, logic [PKF93] CM-2 16384 search [PW94] CM-2 16384 natural language [WD94] CM-2 16384 neural net, learning [ND94] CM-2 16384 neural net, learning [AHEK94] CM-2 16384 SN and related [EAH94] CM-2 16384 SN and related [JB94] MP-1 16384 vision,
Reference: [CGA94] <author> Chen-Chau Chu, Joydeep Gosh, and J. K. Aggarwal. </author> <title> On supporting rule-based image interpretation using a distributed memory multicomputer. </title> <editor> In V. Kumar et al., editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <pages> pages 21-44, </pages> <address> New York, </address> <year> 1994. </year> <journal> North-Holland. </journal> <volume> volume 1. </volume>
Reference-contexts: constraint satisfaction [Rob93] custom other [HIM94] custom other [LCV94] transputer neural net [OMT94] custom 10 other [SJ94a] transputer 16 theorem proving, logic [Gel93b] CM-5 32 SN and related [CS94] CM-5 32 other [GW94] WARP 32 vision [LP94] iPSC 32 constraint satisfaction [ZM94] transputer 48 constraint satisfaction [MNT94] AP1000 64 other <ref> [CGA94] </ref> Pixel 64 rule-based system, vision [SS94b] many 64 theorem proving [NW93] PIM 70 theorem proving, logic [Ert93] LAN 110 search [Pea94] Fujitsu 128 theorem proving, logic [CMD94] SNAP 144 SN, natural language [Sat94] nCUBE3 256 natural language Table 2.5: Sample of Recent Parallel AI Publications 43 Paper Machine PEs Category
Reference: [CGB91] <author> David R. Cheriton, Hendrik A. Goosen, and Patrick D. Boyle. </author> <title> Paradigm: A highly scalable shared-memory multicomputer architecture. </title> <journal> Computer, </journal> <volume> 24(2) </volume> <pages> 33-45, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: PASM is a research platform for AI applications, including vision, speech understanding, and expert systems [MSS85]. PASM has SIMD and MIMD operating modes in a partitionable topology scalable to a thousand processors. Paradigm and the much older cm* feature hierarchical communication networks and multi-level caching <ref> [CGB91, SFS77] </ref>. WASP, currently under development, was originally intended for image and signal processing. 31 Target applications now include supporting expert systems, LISP, and Prolog with low level matching operations [LJ91]. Array modules can operate independently as SIMD or in groups as MIMD. <p> 1-d mesh 470 108 MOPS NAP [KF90] 1990 SIMD, various simulated SNAP [M + 92] 1990 MSIMD, modified hypercube 144 CM-5 [Thi92] 1991 MSIMD, fat-tree, vector 1 K 128,000 MFLOPS Paragon [Hwa93] 1991 MIMD, 2-d mesh 300,000 MFLOPS IXM2 [H + 91] 1991 pseudo MIMD, hierarchical 64 278 MIPS Paradigm <ref> [CGB91] </ref> 1991 MIMD, hierarchical 100+ Coherent [Sto91] 1992 SIMD, CAM 4 K MasPar MP-2 [Mas92a] 1992 SIMD, 2-d mesh, MIN 16 K 68,000 MIPS Cray T3D [Oed93] 1993 MIMD, 3-d mesh 2 K 300,000 MFLOPS WASP [LJ91] devel MSIMD, bypass ring 64 K 40,000 MIPS Table 2.2: Sample of Other Parallel <p> For a truly scalable system with no communication slowdown as the system grows, advantage must be taken of communication locality. For all communications patterns with a planar topology, the required communication bisection width is O (N 1=2 ) <ref> [CGB91] </ref>, a value that shows strong communication locality and permits operation within the bounds of planar packaging. AI codes may not be so fortunate, with data-dependent "random" communication patterns. There has been little empirical work in this area. <p> Cheriton et al. claim that communication clustering occurs in many applications, particularly simulations and partial-differential equation solvers <ref> [CGB91] </ref>. For 2-dimensional problems region crossing analysis also shows that processor load increases quadratically with region size while communication increases only linearly [CGB91]. This gives the O ( p N ) communication scaling typical of engineering and scientific codes. Global Comm. Internal External Internal to Example M bytes/s Processing Comm. <p> Cheriton et al. claim that communication clustering occurs in many applications, particularly simulations and partial-differential equation solvers <ref> [CGB91] </ref>. For 2-dimensional problems region crossing analysis also shows that processor load increases quadratically with region size while communication increases only linearly [CGB91]. This gives the O ( p N ) communication scaling typical of engineering and scientific codes. Global Comm. Internal External Internal to Example M bytes/s Processing Comm.
Reference: [CGK93] <author> Daniel J. Challou, Maria Gini, and Vipin Kumar. </author> <title> Parallel search algorithms for robot motion planning. </title> <editor> In James Geller, editor, </editor> <booktitle> Innovative Applications of Massive Parallelism, </booktitle> <pages> pages 40-47. </pages> <publisher> AAAI, </publisher> <year> 1993. </year> <booktitle> Working Notes, Spring Symposium Series, </booktitle> <publisher> Stanford University. </publisher>
Reference-contexts: IXM2 SN, natural language [O + 94] IXM2 SN and related [Fah93] NETL SN and related [Twa94] custom genetic, classifier systems [LH94] custom other [YK94] custom neural net, learning [CGK94] nCUBE2 512 other [HJ94] transputer 512 theorem proving, logic [BKS93] CM-2 1024 other [S + 93c] DADO2 1024 rule-based system <ref> [CGK93] </ref> nCUBE 1024 search [NM93] CM-2 4096 genetic, classifier systems [Gel94a] CM-2 8192 SN and related [CG94] CM-2 8192 search [KK93] CM-2 8192 search [JSS93] MP-1 8192 theorem proving, logic [PKF93] CM-2 16384 search [PW94] CM-2 16384 natural language [WD94] CM-2 16384 neural net, learning [ND94] CM-2 16384 neural net, learning
Reference: [CGK94] <author> Daniel J. Challou, Maria Gini, and Vipin Kumar. </author> <title> Toward real-time motion planning. </title> <editor> In V. Kumar et al., editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <pages> pages 163-176, </pages> <address> New York, </address> <year> 1994. </year> <journal> North-Holland. </journal> <volume> volume 2. </volume>
Reference-contexts: [Kur94] theorem proving, logic [DN94] CM-2 vision, search [EAH93] CM-2 SN and related [Lan93] CM-2 SN, natural language [H + 94] IXM2 SN, natural language [O + 94] IXM2 SN and related [Fah93] NETL SN and related [Twa94] custom genetic, classifier systems [LH94] custom other [YK94] custom neural net, learning <ref> [CGK94] </ref> nCUBE2 512 other [HJ94] transputer 512 theorem proving, logic [BKS93] CM-2 1024 other [S + 93c] DADO2 1024 rule-based system [CGK93] nCUBE 1024 search [NM93] CM-2 4096 genetic, classifier systems [Gel94a] CM-2 8192 SN and related [CG94] CM-2 8192 search [KK93] CM-2 8192 search [JSS93] MP-1 8192 theorem proving, logic
Reference: [CH67] <author> T. M. Cover and P. E. Hart. </author> <title> Nearest neighbor pattern classification. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-13(1):21-27, </volume> <month> January </month> <year> 1967. </year>
Reference-contexts: Hart show that nearest-neighbors classifiers make no more than twice the number of classification errors of a Bayes classifier (which uses the statistics of all observed points as well as a priori expectations of the occurrence of class members) for large samples of pre-classified points, irrespective of the underlying distributions <ref> [CH67] </ref>. For low-dimensional inputs, a k-d tree data structure permits locating the nearest neighbors in less time than exhaustive search.
Reference: [Chr90] <author> Peter Christy. </author> <title> Software to support massively parallel computing on the MasPar MP-1. </title> <booktitle> In Proceedings of CompCon, </booktitle> <pages> pages 29-33. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1990. </year> <month> 244 </month>
Reference-contexts: C* [Ble90] SIMD ffi * ffi ffi d CM-LISP [Ble90] 1986 SIMD ffi * * ffi e Cray Fortran [Ble90] 1978 vector ffi * ffi ffi f Gamma [BM93] 1986 * ffi ffi * g Linda [CG89] 1982 MIMD ffi h *LISP [Ble90] SIMD ffi * ffi ffi i MPL <ref> [Chr90] </ref> 1990 SIMD ffi ffi ffi j MultiLISP [Ble90] ffi * * ffi k NIAL [Ble90] 1982 l Occam [Bur88] MIMD * ffi m Paralation [Ble90] 1987 serial, vector, ffi SIMD, MIMD n Parallel Pascal [Ble90] ffi * ffi ffi o Pascal-L [FKS85] 1983 SIMD ffi ffi p pSather, dSather [FLR91,
Reference: [CK80] <author> D. G. Corneil and D. G. Kirkpatrick. </author> <title> A theoretical analysis of various heuristics for the graph isomorphism problem. </title> <journal> Siam Journal on Computing, </journal> <volume> 9(2), </volume> <month> May </month> <year> 1980. </year>
Reference-contexts: Fortunately, the expected case time is empirically polynomial [Ull76, WWR91]. The worst-case time results from generally pathological cases that rarely occur in practice <ref> [Bar88, CK80, Sus65] </ref> so that parallelization can provide a difference in tractable graph size. Further, graph size is bounded for most applications.
Reference: [CLR90] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1990. </year>
Reference-contexts: An n-node graph results in an n fi n adjacency matrix, and a single matrix multiplication requires O (n 3 ) time using the simplest algorithm on a sequential machine. The best known sequential matrix multiplication algorithm requires O (n 2:376 ), but with higher constant factors <ref> [CLR90] </ref>. The Floyd-Washall Algorithm provides significant improvement. Floyd-Washall is an incremental dynamic-programming formulation of the all pairs shortest path graph problem, which can be modified to compute the transitive closure of the graph in total time O (n 3 ) [CLR90]. <p> requires O (n 2:376 ), but with higher constant factors <ref> [CLR90] </ref>. The Floyd-Washall Algorithm provides significant improvement. Floyd-Washall is an incremental dynamic-programming formulation of the all pairs shortest path graph problem, which can be modified to compute the transitive closure of the graph in total time O (n 3 ) [CLR90]. The inner loop of the algorithm can be parallelized using n PEs, giving time O (n 2 ). 12 Additionally, the inner loop requires only one row of the matrix and a global scalar. <p> For concept-type subsumption tests, the sparse connectivity matrix can be converted into a hash table <ref> [CLR90] </ref>. Proof 1. A balanced binary tree of N nodes has O (lg N ) levels. Number them 0::: lg N . 2.
Reference: [CM87] <author> Nick Cercone and Gordon McCalla. </author> <title> What is knowledge representation. </title> <booktitle> In The Knowledge Frontier, chapter 1, </booktitle> <pages> pages 1-43. </pages> <address> Springler-Verlag, New York, </address> <year> 1987. </year>
Reference-contexts: Inheritance and various inference techniques are supported [DM93]. Extensive research over the last two decades has demonstrated the expressive power of semantic networks and their similarity to human associative recall and analogic reasoning <ref> [CM87] </ref>. Semantic networks are equivalent in representational power to first order predicate calculus, but their knowledge representation can greatly reduce both computation and storage requirements. Semantic networks have been particularly successful in research relating to end-user areas such as natural language understanding [DM93]. Semantic networks operate by marker propagation.
Reference: [CMD94] <author> Sang-Hwa Chung, Dan I. Moldovan, and Ronald F. DeMara. </author> <title> A parallel computational model for integrated speech and natural language understanding. </title> <editor> In Hiroaki Kitano and James A. Hendler, editors, </editor> <booktitle> Massively Parallel Artificial Intelligence, </booktitle> <pages> pages 138-170, </pages> <address> Menlo Park, CA, 1994. </address> <publisher> AAAI Press / The MIT Press. </publisher>
Reference-contexts: related [CS94] CM-5 32 other [GW94] WARP 32 vision [LP94] iPSC 32 constraint satisfaction [ZM94] transputer 48 constraint satisfaction [MNT94] AP1000 64 other [CGA94] Pixel 64 rule-based system, vision [SS94b] many 64 theorem proving [NW93] PIM 70 theorem proving, logic [Ert93] LAN 110 search [Pea94] Fujitsu 128 theorem proving, logic <ref> [CMD94] </ref> SNAP 144 SN, natural language [Sat94] nCUBE3 256 natural language Table 2.5: Sample of Recent Parallel AI Publications 43 Paper Machine PEs Category [Kur93] theorem proving, logic [JS94] vision [Gel94b] SN and related [DB93] other [Kur94] theorem proving, logic [DN94] CM-2 vision, search [EAH93] CM-2 SN and related [Lan93] CM-2
Reference: [Coo93] <author> Diane J. Cook. </author> <title> Fast information distribution for massively parallel IDA* search. </title> <editor> In James Geller, editor, </editor> <booktitle> Innovative Applications of Massive Parallelism, </booktitle> <pages> pages 48-53. </pages> <publisher> AAAI, </publisher> <year> 1993. </year> <booktitle> Working Notes, Spring Symposium Series, </booktitle> <publisher> Stanford University. </publisher>
Reference-contexts: Cost and Salzberg recently developed a symbolic-distance measure for a nearest-neighbors classifier, with good results in predicting protein secondary structure and identifying DNA promoter sequences [CS93]. Miscellaneous More traditional AI algorithms, such as IDA*, unstructured tree search, and general tree-recursive algorithms map well to SIMD and MSIMD architectures <ref> [Coo93, Kum93] </ref>, as do branch and bound and other state-space search methods. <p> MP-1 8192 theorem proving, logic [PKF93] CM-2 16384 search [PW94] CM-2 16384 natural language [WD94] CM-2 16384 neural net, learning [ND94] CM-2 16384 neural net, learning [AHEK94] CM-2 16384 SN and related [EAH94] CM-2 16384 SN and related [JB94] MP-1 16384 vision, neural net [ZMHV93] MP-1 16384 neural net, learning <ref> [Coo93] </ref> CM-2 32768 search Table 2.6: Sample of Recent Massively Parallel AI Publications Prior work emphasizes a relatively small number of algorithms, with generally disappointing results for logic programming and expert systems.
Reference: [Cor95] <institution> Digi-Key Corp. Distributor's catalog. Thief River Falls, MN, </institution> <address> 951 edition, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: The cost analysis is conservative. Custom 132-pin PGA packages are currently $0.26 per pin [ISI93], and 447-pin PGA sockets are under $0.04 per pin <ref> [Cor95] </ref>. At $0.04 per pin, sharing a memory between two PEs has a 35% cost-performance penalty (the difference is not greater as nearly half the cost is in silicon and a large number of pins are needed for control and communication).
Reference: [CR94] <author> Alok Choudhary and Sanjay Ranka. </author> <title> A perspective on parallel processing in computer vision and image understanding. </title> <editor> In V. Kumar et al., editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <pages> pages 3-20, </pages> <address> New York, </address> <year> 1994. </year> <journal> North-Holland. </journal> <volume> volume 1. </volume>
Reference-contexts: Semantic networks (particularly Fahlman [Fah85]) and neural networks (particularly Hopfield, Rumelhart, and Hinton [Hop82, RHW86]) were especially influential in reviving work on massively parallel AI. 42 Paper Machine PEs Category [SJ94b] search [ACP93] search <ref> [CR94] </ref> vision [Hen94] search [HJ93] constraint satisfaction [FFG94] other [SLH94] other [Pin94] neural net, logic [Ass94] theorem proving, logic [SS94a] rule-based system [Sha93] neural net [AG93] search [Kum93] SN and related [AB93] neural net [WG93] CM-5 other [LHB93] Symmetry constraint satisfaction [Rob93] custom other [HIM94] custom other [LCV94] transputer neural net
Reference: [CS93] <author> Scott Cost and Steven Salzberg. </author> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10(1) </volume> <pages> 57-78, </pages> <year> 1993. </year>
Reference-contexts: Cost and Salzberg recently developed a symbolic-distance measure for a nearest-neighbors classifier, with good results in predicting protein secondary structure and identifying DNA promoter sequences <ref> [CS93] </ref>. Miscellaneous More traditional AI algorithms, such as IDA*, unstructured tree search, and general tree-recursive algorithms map well to SIMD and MSIMD architectures [Coo93, Kum93], as do branch and bound and other state-space search methods.
Reference: [CS94] <author> Kevin J. Cherkauer and Jude W. Shavlik. </author> <title> Selecting salient features for machine learning from large candidate pools through parallel decision-tree construction. </title> <editor> In Hiroaki Kitano and James A. Hendler, editors, </editor> <booktitle> Massively Parallel Artificial Intelligence, </booktitle> <pages> pages 102-137, </pages> <address> Menlo Park, CA, 1994. </address> <publisher> AAAI Press / The MIT Press. </publisher>
Reference-contexts: proving, logic [SS94a] rule-based system [Sha93] neural net [AG93] search [Kum93] SN and related [AB93] neural net [WG93] CM-5 other [LHB93] Symmetry constraint satisfaction [Rob93] custom other [HIM94] custom other [LCV94] transputer neural net [OMT94] custom 10 other [SJ94a] transputer 16 theorem proving, logic [Gel93b] CM-5 32 SN and related <ref> [CS94] </ref> CM-5 32 other [GW94] WARP 32 vision [LP94] iPSC 32 constraint satisfaction [ZM94] transputer 48 constraint satisfaction [MNT94] AP1000 64 other [CGA94] Pixel 64 rule-based system, vision [SS94b] many 64 theorem proving [NW93] PIM 70 theorem proving, logic [Ert93] LAN 110 search [Pea94] Fujitsu 128 theorem proving, logic [CMD94] SNAP
Reference: [Dal90] <author> William J. Dally. </author> <title> Performance analysis of k-ary n-cube interconnection networks. </title> <editor> In Patrick Henry Winston and Sara Alexandra Shellard, editors, </editor> <booktitle> Artificial Intelligence at MIT: Expanding Frontiers, </booktitle> <pages> pages 521-547. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: indicate that providing an external memory interface for every one PE is worthwhile even with high pincost, requiring and justifying very high pin count packaging. 6.2.3 Array Communication The most common communication topologies for SIMD machines are hypercubes and 2-dimensional meshes, opposing ends of a class of k-ary n-cube networks <ref> [Dal90] </ref>. Here k indicates the number of processors along each dimension, and n indicates the dimensionality . A 2-d mesh is a p P -ary 2-cube and a hypercube is a binary lg P -cube. <p> have a network diameter O (P 1=n ) versus O (lg P ) for the hypercube, Dally shows that for a given amount of wiring resources and constant bisection bandwidth, low-dimensional k-ary n-cubes have lower latency, less contention, and higher hot-spot throughput than high-dimensional ones due to greater resource sharing <ref> [Dal90] </ref>. 124 Mesh The 2-d mesh also has several advantages for implementation in planar substrates. Meshes have the advantage of a constant, small number of ports per processor and only nearest neighbor wiring. This decreases network interface costs and greatly reduces interconnect costs.
Reference: [DB93] <author> Rumi M. Dubash and Farokh B. Bastani. </author> <title> Decentralized massively parallel path planning and its application to process control and multi robot systems. </title> <editor> In James Geller, editor, </editor> <booktitle> Innovative Applications of Massive Parallelism, </booktitle> <pages> pages 54-61. </pages> <publisher> AAAI, </publisher> <year> 1993. </year> <booktitle> Working Notes, Spring Symposium Series, </booktitle> <publisher> Stanford University. </publisher>
Reference-contexts: [NW93] PIM 70 theorem proving, logic [Ert93] LAN 110 search [Pea94] Fujitsu 128 theorem proving, logic [CMD94] SNAP 144 SN, natural language [Sat94] nCUBE3 256 natural language Table 2.5: Sample of Recent Parallel AI Publications 43 Paper Machine PEs Category [Kur93] theorem proving, logic [JS94] vision [Gel94b] SN and related <ref> [DB93] </ref> other [Kur94] theorem proving, logic [DN94] CM-2 vision, search [EAH93] CM-2 SN and related [Lan93] CM-2 SN, natural language [H + 94] IXM2 SN, natural language [O + 94] IXM2 SN and related [Fah93] NETL SN and related [Twa94] custom genetic, classifier systems [LH94] custom other [YK94] custom neural net,
Reference: [DeM92] <author> Ronald F. DeMara. </author> <title> Parallelism, design, and performance of a marker-propagation reasoning system. </title> <type> Technical Report PKPL 92-9, </type> <institution> Department of Electrical Engineering Systems, USC, </institution> <year> 1992. </year>
Reference-contexts: In the light of the link bandwidth required to avoid congestion, the latter is not realistic despite the hierarchical approach. SNAP SNAP combines CAM and additional processing to support the marker-passing SN paradigm <ref> [DeM92] </ref>. The initial design and long-range plans include fully custom processors combining CAM (for relation memory) with control, communication, marker, and ALU units [MLL90]. The current prototype implements these functions with several standard digital signal processing chips and multi-port memories. <p> DeMara proposes a similar model but with a separate term to adjust for marker fan-in <ref> [DeM92] </ref>. The MISC model is necessary to project SN behavior for networks much larger than those implemented to date. Consider two extremes. <p> Let f be the portion of fanout propagation cycles. Substitution into the above gives E a = log A N+1 (A 1) f Surveying several semantic networks, network arity A appears to asymptotically approach 4 for large networks, except for certain natural-language processing approaches <ref> [DM93, DeM92] </ref>. Propagation is usually restricted to certain types (labels) of edges and some node types "absorb" markers and do not further propagate. We fit parameters in the above equation to the two largest semantic networks for which we have the appropriate information.
Reference: [Dev93] <author> Keith Devlin. </author> <title> The Joy of Sets. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: This necessitates the ability to deal with global information and non-match-based control structures. Nested Collections and Relational Structures The associative model emphasizes collections such as sets and tuples. The concept of a set as "the ability to regard a collection of objects as a single entity" <ref> [Dev93] </ref> is fundamental to both abstraction in knowledge processing (problem conceptualization) and the inherent parallelism in collection-oriented programming languages (problem implementation). The model uses nested collections, such as sets of tuples, to represent relational structures. <p> The programming model and notation informally draws from a number of collection oriented and parallel programming languages (the paralation model [Ble90], SETL [SDDS86], Gamma [BM93], and Linda [CG89]) and is called SEGUL as a mutilated acronym of these. 2 The notation is primarily that of naive set theory <ref> [Dev93] </ref>, thus incorporating a widely understood formalism and resulting in many similarities with the set- and tuple-oriented language SETL. Low-level parallelism is implicit by treating manipulations of set-like collections as a single operation, and multi-level parallelism is implicit in nested collections as in the paralation model.
Reference: [DFM89] <author> Jose G. Delgado-Frias and Will R. Moore. </author> <booktitle> VLSI for Artificial Intelligence. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, Mass., </address> <year> 1989. </year>
Reference-contexts: Instead, this section summarizes and compares examples of different architectural classes to provide a context for MISC. The reader is referred to several surveys of specialized database, knowledge base, neural network, CAM, LISP, Prolog, and functional programming machines <ref> [KT88, Kog91, DFM89, CD89, WLL90] </ref>. 2.3.1 Associative Processors Associative processors offer fast highly-parallel matching with low silicon area and minimal communication.
Reference: [DM93] <author> R. F. DeMara and D. I. Moldovan. </author> <title> The SNAP-1 parallel AI prototype. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(8) </volume> <pages> 841-854, </pages> <month> August </month> <year> 1993. </year> <month> 245 </month>
Reference-contexts: As shown in Figure 2.3, a semantic network (SN) is a 17 graph in which nodes correspond to objects, facts, concepts, or attributes and edges represent typed relationships, making structural knowledge explicit. Inheritance and various inference techniques are supported <ref> [DM93] </ref>. Extensive research over the last two decades has demonstrated the expressive power of semantic networks and their similarity to human associative recall and analogic reasoning [CM87]. <p> Semantic networks are equivalent in representational power to first order predicate calculus, but their knowledge representation can greatly reduce both computation and storage requirements. Semantic networks have been particularly successful in research relating to end-user areas such as natural language understanding <ref> [DM93] </ref>. Semantic networks operate by marker propagation. Operation is typically in three phases. The first activates nodes via various matching operations. The second propagates flags (or values such as probabilities or confidence levels) along the edges of the graph. <p> Let f be the portion of fanout propagation cycles. Substitution into the above gives E a = log A N+1 (A 1) f Surveying several semantic networks, network arity A appears to asymptotically approach 4 for large networks, except for certain natural-language processing approaches <ref> [DM93, DeM92] </ref>. Propagation is usually restricted to certain types (labels) of edges and some node types "absorb" markers and do not further propagate. We fit parameters in the above equation to the two largest semantic networks for which we have the appropriate information. <p> Working backwards, this gives an effective arity of 1.62, suggesting that markers propagate on slightly half the network links. For a SNAP application with N = 32; 000, the average number of markers per cycle is 100 <ref> [DM93] </ref>. Substituting into the above equation with effective arity A = 1:62 gives f = 1:68, which includes the effect of starting and finishing with more than 1 node as well as the relative number of fanin versus fanout cycles.
Reference: [DN94] <author> Larry Davis and P. J. Narayanan. </author> <title> Massively parallel search for the interpretation of aerial images. </title> <editor> In Hiroaki Kitano and James A. Hendler, editors, </editor> <booktitle> Massively Parallel Artificial Intelligence, </booktitle> <pages> pages 246-279, </pages> <address> Menlo Park, CA, 1994. </address> <publisher> AAAI Press / The MIT Press. </publisher>
Reference-contexts: [Ert93] LAN 110 search [Pea94] Fujitsu 128 theorem proving, logic [CMD94] SNAP 144 SN, natural language [Sat94] nCUBE3 256 natural language Table 2.5: Sample of Recent Parallel AI Publications 43 Paper Machine PEs Category [Kur93] theorem proving, logic [JS94] vision [Gel94b] SN and related [DB93] other [Kur94] theorem proving, logic <ref> [DN94] </ref> CM-2 vision, search [EAH93] CM-2 SN and related [Lan93] CM-2 SN, natural language [H + 94] IXM2 SN, natural language [O + 94] IXM2 SN and related [Fah93] NETL SN and related [Twa94] custom genetic, classifier systems [LH94] custom other [YK94] custom neural net, learning [CGK94] nCUBE2 512 other [HJ94]
Reference: [EAH93] <author> Matthew P. Evett, William A. Anderson, and James A. Hendler. </author> <title> Massively parallel support for computationally effective recognition queries. </title> <editor> In James Geller, editor, </editor> <booktitle> Innovative Applications of Massive Parallelism, </booktitle> <pages> pages 70-84. </pages> <publisher> AAAI, </publisher> <year> 1993. </year> <booktitle> Working Notes, Spring Symposium Series, </booktitle> <publisher> Stanford University. </publisher>
Reference-contexts: [Pea94] Fujitsu 128 theorem proving, logic [CMD94] SNAP 144 SN, natural language [Sat94] nCUBE3 256 natural language Table 2.5: Sample of Recent Parallel AI Publications 43 Paper Machine PEs Category [Kur93] theorem proving, logic [JS94] vision [Gel94b] SN and related [DB93] other [Kur94] theorem proving, logic [DN94] CM-2 vision, search <ref> [EAH93] </ref> CM-2 SN and related [Lan93] CM-2 SN, natural language [H + 94] IXM2 SN, natural language [O + 94] IXM2 SN and related [Fah93] NETL SN and related [Twa94] custom genetic, classifier systems [LH94] custom other [YK94] custom neural net, learning [CGK94] nCUBE2 512 other [HJ94] transputer 512 theorem proving,
Reference: [EAH94] <author> M. P. Evett, W. A. Anderson, and J. A. Hendler. </author> <title> Providing computationally effective knowledge representation via massive parallelism. </title> <editor> In V. Kumar et al., editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <pages> pages 115-138, </pages> <address> New York, </address> <year> 1994. </year> <journal> North-Holland. </journal> <volume> volume 1. </volume>
Reference-contexts: genetic, classifier systems [Gel94a] CM-2 8192 SN and related [CG94] CM-2 8192 search [KK93] CM-2 8192 search [JSS93] MP-1 8192 theorem proving, logic [PKF93] CM-2 16384 search [PW94] CM-2 16384 natural language [WD94] CM-2 16384 neural net, learning [ND94] CM-2 16384 neural net, learning [AHEK94] CM-2 16384 SN and related <ref> [EAH94] </ref> CM-2 16384 SN and related [JB94] MP-1 16384 vision, neural net [ZMHV93] MP-1 16384 neural net, learning [Coo93] CM-2 32768 search Table 2.6: Sample of Recent Massively Parallel AI Publications Prior work emphasizes a relatively small number of algorithms, with generally disappointing results for logic programming and expert systems. <p> Semantic Networks and Structure Matching PARKA is a knowledge representation and inferencing system combining the features of frames and semantic networks <ref> [EH90, EAH94] </ref>. Hendler et al. present an algorithm for finding subgraph isomorphisms in the large "semantic network" graph given a query graph in linear time [Kit94]. <p> Hendler et al. report queries taking from 0.5 to 1.6 s on a 16 K processor CM-2. They also more restricted structure matching by DeMara on SNAP <ref> [EAH94] </ref> and by Higuchi on the IXM2 [AHEK94]. Memory-Based Reasoning In essence, memory-based reasoning stores a large body of knowledge and experience and deals with new situations by retrieving and processing old situations, making the knowledge-rule tradeoff clearly on the side of knowledge.
Reference: [EH90] <author> Matthew P. Evett and James Hendler. PARKA: </author> <title> Frame-based property inheritance on the Connection Machine. </title> <booktitle> In Parallel Computing 89, </booktitle> <pages> pages 381-386, </pages> <year> 1990. </year>
Reference-contexts: Semantic Networks and Structure Matching PARKA is a knowledge representation and inferencing system combining the features of frames and semantic networks <ref> [EH90, EAH94] </ref>. Hendler et al. present an algorithm for finding subgraph isomorphisms in the large "semantic network" graph given a query graph in linear time [Kit94].
Reference: [EL92] <author> Gerard Ellis and Robert Levinson, </author> <title> editors. </title> <booktitle> Proceedings of the First International Workshop on PEIRCE: A Conceptual Graphs Workbench. </booktitle> <institution> Department of Computer Science, The University of Queensland, </institution> <year> 1992. </year>
Reference-contexts: to show hierarchies of containment and overlap denoting inheritance of commonalities, important in knowledge representation. 6 Graph and Set Based Inference and Retrieval In the late 19 th century, Charles Peirce developed a somewhat similar notation, existential graphs, for graphically notating symbolic logic with an expressiveness equivalent to first-order logic <ref> [EL92] </ref>. This work has recently gained renewed interest ([Sow84]) as an alternative to Russell and Whitehead's linear notation. In a survey book, Kurzweil describes knowledge as facts and their relationships and discusses the importance of inheritance, the characteristics of a set applying to its subsets, in knowledge processing [Kur90]. <p> techniques such as multi-level indexed search and hierarchy coding are needed to manage graphs and similar complex objects [ELR95]. 21 The CG workbench PEIRCE is the result of a significant group effort to provide a common toolkit for research and applications in CGs and related graph-based retrieval and knowledge processing <ref> [EL92] </ref>. Organized in 1992, the PEIRCE group includes over 80 researchers with subgroups covering topics such as programming standards, storage and retrieval, parallel hardware, linear and graphic notation, conceptual catalogs, constraint programming, inference and theorem proving, learning, natural language, and vision.
Reference: [Ell92] <author> Gerard Ellis. </author> <title> Compiling conceptual graphs. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <note> 1992. to appear. </note>
Reference-contexts: The multiple hierarchies support multiple views of the data, offering the benefits of SN, relational database, and CG systems as appropriate for varying types of queries. CGs are stored in a compacted relation-based form speeding subgraph isomorphism testing, with most subgraph matchings pre-compiled <ref> [Ell92] </ref>, further speeding matching and retrieval. The dependency on the extrinsic representations (i.e. which structures and sub-structures are stored) is a current difficulty. UDS is currently being tested on specific problem-search domains as well as in the PEIRCE workbench.
Reference: [Ell93] <author> Gerard Ellis. </author> <title> Efficient retrieval from hierarchies of objects using lattice operations. </title> <booktitle> In Conceptual Graphs for Knowledge Representations, </booktitle> <pages> pages 274-293, </pages> <address> New York, 1993. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: These codes are N bits long, where N is the number of objects in the graph, for total storage O (N 2 ). More efficient coding schemes use don't care states and reuse bit-patterns to reduce storage to O (N lg N ) <ref> [Ell93] </ref>. The lattice codes can be used to test subsumption and find greatest-lower and least-upper bounds in constant time. Other operations, such as marking sub-hierarchies and determining the intersection of sub-hierarchies, can be done in constant time on a parallel processor. <p> Ellis has shown that these operations can in turn be used to further prune graph comparisons, provided the "query" graph is known to be in the knowledge base, as described in detail in <ref> [Ell93] </ref>. Otherwise poset codes can prune the Method III search by considering only those graphs in the intersection of known predecessors of the query graph [ELR95]. On a parallel machine, a lattice code can be simultaneously compared against numerous others.
Reference: [ELR95] <author> Gerard Ellis, Robert A. Levinson, and Peter J. Robinson. </author> <title> Managing complex objects in peirce. </title> <journal> International Journal on Man-Machine Studies, </journal> <note> 1995. To appear. </note>
Reference-contexts: Many common representations such as type hierarchies, entity-relationship diagrams, parse trees, dataflow graphs, flow charts, state-transition diagrams, and petri nets can be considered special cases of CGs, and other knowledge such as chemical formulas and chess positions are readily expressible in CGs <ref> [ELR95] </ref>. CGs represent knowledge as a set of small graphs (i.e. semantic networks) with a standard mapping to both logic and natural language [Sow84] in a fully detailed formalism. This knowledge representation is particularly demanding in its need for match and search operations involving relational and recursively defined patterns. <p> The subsumption ordering relation allows quick access to generalizations, directly useful in case-based reasoning (reasoning by analogy), as well as to specializations, useful as a basis for machine learning <ref> [ELR95] </ref>. <p> As these algorithms are time-consuming, special techniques such as multi-level indexed search and hierarchy coding are needed to manage graphs and similar complex objects <ref> [ELR95] </ref>. 21 The CG workbench PEIRCE is the result of a significant group effort to provide a common toolkit for research and applications in CGs and related graph-based retrieval and knowledge processing [EL92]. <p> Relational structures directly represent logical, structural, and causal relationships among objects and attributes, "pre-computing" connections and supporting operations such as generalization at a higher level of abstraction. Graph retrieval from a partial order is itself classification <ref> [ELR95] </ref>. Recent work by Levinson and Karplus demonstrates that state-space search can be represented and efficiently executed within this framework by representing the current state, the goal state, and the operators as graphs [Lev93, LK93]. <p> A relational structure KB can thus form the basis of classification, inferencing, reasoning, and planning systems. By replacing Prolog's terms with recursive labeled hypergraphs (CGs) such a KB becomes a generalization of Prolog and provides sub-linear indexing for first order logic formulas, independent of the knowledge representation or domain <ref> [ELR95] </ref>. <p> As shown in also be described as a subset of nodes. Graphs are an intuitive representation of complex objects, and can exploit ordering information in retrieval and other processing <ref> [ELR95] </ref>. Nested collections flexibly represent complex relational structures, as well as more conventional data structures such arrays and lists, using standard mathematical notation. Semantic networks are formally directed, labeled hypergraphs [Lev94b]. Both a directed hypergraph and a RDB relation instance correspond to a set of n-tuples. <p> Otherwise poset codes can prune the Method III search by considering only those graphs in the intersection of known predecessors of the query graph <ref> [ELR95] </ref>. On a parallel machine, a lattice code can be simultaneously compared against numerous others. <p> Ellis presents a lattice encoding method using "match-all-below" variables and "constants" <ref> [ELR95] </ref>. Through the match-all and re-use of constants, code lengths are empirically of length O (lg N ). Operations Using Hierarchy Codes For transitive closure codes, operations such as greatest-lower-bound (GLB) and least-upper-bound (LUB) are simply logical-and or logical-or operations between the codes, provided the GLB or LUB exist. <p> The number of subgraph isomorphism tests per query is taken as 1:3 lg 2 N (the constant factor empirical over a test database <ref> [ELR95] </ref>), so that the total number of SI tests is 1:3 ( i=1 As Ullmann's algorithm is empirically O (n 4 ) and refinement alone is O (n 3 ) bit operations, the analysis assumes n calls to refinement for each SI test.
Reference: [Ert93] <author> Wolfgang Ertel. </author> <title> Massively parallel search with random competetion. </title> <editor> In James Geller, editor, </editor> <booktitle> Innovative Applications of Massive Parallelism, </booktitle> <pages> pages 62-69. </pages> <publisher> AAAI, </publisher> <year> 1993. </year> <booktitle> Working Notes, Spring Symposium Series, </booktitle> <publisher> Stanford University. </publisher>
Reference-contexts: transputer 16 theorem proving, logic [Gel93b] CM-5 32 SN and related [CS94] CM-5 32 other [GW94] WARP 32 vision [LP94] iPSC 32 constraint satisfaction [ZM94] transputer 48 constraint satisfaction [MNT94] AP1000 64 other [CGA94] Pixel 64 rule-based system, vision [SS94b] many 64 theorem proving [NW93] PIM 70 theorem proving, logic <ref> [Ert93] </ref> LAN 110 search [Pea94] Fujitsu 128 theorem proving, logic [CMD94] SNAP 144 SN, natural language [Sat94] nCUBE3 256 natural language Table 2.5: Sample of Recent Parallel AI Publications 43 Paper Machine PEs Category [Kur93] theorem proving, logic [JS94] vision [Gel94b] SN and related [DB93] other [Kur94] theorem proving, logic [DN94]
Reference: [Fah85] <author> Scott E. Fahlman. </author> <title> Extended abstract: </title> <booktitle> Parallel processing in artificial intelligence. Parallel Computing, </booktitle> <volume> 2 </volume> <pages> 283-286, </pages> <year> 1985. </year>
Reference-contexts: Moldovan considers matching to be one of the "inner loops" of reasoning, and comments that it is trivial for single attributes but difficult for structured relations [M + 92]. Similarly, Fahlman characterizes knowledge processing programs as matching operations on a large databases followed by minimal processing <ref> [Fah85] </ref>, while Kogge emphasizes matching, substitution, sets and relations, and Levinson argues for inference and intelligent behavior based on pattern associativity [Kog91, Lev92b]. <p> to the current input are processed and SDM does not require multiplication or matrix operations, greatly improving computational efficiency. 2.2.2 Semantic Networks Semantic networks also attempt to capture human memory performance, and are considered a form of associative recall in which data is retrieved based on content and structural relations <ref> [Fah85] </ref>. Semantic networks represent symbolic knowledge in terms of concepts, properties, and their relations. As shown in Figure 2.3, a semantic network (SN) is a 17 graph in which nodes correspond to objects, facts, concepts, or attributes and edges represent typed relationships, making structural knowledge explicit. <p> Many of these papers emphasize the difficulties of achieving good processor utilization and the importance of load balancing. Kitano and Hendler note the emphasis on associative processing, marker passing, nearest neighbor and subsumption operations, memory-based reasoning, neural networks, and genetic algorithms [KH94]. Semantic networks (particularly Fahlman <ref> [Fah85] </ref>) and neural networks (particularly Hopfield, Rumelhart, and Hinton [Hop82, RHW86]) were especially influential in reviving work on massively parallel AI. 42 Paper Machine PEs Category [SJ94b] search [ACP93] search [CR94] vision [Hen94] search [HJ93] constraint satisfaction [FFG94] other [SLH94] other [Pin94] neural net, logic [Ass94] theorem proving, logic [SS94a] rule-based <p> Similarly, Fahlman characterizes most AI programs as a search in a large database followed by minimal processing, suggesting a massively parallel approach for AI primitives <ref> [Fah85] </ref>. Fahlman's NETL semantic network model was among the first proposed massively parallel AI systems. Kogge emphasizes 58 matching, substitution, sets and relations, and Levinson builds a case for inference and intelligent behavior based on pattern associativity [Kog91, Lev92b, Lev91]. <p> Additionally, weights associated with graphs can form the basis of learning, as in Morph [AL93]. Semantic networks support several types of inference through marker or message propagation <ref> [Fah85, M + 92] </ref>. Semantic networks and relational structure processing can also be combined in supporting AI.
Reference: [Fah93] <author> Scott E. Fahlman. </author> <title> Some thoughts on NETL 15 years later. </title> <editor> In James Geller, editor, </editor> <booktitle> Innovative Applications of Massive Parallelism, </booktitle> <pages> pages 85-89. </pages> <publisher> AAAI, </publisher> <year> 1993. </year> <booktitle> Working Notes, Spring Symposium Series, </booktitle> <publisher> Stanford University. </publisher>
Reference-contexts: Publications 43 Paper Machine PEs Category [Kur93] theorem proving, logic [JS94] vision [Gel94b] SN and related [DB93] other [Kur94] theorem proving, logic [DN94] CM-2 vision, search [EAH93] CM-2 SN and related [Lan93] CM-2 SN, natural language [H + 94] IXM2 SN, natural language [O + 94] IXM2 SN and related <ref> [Fah93] </ref> NETL SN and related [Twa94] custom genetic, classifier systems [LH94] custom other [YK94] custom neural net, learning [CGK94] nCUBE2 512 other [HJ94] transputer 512 theorem proving, logic [BKS93] CM-2 1024 other [S + 93c] DADO2 1024 rule-based system [CGK93] nCUBE 1024 search [NM93] CM-2 4096 genetic, classifier systems [Gel94a] CM-2
Reference: [FBF77] <author> Jerome H. Friedman, Jon Louis Bentley, and Raphael Ari Finkel. </author> <title> An algorithm for finding best matches in logarithmic expected time. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 3(3) </volume> <pages> 209-226, </pages> <month> September </month> <year> 1977. </year>
Reference-contexts: In Boolean spaces, a point is always at a corner. Friedman et al. provide a rigorous analysis of generalized nearest neighbors search, deriving an expected value of the number of points that must be examined, independent on the underlying distribution of points and the number of points per bin <ref> [FBF77] </ref>. This expected value is based on the ` 1 norm, and is a lower bound for all `-norms (including the ` 1 Hamming distance and ` 2 Euclidean distance). Switching to this dissertation's notation, the expected number of points examined ([FBF77], eqn. 19) is R 1 = (k 1=m +
Reference: [FFG94] <author> Michael Factor, Scott J. Fertig, and David H. Gelernter. </author> <title> Process trellis and FGP: Software architectures for data filtering and mining. </title> <editor> In V. Kumar et al., editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <pages> pages 409-428, </pages> <address> New York, </address> <year> 1994. </year> <journal> North-Holland. </journal> <volume> volume 1. </volume>
Reference-contexts: Semantic networks (particularly Fahlman [Fah85]) and neural networks (particularly Hopfield, Rumelhart, and Hinton [Hop82, RHW86]) were especially influential in reviving work on massively parallel AI. 42 Paper Machine PEs Category [SJ94b] search [ACP93] search [CR94] vision [Hen94] search [HJ93] constraint satisfaction <ref> [FFG94] </ref> other [SLH94] other [Pin94] neural net, logic [Ass94] theorem proving, logic [SS94a] rule-based system [Sha93] neural net [AG93] search [Kum93] SN and related [AB93] neural net [WG93] CM-5 other [LHB93] Symmetry constraint satisfaction [Rob93] custom other [HIM94] custom other [LCV94] transputer neural net [OMT94] custom 10 other [SJ94a] transputer 16
Reference: [FKB87] <author> M. J. Flynn, P. Kanerva, and N. Bhadkamkar. </author> <title> Sparse distributed memory: Principles of operation. </title> <type> Technical Report CSL-TR87-338, </type> <institution> Stanford University, </institution> <year> 1987. </year> <month> 246 </month>
Reference-contexts: Hardware would include a 1-bit input register and exclusive-or gate for each of the 1024 columns, followed by an adder tree to compute the ones count. This is similar to the approach taken by the Stanford SDM hardware prototype <ref> [FKB87] </ref>. The difficulty is with hardware cost. Despite starting with 1-bit adders, the adder tree requires over 65,000 transistors. Including the registers and gates, a total of nearly 84,000 transistors are needed. Hardware could be reduced somewhat by truncating the tree midway and using the "bypass-mesh" log sum.
Reference: [FKB89] <author> M. J. Flynn, P. Kanerva, and N. Bhadkamkar. </author> <title> Sparse distributed memory principles of operation. </title> <type> Technical Report 89.53, </type> <institution> NASA Ames Research Center, RIACS, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: Designed primarily for DNA sequence comparison, BSYS should be efficient for numerous other algorithms. Stanford SDM Prototype Stanford developed an application-specific processor for SDM capable of iterating a 256-bit input 80 K storage location SDM 50 times per second (10 9 connections per second) <ref> [FKB89] </ref>. The system supports Kanerva's k-fold model, with 8192 storage locations per fold and up to 16 folds. Hamming distance calculation and radius thresholding is performed by special-purpose hardware. Calculation is location-serial, with a 256 bit wide computation circuit. <p> This is over 30 times faster than the Stanford application-specific SDM prototype and 200 times faster than a 16 K PE CM-2 executing SDM <ref> [FKB89] </ref>, and represents "real-time" performance with thousands of input-output iterations per second. Although no alternatives are clearly viable in reducing Phase 1 time on a parallel implementation, there are still numerous options whose investigation requires the flexibility of a general-purpose machine.
Reference: [FKS85] <author> Christer Fernstrom, Ivan Kruzela, and Bertil Svensson. </author> <title> LUCAS Associative Array Processor: Design, Programming, and Application Studies. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: multiply O (lg N ) set membership, is set empty O (B lg N ) permute, median, size of set, merge, log-sum O (N ) minimum spanning tree O (N 2 ) all pairs shortest path O (N 2 B 2 ) matrix multiplication Table 2.1: Bit-Cell CAM Operations (after <ref> [FKS85, Fos76] </ref>) of 32 to 64 bits each) on a single chip. In comparison to generic SIMD architectures, associative processors emphasize matching operations and broadcast comparands, and include a multiple-response resolver. <p> The ` 2 norm is Euclidean distance. The ` 1 norm (Minkowski metric in the limit as approaches infinity) is also commonly used in pattern matching and classification. 24 LUCAS LUCAS (1986) is a typical bit-serial associative array processor <ref> [FKS85] </ref>. Each PE consists of a 5-input 4-output ALU implemented as a 1 K by 4 bit PROM, and four 1-bit flags. A single-stage shu*e-exchange interconnect network connects the PEs to each other and to the off-chip memory, a mere 4 K bits per PE. <p> LUCAS also includes a multiple-response resolver network. A full system includes 128 PEs, a control processor, and an I/O processor. Proposed PE enhancements include an up/down counter, local addressing, and a bit-serial multiplier <ref> [FKS85] </ref>. Although containing the necessary minimal support for associative processing, most of the example algorithms used to evaluate LUCAS are numerical. The algorithms cover areas such as matrix multiplication and FFTs, simple graph algorithms, relational database operations, and image processing. <p> H.A.P.S. [Stu85] 1985 SIMD, bus, assoc. proposed nCUBE [H + 86] 1985 MIMD, hypercube 1 K 1,945 MIPS NON-VON [AG89] 1985 MSIMD, bit-byte, tree, MIN 16 K+ AAP2 [K + 86] 1986 SIMD, 2-d mesh, assoc. 64 K 10,000 MOPS CLIP7A [FMD88] 1986 SIMD, 2-d mesh 256 64 MIPS LUCAS <ref> [FKS85] </ref> 1986 SIMD, bit-serial, MIN, assoc. 128 CM-2 [TR88] 1987 SIMD, bit-serial, hypercube 64 K 4,000 MIPS AMT DAP [TW91] 1987 SIMD, 2-d mesh, orth. bus 1 K 800 MFLOPS GAPP [NCR87] 1987 SIMD, 1,2-d mesh 2 K 900 MOPS DADO [AG89] 1987 MIMD, tree 8 K PASM [MSS85] 1987 MSIMD, <p> [Ble90] SIMD ffi * ffi ffi i MPL [Chr90] 1990 SIMD ffi ffi ffi j MultiLISP [Ble90] ffi * * ffi k NIAL [Ble90] 1982 l Occam [Bur88] MIMD * ffi m Paralation [Ble90] 1987 serial, vector, ffi SIMD, MIMD n Parallel Pascal [Ble90] ffi * ffi ffi o Pascal-L <ref> [FKS85] </ref> 1983 SIMD ffi ffi p pSather, dSather [FLR91, Sch92] 1991 MIMD, SIMD ffi ffi * q SETL [SDDS86] 1981 serial r SQL [Ble90] serial (ffi) ffi no somewhat * yes Table 2.3: Parallel Languages Characteristic a b c d e f g h i j k l m n o <p> The dot product summation is simply a zero-detect; the ones counter already justified for Hamming distance can be used if a non-Boolean result is needed. Cyclic-shifting intermediate values across the PEs favors a mesh topology. Comparison to the matrix multiplication algorithm on Lucas <ref> [FKS85] </ref> indicates that a multistage interconnect network would double total Boolean matrix multiplication time. A mesh topology is also beneficial to the required matrix transposition. With single-bit extraction and insertion and insertion at any position, transposition an n fi n matrix on n 2 =b PEs is 12n cycles. <p> Backtracking search methods include state-space (tree) search and IDA*. LUCAS Similar arguments hold for LUCAS as for the CM-2. LUCAS is an associative processor whose PEs are also composed of bit-serial 3-input ULB with several 1-bit flags <ref> [FKS85] </ref>. Simple analysis shows that MISC's word-wide bypass-mesh network outperforms LUCAS's bit-serial shu*e-exchange network, and MISC includes a multiple-response resolver with the same capabilities as LUCAS's. Fernstrom et al. include execution of numerous algorithms on LUCAS as part of its evaluation [FKS85]. <p> composed of bit-serial 3-input ULB with several 1-bit flags <ref> [FKS85] </ref>. Simple analysis shows that MISC's word-wide bypass-mesh network outperforms LUCAS's bit-serial shu*e-exchange network, and MISC includes a multiple-response resolver with the same capabilities as LUCAS's. Fernstrom et al. include execution of numerous algorithms on LUCAS as part of its evaluation [FKS85]. Although LUCAS is presented as an associative array processor the algorithms include areas such as relational database operations, matrix operations, graph algorithms, and image processing. Table 8.2 lists the algorithms. <p> (N lg N ) O (N ) O (lg N ) Union O (N lg N 2 ) O (N 2 p) O ( N 2 ) Product O (N 1 fl N 2 ) O (N 1 fl N 2 ) O (1) Table 8.1: Relational Database Parallelization (after <ref> [FKS85] </ref>) 185 Other than the non-indexed select, the RDB asymptotic speedups in Table 8.1 are disappointing, typically not much better than a factor of lg N . In most cases this is the result of checking for duplicates serializing over the size of the result.
Reference: [FLR91] <author> Jerome A. Feldman, Chu-Cheow Lim, and Thomas Rauber. </author> <title> The shared-memory language pSather on a distributed-memory multiprocessor. </title> <type> Technical report, </type> <institution> International Computer Science Institute, University of California, Berkeley, </institution> <year> 1991. </year>
Reference-contexts: [Chr90] 1990 SIMD ffi ffi ffi j MultiLISP [Ble90] ffi * * ffi k NIAL [Ble90] 1982 l Occam [Bur88] MIMD * ffi m Paralation [Ble90] 1987 serial, vector, ffi SIMD, MIMD n Parallel Pascal [Ble90] ffi * ffi ffi o Pascal-L [FKS85] 1983 SIMD ffi ffi p pSather, dSather <ref> [FLR91, Sch92] </ref> 1991 MIMD, SIMD ffi ffi * q SETL [SDDS86] 1981 serial r SQL [Ble90] serial (ffi) ffi no somewhat * yes Table 2.3: Parallel Languages Characteristic a b c d e f g h i j k l m n o p q r Collections dynamic creation * *
Reference: [FMD88] <author> T. J. Fountain, K. N. Mathews, and M. J. B. Duff. </author> <title> The CLIP7A image processor. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 10 </volume> <pages> 310-319, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: 1985 SIMD, bit serial, hypercube 64 K 2,000 MIPS H.A.P.S. [Stu85] 1985 SIMD, bus, assoc. proposed nCUBE [H + 86] 1985 MIMD, hypercube 1 K 1,945 MIPS NON-VON [AG89] 1985 MSIMD, bit-byte, tree, MIN 16 K+ AAP2 [K + 86] 1986 SIMD, 2-d mesh, assoc. 64 K 10,000 MOPS CLIP7A <ref> [FMD88] </ref> 1986 SIMD, 2-d mesh 256 64 MIPS LUCAS [FKS85] 1986 SIMD, bit-serial, MIN, assoc. 128 CM-2 [TR88] 1987 SIMD, bit-serial, hypercube 64 K 4,000 MIPS AMT DAP [TW91] 1987 SIMD, 2-d mesh, orth. bus 1 K 800 MFLOPS GAPP [NCR87] 1987 SIMD, 1,2-d mesh 2 K 900 MOPS DADO [AG89]
Reference: [Fos76] <author> Caxton C. Foster. </author> <title> Content-Addressable Parallel Processors. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: These designs have been plagued by high cost and the difficult competition with the ubiquitous von Neumann architecture. Efforts to reduce cost resulted in the forerunners to today's massively parallel SIMD machines, such as in the STARAN (1972 <ref> [Fos76] </ref>). Unfortunately, CAM architectures require data to be in a fixed length, fixed location format. More importantly, as just discussed, associative memory and simple matching are not enough to provide thorough massively parallel support for knowledge processing. <p> multiply O (lg N ) set membership, is set empty O (B lg N ) permute, median, size of set, merge, log-sum O (N ) minimum spanning tree O (N 2 ) all pairs shortest path O (N 2 B 2 ) matrix multiplication Table 2.1: Bit-Cell CAM Operations (after <ref> [FKS85, Fos76] </ref>) of 32 to 64 bits each) on a single chip. In comparison to generic SIMD architectures, associative processors emphasize matching operations and broadcast comparands, and include a multiple-response resolver. <p> In many cases it is preferable to use an associatively-linked representation similar to that proposed for Foster for list and string processing <ref> [Fos76] </ref>. Elements of both ordered and unordered collections are assigned to arbitrary, non-contiguous processing elements along with fields to indicate collection membership, ordinality and/or cardinality (formally, each atomic value becomes a tuple). Data movement is not required, so that insertions and deletions can be performed in constant time. <p> This section then presents an analytical model of the number of markers propagated. This section also covers analysis of the node-based network followed by instruction profiles and processing characteristics. 5.2.1 Edge-Based SN Representation The edge-based representation, suggested by Foster's CAM shortest-path algorithm <ref> [Fos76] </ref>, uses an edge-based graph description where each edge is represented as a tuple 84 of [source,destination] with additional elements for node markers and to indicate if the tuple has been processed or not. <p> Circuit estimates give 48 transistors per word for logic, with equivalent logic depth (f ld) of 28, including memory access. * Foster's bit-serial CAM, as detailed in Chapter 10 of <ref> [Fos76] </ref>. This design includes 3 flags and functions such as add, subtract, and flag move and complement. * Universal bit-serial ALU, similar to the CM-1, consisting of a 3-input 2-output universal logic block, one flag, and response resolver. <p> Cycle counts for bit-cell, minimum bit-serial, and Foster's CAM are taken from Foster <ref> [Fos76] </ref>. Here m is the number of bits in the data representation, m 0 is the number of relevant bits in a partial match, and b is the number of bits in a word for the word-wide ALU. <p> For log-sum and similar reductions using the bypass-mesh requires 3 (lg (N=32) + 5)(m + :5 lg N )=b cycles, assuming a 32 fi 32 mesh. Other options such as Foster's carry shower <ref> [Fos76] </ref> or Blelloch's scan-tree [Ble90] require more cycles under typical conditions. An adder tree would have f ld = 2 lg N , and including chip crossings a reduction over a full PN would require about 8 PE cycle times to do plus and minimum reductions. <p> Output can also be sent directly to the address adder for accumulating counts over multiple words. The circuit for the ones counter itself is a tree of 1-bit full adders, similar to the carry shower described by Foster for counting the number of responders in a CAM array <ref> [Fos76] </ref>. Resolver Tag The resolver tag interfaces the PE into the array-wide multiple response resolver. The tag can be reset in a single cycle directly by the resolver without the identifier from the controller or supervisor. <p> For example, examining if there are any PEs meeting an if-conditional before executing the conditional segment of code improves speed and processor utilization. The multiple-response resolver is a modular look-ahead circuit based on Anderson's "P-generator" block and provides several operations <ref> [Fos76] </ref>. Given a 1-bit tag from each PE as input, the resolver generates the pointer vector (a vector of all 0s except for a single 1 at the first PE with the tag set in the physical ordering). <p> execute a match, T c m, and a multiwrite, T cmw , after the match and comparand mask registers have been loaded are T cm = t bl + 3T g + t blm The CAM model's multiple-response resolver is based on Anderson's resolver tree with Koo's first-responder address generation <ref> [Fos76] </ref>, which has a cell with 23 transistors. Resolver logic area is then A rl =area (23)r ws N a and logic delay is T rl = 4T g (lg r ws N a 1).
Reference: [Gei93] <author> G. A. Geist. </author> <title> PVM 3 beyond network computing. </title> <booktitle> In 2nd International ACPC Conference, </booktitle> <pages> pages 194-203, </pages> <year> 1993. </year>
Reference-contexts: A MISC node would be a workstation with a single PN as an accelerator. In this incarnation MISC could be part of a heterogeneous parallel processing environment such as PVM 3 <ref> [Gei93] </ref>. As several viable options exists, a choice is not necessary at this time; this dissertation focuses on the associative arrays. The MISC analysis and evaluation in this dissertation is based on the simpler assumptions; the global communication net includes only basic router functionality.
Reference: [Gel93a] <author> James Geller, </author> <title> editor. Innovative Applications of Massive Parallelism. </title> <booktitle> AAAI, 1993. Working Notes, Spring Symposium Series, </booktitle> <publisher> Stanford University. </publisher>
Reference-contexts: In the remainder of this section, we summarize these recent publications, noting the difference in approaches between parallel and massively parallel knowledge processing and the difficulties SIMD architecture and AI processing pose for one another. 2.5.1 AI Research and Applications on Massively Parallel Processors Table 2.5 lists papers from <ref> [K + 94a, K + 94b, KH94, IEE94, Gel93a] </ref> for parallel AI, defined here somewhat arbitrarily as using fewer than 512 processors. Table 2.6 does the same for massively parallel AI (work using 512 or more processors). <p> Most conventional AI programs have limited coarse-grain (control-level) parallelism; efforts to parallelize existing serial AI codes generally show poor processor utilization beyond 16 processors and negligible speedup beyond 32 <ref> [KT88, Kog91, WLL90, K + 94a, K + 94b, Gel93a, KH94] </ref>. This is true even for control-level parallel machines designed specifically for Prolog and LISP.
Reference: [Gel93b] <author> James Geller. </author> <title> Massively parallel knowledge representation. </title> <editor> In James Geller, editor, </editor> <booktitle> Innovative Applications of Massive Parallelism, </booktitle> <pages> pages 90-97. </pages> <publisher> AAAI, </publisher> <year> 1993. </year> <booktitle> Working Notes, Spring Symposium Series, </booktitle> <publisher> Stanford University. </publisher>
Reference-contexts: [Pin94] neural net, logic [Ass94] theorem proving, logic [SS94a] rule-based system [Sha93] neural net [AG93] search [Kum93] SN and related [AB93] neural net [WG93] CM-5 other [LHB93] Symmetry constraint satisfaction [Rob93] custom other [HIM94] custom other [LCV94] transputer neural net [OMT94] custom 10 other [SJ94a] transputer 16 theorem proving, logic <ref> [Gel93b] </ref> CM-5 32 SN and related [CS94] CM-5 32 other [GW94] WARP 32 vision [LP94] iPSC 32 constraint satisfaction [ZM94] transputer 48 constraint satisfaction [MNT94] AP1000 64 other [CGA94] Pixel 64 rule-based system, vision [SS94b] many 64 theorem proving [NW93] PIM 70 theorem proving, logic [Ert93] LAN 110 search [Pea94] Fujitsu
Reference: [Gel94a] <author> James Geller. </author> <title> Advanced update operations in massively parallel knowledge representation. </title> <editor> In Hiroaki Kitano and James A. Hendler, editors, </editor> <booktitle> Massively Parallel Artificial Intelligence, </booktitle> <pages> pages 74-101, </pages> <address> Menlo Park, CA, 1994. </address> <publisher> AAAI Press / The MIT Press. </publisher>
Reference-contexts: related [Fah93] NETL SN and related [Twa94] custom genetic, classifier systems [LH94] custom other [YK94] custom neural net, learning [CGK94] nCUBE2 512 other [HJ94] transputer 512 theorem proving, logic [BKS93] CM-2 1024 other [S + 93c] DADO2 1024 rule-based system [CGK93] nCUBE 1024 search [NM93] CM-2 4096 genetic, classifier systems <ref> [Gel94a] </ref> CM-2 8192 SN and related [CG94] CM-2 8192 search [KK93] CM-2 8192 search [JSS93] MP-1 8192 theorem proving, logic [PKF93] CM-2 16384 search [PW94] CM-2 16384 natural language [WD94] CM-2 16384 neural net, learning [ND94] CM-2 16384 neural net, learning [AHEK94] CM-2 16384 SN and related [EAH94] CM-2 16384 SN <p> Inheritance Codes in Semantic Networks Geller presents a coding for trees in which each node is represented by an integer-valued interval and proposes their use in semantic networks to avoid propagating across inheritance hierarchy links <ref> [Gel94a, Gel94b] </ref>. The code is formed by a right to left preorder tree traversal; the resulting node label forms the lower boundary of the interval. The maximum of all descendants' labels form the upper boundary. <p> These codes can be efficiently created, and Geller gives algorithms for incrementally updating the coded tree in time linear in the number of trees moved, logarithmic or constant in the size of the trees <ref> [Gel94a] </ref>. Operations on the codes appear to be limited to inheritance, however, and have not been extended beyond tree structures as would be required for typical hierarchies with multiple inheritance. <p> A node might thereby include one or more subsumption codes in addition to the outgoing edges and label. 5 The network topology would then have few if any tree portions, and can be approximated as a random, disconnected directed graph. 4 This approach was independently developed by Geller somewhat earlier <ref> [Gel94a, Gel94b] </ref>. 5 The labels are used as the destinations in the outgoing descriptions and could be arbitrary or they could specify the parallel node storage location. <p> Since the resulting codes are of O (lg N ) length and functions are implementable by simple bit-wise operations, time is effectively O (1). Geller presents coding for trees in which each node is represented by an integer-valued interval <ref> [Gel94a, Gel94b] </ref>. A node subsumes its descendants by having an interval that covers those of all its descendents. These codes can be efficiently created, and Geller gives algorithms for incrementally updating the coded tree in constant time. <p> Semantic Network The semantic network example incorporates lattice codes for inheritance, which improves performance by reducing the size of the network and eliminating the marker propagation cycles otherwise needed to establish inheritance. This was first proposed by Geller <ref> [Gel94a, Gel94b] </ref> and is also used in the IXM2 semantic network processor. Our simulations provide a preliminary evaluation of performance using this technique on synthetic data sets.
Reference: [Gel94b] <author> James Geller. </author> <title> Inheritance operations in massively parallel knowledge representation. </title> <editor> In V. Kumar et al., editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <pages> pages 95-114, </pages> <address> New York, </address> <year> 1994. </year> <journal> North-Holland. </journal> <volume> volume 1. </volume>
Reference-contexts: many 64 theorem proving [NW93] PIM 70 theorem proving, logic [Ert93] LAN 110 search [Pea94] Fujitsu 128 theorem proving, logic [CMD94] SNAP 144 SN, natural language [Sat94] nCUBE3 256 natural language Table 2.5: Sample of Recent Parallel AI Publications 43 Paper Machine PEs Category [Kur93] theorem proving, logic [JS94] vision <ref> [Gel94b] </ref> SN and related [DB93] other [Kur94] theorem proving, logic [DN94] CM-2 vision, search [EAH93] CM-2 SN and related [Lan93] CM-2 SN, natural language [H + 94] IXM2 SN, natural language [O + 94] IXM2 SN and related [Fah93] NETL SN and related [Twa94] custom genetic, classifier systems [LH94] custom other <p> Inheritance Codes in Semantic Networks Geller presents a coding for trees in which each node is represented by an integer-valued interval and proposes their use in semantic networks to avoid propagating across inheritance hierarchy links <ref> [Gel94a, Gel94b] </ref>. The code is formed by a right to left preorder tree traversal; the resulting node label forms the lower boundary of the interval. The maximum of all descendants' labels form the upper boundary. <p> A node might thereby include one or more subsumption codes in addition to the outgoing edges and label. 5 The network topology would then have few if any tree portions, and can be approximated as a random, disconnected directed graph. 4 This approach was independently developed by Geller somewhat earlier <ref> [Gel94a, Gel94b] </ref>. 5 The labels are used as the destinations in the outgoing descriptions and could be arbitrary or they could specify the parallel node storage location. <p> Since the resulting codes are of O (lg N ) length and functions are implementable by simple bit-wise operations, time is effectively O (1). Geller presents coding for trees in which each node is represented by an integer-valued interval <ref> [Gel94a, Gel94b] </ref>. A node subsumes its descendants by having an interval that covers those of all its descendents. These codes can be efficiently created, and Geller gives algorithms for incrementally updating the coded tree in constant time. <p> Semantic Network The semantic network example incorporates lattice codes for inheritance, which improves performance by reducing the size of the network and eliminating the marker propagation cycles otherwise needed to establish inheritance. This was first proposed by Geller <ref> [Gel94a, Gel94b] </ref> and is also used in the IXM2 semantic network processor. Our simulations provide a preliminary evaluation of performance using this technique on synthetic data sets.
Reference: [Gen83] <author> D. Gentner. Structure-mapping: </author> <title> a theoretical framework for analogy. </title> <journal> Cognitive Science, </journal> <volume> 7(2), </volume> <year> 1983. </year>
Reference-contexts: There is a growing recognition of the importance of graphs as a representation of logical relations in general <ref> [Gen83, Win79] </ref> so that SI is an important operation despite its time complexity. Fortunately, the expected case time is empirically polynomial [Ull76, WWR91]. The worst-case time results from generally pathological cases that rarely occur in practice [Bar88, CK80, Sus65] so that parallelization can provide a difference in tractable graph size.
Reference: [GJ79] <author> Michael R. Garey and David S. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> San Francisco, </address> <year> 1979. </year>
Reference-contexts: Comparing nested collections requires more complex pattern matching; each collection's nesting induces a tree structure to be compared with the other's. The recursive comparison bottoms out at subsumption tests of simple collections. This form of subgraph isomorphism test has polynomial time since both structures are trees <ref> [GJ79] </ref>. In addition to exact matching, subsumption operators can be specified for subsumption-code atomic elements. SEGUL uses to indicate atomic subsumption to maintain the distinction between an element and a set of one element. Other subsumptioncode operations include greatest lower bound and least upper bound. <p> Ordered nested collections can also be represented as a regular expression; Figure 4.10 shows an example. Comparison of these knowledge representations then becomes one of subgraph isomorphism, which can be done in polynomial time since both graphs are trees <ref> [GJ79] </ref>. The paralation internal representation for nested collections indicates groupings at each level of nesting, supporting both bottom-up and top-down methods, such as those presented by Hoffmann and O'Donnel [HO82].
Reference: [GR90] <author> Amit Gulati and Don Roberts. </author> <title> A cascadable content-addressable memory based database query accelerator with proximity resolution. (class project report), </title> <year> 1990. </year>
Reference-contexts: Except for the PE control section, these estimates are based on other implementations such as BSYS and PCAM <ref> [Hug91, GR90] </ref>, RTL, gate, or transistor level sketches of features unique to MISC. The additional control is estimated as 5% of the total PE transistor count. The transistor counts in the table are conservative, being the most straightforward and general approach.
Reference: [GW94] <author> George Gusciora and Jon A. Webb. </author> <title> Parallel affine image warping. </title> <editor> In V. Kumar et al., editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <pages> pages 45-66, </pages> <address> New York, </address> <year> 1994. </year> <journal> North-Holland. </journal> <volume> volume 1. </volume>
Reference-contexts: system [Sha93] neural net [AG93] search [Kum93] SN and related [AB93] neural net [WG93] CM-5 other [LHB93] Symmetry constraint satisfaction [Rob93] custom other [HIM94] custom other [LCV94] transputer neural net [OMT94] custom 10 other [SJ94a] transputer 16 theorem proving, logic [Gel93b] CM-5 32 SN and related [CS94] CM-5 32 other <ref> [GW94] </ref> WARP 32 vision [LP94] iPSC 32 constraint satisfaction [ZM94] transputer 48 constraint satisfaction [MNT94] AP1000 64 other [CGA94] Pixel 64 rule-based system, vision [SS94b] many 64 theorem proving [NW93] PIM 70 theorem proving, logic [Ert93] LAN 110 search [Pea94] Fujitsu 128 theorem proving, logic [CMD94] SNAP 144 SN, natural language
Reference: [H + 86] <author> J. P. Hayes et al. </author> <title> A microprocessor-based hypercube supercomputer. </title> <journal> IEEE Micro, </journal> <volume> 6(5) </volume> <pages> 6-17, </pages> <month> October </month> <year> 1986. </year> <month> 247 </month>
Reference-contexts: 500 MFLOPS cm* [SFS77] 1977 MIMD, multibus clusters 100 10 MIPS UNC Cellular [AG89] 1979 MSIMD, reduction, tree 1 M MPP [Bat82] 1983 SIMD, bit-serial, 2-d mesh 16 K 3,000 MIPS CM-1 [AG89] 1985 SIMD, bit serial, hypercube 64 K 2,000 MIPS H.A.P.S. [Stu85] 1985 SIMD, bus, assoc. proposed nCUBE <ref> [H + 86] </ref> 1985 MIMD, hypercube 1 K 1,945 MIPS NON-VON [AG89] 1985 MSIMD, bit-byte, tree, MIN 16 K+ AAP2 [K + 86] 1986 SIMD, 2-d mesh, assoc. 64 K 10,000 MOPS CLIP7A [FMD88] 1986 SIMD, 2-d mesh 256 64 MIPS LUCAS [FKS85] 1986 SIMD, bit-serial, MIN, assoc. 128 CM-2 [TR88] <p> 1,2-d mesh 2 K 900 MOPS DADO [AG89] 1987 MIMD, tree 8 K PASM [MSS85] 1987 MSIMD, bus, xbar 1 K TRAC [AG89] 1987 MSIMD, banyan 4 FAIM [A + 87] 1987 MSIMD, 2-d mesh 1 K+ MasPar MP-1 [Nic90] 1989 SIMD, 2-d mesh, MIN 16 K 26,000 MIPS nCUBE2 <ref> [H + 86] </ref> 1989 MIMD, hypercube 8 K 61,440 MIPS April [A + 90] 1990 MIMD, 2,3-d mesh, dataflow 1000+ BLITZEN [B + 90a] 1990 SIMD, bit serial, 2-d mesh 16 K 3,000 MIPS iWarp [B + 90b] 1990 MIMD, 2-d mesh 64 1,300 MFLOPS BSYS [Hug91] 1990 systolic, 1-d mesh
Reference: [H + 91] <author> Tetsuya Higuchi et al. IXM2: </author> <title> a parallel associative processor. </title> <journal> ACM SIGARCH, </journal> <volume> 19(3) </volume> <pages> 22-33, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: In addition to special-purpose machines for vision and image processing, contemporary work includes machines designed specifically for databases and knowledge base processing <ref> [KT88, S + 85, M + 92, H + 91] </ref>. Unfortunately these do not address the full needs of knowledge processing. Special-purpose machines lack the flexibility to perform well on differing algorithms and problem domains. <p> Each processing node is augmented with 4 K 40-bit words of bit-cell CAM. The IXM2 has a hierarchical interconnection scheme in which clusters of 8 processors are fully interconnected, and the 8 resulting clusters are fully interconnected (using 9 additional Transputers for communication) <ref> [H + 91] </ref>. The machine is used primarily for research in natural language processing, including translation. 2.3.5 Other Processors Table 2.2 presents additional systems surveyed in background research for the MISC architecture. <p> 1990 MIMD, 2-d mesh 64 1,300 MFLOPS BSYS [Hug91] 1990 systolic, 1-d mesh 470 108 MOPS NAP [KF90] 1990 SIMD, various simulated SNAP [M + 92] 1990 MSIMD, modified hypercube 144 CM-5 [Thi92] 1991 MSIMD, fat-tree, vector 1 K 128,000 MFLOPS Paragon [Hwa93] 1991 MIMD, 2-d mesh 300,000 MFLOPS IXM2 <ref> [H + 91] </ref> 1991 pseudo MIMD, hierarchical 64 278 MIPS Paradigm [CGB91] 1991 MIMD, hierarchical 100+ Coherent [Sto91] 1992 SIMD, CAM 4 K MasPar MP-2 [Mas92a] 1992 SIMD, 2-d mesh, MIN 16 K 68,000 MIPS Cray T3D [Oed93] 1993 MIMD, 3-d mesh 2 K 300,000 MFLOPS WASP [LJ91] devel MSIMD, bypass
Reference: [H + 93] <author> Takehiro Hasegawa et al. </author> <title> An experimental RAM with a NAND-structured cell. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> 28(11) </volume> <pages> 1099-1103, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Detailed information on 3 experimental 16 M bit SRAM chips [S + 93a, S + 93b, U + 93] and 3 experimental 256 M bit memories <ref> [T + 93, K + 93, H + 93] </ref> allowed accurate validation against advanced memory designs. Suspense's accuracy was surprisingly good considering the numerous circuit `tricks' used in these experimental memories.
Reference: [H + 94] <author> Tetsuya Higuchi et al. </author> <title> The IMX2 parallel associative processor for AI. </title> <booktitle> Computer, </booktitle> <pages> pages 53-63, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: IXM2 The IXM2 is a semantic-network machine directly inspired by the NETL model, its developers viewing "episodic memory as the foundation of intelligence" <ref> [H + 94] </ref>. The specific semantic network model incorporates "group identifiers," in essence basic transitive-closure subsumption codes, to speed inheritance operations. The IMX2 features 64 processing nodes using Transputers, standard microprocessor chips designed specifically for MIMD parallel processing and featuring built-in serial communication links. <p> natural language [Sat94] nCUBE3 256 natural language Table 2.5: Sample of Recent Parallel AI Publications 43 Paper Machine PEs Category [Kur93] theorem proving, logic [JS94] vision [Gel94b] SN and related [DB93] other [Kur94] theorem proving, logic [DN94] CM-2 vision, search [EAH93] CM-2 SN and related [Lan93] CM-2 SN, natural language <ref> [H + 94] </ref> IXM2 SN, natural language [O + 94] IXM2 SN and related [Fah93] NETL SN and related [Twa94] custom genetic, classifier systems [LH94] custom other [YK94] custom neural net, learning [CGK94] nCUBE2 512 other [HJ94] transputer 512 theorem proving, logic [BKS93] CM-2 1024 other [S + 93c] DADO2 1024
Reference: [Har92] <author> Charles A. Hart. </author> <title> Dynamic RAM as secondary cache. </title> <journal> IEEE Spectrum, </journal> <month> October </month> <year> 1992. </year>
Reference-contexts: Further, Rambus requires a large on-chip area per memory port. EDRAM <ref> [Bon92, Jon92, Har92, Ng92] </ref> extends DRAM with an on-chip SRAM "cache," taking advantage of the wide on-chip DRAM line size.
Reference: [HC91] <author> Young-Sik Hong and Su-Shing Chen. </author> <title> Character recognition in a sparse distributed memory. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 21(3) </volume> <pages> 675-678, </pages> <month> May/June </month> <year> 1991. </year>
Reference-contexts: Kanerva proposed a k-fold network for distinguishing between temporal patterns such as `ABDA' versus `ACDG.' Research into potential end-user applications have included speech recognition [PF89], character recognition <ref> [HC91] </ref>, and weather prediction [Rog90]. Investigating these variations requires the flexibility of a general-purpose processor, and exploration of modular nets composed of multiple SDMs requires extremely high processing speed. SDM provides associative memory and direct inference behavior through activating near neighbors in the input space, offering several advantages.
Reference: [HCWS94] <author> Martin C. Herbordt, James C. Corbett, Charles C. Weems, and John Spald-ing. </author> <title> Practical algorithms for online routing on fixed and reconfigurable meshes. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 20 </volume> <pages> 341-355, </pages> <year> 1994. </year>
Reference-contexts: Marking thereby takes 2 2 fi2 2 fi2 15 = 2 19 byte operations and marker propagation moves 2 4 fi 2 7 = 2 11 bytes. Assume a 2-d mesh for local communication with a random communication taking 3 p P communication cycles <ref> [HCWS94] </ref>. With each cycle taking 12 instructions at 60 MHz, total time is 840 s. With the query and response totaling approximately 128 bytes (some 28 node-ids as the returned result) communication is 128 K bytes/s. <p> Corbett et al.'s algorithm forms coteries based on message destinations to skip over intermediate PEs and uses a 1-bit handshake line to signal blocking to other PEs in the coterie <ref> [HCWS94] </ref>. Their empirical tests give 2 p P routing cycles for typical patterns, with small variance. Analytically, worst-case time is fi (P ). Each routing cycle is fairly complex, requiring at least 30 instructions each on MISC, giving 60 p P cycles expected-case time. <p> In conjunction with a handshaking line, the bypass also supports a coterie routing scheme which reduces the constant factor of the O ( p P ) mesh routing cycles to 3 with a very small standard deviation <ref> [HCWS94] </ref>, although for MISC it appears faster to use a simpler routing algorithm (described in Section 7.4.2). The mesh bypass asymptotically improves times for parallel prefix operations such as log-sum. <p> Either approach has the drawback of more complex compiler design or more difficult programming. Means of reducing the instruction count for each routing cycle of the coterie routing algorithm <ref> [HCWS94] </ref> should be investigated. <p> On MISC, each routing cycle of the algorithm requires 12 instructions per routing cycle for messages consisting of one destination word and two data words, as is the case for the SN simulation. As with Corbett et al.'s coterie-routing algorithm <ref> [HCWS94] </ref>, worst case time is fi (P ) routing cycles; a queue could end up with as many as p P messages, giving Phase 173 /**** Phase 1 ****/ rload (always) comctl (meshE, meshS, x) rload (always) mar queuestart rload (always) maskreg thecolmask rload (always) adu (mar PLUS (mar, 1)) /* <p> The mesh bypass feature of the SIMD array improves performance for longer-distance communications. Similar reconfigurable-mesh networks are used in a variety of systems, for purposes such as fault-tolerance, machine vision, signal processing, sorting, and general routing <ref> [LS92, JS94, HCWS94] </ref>. Here we develop and evaluate the network for multiple Boolean matrix multiplications and paralation operations with nested parallelism. The mesh bypass is beneficial for Boolean matrix operations, supporting broadcast and array partitioning. <p> Many algorithms, such as marker passing, require data-driven "random" communication patterns (i.e. general routing). This is also well supported by the wide-link mesh network, and performance might be further improved by exploiting bypass-mesh configurations <ref> [HCWS94] </ref>. Processing Elements The MISC processing element microarchitecture includes numerous features that support compound, dynamic, and/or linked data structures. The local address unit provides considerable flexibility, including local data-dependency in selecting data objects and portions of objects.
Reference: [Hen94] <author> Dominik Henrich. </author> <title> Initialization of parallel branch-and-bound algorithms. </title> <editor> In V. Kumar et al., editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <pages> pages 131-144, </pages> <address> New York, </address> <year> 1994. </year> <journal> North-Holland. </journal> <volume> volume 2. </volume>
Reference-contexts: Semantic networks (particularly Fahlman [Fah85]) and neural networks (particularly Hopfield, Rumelhart, and Hinton [Hop82, RHW86]) were especially influential in reviving work on massively parallel AI. 42 Paper Machine PEs Category [SJ94b] search [ACP93] search [CR94] vision <ref> [Hen94] </ref> search [HJ93] constraint satisfaction [FFG94] other [SLH94] other [Pin94] neural net, logic [Ass94] theorem proving, logic [SS94a] rule-based system [Sha93] neural net [AG93] search [Kum93] SN and related [AB93] neural net [WG93] CM-5 other [LHB93] Symmetry constraint satisfaction [Rob93] custom other [HIM94] custom other [LCV94] transputer neural net [OMT94] custom
Reference: [HGC87] <author> Kai Hwang, Joydeep Ghosh, and Raymond Chowkwanyun. </author> <booktitle> Computer architecture for artificial intelligence processing. Computer, </booktitle> <pages> pages 19-27, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: Knowledge Processing Characteristics Some characteristics of knowledge processing algorithms place special demands on parallel processors <ref> [HGC87, WL89, WLL90] </ref> while others suggest promising avenues for research. Most knowledge processing programs have limited coarse-grain (control) parallelism; efforts to parallelize existing codes generally show poor processor utilization beyond 16 processors and negligible speedup beyond 32 [KT88, Kog91, WLL90]. <p> This is true even for control-level parallel machines designed specifically for Prolog and LISP. Fine-grain (massive) and medium-grain parallelism must be used to achieve significant speedup, but AI and symbolic processing have several characteristics that make such parallelization particularly difficult <ref> [HGC87, WL89, WLL90] </ref>.
Reference: [Hig90] <author> Lee Higbie. </author> <title> Quick and easy cache performance analysis. </title> <journal> ACM Computer Architecture News, </journal> <volume> 18(2), </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: Historically, microprocessor speeds have increased much faster than memory speeds, resulting in the current "cache crisis" in microprocessor system design <ref> [Hig90] </ref>. This is largely due to the emphasis on increasing capacity per chip in new memory development.
Reference: [Hil85] <author> W. Daniel Hillis. </author> <title> The Connection Machine. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1985. </year>
Reference-contexts: Unfortunately, speed and processor utilization are often severely compromised due to the single instruction stream. 25 CM-1, CM-2 The Connection Machine CM-1 (1985) is an early massively parallel machine with up to 64 K PEs, and was among the first to use a hypercube interconnection network <ref> [Hil85] </ref>. Originally designed for semantic networks and AI, it turned out to be poor for such applications. The CM-1 PE features a simple bit-serial ALU that can compute arbitrary 3-input, 2-output Boolean functions. Each PE has eight 1-bit flags and only 4 K bits of off-chip memory. <p> The controller is also responsible for queuing array input data and results for the supervisor and includes a writable control store so that highly optimized macro-instructions can be compiled for 3 The term introduced by Hillis <ref> [Hil85] </ref>. 149 particular applications. As a result, the ACU assumed in the system early analysis includes the following blocks.
Reference: [HIM94] <author> Tetsuya Higuchi, Hitoshi Iba, and Bernard Manderick. </author> <title> Evolvable hardware with genetic learning. </title> <editor> In Hiroaki Kitano and James A. Hendler, editors, </editor> <booktitle> Massively Parallel Artificial Intelligence, </booktitle> <pages> pages 398-422, </pages> <address> Menlo Park, CA, 1994. </address> <publisher> AAAI Press / The MIT Press. </publisher>
Reference-contexts: PEs Category [SJ94b] search [ACP93] search [CR94] vision [Hen94] search [HJ93] constraint satisfaction [FFG94] other [SLH94] other [Pin94] neural net, logic [Ass94] theorem proving, logic [SS94a] rule-based system [Sha93] neural net [AG93] search [Kum93] SN and related [AB93] neural net [WG93] CM-5 other [LHB93] Symmetry constraint satisfaction [Rob93] custom other <ref> [HIM94] </ref> custom other [LCV94] transputer neural net [OMT94] custom 10 other [SJ94a] transputer 16 theorem proving, logic [Gel93b] CM-5 32 SN and related [CS94] CM-5 32 other [GW94] WARP 32 vision [LP94] iPSC 32 constraint satisfaction [ZM94] transputer 48 constraint satisfaction [MNT94] AP1000 64 other [CGA94] Pixel 64 rule-based system, vision
Reference: [HJ93] <author> Walter Hower and Stephan Jacobi. </author> <title> Massively distributed constraint satisfaction. </title> <editor> In James Geller, editor, </editor> <booktitle> Innovative Applications of Massive Parallelism, </booktitle> <pages> pages 99-105. </pages> <publisher> AAAI, </publisher> <year> 1993. </year> <booktitle> Working Notes, Spring Symposium Series, </booktitle> <publisher> Stanford University. </publisher>
Reference-contexts: Semantic networks (particularly Fahlman [Fah85]) and neural networks (particularly Hopfield, Rumelhart, and Hinton [Hop82, RHW86]) were especially influential in reviving work on massively parallel AI. 42 Paper Machine PEs Category [SJ94b] search [ACP93] search [CR94] vision [Hen94] search <ref> [HJ93] </ref> constraint satisfaction [FFG94] other [SLH94] other [Pin94] neural net, logic [Ass94] theorem proving, logic [SS94a] rule-based system [Sha93] neural net [AG93] search [Kum93] SN and related [AB93] neural net [WG93] CM-5 other [LHB93] Symmetry constraint satisfaction [Rob93] custom other [HIM94] custom other [LCV94] transputer neural net [OMT94] custom 10 other
Reference: [HJ94] <author> Walter Hower and Stephan Jacobi. </author> <title> Parallel distributed realization for constraint satisfaction. </title> <editor> In V. Kumar et al., editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <pages> pages 107-116, </pages> <address> New York, </address> <year> 1994. </year> <journal> North-Holland. </journal> <volume> volume 2. </volume>
Reference-contexts: [DN94] CM-2 vision, search [EAH93] CM-2 SN and related [Lan93] CM-2 SN, natural language [H + 94] IXM2 SN, natural language [O + 94] IXM2 SN and related [Fah93] NETL SN and related [Twa94] custom genetic, classifier systems [LH94] custom other [YK94] custom neural net, learning [CGK94] nCUBE2 512 other <ref> [HJ94] </ref> transputer 512 theorem proving, logic [BKS93] CM-2 1024 other [S + 93c] DADO2 1024 rule-based system [CGK93] nCUBE 1024 search [NM93] CM-2 4096 genetic, classifier systems [Gel94a] CM-2 8192 SN and related [CG94] CM-2 8192 search [KK93] CM-2 8192 search [JSS93] MP-1 8192 theorem proving, logic [PKF93] CM-2 16384 search
Reference: [HL91] <author> Richard Hughey and Daniel P. Lopresti. B-SYS: </author> <title> A 470-processor programmable systolic array. In Chuan lin Wu, editor, </title> <booktitle> Proc. Int. Conf. Parallel Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 580-583. </pages> <publisher> CRC Press, </publisher> <month> August </month> <year> 1991. </year>
Reference-contexts: Word-Wide ALU The conventional main ALU supports standard numeric and logic operations. Implementation details for analysis are based on the ALU in the Brown Systolic Array, which 151 is in turn loosely based on the Mead and Conway's OM2 ALU <ref> [HL91, MC80] </ref>. The circuit features great flexibility at the penalty of slightly reduced speed and increased transistor count. 5 It includes "universal logic blocks" (ULBs) which are capable of generating all Boolean functions of 2 or 3 inputs, and a Manchester carry chain.
Reference: [HLR93] <author> Richard Hughey, Robert Levinson, and James D. Roberts. </author> <title> Issues in parallel hardware for graph retrieval. </title> <booktitle> In Proceedings of the 1st International Conference on Conceptual Structures, </booktitle> <year> 1993. </year>
Reference-contexts: Figure 5.7 diagrams the overall structure of the algorithm. First graphs are selected for comparison; those selected could include all graphs (exhaustive comparison) [WWR91], those which pass filtering criteria (single-level indexing) [WWR91, Len88, Len92], or those passing multi-level indexing (initial graph SI tests filtering subsequent graphs) <ref> [LE92, HLR93] </ref>. Second, the match matrix M is formed. Third, the backtracking refinement search is performed. The refinement procedure is called at each branch of the backtracking search tree, dramatically pruning the number of branches explored. <p> Determining if the smaller organic compound of Figure 2.4 (page 19) is a generalization (subgraph) of the larger compound demonstrates that the refinement procedure alone is insufficient; backtracking search is still required in many cases, particularly those involving graphs with cycles <ref> [HLR93] </ref>. /******** Refinement Procedure ********/ bitmatrix: A, B, M REPEAT Mcopy = M /* R = M x B */ /* M = M and (not (A x not (R))) */ 9 (x) st (A (i; x) and not (R (x; j))) ] return "fail" UNTIL M=Mcopy return "unknown" Backtracking Ullman <p> The full algorithm terminates when either an isomorphism has been found or there are no more bindings to try. Hughey et al. present examples of the SI refinement algorithm on conceptual structures <ref> [HLR93] </ref>. <p> Previously, a parallel array of a thousand processors could handle only one to a few graphs with many more (O (lg N )) available at any stage in the MIS, comparisons would be farmed out to several processor arrays <ref> [HLR93] </ref> exactly as in Figure 5.14. <p> to the sub-linear scaling of P (the number of PEs), the memory per PE grows superlinearly, posing a significant design challenge. 5.4 Full Data Parallelism versus Pruned Search This section compares the speed of exhaustive search with full data parallelism against search with pruning techniques, summarizing the author's prior work <ref> [HLR93] </ref>. Considering the massive data parallelism of SIMD processing, it at first seems intuitive that placing one 107 graph per PE would be the fastest. All graphs are compared simultaneously, and any pruning heuristics would mask out certain PEs but would not reduce time to completion. <p> The superiority of nested parallelism with pruning over exhaustive data parallelism is analyzed by the author <ref> [HLR93] </ref>, and summarized in Section 5.4 Coarse-grain parallelism is explicit in the SEGUL notation, with independent parallel processes communicating by messages. A message is itself a nested collection, providing data communication between processes. Matching on certain elements of the message are used to activate and synchronize the processes.
Reference: [HO82] <author> Christoph M. Hoffmann and Michael J. O'Donnel. </author> <title> Pattern matching in trees. </title> <journal> Journal of the ACM, </journal> <volume> 29(1) </volume> <pages> 68-95, </pages> <month> January </month> <year> 1982. </year>
Reference-contexts: The paralation internal representation for nested collections indicates groupings at each level of nesting, supporting both bottom-up and top-down methods, such as those presented by Hoffmann and O'Donnel <ref> [HO82] </ref>. More specifically, Lee presents SIMD systolic algorithms for matching regular expressions in time O (n), where n is the length of the regular expression, using O (n) processing elements.
Reference: [Hoa78] <author> C. A. R. Hoare. </author> <title> Communicating sequential processes. </title> <journal> Communications of the ACM, </journal> <volume> 21(8) </volume> <pages> 666-667, </pages> <month> August </month> <year> 1978. </year> <month> 248 </month>
Reference-contexts: Only Occam and Gamma were parallel from the beginning. Occam was developed specifically for the Oc-cam follows the communicating sequential process model <ref> [Hoa78] </ref> rather than data-parallel processing. Each MIMD processor executes its own program and shares information by message passing. Computation continues after a send, but waits if a receive is executed and the information has not yet arrived. Occam also features multiple processes executing on a single microprocessor.
Reference: [Hop82] <author> J. J. </author> <title> Hopfield. Neural networks and physical systems with emergent collective computational abilities. </title> <booktitle> Proceedings of the National Academy of Sciences, </booktitle> <volume> 79 </volume> <pages> 2554-2558, </pages> <year> 1982. </year>
Reference-contexts: Kitano and Hendler note the emphasis on associative processing, marker passing, nearest neighbor and subsumption operations, memory-based reasoning, neural networks, and genetic algorithms [KH94]. Semantic networks (particularly Fahlman [Fah85]) and neural networks (particularly Hopfield, Rumelhart, and Hinton <ref> [Hop82, RHW86] </ref>) were especially influential in reviving work on massively parallel AI. 42 Paper Machine PEs Category [SJ94b] search [ACP93] search [CR94] vision [Hen94] search [HJ93] constraint satisfaction [FFG94] other [SLH94] other [Pin94] neural net, logic [Ass94] theorem proving, logic [SS94a] rule-based system [Sha93] neural net [AG93] search [Kum93] SN and
Reference: [Hug91] <author> Richard P. Hughey. </author> <title> Programmable sytolic arrays. </title> <type> Technical Report CS-92-34, </type> <institution> Department of Computer Science, Brown University, </institution> <year> 1991. </year>
Reference-contexts: System speed is 12.5 MHz with 32 PEs per 1.0 m CMOS chip. Other than a global-or for status checking, the MasPars provide no special support for associative or symbolic processing. BSYS BSYS is a 1-d systolic processor <ref> [Hug91] </ref>. Although this dissertation research does not survey application-specific systolic architectures, BSYS is a programmable systolic array and suggests several algorithms that could be efficiently executed on SIMD arrays using a systolic programming style. It influenced the specific MISC implementation proposed and analyzed in Section 7.2.1. <p> K 26,000 MIPS nCUBE2 [H + 86] 1989 MIMD, hypercube 8 K 61,440 MIPS April [A + 90] 1990 MIMD, 2,3-d mesh, dataflow 1000+ BLITZEN [B + 90a] 1990 SIMD, bit serial, 2-d mesh 16 K 3,000 MIPS iWarp [B + 90b] 1990 MIMD, 2-d mesh 64 1,300 MFLOPS BSYS <ref> [Hug91] </ref> 1990 systolic, 1-d mesh 470 108 MOPS NAP [KF90] 1990 SIMD, various simulated SNAP [M + 92] 1990 MSIMD, modified hypercube 144 CM-5 [Thi92] 1991 MSIMD, fat-tree, vector 1 K 128,000 MFLOPS Paragon [Hwa93] 1991 MIMD, 2-d mesh 300,000 MFLOPS IXM2 [H + 91] 1991 pseudo MIMD, hierarchical 64 278 <p> + b a = 0 a a _ b a b a = b c a ^ c a b c a b b a c a + 1 a b b a fi b a 1 a _ b a ^ b Table 7.2: ALU Output Functions (based on <ref> [Hug91, MC80] </ref>) Register Set The MISC-1 on-chip memory is a conventional multi-ported (1 read, 1 read/write) register set. For maximum throughput and flexibility the registers can supply 2 operands and store one result for each PE clock cycle. <p> b on c ac ^ bc select a or b on c ac ^ bc multiply step divide step there exists matching bit (s) 9i : a i = b i there exists mis-matching bit (s) 9i : a i 6= b i Table 7.3: Additional ALU Operations (based on <ref> [Hug91, MC80] </ref>) 1-Bit Unit The 1-bit unit, a special feature of the MISC architecture, consists of a 2-input ULB, single-bit flags, and 2 up-down counters. 6 In MISC the 1-bit unit is used for single-bit manipulations and flag management rather than as a bit-serial basis of all computation, and is closely <p> Except for the PE control section, these estimates are based on other implementations such as BSYS and PCAM <ref> [Hug91, GR90] </ref>, RTL, gate, or transistor level sketches of features unique to MISC. The additional control is estimated as 5% of the total PE transistor count. The transistor counts in the table are conservative, being the most straightforward and general approach. <p> Consider BSYS as a specific example. The BSYS processing element is essentially the same ALU as MISC (but is 8 rather than 16 bits) plus a register set and bank of 1-bit flags <ref> [Hug91] </ref>, so that MISC is for the most part a superset of the architecture. Its shared register architecture combines memory access and systolic communication, so that MISC requires additional cycles for communication. <p> Calculated speed was also in line with [S + 91]. The CAM was validated against 2 static chips [J + 88, O + 89] and one dynamic chip [WS89] and showed good agreement. The Suspense cell-area model was compared to numerous layouts <ref> [WE88, Hug91, Rob89b] </ref>. Average error was 24% with a standard deviation of 17%. <p> Suspense's accuracy was surprisingly good considering the numerous circuit `tricks' used in these experimental memories. Die validation included two SIMD processor chips (BLITZEN and the MasPar MP-2 [B + 90a, Mas92a]), a systolic array (BSYS <ref> [Hug91] </ref>), a bit-serial CAM (PCAM [Rob89b]) and a CAM with a small 66 transistor bit-serial PE per word ([WS89]).
Reference: [Hwa93] <author> Kai Hwang. </author> <title> Advanced Computer Architecture. </title> <publisher> McGraw Hill, </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: 16 K 3,000 MIPS iWarp [B + 90b] 1990 MIMD, 2-d mesh 64 1,300 MFLOPS BSYS [Hug91] 1990 systolic, 1-d mesh 470 108 MOPS NAP [KF90] 1990 SIMD, various simulated SNAP [M + 92] 1990 MSIMD, modified hypercube 144 CM-5 [Thi92] 1991 MSIMD, fat-tree, vector 1 K 128,000 MFLOPS Paragon <ref> [Hwa93] </ref> 1991 MIMD, 2-d mesh 300,000 MFLOPS IXM2 [H + 91] 1991 pseudo MIMD, hierarchical 64 278 MIPS Paradigm [CGB91] 1991 MIMD, hierarchical 100+ Coherent [Sto91] 1992 SIMD, CAM 4 K MasPar MP-2 [Mas92a] 1992 SIMD, 2-d mesh, MIN 16 K 68,000 MIPS Cray T3D [Oed93] 1993 MIMD, 3-d mesh 2 <p> In the second model data in a single or distributed memory is shared between processors. Here the communication network serves to interconnect processors to memory rather than processor to processor. Mechanisms such as locks and semaphores are used to control access to data, maintaining consistent behavior and synchronization <ref> [Hwa93] </ref>. Most massively parallel machines use data parallelism in which a single operation is applied uniformly to a given collection of data. Data parallelism blurs the distinction between parallel extensions to existing languages and inherently parallel languages.
Reference: [IEE94] <institution> IEEE. Computer, </institution> <month> November </month> <year> 1994. </year> <note> Special issue on associative processing. </note>
Reference-contexts: Kitano and Hendler note that mainstream AI conferences now routinely publish work on massively parallel AI [KH94], and in a special issue of IEEE Computer on associative processing two of the six papers concern knowledge processing <ref> [IEE94] </ref>. Unfortunately, these publications arrived during the final stages of this dissertation and could not be integrated. <p> In the remainder of this section, we summarize these recent publications, noting the difference in approaches between parallel and massively parallel knowledge processing and the difficulties SIMD architecture and AI processing pose for one another. 2.5.1 AI Research and Applications on Massively Parallel Processors Table 2.5 lists papers from <ref> [K + 94a, K + 94b, KH94, IEE94, Gel93a] </ref> for parallel AI, defined here somewhat arbitrarily as using fewer than 512 processors. Table 2.6 does the same for massively parallel AI (work using 512 or more processors).
Reference: [ISI93] <author> ISI. </author> <title> MOSIS price list. </title> <type> email, </type> <institution> MOSIS fabrication service, </institution> <year> 1993. </year>
Reference-contexts: The cost analysis is conservative. Custom 132-pin PGA packages are currently $0.26 per pin <ref> [ISI93] </ref>, and 447-pin PGA sockets are under $0.04 per pin [Cor95]. <p> Otherwise die area is the active die area plus the total area of all the pads. The die cost-metric c mt combines fixed-cost (masking charges), per-unit costs, and yield. Coefficients approximate dollar cost in 1993 dollars based on regression to sample custom-fab pricing <ref> [ISI93, Tan92] </ref>. c mt = (1 + 1:8 (2 ))(20000=v mfr + 11A d )e A d d ; where v mfr is the target quantity of good die and d is the defect rate per cm 2 .
Reference: [J + 88] <editor> Simon R. Jones et al. </editor> <title> A 9-kbit associative memory for high-speed parallel processing applications. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> 23(2) </volume> <pages> 543-548, </pages> <year> 1988. </year>
Reference-contexts: Calculated speed was also in line with [S + 91]. The CAM was validated against 2 static chips <ref> [J + 88, O + 89] </ref> and one dynamic chip [WS89] and showed good agreement. The Suspense cell-area model was compared to numerous layouts [WE88, Hug91, Rob89b]. Average error was 24% with a standard deviation of 17%.
Reference: [Jae89a] <author> L. A. Jaeckel. </author> <title> An alternative design for a sparse distributed memory. </title> <type> Technical report, </type> <institution> Research Institute for Advanced Computer Science, NASA Ames Research Center, </institution> <year> 1989. </year>
Reference-contexts: In Jaeckel's selected-coordinate design, only some of the input pattern bits are processed for storage location activation, and in his hyperplane design each storage location corresponds to a hyperplane in the parameter space <ref> [Jae89a, Jae89b] </ref>. Rogers applied genetic learning to modifying the first layer of weights, significantly improving accuracy and developed methods for interpreting the resulting weights [Rog90]. <p> Jaeckel's variations, while reducing the number of bits to be compared in computing an individual location's distance, does not appear to offer speed benefits only for hard-wired special-purpose implementations <ref> [Jae89a, Jae89b] </ref>. For general-purpose parallel processing all input-bits must be broadcast to the array, requiring O (m) time. Inverted indexing turns out not to be an option for Jaeckel's methods for either serial or parallel implementations.
Reference: [Jae89b] <author> L. A. Jaeckel. </author> <title> A class of designs for a sparse distributed memory. </title> <type> Technical report, </type> <institution> Research Institute for Advanced Computer Science, NASA Ames Research Center, </institution> <year> 1989. </year>
Reference-contexts: In Jaeckel's selected-coordinate design, only some of the input pattern bits are processed for storage location activation, and in his hyperplane design each storage location corresponds to a hyperplane in the parameter space <ref> [Jae89a, Jae89b] </ref>. Rogers applied genetic learning to modifying the first layer of weights, significantly improving accuracy and developed methods for interpreting the resulting weights [Rog90]. <p> Jaeckel's variations, while reducing the number of bits to be compared in computing an individual location's distance, does not appear to offer speed benefits only for hard-wired special-purpose implementations <ref> [Jae89a, Jae89b] </ref>. For general-purpose parallel processing all input-bits must be broadcast to the array, requiring O (m) time. Inverted indexing turns out not to be an option for Jaeckel's methods for either serial or parallel implementations.
Reference: [JB94] <author> Todd Jochem and Shumeet Baluja. </author> <title> Massively parallel, adaptive, color image processing for autonomous road following. </title> <editor> In Hiroaki Kitano and James A. Hendler, editors, </editor> <booktitle> Massively Parallel Artificial Intelligence, </booktitle> <pages> pages 280-315, </pages> <address> Menlo Park, CA, 1994. </address> <publisher> AAAI Press / The MIT Press. </publisher>
Reference-contexts: SN and related [CG94] CM-2 8192 search [KK93] CM-2 8192 search [JSS93] MP-1 8192 theorem proving, logic [PKF93] CM-2 16384 search [PW94] CM-2 16384 natural language [WD94] CM-2 16384 neural net, learning [ND94] CM-2 16384 neural net, learning [AHEK94] CM-2 16384 SN and related [EAH94] CM-2 16384 SN and related <ref> [JB94] </ref> MP-1 16384 vision, neural net [ZMHV93] MP-1 16384 neural net, learning [Coo93] CM-2 32768 search Table 2.6: Sample of Recent Massively Parallel AI Publications Prior work emphasizes a relatively small number of algorithms, with generally disappointing results for logic programming and expert systems.
Reference: [Jon92] <author> Fred Jones. </author> <title> A new era of fast dynamic RAMs. </title> <journal> IEEE Spectrum, </journal> <month> October </month> <year> 1992. </year>
Reference-contexts: Further, Rambus requires a large on-chip area per memory port. EDRAM <ref> [Bon92, Jon92, Har92, Ng92] </ref> extends DRAM with an on-chip SRAM "cache," taking advantage of the wide on-chip DRAM line size.
Reference: [JS94] <author> Jing-Fu Jenq and Sartaj Sahni. </author> <title> Image processing on reconfigurable meshes with buses. </title> <editor> In V. Kumar et al., editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <pages> pages 67-94, </pages> <address> New York, </address> <year> 1994. </year> <journal> North-Holland. </journal> <volume> volume 1. </volume>
Reference-contexts: vision [SS94b] many 64 theorem proving [NW93] PIM 70 theorem proving, logic [Ert93] LAN 110 search [Pea94] Fujitsu 128 theorem proving, logic [CMD94] SNAP 144 SN, natural language [Sat94] nCUBE3 256 natural language Table 2.5: Sample of Recent Parallel AI Publications 43 Paper Machine PEs Category [Kur93] theorem proving, logic <ref> [JS94] </ref> vision [Gel94b] SN and related [DB93] other [Kur94] theorem proving, logic [DN94] CM-2 vision, search [EAH93] CM-2 SN and related [Lan93] CM-2 SN, natural language [H + 94] IXM2 SN, natural language [O + 94] IXM2 SN and related [Fah93] NETL SN and related [Twa94] custom genetic, classifier systems [LH94] <p> The mesh bypass feature of the SIMD array improves performance for longer-distance communications. Similar reconfigurable-mesh networks are used in a variety of systems, for purposes such as fault-tolerance, machine vision, signal processing, sorting, and general routing <ref> [LS92, JS94, HCWS94] </ref>. Here we develop and evaluate the network for multiple Boolean matrix multiplications and paralation operations with nested parallelism. The mesh bypass is beneficial for Boolean matrix operations, supporting broadcast and array partitioning.
Reference: [JSS93] <author> Ken Jung, Evangelos Simoudis, and Ramesh Subramonian. </author> <title> Parallel induction systems based on branch and bound. </title> <editor> In James Geller, editor, </editor> <booktitle> Innovative Applications of Massive Parallelism, </booktitle> <pages> pages 106-112. </pages> <publisher> AAAI, </publisher> <year> 1993. </year> <booktitle> Working Notes, Spring Symposium Series, </booktitle> <publisher> Stanford University. </publisher>
Reference-contexts: [YK94] custom neural net, learning [CGK94] nCUBE2 512 other [HJ94] transputer 512 theorem proving, logic [BKS93] CM-2 1024 other [S + 93c] DADO2 1024 rule-based system [CGK93] nCUBE 1024 search [NM93] CM-2 4096 genetic, classifier systems [Gel94a] CM-2 8192 SN and related [CG94] CM-2 8192 search [KK93] CM-2 8192 search <ref> [JSS93] </ref> MP-1 8192 theorem proving, logic [PKF93] CM-2 16384 search [PW94] CM-2 16384 natural language [WD94] CM-2 16384 neural net, learning [ND94] CM-2 16384 neural net, learning [AHEK94] CM-2 16384 SN and related [EAH94] CM-2 16384 SN and related [JB94] MP-1 16384 vision, neural net [ZMHV93] MP-1 16384 neural net, learning
Reference: [K + 86] <author> Toshiro Kondo et al. </author> <title> Pseudo MIMD array processor | AAP2. </title> <booktitle> In 13th Conf. on Computer Architecture, </booktitle> <pages> pages 330-377, </pages> <year> 1986. </year>
Reference-contexts: The H.A.P.S. is a hierarchical associative processor. The lower level performs high-speed matching via a 16-bit ALU, and includes an additional adder for local addressing. The second level performs slower disk data-stream processing. The AAP2, designed for circuit simulation, can modify bit-level instructions based on a local table <ref> [K + 86] </ref>. PASM is a research platform for AI applications, including vision, speech understanding, and expert systems [MSS85]. PASM has SIMD and MIMD operating modes in a partitionable topology scalable to a thousand processors. Paradigm and the much older cm* feature hierarchical communication networks and multi-level caching [CGB91, SFS77]. <p> MPP [Bat82] 1983 SIMD, bit-serial, 2-d mesh 16 K 3,000 MIPS CM-1 [AG89] 1985 SIMD, bit serial, hypercube 64 K 2,000 MIPS H.A.P.S. [Stu85] 1985 SIMD, bus, assoc. proposed nCUBE [H + 86] 1985 MIMD, hypercube 1 K 1,945 MIPS NON-VON [AG89] 1985 MSIMD, bit-byte, tree, MIN 16 K+ AAP2 <ref> [K + 86] </ref> 1986 SIMD, 2-d mesh, assoc. 64 K 10,000 MOPS CLIP7A [FMD88] 1986 SIMD, 2-d mesh 256 64 MIPS LUCAS [FKS85] 1986 SIMD, bit-serial, MIN, assoc. 128 CM-2 [TR88] 1987 SIMD, bit-serial, hypercube 64 K 4,000 MIPS AMT DAP [TW91] 1987 SIMD, 2-d mesh, orth. bus 1 K 800
Reference: [K + 89] <author> Peter Kogge et al. </author> <title> VLSI and rule-based systems. </title> <editor> In Jose G. Delgado-Frias and Will R. Moore, editors, </editor> <booktitle> VLSI for Artificial Intelligence. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, Mass., </address> <year> 1989. </year>
Reference: [K + 93] <editor> Goro Kitsukawa et al. </editor> <title> 256-Mb DRAM circuit technologies for file applications. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> 28(11) </volume> <pages> 1105-1112, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Detailed information on 3 experimental 16 M bit SRAM chips [S + 93a, S + 93b, U + 93] and 3 experimental 256 M bit memories <ref> [T + 93, K + 93, H + 93] </ref> allowed accurate validation against advanced memory designs. Suspense's accuracy was surprisingly good considering the numerous circuit `tricks' used in these experimental memories.
Reference: [K + 94a] <editor> V. Kumar et al., editors. </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <volume> volume 1. </volume> <publisher> North-Holland, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: The summary highlights general characteristics of massively parallel AI and their requirements of computer architectures. Although a fairly new topic, massively parallel AI is gaining widespread acceptance. In a recent two-volume book on parallel AI, over one fourth of the papers are on massively parallel AI <ref> [K + 94a, K + 94b] </ref>. Kitano and Hendler note that mainstream AI conferences now routinely publish work on massively parallel AI [KH94], and in a special issue of IEEE Computer on associative processing two of the six papers concern knowledge processing [IEE94]. <p> In the remainder of this section, we summarize these recent publications, noting the difference in approaches between parallel and massively parallel knowledge processing and the difficulties SIMD architecture and AI processing pose for one another. 2.5.1 AI Research and Applications on Massively Parallel Processors Table 2.5 lists papers from <ref> [K + 94a, K + 94b, KH94, IEE94, Gel93a] </ref> for parallel AI, defined here somewhat arbitrarily as using fewer than 512 processors. Table 2.6 does the same for massively parallel AI (work using 512 or more processors). <p> Most conventional AI programs have limited coarse-grain (control-level) parallelism; efforts to parallelize existing serial AI codes generally show poor processor utilization beyond 16 processors and negligible speedup beyond 32 <ref> [KT88, Kog91, WLL90, K + 94a, K + 94b, Gel93a, KH94] </ref>. This is true even for control-level parallel machines designed specifically for Prolog and LISP.
Reference: [K + 94b] <editor> V. Kumar et al., editors. </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <volume> volume 2. </volume> <publisher> North-Holland, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: The summary highlights general characteristics of massively parallel AI and their requirements of computer architectures. Although a fairly new topic, massively parallel AI is gaining widespread acceptance. In a recent two-volume book on parallel AI, over one fourth of the papers are on massively parallel AI <ref> [K + 94a, K + 94b] </ref>. Kitano and Hendler note that mainstream AI conferences now routinely publish work on massively parallel AI [KH94], and in a special issue of IEEE Computer on associative processing two of the six papers concern knowledge processing [IEE94]. <p> In the remainder of this section, we summarize these recent publications, noting the difference in approaches between parallel and massively parallel knowledge processing and the difficulties SIMD architecture and AI processing pose for one another. 2.5.1 AI Research and Applications on Massively Parallel Processors Table 2.5 lists papers from <ref> [K + 94a, K + 94b, KH94, IEE94, Gel93a] </ref> for parallel AI, defined here somewhat arbitrarily as using fewer than 512 processors. Table 2.6 does the same for massively parallel AI (work using 512 or more processors). <p> Most conventional AI programs have limited coarse-grain (control-level) parallelism; efforts to parallelize existing serial AI codes generally show poor processor utilization beyond 16 processors and negligible speedup beyond 32 <ref> [KT88, Kog91, WLL90, K + 94a, K + 94b, Gel93a, KH94] </ref>. This is true even for control-level parallel machines designed specifically for Prolog and LISP.
Reference: [Kan88] <author> Pentti Kanerva. </author> <title> Sparse Distributed Memory. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1988. </year> <month> 249 </month>
Reference-contexts: Other applications and the general characteristics of AI codes were also an important part of MISC's development. 2.2.1 Sparse Distributed Memory Kanerva's sparse distributed memory (SDM) <ref> [Kan88] </ref> implements a neural network associative memory but avoids the vector-matrix multiplication normally used to determine neural activations.
Reference: [Kan92] <author> Pentti Kanerva. </author> <title> Sparse distributed memory and related models. </title> <editor> In H. M. Hassoun, editor, </editor> <title> Associative Neural Memories: Theory and Implementation. </title> <publisher> Oxford University Press, </publisher> <address> New York, </address> <year> 1992. </year> <month> Pre-publication. </month>
Reference-contexts: Activated Locations Kanerva gives the optimal probability of activation to maximize the signal-to-noise ratio in SDM recall as p (2N T ) 1=3 where T is the total number of patterns to be stored <ref> [Kan92] </ref>. <p> the bound approaches 2 m ; for a small SDM (m = 256, a = 67), k 1=m = 1:017 and for a large SDM (m = 1024, a = 138) k 1=m = 1:005. 2 Storage capacity (number of uncorrupted patterns) has been shown to be O (N ) <ref> [Kan92, Kee88] </ref>; there may be an indirect relationship between the m and the number of patterns to be stored for good performance. 82 Therefore the expected-case number of points searched is (2 m ) and exhaustive search is faster where N &lt; (2 m ). 5.1.3 Alternative SDM Algorithms Alternatives to
Reference: [Kee88] <author> James D. Keeler. </author> <title> Comparison between Kanerva's SDM and Hopfield-type neural networks. </title> <journal> Cognitive Science, </journal> <volume> 12 </volume> <pages> 299-329, </pages> <year> 1988. </year>
Reference-contexts: Read ********/ when receive ['SDMread', thisPN, ?code (m) input]: activePlocs f x 2 plocs j hdistance (plocs (addr),input) radiusg foreach x 2 activePlocs sum sum + weights (x) code (m) output sum 0 send ['SDMresult',thisPN, output] ; a given output error rate " is the same as the Hopfield net <ref> [Kee88] </ref>, giving T :15N for " = :995. Kanerva suggests that the number of patterns stored be reduced to allow for approximate input and greater output accuracy, and suggests 1% to 5% of N . <p> the bound approaches 2 m ; for a small SDM (m = 256, a = 67), k 1=m = 1:017 and for a large SDM (m = 1024, a = 138) k 1=m = 1:005. 2 Storage capacity (number of uncorrupted patterns) has been shown to be O (N ) <ref> [Kan92, Kee88] </ref>; there may be an indirect relationship between the m and the number of patterns to be stored for good performance. 82 Therefore the expected-case number of points searched is (2 m ) and exhaustive search is faster where N &lt; (2 m ). 5.1.3 Alternative SDM Algorithms Alternatives to
Reference: [KF90] <author> Dradley C. Kuszmaul and Jeff Fried. </author> <title> NAP (No ALU Processor): The great communicator. </title> <journal> J. Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 169-179, </pages> <year> 1990. </year>
Reference-contexts: hypercube 8 K 61,440 MIPS April [A + 90] 1990 MIMD, 2,3-d mesh, dataflow 1000+ BLITZEN [B + 90a] 1990 SIMD, bit serial, 2-d mesh 16 K 3,000 MIPS iWarp [B + 90b] 1990 MIMD, 2-d mesh 64 1,300 MFLOPS BSYS [Hug91] 1990 systolic, 1-d mesh 470 108 MOPS NAP <ref> [KF90] </ref> 1990 SIMD, various simulated SNAP [M + 92] 1990 MSIMD, modified hypercube 144 CM-5 [Thi92] 1991 MSIMD, fat-tree, vector 1 K 128,000 MFLOPS Paragon [Hwa93] 1991 MIMD, 2-d mesh 300,000 MFLOPS IXM2 [H + 91] 1991 pseudo MIMD, hierarchical 64 278 MIPS Paradigm [CGB91] 1991 MIMD, hierarchical 100+ Coherent [Sto91]
Reference: [KH94] <editor> Hiroaki Kitano and James A. Hendler, editors. </editor> <booktitle> Massively Parallel Artificial Intelligence, </booktitle> <address> Menlo Park, CA, 1994. </address> <publisher> AAAI Press / The MIT Press. </publisher>
Reference-contexts: In a recent two-volume book on parallel AI, over one fourth of the papers are on massively parallel AI [K + 94a, K + 94b]. Kitano and Hendler note that mainstream AI conferences now routinely publish work on massively parallel AI <ref> [KH94] </ref>, and in a special issue of IEEE Computer on associative processing two of the six papers concern knowledge processing [IEE94]. Unfortunately, these publications arrived during the final stages of this dissertation and could not be integrated. <p> In the remainder of this section, we summarize these recent publications, noting the difference in approaches between parallel and massively parallel knowledge processing and the difficulties SIMD architecture and AI processing pose for one another. 2.5.1 AI Research and Applications on Massively Parallel Processors Table 2.5 lists papers from <ref> [K + 94a, K + 94b, KH94, IEE94, Gel93a] </ref> for parallel AI, defined here somewhat arbitrarily as using fewer than 512 processors. Table 2.6 does the same for massively parallel AI (work using 512 or more processors). <p> The parallel papers cover mostly logic programming and theorem proving, expert systems (rule-based systems), constraint satisfaction, and state-space search. As Kitano and Hendler note, this is for the most part serial reasoning <ref> [KH94] </ref>. Knowledge base machines, although their processor architectures are optimized for search, have almost exclusively limited their applications to supporting logic programming and production systems [KT88]. The work in Table 2.5 typically uses about 64 processors and achieves a factor of 16 to 32 speedup. <p> Many of these papers emphasize the difficulties of achieving good processor utilization and the importance of load balancing. Kitano and Hendler note the emphasis on associative processing, marker passing, nearest neighbor and subsumption operations, memory-based reasoning, neural networks, and genetic algorithms <ref> [KH94] </ref>. <p> Most conventional AI programs have limited coarse-grain (control-level) parallelism; efforts to parallelize existing serial AI codes generally show poor processor utilization beyond 16 processors and negligible speedup beyond 32 <ref> [KT88, Kog91, WLL90, K + 94a, K + 94b, Gel93a, KH94] </ref>. This is true even for control-level parallel machines designed specifically for Prolog and LISP.
Reference: [Kit94] <author> Hiroaki Kitano. </author> <title> The challenge of massive parallelism. </title> <editor> In Hiroaki Kitano and James A. Hendler, editors, </editor> <booktitle> Massively Parallel Artificial Intelligence, </booktitle> <pages> pages 1-51, </pages> <address> Menlo Park, CA, 1994. </address> <publisher> AAAI Press / The MIT Press. </publisher>
Reference-contexts: Semantic Networks and Structure Matching PARKA is a knowledge representation and inferencing system combining the features of frames and semantic networks [EH90, EAH94]. Hendler et al. present an algorithm for finding subgraph isomorphisms in the large "semantic network" graph given a query graph in linear time <ref> [Kit94] </ref>. This is, however, at the expense of O (N n 2 ) processors (to use the graph knowledge base terminology and symbols) as exhaustive parallelism is needed to achieve linear time; each node and edge in the network must have its own processor.
Reference: [KK93] <author> George Karypis and Vipin Kumar. </author> <title> Unstructured tree search on SIMD parallel computers: Experimental results. </title> <editor> In James Geller, editor, </editor> <booktitle> Innovative Applications of Massive Parallelism, </booktitle> <pages> pages 113-119. </pages> <publisher> AAAI, </publisher> <year> 1993. </year> <booktitle> Working Notes, Spring Symposium Series, </booktitle> <publisher> Stanford University. </publisher>
Reference-contexts: systems [LH94] custom other [YK94] custom neural net, learning [CGK94] nCUBE2 512 other [HJ94] transputer 512 theorem proving, logic [BKS93] CM-2 1024 other [S + 93c] DADO2 1024 rule-based system [CGK93] nCUBE 1024 search [NM93] CM-2 4096 genetic, classifier systems [Gel94a] CM-2 8192 SN and related [CG94] CM-2 8192 search <ref> [KK93] </ref> CM-2 8192 search [JSS93] MP-1 8192 theorem proving, logic [PKF93] CM-2 16384 search [PW94] CM-2 16384 natural language [WD94] CM-2 16384 neural net, learning [ND94] CM-2 16384 neural net, learning [AHEK94] CM-2 16384 SN and related [EAH94] CM-2 16384 SN and related [JB94] MP-1 16384 vision, neural net [ZMHV93] MP-1
Reference: [Kni91] <author> Thomas F. Knight Jr. </author> <title> Technologies for low latency interconnection switches. Computer Architecture News, </title> <month> March </month> <year> 1991. </year>
Reference-contexts: Most PCB stacking methods use peripheral connections, but connections through the PCB are part of current manufacturing technology. There are several "3-d" board stacking technologies offering hundreds to thousands of interconnects between boards, such as the AMP Interposer [LAS89] and the "button board" <ref> [Kni91] </ref>. At the board level, mating force poses significant restrictions on the number of connections and layers. The effective die edge-crossings can potentially be at less than the bonding pad pitch, giving advantage to array (versus peripheral) pad placement when multiple routing layers are available.
Reference: [Kog82] <author> Peter M. Kogge. </author> <title> An architectural trail to threaded-code systems. </title> <booktitle> Computer, </booktitle> <month> March </month> <year> 1982. </year>
Reference-contexts: Others have shown that 2-4 execution threads are usually sufficient to keep a processor busy in a multiprocessing environment [A + 90]. Multi-threading, in conjunction with a superscalar microprocessor would address medium-grain parallelism. Kogge and others <ref> [Kog91, Kog82, Ode91, PJK89, A + 87] </ref> have shown significant performance improvements from stack architectures for production systems.
Reference: [Kog91] <author> Peter M. Kogge. </author> <title> The Architecture of Symbolic Computers. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Most knowledge processing programs have limited coarse-grain (control) parallelism; efforts to parallelize existing codes generally show poor processor utilization beyond 16 processors and negligible speedup beyond 32 <ref> [KT88, Kog91, WLL90] </ref>. On the other hand, there are numerous effective codes developed specifically for massively parallel machines including neural networks and natural language processing [AAA93]. Fine-grain (massive) parallelism is highly encouraging but there has been limited work to date. <p> Similarly, Fahlman characterizes knowledge processing programs as matching operations on a large databases followed by minimal processing [Fah85], while Kogge emphasizes matching, substitution, sets and relations, and Levinson argues for inference and intelligent behavior based on pattern associativity <ref> [Kog91, Lev92b] </ref>. Associative Processing, SIMD Architecture, and AI Researchers and designers have long sought to exploit extensive low-level matching for improved performance by developing associative processors based on content-addressable memory (CAM). Indeed, the beginnings of this effort were contemporaneous with the development of programmable digital computers. <p> Instead, this section summarizes and compares examples of different architectural classes to provide a context for MISC. The reader is referred to several surveys of specialized database, knowledge base, neural network, CAM, LISP, Prolog, and functional programming machines <ref> [KT88, Kog91, DFM89, CD89, WLL90] </ref>. 2.3.1 Associative Processors Associative processors offer fast highly-parallel matching with low silicon area and minimal communication. <p> Most conventional AI programs have limited coarse-grain (control-level) parallelism; efforts to parallelize existing serial AI codes generally show poor processor utilization beyond 16 processors and negligible speedup beyond 32 <ref> [KT88, Kog91, WLL90, K + 94a, K + 94b, Gel93a, KH94] </ref>. This is true even for control-level parallel machines designed specifically for Prolog and LISP. <p> Fahlman's NETL semantic network model was among the first proposed massively parallel AI systems. Kogge emphasizes 58 matching, substitution, sets and relations, and Levinson builds a case for inference and intelligent behavior based on pattern associativity <ref> [Kog91, Lev92b, Lev91] </ref>. These views lend credence to the importance of an associative approach as a foundation for knowledge processing. Associative processing can clearly serve as a basis of knowledge processing systems. <p> One system showed a 50 times speedup for the Rete algorithm over a system without CAM and 10 times over other Rete parallelizations, providing an important advantage for production system applications <ref> [Kog91] </ref>. Applications that especially benefit from content-addressing include natural language processing, neural networks, pattern and symbolic recognition, and semantic networks [Kog91, M + 90, Sto91, WL89, YZ90]. Unfortunately, CAM hardware implementations require uniform-length fixed-location data and exhibit poor utilization of hardware resources for non-matching operations. <p> Applications that especially benefit from content-addressing include natural language processing, neural networks, pattern and symbolic recognition, and semantic networks <ref> [Kog91, M + 90, Sto91, WL89, YZ90] </ref>. Unfortunately, CAM hardware implementations require uniform-length fixed-location data and exhibit poor utilization of hardware resources for non-matching operations. <p> Others have shown that 2-4 execution threads are usually sufficient to keep a processor busy in a multiprocessing environment [A + 90]. Multi-threading, in conjunction with a superscalar microprocessor would address medium-grain parallelism. Kogge and others <ref> [Kog91, Kog82, Ode91, PJK89, A + 87] </ref> have shown significant performance improvements from stack architectures for production systems.
Reference: [Koh80] <author> Tuevo Kohonen. </author> <title> Content-Addressable Memories. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: Aristotle first formulated the classical laws of association in On Memory and Reminiscence. Concepts, memories, and sensations are said to be connected if they occur simultaneously (spatial association), if they occur in close succession (temporal association), if they are similar, or if they are contrary <ref> [Koh80] </ref>. Such work continues into the present, but associative recall and mappings alone are not enough. Although they can be a critical component of a knowledge processing system, additional mechanisms are necessary. <p> Unfortunately, CAM architectures require data to be in a fixed length, fixed location format. More importantly, as just discussed, associative memory and simple matching are not enough to provide thorough massively parallel support for knowledge processing. One early associative processor proposal, the ASP (1962 <ref> [Koh80] </ref>) showed that SIMD processing can support operations on relational structures, and more recently Willett et al. [WWR91] have demonstrated subgraph isomorphism on a general-purpose SIMD architecture. In general, however, SIMD architectures are restricted to well-structured problems that feature regular patterns of control and uniformly structured data. <p> Associative Memory Associative memory encompasses a wide variety of phenomena related to human memory performance. Kohonen defines two types of associative memory <ref> [Koh80] </ref>. The first, direct association (the most common usage of the term), refers to the recall of one or more patterns by the input of another pattern. <p> Second, indirect association involves inference via multiple intermediate associative mappings. In indirect association, structural relationships are an important part of the patterns <ref> [Koh80] </ref>. By these general definitions, all three of the example algorithms (SDM, semantic nets, and graph database) described in Section 2.2 are different types of associative memory. In addition to mappings and inference, pattern completion from a partial or erroneous input is an important feature of associative memory. <p> in 1.2 m CMOS could accelerate a workstation by a factor of nearly 100 for high-dimensional k-nearest neighbors search. 1 The `-norms are specific instances of the Minkowski distance metric, M (x; y) = P n where x and y are two vectors of length (dimensionality) n and 2 R <ref> [Koh80] </ref>. The ` 1 norm, also called Manhattan or city-block distance, is the Minkowski distance metric with of 1. When x and y are Boolean, the ` 1 norm is Hamming distance. The ` 2 norm is Euclidean distance. <p> The algorithms cover areas such as matrix multiplication and FFTs, simple graph algorithms, relational database operations, and image processing. ASP The Association-Storing Processor (ASP, 1967) is of both historical and architectural interest <ref> [Koh80] </ref>. Although apparently never built, the architecture foreshadowed recent work in semantic network parallel processors, relation-based processing, and routing algorithms. The design is a 2-dimensional mesh of PEs, each of which contains one word of CAM and routing circuitry. <p> His model features "descriptors," a method for using CAM fields as indices into large, dynamic data structures. This work is not is not included in Table 2.2 as it did not progress past a general architectural concept. Machine Year Description Processors Peak Performance ASP <ref> [Koh80] </ref> 1967 SIMD, 2-d mesh, assoc. proposed ILLIAC IV [AG89] 1968 SIMD, 2-d mesh 64 500 MFLOPS cm* [SFS77] 1977 MIMD, multibus clusters 100 10 MIPS UNC Cellular [AG89] 1979 MSIMD, reduction, tree 1 M MPP [Bat82] 1983 SIMD, bit-serial, 2-d mesh 16 K 3,000 MIPS CM-1 [AG89] 1985 SIMD, bit
Reference: [Kol93] <author> Janet Kolodner. </author> <title> Case-Based Reasoning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: Retrieval algorithms to date, however, have concentrated primarily on matching attribute vectors rather than relational structures and exhaustive or single-level indexed search. Case-based reasoning <ref> [Kol93] </ref> also follows this generic memory-based reasoning model, and appears ripe for parallelization and advanced retrieval techniques. In Chapter 4 we briefly summarize additional projects incorporating content-addressable memory in support of associative processing for traditional AI methodologies such as logic programming and rule-based systems.
Reference: [KT88] <author> Masaru Kitsuregawa and Hidehiko Tanaka, </author> <title> editors. Database Machines and Knowledge Base Machines. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, Mass., </address> <year> 1988. </year>
Reference-contexts: In addition to special-purpose machines for vision and image processing, contemporary work includes machines designed specifically for databases and knowledge base processing <ref> [KT88, S + 85, M + 92, H + 91] </ref>. Unfortunately these do not address the full needs of knowledge processing. Special-purpose machines lack the flexibility to perform well on differing algorithms and problem domains. <p> Unfortunately these do not address the full needs of knowledge processing. Special-purpose machines lack the flexibility to perform well on differing algorithms and problem domains. Knowledge base machines, typically optimized for search, are largely limited to supporting logic programming and production systems <ref> [KT88] </ref>. Relational database processors store n-ary relations independently, requiring time consuming operations to access and manipulate particular sets of relations. <p> Most knowledge processing programs have limited coarse-grain (control) parallelism; efforts to parallelize existing codes generally show poor processor utilization beyond 16 processors and negligible speedup beyond 32 <ref> [KT88, Kog91, WLL90] </ref>. On the other hand, there are numerous effective codes developed specifically for massively parallel machines including neural networks and natural language processing [AAA93]. Fine-grain (massive) parallelism is highly encouraging but there has been limited work to date. <p> Instead, this section summarizes and compares examples of different architectural classes to provide a context for MISC. The reader is referred to several surveys of specialized database, knowledge base, neural network, CAM, LISP, Prolog, and functional programming machines <ref> [KT88, Kog91, DFM89, CD89, WLL90] </ref>. 2.3.1 Associative Processors Associative processors offer fast highly-parallel matching with low silicon area and minimal communication. <p> As Kitano and Hendler note, this is for the most part serial reasoning [KH94]. Knowledge base machines, although their processor architectures are optimized for search, have almost exclusively limited their applications to supporting logic programming and production systems <ref> [KT88] </ref>. The work in Table 2.5 typically uses about 64 processors and achieves a factor of 16 to 32 speedup. The papers on massively parallel AI cover mostly semantic network and related methodologies, neural networks, and search. <p> Most conventional AI programs have limited coarse-grain (control-level) parallelism; efforts to parallelize existing serial AI codes generally show poor processor utilization beyond 16 processors and negligible speedup beyond 32 <ref> [KT88, Kog91, WLL90, K + 94a, K + 94b, Gel93a, KH94] </ref>. This is true even for control-level parallel machines designed specifically for Prolog and LISP.
Reference: [Kum93] <author> Deepak Kumar. </author> <title> An AI architecture based on message passing. </title> <editor> In James Geller, editor, </editor> <booktitle> Innovative Applications of Massive Parallelism, </booktitle> <pages> pages 127-132. </pages> <publisher> AAAI, </publisher> <year> 1993. </year> <booktitle> Working Notes, Spring Symposium Series, </booktitle> <publisher> Stanford University. </publisher>
Reference-contexts: Cost and Salzberg recently developed a symbolic-distance measure for a nearest-neighbors classifier, with good results in predicting protein secondary structure and identifying DNA promoter sequences [CS93]. Miscellaneous More traditional AI algorithms, such as IDA*, unstructured tree search, and general tree-recursive algorithms map well to SIMD and MSIMD architectures <ref> [Coo93, Kum93] </ref>, as do branch and bound and other state-space search methods. <p> and Hinton [Hop82, RHW86]) were especially influential in reviving work on massively parallel AI. 42 Paper Machine PEs Category [SJ94b] search [ACP93] search [CR94] vision [Hen94] search [HJ93] constraint satisfaction [FFG94] other [SLH94] other [Pin94] neural net, logic [Ass94] theorem proving, logic [SS94a] rule-based system [Sha93] neural net [AG93] search <ref> [Kum93] </ref> SN and related [AB93] neural net [WG93] CM-5 other [LHB93] Symmetry constraint satisfaction [Rob93] custom other [HIM94] custom other [LCV94] transputer neural net [OMT94] custom 10 other [SJ94a] transputer 16 theorem proving, logic [Gel93b] CM-5 32 SN and related [CS94] CM-5 32 other [GW94] WARP 32 vision [LP94] iPSC 32
Reference: [Kur90] <author> Raymond Kurzweil. </author> <title> The Age of Intelligent Machines. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1990. </year>
Reference-contexts: The potential for such manipulations to achieve results that would be said to require intelligence if performed by a human forms both a definition of AI and its most central tenet, the physical symbol hypothesis <ref> [Kur90] </ref>. There are numerous other definitions, and to most AI is a yet to be achieved goal so that other terms may be more appropriate for existing systems. <p> This work has recently gained renewed interest ([Sow84]) as an alternative to Russell and Whitehead's linear notation. In a survey book, Kurzweil describes knowledge as facts and their relationships and discusses the importance of inheritance, the characteristics of a set applying to its subsets, in knowledge processing <ref> [Kur90] </ref>. Knowledge processing systems often feature an ontology (a set of all objects and relationships in the knowledge system), elucidating inheritance and other relationships between concepts and categories.
Reference: [Kur93] <author> Franz Kurfess. </author> <title> Massive parallelism in logic. </title> <editor> In James Geller, editor, </editor> <booktitle> Innovative Applications of Massive Parallelism, </booktitle> <pages> pages 132-143. </pages> <publisher> AAAI, </publisher> <year> 1993. </year> <booktitle> Working Notes, Spring Symposium Series, </booktitle> <publisher> Stanford University. </publisher>
Reference-contexts: Pixel 64 rule-based system, vision [SS94b] many 64 theorem proving [NW93] PIM 70 theorem proving, logic [Ert93] LAN 110 search [Pea94] Fujitsu 128 theorem proving, logic [CMD94] SNAP 144 SN, natural language [Sat94] nCUBE3 256 natural language Table 2.5: Sample of Recent Parallel AI Publications 43 Paper Machine PEs Category <ref> [Kur93] </ref> theorem proving, logic [JS94] vision [Gel94b] SN and related [DB93] other [Kur94] theorem proving, logic [DN94] CM-2 vision, search [EAH93] CM-2 SN and related [Lan93] CM-2 SN, natural language [H + 94] IXM2 SN, natural language [O + 94] IXM2 SN and related [Fah93] NETL SN and related [Twa94] custom
Reference: [Kur94] <author> Franz Kurfess. </author> <title> Massive parallelism in inference systems. </title> <editor> In V. Kumar et al., editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <pages> pages 259-278, </pages> <address> New York, </address> <year> 1994. </year> <journal> North-Holland. </journal> <volume> volume 1. </volume>
Reference-contexts: 70 theorem proving, logic [Ert93] LAN 110 search [Pea94] Fujitsu 128 theorem proving, logic [CMD94] SNAP 144 SN, natural language [Sat94] nCUBE3 256 natural language Table 2.5: Sample of Recent Parallel AI Publications 43 Paper Machine PEs Category [Kur93] theorem proving, logic [JS94] vision [Gel94b] SN and related [DB93] other <ref> [Kur94] </ref> theorem proving, logic [DN94] CM-2 vision, search [EAH93] CM-2 SN and related [Lan93] CM-2 SN, natural language [H + 94] IXM2 SN, natural language [O + 94] IXM2 SN and related [Fah93] NETL SN and related [Twa94] custom genetic, classifier systems [LH94] custom other [YK94] custom neural net, learning [CGK94]
Reference: [L + 89] <author> David D. Lee et al. </author> <title> A VLSI chip set for a multiprocesspr workstation | part I: A RISC microprocessor with coprocessor interface and support for symbolic processing. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> 24(6) </volume> <pages> 1688-1697, </pages> <month> December </month> <year> 1989. </year> <month> 250 </month>
Reference-contexts: These are high-performance off-the-shelf microprocessors to take advantage of large commercial design efforts and the economies of high-volume production and existing assembler, compiler, and operating system software. Although there are several excellent experimental research projects designing microprocessors specifically for parallel processing and AI applications (including <ref> [A + 93a, L + 89] </ref>), it is doubtful that their special features will provide speeds significantly greater than those of lower-cost commercial processors. 147 The simulations described in Appendix C show that a third of the execution time for SDM and semantic networks is executed by the supervisor, even with
Reference: [L + 90] <author> M. J. Little et al. </author> <title> The 3-d computer. </title> <journal> Journal of VLSI Signal Processing, </journal> <volume> 2 </volume> <pages> 79-87, </pages> <year> 1990. </year>
Reference-contexts: Stacking multiple die is not generally attractive either. For true 3-d connections anywhere on the die surface it is necessary to diffuse metal completely through the die. Although there is existing work in this area, including a prototype SIMD processor composed of stacked wafers <ref> [L + 90] </ref>, this has not proven commercially viable. Die can be stacked with interconnect on the die edges, as described in Section 3.2.2 for the Suspense model, but this gives the same connection constraints as planar packaging.
Reference: [Lan93] <author> Trent E. Lange. </author> <title> Massively-parallel inferencing for natural language understanding and memory retrieval in structured spreading-activation networks. </title> <editor> In James Geller, editor, </editor> <booktitle> Innovative Applications of Massive Parallelism, </booktitle> <pages> pages 144-149. </pages> <publisher> AAAI, </publisher> <year> 1993. </year> <booktitle> Working Notes, Spring Symposium Series, </booktitle> <publisher> Stanford University. </publisher>
Reference-contexts: logic [CMD94] SNAP 144 SN, natural language [Sat94] nCUBE3 256 natural language Table 2.5: Sample of Recent Parallel AI Publications 43 Paper Machine PEs Category [Kur93] theorem proving, logic [JS94] vision [Gel94b] SN and related [DB93] other [Kur94] theorem proving, logic [DN94] CM-2 vision, search [EAH93] CM-2 SN and related <ref> [Lan93] </ref> CM-2 SN, natural language [H + 94] IXM2 SN, natural language [O + 94] IXM2 SN and related [Fah93] NETL SN and related [Twa94] custom genetic, classifier systems [LH94] custom other [YK94] custom neural net, learning [CGK94] nCUBE2 512 other [HJ94] transputer 512 theorem proving, logic [BKS93] CM-2 1024 other
Reference: [LAS89] <author> Y. C. Lee, J. P. Avery, and R. Su. </author> <title> Quick prototyping and manufacturing for HWSI-based supercompact systems. </title> <booktitle> In Government Microcircuit Applications Conference, </booktitle> <year> 1989. </year>
Reference-contexts: Most PCB stacking methods use peripheral connections, but connections through the PCB are part of current manufacturing technology. There are several "3-d" board stacking technologies offering hundreds to thousands of interconnects between boards, such as the AMP Interposer <ref> [LAS89] </ref> and the "button board" [Kni91]. At the board level, mating force poses significant restrictions on the number of connections and layers. The effective die edge-crossings can potentially be at less than the bonding pad pitch, giving advantage to array (versus peripheral) pad placement when multiple routing layers are available. <p> A Hypothetical Advanced Implementation Lee presents high-density protyping technology for stacking MCMs 9 which can dissipate up to 250 watts <ref> [LAS89] </ref>. Each MISC sub-array substrate would require about 900 interconnections (2-d mesh communication, control, power and ground). Using this technology, a 1 K PE PN, including 1 GByte memory, supervisor, and co-processor could be packaged within a 12 cm (5 in) cube. <p> Another advantage is that an entire PN can fit on a cost-effective sized MCM greatly reducing area and power and somewhat improving speed. For high-density application that do not require the additional memory an 8-PN system could be placed in the 100 cubic inch "prototyping stack" <ref> [LAS89] </ref>. 14 Adding large off-chip memory does not significantly affect the size or other characteristics of a 256 PE processor node using printed circuit board technology. It does, however, force a larger MCM substrate which is not cost-effective due to low MCM substrate yield.
Reference: [LCV94] <author> Yannick Lallement, Thierry Cornu, and Stephane Vialle. </author> <title> An abstract machine for implementing connectionist and hybrid systems on multi-processor architectures. </title> <editor> In V. Kumar et al., editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <pages> pages 11-28, </pages> <address> New York, </address> <year> 1994. </year> <journal> North-Holland. </journal> <volume> volume 2. </volume>
Reference-contexts: search [ACP93] search [CR94] vision [Hen94] search [HJ93] constraint satisfaction [FFG94] other [SLH94] other [Pin94] neural net, logic [Ass94] theorem proving, logic [SS94a] rule-based system [Sha93] neural net [AG93] search [Kum93] SN and related [AB93] neural net [WG93] CM-5 other [LHB93] Symmetry constraint satisfaction [Rob93] custom other [HIM94] custom other <ref> [LCV94] </ref> transputer neural net [OMT94] custom 10 other [SJ94a] transputer 16 theorem proving, logic [Gel93b] CM-5 32 SN and related [CS94] CM-5 32 other [GW94] WARP 32 vision [LP94] iPSC 32 constraint satisfaction [ZM94] transputer 48 constraint satisfaction [MNT94] AP1000 64 other [CGA94] Pixel 64 rule-based system, vision [SS94b] many 64
Reference: [LE92] <author> Robert A. Levinson and Gerard Ellis. </author> <title> Multi-level hierarchical retrieval. </title> <journal> Knowledge-Based Systems Journal, </journal> <volume> 5(3), </volume> <month> September </month> <year> 1992. </year>
Reference-contexts: Figure 5.7 diagrams the overall structure of the algorithm. First graphs are selected for comparison; those selected could include all graphs (exhaustive comparison) [WWR91], those which pass filtering criteria (single-level indexing) [WWR91, Len88, Len92], or those passing multi-level indexing (initial graph SI tests filtering subsequent graphs) <ref> [LE92, HLR93] </ref>. Second, the match matrix M is formed. Third, the backtracking refinement search is performed. The refinement procedure is called at each branch of the backtracking search tree, dramatically pruning the number of branches explored.
Reference: [Lee86] <author> Dik Lun Lee. </author> <title> ALTEP acellular processor for high-speed pattern matching. </title> <journal> New Generation Computing, </journal> <volume> 4 </volume> <pages> 225-244, </pages> <year> 1986. </year>
Reference-contexts: the word-wide ALU. 154 ALU 1-Bit Unit Memory Adder Other carry in global data carry in match carry out communication carry out resolver tag zero detect stack overflow PE enable shift in stack 1 empty write enable shift out stack 2 empty Table 7.4: Special-Purpose Flags regular expression matching algorithms <ref> [Lee86] </ref>, the counters could locally determine the number of instruction executions (or carry outs, etc.) before a PE sets its disable flag. It may be desirable to perform 1-bit, counter, and word-wide ALU operations simultaneously, such as in PCAM's technique for weighted Hamming distance.
Reference: [Leh92] <author> Fritz Lehmann. </author> <title> Semantic networks. </title> <booktitle> In Semantic Networks in Artificial Intelligence, </booktitle> <pages> pages 1-50. </pages> <publisher> Pergamon Press, </publisher> <year> 1992. </year>
Reference: [Lei85] <author> Charles E. Leiserson. Fat-trees: </author> <title> Universal networks for hardware-efficient supercomputing. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34(10):892-910, </volume> <month> October </month> <year> 1985. </year>
Reference-contexts: The smaller systems have negligible burden for future expansion. Hierarchical networks also simplify partitioning a system among multiple users. Networks such as fat trees can better exploit communication locality and adapt to particular communication patterns <ref> [Lei85] </ref>, especially important for AI applications, reducing hardware requirements and cost. <p> Further, hierarchical communication topologies such as fat-trees respond well to non-uniform communication patterns and make the best use of available communication resources to minimize latency and contention <ref> [Lei85] </ref>. Elaborate topologies may not be necessary, as simulation results show that simple interconnections such as a shared bus may permit MISC systems as many as a hundred PNs.
Reference: [Lei90] <author> Michael R. Leibowitz. </author> <title> System makers eye 3D chip pack. EDN News, </title> <month> April 19 </month> <year> 1990. </year>
Reference-contexts: The cost model also adds a typical cost per pin for each package type to approximate the added packaging cost. The model includes calculations for stacked-die (3-D packaging) based on data in <ref> [Lei90, VL90] </ref>. Module Model The Suspense module model uses the SUSPENS equations for module area, speed, and power, modified by the improved propagation delay calculation. Unlike SUSPENS, Suspense allows a mix of package sizes and specific package-interconnect topologies. <p> Emerging products use lines with a 310 m pitch one edge of the die stack <ref> [Lei90] </ref>. Even for large memory die this gives only 40-50 lines, and it is doubtful that more than 100 lines will become available. This is clearly insufficient for PE chips. <p> The coefficients correspond to $720 per 10 cm wafer at = 2 m. 226 B.5 Packaging Model The model includes calculations for stacked-die (3-D packaging) based on data in <ref> [Lei90, VL90] </ref>. As in the die model, the user can specify the number of each die to be included in the stack and the number on the critical-path for timing.
Reference: [Len88] <author> George G. Lendaris. </author> <title> Representing conceptual graphs for parallel processing. </title> <booktitle> In Conceptual Graphs Workshop, </booktitle> <year> 1988. </year>
Reference-contexts: Figure 5.7 diagrams the overall structure of the algorithm. First graphs are selected for comparison; those selected could include all graphs (exhaustive comparison) [WWR91], those which pass filtering criteria (single-level indexing) <ref> [WWR91, Len88, Len92] </ref>, or those passing multi-level indexing (initial graph SI tests filtering subsequent graphs) [LE92, HLR93]. Second, the match matrix M is formed. Third, the backtracking refinement search is performed.
Reference: [Len92] <author> George G. Lendaris. </author> <title> A neural-network approach to implementing conceptual graphs. </title> <editor> In Timothy E. Nagel et al., editors, </editor> <booktitle> Conceptual Structures, Current Research and Practice, chapter 8, </booktitle> <pages> pages 155-188. </pages> <publisher> Ellis Horwood, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: Figure 5.7 diagrams the overall structure of the algorithm. First graphs are selected for comparison; those selected could include all graphs (exhaustive comparison) [WWR91], those which pass filtering criteria (single-level indexing) <ref> [WWR91, Len88, Len92] </ref>, or those passing multi-level indexing (initial graph SI tests filtering subsequent graphs) [LE92, HLR93]. Second, the match matrix M is formed. Third, the backtracking refinement search is performed.
Reference: [Lev91] <author> R. A. Levinson. </author> <title> A self-organizing pattern retrieval system. </title> <journal> International J. of Intelligent Systems, </journal> <volume> 6 </volume> <pages> 717-738, </pages> <year> 1991. </year>
Reference-contexts: Fahlman's NETL semantic network model was among the first proposed massively parallel AI systems. Kogge emphasizes 58 matching, substitution, sets and relations, and Levinson builds a case for inference and intelligent behavior based on pattern associativity <ref> [Kog91, Lev92b, Lev91] </ref>. These views lend credence to the importance of an associative approach as a foundation for knowledge processing. Associative processing can clearly serve as a basis of knowledge processing systems.
Reference: [Lev92a] <author> R. A. Levinson. </author> <title> Pattern associativity and the retrieval of semantic networks. </title> <booktitle> Semantic Networks in AI, </booktitle> <volume> 23 </volume> <pages> 573-600, </pages> <year> 1992. </year>
Reference-contexts: Multi-Level Indexed Search Multi-level indexed search (MIS) techniques can dramatically reduce search time when individual comparisons are time consuming (such as with subgraph isomorphism) by reducing the number of necessary comparisons <ref> [Lev92a] </ref>. Instead of providing only a single level of indexing as in conventional databases, Levinson's Method III creates a partial ordering (poset) over the knowledge base (KB), typically using the subsumption relation more-general-than. Objects are screened by predecessors in the poset and in turn screen successors. <p> Method III empirically prunes by a factor of O (N= lg 2 N ), where N is the number of objects in the KB. It has been proven that Method III is superior to conventional single-level indexing if there is structure to be taken advantage of in the KB <ref> [Lev92a] </ref>. Baader, Hollunder, 20 and Nebel show Method III to be the fastest of all surveyed complex-object retrieval methods [BHN + 92]. In addition to significantly increasing the speed and efficiency of retrieval, Method III supports conceptual clustering, generalization, planning, parsing, rule selection, and machine learning.
Reference: [Lev92b] <author> Robert Levinson. </author> <title> Pattern associativity and the retrieval of semantic networks. </title> <journal> Journal of Computers and Mathematics with Applications, </journal> <volume> 23 </volume> <pages> 573-600, </pages> <year> 1992. </year>
Reference-contexts: Similarly, Fahlman characterizes knowledge processing programs as matching operations on a large databases followed by minimal processing [Fah85], while Kogge emphasizes matching, substitution, sets and relations, and Levinson argues for inference and intelligent behavior based on pattern associativity <ref> [Kog91, Lev92b] </ref>. Associative Processing, SIMD Architecture, and AI Researchers and designers have long sought to exploit extensive low-level matching for improved performance by developing associative processors based on content-addressable memory (CAM). Indeed, the beginnings of this effort were contemporaneous with the development of programmable digital computers. <p> Fahlman's NETL semantic network model was among the first proposed massively parallel AI systems. Kogge emphasizes 58 matching, substitution, sets and relations, and Levinson builds a case for inference and intelligent behavior based on pattern associativity <ref> [Kog91, Lev92b, Lev91] </ref>. These views lend credence to the importance of an associative approach as a foundation for knowledge processing. Associative processing can clearly serve as a basis of knowledge processing systems.
Reference: [Lev93] <author> R. A. Levinson. </author> <title> Exploiting the physics of state-space search. </title> <editor> In Susan Epstein and R. A. Levinson, editors, </editor> <booktitle> Proceedings AAAI Fall Symposium on Games: Planning and Learning, </booktitle> <pages> pages 157-165, </pages> <address> Menlo Park, CA, 1993. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Graph retrieval from a partial order is itself classification [ELR95]. Recent work by Levinson and Karplus demonstrates that state-space search can be represented and efficiently executed within this framework by representing the current state, the goal state, and the operators as graphs <ref> [Lev93, LK93] </ref>. Attribute inheritance and concept subsumption can be supported by a partial order, or more precisely in this context the transitive closure of the subsumption relations. Inference can also be viewed as the transitive closure over recursive relations.
Reference: [Lev94a] <author> Robert Levinson. </author> <title> Morph II, a universal agent: Progress report and proposal. </title> <type> Technical Report UCSC-CRL-94-22, </type> <institution> University of California, Santa Cruz, </institution> <year> 1994. </year> <month> 251 </month>
Reference-contexts: Stored information, including prior solutions and partial solutions, can be a powerful tool in reducing the amount of searching. There is also a trade-off between search and the complexity of knowledge representation <ref> [Lev94a] </ref>. Effective knowledge representation is thereby highly important in knowledge processing. In addition to including all non-redundant information, 4 a knowledge representation must support fast access and updates and efficient processing as well as compact storage.
Reference: [Lev94b] <author> Robert Levinson. UDS: </author> <title> A universal data structure. </title> <editor> In W. M. Tepfenhart, I. P. Dide, and J. F. Sowa, editors, </editor> <booktitle> Conceptual Structures: Theory and Practice, </booktitle> <pages> pages 230-250. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year> <booktitle> Lecture Notes in AI 835. </booktitle>
Reference-contexts: The author believes that this approach is applicable to a number of other approaches to knowledge processing. Here we briefly discuss a few examples. Combining Representations Levinson has recently proposed a universal data structure (UDS) that combines semantic network, conceptual graph, and relational database representations <ref> [Lev94b] </ref>. It incorporates features of neural networks, case-based reasoning, and the Rete match-compilation algorithm. The foundation of UDS is the representation of all data as sets of objects and mathematical relations over the objects, organized as in the multi-level indexed search methods. <p> Here we extract a few principles, including emphasis on the knowledge side of the knowledge-search tradeoff. The Pillars of AI base retrieval is shown separately, the distinction being that search generates and/or gathers information without regard to structure whereas retrieval utilizes structure to access pre-existing knowledge <ref> [Lev94b] </ref>. Knowledge representation (KR) is a central component of knowledge processing; a representation must enable efficient access and operations in addition to just containing the key data. Applications often receive significant speedup with an alternative KR, and in learning the KR can make the difference between practicality and impossibility. <p> productions with pointers to one another, with 1 As part of this long-range goal, the MISC associative arrays are intended primarily for massively parallel algorithms while the supervisor (or network of multiple supervisors) serves serial or coarse-grain parallel algorithms. 61 the predecessor of a graph indicating all its possible productions <ref> [Lev94b] </ref>. Additionally, weights associated with graphs can form the basis of learning, as in Morph [AL93]. Semantic networks support several types of inference through marker or message propagation [Fah85, M + 92]. Semantic networks and relational structure processing can also be combined in supporting AI. <p> Recent work by Levinson defines a "universal data structure" (UDS) combining the representations and benefits of relational data bases, semantic networks, conceptual graphs, the Rete algorithm, neural networks, and case based reasoning for a finite set of domain objects <ref> [Lev94b] </ref>. In UDS all data is represented by sets of tuples, and a partial order (poset) is maintained over data objects. It can be viewed as a three-tiered poset consisting of a node hierarchy, a relation hierarchy, a CG hierarchy, and additional pointers between objects at the various levels. <p> Graphs are an intuitive representation of complex objects, and can exploit ordering information in retrieval and other processing [ELR95]. Nested collections flexibly represent complex relational structures, as well as more conventional data structures such arrays and lists, using standard mathematical notation. Semantic networks are formally directed, labeled hypergraphs <ref> [Lev94b] </ref>. Both a directed hypergraph and a RDB relation instance correspond to a set of n-tuples. Peirce's existential graphs are hypergraphs in which an n-ary relation is a directed hyperedge of n ordered nodes (a label can be represented by an additional node) and a context is an undirected hyperedge.
Reference: [LG90] <author> D. B. Lenat and R. V. Guha. </author> <title> Building Large Knowledge-Based Systems. </title> <publisher> Addison Wesley, </publisher> <address> Reading, Mass., </address> <year> 1990. </year>
Reference-contexts: We fit parameters in the above equation to the two largest semantic networks for which we have the appropriate information. For a portion (40,000 nodes) of the cyc SN knowledge base <ref> [LG90] </ref>, the average path length is 22. Working backwards, this gives an effective arity of 1.62, suggesting that markers propagate on slightly half the network links. For a SNAP application with N = 32; 000, the average number of markers per cycle is 100 [DM93].
Reference: [LH94] <author> Ahmed Louri and James A. Hatch Jr. </author> <title> An optical parallel associative processor for high-speed database processing. </title> <booktitle> Computer, </booktitle> <pages> pages 65-72, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: [JS94] vision [Gel94b] SN and related [DB93] other [Kur94] theorem proving, logic [DN94] CM-2 vision, search [EAH93] CM-2 SN and related [Lan93] CM-2 SN, natural language [H + 94] IXM2 SN, natural language [O + 94] IXM2 SN and related [Fah93] NETL SN and related [Twa94] custom genetic, classifier systems <ref> [LH94] </ref> custom other [YK94] custom neural net, learning [CGK94] nCUBE2 512 other [HJ94] transputer 512 theorem proving, logic [BKS93] CM-2 1024 other [S + 93c] DADO2 1024 rule-based system [CGK93] nCUBE 1024 search [NM93] CM-2 4096 genetic, classifier systems [Gel94a] CM-2 8192 SN and related [CG94] CM-2 8192 search [KK93] CM-2
Reference: [LHB93] <author> Q. Y. Luo, P. G. Hendry, and J. T. Buchanan. </author> <title> Comparison of different approaches for solving distributed constraint satisfaction problems. </title> <editor> In James Geller, editor, </editor> <booktitle> Innovative Applications of Massive Parallelism, </booktitle> <pages> pages 150-159. </pages> <publisher> AAAI, </publisher> <year> 1993. </year> <booktitle> Working Notes, Spring Symposium Series, </booktitle> <publisher> Stanford University. </publisher>
Reference-contexts: on massively parallel AI. 42 Paper Machine PEs Category [SJ94b] search [ACP93] search [CR94] vision [Hen94] search [HJ93] constraint satisfaction [FFG94] other [SLH94] other [Pin94] neural net, logic [Ass94] theorem proving, logic [SS94a] rule-based system [Sha93] neural net [AG93] search [Kum93] SN and related [AB93] neural net [WG93] CM-5 other <ref> [LHB93] </ref> Symmetry constraint satisfaction [Rob93] custom other [HIM94] custom other [LCV94] transputer neural net [OMT94] custom 10 other [SJ94a] transputer 16 theorem proving, logic [Gel93b] CM-5 32 SN and related [CS94] CM-5 32 other [GW94] WARP 32 vision [LP94] iPSC 32 constraint satisfaction [ZM94] transputer 48 constraint satisfaction [MNT94] AP1000 64
Reference: [LJ91] <author> R. Mike Lea and Ian P. Jalowiecki. </author> <title> Associative massively parallel computers. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 79(4) </volume> <pages> 469-479, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Paradigm and the much older cm* feature hierarchical communication networks and multi-level caching [CGB91, SFS77]. WASP, currently under development, was originally intended for image and signal processing. 31 Target applications now include supporting expert systems, LISP, and Prolog with low level matching operations <ref> [LJ91] </ref>. Array modules can operate independently as SIMD or in groups as MIMD. Design principles emphasize waferscale integration (WSI) and fault tolerance. Each PE includes a 70 bit comparator, data and activity registers, a bit-serial adder and 1-bit flags. <p> 300,000 MFLOPS IXM2 [H + 91] 1991 pseudo MIMD, hierarchical 64 278 MIPS Paradigm [CGB91] 1991 MIMD, hierarchical 100+ Coherent [Sto91] 1992 SIMD, CAM 4 K MasPar MP-2 [Mas92a] 1992 SIMD, 2-d mesh, MIN 16 K 68,000 MIPS Cray T3D [Oed93] 1993 MIMD, 3-d mesh 2 K 300,000 MFLOPS WASP <ref> [LJ91] </ref> devel MSIMD, bypass ring 64 K 40,000 MIPS Table 2.2: Sample of Other Parallel Machines 32 2.3.6 Relationship to MISC The MISC architecture, developed through analysis of the example algorithms and the general characteristics of AI codes, follows many of the general trends in parallel processing.
Reference: [LK93] <author> R. Levinson and K. Karplus. </author> <title> Graph-isomorphism and experience-based planning. </title> <editor> In D. Subramaniam, editor, </editor> <booktitle> Proceedings Workshop on Knowledge Compilation and Speedup Learning, </booktitle> <address> Amherst, Mass., </address> <year> 1993. </year> <booktitle> Machine Learning Conference. </booktitle>
Reference-contexts: Graph retrieval from a partial order is itself classification [ELR95]. Recent work by Levinson and Karplus demonstrates that state-space search can be represented and efficiently executed within this framework by representing the current state, the goal state, and the operators as graphs <ref> [Lev93, LK93] </ref>. Attribute inheritance and concept subsumption can be supported by a partial order, or more precisely in this context the transitive closure of the subsumption relations. Inference can also be viewed as the transitive closure over recursive relations.
Reference: [Lou91] <author> Ahmed Loure. </author> <title> Design of an optical content-addressable parallel processor with applications to fast searching and information retrieval. </title> <editor> In V. K. Prasanna Kumar, editor, </editor> <booktitle> Proc. 5th International Parallel Processing Symposium, </booktitle> <pages> pages 234-239. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: The asymptotic times in the table require full data parallelism (one processing element per data item). CAM operations are highly implementation independent; several proposed optical CAMs provide virtually the same matching operations <ref> [AB91, Lou91] </ref>. Both types of CAM are restricted to static, fixed-length, ordered lists of attributes. Associative Processing CAM hardware and programming techniques can be extended to support more sophisticated matching and faster manipulation of the selected data.
Reference: [LP94] <author> Wei-Ming Lin and Wiktor K. Prasanna. </author> <title> Parallel algorithms and architectures for consistent labeling. </title> <editor> In V. Kumar et al., editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <pages> pages 335-364, </pages> <address> New York, </address> <year> 1994. </year> <journal> North-Holland. </journal> <volume> volume 1. </volume>
Reference-contexts: [AG93] search [Kum93] SN and related [AB93] neural net [WG93] CM-5 other [LHB93] Symmetry constraint satisfaction [Rob93] custom other [HIM94] custom other [LCV94] transputer neural net [OMT94] custom 10 other [SJ94a] transputer 16 theorem proving, logic [Gel93b] CM-5 32 SN and related [CS94] CM-5 32 other [GW94] WARP 32 vision <ref> [LP94] </ref> iPSC 32 constraint satisfaction [ZM94] transputer 48 constraint satisfaction [MNT94] AP1000 64 other [CGA94] Pixel 64 rule-based system, vision [SS94b] many 64 theorem proving [NW93] PIM 70 theorem proving, logic [Ert93] LAN 110 search [Pea94] Fujitsu 128 theorem proving, logic [CMD94] SNAP 144 SN, natural language [Sat94] nCUBE3 256 natural
Reference: [LS92] <editor> Hungwen Li and Quentin F. Stout, editors. </editor> <title> Reconfigurable Massively Parallel Computers. </title> <publisher> Prentis Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1992. </year>
Reference-contexts: 1 cycle per PE distance (up to 16 cycles in a 1024 PE array, plus at least 64 cycles set-up) [Mas92b]. 150 Several prior massively parallel processors incorporate a nearly identical network for recon-figurability, providing switching for fault-tolerance and improving performance for a variety of algorithms, especially image processing algorithms <ref> [LS92] </ref>. A separate multiple-response resolver network determines if a particular flag is set in any PE, and iterates to the next PE with that flag set, in logarithmic time. Array memory features a separate enhanced DRAM (EDRAM) for each PE. <p> The mesh bypass feature of the SIMD array improves performance for longer-distance communications. Similar reconfigurable-mesh networks are used in a variety of systems, for purposes such as fault-tolerance, machine vision, signal processing, sorting, and general routing <ref> [LS92, JS94, HCWS94] </ref>. Here we develop and evaluate the network for multiple Boolean matrix multiplications and paralation operations with nested parallelism. The mesh bypass is beneficial for Boolean matrix operations, supporting broadcast and array partitioning. <p> Other machines using partionable meshes (and PEs simpler than MISC's) also approach the CRCW lower bound for several image processing algorithms <ref> [LS92] </ref>. Summary To summarize, Tables 8.2 and 8.3 list the broad range of algorithms that could be demonstrated on the MISC architecture.
Reference: [Luc89] <author> D. Lucarella. </author> <title> Heuristics to locate the best document set in information retrieval systems. </title> <booktitle> In International Phoenix Conference on Computers and Communications. IEEE Communications Society, </booktitle> <year> 1989. </year>
Reference-contexts: Jaeckel's work to indicate improved recall accuracy, but the analysis is restricted to uncorrelated inputs. For inverted indexing, time is O (qr) where q is the average number of attributes relevant to a given query and r is the average number of records having a given attribute <ref> [Luc89] </ref>. For the case of SDM Phase 1 search with uniform distribution, q = :5m and r = :5N , so time is O (N m) and we are still no better off than exhaustive Hamming distance calculation unless the input vectors are sparse.
Reference: [M + 90] <author> Masato Motomura et al. </author> <title> A 1.2 million transistor, 33-MHz, 20-b dictionary search (DISP) ULSI with a 160-kb CAM. </title> <journal> Journal of Solid-State Circuits, </journal> <volume> 25(5) </volume> <pages> 1158-1165, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Two 20 K bit CAMs, one by Motomura et a. and one by Ogura et al. are among the largest research research chips <ref> [M + 90, O + 89] </ref>. Content-addressable memories are, however, generally restricted to "flat" data such as attribute lists and tuples in a relational database, and are usually slow at operations other than matching. We note a few exceptions below. <p> Applications that especially benefit from content-addressing include natural language processing, neural networks, pattern and symbolic recognition, and semantic networks <ref> [Kog91, M + 90, Sto91, WL89, YZ90] </ref>. Unfortunately, CAM hardware implementations require uniform-length fixed-location data and exhibit poor utilization of hardware resources for non-matching operations.
Reference: [M + 92] <author> Dan Moldovan et al. </author> <title> SNAP: Parallel processing applied to AI. </title> <journal> Computer, </journal> <volume> 25(5) </volume> <pages> 39-49, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: In addition to special-purpose machines for vision and image processing, contemporary work includes machines designed specifically for databases and knowledge base processing <ref> [KT88, S + 85, M + 92, H + 91] </ref>. Unfortunately these do not address the full needs of knowledge processing. Special-purpose machines lack the flexibility to perform well on differing algorithms and problem domains. <p> Moldovan considers matching to be one of the "inner loops" of reasoning, and comments that it is trivial for single attributes but difficult for structured relations <ref> [M + 92] </ref>. Similarly, Fahlman characterizes knowledge processing programs as matching operations on a large databases followed by minimal processing [Fah85], while Kogge emphasizes matching, substitution, sets and relations, and Levinson argues for inference and intelligent behavior based on pattern associativity [Kog91, Lev92b]. <p> The SNAP prototype with 144 processing nodes is several thousand times faster than contemporary workstation SN implementations on applications involving inheritance, recognition, probabilistic reasoning, and learning, such as integrated speech and natural language understanding <ref> [M + 92] </ref>. IXM2 The IXM2 is a semantic-network machine directly inspired by the NETL model, its developers viewing "episodic memory as the foundation of intelligence" [H + 94]. The specific semantic network model incorporates "group identifiers," in essence basic transitive-closure subsumption codes, to speed inheritance operations. <p> [A + 90] 1990 MIMD, 2,3-d mesh, dataflow 1000+ BLITZEN [B + 90a] 1990 SIMD, bit serial, 2-d mesh 16 K 3,000 MIPS iWarp [B + 90b] 1990 MIMD, 2-d mesh 64 1,300 MFLOPS BSYS [Hug91] 1990 systolic, 1-d mesh 470 108 MOPS NAP [KF90] 1990 SIMD, various simulated SNAP <ref> [M + 92] </ref> 1990 MSIMD, modified hypercube 144 CM-5 [Thi92] 1991 MSIMD, fat-tree, vector 1 K 128,000 MFLOPS Paragon [Hwa93] 1991 MIMD, 2-d mesh 300,000 MFLOPS IXM2 [H + 91] 1991 pseudo MIMD, hierarchical 64 278 MIPS Paradigm [CGB91] 1991 MIMD, hierarchical 100+ Coherent [Sto91] 1992 SIMD, CAM 4 K MasPar <p> This view is shared by Moldovan and the SNAP group (Section 2.3.4), who hold matching as the inner loop in reasoning, describing it as trivial for single keys but difficult for structured relations <ref> [M + 92] </ref>. Similarly, Fahlman characterizes most AI programs as a search in a large database followed by minimal processing, suggesting a massively parallel approach for AI primitives [Fah85]. Fahlman's NETL semantic network model was among the first proposed massively parallel AI systems. <p> Additionally, weights associated with graphs can form the basis of learning, as in Morph [AL93]. Semantic networks support several types of inference through marker or message propagation <ref> [Fah85, M + 92] </ref>. Semantic networks and relational structure processing can also be combined in supporting AI. <p> The 144 DSP processors in the SNAP prototype total 1,700 MIPS compared to the projected 30,000 MIPS for a 1 K PE single PN MISC. Moldovan proposes a larger SNAP with custom processing elements <ref> [M + 92] </ref>. <p> Marker propagation is actually more like a short program. Assuming 120 bytes to specify the 10 initial node markings, 160 bytes to specify propagation rules, and 120 bytes to specify Boolean and math operations (based loosely on the SNAP instruction set <ref> [M + 92] </ref>) a total of 400 bytes are needed per query. At 530 queries per second, this gives 0.2 Mbyte/s external communication.
Reference: [MA90] <author> Daniel P. Miranker and Archie D. Andrews. </author> <title> On balanced synchronous parallel computers for AI. </title> <booktitle> In International Conference on Parallel Processing, pages I489-I493. </booktitle> <publisher> CRC Press, </publisher> <year> 1990. </year>
Reference-contexts: wave (processed) 1 foreach x in wave adjacent f y j graph (source)=x (dest) g adjacent (marker) 1 5.2.2 Node-Based SN Representation The node-based representation uses an edge-list description where each tuple represents a particular node followed by a list of its outgoing edge destinations, as in Miranker's NETL simulator <ref> [MA90] </ref>, along with additional elements to indicate if the tuple has been marked and whether it has been processed or not. In this algorithm, shown in Figures 5.4 and 5.5, each marker propagation iteration creates a local communication. <p> straight storage, and Figure C.2 presents the code for transposed storage. 232 Figure C.1: The Straight-Storage SDM 2MISC Code 233 Figure C.2: The Transposed-Storage SDM 2MISC Code 234 C.3 Semantic Network C.3.1 Simulation Description The SN simulations are for a minimal semantic network model, similar to that used by Miranker <ref> [MA90] </ref>, with operation approximated by subsumption node matching, a probability that an outgoing edge will be activated, and each query proceeding through a number of propagation cycles bounded by the network diameter.
Reference: [Mal93] <author> David Maliniak. </author> <title> Laminated-film MCMs aim for mass market. </title> <booktitle> Electronic Design, </booktitle> <pages> pages 57-64, </pages> <month> November 11 </month> <year> 1993. </year>
Reference: [Mar69] <author> D. Marr. </author> <title> A theory of cerebellar cortex. </title> <journal> J. Physiology, </journal> <volume> 202 </volume> <pages> 437-470, </pages> <year> 1969. </year>
Reference-contexts: Kanerva's key contribution is the analysis of the associative memory's behavior in computational geometry terms, drawing directly from prior models of cerebellar function <ref> [Mar69, Alb91] </ref> to provide multi-layer functionality without backpropagation. It is both computationally efficient and biologically plausible. In SDM an m-dimensional input vector induces a f0; 1g m parameter space, which is sparsely populated by storage locations (weights).
Reference: [Mar90] <author> Tony R. Martinez. </author> <title> Smart memory architecture and methods. </title> <journal> Future Generation Computer Systems, </journal> <volume> 6 </volume> <pages> 145-162, </pages> <year> 1990. </year>
Reference-contexts: Each PE includes a 70 bit comparator, data and activity registers, a bit-serial adder and 1-bit flags. Memory is small, less than 16 K bytes per PE and communication is via a ring with bypass. Martinez proposed a "smart-memory" model architecture that combines CAM with additional processing <ref> [Mar90] </ref>. His model features "descriptors," a method for using CAM fields as indices into large, dynamic data structures. This work is not is not included in Table 2.2 as it did not progress past a general architectural concept. <p> The local address unit provides considerable flexibility, including local data-dependency in selecting data objects and portions of objects. The indirect addressing may prove especially important in methods such as Martinez' "descriptors" for accessing large, variable-sized associative data structures <ref> [Mar90] </ref>. The local address unit can double as an adder pipelined to the PE's word-wide ALU for computing distance metrics.
Reference: [Mas92a] <institution> MasPar Computer Corp., </institution> <address> Sunnyvale, CA. </address> <booktitle> The Design of the MasPar MP-2: A Cost Effective Massively Parallel Computer, </booktitle> <year> 1992. </year> <month> 252 </month>
Reference-contexts: Although a 1-bit wired-or line is provided for matching operations, far more support is needed; Blitzen is designed primarily for numeric computation. 27 The MasPar MP-1 (1989) [Nic90] and MP-2 (1992) <ref> [Mas92a] </ref> are contemporary commercial processors, and feature 32-bit PEs. The MP-2 increases on-chip datapath widths by a factor of eight and doubles bandwidth to off-chip memory, more than doubling speed over the MP-1. In addition to the ALU, a separate exponent unit speeds floating point operations. <p> 92] 1990 MSIMD, modified hypercube 144 CM-5 [Thi92] 1991 MSIMD, fat-tree, vector 1 K 128,000 MFLOPS Paragon [Hwa93] 1991 MIMD, 2-d mesh 300,000 MFLOPS IXM2 [H + 91] 1991 pseudo MIMD, hierarchical 64 278 MIPS Paradigm [CGB91] 1991 MIMD, hierarchical 100+ Coherent [Sto91] 1992 SIMD, CAM 4 K MasPar MP-2 <ref> [Mas92a] </ref> 1992 SIMD, 2-d mesh, MIN 16 K 68,000 MIPS Cray T3D [Oed93] 1993 MIMD, 3-d mesh 2 K 300,000 MFLOPS WASP [LJ91] devel MSIMD, bypass ring 64 K 40,000 MIPS Table 2.2: Sample of Other Parallel Machines 32 2.3.6 Relationship to MISC The MISC architecture, developed through analysis of the <p> Suspense's accuracy was surprisingly good considering the numerous circuit `tricks' used in these experimental memories. Die validation included two SIMD processor chips (BLITZEN and the MasPar MP-2 <ref> [B + 90a, Mas92a] </ref>), a systolic array (BSYS [Hug91]), a bit-serial CAM (PCAM [Rob89b]) and a CAM with a small 66 transistor bit-serial PE per word ([WS89]).
Reference: [Mas92b] <institution> MasPar Computer Corp., Sunnyvale, CA. </institution> <note> MasPar Programming Language Reference Manual, </note> <year> 1992. </year>
Reference-contexts: Mesh communications in the MasPar MP-2 require at least 1 cycle per PE distance (up to 16 cycles in a 1024 PE array, plus at least 64 cycles set-up) <ref> [Mas92b] </ref>. 150 Several prior massively parallel processors incorporate a nearly identical network for recon-figurability, providing switching for fault-tolerance and improving performance for a variety of algorithms, especially image processing algorithms [LS92].
Reference: [MC80] <author> Carver Mead and Lynn Conway. </author> <title> Introduction to VLSI Systems. </title> <publisher> Addison Wesley, </publisher> <address> Reading, Mass., </address> <year> 1980. </year>
Reference-contexts: Word-Wide ALU The conventional main ALU supports standard numeric and logic operations. Implementation details for analysis are based on the ALU in the Brown Systolic Array, which 151 is in turn loosely based on the Mead and Conway's OM2 ALU <ref> [HL91, MC80] </ref>. The circuit features great flexibility at the penalty of slightly reduced speed and increased transistor count. 5 It includes "universal logic blocks" (ULBs) which are capable of generating all Boolean functions of 2 or 3 inputs, and a Manchester carry chain. <p> + b a = 0 a a _ b a b a = b c a ^ c a b c a b b a c a + 1 a b b a fi b a 1 a _ b a ^ b Table 7.2: ALU Output Functions (based on <ref> [Hug91, MC80] </ref>) Register Set The MISC-1 on-chip memory is a conventional multi-ported (1 read, 1 read/write) register set. For maximum throughput and flexibility the registers can supply 2 operands and store one result for each PE clock cycle. <p> b on c ac ^ bc select a or b on c ac ^ bc multiply step divide step there exists matching bit (s) 9i : a i = b i there exists mis-matching bit (s) 9i : a i 6= b i Table 7.3: Additional ALU Operations (based on <ref> [Hug91, MC80] </ref>) 1-Bit Unit The 1-bit unit, a special feature of the MISC architecture, consists of a 2-input ULB, single-bit flags, and 2 up-down counters. 6 In MISC the 1-bit unit is used for single-bit manipulations and flag management rather than as a bit-serial basis of all computation, and is closely
Reference: [Men91] <author> Lawrence Mennemeier. </author> <title> Hardware mechanisms to support concurrent threads on RISC and superscalar multiprocessors. </title> <type> Master's thesis, </type> <institution> University of California, Santa Cruz, </institution> <year> 1991. </year>
Reference-contexts: Rapid task switching (typically accomplished through hardware stacks) is also important to general operation, hiding global communication and shared-memory reference latencies, long associative array operations, and cache refills by executing other operations while awaiting results. Mennemeier <ref> [Men91] </ref> details an external co-processor for performing rapid task switching and state saving for conventional microprocessors. Others have shown that 2-4 execution threads are usually sufficient to keep a processor busy in a multiprocessing environment [A + 90]. Multi-threading, in conjunction with a superscalar microprocessor would address medium-grain parallelism.
Reference: [Mic89] <author> Micro Devices. </author> <title> Micro Devices Application Notes, MD1210 Fuzzy Set Comparator, </title> <month> December </month> <year> 1989. </year>
Reference-contexts: Among the numerous CAM integrated circuits developed in the last two decades, two are (unsuccessful) commercial offerings: the Am95C85, a CAM-based "articulated queue," [Adv88b, Adv88a] and the MD1210, a bit-serial fuzzy matcher <ref> [Mic89] </ref>. Two 20 K bit CAMs, one by Motomura et a. and one by Ogura et al. are among the largest research research chips [M + 90, O + 89].
Reference: [MLL90] <author> Dan Moldovan, Wing Lee, and Changhwa Lin. </author> <title> SNAP: A marker-passing architecture for knowledge processing. </title> <type> Technical Report PKPL 90-4, </type> <institution> University of Southern California, </institution> <year> 1990. </year>
Reference-contexts: SNAP SNAP combines CAM and additional processing to support the marker-passing SN paradigm [DeM92]. The initial design and long-range plans include fully custom processors combining CAM (for relation memory) with control, communication, marker, and ALU units <ref> [MLL90] </ref>. The current prototype implements these functions with several standard digital signal processing chips and multi-port memories.
Reference: [MNT94] <author> Takao Mohri, Masaaki Nakamura, and Hidehiko Tanaka. </author> <title> Weather forecasting using memory-based reasoning. </title> <editor> In V. Kumar et al., editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <pages> pages 185-198, </pages> <address> New York, </address> <year> 1994. </year> <journal> North-Holland. </journal> <volume> volume 2. </volume>
Reference-contexts: CM-5 other [LHB93] Symmetry constraint satisfaction [Rob93] custom other [HIM94] custom other [LCV94] transputer neural net [OMT94] custom 10 other [SJ94a] transputer 16 theorem proving, logic [Gel93b] CM-5 32 SN and related [CS94] CM-5 32 other [GW94] WARP 32 vision [LP94] iPSC 32 constraint satisfaction [ZM94] transputer 48 constraint satisfaction <ref> [MNT94] </ref> AP1000 64 other [CGA94] Pixel 64 rule-based system, vision [SS94b] many 64 theorem proving [NW93] PIM 70 theorem proving, logic [Ert93] LAN 110 search [Pea94] Fujitsu 128 theorem proving, logic [CMD94] SNAP 144 SN, natural language [Sat94] nCUBE3 256 natural language Table 2.5: Sample of Recent Parallel AI Publications 43
Reference: [Mor90] <author> Larry L. Moresco. </author> <title> Electronic system packaging: The search for manufacturing the optimum in a sea of constraints. </title> <journal> IEEE Transactions on Components, Hybrids, and Manufacturing Technology, </journal> <volume> 13(3) </volume> <pages> 494-508, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: The user can specify an off-chip communication voltage V c different from the die V dd to allow modeling reduced voltage-swing outputs. The package cost-metric also includes cooling, based on data published by Moresco <ref> [Mor90] </ref>. Natural convection is adequate below 0.05 W=cm 2 and no cost is added. Forced convection is needed between 0.05 and 0.15 W=cm 2 with cost approximated as 0:2W where W is the total package wattage.
Reference: [MQF91] <author> Johannes M. Mulder, Nhon T. Quach, and Michael J. Flynn. </author> <title> An area model for on-chip memories and its application. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> 26(2) </volume> <pages> 98-106, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: Rents rule is applied for type supercomputer (K p = 82 and fi = 0:25 [Bak90]) and all other module types (K p = 7 and fi = 0:21 <ref> [MQF91] </ref>). Area and capacitance for inter-module connections area added based on typical pad pitch and capacitance. Inter-chip communication power is calculated from C mi , V c , and module frequency. Total power adds that of all packages, adjusted to the module frequency. <p> Area and speed estimation are somewhat better for microprocessors, typically 1% to 15% (average 1). The tile models received the most extensive verification. Memory tiles were verified against register files presented by Mulder in <ref> [MQF91] </ref>, compiled multiport SRAM in [S + 91], and Don Speck's DRAM layout [Spe91]. All use standard fabrication rather than custom memory process.
Reference: [MSS85] <author> David G. Meyer, Howard Jay Siegel, and Thomas Schwederski. </author> <title> The PASM parallel system prototype. </title> <booktitle> In Proceedings of COMPCON, </booktitle> <pages> pages 429 - 434. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1985. </year>
Reference-contexts: The second level performs slower disk data-stream processing. The AAP2, designed for circuit simulation, can modify bit-level instructions based on a local table [K + 86]. PASM is a research platform for AI applications, including vision, speech understanding, and expert systems <ref> [MSS85] </ref>. PASM has SIMD and MIMD operating modes in a partitionable topology scalable to a thousand processors. Paradigm and the much older cm* feature hierarchical communication networks and multi-level caching [CGB91, SFS77]. <p> MIPS LUCAS [FKS85] 1986 SIMD, bit-serial, MIN, assoc. 128 CM-2 [TR88] 1987 SIMD, bit-serial, hypercube 64 K 4,000 MIPS AMT DAP [TW91] 1987 SIMD, 2-d mesh, orth. bus 1 K 800 MFLOPS GAPP [NCR87] 1987 SIMD, 1,2-d mesh 2 K 900 MOPS DADO [AG89] 1987 MIMD, tree 8 K PASM <ref> [MSS85] </ref> 1987 MSIMD, bus, xbar 1 K TRAC [AG89] 1987 MSIMD, banyan 4 FAIM [A + 87] 1987 MSIMD, 2-d mesh 1 K+ MasPar MP-1 [Nic90] 1989 SIMD, 2-d mesh, MIN 16 K 26,000 MIPS nCUBE2 [H + 86] 1989 MIMD, hypercube 8 K 61,440 MIPS April [A + 90] 1990
Reference: [NCR87] <author> NCR Corporation. </author> <title> Geometric Arithmetic Parallel Processor Data Sheet, </title> <year> 1987. </year>
Reference-contexts: 1986 SIMD, 2-d mesh, assoc. 64 K 10,000 MOPS CLIP7A [FMD88] 1986 SIMD, 2-d mesh 256 64 MIPS LUCAS [FKS85] 1986 SIMD, bit-serial, MIN, assoc. 128 CM-2 [TR88] 1987 SIMD, bit-serial, hypercube 64 K 4,000 MIPS AMT DAP [TW91] 1987 SIMD, 2-d mesh, orth. bus 1 K 800 MFLOPS GAPP <ref> [NCR87] </ref> 1987 SIMD, 1,2-d mesh 2 K 900 MOPS DADO [AG89] 1987 MIMD, tree 8 K PASM [MSS85] 1987 MSIMD, bus, xbar 1 K TRAC [AG89] 1987 MSIMD, banyan 4 FAIM [A + 87] 1987 MSIMD, 2-d mesh 1 K+ MasPar MP-1 [Nic90] 1989 SIMD, 2-d mesh, MIN 16 K 26,000
Reference: [ND94] <author> Valeriy I. Nenov and Michael G. Dyer. </author> <title> Language learning via perceptual/motor association: A massively parallel model. </title> <editor> In Hiroaki Kitano and James A. Hendler, editors, </editor> <booktitle> Massively Parallel Artificial Intelligence, </booktitle> <pages> pages 202-245, </pages> <address> Menlo Park, CA, 1994. </address> <publisher> AAAI Press / The MIT Press. </publisher>
Reference-contexts: 93c] DADO2 1024 rule-based system [CGK93] nCUBE 1024 search [NM93] CM-2 4096 genetic, classifier systems [Gel94a] CM-2 8192 SN and related [CG94] CM-2 8192 search [KK93] CM-2 8192 search [JSS93] MP-1 8192 theorem proving, logic [PKF93] CM-2 16384 search [PW94] CM-2 16384 natural language [WD94] CM-2 16384 neural net, learning <ref> [ND94] </ref> CM-2 16384 neural net, learning [AHEK94] CM-2 16384 SN and related [EAH94] CM-2 16384 SN and related [JB94] MP-1 16384 vision, neural net [ZMHV93] MP-1 16384 neural net, learning [Coo93] CM-2 32768 search Table 2.6: Sample of Recent Massively Parallel AI Publications Prior work emphasizes a relatively small number of
Reference: [Ng92] <author> Ray Ng. </author> <title> Fast computer memories. </title> <journal> IEEE Spectrum, </journal> <month> October </month> <year> 1992. </year>
Reference-contexts: Further, Rambus requires a large on-chip area per memory port. EDRAM <ref> [Bon92, Jon92, Har92, Ng92] </ref> extends DRAM with an on-chip SRAM "cache," taking advantage of the wide on-chip DRAM line size.
Reference: [NGC89] <author> Yan Ng, Raymond Glover, and Chew-Lye Chng. </author> <title> Unify with active memory. </title> <editor> In Jose G. Delgado-Frias and Will R. Moore, editors, </editor> <booktitle> VLSI for Artificial Intelligence. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, Mass., </address> <year> 1989. </year>
Reference: [Nic90] <author> John R. Nickolls. </author> <title> The design of the MasPar MP-1: A cost effective massively parallel computer. </title> <booktitle> In IEEE Proceedings of Compcon, </booktitle> <pages> pages 25-28, </pages> <year> 1990. </year>
Reference-contexts: Although a 1-bit wired-or line is provided for matching operations, far more support is needed; Blitzen is designed primarily for numeric computation. 27 The MasPar MP-1 (1989) <ref> [Nic90] </ref> and MP-2 (1992) [Mas92a] are contemporary commercial processors, and feature 32-bit PEs. The MP-2 increases on-chip datapath widths by a factor of eight and doubles bandwidth to off-chip memory, more than doubling speed over the MP-1. In addition to the ALU, a separate exponent unit speeds floating point operations. <p> mesh, orth. bus 1 K 800 MFLOPS GAPP [NCR87] 1987 SIMD, 1,2-d mesh 2 K 900 MOPS DADO [AG89] 1987 MIMD, tree 8 K PASM [MSS85] 1987 MSIMD, bus, xbar 1 K TRAC [AG89] 1987 MSIMD, banyan 4 FAIM [A + 87] 1987 MSIMD, 2-d mesh 1 K+ MasPar MP-1 <ref> [Nic90] </ref> 1989 SIMD, 2-d mesh, MIN 16 K 26,000 MIPS nCUBE2 [H + 86] 1989 MIMD, hypercube 8 K 61,440 MIPS April [A + 90] 1990 MIMD, 2,3-d mesh, dataflow 1000+ BLITZEN [B + 90a] 1990 SIMD, bit serial, 2-d mesh 16 K 3,000 MIPS iWarp [B + 90b] 1990 MIMD,
Reference: [NM93] <author> J. Thomas Ngo and Joe Marks. </author> <title> Massively parallel genetic algorithm for physically correct articulated figure locomotion. </title> <editor> In James Geller, editor, </editor> <booktitle> Innovative Applications of Massive Parallelism, </booktitle> <pages> pages 167-173. </pages> <publisher> AAAI, </publisher> <year> 1993. </year> <booktitle> Working Notes, Spring Symposium Series, </booktitle> <publisher> Stanford University. </publisher>
Reference-contexts: [O + 94] IXM2 SN and related [Fah93] NETL SN and related [Twa94] custom genetic, classifier systems [LH94] custom other [YK94] custom neural net, learning [CGK94] nCUBE2 512 other [HJ94] transputer 512 theorem proving, logic [BKS93] CM-2 1024 other [S + 93c] DADO2 1024 rule-based system [CGK93] nCUBE 1024 search <ref> [NM93] </ref> CM-2 4096 genetic, classifier systems [Gel94a] CM-2 8192 SN and related [CG94] CM-2 8192 search [KK93] CM-2 8192 search [JSS93] MP-1 8192 theorem proving, logic [PKF93] CM-2 16384 search [PW94] CM-2 16384 natural language [WD94] CM-2 16384 neural net, learning [ND94] CM-2 16384 neural net, learning [AHEK94] CM-2 16384 SN
Reference: [NW93] <author> Katsumi Nitta and Stephen Wong. </author> <title> The role of parallelism in parallel inference applications. </title> <editor> In James Geller, editor, </editor> <booktitle> Innovative Applications of Massive Parallelism, </booktitle> <pages> pages 174-180. </pages> <publisher> AAAI, </publisher> <year> 1993. </year> <booktitle> Working Notes, Spring Symposium Series, </booktitle> <publisher> Stanford University. </publisher> <pages> 253 </pages>
Reference-contexts: net [OMT94] custom 10 other [SJ94a] transputer 16 theorem proving, logic [Gel93b] CM-5 32 SN and related [CS94] CM-5 32 other [GW94] WARP 32 vision [LP94] iPSC 32 constraint satisfaction [ZM94] transputer 48 constraint satisfaction [MNT94] AP1000 64 other [CGA94] Pixel 64 rule-based system, vision [SS94b] many 64 theorem proving <ref> [NW93] </ref> PIM 70 theorem proving, logic [Ert93] LAN 110 search [Pea94] Fujitsu 128 theorem proving, logic [CMD94] SNAP 144 SN, natural language [Sat94] nCUBE3 256 natural language Table 2.5: Sample of Recent Parallel AI Publications 43 Paper Machine PEs Category [Kur93] theorem proving, logic [JS94] vision [Gel94b] SN and related [DB93]
Reference: [O + 89] <author> Takeshi Ogura et al. </author> <title> A 20-kbit associative memory LSI for artificial intelligence machines. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> 24(4) </volume> <pages> 1014-1020, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Two 20 K bit CAMs, one by Motomura et a. and one by Ogura et al. are among the largest research research chips <ref> [M + 90, O + 89] </ref>. Content-addressable memories are, however, generally restricted to "flat" data such as attribute lists and tuples in a relational database, and are usually slow at operations other than matching. We note a few exceptions below. <p> Calculated speed was also in line with [S + 91]. The CAM was validated against 2 static chips <ref> [J + 88, O + 89] </ref> and one dynamic chip [WS89] and showed good agreement. The Suspense cell-area model was compared to numerous layouts [WE88, Hug91, Rob89b]. Average error was 24% with a standard deviation of 17%.
Reference: [O + 94] <author> Kozo Oi et al. </author> <title> Toward massively parallel spoken language translation. </title> <editor> In V. Kumar et al., editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <pages> pages 177-184, </pages> <address> New York, </address> <year> 1994. </year> <journal> North-Holland. </journal> <volume> volume 2. </volume>
Reference-contexts: Table 2.5: Sample of Recent Parallel AI Publications 43 Paper Machine PEs Category [Kur93] theorem proving, logic [JS94] vision [Gel94b] SN and related [DB93] other [Kur94] theorem proving, logic [DN94] CM-2 vision, search [EAH93] CM-2 SN and related [Lan93] CM-2 SN, natural language [H + 94] IXM2 SN, natural language <ref> [O + 94] </ref> IXM2 SN and related [Fah93] NETL SN and related [Twa94] custom genetic, classifier systems [LH94] custom other [YK94] custom neural net, learning [CGK94] nCUBE2 512 other [HJ94] transputer 512 theorem proving, logic [BKS93] CM-2 1024 other [S + 93c] DADO2 1024 rule-based system [CGK93] nCUBE 1024 search [NM93]
Reference: [Ode91] <author> Louis L. Odette. </author> <title> Intelligent Embedded Systems. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1991. </year>
Reference-contexts: Others have shown that 2-4 execution threads are usually sufficient to keep a processor busy in a multiprocessing environment [A + 90]. Multi-threading, in conjunction with a superscalar microprocessor would address medium-grain parallelism. Kogge and others <ref> [Kog91, Kog82, Ode91, PJK89, A + 87] </ref> have shown significant performance improvements from stack architectures for production systems.
Reference: [Oed93] <author> Wilfried Oed. </author> <title> The Cray Research Massively Parallel Processor System CRAY T3D. </title> <institution> Cray Research GmbH, Munich, Germany, </institution> <month> November 15 </month> <year> 1993. </year>
Reference-contexts: The system backplane includes a router for message-passing communication. T3D The Cray T3D (1993) is a 3-d mesh architecture designed for use in conjunction with the Cray Y-MP and C90 vector processors <ref> [Oed93] </ref>. Each processor is a DEC-Alpha RISC microprocessor with DRAM memory. Two processors share a routing interface to the communication network. Memory is distributed and globally addressed. The Alpha chips incorporate features for parallelism: a fetch-and-increment register, an atomic-swap register, and message processing independent of the ALU. <p> 1 K 128,000 MFLOPS Paragon [Hwa93] 1991 MIMD, 2-d mesh 300,000 MFLOPS IXM2 [H + 91] 1991 pseudo MIMD, hierarchical 64 278 MIPS Paradigm [CGB91] 1991 MIMD, hierarchical 100+ Coherent [Sto91] 1992 SIMD, CAM 4 K MasPar MP-2 [Mas92a] 1992 SIMD, 2-d mesh, MIN 16 K 68,000 MIPS Cray T3D <ref> [Oed93] </ref> 1993 MIMD, 3-d mesh 2 K 300,000 MFLOPS WASP [LJ91] devel MSIMD, bypass ring 64 K 40,000 MIPS Table 2.2: Sample of Other Parallel Machines 32 2.3.6 Relationship to MISC The MISC architecture, developed through analysis of the example algorithms and the general characteristics of AI codes, follows many of <p> Other algorithms and applications may also have higher communication rates. The 200 MByte/s links of the Cray T3D <ref> [Oed93] </ref> processor nodes could support as many as a thousand MISC PNs in a mesh topology, even at 20 MByte/s per node.
Reference: [OL92] <author> Stephen M. Omohundro and Chu-Chow Lim. </author> <title> The Sather language and libraries. </title> <type> Technical report, </type> <institution> International Computer Science Institute, University of Cali-fornia, Berkeley, </institution> <year> 1992. </year>
Reference-contexts: Of particular interest, the Sather object libraries include numerous neural network models, computational geometry routines (including k-D trees), graph algorithms, string search, parsers, image processing and numerous additional data structures <ref> [OL92] </ref>. 2.4.2 ASC ASC is a recent associative programming model featuring dynamic memory allocation of structured data as well as MSIMD operations [P + 94]. The underlying associative virtual machine can be emulated on a wide variety of platforms.
Reference: [Omo87] <author> Stephen M. Omohundro. </author> <title> Efficient algorithms with neural network behavior. </title> <type> Technical Report UIUCDCS-R-87-1331, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1987. </year>
Reference-contexts: Unfortunately, for nearest neighbor search O (2 k ) adjacent bins must be searched. In addition to summarizing k-D tree algorithms, Omohundro shows how nearest-neighbor search can serve as the basis for neural-net like behavior, including generalization, clustering, dimension reduction, and nonlinear mapping <ref> [Omo87] </ref>. Cost and Salzberg recently developed a symbolic-distance measure for a nearest-neighbors classifier, with good results in predicting protein secondary structure and identifying DNA promoter sequences [CS93]. <p> Phase 1 corresponds to a k-nearest neighbors search. Omohundro shows that the single closest match can be found in O (lg N ) time using a K-d tree search and claims that at most k adjacent bins must be searched for a k-NN search <ref> [Omo87] </ref>. Here k refers to the number of nearest neighbors sought (a for SDM), K the dimensionality of the search parameter space (m for SDM) and N the number of stored points (N "physical locations" for SDM).
Reference: [Omo91] <author> Stephen M. Omohundro. </author> <title> The Sather language. </title> <type> Technical report, </type> <institution> International Computer Science Institute, University of California, Berkeley, </institution> <year> 1991. </year>
Reference-contexts: It includes instructions to find the first marked PE and sequence to the next marked PE using a multiple response resolver network. Sather is a serial inherently object-oriented language which compiles to efficient C code. It is public domain, and two groups are involved in developing parallel versions <ref> [Omo91] </ref>. pSather is a MIMD implementation using monitors. It allows multiple threads within a single object, and implementation includes versions for the CM-5, Sequent Symmetry, and Sparc Stations. dSather is a data-parallel SIMD version for the MasPar MP-2 and Fujitsu AP1000.
Reference: [OMT94] <author> Gerald Ouvradou, Aymeric Poulain Maubant, and Andre Thepaut. </author> <title> Hybrid systems on a multi-grain parallel architecture. </title> <editor> In V. Kumar et al., editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <pages> pages 3-10, </pages> <address> New York, </address> <year> 1994. </year> <journal> North-Holland. </journal> <volume> volume 2. </volume>
Reference-contexts: vision [Hen94] search [HJ93] constraint satisfaction [FFG94] other [SLH94] other [Pin94] neural net, logic [Ass94] theorem proving, logic [SS94a] rule-based system [Sha93] neural net [AG93] search [Kum93] SN and related [AB93] neural net [WG93] CM-5 other [LHB93] Symmetry constraint satisfaction [Rob93] custom other [HIM94] custom other [LCV94] transputer neural net <ref> [OMT94] </ref> custom 10 other [SJ94a] transputer 16 theorem proving, logic [Gel93b] CM-5 32 SN and related [CS94] CM-5 32 other [GW94] WARP 32 vision [LP94] iPSC 32 constraint satisfaction [ZM94] transputer 48 constraint satisfaction [MNT94] AP1000 64 other [CGA94] Pixel 64 rule-based system, vision [SS94b] many 64 theorem proving [NW93] PIM
Reference: [P + 94] <author> Jerry Potter et al. </author> <title> ASC: An associative-computing paradigm. </title> <booktitle> Computer, </booktitle> <pages> pages 19-25, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Roughly half the languages allow explicit user control of communication. The remainder make communication implicit as part of the computation functions. Very 34 Base Parallel Language Year Type Language Side Garbage Matching Parallel Effects Collection Emphasized a APL [Ble90] 1962 serial * * ffi b ASC <ref> [P + 94] </ref> 1992 `all' * * * c C* [Ble90] SIMD ffi * ffi ffi d CM-LISP [Ble90] 1986 SIMD ffi * * ffi e Cray Fortran [Ble90] 1978 vector ffi * ffi ffi f Gamma [BM93] 1986 * ffi ffi * g Linda [CG89] 1982 MIMD ffi h *LISP <p> interest, the Sather object libraries include numerous neural network models, computational geometry routines (including k-D trees), graph algorithms, string search, parsers, image processing and numerous additional data structures [OL92]. 2.4.2 ASC ASC is a recent associative programming model featuring dynamic memory allocation of structured data as well as MSIMD operations <ref> [P + 94] </ref>. The underlying associative virtual machine can be emulated on a wide variety of platforms. Potter et al. have implemented ASC on the CM-2, Staran, Aspro, and other processors and propose associative processing 36 as a programming paradigm for massively parallel computing.
Reference: [Par90] <author> Behrooz Parhami. </author> <title> Scalable architectures for VLSI based associative memories. </title> <editor> In N. Rishe et al., editors, </editor> <booktitle> Parallel Architectures. </booktitle> <publisher> IEEE Computer Science Press, </publisher> <address> Washington, D.C., </address> <year> 1990. </year>
Reference-contexts: Suspense shows a linear reduction of gate delay in terms of , but increasing propagation delay below 1.2 m for a 1 cm line even with a driver transistor 10 times the minimum and a 4 gate load. This is in agreement with other studies, such as Parhami's <ref> [Par90] </ref>. As is reduced die size for the same number of transistors will be proportionately smaller and the typical line length shorter.
Reference: [Pea94] <author> Robert Pearson. </author> <title> A coarse grained parallel induction heuristic. </title> <editor> In V. Kumar et al., editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <pages> pages 207-226, </pages> <address> New York, </address> <year> 1994. </year> <journal> North-Holland. </journal> <volume> volume 2. </volume>
Reference-contexts: logic [Gel93b] CM-5 32 SN and related [CS94] CM-5 32 other [GW94] WARP 32 vision [LP94] iPSC 32 constraint satisfaction [ZM94] transputer 48 constraint satisfaction [MNT94] AP1000 64 other [CGA94] Pixel 64 rule-based system, vision [SS94b] many 64 theorem proving [NW93] PIM 70 theorem proving, logic [Ert93] LAN 110 search <ref> [Pea94] </ref> Fujitsu 128 theorem proving, logic [CMD94] SNAP 144 SN, natural language [Sat94] nCUBE3 256 natural language Table 2.5: Sample of Recent Parallel AI Publications 43 Paper Machine PEs Category [Kur93] theorem proving, logic [JS94] vision [Gel94b] SN and related [DB93] other [Kur94] theorem proving, logic [DN94] CM-2 vision, search [EAH93]
Reference: [PF89] <author> R. W. Prager and F. Fallside. </author> <title> The modified Kanerva model for automatic speech recognition. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 3(1) </volume> <pages> 61-81, </pages> <year> 1989. </year>
Reference-contexts: SDM continues to be a rich research area (in topics such as modeling human memory, algorithm alternatives, and emerging applications), requiring a general-purpose high-performance computer. Prager and Fallside have extended the model to include continuous 16 variable inputs, with significant improvement in accuracy over the Boolean version <ref> [PF89] </ref>. In Jaeckel's selected-coordinate design, only some of the input pattern bits are processed for storage location activation, and in his hyperplane design each storage location corresponds to a hyperplane in the parameter space [Jae89a, Jae89b]. <p> Rogers applied genetic learning to modifying the first layer of weights, significantly improving accuracy and developed methods for interpreting the resulting weights [Rog90]. Kanerva proposed a k-fold network for distinguishing between temporal patterns such as `ABDA' versus `ACDG.' Research into potential end-user applications have included speech recognition <ref> [PF89] </ref>, character recognition [HC91], and weather prediction [Rog90]. Investigating these variations requires the flexibility of a general-purpose processor, and exploration of modular nets composed of multiple SDMs requires extremely high processing speed. <p> adjusting the address distribution in response to input distributions (particularly in minimizing the number of locations to mitigate SDM's large memory requirements) and dynamically adjusting the distance radius, the importance of weighting regions and/or dimensions to improve classification [Wat85], Prager and Fallside's adaptation to integer-valued inputs, and other input codings <ref> [PF89] </ref>. 5.2 Semantic Networks This section presents parallelizations of simple marker-passing semantic networks, approximating the behavior of more complex SN systems. After comparing edge-based and node-based network representations we introduce a new development in SNs, using inheritance codes to replace many propagation cycles with a single parallel code comparison.
Reference: [Pin94] <author> Gadi Pinkas. </author> <title> Representing propositional logic and searching for satisfiability in connectionist networks. </title> <editor> In V. Kumar et al., editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <pages> pages 279-304, </pages> <address> New York, </address> <year> 1994. </year> <journal> North-Holland. </journal> <volume> volume 1. </volume>
Reference-contexts: Semantic networks (particularly Fahlman [Fah85]) and neural networks (particularly Hopfield, Rumelhart, and Hinton [Hop82, RHW86]) were especially influential in reviving work on massively parallel AI. 42 Paper Machine PEs Category [SJ94b] search [ACP93] search [CR94] vision [Hen94] search [HJ93] constraint satisfaction [FFG94] other [SLH94] other <ref> [Pin94] </ref> neural net, logic [Ass94] theorem proving, logic [SS94a] rule-based system [Sha93] neural net [AG93] search [Kum93] SN and related [AB93] neural net [WG93] CM-5 other [LHB93] Symmetry constraint satisfaction [Rob93] custom other [HIM94] custom other [LCV94] transputer neural net [OMT94] custom 10 other [SJ94a] transputer 16 theorem proving, logic [Gel93b]
Reference: [PJK89] <author> Jr. Philip J. Koopman. </author> <title> Stack Computers, the New Wave. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: Others have shown that 2-4 execution threads are usually sufficient to keep a processor busy in a multiprocessing environment [A + 90]. Multi-threading, in conjunction with a superscalar microprocessor would address medium-grain parallelism. Kogge and others <ref> [Kog91, Kog82, Ode91, PJK89, A + 87] </ref> have shown significant performance improvements from stack architectures for production systems.
Reference: [PKF93] <author> Curt Powley, Richard E. Korf, and Chris Ferfuson. </author> <title> Parallelization of tree-recursive algorithms on a SIMD machine. </title> <editor> In James Geller, editor, </editor> <booktitle> Innovative Applications of Massive Parallelism, </booktitle> <pages> pages 181-186. </pages> <publisher> AAAI, </publisher> <year> 1993. </year> <booktitle> Working Notes, Spring Symposium Series, </booktitle> <publisher> Stanford University. </publisher>
Reference-contexts: nCUBE2 512 other [HJ94] transputer 512 theorem proving, logic [BKS93] CM-2 1024 other [S + 93c] DADO2 1024 rule-based system [CGK93] nCUBE 1024 search [NM93] CM-2 4096 genetic, classifier systems [Gel94a] CM-2 8192 SN and related [CG94] CM-2 8192 search [KK93] CM-2 8192 search [JSS93] MP-1 8192 theorem proving, logic <ref> [PKF93] </ref> CM-2 16384 search [PW94] CM-2 16384 natural language [WD94] CM-2 16384 neural net, learning [ND94] CM-2 16384 neural net, learning [AHEK94] CM-2 16384 SN and related [EAH94] CM-2 16384 SN and related [JB94] MP-1 16384 vision, neural net [ZMHV93] MP-1 16384 neural net, learning [Coo93] CM-2 32768 search Table 2.6:
Reference: [Pou91] <author> Dick Pountain. </author> <title> The Transputer strikes back. </title> <journal> Byte, </journal> <pages> pages 265-275, </pages> <month> August </month> <year> 1991. </year> <month> 254 </month>
Reference-contexts: A 2-d mesh network with 10-20 MByte/s links, such as those in the TMS320C40s and the T9000 Transputer chip <ref> [Pou91] </ref>, could also support a hundred PNs. Higher global communication rates would require more advanced global communication networks.
Reference: [PW94] <author> Michael A. Palis and David S. L. Wei. </author> <title> Massively parallel parsing algorithms for natural language. </title> <editor> In V. Kumar et al., editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <pages> pages 365-408, </pages> <address> New York, </address> <year> 1994. </year> <journal> North-Holland. </journal> <volume> volume 1. </volume>
Reference-contexts: transputer 512 theorem proving, logic [BKS93] CM-2 1024 other [S + 93c] DADO2 1024 rule-based system [CGK93] nCUBE 1024 search [NM93] CM-2 4096 genetic, classifier systems [Gel94a] CM-2 8192 SN and related [CG94] CM-2 8192 search [KK93] CM-2 8192 search [JSS93] MP-1 8192 theorem proving, logic [PKF93] CM-2 16384 search <ref> [PW94] </ref> CM-2 16384 natural language [WD94] CM-2 16384 neural net, learning [ND94] CM-2 16384 neural net, learning [AHEK94] CM-2 16384 SN and related [EAH94] CM-2 16384 SN and related [JB94] MP-1 16384 vision, neural net [ZMHV93] MP-1 16384 neural net, learning [Coo93] CM-2 32768 search Table 2.6: Sample of Recent Massively
Reference: [RD92] <author> James D. Roberts and W. W-M. Dai. </author> <title> Early system analysis of cache performance for RISC systems. </title> <booktitle> In IEEE Multi-Chip Module Conference, </booktitle> <pages> pages 130-133, </pages> <year> 1992. </year>
Reference-contexts: Gate input capacitance is greater than in SUSPENSE; accounting for additional parasitic capacitances gives C gin = 4:5k i C tr . An improved signal propagation delay equation is developed by the author in <ref> [RD92] </ref> giving T p = prop (L; R d ; C l ; C int ; R int ; v m ) = R d C l + R d C int L + R int C l L + 0:5R int C int L 2 + L=v m where k <p> The bit-cells represent numerous loads along the bit- (column) and word- (row) lines, as do the row- and column-decoder circuits for the address and data lines. These discrete loads are approximated by an effective distributed line capacitance as developed by the author in <ref> [RD92] </ref>. <p> For nearest-neighbor, Rents, and module edge topologies load capacitance C l = N p (C pad + C gin ) where C pad is pad capacitance and effective line capacitance c e = C int . For the bus topology pads are treated as a distributed capacitance as in <ref> [RD92] </ref>; C l = C pad + C gin and c e = C int + (N p 1)(C pad + C gin )=L.
Reference: [RG91] <author> D. P. Rodohan and R. J. Glover. </author> <title> An overview of the A* architecture for optimization problem in a logic programming environment. </title> <journal> ACM Computer Architecture News, </journal> <volume> 19(4), </volume> <month> June </month> <year> 1991. </year>
Reference: [RHW86] <author> D. Rumelhart, G. Hinton, and R. Williams. </author> <title> Learning representations by back-propagating errors. </title> <journal> Nature, </journal> <volume> 323 </volume> <pages> 533-536, </pages> <year> 1986. </year>
Reference-contexts: Kitano and Hendler note the emphasis on associative processing, marker passing, nearest neighbor and subsumption operations, memory-based reasoning, neural networks, and genetic algorithms [KH94]. Semantic networks (particularly Fahlman [Fah85]) and neural networks (particularly Hopfield, Rumelhart, and Hinton <ref> [Hop82, RHW86] </ref>) were especially influential in reviving work on massively parallel AI. 42 Paper Machine PEs Category [SJ94b] search [ACP93] search [CR94] vision [Hen94] search [HJ93] constraint satisfaction [FFG94] other [SLH94] other [Pin94] neural net, logic [Ass94] theorem proving, logic [SS94a] rule-based system [Sha93] neural net [AG93] search [Kum93] SN and
Reference: [RN91] <author> James D. Roberts and Claude Noshpitz. </author> <title> Implementing Kanerva's sparse distributed memory on the Connection Machine. </title> <type> Forthcoming technical report, </type> <institution> UCSC, </institution> <year> 1991. </year>
Reference-contexts: For writes, time is nearly identical for the two storage methods, so that at 50% reads and 50% writes the speed advantage for transposed storage is reduced to about 1.5 to 1.8. These alternative weight allocations have been discussed in other studies as well <ref> [RN91, Rog90] </ref>. Since the transposed method is the fastest known mapping of the problem to a SIMD array, these speed penalties are not intolerable for a high-level expression of the algorithm.
Reference: [Rob89a] <author> James D. Roberts. </author> <title> A CAM-based search accelerator. </title> <journal> IEEE Computing Futures, </journal> <month> Winter </month> <year> 1989. </year>
Reference-contexts: PCAM Proximity Content-Addressable Memory (PCAM) (1990) is a smart-memory architecture for close-match database search. Designed by the author, it is the kernel from which the MISC processor microarchitecture grew. PCAM exploits massively parallel hardware for flexible searches in large knowledge and databases <ref> [Rob89a, Rob89b, Rob90] </ref>. Several matching functions are supported: individual Boolean attributes, bit strings, integer magnitude, and within/outside of range. The ` 1 norm 1 is used as the distance metric, with the extension that distances in each dimension can be individually weighted.
Reference: [Rob89b] <author> James D. Roberts. </author> <title> Proximity search via content-addressable memory PCAM. </title> <type> Technical Report UCSC-CRL-89-43, </type> <institution> University of California, Santa Cruz, </institution> <year> 1989. </year>
Reference-contexts: PCAM Proximity Content-Addressable Memory (PCAM) (1990) is a smart-memory architecture for close-match database search. Designed by the author, it is the kernel from which the MISC processor microarchitecture grew. PCAM exploits massively parallel hardware for flexible searches in large knowledge and databases <ref> [Rob89a, Rob89b, Rob90] </ref>. Several matching functions are supported: individual Boolean attributes, bit strings, integer magnitude, and within/outside of range. The ` 1 norm 1 is used as the distance metric, with the extension that distances in each dimension can be individually weighted. <p> Calculated speed was also in line with [S + 91]. The CAM was validated against 2 static chips [J + 88, O + 89] and one dynamic chip [WS89] and showed good agreement. The Suspense cell-area model was compared to numerous layouts <ref> [WE88, Hug91, Rob89b] </ref>. Average error was 24% with a standard deviation of 17%. <p> Suspense's accuracy was surprisingly good considering the numerous circuit `tricks' used in these experimental memories. Die validation included two SIMD processor chips (BLITZEN and the MasPar MP-2 [B + 90a, Mas92a]), a systolic array (BSYS [Hug91]), a bit-serial CAM (PCAM <ref> [Rob89b] </ref>) and a CAM with a small 66 transistor bit-serial PE per word ([WS89]).
Reference: [Rob90] <author> James D. Roberts. </author> <title> Proximity content-addressable memory: an efficient extension to k-nearest neighbors search. </title> <type> Master's thesis, </type> <institution> University of California Santa Cruz, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: PCAM Proximity Content-Addressable Memory (PCAM) (1990) is a smart-memory architecture for close-match database search. Designed by the author, it is the kernel from which the MISC processor microarchitecture grew. PCAM exploits massively parallel hardware for flexible searches in large knowledge and databases <ref> [Rob89a, Rob89b, Rob90] </ref>. Several matching functions are supported: individual Boolean attributes, bit strings, integer magnitude, and within/outside of range. The ` 1 norm 1 is used as the distance metric, with the extension that distances in each dimension can be individually weighted.
Reference: [Rob93] <author> Ian N. Robinson. PAM: </author> <title> Massive parallelism in support of run-time intelligence. </title> <editor> In James Geller, editor, </editor> <booktitle> Innovative Applications of Massive Parallelism, </booktitle> <pages> pages 193-199. </pages> <publisher> AAAI, </publisher> <year> 1993. </year> <booktitle> Working Notes, Spring Symposium Series, </booktitle> <publisher> Stanford University. </publisher>
Reference-contexts: 42 Paper Machine PEs Category [SJ94b] search [ACP93] search [CR94] vision [Hen94] search [HJ93] constraint satisfaction [FFG94] other [SLH94] other [Pin94] neural net, logic [Ass94] theorem proving, logic [SS94a] rule-based system [Sha93] neural net [AG93] search [Kum93] SN and related [AB93] neural net [WG93] CM-5 other [LHB93] Symmetry constraint satisfaction <ref> [Rob93] </ref> custom other [HIM94] custom other [LCV94] transputer neural net [OMT94] custom 10 other [SJ94a] transputer 16 theorem proving, logic [Gel93b] CM-5 32 SN and related [CS94] CM-5 32 other [GW94] WARP 32 vision [LP94] iPSC 32 constraint satisfaction [ZM94] transputer 48 constraint satisfaction [MNT94] AP1000 64 other [CGA94] Pixel 64
Reference: [Rog90] <author> David Rogers. </author> <title> Predicting weather using a genetic memory: A combination of Kanerva's sparse distributed memory and Holland's genetic algorithms. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 275-289. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: Rogers applied genetic learning to modifying the first layer of weights, significantly improving accuracy and developed methods for interpreting the resulting weights <ref> [Rog90] </ref>. Kanerva proposed a k-fold network for distinguishing between temporal patterns such as `ABDA' versus `ACDG.' Research into potential end-user applications have included speech recognition [PF89], character recognition [HC91], and weather prediction [Rog90]. <p> to modifying the first layer of weights, significantly improving accuracy and developed methods for interpreting the resulting weights <ref> [Rog90] </ref>. Kanerva proposed a k-fold network for distinguishing between temporal patterns such as `ABDA' versus `ACDG.' Research into potential end-user applications have included speech recognition [PF89], character recognition [HC91], and weather prediction [Rog90]. Investigating these variations requires the flexibility of a general-purpose processor, and exploration of modular nets composed of multiple SDMs requires extremely high processing speed. SDM provides associative memory and direct inference behavior through activating near neighbors in the input space, offering several advantages. <p> For writes, time is nearly identical for the two storage methods, so that at 50% reads and 50% writes the speed advantage for transposed storage is reduced to about 1.5 to 1.8. These alternative weight allocations have been discussed in other studies as well <ref> [RN91, Rog90] </ref>. Since the transposed method is the fastest known mapping of the problem to a SIMD array, these speed penalties are not intolerable for a high-level expression of the algorithm.
Reference: [Rya94] <author> Bob Ryan. </author> <title> Alpha rides high. </title> <journal> Byte, </journal> <pages> pages 197-198, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: In both chips, communication duties are off-loaded from the CPU. The Alpha chips include hardware for shared memory and message passing, supporting parallel processor communication. The Alpha 21164 should move from 0.5 m to 0.35 m CMOS in 1995 <ref> [Rya94] </ref>, improving sustained performance to 400 MIPS or more, and NEC has recently announced an experimental 500 MHz 32-bit CMOS RISC chip [S + 94]. This dissertation does not include an evaluation of the microprocessor alternatives, and instead concentrates on the associative array. <p> Although this may at first seem optimistic, it is in fact a conservative estimate. Intel targets targets 1000 MIPS microprocessors for the year 2000 and DEC has announced a 300 MIPS superscalar RISC that peaks at 1,200 MIPS when all 4 functional units are fully utilized <ref> [Rya94] </ref>. Over the last twelve years microprocessor MIPS have increased an average of 48% per year. Extrapolating forward and backwards 231 puts commodity microprocessors at 370-1200 MIPS (averaging 700 MIPS) 4 to 5 years from now when a full MISC machine could be manufactured.
Reference: [S + 85] <editor> Lawrence Snyder et al., editors. </editor> <title> Algorithmically Specialized Parallel Computers. </title> <publisher> Academic Press, </publisher> <address> San Diego, CA, </address> <year> 1985. </year>
Reference-contexts: In addition to special-purpose machines for vision and image processing, contemporary work includes machines designed specifically for databases and knowledge base processing <ref> [KT88, S + 85, M + 92, H + 91] </ref>. Unfortunately these do not address the full needs of knowledge processing. Special-purpose machines lack the flexibility to perform well on differing algorithms and problem domains.
Reference: [S + 91] <author> Hirofumi Shinohara et al. </author> <title> A flexible multiport RAM compiler for data path. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> 26(3) </volume> <pages> 343-349, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: Area and speed estimation are somewhat better for microprocessors, typically 1% to 15% (average 1). The tile models received the most extensive verification. Memory tiles were verified against register files presented by Mulder in [MQF91], compiled multiport SRAM in <ref> [S + 91] </ref>, and Don Speck's DRAM layout [Spe91]. All use standard fabrication rather than custom memory process. <p> Calculated speed was also in line with <ref> [S + 91] </ref>. The CAM was validated against 2 static chips [J + 88, O + 89] and one dynamic chip [WS89] and showed good agreement. The Suspense cell-area model was compared to numerous layouts [WE88, Hug91, Rob89b]. Average error was 24% with a standard deviation of 17%.
Reference: [S + 93a] <author> Katsuro Sasaki et al. </author> <title> A 16-Mb CMOS SRAM with a 2.3-m 2 single-bit-line memory cell. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> 28(11) </volume> <pages> 1125-1130, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Microprocessor results were evaluated against the 80386, 80486, 68020, and the experimental Bellmac-32A [Bak90]. Verification against commercial SRAMS and DRAMS required fitting , the number of sub-arrays, and the cell area simultaneously so the accuracy results are suspect. Detailed information on 3 experimental 16 M bit SRAM chips <ref> [S + 93a, S + 93b, U + 93] </ref> and 3 experimental 256 M bit memories [T + 93, K + 93, H + 93] allowed accurate validation against advanced memory designs. Suspense's accuracy was surprisingly good considering the numerous circuit `tricks' used in these experimental memories.
Reference: [S + 93b] <editor> Katsunori Seno et al. </editor> <title> A 9-ns 16-Mb CMOS SRAM with offset-compensated current sense amplifier. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> 28(11) </volume> <pages> 1119-1123, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Microprocessor results were evaluated against the 80386, 80486, 68020, and the experimental Bellmac-32A [Bak90]. Verification against commercial SRAMS and DRAMS required fitting , the number of sub-arrays, and the cell area simultaneously so the accuracy results are suspect. Detailed information on 3 experimental 16 M bit SRAM chips <ref> [S + 93a, S + 93b, U + 93] </ref> and 3 experimental 256 M bit memories [T + 93, K + 93, H + 93] allowed accurate validation against advanced memory designs. Suspense's accuracy was surprisingly good considering the numerous circuit `tricks' used in these experimental memories.
Reference: [S + 93c] <editor> Salvatore J. Stollfo et al. </editor> <title> A parallel and distributed environment for database rule processing: Open problems and future directions. </title> <editor> In James Geller, editor, </editor> <booktitle> Innovative Applications of Massive Parallelism, </booktitle> <pages> pages 207-215. </pages> <publisher> AAAI, </publisher> <year> 1993. </year> <booktitle> Working Notes, Spring Symposium Series, </booktitle> <publisher> Stanford University. </publisher> <pages> 255 </pages>
Reference-contexts: CM-2 SN, natural language [H + 94] IXM2 SN, natural language [O + 94] IXM2 SN and related [Fah93] NETL SN and related [Twa94] custom genetic, classifier systems [LH94] custom other [YK94] custom neural net, learning [CGK94] nCUBE2 512 other [HJ94] transputer 512 theorem proving, logic [BKS93] CM-2 1024 other <ref> [S + 93c] </ref> DADO2 1024 rule-based system [CGK93] nCUBE 1024 search [NM93] CM-2 4096 genetic, classifier systems [Gel94a] CM-2 8192 SN and related [CG94] CM-2 8192 search [KK93] CM-2 8192 search [JSS93] MP-1 8192 theorem proving, logic [PKF93] CM-2 16384 search [PW94] CM-2 16384 natural language [WD94] CM-2 16384 neural net,
Reference: [S + 94] <author> Kazumasa Suzuki et al. </author> <title> A 500 MHz, 32 bit, 0.4 micron CMOS RISC processor. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> 29(12) </volume> <pages> 1464-1473, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: The Alpha 21164 should move from 0.5 m to 0.35 m CMOS in 1995 [Rya94], improving sustained performance to 400 MIPS or more, and NEC has recently announced an experimental 500 MHz 32-bit CMOS RISC chip <ref> [S + 94] </ref>. This dissertation does not include an evaluation of the microprocessor alternatives, and instead concentrates on the associative array. System early analysis assumes a separate communication co-processor as a margin of safety. The Co-Processor Most off-the-shelf microprocessors lack many features needed for MISC, including inter-processor communication.
Reference: [Sab88] <author> Gary Sabot. </author> <title> The Paralation Model. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference-contexts: The discussion is from the perspective of applications programming, excluding operating system and compiler issues. The reader is referred to Andrews and Schneider for a survey of general parallel languages and additional discussion of parallel programming requirements [AS83]. Sabot and Blelloch provide concise summaries of several collection-oriented programming languages <ref> [Sab88, Ble90] </ref>. Tables 2.3 and 2.4 combine these summaries and additional information. Prolog and LISP are excluded from the scope of this dissertation, due in part to limited speedups with control-level parallelism and their difficulty in programming applications. The sample also excludes systolic languages. <p> This, however, excludes iterative and other coarse-grain parallelism typical of MIMD models. Blelloch extends [Ble90] Sabot's prior work <ref> [Sab88] </ref> to include compiler techniques for compiling operations on nested collections, including conditionals, by concatenating the nestings and including descriptor information to "flattened" vector operations. In a SIMD implementation, elements of a collection are uniformly and sequentially divided over the available PEs.
Reference: [Sat94] <author> Satoshi Sato. </author> <title> Example-based translation and its MIMD implementation. </title> <editor> In Hiroaki Kitano and James A. Hendler, editors, </editor> <booktitle> Massively Parallel Artificial Intelligence, </booktitle> <pages> pages 171-201, </pages> <address> Menlo Park, CA, 1994. </address> <publisher> AAAI Press / The MIT Press. </publisher>
Reference-contexts: WARP 32 vision [LP94] iPSC 32 constraint satisfaction [ZM94] transputer 48 constraint satisfaction [MNT94] AP1000 64 other [CGA94] Pixel 64 rule-based system, vision [SS94b] many 64 theorem proving [NW93] PIM 70 theorem proving, logic [Ert93] LAN 110 search [Pea94] Fujitsu 128 theorem proving, logic [CMD94] SNAP 144 SN, natural language <ref> [Sat94] </ref> nCUBE3 256 natural language Table 2.5: Sample of Recent Parallel AI Publications 43 Paper Machine PEs Category [Kur93] theorem proving, logic [JS94] vision [Gel94b] SN and related [DB93] other [Kur94] theorem proving, logic [DN94] CM-2 vision, search [EAH93] CM-2 SN and related [Lan93] CM-2 SN, natural language [H + 94]
Reference: [Sch92] <author> Heinz W. Schmidt. </author> <title> Data-parallel object-oriented programming. </title> <booktitle> In Fifth Aus-tralian Supercomputer Conference, </booktitle> <pages> pages 263-272, </pages> <year> 1992. </year>
Reference-contexts: [Chr90] 1990 SIMD ffi ffi ffi j MultiLISP [Ble90] ffi * * ffi k NIAL [Ble90] 1982 l Occam [Bur88] MIMD * ffi m Paralation [Ble90] 1987 serial, vector, ffi SIMD, MIMD n Parallel Pascal [Ble90] ffi * ffi ffi o Pascal-L [FKS85] 1983 SIMD ffi ffi p pSather, dSather <ref> [FLR91, Sch92] </ref> 1991 MIMD, SIMD ffi ffi * q SETL [SDDS86] 1981 serial r SQL [Ble90] serial (ffi) ffi no somewhat * yes Table 2.3: Parallel Languages Characteristic a b c d e f g h i j k l m n o p q r Collections dynamic creation * *
Reference: [SDDS86] <author> J. T. Schwartz, R. B. K. Dewar, E. Dubinsky, and E. Schonberg. </author> <title> Programming with Sets: An Introduction to SETL. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: * * ffi k NIAL [Ble90] 1982 l Occam [Bur88] MIMD * ffi m Paralation [Ble90] 1987 serial, vector, ffi SIMD, MIMD n Parallel Pascal [Ble90] ffi * ffi ffi o Pascal-L [FKS85] 1983 SIMD ffi ffi p pSather, dSather [FLR91, Sch92] 1991 MIMD, SIMD ffi ffi * q SETL <ref> [SDDS86] </ref> 1981 serial r SQL [Ble90] serial (ffi) ffi no somewhat * yes Table 2.3: Parallel Languages Characteristic a b c d e f g h i j k l m n o p q r Collections dynamic creation * * ffi * ffi * * * ffi ffi * * <p> dissertation explores these issues and adds exploring nested collections for knowledge representation and nested parallelism for knowledge retrieval and processing. 2.4.6 SETL Although intended for serial processors, the set-oriented language SETL has implicit parallelism in its syntax through its use of sets and tuples as data objects with compound operators <ref> [SDDS86] </ref>. These collections can be of arbitrary length, and their elements can themselves be sets or tuples. Operations on sets include the standard set-theoretic operations, and operations on tuples include concatenation, indexed retrieval, and sub-tuple operations. <p> The programming model and notation informally draws from a number of collection oriented and parallel programming languages (the paralation model [Ble90], SETL <ref> [SDDS86] </ref>, Gamma [BM93], and Linda [CG89]) and is called SEGUL as a mutilated acronym of these. 2 The notation is primarily that of naive set theory [Dev93], thus incorporating a widely understood formalism and resulting in many similarities with the set- and tuple-oriented language SETL. <p> MISC can thereby efficiently execute the algorithms listed for SETL in Table 8.3. Other example algorithms include binary search and text processing. The SETL literature includes the classic introductory AI example "Towers of Hanoi," demonstrating backtracking search within the framework of a set-based language <ref> [SDDS86] </ref>. Topological sort is important in its use in Levinson's Method III multi-level indexed sort and in "untangling" hierarchies for encoding. Paralation Sections 4.3 and 8.2.2 demonstrate MISC's ability to efficiently incorporate Blelloch and Sabot's paralation model [BS90]. Their model has been implemented on the CM-2.
Reference: [SFS77] <author> R. J. Swan, S. H. Fuller, and D. P. Siewiorek. </author> <title> Cm* | a modular, multi-microprocessor. </title> <booktitle> In Proc. AFIPS National Computer Conference, </booktitle> <volume> volume 46, </volume> <pages> pages 637-644, </pages> <year> 1977. </year>
Reference-contexts: PASM is a research platform for AI applications, including vision, speech understanding, and expert systems [MSS85]. PASM has SIMD and MIMD operating modes in a partitionable topology scalable to a thousand processors. Paradigm and the much older cm* feature hierarchical communication networks and multi-level caching <ref> [CGB91, SFS77] </ref>. WASP, currently under development, was originally intended for image and signal processing. 31 Target applications now include supporting expert systems, LISP, and Prolog with low level matching operations [LJ91]. Array modules can operate independently as SIMD or in groups as MIMD. <p> This work is not is not included in Table 2.2 as it did not progress past a general architectural concept. Machine Year Description Processors Peak Performance ASP [Koh80] 1967 SIMD, 2-d mesh, assoc. proposed ILLIAC IV [AG89] 1968 SIMD, 2-d mesh 64 500 MFLOPS cm* <ref> [SFS77] </ref> 1977 MIMD, multibus clusters 100 10 MIPS UNC Cellular [AG89] 1979 MSIMD, reduction, tree 1 M MPP [Bat82] 1983 SIMD, bit-serial, 2-d mesh 16 K 3,000 MIPS CM-1 [AG89] 1985 SIMD, bit serial, hypercube 64 K 2,000 MIPS H.A.P.S. [Stu85] 1985 SIMD, bus, assoc. proposed nCUBE [H + 86] 1985
Reference: [Sha88] <author> David Elliot Shaw. </author> <title> Organization and operation of massively parallel machines. </title> <editor> In Guy Rabbat, editor, </editor> <booktitle> Handbook of Advanced Semiconductor Technology and Computer Systems. </booktitle> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: The designers projected a second-generation DADO with ten times the performance, but it was never implemented. NON-VON NON-VON (1985) is a machine for AI that emphasizes basic database search for production systems and similar applications <ref> [Sha88] </ref>. SIMD PEs incorporate bit-serial and byte-wide functional units and registers but limits their simultaneous use. Multiple binary trees of SIMD PEs are rooted at a MIMD processor, and multiple trees are interconnected by a shu*e-based network. Memory is extremely small, at most 256 bytes per PE. <p> Multiple binary trees of SIMD PEs are rooted at a MIMD processor, and multiple trees are interconnected by a shu*e-based network. Memory is extremely small, at most 256 bytes per PE. Estimates showed a 16 K PE NON-VON to be only 10 times faster than earlier database machines <ref> [Sha88, AG89] </ref>. FAIM FAIM (1987) supports object-oriented, logic, and procedural programming (LISP and Prolog) by attaching a small amount of CAM to RISC processors [A + 87]. Each CAM has only a small number of data objects per match circuit and provides only masked-comparison.
Reference: [Sha93] <author> Lokendra Shastri. </author> <title> Leveraging massive parallelism for tractable reasoning | taking inspiration from cognition. </title> <editor> In James Geller, editor, </editor> <booktitle> Innovative Applications of Massive Parallelism, </booktitle> <pages> pages 200-206. </pages> <publisher> AAAI, </publisher> <year> 1993. </year> <booktitle> Working Notes, Spring Symposium Series, </booktitle> <publisher> Stanford University. </publisher>
Reference-contexts: neural networks (particularly Hopfield, Rumelhart, and Hinton [Hop82, RHW86]) were especially influential in reviving work on massively parallel AI. 42 Paper Machine PEs Category [SJ94b] search [ACP93] search [CR94] vision [Hen94] search [HJ93] constraint satisfaction [FFG94] other [SLH94] other [Pin94] neural net, logic [Ass94] theorem proving, logic [SS94a] rule-based system <ref> [Sha93] </ref> neural net [AG93] search [Kum93] SN and related [AB93] neural net [WG93] CM-5 other [LHB93] Symmetry constraint satisfaction [Rob93] custom other [HIM94] custom other [LCV94] transputer neural net [OMT94] custom 10 other [SJ94a] transputer 16 theorem proving, logic [Gel93b] CM-5 32 SN and related [CS94] CM-5 32 other [GW94] WARP
Reference: [Sie90] <author> Howard Jay Siegel. </author> <title> Interconnection Networks for Large-Scale Parallel Processing. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: connection is linear in the distance, but as we will see this has negligible practical effect in the fixed-sized MISC arrays. 126 Siegel shows that a mesh with edge wrap-around (torus) can emulate a hypercube with p P =2+1 slowdown and a shu*e-exchange network with 2 p P 1 slowdown <ref> [Sie90] </ref>. It might first appear that the bypass-mesh could efficiently emulate a 2 i network, in which all PEs can simultaneously send to another PE a power of 2 distance away.
Reference: [SJ94a] <author> Johannes Schumann and Manfred Jobmann. </author> <title> Scalability of an OR-parallel theorem prover on a network of transputers | a modeling approach. </title> <editor> In V. Kumar et al., editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <pages> pages 199-206, </pages> <address> New York, </address> <year> 1994. </year> <journal> North-Holland. </journal> <volume> volume 2. </volume>
Reference-contexts: constraint satisfaction [FFG94] other [SLH94] other [Pin94] neural net, logic [Ass94] theorem proving, logic [SS94a] rule-based system [Sha93] neural net [AG93] search [Kum93] SN and related [AB93] neural net [WG93] CM-5 other [LHB93] Symmetry constraint satisfaction [Rob93] custom other [HIM94] custom other [LCV94] transputer neural net [OMT94] custom 10 other <ref> [SJ94a] </ref> transputer 16 theorem proving, logic [Gel93b] CM-5 32 SN and related [CS94] CM-5 32 other [GW94] WARP 32 vision [LP94] iPSC 32 constraint satisfaction [ZM94] transputer 48 constraint satisfaction [MNT94] AP1000 64 other [CGA94] Pixel 64 rule-based system, vision [SS94b] many 64 theorem proving [NW93] PIM 70 theorem proving, logic
Reference: [SJ94b] <author> Christian B. Suttner and Manfred R. Jobmann. </author> <title> Simulation analysis of static partitioning with slackness. </title> <editor> In V. Kumar et al., editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <pages> pages 93-106, </pages> <address> New York, </address> <year> 1994. </year> <journal> North-Holland. </journal> <volume> volume 2. </volume>
Reference-contexts: Semantic networks (particularly Fahlman [Fah85]) and neural networks (particularly Hopfield, Rumelhart, and Hinton [Hop82, RHW86]) were especially influential in reviving work on massively parallel AI. 42 Paper Machine PEs Category <ref> [SJ94b] </ref> search [ACP93] search [CR94] vision [Hen94] search [HJ93] constraint satisfaction [FFG94] other [SLH94] other [Pin94] neural net, logic [Ass94] theorem proving, logic [SS94a] rule-based system [Sha93] neural net [AG93] search [Kum93] SN and related [AB93] neural net [WG93] CM-5 other [LHB93] Symmetry constraint satisfaction [Rob93] custom other [HIM94] custom other
Reference: [SLH94] <author> Kilian Stoffel, Ian Law, and Beat Hirsbrunner. </author> <title> Fuzzy logic controlled dynamic allocation system. </title> <editor> In V. Kumar et al., editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <pages> pages 227-239, </pages> <address> New York, </address> <year> 1994. </year> <journal> North-Holland. </journal> <volume> volume 2. </volume>
Reference-contexts: Semantic networks (particularly Fahlman [Fah85]) and neural networks (particularly Hopfield, Rumelhart, and Hinton [Hop82, RHW86]) were especially influential in reviving work on massively parallel AI. 42 Paper Machine PEs Category [SJ94b] search [ACP93] search [CR94] vision [Hen94] search [HJ93] constraint satisfaction [FFG94] other <ref> [SLH94] </ref> other [Pin94] neural net, logic [Ass94] theorem proving, logic [SS94a] rule-based system [Sha93] neural net [AG93] search [Kum93] SN and related [AB93] neural net [WG93] CM-5 other [LHB93] Symmetry constraint satisfaction [Rob93] custom other [HIM94] custom other [LCV94] transputer neural net [OMT94] custom 10 other [SJ94a] transputer 16 theorem proving,
Reference: [Sow] <author> John Sowa. </author> <title> Inference engines using peirce's rules. Email communication to the Conceptual Graphs Internet mailing list, </title> <month> November 11, </month> <year> 1992. </year>
Reference-contexts: Sowa has noted that Method III, with its inherent retrieval of specializations and generalizations of a graph (rather than just a single best or exact match), has important implications for inferencing and may prove dramatically better than "any other knowledge-based system going" for large KBs <ref> [Sow] </ref>, and the methodology is being incorporated into KL-ONE. Hierarchy Coding CG and other KR systems include a concept-type hierarchy to represent subsumption between the atomic concepts. Figure 2.5 shows a small example; the hierarchy is essentially a semantic network with only isa relations.
Reference: [Sow84] <author> J. F. Sowa. </author> <title> Conceptual Structures: Information Processing in Mind and Machine. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1984. </year>
Reference-contexts: Sowa combined earlier work in existential graphs (Charles Peirce, 1896) with more recent work in semantic networks to develop CGs with the representational and inference power of predicate calculus <ref> [Sow84] </ref>. Many common representations such as type hierarchies, entity-relationship diagrams, parse trees, dataflow graphs, flow charts, state-transition diagrams, and petri nets can be considered special cases of CGs, and other knowledge such as chemical formulas and chess positions are readily expressible in CGs [ELR95]. <p> CGs represent knowledge as a set of small graphs (i.e. semantic networks) with a standard mapping to both logic and natural language <ref> [Sow84] </ref> in a fully detailed formalism. This knowledge representation is particularly demanding in its need for match and search operations involving relational and recursively defined patterns. Key operations include subgraph isomorphism, subsump-tion, and canonical formation operations [Sow84]. <p> graphs (i.e. semantic networks) with a standard mapping to both logic and natural language <ref> [Sow84] </ref> in a fully detailed formalism. This knowledge representation is particularly demanding in its need for match and search operations involving relational and recursively defined patterns. Key operations include subgraph isomorphism, subsump-tion, and canonical formation operations [Sow84]. A recent call-for-papers announced that CGs have been approved as a standard for information interchange and recently proposed as a normative language for conceptual schemas by the ANSI Committee on Information Resources Dictionary Systems [Tep93]. <p> Let relations (with 90 relation equivalence requiring exact matching) and node- or concept-types (with equivalence including concept-type subsumption and individual instantiations) be represented as nodes in a graph. SI then corresponds to the canonical CG operation of projection <ref> [Sow84] </ref>, and G b is a specialization of G a . Determination of subsets of concept-types and relations and their immediate adjacencies is a necessary but not sufficient condition for testing generalization and specializations of graphs. <p> Monadic relations can be handled as attributes attached to the concept-type node. Higher-order relations can be handled as graph nodes with appropriate correspondence criteria. Converting nodes to edges reduces the number of nodes in the graphs to be compared. In Sowa's "conceptual catalog" <ref> [Sow84] </ref>, some 90% of the relations are binary and nearly all the remainder are monadic. Treating relations as labeled edges rather than nodes has a major speed advantage, as empirical time is O (n 2 ) in parallel implementation [WWR91], where n is the number of nodes.
Reference: [Spe91] <author> Don Speck. </author> <title> The MOSAIC fast 512K scalable CMOS DRAM. </title> <editor> In Carlo Sequin, editor, </editor> <booktitle> Advanced Research in VLSI: Proceedings of the 1991 University of California / Santa Cruz Conference, </booktitle> <pages> pages 229-244. </pages> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Is a large on-chip memory practical? Suspense shows that 32 K bytes per PE of DRAM using the MOSIS IC process and a 2-transistor DRAM cell occupying 190 2 <ref> [Spe91] </ref> doubles die area and more than halves yield, increasing PE bare die cost by a factor of 5. This overwhelms any pin cost savings. <p> Area and speed estimation are somewhat better for microprocessors, typically 1% to 15% (average 1). The tile models received the most extensive verification. Memory tiles were verified against register files presented by Mulder in [MQF91], compiled multiport SRAM in [S + 91], and Don Speck's DRAM layout <ref> [Spe91] </ref>. All use standard fabrication rather than custom memory process. In addition to specific example register areas (MIPS-X, DEC3, HP1, and the 80860; the radiation-hardened examples were not used in validation), Mulder presents a simple on-chip memory area model including a scaling factor for additional bit-lines (multiport registers).
Reference: [SS94a] <author> James G. Schmolze and Wayne Snyder. </author> <title> Using confluence to control parallel production systems. </title> <editor> In V. Kumar et al., editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <pages> pages 41-56, </pages> <address> New York, </address> <year> 1994. </year> <journal> North-Holland. </journal> <volume> volume 2. </volume> <pages> 256 </pages>
Reference-contexts: Fahlman [Fah85]) and neural networks (particularly Hopfield, Rumelhart, and Hinton [Hop82, RHW86]) were especially influential in reviving work on massively parallel AI. 42 Paper Machine PEs Category [SJ94b] search [ACP93] search [CR94] vision [Hen94] search [HJ93] constraint satisfaction [FFG94] other [SLH94] other [Pin94] neural net, logic [Ass94] theorem proving, logic <ref> [SS94a] </ref> rule-based system [Sha93] neural net [AG93] search [Kum93] SN and related [AB93] neural net [WG93] CM-5 other [LHB93] Symmetry constraint satisfaction [Rob93] custom other [HIM94] custom other [LCV94] transputer neural net [OMT94] custom 10 other [SJ94a] transputer 16 theorem proving, logic [Gel93b] CM-5 32 SN and related [CS94] CM-5 32
Reference: [SS94b] <author> Christian B. Suttner and Johann M. Schumann. </author> <title> Parallel automated theorem proving. </title> <editor> In V. Kumar et al., editors, </editor> <booktitle> Parallel Processing for Artificial Intelligence, </booktitle> <pages> pages 209-258, </pages> <address> New York, </address> <year> 1994. </year> <journal> North-Holland. </journal> <volume> volume 1. </volume>
Reference-contexts: custom other [LCV94] transputer neural net [OMT94] custom 10 other [SJ94a] transputer 16 theorem proving, logic [Gel93b] CM-5 32 SN and related [CS94] CM-5 32 other [GW94] WARP 32 vision [LP94] iPSC 32 constraint satisfaction [ZM94] transputer 48 constraint satisfaction [MNT94] AP1000 64 other [CGA94] Pixel 64 rule-based system, vision <ref> [SS94b] </ref> many 64 theorem proving [NW93] PIM 70 theorem proving, logic [Ert93] LAN 110 search [Pea94] Fujitsu 128 theorem proving, logic [CMD94] SNAP 144 SN, natural language [Sat94] nCUBE3 256 natural language Table 2.5: Sample of Recent Parallel AI Publications 43 Paper Machine PEs Category [Kur93] theorem proving, logic [JS94] vision
Reference: [SSS81] <author> Edmond Schonberg, Jacob T. Schwartz, and Micha Sharir. </author> <title> An automatic technique for selection of data representations in SETL programs. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 3(2) </volume> <pages> 126-143, </pages> <month> April </month> <year> 1981. </year>
Reference-contexts: Selection is currently by programmer declaration, but subsequent work investigates heuristics for automating this, including a suggestion to use CAM for dynamic basis sets <ref> [SSS81] </ref>.
Reference: [Sto87] <author> Salvatore Stolfo. </author> <title> Initial performance of the DADO2 prototype. </title> <booktitle> Computer, </booktitle> <pages> pages 75-83, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: Three such machines, DADO, NON-VON, and FAIM, are particularly relevant to MISC. DADO DADO (1987) was designed for production systems, sorting, and matching <ref> [Sto87] </ref>. Systems include 1,023 standard microprocessors each with a custom communication coprocessor and 20 K bytes memory, and were intended to scale to 8,191 processors. The binary tree communication network allows dynamic partitioning of the machine into multiple SIMD-mode subtrees rooted at a processor operating in MIMD mode.
Reference: [Sto91] <author> Charles D. Stormon. </author> <title> The Coherent processor, an associative processor architecture and applications. </title> <booktitle> In Proceedings of COMPCON, </booktitle> <pages> pages 270-275. </pages> <publisher> IEEE Press, </publisher> <year> 1991. </year>
Reference-contexts: Several associative processors combine a bit-cell CAM array with simple bit-serial units for additional operations. The Coherent Processor features trinary logic (stored don't-care 23 states) for applications such as tree and substring search, graphics, pattern and symbol recognition, neural networks, and as a Prolog accelerator <ref> [Sto91] </ref>. Wade presents a system with a similar architecture [WS89]. PCAM Proximity Content-Addressable Memory (PCAM) (1990) is a smart-memory architecture for close-match database search. Designed by the author, it is the kernel from which the MISC processor microarchitecture grew. <p> [KF90] 1990 SIMD, various simulated SNAP [M + 92] 1990 MSIMD, modified hypercube 144 CM-5 [Thi92] 1991 MSIMD, fat-tree, vector 1 K 128,000 MFLOPS Paragon [Hwa93] 1991 MIMD, 2-d mesh 300,000 MFLOPS IXM2 [H + 91] 1991 pseudo MIMD, hierarchical 64 278 MIPS Paradigm [CGB91] 1991 MIMD, hierarchical 100+ Coherent <ref> [Sto91] </ref> 1992 SIMD, CAM 4 K MasPar MP-2 [Mas92a] 1992 SIMD, 2-d mesh, MIN 16 K 68,000 MIPS Cray T3D [Oed93] 1993 MIMD, 3-d mesh 2 K 300,000 MFLOPS WASP [LJ91] devel MSIMD, bypass ring 64 K 40,000 MIPS Table 2.2: Sample of Other Parallel Machines 32 2.3.6 Relationship to MISC <p> Applications that especially benefit from content-addressing include natural language processing, neural networks, pattern and symbolic recognition, and semantic networks <ref> [Kog91, M + 90, Sto91, WL89, YZ90] </ref>. Unfortunately, CAM hardware implementations require uniform-length fixed-location data and exhibit poor utilization of hardware resources for non-matching operations. <p> Tree Codes Stormon presents an encoding for binary trees which can be used for determining subsumption or inheritance relations (connectivity), marking subtrees rooted at a particular node, and finding parents and siblings <ref> [Sto91] </ref>. As shown in Figure 5.15 for a complete balanced binary tree the codes correspond to an ordered numbering of the nodes, starting at the root. The resulting binary values are left-justified and padded on the right with 104 "match-anything," a type of trinary logic equivalent to masked compare.
Reference: [Stu85] <author> Heinrich J. Stuttgen. </author> <title> A Hierarchical Associative Processing System. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: IV [AG89] 1968 SIMD, 2-d mesh 64 500 MFLOPS cm* [SFS77] 1977 MIMD, multibus clusters 100 10 MIPS UNC Cellular [AG89] 1979 MSIMD, reduction, tree 1 M MPP [Bat82] 1983 SIMD, bit-serial, 2-d mesh 16 K 3,000 MIPS CM-1 [AG89] 1985 SIMD, bit serial, hypercube 64 K 2,000 MIPS H.A.P.S. <ref> [Stu85] </ref> 1985 SIMD, bus, assoc. proposed nCUBE [H + 86] 1985 MIMD, hypercube 1 K 1,945 MIPS NON-VON [AG89] 1985 MSIMD, bit-byte, tree, MIN 16 K+ AAP2 [K + 86] 1986 SIMD, 2-d mesh, assoc. 64 K 10,000 MOPS CLIP7A [FMD88] 1986 SIMD, 2-d mesh 256 64 MIPS LUCAS [FKS85] 1986
Reference: [Sus65] <author> E. H. Sussenguth, Jr. </author> <title> A graph-theoretic algorithm for matching chemical structures. </title> <journal> Journal of Chemical Doc., </journal> <volume> 5 </volume> <pages> 36-43, </pages> <year> 1965. </year>
Reference-contexts: Fortunately, the expected case time is empirically polynomial [Ull76, WWR91]. The worst-case time results from generally pathological cases that rarely occur in practice <ref> [Bar88, CK80, Sus65] </ref> so that parallelization can provide a difference in tractable graph size. Further, graph size is bounded for most applications.
Reference: [SW86] <author> C. Stanfill and D. Waltz. </author> <title> Towards memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1213-1228, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Retrieval techniques and measures of similarity are important. More formally, as developed by Stanfill and Waltz memory-based reasoning (MBR) combines retrieved results for a statistical interpretation <ref> [SW86] </ref>. As first proposed, MBR refers specifically to similarity-based induction directly from memory. MBR has been used in a wide variety of applications [YK94].
Reference: [T + 93] <author> Tadahiko et al. </author> <title> A 30-ns 256-Mb DRAM with a multidivided array structure. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> 28(11) </volume> <pages> 1192-1097, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Detailed information on 3 experimental 16 M bit SRAM chips [S + 93a, S + 93b, U + 93] and 3 experimental 256 M bit memories <ref> [T + 93, K + 93, H + 93] </ref> allowed accurate validation against advanced memory designs. Suspense's accuracy was surprisingly good considering the numerous circuit `tricks' used in these experimental memories.
Reference: [Tan92] <author> Tanner Research. </author> <title> Volume ASIC Costs, </title> <year> 1992. </year>
Reference-contexts: Otherwise die area is the active die area plus the total area of all the pads. The die cost-metric c mt combines fixed-cost (masking charges), per-unit costs, and yield. Coefficients approximate dollar cost in 1993 dollars based on regression to sample custom-fab pricing <ref> [ISI93, Tan92] </ref>. c mt = (1 + 1:8 (2 ))(20000=v mfr + 11A d )e A d d ; where v mfr is the target quantity of good die and d is the defect rate per cm 2 .
Reference: [TD91] <author> Manu Thapar and Bruce Delagi. </author> <title> Cache coherence for large scale shared memory multiprocessors. </title> <booktitle> Computer Architecture News, </booktitle> <pages> pages 114-119, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: In addition to the memory sharing itself, this would add the requirement of cache coherency, as the high-performance microprocessors require cache. Several studies demonstrate the practicality of multi-bus cache coherency without requiring any bus to be common to all processors <ref> [ASHH88, TD91] </ref>. In summary, a co-processor might provide the following functions. * Managing inter-PN communication independently of the supervisor.
Reference: [Tep93] <author> William T. Tepfenhart. </author> <title> ICCS 94 call for papers. Internet, </title> <month> September </month> <year> 1993. </year>
Reference-contexts: Key operations include subgraph isomorphism, subsump-tion, and canonical formation operations [Sow84]. A recent call-for-papers announced that CGs have been approved as a standard for information interchange and recently proposed as a normative language for conceptual schemas by the ANSI Committee on Information Resources Dictionary Systems <ref> [Tep93] </ref>. The same call-for-papers notes that Over the past 10 years, they [conceptual graphs] have been widely used as a semantic representation for natural language and as a graphic system of logic for expert systems, theorem provers, and database design. <p> Researchers have developed a sizable software base and continue to build upon it. Successful implementations include: rule-based systems, database systems, knowledge-based systems, knowledge engineering tools, enterprise modeling, management information systems, conceptual information retrieval, medical informatics and natural language applications, among others <ref> [Tep93] </ref>. Subgraph Isomorphism Subgraph isomorphism (SI) has been practically applied for years in chemical database retrieval, CAD, and more recently in CG implementations but is nevertheless NP-complete; in the worst case comparison time is exponential in the size of the graphs.
Reference: [Tex91] <institution> Texas Instruments. </institution> <note> TMS320C4x User's Guide, </note> <year> 1991. </year>
Reference-contexts: This is supported by the simulations described in Appendix C. At this communication rate, a single high-speed bus, such as the 100 MByte/s Futurebus II standard or the TMS320C40 <ref> [Tex91] </ref> DSP chip's parallel processor global bus, is sufficient for MISC systems of a hundred PNs. A 2-d mesh network with 10-20 MByte/s links, such as those in the TMS320C40s and the T9000 Transputer chip [Pou91], could also support a hundred PNs.
Reference: [TH88] <author> Stuart K. Tewksbury and L. A. Hornak. </author> <title> Communication network issues and high-density interconnects in large-scale distributed computing systems. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 6(3), </volume> <month> April </month> <year> 1988. </year>
Reference-contexts: In packaging die, pin count is usually a limiting factor. At the system level, thermal contraction and expansion (TCE) and alignment accuracy limit packaging to about a hundred connections per edge <ref> [TH88] </ref>. The same pin count and wiring problems occur at each level in the system packaging and interconnection hierarchy.
Reference: [Thi92] <institution> Thinking Machines Corporation. </institution> <type> CM-5 Technical Summary, </type> <year> 1992. </year>
Reference-contexts: CM-5 The CM-5 (1991) is a significant departure from the prior Connection Machines <ref> [Thi92] </ref>. The designers abandoned bit-serial PEs, SIMD, and the hypercube network of the CM-2. The CM-5 does retain a data-parallel SIMD-like programming model. Each processing node is a 40-MHz RISC microprocessor with four 20-MHz floating-point vector co-processors 2 . <p> BLITZEN [B + 90a] 1990 SIMD, bit serial, 2-d mesh 16 K 3,000 MIPS iWarp [B + 90b] 1990 MIMD, 2-d mesh 64 1,300 MFLOPS BSYS [Hug91] 1990 systolic, 1-d mesh 470 108 MOPS NAP [KF90] 1990 SIMD, various simulated SNAP [M + 92] 1990 MSIMD, modified hypercube 144 CM-5 <ref> [Thi92] </ref> 1991 MSIMD, fat-tree, vector 1 K 128,000 MFLOPS Paragon [Hwa93] 1991 MIMD, 2-d mesh 300,000 MFLOPS IXM2 [H + 91] 1991 pseudo MIMD, hierarchical 64 278 MIPS Paradigm [CGB91] 1991 MIMD, hierarchical 100+ Coherent [Sto91] 1992 SIMD, CAM 4 K MasPar MP-2 [Mas92a] 1992 SIMD, 2-d mesh, MIN 16 K
Reference: [Tou88] <author> David S. Touretzky. </author> <title> Beyond associative memory: Connectionists must search for other cognitive primitives. </title> <booktitle> In AAAI Spring Symposium Series: Parallel Models of Intelligence, </booktitle> <pages> pages 231-236, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Although they can be a critical component of a knowledge processing system, additional mechanisms are necessary. As Touretzky notes, associative memories require training data covering most of the input space and may not scale to efficiently construct rule-following behavior <ref> [Tou88] </ref>. In particular he comments that the compositional structure of language cannot be reduced to input-output mappings unless the training examples are nearly exhaustive. Associative memory models' ability to express relationships between objects and to efficiently support variables and binding between variables is an open question.
Reference: [TR88] <author> L. W. Tucker and G. G. Robertson. </author> <title> Architecture and applications of the Connection Machine. </title> <journal> Computer, </journal> <volume> 21(8) </volume> <pages> 26-38, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: The CM-2 has more memory than the CM-1 (up to 64 K bits per PE), adds one floating-point processor for every 32 bit-serial PEs, and doubles the clock speed to 8 MHz (2 m CMOS) <ref> [TR88] </ref>. These improvements (in particular the floating point chips) greatly increase performance, primarily for numeric codes. MPP The Goodyear Massively Parallel Processor (MPP) (1983), developed for NASA / God-dard, is based on the STARAN and ASPRO associative processors [Bat82]. <p> [H + 86] 1985 MIMD, hypercube 1 K 1,945 MIPS NON-VON [AG89] 1985 MSIMD, bit-byte, tree, MIN 16 K+ AAP2 [K + 86] 1986 SIMD, 2-d mesh, assoc. 64 K 10,000 MOPS CLIP7A [FMD88] 1986 SIMD, 2-d mesh 256 64 MIPS LUCAS [FKS85] 1986 SIMD, bit-serial, MIN, assoc. 128 CM-2 <ref> [TR88] </ref> 1987 SIMD, bit-serial, hypercube 64 K 4,000 MIPS AMT DAP [TW91] 1987 SIMD, 2-d mesh, orth. bus 1 K 800 MFLOPS GAPP [NCR87] 1987 SIMD, 1,2-d mesh 2 K 900 MOPS DADO [AG89] 1987 MIMD, tree 8 K PASM [MSS85] 1987 MSIMD, bus, xbar 1 K TRAC [AG89] 1987 MSIMD,
Reference: [TW91] <author> Arthur Trew and Greg Wilson, </author> <title> editors. Past, Present, Parallel: A Survey of Available Computer Systems. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: [AG89] 1985 MSIMD, bit-byte, tree, MIN 16 K+ AAP2 [K + 86] 1986 SIMD, 2-d mesh, assoc. 64 K 10,000 MOPS CLIP7A [FMD88] 1986 SIMD, 2-d mesh 256 64 MIPS LUCAS [FKS85] 1986 SIMD, bit-serial, MIN, assoc. 128 CM-2 [TR88] 1987 SIMD, bit-serial, hypercube 64 K 4,000 MIPS AMT DAP <ref> [TW91] </ref> 1987 SIMD, 2-d mesh, orth. bus 1 K 800 MFLOPS GAPP [NCR87] 1987 SIMD, 1,2-d mesh 2 K 900 MOPS DADO [AG89] 1987 MIMD, tree 8 K PASM [MSS85] 1987 MSIMD, bus, xbar 1 K TRAC [AG89] 1987 MSIMD, banyan 4 FAIM [A + 87] 1987 MSIMD, 2-d mesh 1
Reference: [Twa94] <author> Kirk Twardowski. </author> <title> An associative architecture for genetic algorithm-based machine learning. </title> <booktitle> Computer, </booktitle> <pages> pages 27-38, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Category [Kur93] theorem proving, logic [JS94] vision [Gel94b] SN and related [DB93] other [Kur94] theorem proving, logic [DN94] CM-2 vision, search [EAH93] CM-2 SN and related [Lan93] CM-2 SN, natural language [H + 94] IXM2 SN, natural language [O + 94] IXM2 SN and related [Fah93] NETL SN and related <ref> [Twa94] </ref> custom genetic, classifier systems [LH94] custom other [YK94] custom neural net, learning [CGK94] nCUBE2 512 other [HJ94] transputer 512 theorem proving, logic [BKS93] CM-2 1024 other [S + 93c] DADO2 1024 rule-based system [CGK93] nCUBE 1024 search [NM93] CM-2 4096 genetic, classifier systems [Gel94a] CM-2 8192 SN and related [CG94]
Reference: [U + 93] <editor> Motomu Ukita et al. </editor> <title> A single-bit-line cross-point cell activation (SCPA) architecture for ultra-low-power SRAMs. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> 28(11) </volume> <pages> 1114-1118, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Microprocessor results were evaluated against the 80386, 80486, 68020, and the experimental Bellmac-32A [Bak90]. Verification against commercial SRAMS and DRAMS required fitting , the number of sub-arrays, and the cell area simultaneously so the accuracy results are suspect. Detailed information on 3 experimental 16 M bit SRAM chips <ref> [S + 93a, S + 93b, U + 93] </ref> and 3 experimental 256 M bit memories [T + 93, K + 93, H + 93] allowed accurate validation against advanced memory designs. Suspense's accuracy was surprisingly good considering the numerous circuit `tricks' used in these experimental memories.
Reference: [Ull76] <author> J. R. </author> <title> Ullmann. An algorithm for subgraph isomorphism. </title> <journal> Journal of the ACM, </journal> <volume> 23(1) </volume> <pages> 31-42, </pages> <month> January </month> <year> 1976. </year> <month> 257 </month>
Reference-contexts: There is a growing recognition of the importance of graphs as a representation of logical relations in general [Gen83, Win79] so that SI is an important operation despite its time complexity. Fortunately, the expected case time is empirically polynomial <ref> [Ull76, WWR91] </ref>. The worst-case time results from generally pathological cases that rarely occur in practice [Bar88, CK80, Sus65] so that parallelization can provide a difference in tractable graph size. Further, graph size is bounded for most applications. <p> Further, graph size is bounded for most applications. Ullmann's refinement SI algorithm uses Boolean matrix multiplication to prune the binding search, empirically giving expected 19 time O (n 4 ) where n is the number of nodes in the largest graph <ref> [Ull76] </ref>. Willett et al. have parallelized Ullmann's algorithm, using O (n 2 ) 1-bit SIMD PEs to reduce time to typically O (n 2 ) [WWR91]. <p> Willett et al. present algorithms for testing subgraph isomorphism on a single SIMD array and controller based on Ullmann's refinement, with expected time O (n 2 ) on O (n 2 ) processing elements for random graphs <ref> [WWR91, Ull76] </ref>. Since a tree-node can have at most one parent the number of nodes in the tree and symbols in the regular expression is O (e), where e is the total number of atomic elements, provided `empty' nestings such as [[[[x]]]] are prohibited. <p> subgraph of G b if all nodes and edges in G a are present in G b ; G a is isomorphic to a subgraph of G b iff there exists a one-to-one correspondence between the set of nodes of this subgraph and those of G a that preserves adjacency <ref> [Ull76] </ref>. Let relations (with 90 relation equivalence requiring exact matching) and node- or concept-types (with equivalence including concept-type subsumption and individual instantiations) be represented as nodes in a graph. SI then corresponds to the canonical CG operation of projection [Sow84], and G b is a specialization of G a . <p> Ullmann presents a refinement technique to drastically prune the search tree <ref> [Ull76] </ref>. The algorithm represents the graphs and the binding between nodes as three Boolean matrices: A the adjacency matrix of G a , B the adjacency matrix of G b , and M the `match' matrix. <p> These experiments only approximate application behavior. The adjacency and match matrices are generated randomly using the same procedure used by Ullmann to evaluate his algorithm <ref> [Ull76] </ref> and do not capture any structures common to an application domain. 11 9 With random assignment of graphs the probability that any array section contains more than one graph to process is less than 5%. <p> Although exponential in the worst case, Ullmann's full subgraph isomorphism algorithm requires O (n 4 ) bit operations in the expected case, where n is the number of graph nodes <ref> [Ull76] </ref> and each call to the refinement algorithm has time O (n 3 ) on a serial machine [WWR91]. In comparing different parallelizations, the expected-case n calls to refinement appear in all cases and thereby cancels out.
Reference: [VL90] <author> Christian Val and Thierry Lemoine. </author> <title> 3-D interconnection for ultra-dense multichip modules. </title> <journal> IEEE Transactions on Components, Hybrids, and Manufacturing Technology, </journal> <volume> 13(4) </volume> <pages> 814-821, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: The cost model also adds a typical cost per pin for each package type to approximate the added packaging cost. The model includes calculations for stacked-die (3-D packaging) based on data in <ref> [Lei90, VL90] </ref>. Module Model The Suspense module model uses the SUSPENS equations for module area, speed, and power, modified by the improved propagation delay calculation. Unlike SUSPENS, Suspense allows a mix of package sizes and specific package-interconnect topologies. <p> The coefficients correspond to $720 per 10 cm wafer at = 2 m. 226 B.5 Packaging Model The model includes calculations for stacked-die (3-D packaging) based on data in <ref> [Lei90, VL90] </ref>. As in the die model, the user can specify the number of each die to be included in the stack and the number on the critical-path for timing.
Reference: [Wat85] <author> Satoshi Watanabe. </author> <title> Pattern Recognition Human and Mechanical. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: At higher levels, structural similarities including isomorphisms are important in reasoning by analogy and retrieving similar prior experiences for case-based reasoning. Such computationally derived distances are syntactic distances. By the Theorem of the Ugly Duckling, differences based on collections of attributes are insufficient <ref> [Wat85] </ref>. It is necessary to establish the relevance of particular features to an output such as a classification, and the criticality of the result to system goals. <p> of the physical location addresses, other methods proposed by the author for adjusting the address distribution in response to input distributions (particularly in minimizing the number of locations to mitigate SDM's large memory requirements) and dynamically adjusting the distance radius, the importance of weighting regions and/or dimensions to improve classification <ref> [Wat85] </ref>, Prager and Fallside's adaptation to integer-valued inputs, and other input codings [PF89]. 5.2 Semantic Networks This section presents parallelizations of simple marker-passing semantic networks, approximating the behavior of more complex SN systems.
Reference: [WCHC89] <author> Jr. W. C. Hobart and H. G. Cragon. </author> <title> Locality characteristics of symbolic programs. </title> <booktitle> In Conference on Computer Design. IEEE Computer Society, </booktitle> <year> 1989. </year>
Reference-contexts: In one study, AI codes required 20% smaller cache for an optimal hit rate and exhibited significantly greater spatial and structural locality <ref> [WCHC89] </ref>. Massively parallel AI, in distinction to coarse-grain AI, has generally operated within the limits of numerically-oriented SIMD machines. AI codes, however, pose special difficulties for SIMD architectures. <p> The co-processor may also support caching strategies specific to AI codes, emphasizing structural locality <ref> [WCHC89] </ref>.
Reference: [WD94] <author> Gregory M. Werner and Michael G. Dyer. BioLand: </author> <title> A massively parallel simulation environment for evolving distributed forms of intelligent behavior. </title> <editor> In Hiroaki Kitano and James A. Hendler, editors, </editor> <booktitle> Massively Parallel Artificial Intelligence, </booktitle> <pages> pages 316-349, </pages> <address> Menlo Park, CA, 1994. </address> <publisher> AAAI Press / The MIT Press. </publisher>
Reference-contexts: [BKS93] CM-2 1024 other [S + 93c] DADO2 1024 rule-based system [CGK93] nCUBE 1024 search [NM93] CM-2 4096 genetic, classifier systems [Gel94a] CM-2 8192 SN and related [CG94] CM-2 8192 search [KK93] CM-2 8192 search [JSS93] MP-1 8192 theorem proving, logic [PKF93] CM-2 16384 search [PW94] CM-2 16384 natural language <ref> [WD94] </ref> CM-2 16384 neural net, learning [ND94] CM-2 16384 neural net, learning [AHEK94] CM-2 16384 SN and related [EAH94] CM-2 16384 SN and related [JB94] MP-1 16384 vision, neural net [ZMHV93] MP-1 16384 neural net, learning [Coo93] CM-2 32768 search Table 2.6: Sample of Recent Massively Parallel AI Publications Prior work
Reference: [WE88] <author> Neil Weste and Kamran Eshraghian. </author> <title> Principles of CMOS VLSI Design. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1988. </year>
Reference-contexts: For addressed memories, row address decoder, row driver, column multiplex, and column sense amplifier areas are added based on estimated transistor count and the general tile model. The the row decoder and drivers are based on designs by Weste and Eshraghian <ref> [WE88] </ref> The number of transistors is lg r ws + 11; if there are two or more sub-arrays one row decoder is shared between each pair. <p> Row decoder delay, T rdc , is 3T g and capacitance is wcap (A) + C tr r ws (lg R ws + 11)N a =2: The area and capacitance for the column multiplexers and sense amplifiers is calculated similarly, also based on designs by Weste and Eshraghian <ref> [WE88] </ref>. Equations for transistor counts are shown in Table B.1. If there is more than one sub-array, a column multiplexer and sense amplifier section is shared between 2 sub-arrays. <p> Calculated speed was also in line with [S + 91]. The CAM was validated against 2 static chips [J + 88, O + 89] and one dynamic chip [WS89] and showed good agreement. The Suspense cell-area model was compared to numerous layouts <ref> [WE88, Hug91, Rob89b] </ref>. Average error was 24% with a standard deviation of 17%.
Reference: [WG93] <author> Stefan Winz and James Geller. </author> <title> Methods of large grammar representation in massively parallel parsing systems. </title> <editor> In James Geller, editor, </editor> <booktitle> Innovative Applications of Massive Parallelism, </booktitle> <pages> pages 225-233. </pages> <publisher> AAAI, </publisher> <year> 1993. </year> <booktitle> Working Notes, Spring Symposium Series, </booktitle> <publisher> Stanford University. </publisher>
Reference-contexts: in reviving work on massively parallel AI. 42 Paper Machine PEs Category [SJ94b] search [ACP93] search [CR94] vision [Hen94] search [HJ93] constraint satisfaction [FFG94] other [SLH94] other [Pin94] neural net, logic [Ass94] theorem proving, logic [SS94a] rule-based system [Sha93] neural net [AG93] search [Kum93] SN and related [AB93] neural net <ref> [WG93] </ref> CM-5 other [LHB93] Symmetry constraint satisfaction [Rob93] custom other [HIM94] custom other [LCV94] transputer neural net [OMT94] custom 10 other [SJ94a] transputer 16 theorem proving, logic [Gel93b] CM-5 32 SN and related [CS94] CM-5 32 other [GW94] WARP 32 vision [LP94] iPSC 32 constraint satisfaction [ZM94] transputer 48 constraint satisfaction
Reference: [Win79] <author> P.H. Winston. </author> <title> Learning and reasoning by analogy. </title> <journal> Communications of the ACM, </journal> <volume> 23(12) </volume> <pages> 689-703, </pages> <year> 1979. </year>
Reference-contexts: There is a growing recognition of the importance of graphs as a representation of logical relations in general <ref> [Gen83, Win79] </ref> so that SI is an important operation despite its time complexity. Fortunately, the expected case time is empirically polynomial [Ull76, WWR91]. The worst-case time results from generally pathological cases that rarely occur in practice [Bar88, CK80, Sus65] so that parallelization can provide a difference in tractable graph size.
Reference: [WL89] <author> Benjamin W. Wah and Guo Jie Li. </author> <title> A survey on the design of multiprocessing systems for artificial intelligence applications. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 19(4) </volume> <pages> 667-692, </pages> <month> July/August </month> <year> 1989. </year>
Reference-contexts: Knowledge Processing Characteristics Some characteristics of knowledge processing algorithms place special demands on parallel processors <ref> [HGC87, WL89, WLL90] </ref> while others suggest promising avenues for research. Most knowledge processing programs have limited coarse-grain (control) parallelism; efforts to parallelize existing codes generally show poor processor utilization beyond 16 processors and negligible speedup beyond 32 [KT88, Kog91, WLL90]. <p> This is true even for control-level parallel machines designed specifically for Prolog and LISP. Fine-grain (massive) and medium-grain parallelism must be used to achieve significant speedup, but AI and symbolic processing have several characteristics that make such parallelization particularly difficult <ref> [HGC87, WL89, WLL90] </ref>. <p> Applications that especially benefit from content-addressing include natural language processing, neural networks, pattern and symbolic recognition, and semantic networks <ref> [Kog91, M + 90, Sto91, WL89, YZ90] </ref>. Unfortunately, CAM hardware implementations require uniform-length fixed-location data and exhibit poor utilization of hardware resources for non-matching operations.
Reference: [WLL90] <author> Benjamin W. Wah, Matthew B. Lowrie, and Guo-Jie Li. </author> <title> Computers for symbolic processing. </title> <editor> In Benjamin W. Wah and C. V. Ramamoorthy, editors, </editor> <booktitle> Computers for Artificial Intelligence Processing, </booktitle> <pages> pages 1-73. </pages> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: Conventional computers have been designed with numeric processing power as their focus. The disparity between symbolic and numeric operations, therefore, calls for innovative research in alternative architectures for symbolic processing. <ref> [WLL90] </ref> Although early efforts, such as the Japanese 5 th Generation and related U.S. and Eu-ropean projects gave less than the expected results, perhaps due to emphasis on logic programming and language-oriented processors, the progress and potential indicate the merits of continued research. <p> Knowledge Processing Characteristics Some characteristics of knowledge processing algorithms place special demands on parallel processors <ref> [HGC87, WL89, WLL90] </ref> while others suggest promising avenues for research. Most knowledge processing programs have limited coarse-grain (control) parallelism; efforts to parallelize existing codes generally show poor processor utilization beyond 16 processors and negligible speedup beyond 32 [KT88, Kog91, WLL90]. <p> Most knowledge processing programs have limited coarse-grain (control) parallelism; efforts to parallelize existing codes generally show poor processor utilization beyond 16 processors and negligible speedup beyond 32 <ref> [KT88, Kog91, WLL90] </ref>. On the other hand, there are numerous effective codes developed specifically for massively parallel machines including neural networks and natural language processing [AAA93]. Fine-grain (massive) parallelism is highly encouraging but there has been limited work to date. <p> Instead, this section summarizes and compares examples of different architectural classes to provide a context for MISC. The reader is referred to several surveys of specialized database, knowledge base, neural network, CAM, LISP, Prolog, and functional programming machines <ref> [KT88, Kog91, DFM89, CD89, WLL90] </ref>. 2.3.1 Associative Processors Associative processors offer fast highly-parallel matching with low silicon area and minimal communication. <p> Most conventional AI programs have limited coarse-grain (control-level) parallelism; efforts to parallelize existing serial AI codes generally show poor processor utilization beyond 16 processors and negligible speedup beyond 32 <ref> [KT88, Kog91, WLL90, K + 94a, K + 94b, Gel93a, KH94] </ref>. This is true even for control-level parallel machines designed specifically for Prolog and LISP. <p> This is true even for control-level parallel machines designed specifically for Prolog and LISP. Fine-grain (massive) and medium-grain parallelism must be used to achieve significant speedup, but AI and symbolic processing have several characteristics that make such parallelization particularly difficult <ref> [HGC87, WL89, WLL90] </ref>.
Reference: [WS89] <author> Jon P. Wade and Charles G. Sodini. </author> <title> A ternary content addressable search engine. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> 24(4) </volume> <pages> 1003-1013, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: The Coherent Processor features trinary logic (stored don't-care 23 states) for applications such as tree and substring search, graphics, pattern and symbol recognition, neural networks, and as a Prolog accelerator [Sto91]. Wade presents a system with a similar architecture <ref> [WS89] </ref>. PCAM Proximity Content-Addressable Memory (PCAM) (1990) is a smart-memory architecture for close-match database search. Designed by the author, it is the kernel from which the MISC processor microarchitecture grew. PCAM exploits massively parallel hardware for flexible searches in large knowledge and databases [Rob89a, Rob89b, Rob90]. <p> Calculated speed was also in line with [S + 91]. The CAM was validated against 2 static chips [J + 88, O + 89] and one dynamic chip <ref> [WS89] </ref> and showed good agreement. The Suspense cell-area model was compared to numerous layouts [WE88, Hug91, Rob89b]. Average error was 24% with a standard deviation of 17%.
Reference: [WWR91] <author> Peter Willett, Terence Wilson, and Stewart F. Reddaway. </author> <title> Atom-by-atom searching using massive parallelism: Implementation of the Ullmann subgraph isomor-phism algorithm on the distributed array processor. </title> <journal> Journal of the Chemical Information Computation Society, </journal> <volume> 31 </volume> <pages> 225-233, </pages> <year> 1991. </year>
Reference-contexts: More importantly, as just discussed, associative memory and simple matching are not enough to provide thorough massively parallel support for knowledge processing. One early associative processor proposal, the ASP (1962 [Koh80]) showed that SIMD processing can support operations on relational structures, and more recently Willett et al. <ref> [WWR91] </ref> have demonstrated subgraph isomorphism on a general-purpose SIMD architecture. In general, however, SIMD architectures are restricted to well-structured problems that feature regular patterns of control and uniformly structured data. <p> There is a growing recognition of the importance of graphs as a representation of logical relations in general [Gen83, Win79] so that SI is an important operation despite its time complexity. Fortunately, the expected case time is empirically polynomial <ref> [Ull76, WWR91] </ref>. The worst-case time results from generally pathological cases that rarely occur in practice [Bar88, CK80, Sus65] so that parallelization can provide a difference in tractable graph size. Further, graph size is bounded for most applications. <p> Willett et al. have parallelized Ullmann's algorithm, using O (n 2 ) 1-bit SIMD PEs to reduce time to typically O (n 2 ) <ref> [WWR91] </ref>. Multi-Level Indexed Search Multi-level indexed search (MIS) techniques can dramatically reduce search time when individual comparisons are time consuming (such as with subgraph isomorphism) by reducing the number of necessary comparisons [Lev92a]. <p> Willett et al. present algorithms for testing subgraph isomorphism on a single SIMD array and controller based on Ullmann's refinement, with expected time O (n 2 ) on O (n 2 ) processing elements for random graphs <ref> [WWR91, Ull76] </ref>. Since a tree-node can have at most one parent the number of nodes in the tree and symbols in the regular expression is O (e), where e is the total number of atomic elements, provided `empty' nestings such as [[[[x]]]] are prohibited. <p> Figure 5.7 diagrams the overall structure of the algorithm. First graphs are selected for comparison; those selected could include all graphs (exhaustive comparison) <ref> [WWR91] </ref>, those which pass filtering criteria (single-level indexing) [WWR91, Len88, Len92], or those passing multi-level indexing (initial graph SI tests filtering subsequent graphs) [LE92, HLR93]. Second, the match matrix M is formed. Third, the backtracking refinement search is performed. <p> Figure 5.7 diagrams the overall structure of the algorithm. First graphs are selected for comparison; those selected could include all graphs (exhaustive comparison) [WWR91], those which pass filtering criteria (single-level indexing) <ref> [WWR91, Len88, Len92] </ref>, or those passing multi-level indexing (initial graph SI tests filtering subsequent graphs) [LE92, HLR93]. Second, the match matrix M is formed. Third, the backtracking refinement search is performed. <p> In Sowa's "conceptual catalog" [Sow84], some 90% of the relations are binary and nearly all the remainder are monadic. Treating relations as labeled edges rather than nodes has a major speed advantage, as empirical time is O (n 2 ) in parallel implementation <ref> [WWR91] </ref>, where n is the number of nodes. In Levinson's UDS, CS are transformed such that the relations and their adjacent concept-types become nodes in the UDS graph. Edges in the UDS graph represent the bindings between the concept-type arguments of their relations. <p> performing time-consuming operations for all possible node mappings, it is better to perform those operations only on the mappings in an otherwise subgraph isomorphism. 5.3.2 Parallel Subgraph Isomorphism Prior Parallelizations for Single-Bit PE SIMD Arrays Work by Willett, Wilson, and Reddaway forms the basis of our parallelization of Ull-mann's algorithm <ref> [WWR91] </ref>. Their work details two parallel algorithms, the first performing pair-wise graph tests, the second comparing a query with all graphs in the database simultaneously. The matrix operations are readily parallelizable. Their parallelizations are for a bit-serial SIMD array processor, the DAP. <p> Further, due to the single control stream, conditional operations must be executed by enabling PEs based on the conditional test. As a result, on average only 10% of the PEs are active at any given time <ref> [WWR91] </ref>. <p> 4,096 graphs (sampled from a chemical database) having an average size of 22 nodes, search of the full DB with query graphs averaging 12 nodes was two times slower for Algorithm 2 on N PEs (where N is the number of graphs) than for Algorithm 1 on n 2 PEs <ref> [WWR91] </ref>. Willett et al. also present modifications in which Algorithm 2 is used until only some number of graphs are not complete; these remaining graphs are then processed individually using Algorithm 1. This gives a total DB search time half that of Algorithm 1 alone. <p> They assert that having fewer than N PEs, pulling in new graphs once a PE is free, and improvements in the looping and control functions would result in Algorithm 2 performing about ten times faster than Algorithm 1 on an exhaustive DB search. See <ref> [WWR91] </ref> for a more complete discussion of these implementations of Ullmann's refinement, with more detailed benchmark results. 95 A New Parallelization for Multi-Bit SIMD Processors Recent machines feature wide data words in each processor, typically 16-32 bits. <p> The first experiment ("small") is for a small graph, the second ("cg") roughly approximates a small conceptual graph, the third ("chem") roughly approximates the chemical database used by Willett et al. <ref> [WWR91] </ref>, and the fourth "large" a large graph. The second and third experiments are for relatively sparse graphs, the specified densities resulting in 1-2 edges per node; the others average 2-9 edges per node. <p> Although exponential in the worst case, Ullmann's full subgraph isomorphism algorithm requires O (n 4 ) bit operations in the expected case, where n is the number of graph nodes [Ull76] and each call to the refinement algorithm has time O (n 3 ) on a serial machine <ref> [WWR91] </ref>. In comparing different parallelizations, the expected-case n calls to refinement appear in all cases and thereby cancels out. By processing multiple bits in a b-bit processor, a factor of n is reduced to dn=be. Again, this affects all algorithms and can thereby be formally ignored. <p> Blelloch's dissertation shows how conditionals can be efficiently applied to nested collections [Ble90], and Willett's parallelization of Ullmann's algorithm demonstrates that recursive backtracking search algorithms can be run with nested parallelism on a SIMD architecture <ref> [WWR91] </ref>. SEGUL also includes a control structure for iterating through collections. Multiple Levels of Parallelism The SEGUL notation incorporates both implicit and explicit expression of multiple levels and grains of parallelism. Fine and medium grains of parallelism are implicit in treating the nested collections as a single object. <p> Results are adjusted for the typical number of calls to refinement <ref> [WWR91] </ref> to amortize the time for retrieving the knowledge base graphs and form the M match matrices. Boolean matrix multiply clearly dominates runtime, even with multiple bits per processor. Of the matrix multiply time, roughly 60% is in the matrix transposition and 40% in the multiply itself.
Reference: [YK94] <author> Moritoshi Yasunaga and Hiroaki Kitano. </author> <title> Wafer-scale integration for massively parallel AI. </title> <editor> In Hiroaki Kitano and James A. Hendler, editors, </editor> <booktitle> Massively Parallel Artificial Intelligence, </booktitle> <pages> pages 350-397, </pages> <address> Menlo Park, CA, 1994. </address> <publisher> AAAI Press / The MIT Press. </publisher>
Reference-contexts: SN and related [DB93] other [Kur94] theorem proving, logic [DN94] CM-2 vision, search [EAH93] CM-2 SN and related [Lan93] CM-2 SN, natural language [H + 94] IXM2 SN, natural language [O + 94] IXM2 SN and related [Fah93] NETL SN and related [Twa94] custom genetic, classifier systems [LH94] custom other <ref> [YK94] </ref> custom neural net, learning [CGK94] nCUBE2 512 other [HJ94] transputer 512 theorem proving, logic [BKS93] CM-2 1024 other [S + 93c] DADO2 1024 rule-based system [CGK93] nCUBE 1024 search [NM93] CM-2 4096 genetic, classifier systems [Gel94a] CM-2 8192 SN and related [CG94] CM-2 8192 search [KK93] CM-2 8192 search [JSS93] <p> Retrieval techniques and measures of similarity are important. More formally, as developed by Stanfill and Waltz memory-based reasoning (MBR) combines retrieved results for a statistical interpretation [SW86]. As first proposed, MBR refers specifically to similarity-based induction directly from memory. MBR has been used in a wide variety of applications <ref> [YK94] </ref>. MISC and the Memory-Based Reasoning Philosophy Most of the massively parallel implementations, including our example algorithms, can be loosely categorized as memory-based reasoning, the remainder as primarily state-space search.
Reference: [YZ90] <author> Joseph Yestrebsky and Michael Ziemacki. </author> <title> Data Preprocessing for AI Systems: </title>
Reference-contexts: Applications that especially benefit from content-addressing include natural language processing, neural networks, pattern and symbolic recognition, and semantic networks <ref> [Kog91, M + 90, Sto91, WL89, YZ90] </ref>. Unfortunately, CAM hardware implementations require uniform-length fixed-location data and exhibit poor utilization of hardware resources for non-matching operations.
References-found: 275

