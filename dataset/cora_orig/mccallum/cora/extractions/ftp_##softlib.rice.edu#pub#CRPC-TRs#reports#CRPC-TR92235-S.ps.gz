URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR92235-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: ADIFOR Working Note #4: ADIFOR: Fortran Source Translation for Efficient Derivatives  
Author: Christian Bischof Alan Carle George Corliss Andreas Griewank Paul Hovland 
Keyword: Key words. Large-scale problems, derivative, gradient, Jacobian, automatic differentiation, optimization, stiff ordinary differential equations, chain rule, parallel, ParaScope Parallel Programming Environment, source transformation and optimization.  
Note: Preprint MCS-P278-1291  
Affiliation: Argonne  
Abstract: The numerical methods employed in the solution of many scientific computing problems require the computation of derivatives of a function f : R n ! R m . Both the accuracy and the computational requirements of the derivative computation are usually of critical importance for the robustness and speed of the numerical method. ADIFOR (Automatic Differentiation In FORtran) is a source translation tool implemented by using the data abstractions and program analysis capabilities of the ParaScope Parallel Programming Environment. ADIFOR accepts arbitrary Fortran 77 code defining the computation of a function and writes portable Fortran 77 code for the computation of its derivatives. In contrast to previous approaches, ADIFOR views automatic differentiation as a process of source translation that exploits computational context to reduce the cost of derivative computations. Experimental results show that ADIFOR can handle real-life codes, providing exact derivatives with a running time that is competitive with the standard divided-difference approximations of derivatives and that may perform orders of magnitude faster than divided-differences in certain cases. The computational scientist using ADIFOR is freed from worrying about the accurate and efficient computation of derivatives, even for complicated "functions" and hence is able to concentrate on the more important issues of algorithm design or system modeling. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, Ravi I. Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <note> second edition, </note> <year> 1986. </year>
Reference-contexts: By applying constant folding and forward substitution, we eliminate multiplications by 1.0, and additions of 0.0, and we reduce the number of variables that must be allocated to hold derivative values <ref> [1] </ref>. In summary, ADIFOR proceeds as follows: 1. The user specifies the subroutine that corresponds to the "function" for which he wishes derivatives, as well as the variable names that correspond to dependent and independent variables. These names can be subroutine parameters or variables in common blocks.
Reference: [2] <author> Christian Bischof. </author> <title> Issues in parallel automatic differentiation. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, Implementation, and Application. </title> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991, </year> <pages> 100-113. </pages>
Reference-contexts: The resulting disadvantages, especially those associated with the exploitation of parallelism, are discussed in <ref> [2] </ref>. Loss of Efficiency: The overwhelming majority of codes for which computational scientists want derivatives are written in Fortran 77, which does not support operator overloading.
Reference: [3] <author> Christian Bischof, Alan Carle, George Corliss, Andreas Griewank, and Paul Hovland. </author> <title> Generating derivative codes from Fortran programs. </title> <type> Preprint MCS-P263-0991, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <year> 1991. </year> <note> Also appeared as Technical Report 91185, Center for Research in Parallel Computation, </note> <institution> Rice University, Houston, Texas. </institution>
Reference-contexts: Automatic differentiation takes advantage of the fact that the source code also contains information about derivatives of the function. ADIFOR (Automatic Differentiation In FORtran) <ref> [3] </ref> augments the original source code with additional statements that propagate values of derivative objects in addition to the values fl This work was supported by the Applied Mathematical Sciences subprogram of the Office of Energy Research, U.S. Department of Energy, under Contract W-31-109-Eng-38, through NSF Cooperative Agreement No.
Reference: [4] <author> Christian Bischof and Paul Hovland. </author> <title> Using ADIFOR to compute dense and sparse Jacobians. </title> <type> Technical Memorandum ANL/MCS-TM-158, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: The resulting code generated by ADIFOR can be called by user programs in a flexible manner to be used in conjunction with standard software tools for optimization, solving nonlinear equations, or for stiff ordinary differential equations. A discussion of calling the ADIFOR-generated code from users' programs in included in <ref> [4] </ref>. 4 Using ADIFOR The issues of ease of use and portability have received scant attention in software for automatic differentiation. <p> The user then selects the variables (in either parameter lists or common blocks) that correspond to the independent and dependent variables. ADIFOR then determines which other variables throughout the program require derivative information. A detailed description of the use of ADIFOR-generated code appears in <ref> [4] </ref>. Intuitive Interface: An X-windows interface for ADIFOR (called xadifor) makes it easy for the user to set up the ASCII script file that ADIFOR reads.
Reference: [5] <author> Christian Bischof and James Hu. </author> <title> Utilities for building and optimizing a computational graph for algorithmic decomposition. </title> <type> Technical Memorandum ANL/MCS-TM-148, </type> <institution> Mathematics and Computer Sciences Division, Argonne National Laboratory, Argonne, Ill., </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: The only drawback is that for straightforward implementations, the length of the tape is proportional to the number of arithmetic operations performed by f <ref> [20, 5] </ref>. Recently, Griewank [18] suggested an approach to overcome this limitation through clever checkpointing.
Reference: [6] <author> Paul T. Boggs and Janet E. Rogers. </author> <title> Orthogonal distance regression. </title> <journal> Contemporary Mathematics, </journal> <volume> 112 </volume> <pages> 183-193, </pages> <year> 1990. </year>
Reference-contexts: The test codes submitted to ADIFOR compute elementary Jacobian matrices that are then assembled to form a large sparse Jacobian matrix that is used in an orthogonal-distance regression fit <ref> [6] </ref>. The code named "adiabatic" is from Larry Biegler, Carnegie-Mellon University Chemical Engineering Department, and implements adiabatic flow, a common module in chemical engineering [31]. The code named "shock" was given to us by Greg Shubin, Boeing Computer Services, Seattle, Washington.
Reference: [7] <author> Preston Briggs, Keith D. Cooper, Mary W. Hall, and Linda Torczon. </author> <title> Goal-directed interprocedural optimization. </title> <type> CRPC Report CRPC-TR90102, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: The decision to do cloning based on active/passive variable context will eventually be based on an assessment of the savings made possible by introducing the cloned procedures, in accordance with the goal-directed interprocedural transformation approach being adopted within ParaScope <ref> [7] </ref>. Another advantage of basing ADIFOR within a sophisticated code optimization framework is that mechanisms are already in place for simplifying the derivative code that we generate by application of the statement-by-statement hybrid mode translation rules.
Reference: [8] <author> J. C. Butcher. </author> <title> The Numerical Analysis of Ordinary Differential Equations (Runge-Kutta and General Linear Methods). </title> <publisher> John Wiley and Sons, </publisher> <year> 1987. </year>
Reference-contexts: Probably best known are gradient methods for optimization [13], Newton's method for the solution of nonlinear systems [13], and the numerical solution of stiff ordinary differential equations <ref> [8] </ref>. These methods are examples of a large class of methods for numerical computation, where the computation of derivatives is a crucial ingredient in the computation of a numerical solution.
Reference: [9] <author> D. G. Cacuci. </author> <title> Sensitivity theory for nonlinear systems. I. Nonlinear functional analysis approach. </title> <journal> J. Math. Phys., </journal> <volume> 22(12) </volume> <pages> 2794-2802, </pages> <year> 1981. </year>
Reference-contexts: These quantities, usually referred to as adjoints, measure the sensitivity of the final result with respect to some intermediate quantity. This approach is closely related to the adjoint sensitivity analysis for differential equations that has been used at least since the late sixties, especially in nuclear engineering <ref> [9, 10] </ref>, weather forecasting [26], and neural networks [35]. The reverse mode requires fewer operations than the forward mode if the number of independent variables is larger than the number of dependent variables.
Reference: [10] <author> D. G. Cacuci. </author> <title> Sensitivity theory for nonlinear systems. II. Extension to additional classes of responses. </title> <journal> J. Math. Phys., </journal> <volume> 22(12) </volume> <pages> 2803-2812, </pages> <year> 1981. </year>
Reference-contexts: These quantities, usually referred to as adjoints, measure the sensitivity of the final result with respect to some intermediate quantity. This approach is closely related to the adjoint sensitivity analysis for differential equations that has been used at least since the late sixties, especially in nuclear engineering <ref> [9, 10] </ref>, weather forecasting [26], and neural networks [35]. The reverse mode requires fewer operations than the forward mode if the number of independent variables is larger than the number of dependent variables.
Reference: [11] <author> D. Callahan, K. Cooper, R. T. Hood, Ken Kennedy, and Linda M. Torczon. </author> <title> ParaScope: A parallel programming environment. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 2(4), </volume> <month> December </month> <year> 1988. </year>
Reference-contexts: In contrast, the reverse mode code requires space for five scalar auxiliary adjoint objects and has only one vector assignment. 3 ADIFOR Design and Implementation ADIFOR has been developed within the context of the ParaScope Parallel Programming Environment <ref> [11] </ref>, which combines dependence analysis with interprocedural analysis to support ambitious interprocedural code optimization and semi-automatic parallelization of Fortran programs.
Reference: [12] <author> Bruce D. Christianson. </author> <title> Automatic Hessians by reverse accumulation. </title> <type> Technical Report NOC TR228, </type> <institution> The Numerical Optimisation Center, Hatfield Polytechnic, Hatfield, U.K., </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: We also note that even though we showed the computation only of first derivatives, the automatic differentiation approach can easily be generalized to the computation of univariate Taylor series or Hessians and multivariate higher-order derivatives <ref> [12, 17, 29] </ref>. This discussion is intended to demonstrate that the principles underlying automatic differentiation are not complicated: We just associate extra computations (which are entirely specified on a statement-by-statement basis) with the statements executed in the original code.
Reference: [13] <author> John Dennis and R. Schnabel. </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1983. </year>
Reference-contexts: 1 Automatic Differentiation The methods employed for the solution of many scientific computing problems require the evaluation of derivatives of some function f that is usually represented as a computer program, not in closed form. Probably best known are gradient methods for optimization <ref> [13] </ref>, Newton's method for the solution of nonlinear systems [13], and the numerical solution of stiff ordinary differential equations [8]. These methods are examples of a large class of methods for numerical computation, where the computation of derivatives is a crucial ingredient in the computation of a numerical solution. <p> Probably best known are gradient methods for optimization <ref> [13] </ref>, Newton's method for the solution of nonlinear systems [13], and the numerical solution of stiff ordinary differential equations [8]. These methods are examples of a large class of methods for numerical computation, where the computation of derivatives is a crucial ingredient in the computation of a numerical solution.
Reference: [14] <author> Lawrence C. W. Dixon. </author> <title> Automatic differentiation and parallel processing in optimisation. </title> <type> Technical Report No. 180, </type> <institution> The Numerical Optimisation Center, Hatfield Polytechnic, Hatfield, U.K., </institution> <year> 1987. </year> <month> 10 </month>
Reference-contexts: Current tools (see [24]) achieve this by storing a record of every computation performed. An interpreter performs a backward pass on this "tape." The resulting overhead often dominates the complexity advantage of the reverse mode in an actual implementation (see <ref> [14, 15] </ref>). We also note that even though we showed the computation only of first derivatives, the automatic differentiation approach can easily be generalized to the computation of univariate Taylor series or Hessians and multivariate higher-order derivatives [12, 17, 29].
Reference: [15] <author> Lawrence C. W. Dixon. </author> <title> Use of automatic differentiation for calculating Hessians and Newton steps. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, Implementation, and Application. </title> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991, </year> <pages> 114-125. </pages>
Reference-contexts: Current tools (see [24]) achieve this by storing a record of every computation performed. An interpreter performs a backward pass on this "tape." The resulting overhead often dominates the complexity advantage of the reverse mode in an actual implementation (see <ref> [14, 15] </ref>). We also note that even though we showed the computation only of first derivatives, the automatic differentiation approach can easily be generalized to the computation of univariate Taylor series or Hessians and multivariate higher-order derivatives [12, 17, 29].
Reference: [16] <author> Andreas Griewank. </author> <title> On automatic differentiation. </title> <editor> In M. Iri and K. Tanabe, editors, </editor> <booktitle> Mathematical Programming: Recent Developments and Applications. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1989, </year> <pages> 83-108. </pages>
Reference-contexts: We call x the independent variable and y the dependent variable. While the terms "dependent", "independent", and "variable" are used in many different contexts, this terminology corresponds to the mathematical use of derivatives. There are four approaches to computing derivatives <ref> [16] </ref>: By Hand: As the problem complexity increases, this approach becomes increasingly difficult and error prone. Divided differences: The derivative of f with respect to the ith component of x at a particular point x 0 is approximated by either one-sided differences or central differences. <p> (i) end do ry (1) = 1.0/x (2) * ra - a/(x (2)*x (2)) * rx (2) ry (2) = cos (x (2)) * rx (2) variables x, rt = @ t @ t ! We can propagate these derivatives by using elementary differentiation arithmetic based on the chain rule <ref> [16, 29] </ref> for computing the derivatives of y (1) and y (2) as shown in Figure 2. In this example, each assignment to a derivative is actually a vector assignment of length 2. <p> This is exactly the case for computing a gradient, which can be viewed as a Jacobian matrix with only one row. This issue is discussed in more detail in <ref> [16, 18, 20] </ref>. Despite its advantages from the viewpoint of complexity, the implementation of the reverse mode for the general case is quite complicated. It requires the ability to access in reverse order the instructions performed for the computation of f and the values of their operands and results.
Reference: [17] <author> Andreas Griewank. </author> <title> Automatic evaluation of first- and higher-derivative vectors. </title> <editor> In R. Seydel, F. W. Schneider, T. Kupper, and H. Troger, editors, </editor> <booktitle> Proceedings of the Conference at Wurzburg, </booktitle> <month> Aug. </month> <year> 1990, </year> <title> Bifurcation and Chaos: Analysis, Algorithms, </title> <journal> Applications, </journal> <volume> volume 97. </volume> <publisher> Birkhauser Verlag, </publisher> <address> Basel, Switzer-land, </address> <year> 1991, </year> <pages> 135-148. </pages>
Reference-contexts: We also note that even though we showed the computation only of first derivatives, the automatic differentiation approach can easily be generalized to the computation of univariate Taylor series or Hessians and multivariate higher-order derivatives <ref> [12, 17, 29] </ref>. This discussion is intended to demonstrate that the principles underlying automatic differentiation are not complicated: We just associate extra computations (which are entirely specified on a statement-by-statement basis) with the statements executed in the original code.
Reference: [18] <author> Andreas Griewank. </author> <title> Achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation. Optimization Methods and Software, </title> <note> to appear. Also appeared as Preprint MCS-P228-0491, </note> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <year> 1991. </year>
Reference-contexts: This is exactly the case for computing a gradient, which can be viewed as a Jacobian matrix with only one row. This issue is discussed in more detail in <ref> [16, 18, 20] </ref>. Despite its advantages from the viewpoint of complexity, the implementation of the reverse mode for the general case is quite complicated. It requires the ability to access in reverse order the instructions performed for the computation of f and the values of their operands and results. <p> The only drawback is that for straightforward implementations, the length of the tape is proportional to the number of arithmetic operations performed by f [20, 5]. Recently, Griewank <ref> [18] </ref> suggested an approach to overcome this limitation through clever checkpointing.
Reference: [19] <author> Andreas Griewank and George F. Corliss, </author> <title> editors. Automatic Differentiation of Algorithms: Theory, Implementation, and Application. </title> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: automatic differentiation tools has prevented automatic differentiation from becoming a standard tool for mainstream high-performance computing, even though there are numerous applications where the need for accurate first- and higher-order derivatives has essentially mandated the use of automatic differentiation techniques and prompted the development of custom-tailored automatic differentiation systems (see <ref> [19] </ref>). For the majority of applications, however, existing automatic differentiation implementations have provided derivatives substantially slower than divided-difference approximations, discouraging potential users.
Reference: [20] <author> Andreas Griewank, David Juedes, Jay Srinivasan, and Charles Tyner. ADOL-C, </author> <title> a package for the automatic differentiation of algorithms written in C/C++. </title> <journal> ACM Trans. Math. Software, </journal> <note> to appear. Also appeared as Preprint MCS-P180-1190, </note> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <year> 1990. </year>
Reference-contexts: This is exactly the case for computing a gradient, which can be viewed as a Jacobian matrix with only one row. This issue is discussed in more detail in <ref> [16, 18, 20] </ref>. Despite its advantages from the viewpoint of complexity, the implementation of the reverse mode for the general case is quite complicated. It requires the ability to access in reverse order the instructions performed for the computation of f and the values of their operands and results. <p> The only drawback is that for straightforward implementations, the length of the tape is proportional to the number of arithmetic operations performed by f <ref> [20, 5] </ref>. Recently, Griewank [18] suggested an approach to overcome this limitation through clever checkpointing.
Reference: [21] <author> Kenneth E. Hillstrom. </author> <title> JAKEF A portable symbolic differentiator of functions given by algorithms. </title> <type> Technical Report ANL-82-48, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <year> 1982. </year>
Reference-contexts: Examples of this approach are DAPRE [28, 33], GRESS/ADGEN [22, 23], and JAKEF <ref> [21] </ref>.
Reference: [22] <author> Jim E. Horwedel. GRESS: </author> <title> A preprocessor for sensitivity studies on Fortran programs. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, Implementation, and Application. </title> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991, </year> <pages> 243-250. </pages>
Reference-contexts: While we can emulate operator overloading by associating a subroutine call with each elementary operation, this approach slows down computation considerably, and usually also imposes some restrictions on the syntactic structure of the code that can be processed. Examples of this approach are DAPRE [28, 33], GRESS/ADGEN <ref> [22, 23] </ref>, and JAKEF [21].
Reference: [23] <author> Jim E. Horwedel, Brian A. Worley, E. M. Oblow, and F. G. Pin. </author> <note> GRESS version 1.0 users manual. Technical Memorandum ORNL/TM 10835, </note> <institution> Martin Marietta Energy Systems, Inc., Oak Ridge National Laboratory, Oak Ridge, Tenn., </institution> <year> 1988. </year>
Reference-contexts: While we can emulate operator overloading by associating a subroutine call with each elementary operation, this approach slows down computation considerably, and usually also imposes some restrictions on the syntactic structure of the code that can be processed. Examples of this approach are DAPRE [28, 33], GRESS/ADGEN <ref> [22, 23] </ref>, and JAKEF [21].
Reference: [24] <author> David Juedes. </author> <title> A taxonomy of automatic differentiation tools. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, Implementation, and Application. </title> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991, </year> <pages> 315-329. </pages>
Reference-contexts: Despite its advantages from the viewpoint of complexity, the implementation of the reverse mode for the general case is quite complicated. It requires the ability to access in reverse order the instructions performed for the computation of f and the values of their operands and results. Current tools (see <ref> [24] </ref>) achieve this by storing a record of every computation performed. An interpreter performs a backward pass on this "tape." The resulting overhead often dominates the complexity advantage of the reverse mode in an actual implementation (see [14, 15]). <p> As a result, a variety of implementations of automatic differentiation have been developed over the years (see <ref> [24] </ref> for a survey). Most of these implementations implement automatic differentiation by means of operator overloading, which is a language feature of several modern programming languages, including C++, Ada, Pascal-XSC, and Fortran 90. Operator overloading provides the possibility of associating side-effects with the elementary arithmetic operations. <p> By taking a source translator view, we can bring the many man-years of effort of the compiler community to bear on this problem. ADIFOR differs from other implementations of automatic differentiation (see <ref> [24] </ref> for a survey) by being based on a source translation paradigm and by having been designed from the outset with large-scale codes and the need for highly efficient derivative computations in mind. ADIFOR provides the following features: Portability: ADIFOR produces vanilla Fortran 77 code.
Reference: [25] <author> Jorge J. </author> <title> More. On the performance of algorithms for large-scale bound constrained problems. </title> <editor> In T. F. Coleman and Y. Li, editors, </editor> <title> Large-Scale Numerical Optimization. </title> <publisher> SIAM, </publisher> <year> 1991, </year> <pages> 32-45. </pages>
Reference-contexts: We also see that ADIFOR can handle problems where symbolic techniques would be almost certain to fail, such as the "shock" or "reactor" codes. ADIFOR-generated derivatives can also outperform hand-coded derivatives. For example, consider the swirling flow problem from the MINPACK-2 test problem collection <ref> [25] </ref>. The problem consists of a coupled system of boundary value problems describing the steady flow of a viscous, incompressible, axisymmetric fluid between two rotating, infinite coaxial disks. The number of variables in the resulting optimization problem depends on the discretization.
Reference: [26] <author> I. Michael Navon and U. Muller. </author> <title> FESW | A finite-element Fortran IV program for solving the shallow water equations. </title> <booktitle> Advances in Engineering Software, </booktitle> <volume> 1 </volume> <pages> 77-84, </pages> <year> 1970. </year>
Reference-contexts: This approach is closely related to the adjoint sensitivity analysis for differential equations that has been used at least since the late sixties, especially in nuclear engineering [9, 10], weather forecasting <ref> [26] </ref>, and neural networks [35]. The reverse mode requires fewer operations than the forward mode if the number of independent variables is larger than the number of dependent variables. This is exactly the case for computing a gradient, which can be viewed as a Jacobian matrix with only one row.
Reference: [27] <author> Arnold Neumaier. </author> <title> Rigorous recursive calculations with functions. </title> <booktitle> Talk presented at Second International Conference on Industrial and Applied Mathematics (Washington, </booktitle> <address> D.C.), </address> <month> July </month> <year> 1991. </year>
Reference-contexts: Stetter has observed that in many applications, high-quality scientific computing requires the extraction of more mathematical information than just the function values [34]. For example, Neumaier <ref> [27] </ref> listed 15 mathematical properties (including derivative values, Lipschitz constants, enclosures, and asymptotic expansions) that might be propagated along with the values of the variable. Automatic differentiation takes advantage of the fact that the source code also contains information about derivatives of the function.
Reference: [28] <author> John D. Pryce and Paul H. Davis. </author> <title> A new implementation of automatic differentiation for use with numerical software. </title> <type> Technical Report TR AM-87-11, </type> <institution> Mathematics Department, Bristol University, </institution> <year> 1987. </year>
Reference-contexts: While we can emulate operator overloading by associating a subroutine call with each elementary operation, this approach slows down computation considerably, and usually also imposes some restrictions on the syntactic structure of the code that can be processed. Examples of this approach are DAPRE <ref> [28, 33] </ref>, GRESS/ADGEN [22, 23], and JAKEF [21].
Reference: [29] <author> Louis B. Rall. </author> <title> Automatic Differentiation: Techniques and Applications, </title> <booktitle> volume 120 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1981. </year>
Reference-contexts: (i) end do ry (1) = 1.0/x (2) * ra - a/(x (2)*x (2)) * rx (2) ry (2) = cos (x (2)) * rx (2) variables x, rt = @ t @ t ! We can propagate these derivatives by using elementary differentiation arithmetic based on the chain rule <ref> [16, 29] </ref> for computing the derivatives of y (1) and y (2) as shown in Figure 2. In this example, each assignment to a derivative is actually a vector assignment of length 2. <p> We also note that even though we showed the computation only of first derivatives, the automatic differentiation approach can easily be generalized to the computation of univariate Taylor series or Hessians and multivariate higher-order derivatives <ref> [12, 17, 29] </ref>. This discussion is intended to demonstrate that the principles underlying automatic differentiation are not complicated: We just associate extra computations (which are entirely specified on a statement-by-statement basis) with the statements executed in the original code.
Reference: [30] <author> G. R. Shubin, A. B. Stephens, H. M. Glaz, A. B. Wardlaw, and L. B. Hackerman. </author> <title> Steady shock tracking, Newton's method, and the supersonic blunt body problem. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 3(2) </volume> <pages> 127-144, </pages> <month> June </month> <year> 1982. </year> <month> 11 </month>
Reference-contexts: 42.34 36.14 15% Sparc 4/490 Reactor 3 fi 29 1455 13.34 8.33 38% RS6000 Shock 190 fi 190 1403 0.041 0.023 44% RS6000 Shock 190 fi 190 1403 0.46 0.31 33% Sparc1 Table 2: Performance of ADIFOR-generated derivative codes compared to divided-difference approximations method for the axisymmetric blunt body problem <ref> [30] </ref>. The Jacobian has a banded structure, and the compressed Jacobian has 28 columns, compared with 190 for the "normal" uncompressed Jacobian. Lastly, the code named "reactor" was given to us by Hussein Khalil, Argonne National Laboratory Reactor Analysis and Safety Division.
Reference: [31] <author> J. M. Smith and H. C. Van Ness. </author> <title> Introduction to Chemical Engineering. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: The code named "adiabatic" is from Larry Biegler, Carnegie-Mellon University Chemical Engineering Department, and implements adiabatic flow, a common module in chemical engineering <ref> [31] </ref>. The code named "shock" was given to us by Greg Shubin, Boeing Computer Services, Seattle, Washington.
Reference: [32] <author> Edgar J. Soulie. </author> <title> User's experience with Fortran precompilers for least squares optimization problems. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, Implementation, and Application. </title> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991, </year> <pages> 297-306. </pages>
Reference-contexts: Examples of this approach are DAPRE [28, 33], GRESS/ADGEN [22, 23], and JAKEF [21]. Experiments with some of these systems are described in <ref> [32] </ref>. 2 Hybrid Mode of Automatic Differentiation We believe that the lack of efficiency of previously existing automatic differentiation tools has prevented automatic differentiation from becoming a standard tool for mainstream high-performance computing, even though there are numerous applications where the need for accurate first- and higher-order derivatives has essentially mandated
Reference: [33] <author> Bruce R. Stephens and John D. Pryce. </author> <title> The DAPRE/UNIX Preprocessor Users' Guide v1.2. </title> <institution> Royal Military College of Science at Shrivenham, </institution> <year> 1990. </year>
Reference-contexts: While we can emulate operator overloading by associating a subroutine call with each elementary operation, this approach slows down computation considerably, and usually also imposes some restrictions on the syntactic structure of the code that can be processed. Examples of this approach are DAPRE <ref> [28, 33] </ref>, GRESS/ADGEN [22, 23], and JAKEF [21].
Reference: [34] <author> Hans J. Stetter. </author> <title> Inclusion algorithms with functions as data. </title> <journal> Computing, </journal> <volume> Suppl., 6 </volume> <pages> 213-224, </pages> <year> 1988. </year>
Reference-contexts: More sophisticated compilers extract from the source code information that allows some of the computations to be executed efficiently on vector or parallel computers. Stetter has observed that in many applications, high-quality scientific computing requires the extraction of more mathematical information than just the function values <ref> [34] </ref>. For example, Neumaier [27] listed 15 mathematical properties (including derivative values, Lipschitz constants, enclosures, and asymptotic expansions) that might be propagated along with the values of the variable. Automatic differentiation takes advantage of the fact that the source code also contains information about derivatives of the function.
Reference: [35] <author> P. Werbos. </author> <title> Applications of advances in nonlinear sensitivity analysis. In Systems Modeling and Optimization, </title> <address> New York, 1982. </address> <publisher> Springer Verlag, </publisher> <pages> 762-777. 12 </pages>
Reference-contexts: This approach is closely related to the adjoint sensitivity analysis for differential equations that has been used at least since the late sixties, especially in nuclear engineering [9, 10], weather forecasting [26], and neural networks <ref> [35] </ref>. The reverse mode requires fewer operations than the forward mode if the number of independent variables is larger than the number of dependent variables. This is exactly the case for computing a gradient, which can be viewed as a Jacobian matrix with only one row.
References-found: 35

