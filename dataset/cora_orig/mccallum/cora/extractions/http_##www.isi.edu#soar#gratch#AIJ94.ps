URL: http://www.isi.edu/soar/gratch/AIJ94.ps
Refering-URL: http://www.isi.edu/soar/gratch/home.html
Root-URL: 
Title: Decision-theoretic Approach to Adaptive Problem Solving  
Author: Jonathan Gratch and Gerald DeJong 
Date: May 1994  
Address: 405 N. Mathews, Urbana, IL 61801  
Affiliation: Beckman Institute University of Illinois  
Note: A  
Abstract: Technical Report UIUC-BI-AI-94-04, 1994 (submitted to the Journal of Artificial Intelligence) Abstract This article argues that it is desirable, and possible, to construct general problem solving techniques that automatically adapt to the characteristics of a specific application. Adaptive problem solving is a means of reconciling two seemingly contradictory needs. On the one hand, general purpose techniques can ease much of the burden of developing a application and satisfy the oft argued need for declarative and modular knowledge representation. On the other hand, general purpose approaches are ill-suited to the specialized demands of individual applications. General approaches have proven successful, only after a tedious cycle of manual experimentation and modification. Adaptive techniques promise to reduce the burden of this modification process and, thereby, take a long step toward reconciling the conflicting needs of generality and efficiency. A principal impediment to adaptive techniques is the utility problem - the realization that learning strategies can degrade performance under difficult to predict circumstances. We develop a formal characterization of the utility problem and introduce COMPOSER, a statistically rigorous approach to this problem. COMPOSER has been successfully applied to learning heuristics for planning and scheduling systems. This article includes theoretical results and an extensive empirical evaluation of the approach on learning adaptations to a domain-independent planner. The approach significantly outperforms several other proposed solutions to the utility problem. 
Abstract-found: 1
Intro-found: 1
Reference: [Berger80] <author> J. O. Berger, </author> <title> Statistical Decision Theory and Bayesian Analysis, </title> <publisher> Springer Ver-lag, </publisher> <year> 1980. </year>
Reference-contexts: The learning system must decide which among many hypothesized transformations to actually adopt. To make this statement concrete we use decision theory as a common framework for characterizing this decision problem. Decision theory provides a widely accepted framework for characterizing decision making under uncertainty (see <ref> [Berger80] </ref>). Jon Doyle has persuasively argued for the merits of decision-theory as a standard for evaluating artificial intelligence systems [Doyle90] and it has seen increasing acceptance, both in artificial intelligence at large [Horvitz89, Russell89, Schwuttke92, Wellman92] and machine learning in particular [Gratch92a, Greiner92a, Laird92, Subramanian92].
Reference: [Borgida89] <author> A. Borgida and D. W. Etherington, </author> <title> Hierarchical Knowledge Bases and Efficient Disjunctive Reasoning, </title> <booktitle> Proceedings of the First International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <address> Toronto, On-tario, Canada, </address> <month> May </month> <year> 1989, </year> <pages> pp. 33-43. </pages>
Reference-contexts: For example, Goldberg suggests that naturally occurring satisfiability problems are frequently solved in O (n 2 ) time [Goldberg82]. Recent work has focused on characterizing these easy distributions [Cheeseman89, Mitchell92] or devising techniques that can exploit specific distribution information when it is available <ref> [Borgida89] </ref>. Research into self-organizing systems [Melhorn84 pp. 252-285] and dynamic optimization [Laird92] exploit the fact that learning the expected distribution of tasks can allow the construction of a problem solver with substantially better expected performance.
Reference: [Bringer80] <author> H. Bringer, H. Martin, and K. -H. Scriever, </author> <title> Nonparametric Sequential Selection Procedures, </title> <publisher> Birkhuser Publishers, </publisher> <address> Boston, </address> <year> 1980. </year> <month> 40 </month>
Reference-contexts: As was shown in the theoretical analysis, the number of examples needed to make statistical inferences grows with the variance in the incremental utility values. Using a sampling technique called blocking it is often possible to minimize this variance <ref> [Bringer80] </ref>. To understand blocking, consider the problem of finding the highest yielding variety of wheat. Wheat yield is effected by several factors including the factor of interest, the variety of wheat, and other nuisance factors (e.g., the weather conditions in the year the crop was grown).
Reference: [Bylander92] <author> T. Bylander, </author> <title> Complexity Results for Extended Planning, </title> <booktitle> Proceedings of the First International Conference on Artificial Intelligence Planning Systems, </booktitle> <address> College Park, Maryland, </address> <month> June </month> <year> 1992, </year> <pages> pp. 20-27. </pages>
Reference-contexts: Many structural constraints have been identified that a learning system could exploit for more effective inference. For example, general purpose problem solvers provide flexible representations that can encode a variety of domains of discourse, but these cannot be reasoned with tractably <ref> [Bylander92, Chapman87, Eorl92, Freuder82] </ref>. However, this full generality is often unnecessary given the structure of a particular application. In contrast, there are a variety of effective problem solving techniques that apply if a domain obeys certain structural restrictions.
Reference: [Chapman87] <author> D. Chapman, </author> <title> Planning for Conjunctive Goals, </title> <booktitle> Artificial Intelligence 32, 3 (1987), </booktitle> <pages> pp. 333-378. </pages>
Reference-contexts: Many structural constraints have been identified that a learning system could exploit for more effective inference. For example, general purpose problem solvers provide flexible representations that can encode a variety of domains of discourse, but these cannot be reasoned with tractably <ref> [Bylander92, Chapman87, Eorl92, Freuder82] </ref>. However, this full generality is often unnecessary given the structure of a particular application. In contrast, there are a variety of effective problem solving techniques that apply if a domain obeys certain structural restrictions.
Reference: [Cheeseman89] <author> P. Cheeseman, B. Kanefsky and W. M. Taylor, </author> <title> Where the Really Hard Problems Are, </title> <booktitle> Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <address> Sidney, Australia, </address> <month> August </month> <year> 1989, </year> <pages> pp. 163-169. </pages>
Reference-contexts: Even worst-case intractable algorithms perform well under certain distributions. For example, Goldberg suggests that naturally occurring satisfiability problems are frequently solved in O (n 2 ) time [Goldberg82]. Recent work has focused on characterizing these easy distributions <ref> [Cheeseman89, Mitchell92] </ref> or devising techniques that can exploit specific distribution information when it is available [Borgida89]. Research into self-organizing systems [Melhorn84 pp. 252-285] and dynamic optimization [Laird92] exploit the fact that learning the expected distribution of tasks can allow the construction of a problem solver with substantially better expected performance.
Reference: [Chien94] <author> S. A. Chien, J. M. Gratch, and M. C. Burl, </author> <title> On the Efficient Allocation of Resources for Hypothesis Evaluation in Machine Learning: A Statistical Approach, </title> <type> Technical Report UIUC-BI-AI-94-01, </type> <institution> Beckman Institute, University of Illinois at Urbana-Champaign, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: However, given that other transformations may be better than the default, transformations could be more quickly eliminated if they are compared with the most promising transformation rather than the default control strategy. We investigate this and other extensions to COMPOSER in <ref> [Chien94] </ref>. Similar strategies for improving the statistical inference appear in [Maron94, Moore94]. Heuristics and prior information can replace or augment statistical estimates. Syntactic measures like the operationality criteria of Mitchell, Keller, and Kedar-Cabelli [Mitchell86] can be seen as approximate binary measures of incremental utility.
Reference: [Dean91] <author> T. L. Dean and M. P. Wellman, </author> <title> Planning and Control, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference: [DeJong86] <author> G. F. DeJong and R. J. Mooney, </author> <title> Explanation-Based Learning: An Alternative View, </title> <booktitle> Machine Learning 1, </booktitle> <month> 2 (April </month> <year> 1986), </year> <pages> pp. 145-176. </pages> <note> (Also appears as Technical Report UILU-ENG-86-2208, </note> <institution> AI Research Group, Coordinated Science Laboratory, University of Illinois at Urbana-Champaign.) </institution>
Reference-contexts: Maximizing expected utility therefore translates into decreasing the average time to required to solve a problem. 4.2 TRANSFORMATION GENERATOR Minton introduced a technique for generating atomic modifications to the PRODIGY control strategy. The approach uses explanation-based learning (EBL) <ref> [DeJong86, Mitchell86] </ref> to construct atomic control rules based on traces of problem solving behavior and a theory of the problem solver. Sets of control rules can be associated with any of the four control points.
Reference: [Dean88] <author> T. Dean and M. Boddy, </author> <title> An Analysis of Time-Dependent Planning, </title> <booktitle> Proceedings of The Seventh National Conference on Artificial Intelligence, </booktitle> <address> Saint Paul, MN, </address> <month> August </month> <year> 1988, </year> <pages> pp. 49-54. </pages>
Reference-contexts: This cycle repeats until the training set is exhausted. Each time a transformation is adopted the expected utility of the resulting problem solver is higher than its predecessor, giving COMPOSER an anytime behavior <ref> [Dean88] </ref>. [1] PS := PS old ; T := TG (PS); i := 0; n := 0; a := Bound (d, |T|); [2] While more examples and T p j do [4] "tQT: Get DU n (t|PS) fl-estimates := * fi : n fi n 0 and n (|PS) n T
Reference: [Dechter87] <author> R. Dechter and J. Pearl, </author> <title> Network-Based Heuristics for Constraint-Satisfaction Problems, </title> <booktitle> Artificial Intelligence 34, </booktitle> <month> 1 (December </month> <year> 1987), </year> <pages> pp. 1-38. </pages>
Reference-contexts: For example, Eorl, Nau and Subrahmanian summarize complexity results for several specializations of STRIPS operators, some of which support worst-case polynomial planning [Eorl92]. Constraint satisfaction problems can be solved efficiently if the constraint graph is expressible as a tree <ref> [Dechter87, Freuder82] </ref>. Korf shows a domain supports efficient problem solving if it exhibits the property of serial decomposability [Korf87]. Many integer programming programming problems can be efficiently solved in practice by exploiting their structural properties [Fisher81].
Reference: [Dechter92] <author> R. Dechter, </author> <booktitle> Constraint Networks,in Encyclopedia of Artificial Intelligence, </booktitle> <editor> Stuart C Shapiro (ed.), </editor> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1992. </year>
Reference: [DeGroot70] <author> M. H. </author> <title> DeGroot, Optimal Statistical Decisions, </title> <publisher> McGrawHill Book Co., </publisher> <address> New York, </address> <year> 1970. </year>
Reference-contexts: Decision theory posits the expected-utility hypothesis that there exists such a utility function for any consistent set of preferences (see <ref> [DeGroot70] </ref> Chapter 7). The utility function must also be computable by the learning system. For example, if the application requires an efficient problem solver, the utility function could be the CPU cost to solve a problem.
Reference: [Doyle90] <author> J. Doyle, </author> <title> Rationality and its Roles in Reasoning (extended version), </title> <booktitle> Proceedings of the National Conference on Artificial Intelligence, </booktitle> <address> Boston, MA, </address> <year> 1990, </year> <pages> pp. 1093-1100. </pages>
Reference-contexts: Decision theory provides a widely accepted framework for characterizing decision making under uncertainty (see [Berger80]). Jon Doyle has persuasively argued for the merits of decision-theory as a standard for evaluating artificial intelligence systems <ref> [Doyle90] </ref> and it has seen increasing acceptance, both in artificial intelligence at large [Horvitz89, Russell89, Schwuttke92, Wellman92] and machine learning in particular [Gratch92a, Greiner92a, Laird92, Subramanian92]. <p> This suggest a much different control structure than simple hill-climbing search. Future work will consider such alternatives. Finally, we are investigating how to apply notions from work on bounded rationality to help control the search <ref> [Doyle90, Horvitz89] </ref>. COMPOSER is directed towards identifying transformations with high incremental utility. The utility function determines the value of transformations, and thus the direction of this search. The efficiency of search is ensured by imposing some restrictive bias on how exploration proceeds.
Reference: [Eorl92] <author> K. Eorl, D. S. Nau and V. S. Subrahmanian, </author> <title> On the Complexity of Domain-Independent Planning, </title> <booktitle> Proceedings of the National Conference on Artificial Intelligence, </booktitle> <address> San Jose, CA, </address> <month> July </month> <year> 1992, </year> <pages> pp. 381-386. </pages>
Reference-contexts: Many structural constraints have been identified that a learning system could exploit for more effective inference. For example, general purpose problem solvers provide flexible representations that can encode a variety of domains of discourse, but these cannot be reasoned with tractably <ref> [Bylander92, Chapman87, Eorl92, Freuder82] </ref>. However, this full generality is often unnecessary given the structure of a particular application. In contrast, there are a variety of effective problem solving techniques that apply if a domain obeys certain structural restrictions. <p> In contrast, there are a variety of effective problem solving techniques that apply if a domain obeys certain structural restrictions. For example, Eorl, Nau and Subrahmanian summarize complexity results for several specializations of STRIPS operators, some of which support worst-case polynomial planning <ref> [Eorl92] </ref>. Constraint satisfaction problems can be solved efficiently if the constraint graph is expressible as a tree [Dechter87, Freuder82]. Korf shows a domain supports efficient problem solving if it exhibits the property of serial decomposability [Korf87].
Reference: [Etzioni90a] <author> O. Etzioni, </author> <title> A Structural Theory of Search Control, </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer Science, Carnegie-Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1990. </year> <month> 41 </month>
Reference-contexts: One solution is to develop independent measures. These are local measures that assess the quality of an atomic transformation independently of whatever other transformations appear, or will appear, in the final composite transformation. Mitchell et. al.'s operationality criteria [Mitchell86] and Etzio-ni's non-recursive hypothesis <ref> [Etzioni90a] </ref> can be seen as attempts to provide such a measure. Unfortunately, independent measures are in general not possible. Atomic transformations potentially interact with each other in difficult to predict ways. <p> For example, SOAR can be viewed as a transformation generator takes the current problem solver and the single training problem that produced an impasse, and generates a set of chunks. Oren Etzioni's STATIC system <ref> [Etzioni90a] </ref> can be seen as a transformation generator that takes the original problem solver and no training examples, and generates a set of control rules. (3) 3.2.3 Statistical Inference COMPOSER must estimate incremental utility from training data and assess the accuracy of these estimates. <p> Due to the relatively low variable of control rules we used an initial sample size of three. 19 4.4 EVALUATION We evaluated COMPOSER/PRODIGY's performance against four other proposed criteria for addressing the utility problem: the heuristic utility analysis of PRODIGY/EBL [Minton88], the non-recursive hypothesis of STATIC <ref> [Etzioni90a] </ref>, a hybrid of PRODIGY/EBL and STATIC suggested by Etzioni [Etzioni90a] to overcome limitations of the two systems, and PALO [Greiner92a], a statistical approach similar to COMPOSER but based on a more conservative statistical model. Before discussing the experiments we review these techniques. <p> of control rules we used an initial sample size of three. 19 4.4 EVALUATION We evaluated COMPOSER/PRODIGY's performance against four other proposed criteria for addressing the utility problem: the heuristic utility analysis of PRODIGY/EBL [Minton88], the non-recursive hypothesis of STATIC <ref> [Etzioni90a] </ref>, a hybrid of PRODIGY/EBL and STATIC suggested by Etzioni [Etzioni90a] to overcome limitations of the two systems, and PALO [Greiner92a], a statistical approach similar to COMPOSER but based on a more conservative statistical model. Before discussing the experiments we review these techniques. For the evaluations we tried to minimize differences between the systems that are not theoretically motivated. <p> NONREC replaces PRODIGY/EBL's empirical utility analysis with a syntactic criterion which only adopts nonrecursive control rules. This acts as a filter, only allowing PRODIGY/EBL to generate non-recursive rules. 4.4.3 A Composite Algorithm Etzioni suggests that the strengths of STATIC and PRODIGY/EBL can be combined into a single approach <ref> [Etzioni90a] </ref>. He proposed a hybrid algorithm which embodies several advancements including a two layered utility criterion. The nonrecursive hypothesis acts as an initial filter, but the remaining nonrecursive control rules are subject to utility analysis and may be later discarded. We implemented the NONREC-UA algorithm to test this hybrid criterion. <p> In addition to the conservative stopping rule, PALO adopts the conservative definition of Bound that follows from adopting Equations 5 and 9. 21 4.4.5 Experimental Procedure We investigated the STRIPS domain from [Minton88], the AB-WORLD domain from <ref> [Etzioni90a] </ref> for which PRODIGY/EBL produced harmful strategies, and the BIN-WORLD domain from [Gratch91b] which yielded detrimental results for both STATIC's and PRODIGY/EBL's learning criteria. Results are summarized in Figure 6. In each domain the algorithms were trained on 100 training examples drawn randomly from a fixed distribution.
Reference: [Etzioni90b] <author> O. Etzioni, </author> <title> Why Prodigy/EBL Works, </title> <booktitle> Proceedings of the National Conference on Artificial Intelligence, </booktitle> <address> Boston, MA, </address> <month> August </month> <year> 1990, </year> <pages> pp. 916-922. </pages>
Reference-contexts: Minton originally discussed the problem in the context of improving the average problem solving speed via search control heuristics called control rules. While there has been considerable progress on this issue <ref> [Etzioni90b, Holder92, Lewins93, Minton88] </ref>, the proposed methods are often ad hoc, poorly understood, and can fail to improve performance, or worse, actually degrade problem solving performance under certain circumstances. In this article we introduce a more general characterization of the utility problem, formalized in the terminology of decision theory. <p> We describe an application of COMPOSER to learning search control strategies for the PRODIGY planning system [Minton88]. PRODIGY is a well studied planning system that has served 16 as the basis for several learning investigations <ref> [Etzioni90b, Knoblock89, Minton88] </ref> and has attained the status as a benchmark for learning systems. COMPOSER has also been successfully applied to the problem of learning heuristic control strategies for a NASA scheduling domain, which is described elsewhere [Gratch93]. <p> Minton calls the overall approach of EBL learning and heuristic utility analysis PRODIGY/EBL. Unfortunately, while it performs quite well on some domains, PRODIGY/ EBL has since been shown to have undesirable properties. Oren Etzioni illustrated how seemingly innocuous changes to a domain theory result in degraded problem solving performance <ref> [Etzioni90b] </ref>. We showed that this behavior is due to the utility procedure's inability to correctly estimate distribution information and to handle the composability problem (see [Gratch91]). COMPOSER's statisti 18 cally sound utility estimation procedure corrects these problems and should result in a more effective learning algorithm. <p> The issue of interactions between transformations is also not addressed. STATIC applies this criterion to control rules but the issue is important in macro-operators as well [Letovsky90, Subramanian90]. STATIC out-performs PRODIGY/EBL's on several domains. The nonrecursive hypothesis is cited as a principal reason for this success <ref> [Etzioni90b] </ref> 7 . This claim is difficult to evaluate as these algorithms use different vocabularies to construct their control rules. We wish to focus on the effectiveness of the non-recursive hypothesis, and therefore must remove the complicating factor of a different 7.
Reference: [Etzioni91] <author> O. Etzioni, </author> <title> STATIC a Problem-Space Compiler for PRODIGY, </title> <booktitle> Proceedings of the National Conference on Artificial Intelligence, </booktitle> <address> Anaheim, CA, </address> <month> July </month> <year> 1991, </year> <pages> pp. 533-540. </pages>
Reference-contexts: The STRIPS domain was reported in [Minton88]. It is a problem of a robot moving boxes through interconnected rooms with lockable doors. The AB-WORLD domain was reported in <ref> [Etzioni91] </ref>. It is a variant of the standard blocksworld domain, designed to highlight deficiencies in Minton's PRODIGY/EBL approach. The BIN-WORLD domain was introduced in [Gratch91a] and was designed to highlight deficiencies in both PRODIGY/EBL and Etzioni's STATIC approach. This domain is a simple construction domain. <p> This is described in Appendix B. A more detailed description of the experiments appears in [Gratch93d]. 17 4.1.2 Expected Utility We follow the established PRODIGY methodology of measuring problem solving performance by the cumulative time in CPU seconds to solve a set of problems (e.g. <ref> [Etzioni91, Minton88] </ref>). Under the decision-theoretic interpretation of the evaluation criterion, this is captured as a utility function based on the time required to solve a problem. In particular, we let the utility of the problem solver over a problem be the negative of the CPU time required to solve it.
Reference: [Etzioni92] <author> O. Etzioni and S. Minton, </author> <title> Why EBL Produces Overly-Specific Knowledge: a Critique of the PRODIGY Approaches, </title> <booktitle> Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <address> Aberdeen, Scotland, </address> <month> July </month> <year> 1992, </year> <pages> pp. 137-143. </pages>
Reference-contexts: We wish to focus on the effectiveness of the non-recursive hypothesis, and therefore must remove the complicating factor of a different 7. Etzioni and Minton have subsequently suggested that some of the success of STATIC is due to the fact that its global analysis allows for more concise rules <ref> [Etzioni92] </ref>. 20 rule generator. To achieve this goal we constructed the NONREC algorithm, a re-implementation of STATIC's nonrecursive hypothesis within the PRODIGY/EBL framework. NONREC replaces PRODIGY/EBL's empirical utility analysis with a syntactic criterion which only adopts nonrecursive control rules. <p> In our experiments we constrained NONREC to use the rule vocabulary which was available to PRODIGY/EBL while in Etzioni's experiments STATIC entertained a somewhat different space of rules. This conjecture was recently supported by Minton and Etzioni <ref> [Etzioni92] </ref>. Finally, although PALO-RI did not improve performance within the 100 training examples, if given sufficient examples it would likely out-perform all other approaches. With extended examples it did exceed COMPOSER/PRODIGY's performance in AB-WORLD.
Reference: [Fikes71] <author> R. E. Fikes and N. J. Nilsson, </author> <title> STRIPS: A New Approach to the Application of Theorem Proving to Problem Solving, </title> <booktitle> Artificial Intelligence 2, 3/4 (1971), </booktitle> <pages> pp. 189-208. </pages>
Reference-contexts: A general algorithm might learn the idiosyncrasies of a domain through its problem solving experiences and automatically transform itself into an effective problem solver. In fact, machine learning techniques have successfully demonstrated this capacity in limited contexts <ref> [Fikes71, Laird86, Minton88] </ref>. Nonetheless, adaptive problem solving is still far from realization in any general sense. The principal impediment to adaptive problem solving is characterizing when an automatically hypothesized transformation actually results in improved problem solving performance. <p> PRODIGY is a general purpose means-ends problem solver based on the STRIPS planner <ref> [Fikes71] </ref> with a few enhancements. Plans are identified by depth-first search.
Reference: [Fisher81] <author> M. Fisher, </author> <title> The Lagrangian Relaxation Method for Solving Integer Programming Problems, </title> <booktitle> Management Science 27, 1 (1981), </booktitle> <pages> pp. 1-18. </pages>
Reference-contexts: Korf shows a domain supports efficient problem solving if it exhibits the property of serial decomposability [Korf87]. Many integer programming programming problems can be efficiently solved in practice by exploiting their structural properties <ref> [Fisher81] </ref>. In a sense, structural constraints arise from constraints on the distribution of tasks, thus highlighting the importance of acquiring distributional information. Structural constraints are rarely justifiable in principle as for almost any realistic problem solving task one can identify circumstances that require fully general reasoning mechanisms.
Reference: [Freuder82] <author> E. C. Freuder, </author> <title> A Sufficient Condition for Backtrack-Free Search, </title> <journal> J. Association for Computing Machinery 29, </journal> <month> 1 (January </month> <year> 1982), </year> <pages> pp. 24-32. </pages>
Reference-contexts: Many structural constraints have been identified that a learning system could exploit for more effective inference. For example, general purpose problem solvers provide flexible representations that can encode a variety of domains of discourse, but these cannot be reasoned with tractably <ref> [Bylander92, Chapman87, Eorl92, Freuder82] </ref>. However, this full generality is often unnecessary given the structure of a particular application. In contrast, there are a variety of effective problem solving techniques that apply if a domain obeys certain structural restrictions. <p> For example, Eorl, Nau and Subrahmanian summarize complexity results for several specializations of STRIPS operators, some of which support worst-case polynomial planning [Eorl92]. Constraint satisfaction problems can be solved efficiently if the constraint graph is expressible as a tree <ref> [Dechter87, Freuder82] </ref>. Korf shows a domain supports efficient problem solving if it exhibits the property of serial decomposability [Korf87]. Many integer programming programming problems can be efficiently solved in practice by exploiting their structural properties [Fisher81].
Reference: [Goldberg82] <author> A. Goldberg, P. W. Purdom and C. A. Brown, </author> <title> Average time analysis of simplified Davis-Putnam procedures, Information Process. </title> <journal> Lett. </journal> <volume> 15, </volume> <year> (1982), </year> <pages> pp. 72-75. </pages>
Reference-contexts: Even worst-case intractable algorithms perform well under certain distributions. For example, Goldberg suggests that naturally occurring satisfiability problems are frequently solved in O (n 2 ) time <ref> [Goldberg82] </ref>. Recent work has focused on characterizing these easy distributions [Cheeseman89, Mitchell92] or devising techniques that can exploit specific distribution information when it is available [Borgida89].
Reference: [Govindarajulu81] <author> Z. Govindarajulu, </author> <title> The Sequential Statistical Analysis, </title> <publisher> American Sciences Press, INC., </publisher> <address> Columbus, OH, </address> <year> 1981. </year>
Reference-contexts: The estimates must be accurate but we would like them to be based on as few examples as possible. COMPOSER relies on a sequential statistical technique to determine how many examples are sufficient to make this inference <ref> [Govindarajulu81] </ref>. Sequential procedures differ from the more common fixed-sample techniques in that the number of examples is not determined in advance, but is a function of the observations. Sequential procedures provide a test called a stopping rule that determines when sufficient examples have been taken.
Reference: [Gratch91] <author> J. Gratch and G. DeJong, </author> <title> A Hybrid Approach to Guaranteed Effective Control Strategies, </title> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <address> Evanston, IL, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Oren Etzioni illustrated how seemingly innocuous changes to a domain theory result in degraded problem solving performance [Etzioni90b]. We showed that this behavior is due to the utility procedure's inability to correctly estimate distribution information and to handle the composability problem (see <ref> [Gratch91] </ref>). COMPOSER's statisti 18 cally sound utility estimation procedure corrects these problems and should result in a more effective learning algorithm. For this evaluation the EBL technique serves as the transformation generator. <p> The EBL unit produces two control rule types: rejection rules and preference rules. Minton has noted that preference rules seem to be less effective. We have verified that PRODIGY/EBL actually produces strategies with higher utility if it is prevented from producing preference rules <ref> [Gratch91] </ref>. We disabled the learning of preference rules in this implementation because it enabled a more efficient means of gathering incremental utility data points. 6. Appendix A describes how for some applications the Bound function can be modified to improve the efficiency of utility analysis.
Reference: [Gratch92a] <author> J. Gratch and G. DeJong, COMPOSER: </author> <title> A Probabilistic Solution to the Utility Problem in Speed-up Learning, </title> <booktitle> Proceedings of the National Conference on Artificial Intelligence, </booktitle> <address> San Jose, CA, </address> <month> July </month> <year> 1992, </year> <pages> pp. 235-240. </pages>
Reference-contexts: Jon Doyle has persuasively argued for the merits of decision-theory as a standard for evaluating artificial intelligence systems [Doyle90] and it has seen increasing acceptance, both in artificial intelligence at large [Horvitz89, Russell89, Schwuttke92, Wellman92] and machine learning in particular <ref> [Gratch92a, Greiner92a, Laird92, Subramanian92] </ref>. We will use decision theory as a common framework for characterizing the utility problem. 5 2.1 EXPECTED UTILITY Different learning decisions result in different transformed problem solvers.
Reference: [Gratch92b] <author> J. Gratch and G. DeJong, </author> <title> A Framework of Simplifications in Learning to Plan, </title> <booktitle> First International Conference on Artificial Intelligence Planning Systems, </booktitle> <address> College Park, MD, </address> <year> 1992, </year> <pages> pp. 78-87. </pages>
Reference-contexts: COMPOSER embodies a particular set of commitments for each of these aspects and thus can be seen as one point in a large space of possible commitments (see also <ref> [Gratch92b] </ref>). 6.1 APPLICABILITY CONDITIONS COMPOSER embodies numerous restrictions in achieving the goal of efficient learning. As a result, the technique will not apply well to every situation. We can summarize four basic conditions that, if satisfied by an application, should lead to successful results. 1.
Reference: [Gratch93a] <author> J. Gratch and G. DeJong, </author> <title> Rational Learning: A Principled Approach to Balancing Learning and Action, </title> <type> Technical Report UIUCDCS-R-93-1801, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: We would like to develop a more principled method for characterizing the value of investigating transformations which directly relates their incremental utility and the cost to achieve this improvement. For some results in this area see <ref> [Gratch93a, Gratch94] </ref>. 6.3 ESTIMATING INCREMENTAL UTILITY The search through the transformation space relies on the ability to accurately estimate the incremental utility of alternative transformations. This is made difficult by the distributional nature of incremental utility.
Reference: [Gratch93b] <author> J. Gratch and S. Chien, </author> <title> Learning Search Control Knowledge for the Deep Space Network Scheduling Problem, </title> <booktitle> Proceedings of the Tenth International Conference onf Machine Learning, </booktitle> <address> Amherst, MA, </address> <month> July </month> <year> 1993. </year> <month> 42 </month>
Reference-contexts: We discuss this possibility in Chapter 6. 6 Limitations and Future Work COMPOSER provides a probabilistic solution to the utility problem and has demonstrated its practicality in the PRODIGY application and in two other implementations reported elsewhere <ref> [Gratch93b, Gratch93c] </ref>. These successes are encouraging, however it is important to realize that COMPOSER embodies many design commitments that restrict its generality. In this chapter we discuss these limitation and possible extensions to the approach. We first characterize some conditions on when COMPOSER will lead to successful results.
Reference: [Gratch93c] <author> J. Gratch, S, Chien, and G, DeJong, </author> <title> Learning Search Control Knowledge to Improve Schedule Quality, </title> <booktitle> IJCAI93 scheduling workshop. </booktitle>
Reference-contexts: We discuss this possibility in Chapter 6. 6 Limitations and Future Work COMPOSER provides a probabilistic solution to the utility problem and has demonstrated its practicality in the PRODIGY application and in two other implementations reported elsewhere <ref> [Gratch93b, Gratch93c] </ref>. These successes are encouraging, however it is important to realize that COMPOSER embodies many design commitments that restrict its generality. In this chapter we discuss these limitation and possible extensions to the approach. We first characterize some conditions on when COMPOSER will lead to successful results.
Reference: [Gratch93d] <author> J. M. Gratch, COMPOSER: </author> <title> A Decision-Theoretic Approach to Adaptive Problem Solving, </title> <type> Technical Report UIUCDCS-R-93-1806, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <year> 1993. </year>
Reference-contexts: Problems for the BIN-WORLD were generated in a distribution designed to highlight deficiencies in PRODIGY/EBL and STATIC. This is described in Appendix B. A more detailed description of the experiments appears in <ref> [Gratch93d] </ref>. 17 4.1.2 Expected Utility We follow the established PRODIGY methodology of measuring problem solving performance by the cumulative time in CPU seconds to solve a set of problems (e.g. [Etzioni91, Minton88]). <p> More precisely, this shows Q () fi 2flfl ffi 1fl . For large 1/a, Q (a) converges to about ln (1fl) ffi . This can be obtained using the asymptotic expansion of the standard normal distribution. Both derivations are given in <ref> [Gratch93d] </ref>. 5.2 SAMPLE COMPLEXITY The sample complexity is the number of examples required to perform statistical inference. COMPOSER takes some number of examples at each step of the hill-climbing search.
Reference: [Gratch94] <author> J. Gratch, S. Chien, and G. DeJong, </author> <title> Improving Learning Performance Through Rational Resource Allocation, </title> <booktitle> Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: We would like to develop a more principled method for characterizing the value of investigating transformations which directly relates their incremental utility and the cost to achieve this improvement. For some results in this area see <ref> [Gratch93a, Gratch94] </ref>. 6.3 ESTIMATING INCREMENTAL UTILITY The search through the transformation space relies on the ability to accurately estimate the incremental utility of alternative transformations. This is made difficult by the distributional nature of incremental utility.
Reference: [Greiner89] <author> R. Greiner and J. Likuski, </author> <title> Incorporating Redundant Learned Rules: A Preliminary Formal Analysis of EBL, </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, MI, </address> <month> August </month> <year> 1989, </year> <pages> pp. 744-749. </pages>
Reference-contexts: For many machine learning algorithms it can be shown that it is computa-tionally intractable to identify the optimal transformation. For example Greiner shows the inherent difficulties when transformations are constructed from macro-operators <ref> [Greiner89] </ref>. Therefore we have chosen not to insist on optimality and instead we adopt a weaker requirement for solving the utility problem. <p> Learning cost can be dramatically reduced if there is a detailed cost model of the problem solver that efficiently derives the ramification of proposed transformations without actually solving the problem (e.g. <ref> [Greiner89, Subramanian90] </ref>). With such a model we could simply draw a random training example and then use the model to determine the effect of different transformations. Such models are rarely available.
Reference: [Greiner92a] <author> R. Greiner and I. Jurisica, </author> <title> A Statistical Approach to Solving the EBL Utility Problem, </title> <booktitle> Proceedings of the National Conference on Artificial Intelligence, </booktitle> <address> San Jose, CA, </address> <month> July </month> <year> 1992, </year> <pages> pp. 241-248. </pages>
Reference-contexts: Jon Doyle has persuasively argued for the merits of decision-theory as a standard for evaluating artificial intelligence systems [Doyle90] and it has seen increasing acceptance, both in artificial intelligence at large [Horvitz89, Russell89, Schwuttke92, Wellman92] and machine learning in particular <ref> [Gratch92a, Greiner92a, Laird92, Subramanian92] </ref>. We will use decision theory as a common framework for characterizing the utility problem. 5 2.1 EXPECTED UTILITY Different learning decisions result in different transformed problem solvers. <p> three. 19 4.4 EVALUATION We evaluated COMPOSER/PRODIGY's performance against four other proposed criteria for addressing the utility problem: the heuristic utility analysis of PRODIGY/EBL [Minton88], the non-recursive hypothesis of STATIC [Etzioni90a], a hybrid of PRODIGY/EBL and STATIC suggested by Etzioni [Etzioni90a] to overcome limitations of the two systems, and PALO <ref> [Greiner92a] </ref>, a statistical approach similar to COMPOSER but based on a more conservative statistical model. Before discussing the experiments we review these techniques. For the evaluations we tried to minimize differences between the systems that are not theoretically motivated. <p> Perhaps the issue can be resolved by identifying hybrid statistical/analytic means to estimate utility values. An important area of future work is the possibility of basing incremental utility estimates on weaker and cheaper-to-obtain information. For example, Greiner and Jurisica <ref> [Greiner92a] </ref> propose one method for evaluating several transformations from a single solution attempt by maintaining upper and lower bounds on the utility of the novel search paths. <p> Equation 9 satisfies this requirement. 11 11. This equation was suggested to us by Russell Greiner. and is the basis for his PALO algorithm (see <ref> [Greiner92a] </ref>). 36 Once the application implementor chooses a model for the error within a step and a model for the error across steps, these should be unified into a overall function Bound (d, i, |T|) which combines the two choices into a error level for each estimate for the ith step <p> In Section 4 we describe one such unobtrusive implementation. Sometimes it is only possible to extract partial information in this way. Greiner and Jurisica <ref> [Greiner92a] </ref> propose one method for using such partial information that does not conflict with COMPOSER's assumptions and could incorporated. Appendix B BIN-WORLD Domain This domain, introduced in [Gratch91b], highlights deficiencies in PRODIGY/EBL's utility analysis and Etzioni's non-recursive hypothesis.
Reference: [Greiner92b] <author> R. Greiner and W. W. Cohen, </author> <title> Probabilistic Hill-Climbing, Proceedings of Computational Learning Theory and 'Natural' Learning Systems, </title> <note> 1992 (to appear). </note>
Reference-contexts: As control rules are proposed by PRODIGY/EBL's learning module, they are first filtered on the basis of the nonrecursive hypothesis. The remaining rules undergo utility analysis as in PRODIGY/EBL. 4.4.4 PALO'S Chernoff Bounds Greiner and Cohen have proposed an approach similar to COMPOSER's <ref> [Greiner92b] </ref>. The Probably Approximately Locally Optimal (PALO) approach also adopts a hill-climbing technique and evaluates transformations by a statistical method. PALO differs in its stopping rule and and that it incorporates a criterion for when to stop learning.
Reference: [Hogg78] <author> R. V. Hogg and A. T. Craig, </author> <title> Introduction to Mathematical Statistics, </title> <publisher> Macmillan Publishing Co., Inc., </publisher> <address> London, </address> <year> 1978. </year>
Reference: [Holder92] <author> L. B. Holder, </author> <title> Empirical Analysis of the General Utility Problem in Machine Learning, </title> <booktitle> Proceedings of the National Conference on Artificial Intelligence, </booktitle> <address> San Jose, CA, </address> <month> July </month> <year> 1992, </year> <pages> pp. 249-254. </pages>
Reference-contexts: Minton originally discussed the problem in the context of improving the average problem solving speed via search control heuristics called control rules. While there has been considerable progress on this issue <ref> [Etzioni90b, Holder92, Lewins93, Minton88] </ref>, the proposed methods are often ad hoc, poorly understood, and can fail to improve performance, or worse, actually degrade problem solving performance under certain circumstances. In this article we introduce a more general characterization of the utility problem, formalized in the terminology of decision theory.
Reference: [Horvitz89] <author> E. J. Horvitz, G. F. Cooper and D. E. Heckerman, </author> <title> Reflection and Action Under Scarce Resources: Theoretical Principles and Empirical Study, </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, MI, </address> <month> August </month> <year> 1989, </year> <pages> pp. 1121-1127. </pages>
Reference-contexts: Decision theory provides a widely accepted framework for characterizing decision making under uncertainty (see [Berger80]). Jon Doyle has persuasively argued for the merits of decision-theory as a standard for evaluating artificial intelligence systems [Doyle90] and it has seen increasing acceptance, both in artificial intelligence at large <ref> [Horvitz89, Russell89, Schwuttke92, Wellman92] </ref> and machine learning in particular [Gratch92a, Greiner92a, Laird92, Subramanian92]. We will use decision theory as a common framework for characterizing the utility problem. 5 2.1 EXPECTED UTILITY Different learning decisions result in different transformed problem solvers. <p> This suggest a much different control structure than simple hill-climbing search. Future work will consider such alternatives. Finally, we are investigating how to apply notions from work on bounded rationality to help control the search <ref> [Doyle90, Horvitz89] </ref>. COMPOSER is directed towards identifying transformations with high incremental utility. The utility function determines the value of transformations, and thus the direction of this search. The efficiency of search is ensured by imposing some restrictive bias on how exploration proceeds.
Reference: [Knoblock89] <author> C. Knoblock, </author> <title> Learning Hierarchies of Abstraction Spaces, </title> <booktitle> Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <address> Ithaca, NY, </address> <year> 1989, </year> <pages> pp. 241-245. </pages>
Reference-contexts: We describe an application of COMPOSER to learning search control strategies for the PRODIGY planning system [Minton88]. PRODIGY is a well studied planning system that has served 16 as the basis for several learning investigations <ref> [Etzioni90b, Knoblock89, Minton88] </ref> and has attained the status as a benchmark for learning systems. COMPOSER has also been successfully applied to the problem of learning heuristic control strategies for a NASA scheduling domain, which is described elsewhere [Gratch93].
Reference: [Korf87] <author> R. E. Korf, </author> <title> Planning as Search: A Quantitative Approach, </title> <booktitle> Artificial Intelligence 33, </booktitle> <year> (1987), </year> <pages> pp. 65-88. </pages>
Reference-contexts: Constraint satisfaction problems can be solved efficiently if the constraint graph is expressible as a tree [Dechter87, Freuder82]. Korf shows a domain supports efficient problem solving if it exhibits the property of serial decomposability <ref> [Korf87] </ref>. Many integer programming programming problems can be efficiently solved in practice by exploiting their structural properties [Fisher81]. In a sense, structural constraints arise from constraints on the distribution of tasks, thus highlighting the importance of acquiring distributional information.
Reference: [Laird86] <author> J. E. Laird, P. S. Rosenbloom and A. Newell, </author> <title> Universal Subgoaling and Chunking: The Automatic Generation and Learning of Goal Hierarchies, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Hingham, MA, </address> <year> 1986. </year>
Reference-contexts: A general algorithm might learn the idiosyncrasies of a domain through its problem solving experiences and automatically transform itself into an effective problem solver. In fact, machine learning techniques have successfully demonstrated this capacity in limited contexts <ref> [Fikes71, Laird86, Minton88] </ref>. Nonetheless, adaptive problem solving is still far from realization in any general sense. The principal impediment to adaptive problem solving is characterizing when an automatically hypothesized transformation actually results in improved problem solving performance. <p> Frequently there is some internal structure to transformations that can be exploited. In most learning techniques a composite transformation consists of many individual atomic transformations (later we refer to atomic modifications simply as transformations). For example, SOAR constructs a new problem solver from individual chunks <ref> [Laird86] </ref>. PRODIGY/EBL builds a learned control strategy from individual control rules [Minton88]. Two different composite transformations may share many of the same individual components. Instead of making a global decision among all possible composite transformations, an incremental learning system efficiently builds up a composite transformation by making many local decisions.
Reference: [Laird92] <author> P. Laird, </author> <title> Dynamic Optimization, </title> <booktitle> Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <address> Aberdeen, Scotland, </address> <month> July </month> <year> 1992, </year> <pages> pp. 263-272. 43 </pages>
Reference-contexts: Recent work has focused on characterizing these easy distributions [Cheeseman89, Mitchell92] or devising techniques that can exploit specific distribution information when it is available [Borgida89]. Research into self-organizing systems [Melhorn84 pp. 252-285] and dynamic optimization <ref> [Laird92] </ref> exploit the fact that learning the expected distribution of tasks can allow the construction of a problem solver with substantially better expected performance. <p> Jon Doyle has persuasively argued for the merits of decision-theory as a standard for evaluating artificial intelligence systems [Doyle90] and it has seen increasing acceptance, both in artificial intelligence at large [Horvitz89, Russell89, Schwuttke92, Wellman92] and machine learning in particular <ref> [Gratch92a, Greiner92a, Laird92, Subramanian92] </ref>. We will use decision theory as a common framework for characterizing the utility problem. 5 2.1 EXPECTED UTILITY Different learning decisions result in different transformed problem solvers. <p> Sometimes it may be possible to develop a single statistical model from which one can derive the incremental utility of multiple transformations. This is the approach taken by [Subramanian92] and <ref> [Laird92] </ref>. While not always possible, this is an important area of future research. Several statistical methods can be applied to further improve the efficiency of the estimation process. For example, currently a transformation is eliminated only if significantly worse than the default con 32 trol strategy.
Reference: [Letovsky90] <author> S. Letovsky, </author> <title> Operationality Criteria for Recursive Predicates, </title> <booktitle> Proceedings of the National Conference on Artificial Intelligence, </booktitle> <address> Boston, MA, </address> <month> August </month> <year> 1990, </year> <pages> pp. 936-941. </pages>
Reference-contexts: The issue of interactions between transformations is also not addressed. STATIC applies this criterion to control rules but the issue is important in macro-operators as well <ref> [Letovsky90, Subramanian90] </ref>. STATIC out-performs PRODIGY/EBL's on several domains. The nonrecursive hypothesis is cited as a principal reason for this success [Etzioni90b] 7 . This claim is difficult to evaluate as these algorithms use different vocabularies to construct their control rules.
Reference: [Lewins93] <author> N. J. Lewins, </author> <title> Practical Solution-caching for PROLOG: An Explanation-based Learning Approach, </title> <type> Ph.D., Thesis, </type> <institution> Department of Computer Science, University of Western Australia, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: Minton originally discussed the problem in the context of improving the average problem solving speed via search control heuristics called control rules. While there has been considerable progress on this issue <ref> [Etzioni90b, Holder92, Lewins93, Minton88] </ref>, the proposed methods are often ad hoc, poorly understood, and can fail to improve performance, or worse, actually degrade problem solving performance under certain circumstances. In this article we introduce a more general characterization of the utility problem, formalized in the terminology of decision theory.
Reference: [Maron94] <author> O. Maron and A W. Moore, </author> <title> Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation, </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <publisher> Morgan Kaufmann, </publisher> <month> April </month> <year> 1994. </year>
Reference-contexts: We investigate this and other extensions to COMPOSER in [Chien94]. Similar strategies for improving the statistical inference appear in <ref> [Maron94, Moore94] </ref>. Heuristics and prior information can replace or augment statistical estimates. Syntactic measures like the operationality criteria of Mitchell, Keller, and Kedar-Cabelli [Mitchell86] can be seen as approximate binary measures of incremental utility. Unfortunately, syntactic measures have difficulty capturing the distributional nature of incremental utility.
Reference: [Melhorn84] <author> K. Melhorn, </author> <title> Data Structures and Algorithms 1: Sorting and Searching, </title> <publisher> Springer Verlag, </publisher> <year> 1984. </year>
Reference: [Miller92] <author> D. P. Miller, R. S. Desai, E. Gat, R. Ivlev and J. Loch, </author> <title> Reactive Navigation through Rough Terrain: Experimental Results, </title> <booktitle> Proceedings of the National Conference on Artificial Intelligence, </booktitle> <address> San Jose, CA, </address> <month> July </month> <year> 1992, </year> <pages> pp. 823-828. </pages>
Reference-contexts: 1 Introduction There is a wide gulf between general approaches and effective approaches to problem solving. Practical success has come from custom techniques like expert systems, reactive planners <ref> [Miller92, Schoppers92] </ref>, or other application specific techniques that require extensive human investment to complete. AI researchers have also developed domain-independent algorithms such as non-linear planning and constraint satisfaction algorithms. Unfortunately, when general approaches show success, it is usually only after extensive domain-specific adjustments.
Reference: [Minton88] <author> S. Minton, </author> <title> in Learning Search Control Knowledge: An Explanation-Based Approach, </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1988. </year>
Reference-contexts: A general algorithm might learn the idiosyncrasies of a domain through its problem solving experiences and automatically transform itself into an effective problem solver. In fact, machine learning techniques have successfully demonstrated this capacity in limited contexts <ref> [Fikes71, Laird86, Minton88] </ref>. Nonetheless, adaptive problem solving is still far from realization in any general sense. The principal impediment to adaptive problem solving is characterizing when an automatically hypothesized transformation actually results in improved problem solving performance. <p> The principal impediment to adaptive problem solving is characterizing when an automatically hypothesized transformation actually results in improved problem solving performance. Steve Minton introduced to machine learning the term utility problem to refer to this difficulty in insuring performance improvements <ref> [Minton88] </ref>. Minton originally discussed the problem in the context of improving the average problem solving speed via search control heuristics called control rules. <p> Minton originally discussed the problem in the context of improving the average problem solving speed via search control heuristics called control rules. While there has been considerable progress on this issue <ref> [Etzioni90b, Holder92, Lewins93, Minton88] </ref>, the proposed methods are often ad hoc, poorly understood, and can fail to improve performance, or worse, actually degrade problem solving performance under certain circumstances. In this article we introduce a more general characterization of the utility problem, formalized in the terminology of decision theory. <p> A composite transformation is whatever is required to transform PS old into PS new and it may be built from several individual structural changes. For example, in the adaptive problem solving system PRODIGY/EBL <ref> [Minton88] </ref> a composite transformation is built from a sequence of learned control rules. A given learning algorithm has the potential to produce a variety of composite transformations depending on the initial problem solver and the distribution of problems. <p> In most learning techniques a composite transformation consists of many individual atomic transformations (later we refer to atomic modifications simply as transformations). For example, SOAR constructs a new problem solver from individual chunks [Laird86]. PRODIGY/EBL builds a learned control strategy from individual control rules <ref> [Minton88] </ref>. Two different composite transformations may share many of the same individual components. Instead of making a global decision among all possible composite transformations, an incremental learning system efficiently builds up a composite transformation by making many local decisions. <p> We illustrate the difficulty with interactions as they arise in one common learning for planning formalism. Consider the problem of building a composite transformation where the atomic transformations are search control rules, as in PRODIGY/EBL <ref> [Minton88] </ref>. Let utility be the time to solve a problem. Control rules can reduce the time to solve a problem by eliminating search, but they introduce an evaluation overhead: the control rule's preconditions must be continually evaluated to see if a portion of the search tree can be pruned. <p> We describe an application of COMPOSER to learning search control strategies for the PRODIGY planning system <ref> [Minton88] </ref>. PRODIGY is a well studied planning system that has served 16 as the basis for several learning investigations [Etzioni90b, Knoblock89, Minton88] and has attained the status as a benchmark for learning systems. <p> We describe an application of COMPOSER to learning search control strategies for the PRODIGY planning system [Minton88]. PRODIGY is a well studied planning system that has served 16 as the basis for several learning investigations <ref> [Etzioni90b, Knoblock89, Minton88] </ref> and has attained the status as a benchmark for learning systems. COMPOSER has also been successfully applied to the problem of learning heuristic control strategies for a NASA scheduling domain, which is described elsewhere [Gratch93]. <p> Control rules are described in Section 6.2. 4.1.1 Problem Distributions We evaluate COMPOSER's ability to identify effective modifications to PRODIGY on three different domain theories. The STRIPS domain was reported in <ref> [Minton88] </ref>. It is a problem of a robot moving boxes through interconnected rooms with lockable doors. The AB-WORLD domain was reported in [Etzioni91]. It is a variant of the standard blocksworld domain, designed to highlight deficiencies in Minton's PRODIGY/EBL approach. <p> This domain is a simple construction domain. Problem distributions for the STRIPS domain and AB-WORLD domain are constructed with the problem generators provided with PRODIGY 2.0. Following the methodology in <ref> [Minton88] </ref>, the set of problems were biased by filtering out problems that were judged too difficult or too easy; problems were excluded if the default PRODIGY control strategy required less than 1 CPU second or more than 80 CPU seconds. <p> This is described in Appendix B. A more detailed description of the experiments appears in [Gratch93d]. 17 4.1.2 Expected Utility We follow the established PRODIGY methodology of measuring problem solving performance by the cumulative time in CPU seconds to solve a set of problems (e.g. <ref> [Etzioni91, Minton88] </ref>). Under the decision-theoretic interpretation of the evaluation criterion, this is captured as a utility function based on the time required to solve a problem. In particular, we let the utility of the problem solver over a problem be the negative of the CPU time required to solve it. <p> Due to the relatively low variable of control rules we used an initial sample size of three. 19 4.4 EVALUATION We evaluated COMPOSER/PRODIGY's performance against four other proposed criteria for addressing the utility problem: the heuristic utility analysis of PRODIGY/EBL <ref> [Minton88] </ref>, the non-recursive hypothesis of STATIC [Etzioni90a], a hybrid of PRODIGY/EBL and STATIC suggested by Etzioni [Etzioni90a] to overcome limitations of the two systems, and PALO [Greiner92a], a statistical approach similar to COMPOSER but based on a more conservative statistical model. Before discussing the experiments we review these techniques. <p> We discuss the setting of the system's various parameters in the next section. 8. In addition to the conservative stopping rule, PALO adopts the conservative definition of Bound that follows from adopting Equations 5 and 9. 21 4.4.5 Experimental Procedure We investigated the STRIPS domain from <ref> [Minton88] </ref>, the AB-WORLD domain from [Etzioni90a] for which PRODIGY/EBL produced harmful strategies, and the BIN-WORLD domain from [Gratch91b] which yielded detrimental results for both STATIC's and PRODIGY/EBL's learning criteria. Results are summarized in Figure 6. <p> In fact, it does not appear that any control rule improves 9. PRODIGY/EBL's utility analysis requires an additional settling phase after training. Each control strategy produced by PRODIGY/EBL and NONREC+UA received a settling phase of 20 problems following the methodology outlined in <ref> [Minton88] </ref>. 10. C was fixed based on the size of the candidate list observed in practice. In the best case a rule can save the entire cost of solving a problem, so for each domain, lambda for each rule was set at the maximum problem solving cost observed in practice.
Reference: [Mitchell86] <author> T. M. Mitchell, R. Keller and S. Kedar-Cabelli, </author> <title> Explanation-Based Generalization: A Unifying View, </title> <booktitle> Machine Learning 1, </booktitle> <month> 1 (January </month> <year> 1986), </year> <pages> pp. 47-80. </pages>
Reference-contexts: One solution is to develop independent measures. These are local measures that assess the quality of an atomic transformation independently of whatever other transformations appear, or will appear, in the final composite transformation. Mitchell et. al.'s operationality criteria <ref> [Mitchell86] </ref> and Etzio-ni's non-recursive hypothesis [Etzioni90a] can be seen as attempts to provide such a measure. Unfortunately, independent measures are in general not possible. Atomic transformations potentially interact with each other in difficult to predict ways. <p> Maximizing expected utility therefore translates into decreasing the average time to required to solve a problem. 4.2 TRANSFORMATION GENERATOR Minton introduced a technique for generating atomic modifications to the PRODIGY control strategy. The approach uses explanation-based learning (EBL) <ref> [DeJong86, Mitchell86] </ref> to construct atomic control rules based on traces of problem solving behavior and a theory of the problem solver. Sets of control rules can be associated with any of the four control points. <p> We investigate this and other extensions to COMPOSER in [Chien94]. Similar strategies for improving the statistical inference appear in [Maron94, Moore94]. Heuristics and prior information can replace or augment statistical estimates. Syntactic measures like the operationality criteria of Mitchell, Keller, and Kedar-Cabelli <ref> [Mitchell86] </ref> can be seen as approximate binary measures of incremental utility. Unfortunately, syntactic measures have difficulty capturing the distributional nature of incremental utility. Instead, we are investigating the use of such measures as a bias on the estimation process.
Reference: [Mitchell92] <author> D. Mitchell, B. Selman and H. Levesque, </author> <title> Hard and Easy Distributions of SAT Problems, </title> <booktitle> Proceedings of the National Conference on Artificial Intelligence, </booktitle> <address> San Jose, CA, </address> <month> July </month> <year> 1992, </year> <pages> pp. 459-465. </pages>
Reference-contexts: Even worst-case intractable algorithms perform well under certain distributions. For example, Goldberg suggests that naturally occurring satisfiability problems are frequently solved in O (n 2 ) time [Goldberg82]. Recent work has focused on characterizing these easy distributions <ref> [Cheeseman89, Mitchell92] </ref> or devising techniques that can exploit specific distribution information when it is available [Borgida89]. Research into self-organizing systems [Melhorn84 pp. 252-285] and dynamic optimization [Laird92] exploit the fact that learning the expected distribution of tasks can allow the construction of a problem solver with substantially better expected performance.
Reference: [Moore94] <author> A. W. Moore and M. S. Lee, </author> <title> Efficient Algorithms for Minimizing Cross Validation Error, </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: We investigate this and other extensions to COMPOSER in [Chien94]. Similar strategies for improving the statistical inference appear in <ref> [Maron94, Moore94] </ref>. Heuristics and prior information can replace or augment statistical estimates. Syntactic measures like the operationality criteria of Mitchell, Keller, and Kedar-Cabelli [Mitchell86] can be seen as approximate binary measures of incremental utility. Unfortunately, syntactic measures have difficulty capturing the distributional nature of incremental utility.
Reference: [Mostow81] <author> J. Mostow, </author> <title> Mechanical Transformation of Task Heuristics into Operational Procedures, </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer Science, </institution> <address> CMU, Pittsburgh, PA, </address> <year> 1981. </year>
Reference: [Ndas69] <author> A. Ndas, </author> <title> An extension of a theorem of Chow and Robbins on sequential confidence intervals for the mean, </title> <journal> The Annals of Mathematical Statistics 40, </journal> <volume> 2 (1969), </volume> <pages> pp. 667-671. </pages>
Reference-contexts: An important advantage of sequential procedures is that the average number of examples required is smaller than the number required by a fixed-sample technique. We determine the sample size using a stopping rule proposed by Ndas <ref> [Ndas69] </ref>. The intuition behind the stopping rule is quite simple. Given a sample of values and a confidence level a, we can construct confidence intervals that contain, with probability 1 - a, the true incremental utility lies. <p> Given some transformation, t, and an error level a, this stopping rule determines sufficient examples to show that the incremental utility of t is positive (negative) with probability 1-a. The characteristics of this stopping rule have been proven by Author Ndas in <ref> [Ndas69] </ref>. The proofs are technical but we will restate the results and give an intuitive explanation of why they hold.
Reference: [Natarajan89] <author> B. K. Natarajan, </author> <title> On Learning from Exercises, </title> <booktitle> Proceedings of the Second Annual Workshop on Computational Learning Theory, </booktitle> <address> Santa Cruz, CA, </address> <month> JULY </month> <year> 1989, </year> <pages> pp. 72-87. 44 </pages>
Reference-contexts: Other authors have suggested that it may be possible to gain useful information about currently intractable problems by first learning from simpler problems (e.g. <ref> [Natarajan89] </ref>) or by observing a teacher (e.g. [Tadepalli91]). 7 Conclusion This article has argued that it is desirable, and possible, to construct general problem solving techniques that automatically adapt to the characteristics of a specific application. Adaptive problem 33 solving is a means of reconciling two seemingly contradictory needs.
Reference: [Prez92] <author> M. A. Prez and O. Etzioni, </author> <title> DYNAMIC: a new rule for trining problems in EBL, </title> <booktitle> Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <address> Aberdeen, Scotland, </address> <month> July </month> <year> 1992, </year> <pages> pp. 367-372. </pages>
Reference: [Roy71] <author> B. Roy, </author> <title> Problems and Methods with Multiple Objective Functions, </title> <journal> Mathematical Programming, </journal> <volume> Vol. 1, No. 2, </volume> <year> 1971. </year>
Reference-contexts: we may care not only about how fast a plan is created, but also about other attributes such as plan intelligibility, execution cost, and robustness. (1) A large literature in the decision sciences is devoted to how to translate these multi-attribute problems into a single utility function (see, for example, <ref> [Roy71] </ref>). With the fixed distribution and expected-utility assumptions we can characterize the value of a problem solver by its expected utility.
Reference: [Russell89] <author> S. Russell and E. </author> <title> Wefald, </title> <booktitle> Principles of Metareasoning, Proceedings of the First International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <address> Toronto, Ontario, Canada, </address> <month> May </month> <year> 1989, </year> <pages> pp. 400-411. </pages>
Reference-contexts: Decision theory provides a widely accepted framework for characterizing decision making under uncertainty (see [Berger80]). Jon Doyle has persuasively argued for the merits of decision-theory as a standard for evaluating artificial intelligence systems [Doyle90] and it has seen increasing acceptance, both in artificial intelligence at large <ref> [Horvitz89, Russell89, Schwuttke92, Wellman92] </ref> and machine learning in particular [Gratch92a, Greiner92a, Laird92, Subramanian92]. We will use decision theory as a common framework for characterizing the utility problem. 5 2.1 EXPECTED UTILITY Different learning decisions result in different transformed problem solvers.
Reference: [Schoppers92] <author> M. Schoppers, </author> <title> Building Plans to Monitor and Expoint Open-Loop and Closed-Loop Dynamics, </title> <booktitle> Proceedings of the First International Conference on Artificial Intelligence Planning Systems, </booktitle> <address> College Park, Maryland, </address> <month> June </month> <year> 1992, </year> <pages> pp. 204-213. </pages>
Reference-contexts: 1 Introduction There is a wide gulf between general approaches and effective approaches to problem solving. Practical success has come from custom techniques like expert systems, reactive planners <ref> [Miller92, Schoppers92] </ref>, or other application specific techniques that require extensive human investment to complete. AI researchers have also developed domain-independent algorithms such as non-linear planning and constraint satisfaction algorithms. Unfortunately, when general approaches show success, it is usually only after extensive domain-specific adjustments.
Reference: [Schwuttke92] <author> U. M. Schwuttke and L. Gasser, </author> <title> Real-time Metareasoning with Dynamic Trade-off Evaluation, </title> <booktitle> Proceedings of the National Conference on Artificial Intelligence, </booktitle> <address> San Jose, CA, </address> <month> July </month> <year> 1992, </year> <pages> pp. 500-506. </pages>
Reference-contexts: Decision theory provides a widely accepted framework for characterizing decision making under uncertainty (see [Berger80]). Jon Doyle has persuasively argued for the merits of decision-theory as a standard for evaluating artificial intelligence systems [Doyle90] and it has seen increasing acceptance, both in artificial intelligence at large <ref> [Horvitz89, Russell89, Schwuttke92, Wellman92] </ref> and machine learning in particular [Gratch92a, Greiner92a, Laird92, Subramanian92]. We will use decision theory as a common framework for characterizing the utility problem. 5 2.1 EXPECTED UTILITY Different learning decisions result in different transformed problem solvers.
Reference: [Subramanian90] <author> D. Subramanian and R. Feldman, </author> <title> The Utility of EBL in Recursive Domain Theories, </title> <booktitle> Proceedings of the National Conference on Artificial Intelligence, </booktitle> <address> Boston, MA, </address> <month> August </month> <year> 1990, </year> <pages> pp. 942-949. </pages>
Reference-contexts: The issue of interactions between transformations is also not addressed. STATIC applies this criterion to control rules but the issue is important in macro-operators as well <ref> [Letovsky90, Subramanian90] </ref>. STATIC out-performs PRODIGY/EBL's on several domains. The nonrecursive hypothesis is cited as a principal reason for this success [Etzioni90b] 7 . This claim is difficult to evaluate as these algorithms use different vocabularies to construct their control rules. <p> Learning cost can be dramatically reduced if there is a detailed cost model of the problem solver that efficiently derives the ramification of proposed transformations without actually solving the problem (e.g. <ref> [Greiner89, Subramanian90] </ref>). With such a model we could simply draw a random training example and then use the model to determine the effect of different transformations. Such models are rarely available.
Reference: [Subramanian92] <author> D. Subramanian and S. Hunter, </author> <title> Measuring Utility and the Design of Provably Good EBL Algorithms, </title> <booktitle> Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <address> Aberdeen, Scotland, </address> <month> July </month> <year> 1992, </year> <pages> pp. 426-435. </pages>
Reference-contexts: Jon Doyle has persuasively argued for the merits of decision-theory as a standard for evaluating artificial intelligence systems [Doyle90] and it has seen increasing acceptance, both in artificial intelligence at large [Horvitz89, Russell89, Schwuttke92, Wellman92] and machine learning in particular <ref> [Gratch92a, Greiner92a, Laird92, Subramanian92] </ref>. We will use decision theory as a common framework for characterizing the utility problem. 5 2.1 EXPECTED UTILITY Different learning decisions result in different transformed problem solvers. <p> As an extreme example, if two identical transformations are provided to COMPOSER, the algorithm will maintain twice the statistics necessary. Sometimes it may be possible to develop a single statistical model from which one can derive the incremental utility of multiple transformations. This is the approach taken by <ref> [Subramanian92] </ref> and [Laird92]. While not always possible, this is an important area of future research. Several statistical methods can be applied to further improve the efficiency of the estimation process. For example, currently a transformation is eliminated only if significantly worse than the default con 32 trol strategy.
Reference: [Tadepalli91] <author> P. Tadepalli, </author> <title> Learning with Inscrutable Theories, </title> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <address> Evanston, IL, </address> <month> June </month> <year> 1991, </year> <pages> pp. 544-548. </pages>
Reference-contexts: Other authors have suggested that it may be possible to gain useful information about currently intractable problems by first learning from simpler problems (e.g. [Natarajan89]) or by observing a teacher (e.g. <ref> [Tadepalli91] </ref>). 7 Conclusion This article has argued that it is desirable, and possible, to construct general problem solving techniques that automatically adapt to the characteristics of a specific application. Adaptive problem 33 solving is a means of reconciling two seemingly contradictory needs.
Reference: [Wellman92] <author> M. P. Wellman and J. Doyle, </author> <title> Modular Utility Representation for Decision-Theoretic Planning, </title> <booktitle> Proceedings of the First International Conference on Artificial Intelligence Planning Systems, </booktitle> <address> College Park, Maryland, </address> <month> June </month> <year> 1992, </year> <pages> pp. 236-242. </pages>
Reference-contexts: Decision theory provides a widely accepted framework for characterizing decision making under uncertainty (see [Berger80]). Jon Doyle has persuasively argued for the merits of decision-theory as a standard for evaluating artificial intelligence systems [Doyle90] and it has seen increasing acceptance, both in artificial intelligence at large <ref> [Horvitz89, Russell89, Schwuttke92, Wellman92] </ref> and machine learning in particular [Gratch92a, Greiner92a, Laird92, Subramanian92]. We will use decision theory as a common framework for characterizing the utility problem. 5 2.1 EXPECTED UTILITY Different learning decisions result in different transformed problem solvers.
References-found: 63

