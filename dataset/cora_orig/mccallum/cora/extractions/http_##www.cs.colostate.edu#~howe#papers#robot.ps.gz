URL: http://www.cs.colostate.edu/~howe/papers/robot.ps.gz
Refering-URL: http://www.cs.colostate.edu/~howe/students.html
Root-URL: 
Email: email: fpyeatt,howeg@cs.colostate.edu  
Phone: tele: 1-970-491-7589 fax: 1-970-491-2466  
Title: Coordinating Reactive Behaviors  keywords: reactive systems, planning and learning  
Author: Larry D. Pyeatt Adele E. Howe 
Note: This research was supported in payment by ARPA-AFOSR contract F30602-93-C-0100 and by NSF Research Initiation Award #RIA IRI-930857. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright notation herein.  
Web: URL: http://www.cs.colostate.edu/~fpyeatt,howeg  
Address: Fort Collins, CO 80523  
Affiliation: Computer Science Department Colorado State University  
Abstract: Combinating reactivity with planning has been proposed as a means of compensating for potentially slow response times of planners while still making progress toward long term goals. The demands of rapid response and the complexity of many environments make it difficult to decompose, tune and coordinate reactive behaviors while ensuring consistency. Neural networks can address the tuning problem, but are less useful for decomposition and coordination. We hypothesize that interacting reactions can be decomposed into separate behaviors resident in separate networks and that the interaction can be coordinated through the tuning mechanism and a higher level controller. To explore these issues, we have implemented a neural network architecture as the reactive component of a two layer control system for a simulated race car. By varying the architecture, we test whether decomposing reactivity into separate behaviors leads to superior overall performance, coordination and learning convergence. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Charles W. Anderson. </author> <title> Strategy learning with multilayer connectionist representations. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <pages> pages 103-114, </pages> <year> 1989. </year>
Reference-contexts: Our basic agent architecture consists of one or two low level behaviors and a higher level coordinating mechanism. Each low level behavior was implemented as a reinforcement learning network similar to those used by Barto, Sutton and Watkins [2], Anderson <ref> [1] </ref> and Lin [13]. The higher level coordinating mechanism is a simple set of heuristics that are responsible for ensuring that the car moves around the track. <p> The possible steering commands are to steer left by 0.01 radians, steer straight ahead, or steer right by 0.01 radians. Limiting the options to three possible actions is similar to the bang-bang control strategy used by Anderson <ref> [1] </ref> for controlling an inverted pendulum. We tested three alternative network architectures that were derived from the reinforcement learning network of Figure 3. The three architectures are characterized by the number of utility and action networks that they have.
Reference: [2] <author> A.G. Barto, R.S. Sutton, and C.J.C.H. Watkins. </author> <title> Learning and sequential decision making. </title> <type> Technical report, COINS Technical Report 89-95, </type> <institution> Dept. of Computer and Information Science, University of Massachusetts, </institution> <year> 1989. </year>
Reference-contexts: Our basic agent architecture consists of one or two low level behaviors and a higher level coordinating mechanism. Each low level behavior was implemented as a reinforcement learning network similar to those used by Barto, Sutton and Watkins <ref> [2] </ref>, Anderson [1] and Lin [13]. The higher level coordinating mechanism is a simple set of heuristics that are responsible for ensuring that the car moves around the track.
Reference: [3] <author> Daniel Bullock, Stephen Grossberg, and Frank H. Guenther. </author> <title> A self-organizing neural network model for redundant sensory-motor control, motor equivalence, and tool use. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <pages> pages 91-96, </pages> <address> Baltimore, 1992. </address> <publisher> IEEE. </publisher>
Reference-contexts: The adaptability and noise rejection of a neural based robot controller was studied by Poo [16], who found that the neural based controller performed better than a standard model based control algorithm. Bullock <ref> [3] </ref> demonstrated a neural network system for positioning a robotic manipulator with various tools attached and under various hardware failure conditions. Mixed systems typically use neural networks for low level control and symbolic planning for high level coordination.
Reference: [4] <author> R. F. Comoglio and A. S. Pandya. </author> <title> Using a cerebellar model arithmetic computer (cmac) neural network to control an autonomous underwater vehicle. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <pages> pages 781-786, </pages> <address> Baltimore, 1992. </address> <publisher> IEEE. </publisher>
Reference-contexts: Purely sub-symbolic, neural network approaches have been used for control of autonomous robots for several years. Krishnaswamy [7] demonstrated a structured neural network approach to controlling a robotic manipulator. Liu [14] used neural networks to control grasping of a robotic hand. Comoglio <ref> [4] </ref> presented results of simulations for the control of an autonomous underwater vehicle using neural networks. The adaptability and noise rejection of a neural based robot controller was studied by Poo [16], who found that the neural based controller performed better than a standard model based control algorithm.
Reference: [5] <author> David S. Day. </author> <title> Integrating reaction and reflection in an autonomous agent: An empirical study. </title> <year> 1990. </year>
Reference-contexts: At present, this mechanism is crude but it serves as a placeholder in the architecture for the later addition of a higher level planning component. 2 Integrating Reactive Behaviors A variety of approaches have been adopted to coordinate reactivity: purely symbolic, purely sub-symbolic or mixed. Cypress [21], plastyc <ref> [5] </ref> and Phoenix [10] are examples of purely symbolic approaches to two level control of reactivity. Cypress implements a model in which the planner is responsible for only high level operation.
Reference: [6] <editor> Marco Dorigo and Marco Colombetti. </editor> <title> Robot shaping: developing autonomous agents through learning. </title> <journal> Artificial Intelligence, </journal> <volume> 71(2) </volume> <pages> 321-370, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: In response to concerns about realism, simulators are becoming increasingly detailed and sophisticated, such as the one described by Feiten [8] that attempts to accurately model robotic sensors and effectors. Dorigo <ref> [6] </ref> showed that reactive behaviors could successfully be transferred from a simulator to a physical robot. As an initial testbed, our research uses the Robot Automobile Racing Simulator (RARS) [20]. This simulator is designed to allow different agents to compete in auto 2 mobile races.
Reference: [7] <author> Gita Drishnaswamy, Marcelo H. Ang Jr., and Gerry B. Andeen. </author> <title> Structured neural-network approach to robot motion control. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <pages> pages 1059-1066. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: Phoenix agents incorporate reactions, called reflexes, which change effector settings in response to sensory events; reflexes are activated and deactivated by the planner in the context of plan actions. Purely sub-symbolic, neural network approaches have been used for control of autonomous robots for several years. Krishnaswamy <ref> [7] </ref> demonstrated a structured neural network approach to controlling a robotic manipulator. Liu [14] used neural networks to control grasping of a robotic hand. Comoglio [4] presented results of simulations for the control of an autonomous underwater vehicle using neural networks.
Reference: [8] <author> W. Feiten, U. Wienkop, A. Huster, and G. Lawitsky. </author> <title> Simulation in the design of an autonomous mobile robot. </title> <editor> In R. Trappl, editor, </editor> <booktitle> Cybernetics and Systems '94, </booktitle> <pages> pages 1499-1506. </pages> <publisher> World Scientific Publishing, </publisher> <address> Singapore, </address> <year> 1994. </year>
Reference-contexts: In response to concerns about realism, simulators are becoming increasingly detailed and sophisticated, such as the one described by Feiten <ref> [8] </ref> that attempts to accurately model robotic sensors and effectors. Dorigo [6] showed that reactive behaviors could successfully be transferred from a simulator to a physical robot. As an initial testbed, our research uses the Robot Automobile Racing Simulator (RARS) [20].
Reference: [9] <author> D.A. Handelman, S.H. Lane, and J.J. Gelfand. </author> <title> Integrating knowledge-based system and neural network techniques for robotic skill acquisition. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (IJCAI-89), </booktitle> <pages> pages 193-198, </pages> <address> Los Altos, CA, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Bullock [3] demonstrated a neural network system for positioning a robotic manipulator with various tools attached and under various hardware failure conditions. Mixed systems typically use neural networks for low level control and symbolic planning for high level coordination. For example, Handelman <ref> [9] </ref> used a rule based system to train a neural network and to control a robotic system during training. Shavlik [17] proposed a method for encoding symbolic processing in a neural network structure. In Knick's system [11], a mobile robot is equipped with both symbolic and sub-symbolic processing.
Reference: [10] <author> Adele E. Howe and Paul R. Cohen. </author> <title> Responding to environmental change. </title> <editor> In Katia P. Sycara, editor, </editor> <booktitle> Proceedings of the Workshop on Innovative Approaches to Planning, Scheduling and Control, </booktitle> <pages> pages 85-92. </pages> <publisher> Morgan Kaufmann Publishers, Inc, </publisher> <month> November </month> <year> 1990. </year>
Reference-contexts: Cypress [21], plastyc [5] and Phoenix <ref> [10] </ref> are examples of purely symbolic approaches to two level control of reactivity. Cypress implements a model in which the planner is responsible for only high level operation.
Reference: [11] <author> M. Knick and F.J. Radermacher. </author> <title> Integration of sub-symbolic and symbolic infor-mation processing in robot control. </title> <booktitle> In Proceedings of Third Annual Conference on AI, Simulation and Planning in High Autonomy Systems, </booktitle> <pages> pages 238-243, </pages> <address> Perth, Western Australia, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: For example, Handelman [9] used a rule based system to train a neural network and to control a robotic system during training. Shavlik [17] proposed a method for encoding symbolic processing in a neural network structure. In Knick's system <ref> [11] </ref>, a mobile robot is equipped with both symbolic and sub-symbolic processing. The sub-symbolic system learns and translates collections of information into new concepts that are integrated into the world model of the symbolic system, thereby allowing the robot to learn about its world and improve its performance over time.
Reference: [12] <author> Kurt Konolige. </author> <title> Erratic robot simulator. </title> <note> Anonymous ftp from ftp.ai.sri.com: /pub/konolige/erratic-ver2b.tar.Z, </note> <year> 1994. </year>
Reference-contexts: Our future work will include replacing the heuristic passing behavior with reinforcement learning, adding a pit-stop behavior, and improving the high level control function to select the appropriate reactive behavior. We will also be experimenting with other simulation environments, in particular a modified version of the SRI Erratic simulator <ref> [12] </ref>. The Erratic simulator is a real-time simulator for the SRI Flakey robot. It includes a detailed simulation of the servos and sensors available on that robot.
Reference: [13] <author> Long-H Lin. </author> <title> Self-improving reactive agents based on reinforcement learning, planning and teaching. </title> <journal> Machine Learning, </journal> 8(3/4):69-97, 1992. 
Reference-contexts: Our basic agent architecture consists of one or two low level behaviors and a higher level coordinating mechanism. Each low level behavior was implemented as a reinforcement learning network similar to those used by Barto, Sutton and Watkins [2], Anderson [1] and Lin <ref> [13] </ref>. The higher level coordinating mechanism is a simple set of heuristics that are responsible for ensuring that the car moves around the track. If the car leaves 4 the track, the low level behaviors are disabled temporarily while a heuristic control strategy pilots the car back onto the track. <p> The output of the utility network trains the lower network (called the action network), which learns to select the actions that lead to world states with higher utility. The stochastic action selector is a mechanism that forces the action network to explore 5 the space of possible actions <ref> [13] </ref> by occasionally choosing an action that is not the one selected by the action network. Without stochastic action selection, the network is more likely to learn a less general control strategy. The utility network uses temporal difference methods, which are the standard backpropagation learning algorithm with one modification.
Reference: [14] <author> Huan Liu, Thea Iberall, and George A. Beckey. </author> <title> Neural network architecture for robot hand control. </title> <booktitle> In Proceedings of IEEE International Conference on Neural Networks. IEEE, </booktitle> <month> July </month> <year> 1989. </year>
Reference-contexts: Purely sub-symbolic, neural network approaches have been used for control of autonomous robots for several years. Krishnaswamy [7] demonstrated a structured neural network approach to controlling a robotic manipulator. Liu <ref> [14] </ref> used neural networks to control grasping of a robotic hand. Comoglio [4] presented results of simulations for the control of an autonomous underwater vehicle using neural networks.
Reference: [15] <author> Min Meng and A.C. Kak. </author> <title> Mobile robot navigation using neural networks and nonmetrical environment models. </title> <journal> IEEE Control Systems, </journal> <pages> pages 30-39, </pages> <month> October </month> <year> 1993. </year>
Reference: [16] <author> A.N. Poo, M.H. Ang Jr., C.L. Teo, and Qing Li. </author> <title> Performance of a neuro-model-based robot controller: adaptability and noise rejection. </title> <journal> Intelligent Systems Engineering, </journal> <volume> 1(1) </volume> <pages> 50-62, </pages> <year> 1992. </year>
Reference-contexts: Liu [14] used neural networks to control grasping of a robotic hand. Comoglio [4] presented results of simulations for the control of an autonomous underwater vehicle using neural networks. The adaptability and noise rejection of a neural based robot controller was studied by Poo <ref> [16] </ref>, who found that the neural based controller performed better than a standard model based control algorithm. Bullock [3] demonstrated a neural network system for positioning a robotic manipulator with various tools attached and under various hardware failure conditions.
Reference: [17] <author> Jude W. Shavlik. </author> <title> Combining symbolic and neural learning. </title> <journal> Machine Learning, </journal> <volume> 14(3) </volume> <pages> 321-331, </pages> <year> 1994. </year>
Reference-contexts: Mixed systems typically use neural networks for low level control and symbolic planning for high level coordination. For example, Handelman [9] used a rule based system to train a neural network and to control a robotic system during training. Shavlik <ref> [17] </ref> proposed a method for encoding symbolic processing in a neural network structure. In Knick's system [11], a mobile robot is equipped with both symbolic and sub-symbolic processing.
Reference: [18] <author> Richard S. Sutton. </author> <title> Temporal Credit Assignment in Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> Dept. of Computer and Information Science, University of Massachusetts, </institution> <year> 1984. </year>
Reference-contexts: To accomplish the two subtasks, a reinforcement learning network uses two feed-forward neural networks with the backpropagation learning algorithm and a stochastic action selector (see Figure 3). The upper network (called the utility network) learns to predict the utility of each world state, performing temporal credit assignment <ref> [18] </ref> by using temporal difference methods [19]. The output of the utility network trains the lower network (called the action network), which learns to select the actions that lead to world states with higher utility.
Reference: [19] <author> Richard S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: The upper network (called the utility network) learns to predict the utility of each world state, performing temporal credit assignment [18] by using temporal difference methods <ref> [19] </ref>. The output of the utility network trains the lower network (called the action network), which learns to select the actions that lead to world states with higher utility.
Reference: [20] <author> Mitchell E. Timin. RARS. </author> <note> Anonymous ftp from ftp.ijs.com:/rars, </note> <year> 1995. </year>
Reference-contexts: To explore these questions, we have implemented and tested three alternative two layer control systems for a simulated robot race car (the RARS simulator <ref> [20] </ref>). The reactive component, which controls steering and acceleration, is implemented as a set of neural networks. Each neural network uses a form of reinforcement learning to implement a single behavior, although behavior is defined differently in each alternative design. <p> Dorigo [6] showed that reactive behaviors could successfully be transferred from a simulator to a physical robot. As an initial testbed, our research uses the Robot Automobile Racing Simulator (RARS) <ref> [20] </ref>. This simulator is designed to allow different agents to compete in auto 2 mobile races. This simulator is complex enough to be interesting, while simple enough that all the variables can be examined and/or controlled.
Reference: [21] <author> David E. Wilkins, Karen L. Myers, John D. Lowrance, and Leonard P. Wesley. </author> <title> Planning and reacting in uncertain and dynamic environments. </title> <journal> Journal of Experimental and Theoretical AI, </journal> <volume> 7, </volume> <year> 1994. </year> <month> 14 </month>
Reference-contexts: At present, this mechanism is crude but it serves as a placeholder in the architecture for the later addition of a higher level planning component. 2 Integrating Reactive Behaviors A variety of approaches have been adopted to coordinate reactivity: purely symbolic, purely sub-symbolic or mixed. Cypress <ref> [21] </ref>, plastyc [5] and Phoenix [10] are examples of purely symbolic approaches to two level control of reactivity. Cypress implements a model in which the planner is responsible for only high level operation.
References-found: 21

