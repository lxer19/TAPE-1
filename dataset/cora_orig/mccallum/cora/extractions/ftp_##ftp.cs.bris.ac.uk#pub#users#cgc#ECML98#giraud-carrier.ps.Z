URL: ftp://ftp.cs.bris.ac.uk/pub/users/cgc/ECML98/giraud-carrier.ps.Z
Refering-URL: http://www.cs.bris.ac.uk/~cgc/ECML98-WS/Summary.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: cgc@cs.bris.ac.uk  
Title: Beyond predictive accuracy: what?  
Author: Christophe Giraud-Carrier 
Address: Merchant Venturers Building Bristol BS8 1UB, UK  
Affiliation: Department of Computer Science University of Bristol  
Abstract: Today's potential users of machine learning technology are faced with the non-trivial problem of choosing, from the large, ever-increasing number of available tools, the one most appropriate for their particular task. To assist the often non-initiated users, it is desirable that this model selection process be automated. Using experience from base level learning, researchers have proposed meta-learning as a possible solution. Historically, predictive accuracy has been the de facto criterion, with most work in meta-learning focusing on the discovery of rules that match applications to models based on accuracy only. Although predictive accuracy is clearly an important criterion, it is also the case that there are a number of other criteria that could, and often ought to, be considered when learning about model selection. This paper presents a number of such criteria and discusses the impact they have on meta-level approaches to model selection.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P.W. </author> <title> Adriaans (1997). Industrial Requirements for ML Application Technology. </title> <booktitle> Proc. of the ICML'97 Workshop on Machine Learning Application in the Real World: Methodological Aspects and Implications, </booktitle> <pages> 6-10. </pages>
Reference-contexts: For real-time, on-line learning, computational complexity becomes a key issue. There is often a trade-off between efficiency in learning and efficiency in predicting/classifying, as it has been argued that algorithms that learn quickly tend to execute slowly and vice versa <ref> [1] </ref>. 2.4 Expressiveness Expressiveness has to do with representation languages, both for the examples and for the induced generalisations/hypotheses. Most classical learning systems are attribute-based. The examples are tuples of constants and the generalisations take the form of simple if-then rules or decision trees.
Reference: [2] <author> D.W. Aha, D. Kibler and M.K. </author> <title> Albert (1991). Instance-Based Learning Algorithms. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 37-66. </pages>
Reference-contexts: Some learning models, such as ID3 [16] have a built-in preference for more compact (or simpler) generalisations, usually relying on the Occam Razor Principle, whilst others, such as IBL <ref> [2] </ref> place their emphasis on other aspects of learning (e.g., similarity-based generalisations). 2.6 Comprehensibility Comprehensibility is concerned with the understandability (by a human) of the knowledge/hypotheses generated by the system. It is related to compactness and expressiveness.
Reference: [3] <author> A. Andersson, P. Davidsson and J. </author> <title> Linden (1998). Model selection using measure functions. </title> <booktitle> Proc. of the ECML'98 Workshop on Upgrading Learning to the Meta-Level: Model Selection and Data Transformation, </booktitle> <pages> 54-65. </pages>
Reference-contexts: The work presented in <ref> [3] </ref> offers a promising starting point.
Reference: [4] <author> P. Brazdil and B. </author> <month> Henery </month> <year> (1994). </year> <title> Analysis of Results. Machine Learning, Neural and Statistical Classification, </title> <editor> D. Michie et al (Eds.), </editor> <booktitle> Chapter 10, </booktitle> <publisher> Ellis Horwood. </publisher>
Reference-contexts: However, the question leaves open the issues of what is meant by superior and what criteria can be used to compare learning models/methods. Since performance is generally paramount, predictive accuracy has become the de facto criterion for model selection (e.g., see <ref> [4, 6, 8] </ref>). That is, meta-learning has focused on the induction of rules that match applications to learning models using accuracy only. These rules are then used to choose, for a given application, the learning model that will produce a hypothesis with the highest predictive accuracy.
Reference: [5] <author> P. </author> <title> Brazdil (1998). Data transformation and model selection by experimentation and meta-learning. Proc. of the ECML'98 Workshop on Upgrading Learning to the Meta-Level: Model Selection and Data Transformation, </title> <type> 11-17. 84 </type>
Reference-contexts: As suggested in [14] for the problem of feature engineering and data preparation, careful examination of successful experiments may highlight general principles and heuristics useful to the automation of range selection. This is also consistent with the discussion of meta-knowledge acquisition by experimentation in <ref> [5] </ref>.
Reference: [6] <author> P. Chan and S. </author> <title> Stolfo (1996). On the Accuracy of Meta-Learning for Salable Data Mining. </title> <journal> Journal of Intelligent Information Systems, </journal> <volume> 8 </volume> <pages> 3-28. </pages>
Reference-contexts: However, the question leaves open the issues of what is meant by superior and what criteria can be used to compare learning models/methods. Since performance is generally paramount, predictive accuracy has become the de facto criterion for model selection (e.g., see <ref> [4, 6, 8] </ref>). That is, meta-learning has focused on the induction of rules that match applications to learning models using accuracy only. These rules are then used to choose, for a given application, the learning model that will produce a hypothesis with the highest predictive accuracy.
Reference: [7] <author> T. </author> <title> Dietterich (1989). Limitations of Inductive Learning. </title> <booktitle> Proc. of the Sixth International Workshop on Machine Learning (IWML'89), </booktitle> <pages> 124-128. </pages>
Reference-contexts: These rules are then used to choose, for a given application, the learning model that will produce a hypothesis with the highest predictive accuracy. This bias on predictive accuracy in meta-learning is largely justified by both theoretical results and pragmatic considerations, including: * The Law of Conservation of Generalisation <ref> [7, 17] </ref>, also known as the No Free Lunch Theorem, suggests that no single learning model will construct hypotheses of high accuracy on all applications. In other words, if a learning algorithm performs well on a particular task, there exists another task on which it performs poorly.
Reference: [8] <author> J. Gama and P. </author> <title> Brazdil (1995). Chracterization of Classification Algorithms. </title> <booktitle> Proc. of the Seventh Portuguese Conference on Artificial Intelligence (EPIA'95). </booktitle>
Reference-contexts: However, the question leaves open the issues of what is meant by superior and what criteria can be used to compare learning models/methods. Since performance is generally paramount, predictive accuracy has become the de facto criterion for model selection (e.g., see <ref> [4, 6, 8] </ref>). That is, meta-learning has focused on the induction of rules that match applications to learning models using accuracy only. These rules are then used to choose, for a given application, the learning model that will produce a hypothesis with the highest predictive accuracy.
Reference: [9] <author> C. Giraud-Carrier and T. </author> <title> Martinez (1993). Using Precepts to Augment Training Set Learning. </title> <booktitle> Proc. of the First New Zealand International Two-Stream Conference on Artificial Neural Networks and Expert Systems (ANNES'93), </booktitle> <pages> 46-51. </pages>
Reference-contexts: Generally, prior knowledge takes the form of pre-encoded rules or background theories. The use of prior knowledge speeds up learning, reduces the size of the knowledge base or hypotheses, limits the impact of poor or atypical learning environments on inductive processes and increases predictive accuracy (e.g., see <ref> [9, 10] </ref>). Prior knowledge requires the ability to reason or perform deduction. In addition to common, implicit biases (e.g., language, architecture), it is generally considered desirable for a system to be able to make use of explicit prior knowledge.
Reference: [10] <author> C. </author> <month> Giraud-Carrier </month> <year> (1996). </year> <title> FLARE: Induction with Prior Knowledge. </title> <booktitle> Research and Development in Expert Systems XIII (Proc. of ES'96), </booktitle> <editor> J.L. Nealon and J. Hunt (Eds.), </editor> <publisher> SGES Publications. </publisher>
Reference-contexts: Generally, prior knowledge takes the form of pre-encoded rules or background theories. The use of prior knowledge speeds up learning, reduces the size of the knowledge base or hypotheses, limits the impact of poor or atypical learning environments on inductive processes and increases predictive accuracy (e.g., see <ref> [9, 10] </ref>). Prior knowledge requires the ability to reason or perform deduction. In addition to common, implicit biases (e.g., language, architecture), it is generally considered desirable for a system to be able to make use of explicit prior knowledge.
Reference: [11] <author> C. </author> <month> Giraud-Carrier </month> <year> (1998). </year> <title> Pre-pruning Decision Tree Induction. </title> <note> In preparation. </note>
Reference-contexts: Yet they often exhibit extreme variance along other interesting dimensions. For example, some of our recent experiments with top-down induction of decision trees show significant differences in terms of compactness (section 2) with no significant difference in predictive accuracy <ref> [11] </ref>. Furthermore, this (ab)use of accuracy is a result of two of machine learning/data mining's strongest assumptions, namely that data adequately represents what it models and that predicting data values equates to predicting useful business outcomes [15].
Reference: [12] <author> R.C. </author> <title> Holte (1993). Very Simple Classification Rules Perform Well on Most Commonly Used Datasets. </title> <journal> Machine Learning, </journal> <volume> 11 </volume> <pages> 63-91. </pages>
Reference-contexts: In fact, empirical evidence suggests that for large classes of applications, most learning models perform well in terms of accuracy <ref> [12] </ref>. Yet they often exhibit extreme variance along other interesting dimensions. For example, some of our recent experiments with top-down induction of decision trees show significant differences in terms of compactness (section 2) with no significant difference in predictive accuracy [11].
Reference: [13] <author> P. Langley and H.A. </author> <title> Simon (1995). Applications of Machine Learning and Rule Induction. </title> <journal> Communications of the ACM, </journal> <volume> 38(11) </volume> <pages> 55-64. </pages>
Reference-contexts: 1 Introduction Over the past decade, machine learning (ML) techniques have successfully started the transition from research laboratories to the real world. The number of fielded applications has grown steadily, evidence that industry needs and uses ML technology <ref> [13] </ref>. However, the large, ever-increasing number of available ML models makes it difficult for the non-initiated users to access the much needed technology directly.
Reference: [14] <author> P. </author> <title> Langley (1997). Challenges for the Application of Machine Learning. </title> <booktitle> Proc. of the ICML'97 Workshop on Machine Learning Application in the Real World: Methodological Aspects and Implications, </booktitle> <pages> 15-18. </pages>
Reference-contexts: Generally, the range selection would be obtained from the more qualitative criteria, possibly using meta-knowledge encoded in an expert system, whilst the final model selection would be had from rules induced by meta-learning. As suggested in <ref> [14] </ref> for the problem of feature engineering and data preparation, careful examination of successful experiments may highlight general principles and heuristics useful to the automation of range selection. This is also consistent with the discussion of meta-knowledge acquisition by experimentation in [5].
Reference: [15] <author> A. </author> <title> Montgomery (1998). Data Mining Business Hunching, Not Just Data Crunching. </title> <booktitle> Proc. of the Second International Conference on the Application of Knowledge Discovery and Data Mining (PADD'98), </booktitle> <pages> 39-48. </pages>
Reference-contexts: Furthermore, this (ab)use of accuracy is a result of two of machine learning/data mining's strongest assumptions, namely that data adequately represents what it models and that predicting data values equates to predicting useful business outcomes <ref> [15] </ref>. Again, evidence suggests that this assumption may not always hold and other factors should be considered. This position paper presents a number of criteria relevant to model selection and discusses the impact they have on current meta-level approaches to model selection. <p> It has been argued that current approaches to the application of machine learning and data mining techniques are smart data-wise, but quite dumb business-wise <ref> [15] </ref>. The selection of a learning model, as well as necessary data transformations, must ultimately be guided by the objectives of the business. Finding patterns in data is not an end in itself, but a means to an end. <p> Finding patterns in data is not an end in itself, but a means to an end. The end is to improve business and therefore, the outcome of the learning process should be actionable knowledge <ref> [15] </ref>. This means that one of the prerequisites to successful ML application is business sense. In some cases, common sense is sufficient. <p> In other cases, expert business insight is warranted. For example, it may be that one of the key predictor is not provided but must be derived (e.g., a sum such as household income, a trend such as 82 rate of sales decrease) <ref> [15] </ref> or that predictions based on past experience are unlikely to be directly useful in the future (e.g., stock market trends). 3 Discussion The list of criteria in section 2 clearly highlights the difficulty in automating model selection.
Reference: [16] <author> J.R. </author> <title> Quinlan (1986). Inductive Learning of Decision Trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106. </pages>
Reference-contexts: Regardless of the representation used, it is generally accepted that the system's encoding of knowledge should be more compact than the training set used to derive it. Some learning models, such as ID3 <ref> [16] </ref> have a built-in preference for more compact (or simpler) generalisations, usually relying on the Occam Razor Principle, whilst others, such as IBL [2] place their emphasis on other aspects of learning (e.g., similarity-based generalisations). 2.6 Comprehensibility Comprehensibility is concerned with the understandability (by a human) of the knowledge/hypotheses generated by
Reference: [17] <author> C. </author> <title> Schaffer (1994). A Conservation Law for Generalization Performance. </title> <booktitle> Proc. of the Eleventh International Conference on Machine Learning (ICML'94), </booktitle> <pages> 259-265. 85 </pages>
Reference-contexts: These rules are then used to choose, for a given application, the learning model that will produce a hypothesis with the highest predictive accuracy. This bias on predictive accuracy in meta-learning is largely justified by both theoretical results and pragmatic considerations, including: * The Law of Conservation of Generalisation <ref> [7, 17] </ref>, also known as the No Free Lunch Theorem, suggests that no single learning model will construct hypotheses of high accuracy on all applications. In other words, if a learning algorithm performs well on a particular task, there exists another task on which it performs poorly.
References-found: 17

