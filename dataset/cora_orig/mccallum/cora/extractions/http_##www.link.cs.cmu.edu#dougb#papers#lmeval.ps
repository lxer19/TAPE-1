URL: http://www.link.cs.cmu.edu/dougb/papers/lmeval.ps
Refering-URL: http://www.cs.cmu.edu/People/dougb/research.html
Root-URL: 
Email: fsfc,dougb,ronig@cs.cmu.edu  
Title: EVALUATION METRICS FOR LANGUAGE MODELS  
Author: Stanley Chen, Douglas Beeferman, Ronald Rosenfeld 
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: The most widely-used evaluation metric for language models for speech recognition is the perplexity of test data. While perplexities can be calculated efficiently and without access to a speech recognizer, they often do not correlate well with speech recognition word-error rates. In this research, we attempt to find a measure that like perplexity is easily calculated but which better predicts speech recognition performance. We investigate two approaches; first, we attempt to extend perplexity by using similar measures that utilize information about language models that perplexity ignores. Second, we attempt to imitate the word-error calculation without using a speech recognizer by artificially generating speech recognition lattices. To test our new metrics, we have built over thirty varied language models. We find that perplexity correlates with word-error rate remarkably well when only considering n-gram models trained on in-domain data. When considering other types of models, our novel metrics are superior to perplexity for predicting speech recognition performance. However, we conclude that none of these measures predict word-error rate sufficiently accurately to be effective tools for language model evaluation in speech recognition. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> S.C. Martin, J. Liermann, and H. Ney. </author> <title> Adaptive topic-dependent language modelling using word-based varigrams. </title> <booktitle> In Proceedings of Eurospeech '97, </booktitle> <year> 1997. </year>
Reference-contexts: Unfortunately, while language models with lower perplexities tend to have lower word-error rates, there have been numerous examples in the literature where language models providing a large improvement in perplexity over a baseline model have yielded little or no improvement in word-error rate <ref> [1, 2] </ref>. In addition, perplexity is inapplicable to unnormalized language models (i.e., models that are not true probability distributions that sum to 1), and perplexity is not comparable between language models with different vocabularies.
Reference: 2. <author> R. Iyer, M. Ostendorf, and M. Meteer. </author> <title> Analyzing and predicting language model improvements. </title> <booktitle> In Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, </booktitle> <year> 1997. </year>
Reference-contexts: Unfortunately, while language models with lower perplexities tend to have lower word-error rates, there have been numerous examples in the literature where language models providing a large improvement in perplexity over a baseline model have yielded little or no improvement in word-error rate <ref> [1, 2] </ref>. In addition, perplexity is inapplicable to unnormalized language models (i.e., models that are not true probability distributions that sum to 1), and perplexity is not comparable between language models with different vocabularies.
Reference: 3. <author> Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jen-nifer C. Lai, and Robert L. Mercer. </author> <title> Class-based n-gram models of natural language. </title> <journal> Computational Linguistics, </journal> <volume> 18(4):467 479, </volume> <month> December </month> <year> 1992. </year>
Reference-contexts: Instead, we artificially generate lattices and evaluate language models through their word-error rates on these artificial lattices. To evaluate our novel language model measures, we have constructed over thirty language models of varying types, including class n-gram <ref> [3, 4] </ref>, trigger [5], and cache [6] language models. We find that perplexity correlates with word-error rate remarkably well when only considering n-gram models trained on in-domain data. When considering other types of models, our novel metrics are superior to perplexity for predicting speech recognition performance.
Reference: 4. <author> Hermann Ney, Ute Essen, and Reinhard Kneser. </author> <title> On structuring probabilistic dependences in stochastic language modeling. </title> <booktitle> Computer, Speech, and Language, </booktitle> <address> 8:138, </address> <year> 1994. </year>
Reference-contexts: Instead, we artificially generate lattices and evaluate language models through their word-error rates on these artificial lattices. To evaluate our novel language model measures, we have constructed over thirty language models of varying types, including class n-gram <ref> [3, 4] </ref>, trigger [5], and cache [6] language models. We find that perplexity correlates with word-error rate remarkably well when only considering n-gram models trained on in-domain data. When considering other types of models, our novel metrics are superior to perplexity for predicting speech recognition performance.
Reference: 5. <author> D. Beeferman, A. Berger, and J. Lafferty. </author> <title> A model of lexical attraction and repulsion. </title> <booktitle> In Proceedings of the ACL, </booktitle> <address> Madrid, Spain, </address> <year> 1997. </year>
Reference-contexts: Instead, we artificially generate lattices and evaluate language models through their word-error rates on these artificial lattices. To evaluate our novel language model measures, we have constructed over thirty language models of varying types, including class n-gram [3, 4], trigger <ref> [5] </ref>, and cache [6] language models. We find that perplexity correlates with word-error rate remarkably well when only considering n-gram models trained on in-domain data. When considering other types of models, our novel metrics are superior to perplexity for predicting speech recognition performance.
Reference: 6. <author> R. Kuhn and R. De Mori. </author> <title> A cache-based natural language model for speech reproduction. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12(6):570583, </volume> <year> 1990. </year>
Reference-contexts: Instead, we artificially generate lattices and evaluate language models through their word-error rates on these artificial lattices. To evaluate our novel language model measures, we have constructed over thirty language models of varying types, including class n-gram [3, 4], trigger [5], and cache <ref> [6] </ref> language models. We find that perplexity correlates with word-error rate remarkably well when only considering n-gram models trained on in-domain data. When considering other types of models, our novel metrics are superior to perplexity for predicting speech recognition performance.
Reference: 7. <author> P. Placeway, S. Chen, M. Eskenazi, U. Jain, V. Parikh, B. Raj, M. Ravishankar, R. Rosenfeld, K. Seymore, M. Siegler, R. Stern, and E. Thayer. </author> <booktitle> The 1996 Hub-4 Sphinx-3 system. In Proceedings of the DARPA Speech Recognition Workshop, </booktitle> <month> February </month> <year> 1997. </year>
Reference-contexts: In this work, we would like to investigate what is possible with measures like perplexity that ignore detailed lexical information. 1.2. Methodology In this research, we investigate speech recognition performance in the Broadcast News domain. We generated narrow-beam lattices with the Sphinx-III recognition system <ref> [7] </ref> using a trigram model trained on 130M words of Broadcast News text; trigrams occurring only once were excluded from the model. The word-error rates reported in this work were calculated by rescoring these lattices with the given language model.
Reference: 8. <author> Reinhard Kneser and Hermann Ney. </author> <title> Improved backing-off for m-gram language modeling. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 181184, </pages> <year> 1995. </year>
Reference: 9. <author> Slava M. Katz. </author> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> ASSP-35(3):400401, </volume> <month> March </month> <year> 1987. </year>
Reference-contexts: To attempt to shed light on why these two apparently unrelated quantities are related, in Figure 2 we graph the relation set A data smooth n (wds) alg 2 5M K-N 4 5M K-N 3 5M Katz <ref> [9] </ref> 3 5M poor 3 10M poor 3 25M poor 3 5M K-N (i) 3 1M K-N 3 130M K-N 2 25M K-N set B n description 2 class n-gram model 3 class n-gram model 4 class n-gram model 3 trigram model + cache 1 3 trigram model + cache 2
Reference: 10. <author> Ivica Rogina and Alex Waibel. </author> <title> The Janus speech recognizer. </title> <booktitle> In ARPA SLT Workshop, </booktitle> <year> 1995. </year>
Reference-contexts: Perplexity is marginally better on set A, but artificial word-error rate is substantially superior on set B, the motley mix of models. We have also performed experiments on the Switchboard task using lattices generated by the Janus speech recognition system <ref> [10] </ref>. models in sets A and B Generating artificial lattices with the values k = 3 and ff = 0:5, we compared the correlation between perplexity and artificial word-error rate with actual word-error rate over nine n-gram models.
References-found: 10

