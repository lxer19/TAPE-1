URL: http://iacoma.cs.uiuc.edu/iacoma-papers/point.ps
Refering-URL: http://iacoma.cs.uiuc.edu/welcome.html
Root-URL: http://www.cs.uiuc.edu
Email: torrella,padua@csrd.uiuc.edu  
Title: The Illinois Aggressive Coma Multiprocessor Project (I-ACOMA) 1  
Author: Josep Torrellas and David Padua 
Web: http://www.csrd.uiuc.edu/iacoma  
Address: IL 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign,  
Abstract: While scalable shared-memory multiprocessors with hardware-assisted cache coherence are relatively easy to program, if truly high-performance is desired, they still require substantial programmer effort. For example, data must be allocated close to the processors that will use them and the application must be tuned so that the working set fits in the caches. This is unfortunate because the most important obstacle to widespread use of parallel computing is the hardship of programming parallel machines. The goal of the I-ACOMA project is to explore how to design a highly programmable high-performance multiprocessor. We focus on a flat-coma scalable multiprocessor supported by a parallelizing compiler. The main issues that we are studying are advanced processor organizations, techniques to handle long memory access latencies, and support for important classes of workloads like databases and scientific applications with loops that cannot be compiler-analyzed. The project also involves building a prototype that includes some of the features discussed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Semiconductor Industry Association. The National Technology Roadmap for Semiconductors. SIA, </institution> <year> 1994. </year>
Reference-contexts: It is currently impossible to integrate in the same chip a processor as complicated as the one used for I-ACOMA with enough DRAM memory to be useful. However, as DRAM technology advances and denser DRAMs become available <ref> [1] </ref>, this will change. Since it is especially interesting to integrate processor and memory in a chip in the context of a coma machine, we are actively evaluating such a configuration for a possible future I-ACOMA machine.
Reference: [2] <author> M. Berry et al. </author> <title> The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: Polaris is a parallelizing compiler recently developed at the University of Illinois [3]. The compiler takes scientific programs written in Fortran and performs transformations to (i) expose parallelism, (ii) enhance locality, and (iii) reduce communication overhead. As a result, several of the applications in the Perfect suite <ref> [2] </ref> are parallelized automatically to a greater degree than by any other translator, experimental or commercial, that has been tested to date. The compiler currently generates parallel code for several shared-memory machines, including the SGI Power Challenge, the Cray T3D, the Convex C-3, the Convex Exemplar, and multiprocessor Sparc workstations.
Reference: [3] <author> W. Blume, R. Eigenmann, K. Faigin, J. Grout, J. Hoe-flinger, D. Padua, P. Petersen, W. Pottenger, L. Rauch-werger, P. Tu, and S. Weatherford. </author> <title> Effective Automatic Parallelization with Polaris. </title> <journal> International Journal of Parallel Programming, </journal> <month> May </month> <year> 1995. </year>
Reference-contexts: For this reason, a sizable effort in the I-ACOMA project is devoted to providing software support for the machine. This support is provided by the Polaris parallelizing compiler. Polaris is a parallelizing compiler recently developed at the University of Illinois <ref> [3] </ref>. The compiler takes scientific programs written in Fortran and performs transformations to (i) expose parallelism, (ii) enhance locality, and (iii) reduce communication overhead.
Reference: [4] <author> D. K. Chen, J. Torrellas, and P. C. Yew. </author> <title> An Efficient Algorithm for the Run-Time Parallelization of Do-Across Loops. </title> <booktitle> In Supercomputing '94, </booktitle> <pages> pages 518-527, </pages> <month> Novem-ber </month> <year> 1994. </year>
Reference-contexts: DO i=1,N A (f (i)) = : : : (S 1 ) : : : = A (g (i)) + : : : (S 2 ) ENDDO be analyzed at compile time. One possibility is to parallelize these loops with costly run-time parallelization algorithms <ref> [4, 10, 12, 13, 16, 17, 19, 28] </ref>. In most of these proposals, the compiler inserts code that, at run time, determines what iterations do not depend on each other (inspector phase) and then executes these iterations in parallel (executor phase).
Reference: [5] <author> E. Hagersten, A. Landin, and S. Haridi. </author> <title> DDM a Cache-Only Memory Architecture. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 44-54, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: The goal of the I-ACOMA project is to explore design alternatives for a highly programmable high-performance multiprocessor. Given the chief importance of programmability, we focus on cache-only memory architectures (COMAs) and their use by a parallelizing compiler. Roughly speaking, comas <ref> [5, 6, 11, 21, 22] </ref> have a more complicated hardware than traditional cache-coherent shared-memory machines, but are easier to program. Indeed, in a coma, data automatically migrates to the local memory of the processor that is using it. Such memory is called attraction memory in coma machines. <p> In fact, ideally, we should be able to take a program written for a bus-based machine, run it on a scalable coma machine without changes in the data layout, and still achieve high performance <ref> [5] </ref>. Similarly, it can be argued that the need to tune the size of an application's working set to the caches of the machine is not as pressing. This is because in coma, even if the caches overflow, the data is still likely to be in the local attraction memory. <p> This is because in coma, even if the caches overflow, the data is still likely to be in the local attraction memory. All these issues make coma architectures more programmable. Traditional coma machines like KSR1 [6] and DDM1 <ref> [5] </ref>, however, had at least one shortcoming. To help a processor find a memory line efficiently, the networks of these machines were organized in a hierarchy. Specifically, KSR1 had a hierarchy of rings while DDM1 had a hierarchy of busses.
Reference: [6] <author> H. Burkhardt III et al. </author> <title> Overview of the KSR1 Computer System. </title> <type> Technical Report 9202001, </type> <institution> Kendall Square Research, </institution> <address> Waltham, MA, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: The goal of the I-ACOMA project is to explore design alternatives for a highly programmable high-performance multiprocessor. Given the chief importance of programmability, we focus on cache-only memory architectures (COMAs) and their use by a parallelizing compiler. Roughly speaking, comas <ref> [5, 6, 11, 21, 22] </ref> have a more complicated hardware than traditional cache-coherent shared-memory machines, but are easier to program. Indeed, in a coma, data automatically migrates to the local memory of the processor that is using it. Such memory is called attraction memory in coma machines. <p> This is because in coma, even if the caches overflow, the data is still likely to be in the local attraction memory. All these issues make coma architectures more programmable. Traditional coma machines like KSR1 <ref> [6] </ref> and DDM1 [5], however, had at least one shortcoming. To help a processor find a memory line efficiently, the networks of these machines were organized in a hierarchy. Specifically, KSR1 had a hierarchy of rings while DDM1 had a hierarchy of busses.
Reference: [7] <institution> Kendall Square Research. </institution> <type> KSR1 Technical Summary. </type> <address> Waltham, MA, </address> <year> 1992. </year>
Reference-contexts: In addition, since the data automatically migrates to where it is being used, the initial placement of the data is largely unimportant. There is no need to try to predict the access patterns at compile time. These arguments, which agree with the thoughts of other researchers <ref> [7, 18] </ref>, need to be tested thoroughly. 3 Compiler Support While the architectural organization of a machine affects how programmable the machine is, the software infrastructure available to write programs for the machine is a key determinant of end-user programming ease.
Reference: [8] <author> P. Kogge. </author> <title> Execube ANew Architecture for Scaleable MPPs. </title> <booktitle> In Proceedings of the 1994 International Conference on Parallel Processing, </booktitle> <pages> pages I77-I84, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: It is therefore important to increase the integration between processor and memory. Ultimately, this integration will lead to putting processor and memory in the same chip <ref> [8, 20] </ref>. It is currently impossible to integrate in the same chip a processor as complicated as the one used for I-ACOMA with enough DRAM memory to be useful. However, as DRAM technology advances and denser DRAMs become available [1], this will change.
Reference: [9] <author> D. Koufaty, X. Chen, D. Poulsen, and J. Torrellas. </author> <title> Data Forwarding in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In IEEE Trans. on Parallel and Distributed Systems, to appear 1996. A shorter version appeared in Proceedings of the 9th International Conference on Supercomputing, </booktitle> <pages> pages 255-264, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: Note that these consumer processors may or may not be current shar--ers. The current sharer processors that are not specified in the forwarding register are invalidated. Write-forwards are used when it is possible to determine with high confidence which processors will consume the write. In previous experiments <ref> [9] </ref>, we found that, in many cases, data is write-forwarded to all processors. In this case, we use a special primitive called write-broadcast. No forwarding register is necessary here. A write-update primitive updates all current sharers of the data, like in update-based cache coherence protocols.
Reference: [10] <author> V. Krothapalli and P. Sadayappan. </author> <title> An Approach to Synchronization of Parallel Computing. </title> <booktitle> In Proceedings of the 1988 International Conference on Supercomputing, </booktitle> <pages> pages 573-581, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: DO i=1,N A (f (i)) = : : : (S 1 ) : : : = A (g (i)) + : : : (S 2 ) ENDDO be analyzed at compile time. One possibility is to parallelize these loops with costly run-time parallelization algorithms <ref> [4, 10, 12, 13, 16, 17, 19, 28] </ref>. In most of these proposals, the compiler inserts code that, at run time, determines what iterations do not depend on each other (inspector phase) and then executes these iterations in parallel (executor phase).
Reference: [11] <author> A. Landin and F. Dahlgren. </author> <title> Bus-Based COMA - Reducing Traffic in Shared-Bus Multiprocessors. </title> <booktitle> In Proceedings of the 2nd International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 95-105, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: The goal of the I-ACOMA project is to explore design alternatives for a highly programmable high-performance multiprocessor. Given the chief importance of programmability, we focus on cache-only memory architectures (COMAs) and their use by a parallelizing compiler. Roughly speaking, comas <ref> [5, 6, 11, 21, 22] </ref> have a more complicated hardware than traditional cache-coherent shared-memory machines, but are easier to program. Indeed, in a coma, data automatically migrates to the local memory of the processor that is using it. Such memory is called attraction memory in coma machines.
Reference: [12] <author> S.-T. Leung and J. Zahorjan. </author> <title> Improving the Performance of Runtime Parallelization. </title> <booktitle> In 4th ACM SIG-PLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 83-91, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: DO i=1,N A (f (i)) = : : : (S 1 ) : : : = A (g (i)) + : : : (S 2 ) ENDDO be analyzed at compile time. One possibility is to parallelize these loops with costly run-time parallelization algorithms <ref> [4, 10, 12, 13, 16, 17, 19, 28] </ref>. In most of these proposals, the compiler inserts code that, at run time, determines what iterations do not depend on each other (inspector phase) and then executes these iterations in parallel (executor phase).
Reference: [13] <author> S. Midkiff and D. Padua. </author> <title> Compiler Algorithms for Synchronization. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-36(12), </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: DO i=1,N A (f (i)) = : : : (S 1 ) : : : = A (g (i)) + : : : (S 2 ) ENDDO be analyzed at compile time. One possibility is to parallelize these loops with costly run-time parallelization algorithms <ref> [4, 10, 12, 13, 16, 17, 19, 28] </ref>. In most of these proposals, the compiler inserts code that, at run time, determines what iterations do not depend on each other (inspector phase) and then executes these iterations in parallel (executor phase).
Reference: [14] <author> A. Nguyen, M. Michael, A. Sharma, and J. Torrellas. </author> <title> The Augmint Multiprocessor Simulation Toolkit for In-tel x86 Architectures. </title> <booktitle> In IEEE International Conference on Computer Design, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Augmint <ref> [14] </ref> is a public-domain multiprocessor tracing and simulation package that takes as input explicitly parallel codes and runs on any Intel x86-based uniprocessor PC. However, while software simulations provide valuable insights into many of these features, they are sometimes limited in the information that they provide.
Reference: [15] <author> D. Poulsen and P. Yew. </author> <title> Data Prefetching and Data Forwarding in Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the 1994 International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 276-280, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: When a processor issues a write-forward to a word, the word is updated in the processor's cache and a copy of the updated cache line is sent to the caches of a set of processors specified as a bit map in the forwarding register <ref> [15] </ref>. The forwarding register is set by the compiler or by the programmer prior to the write-forward instruction. It identifies the processors that are likely to be consumers of the write.
Reference: [16] <author> L. Rauchwerger and D. Padua. </author> <title> The PRIVATIZING DOALL Test: A Run-Time Technique for DOALL Loop Identification and Array Privatization. </title> <booktitle> In Proceedings of the 1994 International Conference on Supercomputing, </booktitle> <pages> pages 33-43, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: DO i=1,N A (f (i)) = : : : (S 1 ) : : : = A (g (i)) + : : : (S 2 ) ENDDO be analyzed at compile time. One possibility is to parallelize these loops with costly run-time parallelization algorithms <ref> [4, 10, 12, 13, 16, 17, 19, 28] </ref>. In most of these proposals, the compiler inserts code that, at run time, determines what iterations do not depend on each other (inspector phase) and then executes these iterations in parallel (executor phase).
Reference: [17] <author> L. Rauchwerger and D. Padua. </author> <title> The LRPD Test: Speculative Run-Time Parallelization of Loops with Privati-zation and Reduction Parallelization. </title> <booktitle> In Proceedings of the SIGPLAN 1995 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: DO i=1,N A (f (i)) = : : : (S 1 ) : : : = A (g (i)) + : : : (S 2 ) ENDDO be analyzed at compile time. One possibility is to parallelize these loops with costly run-time parallelization algorithms <ref> [4, 10, 12, 13, 16, 17, 19, 28] </ref>. In most of these proposals, the compiler inserts code that, at run time, determines what iterations do not depend on each other (inspector phase) and then executes these iterations in parallel (executor phase). <p> However, these algorithms often have high overhead that precludes any speedup from parallelism. One of these algorithms <ref> [17] </ref> makes a backup copy of the arrays involved (array A in Figure 2) and then executes the loop as if it were completely parallel, while saving some information on what array elements are being accessed during execution.
Reference: [18] <author> J. Robinson, D. Baxter, and J. Gray. </author> <title> Advantages of COMA. </title> <type> Unpublished manuscript, </type> <year> 1995. </year>
Reference-contexts: In addition, since the data automatically migrates to where it is being used, the initial placement of the data is largely unimportant. There is no need to try to predict the access patterns at compile time. These arguments, which agree with the thoughts of other researchers <ref> [7, 18] </ref>, need to be tested thoroughly. 3 Compiler Support While the architectural organization of a machine affects how programmable the machine is, the software infrastructure available to write programs for the machine is a key determinant of end-user programming ease.
Reference: [19] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> Run-Time Parallelization and Scheduling of Loops. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(5) </volume> <pages> 603-611, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: DO i=1,N A (f (i)) = : : : (S 1 ) : : : = A (g (i)) + : : : (S 2 ) ENDDO be analyzed at compile time. One possibility is to parallelize these loops with costly run-time parallelization algorithms <ref> [4, 10, 12, 13, 16, 17, 19, 28] </ref>. In most of these proposals, the compiler inserts code that, at run time, determines what iterations do not depend on each other (inspector phase) and then executes these iterations in parallel (executor phase).
Reference: [20] <author> A. Saulsbury, F. Pong, and A. Nowatzyk. </author> <title> Missing the Memory Wall: The Case for Processor/Memory Integration. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 90-101, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: It is therefore important to increase the integration between processor and memory. Ultimately, this integration will lead to putting processor and memory in the same chip <ref> [8, 20] </ref>. It is currently impossible to integrate in the same chip a processor as complicated as the one used for I-ACOMA with enough DRAM memory to be useful. However, as DRAM technology advances and denser DRAMs become available [1], this will change.
Reference: [21] <author> A. Saulsbury, T. Wilkinson, J. Carter, and A. Landin. </author> <title> An Argument for Simple COMA. </title> <booktitle> In Proceedings of the 1st International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 276-285, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: The goal of the I-ACOMA project is to explore design alternatives for a highly programmable high-performance multiprocessor. Given the chief importance of programmability, we focus on cache-only memory architectures (COMAs) and their use by a parallelizing compiler. Roughly speaking, comas <ref> [5, 6, 11, 21, 22] </ref> have a more complicated hardware than traditional cache-coherent shared-memory machines, but are easier to program. Indeed, in a coma, data automatically migrates to the local memory of the processor that is using it. Such memory is called attraction memory in coma machines.
Reference: [22] <author> P. Stenstrom, T. Joe, and A. Gupta. </author> <title> Comparative Performance Evaluation of Cache-Coherent NUMA and COMA Architectures. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 80-91, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The goal of the I-ACOMA project is to explore design alternatives for a highly programmable high-performance multiprocessor. Given the chief importance of programmability, we focus on cache-only memory architectures (COMAs) and their use by a parallelizing compiler. Roughly speaking, comas <ref> [5, 6, 11, 21, 22] </ref> have a more complicated hardware than traditional cache-coherent shared-memory machines, but are easier to program. Indeed, in a coma, data automatically migrates to the local memory of the processor that is using it. Such memory is called attraction memory in coma machines. <p> To help a processor find a memory line efficiently, the networks of these machines were organized in a hierarchy. Specifically, KSR1 had a hierarchy of rings while DDM1 had a hierarchy of busses. Such organization, called Hierarchical Coma <ref> [22] </ref> made it possible to search for a memory line efficiently, but it slowed down remote accesses relative to non-coma machines and complicated the machine. A second problem of the KSR1 was that memory was allocated in page-sized chunks in the attraction memories. <p> This physical memory fragmentation caused the displacement of many lines from the attraction memory and, as a result, eventually increased the number of attraction memory conflicts. These two problems do not exist in I-ACOMA because the machine is organized as a Flat Coma <ref> [22] </ref>. Flat coma is different from hierarchical coma in that each memory line has a node that acts as its home. The home contains the line's directory entry. <p> This paper is organized as follows. Section 2 describes the I-ACOMA architecture with emphasis on the main issues, Section 3 describes the compiler efforts, and Section 4 discusses the implementation of the prototype. 2 The I-ACOMA Architecture I-ACOMA is a scalable shared-memory multiprocessor organized as a flat coma <ref> [22] </ref>. Each node in the machine is envisioned to be a small bus-based multiprocessor with a portion of the memory of the machine. Such design in clusters helps keep the cost of the machine down. The nodes are interconnected in a 2-D mesh for ease of implementation.
Reference: [23] <author> J. Torrellas, C. Xia, and R. Daigle. </author> <title> Optimizing Instruction Cache Performance for Operating System Intensive Workloads. </title> <booktitle> In IEEE Trans. on Computers, to appear 1996. A shorter version appeared in Proceedings of the 1st International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 360-369, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: For our scheme to work, we need a two step approach. In the first step, we profile the code to determine the execution frequency of its basic blocks and paths among them, and place the basic blocks in memory to minimize cache conflicts <ref> [23] </ref>. This profiling step is motivated by the observation that operating system traces often contain fairly-deterministic repeated sequences of hundreds of instructions, often spanning several routines, that are not part of any obvious loop. These sequences we call instruction sequences.
Reference: [24] <author> D. Tullsen, S. Eggers, and H. Levy. </author> <title> Simultaneous Mul-tithreading: Maximizing On-Chip Parallelism. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 392-403, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: We consider each issue in turn. 2.1 Processor Organization The processors used in I-ACOMA are simultaneous multi-threaded (SMT) processors <ref> [24] </ref>. A SMT processor is a wide-issue superscalar processor with multiple threads running at the same time such that, in a given cycle, the processor can issue instructions from different threads. This approach delivers higher performance than aggressive implementations of single-threaded superscalar processors.
Reference: [25] <author> W. Weber and A. Gupta. </author> <title> Cache Invalidation Patterns in Shared-Memory Multiprocessors. </title> <journal> In IEEE Transactions on Computers, </journal> <pages> pages 794-811, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: This will be accomplished by a simple finite state machine in the directory that interprets the N and B bits of the line being read and, if necessary, forces a memory read for the next line. This scheme is specially targeted to migratory shared data <ref> [25] </ref>. Finally, I-ACOMA also has a very cheap yet effective support for instruction prefetching. This scheme is called Guarded Sequential Prefetching [26]. It is useful for systems codes like operating systems, which suffer many instruction misses because they have few tight loops and a large instruction working set.
Reference: [26] <author> C. Xia and J. Torrellas. </author> <title> Instruction Prefetching of Systems Codes With Layout Optimized for Reduced Cache Misses. </title> <booktitle> In Proceedings of the 23nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 271-282, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: This scheme is specially targeted to migratory shared data [25]. Finally, I-ACOMA also has a very cheap yet effective support for instruction prefetching. This scheme is called Guarded Sequential Prefetching <ref> [26] </ref>. It is useful for systems codes like operating systems, which suffer many instruction misses because they have few tight loops and a large instruction working set. For our scheme to work, we need a two step approach. <p> Furthermore, it does so as soon as possible, namely when the processor enters the sequence. If the processor jumps off before reaching the end of the sequence, the prefetch engine detects the new range of addresses issued and jumps to the new sequence too <ref> [26] </ref>. Interestingly, to implement our scheme we only need to make a small modification to existing next-line sequential prefetch-ers: the addition of a module to detect the guard bit. Such module can introduce as little delay as one gate delay.
Reference: [27] <author> Z. Zhang and J. Torrellas. </author> <title> Speeding up Irregular Applications in Shared-Memory Multiprocessors: Memory Binding and Group Prefetching. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 188-199, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: A processor often accesses several fields in a record and then dereferences a pointer. For these applications, I-ACOMA has support for what we call Group Prefetching via Memory Binding <ref> [27] </ref>. The idea is to bind together in hardware a set of variables that are strongly related to each other and from then on prefetch them together. The scheme works as follows.
Reference: [28] <author> C. Q. Zhu and P. C. Yew. </author> <title> A Scheme to Enforce Data Dependence on Large Multiprocessor Systems. </title> <journal> In IEEE Transactions on Software Engineering, </journal> <pages> pages 726-739, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: DO i=1,N A (f (i)) = : : : (S 1 ) : : : = A (g (i)) + : : : (S 2 ) ENDDO be analyzed at compile time. One possibility is to parallelize these loops with costly run-time parallelization algorithms <ref> [4, 10, 12, 13, 16, 17, 19, 28] </ref>. In most of these proposals, the compiler inserts code that, at run time, determines what iterations do not depend on each other (inspector phase) and then executes these iterations in parallel (executor phase).
References-found: 28

