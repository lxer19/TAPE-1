URL: http://www.cis.ohio-state.edu/~kchen/ijns96.ps
Refering-URL: http://www.cis.ohio-state.edu/~kchen/on-line.html
Root-URL: 
Title: Speaker Identification Using Time-Delay HMEs  
Author: KE CHEN, DAHONG XIE and HUISHENG CHI 
Date: 29-43.  
Note: Machine Perception and Center for Information Science Peking University, Beijing 100871, China International Journal of Neural Systems, Vol. 7, No. 1, 1996, pp.  
Affiliation: National Lab of  
Abstract: In this paper, we extend the Hierarchical Mixtures of Experts (HME) to temporal processing and explore it for a substantial problem, that of text-dependent speaker identification. For a specific multiway classification, we propose a generalized Bernoulli density instead of the multinomial logit density to avoid the instability during training. Time-delay technique is applied for spatio-temporal processing in the HME and a combining scheme is presented for combining multiple time-delay HMEs in order to complete multi-scale analysis for the temporal data. Using the time-delay HME along with the EM algorithm as well as the combination of multiple time-delay HMEs, the speaker identification system has a good performance and yields significantly fast training. We have also addressed some issues about the time-delay techniques in the HME.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Matsui and S. Furui, </author> <title> Speaker recognition technology, </title> <journal> NTT Review, Vol.7, No.2, </journal> <month> March, </month> <year> 1995, </year> <pages> pp. 40-48. </pages>
Reference-contexts: These services include banking transaction over a telephone network, telephone shopping, database access services, information services, voice mail, security control for confidential information areas, and remote access to computers <ref> [1] </ref>. From the viewpoint of technology, speaker recognition is a general term which refers to any task to discriminate people based upon their voice characteristics [2]. Within this general task description, there are two specific tasks that have been studied extensively. <p> The mean-square-error threshold is 0.05 and the learning rate is 0:4 in Eq.(9) in all our experiments. In experiments, we have already applied three different time-delay HMEs, i.e. TD-HME <ref> [1] </ref>, TD-HME [2] and TD-HME [3], to the problem of speaker identification. In order to evaluate the performance of the system, we also adopt the digit-based method to test the system. The experimental results are summarized in TABLE V and TABLE VI, respectively. <p> The experimental results are summarized in TABLE V and TABLE VI, respectively. TABLE V The identifying accuracies of three time-delay HMEs in Test-1 Text '0' '1' '2' '3' '4' '5' '6' '7' '8' '9' mean TD-HME <ref> [1] </ref> 92.0 96.0 89.0 88.0 84.0 91.0 90.0 88.0 85.0 92.0 89.5 TD-HME [3] 90.0 97.0 87.0 85.0 86.0 87.0 88.0 92.0 86.0 95.0 89.3 TABLE VI The identifying accuracies of three time-delay HMEs in Test-2 Text '0' '1' '2' '3' '4' '5' '6' '7' '8' '9' mean TD-HME [1] 87.0 <p> TD-HME <ref> [1] </ref> 92.0 96.0 89.0 88.0 84.0 91.0 90.0 88.0 85.0 92.0 89.5 TD-HME [3] 90.0 97.0 87.0 85.0 86.0 87.0 88.0 92.0 86.0 95.0 89.3 TABLE VI The identifying accuracies of three time-delay HMEs in Test-2 Text '0' '1' '2' '3' '4' '5' '6' '7' '8' '9' mean TD-HME [1] 87.0 95.0 91.0 84.0 82.0 88.0 87.0 88.0 86.0 89.0 87.7 TD-HME [3] 90.0 95.0 92.0 87.0 85.0 87.0 85.0 91.0 84.0 92.0 88.8 According to the above experimental results in TABLE II, TABLE V and TABLE VI, we may make a conclusion that the performance of time-delay HMEs is <p> With the consideration, based on the above experimental results in TABLE VI, we may draw an outcome that the longer input-window seems to be better in capturing a speaker's individual features. However, the computational cost is quite expensive; in comparison with the TD-HME <ref> [1] </ref>, the training time of the TD-HME [3] is about twice as much as one of TD-HME [1] for the same problem. We also adopt another method called sequence-based method to test the systems supported by time-delay HMEs in order to investigate the robustness of these systems. <p> However, the computational cost is quite expensive; in comparison with the TD-HME <ref> [1] </ref>, the training time of the TD-HME [3] is about twice as much as one of TD-HME [1] for the same problem. We also adopt another method called sequence-based method to test the systems supported by time-delay HMEs in order to investigate the robustness of these systems. <p> After obtaining all results, the system tolls a vote with the principle of majority that a speaker can be identified only if there are at least three same identification results for the speaker; otherwise, the system refuses to identify the unknown speaker. Using the method, all systems with TD-HME <ref> [1] </ref>, TD-HME [2] and TD-HME [3] are robust with 100% identification accuracies over 2000 tests up to now. 5.3 Results by Combining Time-Delay HMEs The experimental results on an individual time-delay HME with the short input-window indicate that it is difficult to find an appropriate size of the short input-window for <p> In our current experiments, we consider combining the HME without time delay, TD-HME [0], and the aforementioned three time-delay HMEs, i.e. TD-HME <ref> [1] </ref>, TD-HME [2] and TD-HME [3]. Applying these trained HMEs on the aforementioned data set, we may achieve four confusion matrices corresponding to TD-HME [0], TD-HME [1], TD-HME [2] and TD-HME [3], respectively. During test, we complete three tests, i.e. Test-1, Test-2 and Test-1A. <p> In our current experiments, we consider combining the HME without time delay, TD-HME [0], and the aforementioned three time-delay HMEs, i.e. TD-HME <ref> [1] </ref>, TD-HME [2] and TD-HME [3]. Applying these trained HMEs on the aforementioned data set, we may achieve four confusion matrices corresponding to TD-HME [0], TD-HME [1], TD-HME [2] and TD-HME [3], respectively. During test, we complete three tests, i.e. Test-1, Test-2 and Test-1A. Test-1A refers to that the test data set consists of all data in Set-2 except those four times utterances of each digit used to achieve confusion matrices.
Reference: [2] <author> G.R. Doddington, </author> <title> Speaker recognitionidentifying people by their voices, </title> <booktitle> Proceedings of IEEE, </booktitle> <address> Vol.73, No.11, </address> <year> 1986, </year> <pages> pp. 1651-1664. </pages>
Reference-contexts: From the viewpoint of technology, speaker recognition is a general term which refers to any task to discriminate people based upon their voice characteristics <ref> [2] </ref>. Within this general task description, there are two specific tasks that have been studied extensively. These are referred to as speaker identification and speaker verification. <p> In the stage, we first divide the training set into two subsets; one for training and the other for test. Using this method, we have investigated 7 architectures covering from one level to three levels. Using a time-delay HME architecture, TD-HME <ref> [2] </ref>, in which the size of input-window is 2 (n = 2) for instance, we show the results in TABLE IV. TABLE IV The results of cross-validation for the TD-HME [2] Architecture 1-16 2-2 2-8 2-16 2-2-8 2-4-8 2-4-16 Epoches 11 21 4 5 5 5 6 Time (min) 48 82 <p> Using a time-delay HME architecture, TD-HME <ref> [2] </ref>, in which the size of input-window is 2 (n = 2) for instance, we show the results in TABLE IV. TABLE IV The results of cross-validation for the TD-HME [2] Architecture 1-16 2-2 2-8 2-16 2-2-8 2-4-8 2-4-16 Epoches 11 21 4 5 5 5 6 Time (min) 48 82 23 38 54 69 92 Identifying Accuracy (%) 97.0 96.0 99.0 96.0 98.0 98.0 98.0 According to the performance and training time, finally, we choose the two levels HME with <p> The mean-square-error threshold is 0.05 and the learning rate is 0:4 in Eq.(9) in all our experiments. In experiments, we have already applied three different time-delay HMEs, i.e. TD-HME [1], TD-HME <ref> [2] </ref> and TD-HME [3], to the problem of speaker identification. In order to evaluate the performance of the system, we also adopt the digit-based method to test the system. The experimental results are summarized in TABLE V and TABLE VI, respectively. <p> Using the method, all systems with TD-HME [1], TD-HME <ref> [2] </ref> and TD-HME [3] are robust with 100% identification accuracies over 2000 tests up to now. 5.3 Results by Combining Time-Delay HMEs The experimental results on an individual time-delay HME with the short input-window indicate that it is difficult to find an appropriate size of the short input-window for the time-delay <p> In our current experiments, we consider combining the HME without time delay, TD-HME [0], and the aforementioned three time-delay HMEs, i.e. TD-HME [1], TD-HME <ref> [2] </ref> and TD-HME [3]. Applying these trained HMEs on the aforementioned data set, we may achieve four confusion matrices corresponding to TD-HME [0], TD-HME [1], TD-HME [2] and TD-HME [3], respectively. During test, we complete three tests, i.e. Test-1, Test-2 and Test-1A. <p> TD-HME [1], TD-HME <ref> [2] </ref> and TD-HME [3]. Applying these trained HMEs on the aforementioned data set, we may achieve four confusion matrices corresponding to TD-HME [0], TD-HME [1], TD-HME [2] and TD-HME [3], respectively. During test, we complete three tests, i.e. Test-1, Test-2 and Test-1A. Test-1A refers to that the test data set consists of all data in Set-2 except those four times utterances of each digit used to achieve confusion matrices.
Reference: [3] <author> D. O'Shaughnessy, </author> <title> Speaker recognition, </title> <journal> IEEE ASSP Magazine, </journal> <volume> Vol. 3, No. 4, </volume> <month> Oct. </month> <year> 1986, </year> <pages> pp. 4-17. </pages>
Reference-contexts: This is a different problem in comparison with text-independent identification, where the text should be any text in either training or testing. Moreover, speaker identification can be subdivided into two further categories, closed-set and open-set problems <ref> [3] </ref>. The closed set problem is to identify a speaker from a group of N known speakers. Alternatively, one may want to decide whether the speaker of a test utterance belongs to a group of N known speakers. <p> The mean-square-error threshold is 0.05 and the learning rate is 0:4 in Eq.(9) in all our experiments. In experiments, we have already applied three different time-delay HMEs, i.e. TD-HME [1], TD-HME [2] and TD-HME <ref> [3] </ref>, to the problem of speaker identification. In order to evaluate the performance of the system, we also adopt the digit-based method to test the system. The experimental results are summarized in TABLE V and TABLE VI, respectively. <p> The experimental results are summarized in TABLE V and TABLE VI, respectively. TABLE V The identifying accuracies of three time-delay HMEs in Test-1 Text '0' '1' '2' '3' '4' '5' '6' '7' '8' '9' mean TD-HME [1] 92.0 96.0 89.0 88.0 84.0 91.0 90.0 88.0 85.0 92.0 89.5 TD-HME <ref> [3] </ref> 90.0 97.0 87.0 85.0 86.0 87.0 88.0 92.0 86.0 95.0 89.3 TABLE VI The identifying accuracies of three time-delay HMEs in Test-2 Text '0' '1' '2' '3' '4' '5' '6' '7' '8' '9' mean TD-HME [1] 87.0 95.0 91.0 84.0 82.0 88.0 87.0 88.0 86.0 89.0 87.7 TD-HME [3] 90.0 <p> TD-HME <ref> [3] </ref> 90.0 97.0 87.0 85.0 86.0 87.0 88.0 92.0 86.0 95.0 89.3 TABLE VI The identifying accuracies of three time-delay HMEs in Test-2 Text '0' '1' '2' '3' '4' '5' '6' '7' '8' '9' mean TD-HME [1] 87.0 95.0 91.0 84.0 82.0 88.0 87.0 88.0 86.0 89.0 87.7 TD-HME [3] 90.0 95.0 92.0 87.0 85.0 87.0 85.0 91.0 84.0 92.0 88.8 According to the above experimental results in TABLE II, TABLE V and TABLE VI, we may make a conclusion that the performance of time-delay HMEs is really better than one of the HME without time-delay, but the improvement is <p> However, the computational cost is quite expensive; in comparison with the TD-HME [1], the training time of the TD-HME <ref> [3] </ref> is about twice as much as one of TD-HME [1] for the same problem. We also adopt another method called sequence-based method to test the systems supported by time-delay HMEs in order to investigate the robustness of these systems. <p> Using the method, all systems with TD-HME [1], TD-HME [2] and TD-HME <ref> [3] </ref> are robust with 100% identification accuracies over 2000 tests up to now. 5.3 Results by Combining Time-Delay HMEs The experimental results on an individual time-delay HME with the short input-window indicate that it is difficult to find an appropriate size of the short input-window for the time-delay HME in order <p> In our current experiments, we consider combining the HME without time delay, TD-HME [0], and the aforementioned three time-delay HMEs, i.e. TD-HME [1], TD-HME [2] and TD-HME <ref> [3] </ref>. Applying these trained HMEs on the aforementioned data set, we may achieve four confusion matrices corresponding to TD-HME [0], TD-HME [1], TD-HME [2] and TD-HME [3], respectively. During test, we complete three tests, i.e. Test-1, Test-2 and Test-1A. <p> TD-HME [1], TD-HME [2] and TD-HME <ref> [3] </ref>. Applying these trained HMEs on the aforementioned data set, we may achieve four confusion matrices corresponding to TD-HME [0], TD-HME [1], TD-HME [2] and TD-HME [3], respectively. During test, we complete three tests, i.e. Test-1, Test-2 and Test-1A. Test-1A refers to that the test data set consists of all data in Set-2 except those four times utterances of each digit used to achieve confusion matrices.
Reference: [4] <author> S. Furui, </author> <title> An overview of speaker recognition technology, Proceeding of ESCA Workshop on Automatic Speaker Recognition, Identification and Verification, </title> <year> 1994, </year> <month> pp.1-9. </month>
Reference-contexts: In this paper, the closed-set problem is simply considered. In general, the technique of speaker identification includes feature extraction and classification. There have been extensive studies in this field based upon conventional techniques of speech signal processing <ref> [4] </ref>. Employed as the classifier, recently, many kinds of neural networks have been adopted for speaker recognition [5], such as MLP [6], neural tree [7] and a hybrid model [8] etc.
Reference: [5] <author> Y. Bennani and P. Gallinari, </author> <title> Connectionist approaches for automatic speaker recognition, Proceedings of ESCA Workshop on Automatic Speaker Recognition, Identification and Verification, </title> <booktitle> 1994, </booktitle> <pages> pp. 95-102. </pages>
Reference-contexts: In general, the technique of speaker identification includes feature extraction and classification. There have been extensive studies in this field based upon conventional techniques of speech signal processing [4]. Employed as the classifier, recently, many kinds of neural networks have been adopted for speaker recognition <ref> [5] </ref>, such as MLP [6], neural tree [7] and a hybrid model [8] etc. Unfortunately, the systems based on neural networks often suffer from a high computational burden during training and have only a limited improvement over conventional techniques in performance.
Reference: [6] <author> J. He, L. Liu and G. Palm, </author> <title> A text-independent speaker identification system based on neural networks, </title> <booktitle> Proceedings of International Conference on Spoken Language Processing, </booktitle> <address> Yokohama, Japan, </address> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: In general, the technique of speaker identification includes feature extraction and classification. There have been extensive studies in this field based upon conventional techniques of speech signal processing [4]. Employed as the classifier, recently, many kinds of neural networks have been adopted for speaker recognition [5], such as MLP <ref> [6] </ref>, neural tree [7] and a hybrid model [8] etc. Unfortunately, the systems based on neural networks often suffer from a high computational burden during training and have only a limited improvement over conventional techniques in performance.
Reference: [7] <author> K.R. Farrell and R.J. Mammone, </author> <title> Speaker identification using neural tree networks, </title> <booktitle> Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <year> 1994, </year> <pages> pp. </pages> <month> I165-I168. </month>
Reference-contexts: There have been extensive studies in this field based upon conventional techniques of speech signal processing [4]. Employed as the classifier, recently, many kinds of neural networks have been adopted for speaker recognition [5], such as MLP [6], neural tree <ref> [7] </ref> and a hybrid model [8] etc. Unfortunately, the systems based on neural networks often suffer from a high computational burden during training and have only a limited improvement over conventional techniques in performance.
Reference: [8] <author> X. Jiang, Z. Gong, F. Sun and H. Chi, </author> <title> A speaker recognition system based on auditory model, </title> <booktitle> Proceedings of World Congress on Neural Networks, </booktitle> <address> San Diego, </address> <year> 1994, </year> <pages> pp. </pages> <month> IV595-IV600. </month>
Reference-contexts: There have been extensive studies in this field based upon conventional techniques of speech signal processing [4]. Employed as the classifier, recently, many kinds of neural networks have been adopted for speaker recognition [5], such as MLP [6], neural tree [7] and a hybrid model <ref> [8] </ref> etc. Unfortunately, the systems based on neural networks often suffer from a high computational burden during training and have only a limited improvement over conventional techniques in performance.
Reference: [9] <author> M.I. Jordan and R.A. Jacobs, </author> <title> Hierarchical mixture of experts and EM algorithm, </title> <journal> Neural Computation, </journal> <volume> Vol. 6, </volume> <year> 1994, </year> <pages> pp. 181-214. </pages>
Reference-contexts: There has recently been widespread interest in the use of multiple models for classification and regression in the statistics and neural networks communities. The Hierarchical Mixtures of Experts (HME) <ref> [9] </ref> is just a modular architecture in which the outputs of a number of modular nets called `experts', each mapping a particular portion of the input space, are combined in a probabilistic way by `gating' net which is modeling the probability that each portion of the input space generated the output. <p> The expert nets sit at the leaves of the tree. Each expert produces an output vector for each input vector. These output vectors proceed up the tree, being blended by the gating net outputs <ref> [9] </ref>. In Fig. 1, there are only two levels in the architecture with 2-2 1 . In general, it is easy to extend this architecture to multiple levels. <p> Accordingly, an increase in Q implies an increase in the incomplete-data likelihood [20]: l ( (p+1) ; X obs ) l ( (p) ; X obs ) For training the HME, there are two methods <ref> [9] </ref>, i.e. gradient-based and EM algorithms, while the performance of the EM algorithm is usually much better than one of the gradient-based algorithm [9]- [13][15][21]. Jordan and Jacobs have already applied the EM algorithm to the HME architecture [9] by introducing indicator variables as missing data to the original observable data <p> in the incomplete-data likelihood [20]: l ( (p+1) ; X obs ) l ( (p) ; X obs ) For training the HME, there are two methods <ref> [9] </ref>, i.e. gradient-based and EM algorithms, while the performance of the EM algorithm is usually much better than one of the gradient-based algorithm [9]- [13][15][21]. Jordan and Jacobs have already applied the EM algorithm to the HME architecture [9] by introducing indicator variables as missing data to the original observable data for simplifying the original likelihood function only with the observable data. <p> ; X obs ) For training the HME, there are two methods <ref> [9] </ref>, i.e. gradient-based and EM algorithms, while the performance of the EM algorithm is usually much better than one of the gradient-based algorithm [9]- [13][15][21]. Jordan and Jacobs have already applied the EM algorithm to the HME architecture [9] by introducing indicator variables as missing data to the original observable data for simplifying the original likelihood function only with the observable data. The indicators may be explained as the labels that correspond to the decisions or specify the expert in the probability model [9]. <p> algorithm to the HME architecture <ref> [9] </ref> by introducing indicator variables as missing data to the original observable data for simplifying the original likelihood function only with the observable data. The indicators may be explained as the labels that correspond to the decisions or specify the expert in the probability model [9]. <p> And observation weights are fh (t) 1 . 3 For details, readers are referred to Appendix A in <ref> [9] </ref> for the complete derivation of the algorithm. 4 (c) For each lower-level gating network, solve the weighted IRLS problem in Eq.(6) using Eq.(7) with observations f (x (t) ; h (t) 1 and observation weights fh (t) 1 , fh ljk g N In this algorithm, each complete update including <p> ln 1 p k K X ln (1 p k )g (9) Accordingly, we may also derive its link and variance functions [17] from Eq.(2) and Eq.(4) as f (t) = 1 For a general multiway classification, the multinomial logit density has already been chosen in the original HME model <ref> [9] </ref>. From the viewpoint of neural computing, when both the multinomial logit density and the generalized Bernoulli density are used as the probabilistic model of an expert net, the difference between them lies in that they define different activation functions for the expert net. <p> For solving the IRLS problem, we have introduced a changeable learning rate to the updated formula in Eq.(7) instead of that the value is always one in the previous formula <ref> [9] </ref>. As a result, the modified updated formula may alleviate the instability and accelerate training during training. In addition, we have also proposed the generalized Bernoulli density for the specific multiway classification instead of the multinomial logit density.
Reference: [10] <author> S.R. </author> <title> Waterhouse, </title> <type> Personal communication, </type> <year> 1994. </year>
Reference: [11] <author> S.R. Waterhouse and A.J. Robinson, </author> <title> Classification using hierarchical mixtures of experts, </title> <booktitle> Proceedings of IEEE Conference on Neural Networks and Signal Processing, </booktitle> <year> 1994. </year>
Reference: [12] <author> S.R. Waterhouse and A.J. Robinson, </author> <title> Prediction of acoustic vectors using hierarchical mixture of experts, Advance in Neural Information Processing Systems 7, </title> <editor> J.D. Cowan, G. Tesauro and J. Alspector eds., </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: The HME has been successful 1 in a number of regression and some classification problems [9]-[11], yielding significantly faster training through the use of the Expectation Maximization (EM) algorithm. In addition, it is also applied successfully to the non-linear prediction of acoustic vectors for speech processing <ref> [12] </ref>. In our previous work, we have already applied HME along with EM algorithm to cope with the text-dependent speaker identification [13]. In fact, many real-world applications require the processing of patterns that evolve over time and speech processing is a typical case.
Reference: [13] <author> K. Chen, D. Xie and H. Chi, </author> <title> Speaker identification based on hierarchical mixture of experts, </title> <booktitle> Proceedings of World Congress on Neural Networks, </booktitle> <address> Washington D. C., </address> <month> July, </month> <year> 1995, </year> <pages> pp. </pages> <address> I493-I496. </address> <month> 14 </month>
Reference-contexts: In addition, it is also applied successfully to the non-linear prediction of acoustic vectors for speech processing [12]. In our previous work, we have already applied HME along with EM algorithm to cope with the text-dependent speaker identification <ref> [13] </ref>. In fact, many real-world applications require the processing of patterns that evolve over time and speech processing is a typical case. However, all aforementioned applications of HME architecture simply consider patterns as static ones.
Reference: [14] <author> A. Waibel, T. Hanazawa, G. Hinton, K. Shikano and K. Lang, </author> <title> Phoneme recognition using time-delay neural networks, </title> <booktitle> IEEE Transaction on Acoustics, Speech and Signal Processing, </booktitle> <volume> ASSP-37, </volume> <month> March, </month> <year> 1989, </year> <pages> pp. 328-339. </pages>
Reference-contexts: However, all aforementioned applications of HME architecture simply consider patterns as static ones. For the current task of speaker identification, the temporal characteristics of patterns play an important role in the final recognition results. Time-Delay Neural Network (TDNN) architecture was originally designed for speech recognition <ref> [14] </ref> and has strong abilities to spatio-temporal processing. Motivated by the TDNN architecture, in the paper we introduce the time-delay concept to HME architecture. For the time-delay, a time window with the fixed size should be chosen in advance [14]. <p> Time-Delay Neural Network (TDNN) architecture was originally designed for speech recognition <ref> [14] </ref> and has strong abilities to spatio-temporal processing. Motivated by the TDNN architecture, in the paper we introduce the time-delay concept to HME architecture. For the time-delay, a time window with the fixed size should be chosen in advance [14]. In general, the larger window may capture more temporal information, but it suffers from an expensive computational cost. Due to the lack of understanding in features of speaker's identity, moreover, it is rather hard for us to choose an appropriate time window. <p> It is worth pointing out that the time-delay used in the HME is slightly different from the original one in the time-delay neural network (TDNN) architecture <ref> [14] </ref> due to the different architectures. For the TDNN architecture, it employs a multilayer perceptron structure. Accordingly, the time-delay processing occurs between any two adjacent layers. For the HME, each component net can be modeled as a specific probability density belonging to the exponential family based upon GLIM theory [9][17]. <p> It is well known that a neural network can be viewed as a probability approximator [27]-[31]. Thus, we may use a classic time-delay neural network with at least one hidden layer <ref> [14] </ref> or an adaptive time-delay neural network [32] as expert nets in the HME. Hence, the EM algorithm is still employed to deal with the problem of parameter estimation in the HME. There is no change in the E-step and the optimization of gating net (s) in the M-step.
Reference: [15] <author> K. Chen, D. Xie and H. Chi, </author> <title> Speaker identification based on a time-delay modular neural network, </title> <journal> Journal of Advanced Software Research, </journal> <note> Allerton Press, Vol.4, No.1, 1996. (in press) </note>
Reference-contexts: Due to the lack of understanding in features of speaker's identity, moreover, it is rather hard for us to choose an appropriate time window. This problem has already been encountered in our previous work <ref> [15] </ref>. To handle the problem, in this paper, we propose a combining scheme in which multiple time-delay HMEs with different short-term window sizes are combined under the framework of Bayesian formalism, while this idea was originally adopted in combining multiple classifiers to achieve the better performance of classification [16].
Reference: [16] <author> L. Xu, A. Krzyzak and C.Y. Suen, </author> <title> Methods of combining multiple classifiers and their applications to handwriting recognition, </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, Vol.23, </journal> <volume> No.3, </volume> <year> 1992, </year> <pages> pp. 418-435. </pages>
Reference-contexts: To handle the problem, in this paper, we propose a combining scheme in which multiple time-delay HMEs with different short-term window sizes are combined under the framework of Bayesian formalism, while this idea was originally adopted in combining multiple classifiers to achieve the better performance of classification <ref> [16] </ref>. On the other hand, we also extend the HME model to a specific multiway classification base on a proposed probability density called generalized Bernoulli density instead of multinomial logit density to improve the performance of the HME and yield fast training. <p> This idea has already been extensively used in combining multiple classifiers to improve the performance of individual classifiers for the development of highly reliable character recognition systems <ref> [16] </ref>[22]-[25]. Here, we adopt the combining scheme in Bayesian formalism proposed in [16] to combine multiple time-delay HMEs with different short time windows. Fig. 3 illustrates such a scheme in which n + 1 time-delay HMEs (n &gt; 0), labeled as TD-HME [i] (i = 0; 1; ; n), are combined. <p> In particular, TD-HME [0] denotes the HME without time-delay when i = 0. 6 In the sequel, we describe a combining scheme based upon the Bayesian formalism for combining multiple time-delay HMEs. This scheme was originally proposed for combining multiple classifiers with the static input <ref> [16] </ref>. Here, we extend it to sequence processing. In the context of combining multiple classifiers, all combined classifiers must be used on the same static input [16]. <p> This scheme was originally proposed for combining multiple classifiers with the static input <ref> [16] </ref>. Here, we extend it to sequence processing. In the context of combining multiple classifiers, all combined classifiers must be used on the same static input [16]. For sequence processing, here, we have to relax the condition so that all combined time-delay HMEs can be used on the different inputs which belong to the same sequence (utterance). <p> The errors of each combined time-delay HME CL k are usually described by its confusion matrix <ref> [16] </ref> as follows, P T k = 6 6 4 (k) (k) (k) n 21 n 22 : : : n 2N n N1 n N2 : : : n NN 7 7 5 for k = 0; 1; ; K 1 if there are K time-delay HMEs to be combined; <p> Although there may exist a better estimation of P (x 2 C i jEN ), here, we still follow the original estimation in <ref> [16] </ref> so that we may achieve the integrated belief values as follows, bel (i) = fi k=0 (k) where fi = [ P N Q K1 (k) t ) = j k )] 1 .
Reference: [17] <author> P. McCullagh and J.A. Nelder, </author> <title> Generalized linear models, </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1989. </year>
Reference-contexts: Accordingly, expert net (i; j) produces an output as a generalized linear function, hereafter called link function <ref> [17] </ref>, of the input x: ij = f (W ij x), where W ij is a weight matrix and f is a fixed continuous nonlinearity. <p> P (yjx; ij ) is the probabilistic component of the HME model and can be generally modeled as a density in the exponential family <ref> [17] </ref> with the following form: P (y; ; ) = expf y b () + c (y; )g (2) where is called the natural parameter and is the dispersion parameter in Generalized Linear Model (GLIM) [17]. <p> the HME model and can be generally modeled as a density in the exponential family <ref> [17] </ref> with the following form: P (y; ; ) = expf y b () + c (y; )g (2) where is called the natural parameter and is the dispersion parameter in Generalized Linear Model (GLIM) [17]. <p> Since all components of the HME architecture are based upon the GLIM theory which provides the basic statistic structure for the HME, the likelihood is a product of densities from the exponential family of distributions. It may be solved by using the Fisher scoring algorithm <ref> [17] </ref>. Using the Fisher scoring algorithm, Jordan and Jacobs derive a particular iterative algorithm called IRLS algorithm for computing a maximum likelihood estimate of the parameters of the HME. 3 . <p> 1y k )g = expf K X (y k ln p k + (1 y k ) ln (1 p k ))g = expf K X y k ln 1 p k K X ln (1 p k )g (9) Accordingly, we may also derive its link and variance functions <ref> [17] </ref> from Eq.(2) and Eq.(4) as f (t) = 1 For a general multiway classification, the multinomial logit density has already been chosen in the original HME model [9].
Reference: [18] <author> L. Xu, M.I. Jordan and G.E. Hinton, </author> <title> A modified gating network for the mixture of experts architecture, </title> <booktitle> Proceedings of World Congress on Neural Networks, </booktitle> <address> San Diego, </address> <year> 1994, </year> <pages> pp. </pages> <month> II405-II410. </month>
Reference: [19] <author> L. Xu, M.I. Jordan and G.E. Hinton, </author> <title> An alternative model for mixtures of experts, Advance in Neural Information Processing Systems 7, </title> <editor> J.D. Cowan, G. Tesauro and J. Alspector eds., </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference: [20] <author> A.P. Dempster, N.M. Laird and D.B. Rubin, </author> <title> Maximum likelihood from incomplete data via the EM algorithm, </title> <journal> J. R. Statist. Soc. </journal> <volume> B-39, </volume> <year> 1977, </year> <pages> pp. 1-38. </pages>
Reference-contexts: Accordingly, the link function f () = b 0 () is known as the cononical link and V ar (y) = b 00 () is known as the variance function in the GLIM theory, respectively. 2.2 Expectation-Maximization (EM) Algorithm The EM algorithm is a general technique for maximum likelihood estimation <ref> [20] </ref>. Assume that X obs denotes all observable data and a likelihood function based on the data is l (; X obs ). <p> It has already been shown that an iterative step of EM chooses a parameter value which increases the value of Q, the expectation of the complete-data likelihood. Accordingly, an increase in Q implies an increase in the incomplete-data likelihood <ref> [20] </ref>: l ( (p+1) ; X obs ) l ( (p) ; X obs ) For training the HME, there are two methods [9], i.e. gradient-based and EM algorithms, while the performance of the EM algorithm is usually much better than one of the gradient-based algorithm [9]- [13][15][21].
Reference: [21] <author> M.I. Jordan and L. Xu, </author> <title> Convergence results for the EM approach to mixture-of-experts architectures, </title> <institution> TR-9302, Computational Cognitive Science Lab, Massachusetts Institute of Technology, </institution> <year> 1993. </year>
Reference: [22] <author> C.Y. Suen, C. Nadal, T.A. Mai, R. Legault and L. Lam, </author> <title> Recognition of totally unconstrained handwritten neurals based on the concept of multiple experts, </title> <booktitle> Proceedings of International Workshop on Frontiers in Handwriting Recognition, </booktitle> <address> Montreal, </address> <month> April, </month> <year> 1990, </year> <pages> pp. 131-143. </pages>
Reference: [23] <author> T.K. Ho, J.J. Hull and S.N. Srihari, </author> <title> Combination of structural classifiers, </title> <booktitle> Proceedings of IAPR Workshop on Syntactic and Structural Pattern Recognition, </booktitle> <month> June, </month> <year> 1990, </year> <month> pp.123-137. </month>
Reference: [24] <author> R. Battiti and A.M. Colla, </author> <title> Democracy in neural nets: voting schemes for classification, </title> <booktitle> Neural Networks, </booktitle> <volume> Vol. 7, No. 4, </volume> <year> 1994, </year> <month> pp.691-708. </month>
Reference: [25] <author> G. Rogova, </author> <title> Combining the results of several neural network classifiers, </title> <booktitle> Neural Networks, </booktitle> <volume> Vol. 7, No. 5, </volume> <year> 1994, </year> <month> pp.771-781. </month>
Reference: [26] <author> Xin Jiang, </author> <title> Text-dependent speaker identification based on artificial neural networks, </title> <type> Master Thesis, </type> <institution> Center for Information Science, Peking University, </institution> <year> 1994. </year> <note> (in Chinese) </note>
Reference-contexts: In the current system, we adopt the linear predictive coding (LPC) power spectrum. With respect to speaker features used for training neural networks, there were an investigation and a comparison among several common LPC-based features <ref> [26] </ref>, such as power spectrum, cepstral coefficient and autocorrelation coefficient etc. The experiment shows that the LPC power spectrum has the best performance for text-dependent speaker identification. <p> A 16th-order linear predictive analysis is first performed for each speech frame, then LPC power spectrum is computed with a 256 points FFT for each frame. Moreover, a critical bandwidth filter is employed for processing the LPC power spectrum further <ref> [26] </ref>, which is considered as a simulation of the processing mechanism in the human peripheral auditory system. For this purpose, the power spectrum is divided from low frequency to high one (0 ~ 5:512 KHz) into 24 channels.
Reference: [27] <author> E.B. Baum and F. Wilczek, </author> <title> Supervised learning of probability distributions by neural networks, Advance in Neural Information Systems, </title> <editor> D.Z. Anderson ed., </editor> <address> New York: </address> <institution> American Institute of Physics, </institution> <year> 1988, </year> <pages> pp. 52-61. </pages>
Reference: [28] <author> H. Gish, </author> <title> A probabilistic approach to the understanding and training of neural network classifiers, </title> <booktitle> Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <address> Albu-querque, NM, </address> <year> 1990, </year> <pages> pp. 1361-1364. </pages>
Reference: [29] <author> H. Bourlard and C.J. Wellekens, </author> <title> Links between Markov models and multilayer perceptrons, </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> Vol. 12, </volume> <year> 1990, </year> <pages> pp. 1167-1178. </pages>
Reference: [30] <author> M.D. Richard and R.P. Lippmann, </author> <title> Neural network classifiers estimate Bayesian a posteriori probabilities, </title> <booktitle> Neural Computation 3, </booktitle> <year> 1991, </year> <pages> pp. 461-483. 15 </pages>
Reference: [31] <author> H. Bourlard and C.J. Wellekens, </author> <title> Connectionist speech recognition a hybrid approach, </title> <publisher> Amsterdam: Kluwer, </publisher> <year> 1994. </year>
Reference: [32] <author> S.P. Day and M.R. Davenport, </author> <title> Continuous-time temporal back-propagation with adaptable time delays, </title> <journal> IEEE Transactions on Neural Networks, Vol.4, No.2, </journal> <year> 1993, </year> <pages> pp. 348-354. </pages>
Reference-contexts: It is well known that a neural network can be viewed as a probability approximator [27]-[31]. Thus, we may use a classic time-delay neural network with at least one hidden layer [14] or an adaptive time-delay neural network <ref> [32] </ref> as expert nets in the HME. Hence, the EM algorithm is still employed to deal with the problem of parameter estimation in the HME. There is no change in the E-step and the optimization of gating net (s) in the M-step.
Reference: [33] <author> R. Fletcher, </author> <title> Practical methods of optimization, </title> <publisher> John Wiley&Sons, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: ij ln P (yjx; ij ) ij t (t) (t) (t) = arg min X h ij [(y (t) ij ) T (y (t) ij )] (19) Instead of the IRLS algorithm, thus, there are many optimization algorithms for the problem such as the gradient-based method and Newton method etc. <ref> [33] </ref>.
Reference: [34] <author> D.W. Tank and J.J. </author> <title> Hopfield, Neural computation by concentrating information in time, </title> <booktitle> Proceedings National Academy of Sciences, </booktitle> <month> April, </month> <year> 1987, </year> <pages> pp. 1896-1900. </pages>
Reference: [35] <author> K.P. Unnikrsihnan, J.J. Hopfield and D.W. Tank, </author> <title> Connected digit speaker dependent speech recognition using a network with time-delay connections, </title> <journal> IEEE Transactions on Signal Processing 39, </journal> <year> 1991, </year> <pages> pp. 698-713. </pages>
Reference: [36] <author> K.P. Unnikrsihnan, J.J. Hopfield and D.W. Tank, </author> <title> Speaker-independent digit recognition using a neural network with time-delayed connections, </title> <booktitle> Neural Computation 4, </booktitle> <year> 1992, </year> <pages> pp. 108-119. </pages>
Reference: [37] <author> B. de Vires and J.C. Principe, </author> <title> A theory for neural networks with time delays, </title> <booktitle> Advances in Neural Information Systems 3, </booktitle> <editor> Lippmann, Moody and Touretzky eds., </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1991, </year> <pages> pp. 162-168. </pages>
Reference: [38] <author> B. de Vires and J.C. Principe, </author> <title> The Gamma model a new neural net model for temporal processing neural networks, </title> <booktitle> Neural Networks 5, </booktitle> <year> 1992, </year> <pages> pp. 565-576. </pages> <address> 16 Fig. </address> <month> 1. </month> <title> The architecture of hierarchical mixture of experts. Fig. 2. The input of expert and gating nets in the time-delay HME. 17 Fig. 3. The scheme of combining multiple time-delay HMEs. Fig. 4. The scheme of speaker identification system based on the time-delay HME. </title> <type> 18 </type>
References-found: 38

