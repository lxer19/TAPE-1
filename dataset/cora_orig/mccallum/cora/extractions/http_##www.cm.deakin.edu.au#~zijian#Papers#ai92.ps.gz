URL: http://www.cm.deakin.edu.au/~zijian/Papers/ai92.ps.gz
Refering-URL: http://www.cm.deakin.edu.au/~zijian/publications.html
Root-URL: 
Email: Email: zijian@cs.su.oz.au  
Title: CONSTRUCTING CONJUNCTIVE TESTS FOR DECISION TREES  
Author: ZIJIAN ZHENG 
Address: Sydney, NSW 2006  
Affiliation: Basser Department of Computer Science, University of  
Note: In Proceedings of The 5th Australian Joint Conference on Artificial Intelligence, World Scientific, 355-360, 1992.  
Abstract: This paper discusses an approach of constructing new attributes based on decision trees and production rules. It can improve the concepts learned in the form of decision trees by simplifying them and improving their predictive accuracy. In addition, this approach can distinguish relevant primitive attributes from irrelevant primitive attributes. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone, </author> <title> Classification And Regression Trees, </title> <address> (Belmont, CA: </address> <publisher> Wadsworth, </publisher> <year> 1984). </year>
Reference-contexts: If the size of data set is small, a 2 (on Tic-Tac-Toe problem) (on Heart-disease problem) 10-fold cross-validation <ref> [1] </ref> is performed. The output of the algorithm CI1 and CI2 is the decision tree that has the greatest predictive accuracy. We now turn to constructing new attributes from production rules and selecting the best constructed attributes from them using a decision tree. <p> Unlike attribute construction approaches discussed above that use the learning result of selective induction to guide their construction, another kind of approaches constructs new attributes from training data directly during creating concepts, although all of them learn decision trees as their target concepts. For example, CART <ref> [1] </ref> uses Boolean combinations of primitive attributes as new 5 attributes.
Reference: 2. <author> C.J. Matheus and L.A. Rendell, </author> <title> Constructive induction on decision trees, </title> <booktitle> Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI) (1989), p. </booktitle> <pages> 645-650. </pages>
Reference-contexts: The FRINGE has two approaches to new attribute selection. One is keeping attributes that have been used at least once. Another is keeping attributes that have been used in the last iteration [4]. CITRE <ref> [2] </ref> uses "utility" to select attributes. The "utility" of each new attribute is measured in terms of the information gained by using the attribute to split the entire training set into disjoint subsets. <p> They are then ordered by utility, and those attributes with the lowest utilities are deleted until the total count of primitive attributes and new attributes is down to a predecided number. As pointed out by Matheus and Rendell <ref> [2] </ref>, this greedy method can fail if a new attribute having a poor utility on the entire data set exhibits a relatively high utility sometime after the first split. For this reason, we select new attributes through building a decision tree.
Reference: 3. <author> G. Pagallo and D. Haussler, </author> <title> Boolean feature discovery in empirical learning. </title> <booktitle> Machine Learning, 5 (1990), p. </booktitle> <pages> 71-100. </pages>
Reference-contexts: The following four methods are used to construct new attributes. 1. For every production rule that has more than one condition, use the conjunction of two conditions of the rule as a new attribute. These two conditions are nearest the leaf of the decision tree <ref> [3] </ref>. 2. For every production rule that has more than one condition, use the conjunction of two conditions of the rule as a new attribute. These two conditions are nearest the root of the decision tree. 3.
Reference: 4. <author> G. Pagallo, </author> <title> Adaptive Decision Tree Algorithms for Learning from Examples, </title> <type> PHD thesis, </type> <institution> (University of California at Santa Cruz, </institution> <year> 1990). </year>
Reference-contexts: Pagallo and Haussler's FRINGE [3,4] constructs new attributes by conjoining the parent and grandparent nodes of all positive leaves in the decision tree. It is appropriate for learning DNF expressions. To deal with CNF problems as well, Pagallo implemented SymFringe <ref> [4] </ref> that constructs new attributes by using the conjunctions of the parent and grandparent nodes of all positive and negative leaves in the decision tree. <p> The FRINGE has two approaches to new attribute selection. One is keeping attributes that have been used at least once. Another is keeping attributes that have been used in the last iteration <ref> [4] </ref>. CITRE [2] uses "utility" to select attributes. The "utility" of each new attribute is measured in terms of the information gained by using the attribute to split the entire training set into disjoint subsets.
Reference: 5. <author> J.R. Quinlan, </author> <title> Induction of decision trees, </title> <booktitle> Machine Learning, 1 (1986), p. </booktitle> <pages> 81-106. </pages>
Reference-contexts: 1. Introduction If the training examples are presented in a suitable form, learning classifiers from them can be relatively easy. Selective induction algorithms such as ID3 <ref> [5] </ref> can learn good concepts in this situation. When the attributes used in describing training examples are inappropriate for the concept to be learned, however, learning using only selective induction methods can be difficult. <p> Removing irrelevant primitive attributes usually improves the decision tree. We first briefly describe decision trees and production rules in section 2. The details can be found in <ref> [5] </ref> and [6]. Then, in section 3, we present our constructive induction learning algorithms, and discuss constructing new attributes and distinguishing relevant primitive attributes from irrelevant ones. Section 4 summarizes experimental results of attribute construction on several real domains. <p> Section 5 discusses this approach and its relationship to other attribute construction approaches. Finally, section 6 concludes with areas for further research. 2. Decision Trees and Production Rules We use the selective inductive learning algorithm C4.5 (an extension of ID3) <ref> [5] </ref> as the base of our constructive induction learning algorithms, although any algorithms that constructs decision trees would serve as well. C4.5 accepts a set of examples, a training set, described by a set of attributes and classes.
Reference: 6. <author> J.R. Quinlan, </author> <title> Generating production rules from decision trees, </title> <booktitle> Proceedings of IJCAI (1987), p. </booktitle> <pages> 304-307. </pages>
Reference-contexts: To overcome this problem, a learning system needs to be able to create new attributes that are more appropriate than the primitive ones for the concept to be learned. Our work on constructive induction is based on decision trees and production rules generated from the decision trees <ref> [6] </ref>. We use ^ (logical and) as the operator for attribute construction with the conditions of production rules as candidate operands. <p> Removing irrelevant primitive attributes usually improves the decision tree. We first briefly describe decision trees and production rules in section 2. The details can be found in [5] and <ref> [6] </ref>. Then, in section 3, we present our constructive induction learning algorithms, and discuss constructing new attributes and distinguishing relevant primitive attributes from irrelevant ones. Section 4 summarizes experimental results of attribute construction on several real domains. Section 5 discusses this approach and its relationship to other attribute construction approaches. <p> It produces a learned concept in the form of a decision tree, each leaf of which denotes a class. A decision node denotes a test on an attribute with a subsidiary decision tree for each possible outcome of the test. The algorithm C4.5rules <ref> [6] </ref> is used to generate production rules from decision trees. From a decision tree, a set of production rules are derived based on the same training set used when building the tree.
Reference: 7. <author> D. Yang, L. Rendell, and G. Blix, </author> <title> A scheme for feature construction and a comparison of empirical methods, </title> <booktitle> Proceedings of IJCAI (1991), p. </booktitle> <pages> 699-704. </pages>
Reference-contexts: It is appropriate for learning DNF expressions. To deal with CNF problems as well, Pagallo implemented SymFringe [4] that constructs new attributes by using the conjunctions of the parent and grandparent nodes of all positive and negative leaves in the decision tree. Yang et al <ref> [7] </ref> present an algorithm DCFringe that also conjoins the parent and grandparent nodes of all positive leaves in the decision tree but considers the pattern of tree [7] and uses both ^ and _ as constructive operators. <p> Yang et al <ref> [7] </ref> present an algorithm DCFringe that also conjoins the parent and grandparent nodes of all positive leaves in the decision tree but considers the pattern of tree [7] and uses both ^ and _ as constructive operators. All of these algorithms directly use the decision nodes near the leaves of trees as constructive operands and use pairwise conjunction (DCFringe uses disjunction as well) as the constructive operator.
Reference: 8. <author> P.M. Murphy and M.J. Pazzani, ID2-of-3: </author> <title> Constructive induction of M-of-N concepts for discriminators in decision trees, </title> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <publisher> (Morgan Kaufmann, </publisher> <year> 1991), </year> <pages> p. 183-187. 6 </pages>
Reference-contexts: Like CART, ID2-of-3 <ref> [8] </ref> constructs new attributes from training data directly during building decision trees, but it uses M-of-N concepts instead of Boolean combinations. 6. Conclusion and Future Work We have described the approach of attribute construction on decision trees and production rules.
References-found: 8

