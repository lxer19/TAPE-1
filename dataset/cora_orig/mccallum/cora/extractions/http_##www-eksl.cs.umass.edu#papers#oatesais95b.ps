URL: http://www-eksl.cs.umass.edu/papers/oatesais95b.ps
Refering-URL: http://eksl-www.cs.umass.edu/publications.html
Root-URL: 
Email: foates,gregory,coheng@cs.umass.edu  
Title: Detecting Complex Dependencies in Categorical Data  
Author: Tim Oates, Dawn Gregory, and Paul R. Cohen 
Address: Box 34610 Amherst, MA 01003-4610  
Affiliation: Computer Science Department, LGRC University of Massachusetts  
Abstract: Locating and evaluating relationships among values in multiple streams of data is a difficult and important task. Consider the data flowing from monitors in an intensive care unit. Readings from various subsets of the monitors are indicative and predictive of certain aspects of the patient's state. We present an algorithm that facilitates discovery and assessment of the strength of such predictive relationships called Multi-stream Dependency Detection (msdd). We use heuristic search to guide our exploration of the space of potentially interesting dependencies to uncover those that are significant. We begin by reviewing the dependency detection technique described in [3], and extend it to the multiple stream case, describing in detail our heuristic search over the space of possible dependencies. Quantitative evidence for the utility of our approach is provided through a series of experiments with artificially-generated data. In addition, we present results from the application of our algorithm to two real problem domains: feature-based classification and prediction of pathologies in a simulated shipping network. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bennett, K. P. and Mangasarian, O. L. </author> <title> "Robust linear programming discrimination of two linearly inseparable sets", Optimization Methods and Software 1, 1992, </title> <publisher> 23-34 (Gordon and Breach Science Publishers). </publisher>
Reference-contexts: Medical Center, Long Beach and Cleveland Clinic Foundation. The lymphography data was obtained from the University Medical Centre, Institute of Oncology, Ljubljana, Yugoslavia from M. Zwitter and M. Soklic. The breast cancer data was obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg <ref> [1] </ref>.
Reference: [2] <author> Holte, Robert C. </author> <title> Very simple classification rules perform well on most commonly used datasets. </title> <booktitle> In Machine Learning, </booktitle> <volume> (11), </volume> <pages> pp. 63-91, </pages> <year> 1993. </year>
Reference-contexts: We compared msdd's performance with other published results for each dataset <ref> [2, 6, 7] </ref>. On ten datasets for which we had multiple published results, msdd performance exceeds half of the reported results on six datasets. In no case did it perform badly, and it often performed extremely well. For a more complete comparison, refer to [4].
Reference: [3] <author> Howe, Adele E. </author> <title> Accepting the inevitable: The role of failure recovery in the design of planners. </title> <type> Ph.D. thesis, </type> <institution> Department of Computer Science, University of Massachusetts, Amherst. </institution>
Reference-contexts: In general, if stream j contains t j distinct tokens, there are [ Q n j=1 t j + 1] 2 possible dependencies between two items. The dependency detection technique in <ref> [3] </ref> uses contingency tables to assess the significance of dependencies in a single stream of data. Let (t p ; t s ; ffi) denote a dependency. <p> Conclusion In this paper we described how the problem of finding significant dependencies between the tokens in multiple streams of data can be framed in terms of search. The notion of dependencies between pairs of tokens introduced in <ref> [3] </ref> was extended to pairs of multi-tokens, where a multi-token describes the contents of several streams rather than just one. We introduced the Multi-stream Dependency Detection (MSDD) algorithm that performs a general-to-specific best-first search over the exponentially sized space of possible dependencies between multi-tokens.
Reference: [4] <author> Oates, Tim. </author> <title> MSDD as a Tool for Classification. </title> <type> Memo 94-29, </type> <institution> Experimental Knowledge Systems Laboratory, Department of Computer Science, University of Massachusetts, Amherst, </institution> <year> 1994. </year> <note> Available via the WWW at http://eksl-www.cs.umass.edu/papers/msdd-classification.ps. </note>
Reference-contexts: On ten datasets for which we had multiple published results, msdd performance exceeds half of the reported results on six datasets. In no case did it perform badly, and it often performed extremely well. For a more complete comparison, refer to <ref> [4] </ref>.
Reference: [5] <author> Oates, Tim and Cohen, Paul R. </author> <title> Toward a plan steering agent: experi-ments with schedule maintenance. </title> <booktitle> In Proceedings of the Second International Conference on Artificial Intelligence Planning Systems, </booktitle> <pages> pp. 134-139, </pages> <year> 1994. </year> <note> Also Department of Computer Science Technical Report 94-02, </note> <institution> University of Massachusetts, Amherst. </institution> <note> Available via the WWW at ftp://ftp.cs.umass.edu/pub/eksl/tech-reports/94-02.ps. </note>
Reference: [6] <author> Thrun, </author> <title> S.B. The MONK's problems: A performance comparison of different learning algorithms. </title> <institution> Carnegie Mellon University, CMU-CS-91-197. </institution>
Reference-contexts: The exceptions are NetTalk (training data was generated from a list of the 1000 most common English words, and accuracy was tested on the full 20,008 word corpus), Monks-2 (a single trial with 169 training instances and 432 test instances to facilitate comparison with results contained in <ref> [6] </ref>), and Mushroom (500 training instances and 7624 test instances). We compared msdd's performance with other published results for each dataset [2, 6, 7]. On ten datasets for which we had multiple published results, msdd performance exceeds half of the reported results on six datasets. <p> We compared msdd's performance with other published results for each dataset <ref> [2, 6, 7] </ref>. On ten datasets for which we had multiple published results, msdd performance exceeds half of the reported results on six datasets. In no case did it perform badly, and it often performed extremely well. For a more complete comparison, refer to [4].
Reference: [7] <author> Wirth, J. and Catlett, J. </author> <title> Experiments on the costs and benefits of window-ing in ID3. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> pp. 87-99, </pages> <year> 1988. </year>
Reference-contexts: We compared msdd's performance with other published results for each dataset <ref> [2, 6, 7] </ref>. On ten datasets for which we had multiple published results, msdd performance exceeds half of the reported results on six datasets. In no case did it perform badly, and it often performed extremely well. For a more complete comparison, refer to [4].
Reference: [8] <author> Zheng, Zijian. </author> <title> A benchmark for classifier learning. </title> <institution> Basser Department of Computer Science, University of Sydney, NSW. </institution>
Reference-contexts: Applications 4.1 Feature Based Classification In the interest of generality, we applied msdd to a task for which it was not explicitly designed: feature-based classification. We present results for twelve datasets from the UC Irvine collection. Eleven of those datasets were selected from a list of thirteen presented in <ref> [8] </ref> as being a minimal representative set that covers several important features that distinguish problem domains. The precursor multi-tokens were n-ary feature vectors and the successor "multi-tokens" contained only the class label. These pairs of multi-tokens serve as input to the msdd algorithm.
References-found: 8

