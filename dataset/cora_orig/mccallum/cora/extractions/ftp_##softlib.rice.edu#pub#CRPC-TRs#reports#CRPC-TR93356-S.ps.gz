URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR93356-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Parallelizing Molecular Dynamics using Spatial Decomposition  
Author: Terry W. Clark Reinhard v. Hanxleden J. Andrew McCammon L. Ridgway Scott 
Address: P.O. Box 1892 Houston, TX 77251-1892  
Affiliation: Rice University  
Note: Center for Research on Parallel Computation  
Date: November, 1993  
Pubnum: CRPC-TR93356-S  
Abstract: Revised March, 1994. From the Proceedings of the Scalable High Performance Computing Conference, Knoxville, TN, May 1994. Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR93356-S. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bernard R. Brooks, Robert E. Bruccoleri, Barry D. Olaf-son, David J. States, S. Swaminathan, and Martin Karplus. CHARMM: </author> <title> A program for macromolecular energy, minimization and dynamics calculations. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 4(2) </volume> <pages> 187-217, </pages> <year> 1983. </year>
Reference-contexts: 1 Introduction Molecular dynamics (MD) simulations are useful computational approaches for studying various kinetic, thermodynamic, mechanistic, and structural properties [15]. Molecular dynamics programs tend to be complex, taking many years to write, with frequent modification. There exist several MD programs, like Gromos [9] or Charmm <ref> [1] </ref>, that are well established and are routinely used to solve a broad range of different simulation problems. However, despite the matu fl From the Proceedings of the Scalable High Performance Computing Conference, Knoxville, TN, May 1994.
Reference: [2] <author> Bernard R. Brooks and Milan Hodoscek. </author> <title> Parallelization of CHARMM for MIMD machines. </title> <journal> Chemical Design Automation News, </journal> <volume> 7(12) </volume> <pages> 16-22, </pages> <year> 1992. </year>
Reference-contexts: A number of researchers have already shown that MD is amenable for parallelization <ref> [2, 3, 8, 12, 13, 20] </ref>. However, certain difficulties arise when trying to achieve high efficiencies with large numbers of processors, largely due to the computationally irregular nature of MD codes in general. This paper presents EulerGromos, a paral-lelization of Gromos that focuses on overcoming these scalability problems.
Reference: [3] <author> T. W. Clark and J. A. McCammon. </author> <title> Parallelization of a molecular dynamics non-bonded force algorithm for MIMD architectures. </title> <journal> Computers & Chemistry, </journal> <volume> 14(3) </volume> <pages> 219-224, </pages> <year> 1990. </year>
Reference-contexts: A number of researchers have already shown that MD is amenable for parallelization <ref> [2, 3, 8, 12, 13, 20] </ref>. However, certain difficulties arise when trying to achieve high efficiencies with large numbers of processors, largely due to the computationally irregular nature of MD codes in general. This paper presents EulerGromos, a paral-lelization of Gromos that focuses on overcoming these scalability problems.
Reference: [4] <author> T. W. Clark, J. A. McCammon, and L. R. Scott. </author> <title> Parallel molecular dynamics. </title> <booktitle> In Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 338-344, </pages> <address> Houston, TX, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: We also use our subbox structure to limit our search for nonbonded interaction partners of a given atom, which allows us to avoid the nave O (N 2 ) pairlist generation algorithm <ref> [4, 21] </ref>. For that purpose it is advantageous if box d is an integral fraction of R cut [17]. The hierarchical decomposition should also be able to balance the workload for the trivial case of a system with constant density. <p> Load imbalance is another effect detracting from ideal speedup when decreasing the atom to processor ratio. We did not use load balancing in these runs. 3.5 EulerGromos vs. UHGromos We are also interested in how EulerGromos performs relative to its cousin, UHGromos <ref> [4] </ref>. UH-Gromos is a parallelization of Gromos using the replicated algorithm [4, 21]. The replicated algorithm replicates the full force and coordinate array at each processor. A global sum of the forces is required at every timestep due to a lack of locality [5]. <p> We did not use load balancing in these runs. 3.5 EulerGromos vs. UHGromos We are also interested in how EulerGromos performs relative to its cousin, UHGromos [4]. UH-Gromos is a parallelization of Gromos using the replicated algorithm <ref> [4, 21] </ref>. The replicated algorithm replicates the full force and coordinate array at each processor. A global sum of the forces is required at every timestep due to a lack of locality [5].
Reference: [5] <author> T. W. Clark, R. v. Hanxleden, K. Kennedy, C. Koelbel, and L. R. Scott. </author> <title> Evaluating parallel languages for molecular dynamics computations. </title> <booktitle> In Scalable High Performance Computing Conference, </booktitle> <address> Williamsburg, VA, </address> <year> 1992. </year> <note> Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR992202-S. </note>
Reference-contexts: The calculation of a single timestep involves an iteration over several major phases in the MD program. EulerGromos parallelized all of those major phases, including numerical integration with constraints (or SHAKE <ref> [5] </ref>), pairlist construction, and the computation of non-bonded forces (NBF). Since most MD runs perform the bulk of the work (around 90%) in the NBF routine, this phase is of particular interest. <p> UH-Gromos is a parallelization of Gromos using the replicated algorithm [4, 21]. The replicated algorithm replicates the full force and coordinate array at each processor. A global sum of the forces is required at every timestep due to a lack of locality <ref> [5] </ref>. In Figure 8, the total time for UHGromos simulating myoglobin is less than the EulerGromos time for P &lt; 64, but greater than EulerGromos time for P &gt; 64.
Reference: [6] <author> J. Eastwood, R. Hockney, and D. Lawrence. </author> <title> PM3DP the three dimensional periodic particle-particle/particle-mesh program. </title> <journal> Computer Physics Communications, </journal> <volume> 19 </volume> <pages> 215-261, </pages> <year> 1977. </year>
Reference-contexts: There are n d subboxes along dimension d, resulting in a total on n 1 fl n 2 fl n 3 subboxes. Each sub-box contains a list representation of the atoms resident within its spatial extent <ref> [6] </ref>. To amortize some of the data access overhead, the linked lists are packed densely and in subbox order for linear traversal and better cache locality.
Reference: [7] <author> K. Esselink, B. Smit, and P.A.J. Hilbers. </author> <title> Efficient parallel implementation of molecular dynamics on a toroidal network. Part II. </title> <journal> Multi-particle potentials. Journal of Computational Physics, </journal> <volume> 106 </volume> <pages> 101-107, </pages> <year> 1993. </year>
Reference-contexts: MD programs calculate each NBF interaction only once per pair, instead of twice <ref> [7, 21, 11] </ref>. Our present implementation uses Newton's Third Law within sub-domains, but not across subdomains. However, we are currently implementing a version that exploits New-ton's Third Law across subdomains; our preliminary analysis indicates a potential for significant savings. <p> Esselink, et al., report a geometric decomposition where the subdomain size restriction has been lifted <ref> [7] </ref>. However, they assume a homogeneous distribution of particles with equally sized processor subdomains.
Reference: [8] <author> David Fincham. </author> <title> Parallel computers and molecular simulation. </title> <journal> Molecular Simulation, </journal> <volume> 1 </volume> <pages> 1-45, </pages> <year> 1987. </year>
Reference-contexts: A number of researchers have already shown that MD is amenable for parallelization <ref> [2, 3, 8, 12, 13, 20] </ref>. However, certain difficulties arise when trying to achieve high efficiencies with large numbers of processors, largely due to the computationally irregular nature of MD codes in general. This paper presents EulerGromos, a paral-lelization of Gromos that focuses on overcoming these scalability problems.
Reference: [9] <author> W. F. van Gunsteren and H. J. C. Berendsen. GRO-MOS: </author> <title> GROningen MOlecular Simulation software. </title> <type> Technical report, </type> <institution> Laboratory of Physical Chemistry, University of Groningen, </institution> <address> Nijenborgh, The Netherlands, </address> <year> 1988. </year>
Reference-contexts: 1 Introduction Molecular dynamics (MD) simulations are useful computational approaches for studying various kinetic, thermodynamic, mechanistic, and structural properties [15]. Molecular dynamics programs tend to be complex, taking many years to write, with frequent modification. There exist several MD programs, like Gromos <ref> [9] </ref> or Charmm [1], that are well established and are routinely used to solve a broad range of different simulation problems. However, despite the matu fl From the Proceedings of the Scalable High Performance Computing Conference, Knoxville, TN, May 1994.
Reference: [10] <author> R. v. Hanxleden and L. R. Scott. </author> <title> Load balancing on message passing architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13 </volume> <pages> 312-324, </pages> <year> 1991. </year>
Reference-contexts: This is done hierarchically: first, subdo-main boundaries are shifted between slices, followed by column shifts within slices and finally shifts within columns. Figure 3 shows a possible hierarchical decomposition. A smoothing factor is applied to avoid boundary oscillations <ref> [10] </ref>. 2.4 Communication EulerGromos communicates off-processor data accesses in one of two possible ways: Point-to-Point or Shift.
Reference: [11] <author> Fredrik Hedman and Aatto Laaksonen. </author> <title> Data parallel large-scale molecular dynamics for liquids. </title> <journal> International Journal of Quantum Chemistry, </journal> <volume> 46 </volume> <pages> 27-38, </pages> <year> 1993. </year>
Reference-contexts: MD programs calculate each NBF interaction only once per pair, instead of twice <ref> [7, 21, 11] </ref>. Our present implementation uses Newton's Third Law within sub-domains, but not across subdomains. However, we are currently implementing a version that exploits New-ton's Third Law across subdomains; our preliminary analysis indicates a potential for significant savings. <p> of the replicated and spatial approaches, for example, the shake algorithm, are beyond the scope of our discussion. 4 Related Work A common feature of spatial-mapping approaches is to restrict communication to nearest neighbors; consequently, each processor subdomain has to be greater than or equal to the cutoff radius size <ref> [11, 12, 17, 21] </ref>, which in turn limits scalability. Esselink, et al., report a geometric decomposition where the subdomain size restriction has been lifted [7]. However, they assume a homogeneous distribution of particles with equally sized processor subdomains.
Reference: [12] <author> Tsutomu Hoshino and Kiyo Takenouchi. </author> <title> Processing of the molecular dynamics model by the parallel computer PAX. </title> <journal> Computer Physics Communications, </journal> <volume> 31(4), </volume> <year> 1984. </year>
Reference-contexts: A number of researchers have already shown that MD is amenable for parallelization <ref> [2, 3, 8, 12, 13, 20] </ref>. However, certain difficulties arise when trying to achieve high efficiencies with large numbers of processors, largely due to the computationally irregular nature of MD codes in general. This paper presents EulerGromos, a paral-lelization of Gromos that focuses on overcoming these scalability problems. <p> of the replicated and spatial approaches, for example, the shake algorithm, are beyond the scope of our discussion. 4 Related Work A common feature of spatial-mapping approaches is to restrict communication to nearest neighbors; consequently, each processor subdomain has to be greater than or equal to the cutoff radius size <ref> [11, 12, 17, 21] </ref>, which in turn limits scalability. Esselink, et al., report a geometric decomposition where the subdomain size restriction has been lifted [7]. However, they assume a homogeneous distribution of particles with equally sized processor subdomains.
Reference: [13] <author> S. L. Lin, J. Mellor-Crummey, B. M. Pettitt, and G. N. Phillips, Jr. </author> <title> Molecular dynamics on a distributed-memory multiprocessor. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 13(8) </volume> <pages> 1022-1035, </pages> <year> 1992. </year>
Reference-contexts: A number of researchers have already shown that MD is amenable for parallelization <ref> [2, 3, 8, 12, 13, 20] </ref>. However, certain difficulties arise when trying to achieve high efficiencies with large numbers of processors, largely due to the computationally irregular nature of MD codes in general. This paper presents EulerGromos, a paral-lelization of Gromos that focuses on overcoming these scalability problems.
Reference: [14] <author> Richard J. Loncharich and Bernard R. Brooks. </author> <title> The effects of truncating long-range forces on protein dynamics. PROTEINS: Structure, Function and Genetics, </title> <booktitle> 6 </booktitle> <pages> 32-45, </pages> <year> 1989. </year>
Reference-contexts: Morales and Nuevo decrease the processor subdomain size such that it is less than R cut , but they continue to restrict interactions to neighboring subdomains only, thereby effectively reducing R cut ; they evaluate the effect on thermodynamic properties [16]. (See <ref> [14] </ref> for a detailed study on cutoff radius effects.) Plimpton also allows for subdomains smaller than R cut in an implementation reported for Lennard-Jones particles that is similar to our blockwise decomposition with the Shift algorithm [18]. 5 Summary This paper gave a description of EulerGromos, a parallel molecular dynamics program,
Reference: [15] <author> J. A. McCammon and Stephen C. Harvey. </author> <title> Dynamics of proteins and nucleic acids. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1987. </year>
Reference-contexts: 1 Introduction Molecular dynamics (MD) simulations are useful computational approaches for studying various kinetic, thermodynamic, mechanistic, and structural properties <ref> [15] </ref>. Molecular dynamics programs tend to be complex, taking many years to write, with frequent modification. There exist several MD programs, like Gromos [9] or Charmm [1], that are well established and are routinely used to solve a broad range of different simulation problems.
Reference: [16] <author> Juan J. Morales and Maris J. Nuevo. </author> <title> A technique for improving the link-cell method. </title> <journal> Computer Physics Communications, </journal> <volume> 60 </volume> <pages> 195-199, </pages> <year> 1990. </year>
Reference-contexts: Morales and Nuevo decrease the processor subdomain size such that it is less than R cut , but they continue to restrict interactions to neighboring subdomains only, thereby effectively reducing R cut ; they evaluate the effect on thermodynamic properties <ref> [16] </ref>. (See [14] for a detailed study on cutoff radius effects.) Plimpton also allows for subdomains smaller than R cut in an implementation reported for Lennard-Jones particles that is similar to our blockwise decomposition with the Shift algorithm [18]. 5 Summary This paper gave a description of EulerGromos, a parallel molecular
Reference: [17] <author> M. R. S. Pinches and D. J. Tildesley. </author> <title> Large scale molecular dynamics on parallel computers using the link-cell algorithm. </title> <journal> Molecular Simulation, </journal> <volume> 6 </volume> <pages> 51-87, </pages> <year> 1991. </year>
Reference-contexts: For that purpose it is advantageous if box d is an integral fraction of R cut <ref> [17] </ref>. The hierarchical decomposition should also be able to balance the workload for the trivial case of a system with constant density. Therefore it must be possible to create subdomains of equal size; for all d, n d should be a multiple of p d . <p> of the replicated and spatial approaches, for example, the shake algorithm, are beyond the scope of our discussion. 4 Related Work A common feature of spatial-mapping approaches is to restrict communication to nearest neighbors; consequently, each processor subdomain has to be greater than or equal to the cutoff radius size <ref> [11, 12, 17, 21] </ref>, which in turn limits scalability. Esselink, et al., report a geometric decomposition where the subdomain size restriction has been lifted [7]. However, they assume a homogeneous distribution of particles with equally sized processor subdomains.
Reference: [18] <author> Steve Plimpton. </author> <title> Fast parallel algorithms for short-range molecular dynamics. </title> <type> Technical Report SAND91-1144, </type> <institution> Sandia National Laboratories, </institution> <address> Albuquerque, New Mexico 87185, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: effectively reducing R cut ; they evaluate the effect on thermodynamic properties [16]. (See [14] for a detailed study on cutoff radius effects.) Plimpton also allows for subdomains smaller than R cut in an implementation reported for Lennard-Jones particles that is similar to our blockwise decomposition with the Shift algorithm <ref> [18] </ref>. 5 Summary This paper gave a description of EulerGromos, a parallel molecular dynamics program, and evaluated its performance characteristics. The measurements reported here indicate that the main design goal, scalability, has been achieved.
Reference: [19] <author> A. Rahman. </author> <title> Correlations in the motion of atoms in liquid argon. </title> <journal> Physical Review, </journal> <volume> 136(2A):405-411, </volume> <year> 1964. </year>
Reference-contexts: Since most MD runs perform the bulk of the work (around 90%) in the NBF routine, this phase is of particular interest. A common technique to accelerate the NBF calculation is to ignore all NBF interactions beyond a certain cutoff radius, R cut <ref> [19] </ref>. This in turn provides an access locality that makes Eulerian, application space oriented data distributions desirable. An Eulerian, or geometric, decomposition assigns application space to processors, as compared to Lagrangian mappings where particles are assigned to processors.
Reference: [20] <author> D. C. Rapaport. </author> <title> Multi-million particle molecular dynamics, II. Design considerations for distributed processing. </title> <journal> Computer Physics Communications, </journal> <volume> 62 </volume> <pages> 217-228, </pages> <year> 1991. </year>
Reference-contexts: A number of researchers have already shown that MD is amenable for parallelization <ref> [2, 3, 8, 12, 13, 20] </ref>. However, certain difficulties arise when trying to achieve high efficiencies with large numbers of processors, largely due to the computationally irregular nature of MD codes in general. This paper presents EulerGromos, a paral-lelization of Gromos that focuses on overcoming these scalability problems.
Reference: [21] <author> W. Smith. </author> <title> Molecular dynamics on hypercube parallel computers. </title> <journal> Computer Physics Communications, </journal> <volume> 62 </volume> <pages> 229-248, </pages> <year> 1991. </year>
Reference-contexts: We also use our subbox structure to limit our search for nonbonded interaction partners of a given atom, which allows us to avoid the nave O (N 2 ) pairlist generation algorithm <ref> [4, 21] </ref>. For that purpose it is advantageous if box d is an integral fraction of R cut [17]. The hierarchical decomposition should also be able to balance the workload for the trivial case of a system with constant density. <p> MD programs calculate each NBF interaction only once per pair, instead of twice <ref> [7, 21, 11] </ref>. Our present implementation uses Newton's Third Law within sub-domains, but not across subdomains. However, we are currently implementing a version that exploits New-ton's Third Law across subdomains; our preliminary analysis indicates a potential for significant savings. <p> We did not use load balancing in these runs. 3.5 EulerGromos vs. UHGromos We are also interested in how EulerGromos performs relative to its cousin, UHGromos [4]. UH-Gromos is a parallelization of Gromos using the replicated algorithm <ref> [4, 21] </ref>. The replicated algorithm replicates the full force and coordinate array at each processor. A global sum of the forces is required at every timestep due to a lack of locality [5]. <p> of the replicated and spatial approaches, for example, the shake algorithm, are beyond the scope of our discussion. 4 Related Work A common feature of spatial-mapping approaches is to restrict communication to nearest neighbors; consequently, each processor subdomain has to be greater than or equal to the cutoff radius size <ref> [11, 12, 17, 21] </ref>, which in turn limits scalability. Esselink, et al., report a geometric decomposition where the subdomain size restriction has been lifted [7]. However, they assume a homogeneous distribution of particles with equally sized processor subdomains.
References-found: 21

