URL: http://www.cs.indiana.edu/hyplan/leake/papers/p-92-02.ps.Z
Refering-URL: http://www.cs.indiana.edu/hyplan/leake/papers/INDEX.html
Root-URL: 
Email: leake@cs.indiana.edu  
Title: Using Goals and Experience to Guide Abduction  
Author: David B. Leake 
Address: Lindley Hall 215, Bloomington, IN 47405  
Affiliation: Department of Computer Science, Indiana University  
Pubnum: Technical Report #359  
Abstract: Standard methods for abductive understanding are neutral to prior experience and current goals. Candidate explanations are built from scratch by backwards chaining, without considering how similar situations were previously explained, and selection of the candidate to accept is based on its likelihood, without considering the information needs beyond routine understanding. Problems arise when applying these methods to everyday understanding: The vast range of possible explanations makes it difficult to control the cost of explanation construction and to assure that the explanations generated will actually be useful. We argue that these problems can be overcome by using goals and experience to guide both explanation generation and evaluation. Our work is within the framework of case-based explanation, which builds explanations by retrieving and adapting prior explanations stored in memory. We substantiate our model by describing mechanisms that enable it to effectively generate good explanations. First, we demonstrate that there exists a theory of anomaly and explanation that can guide retrieval of relevant explanations. Second, we present a plausibility evaluation process that efficiently detects conflicts and confirmations of an explanation's assumptions by prior patterns, making it possible to focus explanation adaptation when retrieved explanations are implausible. Third, we present methods for judging whether explanations provide the information needed to satisfy explainer goals beyond routine understanding. By reflecting experience and goals in the search for explanations, case-based explanation provides a practical mechanism for guiding search towards explanations that are both plausible and useful. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bareiss, R. (Ed.)., </author> <title> Bareiss (1991). </title> <booktitle> Proceedings of the Case-Based Reasoning Workshop, </booktitle> <address> Palo Alto. </address> <publisher> DARPA, Morgan Kaufmann, Inc. </publisher>
Reference: <author> Charniak, E. </author> <year> (1978). </year> <title> On the use of framed knowledge in language comprehension. </title> <journal> Artificial Intelligence, </journal> <volume> 11 (3), </volume> <pages> 225-265. </pages>
Reference: <author> Charniak, E. </author> <year> (1986). </year> <title> A neat theory of marker passing. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 584-588 Philadelphia, PA. </address> <publisher> AAAI. </publisher>
Reference: <author> Charniak, E. & Goldman, R. </author> <year> (1991). </year> <title> A probabilistic model of plan recognition. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 160-165 Anaheim, CA. </address> <publisher> AAAI. </publisher>
Reference: <author> Collins, G. & Birnbaum, L. </author> <year> (1988). </year> <title> An explanation-based approach to the transfer of planning knowledge across domains. </title> <booktitle> In Proceedings of the 1988 AAAI Spring Symposium on Explanation-based Learning Stanford, </booktitle> <address> CA. </address> <publisher> AAAI. </publisher>
Reference: <author> Cress, D. </author> <year> (1984). </year> <editor> Clot suspected in swale's death. </editor> <address> The Washington Post, E1. </address> <month> June 19. </month>
Reference-contexts: However, human explainers had little trouble generating plausible hypotheses, and prior experiences were often cited as the reasons for the hypotheses proposed. For example, one veterinarian's immediate reaction to the news was "this sounds like an aneurysm. I've seen this sort of thing before" <ref> (Cress, 1984) </ref>. A student hypothesized a heart attack after being reminded of the death of the runner Jim Fixx, who died when the exertion of recreational jogging overtaxed a hereditary heart defect.
Reference: <author> Cullingford, R. </author> <year> (1978). </year> <title> Script Application: Computer Understanding of Newspaper Stories. </title> <type> Ph.D. thesis, </type> <institution> Yale University. Computer Science Department Technical Report 116. </institution>
Reference: <author> DeJong, G. & Mooney, R. </author> <year> (1986). </year> <title> Explanation-based learning: an alternative view. </title> <journal> Machine Learning, </journal> <volume> 1 (1), </volume> <pages> 145-176. </pages>
Reference: <author> DeKleer, J. & Williams, B. </author> <year> (1989). </year> <title> Diganosis with behavioral modes. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 1324-1330 Detroit, MI. IJCAI. </address>
Reference: <author> Hammond, K. </author> <year> (1987). </year> <title> Learning and reusing explanations. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <pages> pp. </pages> <address> 141-147 Irvine, CA. </address> <booktitle> Machine Learning, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A chaining-based explainer could take retrieved explanations as its starting point for additional backwards chaining, in that way avoiding some of the inference otherwise needed <ref> (Hammond, 1987) </ref>. In addition, our evaluation process could be used to replace or augment standard minimality criteria for choosing between candidate explanations. However, the methods presented here also facilitate application of more sophisticated strategies that use the evaluation phase to direct incremental explanation construction. <p> Using the characterization to suggest abstract explanation strategies: No explainer can hope to retrieve an appropriate prior explanation for every new anomaly. However, the previously described anomaly vocabulary can also facilitate explanation from scratch by organizing abstract explanation strategies <ref> (Hammond, 1987) </ref> in addition to specific prior explanations. 4 Explanation strategies provide information about types of factors that are particularly likely to be relevant when explaining a particular type of anomaly; that information is then used to guide the search for situation-specific information needed to complete the explanation.
Reference: <author> Hammond, K. (Ed.). </author> <year> (1989). </year> <booktitle> Proceedings of the Case-Based Reasoning Workshop. </booktitle> <publisher> Morgan Kauf-mann, Inc., </publisher> <address> San Mateo. </address>
Reference: <author> Harman, G. </author> <year> (1965). </year> <title> The inference to the best explanation. </title> <journal> Philosophical Review, </journal> <volume> 74, </volume> <pages> 88-95. </pages>
Reference-contexts: 1 Introduction Abduction is often described as "inference to the best explanation" <ref> (Harman, 1965) </ref>. This description suggests two fundamental issues facing abductive reasoning systems: performing the inference to generate candidate explanations and selecting which of the resulting candidates to accept. In the growing body of research on automated abduction, standard methods have emerged for addressing both of these issues.
Reference: <author> Hobbs, J., Stickel, M., Appelt, D., & Martin, P. </author> <year> (1990). </year> <title> Interpretation as abduction. </title> <type> Tech. rep. 499, </type> <institution> SRI International. </institution>
Reference-contexts: Other methods concentrate on constraining the chaining process, by means such as combining of top-down and bottom-up processing (Wilensky, 1983), limiting the amount of chaining allowed (Mooney, 1990), heuristics to limit the branching factor of search <ref> (Hobbs, Stickel, Appelt, & Martin, 1990) </ref>, and using marker-passing to propose candidate paths (Charniak, 1986; Norvig, 1989). Case-based explanation is a third alternative.
Reference: <author> Josephson, J. </author> <year> (1991). </year> <title> Abduction: conceptual analysis of a fundamental pattern of inference. </title> <type> Technical Research Report 91-JJ-DRAFT, </type> <institution> Laboratory for Artificial Intelligence Research, The Ohio State University. </institution>
Reference-contexts: competing candidates when deciding if an explanation is sufficiently plausible, but the certainty with which people accept a given explanation depends not only on the plausibility of the explanation in isolation but on the plausibility of competing alternatives and on reasoning about the completeness of the set of alternatives considered <ref> (Josephson, 1991) </ref>. 5.2 Beyond Validity: Judging Explanations' Usefulness 5.2.1 Judging Relevance to Anomalies Regardless of an explanation's validity, it will serve no purpose unless it provides useful information. Consequently, explanation evaluation must reflect explainer needs. Anomalies show that an understander's world model is flawed.
Reference: <author> Kahneman, D., Slovic, P., & Tversky, A. </author> <year> (1982). </year> <title> Judgement under uncertainty: Heuristics and biases. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge. </address>
Reference-contexts: To make verification practical in rich domains, ways are needed to achieve a reasonable level of verification without incurring this overwhelming cost. When people have no specific knowledge about whether a fact is true, they estimate its likelihood by comparing the hypothesis to standard stereotypes. For example, <ref> (Kahneman, Slovic, & Tversky, 1982) </ref> shows that people use stereotypes to decide if it is reasonable for someone to have a given profession.
Reference: <author> Kass, A. </author> <year> (1986). </year> <title> Modifying explanations to understand stories. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society Amherst, </booktitle> <address> MA. </address> <publisher> Cognitive Science Society. </publisher>
Reference: <author> Kass, A. </author> <year> (1990). </year> <title> Developing Creative Hypotheses by Adapating Explanations. </title> <type> Ph.D. thesis, </type> <institution> Yale University. Northwestern University Institute for the Learning Sciences, </institution> <type> Technical Report 6. </type> <note> 29 Kass, </note> <author> A. & Leake, D. </author> <year> (1988). </year> <title> Case-based reasoning applied to constructing explanations. </title> <editor> In Kolod--ner, J. (Ed.), </editor> <booktitle> Proceedings of the Case-Based Reasoning Workshop, </booktitle> <pages> pp. </pages> <address> 190-208 Palo Alto. </address> <publisher> DARPA, Morgan Kaufmann, Inc. </publisher>
Reference-contexts: The ability to retrieve relevant explanations from other domains, combined with the ability to adapt prior explanations to new situations, provides flexibility unavailable in schema-based approaches while maintaining efficiency <ref> (Kass, 1990) </ref>. In addition, as the system's library of explanations grows, it learns new cases that allow it to draw on a wider range of stored explanations and increasing the likelihood of having a relevant explanation in memory. <p> ACCEPTER was initially developed as the main module of SWALE; the current version is a stand-alone system in which a human user replaces SWALE's adaptation component (that component has also been investigated in an independent system; see <ref> (Kass, 1990) </ref>). ACCEPTER processes about 20 simple (1-4 line) stories about anomalous events, primarily stories from the news, and evaluates the goodness of a total of about 30 candidate explanations from the perspectives of five types of overarching goals.
Reference: <author> Kautz, H. & Allen, J. </author> <year> (1986). </year> <title> Generalized plan recognition. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 32-37 Philadelphia, PA. </address> <publisher> AAAI. </publisher>
Reference: <author> Kedar-Cabelli, S. </author> <year> (1987). </year> <title> Formulating concepts according to purpose. </title> <booktitle> In Proceedings of the Sixth Annual National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 477-481 Seattle, WA. </address> <publisher> AAAI. </publisher>
Reference: <author> Keller, R. </author> <year> (1987). </year> <title> The Role of Explicit Contextual Knowledge in Learning Concepts to Improve Performance. </title> <type> Ph.D. thesis, </type> <institution> Rutgers University. Computer Science Department Technical Report ML-TR-7. </institution>
Reference: <author> Keller, R. </author> <year> (1988). </year> <title> Defining operationality for explanation-based learning. </title> <journal> Artificial Intelligence, </journal> <volume> 35 (2), </volume> <pages> 227-241. </pages>
Reference-contexts: However, abductive understanding systems take a goal-neutral view. Artificial Intelligence investigation of goal-based explanation evaluation has been concentrated in explanation-based learning (EBL) research (Mitchell, Keller, & Kedar-Cabelli, 1986; DeJong & Mooney, 1986), and focuses on the task of concept recognition (see <ref> (Keller, 1988) </ref> for an overview of some of this work). Rich models of usefulness have been developed for the recognition task (e.g., (Keller, 1987; Minton, 1988)), and other research has considered how the recognition task is motivated by other goals (Kedar-Cabelli, 1987), but other tasks have received little attention.
Reference: <author> Kolodner, J. (Ed.). </author> <year> (1988). </year> <booktitle> Proceedings of the Case-Based Reasoning Workshop. </booktitle> <publisher> Morgan Kaufmann, Inc., </publisher> <address> Palo Alto. </address>
Reference: <author> Krulwich, B., Birnbaum, L., & Collins, G. </author> <year> (1990). </year> <title> Goal-directed diagnosis of expectation failures. </title> <editor> In O'Rorke, P. (Ed.), </editor> <booktitle> Working Notes of the 1990 Spring Symposium on Automated Abduction, </booktitle> <pages> pp. 116-119. </pages> <institution> AAAI. </institution> <type> Technical Report 90-32, </type> <institution> Department of Information and Computer Science, University of California, Irvine. </institution>
Reference: <author> Lalljee, M. & Abelson, R. </author> <year> (1983). </year> <title> The organization of explanations. </title> <editor> In Hewstone, M. (Ed.), </editor> <title> Attribution Theory: Social and Functional Extensions. </title> <publisher> Blackwell, Oxford. </publisher>
Reference: <author> Leake, D. </author> <year> (1988). </year> <title> Evaluating explanations. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 251-255 Minneapolis, MN. </address> <publisher> AAAI, Morgan Kaufmann Publishers, Inc. </publisher>
Reference: <author> Leake, D. </author> <year> (1990). </year> <title> Task-based criteria for judging explanations. </title> <booktitle> In Proceedings of the Twelfth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp. </pages> <address> 325-332 Cambridge, MA. </address> <publisher> Cognitive Science Society. </publisher>
Reference: <author> Leake, D. </author> <year> (1992). </year> <title> Evaluating Explanations: A Content Theory. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: Reflecting explainer needs in explanation evaluation depends on identifying the goals that can drive explanation, and we describe our theory of goal-based influences in the later part of the chapter. The theory described here is implemented in the testbed system ACCEPTER <ref> (Leake, 1992) </ref>, a story understanding program that detects anomalous events in the stories it processes, characterizes the anomalies to facilitate explanation retrieval and adaptation, and evaluates candidate explanations for a range of tasks. 2 Issues in Everyday Abduction Before discussing specifics of our model, we illustrate some of the general issues <p> Although the stand-alone version of ACCEPTER is not a learning system, the version of ACCEPTER in SWALE stores new explanations in memory, making them available to facilitate future explanation of similar anomalies. A complete description of ACCEPTER's implementation is available in <ref> (Leake, 1992) </ref>; here we concentrate on the underlying theory. 4 Focusing Explanation Retrieval If a case-based explanation system can retrieve relevant prior explanations when it encounters a new situation, it can facilitate the explanation process by re-using the applicable portions of prior experiences. <p> The components of this structure direct retrieval toward unusual plans that the actor (or similar actors) selected in similar circumstances (e.g., that another thief switched to torches because crowbars are hard to maneuver in cramped ATM enclosures). For more details, see <ref> (Leake, 1992) </ref>. Using the characterization to guide explanation retrieval: In ACCEPTER's explanation memory, explanations are organized by the anomaly vocabulary: the primary index for explanation retrieval is the vocabulary element for the anomaly it explains. <p> In fact, <ref> (Leake, 1992) </ref> shows that for each subcategory, there does exist a distinct set of relevant explanation strategies. (Some of the main anomaly categories also organize their own strategies, disjoint from the strategies for other main categories, and applicable to their subcategories as well.) The existence of these sets of explanation strategies <p> In <ref> (Leake, 1992) </ref> we survey the types of anomalies that this method can and cannot detect and substantiate its coverage by presenting a family of patterns that covers a wide range of everyday anomalies. 5.1.2 Accepting explanations based on plausibility Based on the plausibility problems it finds, ACCEPTER does a simple scoring <p> To complete the evaluation process, heuristics are needed for judging along each of the evaluation dimensions (e.g., for judging how likely the system is to be aware of future occurrence of similar factors). Evaluation dimensions and heuristics for judging them are discussed further in <ref> (Leake, 1992) </ref>.
Reference: <author> Leake, D. & Owens, C. </author> <year> (1986). </year> <title> Organizing memory for explanation. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp. </pages> <address> 710-715 Amherst, MA. </address> <publisher> Cognitive Science Society. </publisher>
Reference: <author> Mackie, J. </author> <year> (1965). </year> <title> Causes and conditions. </title> <journal> American Philosophical Quarterly, </journal> <volume> 2 (4), </volume> <pages> 245-264. </pages>
Reference: <author> McDermott, D. </author> <year> (1982). </year> <title> A temporal logic for reasoning about processes and plans. </title> <journal> Cognitive Science, </journal> <volume> 6, </volume> <pages> 101-155. </pages>
Reference-contexts: The category for such anomalies is UNUSUAL-OBJECT-FEATURE. 10 * Generalizations about persistence of features <ref> (McDermott, 1982) </ref>: Successful theft from an ATM would be surprising if we believed that the machine contained no money, and would not be refilled until the next day. The anomaly of money being in the ATM is STRANGE-FEATURE-CHANGE.
Reference: <author> Minsky, M. </author> <year> (1975). </year> <title> A framework for representing knowledge. </title> <editor> In Winston, P. (Ed.), </editor> <booktitle> The Psychology of Computer Vision, chap. </booktitle> <volume> 6, </volume> <pages> pp. 211-277. </pages> <publisher> McGraw-Hill, </publisher> <address> New York. </address>
Reference: <author> Minton, S. </author> <year> (1988). </year> <title> Learning Search Control Knowledge: An Explanation-Based Approach. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston. </address>
Reference: <author> Mitchell, T., Keller, R., & Kedar-Cabelli, S. </author> <year> (1986). </year> <title> Explanation-based generalization: a unifying view. </title> <journal> Machine Learning, </journal> <volume> 1 (1), </volume> <pages> 47-80. </pages> <note> 30 Mooney, </note> <author> R. </author> <year> (1990). </year> <title> A General Explanation-based Learning Mechanism and its Application to Nar--rative Understanding. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo. </address>
Reference: <author> Ng, H. & Mooney, R. </author> <year> (1990). </year> <title> On the role of coherence in abductive explanation. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 337-342 Boston, MA. </address> <publisher> AAAI. </publisher>
Reference: <author> Norvig, P. </author> <year> (1989). </year> <title> Marker passing as a weak method for text inferencing. </title> <journal> Cognitive Science, </journal> <volume> 13 (4), </volume> <pages> 569-620. </pages>
Reference: <author> O'Rorke, P. </author> <year> (1983). </year> <title> Reasons for beliefs in understanding: applications of non-monotonic dependencies to story processing. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence Washington, </booktitle> <address> DC. </address>
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference: <author> Peng, Y. & Reggia, J. </author> <year> (1990). </year> <title> Abductive Inference Models for Diagnostic Problem Solving. </title> <publisher> Springer Verlag, </publisher> <address> New York. </address>
Reference: <author> Ram, A. </author> <year> (1990). </year> <title> Goal-based explanation. </title> <editor> In O'Rorke, P. (Ed.), </editor> <booktitle> Working Notes of the 1990 Spring Symposium on Automated Abduction, </booktitle> <pages> pp. 26-29. </pages> <institution> AAAI. </institution> <type> Technical Report 90-32, </type> <institution> Department of Information and Computer Science, University of California, Irvine. </institution>
Reference-contexts: Choosing the better of these two explanations is impossible without knowing the purpose of the explanation effort. Information needs can be triggered by goals in the world and by knowledge goals <ref> (Ram, 1990) </ref> to change an understander's internal knowledge state. Information needs determine the types of antecedents and rules that must be included in a good explanation, and a goal-driven explainer must find explanations including these factors.
Reference: <author> Ram, A. & Leake, D. </author> <year> (1991). </year> <title> Evaluation of explanatory hypotheses. </title> <booktitle> In Proceedings of the Thirteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp. </pages> <address> 867-871 Chicago, IL. </address> <publisher> Cognitive Science Society. </publisher>
Reference: <author> Read, S. & Cesa, I. </author> <year> (1991). </year> <title> This reminds me of the time when : : : : expectation failures in reminding and explanation. </title> <journal> Journal of Experimental Social Psychology, </journal> <volume> 27, </volume> <pages> 1-25. </pages>
Reference-contexts: Psychological experiments provide support for the view that people use a reminding process to generate explanations, and that they give increased plausibility ratings for explanations suggested by remindings <ref> (Read & Cesa, 1991) </ref>. In addition to being applicable when very similar situations have been explained before, case-based explanation can apply in novel situations. With appropriate indexing of stored explanations, it can suggest relevant explanations even in situations quite different from those previously explained.
Reference: <author> Rieger, C. </author> <year> (1975). </year> <title> Conceptual memory and inference. In Conceptual Information Processing. </title> <publisher> North-Holland, Amsterdam. </publisher>
Reference-contexts: It is well known that arbitrary amounts of inference may be needed to derive the relationship between new information and any given piece of background knowledge <ref> (Rieger, 1975) </ref>. Identifying all relevant relationships between beliefs requires checking for matches and conflicts between all ramifications of new information and prior beliefs. To make verification practical in rich domains, ways are needed to achieve a reasonable level of verification without incurring this overwhelming cost.
Reference: <author> Schank, R. </author> <year> (1986). </year> <title> Explanation Patterns: Understanding Mechanically and Creatively. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: SWALE's explanations are stored in memory as explanation patterns (XPs) <ref> (Schank, 1986) </ref>. XPs are dependency networks tracing how a state or event can be inferred from a set of assumptions according to plausible inference rules; they represent a variablized version of the explanatory chain from a prior explanation.
Reference: <author> Schank, R. & Abelson, R. </author> <year> (1977). </year> <title> Scripts, Plans, Goals and Understanding. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> Schank, R. & Leake, D. </author> <year> (1989). </year> <title> Creativity and learning in a case-based explainer. </title> <journal> Artificial Intelligence, </journal> <volume> 40 (1-3), </volume> <pages> 353-385. </pages> <note> Also in Carbonell, </note> <editor> J., editor, </editor> <title> Machine Learning: Paradigms and Methods, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference: <author> Snyder, C., Higgens, R., & Stucky, R. </author> <year> (1983). </year> <title> Excuses: Masquerades in Search of Grace. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference: <author> Thagard, P. </author> <year> (1989). </year> <title> Explanatory coherence. </title> <journal> The Behavioral and Brain Sciences, </journal> <volume> 12 (3), </volume> <pages> 435-502. </pages>
Reference: <author> Van Fraassen, B. </author> <year> (1980). </year> <title> The Scientific Image, </title> <journal> chap. </journal> <volume> 5. </volume> <publisher> Clarendon Press, Oxford. </publisher>
Reference: <author> Wilensky, R. </author> <year> (1983). </year> <title> Planning and Understanding. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address> <month> 31 </month>
Reference-contexts: To accommodate novelty, this method can be augmented with a chaining process to allow the system to explain events outside of its schemas and to generate new schemas (Mooney, 1990). Other methods concentrate on constraining the chaining process, by means such as combining of top-down and bottom-up processing <ref> (Wilensky, 1983) </ref>, limiting the amount of chaining allowed (Mooney, 1990), heuristics to limit the branching factor of search (Hobbs, Stickel, Appelt, & Martin, 1990), and using marker-passing to propose candidate paths (Charniak, 1986; Norvig, 1989). Case-based explanation is a third alternative.
References-found: 49

