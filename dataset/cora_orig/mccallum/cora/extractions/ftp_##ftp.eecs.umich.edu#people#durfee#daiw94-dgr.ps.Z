URL: ftp://ftp.eecs.umich.edu/people/durfee/daiw94-dgr.ps.Z
Refering-URL: http://ai.eecs.umich.edu/people/durfee/vita.html
Root-URL: http://www.cs.umich.edu
Email: durfee@umich.edu  piotr@ucrengr.ucr.edu  jeff@cs.huji.ac.il  
Title: The Utility of Embedded Communications: Toward the Emergence of Protocols  
Author: Edmund H. Durfee Piotr Gmytrasiewicz Jeffrey S. Rosenschein 
Address: Ann Arbor, MI 48109  Riverside, CA  Jerusalem, ISRAEL  
Affiliation: EECS Department University of Michigan  Computer Science Dept. University of California-Riverside  Computer Science Dept. Hebrew University  
Abstract: A fundamental feature of effective distributed systems is that the entities comprising the system have some set of guidelines|some plan to follow|that leads them into making good decisions about what to communicate and when. Traditionally, these protocols for communication have been given to the entities at the time that they are designed. For example, knowledge-based entities (agents) have been designed with protocols that allow them to make deals, allocate tasks, negotiate over solutions, and so on. Such distributed systems, however, will be brittle if the agents ever need to go beyond the pre-existing protocol. To constitute a robust system, the agents would benefit from the ability to discover new ways of communicating, and to generalize these into new protocols. This paper extends the recursive modeling method to address issues of embedded communications| communications occurring in a larger context of other physical and/or communicative activities, and describes how behaviors like question-answering and order-following could emerge as rational consequences of agents' decisionmaking. These types of embedded communicative acts can form the building blocks of more complex protocols, given that agents can not only derive these embedded communicative acts but can generalize and reuse them appropriately. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Robert J. Aumann and Sergiu Hart. </author> <title> Repetition as a paradigm for cooperation in games of incomplete information. </title> <type> Technical report, </type> <institution> Hebrew University, </institution> <year> 1981. </year>
Reference-contexts: The new structure that reflects how R 1 's knowledge would look after sending M 1 can be solved easily, but now R 1 would be sure that R 2 would observe from point P2, taking action a 2 2 : p R 2 = <ref> [0; 1; 0] </ref>. <p> Assuming that R 1 now expects R 2 to be extremely likely to see P2 (say, with probability .99), it computes R 2 's intentional probabilities as: :01 fi <ref> [0; 0; 1] </ref> + :99 fi [0; 1; 0] = [0; :99; :01]: In turn, this gives R 1 expected utilities of 4.96 for a 1 1 , 2 , and 3.96 for a 1 As detailed in [8], if R 1 were to send R 2 a message to ensure <p> Assuming that R 1 now expects R 2 to be extremely likely to see P2 (say, with probability .99), it computes R 2 's intentional probabilities as: :01 fi [0; 0; 1] + :99 fi <ref> [0; 1; 0] </ref> = [0; :99; :01]: In turn, this gives R 1 expected utilities of 4.96 for a 1 1 , 2 , and 3.96 for a 1 As detailed in [8], if R 1 were to send R 2 a message to ensure that R 2 knew about P2, <p> any observation points in the region on the other side of the trees from me?" The immediate pragmatics of this new knowledge is to cause R 1 to transform its recursive model structure, leading it to now believe that R 2 only knows about P1 and will have intentional probabilities <ref> [0, 0, 1] </ref>, meaning that R 2 will sit still. In turn, now R 1 expects utilities of 1 for a 1 1 , 2 for a 1 a 1 3 . <p> Arguments Based on Iteration Finally, questions and imperatives might also arise not only due to specific interactions, but based on the game-theoretic paradigm of cooperation examined by Aumann in <ref> [1, 2] </ref>, where it is shown how repetitive interactions allow cooperation to emerge as a rational choice of selfish interacting agents.
Reference: [2] <author> Robert J. Aumann and Sylvian Sorin. </author> <title> Cooperation and bounded recall. </title> <journal> Games and Economic Behavior, </journal> <volume> 1 </volume> <pages> 5-39, </pages> <year> 1989. </year>
Reference-contexts: Arguments Based on Iteration Finally, questions and imperatives might also arise not only due to specific interactions, but based on the game-theoretic paradigm of cooperation examined by Aumann in <ref> [1, 2] </ref>, where it is shown how repetitive interactions allow cooperation to emerge as a rational choice of selfish interacting agents.
Reference: [3] <author> Robert Axelrod. </author> <title> The Evolution of Cooperation. </title> <publisher> Basic Books, </publisher> <year> 1984. </year>
Reference-contexts: that obeying imperatives and answering questions may be irrational in a one-time interaction, they may be beneficial overall if one adheres to the "you scratch my back now, I will scratch yours later" maxim, which is a generalized version of a "tit-for-tat" paradigm showed successful for the repeated Prisoner's Dilemma <ref> [3] </ref>. Towards the Emergence of Protocols As the RMM approach is extended to anticipate even more prolonged dialogues among agents, agents will be able to search through possible courses of communicative exchanges to identify the most promising candidates.
Reference: [4] <author> D. Dennett. </author> <title> Intentional systems. </title> <editor> In D. Dennett, editor, Brainstorms. </editor> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: If R 1 thinks that R 2 will attempt to maximize its own expected utility, then R 1 can adopt the intentional stance toward R 2 <ref> [4] </ref>, and treat R 2 as rational.
Reference: [5] <author> Edmund H. Durfee and Victor R. Lesser. </author> <title> Partial global planning: A coordination framework for distributed hypothesis formation. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 21(5) </volume> <pages> 1167-1183, </pages> <month> September </month> <year> 1991. </year> <title> (Special Issue on Distributed Sensor Networks). </title>
Reference-contexts: remotely executing commands and accessing services (as in remote procedure calls); * querying processing in database and knowledge base systems (as in the KQML and SKTP methods [15]); * contracting and negotiation among agents (as in the Contract Net [17], the Unified Negotiation Proto col [18], and partial global planning <ref> [5] </ref>). When developing systems for accomplishing well-understood tasks in well-defined environments, it is reasonable and proper for a system designer to define and institute appropriate protocols.
Reference: [6] <author> Piotr J. Gmytrasiewicz and Edmund H. Durfee. </author> <title> A logic of knowledge and belief for recursive modeling: A preliminary report. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <month> July </month> <year> 1992. </year>
Reference-contexts: This is the notion that the agent issuing the command has more knowledge about the environment in the first place. The modeling of other agents' knowledge using a recursive knowledge-theoretic formulation has been investigated in <ref> [6] </ref>. There, the agents' own knowledge about the world is represented as a Kripke structure, with possible worlds and the agent's inability to tell them apart corresponding to the agents' uncertainty.
Reference: [7] <author> Piotr J. Gmytrasiewicz and Edmund H. Durfee. </author> <title> Toward a theory of honesty and trust among communicating autonomous agents. Group Decision and Negotiation, </title> <booktitle> 2 </booktitle> <pages> 237-258, </pages> <year> 1993. </year>
Reference-contexts: If we assume that communication channels never lose messages (but see [10]) and messages are always believed (see <ref> [7] </ref>, then R 1 can be sure that R 2 will know about the point P2 as a result of the message having been sent. Thus, the recursive model structure will change due to the pragmatic meaning of M 1 , as depicted in Figure 3. <p> For further investigation into honesty and trust among rational agents, the reader is referred to <ref> [7, 19, 20] </ref>. Imperatives as Mitigating the Costs of Computation To understand imperatives requires a similar analysis, except that now we also have to take into account the costs (time, effort) of decision-making. <p> This kind of modeling resembles agents reasoning about whether to believe what they hear <ref> [7] </ref>. There is another aspect of issuing orders, and obeying them, among autonomous agents that is closely related to mitigating the cost of independent decision-making. This is the notion that the agent issuing the command has more knowledge about the environment in the first place.
Reference: [8] <author> Piotr J. Gmytrasiewicz and Edmund H. Durfee. </author> <title> Rational coordination and communication in multiagent environments through recursive modeling. </title> <note> Submitted for Publication, </note> <year> 1994. </year>
Reference-contexts: If it were to model R 1 as rational as well, the nesting of models would continue. For now, however, let us assume that R 1 has no knowledge about how it might be modeled by R 2 . As we discuss more fully in <ref> [8] </ref>, the proper representation of R 1 's lack of knowledge about how it might be modeled by R 2 is to use a uniform probability distribution over R 1 's set of actions on this level. <p> probability .99), it computes R 2 's intentional probabilities as: :01 fi [0; 0; 1] + :99 fi [0; 1; 0] = [0; :99; :01]: In turn, this gives R 1 expected utilities of 4.96 for a 1 1 , 2 , and 3.96 for a 1 As detailed in <ref> [8] </ref>, if R 1 were to send R 2 a message to ensure that R 2 knew about P2, then R 1 could increase its expected utility to 5 (assuming correct message transmission). Thus, in this case, the message would increase the expected utility by 5:0 4:96 = 0:04.
Reference: [9] <author> Piotr J. Gmytrasiewicz, Edmund H. Durfee, and David K. Wehe. </author> <title> A decision-theoretic approach to coordinating multiagent interactions. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: The third level is occupied by uniform probability distributions representing R 1 's lack of knowledge of how it is being modeled by R 2 . Solving the Example As has been summarized elsewhere <ref> [9] </ref>, the recursive model generated by RMM can be solved so as to determine a rational action for the agent represented at the root of the tree, based on expected rational actions taken on the part of agents modeled deeper in the tree. <p> The threatening agent must wait for the action of the threatened agent, and take its own action accordingly. Thus, unlike in the previous case above (and the other cases considered in <ref> [9, 10] </ref>), the actions of the players cannot be considered as simultaneous. Threats will be assumed to have the form "If you do A, then I will do B," where A is an option of an opponent and B is an option available to the threatening agent.
Reference: [10] <author> Piotr J. Gmytrasiewicz, Edmund H. Durfee, and David K. Wehe. </author> <title> The utility of communication in coordinating intelligent agents. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 166-172, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: We conclude with a discussion of how patterns of dialogues could lead to the identification of protocols, and outline our ongoing work in this and other directions. Recursive Modeling and the Utility of Communication In <ref> [10] </ref>, a method called the Recursive Modeling Method (RMM) was used to represent and evaluate all of the knowledge relevant to the problem of choosing the best action, in the presence of other agents. <p> For example, consider what would happen in our original scenario in Figure 1 if R 1 were to send a message M 1 stating "There is an observation point P2, twice as high as P1, behind the trees". If we assume that communication channels never lose messages (but see <ref> [10] </ref>) and messages are always believed (see [7], then R 1 can be sure that R 2 will know about the point P2 as a result of the message having been sent. <p> The threatening agent must wait for the action of the threatened agent, and take its own action accordingly. Thus, unlike in the previous case above (and the other cases considered in <ref> [9, 10] </ref>), the actions of the players cannot be considered as simultaneous. Threats will be assumed to have the form "If you do A, then I will do B," where A is an option of an opponent and B is an option available to the threatening agent.
Reference: [11] <author> Piotr J. Gmytrasiewicz and Jeffrey S. Rosenschein. </author> <title> The utility of embedded knowledge-oriented actions. </title> <booktitle> In Proceedings of the 1993 Distributed AI Workshop, </booktitle> <address> Hidden Valley, PA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: More generally, physical and communicative interactions are interleaved to much greater extents, allowing physical actions to be conditional upon other physical actions, and verbal commitments to physical actions. Consider, for example, the case of a threat <ref> [11] </ref>. An important issue involved in the treatment of threats is that they change the character of our prototypical interactions in a way not considered before. The threatening agent must wait for the action of the threatened agent, and take its own action accordingly. <p> This could be an effective strategy, but only if R 2 takes the threat seriously. Issues of how threats can be made more believable have been covered elsewhere <ref> [11] </ref>.
Reference: [12] <author> Craig A. Knoblock. </author> <title> Generating Abstraction Hierarchies: An automated approach to reducing search in planning. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year>
Reference-contexts: possible detailed plans for these high-level courses of action could be enumerated and represented in a payoff matrix, the abstraction over actions and plans permits evaluation of choices at a level of detail where the quality of the decision is maximized while the costs of making the decision are minimized <ref> [12] </ref>. Cost = 1 Cost = 2 Cost = 2 Cost = 1 worth 2 P1, observation P2, observation worth 4 Trees R R 2 1 point P1, P2, and staying put (or doing something else), respectively.
Reference: [13] <author> Sarit Kraus and Jonathan Wilkenfeld. </author> <title> The function of time in cooperative negotiations. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: R 1 could transfer 1 unit of utility to R 2 so that R 2 would come out even, and R 1 would still be better off. Of course, R 2 could hold out for more. This kind of negotiation has been looked at most recently by Kraus <ref> [13] </ref>. If utility is not transferable, or if R 1 is more greedy (and willing to take some risk), then R 1 could instead threaten R 2 with not going to P1 unless R 2 goes to P2.
Reference: [14] <author> Richard E. </author> <title> Neapolitan. Probabilistic Reasoning in Expert Systems. </title> <publisher> John Wiley and Sons, </publisher> <year> 1990. </year>
Reference-contexts: This distribution, according to the principle of entropy maximization <ref> [14] </ref>, precisely have a complete and accurate map of the terrain, or if it does then it cannot locate its own position on that map. represents the lack of information on the part of the robot R 1 in this case, since its informational content is zero.
Reference: [15] <author> Ramesh S. Patil, Richard E. Fikes, Peter F. Patel-Schneider, Don Mckay, Tim Finin, Thomas Gruber, and Robert Neches. </author> <title> The DARPA knowledge sharing effort: Progress report. </title> <booktitle> In Proceedings of the Eighth IEEE Conference on AI Applications, </booktitle> <month> March </month> <year> 1992. </year>
Reference-contexts: distribution/replication (as in the distributed database literature); * establishing commitments over resource allocation (as in resource locking protocols in distributed sys tems); * remotely executing commands and accessing services (as in remote procedure calls); * querying processing in database and knowledge base systems (as in the KQML and SKTP methods <ref> [15] </ref>); * contracting and negotiation among agents (as in the Contract Net [17], the Unified Negotiation Proto col [18], and partial global planning [5]). When developing systems for accomplishing well-understood tasks in well-defined environments, it is reasonable and proper for a system designer to define and institute appropriate protocols.
Reference: [16] <author> Thomas C. Schelling. </author> <title> The Strategy of Conflict. </title> <publisher> Havard University Press, </publisher> <year> 1960. </year>
Reference-contexts: Threats will be assumed to have the form "If you do A, then I will do B," where A is an option of an opponent and B is an option available to the threatening agent. One of the subtleties of threats, as discussed for example in <ref> [16] </ref>, is that they often seem to involve a degree of irrationality on the part of the threatening side.
Reference: [17] <author> Reid G. Smith. </author> <title> The contract net protocol: High-level communication and control in a distributed problem solver. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-29(12):1104-1113, </volume> <month> December </month> <year> 1980. </year>
Reference-contexts: allocation (as in resource locking protocols in distributed sys tems); * remotely executing commands and accessing services (as in remote procedure calls); * querying processing in database and knowledge base systems (as in the KQML and SKTP methods [15]); * contracting and negotiation among agents (as in the Contract Net <ref> [17] </ref>, the Unified Negotiation Proto col [18], and partial global planning [5]). When developing systems for accomplishing well-understood tasks in well-defined environments, it is reasonable and proper for a system designer to define and institute appropriate protocols.
Reference: [18] <author> Gilad Zlotkin and Jeffrey S. Rosenschein. </author> <title> Cooperation and conflict resolution via negotiation among autonomous agents in non-cooperative domains. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 21(6), </volume> <month> December </month> <year> 1991. </year> <note> (Special Issue on Distributed AI). </note>
Reference-contexts: in distributed sys tems); * remotely executing commands and accessing services (as in remote procedure calls); * querying processing in database and knowledge base systems (as in the KQML and SKTP methods [15]); * contracting and negotiation among agents (as in the Contract Net [17], the Unified Negotiation Proto col <ref> [18] </ref>, and partial global planning [5]). When developing systems for accomplishing well-understood tasks in well-defined environments, it is reasonable and proper for a system designer to define and institute appropriate protocols.
Reference: [19] <author> Gilad Zlotkin and Jeffrey S. Rosenschein. </author> <title> Incomplete information and deception in multi-agent negotiation. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 225-231, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: For further investigation into honesty and trust among rational agents, the reader is referred to <ref> [7, 19, 20] </ref>. Imperatives as Mitigating the Costs of Computation To understand imperatives requires a similar analysis, except that now we also have to take into account the costs (time, effort) of decision-making.
Reference: [20] <author> Gilad Zlotkin and Jeffrey S. Rosenschein. </author> <title> A domain theory for task oriented negotiation. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 416-422, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: For further investigation into honesty and trust among rational agents, the reader is referred to <ref> [7, 19, 20] </ref>. Imperatives as Mitigating the Costs of Computation To understand imperatives requires a similar analysis, except that now we also have to take into account the costs (time, effort) of decision-making.
References-found: 20

