URL: ftp://hpsl.cs.umd.edu/pub/papers/aiaa95.ps.Z
Refering-URL: http://www.cs.umd.edu/projects/hpsl/papers.brandnew/LocalResources/tech-10-23.htm
Root-URL: 
Email: [rpnance, hassan]@jupiter1.mae.ncsu.edu  wilmoth@monte.larc.nasa.gov  [bkmoon, saltz]@cs.umd.edu  
Title: Parallel Monte Carlo Simulation of Three-Dimensional Flow over a Flat Plate  M  
Author: Robert P. Nance H. A. Hassan Richard G. Wilmoth Bongki Moon Joel Saltz p w 
Keyword: Nomenclature Freestream Mach number Re Freestream Reynolds number Stagnation pressure, bars Surface pressure, Pa  
Note: This work was supported by NASA Cooperative Agreement NCCI-112, the Mars Mission Research Center funded in part by NASA Grant NAGW-1331, a National Defense Science and Engineering Graduate Fellowship, NASA Grant NAG-1-1560, ARPA/NASA Grant NAG-1-1485, and NSF/NASA Grant ASC 9213821.  
Address: Raleigh, NC 27695-7910  Hampton, VA 23681-0001  College Park, MD 20742  
Affiliation: Department of Mechanical and Aerospace Engineering North Carolina State University  Aerothermodynamics Branch, Gas Dynamics Division NASA Langley Research Center  Institute for Advanced Computer Studies and Department of Computer Science University of Maryland  
Abstract: This paper describes a parallel implementation of the direct simulation Monte Carlo method. Runtime library support is used for scheduling and execution of communication between nodes, and domain decomposition is performed dynamically to maintain a favorable load balance. Performance tests are conducted using the code to evaluate various remapping and remapping-interval policies, and it is shown that a one-dimensional chain-partitioning method works best for the problems considered. The parallel code is then used to simulate the Mach 20 nitrogen ow over a finite-thickness at plate. It will be shown that the parallel algorithm produces results which are very similar to previous DSMC results, despite the increased resolution available. However, it yields significantly faster execution times than the scalar code, as well as very good load-balance and scalability characteristics. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bird, </author> <title> G.A., Monte Carlo Simulation in an Engineering Context, </title> <booktitle> Progress in Astronautics and Aeronautics: Rarefied Gas Dynamics, </booktitle> <volume> Vol. 74, Pt. 1, </volume> <booktitle> AIAA, </booktitle> <address> New York, </address> <year> 1981, </year> <pages> pp. 239-255. </pages>
Reference-contexts: 1 Introduction The direct simulation Monte Carlo (DSMC) method of Bird <ref> [1] </ref> has become the standard method for the analysis of hypersonic rarefied ows. Since its inception, the method has been applied to more and more complex configurations, including the Space Shuttle orbiter geometry [2] and the Upper Atmosphere Research Satellite [3]. <p> Note that the stagnation temperature of 1100 K is quite low; therefore, vibrational excitation and dissociation are not expected to take place, and the only species considered was N 2 . The Variable Hard Sphere (VHS) model of Bird <ref> [1] </ref> was utilized with a viscosity-temperature exponent of 0.75. Energy exchange between translational and rotational modes was determined through use of the Larsen-Borgnakke method [15] and a rotational relaxation number . The surface of the plate was assumed to be diffusely reective with full thermal accommodation.
Reference: [2] <author> Rault, D. F. G., </author> <title> Aerodynamics of Shuttle Orbiter at High Altitudes, </title> <type> AIAA Paper 93-2815, </type> <month> July </month> <year> 1993. </year>
Reference-contexts: 1 Introduction The direct simulation Monte Carlo (DSMC) method of Bird [1] has become the standard method for the analysis of hypersonic rarefied ows. Since its inception, the method has been applied to more and more complex configurations, including the Space Shuttle orbiter geometry <ref> [2] </ref> and the Upper Atmosphere Research Satellite [3]. Furthermore, many DSMC analyses carried out today include physical phenomena such as thermal and chemical nonequilibrium [4]. The combination of complicated geometries and complicated ow physics leads to large processor-time and storage requirements, even for low-density calculations.
Reference: [3] <author> Woronowicz, M. S. and Rault, D. F. G., </author> <title> On Predicting Contamination Levels of HALOE Optics aboard UARS Using Direct Simulation Monte Carlo, </title> <type> AIAA Paper 93-2869, </type> <month> July </month> <year> 1993. </year>
Reference-contexts: Since its inception, the method has been applied to more and more complex configurations, including the Space Shuttle orbiter geometry [2] and the Upper Atmosphere Research Satellite <ref> [3] </ref>. Furthermore, many DSMC analyses carried out today include physical phenomena such as thermal and chemical nonequilibrium [4]. The combination of complicated geometries and complicated ow physics leads to large processor-time and storage requirements, even for low-density calculations.
Reference: [4] <author> Taylor, J. C., Carlson, A. B., and Hassan, H. A., </author> <title> Monte Carlo Simulation of Radiating Reentry Flows, </title> <journal> Journal of Thermophysics and Heat Transfer, </journal> <volume> Vol. 9, No. 3, </volume> <year> 1994, </year> <pages> pp. 478-485. </pages>
Reference-contexts: Since its inception, the method has been applied to more and more complex configurations, including the Space Shuttle orbiter geometry [2] and the Upper Atmosphere Research Satellite [3]. Furthermore, many DSMC analyses carried out today include physical phenomena such as thermal and chemical nonequilibrium <ref> [4] </ref>. The combination of complicated geometries and complicated ow physics leads to large processor-time and storage requirements, even for low-density calculations. For near-continuum DSMC applications, the resource requirements can render a meaningful simulation infeasible on current scalar architectures.
Reference: [5] <author> Wilmoth, R. G., </author> <title> Adaptive Domain Decomposition for Monte Carlo Simulations on Parallel Processors, </title> <booktitle> Proceedings of the 17th International Symposium on Rarefied Gas Dynamics, </booktitle> <publisher> VCH Publishers, </publisher> <address> New York, </address> <year> 1991, </year> <pages> pp. 709-716. </pages>
Reference: [6] <author> McDonald, J. and Dagum, L., </author> <title> A Comparison of Particle Simulation Implementations on Two Different Parallel Architectures, </title> <booktitle> Proceedings of the Sixth Distributed Memory Computing Conference, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <address> Knoxville, TN, </address> <year> 1991, </year> <pages> pp. 413-419. </pages>
Reference: [7] <author> Fallavollita, M. A., McDonald, J., and Baganoff, D., </author> <title> Parallel Implementation of a Particle Simulation for Modeling Rarefied Gas Dynamic Flow, </title> <journal> Computing Systems in Engineering, </journal> <volume> Vol. 3, No. </volume> <pages> 1-4, </pages> <year> 1992, </year> <pages> pp. 283-289. </pages>
Reference: [8] <author> Bartel, T. J. and Plimpton, S. J., </author> <title> DSMC Simulation of Rarefied Gas Dynamics on a Large Hypercube Supercomputer, </title> <type> AIAA Paper 92-2860, </type> <month> June </month> <year> 1992. </year>
Reference: [9] <author> Boyd, I. and Dietrich, S., </author> <title> A Scalar Optimized Parallel Implementation of the DSMC Method, </title> <type> AIAA Paper 94-0355, </type> <month> January </month> <year> 1994. </year>
Reference: [10] <author> Long, L. N., Wong, B. C., and Myczkowski, J., </author> <title> Deterministic and Nondeterministic Algorithms for Rarefied Gas Dynamics, Rarefied Gas Dynamics: Theory and Simulations, </title> <editor> Shizgal, B. D. and Weaver, D. P., editors; Progress in Astronautics and Aeronautics, </editor> <volume> Vol. 159, </volume> <year> 1994, </year> <pages> pp. 361-370. </pages>
Reference: [11] <author> Allegre, J., Raffin, M., Chpoun, A., and Gottesdiener, L., </author> <title> Rarefied Hypersonic Flow over a Flat Plate with Truncated Leading Edge, Rarefied Gas Dynamics: Space Science and Engineering, </title> <editor> Shizgal, B. D. and Weaver, D. P., editors; Progress in Astronautics and Aeronautics, </editor> <volume> Vol. 160, </volume> <year> 1994, </year> <pages> pp. 285-295. </pages>
Reference-contexts: The problem considered is described below. 2 CNRS Experiment edge tested at zero angle of attack in the SR3 low-density nitrogen tunnel at the Centre National de la Recherche Scientific (CNRS), Meudon, France <ref> [11] </ref>. The experimental results for the surface heat-transfer rate were compared to Navier-Stokes and DSMC results by the CNRS researchers; the computational-uid-dynamics (CFD) results were shown to match the test data quite well, while the DSMC results overpredicted the heat ux across the length of the plate.
Reference: [12] <author> Hash, D. B., Moss, J. N., and Hassan, H. A., </author> <title> Direct Simulation of Diatomic Gases Using the Generalized Hard Sphere Model, </title> <journal> Journal of Thermophysics and Heat Transfer, </journal> <volume> Vol. 6, No. 4, </volume> <year> 1994, </year> <pages> pp. 758-761. 11 </pages>
Reference-contexts: Further efforts to correct the discrepancy in the DSMC results made little difference, as shown in the paper by Hash et al. <ref> [12] </ref> This further study included the consideration of effects such as collision model, grid refinement, and nonuniformities in the upstream test section.
Reference: [13] <author> Rault, D. F. G., </author> <title> Efficient Three-Dimensional Direct Simulation Monte Carlo Code for Complex Geometry Problems, Rarefied Gas Dynamics: Theory and Simulations, </title> <editor> Shizgal, B. D. and Weaver, D. P., editors; Progress in Astronautics and Aeronautics, </editor> <volume> Vol. 159, </volume> <year> 1994, </year> <pages> pp. 137-154. </pages>
Reference-contexts: Therefore, one motivation for pursuing the problem is to obtain a solution for the complete owfield. To this end, the at plate was mod q w T 0 W n ( ) 3 eled first with the modified F3 code of Rault <ref> [13] </ref> and then with a modified version of the DSMC3 code of Bird [14]. The results from these computations suggested that, while no relief effect appeared to be present, increases in grid resolution tended to increase the agreement between the computed and experimental data.
Reference: [14] <author> Bird, G. A., </author> <title> Molecular Gas Dynamics and the Direct Simulation of Gas Flows, </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1994. </year>
Reference-contexts: To this end, the at plate was mod q w T 0 W n ( ) 3 eled first with the modified F3 code of Rault [13] and then with a modified version of the DSMC3 code of Bird <ref> [14] </ref>. The results from these computations suggested that, while no relief effect appeared to be present, increases in grid resolution tended to increase the agreement between the computed and experimental data.
Reference: [15] <author> Borgnakke, C., and Larsen, P. S., </author> <title> Statistical Collision Model for Monte Carlo Simulation of Polyatomic Gas Mixtures, </title> <journal> Journal of Computational Physics, </journal> <volume> Vol. 18, No. 3, </volume> <year> 1975, </year> <pages> pp. 405-420. </pages>
Reference-contexts: The Variable Hard Sphere (VHS) model of Bird [1] was utilized with a viscosity-temperature exponent of 0.75. Energy exchange between translational and rotational modes was determined through use of the Larsen-Borgnakke method <ref> [15] </ref> and a rotational relaxation number . The surface of the plate was assumed to be diffusely reective with full thermal accommodation. It should also be noted that the test conditions correspond to a freestream Knudsen number based on plate length of about .
Reference: [16] <author> Das, R., and Saltz, J., </author> <title> Parallelizing Molecular Dynamics Codes Using the Parti Software, </title> <booktitle> Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <publisher> SIAM, </publisher> <year> 1993, </year> <pages> pp. 187-192. </pages>
Reference: [17] <author> Saltz, J., Mirchandaney, R., and Crowley, K., </author> <title> Run-Time Parallelization and Scheduling of Loops, </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 40, No. 5, </volume> <month> May </month> <year> 1991, </year> <pages> pp. 603-612. </pages>
Reference: [18] <author> Saltz, J., Berryman, H., and Wu, J., </author> <title> Multiprocessors and Run-Time Compilation, </title> <journal> Concurrency: Practice and Experience, </journal> <volume> Vol. 3, No. 6, </volume> <month> December </month> <year> 1991, </year> <pages> pp. 573-592. </pages>
Reference: [19] <author> Hwang, Y. S., Moon, B., Sharma, S., Ponnusamy, R., Das, R., and Saltz, J., </author> <title> Runtime and Language Support for Compiling Adaptive Irregular Programs on Distributed Memory Machines, </title> <journal> Software Practice and Experience, </journal> <note> to appear. </note>
Reference-contexts: CHAOS was developed with such problems in mind, and utilizes a series of preprocessing steps in order to facilitate efficient computation <ref> [19] </ref>. First of all, CHAOS determines how data arrays are to be partitioned. This step involves the generation of a translation table which maps elements of the data arrays to their owner processors. This table is globally accessible.
Reference: [20] <author> Moon, B., Uysal, M., and Saltz, J., </author> <title> Index Translation Schemes for Adaptive Computations on Distributed Memory Multicomputers, </title> <booktitle> Proceedings of the Ninth International Parallel Processing Symposium, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <address> Santa Barbara, CA, </address> <note> April 1995 (to be published). </note>
Reference-contexts: In this particular application, the table is replicated on each processor because the problem size is relatively small. However, memory considerations make it clear that it is not always feasible to replicate the table, so the translation table must be distributed across processors in some applications. Moon et al. <ref> [20] </ref> have recently developed new index translation schemes which use software caching techniques so that extra memory can be exploited adaptively for changeable data access patterns and communication latency can be avoided. However, as mentioned above, simple table replication was utilized for this study.
Reference: [21] <author> Berger, M. J., and Bokhari, S. H., </author> <title> A Partitioning Strategy for Nonuniform Problems on Multiprocessors, </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 36, No. 5, </volume> <month> May </month> <year> 1987, </year> <pages> pp. 570-580. </pages>
Reference-contexts: Several options are available for decomposing the domain; three possibilities investigated here are recursive coordinate bisection (RCB) <ref> [21] </ref>, recursive inertial bisection (RIB) [22], and one-dimensional chain partitioning [23]. In the first two algorithms, the domain is recursively halved (with each new portion of the domain possessing an equal amount of work) until there are as many subdomains as processors.
Reference: [22] <author> Nour-Omid, B., Raefsky, A., and Lyzenga, G., </author> <title> Solving Finite Element Equations on Concurrent Computers, Parallel Computations and their Impact on Mechanics, </title> <booktitle> ASME, </booktitle> <address> New York, </address> <year> 1987, </year> <pages> pp. 209-227. </pages>
Reference-contexts: Several options are available for decomposing the domain; three possibilities investigated here are recursive coordinate bisection (RCB) [21], recursive inertial bisection (RIB) <ref> [22] </ref>, and one-dimensional chain partitioning [23]. In the first two algorithms, the domain is recursively halved (with each new portion of the domain possessing an equal amount of work) until there are as many subdomains as processors.
Reference: [23] <author> Moon, B., and Saltz, J., </author> <title> Adaptive Runtime Support for Direct Simulation Monte Carlo Methods on Distributed Memory Architectures, </title> <booktitle> Proceedings of the Scalable High Performance Computing Conference (SHPCC94), </booktitle> <publisher> IEEE Computer Society Press, </publisher> <address> Knoxville, TN, </address> <month> May </month> <year> 1994, </year> <pages> pp. 357-364. </pages>
Reference-contexts: Several options are available for decomposing the domain; three possibilities investigated here are recursive coordinate bisection (RCB) [21], recursive inertial bisection (RIB) [22], and one-dimensional chain partitioning <ref> [23] </ref>. In the first two algorithms, the domain is recursively halved (with each new portion of the domain possessing an equal amount of work) until there are as many subdomains as processors.
Reference: [24] <author> Nicol, D. M. and Saltz, J., </author> <title> Dynamic Remapping of Parallel Computations with Varying Resource Demands, </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 37, No.9, </volume> <month> September </month> <year> 1988, </year> <month> pp.1073-1087. </month>
Reference-contexts: The former choice is not practicable for most problems, since it requires pre-runtime analysis to determine the optimal interval for remapping. Thus, in this study, a variable-interval remapping policy is investigated as well; the method employed is the Stop at Rise (SAR) policy suggested by Nicol and Saltz <ref> [24] </ref>.

References-found: 24

