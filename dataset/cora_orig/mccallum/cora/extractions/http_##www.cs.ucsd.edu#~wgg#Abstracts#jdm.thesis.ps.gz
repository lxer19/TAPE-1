URL: http://www.cs.ucsd.edu/~wgg/Abstracts/jdm.thesis.ps.gz
Refering-URL: http://www.cs.ucsd.edu/~wgg/Abstracts/jdm.thesis.html
Root-URL: http://www.cs.ucsd.edu
Title: Static Analysis for a Software Transformation Tool  
Author: Professor William G. Griswold, Chairperson Professor Donald W. Anderson Professor Jeanne Ferrante Professor Debra J. Richardson Professor Jan B. Talbot 
Degree: A dissertation submitted in partial satisfaction of the requirements for the degree Doctor of Philosophy in Computer Science by John David Morgenthaler Committee in charge:  
Date: 1997  
Affiliation: UNIVERSITY OF CALIFORNIA, SAN DIEGO  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. V. Aho, J. E. Hopcraft, and J. D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1974. </year>
Reference-contexts: Such methods appear to be infeasible for use on large programs. Instead, Cstructure uses a simple equivalence class scheme similar to the one given in [2]. This technique, based on the fast union-find algorithm <ref> [1] </ref>, must be applied to the entire program before any data flow queries may be answered. The analysis uses non-standard type inference to assign each lvalue to a points-to equivalence class [50, 51, 52].
Reference: [2] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers, Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: In the context of a compiler, these representations are intermediate between the source and target languages <ref> [2] </ref>. They are also the data structures of choice for many types of tools, as their use in static analysis is quite well understood. The following three intermediate representations or variants often appear in software tools: * Abstract Syntax Tree (AST), or parse tree [2]. <p> between the source and target languages <ref> [2] </ref>. They are also the data structures of choice for many types of tools, as their use in static analysis is quite well understood. The following three intermediate representations or variants often appear in software tools: * Abstract Syntax Tree (AST), or parse tree [2]. This representation is closest to the source code, and may be easily annotated to allow regeneration of the original source. Each node in the AST usually reflects a production in the context-free grammar for the programming language in which the program is written. <p> Each node in the AST usually reflects a production in the context-free grammar for the programming language in which the program is written. It can serve as the internal representation of a structure editor. * Control Flow Graph (CFG) <ref> [2] </ref>. Abstracts away the grammatical features of the program source. The CFG consists of nodes representing the computations in a program connected by edges showing the flow of control from node to node. <p> The present work attempts to address this situation. 1.3.4 Representing Control Flow Information Data flow analysis requires information about the flow of control through the program. The CFG is the standard control flow representation expected by algorithms derived from traditional data flow analysis frameworks <ref> [2] </ref>. The cited examples of demand-driven analysis are also based on the usual control flow graph. <p> The ordering of operations is usually specified by construction of a control flow graph (CFG), which explicitly represents this information. A CFG can be 18 19 constructed from the program's source code or AST in linear time using syntax-directed translation <ref> [2] </ref>. The details of the program's flow of control must therefore be present in these representations as well, though perhaps not explicitly. Virtual control flow extracts these details individually through a fine-grained application of the general demand-driven paradigm. <p> whose execution immediately precedes or follows the execution of any syntactic construct large enough to be bounded by sequence points. 2.2.2 Consequences of Expression Granularity Computing control flow information at the granularity of arbitrary C expressions is a departure from the use of more traditional basic blocks of three-address code <ref> [2] </ref>. A CFG provides two levels of control flow information flow between basic blocks and sequential flow within each block. To ensure that it remains fully conservative, virtual control flow information is computed at only a single level. The resulting expression-level granularity provides several advantages, and also some disadvantages. <p> To determine the correct data flow relationships, a data flow analyzer must be able to distinguish expressions that must execute from those which only may execute. A variable definition embedded within a larger expression could be either killing (must-def ) or preserving (may-def ) <ref> [2, 12] </ref>. Differentiating between these two possibilities is necessary, for instance, during the computation of reaching definitions. A must-def definition that is within a may-execute expression becomes a may-def in that context. <p> This flow of values from statement to statement and variable to variable results in semantic dependencies between program components. Information about such dependencies can be used in a software tool to support source-level program understanding [5] and transformation tasks [24, 32], or by a compiler to support optimizations <ref> [2] </ref>. For instance, knowledge of the locations where a given statement's variables could have been defined last can help determine if that statement may be moved without altering the behavior of the program. <p> as the answers to data flow queries such as, "What are the reaching definitions of the variables in this expression?" or, "What are the reachable uses of the variables defined in this statement?" These standard concepts were developed to support compiler optimizations, and in that context are traditionally computed exhaustively <ref> [2] </ref>. This chapter shows how to answer data flow queries on demand, directly from the AST, with the aid of virtual control flow. <p> control flow and demand-driven data flow analysis yield a software tool architecture that is both simpler to build and more efficient to use then tools constructed using conventional compiler-oriented representations and algorithms. 3.2 Demand-driven Analysis Traditional data flow analysis answers a data flow query at all points in a program <ref> [2] </ref>. In contrast to such an exhaustive solution, demand-driven algorithms answer a query at only one program point at a time. For software modification tasks affecting only small portions of a program, such as user-directed restructuring transformations, answering queries on demand can substantially reduce overall computation. <p> The demand-driven technique is basically a breadth-first search through the program's control flow starting at the initial (selected) statement. This search may include any number of lvalues. The algorithm simultaneously checks for all interesting definitions, adding each one encountered to the solution. A killing , or unambiguous, definition <ref> [2, 12] </ref> stops the search for the relevant lvalue along that path. Preserving (ambiguous) definitions [12] have no effect on query propagation. A brief walkthrough of the reaching definitions algorithm will help clarify these concepts. The procedure adds expressions to a worklist as the search propagates backward through the program. <p> Except for function entry nodes, reaching definitions query propagation can be modeled with the help of data flow equations, where IN and OU T are temporary sets discarded after use. Unlike their counterparts in traditional exhaustive data flow analysis <ref> [2] </ref>, the lvalues contained in IN and OU T represent an unsatisfied demand for information (uses in search of definitions) rather than the information itself (definitions). <p> Two lvalues are aliases of each other if they both refer to the same memory location <ref> [2] </ref>. Aliases complicate data flow analysis by introducing ambiguity about the memory locations each expression actually defines or uses. This section describes a conservative technique for computing all potential aliases in a program that runs in almost linear time, and its combination with demand-driven data flow analysis. <p> Such methods appear to be infeasible for use on large programs. Instead, Cstructure uses a simple equivalence class scheme similar to the one given in <ref> [2] </ref>. This technique, based on the fast union-find algorithm [1], must be applied to the entire program before any data flow queries may be answered. The analysis uses non-standard type inference to assign each lvalue to a points-to equivalence class [50, 51, 52]. <p> Thus, only flow-insensitive information is needed to determine if A defines x. On the other hand, if a single entry/single exit program component contains an lvalue use, that use may or may not be upwards exposed <ref> [2] </ref>. A reachable uses data flow query from the component entry point may be used to determine if any uses 82 are upwards exposed. In reality, such a query is only necessary when the component also defines the lvalue. <p> Finally, the CFG can serve as a uniform representation for use with many different programming languages. This allows the separation of responsibilities in the compiler into a language-dependent front end and a language-independent back end <ref> [2] </ref>. The need for similar generic methods in software re-engineering has recently been mentioned [54], but may be misplaced. Software manipulation such as restructuring or refactoring appears to be inherently dependent on the particular programming language being used [32]. Language-dependence is also clearly an intrinsic aspect of virtual control flow.
Reference: [3] <author> M. M. Ammann and R. D. Cameron. </author> <title> Inter-module renaming and reorganizing: examples of program manipulation in-the-large. </title> <booktitle> In Proceedings of the International Conference on Software Maintenance, </booktitle> <pages> pages 354-361, </pages> <year> 1994. </year>
Reference-contexts: Some of these tools also give the user interactive control over when and where individual transformations are performed, rather than blindly restructuring at every opportunity. Many low-level refactoring transformations, such as renaming identifiers or restructuring class hierarchies, require only limited semantic information <ref> [3, 47] </ref>. However, any transformation that reorders computations requires information about control and data flow properties of the program being manipulated. Several tools that automatically restructure source code rely on static analysis (including data flow analysis) to reason about these semantic properties.
Reference: [4] <author> R. S. Arnold. </author> <title> An introduction to software restructuring. </title> <editor> In R. S. Arnold, editor, </editor> <title> Tutorial on Software Restructuring. </title> <publisher> Society Press (IEEE), </publisher> <address> Washington D.C., </address> <year> 1986. </year>
Reference-contexts: As defined by Arnold, Software restructuring is the modification of software to make the software (1) easier to understand and to change or (2) less susceptible to error when future changes are made. "Software" includes external and internal documentation concerning the code, as well as the code itself <ref> [4] </ref>. With such a broad definition, many types of maintenance activities could be considered restructuring.
Reference: [5] <author> Darren C. Atkinson and William G. Griswold. </author> <title> The design of whole-program analysis tools. </title> <booktitle> In Proceedings of the 18th International Conference on Software Engineering, </booktitle> <pages> pages 16-27, </pages> <year> 1996. </year>
Reference-contexts: This approach has been applied to a scalable slicer, which discards the AST after constructing the more useful CFG <ref> [5] </ref>. In order to display a slice once computed, each CFG node records the node number from the associated location in the original AST. <p> As mentioned earlier, this approach has been applied to the construction of complete program representations (AST and CFG) in a large-scale program slicer <ref> [5] </ref>. An implementation of the UIG in a maintenance environment incrementally constructs subgraphs for specified procedures when needed for a particular tool [27]. <p> This flow of values from statement to statement and variable to variable results in semantic dependencies between program components. Information about such dependencies can be used in a software tool to support source-level program understanding <ref> [5] </ref> and transformation tasks [24, 32], or by a compiler to support optimizations [2]. For instance, knowledge of the locations where a given statement's variables could have been defined last can help determine if that statement may be moved without altering the behavior of the program. <p> Once specified, the task of collecting this information should be fairly straightforward. Such a mechanism could also be extended to handle control flow issues such as non-local jumps and functions that terminate the program (exit and abort). Recent work on demand-driven construction of program representations <ref> [5] </ref> can also be applied to construction of the AST alone. This approach operates as a custom memory manager, discarding unused pieces of program representation and rebuilding them from the source code on demand.
Reference: [6] <author> Jeffrey M. Barth. </author> <title> A practical interprocedural data flow analysis algorithm. </title> <journal> Communications of the ACM, </journal> <volume> 21(9) </volume> <pages> 724-736, </pages> <month> September </month> <year> 1978. </year>
Reference-contexts: For these applications, whether definitions are killing or preserving is unimportant, so may-def or may-use information will be sufficient. 63 Well known flow-insensitive techniques exist for the interprocedural computation of this set/use information <ref> [6] </ref>. Such techniques may be easily modified to operate in a demand-driven fashion to deal with program modifications, as explained in the remainder of this section. Unlike flow-sensitive analysis, the information resulting from this analysis is compact enough to store exhaustively. <p> A single walk of each function's AST simultaneously yields all definitions, uses, and function calls appearing directly within the body of that function. Using terminology similar to Barth <ref> [6] </ref>, I call these sets DIRM OD, DIRU SE and CALLS, respectively. These sets are associated with individual functions, rather than being lumped together in a global relation. <p> This scan includes the bodies of all functions called by A and B, along with the transitive closure of their call trees <ref> [6] </ref>. Function level sets can be cached between queries, as described in Section 3.4. Although flow-insensitive, this information is often adequate for dependence analysis. The goal here is to not to compute every dependence, but simply to determine if any unwanted dependence exists involving A or B.
Reference: [7] <author> R. W. Bowdidge and W. G. Griswold. </author> <title> How Software Tools Organize Programmer Behavior During the Task of Data Encapsulation Types. </title> <publisher> Kluwer, </publisher> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: A compiler optimization affects the performance of the machine code, though it is often applied to some intermediate program representation. Restructuring software by hand is tedious and error prone, and can benefit from the use of automated tools <ref> [7, 23] </ref>. Early automated restructuring tools had limited capabilities, such as the removal of goto's from unstructured code [10, 44]. These tools only support batch transformations involving control flow constructs, and thus have limited utility. <p> These tools only support batch transformations involving control flow constructs, and thus have limited utility. Newer tools take advantage of additional semantic information in order to support more sophisticated refactorings for both object-oriented [32, 43, 46, 47] and procedural languages <ref> [7, 20, 24, 41] </ref>. Some of these tools also give the user interactive control over when and where individual transformations are performed, rather than blindly restructuring at every opportunity. Many low-level refactoring transformations, such as renaming identifiers or restructuring class hierarchies, require only limited semantic information [3, 47]. <p> However, this level of performance would strongly inhibit an exploratory style of tool use. This style has been suggested as most appropriate for maintenance tasks involving large systems <ref> [7] </ref>, which would definitely be supported by temporal immediacy. Opdyke also mentions the importance of speed in a tool supporting design exploration [47]. <p> Future work includes the design and implementation in Cstructure of a complete set of general refactorings, and its use in realistic software maintenance situations. Another research direction involves the integration of restructuring transformations with other program visualization techniques such as the star diagram <ref> [7, 8, 22] </ref>. The conclusion (Chapter 6) discusses these and other issues.
Reference: [8] <author> Robert W. Bowdidge. </author> <title> Supporting the Restructuring of Data Abstractions through Manipulation of a Program Visualization. </title> <type> PhD thesis, </type> <institution> University of California, San Diego, Department of Computer Science and Engineering, </institution> <year> 1995. </year> <note> Technical Report CS95-457. </note>
Reference-contexts: Future work includes the design and implementation in Cstructure of a complete set of general refactorings, and its use in realistic software maintenance situations. Another research direction involves the integration of restructuring transformations with other program visualization techniques such as the star diagram <ref> [7, 8, 22] </ref>. The conclusion (Chapter 6) discusses these and other issues.
Reference: [9] <author> Frederick P. Brooks. </author> <title> No silver bullet: Accidents and essence of software engineering. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 10-19, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: Fred Brooks argued ten years ago that "fashioning complex conceptual constructs is the essence" of 1 2 software engineering, while "accidental tasks arise in representing the constructs in language <ref> [9] </ref>." However, Waters and Chikofsky assert that tasks related to understanding and modifying software systems may actually be of greater consequence; understanding the complex conceptual constructs as represented in existing legacy systems has become more economically important than fashioning these constructs: In any event, no matter what we may wish to
Reference: [10] <author> E. Bush. </author> <title> The automatic restructuring of COBOL. </title> <booktitle> In Proceedings of the Conference on Software Maintenance|1985, </booktitle> <pages> pages 35-41, </pages> <year> 1985. </year> <pages> 112 113 </pages>
Reference-contexts: Restructuring software by hand is tedious and error prone, and can benefit from the use of automated tools [7, 23]. Early automated restructuring tools had limited capabilities, such as the removal of goto's from unstructured code <ref> [10, 44] </ref>. These tools only support batch transformations involving control flow constructs, and thus have limited utility. Newer tools take advantage of additional semantic information in order to support more sophisticated refactorings for both object-oriented [32, 43, 46, 47] and procedural languages [7, 20, 24, 41].
Reference: [11] <author> D. Callahan. </author> <title> The program summary graph and flow-sensitive interprocedural data flow analysis. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 47-56, </pages> <year> 1988. </year>
Reference-contexts: The unified interprocedural graph (UIG) "extracts the important features of existing program representations, and adds new information, to provide an integrated representation for maintenance tasks [27]." Constructed only for specified groups of procedures on demand, this single intermediate representation combines relationships found in a call graph, a program summary graph <ref> [11] </ref>, an interproce-dural flow graph [31] and a system dependence graph [30]. As the UIG is based on a call graph, the program's syntax is not present in this representation. Data flow dependencies are explicitly represented, which implies that the UIG could be quite large and costly to compute.
Reference: [12] <author> J. D. Choi, R. Cytron, and J. Ferrante. </author> <title> On the efficient engineering of ambitious program analysis. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 20(2) </volume> <pages> 105-114, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: To determine the correct data flow relationships, a data flow analyzer must be able to distinguish expressions that must execute from those which only may execute. A variable definition embedded within a larger expression could be either killing (must-def ) or preserving (may-def ) <ref> [2, 12] </ref>. Differentiating between these two possibilities is necessary, for instance, during the computation of reaching definitions. A must-def definition that is within a may-execute expression becomes a may-def in that context. <p> The demand-driven technique is basically a breadth-first search through the program's control flow starting at the initial (selected) statement. This search may include any number of lvalues. The algorithm simultaneously checks for all interesting definitions, adding each one encountered to the solution. A killing , or unambiguous, definition <ref> [2, 12] </ref> stops the search for the relevant lvalue along that path. Preserving (ambiguous) definitions [12] have no effect on query propagation. A brief walkthrough of the reaching definitions algorithm will help clarify these concepts. The procedure adds expressions to a worklist as the search propagates backward through the program. <p> This search may include any number of lvalues. The algorithm simultaneously checks for all interesting definitions, adding each one encountered to the solution. A killing , or unambiguous, definition [2, 12] stops the search for the relevant lvalue along that path. Preserving (ambiguous) definitions <ref> [12] </ref> have no effect on query propagation. A brief walkthrough of the reaching definitions algorithm will help clarify these concepts. The procedure adds expressions to a worklist as the search propagates backward through the program.
Reference: [13] <author> R. Cytron, J. Ferrante, B. Rosen, M. Wegman, and K. Zadeck. </author> <title> An efficient method of computing static single assignment form. </title> <booktitle> In Proceedings of the 16th Symposium on Principles of Programming Languages, </booktitle> <pages> pages 25-35, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Disadvantages of providing control flow at expression granularity include the possibility of complicating the existing algorithms that consume this information. For example, data flow analysis using a CFG based on three-address forms such as SSA <ref> [13] </ref> is well understood, and the same techniques will not apply directly to arbitrary C expressions. Second, embedded control flow, though structured, may complicate the algorithms that must use this expression granularity.
Reference: [14] <author> A. Deutsch. </author> <title> Interprocedural may-alias analysis for pointers: beyond k-limiting. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Programming Languages Design and Implementation, </booktitle> <pages> pages 230-241, </pages> <year> 1994. </year> <journal> SIGPLAN Notices, </journal> <volume> 29(6). </volume>
Reference-contexts: Unfortunately, precise aliasing information has been shown to be uncomputable [39], and I could find no existing demand-driven techniques for approximating alias relationships. Many exhaustive approaches for conservatively computing potential aliases have time and space complexity of O (n 2 ), or even O (n 3 ) <ref> [14] </ref>, where n is the number of variables in the program. Such methods appear to be infeasible for use on large programs. Instead, Cstructure uses a simple equivalence class scheme similar to the one given in [2].
Reference: [15] <author> E. Duesterwald, R. Gupta, and M. L. Soffa. </author> <title> Demand-driven computation of interprocedural data flow. </title> <booktitle> In Conference Record of the 22nd ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 37-48, </pages> <year> 1995. </year>
Reference-contexts: A more ambitious use of the demand-driven paradigm involves reformulating the underlying algorithms to answer individual queries on demand. Recent work on demand-driven data flow analysis provides an alternative to the exhaustive computation of data flow information <ref> [15, 16, 49] </ref>. These approaches answer single data flow queries at individual program locations, one at a time. An interactive software refactory based on such algorithms would not require a data flow representation of the program such as a PDG. <p> A simple tree traversal can be used to find the execution status of each sub-expression, and can be easily combined with demand-driven variable use or definition searches. This approach works well with Duesterwald, Gupta and Soffa's demand-driven data flow analysis framework <ref> [15] </ref>. Their framework does not require iteration over an entire control flow graph, and so makes use of virtual control flow attractive. The query algorithm uses a worklist to track expressions as they are encountered. <p> This chapter shows how to answer data flow queries on demand, directly from the AST, with the aid of virtual control flow. This approach, based on a recently developed demand-driven data flow framework <ref> [15] </ref>, eliminates the need for construction or storage of any program representation other than the abstract syntax tree. 44 45 The presence of pointers in languages such as C leads to the possibility that different expressions can refer to the same memory location, making the expressions aliases of each other. <p> Cstructure implements two such algorithms, one for reaching definitions and the other for reachable uses. Both algorithms are based on a demand-driven interprocedural data flow analysis framework <ref> [15] </ref>, modified to use virtual control flow. 46 The next section shows just how brief demand-driven computations can be, using the reaching definitions example shown in Figure 3.1. Definitions are expressions such as assignments or increments that modify the value stored in some memory location (or lvalue). <p> Reverse summary functions abstract all the necessary facts about a call's influence on a reaching definitions query. They are computed on demand, one per function, when a call to that function is first encountered <ref> [15] </ref>. By retaining reverse summary functions for the life of a query, the saved summary information can be used directly if the search reaches another call to the same function. Cstructure creates three sets each time a new function is analyzed, ALL, M OD, and KILL. <p> However, the factored solution may be more convenient for either display of the results, or their use in transformations. Reverse summary functions could be saved between queries to reduce later computation <ref> [15] </ref>, but Cstructure does not use this optimization. After each reaching definitions query, all these sets and all hash table entries are deleted. Doing so avoids the expense of tracking and updating this information every time the program is modified. <p> For some purposes, such as the check for transformation move statement (Chapter 4), the actual uses are not of interest, only whether any uses are reachable. Rather than computing all uses and then checking the size of the resulting set, exploiting early termination can substantially reduce query response time <ref> [15] </ref>. Normally, when a use is encountered during a reachable uses query, the expression in question is added to the solution set and the search continues.
Reference: [16] <author> E. Duesterwald, R. Gupta, and M. L. Soffa. </author> <title> A demand-driven analyzer for data flow testing at the integration level. </title> <booktitle> In Proceedings of the 18th International Conference on Software Engineering, </booktitle> <pages> pages 575-584, </pages> <year> 1996. </year>
Reference-contexts: A more ambitious use of the demand-driven paradigm involves reformulating the underlying algorithms to answer individual queries on demand. Recent work on demand-driven data flow analysis provides an alternative to the exhaustive computation of data flow information <ref> [15, 16, 49] </ref>. These approaches answer single data flow queries at individual program locations, one at a time. An interactive software refactory based on such algorithms would not require a data flow representation of the program such as a PDG. <p> Exhaustive forward propagation is replaced with a goal-oriented backward search. The search is triggered by a query for the definitions that reach a selected node and terminates as soon as all nodes that contain a relevant definition have been found <ref> [16] </ref>. The demand-driven technique is basically a breadth-first search through the program's control flow starting at the initial (selected) statement. This search may include any number of lvalues. The algorithm simultaneously checks for all interesting definitions, adding each one encountered to the solution. <p> Doing so avoids the expense of tracking and updating this information every time the program is modified. Again, the main reason for using a demand-driven approach was to reduce storage and updating costs. 3.2.4 Reachable Uses Reachable uses are the dual of reaching definitions <ref> [16] </ref>. A use is an expression that reads the value of an object stored in memory. That use is reachable from a given point in the program if the memory location referred to could remain 55 unmodified between the two.
Reference: [17] <author> J. Ferrante, K. J. Ottenstein, and J. D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: It can be constructed directly from an AST or during parsing, and serves as the basis for many data flow analysis algorithms. * Program Dependence Graph (PDG) <ref> [17, 48] </ref>. More abstract still, the PDG shows only the control and data dependences between program operations. Building a PDG requires both control flow information (e.g. a CFG) and data flow analysis. The PDG is useful for compiler optimizations [17], program slicing [48] and integrating multiple program versions [29]. <p> More abstract still, the PDG shows only the control and data dependences between program operations. Building a PDG requires both control flow information (e.g. a CFG) and data flow analysis. The PDG is useful for compiler optimizations <ref> [17] </ref>, program slicing [48] and integrating multiple program versions [29]. All of these representations are exhaustive in nature. That is, they represent a portion of the program (often a single function) in its entirety. <p> Rederiving source code from other representations is more difficult. For example, the PDG has been used as the basis of an algorithm for merging program versions [29, 58, 59]. However, the PDG only stores dependences between compu-tationally related program components <ref> [17] </ref>, ignoring explicit statement order. To reconstitute the merged program (as an AST), all operations must first be ordered. Unfortunately, this ordering step is NP-complete, and may also fail if the PDG is not feasible (does not correspond to any possible program). <p> Data dependences are typically computed between atomic operations in a program, as in a data dependence graph [38], or a program dependence graph <ref> [17, 28, 48] </ref>. However, dependences may exist between any arbitrary program pieces, or program components, from individual expressions to loops or compound statements. A program component is any node in the AST, and refers to its entire subtree as a unit.
Reference: [18] <author> Keith Brian Gallagher and James R. Lyle. </author> <title> Using program slicing in software maintenance. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(8) </volume> <pages> 751-761, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: More precise flow-insensitive algorithms than the simple one implemented in Cstructure 111 exist, but their potential impact is unclear. Several other software tools could benefit from elimination of expensive program representations, including interactive program understanding and visualization tools such as slicers <ref> [18, 57] </ref>. This approach is a departure from the trend toward building all-inclusive data structures for these tools. However, none of these architectures has seen widespread use in commercial applications.
Reference: [19] <author> Jason I. Gobat and Darren C. Atkinson. FElt: </author> <title> User's guide and reference manual. </title> <type> Technical Report CS94-376, </type> <institution> Department of Computer Science and Engineering, University of California, </institution> <address> San Diego, </address> <year> 1994. </year>
Reference-contexts: These programs were obtained by anonymous ftp from their maintainers. The velvet application is the primary user interface to the FElt system, a finite element analysis package <ref> [19] </ref>. FElt information may be found online, including a postscript version of the user's manual. 1 The other two programs, GNU emacs and gcc, are maintained by the Free Software Foundation. 2 Emacs is a mature, multipurpose editor with a built-in lisp interpreter.
Reference: [20] <author> W. G. Griswold. </author> <title> Program Restructuring to Aid Software Maintenance. </title> <type> PhD thesis, </type> <institution> University of Washington, Dept. of Computer Science & Engineering, </institution> <year> 1991. </year> <note> Tech. Report No. 91-08-04. </note>
Reference-contexts: These tools only support batch transformations involving control flow constructs, and thus have limited utility. Newer tools take advantage of additional semantic information in order to support more sophisticated refactorings for both object-oriented [32, 43, 46, 47] and procedural languages <ref> [7, 20, 24, 41] </ref>. Some of these tools also give the user interactive control over when and where individual transformations are performed, rather than blindly restructuring at every opportunity. Many low-level refactoring transformations, such as renaming identifiers or restructuring class hierarchies, require only limited semantic information [3, 47]. <p> One such tool|Griswold's restructuring prototype for Scheme programs| supports meaning-preserving structural changes such as code motion, function extraction and inlining, global parameter addition and deletion, as well as variable 5 renaming <ref> [20, 24] </ref>. The tool obtains needed semantic information from exhaustive program representations built following interprocedural data flow analysis. High-level object-oriented refactorings involving class invariants also require data flow analysis [47]. <p> Instead, the source code for the program could be completely rederived from some other representation. For instance, Griswold's prototype <ref> [20, 24] </ref>, Opdyke's refactory [47], and REFINE/Cobol [41] all use the AST to display or output the source. Rederiving source code from other representations is more difficult. For example, the PDG has been used as the basis of an algorithm for merging program versions [29, 58, 59]. <p> Such a method may preclude manipulation of the program. The mappings also consume memory resources. Third, if the tool in question manipulates the program, some technique is needed to keep all the representations consistent. Recomputation of some representations on demand is simple, yet can be costly <ref> [20] </ref>. Incremental update has proved successful in keeping computation costs low, but is complex and costly to implement [21]. Additionally, all necessary representations must still be constructed before any transformations may be performed. <p> Section 4.4 describes in more detail the design of the individual transformation checks as implemented in the Cstructure prototype. 4.2 General Transformation Design Prior research has shown the feasibility of building meaning-preserving transformations in a source-level software tool <ref> [20, 24] </ref>. Griswold's approach divides the design of each transformation into two separate pieces: the checks needed to determine the legality of the desired change, and the actual manipulation of the internal program representation (s). <p> Griswold used a program dependence graph (PDG) to reason about data flow dependences when he designed his transformations <ref> [20, 24] </ref>. The PDG he used [40] represents dependence information between each of the program's primitive operations (as represented by three address code).
Reference: [21] <author> W. G. Griswold. </author> <title> Direct update of dataflow representations for a meaning-preserving program restructuring tool. </title> <booktitle> In ACM SIGSOFT '93 Symposium on the Foundations of Software Engineering, 1993. Software Engineering Notes, </booktitle> <volume> 18(5) </volume> <pages> 42-55, </pages> <month> December </month> <year> 1993. </year> <month> 114 </month>
Reference-contexts: Opdyke also mentions the importance of speed in a tool supporting design exploration [47]. This goal is not met by the 10 minute response time of the REFINE/Cobol modularization tool [41], but Griswold's enhanced tool can perform transformations within a time frame of a few seconds <ref> [21] </ref>. However, Griswold's approach suffers from a different problem|the lack of scalability. Scalability is an important property for any software tool architecture. To be scalable, the internal data structures of the tool must grow (approximately) linearly with the size of the program being manipulated. <p> The initial prototype rebuilt the CFG, PDG and mappings after every transforma 11 tion, but this approach proved very slow. To achieve interactive performance, an incremental update technique was added to keep the representations consistent <ref> [21] </ref>. Such a technique adds to tool complexity and increases the difficulty of verifying correct tool operation. The modularization tool based on REFINE/Cobol also uses separate AST and CFG representations, and some unspecified means of storing data flow analysis results [41]. <p> Third, if the tool in question manipulates the program, some technique is needed to keep all the representations consistent. Recomputation of some representations on demand is simple, yet can be costly [20]. Incremental update has proved successful in keeping computation costs low, but is complex and costly to implement <ref> [21] </ref>. Additionally, all necessary representations must still be constructed before any transformations may be performed. Finally, tools that use multiple representations are difficult to design and implement due to the complexity created by all of the above considerations [25]. <p> Unfortunately, the memory used by these program representations limits the size of programs the prototype can handle, thus motivating the present research. In addition, completing a transforma 71 tion involves maintaining consistency between all the program representations, which adds complexity to the tool implementation <ref> [21] </ref>. An earlier approach based on complete recomputation of the CFG/PDG representations following each transformation proved quite slow. Implementing a transformation in the Cstructure prototype presents somewhat different design issues.
Reference: [22] <author> W. G. Griswold, M. I. Chen, R. W. Bowdidge, and J. D. Morgenthaler. </author> <title> Tool support for planning the restructuring of data abstractions in large systems. </title> <booktitle> In Proceedings of the Fourth ACM SIGSOFT Symposium on the Foundations of Software Engineering (FSE4), 1996. Software Engineering Notes, </booktitle> <volume> 21(6) </volume> <pages> 33-45, </pages> <month> November </month> <year> 1996. </year>
Reference-contexts: This architecture is also well suited for code understanding and visualization tools that utilize static analysis, and has served as the underlying infrastructure for a C star diagram tool <ref> [22] </ref>. 1.1 Motivation 1.1.1 Understanding is Key to Software Maintenance Hundreds of billions of dollars are spent every year on computer software. Creating and modifying that software has become an activity of major economic importance. <p> Future work includes the design and implementation in Cstructure of a complete set of general refactorings, and its use in realistic software maintenance situations. Another research direction involves the integration of restructuring transformations with other program visualization techniques such as the star diagram <ref> [7, 8, 22] </ref>. The conclusion (Chapter 6) discusses these and other issues.
Reference: [23] <author> W. G. Griswold and D. Notkin. </author> <title> Computer-aided vs. manual program restructuring. </title> <booktitle> ACM SIGSOFT Software Engineering Notes, </booktitle> <volume> 17(1) </volume> <pages> 33-41, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: A compiler optimization affects the performance of the machine code, though it is often applied to some intermediate program representation. Restructuring software by hand is tedious and error prone, and can benefit from the use of automated tools <ref> [7, 23] </ref>. Early automated restructuring tools had limited capabilities, such as the removal of goto's from unstructured code [10, 44]. These tools only support batch transformations involving control flow constructs, and thus have limited utility.
Reference: [24] <author> W. G. Griswold and D. Notkin. </author> <title> Automated assistance for program restructuring. </title> <journal> ACM Transactions on Software Engineering and Methodology, </journal> <volume> 2(3) </volume> <pages> 228-269, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: These tools only support batch transformations involving control flow constructs, and thus have limited utility. Newer tools take advantage of additional semantic information in order to support more sophisticated refactorings for both object-oriented [32, 43, 46, 47] and procedural languages <ref> [7, 20, 24, 41] </ref>. Some of these tools also give the user interactive control over when and where individual transformations are performed, rather than blindly restructuring at every opportunity. Many low-level refactoring transformations, such as renaming identifiers or restructuring class hierarchies, require only limited semantic information [3, 47]. <p> One such tool|Griswold's restructuring prototype for Scheme programs| supports meaning-preserving structural changes such as code motion, function extraction and inlining, global parameter addition and deletion, as well as variable 5 renaming <ref> [20, 24] </ref>. The tool obtains needed semantic information from exhaustive program representations built following interprocedural data flow analysis. High-level object-oriented refactorings involving class invariants also require data flow analysis [47]. <p> Instead, the source code for the program could be completely rederived from some other representation. For instance, Griswold's prototype <ref> [20, 24] </ref>, Opdyke's refactory [47], and REFINE/Cobol [41] all use the AST to display or output the source. Rederiving source code from other representations is more difficult. For example, the PDG has been used as the basis of an algorithm for merging program versions [29, 58, 59]. <p> This flow of values from statement to statement and variable to variable results in semantic dependencies between program components. Information about such dependencies can be used in a software tool to support source-level program understanding [5] and transformation tasks <ref> [24, 32] </ref>, or by a compiler to support optimizations [2]. For instance, knowledge of the locations where a given statement's variables could have been defined last can help determine if that statement may be moved without altering the behavior of the program. <p> Section 4.4 describes in more detail the design of the individual transformation checks as implemented in the Cstructure prototype. 4.2 General Transformation Design Prior research has shown the feasibility of building meaning-preserving transformations in a source-level software tool <ref> [20, 24] </ref>. Griswold's approach divides the design of each transformation into two separate pieces: the checks needed to determine the legality of the desired change, and the actual manipulation of the internal program representation (s). <p> Griswold used a program dependence graph (PDG) to reason about data flow dependences when he designed his transformations <ref> [20, 24] </ref>. The PDG he used [40] represents dependence information between each of the program's primitive operations (as represented by three address code). <p> However, only a handful of constructs (e.g. increments, assignments, function calls) require special treatment, as shown in Chapter 3 for ANSI C. 6.2.3 Program Transformation at Interactive Speed User-directed, automated program transformation is an emerging technology <ref> [24, 32, 41] </ref> that requires static analysis information. Data flow analysis is costly, even when demand-driven. In general, previous tools that need this information are rather heavy-weight, that is, slow and/or large memory hogs.
Reference: [25] <author> W. G. Griswold and D. Notkin. </author> <title> Architectural tradeoffs for a meaning-preserving program restructuring tool. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 21(4) </volume> <pages> 275-287, </pages> <year> 1995. </year>
Reference-contexts: To reason about information typically contained in other program representations, a tool can simply manage multiple data structures. 1.3.1 Multiple Representations Restructuring tools have been built using multiple internal program representations. Such a tool architecture leads to design tradeoffs, as was the case in Griswold's Scheme restructuring prototype <ref> [25] </ref>. That tool uses all three representations mentioned above, the AST, CFG, and PDG, together with two-way mappings to interrelate them. The mappings becomes problematic when the program is being modified, as all representations and the mappings between them must be updated. <p> Additionally, all necessary representations must still be constructed before any transformations may be performed. Finally, tools that use multiple representations are difficult to design and implement due to the complexity created by all of the above considerations <ref> [25] </ref>. Tracking down errors due to divergence of program representations after a long series of transformations could be particularly troublesome. 12 1.3.2 Combined Representations Yet another possibility is to combine several representations into a single, all-purpose graph.
Reference: [26] <author> Samuel P. Harbison and Jr. Guy L. Steele. </author> <title> C, a Reference Manual. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, third edition, </address> <year> 1991. </year>
Reference-contexts: expression in a return statement, and the control expressions in a condition, iterative, or switch statement (including each expression in a for statement) * after the first operand of a &&, ||, ?:, or comma operator * after the evaluation of the arguments and function expression in a function call <ref> [26] </ref> A sequence point thus represents a special kind of program point with well specified semantics. Given a particular sequence point, the previous and subsequent instructions to execute are only defined by ANSI C in terms of full expressions or the operands of logic expressions. <p> Definitions are expressions such as assignments or increments that modify the value stored in some memory location (or lvalue). In C, lvalues are the expressions that refer to objects in memory <ref> [26, 34] </ref>. At any point in the program, the reaching definitions of an lvalue are those definitions that could have last modified that particular location in memory. 3.2.1 Computing Reaching Definitions on Demand of the variable i and its two reaching definitions highlighted.
Reference: [27] <author> Mary Jean Harrold and Brian Malloy. </author> <title> A unified interprocedural program representation for a maintenance environment. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 19(6) </volume> <pages> 584-593, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: The two mentioned below were specifically formulated for use in software maintenance environments. The unified interprocedural graph (UIG) "extracts the important features of existing program representations, and adds new information, to provide an integrated representation for maintenance tasks <ref> [27] </ref>." Constructed only for specified groups of procedures on demand, this single intermediate representation combines relationships found in a call graph, a program summary graph [11], an interproce-dural flow graph [31] and a system dependence graph [30]. <p> As mentioned earlier, this approach has been applied to the construction of complete program representations (AST and CFG) in a large-scale program slicer [5]. An implementation of the UIG in a maintenance environment incrementally constructs subgraphs for specified procedures when needed for a particular tool <ref> [27] </ref>. Both of these uses of demand-driven 14 computation operate at the granularity of a full procedure or function, taking advantage of the factored nature of most computer programs. Factoring program representations in this way avoids the construction of unneeded pieces without changing the fundamental analysis techniques.
Reference: [28] <author> S. Horwitz, J. Prins, and T. Reps. </author> <title> On the adequacy of program dependence graphs for representing programs. </title> <booktitle> In Proceedings of the 15th Symposium on Principles of Programming Languages, </booktitle> <pages> pages 146-157, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Data dependences are typically computed between atomic operations in a program, as in a data dependence graph [38], or a program dependence graph <ref> [17, 28, 48] </ref>. However, dependences may exist between any arbitrary program pieces, or program components, from individual expressions to loops or compound statements. A program component is any node in the AST, and refers to its entire subtree as a unit. <p> I will introduce the other types of data dependence as the need arises. Dependences may also be classified as loop carried or loop independent <ref> [28] </ref>. A loop carried dependence connects components that execute on different iterations of a loop. A loop independent dependence connects components without consideration of loop iterations. Rearranging statement order may not alter this aspect of any existing dependence. <p> As previously mentioned, the first two cases are symmetric. Simply stated, no flow dependence is allowed between A and B or between B and A at any time. The final case relates to both the standard output dependence [37], and the more general def-order dependence <ref> [28, 29] </ref>. Like an output dependence, both A and B define the same lvalue. Unlike an output dependence, the lvalue of interest must be live outside A or B, because I require the existence of some reachable use. <p> A def-order dependence, like an output dependence, requires the related statements both define the same lvalue. In addition, both definitions must reach the same use <ref> [28] </ref>. This condition is too strong to use in checking the movability of arbitrary program components, though it suffices between the individual program operations in a PDG. I need an even more general dependence definition, which I will call a live output dependence.
Reference: [29] <author> S. Horwitz, J. Prins, and T. Reps. </author> <title> Integrating noninterfering versions of programs. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <pages> pages 345-387, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: More abstract still, the PDG shows only the control and data dependences between program operations. Building a PDG requires both control flow information (e.g. a CFG) and data flow analysis. The PDG is useful for compiler optimizations [17], program slicing [48] and integrating multiple program versions <ref> [29] </ref>. All of these representations are exhaustive in nature. That is, they represent a portion of the program (often a single function) in its entirety. Every fact of interest appears explicitly in these data structures, usually as an edge between two nodes. <p> For instance, Griswold's prototype [20, 24], Opdyke's refactory [47], and REFINE/Cobol [41] all use the AST to display or output the source. Rederiving source code from other representations is more difficult. For example, the PDG has been used as the basis of an algorithm for merging program versions <ref> [29, 58, 59] </ref>. However, the PDG only stores dependences between compu-tationally related program components [17], ignoring explicit statement order. To reconstitute the merged program (as an AST), all operations must first be ordered. <p> As previously mentioned, the first two cases are symmetric. Simply stated, no flow dependence is allowed between A and B or between B and A at any time. The final case relates to both the standard output dependence [37], and the more general def-order dependence <ref> [28, 29] </ref>. Like an output dependence, both A and B define the same lvalue. Unlike an output dependence, the lvalue of interest must be live outside A or B, because I require the existence of some reachable use.
Reference: [30] <author> S. Horwitz, T. Reps, and D. Binkley. </author> <title> Interprocedural slicing using dependence graphs. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(1) </volume> <pages> 26-60, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: program representations, and adds new information, to provide an integrated representation for maintenance tasks [27]." Constructed only for specified groups of procedures on demand, this single intermediate representation combines relationships found in a call graph, a program summary graph [11], an interproce-dural flow graph [31] and a system dependence graph <ref> [30] </ref>. As the UIG is based on a call graph, the program's syntax is not present in this representation. Data flow dependencies are explicitly represented, which implies that the UIG could be quite large and costly to compute. The authors do not discuss construction or storage costs for the UIG.
Reference: [31] <author> Harrold M. J. and M. L. Soffa. </author> <title> Computation of interprocedural definition and use dependences. </title> <booktitle> In Proceedings of the IEEE Computer Society 1990 International Conference on Computer Languages, </booktitle> <pages> pages 297-306, </pages> <year> 1990. </year>
Reference-contexts: "extracts the important features of existing program representations, and adds new information, to provide an integrated representation for maintenance tasks [27]." Constructed only for specified groups of procedures on demand, this single intermediate representation combines relationships found in a call graph, a program summary graph [11], an interproce-dural flow graph <ref> [31] </ref> and a system dependence graph [30]. As the UIG is based on a call graph, the program's syntax is not present in this representation. Data flow dependencies are explicitly represented, which implies that the UIG could be quite large and costly to compute.
Reference: [32] <author> R. E. Johnson and W. F. Opdyke. </author> <title> Refactoring and aggregation. </title> <booktitle> In Proceedings of the First JSSST International Symposium on Object Technologies for Advanced Software, </booktitle> <pages> pages 264-278, </pages> <year> 1993. </year>
Reference-contexts: With such a broad definition, many types of maintenance activities could be considered restructuring. The narrower concept of refactoring better describes the specific 4 tasks explored in this dissertation: "A refactoring is a program transformation that reorganizes a program without changing its behavior <ref> [32] </ref>." Refactorings specifically apply to source code, but note the similarity to another well-known application that performs program transformations: an optimizing compiler. Each requires a detailed understanding of program semantics, and each holds certain program behavior invariant. <p> These tools only support batch transformations involving control flow constructs, and thus have limited utility. Newer tools take advantage of additional semantic information in order to support more sophisticated refactorings for both object-oriented <ref> [32, 43, 46, 47] </ref> and procedural languages [7, 20, 24, 41]. Some of these tools also give the user interactive control over when and where individual transformations are performed, rather than blindly restructuring at every opportunity. <p> For example, changing the name of a function, or moving a function from one class to another, would change all the places that the function was called <ref> [32] </ref>. As with any program editor, the user directs a refactory to affect individual transformations, one at a time. However, high-level refactoring transformations are more powerful than relatively simple word processing commands. The tool must verify that program behavior will remain unchanged, or make any needed compensating 6 changes. <p> This flow of values from statement to statement and variable to variable results in semantic dependencies between program components. Information about such dependencies can be used in a software tool to support source-level program understanding [5] and transformation tasks <ref> [24, 32] </ref>, or by a compiler to support optimizations [2]. For instance, knowledge of the locations where a given statement's variables could have been defined last can help determine if that statement may be moved without altering the behavior of the program. <p> The need for similar generic methods in software re-engineering has recently been mentioned [54], but may be misplaced. Software manipulation such as restructuring or refactoring appears to be inherently dependent on the particular programming language being used <ref> [32] </ref>. Language-dependence is also clearly an intrinsic aspect of virtual control flow. Virtual control flow, and data flow analysis based on it, require customization for use with each different language. <p> However, only a handful of constructs (e.g. increments, assignments, function calls) require special treatment, as shown in Chapter 3 for ANSI C. 6.2.3 Program Transformation at Interactive Speed User-directed, automated program transformation is an emerging technology <ref> [24, 32, 41] </ref> that requires static analysis information. Data flow analysis is costly, even when demand-driven. In general, previous tools that need this information are rather heavy-weight, that is, slow and/or large memory hogs.
Reference: [33] <author> L. A. Kappelman and J. J. </author> <title> Cappel. </title> <journal> Confronting the year 2000 issue. Journal of Systems Management, </journal> <volume> 47(4) </volume> <pages> 4-13, </pages> <month> July-August </month> <year> 1996. </year> <month> 115 </month>
Reference-contexts: Successful software often lives well beyond the original purposes for which it was designed. The current "year 2000" problem provides an excellent example of this phenomenon <ref> [33, 42] </ref>. The initial developers of the affected systems did not foresee that their programs would still be in operation at the turn of the millennium.
Reference: [34] <author> Brian W. Kernighan and Dennis M. Ritchie. </author> <title> The C Programming Language. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <note> second edition, </note> <year> 1988. </year>
Reference-contexts: The demand-driven data flow framework used in chapter 3, which can be implemented on top of the intraprocedural virtual control flow method described here, solves this dilemma. 2.2 Specific ANSI C Issues As in the rest of this dissertation, the ANSI C programming language <ref> [34] </ref> will be used as a concrete example demonstrating the concepts behind virtual control. In some ways C is an ideal choice for illustrating these ideas, as it presents most of the difficulties inherent in existing imperative programming languages. <p> Full expressions in expression statements or the individual operands of logical expressions within control expressions are completely delineated by sequence points, and the control semantics between such sequence points are clearly specified in the C language definition <ref> [34] </ref>. <p> Definitions are expressions such as assignments or increments that modify the value stored in some memory location (or lvalue). In C, lvalues are the expressions that refer to objects in memory <ref> [26, 34] </ref>. At any point in the program, the reaching definitions of an lvalue are those definitions that could have last modified that particular location in memory. 3.2.1 Computing Reaching Definitions on Demand of the variable i and its two reaching definitions highlighted.
Reference: [35] <author> D. Kinloch and M. Munro. </author> <title> A combined representation for the maintenance of C programs. </title> <booktitle> In Proceedings of the IEEE Second Workshop on Program Comprehension, </booktitle> <pages> pages 119-127, </pages> <year> 1993. </year>
Reference-contexts: The authors do not discuss construction or storage costs for the UIG. The Combined C Graph (CCG) is "a fine-grained intermediate representation for programs written in the C language from which program slices, call graph, flow-sensitive data flow, definition-use and control dependence views can be easily constructed" <ref> [35] </ref>. The CCG is an extension of the earlier UIG that attempts to overcome some of the UIG's limitations as applied to C programs by including pointers, structures and C function call semantics, as well as addressing side effects and control flow embedded within expressions. <p> As a single representation, the authors claim the following three advantages: * Eliminates redundant information. * Reduces access times to different representations. Any algorithm need only 13 access one representation. * Helps comprehension by incorporating all program relationships into one rep resentation <ref> [35, 36] </ref>. Again, the authors do not state storage requirements or CCG construction costs. Single, all-inclusive representations may create scalability problems, however. Computing and storing such a large amount of information may not be practical for large programs, even if demanded on a per-procedure basis.
Reference: [36] <author> D. A. Kinloch and M. Munro. </author> <title> Understanding C programs using the combined C graph representation. </title> <booktitle> In Proceedings of the International Conference on Software Maintenance, </booktitle> <pages> pages 172-180, </pages> <year> 1994. </year>
Reference-contexts: As a single representation, the authors claim the following three advantages: * Eliminates redundant information. * Reduces access times to different representations. Any algorithm need only 13 access one representation. * Helps comprehension by incorporating all program relationships into one rep resentation <ref> [35, 36] </ref>. Again, the authors do not state storage requirements or CCG construction costs. Single, all-inclusive representations may create scalability problems, however. Computing and storing such a large amount of information may not be practical for large programs, even if demanded on a per-procedure basis.
Reference: [37] <author> D. J. Kuck, R. H. Kuhn, B. Leasure, D. A. Padua, and M. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Proceedings of the 8th Symposium on Principles of Programming Languages, </booktitle> <pages> pages 207-218, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: Components may contain executable C code with uses or definitions of variables or other memory locations (lvalues). In constructing move statement's data dependence checks, I used the most general definitions of the standard data dependences <ref> [37] </ref>. These definitions apply to arbitrary program components rather than just individual operations or expression statements. <p> Central to all types of dependence is the concept of a flow dependence. 79 A program component Y is flow dependent on another component X if an lvalue defined in X may be used later in Y with no intervening redefinition <ref> [37, 38] </ref>. This definition implies that there must be some execution path from a definition in X to a use in Y containing no killing definitions. <p> If such a dependence existed prior to the move, it was loop carried, since A executes before B during a single iteration of any containing loop. After the move, this dependence becomes loop independent. The case where no such dependence existed initially is referred to as an antidependence <ref> [37] </ref>. 3. If both A and B define the same lvalue, and some other program component is flow dependent on either definition, then the move would affect this dependence. If the component in question is dependent on B but not A, it will become dependent on A following the move. <p> As previously mentioned, the first two cases are symmetric. Simply stated, no flow dependence is allowed between A and B or between B and A at any time. The final case relates to both the standard output dependence <ref> [37] </ref>, and the more general def-order dependence [28, 29]. Like an output dependence, both A and B define the same lvalue. Unlike an output dependence, the lvalue of interest must be live outside A or B, because I require the existence of some reachable use.
Reference: [38] <author> David J. Kuck. </author> <title> The Structure of Computers and Computations. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: Data dependences are typically computed between atomic operations in a program, as in a data dependence graph <ref> [38] </ref>, or a program dependence graph [17, 28, 48]. However, dependences may exist between any arbitrary program pieces, or program components, from individual expressions to loops or compound statements. A program component is any node in the AST, and refers to its entire subtree as a unit. <p> Central to all types of dependence is the concept of a flow dependence. 79 A program component Y is flow dependent on another component X if an lvalue defined in X may be used later in Y with no intervening redefinition <ref> [37, 38] </ref>. This definition implies that there must be some execution path from a definition in X to a use in Y containing no killing definitions.
Reference: [39] <author> W. Landi. </author> <title> Undecidability of static analysis. </title> <journal> ACM Letters on Programming Languages and Systems, </journal> <volume> 1(4) </volume> <pages> 323-337, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: Ideally, the data flow analyzer would like to compute the precise aliases of any particular lvalue on demand, at any desired program location. Unfortunately, precise aliasing information has been shown to be uncomputable <ref> [39] </ref>, and I could find no existing demand-driven techniques for approximating alias relationships. Many exhaustive approaches for conservatively computing potential aliases have time and space complexity of O (n 2 ), or even O (n 3 ) [14], where n is the number of variables in the program.
Reference: [40] <author> J. R. Larus. </author> <title> Restructuring Symbolic Programs for Concurrent Execution on Multiprocessors. </title> <type> PhD thesis, </type> <institution> UC Berkeley Computer Science, </institution> <month> May </month> <year> 1989. </year> <note> Also Technical Report No. UCB/CSD 89/502. </note>
Reference-contexts: Griswold used a program dependence graph (PDG) to reason about data flow dependences when he designed his transformations [20, 24]. The PDG he used <ref> [40] </ref> represents dependence information between each of the program's primitive operations (as represented by three address code).
Reference: [41] <author> Lawrence Markosian, Phillip Newcomb, Russell Brand, Scott Burson, and Ted Kitzmiller. </author> <title> Using an enabling technology to reengineer legacy systems. </title> <journal> Communications of the ACM, </journal> <volume> 37(5) </volume> <pages> 58-70, </pages> <year> 1994. </year>
Reference-contexts: These tools only support batch transformations involving control flow constructs, and thus have limited utility. Newer tools take advantage of additional semantic information in order to support more sophisticated refactorings for both object-oriented [32, 43, 46, 47] and procedural languages <ref> [7, 20, 24, 41] </ref>. Some of these tools also give the user interactive control over when and where individual transformations are performed, rather than blindly restructuring at every opportunity. Many low-level refactoring transformations, such as renaming identifiers or restructuring class hierarchies, require only limited semantic information [3, 47]. <p> High-level object-oriented refactorings involving class invariants also require data flow analysis [47]. At least one commercial product, an extension of REFINE/Cobol, uses data flow analysis to automate an existing manual modularization process (essentially a refactoring) for the Boeing Payroll Pilot Project <ref> [41] </ref>. The program flow analysis required by such tools is often global in nature, and must be applied on a much larger scale than found in a compiler. This analysis is simply too costly using traditional approaches. <p> For instance, Griswold's prototype only operates on small (hundreds of lines) Scheme programs without exhausting swap space, and even these programs require over 20 minutes for tool initialization. The extended REFINE/Cobol reportedly uses 300 MB of swap space to modularize a 40,000 line Cobol system <ref> [41] </ref>. An unoptimized version of this tool required 190 minutes to perform a single modularization step, which was reduced to 10 minutes after optimizations based on specific programming style. <p> Opdyke also mentions the importance of speed in a tool supporting design exploration [47]. This goal is not met by the 10 minute response time of the REFINE/Cobol modularization tool <ref> [41] </ref>, but Griswold's enhanced tool can perform transformations within a time frame of a few seconds [21]. However, Griswold's approach suffers from a different problem|the lack of scalability. Scalability is an important property for any software tool architecture. <p> Instead, the source code for the program could be completely rederived from some other representation. For instance, Griswold's prototype [20, 24], Opdyke's refactory [47], and REFINE/Cobol <ref> [41] </ref> all use the AST to display or output the source. Rederiving source code from other representations is more difficult. For example, the PDG has been used as the basis of an algorithm for merging program versions [29, 58, 59]. <p> Such a technique adds to tool complexity and increases the difficulty of verifying correct tool operation. The modularization tool based on REFINE/Cobol also uses separate AST and CFG representations, and some unspecified means of storing data flow analysis results <ref> [41] </ref>. Based on the times and memory usage reported by the authors, this approach uses traditional exhaustive data flow analysis techniques. Although they provide necessary semantic information, using multiple program representations in a software manipulation tool poses a number of serious drawbacks. <p> However, only a handful of constructs (e.g. increments, assignments, function calls) require special treatment, as shown in Chapter 3 for ANSI C. 6.2.3 Program Transformation at Interactive Speed User-directed, automated program transformation is an emerging technology <ref> [24, 32, 41] </ref> that requires static analysis information. Data flow analysis is costly, even when demand-driven. In general, previous tools that need this information are rather heavy-weight, that is, slow and/or large memory hogs.
Reference: [42] <author> R. A. Martin. </author> <title> Dealing with dates: solutions for the Year 2000. </title> <journal> IEEE Computer, </journal> <volume> 30(3) </volume> <pages> 44-51, </pages> <month> March </month> <year> 1997. </year>
Reference-contexts: Successful software often lives well beyond the original purposes for which it was designed. The current "year 2000" problem provides an excellent example of this phenomenon <ref> [33, 42] </ref>. The initial developers of the affected systems did not foresee that their programs would still be in operation at the turn of the millennium.
Reference: [43] <author> I. Moore. </author> <title> Automatic inheritance hierarchy restructuring and method refactor-ing. </title> <booktitle> In Proceedings of the Conference on Object Oriented Programming Systems, Languages and Applications, </booktitle> <pages> pages 235-250, </pages> <month> October </month> <year> 1996. </year> <journal> SIGPLAN Notices, </journal> <volume> 31(10). </volume>
Reference-contexts: These tools only support batch transformations involving control flow constructs, and thus have limited utility. Newer tools take advantage of additional semantic information in order to support more sophisticated refactorings for both object-oriented <ref> [32, 43, 46, 47] </ref> and procedural languages [7, 20, 24, 41]. Some of these tools also give the user interactive control over when and where individual transformations are performed, rather than blindly restructuring at every opportunity.
Reference: [44] <author> H. W. Morgan. </author> <title> Evolution of a software maintenance tool. </title> <booktitle> In Proceedings of the Second National Conference EDP Software Maintenance, </booktitle> <pages> pages 268-278, </pages> <year> 1984. </year>
Reference-contexts: Restructuring software by hand is tedious and error prone, and can benefit from the use of automated tools [7, 23]. Early automated restructuring tools had limited capabilities, such as the removal of goto's from unstructured code <ref> [10, 44] </ref>. These tools only support batch transformations involving control flow constructs, and thus have limited utility. Newer tools take advantage of additional semantic information in order to support more sophisticated refactorings for both object-oriented [32, 43, 46, 47] and procedural languages [7, 20, 24, 41].
Reference: [45] <author> Steven S. Muchnick and Neil D. Jones. </author> <title> Program Flow Analysis: Theory and Applications. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: The cited examples of demand-driven analysis are also based on the usual control flow graph. This expectation may be explained by the observation that "program flow analysis originated as a technique to statically determine properties of programs to be exploited in the optimization phase of a compiler <ref> [45, page 1] </ref>." A control flow representation such as a CFG is an ideal intermediate representation for a compiler. Information unimportant to optimization and code generation is abstracted away during CFG construction, yet just those properties essential to compilation remain.
Reference: [46] <author> W. F. Opdyke and R. E. Johnson. </author> <title> Refactoring: An aid in designing application frameworks and evolving object-oriented systems. </title> <booktitle> In Proceedings of the 1990 Symposium on Object-Oriented Programming Emphasizing Practical Applications, </booktitle> <pages> pages 274-282, </pages> <month> September </month> <year> 1990. </year> <month> 116 </month>
Reference-contexts: These tools only support batch transformations involving control flow constructs, and thus have limited utility. Newer tools take advantage of additional semantic information in order to support more sophisticated refactorings for both object-oriented <ref> [32, 43, 46, 47] </ref> and procedural languages [7, 20, 24, 41]. Some of these tools also give the user interactive control over when and where individual transformations are performed, rather than blindly restructuring at every opportunity.
Reference: [47] <author> William F. Opdyke. </author> <title> Refactoring Object-Oriented Frameworks. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <year> 1992. </year>
Reference-contexts: These tools only support batch transformations involving control flow constructs, and thus have limited utility. Newer tools take advantage of additional semantic information in order to support more sophisticated refactorings for both object-oriented <ref> [32, 43, 46, 47] </ref> and procedural languages [7, 20, 24, 41]. Some of these tools also give the user interactive control over when and where individual transformations are performed, rather than blindly restructuring at every opportunity. <p> Some of these tools also give the user interactive control over when and where individual transformations are performed, rather than blindly restructuring at every opportunity. Many low-level refactoring transformations, such as renaming identifiers or restructuring class hierarchies, require only limited semantic information <ref> [3, 47] </ref>. However, any transformation that reorders computations requires information about control and data flow properties of the program being manipulated. Several tools that automatically restructure source code rely on static analysis (including data flow analysis) to reason about these semantic properties. <p> The tool obtains needed semantic information from exhaustive program representations built following interprocedural data flow analysis. High-level object-oriented refactorings involving class invariants also require data flow analysis <ref> [47] </ref>. At least one commercial product, an extension of REFINE/Cobol, uses data flow analysis to automate an existing manual modularization process (essentially a refactoring) for the Boeing Payroll Pilot Project [41]. <p> This style has been suggested as most appropriate for maintenance tasks involving large systems [7], which would definitely be supported by temporal immediacy. Opdyke also mentions the importance of speed in a tool supporting design exploration <ref> [47] </ref>. This goal is not met by the 10 minute response time of the REFINE/Cobol modularization tool [41], but Griswold's enhanced tool can perform transformations within a time frame of a few seconds [21]. However, Griswold's approach suffers from a different problem|the lack of scalability. <p> Instead, the source code for the program could be completely rederived from some other representation. For instance, Griswold's prototype [20, 24], Opdyke's refactory <ref> [47] </ref>, and REFINE/Cobol [41] all use the AST to display or output the source. Rederiving source code from other representations is more difficult. For example, the PDG has been used as the basis of an algorithm for merging program versions [29, 58, 59].
Reference: [48] <author> K. J. Ottenstein and L. M. Ottenstein. </author> <title> The program dependence graph in a software development environment. </title> <booktitle> In Proceedings of the ACM SIG-SOFT/SIGPLAN Software Engineering Symposium on Practical Software Development Environments, </booktitle> <pages> pages 177-184, </pages> <month> April </month> <year> 1984. </year>
Reference-contexts: It can be constructed directly from an AST or during parsing, and serves as the basis for many data flow analysis algorithms. * Program Dependence Graph (PDG) <ref> [17, 48] </ref>. More abstract still, the PDG shows only the control and data dependences between program operations. Building a PDG requires both control flow information (e.g. a CFG) and data flow analysis. The PDG is useful for compiler optimizations [17], program slicing [48] and integrating multiple program versions [29]. <p> More abstract still, the PDG shows only the control and data dependences between program operations. Building a PDG requires both control flow information (e.g. a CFG) and data flow analysis. The PDG is useful for compiler optimizations [17], program slicing <ref> [48] </ref> and integrating multiple program versions [29]. All of these representations are exhaustive in nature. That is, they represent a portion of the program (often a single function) in its entirety. Every fact of interest appears explicitly in these data structures, usually as an edge between two nodes. <p> Data dependences are typically computed between atomic operations in a program, as in a data dependence graph [38], or a program dependence graph <ref> [17, 28, 48] </ref>. However, dependences may exist between any arbitrary program pieces, or program components, from individual expressions to loops or compound statements. A program component is any node in the AST, and refers to its entire subtree as a unit.
Reference: [49] <author> T. Reps, S. Horwitz, and M. Sagiv. </author> <title> Precise interprocedural dataflow analysis via graph reachability. </title> <booktitle> In Conference Record of the 22nd ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 49-61, </pages> <year> 1995. </year>
Reference-contexts: A more ambitious use of the demand-driven paradigm involves reformulating the underlying algorithms to answer individual queries on demand. Recent work on demand-driven data flow analysis provides an alternative to the exhaustive computation of data flow information <ref> [15, 16, 49] </ref>. These approaches answer single data flow queries at individual program locations, one at a time. An interactive software refactory based on such algorithms would not require a data flow representation of the program such as a PDG.
Reference: [50] <author> Bjarne Steensgaard. </author> <title> Points-to analysis in almost linear time. </title> <type> Technical Report MSR-TR-95-08, </type> <institution> Microsoft Research, </institution> <year> 1995. </year>
Reference-contexts: Data flow analysis must take possible aliases into account if the analysis results are to be meaningful. Section 3.3 describes how to combine a fast points-to analysis technique based on type inference with demand-driven data flow analysis to conservatively estimate possible aliases <ref> [50] </ref>. Virtual control flow (Chapter 2) can support any CFG-based data flow analysis algorithm, as both a CFG and virtual control flow provide essentially equivalent information. However, combining demand-driven virtual control flow with exhaustive data flow analysis would defeat the purpose of computing control flow on demand. <p> This technique, based on the fast union-find algorithm [1], must be applied to the entire program before any data flow queries may be answered. The analysis uses non-standard type inference to assign each lvalue to a points-to equivalence class <ref> [50, 51, 52] </ref>. The resulting flow-insensitive approximation holds at every point in the program, and can be used to provide low precision alias information. <p> The analysis also includes function pointers, recording all functions each could possibly point to. When a function call is made through such a pointer, the algorithm takes into account all the functions that may be called, even before they are recorded or encountered. I extended the basic algorithm <ref> [50] </ref> to handle struct and union members, which could in the worst case result in exponential complexity if these aggregate types have arbitrary nesting depth.
Reference: [51] <author> Bjarne Steensgaard. </author> <title> Points-to analysis by type inference of programs with structures and unions. </title> <booktitle> In Proceedings of CC: International Conference on Compiler Construction, </booktitle> <pages> pages 136-150, </pages> <year> 1996. </year>
Reference-contexts: This technique, based on the fast union-find algorithm [1], must be applied to the entire program before any data flow queries may be answered. The analysis uses non-standard type inference to assign each lvalue to a points-to equivalence class <ref> [50, 51, 52] </ref>. The resulting flow-insensitive approximation holds at every point in the program, and can be used to provide low precision alias information. <p> However, since these types can be easily misused to incorrectly address memory locations (intentionally or not), this approach is necessary to assure that the analysis is conservative. A more accurate, though expensive, approach would be to track the offset of every member for every aggregate type <ref> [51] </ref>. I chose the simpler method of merging all classes. When every member of a struct points to the same equivalence class, strange relationships and one or more huge classes can result. <p> However, the use of low precision points-to analysis to capture potential aliases reduces the accuracy of these techniques. I implemented only the most basic version of this points-to analysis, and several improvements have been described elsewhere <ref> [51, 52] </ref>. Imprecision may cause the tool to reject otherwise valid transformations, if data dependence analysis reveals spurious changes to program behavior. Further research is needed to determine the impact that different levels of precision will have on the usability of an interactive software tool. <p> The flow-insensitive points-to analysis used in Cstructure has the major advantage of almost linear time complexity, but lacks precision in situations where pointers and unions are heavily used. Refinements of the basic algorithm are already available <ref> [51, 52] </ref>, and further improvements in precision may be possible at low computational cost through 110 use of hybrid approaches.
Reference: [52] <author> Bjarne Steensgaard. </author> <title> Points-to analysis in almost linear time. </title> <booktitle> In Proceedings of the 23rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 32-41, </pages> <year> 1996. </year>
Reference-contexts: This technique, based on the fast union-find algorithm [1], must be applied to the entire program before any data flow queries may be answered. The analysis uses non-standard type inference to assign each lvalue to a points-to equivalence class <ref> [50, 51, 52] </ref>. The resulting flow-insensitive approximation holds at every point in the program, and can be used to provide low precision alias information. <p> However, the use of low precision points-to analysis to capture potential aliases reduces the accuracy of these techniques. I implemented only the most basic version of this points-to analysis, and several improvements have been described elsewhere <ref> [51, 52] </ref>. Imprecision may cause the tool to reject otherwise valid transformations, if data dependence analysis reveals spurious changes to program behavior. Further research is needed to determine the impact that different levels of precision will have on the usability of an interactive software tool. <p> The flow-insensitive points-to analysis used in Cstructure has the major advantage of almost linear time complexity, but lacks precision in situations where pointers and unions are heavily used. Refinements of the basic algorithm are already available <ref> [51, 52] </ref>, and further improvements in precision may be possible at low computational cost through 110 use of hybrid approaches.
Reference: [53] <author> D. Ungar, H. Lieberman, and C. Fry. </author> <title> Debugging and the experience of immediacy. </title> <journal> Communications of the ACM, </journal> <volume> 40(4) </volume> <pages> 38-43, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: Thus, an expected benefit of developing less memory-intensive analysis techniques is to substantially reduce the time needed to refactor legacy systems automatically. 1.2 Interactive Tool Performance and Scalability Ideally, a software tool provides the user with the experience of immediacy <ref> [53] </ref>. That is, the distance in time, space or meaning between a user's action and the tool's corresponding reaction should be small enough for the user to recognize causality. A tool that requires several minutes to perform a program transformation cannot be considered interactive.
Reference: [54] <author> Mark van den Brand, Paul Klint, and Chris Verhoef. </author> <title> Re-engineering needs generic programming language technology. </title> <journal> SIGPLAN Notices, </journal> <volume> 32(2) </volume> <pages> 54-61, </pages> <month> February </month> <year> 1997. </year>
Reference-contexts: Finally, the CFG can serve as a uniform representation for use with many different programming languages. This allows the separation of responsibilities in the compiler into a language-dependent front end and a language-independent back end [2]. The need for similar generic methods in software re-engineering has recently been mentioned <ref> [54] </ref>, but may be misplaced. Software manipulation such as restructuring or refactoring appears to be inherently dependent on the particular programming language being used [32]. Language-dependence is also clearly an intrinsic aspect of virtual control flow.
Reference: [55] <author> L. I. Vanek and M. N. Culp. </author> <title> Static analysis of program source code using EDSA. </title> <booktitle> In Proceedings of the Conference on Software Maintenance, </booktitle> <pages> pages 192-199, </pages> <year> 1989. </year>
Reference-contexts: These results reveal the advantages of the proposed approaches for the design of a software manipulation tool such as a software refactory. Currently, only two basic transformations are implemented in Cstructure. The tool also provides control flow and data flow visualization capabilities similar to EDSA <ref> [55] </ref>. Future work includes the design and implementation in Cstructure of a complete set of general refactorings, and its use in realistic software maintenance situations. Another research direction involves the integration of restructuring transformations with other program visualization techniques such as the star diagram [7, 8, 22]. <p> The tool operates interactively, with the user first selecting an interesting use or uses and then requesting the reaching definitions. Note that the first definition (i = 0;) does not reach the selected use. The functionality provided by this interface is similar in spirit to that of EDSA <ref> [55] </ref>, a source code static analyzer for Ada. Only four expressions need to be examined in order to find the definitions of i reaching the use in the final statement of the while loop.
Reference: [56] <author> Richard C. Waters and Elliot Chikofsky. </author> <title> Reverse engineering: Progress along many dimensions. </title> <journal> Communications of the ACM, </journal> <volume> 37(5) </volume> <pages> 23-24, </pages> <year> 1994. </year>
Reference-contexts: That is to say, while many of us may dream that the central business of software engineering is creating clearly understood new systems, the central business is really upgrading poorly understood old systems <ref> [56] </ref>. Successful software often lives well beyond the original purposes for which it was designed. The current "year 2000" problem provides an excellent example of this phenomenon [33, 42].
Reference: [57] <author> M. Weiser. </author> <title> Program slicing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-10(4):352-357, </volume> <month> July </month> <year> 1984. </year>
Reference-contexts: More precise flow-insensitive algorithms than the simple one implemented in Cstructure 111 exist, but their potential impact is unclear. Several other software tools could benefit from elimination of expensive program representations, including interactive program understanding and visualization tools such as slicers <ref> [18, 57] </ref>. This approach is a departure from the trend toward building all-inclusive data structures for these tools. However, none of these architectures has seen widespread use in commercial applications.
Reference: [58] <author> W. Yang. </author> <title> A New Algorithm for Semantics-Based Program Integration. </title> <type> PhD thesis, </type> <institution> University of Wisconsin, </institution> <month> August </month> <year> 1990. </year> <note> Computer Sciences Technical Report No. 962. </note>
Reference-contexts: For instance, Griswold's prototype [20, 24], Opdyke's refactory [47], and REFINE/Cobol [41] all use the AST to display or output the source. Rederiving source code from other representations is more difficult. For example, the PDG has been used as the basis of an algorithm for merging program versions <ref> [29, 58, 59] </ref>. However, the PDG only stores dependences between compu-tationally related program components [17], ignoring explicit statement order. To reconstitute the merged program (as an AST), all operations must first be ordered.
Reference: [59] <author> W. Yang, S. Horwitz, and T. Reps. </author> <title> A program integration algorithm that accommodates semantics-preserving transformations. </title> <journal> ACM Transactions on Software Engineering and Methodology, </journal> <volume> 1(3) </volume> <pages> 310-354, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: For instance, Griswold's prototype [20, 24], Opdyke's refactory [47], and REFINE/Cobol [41] all use the AST to display or output the source. Rederiving source code from other representations is more difficult. For example, the PDG has been used as the basis of an algorithm for merging program versions <ref> [29, 58, 59] </ref>. However, the PDG only stores dependences between compu-tationally related program components [17], ignoring explicit statement order. To reconstitute the merged program (as an AST), all operations must first be ordered.
References-found: 59

