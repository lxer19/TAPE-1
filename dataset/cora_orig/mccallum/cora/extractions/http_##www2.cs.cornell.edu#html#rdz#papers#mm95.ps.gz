URL: http://www2.cs.cornell.edu/html/rdz/papers/mm95.ps.gz
Refering-URL: http://www.cs.cornell.edu/home/rdz/rdz.html
Root-URL: 
Email: frdz,jmiller,kmaig@cs.cornell.edu  
Title: A Feature-Based Algorithm for Detecting and Classifying Scene Breaks  
Author: Ramin Zabih Justin Miller Kevin Mai 
Keyword: Content-based indexing and retrieval; video processing  
Date: 607-255-8413  
Address: Ithaca, NY 14853  
Affiliation: Computer Science Department Cornell University  
Abstract: We describe a new approach to the detection and classification of scene breaks in video sequences. Our method can detect and classify a variety of scene breaks, including cuts, fades, dissolves and wipes, even in sequences involving significant motion. We detect the appearance of intensity edges that are distant from edges in the previous frame. A global motion computation is used to handle camera or object motion. The algorithm we propose withstands JPEG and MPEG artifacts, even at very high compression rates. Experimental evidence demonstrates that our method can detect and classify scene breaks that are difficult to detect with previous approaches. An initial implementation runs at approximately 2 frames per second on a Sun workstation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Gilad Adiv. </author> <title> Determining three-dimensional motion and structure from optical flow generated by several moving objects. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 7(4) </volume> <pages> 384-401, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: As the above data shows, our algorithm handles these sequences well. However, the algorithm's handling of multiple moving objects could probably be improved by compensating for multiple motions. A number of algorithms have been proposed for this problem, including <ref> [1, 4] </ref>. When there are two distinct motions in the scene, our motion compensation will track one of them. Edges that undergo the other motion will show up as entering or exiting pixels, assuming that the two mo tions are sufficiently distinct.
Reference: [2] <author> Farshid Arman, Arding Hsu, and Ming-Yee Chiu. </author> <title> Image processing on compressed data for large video databases. </title> <booktitle> In Multimedia Conference, </booktitle> <pages> pages 267-272. </pages> <publisher> ACM, </publisher> <year> 1993. </year>
Reference-contexts: Most approaches are based on intensity histograms, and concentrate on cuts. Otsuji and Tonomura [11] discuss a variety of measures based on image differencing and changes in the image's intensity histogram. Nagasaka and Tanaka [10] present algorithms which use similar measures. Arman, Hsu and Chiu <ref> [2] </ref> have addressed the issue of change detection while operating directly on JPEG or MPEG encoded video; this approach is noteworthy for its efficiency. The above methods have difficulty with "busy" scenes, in which intensities change substantially from frame to frame.
Reference: [3] <author> Lisa Brown. </author> <title> A survey of image registration techniques. </title> <journal> ACM Computing Surveys, </journal> <volume> 24(4), </volume> <month> December </month> <year> 1992. </year>
Reference-contexts: On this sequence, shows clear peaks at the scene breaks, and the detection and the classification algorithm described in section 3.1 performed correctly. 2.1 Motion compensation Our method can be easily extended in order to handle motion. We can use any registration technique <ref> [3] </ref> to compute a global motion between frames. We can then apply 2 Due to the quantization of intensities, new edges will generally not show up until the end of the dissolve. this global motion to align the frames before detecting entering or exiting edge pixels.
Reference: [4] <author> Peter Burt, James Bergen, Rajesh Hingorani, R. Kolczynski, W. Lee, A. Leung, J. Lubin, and H. Shvaytser. </author> <title> Object tracking with a moving camera. </title> <booktitle> In Proceedings of IEEE Workshop on Visual Motion, </booktitle> <pages> pages 2-12, </pages> <year> 1989. </year>
Reference-contexts: As the above data shows, our algorithm handles these sequences well. However, the algorithm's handling of multiple moving objects could probably be improved by compensating for multiple motions. A number of algorithms have been proposed for this problem, including <ref> [1, 4] </ref>. When there are two distinct motions in the scene, our motion compensation will track one of them. Edges that undergo the other motion will show up as entering or exiting pixels, assuming that the two mo tions are sufficiently distinct.
Reference: [5] <author> John Canny. </author> <title> A computational approach to edge detection. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 8(6) </volume> <pages> 679-698, </pages> <year> 1986. </year>
Reference-contexts: The first step in our algorithm is edge detection. In our experiments, we have used an edge detector based on Canny's algorithm <ref> [5] </ref>. We first smooth the image by convolving it with a Gaussian of width . We next compute the gradient magnitude, which indicates how fast the local intensities are changing. The gradient magnitude is thresholded at a value of t to detect edges, Canny-style non-maximum suppression is performed.
Reference: [6] <author> Arun Hampapur, Ramesh Jain, and Terry Wey-mouth. </author> <title> Production model based digital video segmentation. </title> <journal> Journal of Multimedia Tools and Applications, </journal> <volume> 1 </volume> <pages> 1-38, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: However, a dissolve is a gradual change from one scene to another, and thus cannot be easily distinguished from motion. A dissolve can even occur between two scenes which each contain motion; this case is particularly difficult to detect. Hampapur, Jain and Weymouth <ref> [6] </ref> describe a method 1 It is a slight misnomer to call existing approaches intensity-based, since any algorithm must be based on intensities. <p> The first measure is the intensity histogram difference, which is used with slight variations in most work on scene breaks [10, 11, 17]. The second measure is the chromatic scaling method of Hampapur, Jain and Weymouth <ref> [6] </ref>, a recent method for classifying scene breaks. There are a number of ways to use intensity histograms. Let N denote the number of histogram buckets (which is typically a power of 2 no greater than 256), and let H t denote the intensity histogram of the t'th frame. <p> It is an interesting sequence because it contains two dissolves, as well as a moving object (the singer). It has been used to benchmark other algorithms (e.g., <ref> [6] </ref>). Figure 8 shows the performance of several measures on this sequence. Our edge change fraction detects and classifies both dissolves correctly. <p> However, the second dissolve appears to be indistinguishable from the noise. Their method for handling motion would not help here, since the problem is a false negative rather than a false positive. The chromatic scaling feature of <ref> [6] </ref> is shown in figure 8 (c). As the authors state, their method has difficulty with dissolves involving motion. 4.2.2 The Andy sequence Another sequence that caused some difficulty is the andy MPEG. The sequence involves camera and object motion, as well as zooms.
Reference: [7] <author> Daniel Huttenlocher and Eric Jaquith. </author> <title> Computing visual correspondence: Incorporating the probability of a false match. </title> <booktitle> In 5th International Conference on Computer Vision, </booktitle> <address> Cambridge, MA, </address> <pages> pages 515-522, </pages> <year> 1995. </year>
Reference-contexts: If we select the largest such distance, we have the original Hausdorff distance defined in equation 6. Applications which use the partial Hausdorff distance for matching <ref> [7] </ref> can provide a fixed fraction K=jAj, which is equal to 1 . This specifies what fraction of the points in A should be close to their nearest neighbor in B at the best match.
Reference: [8] <author> Daniel Huttenlocher, Greg Klanderman, and William Rucklidge. </author> <title> Comparing images using the Hausdorff distance. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 15(9) </volume> <pages> 850-863, </pages> <year> 1993. </year>
Reference-contexts: We have explored two algorithms, both of which have given satisfactory results. We have experimented with using census transform correlation, a non-parametric approach developed in [16]. This algorithm operates by transforming the image in an outlier-tolerant manner and then using correlation. We have also used the Hausdorff distance <ref> [8] </ref>, an outlier-tolerant method described in section 3.2 that operates on edge-detected images. It is tempting to exploit the motion vectors contained in MPEG-compressed video in order to determine object or camera motion. Indeed, a number of researchers [17] have attempted to do this. <p> This distance which has been used for such tasks as recognition and tracking <ref> [8] </ref>. The Hausdorff distance, which originates in point set topology, is a metric for comparing point sets.
Reference: [9] <author> David Marr and Ellen Hildreth. </author> <title> Theory of edge detection. </title> <journal> Proc. of the Royal Society of London B, </journal> <volume> 207 </volume> <pages> 187-217, </pages> <year> 1980. </year>
Reference-contexts: One approach is to use an edge detector based on zero-crossings of the Laplacian, rather than on the intensity gradient. We have experimented with a Marr-Hildreth style edge detector <ref> [9] </ref>, but have not yet obtained stable edges on MPEG compressed sequences. It is also possible to eliminate the edge-detection threshold t by dynamically thresholding the intensity gradient magnitude. In dynamic thresholding, a constant number of image pixels are labeled as edges.
Reference: [10] <author> Akio Nagasaka and Yuzuru Tanaka. </author> <title> Automatic video indexing and full-video search for object appearances. </title> <booktitle> In 2nd Working Conference on Visual Database Systems, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: Most approaches are based on intensity histograms, and concentrate on cuts. Otsuji and Tonomura [11] discuss a variety of measures based on image differencing and changes in the image's intensity histogram. Nagasaka and Tanaka <ref> [10] </ref> present algorithms which use similar measures. Arman, Hsu and Chiu [2] have addressed the issue of change detection while operating directly on JPEG or MPEG encoded video; this approach is noteworthy for its efficiency. <p> To provide a comparison, we have also implemented two other intensity-based measures used to detect scene breaks. The first measure is the intensity histogram difference, which is used with slight variations in most work on scene breaks <ref> [10, 11, 17] </ref>. The second measure is the chromatic scaling method of Hampapur, Jain and Weymouth [6], a recent method for classifying scene breaks. There are a number of ways to use intensity histograms. <p> The sum of the histogram differences N1 X jH t [i] H t+1 [i]j (8) is one frequently used measure. Another common mea sure <ref> [10] </ref> is the 2 value N1 X (H t [i] H t+1 [i]) 2 : (9) We implemented a variant of equation 8 used by Zhang, Kankanhalli and Smoliar.
Reference: [11] <author> K. Otsuji and Y. Tonomura. </author> <title> Projection-detecting filter for video cut detection. </title> <journal> Multimedia Systems, </journal> <volume> 1 </volume> <pages> 205-210, </pages> <year> 1994. </year>
Reference-contexts: When two consecutive images are sufficiently dissimilar, there may be a scene break. Typically the similarity measure is smoothed and thresholded, using methods such as those of <ref> [11] </ref>. This is done to reduce the effects of noise and to prevent the detector from signaling too many scene breaks in a short period of time. Several researchers have proposed algorithms for detecting cuts and dissolves. <p> These methods have relied directly on intensity data, and have used such techniques as image differencing (which subtracts two consecutive images to determine changes in intensity) and intensity histogramming. Most approaches are based on intensity histograms, and concentrate on cuts. Otsuji and Tonomura <ref> [11] </ref> discuss a variety of measures based on image differencing and changes in the image's intensity histogram. Nagasaka and Tanaka [10] present algorithms which use similar measures. <p> To provide a comparison, we have also implemented two other intensity-based measures used to detect scene breaks. The first measure is the intensity histogram difference, which is used with slight variations in most work on scene breaks <ref> [10, 11, 17] </ref>. The second measure is the chromatic scaling method of Hampapur, Jain and Weymouth [6], a recent method for classifying scene breaks. There are a number of ways to use intensity histograms.
Reference: [12] <author> L. A. Rowe, K. Patel, and B. C. Smith. </author> <title> Performance of a software MPEG video decoder. </title> <booktitle> In Multimedia Conference. ACM, </booktitle> <year> 1993. </year>
Reference-contexts: However, our scheme is reasonably fast, and can be optimized further. Our method also appears to give good results on reduced resolution imagery, as shown in figure 7. Finally, much of the overhead of MPEG decompression is due to dithering (for example <ref> [12] </ref> states that dithering consumed 60% to 80% of the time in their MPEG decoder). Since our approach only uses intensity information, this phase of MPEG decompression can be bypassed. 4.5 Availability Code for running the algorithm is available via FTP from the host ftp.cs.cornell.edu in the directory /pub/dissolve.
Reference: [13] <author> William Rucklidge. </author> <title> Efficient computation of the minimum Hausdorff distance for visual recognition. </title> <type> Technical Report TR94-1454, </type> <institution> Cornell University Department of Computer Science, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: A number of methods can be used to reduce the running time on large images, including performing a coarse-to-fine search. A number of methods for improving the performance of the Hausdorff-distance search are described in <ref> [13] </ref>, and have given impressive speedups in practice. Since the methods we use are fundamentally non-linear, it seems unlikely that we will be able to operate directly on compressed data streams without decompressing. However, our scheme is reasonably fast, and can be optimized further.
Reference: [14] <author> William M. Wells, III. </author> <title> Efficient synthesis of Gaussian filters by cascaded uniform filters. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 8(2) </volume> <pages> 234-239, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: The gradient magnitude is thresholded at a value of t to detect edges, Canny-style non-maximum suppression is performed. For additional efficiency we implement Gaussian smoothing by using a small number of box filters, as described in <ref> [14] </ref>. Next, copies of E and E 0 are created with each edge pixel dilated by a radius r. Let us call these dilated images E and E 0 .
Reference: [15] <author> Ramin Zabih, Justin Miller, and Kevin Mai. </author> <title> Feature-based algorithms for detecting and classifying scene breaks. </title> <type> Technical Report CS-TR-95-1530, </type> <institution> Cor-nell University Computer Science Department, </institution> <month> July </month> <year> 1995. </year>
Reference-contexts: By analyzing the spatial distribution of entering and exiting edge pixels, we can detect and classify wipes. A more detailed description of the algorithm, with additional examples of its performance, can be found in <ref> [15] </ref>. The algorithm we propose takes as input two consecutive images I and I 0 . We first perform an edge detection step, resulting in two binary images E and E 0 . <p> Except where otherwise noted, these were the parameters used to generate the data shown in this paper. We have found that our algorithm's performance does not depend critically upon the precise values of these parameters (see <ref> [15] </ref> for evidence of this). 3.4 Compression tolerance Most video will undergo some form of compression during its existence, and most compression methods are lossy. It is therefore important that our algorithm degrade gracefully in the presence of compression-induced artifacts.
Reference: [16] <author> Ramin Zabih and John Woodfill. </author> <title> Non-parametric local transforms for computing visual correspondence. </title> <editor> In Jan-Olof Eklundh, editor, </editor> <booktitle> 3rd European Conference on Computer Vision, number 801 in LNCS, </booktitle> <pages> pages 151-158. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: We have explored two algorithms, both of which have given satisfactory results. We have experimented with using census transform correlation, a non-parametric approach developed in <ref> [16] </ref>. This algorithm operates by transforming the image in an outlier-tolerant manner and then using correlation. We have also used the Hausdorff distance [8], an outlier-tolerant method described in section 3.2 that operates on edge-detected images.
Reference: [17] <author> H. Zhang, A. Kankanhalli, and S. Smoliar. </author> <title> Automatic partitioning of full-motion video. </title> <journal> Multimedia Systems, </journal> <volume> 1 </volume> <pages> 10-28, </pages> <year> 1993. </year>
Reference-contexts: Another recent paper <ref> [17] </ref> describes motion as a major limitation of histogram-based methods. A particularly interesting approach has been taken by Zhang, Kankanhalli and Smoliar [17]. They have extended conventional histogram-based approaches to handle dissolves and to deal with certain camera motions. <p> Another recent paper <ref> [17] </ref> describes motion as a major limitation of histogram-based methods. A particularly interesting approach has been taken by Zhang, Kankanhalli and Smoliar [17]. They have extended conventional histogram-based approaches to handle dissolves and to deal with certain camera motions. They use a dual threshold on the change in the intensity histogram to detect dissolves. <p> We have also used the Hausdorff distance [8], an outlier-tolerant method described in section 3.2 that operates on edge-detected images. It is tempting to exploit the motion vectors contained in MPEG-compressed video in order to determine object or camera motion. Indeed, a number of researchers <ref> [17] </ref> have attempted to do this. There are a number of reasons that we have not taken this approach. MPEG encoders optimize for compression, and do not necessarily produce accurate motion vectors. <p> To provide a comparison, we have also implemented two other intensity-based measures used to detect scene breaks. The first measure is the intensity histogram difference, which is used with slight variations in most work on scene breaks <ref> [10, 11, 17] </ref>. The second measure is the chromatic scaling method of Hampapur, Jain and Weymouth [6], a recent method for classifying scene breaks. There are a number of ways to use intensity histograms. <p> The intensity histogram difference, shown in figure 8 (b), is a noisier measure on this sequence. It does show a rise during the first dissolve, and it is possible that the dual threshold scheme of <ref> [17] </ref> would detect this (depending on the exact thresholds used). However, the second dissolve appears to be indistinguishable from the noise. Their method for handling motion would not help here, since the problem is a false negative rather than a false positive. <p> This method is appealing in theory, but has not proved successful in practice. Dynamic thresholding tends to reduce the number of entering and exiting edge pixels. This results in lower values from our similarity measure. Another improvement, also discussed in <ref> [17] </ref>, involves handling multiple moving objects. The clapton sequence contains some motion, while the andy sequence contains significant motion (both camera and object motion). As the above data shows, our algorithm handles these sequences well.
References-found: 17

