URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/craven.mlc94.ps
Refering-URL: http://www.cs.wisc.edu/~shavlik/abstracts/craven.mlc94.ps.abstract.html
Root-URL: 
Email: craven@cs.wisc.edu, shavlik@cs.wisc.edu  
Title: Using Sampling and Queries to Extract Rules from Trained Neural Networks  
Author: W. W. Cohen H. Hirsh, Mark W. Craven and Jude W. Shavlik 
Affiliation: Computer Sciences Department University of Wisconsin  
Date: 1994  
Address: San Francisco, CA,  1210 West Dayton St. Madison, WI 53706  
Note: Appears in Machine Learning: Proceedings of the Eleventh International Conference,  eds., Morgan Kaufmann,  
Abstract: Concepts learned by neural networks are difficult to understand because they are represented using large assemblages of real-valued parameters. One approach to understanding trained neural networks is to extract symbolic rules that describe their classification behavior. There are several existing rule-extraction approaches that operate by searching for such rules. We present a novel method that casts rule extraction not as a search problem, but instead as a learning problem. In addition to learning from training examples, our method exploits the property that networks can be efficiently queried. We describe algorithms for extracting both conjunctive and M -of-N rules, and present experiments that show that our method is more efficient than conventional search-based approaches.
Abstract-found: 1
Intro-found: 1
Reference: <author> Angluin, D. </author> <year> (1988). </year> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 319-342. </pages>
Reference-contexts: The approach uses two different oracles that are able to answer queries about the concept being learned. The Examples oracle produces, on demand, training examples for the rule-learning algorithm. The Subset oracle answers restricted subset queries <ref> (Angluin, 1988) </ref>. It takes two arguments: a class label and a conjunctive rule. Subset returns true if all of the instances that are covered by the rule are members of the given class, and false otherwise.
Reference: <author> Craven, M. W. & Shavlik, J. W. </author> <year> (1993). </year> <title> Learning symbolic rules using artificial neural networks. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> (pp. 73-80), </pages> <address> Amherst, MA. </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: In contrast to methods that extract conjunctive rules, Towell and Shavlik (1993) developed an approach for extracting M-of-N rules from knowledge-based neural networks. 1 In later work, we generalized this approach to ordinary neural networks by employing a special training procedure <ref> (Craven & Shavlik, 1993) </ref>. The advantages of describing networks using M-of-N rules are that the search can be made more efficient than for conjunctive rules, and the extracted rule sets are usually more concise.
Reference: <author> Fahlman, S. & Lebiere, C. </author> <year> (1989). </year> <title> The cascade-correlation learning architecture. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 2). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Ma-teo, CA. </address>
Reference-contexts: There are several reasonable criteria that could be used here. For example, a tuning set might be held aside to estimate the accuracy of the extracted rules. Alternatively, we could use a patience criterion <ref> (Fahlman & Lebiere, 1989) </ref> that causes the procedure to quit after a certain number of iterations have resulted in no new rules.
Reference: <author> Fu, L. </author> <year> (1991). </year> <title> Rule learning by searching on adapted nets. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 590-595), </pages> <address> Ana-heim, CA. </address> <publisher> AAAI/MIT Press. </publisher>
Reference: <author> Gallant, S. I. </author> <year> (1993). </year> <title> Neural Network Learning and Expert Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Hinton, G. E. </author> <year> (1986). </year> <title> Learning distributed representations of concepts. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, </booktitle> <pages> (pp. 1-12), </pages> <address> Amherst, MA. </address> <publisher> Erlbaum. </publisher>
Reference-contexts: This approach, however, involves the assumptions that each hidden unit in the network behaves like a Boolean variable, and the individual hidden units of the network correspond to concepts that are meaningful in the context of the domain. Neural networks, however, often learn distributed rep resentations <ref> (Hinton, 1986) </ref> in which each concept is encoded by the activations of many hidden units, and each hidden unit plays a part in representing many different concepts. Given such representations, the "right" intermediate terms might represent patterns of activity across groups of hidden units, instead of individual hidden units themselves.
Reference: <author> Hunter, L. & Klein, T. </author> <year> (1993). </year> <title> Finding relevant biomolecular features. </title> <booktitle> In Proceedings of the First International Conference on Intelligent Systems for Molecular Biology, </booktitle> <pages> (pp. 190-197), </pages> <address> Bethesda, MD. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Learning systems can also play an important role in the process of scientific discovery. A system may discover salient features in the input data whose importance was not previously recognized. If the representations formed by the learner are comprehensible, then these discoveries can be made accessible to human review <ref> (e.g., Hunter & Klein, 1993) </ref>. There are several existing search-based approaches for extracting propositional if-then rules from trained networks (Fu, 1991; Gallant, 1993; Saito & Nakano, 1988; Thrun, 1993).
Reference: <author> Knuth, D. E. </author> <year> (1981). </year> <booktitle> The Art of Computer Programming (volume 3). </booktitle> <publisher> Addison-Wesley, second edition. </publisher>
Reference-contexts: number of antecedents summed over all the rules in a rule set. is O (f ), where f is the number of features, the complexity of answering a query about an M-of-N term is O (f + M log f ), since we have to find the M smallest-weighted antecedents <ref> (Knuth, 1981) </ref>. We therefore calculate the cost of a Subset query in this experiment by summing 1 + M log f f for every term in an M-of-N rule (recall that the basic unit of our accounting is an O (f ) operation).
Reference: <author> Lapedes, A., Barnes, C., Burks, C., Farber, R., & Sirotkin, K. </author> <year> (1989). </year> <title> Application of neural networks and other machine learning algorithms to DNA sequence analysis. </title> <editor> In Bell, G. I. & Marr, T. G., editors, </editor> <title> Computers and DNA (volume VII). </title> <publisher> Addison-Wesley. </publisher>
Reference: <author> Michalski, R. S. </author> <year> (1983). </year> <title> A theory and methodology of inductive learning. </title> <journal> Artificial Intelligence, </journal> <volume> 20 </volume> <pages> 111-161. </pages>
Reference-contexts: For example, 3-of fa; b; cg =) 2-of-fa; bg ^ 1-of-fcg. The add-value operator may either add another possible value for a feature already in the antecedent set <ref> (i.e., create an internal disjunction, Michalski, 1983) </ref>, or add a feature that is not yet represented. The new-term operation, by itself, is not able to generalize a term; therefore this operator is always used in conjunction with the add-value operator.
Reference: <author> Saito, K. & Nakano, R. </author> <year> (1988). </year> <title> Medical diagnostic expert system based on PDP model. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <pages> (pp. 255-262), </pages> <address> San Diego, CA. </address> <publisher> IEEE Press. </publisher>
Reference: <author> Sejnowski, T. & Rosenberg, C. </author> <year> (1987). </year> <title> Parallel networks that learn to pronounce English text. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 145-168. </pages>
Reference: <author> Selman, B., Levesque, H., & Mitchell, D. </author> <year> (1992). </year> <title> A new method for solving hard satisfiability problems. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 440-446), </pages> <address> San Jose, CA. </address> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: Every solution to such a SAT problem is an example that is not covered by any of the extracted rules. Although the general SAT problem is NP-hard, we hypothesize that a greedy local-search algorithm, such as GSAT <ref> (Selman et al., 1992) </ref>, will be able to efficiently find solutions in cases where there are many uncovered examples.
Reference: <author> Thrun, S. B. </author> <year> (1993). </year> <title> Extracting provably correct rules from artificial neural networks. </title> <type> Technical Report IAI-TR-93-5, </type> <institution> Institut fur Informatik III, Uni-versitat Bonn. </institution>
Reference: <author> Towell, G. & Shavlik, J. </author> <year> (1993). </year> <title> Extracting refined rules from knowledge-based neural networks. </title> <journal> Machine Learning, </journal> <volume> 13(1) </volume> <pages> 71-101. </pages>
Reference-contexts: M-of-N rules are better suited to describing neural networks than are conjunctive rules <ref> (Towell & Shavlik, 1993) </ref> because they more closely match the inductive bias of units in a neural network. Hence, fewer M-of-N rules than conjunctive rules are usually required to describe a network. The M-of-N rules that our algorithm extracts consist of conjunctions of one or more M-of-N terms.
Reference: <author> Towell, G. & Shavlik, J. </author> <title> (in press). </title> <booktitle> Knowledge-based artificial neural networks. Artificial Intelligence. </booktitle>
Reference: <author> Towell, G., Shavlik, J., & Noordewier, M. </author> <year> (1990). </year> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 861-866), </pages> <address> Boston, MA. </address> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: Our interest is in measuring the relative efficiency of exploring a rule space using our approach versus a search-based approach; single-layer networks provide an adequate milieu for this experiment. We evaluate the two approaches using the promoter domain <ref> (Towell et al., 1990) </ref>. Promoters are short sequences in DNA that occur before genes and play a critical role during gene transcription. Each feature in this domain represents a position in a DNA sequence; thus each feature takes one of the values in fA, G, C, Tg.
Reference: <author> Valiant, L. G. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142. </pages>
Reference-contexts: Our algorithm for extracting conjunctive rules from trained neural networks is outlined in Table 1. It is an adaptation of the classical algorithm for PAC-learning monotone DNF expressions <ref> (Valiant, 1984) </ref>. The algorithm maintains a DNF expression 2 for each class. The algorithm repeatedly queries Examples, and then determines the class of each returned example.
Reference: <author> Wolberg, W. H., Street, W. N., & Mangasarian, O. L. </author> <title> (in press). Machine learning techniques to diagnose breast cancer from image-processed nuclear features of fine needle aspirates. </title> <journal> Cancer Letters. </journal>
References-found: 19

