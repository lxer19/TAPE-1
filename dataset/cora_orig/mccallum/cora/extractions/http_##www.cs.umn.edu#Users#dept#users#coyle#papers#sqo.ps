URL: http://www.cs.umn.edu/Users/dept/users/coyle/papers/sqo.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/coyle/papers/
Root-URL: http://www.cs.umn.edu
Email: shekhar@cs.umn.edu, hamidzad@cs.umn.edu, akohli@cs.umn.edu, coyle@cs.umn.edu  
Phone: Telephone Numbers (612) 624-8307, (612) 626-7703, (612) 625-4012 Fax (612) 625-0572  
Title: Learning Transformation Rules for Semantic Query Optimization: A Data-Driven Approach  
Author: Shashi Shekhar Babak Hamidzadeh Ashim Kohli Mark Coyle 
Keyword: Rule discovery, semantic query optimization, discovery in data. Areas Addressed: Learning and Discovery in Database, Data Engineering Tools, High level Query Answering, Applications in Query Optimization.  
Note: Electronic Addresses  
Address: Minneapolis, MN 55455  Address 4-192 EE/CS Bldg., 200 Union Street S.E., Minneapolis, MN 55455, USA.  
Affiliation: Computer Science Dept. University of Minnesota  Postal  
Abstract: Learning query transformation rules is vital for the success of semantic query optimization in domains where the user cannot provide a comprehensive set of integrity constraints. Finding these rules is a discovery task because of the lack of target. Previous approaches to learning query transformation rules have been based on analyzing past queries. We propose a new approach to learning query transformation rules based on analyzing the existing data in the database. This paper describes a framework and a closure algorithm to learning rules from a given data-distribution. We characterize the correctness, completeness and complexity of the proposed algorithm and provide a detailed example to illustrate the framework. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> J. J. King, </author> <title> QUIST : A system for semantic query optimization in relational databases, </title> <booktitle> Proc. 7th VLDB Conf., </booktitle> <year> (1981). </year>
Reference-contexts: A modular arrangement of optimization methods makes it possible to add, delete and modify individual methods, without affecting other methods. Since this system is rule-based it provides an extensible system for maintaining and managing optimization. Semantic query optimization is well-motivated in the literature <ref> [1, 2, 8, 9] </ref> as adding a new dimension to conventional query optimization. The success of a semantic query optimizer depends on the availability of an effective set of query transformation rules which can reduce the execution cost of a large set of queries. <p> Discard rules with negligible cost savings. We will use a well-known database schema <ref> [1] </ref> to illustrate various definitions and algorithms. The database schema, the relation sizes, and the various indexes available are shown in Table 1. <p> A candidate rule consists of an antecedent restriction and the set of variables in the consequent. The antecedent restriction is generated from the restriction clauses of the queries arriving at the system. The set of free variables for the consequent is generated by heuristics such as index introduction <ref> [1] </ref>. The antecedent restriction is evaluated against the database state to retrieve and summarize all possible values of the free variables to form the consequent. <p> We then show the correspondence between basic transformation rules and patterns in the data-distribution, which forms the basis of the data-driven rule discovery algorithm. 3.1. Representation Language We follow a logic-based representation proposed in <ref> [1, 14, 15] </ref> for queries, integrity constraints and query transformation rules in this paper. For relations P, the atomic formula will be written as P (a 1 op t 1 , ..., a n op t n ), where a 1 , ..., a n are some attributes of P. <p> We will omit the names of the predicates without loss of precision since attribute names are unique. We will also omit (for all) quantifiers by implying universal quantification of free variables. The formal rules for dropping the quantifiers are discussed in <ref> [1] </ref>. Furthermore, we will assume the queries to be defined on a natural join of all relations in the shipping database. Example: Query = Find the destinations of all ships with DeadWt &gt; 400 and DollarValue &gt; 4000. <p> The attributers and their relationships decide the types of rules that can be learned from the grid. Several strategies can be used in selecting the attribute set. One such strategy is to allow the user to provide a heuristic <ref> [1] </ref> as to what attributes to choose. The user can use his domain knowledge, and his knowledge of attribute spaces to select a set of attributes to be represented in the grid. Another strategy in attribute selection is the use of the information in the database system catalog. <p> In particular, we plan to integrate our work with query-driven approaches, in order to monitor the performance of the discovered query transformation rules during actual operation of the database. We would like to utilize the heuristics for semantic query optimization <ref> [1] </ref> for selecting the attribute sets to create grids. Furthermore we would like to characterize the set of oblique grids which may yield useful query transformation rules. 8. Acknowledgements This research was supported by the graduate school of University of Minnesota, and the Minnesota Department of Transportation. Prof.
Reference: 2. <author> Hammer and Zdonik, </author> <title> Knowledge Based query processing, </title> <booktitle> Proc. 6th Conf. on VLDB, </booktitle> <year> (1980). </year>
Reference-contexts: A modular arrangement of optimization methods makes it possible to add, delete and modify individual methods, without affecting other methods. Since this system is rule-based it provides an extensible system for maintaining and managing optimization. Semantic query optimization is well-motivated in the literature <ref> [1, 2, 8, 9] </ref> as adding a new dimension to conventional query optimization. The success of a semantic query optimizer depends on the availability of an effective set of query transformation rules which can reduce the execution cost of a large set of queries.
Reference: 3. <author> S .T. Shenoy and Z. M. Ozsoyoglu, </author> <title> Design and Implementation of a Semantic Query Optimizer, </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <pages> pp. </pages> <month> 362-375 </month> <year> (1989). </year>
Reference: 4. <author> H. H. Pang, H. J. Lu, and B. C. Ooi, </author> <title> An Efficient Semantic Query Optimization Algorithm, </title> <booktitle> Proc. Data Engineering Conference, IEEE, </booktitle> <year> (1991). </year>
Reference: 5. <author> G. M. Lohman, </author> <title> Panel Discussion on Semantic Query Optimization, </title> <booktitle> Proc. Data Engineering Conf., </booktitle> <year> (1985). </year> <month> - 28 </month> - 
Reference: 6. <author> S. Shekhar, J. Srivastava, and S. Dutta, </author> <title> A Formal Model of Trade-offs between Execution and Optimization Costs in Semantic Query Optimization, </title> <booktitle> Intl. Conf. on Very Large Databases (VLDB), </booktitle> <year> (1988). </year>
Reference: 7. <author> C. T. Yu and W. Sun, </author> <title> Automatic Knowledge Acquisition and Maintenance for Semantic Query Optimization, </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <pages> pp. </pages> <month> 362-375 </month> <year> (1989). </year>
Reference-contexts: Further, the set of query transformation rules can be expanded significantly by incrementally adding additional discovered query transformation rules based on the current state of the database <ref> [7] </ref>. We regard the unearthing of query transformation rules for semantic query optimization as a discovery task [7, 10], rather than as a concept learning task. <p> Further, the set of query transformation rules can be expanded significantly by incrementally adding additional discovered query transformation rules based on the current state of the database [7]. We regard the unearthing of query transformation rules for semantic query optimization as a discovery task <ref> [7, 10] </ref>, rather than as a concept learning task. While there may exist some general heuristics that facilitate the search for regularities, the direction of the search for such regularities is not generally prespecified. <p> Section 4 presents the rule closure algorithm. Section 5 describes an extended example to illustrate the concepts. A summary of the results and conclusions are provided in section 6. 2. Related Approaches and Our Contributions The learning of query transformation rules can be query-driven or data-driven. In query-driven frameworks <ref> [7, 11] </ref> the search for new query transformation rules is guided by the set of queries arriving at the database using query comparisons [7] and hypothesis generation and testing [11, 12]. <p> Related Approaches and Our Contributions The learning of query transformation rules can be query-driven or data-driven. In query-driven frameworks [7, 11] the search for new query transformation rules is guided by the set of queries arriving at the database using query comparisons <ref> [7] </ref> and hypothesis generation and testing [11, 12]. In query comparison, the set of queries arriving after the last update are analyzed by comparing the set of tuples retrieved to answer various queries. <p> If the set of tuples retrieved by two queries are identical then query transformation rules relating the restrictions in the two queries can be added <ref> [7] </ref>. In hypothesis generation and testing approaches [13], the queries are used to generate a candidate query transformation rule. A candidate rule consists of an antecedent restriction and the set of variables in the consequent. <p> These techniques are not suitable for learning FOPL formulas typical of query transformation rules. Contributions: We propose an empirical law discovery system for the problem of query transformation rule learning. Our approach is data-driven in contrast to the previous approaches <ref> [7, 11] </ref> to learning query transformation rules. Being query driven, previous approaches have the disadvantage that cost savings occur only if queries are repeated. This approach might incur larger costs in the case where many new queries arrive. <p> We also attach an alternate - 8 - relational interpretation to query Q as R x (Q), representing a relation containing the set of tuples retrieved by query in a current database state x <ref> [7] </ref>. Definition: Two queries Q 1 and Q 2 are semantically equivalent in the current database state x if R x (Q 1 ) = R x (Q 2 ). <p> Another classification of integrity constraint types is based on the current and historic states of a data base. State constraints refer to all constraints on the data that must hold on each state of the database <ref> [7] </ref>. Transition constraints constrain the way in which one state of the database can be transformed to another state. A rule ensuring monotonically increasing salary values is a transition constraint [32, 34-36]. We distinguish between integrity constraints and query transformation rules to provide a more accurate description of their nature. <p> Query transformation rules subsume the "simple rules" learned in rule discovery for query optimization [12], since "simple rules" are universally quantified. These rules also subsume the rules learned via automatic knowledge acquisition <ref> [7] </ref> , since the latter are based on set comparisons. Query transformation rules represent the useful integrity constraints which can provide savings during semantic query optimization. - 9 - 3.3. User-Defined and Discovered Query Transformation Rules User-defined transformation rules are a subset of user-defined explicit integrity constraints. <p> The interval-operations are discussed with respect to the nature of the underlying attributes, next. The domain of attributes in a database can be divided into two classes: the ordered- and the unordered-valued attributes <ref> [7] </ref>. Unordered attributes are those whose values do not belong to an ordered domain. For example, there is no ordering relationships between the values of the Color attribute (e.g. red, blue, green).
Reference: 8. <author> S. T. Shenoy and Z.M.Ozsoyoglu, </author> <title> A System for Semantic Query Optimization, </title> <booktitle> Proc. ACM-SIGMOD, </booktitle> <pages> pp. </pages> <month> 181-195 </month> <year> (1987). </year>
Reference-contexts: A modular arrangement of optimization methods makes it possible to add, delete and modify individual methods, without affecting other methods. Since this system is rule-based it provides an extensible system for maintaining and managing optimization. Semantic query optimization is well-motivated in the literature <ref> [1, 2, 8, 9] </ref> as adding a new dimension to conventional query optimization. The success of a semantic query optimizer depends on the availability of an effective set of query transformation rules which can reduce the execution cost of a large set of queries.
Reference: 9. <author> A. V. Aho, J. E. Hopcroft, and J. D. Ullman, </author> <title> The Design and Analysis of Computer Algorithms, </title> <institution> Bell Telephone Laboratories (1974). </institution>
Reference-contexts: A modular arrangement of optimization methods makes it possible to add, delete and modify individual methods, without affecting other methods. Since this system is rule-based it provides an extensible system for maintaining and managing optimization. Semantic query optimization is well-motivated in the literature <ref> [1, 2, 8, 9] </ref> as adding a new dimension to conventional query optimization. The success of a semantic query optimizer depends on the availability of an effective set of query transformation rules which can reduce the execution cost of a large set of queries.
Reference: 10. <author> J. Zytkow and J. Baker, </author> <title> Interactive Mining of Regularities in Databases, Knowledge Discovery in Databases, </title> <publisher> The AAAI Press, </publisher> <year> (1991). </year>
Reference-contexts: Further, the set of query transformation rules can be expanded significantly by incrementally adding additional discovered query transformation rules based on the current state of the database [7]. We regard the unearthing of query transformation rules for semantic query optimization as a discovery task <ref> [7, 10] </ref>, rather than as a concept learning task. While there may exist some general heuristics that facilitate the search for regularities, the direction of the search for such regularities is not generally prespecified. <p> These techniques include regression analysis [29] and other function finding methods. Previous work in automatic discovery of quantitative rules includes the Bacon system [13]. Discovering quantitative rules in databases has also been pursued by a system known as the Forty-Niner <ref> [10] </ref>. This system examines several possible subsets of data for possible empirical laws. The subsets are created by using projection, slicing and aggregation operations in a multi-dimensional attribute space.
Reference: 11. <author> M. Siegel, E. Sciore, and S. Salveter, </author> <title> Rule Discovery for Query Optimization, Knowledge Discovery in Databases, </title> <publisher> The AAAI Press, </publisher> <year> (1991). </year>
Reference-contexts: Section 4 presents the rule closure algorithm. Section 5 describes an extended example to illustrate the concepts. A summary of the results and conclusions are provided in section 6. 2. Related Approaches and Our Contributions The learning of query transformation rules can be query-driven or data-driven. In query-driven frameworks <ref> [7, 11] </ref> the search for new query transformation rules is guided by the set of queries arriving at the database using query comparisons [7] and hypothesis generation and testing [11, 12]. <p> Related Approaches and Our Contributions The learning of query transformation rules can be query-driven or data-driven. In query-driven frameworks [7, 11] the search for new query transformation rules is guided by the set of queries arriving at the database using query comparisons [7] and hypothesis generation and testing <ref> [11, 12] </ref>. In query comparison, the set of queries arriving after the last update are analyzed by comparing the set of tuples retrieved to answer various queries. <p> These techniques are not suitable for learning FOPL formulas typical of query transformation rules. Contributions: We propose an empirical law discovery system for the problem of query transformation rule learning. Our approach is data-driven in contrast to the previous approaches <ref> [7, 11] </ref> to learning query transformation rules. Being query driven, previous approaches have the disadvantage that cost savings occur only if queries are repeated. This approach might incur larger costs in the case where many new queries arrive.
Reference: 12. <author> M. Siegel, </author> <title> Automatic Rule Derivation for Semantic Query Optimization. </title> <type> Ph.D. </type> <institution> diss., Boston Univ. </institution> <year> (1988). </year>
Reference-contexts: Related Approaches and Our Contributions The learning of query transformation rules can be query-driven or data-driven. In query-driven frameworks [7, 11] the search for new query transformation rules is guided by the set of queries arriving at the database using query comparisons [7] and hypothesis generation and testing <ref> [11, 12] </ref>. In query comparison, the set of queries arriving after the last update are analyzed by comparing the set of tuples retrieved to answer various queries. <p> These rules are characterized by universal quantification in the well-formed formulas representing those queries. For example, IC 0 is a query transformation rule but IC 1 is not a query transfor mation rule. Query transformation rules subsume the "simple rules" learned in rule discovery for query optimization <ref> [12] </ref>, since "simple rules" are universally quantified. These rules also subsume the rules learned via automatic knowledge acquisition [7] , since the latter are based on set comparisons. Query transformation rules represent the useful integrity constraints which can provide savings during semantic query optimization. - 9 - 3.3.
Reference: 13. <author> P. Langley, J. Zytkow, H. Simon, and G. Bradshaw, </author> <title> Rediscovering Chemistry with the BACON System, pp. 307-330 in Machine Learning: An Artificial Intelligence Approach, </title> <editor> ed. T. M. Mitchell, </editor> <publisher> Tioga, </publisher> <address> Palo Alto, </address> <month> Cal-ifornia </month> <year> (1983). </year>
Reference-contexts: If the set of tuples retrieved by two queries are identical then query transformation rules relating the restrictions in the two queries can be added [7]. In hypothesis generation and testing approaches <ref> [13] </ref>, the queries are used to generate a candidate query transformation rule. A candidate rule consists of an antecedent restriction and the set of variables in the consequent. The antecedent restriction is generated from the restriction clauses of the queries arriving at the system. <p> Quantitative discovery techniques use numeric-valued variables to discover mathematical functions that summarize the data. These techniques include regression analysis [29] and other function finding methods. Previous work in automatic discovery of quantitative rules includes the Bacon system <ref> [13] </ref>. Discovering quantitative rules in databases has also been pursued by a system known as the Forty-Niner [10]. This system examines several possible subsets of data for possible empirical laws. The subsets are created by using projection, slicing and aggregation operations in a multi-dimensional attribute space.
Reference: 14. <author> C. L. Chang, </author> <title> DEDUCE 2: Further Investigations of Deduction in Relational Data bases, pp. 201-236 in Logic and Data Bases, </title> <editor> ed. J. Minker, </editor> <publisher> Plenum Press, </publisher> <address> New York (1978). </address>
Reference-contexts: Data-driven approaches can be based on the learning algorithms developed in Artificial Intelligence (AI). Many of these learning algorithms discover rules represented in languages similar to first order predicate logic (FOPL), which can be used to represent general integrity constraints <ref> [14, 15] </ref> and query transformation rules. For example, the representation languages used in AQ15 [16] and in the conceptual clustering algorithm Cluster 2 [17] are fairly close to FOPL. The AI learning algorithms can be classified into two categories, supervised concept learning and unsupervised discovery. <p> We then show the correspondence between basic transformation rules and patterns in the data-distribution, which forms the basis of the data-driven rule discovery algorithm. 3.1. Representation Language We follow a logic-based representation proposed in <ref> [1, 14, 15] </ref> for queries, integrity constraints and query transformation rules in this paper. For relations P, the atomic formula will be written as P (a 1 op t 1 , ..., a n op t n ), where a 1 , ..., a n are some attributes of P.
Reference: 15. <author> J. M. Nicolas, </author> <title> Logic for Improving Integrity Checking in Relational Databases, </title> <publisher> Acta Informatica 18 pp. 227-253 Springer Verlag, </publisher> <year> (1982). </year>
Reference-contexts: Data-driven approaches can be based on the learning algorithms developed in Artificial Intelligence (AI). Many of these learning algorithms discover rules represented in languages similar to first order predicate logic (FOPL), which can be used to represent general integrity constraints <ref> [14, 15] </ref> and query transformation rules. For example, the representation languages used in AQ15 [16] and in the conceptual clustering algorithm Cluster 2 [17] are fairly close to FOPL. The AI learning algorithms can be classified into two categories, supervised concept learning and unsupervised discovery. <p> We then show the correspondence between basic transformation rules and patterns in the data-distribution, which forms the basis of the data-driven rule discovery algorithm. 3.1. Representation Language We follow a logic-based representation proposed in <ref> [1, 14, 15] </ref> for queries, integrity constraints and query transformation rules in this paper. For relations P, the atomic formula will be written as P (a 1 op t 1 , ..., a n op t n ), where a 1 , ..., a n are some attributes of P.
Reference: 16. <author> R. S. Michalski, </author> <title> A Theory and Methodology of Inductive Learning, in Machine Learning: An Artificial Intelligence Approach, </title> <editor> ed. T. M. Mitchell, </editor> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> Los Altos, California (1986). </address>
Reference-contexts: Many of these learning algorithms discover rules represented in languages similar to first order predicate logic (FOPL), which can be used to represent general integrity constraints [14, 15] and query transformation rules. For example, the representation languages used in AQ15 <ref> [16] </ref> and in the conceptual clustering algorithm Cluster 2 [17] are fairly close to FOPL. The AI learning algorithms can be classified into two categories, supervised concept learning and unsupervised discovery. <p> After training, these algorithms will be able to correctly classify a sample data into one of the learned concepts. Examples of supervised concept learning algorithms include AQ <ref> [16] </ref> and ID3 [18]. Supervised concept learning algorithms can not directly be applied to the problem of learning query transformation rules. In this domain there may be no a priori concepts to provide the set of training examples.
Reference: 17. <author> R. S. Michalski and R. E. Stepp, </author> <title> Learning from Observation: Conceptual Clustering, pp. 331-363 in Machine Learning: An Artificial Intelligence Approach, </title> <editor> ed. T. M. Mitchell, </editor> <publisher> Tioga, </publisher> <address> Palo Alto, California (1983). </address>
Reference-contexts: Many of these learning algorithms discover rules represented in languages similar to first order predicate logic (FOPL), which can be used to represent general integrity constraints [14, 15] and query transformation rules. For example, the representation languages used in AQ15 [16] and in the conceptual clustering algorithm Cluster 2 <ref> [17] </ref> are fairly close to FOPL. The AI learning algorithms can be classified into two categories, supervised concept learning and unsupervised discovery. The supervised concept learning algorithms use an external tutor and a set of training examples with known class labels to learn a set of predetermined concepts. <p> Conceptual clustering arranges data into partitions based on certain conceptual classes. The basic theory and an algorithm for conceptual clustering have been developed in [25]. Other conceptual clustering algorithms include Discon [26], RUMMAGE [27], and Cluster 2 <ref> [17] </ref>. Conceptual clustering uses background knowledge about the functionality of clusters to guide the learning process towards the formation of more useful clusters [17, 28]. The discovery techniques to learn empirical laws can be divided into two categories, namely quantitative and qualitative law discovery. <p> The basic theory and an algorithm for conceptual clustering have been developed in [25]. Other conceptual clustering algorithms include Discon [26], RUMMAGE [27], and Cluster 2 [17]. Conceptual clustering uses background knowledge about the functionality of clusters to guide the learning process towards the formation of more useful clusters <ref> [17, 28] </ref>. The discovery techniques to learn empirical laws can be divided into two categories, namely quantitative and qualitative law discovery. Quantitative discovery techniques use numeric-valued variables to discover mathematical functions that summarize the data. These techniques include regression analysis [29] and other function finding methods.
Reference: 18. <author> J. R. Quinlan, </author> <title> Probabilistic Decision Trees, pp. 140-152 in Machine Learning: An Artificial Intelligence Approach, </title> <editor> ed. Yves Kodratoff, </editor> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, California (1990). </address>
Reference-contexts: After training, these algorithms will be able to correctly classify a sample data into one of the learned concepts. Examples of supervised concept learning algorithms include AQ [16] and ID3 <ref> [18] </ref>. Supervised concept learning algorithms can not directly be applied to the problem of learning query transformation rules. In this domain there may be no a priori concepts to provide the set of training examples. Unsupervised discovery is concerned with learning without the help of external tutors or examples.
Reference: 19. <author> S. C. </author> <title> Shapiro, </title> <journal> Encyclopedia of Artificial Intelligence, </journal> <note> A Wiley-Interscience Publication (1990). </note>
Reference-contexts: In this domain there may be no a priori concepts to provide the set of training examples. Unsupervised discovery is concerned with learning without the help of external tutors or examples. Previous work on algorithms for unsupervised discovery can be divided into two subclasses <ref> [19] </ref> (a) Taxonomy formation and clustering, and (b) Discovery of empirical laws. Taxonomy formation is concerned with the discovery of classification rules. Two approaches to taxonomy formation include numerical clustering and conceptual clustering. Comprehensive surveys on clustering methods can be found in [20-24].
Reference: 20. <author> R. R. Sokal and R. H. Sneath, </author> <title> Principles of Numerical Taxonomy, </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <address> San Francisco (1963). </address>
Reference: 21. <author> R. M. Cormark, </author> <title> A review of classification, </title> <journal> pp. 134-321 in J. Roy. Stat. Soc., Series A, </journal> <year> (1971). </year>
Reference: 22. <author> M. R. Anderberg, </author> <title> Clustering analysis, </title> <publisher> Academic Press, </publisher> <address> New York (1973). </address>
Reference: 23. <author> J. C. Gower, </author> <title> A comparison of some methods of cluster analysis, </title> <type> Biometrics 23, </type> <pages> pp. </pages> <month> 623-637 </month> <year> (1967). </year>
Reference: 24. <author> E. Diday and J. C. Simon, </author> <title> Clustering analysis, Communication and Cybernetics, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> (1976). </year>
Reference: 25. <author> R. S. Michalski, </author> <title> Knowledge acquisition through conceptual clustering: A theoretical framework and an algorithm for partitioning data into conjunctive concepts, </title> <journal> J. Pol. Anal. Inform. </journal> <volume> Sys 4, </volume> <pages> pp. </pages> <month> 219-244 </month> <year> (1980). </year>
Reference-contexts: Numeric - 6 - clustering methods are not interesting since they do not generate FOPL formulas needed to represent query transformation rules. Conceptual clustering arranges data into partitions based on certain conceptual classes. The basic theory and an algorithm for conceptual clustering have been developed in <ref> [25] </ref>. Other conceptual clustering algorithms include Discon [26], RUMMAGE [27], and Cluster 2 [17]. Conceptual clustering uses background knowledge about the functionality of clusters to guide the learning process towards the formation of more useful clusters [17, 28].
Reference: 26. <author> P. Langley and S. Sage, </author> <title> Conceptual Clustering as Discrimination Learning, </title> <booktitle> Proceedings of the Fifth Biennial Conference of the Canadian Society for Computational Studies of Intelligence, </booktitle> <pages> pp. </pages> <month> 95-98 </month> <year> (1984). </year>
Reference-contexts: Conceptual clustering arranges data into partitions based on certain conceptual classes. The basic theory and an algorithm for conceptual clustering have been developed in [25]. Other conceptual clustering algorithms include Discon <ref> [26] </ref>, RUMMAGE [27], and Cluster 2 [17]. Conceptual clustering uses background knowledge about the functionality of clusters to guide the learning process towards the formation of more useful clusters [17, 28]. The discovery techniques to learn empirical laws can be divided into two categories, namely quantitative and qualitative law discovery.
Reference: 27. <author> D. Fisher, </author> <title> A Hierarchical Conceptual Clustering Algorithm, </title> <type> Technical Report, </type> <institution> Department of Information and Computer Science, University of California, </institution> <address> Irvine (1984). </address>
Reference-contexts: Conceptual clustering arranges data into partitions based on certain conceptual classes. The basic theory and an algorithm for conceptual clustering have been developed in [25]. Other conceptual clustering algorithms include Discon [26], RUMMAGE <ref> [27] </ref>, and Cluster 2 [17]. Conceptual clustering uses background knowledge about the functionality of clusters to guide the learning process towards the formation of more useful clusters [17, 28]. The discovery techniques to learn empirical laws can be divided into two categories, namely quantitative and qualitative law discovery.
Reference: 28. <author> S. J. Hanson, </author> <title> Conceptual Clustering and Categorization: Bridging the Gap between Induction and Causal Models, pp. 235-268 in Machine Learning: An Artificial Intelligence Approach, </title> <editor> ed. Yves Kodratoff, </editor> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, California (1990). </address>
Reference-contexts: The basic theory and an algorithm for conceptual clustering have been developed in [25]. Other conceptual clustering algorithms include Discon [26], RUMMAGE [27], and Cluster 2 [17]. Conceptual clustering uses background knowledge about the functionality of clusters to guide the learning process towards the formation of more useful clusters <ref> [17, 28] </ref>. The discovery techniques to learn empirical laws can be divided into two categories, namely quantitative and qualitative law discovery. Quantitative discovery techniques use numeric-valued variables to discover mathematical functions that summarize the data. These techniques include regression analysis [29] and other function finding methods.
Reference: 29. <author> S. P. Ghosh, </author> <title> Statictics Metadata: Linear Regression Analysis, pp. 3-17 in Foundations of Data Organization, </title> <editor> ed. Katsumi Tanaka, </editor> <publisher> Plenum Press, </publisher> <address> New York (1987). </address> - <month> 29 </month> - 
Reference-contexts: The discovery techniques to learn empirical laws can be divided into two categories, namely quantitative and qualitative law discovery. Quantitative discovery techniques use numeric-valued variables to discover mathematical functions that summarize the data. These techniques include regression analysis <ref> [29] </ref> and other function finding methods. Previous work in automatic discovery of quantitative rules includes the Bacon system [13]. Discovering quantitative rules in databases has also been pursued by a system known as the Forty-Niner [10]. This system examines several possible subsets of data for possible empirical laws.
Reference: 30. <author> D. B. Lenat, </author> <title> The Role of Heuristics in Learning by Discovery: Three Case Studies, in Machine Learning: An Artificial Intelligence Approach, </title> <editor> ed. T. M. Mitchell, </editor> <publisher> Tioga, </publisher> <address> Palo Alto, California (1983). </address>
Reference-contexts: Qualitative law discovery refers to techniques which find logical relationships among the data. Such relationships could consist of finding bounds on attribute values or consist of qualitative relationships among attributes, such as X&gt;Y, where X and Y are attributes. Some of the algorithms for qualitative rule discovery include AM <ref> [30] </ref> and Glauber [31]. The Glauber system [31] differs from AM [30] in that as it begins with very little domain knowledge. Both methods, however, use domain heuristics to direct the search for interesting concepts. <p> Such relationships could consist of finding bounds on attribute values or consist of qualitative relationships among attributes, such as X&gt;Y, where X and Y are attributes. Some of the algorithms for qualitative rule discovery include AM <ref> [30] </ref> and Glauber [31]. The Glauber system [31] differs from AM [30] in that as it begins with very little domain knowledge. Both methods, however, use domain heuristics to direct the search for interesting concepts. Frameworks for learning qualitative empirical laws are closest to the tasks of discovering query transformation rules from data. <p> Frameworks for learning qualitative empirical laws are closest to the tasks of discovering query transformation rules from data. Integrity constraints represent regularities in the data distribution, which can be represented in FOPL like languages. Unfortunately, the previously designed learning algorithms like AM <ref> [30] </ref> cannot be used directly for learning query transformation rules due to their dependence on the domains for which they were developed. Most of qualitative empirical law discovery systems focus on learning number-theoretic, algebraic, statistical or qualitative physics oriented formulas.
Reference: 31. <author> P. Langley, J. Zytkow, H. Simon, and G. Bradshaw, </author> <title> The Search for Regularity: Four Aspects of Scientific Discovery, pp. 425-469 in Machine Learning: An Artificial Intelligence Approach, </title> <editor> ed. T. M. Mitchell, </editor> <publisher> Mor-gan Kaufmann Publishers, Inc., </publisher> <address> Los Altos, California (1986). </address>
Reference-contexts: Such relationships could consist of finding bounds on attribute values or consist of qualitative relationships among attributes, such as X&gt;Y, where X and Y are attributes. Some of the algorithms for qualitative rule discovery include AM [30] and Glauber <ref> [31] </ref>. The Glauber system [31] differs from AM [30] in that as it begins with very little domain knowledge. Both methods, however, use domain heuristics to direct the search for interesting concepts. Frameworks for learning qualitative empirical laws are closest to the tasks of discovering query transformation rules from data. <p> Such relationships could consist of finding bounds on attribute values or consist of qualitative relationships among attributes, such as X&gt;Y, where X and Y are attributes. Some of the algorithms for qualitative rule discovery include AM [30] and Glauber <ref> [31] </ref>. The Glauber system [31] differs from AM [30] in that as it begins with very little domain knowledge. Both methods, however, use domain heuristics to direct the search for interesting concepts. Frameworks for learning qualitative empirical laws are closest to the tasks of discovering query transformation rules from data.
Reference: 32. <author> R. Elmasri and G. Wiederhold, </author> <title> Data Model Integration Using the Structural Model, </title> <booktitle> SIGMOD, </booktitle> <year> (1979). </year>
Reference-contexts: State constraints refer to all constraints on the data that must hold on each state of the database [7]. Transition constraints constrain the way in which one state of the database can be transformed to another state. A rule ensuring monotonically increasing salary values is a transition constraint <ref> [32, 34-36] </ref>. We distinguish between integrity constraints and query transformation rules to provide a more accurate description of their nature. Query transformation rules are explicit state integrity constraints, which are useful for semantic query optimization for universally quantified queries.
Reference: 33. <author> M. Hammer and D. McLeod, </author> <title> Semantic Integrity in a Relational Data Base System, </title> <booktitle> VLDB, </booktitle> <year> (1975). </year>
Reference: 34. <author> K. Eswaran and D. D. Chamberlin, </author> <title> Functional Specifications of a Subsystem for Database Integrity, </title> <booktitle> VLDB, </booktitle> <year> (1975). </year>
Reference: 35. <author> E. Codd, </author> <title> Extending the Database Relational Model to Capture More Meaning, </title> <journal> TODS 4:4(December 1979). </journal>
Reference: 36. <author> P. Chen, </author> <title> The Entity Relationship Mode-Toward a Unified View of Data, </title> <journal> TODS 1:1(March 1976). </journal>
Reference: 37. <author> J. Schmidt and J. Swenson, </author> <title> On the Semantics of the Relational Model, </title> <booktitle> SIGMOD, </booktitle> <year> (1975). </year>
Reference: 38. <author> L. F. Mackert and G.M.Lohman, </author> <title> R* Optimizer Validation and Performance Evaluation for Local Queries, </title> <booktitle> Proc. ACM-SIGMOD, </booktitle> <pages> pp. 84-95 ACM, </pages> <year> (1986). </year>
Reference-contexts: The optimizer generates a number of query execution strategies (called query plans) and then attaches an estimated cost with each by using a cost model based on the the expected number of CPU instructions and page fetches <ref> [38] </ref>. The execution cost estimates provided by the optimizer are dependent on the system configuration and thus we do not give any units. However, for our present purpose only their relative values are important. The data is shown in Table 8.
References-found: 38

