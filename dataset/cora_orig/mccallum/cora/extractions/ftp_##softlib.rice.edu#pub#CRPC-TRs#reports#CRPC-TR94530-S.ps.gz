URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR94530-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Email: e-mail: rshankar, ranka@top.cis.syr.edu  
Title: Random Data Accesses on a Coarse-grained Parallel Machine I. One-to-one Mappings  
Author: Ravi V. Shankar Sanjay Ranka 
Date: October 1994  
Address: Syracuse, NY 13244-4100  
Affiliation: School of Computer and Information Science Syracuse University,  
Abstract: This paper describes deterministic communication-efficient algorithms for performing dynamic permutations on a coarse-grained parallel machine. Our analysis shows that the general permutation operation can be completed in Cn=p (+ lower order terms) time and is optimal and scalable provided n O(p 3 +p 2 t =) (n is the size of the permutation or the number of elements distributed across the p processors, t is the start-up overhead and 1= is the data transfer rate). C is a small constant typically between 2 and 3 for write permutations, slightly higher for read permutations. Modifications to exploit locality of access are presented. Special classes of permutations that are optimal for smaller sizes are also described. The dynamic permutation operation provides the framework for the communication-efficient simulation of an EREW PRAM on a coarse-grained distributed memory parallel machine. A companion paper [20] deals with the problem of random data accesses with hot spots. fl A preliminary version of this paper titled Performing Dynamic Permutations on a Coarse-grained Parallel Machine is to be presented at the First International Workshop on Parallel Processing, Bangalore, December 1994. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Seungjo Bae, Sanjay Ranka, Ravi V. Shankar. </author> <title> The Reformat Primitive Runtime Support for Data Redistribution and Array Assignment Statements in HPF, </title> <note> (in preparation). </note>
Reference-contexts: However, efficient data distribution for one phase of computation may in general be different from the next phase. In such cases performance improvement can be achieved by redistribution of data. Redistribution of data elements in HPF can be viewed as permutations <ref> [1] </ref>. Sample based sorting algorithms go through several stages [22]. First, a small sample of all the data elements in each processor is sorted to find approximate partitioners. <p> By formalizing one-to-one random data accesses, this paper provides a framework for solving irregular and unstructured applications such as graph problems in which accesses can be arbitrary/irregular and one-to-one. It also provides a framework for runtime support for languages such as HPF, specifically for data redistributions and array reformatting <ref> [1] </ref> through assignment statements. Conversions between any two regular distributions (block, cyclic,block-cyclic) in HPF, between any two irregular distributions, or between a regular distribution and an irregular distribution can all be viewed as dynamic permutations.
Reference: [2] <author> A. Bar-No. and S. Kipnis. </author> <title> Designing Broadcasting Algorithms for the Postal Model for Message-Passing Systems, </title> <booktitle> Proc. 4th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <year> 1992, </year> <pages> pp. 13-22. </pages>
Reference-contexts: This permits us to use the two-level model and view the underlying interconnection network as a virtual crossbar network connecting the processors. The logP [6] model and the postal model <ref> [2] </ref> are theoretical models, based on the above philosophy, for coarse-grained machines.
Reference: [3] <author> Shahid H. Bokhari. </author> <title> Complete Exchange on the iPSC/860, </title> <type> ICASE Technical Report No. 91-4, </type> <institution> NASA Langley Research Center, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: In particular, the dynamic permutation algorithm requires just two phases of all-to-all personalized communication with equal sized messages. Several algorithms for the all-to-all personalized communication exist, with time requirements proportional to traffic for hypercubes with cut through routing <ref> [3] </ref> or multiport communication [11], and with time requirements proportional to cross-section bandwidth for meshes [8] with cut-through routing.
Reference: [4] <author> Shahid H. Bokhari. </author> <title> Muliphase Complete Exchange on a circuit-switched hypercube, </title> <booktitle> Proceedings of 1991 International Conference on Parallel Processing, </booktitle> <pages> pp. 525-529, </pages> <year> 1991. </year>
Reference-contexts: When message sizes are small, latency becomes a dominating issue. Reduction in latency cost at the expense of sending the message to the final destination processor through several intermediate processors has been successfully achieved for all-to-all personalized communication with uniform messages by using a multiphase approach <ref> [4] </ref>. These techniques reduce on the latency requirements by transferring the data through several intermediate processors (where several messages are combined). These methods are equally applicable to our algorithms.
Reference: [5] <author> Zeki Bozkus, Sanjay Ranka, Geoffrey C. Fox. </author> <title> Benchmarking the CM-5 Multicomputer, </title> <booktitle> Proceedings of the Frontiers of Massively Parallel Computation, </booktitle> <pages> pp. 100-107, </pages> <month> October </month> <year> 1992. </year>
Reference: [6] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramonian, T. von Eicken, </author> <title> LogP: Towards a Realistic Model of Parallel Computation, </title> <booktitle> Proceedings of 4th ACM Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pp. 1-12, </pages> <year> 1993. </year>
Reference-contexts: This permits us to use the two-level model and view the underlying interconnection network as a virtual crossbar network connecting the processors. The logP <ref> [6] </ref> model and the postal model [2] are theoretical models, based on the above philosophy, for coarse-grained machines.
Reference: [7] <author> Willian J. Dally and Chuck L. Seitz. </author> <title> Deadlock-Free Message Routing in Multiprocessor Interconnection Networks, </title> <journal> IEEE Trans. on Computers, </journal> <volume> 36(5):pp. </volume> <pages> 547-553, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: For our complexity analysis we assume that t and are constant, independent of the link congestion and distance between two nodes. With new techniques such as wormhole routing and randomized routing <ref> [13, 12, 7, 15] </ref>, the distance between communicating processors seems to be less of a determining factor on the amount of time needed to complete the communication.
Reference: [8] <author> S. E. Hambrusch, F. Hameed, and A. A. Khokhar, </author> <title> Communication Operations on Coarse-Grained Mesh Architectures, </title> <type> Technical Report, </type> <institution> Department of Computer Science, Purdue University. </institution> <month> 16 </month>
Reference-contexts: Several algorithms for the all-to-all personalized communication exist, with time requirements proportional to traffic for hypercubes with cut through routing [3] or multiport communication [11], and with time requirements proportional to cross-section bandwidth for meshes <ref> [8] </ref> with cut-through routing. The algorithms performing dynamic permutations take time proportional to (the maximum of) the total number of participating elements in a processor instead of p times (the maximum of) the number of elements that could be exchanged between any two processors.
Reference: [9] <author> High Performance Fortran Forum, </author> <title> High Performance Fortran Language Specification, </title> <month> March </month> <year> 1994. </year>
Reference-contexts: Data structures and the corresponding computations are distributed such that most of the computations can be performed using local data. Several distributions for arrays have been found to be useful in practice and have been incorporated into data parallel languages like High Performance Fortran <ref> [9] </ref>. However, efficient data distribution for one phase of computation may in general be different from the next phase. In such cases performance improvement can be achieved by redistribution of data. Redistribution of data elements in HPF can be viewed as permutations [1].
Reference: [10] <author> Joseph Jaja. </author> <title> An Introduction to Parallel Algorithms Addison-Wesley, </title> <year> 1992. </year>
Reference-contexts: The algorithms described in this paper can thus be used to simulate an n processor EREW PRAM on a p processor machine. A wide variety of parallel algorithms have been described in the literature for the theoretical EREW PRAM model <ref> [10] </ref>. The PRAM simulation would provide a transparent method for implementing these algorithms on a real machine.
Reference: [11] <author> S. L. Johnsson, and C. T. Ho, </author> <title> Optimum broadcasting and personalized communication in hy-percubes. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38 (9), </volume> <pages> pp. 1249-1268, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: In particular, the dynamic permutation algorithm requires just two phases of all-to-all personalized communication with equal sized messages. Several algorithms for the all-to-all personalized communication exist, with time requirements proportional to traffic for hypercubes with cut through routing [3] or multiport communication <ref> [11] </ref>, and with time requirements proportional to cross-section bandwidth for meshes [8] with cut-through routing.
Reference: [12] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, George Karypis. </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms, </title> <address> Benjamin-Cummings, </address> <year> 1994. </year>
Reference-contexts: The data movement stage can viewed as a permutation. The scalability of sample sort critically depends on the cost of this permutation <ref> [12] </ref>. <p> For our complexity analysis we assume that t and are constant, independent of the link congestion and distance between two nodes. With new techniques such as wormhole routing and randomized routing <ref> [13, 12, 7, 15] </ref>, the distance between communicating processors seems to be less of a determining factor on the amount of time needed to complete the communication. <p> This result is of significance in the analysis of the time complexity of many algorithms. For instance, the sample sort algorithm's 15 worst-case time complexity is reduced from O (n) <ref> [12, page 246] </ref> and matrix transpose (with checker-board partitioning, i.e., block-block distribution) can be shown to be optimal on hypercubes and meshes with cut-through routing [12, page 158]. <p> For instance, the sample sort algorithm's 15 worst-case time complexity is reduced from O (n) [12, page 246] and matrix transpose (with checker-board partitioning, i.e., block-block distribution) can be shown to be optimal on hypercubes and meshes with cut-through routing <ref> [12, page 158] </ref>. By formalizing one-to-one random data accesses, this paper provides a framework for solving irregular and unstructured applications such as graph problems in which accesses can be arbitrary/irregular and one-to-one.
Reference: [13] <author> C. Leiserson et al. </author> <title> The Network Architecture of the Connection Machine CM-5, </title> <booktitle> Proc. 4th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <address> San Diego, CA, </address> <year> 1992. </year>
Reference-contexts: For our complexity analysis we assume that t and are constant, independent of the link congestion and distance between two nodes. With new techniques such as wormhole routing and randomized routing <ref> [13, 12, 7, 15] </ref>, the distance between communicating processors seems to be less of a determining factor on the amount of time needed to complete the communication.
Reference: [14] <author> MPI Forum. </author> <title> The Message-Passing Interface Standard, </title> <institution> University of Tennessee, Knoxville. </institution>
Reference-contexts: The actual coalescing is done when the elements are sent out of the processor. Such a send primitive that avoids local copying by allowing access to data from non-contiguous areas is available in MPI (Message Passing Interface <ref> [14] </ref>) and can be easily implemented in low-level software.
Reference: [15] <author> Lionel M. Ni and Philip K. McKinley. </author> <title> A Survey of Wormhole Routing Techniques in Direct Networks, </title> <journal> IEEE Computer, </journal> <volume> 26(2) </volume> <pages> 62-76, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: For our complexity analysis we assume that t and are constant, independent of the link congestion and distance between two nodes. With new techniques such as wormhole routing and randomized routing <ref> [13, 12, 7, 15] </ref>, the distance between communicating processors seems to be less of a determining factor on the amount of time needed to complete the communication.
Reference: [16] <author> D. Nassimi and S. Sahni. </author> <title> Data Broadcasting in SIMD Computers, </title> <journal> IEEE Transactions on Computers C-30(2):101-107 (1981). </journal>
Reference-contexts: In fact, a single stage algorithm is sufficient to perform a monotonic permutation optimally and deterministically with no node contention on a virtual crossbar. In this section we have chosen to present the monotonic permutation algorithm through two important primitives, concentrate and distribute <ref> [16] </ref>, where the pointers of the permutation are sorted. These primitives are useful, for instance, when working with sparse arrays, where the concentrate primitive can be used to convert the array from a dense representation to a compact representation [21].
Reference: [17] <author> S. Ranka, J. C. Wang, and G. C. Fox. </author> <title> Static and Runtime Scheduling of All-to-Many Personalized Communication on Permutation Networks, </title> <journal> IEEE Trans. on Parallel and Distributed Systems. </journal> <note> To appear. </note>
Reference-contexts: The algorithm was generalized to deal with the case of unbalanced dynamic permutations. Algorithms for special cases such as monotonic permutations, concentrate/distribute, and multiple permutations were also presented. Scheduling of static permutations has been discussed in <ref> [17, 18] </ref>. The constants in the communication time complexity of the algorithms presented in this paper are very small. This is a necessary requirement for effective utilization of typical coarse-grained machines. When message sizes are small, latency becomes a dominating issue.
Reference: [18] <author> S. Ranka, J. C. Wang and M. Kumar. </author> <title> All-to-many communication avoiding node contention, </title> <journal> Journal of Parallel and Distributed Computing. </journal> <note> To appear. </note>
Reference-contexts: The algorithm was generalized to deal with the case of unbalanced dynamic permutations. Algorithms for special cases such as monotonic permutations, concentrate/distribute, and multiple permutations were also presented. Scheduling of static permutations has been discussed in <ref> [17, 18] </ref>. The constants in the communication time complexity of the algorithms presented in this paper are very small. This is a necessary requirement for effective utilization of typical coarse-grained machines. When message sizes are small, latency becomes a dominating issue.
Reference: [19] <author> Ravi V. Shankar, Khaled A. Alsabti, Sanjay Ranka. </author> <title> The Transportation Primitive, </title> <type> CIS Technical Report, </type> <institution> Syracuse University, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: Sorting all the elements based on the destination element indices is one way of performing a dynamic permutation optimally. Such sorting based algorithms are highly communication inefficient, since the elements are moved around through many intermediate processors during the sort. See <ref> [19] </ref> for details. Using sample based sorting algorithms for performing permutations is not an option since these algorithms themselves require a permutation for data movement. Algorithms using randomization also have large constants and are not very practical for coarse-grained machines. <p> The dynamic permutation problem has the property that each processor sends out no more than dn=pe elements and receives no more than dn=pe elements. Since the outgoing/incoming traffic at any processor is upper-bounded by dn=pe, this is a bounded transportation problem <ref> [19] </ref>. Underlying each permutation is a many-to-many personalized communication problem. The communication matrices in figure 3 show the underlying communication pattern for the permutations in figure 1. A minimal restriction on the size of the permutations for optimality and communication-efficiency is derived next. <p> The O (p 2 ) terms in the constraints for x, y (n=p in the balanced permutation case) represent worst case requirements. A probabilistic analysis <ref> [19] </ref> indicates that these requirements can be brought down to O (p p p ln p) from O (p 2 ).
Reference: [20] <author> Ravi V. Shankar, Sanjay Ranka. </author> <title> Random Data Accesses on a Coarse-Grained Parallel Machine II. </title> <editor> One-to-many and Many-to-one Mappings, </editor> <month> October </month> <year> 1994. </year>
Reference: [21] <author> Ravi V. Shankar, Sanjay Ranka. </author> <title> Parallel Vision Algorithms Using Sparse Array Representations, </title> <journal> Pattern Recognition, 1993, </journal> <volume> vol. 26, No. 10, </volume> <pages> pp. 1511-1519. </pages>
Reference-contexts: These primitives are useful, for instance, when working with sparse arrays, where the concentrate primitive can be used to convert the array from a dense representation to a compact representation <ref> [21] </ref>. After working with the compact representation, the distribute primitive can be used to convert the array back to the dense representation.
Reference: [22] <author> H. Shi, J. Schaeffer. </author> <title> Parallel Sorting by Regular Sampling. </title> <journal> Journal of Parallel and Distributed Computing, Vol.14, </journal> <volume> pp.361-372, </volume> <year> 1990. </year> <month> 17 </month>
Reference-contexts: In such cases performance improvement can be achieved by redistribution of data. Redistribution of data elements in HPF can be viewed as permutations [1]. Sample based sorting algorithms go through several stages <ref> [22] </ref>. First, a small sample of all the data elements in each processor is sorted to find approximate partitioners.
References-found: 22

