URL: http://www.cs.uni-bonn.de/III/lehre/vorlesungen/DataMining/WS96/literatur/holsheimer94:data.ps.gz
Refering-URL: http://www.cs.uni-bonn.de/III/lehre/vorlesungen/DataMining/WS96/
Root-URL: http://cs.uni-bonn.de
Email: fmarcel,arnog@cwi.nl  
Title: Data Mining The Search for Knowledge in Databases data objects is often corrupted or missing.
Author: Marcel Holsheimer, Arno Siebes 
Keyword: CR Subject Classification (1991): Database applications (H.2.8), Information search and retrieval (H.3.3), Learning (I.2.6) concept learning, induction, knowledge acquisition, Clustering (I.5.3) Keywords Phrases: database applications, machine learning, inductive learning, knowledge acquisition, data summarization  
Note: Another important problem is that information in  
Address: P.O. Box 94079, 1090 GB Amsterdam, The Netherlands  
Affiliation: CWI  
Abstract: Data mining is the search for relationships and global patterns that exist in large databases, but are `hidden' among the vast amounts of data, such as a relationship between patient data and their medical diagnosis. These relationships represent valuable knowledge about the database and objects in the database and, if the database is a faithful mirror, of the real world registered by the database. One of the main problems for data mining is that the number of possible relationships is very large, thus prohibiting the search for the correct ones by simple validating each of them. Hence, we need intelligent search strategies, as taken from the area of machine learning. This report provides a survey of current data mining research, it presents the main underlying ideas, such as inductive learning, and search strategies and knowledge representations used in data mine systems. Furthermore, it describes the most important problems and their solutions, and provides an survey of research projects. Report CS-R9406 ISSN 0169-118X CWI P.O. Box 94079, 1090 GB Amsterdam, The Netherlands 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Hassan Ait-Kaci and Patrick Lincoln. </author> <title> LIFE, a natural language for natural language. </title> <editor> T. A. Informations, Revue Internationale du traitement automatique du langage, 30(1-2):51-89, </editor> <year> 1989. </year>
Reference-contexts: This notion of certainty is formalised through the notion of Probably Approximately Correct learning or PAC-learning [62]. Before we formulate this notion, we first recall some basic concepts from probability theory. A probability distribution on U is a function 2 : P (U ) ! <ref> [0; 1] </ref> such that: 1. (;) = 0; 3. for pairwise disjoint sets S 1 ; : : : ; S n 2 P (U ), ( S n P n For a set E U , (E) denotes the change that a randomly chosen x 2 U belongs to E. <p> To formalise this, we need the distributions n : P (U n ) ! <ref> [0; 1] </ref>, where n (Y ) denotes the chance that a random sample of n elements belongs to Y . Then: Definition 5. <p> By definition, any symbol is smaller than the top-symbol, which denotes the entire domain. For example, the user could define that `Holland' is a parent node for `Amsterdam', `Rot-terdam' and `The Hague'. 1.4 Quality function The quality function assigns a value, e.g. in the domain <ref> [0; 1] </ref>, to each description, indicating its quality. There are two aspects to the quality of a description. A description should be general valid, that is, it should classify any unseen object correctly. Furthermore, the description should be correct with respect to classes defined by the user. <p> We define an expectation function, that returns the quality of the description that would be generated by applying a particular operation. Definition 7. Expectation function The expectation function F : D fi O ! <ref> [0; 1] </ref> could thus be defined as: F (D; o) = f (o (D)) where f is the quality function, as defined above, and o (D) is the description, resulting from application of operation o on D. <p> Constraint logic programming languages (see Jaffar and Lassez in [21]) allow complex constraints between numerical variables. These constraints can be very useful for representing numerical relationships, as we discuss in Chapter 8. The language LIFE (Logic Inheritance Functions Equations), developed by Ait-Kaci (see <ref> [1, 2] </ref>), is a synthesis of three different programming paradigms: logic programming, functional programming and object-based programming. In LIFE one can define hierarchies of values, that are useful for representing domain knowledge. <p> In Figure 6.1 such an element, called neuron is shown. The input consists of N values x 0 ; x 1 ; : : : ; x N1 and a single output y, all having continuous values in a particular domain, e.g. <ref> [0; 1] </ref>. The neuron computes the weighted sum of its inputs, subtracts a threshold , and passes 4. Neural networks 47 the result to a non-linear function f , e.g. the sigmoid, shown in Figure 6.1.
Reference: 2. <author> Hassan Ait-Kaci and Roger Nasr. </author> <title> LOGIN: A logic programming language with built-in inheritance. </title> <journal> Journal of Logic Programming, </journal> <volume> 3 </volume> <pages> 185-215, </pages> <year> 1986. </year>
Reference-contexts: Constraint logic programming languages (see Jaffar and Lassez in [21]) allow complex constraints between numerical variables. These constraints can be very useful for representing numerical relationships, as we discuss in Chapter 8. The language LIFE (Logic Inheritance Functions Equations), developed by Ait-Kaci (see <ref> [1, 2] </ref>), is a synthesis of three different programming paradigms: logic programming, functional programming and object-based programming. In LIFE one can define hierarchies of values, that are useful for representing domain knowledge.
Reference: 3. <author> Martin Anthony and Norman Biggs. </author> <title> Computational learning theory: an introduction, </title> <booktitle> volume 30 of Cambridge Tracts in Theoretical Computer Science. </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: In fact, even with infinite training sets some concepts cannot be learned exactly, since some problems are undecidable. However, some of these concepts can be approximated. Although computational learning theory, which studies these problems, is outside the scope of this survey, see e.g. <ref> [3] </ref>, we outline the basic ideas in the last section of this chapter. 1. Induction from databases As described in the previous chapter, learning is tantamount to the construction of rules, based on observations of environmental states and transitions. Automation of a learning process is called machine learning. <p> However, in this section we briefly discuss some of its main results in as far as they apply to data mining. The interested reader is referred to, e.g., <ref> [3] </ref>. 22 Chapter 3. Data mining 2.1 Learning by enumeration, or exhaustive search Clearly, there exist concepts that cannot be learned by any algorithm.
Reference: 4. <author> Chidanand Apte, Sholom Weiss, and Gordon Grout. </author> <title> Predicting defects in disk drive manufacturing: a case study in high-dimensional classification. </title> <booktitle> In Proceedings of the 9th Conference on Artificial Intelligence for Applications, </booktitle> <pages> pages 212 - 218, </pages> <address> Orlando, Florida, </address> <year> 1993. </year>
Reference-contexts: Only few examples can be found in literature. A credit card company has used a data mine tool to infer new, better, acceptance rules based on the credit-history of current clients [9]. IBM has used data mining techniques to predict defects during the assembly of disk drives <ref> [4] </ref>. Other applications can be found in [42]. 1. Overview of the report This report outlines data mining: it describes which forms of information can be derived, how regularities and rules can be discovered, and summarizes the major obstacles and techniques to overcome these.
Reference: 5. <author> Robert L. Blum. </author> <title> Discovery and Representation of Causal Relationships from a Large Time-Oriented Clinical Database: The RX Project, </title> <booktitle> volume 19 of Lecture Notes in Medical Informatics. </booktitle> <publisher> Spinger-Verlag, </publisher> <year> 1982. </year>
Reference-contexts: There are no explicit techniques to handle noise and exceptional data in the rule construction algorithm, but post optimization is used to remove those rules that are due to noisy data. 6. RADIX/RX The RX system is used for the discovery of relationships in a clinical database (see <ref> [5, 63, 64] </ref>). A very important difference with other systems is that the notion of time is included: the data objects in the set of examples store information on patients at different times and the generated knowledge consists of causal relationships.
Reference: 6. <author> Pierre Brezellec and Henri Soldano. Samia: </author> <title> a bottom-up learning method using a simulated annealing algorithm. </title> <booktitle> In Proceedings of the European conference on Machine Learning, Lecture notes in Artificial Intelligence, </booktitle> <pages> pages 297 - 309. </pages> <publisher> Springer-verlag, </publisher> <year> 1993. </year>
Reference-contexts: The temperature is slowly decreased during the search process, and the process stabilizes in the global maximum. The SAMIA system combines a bottom up strategy with simulated annealing (see <ref> [6] </ref>). Here, the next operation is randomly chosen, if application of this operation results in a positive change in quality f .
Reference: 7. <author> Yandong Cai, Nick Cercone, and Jiawei Han. </author> <title> Attribute-oriented induction in relational databases. </title> <booktitle> In Piatetsky-Shapiro and Frawley [44], </booktitle> <pages> pages 213 - 228. </pages>
Reference-contexts: Complete rules If the coverage equals 1, the rule is complete, that is, any object belonging to the class is covered by the description for this class, i.e. C oe D (S). In other words, the description is a necessary condition for the class <ref> [7, p. 222] </ref>. Definition 5. Deterministic rules If the classification accuracy is 1, the rule is deterministic, i.e. always classifies correct. Any object covered by the description belongs to class, C oe D (S). Hence the description is a sufficient condition for the class. Definition 6. <p> The performance of ID3, AQ, and CN2 has been compared on medical and artificial domains. The discovered knowledge structures are equivalent in terms of accuracy and complexity. 4. DBLearn The DBLearn system, designed by Cai, Han and Cercone (see <ref> [7, 16] </ref>), uses domain knowledge to generate descriptions for predefined subsets in a relational database. Special features of this system are its bottom up search strategy, its use of domain knowledge in the form of hierarchies of attribute values, and its use of relational algebra.
Reference: 8. <author> Jaime G. Carbonell, Ryszard S. Michalski, and Tom M. Mitchell. </author> <title> An overview of machine learning. </title> <editor> In Michalski et al. </editor> <volume> [30], </volume> <pages> pages 3 - 24. </pages>
Reference-contexts: For successful operation in such an environment, the system has to be adaptive, that is, it should learn. Learning consists of finding both an appropriate internal representation, i.e. classes, and a model transition function, acting on this representation. There are various learning strategies, as discussed in <ref> [8] </ref>. These strategies differ in the amount of inferential skills, required by the 2. Learning 13 system. In learning by being told, knowledge is acquired from a teacher or any other organized source, such as a textbook. <p> We shortly outline the advantages of FOL, and discuss some representations with equivalent expressive power that offer a better structuring of the knowledge, such as semantic networks and frames. For a more extensive discussion of these representations, the reader is referred to <ref> [8, 11, 54] </ref>. The last part discusses neural networks, a non-symbolic representation, together with a highly parallel learning algorithm. We compare this representation with symbolic representations. 1. Propositional-like representations Propositional representations use a logic formulae, consisting of attribute value conditions.
Reference: 9. <author> Chris Carter and Jason Catlett. </author> <title> Assessing credit card applications using machine learning. </title> <journal> IEEE Expert, </journal> <note> Fall 1987:71 - 79, </note> <year> 1987. </year>
Reference-contexts: Only few examples can be found in literature. A credit card company has used a data mine tool to infer new, better, acceptance rules based on the credit-history of current clients <ref> [9] </ref>. IBM has used data mining techniques to predict defects during the assembly of disk drives [4]. Other applications can be found in [42]. 1.
Reference: 10. <author> Philip K. Chan and Salvatore J. Stolfo. </author> <title> Experiments in multistrategy learning by meta-learning. </title> <booktitle> In Proceedings of the second international conference on information and knowledge management, </booktitle> <pages> pages 314 - 323, </pages> <address> Washington, DC, </address> <year> 1993. </year>
Reference-contexts: However, for many applications, using only a single learning strategy will not work. Multistrategy learning techniques use different algorithms depending on the application domain, or a combination of algorithms. Intelligent systems could select an appropriate learning technique themselves (see <ref> [10] </ref>). A third scenario consists of enhancing all kinds of conventional systems (e.g. editors, databases management systems, user-interfaces) by incorporating learning capabilities in these systems. As a result, computers will learn from their environment, thus narrowing the gap between man and computer. References 75
Reference: 11. <author> Peter Clark. </author> <title> Knowledge representation in machine learning. </title> <editor> In Yves Kodratoff and Alan Hutchinson, editors, </editor> <booktitle> Machine and Human Learning, advances in European Research, </booktitle> <pages> pages 35 - 49. </pages> <publisher> Michael Horwood, </publisher> <address> London, </address> <year> 1989. </year>
Reference-contexts: We shortly outline the advantages of FOL, and discuss some representations with equivalent expressive power that offer a better structuring of the knowledge, such as semantic networks and frames. For a more extensive discussion of these representations, the reader is referred to <ref> [8, 11, 54] </ref>. The last part discusses neural networks, a non-symbolic representation, together with a highly parallel learning algorithm. We compare this representation with symbolic representations. 1. Propositional-like representations Propositional representations use a logic formulae, consisting of attribute value conditions.
Reference: 12. <author> Peter Clark and Tim Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3:261 - 283, </volume> <year> 1989. </year>
Reference-contexts: CN2 The CN2 system, by Clark and Niblett (see <ref> [12] </ref>), is an adaptation of the AQ system. As we mentioned above, a disadvantage of the AQ algorithm is that the algorithm handles noise not by itself, but uses pre- and postprocessing (e.g. rule truncation). <p> In searching for these relationships, the use of statistical techniques seems to be a natural choice. Experiments show that application of even very simple statistical techniques is a very promissing direction, as for example shown in <ref> [12] </ref>. Here, the performance of the AQ, CN2 and ID3 algorithms is compared with the performance of a simple bayesian classifier on several domains.
Reference: 13. <author> P. Compton and R. Jansen. </author> <title> Knowledge in context: a strategy for expert system maintenance. </title> <booktitle> In Proceedings of the 2nd Australian Joint Artificial Intelligence conference, volume 406 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 292 - 306, </pages> <address> Adelaide, 1988. </address> <publisher> Springer. </publisher>
Reference-contexts: This lack of locality makes decision lists difficult to understand, since the rules in the list are only meaningful in the context given by all the preceding rules. Hence, there is a need for representing exceptions in a more localized manner, e.g. by using ripple-down rule sets <ref> [13] </ref>. These rules consist of conditions and exceptions to these conditions that are local to the rule. In fact, these rules are nested if-then statements, e.g. if OE i then if OE j then C j else C i else C j is a ripple-down rule set of depth 2.
Reference: 14. <author> Thomas G. Dietterich and Ryszard S. Michalski. </author> <title> A comparative review of selected methods for learning from examples. </title> <editor> In Michalski et al. </editor> <volume> [30], </volume> <pages> pages 41 - 81. </pages>
Reference-contexts: The system has to find the description for each class. The teacher can either define a single class or multiple classes (see <ref> [14] </ref>): Single class learning The teacher defines a single class C for which a so-called characteristic description has to be constructed: a description that singles out instances of C from any other example that is not an instance of C. <p> Techniques to deal with noise are primitive, application of statistical techniques, such as the O 2 test, could improve results. There are no techniques to deal with missing attribute values. 5. Meta-Dendral The Meta-Dendral system is a special purpose machine learning system (see <ref> [14, 63] </ref>), designed for the automated discovery of rules of mass spectroscopy. We discuss this system, because it applies machine learning techniques to an entirely different data representation. Mass spectroscopy is a technique to analyze the three dimensional structure of a molecule.
Reference: 15. <author> Benjamin S. Duran and Patrick L. Odell. </author> <title> Cluster analysis: a survey, </title> <booktitle> volume 100 of Lecture Notes in Economics and Mathematical Systems. </booktitle> <publisher> Spinger-Verlag, </publisher> <year> 1974. </year>
Reference-contexts: Let us denote by P (N; m) the number of ways in which this can be done. In general the number of ways to partition N elements in m subsets is given by <ref> [15] </ref>: P (N; m) = m! j=0 m ! P (N; m) is a function that grows exponentially fast in N , in fact, for large N we have P (N; m) m N =m! m Nm e m 2m.
Reference: 16. <author> Jiawei Han, Yandong Cai, and Nick Cercone. </author> <title> Knowledge discovery in databases: An 76 References attribute-oriented approach. </title> <booktitle> In Proceedings of the 18th VLDB Conference, </booktitle> <pages> pages 547 - 559, </pages> <address> Vancouver, British Columbia, Canada, </address> <year> 1992. </year>
Reference-contexts: Databases over time We can also look for data evolution regularities, that is, the discovery of global changes in a database over a period <ref> [16] </ref>. Thereto, we construct a training set, composed from two sets S i and S j , taken from the same database at different times. All examples originally from S i belong to class `at Time = i' and examples in S j belong to class `at Time = j'. <p> The performance of ID3, AQ, and CN2 has been compared on medical and artificial domains. The discovered knowledge structures are equivalent in terms of accuracy and complexity. 4. DBLearn The DBLearn system, designed by Cai, Han and Cercone (see <ref> [7, 16] </ref>), uses domain knowledge to generate descriptions for predefined subsets in a relational database. Special features of this system are its bottom up search strategy, its use of domain knowledge in the form of hierarchies of attribute values, and its use of relational algebra.
Reference: 17. <author> Geoffrey E. Hinton. </author> <title> Connectionist learning procedures. </title> <booktitle> In Kodratoff and Michalski [25], </booktitle> <pages> pages 555 - 610. </pages>
Reference-contexts: Knowledge representation output value is higher than the desired value, and increased if the actual value is lower than the desired value. The algorithm terminates when all weights stabilize. An algorithm that is often used is the back-propagation algorithm, discussed in <ref> [17, 28] </ref>.
Reference: 18. <author> John H. Holland. </author> <title> Adaptation in natural artificial systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, </address> <year> 1975. </year>
Reference-contexts: Another problem is that the search process is sometimes not able to escape a local maximum. To overcome these problems, other strategies have been proposed. 4.1 Genetic algorithms Genetic Algorithms (GAs) originated from the studies of cellular automata, conducted by Holland et al. <ref> [18, 19] </ref>. A GA is a search procedure modelled on the mechanics of natural selection rather than a simulated reasoning process.
Reference: 19. <author> John H. Holland. </author> <title> Escaping brittleness: the possibilities of general purpose algorithms applied to parallel rule-based systems. </title> <editor> In Michalski et al. </editor> <volume> [31], </volume> <pages> pages 593 - 623. </pages>
Reference-contexts: Another problem is that the search process is sometimes not able to escape a local maximum. To overcome these problems, other strategies have been proposed. 4.1 Genetic algorithms Genetic Algorithms (GAs) originated from the studies of cellular automata, conducted by Holland et al. <ref> [18, 19] </ref>. A GA is a search procedure modelled on the mechanics of natural selection rather than a simulated reasoning process.
Reference: 20. <author> John H. Holland, Keith J. Holyoak, Richard E. Nisbett, and Paul R. Thagard. </author> <title> Induction: processes of inference, learning and discovery. Computational models of cognition and perception. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: Models Models are used to predict changes in the environment, and to allow the cognitive system to interact more successfully with this environment. For an extensive discussion of models and inductive learning, the reader is referred to Holland et al. in <ref> [20] </ref>. 1.1 Environment The environment of a cognitive system depends on the context, it may be defined in very local terms (a chess board, or all customers of a sales company), or as the whole of the universe, including the system itself. <p> For this reason, the most recent information should be valuated higher then older information, i.e. we should use some learning bias, assigning higher weights to recent experiences in the learning process. Cognitive systems adjust their rules when too many incorrect predictions are made (see Holland et al. <ref> [20] </ref>). This could also trigger rule adjustment in data mine systems: when the actual probability of a rule is out of pace with its predicted probability over a certain period, the rule is adjusted.
Reference: 21. <author> Joxan Jaffar and Jean-Louis Lassez. </author> <title> Constraint logic programming. </title> <booktitle> In Conference record of the 14th annual ACM symposium on principles of programming languages, </booktitle> <pages> pages 111 - 119, </pages> <address> Munich, Germany, </address> <year> 1987. </year>
Reference-contexts: An example of an ILP system is Quinlan's FOIL system (see [51]). There are some extensions to FOL that may prove to be very useful for representing knowledge in a data mine system. Constraint logic programming languages (see Jaffar and Lassez in <ref> [21] </ref>) allow complex constraints between numerical variables. These constraints can be very useful for representing numerical relationships, as we discuss in Chapter 8.
Reference: 22. <author> Kenneth De Jong. </author> <title> Genetic-algorithm-based learning. </title> <booktitle> In Kodratoff and Michalski [25], </booktitle> <pages> pages 611 - 638. </pages>
Reference-contexts: There are two approaches for learning classification rules, named after their university. These approaches differ in the representation of the rules, the crossover operations and the quality function. An overview of learning classification rules can be found in <ref> [22, 59] </ref>: The Michigan approach developed by Holland. An organism represents a set-description. Consequently, all organisms are of the same length. The entire population together forms a classification rule, that is, a rule whose description consists of the disjunction of all descriptions, represented by the organisms.
Reference: 23. <author> H.J. Kappen. Neurale netwerken, </author> <title> fuzzy rules en artificiele intelligentie. Foundation for Neural Networks. </title>
Reference-contexts: One of the objects of data mining is to generate knowledge in a form suitable for verification or interpretation by humans. There has been some research on transforming this knowledge to a format better suited for human reading, as in <ref> [23, 57] </ref>, but this mainly concerns single layer networks, that model simple, linear functions. As with genetic algorithms, it is difficult to incorporate any domain knowledge or user interaction in the learning process.
Reference: 24. <author> Jyrki Kivinen, Heikki Mannila, and Esko Ukkonen. </author> <title> Learning rules with local exceptions. </title> <type> Technical report, </type> <institution> University of Helsinki, </institution> <year> 1993. </year>
Reference-contexts: Note that by giving all the exceptions locally, we have eliminated the need for a global ordering of the rules, as in decision lists. In <ref> [24] </ref>, an efficient algorithm for the construction of ripple-down rule sets is presented. 44 Chapter 6. Knowledge representation 2. First order logic The simple propositional-like representation has been successfully used in many machine learning systems.
Reference: 25. <editor> Yves Kodratoff and Ryszard S. Michalski, editors. </editor> <booktitle> Machine Learning, an Artificial Intelligence approach, </booktitle> <volume> volume 3. </volume> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference: 26. <author> Pat Langley, Gary L. Bradshaw, and Herbert A. Simon. </author> <title> Rediscovering chemistry with the Bacon system. </title> <editor> In Michalski et al. </editor> <volume> [31], </volume> <pages> pages 307 - 329. </pages>
Reference-contexts: In this chapter we review two numerical learning systems, i.e. systems where both the predicting variables and the predicted variable are numerical. 1. Bacon The Bacon system uses a data analysis algorithm to discover mathematical relationships in numerical data <ref> [26, 63] </ref>. It rediscover relationships such as Ohm's law for electric circuits and Archimedes' law of displacement. The training set consists of numerical data, possibly generated at some previous experiment.
Reference: 27. <author> D. Lenat. EURISKO: </author> <title> A program that learns new heuristics and domain concepts. The nature of heuristics III: Background and examples. </title> <journal> Artificial Intelligence, </journal> <volume> 21:61 - 98, </volume> <year> 1983. </year>
Reference-contexts: Although frames can be represented in FOL, they provide a better insight in the structure of knowledge than a set of logically equivalent but unstructured first order predicates. The EURISKO system uses frames for the representation of the acquired knowledge <ref> [27] </ref>. 4. Neural networks Artificial neural networks, also known as connectionist models, are densely interconnected networks of simple computational elements (for an introduction, see [28, 40]). In Figure 6.1 such an element, called neuron is shown.
Reference: 28. <author> Richard P. Lippmann. </author> <title> An introduction to computating with neural nets. </title> <journal> IEEE ASSP Magazine, </journal> <volume> April:4 - 22, </volume> <year> 1987. </year>
Reference-contexts: The EURISKO system uses frames for the representation of the acquired knowledge [27]. 4. Neural networks Artificial neural networks, also known as connectionist models, are densely interconnected networks of simple computational elements (for an introduction, see <ref> [28, 40] </ref>). In Figure 6.1 such an element, called neuron is shown. The input consists of N values x 0 ; x 1 ; : : : ; x N1 and a single output y, all having continuous values in a particular domain, e.g. [0; 1]. <p> Knowledge representation output value is higher than the desired value, and increased if the actual value is lower than the desired value. The algorithm terminates when all weights stabilize. An algorithm that is often used is the back-propagation algorithm, discussed in <ref> [17, 28] </ref>.
Reference: 29. <author> Ryszard S. Michalski. </author> <title> A theory and methodology of inductive learning. </title> <editor> In Michalski et al. </editor> <volume> [30], </volume> <pages> pages 83 - 134. </pages>
Reference-contexts: The data mine system is supplied objects, as in supervised learning, but now, no classes are defined. The system has to observe the examples, and recognize patterns (i.e. class descriptions) by itself. Hence, this learning form is also called learning by observation and discovery <ref> [29] </ref>. The result of an unsupervised learning process is a set of class descriptions, one for each discovered class, that together cover all objects in the environment. These descriptions form a high-level summary of the objects in the environment. <p> In the supervised learning case, this requires that the user defines one or more classes, also known as concepts <ref> [29] </ref>, in the database. Without loss of generality, we may assume that the database contains one or more attributes that denote the class of a tuple, these attributes are called the predicted attributes. The remaining attributes are called predicting attributes. <p> The overall quality is the weighted sum of the qualities for these criteria. Finding the optimal weights is a form of fine-tuning the system. An alternative to this weighted sum is the lexicographic evaluation functional (LEF) <ref> [29] </ref>. <p> This information is specific to a particular domain, and has to be supplied by the user. Michalski distinguishes the following forms of domain knowledge (see <ref> [29] </ref>): Irrelevant attributes Not all attributes in the examples are relevant. For example, first name is not considered to be a relevant attribute for medical diagnoses.
Reference: 30. <author> Ryszard S. Michalski, Jaime G. Carbonell, and Tom M. Mitchell, </author> <title> editors. </title> <booktitle> Machine Learning, an Artificial Intelligence approach, </booktitle> <volume> volume 1. </volume> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1983. </year>
Reference: 31. <author> Ryszard S. Michalski, Jaime G. Carbonell, and Tom M. Mitchell, </author> <title> editors. </title> <booktitle> Machine Learning, an Artificial Intelligence approach, </booktitle> <volume> volume 2. </volume> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1986. </year>
Reference: 32. <author> Ryszard S. Michalski, Igor Mozetic, Jiarong Hong, and Nada Lavrac. </author> <title> The AQ15 inductive learning system: an overview and experiments. </title> <type> Technical Report UIUCDCS-R-86-1260, </type> <institution> University of Illinois, </institution> <month> July </month> <year> 1986. </year>
Reference-contexts: The classification accuracy is high. However, the system does not make any use of domain knowledge. Furthermore, trees are not easy to understand, however, they can be transformed to decision rules, as described in [49]. Michalski's AQ15 system (see <ref> [32, 33] </ref>) is an inductive learning system that generates decision rules, where the conditional part is a logical formulae. A special feature of this system is constructive induction, i.e. the use of domain knowledge to generate new attributes, that are not present in the input data.
Reference: 33. <author> Ryszard S. Michalski, Igor Mozetic, Jiarong Hong, and Nada Lavrac. </author> <title> The multi-purpose References 77 incremental learning system AQ15 and its testing application to three medical domains. </title> <booktitle> In Proceedings of the 5th national conference on Artificial Intelligence, </booktitle> <pages> pages 1041 - 1045, </pages> <address> Philadelphia, </address> <year> 1986. </year>
Reference-contexts: The classification accuracy is high. However, the system does not make any use of domain knowledge. Furthermore, trees are not easy to understand, however, they can be transformed to decision rules, as described in [49]. Michalski's AQ15 system (see <ref> [32, 33] </ref>) is an inductive learning system that generates decision rules, where the conditional part is a logical formulae. A special feature of this system is constructive induction, i.e. the use of domain knowledge to generate new attributes, that are not present in the input data.
Reference: 34. <author> Ryszard S. Michalski and Robert E. Stepp. </author> <title> Learning from observation: conceptual clustering. </title> <editor> In Michalski et al. </editor> <volume> [30], </volume> <pages> pages 331 - 363. </pages>
Reference-contexts: Any simple description that covers the entire training set will be assigned a high quality. Especially, the description true, covering all objects in the training set, will rate very good, because it is extremely simple, and assigns all examples to their correct (and only) class. In <ref> [34] </ref>, Michalski and Stepp suggest that a quality function should not only dependent on the simplicity of the description, but also on the fit: how close does the description approximate the set of examples.
Reference: 35. <author> Marvin Minsky. </author> <title> A framework for representating knowledge. </title> <editor> In Patrick Henry Winston, editor, </editor> <booktitle> The Psychology of Computer Vision, </booktitle> <pages> pages 211 - 277. </pages> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: a FOL program), they provide a more comprehensible representation, e.g. by explicitly stating subtype relationships among objects, or allowing for an exception based representation. 3.1 Semantic nets A semantic network is a graph, where the nodes denote concepts, or meanings, and the arcs denote relationships between these concepts (see Minsky <ref> [35] </ref>). Example 3 The semantic network below states that John is the spouse of Mary, and Mary has red hair. Furthermore, John and Mary are both instances of the sort humans, which are again mammals.
Reference: 36. <author> Tom M. Mitchell, Paul E. Utgoff, and Ranan Banerji. </author> <title> Learning by experimentation: acquiring and refining problem-solving heuristics. </title> <editor> In Michalski et al. </editor> <volume> [30], </volume> <pages> pages 163 - 190. </pages>
Reference-contexts: A condition A i = c i is called an attribute-value condition. The set of all possible descriptions is called the description space and is denoted by D. Some systems, e.g. LEX <ref> [36] </ref>, only try to find elementary descriptions, while others allow the full set of descriptions. Since the domain are assumed to be finite, we can count the number of descriptions these systems can find.
Reference: 37. <author> Raymond J. Mooney. </author> <title> Encouraging experimental results on learning CNF. </title> <type> Technical report, </type> <institution> University of Texas, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: For example, the above formula could be represented as the set-description `color 2 fred, greeng ^ shape 2 fcircleg'. An alternative is the Disjunctive Normal Form (DNF), a disjunction of terms, where terms are conjunctions of attribute value conditions. However, as shown in <ref> [37] </ref>, the CNF performs surprisingly well, that is, the generated descriptions are smaller than the DNF representations for the same learning tasks. Actually, both representations are not really propositional, because they involve a variable|the object itself.
Reference: 38. <editor> Katharina Morik. </editor> <booktitle> Applications of machine learning. In Proc. 6th European Knowledge Acquisition Workshop, </booktitle> <pages> pages 9 - 13. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1992. </year>
Reference-contexts: In such a representation, again trends can be discovered, i.e. one can search for migration streams of objects from one subset to another over a period of time. Furthermore, supervised learning techniques can be used to search for characteristics for each of these migration streams. Morik (in <ref> [38] </ref>) foresees three different scenarios for the application of machine learning techniques. So far, we followed the first scenario: the application of machine learning systems to current applications, such as databases. However, for many applications, using only a single learning strategy will not work.
Reference: 39. <author> Stephen Muggleton. </author> <title> Inductive Logic Programming, </title> <booktitle> volume 38 of A.P.I.C. series. </booktitle> <publisher> Academic Press Ltd., </publisher> <address> London, </address> <year> 1992. </year>
Reference-contexts: In [24], an efficient algorithm for the construction of ripple-down rule sets is presented. 44 Chapter 6. Knowledge representation 2. First order logic The simple propositional-like representation has been successfully used in many machine learning systems. Along with the successes of this technology, the following limitations are becoming apparent <ref> [39] </ref>: Restricted representation Propositional-like representations strongly limit the form of relationships and patterns that can be represented. Patterns that are defined in terms of relationships among objects or attributes cannot be represented. For example, consider a class consisting of all persons with `identical first and lastname'. <p> The above shortcomings can be overcome by moving towards a more powerful representation. A growing number of machine learning systems employ some kind of First Order Logic (FOL) to represent learned knowledge (for an overview see Muggleton <ref> [39] </ref>). In this area, called Inductive Logic Programming (ILP) the aim is to construct a FOL-program that, together with the domain knowledge, has the training set as its logical consequence. If we regard the program as a logical theory, then the training set is a model of this theory.
Reference: 40. <author> Berndt Muller and Joachim Reinhardt. </author> <title> Neural Networks, an introduction. Physics of Neural Networks. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1991. </year>
Reference-contexts: The EURISKO system uses frames for the representation of the acquired knowledge [27]. 4. Neural networks Artificial neural networks, also known as connectionist models, are densely interconnected networks of simple computational elements (for an introduction, see <ref> [28, 40] </ref>). In Figure 6.1 such an element, called neuron is shown. The input consists of N values x 0 ; x 1 ; : : : ; x N1 and a single output y, all having continuous values in a particular domain, e.g. [0; 1].
Reference: 41. <author> Nils J. Nilsson. </author> <booktitle> Principles of Artificial Intelligence. Symbolic Computation. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1982. </year>
Reference-contexts: Hence, we need a more efficient search algorithm. Most data mine systems choose an initial description, and iteratively modify it, thereby improving its quality. These modifications are operations on the description. The set of descriptions, together with these operations, and the quality function, is called the search space (see <ref> [41] </ref>), as we will discuss in the first section. The second section is devoted to search strategies: the choice of an initial description, and the order in which different alternatives are explored, e.g. back-tracking or irrevocable search. Often, a description can be modified using multiple operations. <p> The way in which these sequences are checked is called the search strategy. Basically, there are two kinds of search strategies, as discussed by Nilsson in <ref> [41] </ref>: Irrevocable search In an irrevocable search strategy, an operation is selected and applied irrevocably without provision for reconsideration later. Only a single sequence of operations, i.e. a single path in the graph is evaluated.
Reference: 42. <author> Kamran Parsaye and Mark Chignell. </author> <title> Intelligent databases: tools & applications, chapter 4. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: A credit card company has used a data mine tool to infer new, better, acceptance rules based on the credit-history of current clients [9]. IBM has used data mining techniques to predict defects during the assembly of disk drives [4]. Other applications can be found in <ref> [42] </ref>. 1. Overview of the report This report outlines data mining: it describes which forms of information can be derived, how regularities and rules can be discovered, and summarizes the major obstacles and techniques to overcome these. It also gives a survey of recent work in data mining.
Reference: 43. <author> Gregory Piatetsky-Shapiro. </author> <title> Discovery, analysis, and presentation of strong rules. </title> <booktitle> In Piatetsky-Shapiro and Frawley [44], </booktitle> <pages> pages 229 - 248. </pages>
Reference-contexts: The description is both a necessary and sufficient condition. The correctness-criterion f c is assigned a value 1 if the description is correct, the value for any incorrect rules is smaller than 1. In <ref> [43] </ref>, Piatetsky-Shapiro proposes principles for the construction of a function which assigns a numerical value to any description in D, indicating its correctness. <p> Basically, there are two approaches towards unknown variables: we can either restrict ourselves to deterministic rules, that is, we only construct rules when all variables relevant to the classification are known. This is the discovery of strong rules, as discussed in <ref> [43] </ref>. A disadvantage is that much valuable information that is hidden in the database cannot be found. Alternatively, we can look for rules that do not necessarily classify examples correctly, but merely indicate the probability that an object, covered by the description, belongs to a class.
Reference: 44. <editor> Gregory Piatetsky-Shapiro and William J. Frawley, editors. </editor> <title> Knowledge Discovery in Databases. </title> <publisher> AAAI Press, </publisher> <address> Menlo Park, California, </address> <year> 1991. </year>
Reference: 45. <author> J. Ross Quinlan. </author> <title> Comparing connectionist and symbolic learning methods. </title>
Reference-contexts: However, there are some disadvantages in using neural nets for data mining. First of all, learning processes in neural nets are very slow, compared to symbolic learning systems. In benchmarks described by Quinlan in <ref> [45] </ref>, the ID3 system outperforms back propagation neural nets by a factor 500 to 100; 000. Another disadvantage is that knowledge, generated by neural nets, is not explicitly represented in the form of rules or conceptual patterns, but implicitly in the network itself, as a vast number of weights.
Reference: 46. <author> J. Ross Quinlan. </author> <title> Learning efficient classification procedures and their application to chess end games. </title> <editor> In Michalski et al. </editor> <volume> [30], </volume> <pages> pages 463 - 482. </pages>
Reference-contexts: All systems that we discuss are designed for supervised learning. 1. ID3 The system that had the greatest impact on machine learning research in the last years is ID3, developed in the first half of the eighties by Quinlan (see <ref> [46, 48] </ref>). ID3 stands for Induction of Decision Trees, and is a supervised learning system that constructs decision trees from a set of examples. These examples are tuples, where the domain of each attribute in these tuples is limited to a small number of values, either symbolic or numerical.
Reference: 47. <author> J. Ross Quinlan. </author> <title> The effect of noise on concept learning. </title> <editor> In Michalski et al. </editor> <volume> [31], </volume> <pages> pages 149 - 166. </pages>
Reference-contexts: In experiments with some systems, adding noise to the data resulted in graceful degradation: adding even substantial noise resulted in low levels of misclassification of unseen examples. An interesting phenomenon, discussed in <ref> [47, 48] </ref>, is that rules, learned from a corrupted training set, perform superior on classifying noisy data when compared to rules that are learned on the same, but noise free training set. <p> An interesting phenomenon, discussed in [47, 48], is that rules, learned from a corrupted training set, perform superior on classifying noisy data when compared to rules that are learned on the same, but noise free training set. In <ref> [47] </ref>, Quinlan concludes that "it is not worthwhile expending effort to eliminate noise from the attribute values of objects in the training set if there is going to be a significant amount of noise when the induced classification rule is used in practice". 2.2 Missing attribute values Another problem that also <p> is reflected in the split of examples by that attribute, and so in the corresponding split information and gain.) The process continues until just two value groups remain, or until the gain cannot be further improved by merging. 1.5 Noise Noise can affect both attribute values and class information (see <ref> [47] </ref>). A corrupted set of examples causes two problems: 1. It is no longer possible to generate a tree that classifies all examples correctly, i.e. the consistency condition is violated.
Reference: 48. <author> J. Ross Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1:81 - 106, </volume> <year> 1986. </year>
Reference-contexts: Therefore, it must be able to decide that a particular example or an attribute in this example is corrupted, and thus should be ignored. In <ref> [48] </ref>, Quinlan uses statistical techniques to decide if an attribute should be added to a classification tree. The basic idea is that a small amount of exceptional data is considered to be caused by noise, and can therefore be neglected. <p> In experiments with some systems, adding noise to the data resulted in graceful degradation: adding even substantial noise resulted in low levels of misclassification of unseen examples. An interesting phenomenon, discussed in <ref> [47, 48] </ref>, is that rules, learned from a corrupted training set, perform superior on classifying noisy data when compared to rules that are learned on the same, but noise free training set. <p> Constructing descriptions Examples with missing attributes can be simply discarded, or an attempt can be made to replace the missing value with some `most likely' value. In <ref> [48] </ref>, Quinlan suggests to construct rules that predict the value of a missing attribute, based on the value of other attributes in the example, and the class information. These rules are then used to `fill in' the missing attribute values, the resulting set is used to construct the descriptions. <p> An object is classified by following a path down the tree, by taking the edges, corresponding to the values of the attributes in the object. Example 1 An example, taken from <ref> [48] </ref>, consists of a small training set, storing objects that describe the weather at a particular moment. Objects contain information on the outlook, which is either sunny, overcast or rain, the humidity, which is either high or normal, and some other properties. <p> All systems that we discuss are designed for supervised learning. 1. ID3 The system that had the greatest impact on machine learning research in the last years is ID3, developed in the first half of the eighties by Quinlan (see <ref> [46, 48] </ref>). ID3 stands for Induction of Decision Trees, and is a supervised learning system that constructs decision trees from a set of examples. These examples are tuples, where the domain of each attribute in these tuples is limited to a small number of values, either symbolic or numerical. <p> However, a known problem with this criterion is that it tends to favor attributes with many values. Different solutions to this problem are discussed in <ref> [48] </ref>. 1. ID3 51 1.3 Numerical attributes A condition on an attribute, i.e. an internal node of the decision tree, is a test on the value of an attribute, with branches for all possible values.
Reference: 49. <author> J. Ross Quinlan. </author> <title> Generating production rules from decision trees. </title> <booktitle> In Proceedings of the 10th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 304 - 307, </pages> <address> Milan, </address> <year> 1987. </year>
Reference-contexts: The following tree could be constructed: 1.2 Production rules A disadvantage of decision trees is that they tend to grow very large for realistic applications, and are thus difficult to interpret by humans. Hence, there has been some research in transforming decision trees into other representations. In <ref> [49] </ref>, Quinlan describes a technique to generate propositional-like production rules from decision trees. <p> The classification accuracy is high. However, the system does not make any use of domain knowledge. Furthermore, trees are not easy to understand, however, they can be transformed to decision rules, as described in <ref> [49] </ref>. Michalski's AQ15 system (see [32, 33]) is an inductive learning system that generates decision rules, where the conditional part is a logical formulae.
Reference: 50. <author> J. Ross Quinlan. </author> <title> An emperical comparision of genetic and decision-tree classifiers. </title> <booktitle> In Proceedings of the 5th International Conference on Machine Learning, </booktitle> <pages> pages 135 - 141, </pages> <address> Ann Arbor, </address> <year> 1988. </year>
Reference-contexts: Moreover, in a comparison of a genetic and a decision tree classifier, Quinlan concludes that an incremental genetic classifier Boole needed a much larger training set to achieve a similar accuracy as decision trees <ref> [50] </ref>. Hence, GAs are suited for learning tasks on small databases, where no domain knowledge, such as heuristics, is available. It may be possible to use GA in combination with traditional approaches, where the GA is used for a rough exploration of the search space, and traditional 4.
Reference: 51. <author> J. Ross Quinlan. </author> <title> Determining literals in inductive logic programming. </title> <booktitle> In Proceedings of the 12th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 746 - 750, </pages> <address> 78 References Sydney, Austalia, </address> <year> 1991. </year>
Reference-contexts: If we regard the program as a logical theory, then the training set is a model of this theory. An example of an ILP system is Quinlan's FOIL system (see <ref> [51] </ref>). There are some extensions to FOL that may prove to be very useful for representing knowledge in a data mine system. Constraint logic programming languages (see Jaffar and Lassez in [21]) allow complex constraints between numerical variables.
Reference: 52. <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Although this is convenient for symbolic attributes, it would be useful if one could test on ranges of numerical attributes. An extension of the ID3 algorithm, called C4.5 (see <ref> [52] </ref>) allows tests on the inequality of numerical attributes, such as A i N and A i &gt; N, with two possible outcomes (branches). The information gain of such a test is computed as follows: the examples are first sorted on the values of the attribute being considered.
Reference: 53. <author> R. Bharat Rao and Stephen C-Y. Lu. </author> <title> A knowledge-based equation discovery system for engineering domains. </title> <journal> IEEE Expert, </journal> <note> August 1993:37 - 42, </note> <year> 1993. </year>
Reference-contexts: Even if one is able to construct a single homogeneous function that accurately models the data, this function can be overly complicated and incomprehensible for humans. The KEDS system (Knowledge based Equation Discovery System, see <ref> [53] </ref>) uses a divide-and-conquer strategy, where it breaks the problem space into smaller regions, and constructs functions for each of these regions. <p> The refined equation is again passed to the partitioning phase, to find a refined region, and this process can be repeated until the region-equation pair stops improving (either in accuracy or in covering more data points). Example 2 The following example, taken from <ref> [53] </ref>, uses the training set depicted in Figure 8.1. The horizontal axis is multidimensional, representing the p-dimensional space x of predicting variables. In the initial discovery phase, the linear template y =?ax+?b is chosen.
Reference: 54. <editor> Gordon A. Ringland and David A. Duce, editors. </editor> <title> Approaches to Knowledge Representation: An Introduction. </title> <publisher> Research studies press Ltd., </publisher> <address> Letchworth, England, </address> <year> 1988. </year>
Reference-contexts: We shortly outline the advantages of FOL, and discuss some representations with equivalent expressive power that offer a better structuring of the knowledge, such as semantic networks and frames. For a more extensive discussion of these representations, the reader is referred to <ref> [8, 11, 54] </ref>. The last part discusses neural networks, a non-symbolic representation, together with a highly parallel learning algorithm. We compare this representation with symbolic representations. 1. Propositional-like representations Propositional representations use a logic formulae, consisting of attribute value conditions.
Reference: 55. <author> Ronald L. Rivest. </author> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2:229 - 246, </volume> <year> 1987. </year>
Reference-contexts: The above tree is equivalent to the following set of production rules: if outlook = sunny and humidity = high then class = N if outlook = rain and windy = true then class = N default class = P 1.3 Decision lists Another propositional-like representation, proposed in <ref> [55] </ref>, is the decision list representation. This representation strictly generalizes both decision trees, DNF and CNF representations, i.e. any knowledge structure in these representations can be transformed to a decision list. <p> The reason is that it may be easier for the 3. Structured representations 45 learning algorithm to produce a nearly correct answer from a rich set of alternatives than from a small set of possibilities, as discussed in Section 2 in Chapter 3, and in <ref> [55] </ref>. On the other hand, when using a more expressive formalism, more different descriptions can be constructed, i.e. the description space extends, and finding the best description becomes harder. A solution is to search for particular descriptions only.
Reference: 56. <author> Claude Sammut and Ranan B. Banerji. </author> <title> Learning concepts by asking questions. </title> <editor> In Michalski et al. </editor> <volume> [31], </volume> <pages> pages 167 - 191. </pages>
Reference-contexts: As a solution, the system could interact with its environment, that is, the environment acts as an oracle: the system generates an interesting example, supplies it to the environment, which determines the corresponding class. An example of such a system is MARVIN <ref> [56] </ref>. Unfortunately, a data mine system is not able to manipulate its environment, since it uses an already existing database.
Reference: 57. <author> Sabrina Sestito and Tharam Dillon. </author> <title> Using single layered neural networks for the extraction of conjunctive rules and hierarchical classifications. </title> <journal> Journal of Applied Intelligence, </journal> <volume> 1:157 - 173, </volume> <year> 1991. </year>
Reference-contexts: One of the objects of data mining is to generate knowledge in a form suitable for verification or interpretation by humans. There has been some research on transforming this knowledge to a format better suited for human reading, as in <ref> [23, 57] </ref>, but this mainly concerns single layer networks, that model simple, linear functions. As with genetic algorithms, it is difficult to incorporate any domain knowledge or user interaction in the learning process.
Reference: 58. <author> David C. Sills. </author> <title> William of Ockham. </title> <booktitle> In International Encyclopedia of the Social Sciences, </booktitle> <pages> pages 269 - 270. </pages> <publisher> Macmillan Company & The Free Press, </publisher> <address> New York, </address> <year> 1968. </year>
Reference-contexts: Therefore, we need to estimate the validity of a model: if we can construct multiple models, some of these will be simpler than others. We expect that simpler models are more likely to be correct. The rationale behind this rule- also known as Ockham's razor <ref> [58] </ref> is that if there are multiple explanations for a particular phenomenon, it makes sense to choose the simplest, because it is more likely to capture the nature of the phenomenon. 15 Chapter 3 Data mining The learning processes described in the previous chapter can also be performed by computers.
Reference: 59. <author> William M. Spears and Kenneth De Jong. </author> <title> Using genetic algorithms for supervised concept learning. </title> <booktitle> In Proceedings of tools for AI, </booktitle> <year> 1990. </year>
Reference-contexts: There are two approaches for learning classification rules, named after their university. These approaches differ in the representation of the rules, the crossover operations and the quality function. An overview of learning classification rules can be found in <ref> [22, 59] </ref>: The Michigan approach developed by Holland. An organism represents a set-description. Consequently, all organisms are of the same length. The entire population together forms a classification rule, that is, a rule whose description consists of the disjunction of all descriptions, represented by the organisms. <p> For adaptive systems systems that adjust their rules over time we can use a sliding window, i.e. compute the actual probability of a rule over the last n classifications (as used in <ref> [59] </ref>). 2. Data corruption So far, the information supplied in the set of examples has been assumed to be entirely correct. Sadly, learning systems using real-world data are unlikely to find this assumption to be tenable.
Reference: 60. <author> Jeffrey D. Ullman. </author> <title> Principles of database and knowledge-base systems, </title> <booktitle> volume 2, volume 14 of Principles of Computer Science. </booktitle> <publisher> Computer Science Press, </publisher> <year> 1989. </year>
Reference-contexts: Extending the deductive expressivity of query languages while remaining computationally tractable is pursued in the research area called deductive databases (see Ullman <ref> [60] </ref>). Induction is a technique to infer information that is generalized from the information in the database. For example, from the employee-department and the department-manager tables from the example above, it might be inferred that each employee has a manager. <p> Of course, values in this table may be Nill or unknown. This assumption may seem as severe as the above no relationships assumption. However, it is well-known that the universal relation assumption can be applied to all database schemata and queries (see <ref> [60] </ref>). In fact, the universal relation assumption has been defined as a less cumbersome user interface, and that is exactly the use we will make of it. Definition 1.
Reference: 61. <author> Paul E. Utgoff. </author> <title> Incremental induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 4:161 - 186, </volume> <year> 1989. </year>
Reference-contexts: For such serial learning tasks, one would prefer an incremental algorithm, on the assumption that it is more efficient to revise an existing tree than it is to generate a new tree every time. An adaptation of the ID3 algorithm, called ID5R (see <ref> [61] </ref>), does not create a new tree for each new example, but instead restructures the tree to make it consistent with the current and all previous examples. These previous examples are retained, but not reprocessed, they are used for restructuring purposes only.
Reference: 62. <author> Les G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27:1134 - 1142, </volume> <year> 1984. </year>
Reference-contexts: However, we would like to have some certainty about the correctness of our description. More precisely, we want to minimise the chance that our rule `if D then C' mis-classifies an (unseen) example. This notion of certainty is formalised through the notion of Probably Approximately Correct learning or PAC-learning <ref> [62] </ref>. Before we formulate this notion, we first recall some basic concepts from probability theory.
Reference: 63. <author> Michael G. Walker. </author> <title> How feasible is automated discovery. </title> <journal> IEEE Expert, </journal> <note> Spring 1987:69 - 82, </note> <year> 1987. </year>
Reference-contexts: Techniques to deal with noise are primitive, application of statistical techniques, such as the O 2 test, could improve results. There are no techniques to deal with missing attribute values. 5. Meta-Dendral The Meta-Dendral system is a special purpose machine learning system (see <ref> [14, 63] </ref>), designed for the automated discovery of rules of mass spectroscopy. We discuss this system, because it applies machine learning techniques to an entirely different data representation. Mass spectroscopy is a technique to analyze the three dimensional structure of a molecule. <p> There are no explicit techniques to handle noise and exceptional data in the rule construction algorithm, but post optimization is used to remove those rules that are due to noisy data. 6. RADIX/RX The RX system is used for the discovery of relationships in a clinical database (see <ref> [5, 63, 64] </ref>). A very important difference with other systems is that the notion of time is included: the data objects in the set of examples store information on patients at different times and the generated knowledge consists of causal relationships. <p> In this chapter we review two numerical learning systems, i.e. systems where both the predicting variables and the predicted variable are numerical. 1. Bacon The Bacon system uses a data analysis algorithm to discover mathematical relationships in numerical data <ref> [26, 63] </ref>. It rediscover relationships such as Ohm's law for electric circuits and Archimedes' law of displacement. The training set consists of numerical data, possibly generated at some previous experiment.
Reference: 64. <author> Gio C.M. Wiederhold, Michael G. Walker, Robert L. Blum, and Stephen M. Downs. </author> <title> Acquisition of knowledge from data. </title> <booktitle> In ACM SIGART International Symposium on Methodologies for Intelligent Systems, </booktitle> <pages> pages 74 - 84, </pages> <address> Knoxville, Tennessee, </address> <year> 1986. </year>
Reference-contexts: There are no explicit techniques to handle noise and exceptional data in the rule construction algorithm, but post optimization is used to remove those rules that are due to noisy data. 6. RADIX/RX The RX system is used for the discovery of relationships in a clinical database (see <ref> [5, 63, 64] </ref>). A very important difference with other systems is that the notion of time is included: the data objects in the set of examples store information on patients at different times and the generated knowledge consists of causal relationships.
Reference: 65. <author> Jan M. Zytkow. </author> <title> Combining many searches in the FAHRENHEIT discovery system. </title> <booktitle> In Proceedings of the fourth international workshop on machine learning, </booktitle> <pages> pages 281 - 287, </pages> <address> San Mateo, California, 1987. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Hence, the performance of the system is very bad, even for small data sets. A solution may be to use more sophisticated heuristics. 2. KEDS The Bacon system, and related systems such as Fahrenheit <ref> [65] </ref> and Fortyniner [66], construct a global function, i.e. a relationship that holds for all examples. This can only be successful when relationships in the data are actually homogeneous, that is, when the same relationship among variables holds for the entire training set.
Reference: 66. <author> Jan M. Zytkow and John Baker. </author> <title> Interactive mining for regularities in databases. </title> <booktitle> In Piatetsky-Shapiro and Frawley [44], </booktitle> <pages> pages 31 - 53. </pages>
Reference-contexts: Hence, the performance of the system is very bad, even for small data sets. A solution may be to use more sophisticated heuristics. 2. KEDS The Bacon system, and related systems such as Fahrenheit [65] and Fortyniner <ref> [66] </ref>, construct a global function, i.e. a relationship that holds for all examples. This can only be successful when relationships in the data are actually homogeneous, that is, when the same relationship among variables holds for the entire training set.
References-found: 66

