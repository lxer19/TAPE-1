URL: http://www.cs.bu.edu/techreports/96-005-mermera-model-system.ps.Z
Refering-URL: http://cs-www.bu.edu/techreports/Home.html
Root-URL: 
Title: Distributed Parallel Computing in Mermera: Mixing Noncoherent Shared Memories  
Author: Abdelsalam Heddaya Himanshu Sinha 
Keyword: Distributed parallel computing, noncoherent shared memory, asynchronous itera tive algorithms, network of workstations.  
Address: Waltham, MA  
Affiliation: Computer Science Dept. Boston University  GTE Laboratories, Inc.  
Pubnum: BU-CS-96-005  
Email: heddaya@cs.bu.edu  hsinha@gte.com  
Date: March 7, 1996 (Revised April 19, 1996) (Revised October 3, 1996)  
Abstract: Programmers of parallel processes that communicate through shared globally distributed data structures (DDS) face a difficult choice. Either they must explicitly program DDS management, by partitioning or replicating it over multiple distributed memory modules, or be content with a high latency coherent (sequentially consistent) memory abstraction that hides the DDS' distribution. We present Mermera, a new formalism and system that enable a smooth spectrum of noncoherent shared memory behaviors to coexist between the above two extremes. Our approach allows us to define known noncoherent memories in a new simple way, to identify new memory behaviors, and to characterize generic mixed-behavior computations. The latter are useful for programming using multiple behaviors that complement each others' advantages. On the practical side, we show that the large class of programs that use asynchronous iterative methods (AIM) can run correctly on slow memory, one of the weakest, and hence most efficient and fault-tolerant, noncoherence conditions. An example AIM program to solve linear equations, is developed to illustrate: (1) the need for concurrently mixing memory behaviors, and, (2) the performance gains attainable via noncoherence. Other program classes tolerate weak memory consistency by synchronizing in such a way as to yield executions indistinguishable from coherent ones. AIM computations on noncoherent memory yield noncoherent, yet correct, computations. We report performance data that exemplifies the potential benefits of noncoherence, in terms of raw memory performance, as well as application speed. fl This research was supported in part by NSF under grants IRI-9041581 and CDA-8920936. y On sabbatical leave at Harvard University's Aiken Computation Laboratory, and Department of Biological Chem istry and Molecular Pharmacology. z This document's URL is hhttp://www.cs.bu.edu/techreports/96-005-mermera-model-system.ps.Zi. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak ordering|a new definition. </title> <booktitle> In Proc. 17th Annual International Symposium on Computer Architecture, </booktitle> <month> May 28-31 </month> <year> 1990. </year>
Reference-contexts: In response, several proposals have been made to improve the performance of shared memory systems by relaxing the commonly used correctness condition of sequential consistency. Most systems (see for examples <ref> [1, 15, 16, 17] </ref>) require that all processes agree on a global sequencing of all write operations to the same location, even though they may vary in their perception of the interleaving order of write operations to different locations.
Reference: [2] <author> D. Agrawal, M. Choy, H.V. Leong, and A.K. Singh. </author> <title> Mixed consistency: a model for parallel programming. </title> <booktitle> In Proc. 13th ACM Symp. on Principles of Distributed Computing, </booktitle> <address> Los Angeles, California, </address> <pages> pages 101-110, </pages> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: In [21, 29] we present a formal description of mixed behaviors which permits executions where operations can correspond to slow, PRAM, causal and coherent memories. Attiya et al. [6] give a formalism in which one noncoherent behavior is combined with one noncoherent behavior. Other mixed-coherence behaviors are described in <ref> [2, 30] </ref>. In this paper, we present Mermera 1 , a formal model and system for specifying, reasoning about, and programming shared memory systems. Our contributions can be summarized as follows. 1 The Latin root for memory, mer derives from the the Sanskrit root smar. <p> e ! C e 0 iff e ; e 0 _ e ! r=e e 0 : This definition for causal memory histories, agrees with the original informal proposal by Hutto and Ahamad [23] in 1990, and with its later formalization by us [20] in 1992, and subsequently by others <ref> [3, 2] </ref>. 2 In the context of shared memory, coherence implies causality, but not vice-versa. The following theorem establishes the former statement, while Figure 1 provides an example that ensures that the set of causal histories strictly includes that of coherent histories. <p> The notion of mixed coherence levels turns out to be necessary for the correct exploitation of noncoherence for performance, as illustrated by the programming example in Section 5. 3 In the model of mixed consistency proposed in <ref> [2] </ref>, Agrawal et al. choose a dual scheme, where reads are labeled freely, but writes are not. <p> Singh [28] defines causal and PRAM behaviors separately, using a refined notion of commutativ 20 ity [31]. He develops conditions under which such executions are sequentially consistent. Agrawal et al. <ref> [2] </ref> give a formal definition for a mixture of causal and PRAM memories, in which they label reads, but not writes. They define a PRAM history in terms of the causality of subhistories, as we do. <p> The three examples offered in <ref> [2] </ref> include a linear equation solver that differs from our version, by being both synchronous and centralized, which is unnecessarily restrictive.
Reference: [3] <author> M. Ahamad, G. Neiger, J.E. Burns, P. Kohli, and P.W. Hutto. </author> <title> Causal memory: definitions, implementation, </title> <journal> and programming. Distributed Computing, </journal> <volume> 9(1) </volume> <pages> 37-49, </pages> <year> 1995. </year>
Reference-contexts: These systems typically require the programmer to identify|via program annotations|the units of execution in terms of whose boundaries memory consistency is defined, and to describe the pattern of memory references made by the program. Other proposals like causal memory <ref> [3] </ref>, Pipelined Random Access Memory (PRAM) [26], slow memory [23] and hybrid consistency [5] admit executions in which there may not be a single, globally recognized, total order on writes to a single location. <p> This made it hard to compare different behaviors and to reason about the correctness of programs that run on them. In [22] we present a formalism based on partial orders on the events of an execution to describe these memories. Other formalisms can also be found in the literature <ref> [3, 6, 27] </ref>. These formalisms have been used to prove the correctness of various program classes on different noncoherent memories. Most approaches establish the correctness of executions of certain programs, by showing that each such program induces only sequentially consistent computations on the shared memory system in question. <p> By giving these mixed behavior systems an unambiguous description we make it possible to reason formally about them. * The formalism is used to prove that asynchronous iterations converge when they execute on slow memory. Other authors <ref> [3, 6] </ref> have shown that certain programs (e.g. data race free programs) result in sequentially consistent executions even with some types of noncoherent memory. <p> e ! C e 0 iff e ; e 0 _ e ! r=e e 0 : This definition for causal memory histories, agrees with the original informal proposal by Hutto and Ahamad [23] in 1990, and with its later formalization by us [20] in 1992, and subsequently by others <ref> [3, 2] </ref>. 2 In the context of shared memory, coherence implies causality, but not vice-versa. The following theorem establishes the former statement, while Figure 1 provides an example that ensures that the set of causal histories strictly includes that of coherent histories. <p> One, if the program's intrinsic synchronization, coupled with the appropriate noncoherent memory condition, yields executions that are coherent. Examples of such program classes include data-race free programs <ref> [3, 6] </ref>, which run as coherent on causal memory. Two, a program that can detect inconsistency, can run well on a very weak memory, until such inconsistency arises, then switch to coherent memory behavior to eliminate it. <p> A complete proof that every data-race free program executes on causal in a manner that yields a sequentially consistent behavior later appeared in <ref> [3] </ref>. This discussion illustrates that different problems are amenable to efficient solutions on different kinds of memory. For this reason we propose that programmers be given a choice of behaviors. We expect programmers to first program using coherent behavior because of their familiarity with it. <p> These behaviors are also easy to implement with the tools currently available, as seen in Section 4.3 below. Causal behavior is omitted because existing algorithms for its implementation <ref> [3] </ref> require the significant overhead of the transmission of vector timestamps. 4.2 Programming Interface of Mermera In this section we describe the programming interface presented by the Mermera system. Mermera mixes the behaviors of coherent memory, pipelined RAM, slow memory and locally consistent memory. <p> If the buffer is large then the frequency of messages is low but each process uses less recent values of the components of x being computed by other processes. 7 Related Work and Discussion Ahamad et al. <ref> [3] </ref> present a formal model for describing causal memory, but they do not extend it to other noncoherent behaviors, nor to the mixing of different coherence levels. Their proofs of correctness of data-race-free and concurrent-write free programs on causal memory are applicable to our model.
Reference: [4] <author> Mustaque Ahamad, Philip W. Hutto, and Ranjit John. </author> <title> Implementing and programming causal distributed shared memory. </title> <type> Technical Report GIT-CC-90-49, </type> <institution> Georgia Institute of Technology, College of Computing, </institution> <year> 1990. </year>
Reference-contexts: The use of causal memory to solve the traveling salesman problem, the dictionary problem and to find the solution of a system of linear equations is described in <ref> [4] </ref>. A complete proof that every data-race free program executes on causal in a manner that yields a sequentially consistent behavior later appeared in [3]. This discussion illustrates that different problems are amenable to efficient solutions on different kinds of memory.
Reference: [5] <author> H. Attiya and R. Friedman. </author> <title> A correctness condition for high-performance multiprocessors. </title> <type> Technical Report 719, </type> <institution> Technion|Israel Institute of Technology, Department of Computer Science, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: Other proposals like causal memory [3], Pipelined Random Access Memory (PRAM) [26], slow memory [23] and hybrid consistency <ref> [5] </ref> admit executions in which there may not be a single, globally recognized, total order on writes to a single location. In general, as the degree of noncoherence increases, more opportunities for minimizing communication and synchronization overheads become available to the memory system designer. <p> In [23], Hutto and Ahamad show how a memory that handles exactly one operation at a time (i.e., serially) can be simulated by distributed processors using slow memory. They also argued that one cannot achieve mutual exclusion using slow Memory. Attiya and Friedman <ref> [5] </ref> proved that any solution to the mutual exclusion problem using non-coherent memory will either involve a centralized server or will require the participation of all processes whether they want to enter the critical section or not. 11 But we want to focus on classes of programs that can run directly <p> Their proofs of correctness of data-race-free and concurrent-write free programs on causal memory are applicable to our model. Attiya et al. <ref> [5, 6] </ref> formally define a mixed-coherence memory. They allow for strong and weak operations. An execution consisting of strong operations only, or strong reads and weak writes only, or weak reads and strong writes only, is coherent.
Reference: [6] <author> Hagit Attiya, Roy Friedman, Soma Chaudhuri, and Jennifer L. Welch. </author> <title> Shared memory consistency conditions for non-sequential execution: Definitions and programming strategies. </title> <booktitle> In Proc. 5th Annual ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <address> Velen, Germany, </address> <pages> pages 241-250, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: This made it hard to compare different behaviors and to reason about the correctness of programs that run on them. In [22] we present a formalism based on partial orders on the events of an execution to describe these memories. Other formalisms can also be found in the literature <ref> [3, 6, 27] </ref>. These formalisms have been used to prove the correctness of various program classes on different noncoherent memories. Most approaches establish the correctness of executions of certain programs, by showing that each such program induces only sequentially consistent computations on the shared memory system in question. <p> In [21, 29] we present a formal description of mixed behaviors which permits executions where operations can correspond to slow, PRAM, causal and coherent memories. Attiya et al. <ref> [6] </ref> give a formalism in which one noncoherent behavior is combined with one noncoherent behavior. Other mixed-coherence behaviors are described in [2, 30]. In this paper, we present Mermera 1 , a formal model and system for specifying, reasoning about, and programming shared memory systems. <p> By giving these mixed behavior systems an unambiguous description we make it possible to reason formally about them. * The formalism is used to prove that asynchronous iterations converge when they execute on slow memory. Other authors <ref> [3, 6] </ref> have shown that certain programs (e.g. data race free programs) result in sequentially consistent executions even with some types of noncoherent memory. <p> One, if the program's intrinsic synchronization, coupled with the appropriate noncoherent memory condition, yields executions that are coherent. Examples of such program classes include data-race free programs <ref> [3, 6] </ref>, which run as coherent on causal memory. Two, a program that can detect inconsistency, can run well on a very weak memory, until such inconsistency arises, then switch to coherent memory behavior to eliminate it. <p> Their proofs of correctness of data-race-free and concurrent-write free programs on causal memory are applicable to our model. Attiya et al. <ref> [5, 6] </ref> formally define a mixed-coherence memory. They allow for strong and weak operations. An execution consisting of strong operations only, or strong reads and weak writes only, or weak reads and strong writes only, is coherent.
Reference: [7] <author> Gerard M. Baudet. </author> <title> Asynchronous iterative methods for multiprocessors. </title> <journal> J. ACM, </journal> <volume> 25(2) </volume> <pages> 226-244, </pages> <month> April </month> <year> 1978. </year>
Reference-contexts: Subsection 3.2 contains a detailed discussion of programs of all three types. 3.1 Asynchronous Iterative Methods In this section we show that under certain conditions slow memory is sufficient for the correctness of totally asynchronous iterative algorithms (AIM), the importance of which for parallel computing was first recognized by Baudet <ref> [7] </ref>. Bertsekas and Tsitsiklis give a comprehensive discussion of AIM in [9, 10]. Such algorithms find a fixed point a = f (a) of the iteration x f (x), where x is a vector of length n. The i th component of x is denoted by x i . <p> Examples of iterations that converge when the total asynchrony assumption is satisfied include: 1. A linear system of equations, x Ax + b, such that the spectral radius of A, (jAj) &lt; 1. <ref> [7] </ref> 2. Some graph algorithms, e.g., Bellman-Ford all pairs shortest path algorithm [8]. 3. All dynamic programming algorithms.
Reference: [8] <author> Dimitri Bertsekas and Robert Gallager. </author> <title> Data Networks. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <note> second edition, 1992. Ch 3 surveys queueing theory; ch 2 describes ISDN, TCP/IP, etc. </note>
Reference-contexts: Examples of iterations that converge when the total asynchrony assumption is satisfied include: 1. A linear system of equations, x Ax + b, such that the spectral radius of A, (jAj) &lt; 1. [7] 2. Some graph algorithms, e.g., Bellman-Ford all pairs shortest path algorithm <ref> [8] </ref>. 3. All dynamic programming algorithms. A comprehensive list of fixed point problems that converge in a totally asynchronous iteration can be found in [9]. 3.2 Other Program Classes In Section 2.2 we established the lattice shown in Figure 5.
Reference: [9] <author> Dimitri P. Bertsekas and John N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1989. </year>
Reference-contexts: Most approaches establish the correctness of executions of certain programs, by showing that each such program induces only sequentially consistent computations on the shared memory system in question. In [29] we prove that asynchronous iterations <ref> [9] </ref> converge using slow memory. This is the first instance of a proof of correctness of an algorithm on noncoherent memories that results in executions that are not sequentially consistent. Different programs, even different parts of the same program, can tolerate different levels of noncoherence. <p> Bertsekas and Tsitsiklis give a comprehensive discussion of AIM in <ref> [9, 10] </ref>. Such algorithms find a fixed point a = f (a) of the iteration x f (x), where x is a vector of length n. The i th component of x is denoted by x i . <p> Some graph algorithms, e.g., Bellman-Ford all pairs shortest path algorithm [8]. 3. All dynamic programming algorithms. A comprehensive list of fixed point problems that converge in a totally asynchronous iteration can be found in <ref> [9] </ref>. 3.2 Other Program Classes In Section 2.2 we established the lattice shown in Figure 5. In the figure, the subset relationship implies that the set of computations allowed by one type of memory is a proper subset of computations allowed by a memory higher in the lattice. <p> The remainder of this section is devoted to explaining, through an example program, how to implement an iterative linear equation solver to run on Mermera. The example program implements an asynchronous iterative algorithm <ref> [9] </ref> to solve a linear system of equations Ax + b = 0, using noncoherent memory on p processes. A is an n fi n matrix, x and b are vectors of size n. The program shown in Figure 8 is executed by each of the p participating processes.
Reference: [10] <author> Dimitri P. Bertsekas and John N. Tsitsiklis. </author> <title> A survey of some aspects of parallel and distributed iterative algorithms. </title> <type> Technical Report CICS-P-189, </type> <institution> Center for Intelligent Control Systems, </institution> <address> Cambridge, </address> <month> January </month> <year> 1990. </year> <month> 22 </month>
Reference-contexts: Bertsekas and Tsitsiklis give a comprehensive discussion of AIM in <ref> [9, 10] </ref>. Such algorithms find a fixed point a = f (a) of the iteration x f (x), where x is a vector of length n. The i th component of x is denoted by x i . <p> Bertsekas and Tsitsiklis show in <ref> [10] </ref> that if the instance of the problem satisfies certain conditions and the total asynchrony assumption is satisfied then the iteration described above will converge. The total asynchrony assumption states that: 1. The iteration x f (x) is repeated infinitely often, and, 2.
Reference: [11] <author> K. Birman and T.A. Joseph. </author> <title> Exploiting virtual synchrony in distributed systems. </title> <booktitle> In Proc. 11th ACM Symp. on Operating System Principles, </booktitle> <address> Austin, Texas, </address> <pages> pages 123-138, </pages> <month> Nov. </month> <year> 1987. </year>
Reference-contexts: Local Writes are applied only to the local copy, although their specification permits their propagation. We use version 2.2.5 of the Isis toolkit <ref> [12, 11] </ref> to propagate the values written to memory, because Isis gives us a suite of group multicast primitives that satisfy relevant ordering properties. The broadcasts of interest to us are abcast, fbcast and mbcast.
Reference: [12] <author> Kenneth Birman, Andre Schiper, and Pat Stephenson. </author> <title> Lightweight causal and atomic group multicast. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 9(3) </volume> <pages> 272-314, </pages> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: Interestingly, the notion of potential causality first formalized by Lamport [24], to describe the partial ordering induced by point-to-point messages is akin to coherent memory, not causal memory. Shared memory systems in general correspond to multicasting message passing systems, such as Isis <ref> [12] </ref>, since values written to a single memory location may be read by multiple processes, that can then disagree over the order of values written by causally unrelated writes. <p> Local Writes are applied only to the local copy, although their specification permits their propagation. We use version 2.2.5 of the Isis toolkit <ref> [12, 11] </ref> to propagate the values written to memory, because Isis gives us a suite of group multicast primitives that satisfy relevant ordering properties. The broadcasts of interest to us are abcast, fbcast and mbcast.
Reference: [13] <author> John B. Carter, John K. Bennett, and Willy Zwaenopoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proc. 13th ACM Symp. on Operating System Principles, Asilomar, California, </booktitle> <pages> pages 152-164, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: The modifications in question range from identifying synchronization accesses to the system, as required by some relaxed coherence systems such as release consistency <ref> [13] </ref>, and giving hints on the memory reference pattern. The Mermera programmer, by contrast, checks to see if if the critical program fragments employ|or can be replaced with|algorithms that tolerate noncoherence.
Reference: [14] <author> David Chaiken, John Kubiatowics, and Anant Agarwal. </author> <title> LimitLESS directories: a scalable cache coherence scheme. </title> <booktitle> In Proc. 4th ACM Intl. Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa Clara, California, </address> <pages> pages 224-234, </pages> <month> Apr. </month> <year> 1991. </year> <note> Describes the Alewife machine. </note>
Reference-contexts: With respect to our performance data, one may argue that this comparison of the completion time of coherent writes and noncoherent writes is unfair because few coherent memories are implemented using full replication. More efficient implementations such as directory based schemes (see <ref> [14] </ref> for an example) exist. Our response to this argument is that there are some applications (e.g., the linear solver of Section 5) which intrinsically generate a message traffic that will be similar to the traffic generated by using full replication to implement shared memory.
Reference: [15] <author> Michel Dubois. </author> <title> Delayed consistency protocols. </title> <type> Technical Report CENG 90-21, </type> <institution> Electrical Engineering-Systems Department, University of Southern California, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: In response, several proposals have been made to improve the performance of shared memory systems by relaxing the commonly used correctness condition of sequential consistency. Most systems (see for examples <ref> [1, 15, 16, 17] </ref>) require that all processes agree on a global sequencing of all write operations to the same location, even though they may vary in their perception of the interleaving order of write operations to different locations.
Reference: [16] <author> Kourosh Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proc. 17th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: In response, several proposals have been made to improve the performance of shared memory systems by relaxing the commonly used correctness condition of sequential consistency. Most systems (see for examples <ref> [1, 15, 16, 17] </ref>) require that all processes agree on a global sequencing of all write operations to the same location, even though they may vary in their perception of the interleaving order of write operations to different locations.
Reference: [17] <author> James R. Goodman. </author> <title> Cache consistency and sequential consistency. </title> <type> Technical Report 1006, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: In response, several proposals have been made to improve the performance of shared memory systems by relaxing the commonly used correctness condition of sequential consistency. Most systems (see for examples <ref> [1, 15, 16, 17] </ref>) require that all processes agree on a global sequencing of all write operations to the same location, even though they may vary in their perception of the interleaving order of write operations to different locations.
Reference: [18] <author> A. Heddaya and K. Park. </author> <title> Mapping parallel iterative algorithms onto workstation networks. </title> <booktitle> In Proc. 3rd IEEE International Symposium on High Performance Distributed Computing, </booktitle> <address> San Francisco, </address> <pages> pages 211-218, </pages> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: In contrast, the latency of coherent operations is dependent on the message latency of the network. Therefore, applications using coherent memory are self-limiting in their message generation rate. The impact of network congestion on the performance of asynchronous iterative methods that use noncoherent memories is studied further in <ref> [18, 19] </ref>. Our measurements of the performance of the equation solver reveal that asynchronous iterative methods can exhibit superlinear speedup when executed on noncoherent memory.
Reference: [19] <author> A. Heddaya, K. Park, and H.S. Sinha. </author> <title> Using warp to control network contention in Mermera. </title> <booktitle> In Proc. 27th Hawaii International Conference on System Sciences, Maui, Hawaii, </booktitle> <pages> pages 96-105, </pages> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: In contrast, the latency of coherent operations is dependent on the message latency of the network. Therefore, applications using coherent memory are self-limiting in their message generation rate. The impact of network congestion on the performance of asynchronous iterative methods that use noncoherent memories is studied further in <ref> [18, 19] </ref>. Our measurements of the performance of the equation solver reveal that asynchronous iterative methods can exhibit superlinear speedup when executed on noncoherent memory.
Reference: [20] <author> A. Heddaya and H.S. Sinha. </author> <title> Coherence, non-coherence and Local Consistency in distributed shared memory for parallel computing. </title> <type> Technical Report BU-CS-92-004, </type> <institution> Boston Univ., Computer Science Dept., </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: Our contributions can be summarized as follows. 1 The Latin root for memory, mer derives from the the Sanskrit root smar. Our coinage, `Mermera', forms the feminine parallel of the ancient Greek name, Mermeros, which means care laden. 1 * Our formal model, based on the one proposed in <ref> [20] </ref> captures several known noncoherent behaviors and is significantly simpler than the original proposal. It also enables us to define new behaviors. * The formalism is extended to describe the behavior of systems in which different kinds of memory behaviors can be mixed in a rational manner. <p> This new style of defining memory coherence, first proposed in <ref> [20] </ref> by us, appears at formal variance with Lamport's original and widely used sequential consistency condition [25], which can be paraphrased as follows. <p> C ) is a partial ordering, where e ! C e 0 iff e ; e 0 _ e ! r=e e 0 : This definition for causal memory histories, agrees with the original informal proposal by Hutto and Ahamad [23] in 1990, and with its later formalization by us <ref> [20] </ref> in 1992, and subsequently by others [3, 2]. 2 In the context of shared memory, coherence implies causality, but not vice-versa. The following theorem establishes the former statement, while Figure 1 provides an example that ensures that the set of causal histories strictly includes that of coherent histories.
Reference: [21] <author> A. Heddaya and H.S. Sinha. </author> <title> Computing with non-coherent shared memory. </title> <type> Technical Report BU-CS-93-007, </type> <institution> Boston Univ., Computer Science Dept., </institution> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: Different programs, even different parts of the same program, can tolerate different levels of noncoherence. Therefore, it behooves a system to provide different behaviors to the programmer who can then mix and match the behaviors according to the needs of the different algorithms used in a program. In <ref> [21, 29] </ref> we present a formal description of mixed behaviors which permits executions where operations can correspond to slow, PRAM, causal and coherent memories. Attiya et al. [6] give a formalism in which one noncoherent behavior is combined with one noncoherent behavior. Other mixed-coherence behaviors are described in [2, 30].
Reference: [22] <author> A. Heddaya and H.S. Sinha. </author> <title> An overview of mermera: a system and formalism for noncoherent distributed parallel memory. </title> <booktitle> In Proc. 26th Hawaii International Conference on System Sciences, Maui, Hawaii, </booktitle> <pages> pages 164-173, </pages> <month> Jan. 5-8 </month> <year> 1993. </year>
Reference-contexts: This paper concentrates on noncoherent memory systems. Early specifications of noncoherent memories were given in terms of descriptions of algorithms that implement them. This made it hard to compare different behaviors and to reason about the correctness of programs that run on them. In <ref> [22] </ref> we present a formalism based on partial orders on the events of an execution to describe these memories. Other formalisms can also be found in the literature [3, 6, 27]. These formalisms have been used to prove the correctness of various program classes on different noncoherent memories.
Reference: [23] <author> P.W. Hutto and M. Ahamad. </author> <title> Slow memory: weakening consistency to enhance concurrency in distributed shared memories. </title> <booktitle> In Proc. 10th IEEE Intl. Conference on Distributed Computing Systems, </booktitle> <address> Paris, France, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: These systems typically require the programmer to identify|via program annotations|the units of execution in terms of whose boundaries memory consistency is defined, and to describe the pattern of memory references made by the program. Other proposals like causal memory [3], Pipelined Random Access Memory (PRAM) [26], slow memory <ref> [23] </ref> and hybrid consistency [5] admit executions in which there may not be a single, globally recognized, total order on writes to a single location. In general, as the degree of noncoherence increases, more opportunities for minimizing communication and synchronization overheads become available to the memory system designer. <p> history H = (E; ;) is causal iff (E; ! C ) is a partial ordering, where e ! C e 0 iff e ; e 0 _ e ! r=e e 0 : This definition for causal memory histories, agrees with the original informal proposal by Hutto and Ahamad <ref> [23] </ref> in 1990, and with its later formalization by us [20] in 1992, and subsequently by others [3, 2]. 2 In the context of shared memory, coherence implies causality, but not vice-versa. <p> For example, Lipton and Sandberg's [26] pipelined random access memory (PRAM) differs from causal memory in requiring that processes agree only on the ordering of write events issued by the same process. Similarly, Hutto and Ahamad's <ref> [23] </ref> slow memory stipulates that processes agree on the order of write events issued by the same process to the same location. Writes to different locations by a process may be observed in different orders by different processes. <p> The program uses weak memory, switching to dynamic atomic only when it detects an inconsistency. Such detection is achieved through redundancy introduced in the B-tree data structure itself. In <ref> [23] </ref>, Hutto and Ahamad show how a memory that handles exactly one operation at a time (i.e., serially) can be simulated by distributed processors using slow memory. They also argued that one cannot achieve mutual exclusion using slow Memory.
Reference: [24] <author> L. Lamport. </author> <title> Time, clocks and the ordering of events in a distributed system. </title> <journal> Comm. ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: In other words, coherent memory requires processes to have compatible perceptions of the ordering of even unrelated writes, rejecting the history shown in on the ordering of such concurrent write events. Interestingly, the notion of potential causality first formalized by Lamport <ref> [24] </ref>, to describe the partial ordering induced by point-to-point messages is akin to coherent memory, not causal memory.
Reference: [25] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> Sep. </month> <year> 1979. </year> <month> 23 </month>
Reference-contexts: This new style of defining memory coherence, first proposed in [20] by us, appears at formal variance with Lamport's original and widely used sequential consistency condition <ref> [25] </ref>, which can be paraphrased as follows. Definition 5 A shared memory history (E; ;) is sequentially consistent (SC) if and only if there exists a total ordering (E; ! Sc ), such that: 1. e ! i e 0 ) e ! Sc e 0 .
Reference: [26] <author> R.J. Lipton and J.S. Sandberg. </author> <title> PRAM: a scalable shared memory. </title> <type> Technical Report CS-TR--180-88, </type> <institution> Princeton Univ., Dept. of Computer Science, </institution> <month> Sep. </month> <year> 1988. </year>
Reference-contexts: The latter option has the disadvantage of high latency for some operations, especially on a network of workstations (NOWs) where the access time for remote memory is several orders of magnitude greater than the access time for local memory. In <ref> [26] </ref>, Lipton and Sandberg showed that the worst case access time for sequentially consistent memory is proportional to the worst case communication delay among the participating processes. <p> These systems typically require the programmer to identify|via program annotations|the units of execution in terms of whose boundaries memory consistency is defined, and to describe the pattern of memory references made by the program. Other proposals like causal memory [3], Pipelined Random Access Memory (PRAM) <ref> [26] </ref>, slow memory [23] and hybrid consistency [5] admit executions in which there may not be a single, globally recognized, total order on writes to a single location. <p> For example, Lipton and Sandberg's <ref> [26] </ref> pipelined random access memory (PRAM) differs from causal memory in requiring that processes agree only on the ordering of write events issued by the same process. <p> We showed in Section 3.1 that slow memory with some liveness guarantee is sufficient for the convergence of certain asynchronous iterative algorithms to find fix points. But one would need a stronger behavior for detecting that the iteration has converged. Lipton and Sandberg show in <ref> [26] </ref> that PRAM can be used to solve a large number of applications like FFT, matrix-vector product, matrix-matrix product, dynamic programming and other computations that are in the large class of oblivious computations 4 . <p> Mermera allows different behaviors for a location in a program. 4 "A computation is oblivious if its data motion and the operations it executes at a given step are independent of the actual values of data." <ref> [26] </ref> 5 Logical synchronization by the program, not by the memory. 6 Programmers could get around this drawback by creating new locations for each part of the programs and copying values from old locations into the new locations.
Reference: [27] <author> A.K. Singh. </author> <title> A framework for programming using non-atomic variables. </title> <booktitle> In Proc. 8th IEEE International Parallel Processing Symposium, </booktitle> <pages> pages 133-140. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: This made it hard to compare different behaviors and to reason about the correctness of programs that run on them. In [22] we present a formalism based on partial orders on the events of an execution to describe these memories. Other formalisms can also be found in the literature <ref> [3, 6, 27] </ref>. These formalisms have been used to prove the correctness of various program classes on different noncoherent memories. Most approaches establish the correctness of executions of certain programs, by showing that each such program induces only sequentially consistent computations on the shared memory system in question.
Reference: [28] <author> A.K. Singh. </author> <title> A framework for programming using non-atomic variables. </title> <booktitle> In Proc. 8th IEEE Intl. Parallel Processing Symp., Cancun, Mexico, </booktitle> <pages> pages 133-140, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: The difference between our mixture of coherent and slow memories and their memory is that our model allows slow operations to be propagated in a "best-effort" manner, i.e., they may not be observed by other processes, whereas their weak operations need reliable message transmission. Singh <ref> [28] </ref> defines causal and PRAM behaviors separately, using a refined notion of commutativ 20 ity [31]. He develops conditions under which such executions are sequentially consistent. Agrawal et al. [2] give a formal definition for a mixture of causal and PRAM memories, in which they label reads, but not writes.
Reference: [29] <author> H.S. Sinha. Mermera: </author> <title> Non-coherent Distributed Shared Memory for Parallel Computing. </title> <type> PhD thesis, </type> <institution> Boston University, </institution> <note> (http://www.cs.bu.edu/techreports), May 1993. </note>
Reference-contexts: These formalisms have been used to prove the correctness of various program classes on different noncoherent memories. Most approaches establish the correctness of executions of certain programs, by showing that each such program induces only sequentially consistent computations on the shared memory system in question. In <ref> [29] </ref> we prove that asynchronous iterations [9] converge using slow memory. This is the first instance of a proof of correctness of an algorithm on noncoherent memories that results in executions that are not sequentially consistent. <p> Different programs, even different parts of the same program, can tolerate different levels of noncoherence. Therefore, it behooves a system to provide different behaviors to the programmer who can then mix and match the behaviors according to the needs of the different algorithms used in a program. In <ref> [21, 29] </ref> we present a formal description of mixed behaviors which permits executions where operations can correspond to slow, PRAM, causal and coherent memories. Attiya et al. [6] give a formalism in which one noncoherent behavior is combined with one noncoherent behavior. Other mixed-coherence behaviors are described in [2, 30]. <p> Measurements Our measurements are summarized in Table 5. The fastest convergence times regardless of the buffer size used are shown. The measurements for each buffer size can be found in <ref> [29] </ref>. The main conclusions we derive from these data are: 1. Using noncoherent behavior instead of coherent behavior alone improves the performance of our asynchronous iterative algorithm by a factor ranging from 5.5 to 11.0. <p> Prototypes of Mermera have been implemented on a network of workstations, a CM-5 and a BBN Butterfly TC2000. The implementation of CO Write on the BBN Butterfly described in <ref> [29] </ref> uses a highly optimized pipelined locking protocol. The raw memory performance on these platforms corroborate the potential for significant performance improvements in some applications. 8 Conclusion In this paper we presented a formal model for describing the behavior of several proposals for noncoherent memories.
Reference: [30] <author> P. Wang and W.E. Weihl. </author> <title> Scalable concurrent B-trees using multi-version memory. </title> <journal> J. Parallel and Distributed Computing, </journal> <volume> 32(1) </volume> <pages> 28-48, </pages> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: In [21, 29] we present a formal description of mixed behaviors which permits executions where operations can correspond to slow, PRAM, causal and coherent memories. Attiya et al. [6] give a formalism in which one noncoherent behavior is combined with one noncoherent behavior. Other mixed-coherence behaviors are described in <ref> [2, 30] </ref>. In this paper, we present Mermera 1 , a formal model and system for specifying, reasoning about, and programming shared memory systems. Our contributions can be summarized as follows. 1 The Latin root for memory, mer derives from the the Sanskrit root smar. <p> Two, a program that can detect inconsistency, can run well on a very weak memory, until such inconsistency arises, then switch to coherent memory behavior to eliminate it. For instance, certain parallel B-tree algorithms are able to detect inconsistencies in the tree structure that result from concurrent modification <ref> [30] </ref>, and hence are amenable to execution on an especially weak multiversion memory. The third type of programs, of which no instance was previously known, covers programs that can function correctly with a noncoherent execution that is not equivalent to a coherent one. <p> Weak memory|whose histories can be defined to obey the truly "weak" condition: 8 r a 2 E : 9 w a 2 E|would be the most efficient to implement. Multi-version memory (MVM), proposed by Wang and Weihl <ref> [30] </ref> to support highly concurrent B-trees, is a memory in which the programmer has access to a variant of weak memory, as well as to dynamic atomic memory.
Reference: [31] <author> William E. Weihl. </author> <title> The impact of recovery on concurrency control. </title> <booktitle> In Proc. 8th ACM Symp. on the Principles of Database Systems, </booktitle> <address> Philadelphia, Pennsylvania, </address> <pages> pages 259-269, </pages> <month> Mar. </month> <year> 1989. </year> <month> 24 </month>
Reference-contexts: Singh [28] defines causal and PRAM behaviors separately, using a refined notion of commutativ 20 ity <ref> [31] </ref>. He develops conditions under which such executions are sequentially consistent. Agrawal et al. [2] give a formal definition for a mixture of causal and PRAM memories, in which they label reads, but not writes. They define a PRAM history in terms of the causality of subhistories, as we do.
References-found: 31

