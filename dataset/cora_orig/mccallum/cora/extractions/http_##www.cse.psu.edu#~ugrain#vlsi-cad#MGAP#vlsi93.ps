URL: http://www.cse.psu.edu/~ugrain/vlsi-cad/MGAP/vlsi93.ps
Refering-URL: http://www.cse.psu.edu/~ugrain/publications.html
Root-URL: 
Email: info.pub.permission@ieee.org.  
Note: Copyright c fl1994 IEEE. All rights reserved. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE. For information on obtain ing permission, send a blank email message to  By choosing to view this document, you agree to all provisions of the copyright laws protecting it.  
Abstract-found: 0
Intro-found: 1
Reference: [Bat74] <author> K. E. Batcher. </author> <title> STARAN Parallel Processor System Hardware. </title> <booktitle> In Proc. AFIPS NCC, </booktitle> <pages> pages 405-410, </pages> <year> 1974. </year>
Reference-contexts: Three massively parallel architectures that have been implemented are the CLIP IV system at the University College of London [DF86], the MPP, contracted by NASA Goddard Space Flight Center to Goodyear Aerospace <ref> [Bat80, Bat74] </ref> and, more recently, the Connection Machine I (CM1) [D.86]. The CLIP IV bit-serial processing element architecture has some remote resemblance to the MUX and RAM based designs, but it was designed specifically for image processing and performs poorly for arithmetic operations.
Reference: [Bat80] <author> K. E. Batcher. </author> <title> Design of a Massively Parallel Processor. </title> <journal> IEEE Trans. on Computers., </journal> <volume> 29(9) </volume> <pages> 836-840, </pages> <month> Sep. </month> <year> 1980. </year>
Reference-contexts: Three massively parallel architectures that have been implemented are the CLIP IV system at the University College of London [DF86], the MPP, contracted by NASA Goddard Space Flight Center to Goodyear Aerospace <ref> [Bat80, Bat74] </ref> and, more recently, the Connection Machine I (CM1) [D.86]. The CLIP IV bit-serial processing element architecture has some remote resemblance to the MUX and RAM based designs, but it was designed specifically for image processing and performs poorly for arithmetic operations.
Reference: [BOI93] <author> R. S. Bajwa, R. M. Owens, and M. J. Irwin. </author> <title> Image Processing on the MGAP: A Cost Effective Solution. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <pages> pages 439-443, </pages> <address> Newport Beach, California, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: In this paper we describe a micro-architecture which has evolved from our earlier designs <ref> [JO91, JO92, BOI93] </ref>. All our architectures are based upon very simple processors which are essentially just memory cells. They are put together using only nearest neighbor interconnect and are highly scalable as a consequence of this interconnect pattern. This nearest neighbour communication is programmable. <p> The subscripts refer to the identity of the processor in the pair. Hence all the algorithms described 3 in <ref> [JO92, MIK + 92, BOI93] </ref> can also be computed as efficiently by this design by putting one of the operands in processor 0 and the other in processor 1. Unlike its predecessor this design is better adapted for problems requiring extensive communications. <p> Preliminary evaluations of the capabilities of the micrograined design indicates that for certain applications like image processing it can provide performance comparable to that of the MPP and the CLIP IV at a fraction of its cost <ref> [BOI93] </ref>. In conclusion, our approach strikes a balance between the very fine-grained approach which amounts to configuring hardware at the gate level and the more coarse-grained approach where each processing element is a microprocessor.
Reference: [CR91] <author> D. C. Chen and J. Rabaey. PADDI: </author> <title> Programmable Arithmetic Devices For Digital Signal Processing. </title> <booktitle> In VLSI Signal Processing IV, </booktitle> <pages> pages 240-249. </pages> <publisher> IEEE Press, </publisher> <year> 1991. </year>
Reference-contexts: It provides reconfigurable hardware at the gate level to create a customized architecture for an application. A more coarse-grained example is the PADDI project <ref> [CR91] </ref> at Berkeley. This project's goal was to design a restricted set of building blocks which can then be used to build high performance algorithmic specific datapaths for signal processing.
Reference: [D.86] <author> Hillis W. D. </author> <title> The Connection Machine. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: Three massively parallel architectures that have been implemented are the CLIP IV system at the University College of London [DF86], the MPP, contracted by NASA Goddard Space Flight Center to Goodyear Aerospace [Bat80, Bat74] and, more recently, the Connection Machine I (CM1) <ref> [D.86] </ref>. The CLIP IV bit-serial processing element architecture has some remote resemblance to the MUX and RAM based designs, but it was designed specifically for image processing and performs poorly for arithmetic operations.
Reference: [DF86] <author> M. J. B. Duff and T. J. Fountain. </author> <title> Cellular Logic Image Processing. </title> <publisher> Academic Press, </publisher> <year> 1986. </year>
Reference-contexts: This project's goal was to design a restricted set of building blocks which can then be used to build high performance algorithmic specific datapaths for signal processing. Three massively parallel architectures that have been implemented are the CLIP IV system at the University College of London <ref> [DF86] </ref>, the MPP, contracted by NASA Goddard Space Flight Center to Goodyear Aerospace [Bat80, Bat74] and, more recently, the Connection Machine I (CM1) [D.86].
Reference: [GK89] <author> J. Gray and T. Kean. </author> <title> Configurable Hardware: A New Paradigm for Computation. </title> <booktitle> In Advanced Research in VLSI, Proceedings of the Decennial Cal-tech Conference on VLSI, </booktitle> <pages> pages 5-16, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: A few PLD's surround this chip to provide the specific timing of handshake signals to the host and between the slave and the rest of the board. 5 6 Summary and conclusions Examples of architectures using the fine-grained approach are: Configurable Array Logic (CAL) project <ref> [KG90, GK89] </ref>. It provides reconfigurable hardware at the gate level to create a customized architecture for an application. A more coarse-grained example is the PADDI project [CR91] at Berkeley.
Reference: [JO87] <author> Irwin M. J. and R. M. Owens. </author> <title> Digit Pipelined Arithmetic as Illustrated by the Paste-Up System. </title> <journal> IEEE Computer, </journal> <volume> 20(4) </volume> <pages> 61-73, </pages> <month> Apr. </month> <year> 1987. </year>
Reference-contexts: They are put together using only nearest neighbor interconnect and are highly scalable as a consequence of this interconnect pattern. This nearest neighbour communication is programmable. In order to achieve high performance for arithmetic operations, all three architectures employ a base 4 maximally redundant number system for arithmetic operations <ref> [JO87] </ref>. Each digit in this system is represented as a three bit, two's complement number. This number system is advantageous in that we can perform addition in constant time. Obviously, any operation requiring addition potentially benefits by an order of magnitude.
Reference: [JO91] <author> Irwin M. J. and R. M. Owens. </author> <title> A Two-Dimensional, Distributed Logic Processor. </title> <journal> IEEE Trans. on Computers., </journal> <volume> 40(10) </volume> <pages> 1094-1101, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: In this paper we describe a micro-architecture which has evolved from our earlier designs <ref> [JO91, JO92, BOI93] </ref>. All our architectures are based upon very simple processors which are essentially just memory cells. They are put together using only nearest neighbor interconnect and are highly scalable as a consequence of this interconnect pattern. This nearest neighbour communication is programmable.
Reference: [JO92] <author> Irwin M. J. and R. M. Owens. </author> <title> A Micro-Grained VLSI Signal Processor. </title> <booktitle> In Proc. of the Intl. Conf. on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 641-644, </pages> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: In this paper we describe a micro-architecture which has evolved from our earlier designs <ref> [JO91, JO92, BOI93] </ref>. All our architectures are based upon very simple processors which are essentially just memory cells. They are put together using only nearest neighbor interconnect and are highly scalable as a consequence of this interconnect pattern. This nearest neighbour communication is programmable. <p> For arithmetic operations the digit processor pairs are configured as a word cell in the manner described in section 1. One difference, in terms of operation, between this design and its predecessors <ref> [JO92, MIK + 92] </ref> is that in this design we keep the operands in different processors of the digit processor pair. Each digit processor's memory has two ports, one (port A) for reading and writing while the other (port B) is used only for reading. <p> The subscripts refer to the identity of the processor in the pair. Hence all the algorithms described 3 in <ref> [JO92, MIK + 92, BOI93] </ref> can also be computed as efficiently by this design by putting one of the operands in processor 0 and the other in processor 1. Unlike its predecessor this design is better adapted for problems requiring extensive communications.
Reference: [JOK + 91] <author> Irwin M. J., R. M. Owens, T. P. Kelliher, K.-K. Leung, and M. Vishwanath. </author> <title> The Arithmetic Cube II: A Second Generation VLSI DSP Processor. </title> <booktitle> In Proc. of the Intl. Conf. on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 1125-1128, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: The ALU is used for computing jump addresses, offset addressing modes and for updating loop counts in nested loops. The register file is used to store loop counts and address offsets. The VMEbus interface logic is very similar to the one succesully used in the AC II <ref> [JOK + 91] </ref>, so we will not have to design the slave from scratch. The heart of the VME slave is an off-the-shelf chip available from Signetics. This chip handles nearly all of the asynchronous aspects of the VME bus.
Reference: [KG90] <author> T. Kean and J. Gray. </author> <title> Configurable Hardware: Two Case Studies of Micro-Grain Computation. </title> <journal> Journal of VLSI Signal Processing, </journal> <volume> 2(1) </volume> <pages> 9-16, </pages> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: A few PLD's surround this chip to provide the specific timing of handshake signals to the host and between the slave and the rest of the board. 5 6 Summary and conclusions Examples of architectures using the fine-grained approach are: Configurable Array Logic (CAL) project <ref> [KG90, GK89] </ref>. It provides reconfigurable hardware at the gate level to create a customized architecture for an application. A more coarse-grained example is the PADDI project [CR91] at Berkeley.
Reference: [Lei89] <author> C. E. Leiserson. </author> <title> VLSI Theory and Parallel Supercomputing. </title> <booktitle> In Advanced Research in VLSI, Proceedings of the Decennial Caltech Conference on VLSI, </booktitle> <pages> pages 5-16, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: Among other network topologies, the hypercube has been shown to be efficient at solving a variety of problems and has been touted as a universal network toplogy, i.e., an n-node hypercube can simulate any bounded degree n-node network in O (lg n) time with a polylogarithmic time overhead <ref> [Lei89] </ref>. But in terms of VLSI implemen tion, a hypercube is not appealing because it needs long wires whereas we want to achieve high processor density.
Reference: [MIK + 92] <author> Owens R. M., M. J. Irwin, T. P. Kelliher, M. Vish-wanath, and R. S. Bajwa. </author> <title> Implementing a Family of High Performance, </title> <booktitle> Micrograined Architectures. In Proc. of the Intl. Conf. on Application Specific Array Processors, </booktitle> <pages> pages 191-205, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: The configuration register indicates which external port to use as input to the processing cell. The RAM-based micro-grained architecture differs from the MUX-based design in that the two function MUXes are replaced by function RAMs <ref> [MIK + 92] </ref>. This enhances this design's capabilities to include MIMD like applications; i.e., different processors can sychronously compute different functions. Alternate configurations of the digit processor pair would employ RAM-based processors or one of each type. <p> For arithmetic operations the digit processor pairs are configured as a word cell in the manner described in section 1. One difference, in terms of operation, between this design and its predecessors <ref> [JO92, MIK + 92] </ref> is that in this design we keep the operands in different processors of the digit processor pair. Each digit processor's memory has two ports, one (port A) for reading and writing while the other (port B) is used only for reading. <p> The subscripts refer to the identity of the processor in the pair. Hence all the algorithms described 3 in <ref> [JO92, MIK + 92, BOI93] </ref> can also be computed as efficiently by this design by putting one of the operands in processor 0 and the other in processor 1. Unlike its predecessor this design is better adapted for problems requiring extensive communications. <p> All communication will occur across the VME bus. An on-board microsequencer and control store is used to provide control of the processors. The control store will be loaded by the host system <ref> [MIK + 92] </ref>. In the rest of this section, we describe the components of the prototyping system. 5.1 The processor array The processor array communicates with the rest of the system via two pathways. Control signals for the array are received from the control store of the controller.
Reference: [PR89] <author> Quinton P. and Y. Roberts. </author> <title> Systolic Algorithms and Architectures. </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: Nearest neighbour mesh interconnect between the processors results in an array with a very tight packing of processors. There already exists a rich body of work on solving problems on mesh interconnected systems <ref> [T.92, Y.88, PR89] </ref>. Hence, the choice of a mesh-connected network and the generality of the basic processor provides the user with flexibility in determining the best mapping, for a wide range of applications, onto the architecture. <p> Shifting and exchanging can be done simultaneously by using the shift register network. 4.1 Algorithms The first example is the transitive closure algorithm by Guibas et al <ref> [PR89] </ref>. This algorithm requires a toroidal interconnect and it uses the adjacency matrix representation of the graph. Initialize the array by reserving storage in each processor, say A ij and setting it to zero. <p> Updating A ij . When a ik and a 0 kj meet at cell A ij , replace A ij by A ij := min (A ij ; a ik + a 0 kj ). The array can also be used to implement dynamic programming problems of the type <ref> [PR89] </ref>, 8i; j : 1 i &lt; j n c (i; j) = w (i; j) + minfc (i; k) + c (k; j)g where i &lt; k &lt; j This is a general formulation that applies to a wide set of problems.
Reference: [T.92] <author> Leighton F. T. </author> <title> Introduction to Parallel Algorithms and Architectures. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Nearest neighbour mesh interconnect between the processors results in an array with a very tight packing of processors. There already exists a rich body of work on solving problems on mesh interconnected systems <ref> [T.92, Y.88, PR89] </ref>. Hence, the choice of a mesh-connected network and the generality of the basic processor provides the user with flexibility in determining the best mapping, for a wide range of applications, onto the architecture.
Reference: [Tre83] <author> P. C. Treleaven. </author> <title> Decentralised Computer Architectures for VLSI. </title> <editor> In B. Randel and P.C.Treleaven, editors, </editor> <booktitle> VLSI Architecture, </booktitle> <pages> pages 348-380. </pages> <booktitle> Pren-tice/Hall International, </booktitle> <year> 1983. </year>
Reference-contexts: In <ref> [Tre83] </ref>, the author, describes the design criteria for decentralised VLSI architectures such as simple basic cells, simple and regular data and control paths, localized communication, extensive use of con-currency, and the ability to reuse data, i.e. reduce the number of trips to memory by providing each processor with a small local
Reference: [TY83] <author> Kung H. T. and S. Q. Yu. </author> <title> Integrating High-Performance Special Purpose Devices into a System. </title> <editor> In B. Randel and P.C.Treleaven, editors, </editor> <booktitle> VLSI Architecture, </booktitle> <pages> pages 205-211. </pages> <note> Prentice/Hall International, 1983. 6 </note>
Reference-contexts: 1 Introduction In <ref> [TY83] </ref>, H.T. Kung and S.Q. Yu argue the case for high performance computers based on architectures capable of exploiting VLSI modules, in particular, compact systems that can be plugged in with interchangeable, special purpose, high performance modules.
Reference: [Ull84] <author> J. D. Ullman. </author> <title> Computational Aspects of VLSI. </title> <publisher> Computer Science Press, </publisher> <year> 1984. </year>
Reference-contexts: The first time they arrive at a node is the first time that node sees the corresponding a or a 0 value <ref> [Ull84] </ref>. The transitive closure is computed when the two matrices complete three passes through the array. A slight modification of rule 1 above yields the shortest path algorithm, the modified rule is: * 1. Updating A ij .
Reference: [Y.88] <author> Kung S. Y. </author> <title> VLSI Array Processors. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: Nearest neighbour mesh interconnect between the processors results in an array with a very tight packing of processors. There already exists a rich body of work on solving problems on mesh interconnected systems <ref> [T.92, Y.88, PR89] </ref>. Hence, the choice of a mesh-connected network and the generality of the basic processor provides the user with flexibility in determining the best mapping, for a wide range of applications, onto the architecture.
References-found: 20

