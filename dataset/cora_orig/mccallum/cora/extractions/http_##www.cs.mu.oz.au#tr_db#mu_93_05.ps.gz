URL: http://www.cs.mu.oz.au/tr_db/mu_93_05.ps.gz
Refering-URL: http://www.cs.mu.oz.au/tr_db/TR.html
Root-URL: 
Title: Join Algorithm Costs Revisited  
Author: Evan P. Harris Kotagiri Ramamohanarao 
Address: Parkville, Victoria 3052 Australia  
Affiliation: Department of Computer Science The University of Melbourne  
Pubnum: Technical Report 93/5  
Abstract: A method of analysing join algorithms based upon the time required to access, transfer and perform the relevant CPU based operations on a disk page is proposed. The costs of variations of several of the standard join algorithms, including nested block, sort-merge, GRACE hash and hybrid hash, are presented. For a given total buffer size, the cost of these join algorithms depends on the parts of the buffer allocated for each purpose (for example, when joining two relations using the nested block join algorithm the amount of buffer space allocated for the outer and inner relations can significantly affect the cost of the join). Analysis of expected and experimental results of various join algorithms show that a combination of the optimal nested block and optimal GRACE hash join algorithms usually provide the greatest cost benefit. Algorithms to quickly determine the buffer allocation producing the minimal cost for each of these algorithms are presented. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Aarts and J. Korst. </author> <title> Simulated Annealing and Boltzmann Machines. </title> <publisher> Wiley, </publisher> <year> 1989. </year>
Reference-contexts: To attempt to reduce this we used simulated annealing <ref> [1] </ref> to attempt to find a near optimal buffer arrangement in a much shorter period of time. We would like to know if the improvement in performance achieved using simulated annealing makes determining the minimal arrangement for the hybrid hash join algorithm worthwhile.
Reference: [2] <author> M. W. Blasgen and K. P. Eswaran. </author> <title> Storage and access in relational data bases. </title> <journal> IBM Systems Journal, </journal> <volume> 16(4), </volume> <year> 1977. </year>
Reference-contexts: The nested loop algorithm is the first of these. This algorithm and the sort-merge algorithm are the algorithms used in most current database implementations. The nested loop is used when a small relation is joined to another, and the sort-merge for larger relations <ref> [2] </ref>. For a while it was believed that the sort-merge join was the best possible algorithm [9], however the description of hash based join algorithms [4, 7] indicated that this was not necessarily true. <p> We now compare the four join algorithms, using the same example as above. The result of this is shown in Figures 9 and 10. Figure 10 contains an enlarged version of part of Figure 9. Others using the standard cost model, such as Blasgen and Eswaran <ref> [2] </ref>, found that when the outer relation may be contained within main memory, the nested block algorithm performs the best. Figures 9 and 10 show that this is still the case under our cost model.
Reference: [3] <author> J. Cheng, D. Haderle, R. Hedges, B. R. Iyer, T. Messinger, C. Mohan, and Y. Wang. </author> <title> An efficient hybrid join algorithm: a DB2 prototype. </title> <booktitle> In Proceedings of the Seventh International Conference on Data Engineering, </booktitle> <pages> pages 171-180, </pages> <address> Kobe, Japan, April 1991. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: DeWitt, et al. compared the sort-merge, simple hash, GRACE hash and hybrid hash join algorithms and concluded that hybrid hash has the lowest cost. While some are still unsure, for example, Cheng, et al. <ref> [3] </ref> claimed that when memory is large then sort-merge and hybrid hash have similar disk I/O performance, hybrid hash is regarded as one of the best algorithms for performing the join. A survey of join algorithms appears in [10].
Reference: [4] <author> D. J. DeWitt, R. H. Katz, F. Olken, L. D. Shapiro, M. R. Stonebraker, and D. Wood. </author> <title> Implementation techniques for main memory database systems. </title> <editor> In B. Yormark, editor, </editor> <booktitle> Proceedings of the 1984 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pages 1-8, </pages> <address> Boston, MA, USA, </address> <month> June </month> <year> 1984. </year>
Reference-contexts: The nested loop is used when a small relation is joined to another, and the sort-merge for larger relations [2]. For a while it was believed that the sort-merge join was the best possible algorithm [9], however the description of hash based join algorithms <ref> [4, 7] </ref> indicated that this was not necessarily true. DeWitt, et al. compared the sort-merge, simple hash, GRACE hash and hybrid hash join algorithms and concluded that hybrid hash has the lowest cost. <p> In practice, a lower cost may be found by making two passes to partition the data when the relations are this large so that multiple pages may be read during one read operation. This is discussed further in Section 3.2.1. 2.4 Hybrid Hash The hybrid hash join algorithm <ref> [4, 12] </ref> is very similar to the GRACE hash join algorithm. The difference is that the hybrid hash join algorithm reserves an area of memory to join records in during the partitioning phase. <p> This is a generalisation of the original hybrid hash scheme <ref> [4] </ref>, in which it was assumed that there was only one pass through the data, that is, = 1. 3 Minimising Costs The equations given in Section 2 provide the costs of each join method. <p> As the size of the outer relation gets larger the other join algorithms all perform better than the nested block algorithm. The result that is different to that reported in the past using the standard cost model, such as by DeWitt, et al. in <ref> [4] </ref>, is that, in general, the GRACE hash algorithm performs as well as, or better than, the hybrid hash algorithm for large relation sizes. <p> Again, as reported in <ref> [4] </ref> using the standard cost model, the sort-merge algorithm does not perform as well as either hash-based algorithm, despite the fact that our version of the sort-merge algorithm has a lower cost than the version usually reported. 4.5 Join Algorithm Comparison: Optimisation Times In the previous four sections we have shown
Reference: [5] <author> R. B. Hagmann. </author> <title> An observation on database buffering performance metrics. </title> <editor> In Y. Kam-bayashi, editor, </editor> <booktitle> Proceedings of the Twelfth International Conference on Very Large Data Bases, </booktitle> <pages> pages 289-293, </pages> <address> Kyoto, Japan, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: Each of the aforementioned papers typically uses the number of pages transferred in its description of the cost of disk operations. This assumes that the cost of transferring a number of consecutive pages at once is the same as transferring them individually from random parts of the disk. Hagmann <ref> [5] </ref> argued that, for current disk drive technology, when small numbers of pages are transferred the cost of locating the pages was much greater than the cost of transferring them. <p> The nested block algorithm works by reading a block of records, typically one page, from the outer relation and passing over each record of the inner relation (also read in blocks) joining the records of the outer relation with those of the inner relation. Note that under Hagmann's analysis <ref> [5] </ref> half the available memory is devoted to pages from the inner relation, and half to the outer relation. A slightly more efficient version of this algorithm can be obtained by rocking backwards and forwards across the inner relation. <p> The Nested block (std) line corresponds to the standard version of the nested block algorithm commonly described in the literature, in which B A = B 2, B B = B R = 1. The Nested block (Hag) line corresponds to the version proposed by Hagmann <ref> [5] </ref> in which B A = B B = (B 1)=2, B R = 1. Note that while Hagmann's version is faster than the standard version for larger relations, it is approximately twice as slow for some of the smaller relations.
Reference: [6] <author> M. Kitsuregawa, M. Nakayama, and M. Takagi. </author> <title> The effect of bucket size tuning in the dynamic hybrid GRACE hash join method. </title> <editor> In P. M. G. Apers and G. Wiederhold, editors, </editor> <booktitle> Proceedings of the Fifteenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 257-266, </pages> <address> Amsterdam, The Netherlands, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: Their method dynamically determines which partitions will be stored in memory and which will be stored on disk. This depends upon the distribution of data. In <ref> [6] </ref> they provide an analysis of DHGH and show the effect of varying partition sizes and show that a large number of small partitions is the best method to handle non-uniform distributions. However, these small partitions are combined for the join phase to minimise the join cost.
Reference: [7] <author> M. Kitsuregawa, H. Tanaka, and T. Moto-oka. </author> <title> Application of hash to data base machine and its architecture. </title> <journal> New Generation Computing, </journal> <volume> 1(1) </volume> <pages> 66-74, </pages> <year> 1983. </year>
Reference-contexts: The nested loop is used when a small relation is joined to another, and the sort-merge for larger relations [2]. For a while it was believed that the sort-merge join was the best possible algorithm [9], however the description of hash based join algorithms <ref> [4, 7] </ref> indicated that this was not necessarily true. DeWitt, et al. compared the sort-merge, simple hash, GRACE hash and hybrid hash join algorithms and concluded that hybrid hash has the lowest cost. <p> A similar analysis shows that if 16 Mbytes of memory 4 is available, the maximum size of each relation is 16 Gbytes. Thus, if large main memory is available, one sorting and merging pass is likely to be sufficient. 2.3 GRACE Hash The GRACE hash join algorithm <ref> [7] </ref> also works in two phases, in a similar way to the sort-merge algorithm. In the first, partitioning, phase, each relation is partitioned such that one relation may be contained within memory. This may take a number of passes over the relations.
Reference: [8] <author> L. W. McVoy and S. R. Kleiman. </author> <title> Extent-like performance from a UNIX file system. </title> <booktitle> In Proceedings of the USENIX 1991 Winter Conference, </booktitle> <pages> pages 33-43, </pages> <address> Dallas, Texas, USA, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: Our analysis is a general-isation of these two and allows the relative, or absolute, cost of each disk and CPU operation to be specified. The use of an extent based file system, even under UNIX <ref> [8] </ref>, will provide greater support for our technique than standard file systems which do not guarantee that consecutive blocks are even on the same part of the disk. Although standard file systems do typically try to cluster contiguous blocks, extent based file systems achieve this to a greater degree. <p> This was implemented on a Sun Sparcstation 10 and relied on the UNIX file system for file management. Thus, we had no control over the disk accesses required to retrieve the data. The SunOS file system <ref> [8] </ref> used attempts to keep consecutive pages together in cylinder groups wherever possible as well as buffering disk pages and reading disk pages ahead. To combat these 21 V R = 100, T K = 5T T , T C = T J , T P = T J =8.
Reference: [9] <author> T. H. Merrett. </author> <title> Why sort-merge gives the best implementation of the natural join. </title> <booktitle> SIGMOD Record, </booktitle> <volume> 13(2) </volume> <pages> 39-51, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: The nested loop is used when a small relation is joined to another, and the sort-merge for larger relations [2]. For a while it was believed that the sort-merge join was the best possible algorithm <ref> [9] </ref>, however the description of hash based join algorithms [4, 7] indicated that this was not necessarily true. DeWitt, et al. compared the sort-merge, simple hash, GRACE hash and hybrid hash join algorithms and concluded that hybrid hash has the lowest cost.
Reference: [10] <author> P. Mishra and M. H. Eich. </author> <title> Join processing in relational databases. </title> <journal> ACM Computing Surveys, </journal> <volume> 24(1) </volume> <pages> 63-113, </pages> <month> March </month> <year> 1992. </year> <month> 25 </month>
Reference-contexts: While some are still unsure, for example, Cheng, et al. [3] claimed that when memory is large then sort-merge and hybrid hash have similar disk I/O performance, hybrid hash is regarded as one of the best algorithms for performing the join. A survey of join algorithms appears in <ref> [10] </ref>. Each of the aforementioned papers typically uses the number of pages transferred in its description of the cost of disk operations. This assumes that the cost of transferring a number of consecutive pages at once is the same as transferring them individually from random parts of the disk.
Reference: [11] <author> M. Nakayama, M. Kitsuregawa, and M. Takagi. </author> <title> Hash-partitioned join method using dynamic destaging strategy. </title> <editor> In F. Bancilhon and D. J. DeWitt, editors, </editor> <booktitle> Proceedings of the Fifteenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 468-478, </pages> <address> Los Angeles, CA, USA, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: To address this problem Nakayama, et al. proposed an extension to the hybrid hash method, called the Dynamic Hybrid GRACE Hash join method (DHGH) <ref> [11] </ref>. Their method dynamically determines which partitions will be stored in memory and which will be stored on disk. This depends upon the distribution of data.
Reference: [12] <author> L. D. Shapiro. </author> <title> Join processing in database systems with large main memories. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 11(3) </volume> <pages> 239-264, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: In practice, a lower cost may be found by making two passes to partition the data when the relations are this large so that multiple pages may be read during one read operation. This is discussed further in Section 3.2.1. 2.4 Hybrid Hash The hybrid hash join algorithm <ref> [4, 12] </ref> is very similar to the GRACE hash join algorithm. The difference is that the hybrid hash join algorithm reserves an area of memory to join records in during the partitioning phase.
Reference: [13] <author> J. Vaghani, K. Ramamohanarao, D. B. Kemp, Z. Somogyi, and P. J. Stuckey. </author> <title> Design overview of the Aditi deductive database system. </title> <booktitle> In Proceedings of the Seventh International Conference on Data Engineering, </booktitle> <pages> pages 240-247, </pages> <address> Kobe, Japan, April 1991. </address> <publisher> IEEE Computer Society Press. </publisher> <pages> 26 </pages>
Reference-contexts: When the cost of a join operation is determined, the difference in this time should be taken into account. The CPU cost of a join should also be taken into account. Experience with the Aditi deductive database <ref> [13] </ref> has shown that the disk access and transfer times correspond to 10-20% of the time taken to perform a join. Thus, the CPU time required to perform the join should be taken into account when determining the most efficient method to use to perform any given join. <p> In this way, each relation is only read once after it has been sorted. The variant of the sort-merge algorithm whose cost we present below is a simplified version of that used in the Aditi deductive database system <ref> [13] </ref>. Instead of completely sorting each relation, each relation is partitioned into sorted partitions that are the size of the memory buffer, B. To perform this partitioning, B pages are read from a relation, sorted, and written out, then another B pages, and so on.
References-found: 13

