URL: ftp://synapse.cs.byu.edu/pub/papers/rudolph_95a.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Title: Word Perfect Corp. An Efficient Transformation for Implementing Two-layer Feedforward Neural Networks  
Author: George L. Rudolph Tony R. Martinez 
Address: Provo, Utah 84602  
Affiliation: Computer Science Department Brigham Young University  
Note: To appear in Journal of Artificial Neural Networks, 1995. This research is funded in part by grants from Novell Inc. and  
Abstract: Most Artificial Neural Networks (ANNs) have a fixed topology during learning, and often suffer from a number of shortcomings as a result. Variations of ANNs that use dynamic topologies have shown ability to overcome many of these problems. This paper introduces Location-Independent Transformations (LITs) as a general strategy for implementing distributed feedforward networks that use dynamic topologies (dynamic ANNs) efficiently in parallel hardware. A LIT creates a set of location-independent nodes, where each node computes its part of the network output independent of other nodes, using local information. This type of transformation allows efficient support for adding and deleting nodes dynamically during learning. In particular, this paper presents an LIT for dynamic Backpropagation networks with a single hidden layer. The complexity of both learning and execution algorithms is O(n+p+logm) for a single pattern, where nis the number of inputs, p is the number of outputs, and m is the number of hidden nodes in the original network. Keywords: Neural Networks, Backpropagation, Implementation Design, Dynamic Topologies, Reconfigurable Architectures. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. S. Almasi, A. Gottlieb, </author> <title> Highly Parallel Computing , The Benjamin/Cummings Publishing Company, </title> <publisher> Inc, </publisher> <address> Redwood City, CA, </address> <year> 1989. </year>
Reference: [2] <author> P. T. Baffes, J. M. Zelle, </author> <title> Growing Layers of Perceptrons: Introducing the Extentron Algorithm, </title> <booktitle> Proceedings of 1992 IEEE/INNS International Joint Conference on Neural NetworksBaltimore, </booktitle> <volume> Vol. 2, </volume> <pages> pp 4979-4984, </pages> <year> 1992. </year>
Reference: [3] <author> T. Denoeux, R. Lengelle, </author> <title> Initializing Back Propagation Networks With Prototypes Neural Networks, </title> <journal> Vol. </journal> <volume> 6, #3, </volume> <pages> pp 351-363, </pages> <publisher> Pergamon Press Ltd, </publisher> <address> New York, </address> <year> 1993. </year>
Reference: [4] <author> S. Fahlmann, </author> <title> Faster-Learning Variations on BackPropagation: An Empirical Study, </title> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> pp 38-51, </pages> <year> 1988. </year>
Reference: [5] <author> S. Fahlmann, C. Lebiere, </author> <booktitle> The Cascade-Correlation Learning Architecture, Advances in Neural Information Processing 2, </booktitle> <pages> pp 524-532, </pages> <publisher> Morgan Kaufmann Publishers: </publisher> <address> Los Altos, CA. </address>
Reference: [6] <author> Farhat, N., D. Psaltis, A. Prata, and E. Paek. </author> <title> Optical Implementation of the Hopfield Model. </title> <journal> Applied Optics, </journal> <volume> Vol. 24, #10. </volume> <month> pp.1469-1475 . </month> <year> 1985. </year>
Reference: [7] <author> Graf, H., L. Jackel, W. Hubbard. </author> <title> VLSI Implementation of a Neural Network Model. In Artificial Neural Networks: Electronic Implementations, </title> <publisher> Nelson Morgan, Ed. </publisher> <pages> pp. 34-42. </pages> <year> 1990. </year>
Reference: [8] <author> Hammerstrom, D., W. Henry, M. Kuhn. </author> <title> Neurocomputer System for Neural-Network Applications. In Parallel Digital Implementations of Neural Networks . K. </title> <editor> Przytula, V. Prasanna, Eds. </editor> <publisher> Prentice-Hall, Inc. </publisher> <year> 1991. </year>
Reference: [9] <author> Hillis, W. Daniel. </author> <title> The Connection Machine. </title> <address> Cambridge, Mass.: </address> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference: [10] <author> E. Karnin, </author> <title> A Simple Procedure for Pruning BackPropagation Trained Neural Networks, </title> <journal> IEEE Transactions On Neural Networks, </journal> <volume> Vol. 1, #2, </volume> <pages> pp 239-242, </pages> <month> June, </month> <year> 1990. </year>
Reference: [11] <author> C. Lee, </author> <title> A Simple Procedure for Pruning BackPropagation Trained Neural Networks, </title> <booktitle> Neural Networks, </booktitle> <volume> Vol. 6, #3, </volume> <pages> pp 385-392. 16 </pages>
Reference: [12] <author> T. Martinez, D. Campbell, </author> <title> A Self-Adjusting Dynamic Logic Module, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 11, #4, </volume> <pages> pp 303-313, </pages> <year> 1991. </year>
Reference: [13] <author> C. Mead, </author> <title> Analog VLSI and Neural Systems, </title> <publisher> Addison-Wesley Publishing Company, Inc, </publisher> <year> 1991. </year>
Reference: [14] <author> S. Odri, D. Petrovacki, G. Krstonosic, </author> <title> Evolutional Development of a Multilevel Neural Network, </title> <booktitle> Neural Networks, </booktitle> <volume> Vol. 6, #4. </volume> <pages> pp 583-595, </pages> <publisher> Pergamon Press Ltd, </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: Most algorithms assume that psse and tsse can be maintained, but give no explicit mechanism for doing soLIT provides a mechanism. 10 IV. THE DYNAMIC BP MODEL This section gives an overview the dynamic BP (hereafter DBP) model proposed by Odri, et al <ref> [14] </ref>, then formally extends the learning algoirthm of section III to support these dynamics. The DBP model of [14] is of interest because it allows the addition and deletion of both nodes and individual weights dynamically during learning. <p> THE DYNAMIC BP MODEL This section gives an overview the dynamic BP (hereafter DBP) model proposed by Odri, et al <ref> [14] </ref>, then formally extends the learning algoirthm of section III to support these dynamics. The DBP model of [14] is of interest because it allows the addition and deletion of both nodes and individual weights dynamically during learning. Some dynamic BP models use pruning as a basic technique, while some both grow and prune structures during learning. <p> Some use global state information, and change the topology only after learning is complete, while some use partial, localized information and attempt to change the topology incrementally as learning proceeds. The algorithm of <ref> [14] </ref> can be outlined as follows. Details are discussed after the outline. The steps are numbered to correspond with the algorithm of section III, for convenience. Until Convergence (tsse &lt; ecrit) 1. Present a pattern to the network. 2-5. Perform standard BP learning, including adjusting the weights. 6. <p> This increases the overall stability of the network. Nodes delete themselves in two cases: 1. When u i is 0, or 2. when w j is 0. In case 1, the threshold or bias value may not be 0, even if all of the other weights are 0. <ref> [14] </ref> gives a simple transformation which adjusts the biases of nodes in the subsequent layer to compensate for deleting a node with a non-zero bias q' k =q k +w jk f (q j ). (15) j indicates the node that is deleted, and k indicates the nodes in the subsequent
Reference: [15] <author> Ramacher, U., W. Raab, J. Anlauf, U. Hachmann, J. Beichter, N. Brls, M. Weiling, E. Schneider, R. Mnner, J. Gl. </author> <title> Multiprocessor and Memory Architecture of the Neurocomputer SYNAPSE-1. </title> <booktitle> Proceedings, World Congress on Neural Networks 1993, </booktitle> <volume> Vol. 4. </volume> <pages> pp. 775-778. </pages> <publisher> INNS Press, </publisher> <year> 1993. </year>
Reference: [16] <author> D. Reilly, L. Cooper, C. </author> <title> Erlbaum, A Neural Model Category Learning, </title> <journal> Biological Cybernetics, </journal> <volume> Vol. 45, </volume> <pages> pp 35-41, </pages> <year> 1982. </year>
Reference: [17] <author> G. Rudolph, T. Martinez, </author> <title> An Efficient Static Topology for Modeling ASOCS, Artificial Neural Networks, </title> <editor> Editors, Kohonen et al, </editor> <address> pp 279-734, </address> <publisher> Elsevier Publishers, North Holland, </publisher> <year> 1991. </year>
Reference: [18] <author> G. Rudolph, T. Martinez, </author> <title> A Transformation for Implementing Localist Neural Networks, </title> <note> Submitted, </note> <year> 1994. </year>
Reference: [19] <author> Rudolph G., Martinez, T. R. </author> <title> A Transformation for Implementing Multiple-Hidden-Layer FeedForward Networks, </title> <note> Submitted, </note> <year> 1994. </year>
Reference-contexts: The LIT for networks with more than two layers is presented in <ref> [19] </ref>. Here, and throughout the rest of this paper, given a node j, all the nodes in the same node layer as j are referred to collectively as the current layer. The nodes in the node layer labeled with k are referred to as the subsequent layer.
Reference: [20] <editor> Rumelhart, D., J. McClelland, et. al. </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> VOL. 1, </volume> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: No other nodes are affected. III. TRANSFORMATION OF A 2-LAYER BP NETWORK It is assumed that the reader is familiar with the standard BP model <ref> [20] </ref>, however some necessary definitions are given here. Following that is a discussion of the structure and operation of a transformed node. In this description, variables are introduced, used and defined informally.
Reference: [21] <author> Shams, S. </author> <title> Dream MachineA Platform for Efficient Implementation of Neural Networks with Arbitrarily Complex Interconnect Structures. </title> <type> Technical Report CENG 92-23 . PhD Dissertation, </type> <institution> USC, </institution> <year> 1992. </year>
Reference: [22] <author> A. Sperduti, A. Starita. </author> <title> Speed Up Learning and Network Optimization With Extended Back Propagation, </title> <booktitle> Neural Networks, </booktitle> <volume> Vol. 6, #3, </volume> <pages> pp 365-383, </pages> <publisher> Pergamon Press Ltd, </publisher> <address> New York, </address> <year> 1993. </year>
References-found: 22

