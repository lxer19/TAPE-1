URL: http://www.cs.cmu.edu/~seth/papers/isca92-df.ps
Refering-URL: http://www.cs.cmu.edu/~seth/papers.html
Root-URL: 
Title: Empirical Study of a Dataflow Language on the CM-5  
Author: David E. Culler Seth Copen Goldstein Klaus Erik Schauser Thorsten von Eicken 
Address: Berkeley  
Affiliation: Computer Science Division Department of Electrical Engineering and Computer Sciences College of Engineering University of California,  
Abstract: This paper presents empirical data on the behavior of large dataflow programs on a distributed memory multiprocessor. The programs, written in the dataflow language Id90, are compiled via a Threaded Abstract Machine (TAM) for the CM-5. TAM refines dataflow execution models by addressing critical constraints that modern parallel architectures place on the compilation of general-purpose parallel programming languages. It exposes synchronization, scheduling, and network access so that the compiler can optimize against the cost of these operations. The data presented in this paper evaluates the TAM approach in compiling dataflow languages on stock hardware. We present data on the instruction mix, speedup, scheduling behavior, and locality of large ID90 programs. It is shown that the TAM scheduling hierarchy is able to tolerate long communication latencies, especially when some degree of I-structure locality is present. We investigate how frame allocation strategies, k-bounded loops, and I-structure caching and distribution together affect the overall efficiency. Finally we document some scheduling anomalies. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Arvind and D. E. Culler. </author> <title> Dataflow Architectures. </title> <booktitle> In Annual Reviews in Computer Science, </booktitle> <volume> volume 1, </volume> <pages> pages 225-253. </pages> <publisher> Annual Reviews Inc., </publisher> <address> Palo Alto, CA, </address> <year> 1986. </year> <title> Reprinted in Dataflow and Reduction Architectures, </title> <editor> S. S. Thakkar, editor, </editor> <publisher> IEEE Computer Society Press, </publisher> <year> 1987. </year>
Reference-contexts: Dynamic dataflow architectures implicitly allocate storage for the invocation tree, since the matching store allocates storage on a token-by-token basis. However, if we examine the language implementations on the TTDA <ref> [1] </ref> or Manchester machine [13] more carefully, we see that the function invocation involves allocating a "context" or portion of the tag space.
Reference: [2] <author> Arvind and K. Ekanadham. </author> <title> Future Scientific Programming on Parallel Machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(5) </volume> <pages> 460-493, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: For example, the Id90 binding a = (1,a) defines a to be a pair with itself as its second element. Recursive data structures arise quite reasonably in practice <ref> [2] </ref>. The expressive power of non-strictness and synchronizing data access makes compiling into threads tricky, because static scheduling decisions can potentially introduce deadlock. While compiling a function it may be impossible to determine a static ordering of the operations, even in the absence of conditionals.
Reference: [3] <author> Arvind, R. S. Nikhil, and K. K. Pingali. I-Structures: </author> <title> Data Structures for Parallel Computing. </title> <type> Technical Report CSG Memo 269, </type> <institution> MIT Lab for Comp. Sci., </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <month> February </month> <year> 1987. </year> <title> (Also in Proc. of the Graph Reduction Workshop, </title> <address> Santa Fe, NM. </address> <month> October </month> <year> 1986.). </year>
Reference-contexts: It avoids synchronization waits in the same manner, and further reduces the impact of transient load imbalance, since a processor remains busy as long as any local work is available. Dynamic scheduling supports powerful parallel languages with synchronizing data access <ref> [3] </ref> or non-strict order of evaluation [27, 7]. Dataflow processors operate greedily, grabbing hold of any available useful work, rather than sitting idle.
Reference: [4] <author> M. Beck and K. Pingali. </author> <title> From Control Flow to Dataflow. </title> <type> Technical Report TR 89-1050, </type> <institution> Cornell Univ., Dept. of Computer Science, </institution> <month> October </month> <year> 1989. </year> <month> 21 </month>
Reference-contexts: It has been demonstrated that direct execution of dataflow graphs is not essential to attain these goals, as dataflow graphs are equivalent to threaded instruction sets with very weak addressing modes [9, 18]. Dataflow graphs are, however, very useful in the compilation process <ref> [4, 23, 22, 25, 28] </ref>. At the machine level, the essential aspect of dataflow is efficient dynamic scheduling. Dynamic scheduling provides tolerance to communication latency, since the processor picks up other useful work rather than waiting for each response.
Reference: [5] <author> David Cann. </author> <title> Retire Fortran? A Debate Rekindled. </title> <booktitle> In Proc. of Supercomputing91, </booktitle> <pages> pages 264-72, </pages> <address> Alb., NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Static dataflow machines have been applied primarily to regular, structured problems [10, 11, 24]. Dataflow languages, including Sisal and a restricted form of Id, have been implemented on conventional architectures, by exploiting the regular structure in traditional FORTRAN-like applications <ref> [5, 19] </ref>. Our work attempts to fill some of this empirical vacuum by implementing Id90 on large parallel machines in a manner that retains the efficient dynamic scheduling of dataflow models.
Reference: [6] <author> D. E. Culler. </author> <title> Managing Parallelism and Resources in Scientific Dataflow Programs. </title> <type> Technical Report 446, </type> <institution> MIT Lab for Comp. Sci., </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: It eliminates an extremely serious imbalance in I-structure services and localizes 50% of the accesses. Observe that the computational load is quite even with this policy. Caching results in a five-fold speedup. 4.3 K-bounds Several techniques have been proposed in the literature for controlling parallelism, including k-bounded loops <ref> [6] </ref> and activation tree throttling [20]. For the most part, these approaches have been validated through idealized simulations and execution on a small number of processors. Here we present data on the effects of k-bounded loops on 64 processors.
Reference: [7] <author> D. E. Culler. </author> <title> Multithreading: Fundamental Limits, Potential Gains, and Alternatives. </title> <booktitle> In Proc. of the Supercomputing91 Workshop on Multithreading, </booktitle> <year> 1992. </year>
Reference-contexts: It avoids synchronization waits in the same manner, and further reduces the impact of transient load imbalance, since a processor remains busy as long as any local work is available. Dynamic scheduling supports powerful parallel languages with synchronizing data access [3] or non-strict order of evaluation <ref> [27, 7] </ref>. Dataflow processors operate greedily, grabbing hold of any available useful work, rather than sitting idle.
Reference: [8] <author> D. E. Culler, S. C. Goldstein, K. E. Schauser, and T. von Eicken. </author> <title> TAM | A Compiler Controlled Threaded Abstract Machine. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 347-370, </pages> <month> July </month> <year> 1993. </year> <note> Special Issue on Dataflow. </note>
Reference-contexts: The absence of a high-level program execution strategy makes these interactions difficult to understand or control. 2 Threaded Abstract Machine In this section, we describe TAM <ref> [8] </ref>, a threaded abstract machine that serves as an intermediate step in compiling the dataflow language Id90 for conventional parallel (and sequential) architectures. Historically, Id90 was developed in close connection with dynamic dataflow architectures, especially the MIT Tagged Token Dataflow Architecture. <p> Thus, the task of the compiler in partitioning the dataflow graph into threads is to prove where such external dependences cannot exist. The basic analysis techniques for partitioning are described in <ref> [23, 22, 8] </ref>. In addition [28] shows how this analysis can be carried out globally. 2.4 Translation from TAM The second compilation step translates the TAM program to an executable for a specific parallel machine and addresses specifics of the target, such as the particular network interface, addressing modes, etc.
Reference: [9] <author> D. E. Culler and G. M. Papadopoulos. </author> <title> The Explicit Token Store. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 289-308, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: It has been demonstrated that direct execution of dataflow graphs is not essential to attain these goals, as dataflow graphs are equivalent to threaded instruction sets with very weak addressing modes <ref> [9, 18] </ref>. Dataflow graphs are, however, very useful in the compilation process [4, 23, 22, 25, 28]. At the machine level, the essential aspect of dataflow is efficient dynamic scheduling. <p> Thus, we may think of each argument as initiating a thread of control within the function invocation. Threads are implicitly synchronized and forked in the dataflow graph representation. The Explicit Token Store <ref> [9] </ref> model returns to the conventional idea of allocating storage for local variables with each invocation. It makes the further assumption that the entire invocation will execute on the processor to which the activation frame belongs.
Reference: [10] <author> J. B. Dennis, G. Gao, and K. W. Todd. </author> <title> Modeling the weather with a data flow supercomputer. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-33(7):592-603, </volume> <month> July </month> <year> 1984. </year>
Reference-contexts: The network operates at five times the processor clock rate, so there is little communication latency. Sigma-1 and EM-4 are available in large configurations, but the only programs that have been run are small, regular, and hand tuned. Static dataflow machines have been applied primarily to regular, structured problems <ref> [10, 11, 24] </ref>. Dataflow languages, including Sisal and a restricted form of Id, have been implemented on conventional architectures, by exploiting the regular structure in traditional FORTRAN-like applications [5, 19].
Reference: [11] <author> J. Gaudiot, R. W. Vedder, G. K. Tucker, D. Finn, </author> <title> and M.L. Campbell. A Distributed VLSI Architecture for Efficient Signal Processing. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-34(12):1072-87, </volume> <month> December </month> <year> 1985. </year>
Reference-contexts: The network operates at five times the processor clock rate, so there is little communication latency. Sigma-1 and EM-4 are available in large configurations, but the only programs that have been run are small, regular, and hand tuned. Static dataflow machines have been applied primarily to regular, structured problems <ref> [10, 11, 24] </ref>. Dataflow languages, including Sisal and a restricted form of Id, have been implemented on conventional architectures, by exploiting the regular structure in traditional FORTRAN-like applications [5, 19].
Reference: [12] <author> S. C. Goldstein. </author> <title> Implementation of a Threaded Abstract Machine on Sequential and Multiprocessors. </title> <type> Master's thesis, </type> <institution> Computer Science Division | EECS, U.C. Berkeley, </institution> <year> 1994. </year> <note> (In preparation, to appear as UCB/CSD Technical Report). </note>
Reference-contexts: TAM registers are partitioned by type and must be mapped to physical registers or spill areas by the final translator. A generic TL0 translator maps the TL0 instructions into C and compiles the resulting C code for the machine <ref> [12] </ref>. Data collection code can be inserted while translating the TL0 code to facilitate experimentation at the TL0 level. For dynamically scheduled programs any attempt to record program behavior can possibly modify the behavior. We attempt to minimize the distortion by compiling limited data collection in-line.
Reference: [13] <author> J. Gurd, C.C. Kirkham, and I. Watson. </author> <title> The Manchester Prototype Dataflow Computer. </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> 28(1) </volume> <pages> 34-52, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: Dynamic dataflow architectures implicitly allocate storage for the invocation tree, since the matching store allocates storage on a token-by-token basis. However, if we examine the language implementations on the TTDA [1] or Manchester machine <ref> [13] </ref> more carefully, we see that the function invocation involves allocating a "context" or portion of the tag space.
Reference: [14] <author> R. A. </author> <title> Iannucci. Toward a Dataflow/von Neumann Hybrid Architecture. </title> <booktitle> In Proc. 15th Int. Symp. on Comp. Arch., </booktitle> <pages> pages 131-140, </pages> <address> Hawaii, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: TAM represents an effort to simplify the architecture even further, relying heavily on sophisticated compilation techniques. Traub's "compilation as partitioning" framework [26] and Iannucci's thread generation for the hybrid architecture <ref> [14] </ref> demonstrated that it was possible to reduce the amount of dynamic scheduling required. TAM builds directly on this work, but it addresses three other issues as well. First, there is no hardware management of storage resources. <p> It makes the further assumption that the entire invocation will execute on the processor to which the activation frame belongs. Monsoon [17] associates a presence-bit with each frame slot to support the dataflow view which associates a thread of control with every element of local data. Hybrid <ref> [14] </ref> adopted a complementary view, providing explicit threads of control which suspend upon access to a frame slot that is marked not-present.
Reference: [15] <author> R. S. Nikhil. </author> <title> The Parallel Programming Language Id and its Compilation for Parallel Machines. </title> <booktitle> In Proc. Workshop on Massive Parallelism, </booktitle> <address> Amalfi, Italy, October 1989. </address> <publisher> Academic Press, </publisher> <year> 1991. </year> <note> Also: CSG Memo 313, </note> <institution> MIT Laboratory for Computer Science, 545 Technology Square, </institution> <address> Cambridge, MA 02139, USA. </address>
Reference-contexts: Consider, for example, a tree traversal using accumulation lists. Under non-strict evaluation the two subtrees of a node can be processed in parallel, even though the result of processing one subtree will be used in processing the other <ref> [15] </ref>. Parallelism aside, if non-strict evaluation is provided to the programmer as part of the language semantics, the expressive power of the language is enhanced. For example, it is possible to define circular data structures in a non-strict functional language.
Reference: [16] <author> R. S. Nikhil and Arvind. </author> <title> Can Dataflow Subsume von Neumann Computing? In Proc. </title> <booktitle> of the 16th Annual Int. Symp. on Comp. Arch., </booktitle> <address> Jerusalem, Israel, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: Hybrid [14] adopted a complementary view, providing explicit threads of control which suspend upon access to a frame slot that is marked not-present. P-Risc <ref> [16] </ref> observed that presence-bits can be kept in the frame like local data, rather than as special tags, and that matching could be simulated by toggling the tag bit atomically and suspending on the result.
Reference: [17] <author> G. M. Papadopoulos and D. E. Culler. Monsoon: </author> <title> an Explicit Token-Store Architecture. </title> <booktitle> In Proc. of the 17th Annual Int. Symp. on Comp. Arch., </booktitle> <address> Seattle, Washington, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: The results are based on the 1992 version of our compiler and runtime system. 1 prototypes provide only limited data, since they cannot model the behavior of large programs in-the-large. The Manchester dataflow machine is correctly considered a single processor, as the multiple bit-slice-ALUs are a technological artifact. Monsoon <ref> [17] </ref> is only available in very small configurations. At the time the machine was designed, the characteristics of the per-processor storage hierarchy were not understood, so the issue was intentionally avoided by using fast static RAM for all the memory in the processor. <p> The Explicit Token Store [9] model returns to the conventional idea of allocating storage for local variables with each invocation. It makes the further assumption that the entire invocation will execute on the processor to which the activation frame belongs. Monsoon <ref> [17] </ref> associates a presence-bit with each frame slot to support the dataflow view which associates a thread of control with every element of local data. Hybrid [14] adopted a complementary view, providing explicit threads of control which suspend upon access to a frame slot that is marked not-present.
Reference: [18] <author> G. M. Papadopoulos and K. R. Traub. </author> <title> A Revisionist View of Dataflow Architectures. </title> <booktitle> In Proc. of the 18th Int'l Symp. on Computer Architecture, </booktitle> <pages> pages 342-351, </pages> <address> Toronto, Canada, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: It has been demonstrated that direct execution of dataflow graphs is not essential to attain these goals, as dataflow graphs are equivalent to threaded instruction sets with very weak addressing modes <ref> [9, 18] </ref>. Dataflow graphs are, however, very useful in the compilation process [4, 23, 22, 25, 28]. At the machine level, the essential aspect of dataflow is efficient dynamic scheduling.
Reference: [19] <author> A. Rogers and K. Pingali. </author> <title> Compiling Porgrams for Distributed Memory Architectures. </title> <booktitle> In Proc. of the 4th Hypercube Computers and Applications Conference, </booktitle> <address> Monterey, CA, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: Static dataflow machines have been applied primarily to regular, structured problems [10, 11, 24]. Dataflow languages, including Sisal and a restricted form of Id, have been implemented on conventional architectures, by exploiting the regular structure in traditional FORTRAN-like applications <ref> [5, 19] </ref>. Our work attempts to fill some of this empirical vacuum by implementing Id90 on large parallel machines in a manner that retains the efficient dynamic scheduling of dataflow models.
Reference: [20] <author> C. A. Ruggiero. </author> <title> Throttle Mechanisms for the Manchester Dataflow Machine. </title> <type> PhD thesis, </type> <institution> University of Manchester, </institution> <address> Manchester M13 9PL, England, </address> <month> July </month> <year> 1987. </year> <month> 22 </month>
Reference-contexts: Observe that the computational load is quite even with this policy. Caching results in a five-fold speedup. 4.3 K-bounds Several techniques have been proposed in the literature for controlling parallelism, including k-bounded loops [6] and activation tree throttling <ref> [20] </ref>. For the most part, these approaches have been validated through idealized simulations and execution on a small number of processors. Here we present data on the effects of k-bounded loops on 64 processors.
Reference: [21] <author> S. Sakai, Y. Yamaguchi, K. Hiraki, Y. Kodama, and T. Yuba. </author> <title> An Architecture of a Dataflow Single Chip Processor. </title> <booktitle> In Proc. of the 16th Annual Int. Symp. on Comp. Arch., </booktitle> <pages> pages 46-53, </pages> <address> Jerusalem, Israel, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: The third row shows the average entry count for synchronizing threads. Under traditional dataflow execution mechanisms the entry count would be two. Grouping together the nodes that depend on a single matching event to form a thread, as on Monsoon or EM-4 <ref> [21] </ref>, does not change the entry count. Our partitioning algorithm is more aggressive and will group larger collections of nodes together to form a thread with a higher entry count. However, it will also eliminate redundant forks, thereby reducing the entry count.
Reference: [22] <author> K. E. Schauser, D. Culler, and T. von Eicken. </author> <title> Compiler-controlled Multithreading for Lenient Parallel Languages. </title> <booktitle> In Proceedings of the 1991 Conference on Functional Programming Languages and Computer Architecture, </booktitle> <address> Cambridge, MA, </address> <month> August </month> <year> 1991. </year> <note> (Also available as Technical Report UCB/CSD 91/640, </note> <institution> CS Div., University of California at Berkeley). </institution>
Reference-contexts: It has been demonstrated that direct execution of dataflow graphs is not essential to attain these goals, as dataflow graphs are equivalent to threaded instruction sets with very weak addressing modes [9, 18]. Dataflow graphs are, however, very useful in the compilation process <ref> [4, 23, 22, 25, 28] </ref>. At the machine level, the essential aspect of dataflow is efficient dynamic scheduling. Dynamic scheduling provides tolerance to communication latency, since the processor picks up other useful work rather than waiting for each response. <p> The Id90 compiler has been substantially rewritten to target TL0, the TAM assembly language, rather than a dataflow instruction set. A key step in the compilation process is partitioning the dataflow program graph into threads <ref> [22, 28] </ref>. The other new aspects of the compilation process involve management of registers and local storage in the context of dynamic scheduling and code generation for threads. <p> Thus, the task of the compiler in partitioning the dataflow graph into threads is to prove where such external dependences cannot exist. The basic analysis techniques for partitioning are described in <ref> [23, 22, 8] </ref>. In addition [28] shows how this analysis can be carried out globally. 2.4 Translation from TAM The second compilation step translates the TAM program to an executable for a specific parallel machine and addresses specifics of the target, such as the particular network interface, addressing modes, etc.
Reference: [23] <author> Klaus Erik Schauser. </author> <title> Compiling Dataflow into Threads. </title> <type> Master's thesis, </type> <institution> University of Cali-fornia, Berkeley, Computer Science Div., </institution> <month> July </month> <year> 1991. </year> <note> Report No. UCB/CSD 91/644. </note>
Reference-contexts: It has been demonstrated that direct execution of dataflow graphs is not essential to attain these goals, as dataflow graphs are equivalent to threaded instruction sets with very weak addressing modes [9, 18]. Dataflow graphs are, however, very useful in the compilation process <ref> [4, 23, 22, 25, 28] </ref>. At the machine level, the essential aspect of dataflow is efficient dynamic scheduling. Dynamic scheduling provides tolerance to communication latency, since the processor picks up other useful work rather than waiting for each response. <p> Thus, the task of the compiler in partitioning the dataflow graph into threads is to prove where such external dependences cannot exist. The basic analysis techniques for partitioning are described in <ref> [23, 22, 8] </ref>. In addition [28] shows how this analysis can be carried out globally. 2.4 Translation from TAM The second compilation step translates the TAM program to an executable for a specific parallel machine and addresses specifics of the target, such as the particular network interface, addressing modes, etc.
Reference: [24] <author> T. Temma, Hasegawa S., and S. Hanaki. </author> <title> Dataflow Processor for Image Processing. </title> <booktitle> In Proc. of 11 Int'l Symp. on Mini and Microcomputers, </booktitle> <pages> pages 52-56, </pages> <year> 1980. </year>
Reference-contexts: The network operates at five times the processor clock rate, so there is little communication latency. Sigma-1 and EM-4 are available in large configurations, but the only programs that have been run are small, regular, and hand tuned. Static dataflow machines have been applied primarily to regular, structured problems <ref> [10, 11, 24] </ref>. Dataflow languages, including Sisal and a restricted form of Id, have been implemented on conventional architectures, by exploiting the regular structure in traditional FORTRAN-like applications [5, 19].
Reference: [25] <author> K. R. Traub. </author> <title> A Compiler for the MIT Tagged-Token Dataflow Architecture. </title> <type> Technical Report TR-370, </type> <institution> MIT Lab for Comp. Sci., </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <month> August </month> <year> 1986. </year> <type> (MS Thesis, </type> <institution> Dept. of EECS, MIT). </institution>
Reference-contexts: It has been demonstrated that direct execution of dataflow graphs is not essential to attain these goals, as dataflow graphs are equivalent to threaded instruction sets with very weak addressing modes [9, 18]. Dataflow graphs are, however, very useful in the compilation process <ref> [4, 23, 22, 25, 28] </ref>. At the machine level, the essential aspect of dataflow is efficient dynamic scheduling. Dynamic scheduling provides tolerance to communication latency, since the processor picks up other useful work rather than waiting for each response.
Reference: [26] <author> K. R. Traub. </author> <title> Compilation as Partitioning: A New Approach to Compiling Non-strict Functional Languages. </title> <booktitle> In Proc. of the Apenas Workshop on the Implmentation of Lazy Functional Languages, </booktitle> <institution> Chalmers Univ., Goteborg Sweden, </institution> <month> September </month> <year> 1988. </year> <note> also CSG Group Memo 291, </note> <institution> MIT Lab. for Computer Science. </institution>
Reference-contexts: TAM represents an effort to simplify the architecture even further, relying heavily on sophisticated compilation techniques. Traub's "compilation as partitioning" framework <ref> [26] </ref> and Iannucci's thread generation for the hybrid architecture [14] demonstrated that it was possible to reduce the amount of dynamic scheduling required. TAM builds directly on this work, but it addresses three other issues as well. First, there is no hardware management of storage resources.
Reference: [27] <author> K. R. Traub. </author> <title> Sequential Implementation of Lenient Programming Languages. </title> <type> Technical Report TR-417, </type> <institution> MIT Lab for Comp. Sci., </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <month> September </month> <year> 1988. </year> <type> (PhD Thesis, </type> <institution> Dept. of EECS, MIT). </institution>
Reference-contexts: It avoids synchronization waits in the same manner, and further reduces the impact of transient load imbalance, since a processor remains busy as long as any local work is available. Dynamic scheduling supports powerful parallel languages with synchronizing data access [3] or non-strict order of evaluation <ref> [27, 7] </ref>. Dataflow processors operate greedily, grabbing hold of any available useful work, rather than sitting idle.
Reference: [28] <author> K. R. Traub, D. E. Culler, and K. E. Schauser. </author> <title> Global Analysis for Partitioning Non-Strict Programs into Sequential Threads. </title> <booktitle> In Proc. of the ACM Conf. on LISP and Functional Programming, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: It has been demonstrated that direct execution of dataflow graphs is not essential to attain these goals, as dataflow graphs are equivalent to threaded instruction sets with very weak addressing modes [9, 18]. Dataflow graphs are, however, very useful in the compilation process <ref> [4, 23, 22, 25, 28] </ref>. At the machine level, the essential aspect of dataflow is efficient dynamic scheduling. Dynamic scheduling provides tolerance to communication latency, since the processor picks up other useful work rather than waiting for each response. <p> The Id90 compiler has been substantially rewritten to target TL0, the TAM assembly language, rather than a dataflow instruction set. A key step in the compilation process is partitioning the dataflow program graph into threads <ref> [22, 28] </ref>. The other new aspects of the compilation process involve management of registers and local storage in the context of dynamic scheduling and code generation for threads. <p> Thus, the task of the compiler in partitioning the dataflow graph into threads is to prove where such external dependences cannot exist. The basic analysis techniques for partitioning are described in [23, 22, 8]. In addition <ref> [28] </ref> shows how this analysis can be carried out globally. 2.4 Translation from TAM The second compilation step translates the TAM program to an executable for a specific parallel machine and addresses specifics of the target, such as the particular network interface, addressing modes, etc.
Reference: [29] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proc. of the 19th Int'l Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year> <note> (Also available as Technical Report UCB/CSD 92/675, </note> <institution> CS Div., University of California at Berkeley). </institution> <month> 23 </month>
Reference-contexts: This is the first commercial machine to provide a sufficiently accessible and efficient processor/network interface allowing a meaningful study of this kind. Our active message layer on the machine <ref> [29] </ref> imposes a per-message overhead that is an order of magnitude less than that of commercial message passing systems and approaches current hardware implementations of shared-memory and message-driven models. <p> The processors are connected as an incomplete fat-tree of degree four, so the maximum distance between processors is 6 hops and the average distance is 5.4 hops. Communication is supported by active messages <ref> [29] </ref> driving a custom network interface via the memory bus. Sending or receiving a message takes approximately 50 cycles. We focus on two application benchmark programs written in Id90.
References-found: 29

