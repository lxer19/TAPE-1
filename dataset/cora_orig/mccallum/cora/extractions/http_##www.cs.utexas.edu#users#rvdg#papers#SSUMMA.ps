URL: http://www.cs.utexas.edu/users/rvdg/papers/SSUMMA.ps
Refering-URL: http://www.cs.utexas.edu/users/rvdg/abstracts/SSUMMA.html
Root-URL: 
Email: bgrayson@pine.ece.utexas.edu  ajay@cs.utexas.edu  rvdg@cs.utexas.edu  
Title: A High Performance Parallel Strassen Implementation  
Author: Brian Grayson Ajay Pankaj Shah Robert A. van de Geijn 
Date: June 13, 1995  
Address: Austin, TX 78712  Austin, TX 78712  Austin, TX 78712  
Affiliation: Department of Electrical and Computer Engineering The University of Texas at Austin  Department of Computer Sciences The University of Texas at Austin  Department of Computer Sciences The University of Texas at Austin  
Abstract: In this paper, we give what we believe to be the first high performance parallel implementation of Strassen's algorithm for matrix multiplication. We show how under restricted conditions, this algorithm can be implemented plug compatible with standard parallel matrix multiplication algorithms. Results obtained on a large Intel Paragon system show a 10-20% reduction in execution time compared to what we believe to be the fastest standard parallel matrix multiplication implementation available at this time. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Cannon, L.E., </author> <title> A Cellular Computer to Implement the Kalman Filter Algorithm, </title> <type> Ph.D. Thesis (1969), </type> <institution> Mon-tana State University. </institution>
Reference: [2] <author> Choi, J., Dongarra, J. J., and Walker, D. W., </author> <title> "Level 3 BLAS for distributed memory concurrent computers", </title> <booktitle> CNRS-NSF Workshop on Environments and Tools for Parallel Scientific Computing, </booktitle> <address> Saint Hilaire du Touvet, France, </address> <month> Sept. </month> <pages> 7-8, </pages> <address> 1992. </address> <publisher> Elsevier Science Publishers, </publisher> <year> 1992. </year>
Reference: [3] <author> Choi, J., Dongarra, J. J., and Walker, D. W., "PUMMA: </author> <title> Parallel Universal Matrix Multiplication Algorithms on distributed memory concurrent computers," </title> <journal> Concurrency: Practice and Experience, </journal> <volume> Vol 6(7), </volume> <pages> 543-570, </pages> <year> 1994. </year>
Reference-contexts: Second, we had just developed a more efficient standard parallel matrix multiply routine (SUMMA) [17] than the library routine used by Luo and Drake (PUMMA) <ref> [3] </ref>. Our SUMMA implementation has the added benefit that it requires much less work space than PUMMA, allowing us to run larger problems. <p> Preliminary results show that this can again be done with implicit permutations. Our implementation can be easily adjusted to use other parallel matrix multiplication implementations for the lowest level multiplication. A number of implementations based on the "broadcast-multiply-role" method [7, 8] have been developed. For details see <ref> [3, 11, 12] </ref>. Acknowledgements This research was performed in part using the Intel Paragon System operated by the California Institute of Technology on behalf of the Concurrent Supercomputing Consortium. Access to this facility was provided by Intel Supercomputer Systems Division and the California Institute of Technology.
Reference: [4] <author> Coppersmith, D., and Winograd, S., </author> <title> Matrix Multiplication via Arithmetic Progressions," </title> <booktitle> in Proceedings of the Nineteenth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 1-6, </pages> <year> 1987. </year>
Reference: [5] <author> Dongarra, J. J., Du Croz, J., Hammarling, S., and Duff, I., </author> <title> "A Set of Level 3 Basic Linear Algebra Subprograms," </title> <journal> TOMS, </journal> <volume> Vol. 16, No. 1, </volume> <pages> pp. 1-16, </pages> <year> 1990. </year>
Reference-contexts: Using the Strassen method for all levels is not necessarily the most efficient method for matrix multiply. In our implementation, which uses Strassen for large cross-processor matrix multiplies, the Scalable Universal Matrix Multiplication Algorithm (SUMMA) (see appendix) for smaller cross-processor matrix multiplies, and level 3 BLAS calls <ref> [5] </ref> for local matrix multiplies.
Reference: [6] <author> Douglas, C., Heroux, M., and Slishman, G., "GEMMW: </author> <title> A Portable Level 3 BLAS Winograd Variant of Strassen's Matrix-Matrix Multiply Algorithm," </title> <journal> Journal of Computational Physics 110, </journal> <pages> pp. 1-10, </pages> <year> 1994. </year>
Reference: [7] <author> Fox, G. C., Johnson, M. A., Lyzenga, G. A., Otto, S. W., Salmon, J. K. and Walker, D. W., </author> <title> Solving Problems on Concurrent Processors, </title> <journal> Vol. </journal> <volume> 1, </volume> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1988. </year>
Reference-contexts: Preliminary results show that this can again be done with implicit permutations. Our implementation can be easily adjusted to use other parallel matrix multiplication implementations for the lowest level multiplication. A number of implementations based on the "broadcast-multiply-role" method <ref> [7, 8] </ref> have been developed. For details see [3, 11, 12]. Acknowledgements This research was performed in part using the Intel Paragon System operated by the California Institute of Technology on behalf of the Concurrent Supercomputing Consortium.
Reference: [8] <author> Fox, G., Otto, S., and Hey, A., </author> <title> "Matrix algorithms on a hypercube I: matrix multiplication," </title> <booktitle> Parallel Computing 3 (1987), </booktitle> <pages> pp 17-31. </pages>
Reference-contexts: Preliminary results show that this can again be done with implicit permutations. Our implementation can be easily adjusted to use other parallel matrix multiplication implementations for the lowest level multiplication. A number of implementations based on the "broadcast-multiply-role" method <ref> [7, 8] </ref> have been developed. For details see [3, 11, 12]. Acknowledgements This research was performed in part using the Intel Paragon System operated by the California Institute of Technology on behalf of the Concurrent Supercomputing Consortium.
Reference: [9] <author> Golub, G. H. , and C. F. Van Loan, </author> <title> Matrix Computations, </title> <publisher> Johns Hopkins University Press, </publisher> <editor> 2nd ed., </editor> <year> 1989. </year>
Reference-contexts: By partitioning A = A 11 A 12 B 21 B 22 , and C = C 11 C 12 where C kl 2 R m 2 , A kl 2 R 2 fi k k 2 , it can be shown <ref> [9, 16] </ref> that the following computations compute C = AB: P 1 = (A 11 + A 22 )(B 11 + B 22 ) P 2 = (A 21 + A 22 )B 11 P 5 = (A 11 + A 12 )B 22 P 6 = (A 21 A 11
Reference: [10] <author> Gropp, W., Lusk, E., and Skjellum, A., </author> <title> Using MPI: Portable Programming with the Message-Passing Interface, </title> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference: [11] <author> Huss-Lederman, S., Jacobson E., and Tsao, A., </author> <title> "Comparison of Scalable Parallel Matrix Multiplication Libraries," </title> <booktitle> in Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <address> Starksville, MS, </address> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: Preliminary results show that this can again be done with implicit permutations. Our implementation can be easily adjusted to use other parallel matrix multiplication implementations for the lowest level multiplication. A number of implementations based on the "broadcast-multiply-role" method [7, 8] have been developed. For details see <ref> [3, 11, 12] </ref>. Acknowledgements This research was performed in part using the Intel Paragon System operated by the California Institute of Technology on behalf of the Concurrent Supercomputing Consortium. Access to this facility was provided by Intel Supercomputer Systems Division and the California Institute of Technology.
Reference: [12] <author> Huss-Lederman, S., Jacobson, E., Tsao A., and Zhang, G., </author> <title> "Matrix Multiplication on the Intel Touchstone DELTA," </title> <journal> Concurrency: Practice and Experience, </journal> <volume> Vol. 6 (7), </volume> <month> Oct. </month> <year> 1994, </year> <pages> pp. 571-594. 9 </pages>
Reference-contexts: Preliminary results show that this can again be done with implicit permutations. Our implementation can be easily adjusted to use other parallel matrix multiplication implementations for the lowest level multiplication. A number of implementations based on the "broadcast-multiply-role" method [7, 8] have been developed. For details see <ref> [3, 11, 12] </ref>. Acknowledgements This research was performed in part using the Intel Paragon System operated by the California Institute of Technology on behalf of the Concurrent Supercomputing Consortium. Access to this facility was provided by Intel Supercomputer Systems Division and the California Institute of Technology.
Reference: [13] <author> Laderman, J., Pan, V., and Sha, X., </author> <title> "On Practical Algorithms for Accelerated Matrix Multiplication," </title> <booktitle> Linear Algebra and Its Applications, </booktitle> <pages> pp. 557-588, </pages> <year> 1992. </year>
Reference: [14] <author> Lin, C., and Snyder, L., </author> <title> "A Matrix Product Algorithm and its Comparative Performance on Hypercubes," </title> <booktitle> in Proceedings of Scalable High Performance Computing Conference, </booktitle> <editor> (Stout, Q, and M. Wolfe, eds.), </editor> <publisher> IEEE Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1992, </year> <pages> pp. 190-3. </pages>
Reference: [15] <author> Luo, Q, and Drake, J. B., </author> <title> "A Scalable Parallel Strassen's Matrix Multiply Algorithm for Distributed Memory Computers", </title>
Reference-contexts: This research was started when a paper by Luo and Drake <ref> [15] </ref> on the scalable implementation of Strassen's matrix multiply algorithm came to our attention.
Reference: [16] <author> Strassen, V., </author> <title> "Gaussian Elimination is not optimal," </title> <journal> Numer. Math. </journal> <volume> 13, </volume> <pages> pp 354-356, </pages> <note> 1969. </note> <author> [17] van de Geijn, R. and Watts, J., "SUMMA: </author> <title> Scalable Universal Matrix Multiplication Algorithm," </title> <institution> TR-95-13, Department of Computer Sciences, University of Texas, </institution> <month> April </month> <year> 1995. </year> <note> Also: LAPACK Working Note #96, </note> <institution> University of Tennessee, CS-95-286, </institution> <month> April </month> <year> 1995. </year> <month> 10 </month>
Reference-contexts: By partitioning A = A 11 A 12 B 21 B 22 , and C = C 11 C 12 where C kl 2 R m 2 , A kl 2 R 2 fi k k 2 , it can be shown <ref> [9, 16] </ref> that the following computations compute C = AB: P 1 = (A 11 + A 22 )(B 11 + B 22 ) P 2 = (A 21 + A 22 )B 11 P 5 = (A 11 + A 12 )B 22 P 6 = (A 21 A 11 <p> Moreover, if each of the multiplies in 1 are themselves recursively performed in the same way, one can bring down the order of the computation from O (n 3 ) to O (n 2:807 ) for square matrices <ref> [16] </ref>. Restriction: In the rest of this paper, we will assume that n = m = k.
References-found: 16

