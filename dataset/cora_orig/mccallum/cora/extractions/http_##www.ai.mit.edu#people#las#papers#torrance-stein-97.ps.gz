URL: http://www.ai.mit.edu/people/las/papers/torrance-stein-97.ps.gz
Refering-URL: http://www.ai.mit.edu/people/las/papers/
Root-URL: 
Email: torrance@ai.mit.edu las@ai.mit.edu  
Title: Communicating with Martians (and Robots)  
Author: Mark C. Torrance and Lynn Andrea Stein 
Address: 545 Technology Square #811 Cambridge, MA 02139 USA  
Affiliation: MIT Artificial Intelligence Laboratory  
Abstract: If you were to meet a Martian, you would probably find its way of seeing the world somewhat...alien. Yet you might be able to communicate with your new acquaintance by virtue of reference to your shared environment. Using a few shared "ground points", you might well be able to manage a passable working relationship, even accomplishing mutually beneficial tasks without necessarily having any deep understanding of the Martian's "language" or "representations." In this paper, we illustrate the success of such an approach shallow representational alignment through shared grounding with a creature whose perceptual and representational abilities are as different from our own as a Martian's might be: a sonar- and odometry-based robot. We describe a system that allows one such robot to carry on reasonably natural English conversations with a human companion about and during navigation through an unmodified and visually salient office environment. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Connell, J. H. </author> <year> 1992. </year> <title> SSS: A hybrid architecture applied to robot navigation. </title> <booktitle> In Proceedings of the 1992 IEEE International Conference on Robotics and Automation, </booktitle> <pages> 2719-2724. </pages>
Reference-contexts: Our System Our system has three main components: a reactive robot capable of simple corridor following and obstacle avoidance, natural language I/O, and a plan learning and execution system. The reactive robot is controlled by the Servo and Subsumption layers of Con-nell's SSS architecture <ref> (Connell 1992) </ref>. This is implemented in Motorola 6811 assembly language on a distributed network of 5 processors that reside on a sonar-and odometry-equipped RWI B12. On top of this, we have implemented natural language, plan learning, and plan execution systems, all in Common Lisp using the object-oriented extensions in CLOS.
Reference: <author> Knight, K., and Luk, S. K. </author> <year> 1994. </year> <title> Building a large-scale knowledge base for machine translation. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence. </booktitle> <address> Seattle, Washington: </address> <publisher> MIT Press. </publisher>
Reference-contexts: For example, if we don't share a common language, we may point at objects and give them names to align our individual representations of them. <ref> (Knight & Luk 1994) </ref> use this idea to automatically align multilingual lexicons for machine translation. This is apparently how Anne Sullivan taught Helen Keller a blind, deaf, and mute child to comprehend language (Lash 1980).
Reference: <author> Lash, J. P. </author> <year> 1980. </year> <title> Helen and Teacher: The Story of Helen Keller and Anne Sullivan Macy. </title> <publisher> Delacorte. </publisher>
Reference-contexts: This is apparently how Anne Sullivan taught Helen Keller a blind, deaf, and mute child to comprehend language <ref> (Lash 1980) </ref>. Once a portion of the shared environment is aligned in this way, other parts of the space can be bootstrapped in terms of these agreed-upon ground points. In this paper, we demonstrate the utility of this idea to solve a particular communication problem.
Reference: <author> Martin, C. </author> <year> 1993. </year> <title> The situated language project. </title> <institution> Presented at the Massachusetts Institute of Technology Artificial Intelligence Laboratory Revolving Seminar. </institution>
Reference-contexts: Much of the natural language understanding effort is devoted to precisely this task. The few robotic systems which have been coupled to language have generally shared this trait, attempting to model human-like perception and representation abilities. (See, e.g., (Nilsson 1984), <ref> (Martin 1993) </ref>.) Our work diverges signficantly from this, providing only enough of an understanding to allow the robot to perform its task.
Reference: <author> Nilsson, N. J. </author> <year> 1984. </year> <title> Shakey the robot. </title> <type> Technical Note 323, </type> <institution> SRI International, </institution> <address> Menlo Park, CA. </address>
Reference-contexts: Much of the natural language understanding effort is devoted to precisely this task. The few robotic systems which have been coupled to language have generally shared this trait, attempting to model human-like perception and representation abilities. (See, e.g., <ref> (Nilsson 1984) </ref>, (Martin 1993).) Our work diverges signficantly from this, providing only enough of an understanding to allow the robot to perform its task.
Reference: <author> Sibun, P. </author> <year> 1992. </year> <title> Generating text without trees. </title> <booktitle> Computational Intelligence 8(1) </booktitle> <pages> 100-102. </pages>
Reference-contexts: you get [from place 1 ] to place 2 ?" This last question is answered at present with less-than-satisfactory language generation that simply explains the rops that form the path; stepping this answer up to a higher level involves performing better rop merging and inference, perhaps along the lines of <ref> (Sibun 1992) </ref>. To make a mobile robot usable by people other than its designers, it must be possible to interact with it in a reasonably intuitive way. It should not be necessary to understand the robot's internal representations in order to train it in a new environment.
References-found: 6

