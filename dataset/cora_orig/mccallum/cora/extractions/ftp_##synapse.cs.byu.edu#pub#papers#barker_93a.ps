URL: ftp://synapse.cs.byu.edu/pub/papers/barker_93a.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Email: cory@axon.cs.byu.edu, martinez@cs.byu.edu  
Title: GENERALIZATION BY CONTROLLED INTERSECTION OF EXAMPLES  
Author: Cory Barker and Tony Martinez 
Address: Provo, Utah 84602  
Affiliation: Computer Science Department, Brigham Young University,  
Note: In AI'93 Australian Joint Conference on Artificial Intelligence, pp. 142-149, 1994.  1 Introduction This Research Supported in Part by the Air Force Office of Scientific Research  
Abstract: SG (Specific to General) is a network for supervised inductive learning from examples that uses ideas from neural networks and symbolic inductive learning to gain benefits of both methods. The network is built of many simple nodes that learn important features in the input space and then monitor the ability of the features to predict output values. The network avoids the exponential nature of the number of features by creating specific features for each example and then expanding those features; making them more general. Expansion of a feature terminates when it encounters another feature with contradicting outputs. Empirical evaluation of the model on real-world data has shown that the network provides good generalization performance. Convergence is accomplished within a small number of training passes. The network provides these benefits while automatically allocating and deleting nodes and without requiring user adjustment of any parameters. The network learns incrementally and operates in a parallel fashion. This paper describes a network architecture for supervised learning that combines techniques used in neural networks 1,7,8 with symbolic machine learning 3,4,6 to gain advantages of both approaches. In supervised learning the network is given a training set containing examples. Each example gives an input pattern along with the corresponding output that the network should produce when presented with the input. The task of the network is not only to converge to a representation that contains the information given by the training set, but to generalize that information so that the network will respond well to inputs that it has not been trained on. One approach to generalization is to look for important features in the input space. A feature is some subset of network inputs along with their associated values. A feature is matched when the values on the network inputs that are part of the feature are equal to the values for those inputs as given in the feature. Inputs that are not part of the feature can be any value. A feature that predicts an output with high probability is an important feature. The number of inputs contained in a feature is the order of the feature and determines the generality of the feature. A feature with few inputs is a general feature, while a feature with many inputs is a specific feature. It is impractical to monitor all possible input features because the number of features is exponential in the number of inputs. This paper proposes SG (Specific to General), a network that creates specific input features and then generalizes those features. One way SG generalizes is by combining similar specific features. If two features are similar, they are close to each other in the input space. Combining the two features by dropping inputs that are not common between the features creates a new feature that encompasses both of the original features. The new feature is general; it matches points in the input space that have not been defined by any example. This section presents an overview of the model while later sections provide detail about the system. The network is made up of many simple nodes. Each node contains the input feature that it monitors. During training, the node gathers statistics giving the discrete conditional probability of each possible output value given the input feature. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ackley, D. H., Hinton, D. E., & Sejnowski, T. J. </author> <year> (1985). </year> <title> A Learning Algorithm for Boltzmann Machines. </title> <journal> Cognitive Science, </journal> <volume> 9, </volume> <pages> 147-169. </pages>
Reference: [2] <author> Barker, J. C. </author> <year> (1993). </year> <title> Generalization by Controlled Expansion of Examples. </title> <note> In preparation. </note>
Reference: [3] <author> Michalski, R. S. </author> <year> (1983). </year> <title> A Theory and Methodology of Inductive Learning. </title> <journal> Artificial Intelligence, </journal> <volume> 20, </volume> <pages> 111-161. </pages>
Reference: [4] <author> Mitchell, T. M. </author> <year> (1982). </year> <title> Generalization as Search. </title> <journal> Artificial Intelligence, </journal> <volume> 18, </volume> <pages> 203-226 </pages>
Reference: [5] <author> Murphy, P. M. & Aha, D. W. </author> <year> (1992). </year> <title> UCI Repository of machine learning databases. </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science. </institution>
Reference: [6] <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of Decision Trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference: [7] <author> Rosenblatt, F. </author> <year> (1958). </year> <title> The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain. </title> <journal> Psychological Review, </journal> <volume> 65, </volume> <pages> 386-408. </pages>
Reference: [8] <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <year> (1986). </year> <title> Learning Internal Representations by Error Propagation. </title> <editor> In D. E. Rumelhart and J. L. McClelland (Eds.) </editor> <booktitle> Parallel Distributed Processing (Vol. 1). </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
References-found: 8

