URL: http://www.eecs.umich.edu/~zuberi/research/usenix98.ps
Refering-URL: http://www.eecs.umich.edu/~zuberi/research/
Root-URL: http://www.cs.umich.edu
Email: fzuberi,kgshing@eecs.umich.edu  
Title: An Efficient End-Host Protocol Processing Architecture for Real-Time Audio and Video Traffic  
Author: Khawar M. Zuberi and Kang G. Shin 
Address: Ann Arbor, MI 48109-2122  
Affiliation: Real-Time Computing Laboratory Department of Electrical Engineering and Computer Science The University of Michigan  
Abstract: The popularity of the Internet and the web is making real-time communication achieve a new significance. Time-critical applications such as Internet phone, video teleconferencing, and streaming audio/video are becoming increasingly common. Network bandwidth delivered to these applications is limited by host protocol processing overheads, especially the receive-side overhead (since it usually exceeds transmission overhead). We present an architecture for reducing receive-side overhead for processing real-time messages. In our scheme, all protocol processing is performed by the application threads themselves. Moreover, message data needs to be copied only once (without any hardware support from the network adapter or any restrictions on the network API). We implement UDP/IP to evaluate our architecture. Measurements show that non-data-touching overheads are reduced 13% which benefits short messages such as live audio. Data-touching overheads are reduced 20-30% which benefits long messages such as video and streaming data. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Lawton, </author> <title> "In search of real-time Internet service," </title> <journal> IEEE Computer, </journal> <volume> vol. 30, no. 11, </volume> <pages> pp. 14-16, </pages> <month> November </month> <year> 1997. </year>
Reference-contexts: 1 Introduction Real-time audio and video communication is becoming increasingly common over the Internet <ref> [1] </ref>. Applications such as the Internet phone, live broadcast of conferences and meetings, and real-time streaming of audio/video data (both live and on-demand) are rapidly gaining popularity. In order to support such delay-critical applications, both the routers as well as end-hosts must have real-time capabilities.
Reference: [2] <author> L. Zhang, S. Deering, D. Estrin, S. Shenker, and D. Zappala, "RSVP: </author> <title> A new Resource ReSerVation Protocol," </title> <journal> IEEE Network, </journal> <volume> vol. 7, no. 9, </volume> <pages> pp. 8-18, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: In order to support such delay-critical applications, both the routers as well as end-hosts must have real-time capabilities. For routers, the Resource Reservation Protocol (RSVP) <ref> [2] </ref> provides simple network resource reservation while other protocols for delivering more guaranteed performance are being proposed [3]. In this paper, we fl The work reported in this paper was supported in part by the NSF under Grant MIP-9203895, and by the ONR under Grant N00014-94-1-0229.
Reference: [3] <author> W. Feng, D. Kandlur, D. Saha, and K. Shin, </author> <title> "On providing minimum rate guarantees over the internet," </title> <type> Technical report, </type> <institution> IBM Research report RC 20618, </institution> <note> version 2, </note> <month> November </month> <year> 1997. </year>
Reference-contexts: In order to support such delay-critical applications, both the routers as well as end-hosts must have real-time capabilities. For routers, the Resource Reservation Protocol (RSVP) [2] provides simple network resource reservation while other protocols for delivering more guaranteed performance are being proposed <ref> [3] </ref>. In this paper, we fl The work reported in this paper was supported in part by the NSF under Grant MIP-9203895, and by the ONR under Grant N00014-94-1-0229.
Reference: [4] <author> C. Mercer and H. Tokuda, </author> <title> "An evaluation of priority consistency in protocol architectures," </title> <booktitle> in Proc. IEEE Conf. Local Computer Networks, </booktitle> <pages> pp. 386-398, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Predictability is needed to ensure timely processing of incoming and outgoing messages <ref> [4] </ref>. The protocol architecture must provide mechanisms to guarantee that high-priority real-time messages (such as live voice) do not get unnecessarily delayed by the processing of lower-priority or non-real-time messages. At the same time, the protocol architecture must provide for efficient execution of protocol code. <p> We present a new protocol architecture which uses the application threads to perform all network protocol processing (other than interrupt handling). Our scheme provides predictability by minimizing priority inversion <ref> [4] </ref>, reduces non-data-touching overhead by decreasing the number of context switches, and reduces scheduling overhead by keeping the number of threads to a minimum. The predictability is useful for all real-time traffic while the reduced context switch and scheduling overhead benefits applications exchanging short messages such as live audio. <p> The unpredictability arises because once a process starts processing a low-priority (incoming or outgoing) message and a high-priority one arrives, the latter cannot be handled until the former is completely processed. This leads to priority inversion <ref> [4] </ref> | a high-priority message being delayed by a low-priority one. Various techniques have been proposed to make protocol processing efficient and predictable as described next. 2.1 Increasing Efficiency The vertical or process-per-message architecture [18, 19] circumvents many of the problems associated with horizontal architectures.
Reference: [5] <author> W. chi Feng and F. Jahanian, </author> <title> "Providing VCR functionality in a constant quality video-on-demand transportation service," </title> <type> Technical Report CSE-TR 271-95, </type> <institution> University of Michigan, EECS Dept., </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: As a result, memory-bound operations (data copying, context switching, and interrupt handling) are likely to increasingly dominate protocol processing overhead. Data copying is more important when dealing with large messages such as those associated with video. The average video frame size (after compression) can be 10-15 kbytes <ref> [5] </ref>. With one frame per message, the data-touching overheads completely dominate protocol processing overhead. The situation is much different for live audio.
Reference: [6] <author> J. Scourias, </author> <title> "Overview of the Global System for Mobile Communications," </title> <address> http://ccnga.uwaterloo.ca/ js-couria/GSM/gsmreport.html. </address>
Reference-contexts: With one frame per message, the data-touching overheads completely dominate protocol processing overhead. The situation is much different for live audio. The message size can be as small as tens of bytes (as in the GSM audio encoding scheme <ref> [6] </ref> used in various Internet phones), but messages are sent once every 20-30ms [7]. With messages arriving with such high frequency, the context switching, interrupt handling, 1 and associated cache miss overheads become an impor-tant part of protocol processing overhead while data copying overheads become negligible.
Reference: [7] <author> H. Schulzrinne, </author> <title> "RTP profile for audio and video conferences with minimal control," </title> <type> RFC 1890, </type> <month> January </month> <year> 1996. </year>
Reference-contexts: The situation is much different for live audio. The message size can be as small as tens of bytes (as in the GSM audio encoding scheme [6] used in various Internet phones), but messages are sent once every 20-30ms <ref> [7] </ref>. With messages arriving with such high frequency, the context switching, interrupt handling, 1 and associated cache miss overheads become an impor-tant part of protocol processing overhead while data copying overheads become negligible. Previous research in reducing data-touching overheads has largely focused on non-real-time systems [8-11]. <p> Then, the important question is: how often can this happen under the condition that both real-time as well as non-real-time applications (such as telnet and web browsing) are receiving packets? Real-time audio and video applications run with some period T . T for audio is quite short, usually 20-30ms <ref> [7] </ref>, so audio is not a problem. Video applications usually run at 30 frames/s [27] but to conserve CPU and network bandwidth, some may run at a slower rate of 20 or even 10 frames/s, giving a T as large as 0.1s.
Reference: [8] <author> D. Banks and M. Prudence, </author> <title> "A high-performance network architecture for a PA-RISC workstation," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 11, no. 2, </volume> <pages> pp. 191-202, </pages> <month> February </month> <year> 1993. </year>
Reference: [9] <author> C. Dalton, G. Watson, D. Banks, C. Calamvokis, A. Edwards, and J. Lumley, </author> <title> "Afterburner," </title> <journal> IEEE Network, </journal> <volume> vol. 7, no. 4, </volume> <pages> pp. 36-43, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Regarding data-touching overheads (which is important for video), we make use of the periodic nature of video applications. We show that the single-copy scheme presented for non-real-time systems in <ref> [9] </ref> | which requires specialized network adapter hardware to be feasible for non-real-time systems | works well without any hardware support for multimedia applications because of their periodic nature. <p> But this not only increases overhead needed to keep both threads at the same priority, it also increases the thread count which adversely affects the overhead of real-time scheduling (which depends on total number of threads). 2.3 Single-Copy Architectures The single-copy network architecture was proposed by NA designers <ref> [9] </ref>. The idea is to design a NA with enough buffer space so that on transmission, data is copied once from user-space directly to the NA, while on reception, data stays in the NA until the application makes a receive system call. <p> In case NA buffers fill up, data has to be buffered within the kernel, leading to two data copies. This architecture was proposed for general-purpose computing, and to be effective for such applications, the NA not only needs "enough" buffers, it also needs flexible buffers. The Afterburner NA <ref> [9] </ref> uses linked lists to manage NA buffers. For both transmission and reception, it has two queues of in-use and free buffers, so it can continue operating as long as some free buffers are available. <p> We are not interested in modeling network traffic. All we want is to show that receiving even 30-40 non-real-time packets within time interval T is highly improbable and get some idea of how improbable. Since network adapters usually have 128-256 receive buffers <ref> [9, 25] </ref> | and this is likely to increase even further as memory densities increase and cost decreases | receiving even 30-40 packets within T seconds is not enough to disrupt the handling of real-time packets. For evaluation purposes, we chose to use web browsing as a representative non-real-time application. <p> By using the application threads themselves to perform protocol processing for real-time messages, we eliminated one context switch. This technique not only makes protocol processing predictable but also reduces overhead by 13% for short messages. To reduce data-touching overheads, we used the single-copy scheme <ref> [9] </ref>. We showed that for real-time messages, it can be used effectively without any hardware support from the NA, and improves overhead by 20-30%. Our protocol architecture allows standard techniques (such as in-kernel or user-level network threads) to be used for non-real-time messages.
Reference: [10] <author> P. Druschel and L. L. Peterson, "Fbufs: </author> <title> A high-bandwidth cross-domain transfer facility," </title> <booktitle> in Proc. Symposium on Operating Systems Principles, </booktitle> <pages> pp. 189-202, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Virtual memory page re-mapping is commonly used to reduce data copying costs when crossing protection boundaries <ref> [10] </ref>. This technique works well to a certain extent, especially when combined with the mbuf mechanism of BSD [26]. Mbufs are linked lists of buffers. <p> This is not the case with the BSD socket interface, where the location for incoming messages is specified by the user, not by the kernel, resulting in two data copies. Yet another problem with memory remapping is the cost of remapping pages from one domain to another. In <ref> [10] </ref> an optimization is presented in which mappings are cached to reduce overhead significantly. This works reasonably well for transmission, but works for reception only if the network protocol allows packet-level demultiplexing at the device driver level.
Reference: [11] <author> J. Kay and J. Pasquale, </author> <title> "Measurement, analysis, and improvement of UDP/IP throughput for the DECstation 5000," </title> <booktitle> in Proc. Winter USENIX, </booktitle> <pages> pp. 249-258, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: This increases the thread count which increases the scheduling overhead, so some work has been done on reducing scheduling overhead for improved performance [15]. Studies have shown that receive-side protocol processing overhead is higher than send-side overhead <ref> [11, 12, 16] </ref> and this is what limits throughput; so, here we focus on improving receive-side overhead. We present a new protocol architecture which uses the application threads to perform all network protocol processing (other than interrupt handling).
Reference: [12] <author> D. Kandlur, D. Saha, and M. Willebeek-LeMair, </author> <title> "Protocol architecture for multimedia applications over ATM networks," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 14, no. 7, </volume> <pages> pp. 1349-1359, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: Previous research in reducing data-touching overheads has largely focused on non-real-time systems [8-11]. These approaches either rely on specialized hardware, work only with certain networks (like ATM), or require changes to existing protocols in order to be effective. Work done on reducing data-touching overheads for real-time multimedia systems <ref> [12] </ref> also suffers from the same drawbacks. Most of the research on protocol architectures for real-time systems has focused on providing predictable message handling, often through the use of extra kernel or user-level threads [13, 14]. <p> This increases the thread count which increases the scheduling overhead, so some work has been done on reducing scheduling overhead for improved performance [15]. Studies have shown that receive-side protocol processing overhead is higher than send-side overhead <ref> [11, 12, 16] </ref> and this is what limits throughput; so, here we focus on improving receive-side overhead. We present a new protocol architecture which uses the application threads to perform all network protocol processing (other than interrupt handling).
Reference: [13] <author> C. Lee, K. Yoshida, C. Mercer, and R. Rajkumar, </author> <title> "Predictable communication protocol processing in Real-Time Mach," </title> <booktitle> in Proc. Real-Time Technology and Applications Symposium, </booktitle> <pages> pp. 220-229, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: Work done on reducing data-touching overheads for real-time multimedia systems [12] also suffers from the same drawbacks. Most of the research on protocol architectures for real-time systems has focused on providing predictable message handling, often through the use of extra kernel or user-level threads <ref> [13, 14] </ref>. This increases the thread count which increases the scheduling overhead, so some work has been done on reducing scheduling overhead for improved performance [15]. <p> User-level protocols were introduced to make protocol architectures more flexible. They allow new protocols to be used without modifying the kernel. However, performance of user-level protocols is lower than that of kernel-resident protocols because protecting critical sections at the user level is more expensive than inside the kernel. In <ref> [13] </ref> a new use for user-level protocols was presented: increased predictability for real-time communication. On the transmission side, all protocol processing is done by the application thread itself, so there is no priority inversion. <p> We use this fact along with the periodic nature of multimedia applications to design an efficient architecture which lowers both data-touching and non-data-touching overheads. 3.1 Achieving Predictability and Lower ing Non-Data-Touching Overheads Our protocol architecture is similar to that proposed in <ref> [13, 16] </ref>. Their architecture consists of a library implementation of network protocols. All send operations are handled by the application threads themselves using library routines (Figure 1). A special server is first used for connection setup and routing.
Reference: [14] <author> A. Mehra, A. Indiresan, and K. G. Shin, </author> <title> "Structuring communication software for quality-of-service guarantees," </title> <booktitle> in Proc. Real-Time Systems Symposium, </booktitle> <pages> pp. 130-138, </pages> <month> December </month> <year> 1996. </year> <month> 11 </month>
Reference-contexts: Work done on reducing data-touching overheads for real-time multimedia systems [12] also suffers from the same drawbacks. Most of the research on protocol architectures for real-time systems has focused on providing predictable message handling, often through the use of extra kernel or user-level threads <ref> [13, 14] </ref>. This increases the thread count which increases the scheduling overhead, so some work has been done on reducing scheduling overhead for improved performance [15]. <p> If packets get re-ordered, then packet-level demultiplexing is possible only if the network is connection-based such as ATM. In short, caching page mappings cannot work over connectionless networks such as Ethernet. The work presented in <ref> [14, 15] </ref> uses a vertical architecture to provide quality-of-service guarantees. However, their focus is on designing mechanisms to support the real-time channel abstraction [33]. They provide mechanisms for channel establishment, traffic shaping, and channel state maintenance.
Reference: [15] <author> A. Mehra, A. Indiresan, and K. G. Shin, </author> <title> "Re--source management for real-time communication: Making theory meet practice," </title> <booktitle> in Proc. Real-Time Technology and Applications Symposium, </booktitle> <pages> pp. 130-138, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: This increases the thread count which increases the scheduling overhead, so some work has been done on reducing scheduling overhead for improved performance <ref> [15] </ref>. Studies have shown that receive-side protocol processing overhead is higher than send-side overhead [11, 12, 16] and this is what limits throughput; so, here we focus on improving receive-side overhead. <p> If packets get re-ordered, then packet-level demultiplexing is possible only if the network is connection-based such as ATM. In short, caching page mappings cannot work over connectionless networks such as Ethernet. The work presented in <ref> [14, 15] </ref> uses a vertical architecture to provide quality-of-service guarantees. However, their focus is on designing mechanisms to support the real-time channel abstraction [33]. They provide mechanisms for channel establishment, traffic shaping, and channel state maintenance.
Reference: [16] <author> C. Maeda and B. Bershad, </author> <title> "Protocol service decomposition for high-performance networking," </title> <booktitle> in Proc. Symposium on Operating Systems Principles, </booktitle> <pages> pp. 244-255, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: This increases the thread count which increases the scheduling overhead, so some work has been done on reducing scheduling overhead for improved performance [15]. Studies have shown that receive-side protocol processing overhead is higher than send-side overhead <ref> [11, 12, 16] </ref> and this is what limits throughput; so, here we focus on improving receive-side overhead. We present a new protocol architecture which uses the application threads to perform all network protocol processing (other than interrupt handling). <p> Even with these optimizations, protocol architecture overhead is significant. Other optimizations have been proposed, such as giving applications direct (but 2 controlled) access to the network adapter (NA) [20], but these schemes usually require hardware support from the NA. 2.2 Increasing Predictability In <ref> [16, 21] </ref>, the concept of user-level network protocols is presented. Most of the protocol processing is done by user-level threads while certain special functionality (such as connection setup and retrans-missions) is handled by a special server process. <p> We use this fact along with the periodic nature of multimedia applications to design an efficient architecture which lowers both data-touching and non-data-touching overheads. 3.1 Achieving Predictability and Lower ing Non-Data-Touching Overheads Our protocol architecture is similar to that proposed in <ref> [13, 16] </ref>. Their architecture consists of a library implementation of network protocols. All send operations are handled by the application threads themselves using library routines (Figure 1). A special server is first used for connection setup and routing. <p> Advantages: Our receive-side architecture not only improves predictability but also reduces overhead. Regarding performance, one context switch is saved compared to architectures which use special user-level or kernel-level threads to perform receive-side protocol processing. For such architectures (x-kernel [19], library-based architectures <ref> [16, 21] </ref>, etc.), incoming packets cause the network thread to wake up and process the packet after which the thread blocks as shown in Figure 4.
Reference: [17] <author> K. M. Zuberi and K. G. Shin, "EMERALDS: </author> <title> A microkernel for embedded real-time systems," </title> <booktitle> in Proc. Real-Time Technology and Applications Symposium, </booktitle> <pages> pp. 241-249, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: Moreover, our protocol architecture does not rely on any special hardware and is independent of the type of underlying network (ATM, Ethernet, etc.) For the purpose of evaluation, we have implemented UDP/IP using our protocol architecture within the EMERALDS real-time operating system <ref> [17] </ref>. EMERALDS is an OS designed for use in embedded applications such as webTV and digital cellular phones. For our implementation, EMERALDS provides basic OS functionality such as real-time task scheduling, interrupt handling, context switching, and semaphore sig naling. <p> This already happens for the send side. We modify the architecture to enable this feature on the receive side as well. In EMERALDS, when an application thread makes a system call, the thread actually enters the kernel and executes the code for that system call <ref> [17] </ref>. We use this feature of EMERALDS for protocol processing. In our implementation, all protocol code lies within the kernel. To send a message, the application thread makes a socket system call.
Reference: [18] <author> D. C. Schmidt and T. Suda, </author> <title> "Transport system architecture services for high-performance communications systems," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 11, no. 4, </volume> <pages> pp. 489-506, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: In some cases, one process may perform both send and receive processing, while in other cases, transmission and reception may be handled by separate processes. This so-called horizontal or process-per-layer architecture <ref> [18] </ref> is not only inefficient, it also makes protocol processing unpredictable. The inefficiency arises from having to copy data across address spaces and having to switch contexts between processes. <p> This leads to priority inversion [4] | a high-priority message being delayed by a low-priority one. Various techniques have been proposed to make protocol processing efficient and predictable as described next. 2.1 Increasing Efficiency The vertical or process-per-message architecture <ref> [18, 19] </ref> circumvents many of the problems associated with horizontal architectures. In this scheme, the kernel maintains a pool of threads (lightweight processes). One thread is assigned to each outgoing and incoming message. This thread shepherds the message through the entire protocol stack.
Reference: [19] <author> N. C. Hutchinson and L. L. Peterson, </author> <title> "The x-kernel: An architecture for implementing network protocols," </title> <journal> IEEE Trans. Software Engineering, </journal> <volume> vol. 17, no. 1, </volume> <pages> pp. 1-13, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: This leads to priority inversion [4] | a high-priority message being delayed by a low-priority one. Various techniques have been proposed to make protocol processing efficient and predictable as described next. 2.1 Increasing Efficiency The vertical or process-per-message architecture <ref> [18, 19] </ref> circumvents many of the problems associated with horizontal architectures. In this scheme, the kernel maintains a pool of threads (lightweight processes). One thread is assigned to each outgoing and incoming message. This thread shepherds the message through the entire protocol stack. <p> Advantages: Our receive-side architecture not only improves predictability but also reduces overhead. Regarding performance, one context switch is saved compared to architectures which use special user-level or kernel-level threads to perform receive-side protocol processing. For such architectures (x-kernel <ref> [19] </ref>, library-based architectures [16, 21], etc.), incoming packets cause the network thread to wake up and process the packet after which the thread blocks as shown in Figure 4.
Reference: [20] <author> P. Druschel, L. L. Peterson, and B. Davie, </author> <title> "Experiences with a high-speed network adaptor: A software perspective," </title> <booktitle> in SIGCOMM, </booktitle> <pages> pp. 2-13, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: Two copies are also needed on the receive side. Even with these optimizations, protocol architecture overhead is significant. Other optimizations have been proposed, such as giving applications direct (but 2 controlled) access to the network adapter (NA) <ref> [20] </ref>, but these schemes usually require hardware support from the NA. 2.2 Increasing Predictability In [16, 21], the concept of user-level network protocols is presented.
Reference: [21] <author> C. Thekkath, T. Nguyen, E. Moy, and E. La-zowska, </author> <title> "Implementing network protocols at user level," </title> <booktitle> in SIGCOMM, </booktitle> <pages> pp. 64-73, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: Even with these optimizations, protocol architecture overhead is significant. Other optimizations have been proposed, such as giving applications direct (but 2 controlled) access to the network adapter (NA) [20], but these schemes usually require hardware support from the NA. 2.2 Increasing Predictability In <ref> [16, 21] </ref>, the concept of user-level network protocols is presented. Most of the protocol processing is done by user-level threads while certain special functionality (such as connection setup and retrans-missions) is handled by a special server process. <p> Advantages: Our receive-side architecture not only improves predictability but also reduces overhead. Regarding performance, one context switch is saved compared to architectures which use special user-level or kernel-level threads to perform receive-side protocol processing. For such architectures (x-kernel [19], library-based architectures <ref> [16, 21] </ref>, etc.), incoming packets cause the network thread to wake up and process the packet after which the thread blocks as shown in Figure 4.
Reference: [22] <author> J. Mogul, R. Rashid, and M. Accetta, </author> <title> "The packet filter: An efficient mechanism for user-level network code," </title> <booktitle> in Proc. Symposium on Operating Systems Principles, </booktitle> <pages> pp. 39-51, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: Then the message is passed to the kernel-resident device driver which performs the actual transmission over the network. On the receive side, incoming packets trigger interrupts which cause the device driver to execute. The packet filter <ref> [22, 23] </ref> is invoked which passes the message to the final destination process. Within each process is a special network thread which receives messages for threads in that address space. It performs protocol processing and keeps the message until the final destination thread makes a call to receive it. <p> A special server is first used for connection setup and routing. After the connection is set up, the user thread library routines perform all protocol processing, fragment messages into packets, and pass packets to the device driver for transmission. For reception (Figure 2), the packet filter <ref> [22, 23] </ref> passes packets on to a special network thread which performs protocol processing, reassembles the message, and holds it until the application thread makes the receive call. IPC is used to transfer messages from the network thread to the application thread. <p> In [10] an optimization is presented in which mappings are cached to reduce overhead significantly. This works reasonably well for transmission, but works for reception only if the network protocol allows packet-level demultiplexing at the device driver level. Low-level message demultiplexing is quite common by means of packet filters <ref> [22] </ref> but it requires the first packet of a message to arrive before the message's final destination can be determined. If packets get re-ordered, then packet-level demultiplexing is possible only if the network is connection-based such as ATM.
Reference: [23] <author> D. Engler and M. F. Kaashoek, "DPF: </author> <title> Fast, flexible message demultiplexing using dynamic code generation," </title> <booktitle> in SIGCOMM, </booktitle> <pages> pp. 53-59, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: Then the message is passed to the kernel-resident device driver which performs the actual transmission over the network. On the receive side, incoming packets trigger interrupts which cause the device driver to execute. The packet filter <ref> [22, 23] </ref> is invoked which passes the message to the final destination process. Within each process is a special network thread which receives messages for threads in that address space. It performs protocol processing and keeps the message until the final destination thread makes a call to receive it. <p> A special server is first used for connection setup and routing. After the connection is set up, the user thread library routines perform all protocol processing, fragment messages into packets, and pass packets to the device driver for transmission. For reception (Figure 2), the packet filter <ref> [22, 23] </ref> passes packets on to a special network thread which performs protocol processing, reassembles the message, and holds it until the application thread makes the receive call. IPC is used to transfer messages from the network thread to the application thread. <p> We also added our own packet filter rather than using the high-overhead BSD packet filter. For simplicity, we implemented a UDP/IP-specific filter. Interested readers are referred to <ref> [23] </ref> for more generalized high-performance packet filters. 4.2 Performance Improvements We sent datagram messages from one processor to another and measured the total overhead of receive-side protocol processing including interrupt handling and all relevant context switches.
Reference: [24] <author> L. Sha, R. Rajkumar, and J. Lehoczky, </author> <title> "Priority inheritance protocols: An approach to real-time synchronization," </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. 39, no. 3, </volume> <pages> pp. 1175-1198, </pages> <year> 1990. </year>
Reference-contexts: But this can lead to unbounded priority inversion if the destination and network threads are at different priorities. One solution can be to have a separate network thread per application thread and have the kernel ensure that if the priority of the latter changes (e.g., due to priority inheritance <ref> [24] </ref>), the priority of the former also gets adjusted.
Reference: [25] <institution> Am79C90 CMOS Local Area Network Controller for Ethernet (C-LANCE), Advanced Micro Devices, Inc., </institution> <year> 1994. </year>
Reference-contexts: For both transmission and reception, it has two queues of in-use and free buffers, so it can continue operating as long as some free buffers are available. This is a more complicated and expensive NA design than common NAs such as LANCE <ref> [25] </ref>. LANCE uses circular queues of buffers. For transmission, it transmits messages from the ring until it reaches a free buffer at which point it stops. For reception, it fills buffers in the receive ring until it reaches a filled buffer at which point it starts dropping packets. <p> We are not interested in modeling network traffic. All we want is to show that receiving even 30-40 non-real-time packets within time interval T is highly improbable and get some idea of how improbable. Since network adapters usually have 128-256 receive buffers <ref> [9, 25] </ref> | and this is likely to increase even further as memory densities increase and cost decreases | receiving even 30-40 packets within T seconds is not enough to disrupt the handling of real-time packets. For evaluation purposes, we chose to use web browsing as a representative non-real-time application.
Reference: [26] <author> S. Le*er, M. McKusick, M. Karels, and J. Quar-terman, </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System, </title> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <year> 1989. </year>
Reference-contexts: In our scheme, packet arrivals trigger interrupts. The device driver executes and forms an mbuf chain of the packets. Mbufs are linked lists of buffers which allow easy addition and removal of headers (see <ref> [26] </ref> for details). Data is left in the NA and the mbufs are made to point to that data. <p> Virtual memory page re-mapping is commonly used to reduce data copying costs when crossing protection boundaries [10]. This technique works well to a certain extent, especially when combined with the mbuf mechanism of BSD <ref> [26] </ref>. Mbufs are linked lists of buffers. To add a header, all that needs to be done is allocate a memory buffer (anywhere in the address space), put the header data in it, and link this buffer at the head of the mbuf chain.
Reference: [27] <author> A. Reibman and A. Berger, </author> <title> "Traffic descriptions for VBR video teleconferencing over ATM networks," </title> <journal> IEEE/ACM Trans. Networking, </journal> <volume> vol. 3, no. 3, </volume> <pages> pp. 329-339, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: T for audio is quite short, usually 20-30ms [7], so audio is not a problem. Video applications usually run at 30 frames/s <ref> [27] </ref> but to conserve CPU and network bandwidth, some may run at a slower rate of 20 or even 10 frames/s, giving a T as large as 0.1s. This is the maximum time messages for a video application have to stay in NA buffers.
Reference: [28] <author> V. Frost and B. Melamed, </author> <title> "Traffic modeling for telecommunications networks," </title> <journal> IEEE Communications Mag., </journal> <volume> vol. 33, </volume> <pages> pp. 70-80, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Interactive applications are characterized by infrequent exchange of relatively short messages whereas bulk transfer applications tend to have a short request message in one direction followed by a possibly large-sized response in the reverse direction. In the past, Poisson processes have been used to model packet arrivals <ref> [28] </ref>. This simplifies network traffic analysis because of the attractive theoretical properties of Poisson processes. However, various studies have shown that wide-area network traffic is too bursty to be correctly modeled by a Poisson process [29, 30].
Reference: [29] <author> P. B. Danzig, S. Jamin, R. Caceres, D. Mitzel, and D. Estrin, </author> <title> "An empirical workload model for driving wide-area TCP/IP network simulations," </title> <journal> Journal of Internetworking, </journal> <volume> vol. 3, no. 1, </volume> <pages> pp. 1-26, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: In the past, Poisson processes have been used to model packet arrivals [28]. This simplifies network traffic analysis because of the attractive theoretical properties of Poisson processes. However, various studies have shown that wide-area network traffic is too bursty to be correctly modeled by a Poisson process <ref> [29, 30] </ref>. Telnet arrivals have been modeled by a Pareto distribution [30], but only empirical models exist for FTP [29] and web browsing [31, 32]. This precludes any closed-form derivation of non-real-time packet arrival distributions. <p> However, various studies have shown that wide-area network traffic is too bursty to be correctly modeled by a Poisson process [29, 30]. Telnet arrivals have been modeled by a Pareto distribution [30], but only empirical models exist for FTP <ref> [29] </ref> and web browsing [31, 32]. This precludes any closed-form derivation of non-real-time packet arrival distributions.
Reference: [30] <author> V. Paxson and S. Floyd, </author> <title> "Wide area traffic: The failure of poisson modeling," </title> <journal> IEEE/ACM Trans. Networking, </journal> <volume> vol. 3, no. 3, </volume> <pages> pp. 226-244, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: In the past, Poisson processes have been used to model packet arrivals [28]. This simplifies network traffic analysis because of the attractive theoretical properties of Poisson processes. However, various studies have shown that wide-area network traffic is too bursty to be correctly modeled by a Poisson process <ref> [29, 30] </ref>. Telnet arrivals have been modeled by a Pareto distribution [30], but only empirical models exist for FTP [29] and web browsing [31, 32]. This precludes any closed-form derivation of non-real-time packet arrival distributions. <p> This simplifies network traffic analysis because of the attractive theoretical properties of Poisson processes. However, various studies have shown that wide-area network traffic is too bursty to be correctly modeled by a Poisson process [29, 30]. Telnet arrivals have been modeled by a Pareto distribution <ref> [30] </ref>, but only empirical models exist for FTP [29] and web browsing [31, 32]. This precludes any closed-form derivation of non-real-time packet arrival distributions.
Reference: [31] <author> C. Cunha, A. Bestavros, and M. Crovella, </author> <title> "Characteristics of WWW client-based traces," </title> <type> Technical Report BU-CS-95-010, </type> <institution> Boston University, Computer Science Department, </institution> <year> 1995. </year>
Reference-contexts: However, various studies have shown that wide-area network traffic is too bursty to be correctly modeled by a Poisson process [29, 30]. Telnet arrivals have been modeled by a Pareto distribution [30], but only empirical models exist for FTP [29] and web browsing <ref> [31, 32] </ref>. This precludes any closed-form derivation of non-real-time packet arrival distributions. <p> Moreover, with the advent of high-capacity hard disk drives, NFS use is likely to decline. Measurements of web traffic have shown that retrieval of even small web pages take more than 2 seconds <ref> [31] </ref>. This is the time needed to look up the remote host's DNS entry and establish TCP connections. After this initial phase data transfer begins at the rate of 1 byte per 90-100s [31]. Most web pages are relatively small-sized. Measurements in [31] show most pages to be 256-512 bytes, but <p> of web traffic have shown that retrieval of even small web pages take more than 2 seconds <ref> [31] </ref>. This is the time needed to look up the remote host's DNS entry and establish TCP connections. After this initial phase data transfer begins at the rate of 1 byte per 90-100s [31]. Most web pages are relatively small-sized. Measurements in [31] show most pages to be 256-512 bytes, but with the increasing use of in-line images, this is likely to increase. <p> small web pages take more than 2 seconds <ref> [31] </ref>. This is the time needed to look up the remote host's DNS entry and establish TCP connections. After this initial phase data transfer begins at the rate of 1 byte per 90-100s [31]. Most web pages are relatively small-sized. Measurements in [31] show most pages to be 256-512 bytes, but with the increasing use of in-line images, this is likely to increase.
Reference: [32] <author> M. Crovella and A. Bestavros, </author> <title> "Self-similarity in world wide web traffic evidence and possible causes," </title> <booktitle> in Proc. SIGMETRICS, </booktitle> <pages> pp. 160-169, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: However, various studies have shown that wide-area network traffic is too bursty to be correctly modeled by a Poisson process [29, 30]. Telnet arrivals have been modeled by a Pareto distribution [30], but only empirical models exist for FTP [29] and web browsing <ref> [31, 32] </ref>. This precludes any closed-form derivation of non-real-time packet arrival distributions.
Reference: [33] <author> D. D. Kandlur, K. G. Shin, and D. Ferrari, </author> <title> "Real-time communication in multi-hop networks," </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> vol. 5, no. 10, </volume> <pages> pp. 1044-1056, </pages> <month> October </month> <year> 1994. </year> <month> 12 </month>
Reference-contexts: In short, caching page mappings cannot work over connectionless networks such as Ethernet. The work presented in [14, 15] uses a vertical architecture to provide quality-of-service guarantees. However, their focus is on designing mechanisms to support the real-time channel abstraction <ref> [33] </ref>. They provide mechanisms for channel establishment, traffic shaping, and channel state maintenance. As an optimization, they present the process-per-connection model which is the same as process-per-message except that it uses fewer message handling threads and these threads are only partially preemptible.
References-found: 33

