URL: ftp://publications.ai.mit.edu/ai-publications/1500-1999/AIM-1536.ps.Z
Refering-URL: http://www.ai.mit.edu/projects/cbcl/web-pis/poggio/memos.html
Root-URL: 
Email: email: beymer@ai.mit.edu, tp@ai.mit.edu  
Title: Face Recognition From One Example View  
Author: David Beymer and Tomaso Poggio 
Note: Copyright c Massachusetts Institute of Technology, 1995  
Date: 1536 September, 1995  121  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY  
Pubnum: A.I. Memo No.  C.B.C.L. Paper No.  
Abstract: To create a pose-invariant face recognizer, one strategy is the view-based approach, which uses a set of example views at different poses. But what if we only have one example view available, such as a scanned passport photo can we still recognize faces under different poses? Given one example view at a known pose, it is still possible to use the view-based approach by exploiting prior knowledge of faces to generate virtual views, or views of the face as seen from different poses. To represent prior knowledge, we use 2D example views of prototype faces under different rotations. We will develop example-based techniques for applying the rotation seen in the prototypes to essentially "rotate" the single real view which is available. Next, the combined set of one real and multiple virtual views is used as example views in a view-based, pose-invariant face recognizer. Our experiments suggest that for expressing prior knowledge of faces, 2D example-based approaches should be considered alongside the more standard 3D modeling techniques. This report describes research done at the Artificial Intelligence Laboratory and within the Center for Biological and Compu- tational Learning. This research is sponsored by grants from the Office of Naval Research under contracts N00014-91-J-1270 and N00014-92-J-1879; by a grant from the National Science Foundation under contract ASC-9217041. Support for the A.I. Laboratory's artificial intelligence research is provided by ONR contract N00014-91-J-4038. D. Beymer is supported by a Howard Hughes Doctoral Fellowship from the Hughes Aircraft Company. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Andrew C. Aitchison and Ian Craw. </author> <title> Synthetic im-ages of faces an approach to model-based face recognition. </title> <booktitle> In Proc. British Machine Vision Conference, </booktitle> <pages> pages 226-232, </pages> <year> 1991. </year>
Reference-contexts: Generic 3D models of the human face can be used to predict the appearance of a face under different pose- expression-lighting parameters. For synthesizing images of faces, 3D facial models have been explored in the computer graphics, computer vision, and model-based image coding communities (Aitchison and Craw <ref> [1] </ref>, Kang, Chen, and Hsu [24], Essa and Pentland [19], Akimoto, Suennaga, and Wallace [3], Waters and Terzopoulos [47], Aizawa, Harashima, and Saito [2]). In the 3D technique, face shape is represented either by a polygonal model or by a more complicated multilayer mesh that simulates tissue.
Reference: [2] <author> K. Aizawa, H. Harashima, and T. Saito. </author> <title> Modelbased analysis synthesis image coding (MBASIC) system for a person's face. Signal Processing: </title> <journal> Image Communication, </journal> <volume> 1 </volume> <pages> 139-152, </pages> <year> 1989. </year>
Reference-contexts: For synthesizing images of faces, 3D facial models have been explored in the computer graphics, computer vision, and model-based image coding communities (Aitchison and Craw [1], Kang, Chen, and Hsu [24], Essa and Pentland [19], Akimoto, Suennaga, and Wallace [3], Waters and Terzopoulos [47], Aizawa, Harashima, and Saito <ref> [2] </ref>). In the 3D technique, face shape is represented either by a polygonal model or by a more complicated multilayer mesh that simulates tissue.
Reference: [3] <author> Takaaki Akimoto, Yasuhito Suenaga, and Richard S. Wallace. </author> <title> Automatic creation of 3D facial models. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 13(5) </volume> <pages> 16-22, </pages> <year> 1993. </year>
Reference-contexts: For synthesizing images of faces, 3D facial models have been explored in the computer graphics, computer vision, and model-based image coding communities (Aitchison and Craw [1], Kang, Chen, and Hsu [24], Essa and Pentland [19], Akimoto, Suennaga, and Wallace <ref> [3] </ref>, Waters and Terzopoulos [47], Aizawa, Harashima, and Saito [2]). In the 3D technique, face shape is represented either by a polygonal model or by a more complicated multilayer mesh that simulates tissue.
Reference: [4] <author> Robert J. Baron. </author> <title> Mechanisms of human facial recognition. </title> <journal> International Journal of Man Machine Studies, </journal> <volume> 15 </volume> <pages> 137-178, </pages> <year> 1981. </year>
Reference-contexts: 1 Introduction Existing work in face recognition has demonstrated good recognition performance on frontal, expressionless views of faces with controlled lighting (see Baron <ref> [4] </ref>, Turk and Pentland [48], Bichsel [11], Brunelli and Poggio [14], and Gilbert and Yang [20]). One of the key remaining problems in face recognition is to handle the variability in appearance due to changes in pose, expression, and lighting conditions.
Reference: [5] <author> Thaddeus Beier and Shawn Neely. </author> <title> Feature-based image metamorphosis. </title> <booktitle> In SIGGRAPH '92 Proceedings, </booktitle> <pages> pages 35-42, </pages> <address> Chicago, IL, </address> <year> 1992. </year>
Reference-contexts: Example sparse data interpolation techniques include using splines (Litwinowicz and Williams [28], Wol- berg [54]), radial basis functions (Reisfeld, Arad, and Yeshurun [40]), and inverse weighted distance metrics (Beier and Neely <ref> [5] </ref>). If a pixelwise representation is being used for shape in the first place, such as one derived from optical flow, then texture mapping or data interpolation techniques can be avoided. For our vectorized representation, we have chosen a dense, pixelwise set of features. <p> The manual technique is borrowed from Beier and Neely's morphing technique in computer graphics <ref> [5] </ref>. In their technique, a sparse set of corresponding line segment features manually placed on images i a and i b drive pixelwise correspondence between the two images (see Fig. 5). <p> The features on the left were manually located, and the features on the right were automatically located using the vectorizer. sides of the face. Given these sets of correspondences, the interpolation method from Beier and Neely <ref> [5] </ref> (see section 5.1.1) is used to interpolate the correspondences to define a dense, pixelwise mapping from the prototype to novel face. Figures 11 and 12 show example virtual views generated using prototype A with the real view in the center. <p> Since the t p j 's can be put into correspondence manually in an off-line step (using the interpolation technique of Beier and Neely <ref> [5] </ref>), the primary difficulty of this step is in converting i n into its shape free representation t n . Since i n is an m4 view of the face, this step means finding correspondence between i n and view m4's standard face shape.
Reference: [6] <author> James R. Bergen, P. Anandan, Keith J. Hanna, and Rajesh Hingorani. </author> <title> Hierarchical model-based motion estimation. </title> <booktitle> In Proceedings of the European Conference on Computer Vision, </booktitle> <pages> pages 237-252, </pages> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: While this technique al <br>- 7 ways works, it is manual. Ideally, we would like some-thing automatic, which leads us to the next two techniques. The optical flow technique uses the gradient-based, hierarchical method of Bergen and Hingorani [7] (also see Lucas and Kanade [29], Bergen, et al. <ref> [6] </ref>). Before applying optical flow, face images are brought into rough registration using the eyes, which were located manually. Optical flow is useful for computing correspondence among different rotated views of the same prototype.
Reference: [7] <author> J.R. Bergen and R. Hingorani. </author> <title> Hierarchical motionbased frame rate conversion. </title> <type> Technical report, </type> <institution> David Sarnoff Research Center, Princeton, </institution> <address> New Jersey, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Com- pared to a sparser representation, the pixelwise representation increases the difficulty of finding correspondences. However, we have found that a standard optical flow algorithm <ref> [7] </ref>, preceded by normalization based on the eye locations, can do a good job at automatically computing dense pixelwise correspondences. After defining one image as a "reference" image, the (x; y) locations of feature points of a new image are computed by finding optical flow between the two images. <p> While this technique al <br>- 7 ways works, it is manual. Ideally, we would like some-thing automatic, which leads us to the next two techniques. The optical flow technique uses the gradient-based, hierarchical method of Bergen and Hingorani <ref> [7] </ref> (also see Lucas and Kanade [29], Bergen, et al. [6]). Before applying optical flow, face images are brought into rough registration using the eyes, which were located manually. Optical flow is useful for computing correspondence among different rotated views of the same prototype.
Reference: [8] <author> D. Beymer, A. Shashua, and T. Poggio. </author> <title> Example based image analysis and synthesis. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1431, </pages> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1993. </year>
Reference-contexts: As indicated by the grey arrow, correspondences are measured relative to the reference face i std at standard shape. This relative shape representation has been used by Beymer, Shashua, and Poggio <ref> [8] </ref> in an example-based approach to image analysis and synthesis. 2.2 Texture Our texture vector is a geometrically normalized version of the image i a . That is, the geometrical differences among face images are factored out by warping the images to a common reference shape. <p> The implication of these problems is that the expense of multiple prototypes is probably not justified; one is probably better off using just one or a few prototypes. In earlier work aimed primarily at computer graphics <ref> [8] </ref>, we demonstrated parallel deformation for transformations from neutral to smiling expressions. 7.4 Future work For future work on our approach to virtual views, we plan to use multiple prototypes for generating virtual shape.
Reference: [9] <author> David Beymer. </author> <title> Vectorizing face images by interleav-ing shape and texture computations. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1537, </pages> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1995. </year>
Reference-contexts: Thus the shape vector of the new image, really a "relative" shape, is described by a flow or a vector field of correspondences relative to a standard reference shape. Our face vectorizer (see Beymer <ref> [9] </ref>), which uses optical flow as a subroutine, is also used to automatically compute the vectorized representation. Optical flow matches features in the two frames using the local grey level structure of the images. <p> It works for interperson correspondence when the two people are similar enough in grey-level appearance, but this does not happen frequently enough to be useful. Finally, our image vectorizer is a new method for computing pixelwise correspondence between an input and an "average" face shape y std . Beymer <ref> [9] </ref> provides the details; here we only set the problem up and sketch the solution. <p> As the interper- son correspondences are difficult to compute, we evaluated two techniques for establishing feature correspondence: labeling features manually on both faces, and using our face vectorizer (see section 5.1.1 and Beymer <ref> [9] </ref>) to automatically locate features. More will be said about our use of these two approaches shortly. <p> Approach for interperson correspondence. In both the manual and automatic approaches, interperson correspondences are driven by the line segment features shown in Fig. 10. The automatic segments shown on the right were located using our face vec- torizer from Beymer <ref> [9] </ref>. The manual segments on the left include some additional features not returned by the vectorizer, especially around the 9 example manual segments example automatic segments correspondences are driven by the segment features shown in the figure. <p> Since i n is an m4 view of the face, this step means finding correspondence between i n and view m4's standard face shape. Let this standard shape be denoted as y std . Our image vectorizer (Beymer <ref> [9] </ref>) is used to solve for the correspondences y std nstd between i n and standard shape y std .
Reference: [10] <author> David J. Beymer. </author> <title> Face recognition under varying pose. </title> <booktitle> In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 756-761, </pages> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: One of the key remaining problems in face recognition is to handle the variability in appearance due to changes in pose, expression, and lighting conditions. There has been some recent work in this direction, such as pose-invariant recognizers (Pentland, et. al. [34], Beymer <ref> [10] </ref>) and deformable template approaches (Manjunath, et. al. [30]). In addition to recognition, richer models for faces have been studied for analyzing varying illumination (Hallinan [22]) and expression (Yacoob and Davis [55], Essa and Pentland [19]). <p> The full set of views that we would ultimately like to have for our view-based face recognizer are the set of 15 example views shown in Fig. 3 and originally used in the view- based recognizer of Beymer <ref> [10] </ref>. These views evenly sample the two rotation angles out of the image plane. While Fig. 3 shows 15 real views, in virtual views we assume that only view m4 is available and we synthesize the remaining 14 example views. <p> The only difference between the views of different people at a fixed pose will be their texture. 6 Experimental results In this section we report the recognition rates obtained when virtual views were used in our view-based recog- nizer <ref> [10] </ref>. 6.1 View-based recognizer In our view-based face recognizer [10], the 15 example views of Fig. 3 are stored for each person to handle pose invariance. To recognize an input view, our recognizer uses a strategy of registering the input with the example views followed by template matching. <p> The only difference between the views of different people at a fixed pose will be their texture. 6 Experimental results In this section we report the recognition rates obtained when virtual views were used in our view-based recog- nizer <ref> [10] </ref>. 6.1 View-based recognizer In our view-based face recognizer [10], the 15 example views of Fig. 3 are stored for each person to handle pose invariance. To recognize an input view, our recognizer uses a strategy of registering the input with the example views followed by template matching. <p> Lighting for all views is frontal and facial expression is neutral. Table 1 shows recognition rates for parallel deformation for the different prototypes and for manual vs. automatic features. As with the experiments with real views in Beymer <ref> [10] </ref>, the recognition rates were recorded for a forced choice scenario the recognizer always reports the best match. <p> These parameters had yielded the best recognition rates for real views in interperson prototype correspondence A B C manual 84.5% 83.9% 83.9% auto 85.2% 84.0% 83.4% Table 1: Recognition rates for parallel deformation for the different prototypes and for manual vs. automatic features. Beymer <ref> [10] </ref>. The results were fairly consistent, with a mean recognition rate of 84.1% and a standard deviation of only 0.6%. Automatic feature correspondence on average was as good as the manual correspondences, which was a good result for the face vectorizer. <p> Fig. 16 summarizes our experiments with using real and virtual views in the recognizer. Starting on the right, we repeat the result from Beymer <ref> [10] </ref> where we use 15 real views per person. This recognition rate of 98.7% presents a "best case" scenario for virtual views. The real views case is followed by parallel deformation, which gives a recognition rate of 85.2% for prototype A and automatic interperson correspondences.
Reference: [11] <author> Martin Bichsel. </author> <title> Strategies of Robust Object Recognition for the Automatic Identification of Human Faces. </title> <type> PhD thesis, </type> <institution> ETH, </institution> <address> Zurich, </address> <year> 1991. </year>
Reference-contexts: 1 Introduction Existing work in face recognition has demonstrated good recognition performance on frontal, expressionless views of faces with controlled lighting (see Baron [4], Turk and Pentland [48], Bichsel <ref> [11] </ref>, Brunelli and Poggio [14], and Gilbert and Yang [20]). One of the key remaining problems in face recognition is to handle the variability in appearance due to changes in pose, expression, and lighting conditions.
Reference: [12] <author> Vicki Bruce. </author> <title> Changing faces: Visual and non-visual coding processes in face recognition. </title> <journal> British Journal of Psychology, </journal> <volume> 73 </volume> <pages> 105-116, </pages> <year> 1982. </year>
Reference-contexts: Moses, Ull- man, and Edelman [32] have performed this experiment using testing views at a variety of poses and lighting conditions. While high recognition rates were observed in the subjects (97%), the subjects were only asked to discriminate between three different people. Bruce <ref> [12] </ref> performs a similar experiment where the subject is asked whether a face had appeared during training, and detection rates go down to either 76% or 60%, depending on the amount of pose/expression difference between the testing and training views.
Reference: [13] <author> Roberto Brunelli. </author> <title> Estimation of pose and illuminant direction for face processing. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1499, </pages> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1994. </year>
Reference-contexts: Overall, the novel face texture is modulated by the changes in the prototype lighting, an approach that has been explored by Brunelli <ref> [13] </ref>. 7.3.2 Expression In this case, the prototypes are fixed in pose and lighting but differ in expression, with the standard view being, say, a neutral expression and the virtual view being a smile, frown, etc.
Reference: [14] <author> Roberto Brunelli and Tomaso Poggio. </author> <title> Face recog-nition: Features versus templates. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 15(10) </volume> <pages> 1042-1052, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Existing work in face recognition has demonstrated good recognition performance on frontal, expressionless views of faces with controlled lighting (see Baron [4], Turk and Pentland [48], Bichsel [11], Brunelli and Poggio <ref> [14] </ref>, and Gilbert and Yang [20]). One of the key remaining problems in face recognition is to handle the variability in appearance due to changes in pose, expression, and lighting conditions.
Reference: [15] <author> T.F. Cootes and C.J. Taylor. </author> <title> Active shape models - `Smart snakes'. </title> <editor> In David Hogg and Roger Boyle, editors, </editor> <booktitle> Proc. British Machine Vision Conference, </booktitle> <pages> pages 266-275. </pages> <publisher> Springer Verlag, </publisher> <year> 1992. </year>
Reference-contexts: This vectorized representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman [37]), the linear combinations approach to recognition (Ullman and Basri [49], Pog- gio [35]), active shape models (Cootes and Taylor <ref> [15] </ref>, Cootes, et al. [16]) and face recognition (Craw and Cameron [17][18]). In these shape vectors, a sparse set of feature points, on the order of 10's of features, are either manually placed on the object or located using a feature finder. <p> In this equation, x is a 2D point (x; y) in average shape y std . The iterative processing of shape and texture is similar to the active shape models of Cootes and Taylor <ref> [15] </ref>, Cootes, et al. [16], and Lanitis, Taylor, and Cootes [27]. Jones and Poggio [23] also describe a related system that uses linear combinations of prototype shapes to analyze line drawings.
Reference: [16] <author> T.F. Cootes, C.J. Taylor, A. Lanitis, D.H. Cooper, and J. Graham. </author> <title> Building and using flexible mod-els incorporating grey-level information. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 242-246, </pages> <address> Berlin, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: This vectorized representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman [37]), the linear combinations approach to recognition (Ullman and Basri [49], Pog- gio [35]), active shape models (Cootes and Taylor [15], Cootes, et al. <ref> [16] </ref>) and face recognition (Craw and Cameron [17][18]). In these shape vectors, a sparse set of feature points, on the order of 10's of features, are either manually placed on the object or located using a feature finder. <p> In this equation, x is a 2D point (x; y) in average shape y std . The iterative processing of shape and texture is similar to the active shape models of Cootes and Taylor [15], Cootes, et al. <ref> [16] </ref>, and Lanitis, Taylor, and Cootes [27]. Jones and Poggio [23] also describe a related system that uses linear combinations of prototype shapes to analyze line drawings. Correspondence between two arbitrary images can thus be found by vectorizing both, as now both images are in correspondence with the average shape.
Reference: [17] <author> Ian Craw and Peter Cameron. </author> <title> Parameterizing im-ages for recognition and reconstruction. </title> <booktitle> In Proc. British Machine Vision Conference, </booktitle> <pages> pages 367-370, </pages> <year> 1991. </year>
Reference-contexts: That is, the geometrical differences among face images are factored out by warping the images to a common reference shape. This strategy for representing texture has been used, for example, in the face recognition works of Craw and Cameron <ref> [17] </ref>, and Shackleton and Welsh [42].
Reference: [18] <author> Ian Craw and Peter Cameron. </author> <title> Face recognition by computer. </title> <editor> In David Hogg and Roger Boyle, editors, </editor> <booktitle> Proc. British Machine Vision Conference, </booktitle> <pages> pages 498-507. </pages> <publisher> Springer Verlag, </publisher> <year> 1992. </year>
Reference: [19] <author> Irfan A. Essa and Alex Pentland. </author> <title> A vision system for observing and extracting facial action parame-ters. </title> <booktitle> In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 76-83, </pages> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: In addition to recognition, richer models for faces have been studied for analyzing varying illumination (Hallinan [22]) and expression (Yacoob and Davis [55], Essa and Pentland <ref> [19] </ref>). In this paper, we address the problem of recognizing faces under varying pose when only one example view per person is available. For example, perhaps just a driver's license photograph is available for each person in the database. <p> For synthesizing images of faces, 3D facial models have been explored in the computer graphics, computer vision, and model-based image coding communities (Aitchison and Craw [1], Kang, Chen, and Hsu [24], Essa and Pentland <ref> [19] </ref>, Akimoto, Suennaga, and Wallace [3], Waters and Terzopoulos [47], Aizawa, Harashima, and Saito [2]). In the 3D technique, face shape is represented either by a polygonal model or by a more complicated multilayer mesh that simulates tissue.
Reference: [20] <author> Jeffrey M. Gilbert and Woody Yang. </author> <title> A real-time face recognition system using custom VLSI hard-ware. </title> <booktitle> In IEEE Workshop on Computer Architectures for Machine Perception, </booktitle> <pages> pages 58-66, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Existing work in face recognition has demonstrated good recognition performance on frontal, expressionless views of faces with controlled lighting (see Baron [4], Turk and Pentland [48], Bichsel [11], Brunelli and Poggio [14], and Gilbert and Yang <ref> [20] </ref>). One of the key remaining problems in face recognition is to handle the variability in appearance due to changes in pose, expression, and lighting conditions.
Reference: [21] <author> F. Girosi, M. Jones, and T. Poggio. </author> <title> Regularization theory and neural networks architectures. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 219-269, </pages> <year> 1995. </year>
Reference: [22] <author> Peter W. Hallinan. </author> <title> A low-dimensional representa-tion of human faces for arbitrary lighting conditions. </title> <booktitle> In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 995-999, </pages> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: There has been some recent work in this direction, such as pose-invariant recognizers (Pentland, et. al. [34], Beymer [10]) and deformable template approaches (Manjunath, et. al. [30]). In addition to recognition, richer models for faces have been studied for analyzing varying illumination (Hallinan <ref> [22] </ref>) and expression (Yacoob and Davis [55], Essa and Pentland [19]). In this paper, we address the problem of recognizing faces under varying pose when only one example view per person is available. For example, perhaps just a driver's license photograph is available for each person in the database.
Reference: [23] <author> Michael J. Jones and Tomaso Poggio. </author> <title> Model-based matching of line drawings by linear combinations 15 of prototypes. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 531-536, </pages> <address> Boston, Massachusetts, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: In this equation, x is a 2D point (x; y) in average shape y std . The iterative processing of shape and texture is similar to the active shape models of Cootes and Taylor [15], Cootes, et al. [16], and Lanitis, Taylor, and Cootes [27]. Jones and Poggio <ref> [23] </ref> also describe a related system that uses linear combinations of prototype shapes to analyze line drawings. Correspondence between two arbitrary images can thus be found by vectorizing both, as now both images are in correspondence with the average shape.
Reference: [24] <author> Chii-Yuan Kang, Yung-Sheng Chen, and WenHsing Hsu. </author> <title> Mapping a lifelike 2.5D human face via an automatic approach. </title> <booktitle> In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 611-612, </pages> <address> New York, NY, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: For synthesizing images of faces, 3D facial models have been explored in the computer graphics, computer vision, and model-based image coding communities (Aitchison and Craw [1], Kang, Chen, and Hsu <ref> [24] </ref>, Essa and Pentland [19], Akimoto, Suennaga, and Wallace [3], Waters and Terzopoulos [47], Aizawa, Harashima, and Saito [2]). In the 3D technique, face shape is represented either by a polygonal model or by a more complicated multilayer mesh that simulates tissue.
Reference: [25] <author> Martin Lades, Jan C. Vorbruggen, Joachim Buhmann, Jorg Lange, Christoph v.d. Malsburg, Rolf P. Wurtz, and Wolfgang Konen. </author> <title> Distortion invari-ant object recognition in the dynamic link archi-tecture. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 42(3), </volume> <month> March </month> <year> 1993. </year>
Reference: [26] <author> Maria Lando and Shimon Edelman. </author> <title> Generalization from a single view in face recognition. </title> <booktitle> In Proceedings, International Workshop on Automatic Faceand Gesture-Recognition, </booktitle> <pages> pages 80-85, </pages> <address> Zurich, </address> <year> 1995. </year>
Reference-contexts: Recognition performance will be reported on a separate test set of faces that cover a range of rotations both in and out of the image plane. Independent from our work, Lando and Edelman <ref> [26] </ref> have recently investigated the same overall question - generalization from a single view in face recognition - using a similar example-based technique for representing prior knowledge of faces. In addition, Maurer and von der Malsburg [31] have investigated a technique for transforming their "jet" features across rotations in depth. <p> Schyns and Bulthoff [41] obtain a low recognition rate, but their results are difficult to compare since their stimuli are Gouraud shaded 3D faces that exclude texture information. Lando and Edel- man <ref> [26] </ref> have recently performed computational experiments to replicate earlier psychophysical results in [32]. A recognition rate of only 76% was reported, but the authors suggest that this may be improved by using a two-stage classifier instead of a single-stage one. <p> Direct comparison of our results to related face recognition systems is difficult because of differences in exam <p>- ple and testing views. The closest systems are those of Lando and Edelman <ref> [26] </ref> and Maurer and von der Mals- burg [31]. Both systems explore a view transformation method that effectively generates new views from a single view.
Reference: [27] <author> A. Lanitis, C.J. Taylor, and T.F. Cootes. </author> <title> A unified approach to coding and interpreting face images. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 368-373, </pages> <address> Cambridge, MA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: In this equation, x is a 2D point (x; y) in average shape y std . The iterative processing of shape and texture is similar to the active shape models of Cootes and Taylor [15], Cootes, et al. [16], and Lanitis, Taylor, and Cootes <ref> [27] </ref>. Jones and Poggio [23] also describe a related system that uses linear combinations of prototype shapes to analyze line drawings. Correspondence between two arbitrary images can thus be found by vectorizing both, as now both images are in correspondence with the average shape.
Reference: [28] <author> Peter Litwinowicz and Lance Williams. </author> <title> Animating images with drawings. </title> <booktitle> In SIGGRAPH '94 Proceedings, </booktitle> <pages> pages 409-412, </pages> <address> Orlando, FL, </address> <year> 1994. </year>
Reference-contexts: If shape is sparsely defined, then texture mapping or sparse data interpolation techniques can be employed to create the necessary pixelwise level representa <p>- tion. Example sparse data interpolation techniques include using splines (Litwinowicz and Williams <ref> [28] </ref>, Wol- berg [54]), radial basis functions (Reisfeld, Arad, and Yeshurun [40]), and inverse weighted distance metrics (Beier and Neely [5]).
Reference: [29] <author> Bruce D. Lucas and Takeo Kanade. </author> <title> An iterative image registration technique with an application to stereo vision. </title> <booktitle> In Proceedings IJCAI, </booktitle> <pages> pages 674-679, </pages> <address> Vancouver, </address> <year> 1981. </year>
Reference-contexts: While this technique al <br>- 7 ways works, it is manual. Ideally, we would like some-thing automatic, which leads us to the next two techniques. The optical flow technique uses the gradient-based, hierarchical method of Bergen and Hingorani [7] (also see Lucas and Kanade <ref> [29] </ref>, Bergen, et al. [6]). Before applying optical flow, face images are brought into rough registration using the eyes, which were located manually. Optical flow is useful for computing correspondence among different rotated views of the same prototype.
Reference: [30] <author> B.S. Manjunath, R. Chellappa, and C. von der Malsburg. </author> <title> A feature based approach to face recog-nition. </title> <booktitle> In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 373-378, </pages> <year> 1992. </year>
Reference-contexts: There has been some recent work in this direction, such as pose-invariant recognizers (Pentland, et. al. [34], Beymer [10]) and deformable template approaches (Manjunath, et. al. <ref> [30] </ref>). In addition to recognition, richer models for faces have been studied for analyzing varying illumination (Hallinan [22]) and expression (Yacoob and Davis [55], Essa and Pentland [19]). In this paper, we address the problem of recognizing faces under varying pose when only one example view per person is available. <p> In the flexible matching approach (von der Malsburg and collaborators <ref> [30] </ref>[25]), the input image is deformed in 2D to match the example view. In [30], the deformation is driven by a matching of local "end-stop" features so that the resulting transformation between model and input is like a 2D warp rather than a global, rigid transform. <p> They apply this technique to rotating faces about 45 ffi between frontal and half-profile views. They report a recognition rate of 53% on a subset of 90 people from the FERET database. Two other comparable results are from Manjunath, et al. <ref> [30] </ref>, who obtain 86% on a database of 86 people, and Pentland, et al. [34], whose extrapolation experiment with view-based eigenspaces yields 83% on a database of 21 people.
Reference: [31] <author> Thomas Maurer and Christoph von der Malsburg. </author> <title> Single-view based recognition of faces rotated in depth. </title> <booktitle> In Proceedings, International Workshop on Automatic Face- and Gesture-Recognition, </booktitle> <pages> pages 248-253, </pages> <address> Zurich, </address> <year> 1995. </year>
Reference-contexts: Independent from our work, Lando and Edelman [26] have recently investigated the same overall question - generalization from a single view in face recognition - using a similar example-based technique for representing prior knowledge of faces. In addition, Maurer and von der Malsburg <ref> [31] </ref> have investigated a technique for transforming their "jet" features across rotations in depth. <p> Direct comparison of our results to related face recognition systems is difficult because of differences in exam <p>- ple and testing views. The closest systems are those of Lando and Edelman [26] and Maurer and von der Mals- burg <ref> [31] </ref>. Both systems explore a view transformation method that effectively generates new views from a single view.
Reference: [32] <author> Yael Moses, Shimon Ullman, and Shimon Edelman. </author> <title> Generalization to novel images in upright and in-verted faces. </title> <type> Technical Report CC93-14, </type> <institution> The Weizmann Institute of Science, </institution> <year> 1993. </year>
Reference-contexts: After studying the training images, the subject would be asked to identify new images of the people under a variety of poses. Moses, Ull- man, and Edelman <ref> [32] </ref> have performed this experiment using testing views at a variety of poses and lighting conditions. While high recognition rates were observed in the subjects (97%), the subjects were only asked to discriminate between three different people. <p> Schyns and Bulthoff [41] obtain a low recognition rate, but their results are difficult to compare since their stimuli are Gouraud shaded 3D faces that exclude texture information. Lando and Edel- man [26] have recently performed computational experiments to replicate earlier psychophysical results in <ref> [32] </ref>. A recognition rate of only 76% was reported, but the authors suggest that this may be improved by using a two-stage classifier instead of a single-stage one. Direct comparison of our results to related face recognition systems is difficult because of differences in exam <p>- ple and testing views.
Reference: [33] <author> Elizabeth C. Patterson, Peter C. Litwinowicz, and Ned Greene. </author> <title> Facial animation by spatial mapping. </title> <booktitle> In Computer Animation '91, </booktitle> <pages> pages 31-44. </pages> <address> SpringerVerlag, Tokyo, </address> <year> 1991. </year>
Reference-contexts: The technique has been explored previously by Brunelli and Poggio [38] within the context of an "example-based" approach to computer graphics and by researchers in performance-driven animation (Williams [52][53], Patterson, Litwinowicz, and Greene <ref> [33] </ref>). For notation, let i n be the single real view of the novel face in standard pose.
Reference: [34] <author> Alex Pentland, Baback Moghaddam, and Thad Starner. </author> <title> View-based and modular eigenspaces for face recognition. </title> <booktitle> In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 84-91, </pages> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: One of the key remaining problems in face recognition is to handle the variability in appearance due to changes in pose, expression, and lighting conditions. There has been some recent work in this direction, such as pose-invariant recognizers (Pentland, et. al. <ref> [34] </ref>, Beymer [10]) and deformable template approaches (Manjunath, et. al. [30]). In addition to recognition, richer models for faces have been studied for analyzing varying illumination (Hallinan [22]) and expression (Yacoob and Davis [55], Essa and Pentland [19]). <p> Simi- larly to the shape case, this relies on the assumption that the space of grey level textures T is linearly spanned by a set of prototype textures. The validity of this assumption is borne out by recent successful face recognition systems (e.g. eigenfaces, Pentland, et al. <ref> [34] </ref>). <p> They report a recognition rate of 53% on a subset of 90 people from the FERET database. Two other comparable results are from Manjunath, et al. [30], who obtain 86% on a database of 86 people, and Pentland, et al. <ref> [34] </ref>, whose extrapolation experiment with view-based eigenspaces yields 83% on a database of 21 people. In both cases, the system is trained on a set of views (vs. just one for ours) and recognition performance is tested on views from outside the pose-expression space of the training set.
Reference: [35] <author> T. Poggio. </author> <title> 3D object recognition: on a result by Basri and Ullman. </title> <type> Technical Report # 9005-03, IRST, </type> <institution> Povo, Italy, </institution> <year> 1990. </year>
Reference-contexts: Our motivation for using the example-based approach is its potential for being a simple alternative to the more complicated 3D model-based approach. Using an example-based approach to bypass 3D models for 3D object recognition was first explored in the linear combinations approach to recognition (Ullman and Basri [49], Poggio <ref> [35] </ref>). <p> This vectorized representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman [37]), the linear combinations approach to recognition (Ullman and Basri [49], Pog- gio <ref> [35] </ref>), active shape models (Cootes and Taylor [15], Cootes, et al. [16]) and face recognition (Craw and Cameron [17][18]). In these shape vectors, a sparse set of feature points, on the order of 10's of features, are either manually placed on the object or located using a feature finder.
Reference: [36] <author> T. Poggio. </author> <title> 3D object recognition and prototypes: one 2D view may be sufficient. </title> <type> Technical Report 9107-02, I.R.S.T., </type> <institution> Povo, Italy, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Then synthesize the novel face at the virtual pose by taking the linear combination of prototype objects at the virtual pose using the same set of coefficients. Using this approach, as discussed in Poggio <ref> [36] </ref> and Poggio and Vetter [39], it is possible to "learn" a direct mapping from standard pose to a particular virtual pose. 2. Parallel deformation. Using just one prototype object, measure the 2D deformation of object features going from the standard to virtual view.
Reference: [37] <author> T. Poggio and S. Edelman. </author> <title> A network that learns to recognize three-dimensional objects. </title> <journal> Nature, </journal> <volume> 343(6255) </volume> <pages> 263-266, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: This vectorized representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman <ref> [37] </ref>), the linear combinations approach to recognition (Ullman and Basri [49], Pog- gio [35]), active shape models (Cootes and Taylor [15], Cootes, et al. [16]) and face recognition (Craw and Cameron [17][18]).
Reference: [38] <author> Tomaso Poggio and Roberto Brunelli. </author> <title> A novel ap-proach to graphics. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1354, </pages> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: Then map this 2D deformation onto the novel object and use the deformation to distort, or warp, the novel image from the standard pose to the virtual one. The technique has been explored previously by Brunelli and Poggio <ref> [38] </ref> within the context of an "example-based" approach to computer graphics and by researchers in performance-driven animation (Williams [52][53], Patterson, Litwinowicz, and Greene [33]). For notation, let i n be the single real view of the novel face in standard pose. <p> Consider as a special case the deformation approach with just one prototype. In this case, the novel face is deformed in a manner that imitates the deformation seen in the prototype. This is similar to performance-driven animation (Williams [52]), and Poggio and Brunelli <ref> [38] </ref>, who call it parallel deformation, have suggested it as a computer graphics tool for animating objects when provided with just one view.
Reference: [39] <author> Tomaso Poggio and Thomas Vetter. </author> <title> Recognition and structure from one 2D model view: Observations on prototypes, object classes, and symmetries. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1347, </pages> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: Given one view of a person, we will propose two methods for using the information in the prototype views to synthesize new views of the person, views from different rotations in our case. Following Poggio and Vetter <ref> [39] </ref>, we call these synthesized views virtual views. Our motivation for using the example-based approach is its potential for being a simple alternative to the more complicated 3D model-based approach. <p> This suggests an object may be represented using a set of 2D views instead of a 3D model. Poggio and Vetter <ref> [39] </ref> have discussed this linear combinations approach in the case where only one example view is available for an object, laying the groundwork for virtual views. Normally, with just one view, 3D recognition is not possible. <p> Then synthesize the novel face at the virtual pose by taking the linear combination of prototype objects at the virtual pose using the same set of coefficients. Using this approach, as discussed in Poggio [36] and Poggio and Vetter <ref> [39] </ref>, it is possible to "learn" a direct mapping from standard pose to a particular virtual pose. 2. Parallel deformation. Using just one prototype object, measure the 2D deformation of object features going from the standard to virtual view. <p> This idea was first developed by Poggio and Vetter <ref> [39] </ref>. <p> Another way to formulate the solution as a direct mapping is to train a network to learn the association between standard and virtual pose (see Poggio and Vet- ter <ref> [39] </ref>). The (input, output) pairs presented to the network during training would be the prototype pairs (y p j , y p j ;r ). A potential architecture for such a network is suggested by the fact that equation (8) can be implemented by a single layer linear network. <p> For the single real view, an off-center view was favored over, say, a frontal view because of the recognition results for bilaterally symmetric objects of Poggio and Vetter <ref> [39] </ref>. When the single real view is from a nondegenerate pose (i.e. mirror reflection is not equal to original view), then the mirror reflection immediately provides a second view that can be used for recognition.
Reference: [40] <author> Daniel Reisfeld, Nur Arad, and Yehezkel Yeshurun. </author> <title> Normalization of face images using few anchors. </title> <booktitle> In Proceedings Int. Conf. on Pattern Recognition, </booktitle> <volume> vol-ume 1, </volume> <pages> pages 761-763, </pages> <address> Jerusalem, Israel, </address> <year> 1994. </year>
Reference-contexts: If shape is sparsely defined, then texture mapping or sparse data interpolation techniques can be employed to create the necessary pixelwise level representa <p>- tion. Example sparse data interpolation techniques include using splines (Litwinowicz and Williams [28], Wol- berg [54]), radial basis functions (Reisfeld, Arad, and Yeshurun <ref> [40] </ref>), and inverse weighted distance metrics (Beier and Neely [5]). If a pixelwise representation is being used for shape in the first place, such as one derived from optical flow, then texture mapping or data interpolation techniques can be avoided.
Reference: [41] <author> Phillipe G. Schyns and Heinrich H. Bulthoff. </author> <title> Conditions for viewpoint invariant face recognition. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1432, </pages> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1993. </year>
Reference-contexts: The choice of an off-center view is also supported by the psychophysical experiments of Schyns and Bulthoff <ref> [41] </ref>. <p> Bruce [12] performs a similar experiment where the subject is asked whether a face had appeared during training, and detection rates go down to either 76% or 60%, depending on the amount of pose/expression difference between the testing and training views. Schyns and Bulthoff <ref> [41] </ref> obtain a low recognition rate, but their results are difficult to compare since their stimuli are Gouraud shaded 3D faces that exclude texture information. Lando and Edel- man [26] have recently performed computational experiments to replicate earlier psychophysical results in [32].
Reference: [42] <author> M.A. Shackleton and W.J. Welsh. </author> <title> Classification of facial features for recognition. </title> <booktitle> In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 573-579, </pages> <address> Lahaina, Maui, Hawaii, </address> <year> 1991. </year>
Reference-contexts: That is, the geometrical differences among face images are factored out by warping the images to a common reference shape. This strategy for representing texture has been used, for example, in the face recognition works of Craw and Cameron [17], and Shackleton and Welsh <ref> [42] </ref>.
Reference: [43] <author> A. Shashua. </author> <title> Correspondence and affine shape from two orthographic views: Motion and Recognition. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1327, </pages> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: In this paper we use a dense representation of one feature per pixel, a representation originally suggested to us by the object recognition work of Shashua <ref> [43] </ref>. Com- pared to a sparser representation, the pixelwise representation increases the difficulty of finding correspondences. However, we have found that a standard optical flow algorithm [7], preceded by normalization based on the eye locations, can do a good job at automatically computing dense pixelwise correspondences.
Reference: [44] <author> A. Shashua. </author> <title> Geometry and Photometry in 3D visual recognition. </title> <type> PhD thesis, </type> <institution> M.I.T Artificial Intelligence Laboratory, AI-TR-1401, </institution> <month> November </month> <year> 1992. </year>
Reference: [45] <author> Pawan Sinha. </author> <title> Object recognition via image invariances. </title> <institution> Investigative Ophthalmology and Visual Science, 35(4):1626, </institution> <year> 1994. </year>
Reference-contexts: For example, the invariant features approach records features in the example view that do not change as pose-expression- lighting parameters change, features such as color or geometric invariants. While not yet applied to face recognition, this approach has been used for face detection under varying illumination (Sinha <ref> [45] </ref>) and for indexing of packaged grocery items using color (Swain and Bal- lard [46]). In the flexible matching approach (von der Malsburg and collaborators [30][25]), the input image is deformed in 2D to match the example view.
Reference: [46] <author> Michael J. Swain and Dana H. Ballard. </author> <title> Color in-dexing. </title> <journal> International Journal of Computer Vision, </journal> <volume> 7(1) </volume> <pages> 11-32, </pages> <year> 1991. </year>
Reference-contexts: While not yet applied to face recognition, this approach has been used for face detection under varying illumination (Sinha [45]) and for indexing of packaged grocery items using color (Swain and Bal- lard <ref> [46] </ref>). In the flexible matching approach (von der Malsburg and collaborators [30][25]), the input image is deformed in 2D to match the example view.
Reference: [47] <author> Demetri Terzopoulos and Keith Waters. </author> <title> Analysis of facial images using physical and anatomical mod-els. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 727-732, </pages> <address> Osaka, Japan, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: For synthesizing images of faces, 3D facial models have been explored in the computer graphics, computer vision, and model-based image coding communities (Aitchison and Craw [1], Kang, Chen, and Hsu [24], Essa and Pentland [19], Akimoto, Suennaga, and Wallace [3], Waters and Terzopoulos <ref> [47] </ref>, Aizawa, Harashima, and Saito [2]). In the 3D technique, face shape is represented either by a polygonal model or by a more complicated multilayer mesh that simulates tissue.
Reference: [48] <author> Matthew Turk and Alex Pentland. </author> <title> Eigenfaces for recognition. </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> 3(1) </volume> <pages> 71-86, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction Existing work in face recognition has demonstrated good recognition performance on frontal, expressionless views of faces with controlled lighting (see Baron [4], Turk and Pentland <ref> [48] </ref>, Bichsel [11], Brunelli and Poggio [14], and Gilbert and Yang [20]). One of the key remaining problems in face recognition is to handle the variability in appearance due to changes in pose, expression, and lighting conditions.
Reference: [49] <author> Shimon Ullman and Ronen Basri. </author> <title> Recognition by linear combinations of models. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(10) </volume> <pages> 992-1006, </pages> <year> 1991. </year>
Reference-contexts: Our motivation for using the example-based approach is its potential for being a simple alternative to the more complicated 3D model-based approach. Using an example-based approach to bypass 3D models for 3D object recognition was first explored in the linear combinations approach to recognition (Ullman and Basri <ref> [49] </ref>, Poggio [35]). <p> This vectorized representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman [37]), the linear combinations approach to recognition (Ullman and Basri <ref> [49] </ref>, Pog- gio [35]), active shape models (Cootes and Taylor [15], Cootes, et al. [16]) and face recognition (Craw and Cameron [17][18]).
Reference: [50] <author> Thomas Vetter, Anya Hurlbert, and Tomaso Poggio. </author> <title> View-based models of 3D object recognition: Invariance to imaging transformations. </title> <journal> Cerebral Cortex, </journal> <volume> 3 </volume> <pages> 261-269, </pages> <month> May/June </month> <year> 1995. </year>
Reference: [51] <author> Thomas Vetter and Tomaso Poggio. </author> <title> Linear object classes and image synthesis from a single example 16 image. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1531, </pages> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1995. </year>
Reference-contexts: The projection process of going from 3D shape Y to 2D shape y consists of a 3D rotation, occlusion of a set of non-visible feature points, and orthographic projection. Mathematically, we model this using a matrix L y = LY; (2) 1 Vetter and Poggio <ref> [51] </ref> have explored the implications of 3D linear combinations of shape on image grey levels, or texture. If an object is a linear combination of prototype objects in 3D, then so are the surface normals. <p> This synthesized grey level texture is then warped or texture mapped onto the virtual shape to create a finished virtual view. The ideas presented in this section were developed by the authors and also independently by Vetter and Pog- gio <ref> [51] </ref>. To generate the virtual texture t n;r , we propose using the same linear class idea of approximation at the standard view and reconstruction at the virtual view. <p> In earlier work aimed primarily at computer graphics [8], we demonstrated parallel deformation for transformations from neutral to smiling expressions. 7.4 Future work For future work on our approach to virtual views, we plan to use multiple prototypes for generating virtual shape. Vetter and Poggio <ref> [51] </ref> have already done some work in applying the linear class idea to both shape and texture. It would be interesting to test some of their virtual views in a view-based recognizer.
Reference: [52] <author> Lance Williams. </author> <title> Performance-driven facial anima-tion. </title> <booktitle> In SIGGRAPH '90 Proceedings, </booktitle> <pages> pages 235242, </pages> <address> Dallas, TX, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Consider as a special case the deformation approach with just one prototype. In this case, the novel face is deformed in a manner that imitates the deformation seen in the prototype. This is similar to performance-driven animation (Williams <ref> [52] </ref>), and Poggio and Brunelli [38], who call it parallel deformation, have suggested it as a computer graphics tool for animating objects when provided with just one view.
Reference: [53] <author> Lance Williams. </author> <title> Living pictures. </title> <booktitle> In Models and Techniques in Computer Animation, </booktitle> <pages> pages 2-12. </pages> <publisher> Springer-Verlag, </publisher> <address> Tokyo, </address> <year> 1993. </year>
Reference: [54] <author> George Wolberg. </author> <title> Digital Image Warping. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1990. </year>
Reference-contexts: If shape is sparsely defined, then texture mapping or sparse data interpolation techniques can be employed to create the necessary pixelwise level representa <p>- tion. Example sparse data interpolation techniques include using splines (Litwinowicz and Williams [28], Wol- berg <ref> [54] </ref>), radial basis functions (Reisfeld, Arad, and Yeshurun [40]), and inverse weighted distance metrics (Beier and Neely [5]). If a pixelwise representation is being used for shape in the first place, such as one derived from optical flow, then texture mapping or data interpolation techniques can be avoided. <p> Each pixel in the reference image samples the destination image by following its flow vector. Since the destination location is usually between pixels, bilinear interpolation is used to produce a grey level value. Forward warping is solved using the idea of four corner mapping (see Wolberg <ref> [54] </ref>). Basically, we invert the forward warping and then apply the backward warping algorithm. To invert the forward warping, repeat the following for every square source patch of four adjacent pixels in the source. Map the source patch to a quadrilateral in the destination image.
Reference: [55] <author> Yaser Yacoob and Larry Davis. </author> <title> Computing spatiotemporal representations of human faces. </title> <booktitle> In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 70-75, </pages> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: There has been some recent work in this direction, such as pose-invariant recognizers (Pentland, et. al. [34], Beymer [10]) and deformable template approaches (Manjunath, et. al. [30]). In addition to recognition, richer models for faces have been studied for analyzing varying illumination (Hallinan [22]) and expression (Yacoob and Davis <ref> [55] </ref>, Essa and Pentland [19]). In this paper, we address the problem of recognizing faces under varying pose when only one example view per person is available. For example, perhaps just a driver's license photograph is available for each person in the database.
References-found: 55

