URL: http://www.cs.utexas.edu/users/rvdg/papers/icc_vs_other.ps
Refering-URL: http://www.cs.utexas.edu/users/rvdg/abstracts/icc_vs_other.html
Root-URL: 
Title: Fast Collective Communication Libraries, Please  
Author: Prasenjit Mitra David G. Payne Lance Shuler Robert van de Geijn Jerrell Watts 
Note: This work was partially supported by the Intel Research Council and Intel Scalable Systems Division. Jerrell Watts is being supported by an NSF Graduate Research Fellowship.  
Address: Austin, Texas 78712-1188 15201 N.W. Greenbrier Pkwy Beaverton, Oregon 97006  Albuquerque, New Mexico 87185-1109 Austin, Texas 78712-1188  Pasadena, California 91125  
Affiliation: Department of Computer Sciences Scalable Systems Division The University of Texas at Austin Intel Corporation  Parallel Computing Sciences Department, 1424 Department of Computer Sciences Sandia National Laboratory The University of Texas at Austin  Scalable Concurrent Programming Laboratory California Institute of Technology  
Abstract: It has been recognized that many parallel numerical algorithms can be effectively implemented by formulating the required communication as collective communications. Nonetheless, the efficiency of such communications has been suboptimal in many communication library implementations. In this paper, we give a brief overview of techniques that can be used to implement a high performance collective communication library, the iCC library, developed for the Intel family of parallel supercomputers as part of the InterCom project at the University of Texas at Austin. We compare the achieved performance on the Intel Paragon to those of three widely available libraries: Intel's NX collective communication library, the MPICH Message Passing Interface (MPI) implementation developed at Argonne and Mississippi State University and a Basic Linear Algebra Communication Subprograms (BLACS) implementation, developed at the University of Tennessee. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. DuCroz, A. Greenbaum, S. Hammarling, A. McKenney, D. Sorensen. "LA-PACK: </author> <title> A Portable Linear Algebra Library for High-Performance Computers," </title> <note> LAPACK Working Note 20, </note> <institution> University of Tennessee, CS-90-105, </institution> <month> May </month> <year> 1990. </year>
Reference: [2] <author> M. Barnett, S. Gupta, D. Payne, L. Shuler, R. van de Geijn and J. Watts. </author> <title> "Interprocessor Collective Communication Library (InterCom)." </title> <booktitle> Proceedings of Supercomputing 94, </booktitle> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: The idea is to use a long vector approach in subgroups. This has the effect of shortening the vector length until it pays to perform a short vector approach to complete the collective communication. More details of this can be found in our paper <ref> [2] </ref>. 4 Basic Linear Algebra Communica- tion Subprograms The BLACS (Basic Linear Algebra Communication Subprograms) project arose as part of a larger project called ScaLAPACK (Scalable Linear Algebra PACKage) [5].
Reference: [3] <author> M. Barnett, R. Littlefield, D.G. Payne and R. van de Geijn. </author> <title> "On the Efficiency of Global Combine Algorithms for 2-D Meshes With Wormhole Routing," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 24, </volume> <pages> pp. </pages> <month> 191-201 </month> <year> (1995). </year>
Reference: [4] <author> M. Barnett, D.G. Payne, R. van de Geijn and J. Watts. </author> <title> "Broadcasting on Meshes with WormHole Routing," </title> <institution> University of Texas, Department of Computer Sciences, </institution> <month> TR-93-24 </month> <year> (1993). </year>
Reference-contexts: On hypercubes, this can be easily accomplished by staging the algorithms as log p steps during which communication is performed in each hypercube dimension. For meshes, this idea can be utilized as well, provided some care is taken at each stage <ref> [4] </ref>. All our target short vector collective communication operations can be built from four primitives. These are broadcast, combine-to-one, scatter, and gather. Consider the broadcast.
Reference: [5] <author> J. Choi, J. Dongarra, R. Pozo, and D. Walker. </author> <title> "ScaLAPACK: A Scalable Linear Algebra for Distributed Memory Concurrent Computers," </title> <note> LA-PACK Working Note 55, </note> <institution> University of Tennessee, CS-92-181, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: More details of this can be found in our paper [2]. 4 Basic Linear Algebra Communica- tion Subprograms The BLACS (Basic Linear Algebra Communication Subprograms) project arose as part of a larger project called ScaLAPACK (Scalable Linear Algebra PACKage) <ref> [5] </ref>. The goal of the ScaLAPACK project is to implement a core set of the linear algebra routines provided in the sequential library LAPACK (Linear Algebra PACKage)[10] on distributed memory platforms.
Reference: [6] <author> William Gropp, Ewing Lusk, and Anthony Skjel-lum. </author> <title> Using MPI: Portable Parallel Programming with the Message-Passing Interface, </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: It should be noted that in [13] it is shown that this approach to implementation is competitive with the implementations of similar collective communication libraries by all the major vendors. 5 MPI Collective Communication In terface We quote from Using MPI <ref> [6] </ref>: During 1993, a broadly based group of parallel computer vendors, software writers and application scientists collaborated on the development of a standard portable message-passing library definition called MPI, for Message-Passing Interface. As of mid-1994, a number of implementations are in progress, and applications are already being ported.
Reference: [7] <author> C.-T. Ho and S. L. Johnsson. </author> <title> Distributed Routing Algorithms for Broadcasting and Personalized Communication in Hypercubes. </title> <booktitle> Proceedings of the 1986 International Conference on Parallel Processing, pg. </booktitle> <pages> 640-648, </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1986. </year>
Reference: [8] <author> L. M. Ni and P. K McKinley. </author> <title> A Survey of Wormhole Routing Techniques in Direct Networks. </title> <journal> IEEE Computer, </journal> <volume> 26(2) </volume> <pages> 62-76, </pages> <month> Feb. </month> <year> 1993. </year>
Reference: [9] <author> Y. Saad and M. H. Schultz. </author> <title> Data Communication in Parallel Architectures. </title> <journal> Parallel Computing, </journal> <volume> 11(2) </volume> <pages> 131-150, </pages> <month> Aug. </month> <year> 1989. </year>
Reference: [10] <author> Robert van de Geijn. </author> <title> "On Global Combine Operations," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> 22 , pp. </pages> <month> 324-328 </month> <year> (1994). </year>
Reference: [11] <author> R. van de Geijn and J. Watts. </author> <title> A Pipelined Broadcast for Multidimensional Meshes. </title> <note> Parallel Processing Letters, to appear. </note>
Reference: [12] <author> D. W. Walker. </author> <title> The Design of a Standard Message Passing Interface for Distributed Memory Concurrent Computers. </title> <booktitle> Parallel Computing, </booktitle> <month> Apr. </month> <year> 1994. </year> <title> (Up to date information about the MPI standard is available from netlib, directory mpi.) </title>
Reference: [13] <author> R. Clint Whaley, </author> <title> "Basic Linear Algebra Communication Subprograms: Analysis and Implementation Across Multiple Parallel Architectures," </title> <note> LAPACK Working Note 73, </note> <institution> University of Ten-nessee, CS-94-234, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Moreover, they incur (near) minimal messages, thereby reducing the overhead due to message latency. However, the total volume of data communicated is non-optimal, which greatly reduces their effectiveness for long vector lengths. It should be noted that in <ref> [13] </ref> it is shown that this approach to implementation is competitive with the implementations of similar collective communication libraries by all the major vendors. 5 MPI Collective Communication In terface We quote from Using MPI [6]: During 1993, a broadly based group of parallel computer vendors, software writers and application scientists
References-found: 13

