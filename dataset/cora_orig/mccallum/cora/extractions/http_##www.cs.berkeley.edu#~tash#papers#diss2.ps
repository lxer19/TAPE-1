URL: http://www.cs.berkeley.edu/~tash/papers/diss2.ps
Refering-URL: http://www.cs.berkeley.edu/~tash/papers/diss2.html
Root-URL: 
Abstract-found: 0
Intro-found: 1
Reference: <editor> AAAI (1988). </editor> <booktitle> Proceedings of the Seventh National Conference on Artificial Intelligence. American Association for Artificial Intelligence, AAAI Press. AAAI (1990). Proceedings of the Eighth National Conference on Artificial Intelligence. American Association for Artificial Intelligence, AAAI Press. AAAI (1991). Proceedings of the Ninth National Conference on Artificial Intelligence. American Association for Artificial Intelligence, AAAI Press. AAAI (1992). Proceedings of the Tenth National Conference on Artificial Intelligence. American Association for Artificial Intelligence, AAAI Press. AAAI (1994). Proceedings of the Twelfth National Conference on Artificial Intelligence. American Association for Artificial Intelligence, </booktitle> <publisher> AAAI Press, MIT Press. </publisher>
Reference: <author> Baum, E. B. </author> <year> (1992). </year> <title> On optimal game tree propagation for imperfect players. </title> <booktitle> In (AAAI, </booktitle> <year> 1992), </year> <pages> pages 507-512. </pages>
Reference-contexts: This captures an important distinction, one used in <ref> (Baum, 1992) </ref> to improve game playing performance. (Of course, over repeated trials, these distances will converge.) Introspective planning techniques are an extension of common metalevel control techniques (Russell and Wefald, 1991). Computations such as those performed by the fixed-variation planner are controlled somewhat like physical actions by the metalevel.
Reference: <author> Bertsekas, D. P. and Casta~non, D. A. </author> <year> (1989). </year> <title> Adaptive aggregation methods for infinite horizon dynamic programming. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 34(6) </volume> <pages> 589-598. </pages> <note> 106 Birnbaum, </note> <editor> L. and Collins, G., editors (1991). </editor> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <address> San Mateo, Calif. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Boutilier, C. and Dearden, R. </author> <year> (1994). </year> <title> Using abstractions for decision-theoretic planning with time constraints. </title> <booktitle> In (AAAI, </booktitle> <year> 1994). </year>
Reference: <author> Boutilier, C., Dearden, R., and Goldszmidt, M. </author> <year> (1995). </year> <title> Exploiting structure in policy construction. </title> <booktitle> In (IJCAI, </booktitle> <year> 1995). </year>
Reference-contexts: Sometimes a Markov decision process is tractable, and there are often ways of modifying these algorithms for such cases so they can be 18 used (e.g. structured policy iteration <ref> (Boutilier et al., 1995) </ref>). However, the problem class is intractable in general, so an agent cannot be expected to solve it. <p> Sometimes, a small set of features is known to determine the best action. In such cases, methods such as structured policy iteration <ref> (Boutilier et al., 1995) </ref> can take advantage of this structural knowledge to reduce the computational complexity of planning. <p> Sometimes, it is possible to detect that two states will share best actions. This will hold true if they both have the same consequences for all actions. More generally, if all actions lead to consequences of the same value, then the states can be grouped together. (Structured policy iteration <ref> (Boutilier et al., 1995) </ref> operates by applying this criterion to recluster at each iteration.) No such locally testable criterion can be expected to reproduce the behavior of the potential value criterion, however, because lossless abstraction is obtain 82 able even without the same consequences across undistinguished fine states for all actions.
Reference: <author> Breese, J. </author> <year> (1990). </year> <title> Construction of belief and decision networks. </title> <journal> Computational Intelligence. </journal>
Reference: <author> Chapman, D. and Kaelbling, L. </author> <year> (1991). </year> <title> Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. </title> <booktitle> In (IJCAI, </booktitle> <year> 1991). </year>
Reference: <author> Christensen, J. and Grove, A. </author> <year> (1991). </year> <title> A formal model for classical planning. </title> <booktitle> In (IJCAI, </booktitle> <year> 1991). </year>
Reference-contexts: Similar considerations hold for plans invariant under replacement of one action by another. Also, deterministic actions can be considered to be adding and deleting attributes of the world state, depending on the current state, similar to classical planning actions. (See <ref> (Christensen and Grove, 1991) </ref> for a description of classical nonlinear plans.) Computations, treated as actions (as in (Russell and Wefald, 1991)), fit into the above representation without modification. A computation affects only internal state attributes and time.
Reference: <author> Dean, T. and Boddy, M. </author> <year> (1988). </year> <title> An analysis of time-dependent planning. </title> <booktitle> In (AAAI, </booktitle> <year> 1988), </year> <pages> pages 49-54. </pages>
Reference: <author> Dean, T., Kaelbling, L. P., Kirman, J., and Nicholson, A. </author> <year> (1993). </year> <title> Deliberation scheduling for time-critical sequential decision making. </title> <booktitle> In (UAI, </booktitle> <year> 1993), </year> <pages> pages 309-316. </pages>
Reference-contexts: This strategy is similar to (but was developed independently of) those described in the papers of <ref> (Dean et al., 1993) </ref> and (Thiebaux et al., 1994); however, its use of estimated values on non-envelope states provides important additional information for determining envelope action choices, allowing for meaningful action choices to be made before a path to the goal is found, and summarizing previous computational effort outside the current <p> These values can be maintained across problem-solving episodes (differing possibly in initial state but not in reward function) for cumulative performance improvements. In each such episode, the planner alternately computes to improve the value estimates, and chooses moves using these estimates. (This model differs from those considered by <ref> (Dean et al., 1993) </ref>, where computation is done either prior to or concurrently with action, rather than interspersed with action. <p> Principle 5 follows from Principle 4 by an argument following the lines of that for the completeness of A fl with an admissible heuristic. These results are asymptotic. They are not available to envelope algorithms that do not provide similar guarantees of adequate exploration (such as those of <ref> (Dean et al., 1993) </ref> and (Thiebaux et al., 1994)).
Reference: <author> Doyle, J. </author> <year> (1990). </year> <title> Rationality and its roles in reasoning. </title> <booktitle> In (AAAI, </booktitle> <year> 1990), </year> <pages> pages 1093-1100. </pages>
Reference-contexts: Much recent work in artificial intelligence has the goal of implementing an agent capable of rational behavior on a computer <ref> (Doyle, 1990) </ref>. Bayesian decision theory has several limitations as a model for rational decision making. These become apparent when one considers the constraints introduced by any physical implementation.
Reference: <author> Dreyfus, H. </author> <year> (1992). </year> <title> What Computers Still Can't Do. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Drummond, M. and Bresina, J. </author> <year> (1990). </year> <title> Anytime synthetic projection: Max--imizing the probability of goal satisfaction. </title> <booktitle> In (AAAI, </booktitle> <year> 1990). </year>
Reference-contexts: Even an action of "doing nothing" can have uncertain consequences if the planner has a nondeterministic model of the dynamics of the world. (This model for actions is similar to the domain causal theory of <ref> (Drummond and Bresina, 1990) </ref>.) For simplicity, we will assume that actions are applicable in any state, though the notion of an action precondition can be captured by mapping unreasonable initial states into a "dead" state. * Let X denote the set of states. 7 * Let A denote the set of
Reference: <author> Hacking, I. </author> <year> (1967). </year> <title> Slightly more realistic personal probability. </title> <journal> Philosophy of Science, </journal> <volume> 34 </volume> <pages> 311-325. </pages>
Reference: <author> Haddaway, P. and Doan, A. </author> <year> (1994). </year> <title> Abstracting probabilistic actions. </title> <booktitle> In (UAI, </booktitle> <year> 1994). </year>
Reference: <author> Hartman, L. </author> <year> (1990). </year> <title> Decision theory and the cost of planning. </title> <type> Technical Report 355, </type> <institution> University of Rochester, Computer Science. </institution>
Reference: <author> Horvitz, E. </author> <year> (1990). </year> <title> Computation and action under bounded resources. </title> <type> Technical Report KSL-90-76, </type> <institution> Departments of Computer Science and Medicine, Stanford University, Stanford, California 94305. </institution>
Reference: <author> Horvitz, E. J. </author> <year> (1987). </year> <title> Reasoning about beliefs and actions under computational resource constraints. </title> <booktitle> In (UAI, </booktitle> <year> 1987). </year>
Reference: <author> Howard, R. A. </author> <year> (1960). </year> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass. </address>
Reference-contexts: This policy fl is a greedy policy for this value function V fl . Only an optimal policy and value function will have this property. So the search for an optimal policy is a search for a fixed point of the above equations <ref> (Howard, 1960) </ref>. This suggests several iterative algorithms for finding optimal policies. For example, policy iteration (Howard, 1960) involves computing the state values for the current policy, and then switching to the corresponding greedy policy. This is iterated until a fixed point is reached. <p> Only an optimal policy and value function will have this property. So the search for an optimal policy is a search for a fixed point of the above equations <ref> (Howard, 1960) </ref>. This suggests several iterative algorithms for finding optimal policies. For example, policy iteration (Howard, 1960) involves computing the state values for the current policy, and then switching to the corresponding greedy policy. This is iterated until a fixed point is reached. <p> A plan will be a generalization of a concrete policy, which assigns a physical action to each state. The optimal concrete policy for a given Markov decision process can be found using various algorithms such as policy iteration <ref> (Howard, 1960) </ref>, but these are too computationally demanding for use with large state spaces. A planner must be able to generate a useful approximation to such a policy with considerably lower computational overhead.
Reference: <author> Howard, R. A. </author> <year> (1966). </year> <title> Information value theory. </title> <journal> IEEE Transactions on Systems Science and Cybernetics, </journal> <volume> SSC-2(1):22-26. </volume> <booktitle> IJCAI (1991). Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <address> Darling Harbour, Sydney, Australia. </address> <booktitle> IJCAI (1993). Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence. IJCAI (1995). Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> Montreal, Canada. </address> <note> 108 Jeffrey, </note> <author> R. </author> <year> (1983). </year> <title> The Logic of Decision. </title> <publisher> University of Chicago Press, </publisher> <address> Chicago, IL, </address> <note> second edition. </note>
Reference-contexts: When Alf does not know where G is, his best strategy will 42 not favor the left side, and thus will have lower expected utility than the best strategy he would have given knowledge of G. This difference is the value of information associated with knowing G <ref> (Howard, 1966) </ref>. If Alf pays 1 for each move, then his uninformed plan will get him there in one step half the time, and three steps otherwise (if he starts in the wrong direction).
Reference: <author> Knoblock, C. </author> <year> (1991). </year> <title> Search reduction in hierarchical problem solving. </title> <booktitle> In (AAAI, </booktitle> <year> 1991). </year>
Reference-contexts: The planner might therefore choose to compute in the coarse-grained space first, using the results to better choose computations in the original space. Coarse-grained spaces thus function like the abstraction spaces of classical planning (described in <ref> (Knoblock, 1991) </ref>). 6.1 Representing Incomplete Knowledge As well as providing computational advantages by reducing the state space, coarse-graining is useful in representing incompleteness in a planner's knowledge.
Reference: <author> Koenig, S. </author> <year> (1992). </year> <title> Optimal probabilistic and decision-theoretic planning using Markovian decision theory. </title> <type> Technical Report UCB/CSD 92/685, </type> <institution> Computer Science Division, University of California, Berkeley, California 94720. </institution>
Reference: <author> MacKay, D. </author> <year> (1992). </year> <title> Bayesian interpolation. </title> <booktitle> Neural Computation, </booktitle> <pages> 4. </pages>
Reference-contexts: An appropriate coarse-graining can be chosen by test 78 ing the various hypotheses using Bayesian methods, as in <ref> (MacKay, 1992) </ref>. These methods include an "Occam's razor" bias towards simpler representations. We can "translate" between different coarse-grainings by going through the original space. Thus, each state in one space corresponds to a probability distribution over states in the other.
Reference: <author> McCarthy, J. </author> <year> (1968). </year> <title> Programs with common sense. </title> <editor> In Minsky, M., editor, </editor> <booktitle> Semantic Information Processing. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: intractability of decision-theoretic solution, and highlights the impact of computational concerns on the course of planning. 2.1 Stochastic Planning Let us consider a world whose state at any given time is described by a set of attributes or propositions which hold in that state (as in the situation calculus of <ref> (McCarthy, 1968) </ref>). The planner has at its disposal a set of executable actions, which map from a given state to a set of possible results, each of which is a particular state and the time the action took to reach that state, weighted by their probability.
Reference: <author> Moore, A. </author> <year> (1991). </year> <title> Variable resolution dynamic programming: Efficiently learning action maps in multivariate real-valued state-spaces. </title> <editor> In (Birn-baum and Collins, </editor> <year> 1991). </year>
Reference: <author> Papadimitriou, C. and Tsitsiklis, J. </author> <year> (1987). </year> <title> The complexity of Markov decision processes. </title> <journal> Mathematics of Operations Research, </journal> <volume> 12(3) </volume> <pages> 441-450. </pages>
Reference-contexts: The best methods for solving Markov decision processes are polynomial in the size of the state space, as the problem can be cast as a linear programming problem <ref> (Papadimitriou and Tsitsiklis, 1987) </ref>. However, often a problem is specified by stating the transition probabilities P and the immediate rewards R as functions of the state. This representation can be much smaller than the size of the state space. <p> However, these methods (like all general Markov decision process optimization methods) are polynomial (or worse) in the size of the state space <ref> (Papadimitriou and Tsitsiklis, 1987) </ref>. Alf may be in a big world, however, for which the time spent examining the 50 Actions a, b cause transitions with the probabilities indicated on the arcs. entire state space would be more costly than a suboptimal solution path.
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: Thus, each state in one space corresponds to a probability distribution over states in the other. This translation is equivalent to finding the marginal probability of a state in the second coarse-grained space, conditioned on the state in the first coarse-grained space. To adapt an example from <ref> (Pearl, 1988) </ref>, consider a planner trying to decide whether or not to go home after receiving a telephone call from a neighbor reporting the sound of a burglar alarm. In calculating the consequences of such an action, the relevant state information is whether or not a burglary occurred. <p> these two representations in order to make a decision. (It may be that the appropriate translation distribution is not explicitly stored, and the translation must be done through intermediate representations, such as a specification of attributes of the neighbor relevant for determining the veracity of the call.) A Bayesian network <ref> (Pearl, 1988) </ref> will efficiently represent the dependencies between states in the two spaces if they are sparse. An important factor in determining the effect of coarse-graining is what information is lost in ignoring certain distinctions, which depends on the 79 detail of the planner's applicable knowledge.
Reference: <author> Russell, S., Subramanian, D., and Parr, R. </author> <year> (1993). </year> <title> Provably bounded optimal agents. </title> <booktitle> In (IJCAI, </booktitle> <year> 1993). </year>
Reference-contexts: Considerable work in artificial intelligence has focused on development of an algorithmic specification of rationality in the face of limited resources. The most promising of such methods for general application approach the problem with the concepts of bounded optimality <ref> (Russell et al., 1993) </ref> and metalevel control of computational expenditures (Russell and Wefald, 1991; Horvitz, 1990). As discussed below, these methods provide insight into possible agent architectures capable of exhibiting many interesting behaviors that have parallels in human problem-solving.
Reference: <author> Russell, S. and Wefald, E. </author> <year> (1991). </year> <title> Do the Right Thing. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, Mass. </address>
Reference-contexts: Also, deterministic actions can be considered to be adding and deleting attributes of the world state, depending on the current state, similar to classical planning actions. (See (Christensen and Grove, 1991) for a description of classical nonlinear plans.) Computations, treated as actions (as in <ref> (Russell and Wefald, 1991) </ref>), fit into the above representation without modification. A computation affects only internal state attributes and time. The utility of a plan containing computations depends as usual on the relative likelihoods of the resulting world lines. <p> value (if one starts with underestimates of value), and the algorithm is guaranteed to converge on the optimal policy. 2.3 Alf: An Example Markov Decision Pro cess I will use as an example throughout the navigation planning problem facing Alf, prodigal son of RALPH (Rational Agents with Limited Performance Hardware <ref> (Russell and Wefald, 1991) </ref>). This fellow exists in a grid world, and must quickly reach a particular destination. Alf's state is just which grid square he occupies. <p> We therefore consider methods for controlling the amount of computation required of an agent in producing its action choices. Let us use a decision-theoretic method within the agent for controlling computation. Such a controller is an example of a metareasoner <ref> (Russell and Wefald, 1991) </ref>, because it is a reasoning algorithm controlling the reasoning of the agent. One advantage to using a metalevel architecture is that computational actions can be considered on the same level as external actions, and compared with external actions as possible things to do. <p> Alf should add the state f whose computation is most worth doing, and do the resulting computation, until no computation is worth doing. Alf should then just move. This procedure is greedy (as discussed in Section 4.3), but the single-step assumption <ref> (Russell and Wefald, 1991) </ref> is not made, as these distributions represent the total remaining cost to goal, without presuming perfect, computation-free future behavior. As written above, V i can be very difficult to compute. In particular, the number of potential policies 0 can be quite large. <p> This captures an important distinction, one used in (Baum, 1992) to improve game playing performance. (Of course, over repeated trials, these distances will converge.) Introspective planning techniques are an extension of common metalevel control techniques <ref> (Russell and Wefald, 1991) </ref>. Computations such as those performed by the fixed-variation planner are controlled somewhat like physical actions by the metalevel. <p> For example, the methods of the previous chapter cluster together all states lying outside of the envelope and fringe for purposes of policy determination. As another example, when computational effort is controlled using a metalevel architecture, as in <ref> (Russell and Wefald, 1991) </ref>, allocation of resources to a computation is not determined on the basis of all the information available to the planner, which is generally adequate to determine the result of the computation itself, but rather on the basis of certain features of the situation which are more easily
Reference: <author> Savage, L. </author> <year> (1972). </year> <title> The Foundations of Statistics. </title> <publisher> Dover, </publisher> <address> New York, NY. </address> <note> 109 Shachter, </note> <author> R. </author> <year> (1990). </year> <title> Evaluating influence diagrams. In (Shafer and Pearl, </title> <note> 1990). Originally published by Operations Research Society of America, </note> <year> 1986. </year>
Reference: <author> Shafer, G. </author> <year> (1990). </year> <title> Savage revisited. In (Shafer and Pearl, </title> <note> 1990). Originally appeared in Statistical Science, 1:4, </note> <year> 1986. </year>
Reference: <author> Shafer, G. and Pearl, J., </author> <title> editors (1990). Readings in Uncertain Reasoning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Simon, H. </author> <year> (1955). </year> <title> A behavioral model of rational choice. </title> <journal> Quarterly Journal of Economics, </journal> <volume> 69 </volume> <pages> 99-118. </pages>
Reference-contexts: This characterization of abstract planning has several interesting parallels with historic AI planning approaches. The freedom to realize a plan with any stronger action substituted in is reminiscent of satisficing <ref> (Simon, 1955) </ref>; the planner on recursive calls needs only construct a realization of the action of adequate strength, rather than optimize, in order to fulfill the role of the action in producing the abstract approximation to the global optimum.
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-43. </pages>
Reference: <author> Sutton, R. S. </author> <year> (1991). </year> <title> Planning by incremental dynamic programming. </title> <editor> In (Birnbaum and Collins, </editor> <year> 1991), </year> <pages> pages 353-357. </pages>
Reference: <author> Tash, J. </author> <year> (1993). </year> <title> A framework for planning under uncertainty. </title> <booktitle> In Spring Symposium on Foundations of Automatic Planning. </booktitle> <publisher> AAAI. </publisher>
Reference-contexts: Such a representation will expand the state space enormously its self-referentiality can even cause an infinite blowup so careful clustering abstractions would be needed to manage the larger space. Such a definition for planning is cursorily treated in <ref> (Tash, 1993) </ref>. There are many ways in which the traditional clustering abstraction methods of the previous chapter could increase the efficacy of a planner using these new abstraction methods. Estimating realizability from a summary of previous cases of action abstraction would be one use of clustering methods.
Reference: <author> Tash, J. </author> <year> (1994a). </year> <title> Formal rationality and limited agents. </title> <booktitle> In Proceedings of the Sixteenth Annual Conference of the Cognitive Science Society. </booktitle> <publisher> Lawrence Erlbaum. </publisher>
Reference: <author> Tash, J. </author> <year> (1994b). </year> <title> Issues in constructing and learning abstract decision models. </title> <booktitle> In Spring Symposium on Decision-Theoretic Planning. </booktitle> <publisher> AAAI. </publisher>
Reference: <author> Tash, J. and Russell, S. </author> <year> (1994). </year> <title> Control strategies for a stochastic planner. </title> <booktitle> In (AAAI, </booktitle> <year> 1994). </year>
Reference-contexts: If one wanted, one could generalize from states seen to states unseen but with the same Manhattan distance, in order to learn a less coarse P (V i ) with experience. (TD learn 64 65 ing was used to this end in <ref> (Tash and Russell, 1994) </ref>.) In order to isolate the performance improvements due to our computational control mechanism more effectively, however, we will use this and simpler priors. 5.5 The Fixed Variation Approximation to V This section elaborates how our planning system can account for the expense of computing by making tradeoffs
Reference: <author> Thiebaux, S., Hertzberg, J., Shoaff, W., and Schneider, M. </author> <year> (1994). </year> <title> A stochas 110 tic model of actions and plans for anytime planning under uncertainty. </title>
Reference-contexts: This strategy is similar to (but was developed independently of) those described in the papers of (Dean et al., 1993) and <ref> (Thiebaux et al., 1994) </ref>; however, its use of estimated values on non-envelope states provides important additional information for determining envelope action choices, allowing for meaningful action choices to be made before a path to the goal is found, and summarizing previous computational effort outside the current envelope. <p> These results are asymptotic. They are not available to envelope algorithms that do not provide similar guarantees of adequate exploration (such as those of (Dean et al., 1993) and <ref> (Thiebaux et al., 1994) </ref>).
Reference: <editor> Int. J. </editor> <booktitle> of Intelligent Systems. (in press). UAI (1987). Proceedings of the Third Workshop on Uncertainty in Artificial Intelligence. UAI (1993). Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> Washington, D.C. </address> <publisher> Morgan Kaufmann. </publisher> <month> UAI </month> <year> (1994). </year> <booktitle> Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann. </publisher> <editor> von Neumann, J. </editor> <booktitle> (1955). Mathematical Foundations of Quantum Mechanics. </booktitle> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ. </address>
Reference: <author> Winograd, T. and Flores, F. </author> <year> (1987). </year> <title> Understanding Computers and Cognition. </title> <publisher> Addison Wesley, </publisher> <address> Reading, MA. </address> <month> 111 </month>
References-found: 42

