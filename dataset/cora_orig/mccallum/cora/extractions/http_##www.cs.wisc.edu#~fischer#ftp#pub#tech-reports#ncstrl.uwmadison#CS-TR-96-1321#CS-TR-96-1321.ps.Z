URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-96-1321/CS-TR-96-1321.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-96-1321/
Root-URL: http://www.cs.wisc.edu
Title: ARCHITECTURAL CONSIDERATIONS FOR PARALLEL QUERY EVALUATION ALGORITHMS  
Author: By Ambuj Shatdal 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Sciences) at the  
Date: 1996  
Address: WISCONSIN MADISON  
Affiliation: UNIVERSITY OF  
Abstract-found: 0
Intro-found: 1
Reference: [AB86] <author> J. Archibald and J.-L. Baer. </author> <title> Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4) </volume> <pages> 273-298, </pages> <year> 1986. </year>
Reference-contexts: Coherence implies that the data values are guaranteed to be consistent under some coherence semantics, the most common being the sequential semantics i.e. a read returns the value of the most recent preceding write. There are several cache coherence algorithms mentioned in the literature <ref> [AB86] </ref>. For SMPs, which have a shared bus, a possible algorithm is the snooping bus write invalidate. In this algorithm, all cache controllers listen on the system bus and when a write occurs on a cached address, the cached value is invalidated.
Reference: [BBDW83] <author> Dina Bitton, Haran Boral, David J. DeWitt, and W. Kevin Wilkinson. </author> <title> Parallel algorithms for the execution of relational database operations. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 8(3) </volume> <pages> 324-353, </pages> <month> September </month> <year> 1983. </year>
Reference-contexts: This aspect of the algorithms allows them to proceed adaptively without any global synchronization. There has been little work reported in the literature on aggregate processing. Epstein [Eps79] discusses some algorithms for computing scalar aggregates and aggregate functions on a uniprocessor. Bitton et al. <ref> [BBDW83] </ref> discuss two sorting based algorithms for aggregate processing on a shared-disk cache architecture. The first algorithm is somewhat similar to the Two Phase approach mentioned above in that it uses local aggregation.
Reference: [BCL93] <author> Kurt P. Brown, Michael J. Carey, and Miron Livny. </author> <title> Managing Memory to Meet Multiclass Workload Response Time Goals. </title> <booktitle> In Proc. of 19th VLDB Conf., </booktitle> <pages> pages 328-341, </pages> <year> 1993. </year>
Reference-contexts: We assume that the aggregation is being performed directly on a base relation stored on disks as in the example query. The parameters of the study are listed in Table 3 unless otherwise specified. These parameters are similar to those in previous studies e.g. <ref> [BCL93] </ref>. The CPU 52 speed is chosen to reflect the characteristics of the current generation of commercially available microprocessors. The I/O rate was as observed on the SUN disk on the SUN SparcServer20/51. We model a high speed, high bandwidth network as in commercial multiprocessors like IBM SP-2.
Reference: [BF93] <author> J. Bunge and M. Fitzpatrick. </author> <title> Estimating the Number of Species: A Review. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 88(421) </volume> <pages> 364-373, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: This does not require an accurate estimate of the number of groups, especially when it is large, making the problem significantly simpler than the general estimation problem which is fairly complex <ref> [BF93] </ref>. The basic scheme is as follows. First the optimizer will decide what is an appropriate switching point of the algorithms depending on the system characteristics. A reasonable number of groups for switching would be some where in the middle range where both algorithms perform well.
Reference: [BFG + 95] <author> Chaitanya Baru, Gilles Fecteau, Ambuj Goyal, Hui i Hsiao, Anant Jhingran, Sriram Padmanabhan, and Walter Wilson. </author> <title> An Overview of DB2 Parallel Edition. </title> <booktitle> In Proc. of the 1995 ACM-SIGMOD Conference, </booktitle> <address> San Jose, CA, </address> <year> 1995. </year>
Reference-contexts: Finally, even without the technical arguments for SMPs, the reality is that today most parallel machines sold are SMPs. Today, however, most scalable parallel database systems are designed for the shared-nothing hardware paradigm. This includes Informix XPS [Ger95], IBM DB2/PE <ref> [BFG + 95] </ref>, Sybase Navigation Server [Syb], Tandem [Tan87], and Teradata [Ter83].
Reference: [Bhi88] <author> Anupam Bhide. </author> <title> An Analysis of Three Transaction Processing Architectures. </title> <booktitle> In Proc. of 14th Int'l Conference on Very Large Data Bases, </booktitle> <pages> pages 339-350, </pages> <address> Los Angeles, CA, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: In the past, there have been studies comparing or advocating the relative merits of different parallel architectures. For example, Bhide <ref> [Bhi88] </ref> compares the shared-memory, the shared-nothing and the shared-disk architectures for on-line transaction processing (OLTP) queries. Frequently such studies compare significantly different architectures, and tautologically claim that they are different. There is no database study comparing the different software architectures 1 and on the same or similar hardware platforms.
Reference: [CBZ91] <author> John B. Carter, J. K. Bennet, and Willy Zwaenepoel. </author> <title> Implementation and performance of munin. </title> <booktitle> In Proceedings of the 1991 Symposium on Operating System Principles, </booktitle> <pages> pages 152-164, </pages> <year> 1991. </year>
Reference-contexts: This bias is further aggravated by the fact that most shared-nothing machines are optimized for large bulk (message based) transfers [Cha96]. However, it is possible to build shared-memory software architecture on a shared-nothing hardware. Shared virtual memory <ref> [LH89, CBZ91] </ref> provides a single virtual address space shared by all the processors in a shared-nothing architecture. This is achieved through memory mapping managers that implement the mapping between shared virtual memory and local (physical) memories. <p> desirable properties of this algorithm are that, unlike most previously proposed skew-handling join processing algorithms, in the "no skew" case the performance is virtually identical to that of parallel hybrid hash, and that the algorithm generalizes easily to handle multi-way joins. 9 2.2 Brief Overview of SVM Shared virtual memory <ref> [LH89, CBZ91] </ref> provides a single virtual address space shared by all the processors in a shared-nothing architecture. <p> This is mainly because page faults depend principally on program properties and only indirectly on the coherence mechanism. In recent years, research into SVM have produced significant performance enhancements by exploiting specific patterns of sharing in the system <ref> [CBZ91] </ref>. This results in a reduction in the number and size of messages. <p> A hashed partitioning strategy is used, where a randomizing function is applied to the key attribute of each tuple to select a particular disk drive. The performance of SVM is modeled explicitly for the shared hash tables under assumptions similar to those made in <ref> [LH89, CBZ91] </ref>. The status variables are ignored because they are updated very infrequently.
Reference: [Cha96] <author> Satish Chandra, </author> <month> June </month> <year> 1996. </year> <type> Personal Communication. </type>
Reference-contexts: Most algorithms designed for the shared-nothing hardware assume that message passing is the only way of communication between processors. This bias is further aggravated by the fact that most shared-nothing machines are optimized for large bulk (message based) transfers <ref> [Cha96] </ref>. However, it is possible to build shared-memory software architecture on a shared-nothing hardware. Shared virtual memory [LH89, CBZ91] provides a single virtual address space shared by all the processors in a shared-nothing architecture.
Reference: [CLR94] <author> Satish Chandra, James R. Larus, and Anne Rogers. </author> <title> Where is time spent in message-passing and shared-memory programs? In Proc. </title> <booktitle> of the 6th ASPLOS Conference, </booktitle> <pages> pages 61-75, </pages> <month> October </month> <year> 1994. </year> <month> 88 </month>
Reference-contexts: Though many times the software architectures go hand in 2 any software architecture on any hardware, one should compare the different software architectures on similar hardware architectures as done in <ref> [CLR94] </ref> or compare different architectures that are dollar cost-wise equivalent. First, we start with the belief and the argument that both of these hardware architectures differ only in the speed at which a different processor's memory can be reached (as compared to the local processor). <p> An access to a shared variable is even more likely to result in a cache miss because of the maintenance of cache coherence. Consequently, cost of data sharing can not be ignored while designing efficient query processing algorithms for the SMPs. Chandra et al. in <ref> [CLR94] </ref> showed that for some scientific applications, optimized shared-memory and message passing programs perform comparably on comparable hardware. 3.2 The Experimental Setup We compare the performance of the algorithms by implementing them on a SGI PowerChallenge SMP.
Reference: [DG85] <author> David M. DeWitt and Robert Gerber. </author> <title> Multiprocessor hash-based join algorithms. </title> <booktitle> In Proceedings of the 12th International Conference on Very Large Databases, </booktitle> <pages> pages 151-164, </pages> <address> Stockholm, Sweden, </address> <year> 1985. </year>
Reference-contexts: Our parallel join processing algorithm uses both message passing and SVM. Briefly, stream-oriented processing is handled by message passing, while access to shared data structures is provided in SVM. Both of our parallel join processing algorithms are based upon the parallel hybrid hash join <ref> [DG85, SD89] </ref>. In the absence of skew, this algorithm has been shown to have the best performance. However, in the presence of skew, the performance of hybrid hash join degrades since the response time of the parallel join is limited by that of the slowest processor in the join. <p> This information is used in the actual repartitioning of the relations. Using an analytical model to compare the scheduling hash-join algorithm of [WDYT90] and the hybrid hash-join algorithm of Gamma <ref> [DG85, SD89, DGS + 90] </ref>, Walton et al. conclude that scheduling hash effectively handles redistribution skew while hybrid hash degrades and eventually becomes worse than scheduling hash as redistribution skew increases. <p> Any result tuples from the queries are "sent back to the terminals," an operation that consumes some small amount of processor cycles for network protocol overheads 3 . The default join algorithm used in the simulator is the hybrid hash join algorithm <ref> [DG85, SD89] </ref>. The database itself is modeled as a set of relations. All relations are declustered [RE78, LKB87] (horizontally partitioned) across all the disks in the configuration.
Reference: [DGS + 90] <author> D. DeWitt, S. Ghandeharizadeh, D. Schneider, A. Bricker, H.-I Hsiao, and R. Ras-mussen. </author> <title> The Gamma database machine project. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 2(1), </volume> <month> March </month> <year> 1990. </year>
Reference-contexts: This information is used in the actual repartitioning of the relations. Using an analytical model to compare the scheduling hash-join algorithm of [WDYT90] and the hybrid hash-join algorithm of Gamma <ref> [DG85, SD89, DGS + 90] </ref>, Walton et al. conclude that scheduling hash effectively handles redistribution skew while hybrid hash degrades and eventually becomes worse than scheduling hash as redistribution skew increases. <p> This realization led us to develop the dual-paradigm algorithm described in the next subsection. 2.4.2 A Dual-Paradigm Algorithm Our dual-paradigm algorithm begins exactly like the basic parallel hybrid hash algorithm <ref> [DGS + 90] </ref> which is described below. Suppose the join is of relations R and S, say with the join condition R:A = S:B, and that R has been chosen as the building relation. At a high level, the basic parallel hybrid hash join [DGS + 90] proceeds in two stages: <p> the basic parallel hybrid hash algorithm <ref> [DGS + 90] </ref> which is described below. Suppose the join is of relations R and S, say with the join condition R:A = S:B, and that R has been chosen as the building relation. At a high level, the basic parallel hybrid hash join [DGS + 90] proceeds in two stages: 14 1. Build. Each processor p i scans its local fragment of R. As each tuple r in R is processed, the processor computes a hash function h 1 (r:A). <p> This is because it mitigates redistribution skew by doing range partitioning. 2.5.1 Simulation Methodology The simulator is based upon an earlier, event-driven simulation model of the Gamma parallel database machine <ref> [DGS + 90] </ref>. The earlier model was a useful starting point since it had been validated against the actual Gamma implementation; it had also been used extensively in previous work on parallel database machines [GD90, SD90, HD91]. <p> This is generally much faster than speeds achievable in a traditional shared-nothing interconnection network. Since the hardware is fairly fast, the software cost of message passing becomes important and we find the same in our experiments detailed below. A shared-nothing hash join algorithm, as in Gamma <ref> [DGS + 90] </ref>, first repartitions the relations by hashing the tuples on the join attribute, each hash value being assigned a specific node. Each node then joins the partitions of the relations locally as if it were the only node in the system. <p> In the remainder of the chapter we will assume that aggregation is always accompanied with GROUP BY and that scalar aggregation can be considered as a special case where the number of groups is 1. Further, we assume a Gamma <ref> [DGS + 90] </ref> like architecture where each relational operation is represented by operators. The data "flows" through the operators in a pipelined fashion as far as possible. For example, a join of two base relations is implemented as two select operators followed by a join operator. <p> The overflow buckets are processed one by one as in step 1 above. In the following sections we briefly describe the two traditional approaches to parallel aggregation. The first being the Centralized Two Phase algorithm <ref> [DGS + 90] </ref>. 4.2.1 Centralized Two Phase Algorithm Considering the structure of the shared-nothing architecture, the most intuitive approach for parallel aggregation is for each node to do aggregation on the locally generated (or read) tuples in the first phase and then merge these local aggregate values at a central coordinator <p> One way to minimize the number of conflicts in accessing the hash table is to let each processor process a part of the relation independently, as done in the shared-nothing style algorithms <ref> [DGS + 90] </ref>. However, that requires an additional merge phase as detailed below. 5.3.2 Shared Nothing Two Phase Approach on SMP The traditional shared-nothing Centralized Two Phase algorithm (see Section 4.2.1) works as follows.
Reference: [DNSS92] <author> David J. DeWitt, Jeffrey F. Naughton, Donovan A. Schneider, and S. Seshadri. </author> <title> Practical skew handling in parallel joins. </title> <booktitle> In Proceedings of the 19th International Conference on Very Large Databases, </booktitle> <address> Vancouver, British Columbia, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Performance evaluation shows the algorithms to be quite promising in handling data skew. DeWitt et al. <ref> [DNSS92] </ref> investigate the use of sampling coupled with range partitioning (instead of hash partitioning) to balance the work among processors by mitigating redistribution skew. The algorithm was very successful in handling redistribution skew, but much less successful in dealing with join product skew. <p> This is to allow the subsetting of the S buckets other than bucket zero. It is possible that this could require an extra redistribution of some 1 Set valued split tables were also used in <ref> [DNSS92] </ref>. 16 probing tuples; however, in today's multicomputers, with up to 200 MB/sec interconnects, the cost of this redistribution will be insignificant when compared to the cost of the read off the disk. A simplified pseudocode version of the join operator code appears in Figure 3. <p> To see this, note that there is no provision for moving hash table pages from one processor to another during the building phase. To handle this problem, we can use the technique proposed in <ref> [DNSS92] </ref>. The idea is that instead of using hashing to partition the relation among the processors of the system, we use range partitioning. <p> Basic Basic Parallel Hybrid Hash Join. Sampling Sampling based Range Partitioning Hybrid Hash Join. This is the algorithm from <ref> [DNSS92] </ref>. Basic/SVM Basic SVM Hybrid Hash Join. This is the dual-paradigm algorithm discussed in Section 2.4.2. Sampling/SVM Sampling based SVM Range Partitioning Hybrid Hash Join. This is the sampling variant referred to in Section 2.4.3.
Reference: [ELZ86] <author> Derek L. Eager, E. D. Lazowska, and J. Zahorjan. </author> <title> Adaptive load sharing in homogeneous distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-12(5):662-675, </volume> <month> May </month> <year> 1986. </year>
Reference-contexts: Thus, instead of precomputing the load at each node and balancing it, as is done in many previous proposals for skew handling join algorithms, each node follows the policy of "don't be idle if there is work left" using a simple heuristic for selecting a processor. Eager et al. <ref> [ELZ86] </ref> shows and we verify that a simple heuristic, like random, for selecting a busy processor works almost as well as more complex kinds (e.g. those involving estimated work left). 2.4.3 A Sampling Variant The algorithm as described in the previous subsection deals elegantly with join product skew by being adaptive
Reference: [Eps79] <author> Robert Epstein. </author> <title> Techniques for Processing of Aggregates in Relational Database Systems. </title> <institution> Memorandum UCB/ERL M79/8, Electronics Research Laboratory, College of Engineering, University of California, Berkeley, </institution> <month> February </month> <year> 1979. </year>
Reference-contexts: This aspect of the algorithms allows them to proceed adaptively without any global synchronization. There has been little work reported in the literature on aggregate processing. Epstein <ref> [Eps79] </ref> discusses some algorithms for computing scalar aggregates and aggregate functions on a uniprocessor. Bitton et al. [BBDW83] discuss two sorting based algorithms for aggregate processing on a shared-disk cache architecture. The first algorithm is somewhat similar to the Two Phase approach mentioned above in that it uses local aggregation.
Reference: [ER61] <author> Paul Erdos and Alfred Renyi. </author> <title> On a Classical Problem of Probability Theory. MTA Mat. </title> <booktitle> Kut. Int. Kozl, 6A:215-220, 1961. Also in Selected Papers of Alfred Renyi, </booktitle> <volume> volume 2, </volume> <pages> pages 617-621, </pages> <address> Akademiai Kiado, Budapest. </address>
Reference-contexts: This trade-off is explored later. It can be shown that the number of samples required is fairly small (about 10 times the crossover threshold) <ref> [ER61] </ref>. For example, for a crossover threshold of 320 this is approximately 59 2563. This is likely to be less than 1% of any reasonably sized relation for small crossover thresholds. The cost of this algorithm depends upon which algorithm is actually chosen.
Reference: [ESW78] <author> Robert Epstein, Michael Stonebraker, and Eugene Wong. </author> <title> Distributed query processing in a relational database system. </title> <booktitle> In Proceedings of the ACM-SIGMOD International Conference on Management of Data, </booktitle> <year> 1978. </year>
Reference-contexts: Effectively, the local bucket hash table at p 2 has been replicated to p 1 in a lazy, "copy-on-reference" fashion. Note that this accomplishes a dynamic subset-replicate <ref> [ESW78] </ref> join of R 2 and S 2 , with R 2 being replicated (in the hash tables) and S 2 being subsetted (by the random selection of p 1 or p 2 .) In this way, once p 1 has updated the forwarding table entry for p 2 , p
Reference: [FW78] <author> S. Fortune and J. Wyllie. </author> <title> Parallelism in Random Access Machines. </title> <booktitle> In Proc. of the 10th Annual Symposium on Theory of Computing, </booktitle> <year> 1978. </year>
Reference-contexts: This makes the communication of the data value comparatively slow compared to non-shared memory accesses most of which are expected to be hits in the local cache. The naive theoretical PRAM (Parallel Random Access Memory) model <ref> [FW78] </ref> of parallel computation assumes that all memory accesses are of uniform cost. This assumption is commonly made in design and analysis of query processing algorithms for shared memory. However, it is clear that SMPs do not match the PRAM model.
Reference: [GD90] <author> Shahram Ghandeharizadeh and David J. DeWitt. </author> <title> Hybrid-range partitioning strategy: A new declustering strategy for multiprocessor database machines. </title> <booktitle> In Proceedings of the 16th VLDB Conference, </booktitle> <address> Brisbane, Australia, </address> <month> September </month> <year> 1990. </year>
Reference-contexts: The earlier model was a useful starting point since it had been validated against the actual Gamma implementation; it had also been used extensively in previous work on parallel database machines <ref> [GD90, SD90, HD91] </ref>. The new simulator, which is much more modular, is written in the CSIM/C++ process-oriented simulation language [Sch90]. The simulator accurately captures the algorithms and techniques used in Gamma.
Reference: [Ger95] <author> Bob Gerber. </author> <title> Informix Online XPS. </title> <booktitle> In Proc. of the 1995 ACM-SIGMOD Conference, </booktitle> <address> San Jose, CA, </address> <year> 1995. </year> <month> 89 </month>
Reference-contexts: Finally, even without the technical arguments for SMPs, the reality is that today most parallel machines sold are SMPs. Today, however, most scalable parallel database systems are designed for the shared-nothing hardware paradigm. This includes Informix XPS <ref> [Ger95] </ref>, IBM DB2/PE [BFG + 95], Sybase Navigation Server [Syb], Tandem [Tan87], and Teradata [Ter83].
Reference: [Gra93] <author> Goetz Graefe. </author> <title> Query Evaluation Techniques for Large Databases. </title> <journal> ACM Computing Surveys, </journal> <volume> 25(2) </volume> <pages> 73-170, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: This is impractical on today's multiprocessor interconnects, which do not efficiently support broadcasting. Su et al. [SM82] discuss an implementation of the traditional approach. Graefe <ref> [Gra93] </ref> discusses one optimization for dealing with bucket overflow for the Two Phase 50 algorithm. 4.2 The Background An SQL aggregate function is a function that operates on groups of tuples. <p> The sequential bottleneck can be overcome by parallelizing the merging phase resulting in the Two Phase algorithm <ref> [Gra93] </ref>. <p> The hash table will have the final aggregated values for all group values that hash to that node. Here we must mention that <ref> [Gra93] </ref> points out another optimization to the Two Phase algorithm. It suggests that in the local aggregation phase, if the hash table is full then the locally generated tuples are hash partitioned and forwarded to the local aggregation phase.
Reference: [HD91] <author> H. I. Hsiao and David J. DeWitt. </author> <title> A performance study of three high-availability data replication strategies. </title> <booktitle> In Proceedings of the 1st International Conference on Parallel and Distributed Systems, </booktitle> <address> Miami, FL, </address> <month> December </month> <year> 1991. </year>
Reference-contexts: The earlier model was a useful starting point since it had been validated against the actual Gamma implementation; it had also been used extensively in previous work on parallel database machines <ref> [GD90, SD90, HD91] </ref>. The new simulator, which is much more modular, is written in the CSIM/C++ process-oriented simulation language [Sch90]. The simulator accurately captures the algorithms and techniques used in Gamma.
Reference: [HL91] <author> Kien A. Hua and Chiang Lee. </author> <title> Handling data skew in multiprocessor database computers using partition tuning. </title> <booktitle> In Proceedings of the 17th International Conference on Very Large Databases, </booktitle> <pages> pages 525-535, </pages> <address> Barcelona, Spain, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: This result is confirmed in our work. The bucket-spreading parallel hash join [KO90] and its variant tuple interleaving hash join <ref> [HL91] </ref> balance the redistribution skew by ensuring that processors get approximately same number of tuples for the final join phase. This involves sending the tuples twice over the network. Adaptive load balancing hash join [HL91] algorithm attempts to balance the load statically by relocating buckets after initial partition. <p> The bucket-spreading parallel hash join [KO90] and its variant tuple interleaving hash join <ref> [HL91] </ref> balance the redistribution skew by ensuring that processors get approximately same number of tuples for the final join phase. This involves sending the tuples twice over the network. Adaptive load balancing hash join [HL91] algorithm attempts to balance the load statically by relocating buckets after initial partition. Its extended version [HL91] is similar to tuple interleaving hash join but avoids the extra network cost by storing tuples locally. <p> This involves sending the tuples twice over the network. Adaptive load balancing hash join <ref> [HL91] </ref> algorithm attempts to balance the load statically by relocating buckets after initial partition. Its extended version [HL91] is similar to tuple interleaving hash join but avoids the extra network cost by storing tuples locally.
Reference: [HS91] <author> W. Hong and M. Stonebraker. </author> <title> Optimization of Parallel Query Execution Plans in XPRS. </title> <booktitle> In Proceedings of the 1st Int'l Conf. on Parallel and Distributed Information Systems, </booktitle> <address> Miami, Florida, </address> <month> December </month> <year> 1991. </year>
Reference-contexts: The access conflicts to the hash table are taken care of by latching. Thereafter, all processors read their partition of relation S, probing the shared hash table. This approach is explicitly used in [LTS90] and is implicit in the XPRS database system <ref> [HS91] </ref>. This Shared Memory/Naive algorithm is detailed below.
Reference: [HT88] <author> Meichun Hsu and Van-On Tam. </author> <title> Managing databases in distributed virtual memory. </title> <type> Technical Report TR-07-88, </type> <institution> Aiken Computation Lab., Harvard Univ., </institution> <month> March </month> <year> 1988. </year>
Reference-contexts: Shared virtual memory has received very little attention in previous database literature|the only work of which we are aware is some early work by the Hsu et al. on transaction processing in an SVM system (see <ref> [HT89, HT88] </ref> for examples of this work). To our knowledge, no work has appeared on query processing in systems with SVM. 2.4 Join Processing using SVM In this section we consider approaches to using SVM for join processing.
Reference: [HT89] <author> Meichun Hsu and Va-On Tam. </author> <title> Transaction synchronization in distributed shared virtual memory systems. </title> <type> Technical Report TR-05-89, </type> <institution> Center for Research in Computing Technology, Harvard University, </institution> <year> 1989. </year>
Reference-contexts: Shared virtual memory has received very little attention in previous database literature|the only work of which we are aware is some early work by the Hsu et al. on transaction processing in an SVM system (see <ref> [HT89, HT88] </ref> for examples of this work). To our knowledge, no work has appeared on query processing in systems with SVM. 2.4 Join Processing using SVM In this section we consider approaches to using SVM for join processing.
Reference: [KO90] <author> Masaru Kitsuregawa and Yasushi Ogawa. </author> <title> Bucket spreading parallel hash: A new, robust, parallel hash join method for data skew in the Super Database Computer (SDC). </title> <booktitle> In Proceedings of the 16th International Conference on Very Large Data Bases, </booktitle> <address> Brisbane, England, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: This result is confirmed in our work. The bucket-spreading parallel hash join <ref> [KO90] </ref> and its variant tuple interleaving hash join [HL91] balance the redistribution skew by ensuring that processors get approximately same number of tuples for the final join phase. This involves sending the tuples twice over the network. <p> Omiecinski [Omi91] proposed a load balancing hash-join algorithm for systems running on shared physical memory multiprocessors. The algorithm is based on the bucket-spreading algorithm of Kitsuregawa and Ogawa <ref> [KO90] </ref>. Analytical and limited experimental results from a 10 processor Sequent machine show that the algorithm is effective in limiting the effects of attribute value skew for double-skew joins, but again the author did not compare the performance of the algorithm with basic parallel join algorithms.
Reference: [LH89] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: This bias is further aggravated by the fact that most shared-nothing machines are optimized for large bulk (message based) transfers [Cha96]. However, it is possible to build shared-memory software architecture on a shared-nothing hardware. Shared virtual memory <ref> [LH89, CBZ91] </ref> provides a single virtual address space shared by all the processors in a shared-nothing architecture. This is achieved through memory mapping managers that implement the mapping between shared virtual memory and local (physical) memories. <p> desirable properties of this algorithm are that, unlike most previously proposed skew-handling join processing algorithms, in the "no skew" case the performance is virtually identical to that of parallel hybrid hash, and that the algorithm generalizes easily to handle multi-way joins. 9 2.2 Brief Overview of SVM Shared virtual memory <ref> [LH89, CBZ91] </ref> provides a single virtual address space shared by all the processors in a shared-nothing architecture. <p> This protocol allows multiple reader processes to share a page by replication but a writer process obtains an exclusive copy by invalidating the other copies. There are several alternatives to implement the mapping managers using this idea. These are discussed at length in <ref> [LH89] </ref>. When a process wants to access a page which is not in its physical memory, it suffers a page fault. The missing page is then brought in either from the memory of some other processor or from the disk. <p> A hashed partitioning strategy is used, where a randomizing function is applied to the key attribute of each tuple to select a particular disk drive. The performance of SVM is modeled explicitly for the shared hash tables under assumptions similar to those made in <ref> [LH89, CBZ91] </ref>. The status variables are ignored because they are updated very infrequently.
Reference: [LKB87] <author> Miron Livny, S. Khoshafian, and Haran Boral. </author> <title> Multi-disk management algorithms. </title> <booktitle> In Proceedings of the 1987 ACM-SIGMETRICS Conference, </booktitle> <address> Banff, Alberta, Canada, </address> <month> May </month> <year> 1987. </year>
Reference-contexts: The default join algorithm used in the simulator is the hybrid hash join algorithm [DG85, SD89]. The database itself is modeled as a set of relations. All relations are declustered <ref> [RE78, LKB87] </ref> (horizontally partitioned) across all the disks in the configuration. A hashed partitioning strategy is used, where a randomizing function is applied to the key attribute of each tuple to select a particular disk drive.
Reference: [LT91] <author> Hongjun Lu and Kian-Lee Tan. </author> <title> A dynamic load-balanced task-oriented approach to parallel query processing. </title> <type> Technical Report Discs Publication TRC7/91, </type> <institution> Department of Information Systems and Computer Science, National University of Singapore, </institution> <month> July </month> <year> 1991. </year> <month> 90 </month>
Reference-contexts: Analytical and limited experimental results from a 10 processor Sequent machine show that the algorithm is effective in limiting the effects of attribute value skew for double-skew joins, but again the author did not compare the performance of the algorithm with basic parallel join algorithms. Lu and Tan <ref> [LT91] </ref> present a dynamic task-oriented algorithm for load balancing in a shared-disk and hybrid environment having both private and shared memory.
Reference: [LTS90] <author> Hongjun Lu, Kian-Lee Tan, and Ming-Chien Shan. </author> <title> Hash-Based Join Algorithms for Multiprocessor Computers with Shared Memory. </title> <booktitle> In Proceedings of the 16th VLDB Conference, </booktitle> <pages> pages 198-209, </pages> <address> Brisbane, Australia, </address> <month> September </month> <year> 1990. </year>
Reference-contexts: Unless the memory of each processor was large enough to hold the entire global hash table, many of these hash table pages would thrash to and from the local disks of the processors. Other hash based algorithms <ref> [LTS90, Omi91] </ref> suffer the same fate for the same reasons, lack of processor locality and swamping due to replication of hash tables. When we began our work on this problem we developed a series of algorithms that attempted to use the SVM carefully to avoid these two problems. <p> shared-memory hash join algorithm that extends from the uniprocessor hash join where all nodes build a shared hash table and 1 The I/O costs were not considered as the machine had poor I/O support for a realistic study. 33 then each node probes its tuples in the shared hash table <ref> [LTS90] </ref>. The traditional algorithm implemented naively performed poorly. The cause of the poor performance was located in a variable which was possibly suffering from false-sharing, i.e. the variable was on a cache line which had other variable beings accessed by different processors. <p> In general, therefore, we found that the shared-nothing and the shared-memory algorithms are comparable in performance on an SMP. This is because despite our simplifications that exaggerated the differences between algorithms, there was not much difference in performance. In related work, Lu et al. in <ref> [LTS90] </ref> did an analytical performance of algorithms in a shared-everything environment. However, they considered different basic algorithms comparing hybrid hash, hash loop, etc. The analytical model did not truly reflect an SMP environment. Furthermore, the considered data domain was too restricted which resulted in misleading conclusions. <p> The access conflicts to the hash table are taken care of by latching. Thereafter, all processors read their partition of relation S, probing the shared hash table. This approach is explicitly used in <ref> [LTS90] </ref> and is implicit in the XPRS database system [HS91]. This Shared Memory/Naive algorithm is detailed below.
Reference: [LY90] <author> M. Seetha Lakshmi and Philip S. Yu. </author> <title> Effectiveness of parallel joins. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 2(4), </volume> <month> December </month> <year> 1990. </year>
Reference-contexts: In practice, this implied that given any unbiased partitioning strategy, each processor was likely to have the same amount of work to do. With time this assumption has been challenged (see, e.g. <ref> [LY90, SD89] </ref>) by the claim that many real data sets are not uniform but suffer from data skew. In the presence of such skew, an unbiased partitioning strategy like hashing will result in unequal load on participating processors.
Reference: [Mes94] <author> Message Passing Interface Forum. </author> <title> MPI: A Message Passing Interface Standard. </title> <journal> Int'l Journal of Supercomputing Applications, </journal> <volume> 8(3/4), </volume> <year> 1994. </year>
Reference-contexts: Unfortunately, such limitations only erode one of the most touted benefits of shared-memory programming model i.e. ease of programming. Implementing the message passing programming model on top of shared-memory hardware is fairly easy. In fact, some shared-memory machines now provide the standard MPI message passing interface <ref> [Mes94] </ref>. Unfortunately, message passing, implemented naively and in a transparent way usually shows inferior performance than using even inefficient shared memory. 6 Efficient message passing must have no extra memory copies (a bane of most message passing systems). <p> The question 36 as mentioned earlier is: how will the algorithm perform if the SMP provides a message passing library just like a shared-nothing architecture? Since no standard implementation of message passing, like MPI <ref> [Mes94] </ref>, was available on the machine, we implemented two different message passing libraries to evaluate the performance of the shared-nothing algorithm.
Reference: [Oak93] <institution> Oak Ridge National Laboratory. </institution> <note> PVM 3 User's Guide and Reference Manual, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: The nodes are connected by a high-performance switch. We implemented the algorithms on top of the UNIX file system using the PVM parallel library <ref> [Oak93] </ref>. Our implementation had no concurrency control and did not use the buffer pool. Hence the algorithms are significantly more CPU efficient than what would be found in a complete database system. The four million 104 byte tuple relation was partitioned in a round-robin fashion.
Reference: [Omi91] <author> Edward Omiecinski. </author> <title> Performance analysis of a load balancing hash-join algorithm for a shared memory multiprocessor. </title> <booktitle> In Proceedings of the 17th International Conference on Very Large Data Bases, </booktitle> <address> Barcelona, Spain, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: All these algorithms suffer from two major problems: (1) they can not exploit memory the way hybrid hash join algorithm does thus not performing optimally (this difference is analogous to the difference between hybrid hash and GRACE algorithm); and (2) they do not handle join product skew. Omiecinski <ref> [Omi91] </ref> proposed a load balancing hash-join algorithm for systems running on shared physical memory multiprocessors. The algorithm is based on the bucket-spreading algorithm of Kitsuregawa and Ogawa [KO90]. <p> Unless the memory of each processor was large enough to hold the entire global hash table, many of these hash table pages would thrash to and from the local disks of the processors. Other hash based algorithms <ref> [LTS90, Omi91] </ref> suffer the same fate for the same reasons, lack of processor locality and swamping due to replication of hash tables. When we began our work on this problem we developed a series of algorithms that attempted to use the SVM carefully to avoid these two problems.
Reference: [PI96] <author> Viswanath Poosala and Yannis Ioannidis. </author> <title> Estimation of query-result distribution and its application in parallel-join load balancing. </title> <note> To appear in VLDB '96, </note> <month> July </month> <year> 1996. </year>
Reference-contexts: Also, these algorithms have no obvious extension to handling multiway joins without storing the intermediate relations. A notable exception is a recently proposed algorithm by Poosala and Ioannidis <ref> [PI96] </ref>. We discuss the prominent ones in brief. Walton et al. [WDJ91b] present a taxonomy of skew in parallel databases. <p> While they did not implement their algorithm nor simulate its performance, their analytical model shows that it is effective in handling attribute value skew. Poosala et al. <ref> [PI96] </ref> use query-result size estimation using histograms for load balancing in parallel hybrid hash join algorithm. <p> Our experiments show that if the skew in building relation is not severe, then Basic/SVM algorithm performs best, with performance coming close to ideal in many cases. Unlike most other previously proposed skew handling algorithms, except one proposed by Poosala and Ioannidis in <ref> [PI96] </ref>, it suffers almost no performance degradation under the no skew case. If the skew 2 Every update will result in a message to all the nodes. <p> Unlike most previous skew handling algorithms, which require runtime statistics about their input relations making it hard to extend the algorithms to multi-way join case, the Basic/SVM algorithm supports multi-way joins as given. A notable exeception is the load balancing algorithm proposed by Poosala and Ioannidis <ref> [PI96] </ref> which uses precomputed histograms on the relations for load balancing. One way to combine the sampling and non-sampling variants of the algorithm to get close to optimal performance is to use the Sampling/SVM algorithm when the skew is large and use the Basic/SVM algorithm otherwise.
Reference: [RE78] <author> D. Ries and R. Epstein. </author> <title> Evaluation of distribution criteria for distributed database systems. </title> <type> UCB/ERL Technical Report M78/22, </type> <institution> University of California, Berkeley, </institution> <month> May </month> <year> 1978. </year>
Reference-contexts: The default join algorithm used in the simulator is the hybrid hash join algorithm [DG85, SD89]. The database itself is modeled as a set of relations. All relations are declustered <ref> [RE78, LKB87] </ref> (horizontally partitioned) across all the disks in the configuration. A hashed partitioning strategy is used, where a randomizing function is applied to the key attribute of each tuple to select a particular disk drive.
Reference: [Sch90] <author> Herb Schwetman. </author> <title> Csim users' guide. </title> <type> MCC Tech Report ACT-126-90, </type> <institution> Microelectronics and Computer Technology Corp., </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: The new simulator, which is much more modular, is written in the CSIM/C++ process-oriented simulation language <ref> [Sch90] </ref>. The simulator accurately captures the algorithms and techniques used in Gamma. The remainder of this section provides a more detailed description of the relevant portions of the current simulation model, and concludes with a table of the simulation parameter settings used for this study.
Reference: [SD89] <author> Donovan A. Schneider and David J. DeWitt. </author> <title> A performance evaluation of four parallel join algorithms in a shared-nothing multiprocessor environment. </title> <booktitle> In Proceedings of the ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 110-121, </pages> <address> Portland, Oregon, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Our parallel join processing algorithm uses both message passing and SVM. Briefly, stream-oriented processing is handled by message passing, while access to shared data structures is provided in SVM. Both of our parallel join processing algorithms are based upon the parallel hybrid hash join <ref> [DG85, SD89] </ref>. In the absence of skew, this algorithm has been shown to have the best performance. However, in the presence of skew, the performance of hybrid hash join degrades since the response time of the parallel join is limited by that of the slowest processor in the join. <p> In practice, this implied that given any unbiased partitioning strategy, each processor was likely to have the same amount of work to do. With time this assumption has been challenged (see, e.g. <ref> [LY90, SD89] </ref>) by the claim that many real data sets are not uniform but suffer from data skew. In the presence of such skew, an unbiased partitioning strategy like hashing will result in unequal load on participating processors. <p> This information is used in the actual repartitioning of the relations. Using an analytical model to compare the scheduling hash-join algorithm of [WDYT90] and the hybrid hash-join algorithm of Gamma <ref> [DG85, SD89, DGS + 90] </ref>, Walton et al. conclude that scheduling hash effectively handles redistribution skew while hybrid hash degrades and eventually becomes worse than scheduling hash as redistribution skew increases. <p> However, unless the join is significantly skewed, the absolute performance of hybrid hash is significantly better than that of scheduling hash due to the absence of the initial scan of relations. Schneider and DeWitt <ref> [SD89] </ref> conclude that the parallel hash-based join algorithms (Hybrid, Grace, and Simple) are sensitive to redistribution skew resulting from attribute value skew in the "building" relation (due to hash table overflow) but are relatively insensitive to redistribution skew in the "probing" relation. This result is confirmed in our work. <p> Any result tuples from the queries are "sent back to the terminals," an operation that consumes some small amount of processor cycles for network protocol overheads 3 . The default join algorithm used in the simulator is the hybrid hash join algorithm <ref> [DG85, SD89] </ref>. The database itself is modeled as a set of relations. All relations are declustered [RE78, LKB87] (horizontally partitioned) across all the disks in the configuration.
Reference: [SD90] <author> Donovan Schneider and David J. DeWitt. </author> <title> Tradeoffs in processing complex join queries via hashing in multiprocessor database machines. </title> <booktitle> In Proceedings of the 16th VLDB Conference, </booktitle> <address> Brisbane, Australia, </address> <month> September </month> <year> 1990. </year>
Reference-contexts: The earlier model was a useful starting point since it had been validated against the actual Gamma implementation; it had also been used extensively in previous work on parallel database machines <ref> [GD90, SD90, HD91] </ref>. The new simulator, which is much more modular, is written in the CSIM/C++ process-oriented simulation language [Sch90]. The simulator accurately captures the algorithms and techniques used in Gamma.
Reference: [Ses92] <author> S. Seshadri. </author> <title> Probabilistic Methods in Query Processing. </title> <type> PhD thesis, </type> <institution> University of Wisconsin-Madison, Computer Sciences Department, </institution> <year> 1992. </year> <month> 91 </month>
Reference-contexts: Unfortunately, the number of groups is not usually known 58 beforehand. Sampling has been used effectively in past to estimate DBMS parameters <ref> [Ses92] </ref>. The general problem of accurately estimating the number of groups is similar to the projection estimation problem. <p> Page-oriented random sampling has been shown to be quite effective if there is no correlation between tuples in a page <ref> [Ses92] </ref>. Then the tuples in the sampled pages are aggregated using, possibly, the Centralized Two Phase algorithm. The number of groups obtained from the sample provides a lower bound on the number of groups in the relation.
Reference: [SKN94] <author> Ambuj Shatdal, Chander Kant, and Jeffrey F. Naughton. </author> <title> Cache Conscious Algorithms for Relational Query Processing. </title> <booktitle> In Proc. of 20th Int'l Conference on Very Large Data Bases, </booktitle> <pages> pages 510-521, </pages> <address> Santiago, Chile, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: However, these algorithms performed only comparably to the traditional simple approach. Finally we exploited the fact that the repartitioning algorithm can be cache optimized as the join processing after repar-titioning is local to a processor <ref> [SKN94] </ref>. The cache optimized algorithms performed a little faster than the traditional shared-memory algorithm. In general, therefore, we found that the shared-nothing and the shared-memory algorithms are comparable in performance on an SMP. <p> However, they considered different basic algorithms comparing hybrid hash, hash loop, etc. The analytical model did not truly reflect an SMP environment. Furthermore, the considered data domain was too restricted which resulted in misleading conclusions. Our previous work on cache conscious algorithms <ref> [SKN94] </ref> showed that the cache misses on a uniprocessor machine are expensive as the data has to be fetched from (slow) main memory and therefore the algorithms must be designed taking the cache into account. <p> Partitioning the data further into small cache-size partitions is likely to mitigate this shortcoming, as now the entire partitioned hash table is likely to fit in the cache while being accessed <ref> [SKN94] </ref>. Thus, the hash table is likely to be entirely cache resident while being accessed.
Reference: [SM82] <author> Stanley Y. W. Su and Krishna P. Mikkilineni. </author> <booktitle> Parallel Algorithms and Their Implen-tation in MICRONET. In Proc. of 8th VLDB Conf., </booktitle> <pages> pages 310-324, </pages> <year> 1982. </year>
Reference-contexts: The second algorithm of Bitton et al. uses broadcast of the tuples and lets each node process the tuples belonging to a subset of groups. This is impractical on today's multiprocessor interconnects, which do not efficiently support broadcasting. Su et al. <ref> [SM82] </ref> discuss an implementation of the traditional approach. Graefe [Gra93] discusses one optimization for dealing with bucket overflow for the Two Phase 50 algorithm. 4.2 The Background An SQL aggregate function is a function that operates on groups of tuples.
Reference: [SN93] <author> Ambuj Shatdal and Jeffrey F. Naughton. </author> <title> Using Shared Virtual Memory for Parallel Join Processing. </title> <booktitle> In Proc. of the 1993 ACM-SIGMOD Conference, </booktitle> <pages> pages 119-128, </pages> <address> Washington, D.C., </address> <month> May </month> <year> 1993. </year>
Reference-contexts: In the 47 next chapter we attempt to answer similar questions for the hash based aggregation algorithm. One argument against shared-nothing algorithm has been their poor performance when the data is skewed [WDJ91a]. However, techniques that have proven effective for shared-nothing algorithms, e.g. <ref> [SN93] </ref>, would trivially apply to SMPs. 48 Chapter 4 Hash Aggregation on Shared Nothing Hardware The traditional parallel aggregation algorithms for shared-nothing architectures perform poorly when the number of groups is large, as expected in decision support and online analytical processing (OLAP) work loads.
Reference: [Syb] <institution> Sybase Inc. </institution> <note> Sybase Navigation Server . URL: http://www.sybase.com/Offerings/Servers/navserver.html. </note>
Reference-contexts: Finally, even without the technical arguments for SMPs, the reality is that today most parallel machines sold are SMPs. Today, however, most scalable parallel database systems are designed for the shared-nothing hardware paradigm. This includes Informix XPS [Ger95], IBM DB2/PE [BFG + 95], Sybase Navigation Server <ref> [Syb] </ref>, Tandem [Tan87], and Teradata [Ter83]. While one can certainly run a shared-nothing system on an SMP (using the shared memory as a fast communication network), SMPs offer alternatives to algorithm design not present in shared-nothing machines which include load balancing and easier management (as there are fewer independent nodes).
Reference: [Tan87] <author> Tandem Database Group. </author> <title> Nonstop SQL, A Distributed, High-Performance, High-Reliability Implementation of SQL. </title> <booktitle> In Workshop on High Performance Transaction Systems, Asilomar, </booktitle> <address> CA, </address> <year> 1987. </year>
Reference-contexts: Finally, even without the technical arguments for SMPs, the reality is that today most parallel machines sold are SMPs. Today, however, most scalable parallel database systems are designed for the shared-nothing hardware paradigm. This includes Informix XPS [Ger95], IBM DB2/PE [BFG + 95], Sybase Navigation Server [Syb], Tandem <ref> [Tan87] </ref>, and Teradata [Ter83]. While one can certainly run a shared-nothing system on an SMP (using the shared memory as a fast communication network), SMPs offer alternatives to algorithm design not present in shared-nothing machines which include load balancing and easier management (as there are fewer independent nodes).
Reference: [Ter83] <institution> Teradata Corp. Teradata: DBC/1012 Database Computer Concept and Facilities, </institution> <year> 1983. </year>
Reference-contexts: Today, however, most scalable parallel database systems are designed for the shared-nothing hardware paradigm. This includes Informix XPS [Ger95], IBM DB2/PE [BFG + 95], Sybase Navigation Server [Syb], Tandem [Tan87], and Teradata <ref> [Ter83] </ref>. While one can certainly run a shared-nothing system on an SMP (using the shared memory as a fast communication network), SMPs offer alternatives to algorithm design not present in shared-nothing machines which include load balancing and easier management (as there are fewer independent nodes).
Reference: [WDJ91a] <author> C. B. Walton, A. G. Dale, and R. M. Jenevein. </author> <title> A taxonomy and performance model of data skew effects in parallel joins. </title> <booktitle> In Proceedings of the 17th VLDB Conference, </booktitle> <pages> pages 537-548, </pages> <address> Barcelona, Spain, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: This is achieved by partitioning the workload suitably. In the 47 next chapter we attempt to answer similar questions for the hash based aggregation algorithm. One argument against shared-nothing algorithm has been their poor performance when the data is skewed <ref> [WDJ91a] </ref>.
Reference: [WDJ91b] <author> Christopher B. Walton, Alfred G. Dale, and Roy M. Jenevein. </author> <title> A taxonomy and performance model of data skew effects in parallel joins. </title> <booktitle> In Proceedings of the 17th International Conference on Very Large Data Bases, </booktitle> <address> Barcelona, Spain, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: Also, these algorithms have no obvious extension to handling multiway joins without storing the intermediate relations. A notable exception is a recently proposed algorithm by Poosala and Ioannidis [PI96]. We discuss the prominent ones in brief. Walton et al. <ref> [WDJ91b] </ref> present a taxonomy of skew in parallel databases. They made the distinction between attribute value skew, which is skew inherent in the dataset, and partition skew, which occurs in parallel machines when the load is not balanced between the nodes. <p> The number of tuples/node is the same but the number of groups/node varies. We call this output skew. This can be contrasted with data skew in the join algorithms, where placement skew is analogous to input skew and join product skew is analogous to output skew <ref> [WDJ91b] </ref>. However, since hash partitioning is not necessary for aggregate processing, the effect of data skew is significantly different. The data skew, in essence, results in one or more nodes having to do more work than in the uniform distribution case.
Reference: [WDYT90] <author> Joel L. Wolf, Daniel M. Dias, Philip S. Yu, and John J. Turek. </author> <title> An effective algorithm for parallelizing hash joins in the presence of data skew. </title> <institution> IBM T. J. Watson Research Center Tech Report RC 15510, </institution> <year> 1990. </year>
Reference-contexts: tuples based on the "where" clause, redistribution skew where redistribution causes different number of tuples to be processed at each node, and join product skew where the join selectivity is different across the nodes i.e. the result sizes are different across the nodes. 11 The algorithm of Wolf et al. <ref> [WDYT90] </ref> analyzes the base relations by doing an initial scan on them. This information is used in the actual repartitioning of the relations. Using an analytical model to compare the scheduling hash-join algorithm of [WDYT90] and the hybrid hash-join algorithm of Gamma [DG85, SD89, DGS + 90], Walton et al. conclude <p> i.e. the result sizes are different across the nodes. 11 The algorithm of Wolf et al. <ref> [WDYT90] </ref> analyzes the base relations by doing an initial scan on them. This information is used in the actual repartitioning of the relations. Using an analytical model to compare the scheduling hash-join algorithm of [WDYT90] and the hybrid hash-join algorithm of Gamma [DG85, SD89, DGS + 90], Walton et al. conclude that scheduling hash effectively handles redistribution skew while hybrid hash degrades and eventually becomes worse than scheduling hash as redistribution skew increases.
References-found: 49

