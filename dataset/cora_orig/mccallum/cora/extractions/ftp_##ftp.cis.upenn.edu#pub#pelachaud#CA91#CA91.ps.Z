URL: ftp://ftp.cis.upenn.edu/pub/pelachaud/CA91/CA91.ps.Z
Refering-URL: http://www.cis.upenn.edu/~hms/publications.html
Root-URL: 
Title: Linguistic Issues in Facial Animation  
Author: Catherine Pelachaud, Norman I. Badler, Mark Steedman 
Keyword: Key words: facial animation, emotion, intonation, coarticulation, conversational signals  
Address: Philadelphia, PA 19104  
Affiliation: Department of Computer and Information Science University of Pennsylvania  
Abstract: Our goal is to build a system of 3D animation of facial expressions of emotion correlated with the intonation of the voice. Up till now, the existing systems did not take into account the link between these two features. We will look at the rules that control these relations (intonation/emotions and facial expressions/emotions) as well as the coordination of these various modes of expressions. Given an utterance, we consider how the messages (what is new/old information in the given context) transmitted through the choice of accents and their placement, are conveyed through the face. The facial model integrates the action of each muscle or group of muscles as well as the propagation of the muscles' movement. Our first step will be to enumerate and to differentiate facial movements linked to emotions as opposed to those linked to conversation. Then, we will examine what the rules are that drive them and how their different functions interact. 
Abstract-found: 1
Intro-found: 1
Reference: [ARG76] <author> M. Argyle, M. Cook, </author> <title> Gaze and Mutual Gaze, </title> <publisher> Cambridge University Press, </publisher> <year> 1976. </year>
Reference: [BER85] <author> P. Bergeron, P. Lachapelle, </author> <title> "Controlling Facial Expressions and Body Movements in the Computer Generated Animated Short `Tony de Peltrie"', </title> <booktitle> ACM SIGGRAPH'85 Tutorial Notes, Advanced Computer Animation Course, </booktitle> <year> 1985. </year>
Reference-contexts: The separation between conformation parameters and expression parameters imply the independence of the production of an expression and the considered face. But, the use of a limited set of parameters enhances a limited set of facial expressions. Various animations such as "Tony de Peltrie" <ref> [BER85] </ref> and "Sextone for President" [KLE88] were made from a library of digitized expressions. Such systems are very tedious to manipulate and are valid for one particular facial model only. M. Nahas, H. Huitric and M. <p> We will elaborate a repertory of such movements. The computation of the facial expressions linked to one particular utterance with its intonation and emotion is done independently of the facial model. Contrary to the technique of using a stored library of expressions <ref> [BER85] </ref>, [KLE88] which computes facial expressions for one model only, this method used the decomposition of the facial model into two levels: the physical level (described in previous sections) and the expression level. The facial expressions may be applied to any other facial model (having the same underlying structure).
Reference: [BOL86] <author> D. Bolinger, </author> <title> Intonation and its part, </title> <publisher> Stanford University Press, </publisher> <year> 1986. </year>
Reference: [BOU73] <author> G.H. Bourne, </author> <title> The Structure and Function of Muscle, vol. III, Physiology and Biochemistry, </title> <publisher> Academic Press, </publisher> <year> 1973, </year> <note> Second Edition. </note>
Reference: [BUL85] <author> P. Bull, G. Connelly, </author> <title> "Body Movement and Emphasis in Speech", </title> <journal> Journal of Nonverbal Behavior, </journal> <volume> vol. 9, </volume> <editor> n. </editor> <volume> 3, </volume> <year> 1985: </year> <pages> 169-186. </pages>
Reference-contexts: Raised eyebrows can occur to signal a question, especially when it is not syntactically defined. Head and eye motions can illustrate a word; an accented word is often accompanied by a rapid head movement ([HAD84], <ref> [BUL85] </ref>. A blink can also occur on a stressed vowel [CON71]. Each emotion does not activate the same number of facial movements. An angry or happy person will have more facial motions than a sad person. Also, emotion intensity affects the amount and type of facial movements [COL85].
Reference: [CAH89] <author> J. Cahn, </author> <title> "Generating expression in synthesized speech", </title> <type> Masters Thesis, </type> <institution> M.I.T., </institution> <year> 1989. </year>
Reference-contexts: Sadness, however, is characterized by a low pitch level, a narrow pitch range and very small pitch variations. Its intensity is soft, its mean low, its range narrow and its fluctuations small. Its speech rate is slow with the highest number of pauses of the longest duration <ref> [CAH89] </ref>, [LAD85], [WIL81]. To define the syntactic structure of intonation, we are using Janet Pierrehumbert's notation [HIR86]. Under this definition, intonation consists of a linear sequence of accents. Utterances are decomposed into intonational and intermediate phrases. <p> The number of pauses affect the speech rate: a sad person has a slow speech rate due in part to a large number of long pauses, while a frightened person's speech shows very few pauses of short duration <ref> [CAH89] </ref>. Thus the occurrence of punctuators and their type (i.e. their corresponding facial expressions) are emotion-dependent: a happy person has the tendency to punctuate his speech by smiling. Certain types of head movements occur during pauses.
Reference: [CHA89] <author> F. Charpentier, E. Moulines, </author> <title> "Pitch-synchronous waveform processing techniques for text-to-speech synthesis using diphones", </title> <booktitle> Proceedings EUROSPEECH' 89, </booktitle> <volume> vol. 2, </volume> <year> 1989. </year>
Reference-contexts: For the moment, we are using recorded natural speech to guide our animation. After recording a sentence, we extract from its spectrogram the timing of each phoneme and pause. We would like later on to use analysis-and-resynthesis methods to automate the determination of paralanguage parameters and phoneme timing <ref> [CHA89] </ref>, [HAM89] driven by a representation like the above. 6 Steps for Computing Facial Expressions Each facial expression is expressed as a set of AUs. The sentence is scanned at various levels.
Reference: [COL85] <author> G. Collier, </author> <title> Emotional expression, </title> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1985. </year>
Reference-contexts: A blink can also occur on a stressed vowel [CON71]. Each emotion does not activate the same number of facial movements. An angry or happy person will have more facial motions than a sad person. Also, emotion intensity affects the amount and type of facial movements <ref> [COL85] </ref>. Thus we will select the occurrence of conversational signals depending on the emotion and intensity. 8 Punctuators Punctuators can appear at a pause (due to hesitation) or to signal punctuation marks (such as a comma or exclamation marks) [DIT74]. <p> The eye in blinking might close over one syllable and start opening again over another word/syllable. Blink occurrence is also emotion dependent. During fear, tension, and anger, excitement and lying, the amount of blinking increases; it decreases during concentrated thought <ref> [COL85] </ref>. We first compute all the blinks occurring as conversational signals or punctuators. Then, since eyeblinks should occur periodically we add any necessary ones. The period of occurrence is emotion-dependent.
Reference: [CON71] <author> W.S. Condon, W.D. Ogston, </author> <title> "Speech and body motion synchrony of the speaker-hearer", in The perception of Language, </title> <editor> D.H. Horton, J.J. Jenkins ed., </editor> <year> 1971: </year> <pages> 150-185 </pages>
Reference-contexts: Offering a tool to compute separately each of the above groups of facial expressions offers a better grasp and control over the final animation. 3.3 Synchronism An important property linking intonation and facial expression (in fact, it is extended to body movement) is the existence of synchrony between them <ref> [CON71] </ref>. Synchrony implies that changes occurring in speech and in body movements should appear at the same time. Synchrony occurs at all levels of speech. That is, it occurs at the level of phoneme, syllable (these two are defined by how their patterns are articulated), word, phrase or long utterance. <p> Raised eyebrows can occur to signal a question, especially when it is not syntactically defined. Head and eye motions can illustrate a word; an accented word is often accompanied by a rapid head movement ([HAD84], [BUL85]. A blink can also occur on a stressed vowel <ref> [CON71] </ref>. Each emotion does not activate the same number of facial movements. An angry or happy person will have more facial motions than a sad person. Also, emotion intensity affects the amount and type of facial movements [COL85]. <p> Certain types of head movements occur during pauses. A boundary point (between intermediate phrases, for example) will be underlined by slow movement and a final pause will coincide with stillness [HAD84]. Eyeblinks can occur also during pauses <ref> [CON71] </ref>. 9 9 Regulators Regulators correspond to how people take turns speaking in a conversation, or any ritual meeting. We are still in the process of implementing this section. Much study has been given to speaking-turn system. S. <p> They serve not only to accentuate speech but also to address a physical need (to keep the eyes wet). There is at least one eye blink per utterance. The internal structure of an eye blink, i.e., when it is closed and when it opens, is synchronized with the articulation <ref> [CON71] </ref>. The eye in blinking might close over one syllable and start opening again over another word/syllable. Blink occurrence is also emotion dependent. During fear, tension, and anger, excitement and lying, the amount of blinking increases; it decreases during concentrated thought [COL85].
Reference: [CRY75] <author> D. </author> <title> Crystal, "Paralinguistics", in The body as a medium of expression, </title> <editor> J. Benthall, T. Polhemus ed., </editor> <year> 1975: </year> <pages> 163-174. </pages>
Reference-contexts: In our current research, we are not considering the 5 second feature. The third feature, also called paralanguage <ref> [CRY75] </ref>, is differentiated mainly by the pitch (while frequency is a physical property of sound, pitch is a subjective one), loudness (the perceived intensity of a sound), pitch contour (the global envelope of the pitch), tempo (rate of speech) and pause.
Reference: [DIT74] <author> A.T. Dittman, </author> <title> "The body movement-speech rhythm relationship as a cue to speech encoding", in Nonverbal Communication, </title> <editor> Weitz ed., </editor> <year> 1974: </year> <pages> 169-181. </pages>
Reference-contexts: Thus we will select the occurrence of conversational signals depending on the emotion and intensity. 8 Punctuators Punctuators can appear at a pause (due to hesitation) or to signal punctuation marks (such as a comma or exclamation marks) <ref> [DIT74] </ref>. The number of pauses affect the speech rate: a sad person has a slow speech rate due in part to a large number of long pauses, while a frightened person's speech shows very few pauses of short duration [CAH89].
Reference: [DUN74] <author> S. Duncan, </author> <title> "On the structure of the speaker-auditor interaction during speaking turns", </title> <booktitle> in Language in Society, </booktitle> <volume> vol. 3, </volume> <year> 1974: </year> <pages> 161-180. </pages>
Reference-contexts: Eyeblinks can occur also during pauses [CON71]. 9 9 Regulators Regulators correspond to how people take turns speaking in a conversation, or any ritual meeting. We are still in the process of implementing this section. Much study has been given to speaking-turn system. S. Duncan <ref> [DUN74] </ref> enumerates them: * Speaker-Turn-Signal: is emitted when the speaker wants to give his turn of speaking to the auditor. It is composed of several clues in his intonation, paralanguage, body movements and syntax. * Speaker-State-Signal: is displayed at the beginning of a speaking turn.
Reference: [EKM75] <author> P. Ekman, W. Friesen, </author> <title> Unmasking the Face: A guide to recognizing emotions from facial clues, </title> <publisher> Prentice-Hall, </publisher> <year> 1975. </year>
Reference-contexts: Each emotion modifies in a particular way the physiology of a being. The variations of physical organs affect the vocal track while the variations of muscle actions affect the facial expressions. 2 Six emotions (anger, disgust, fear, happiness, sadness and surprise) were found to have universal facial expressions <ref> [EKM75] </ref>. We have chosen to study these. There are three main areas in the face where changes occur: the upper part of the face with the brows and forehead, the eyes, and the lower part of the face with the mouth [EKM75]. <p> and surprise) were found to have universal facial expressions <ref> [EKM75] </ref>. We have chosen to study these. There are three main areas in the face where changes occur: the upper part of the face with the brows and forehead, the eyes, and the lower part of the face with the mouth [EKM75].
Reference: [EKM78] <author> P. Ekman, W. Friesen, </author> <title> Facial Action Coding System, </title> <publisher> Consulting Psychologists Press, </publisher> <year> 1978. </year>
Reference-contexts: The facial model integrates the action of each muscle or group of muscles as well as the propagation of the muscles' movement. It is also adapted to the FACS notation (Facial Action Coding System) created by P. Ekman and W. Friesen <ref> [EKM78] </ref> to describe facial expressions. Our work will resolve the difficulty of manipulating the action of each muscle by offering to the user a higher level of animation by lip synchronization and automatic computation of the facial expressions related to the patterns of the voice. <p> Ekman and W. Friesen <ref> [EKM78] </ref>. It describes all visible facial movements that are either emotional signals or conversational signals. FACS is derived from an analysis of the anatomical basis of facial movements.
Reference: [EKM79] <author> P. Ekman, </author> <title> "About brows: emotional and conversational signals", in Human ethology, </title> <editor> M. von Cranach, K. Foppa, W. Lepenies, D. Ploog ed., </editor> <year> 1979: </year> <pages> 169-249. </pages>
Reference-contexts: It is a very useful scheme since the type of actions performed by a person while talking is still not very well-known by researchers. Most of the people show eyebrow movements to accentuate a word but other facial action may be chosen such as nose wrinkling or eye flashes <ref> [EKM79] </ref>. The user just needs to modify the rule which describes the action and need not alter the rules of occurrence. Another unknown parameter is the effective occurrence of an action. Indeed, a paralanguage feature is not always accompanied by a facial movement.
Reference: [EKM84] <author> P. Ekman, </author> <title> "Expression and the nature of emotion", in Approaches to emotion, </title> <editor> K. Scherer, P. Ekman ed., </editor> <year> 1984. </year>
Reference-contexts: Apex corresponds to the time the action is occurring. Onset and offset define the manner of appearance and disappearance of the action;they are emotion dependent. For example, surprise has the shortest onset time while sadness has the longest offset <ref> [EKM84] </ref>. Having computed the list of AUs for each phoneme, the animation can then be performed. We apply the heuristic that quick abrupt changes for a particular portion of the face cannot occur in too short a time.
Reference: [FAR90] <author> G. Farin, </author> <title> Curves and Surfaces for Computed Aided Geometric Design, </title>
Reference-contexts: That is, the average displacement of all the points inside each region of the face divided by the time separating the two frames. Each point of every frame is then modified by this "weight". The in-between frames are obtained by computing the B-spline going through the weighted points <ref> [FAR90] </ref>. This is the way to handle any brusque movement and some coarticulation problems which the proposed algorithm does not take care of. 13 Example Let us consider the example introduced in a previous section: `Julia prefers popcorn' with the emotion disgust.
References-found: 17

