URL: http://www.cs.huji.ac.il/~rousso/TRs/ego-mtn-cvpr94.ps.gz
Refering-URL: http://www.cs.huji.ac.il/~rousso/
Root-URL: http://www.cs.huji.ac.il
Title: Recovery of Ego-Motion Using Image Stabilization method avoids the inherent problems in the computation of
Author: Michal Irani Benny Rousso Shmuel Peleg 
Note: The presented  
Address: 91904 Jerusalem, ISRAEL  
Affiliation: Institute of Computer Science The Hebrew University of Jerusalem  
Abstract: A method for computing the 3D camera motion (the ego-motion) in a static scene is introduced, which is based on computing the 2D image motion of a single image region directly from image intensities. The computed image motion of this image region is used to register the images so that the detected image region appears stationary. The resulting displacement field for the entire scene between the registered frames is affected only by the 3D translation of the camera. After canceling the effects of the camera rotation by using such 2D image registration, the 3D camera translation is computed by finding the focus-of-expansion in the translation-only set of registered frames. This step is followed by computing the camera rotation to complete the computation of the ego-motion. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Adiv. </author> <title> Determining three-dimensional motion and structure from optical flow generated by several moving objects. </title> <journal> IEEE Trans. on PAMI, </journal> <volume> 7(4) </volume> <pages> 384-401, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: To overcome this difficulty, additional constraints are usually added to the motion model or to the environment model. 3D motion is often estimated from the optical or normal flow derived between two frames <ref> [1, 12, 22] </ref>, or from the correspondence of distinguished features fl This research has been sponsored by the U.S. Office of Naval Research under Grant N00014-93-1-1202, R&T Project Code 4424341|01. y M. Irani is now with David Sarnoff Research Center. (points, lines, contours) extracted from successive frames [10, 13, 7]. <p> When the field of view is not very large and the camera motion has a relatively small rotation <ref> [1] </ref>, the 2D displacement (u; v) of an image point (x; y) in the image plane can be expressed by [20, 3]: v = f c ( T X Z +y Z x 2 Y f c Z X )x Z +y T Z f c +y 2 X # The <p> All points (X; Y; Z) of a planar surface in the 3D scene satisfy a plane equation Z = A+BX+CY , which can be expressed in terms of image coordinates by using Eq. (1) as: Z In a similar manipulation to that in <ref> [1] </ref>, substituting Eq. (4) in Eq. (3) yields: v = a + b x + c y + g x 2 + h xy where: a = f c ffT X f c Y e = Z f c fiT Y c = Z f c flT X g = Y <p> This model is applied only to the segmented region obtained in the translation computation step, to get an affine approximation of the object's motion. The previous seg mentation is refined accordingly. 3. A Moving planar surface (a pseudo 2D projective transformation): 8 parameters <ref> [1, 3] </ref> (see Eq. (5)), u (x; y) = a + bx + cy + gx 2 + hxy, v (x; y) = d + ex + fy + gxy + hy 2 .
Reference: [2] <author> G. Adiv. </author> <title> Inherent ambiguities in recovering 3D motion and structure from a noisy flow field. </title> <journal> IEEE Trans. on PAMI, </journal> <volume> 11 </volume> <pages> 477-489, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Both approaches depend on the accuracy of the feature detection, which can not always be assured. Methods for computing the ego-motion directly from image intensities were also suggested [11, 14]. Camera rotations and translations can induce similar image motions <ref> [2, 8] </ref> causing ambiguities in their interpretation. At depth discontinuities, however, it is much easier to distinguish between the effects of camera rotations and camera translations, as the image motion of neighboring pixels at different depths will have similar rotational components, but different translational components. <p> Moreover, the problem of recovering the 3D camera motion directly from the image motion field is an ill-conditioned problem, since small errors in the 2D flow field usually result in large perturbations in the 3D motion <ref> [2] </ref>. To overcome the difficulties and ambiguities in the computation of the ego-motion, we introduce the fol lowing scheme: The first frame is warped towards the second frame using the computed 2D image motion at the detected image region.
Reference: [3] <author> J.R. Bergen, P. Anandan, K.J. Hanna, and R. Hingo-rani. </author> <title> Hierarchical model-based motion estimation. </title> <booktitle> In ECCV, </booktitle> <pages> pages 237-252, </pages> <year> 1992. </year>
Reference-contexts: When the field of view is not very large and the camera motion has a relatively small rotation [1], the 2D displacement (u; v) of an image point (x; y) in the image plane can be expressed by <ref> [20, 3] </ref>: v = f c ( T X Z +y Z x 2 Y f c Z X )x Z +y T Z f c +y 2 X # The following is noted from Eq. (3): * Since all translations are divided by the unknown depth Z, only the direction <p> This model is applied only to the segmented region obtained in the translation computation step, to get an affine approximation of the object's motion. The previous seg mentation is refined accordingly. 3. A Moving planar surface (a pseudo 2D projective transformation): 8 parameters <ref> [1, 3] </ref> (see Eq. (5)), u (x; y) = a + bx + cy + gx 2 + hxy, v (x; y) = d + ex + fy + gxy + hy 2 .
Reference: [4] <author> J.R. Bergen, P.J. Burt, R. Hingorani, and S. Peleg. </author> <title> A three-frame algorithm for estimating two-component image motion. </title> <journal> IEEE Trans. on PAMI, </journal> <volume> 14 </volume> <pages> 886-895, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: c; d; e; f; g; h) of the transformation (u; v) (see Eq. (5)) which minimize the following error function over the region of support R [16]: Err (t) (a; b; c; d; e; f; g; h) = (x;y)2R (10) The error minimization is performed iteratively using a Gaussian pyramid <ref> [4, 15, 16] </ref>. Unfortunately, the region of support R of a planar object is not known in advance. Applying the error minimization technique to the entire image would usually yield a meaningless result.
Reference: [5] <author> P.J. Burt, R. Hingorani, and R.J. Kolczynski. </author> <title> Mechanisms for isolating component patterns in the sequential analysis of multiple motion. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 187-193, </pages> <year> 1991. </year>
Reference-contexts: Applying the error minimization technique to the entire image would usually yield a meaningless result. This, however, is not true for simple 2D translations, where the 2D motion can be expressed by (u (x; y); v (x; y)) = (a; d). It was shown in <ref> [5] </ref> that the motion parameters of a single translating image region can be recovered accurately by minimizing the error function Err (t) (a; d) = P with respect to a and d over the entire image (again, using iterations on a multiresolution data structure).
Reference: [6] <author> S. Carlsson and J.O. Eklundh. </author> <title> Object detection using model based prediction and motion parallax. </title> <booktitle> In ECCV, </booktitle> <pages> pages 297-306, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Motion parallax methods use this effect to obtain the 3D camera motion. [18, 17, 7]. Other methods use motion parallax for shape representation and analysis <ref> [23, 6, 9] </ref>. In this paper a method for computing the ego-motion directly from image intensities is introduced. At first only 2D image motion is extracted, and later this 2D motion is used to simplify the computation of the 3D ego-motion.
Reference: [7] <author> R. Chipolla, Y. Okamoto, and Y. Kuno. </author> <title> Robust structure from motion using motion paralax. </title> <booktitle> In ICCV, </booktitle> <pages> pages 374-382, </pages> <address> Berlin, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Office of Naval Research under Grant N00014-93-1-1202, R&T Project Code 4424341|01. y M. Irani is now with David Sarnoff Research Center. (points, lines, contours) extracted from successive frames <ref> [10, 13, 7] </ref>. Both approaches depend on the accuracy of the feature detection, which can not always be assured. Methods for computing the ego-motion directly from image intensities were also suggested [11, 14]. Camera rotations and translations can induce similar image motions [2, 8] causing ambiguities in their interpretation. <p> Motion parallax methods use this effect to obtain the 3D camera motion. <ref> [18, 17, 7] </ref>. Other methods use motion parallax for shape representation and analysis [23, 6, 9]. In this paper a method for computing the ego-motion directly from image intensities is introduced. <p> The 2D image region registration technique used in this work allows easy decoupling of the translational and rotational motions, as only motion parallax information remains after the registration. As opposed to other methods using motion parallax <ref> [18, 19, 17, 7] </ref>, our method does not rely on 2D motion information computed near depth discontinuities, where it is inaccurate, but on motion computed over an entire image.
Reference: [8] <author> K. Daniilidis and H.-H. Nagel. </author> <title> The coupling of rotation and translation in motion estimation of planar surfaces. </title> <booktitle> In CVPR, </booktitle> <pages> pages 188-193, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Both approaches depend on the accuracy of the feature detection, which can not always be assured. Methods for computing the ego-motion directly from image intensities were also suggested [11, 14]. Camera rotations and translations can induce similar image motions <ref> [2, 8] </ref> causing ambiguities in their interpretation. At depth discontinuities, however, it is much easier to distinguish between the effects of camera rotations and camera translations, as the image motion of neighboring pixels at different depths will have similar rotational components, but different translational components.
Reference: [9] <author> W. Enkelmann. </author> <title> Obstacle detection by evaluation of optical flow fields from image sequences. </title> <editor> In O. Faugeras, editor, </editor> <booktitle> ECCV, </booktitle> <pages> pages 134-138, </pages> <year> 1990. </year>
Reference-contexts: Motion parallax methods use this effect to obtain the 3D camera motion. [18, 17, 7]. Other methods use motion parallax for shape representation and analysis <ref> [23, 6, 9] </ref>. In this paper a method for computing the ego-motion directly from image intensities is introduced. At first only 2D image motion is extracted, and later this 2D motion is used to simplify the computation of the 3D ego-motion.
Reference: [10] <author> O.D. Faugeras, F. Lustman, and G. Toscani. </author> <title> Motion and structure from motion from point and line matching. </title> <booktitle> In ICCV, </booktitle> <pages> pages 25-34, </pages> <year> 1987. </year>
Reference-contexts: Office of Naval Research under Grant N00014-93-1-1202, R&T Project Code 4424341|01. y M. Irani is now with David Sarnoff Research Center. (points, lines, contours) extracted from successive frames <ref> [10, 13, 7] </ref>. Both approaches depend on the accuracy of the feature detection, which can not always be assured. Methods for computing the ego-motion directly from image intensities were also suggested [11, 14]. Camera rotations and translations can induce similar image motions [2, 8] causing ambiguities in their interpretation.
Reference: [11] <author> K. Hanna. </author> <title> Direct multi-resolution estimation of ego-motion and structure from motion. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 156-162, </pages> <address> Princeton, </address> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: Irani is now with David Sarnoff Research Center. (points, lines, contours) extracted from successive frames [10, 13, 7]. Both approaches depend on the accuracy of the feature detection, which can not always be assured. Methods for computing the ego-motion directly from image intensities were also suggested <ref> [11, 14] </ref>. Camera rotations and translations can induce similar image motions [2, 8] causing ambiguities in their interpretation. <p> Once the 3D motion parameters of the camera are computed, the 3D scene structure can be reconstructed using a scheme similar to that suggested in <ref> [11] </ref>. Correspondences between small image patches (currently 5 fi 5 pixels) are computed only along the radial lines emerging from the FOE (taking the rotations into account). The depth map is computed from the magnitude of these displacements.
Reference: [12] <author> D.J. Heeger and A. Jepson. </author> <title> Simple method for computing 3d motion and depth. </title> <booktitle> In ICCV, </booktitle> <pages> pages 96-100, </pages> <year> 1990. </year>
Reference-contexts: To overcome this difficulty, additional constraints are usually added to the motion model or to the environment model. 3D motion is often estimated from the optical or normal flow derived between two frames <ref> [1, 12, 22] </ref>, or from the correspondence of distinguished features fl This research has been sponsored by the U.S. Office of Naval Research under Grant N00014-93-1-1202, R&T Project Code 4424341|01. y M. Irani is now with David Sarnoff Research Center. (points, lines, contours) extracted from successive frames [10, 13, 7].
Reference: [13] <author> B.K.P. Horn. </author> <title> Relative orientation. </title> <journal> Int. J. of Computer Vision, </journal> <volume> 4(1) </volume> <pages> 58-78, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Office of Naval Research under Grant N00014-93-1-1202, R&T Project Code 4424341|01. y M. Irani is now with David Sarnoff Research Center. (points, lines, contours) extracted from successive frames <ref> [10, 13, 7] </ref>. Both approaches depend on the accuracy of the feature detection, which can not always be assured. Methods for computing the ego-motion directly from image intensities were also suggested [11, 14]. Camera rotations and translations can induce similar image motions [2, 8] causing ambiguities in their interpretation.
Reference: [14] <author> B.K.P. Horn and E.J. Weldon. </author> <title> Direct methods for recovering motion. </title> <journal> Int. J. of Computer Vision, </journal> <volume> 2(1) </volume> <pages> 51-76, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Irani is now with David Sarnoff Research Center. (points, lines, contours) extracted from successive frames [10, 13, 7]. Both approaches depend on the accuracy of the feature detection, which can not always be assured. Methods for computing the ego-motion directly from image intensities were also suggested <ref> [11, 14] </ref>. Camera rotations and translations can induce similar image motions [2, 8] causing ambiguities in their interpretation.
Reference: [15] <author> M. Irani, B. Rousso, and S. Peleg. </author> <title> Detecting and tracking multiple moving objects using temporal integration. </title> <booktitle> In ECCV, </booktitle> <pages> pages 282-287, </pages> <year> 1992. </year>
Reference-contexts: In this paper a method for computing the ego-motion directly from image intensities is introduced. At first only 2D image motion is extracted, and later this 2D motion is used to simplify the computation of the 3D ego-motion. We use previously developed methods <ref> [15, 16] </ref> to detect and track a single image region and to compute its 2D parametric image motion. <p> An example of such stabilization is shown in Fig. 3.d . Alternatively, the rotations can be filtered by a low-pass filter so that the resulting sequence will appear to have only smooth rotations, but no jitter. 4 Computing 2D Motion of a Planar Surface We use previously developed methods <ref> [15, 16] </ref> in order to detect an image region corresponding to a planar surface in the scene with its pseudo 2D projective transformation. These methods treated dynamic scenes, in which there were assumed to be multiple moving planar objects. <p> In this section we describe very briefly how the technique for detecting multiple moving planar objects locks onto the one planar object and its 2D motion parameters. More details appear in <ref> [15, 16] </ref>. The projected 2D image motion (u (x; y); v (x; y) of a planar moving object in the scene can be approximated by the 2D parametric transformation of Eq. (5). <p> c; d; e; f; g; h) of the transformation (u; v) (see Eq. (5)) which minimize the following error function over the region of support R [16]: Err (t) (a; b; c; d; e; f; g; h) = (x;y)2R (10) The error minimization is performed iteratively using a Gaussian pyramid <ref> [4, 15, 16] </ref>. Unfortunately, the region of support R of a planar object is not known in advance. Applying the error minimization technique to the entire image would usually yield a meaningless result. <p> This can be done even in the presence of other moving objects in the region of analysis, and with no prior knowledge of their regions of support. This object is called the dominant translating object, and its 2D translation the dominant 2D translation. In <ref> [15, 16] </ref> this method was extended to compute higher order 2D motions (2D affine, 2D projective) of a single planar object among differently moving objects. A segmentation step, which marks the region corresponding to the computed dominant 2D motion, was added.
Reference: [16] <author> M. Irani, B. Rousso, and S. Peleg. </author> <title> Computing occluding and transparent motions. </title> <journal> Int. J. of Computer Vision, </journal> <volume> 12(1) </volume> <pages> 5-16, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: In this paper a method for computing the ego-motion directly from image intensities is introduced. At first only 2D image motion is extracted, and later this 2D motion is used to simplify the computation of the 3D ego-motion. We use previously developed methods <ref> [15, 16] </ref> to detect and track a single image region and to compute its 2D parametric image motion. <p> An example of such stabilization is shown in Fig. 3.d . Alternatively, the rotations can be filtered by a low-pass filter so that the resulting sequence will appear to have only smooth rotations, but no jitter. 4 Computing 2D Motion of a Planar Surface We use previously developed methods <ref> [15, 16] </ref> in order to detect an image region corresponding to a planar surface in the scene with its pseudo 2D projective transformation. These methods treated dynamic scenes, in which there were assumed to be multiple moving planar objects. <p> In this section we describe very briefly how the technique for detecting multiple moving planar objects locks onto the one planar object and its 2D motion parameters. More details appear in <ref> [15, 16] </ref>. The projected 2D image motion (u (x; y); v (x; y) of a planar moving object in the scene can be approximated by the 2D parametric transformation of Eq. (5). <p> This could be done by computing the eight parameters (a; b; c; d; e; f; g; h) of the transformation (u; v) (see Eq. (5)) which minimize the following error function over the region of support R <ref> [16] </ref>: Err (t) (a; b; c; d; e; f; g; h) = (x;y)2R (10) The error minimization is performed iteratively using a Gaussian pyramid [4, 15, 16]. Unfortunately, the region of support R of a planar object is not known in advance. <p> c; d; e; f; g; h) of the transformation (u; v) (see Eq. (5)) which minimize the following error function over the region of support R [16]: Err (t) (a; b; c; d; e; f; g; h) = (x;y)2R (10) The error minimization is performed iteratively using a Gaussian pyramid <ref> [4, 15, 16] </ref>. Unfortunately, the region of support R of a planar object is not known in advance. Applying the error minimization technique to the entire image would usually yield a meaningless result. <p> This can be done even in the presence of other moving objects in the region of analysis, and with no prior knowledge of their regions of support. This object is called the dominant translating object, and its 2D translation the dominant 2D translation. In <ref> [15, 16] </ref> this method was extended to compute higher order 2D motions (2D affine, 2D projective) of a single planar object among differently moving objects. A segmentation step, which marks the region corresponding to the computed dominant 2D motion, was added. <p> The scheme for locking onto a single planar object and its 2D image motion is gradual, where the complexity of the 2D motion model is increased in each computation step, and the segmentation of the planar object is refined accordingly. More details can be found in <ref> [16] </ref> The 2D motion models used in the gradual locking on a planar object are listed below in increasing complexity: 1. Translation: 2 parameters, u (x; y) = a, v (x; y) = d. This model is applied to the entire image to get an initial motion estimation.
Reference: [17] <author> J. Lawn and R. Chipolla. </author> <title> Epipole estimation using affine motion parallax. </title> <type> Technical Report CUED/F-INFENG/TR-138, </type> <address> Cambridge, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: Motion parallax methods use this effect to obtain the 3D camera motion. <ref> [18, 17, 7] </ref>. Other methods use motion parallax for shape representation and analysis [23, 6, 9]. In this paper a method for computing the ego-motion directly from image intensities is introduced. <p> The 2D image region registration technique used in this work allows easy decoupling of the translational and rotational motions, as only motion parallax information remains after the registration. As opposed to other methods using motion parallax <ref> [18, 19, 17, 7] </ref>, our method does not rely on 2D motion information computed near depth discontinuities, where it is inaccurate, but on motion computed over an entire image.
Reference: [18] <author> D.T. Lawton and J.H. Rieger. </author> <title> The use of difference fields in processing sensor motion. </title> <booktitle> In ARPA IU Workshop, </booktitle> <pages> pages 78-83, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: Motion parallax methods use this effect to obtain the 3D camera motion. <ref> [18, 17, 7] </ref>. Other methods use motion parallax for shape representation and analysis [23, 6, 9]. In this paper a method for computing the ego-motion directly from image intensities is introduced. <p> The 2D image region registration technique used in this work allows easy decoupling of the translational and rotational motions, as only motion parallax information remains after the registration. As opposed to other methods using motion parallax <ref> [18, 19, 17, 7] </ref>, our method does not rely on 2D motion information computed near depth discontinuities, where it is inaccurate, but on motion computed over an entire image. <p> To locate the FOE, the optical flow between the registered frames is computed, and the FOE is located using a search method similar to that described in <ref> [18] </ref>. Candidates for the FOE are sampled over a half sphere and projected onto the image plane. For each such candidate, a global error measure is computed from local deviations of the flow field from the radial lines emerging from the candidate FOE.
Reference: [19] <author> C.H. Lee. </author> <title> Structure and motion from two perspective views via planar patch. </title> <booktitle> In ICCV, </booktitle> <pages> pages 158-164, </pages> <year> 1988. </year>
Reference-contexts: The 2D image region registration technique used in this work allows easy decoupling of the translational and rotational motions, as only motion parallax information remains after the registration. As opposed to other methods using motion parallax <ref> [18, 19, 17, 7] </ref>, our method does not rely on 2D motion information computed near depth discontinuities, where it is inaccurate, but on motion computed over an entire image.
Reference: [20] <author> H.C. Longuet-Higgins. </author> <title> Visual ambiguity of a moving plane. </title> <journal> Proceedings of The Royal Society of London B, </journal> <volume> 223 </volume> <pages> 165-175, </pages> <year> 1984. </year>
Reference-contexts: It is important to emphasize that the 3D camera motion cannot be recovered solely from the 2D parametric image motion of a single image region, as there are a couple of such 3D interpretations <ref> [20] </ref>. It was shown that 3D motion of a planar surface can be computed from its 2D affine motion in the image and from the motion derivatives [21], but motion derivatives introduce sensitivity to noise. <p> When the field of view is not very large and the camera motion has a relatively small rotation [1], the 2D displacement (u; v) of an image point (x; y) in the image plane can be expressed by <ref> [20, 3] </ref>: v = f c ( T X Z +y Z x 2 Y f c Z X )x Z +y T Z f c +y 2 X # The following is noted from Eq. (3): * Since all translations are divided by the unknown depth Z, only the direction
Reference: [21] <author> F. Meyer and P. Bouthemy. </author> <title> Estimation of time-to-collision maps from first order motion models and normal flows. </title> <booktitle> In ICPR, </booktitle> <pages> pages 78-82, </pages> <year> 1992. </year>
Reference-contexts: It was shown that 3D motion of a planar surface can be computed from its 2D affine motion in the image and from the motion derivatives <ref> [21] </ref>, but motion derivatives introduce sensitivity to noise. Moreover, the problem of recovering the 3D camera motion directly from the image motion field is an ill-conditioned problem, since small errors in the 2D flow field usually result in large perturbations in the 3D motion [2].
Reference: [22] <author> S. Negahdaripour and S. Lee. </author> <title> Motion recovery from image sequences using first-order optical flow information. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 132-139, </pages> <address> Princeton, NJ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: To overcome this difficulty, additional constraints are usually added to the motion model or to the environment model. 3D motion is often estimated from the optical or normal flow derived between two frames <ref> [1, 12, 22] </ref>, or from the correspondence of distinguished features fl This research has been sponsored by the U.S. Office of Naval Research under Grant N00014-93-1-1202, R&T Project Code 4424341|01. y M. Irani is now with David Sarnoff Research Center. (points, lines, contours) extracted from successive frames [10, 13, 7].
Reference: [23] <author> A. Shashua. </author> <title> Projective depth: a geometric invariant for 3d reconstruction from two perspective/orthographic views and for visual recognition. </title> <booktitle> In ICCV, </booktitle> <pages> pages 583-590, </pages> <address> Berlin, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Motion parallax methods use this effect to obtain the 3D camera motion. [18, 17, 7]. Other methods use motion parallax for shape representation and analysis <ref> [23, 6, 9] </ref>. In this paper a method for computing the ego-motion directly from image intensities is introduced. At first only 2D image motion is extracted, and later this 2D motion is used to simplify the computation of the 3D ego-motion.
References-found: 23

