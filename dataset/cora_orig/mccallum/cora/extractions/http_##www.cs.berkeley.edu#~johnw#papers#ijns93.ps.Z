URL: http://www.cs.berkeley.edu/~johnw/papers/ijns93.ps.Z
Refering-URL: http://www.cs.berkeley.edu/~johnw/publications.html
Root-URL: 
Title: Designing a Connectionist Network Supercomputer 1  
Author: Krste Asanovic, James Beck, Jerry Feldman, Nelson Morgan, and John Wawrzynek 
Date: September 10, 1993  
Address: Berkeley, California  
Affiliation: University of California and the International Computer Science Institute  
Abstract: This paper describes an effort at UC Berkeley and the International Computer Science Institute to develop a super-computer for artificial neural network applications. Our perspective has been strongly influenced by earlier experiences with the contruc-tion and use of a simpler machine. In particular, we have observed Amdahl's Law in action in our designs and those of others. These observations inspire attention to many factors beyond fast multiply-accumulate arithmetic. We describe a number of these factors, along with rough expressions for their influence, and then give the applications targets, machine goals, and the system architecture for the machine we are currently designing.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Krste Asanovic and Nelson Morgan. </author> <title> Experimental Determination of Precision Requirements for Back-Propagation Training of Artificial Neural Networks. </title> <booktitle> In Proceedings 2nd International Conference on Microelectronics for Neural Networks, </booktitle> <address> Munich, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: CNS-1 will supply orders of magnitude more capability in a form that we already know how to exploit. There are several features of connectionist computation that are exploited in the CNS-1 design. Experiments show that limited precision fixed point arithmetic suffices for almost all algorithms of interest <ref> [1] </ref>. Many problems are highly regular and are well suited to parallel and pipelined execution. Connectionist networks are "embarassingly" parallel and map nicely to distributed memory machines. Communication is usually multi-cast and normally values only are sent, eliminating the read latency that plagues most distributed computing. <p> It is also true in general that these operations do not typically require the kind of range or resolution that is required for general scientific computation; single-precision 32-bit floating point or even 16-bit fixed point representations appear to be sufficient <ref> [1] </ref>. Thus, neurocomputing systems nearly always consist of a parallel system with low or moderate precision dot-product capability [5, 10]. However, it is often true that non-trivial manipulations of the inputs and outputs of the universal matrix operations must take place in real algorithms.
Reference: [2] <author> H. Bourlard and N. Morgan. </author> <title> Connectionist Speech Recognition: a Hybrid Approach. </title> <publisher> Kluwer Academic Press, </publisher> <year> 1993. </year> <month> 12 </month>
Reference-contexts: Together these applications imply a number of system goals for the CNS-1. Within our research group we have been doing large computations for connectionist speech research. In particular, we have been training large Multi-Layer Perceptrons (MLPs) to estimate phonetic probabilities for continuous speech recognition <ref> [2] </ref>. These techniques are currently quite competitive with the best classical statistical approaches. Training our networks for the standard DARPA Resource Management task requires on the order of 10 14 arithmetic operations.
Reference: [3] <author> Jerome A. Feldman, Chu-Cheow Lim, and Thomas Rauber. </author> <title> The Shared-Memory Lan--gauge pSather on a Distributed-Memory Multiprocessor. </title> <booktitle> In Second Workshop on Languages, Compilers, and Run-Time Environments for Distributed-Memory Multiprocessors, </booktitle> <address> Boulder, Colorado, </address> <month> Sept 30 - Oct 2 </month> <year> 1992. </year>
Reference-contexts: This C compiler will then be used as the target for a port of Sather [9]. Parallel language support for distributed memory machines is still very much a research issue. We expect to use experience gained in a pSather <ref> [3] </ref> port to the CM-5 to port pSather to CNS-1. The CNS-1 hardware is simple, fast, and flexible, and should prove an interesting vehicle for further research into parallel languages.
Reference: [4] <author> H. P. Graf, R. Janow, D. Henderson, and R. Lee. </author> <title> Reconfigurable Neural Net Chip with 32K Connections. </title> <booktitle> In Advances in Neural Information Processing Systems 3, Proceedings of the 1991 Conference, </booktitle> <pages> pages 1032-1038. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1992. </year>
Reference-contexts: It is now clear that neither of these perspectives is strictly correct. A number of connectionist researchers have built powerful computational engines with entirely digital elements, entirely analog designs, and sometimes a hybrid of the two. An example of the last category was the NET32K chip from AT&T <ref> [4] </ref>. This chip used bitwise multiplications, digital weight storage, and digital I/O, while performing addition by summing currents.
Reference: [5] <author> D. Hammerstrom. </author> <title> A VLSI architecture for High-Performance, Low-Cost, On-Chip Learning. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <pages> pages II-537-543, </pages> <year> 1990. </year>
Reference-contexts: Thus, neurocomputing systems nearly always consist of a parallel system with low or moderate precision dot-product capability <ref> [5, 10] </ref>. However, it is often true that non-trivial manipulations of the inputs and outputs of the universal matrix operations must take place in real algorithms. In our experience over the last three years with the Ring Array Processor (RAP) [8] we have often found that applications required such functionality.
Reference: [6] <author> J. Hennessy and D. Patterson. </author> <title> Computer Architecture a Quantitative Approach. </title> <publisher> Mor-gan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1990. </year>
Reference-contexts: Put another way, the communication from the host or disk must be roughly as fast as the inter-chip data transfer. Of course this was a worst case analysis; the reason that memory hierarchies are successful is that there is some spatial and temporal locality in real problems <ref> [6] </ref>. These concepts are still relevant for neurocomputing. For instance, a common case of temporally local access is 5 that of convolutional network inputs, or other architectures in which multiple patterns are used as input to a net in a way that overlaps for sequential net evaluations.
Reference: [7] <author> John Lazzaro, John Wawrzynek, Misha Mahowald, Massimo Sivilotti, and David Gille-spie. </author> <title> Silicon Auditory Processor as Computer Peripherals. </title> <booktitle> In Advances in Neural Information Processings Systems, Proceedings of the 1992 Conference, </booktitle> <month> December </month> <year> 1992. </year>
Reference-contexts: In some cases we are interested in interfacing with specific analog interface systems that are part of our research <ref> [7] </ref>. These requirements provide a strong impetus for users to want a significant say in the design of the computing system. In addition to the specific connectionist applications of CNS-1, we are very interested in the potential of CNS-1 as a general purpose connectionist accelerator.
Reference: [8] <author> Nelson Morgan, James Beck, Phil Kohn, Jeff Bilmes, Eric Allman, and Jochaim Beer. </author> <title> The Ring Array Processor(RAP): A Multiprocessing Peripheral for Connectionist Applications. </title> <journal> Journal of Parallel and Distributed Computing, Special Issue on Neural Networks, </journal> <volume> 14 </volume> <pages> 248-259, </pages> <year> 1992. </year>
Reference-contexts: Recent work in our lab and elsewhere has shown the practicality of connectionist systems for a range of important problems, but has also revealed needs for computational resources far exceeding those available to investigators in the field. Our own earlier development of the Ring Array Processor (RAP) <ref> [8] </ref> attached processor has played a crucial role in our research and is proving valuable to other groups who have acquired the system. CNS-1 will supply orders of magnitude more capability in a form that we already know how to exploit. <p> However, it is often true that non-trivial manipulations of the inputs and outputs of the universal matrix operations must take place in real algorithms. In our experience over the last three years with the Ring Array Processor (RAP) <ref> [8] </ref> we have often found that applications required such functionality. For example, inputs often need to be normalized, or preprocessed to compute first or second derivatives. These cannot always be precomputed and stored, as they can take up a prohibitive amount of disk space for real applications.
Reference: [9] <author> Stephen M. Omohundro. </author> <title> The Sather Language. </title> <type> Technical report, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> Ca., </address> <year> 1991. </year>
Reference-contexts: Both C and C++ will be made available by a port of the GNU gcc compiler to the Torrent processor. This C compiler will then be used as the target for a port of Sather <ref> [9] </ref>. Parallel language support for distributed memory machines is still very much a research issue. We expect to use experience gained in a pSather [3] port to the CM-5 to port pSather to CNS-1.
Reference: [10] <author> U. Ramacher, J. Beichter, W. Raab, J. Anlauf, N. Bruls, M. Hachmann, and M. Wes-seling. </author> <title> Design of a 1st Generation Neurocomputer. In VLSI Design of Neural Networks. </title> <publisher> Kluwer Academic, </publisher> <year> 1991. </year>
Reference-contexts: Thus, neurocomputing systems nearly always consist of a parallel system with low or moderate precision dot-product capability <ref> [5, 10] </ref>. However, it is often true that non-trivial manipulations of the inputs and outputs of the universal matrix operations must take place in real algorithms. In our experience over the last three years with the Ring Array Processor (RAP) [8] we have often found that applications required such functionality.
Reference: [11] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proc. Tenth International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year> <month> 13 </month>
Reference-contexts: The communication and management of continuous sensory data streams require rapid processor response to asynchronous external events. Torrent has a fast and simple network interface with messages sent and received directly from processor registers. The active message model <ref> [11] </ref> is directly supported, where an arriving message triggers execution of a local event handler. System design is simplified because each processing node comprises only a single chip processor and DRAM.
References-found: 11

