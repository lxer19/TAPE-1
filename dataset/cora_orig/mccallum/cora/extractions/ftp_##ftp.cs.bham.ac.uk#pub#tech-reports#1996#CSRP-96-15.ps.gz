URL: ftp://ftp.cs.bham.ac.uk/pub/tech-reports/1996/CSRP-96-15.ps.gz
Refering-URL: http://www.cs.bham.ac.uk/~rmp/eebic/index.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: E-mail: R.Poli@cs.bham.ac.uk  
Title: Parallel Distributed Genetic Programming  
Author: Riccardo Poli 
Note: PDGP, and illustrates its behaviour on a large number of problems.  
Address: Birmingham B15 2TT United Kingdom  
Affiliation: School of Computer Science The University of Birmingham  
Abstract: Technical Report: CSRP-96-15 September 1996 Abstract This paper describes Parallel Distributed Genetic Programming (PDGP), a new form of Genetic Programming (GP) which is suitable for the development of programs with a high degree of parallelism and an efficient and effective reuse of partial results. Programs are represented in PDGP as graphs with nodes representing functions and terminals, and links representing the flow of control and results. In the simplest form of PDGP links are directed and unlabelled, in which case PDGP can be considered a generalisation of standard GP. However, more complex (direct) representations can be used, which allow the exploration of a large space of possible programs including standard tree-like programs, logic networks, neural networks, recurrent transition networks, finite state automata, etc. In PDGP, programs are manipulated by special crossover and mutation operators which guarantee the syntactic correctness of the offspring. For this reason PDGP search is very efficient. PDGP programs can be executed in different ways, depending on whether nodes with side effects are used or not. The paper describes the representations, the operators and the interpreters used in 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Late Breaking Papers at the Genetic Programming 1996 Conference, Stanford University, </institution> <address> July 1996. </address> <publisher> Stanford Bookstore. </publisher>
Reference-contexts: This form of GP has been applied successfully to a large number of difficult problems like automated design, pattern recognition, robot control, symbolic regression, music generation, image compression, image analysis, etc. <ref> [16, 17, 14, 15, 1, 22] </ref>. When appropriate terminals, functions and/or interpreters are defined, standard GP can go beyond the production of sequential tree-like programs. <p> Inspired by these "creative" analogue solutions we tried to develop neuro-algebraic solutions by using the same function and terminal set but adding random weights in the range <ref> [1; 1] </ref> to the links.
Reference: [2] <author> David Andre, Forrest H. Bennett III, and John R. Koza. </author> <title> Discovery by genetic programming of a cellular automata rule that is better than any known rule for the majority classification problem. </title> <editor> In John R. Koza, David E. Goldberg, David B. Fogel, and Rick L. Riolo, editors, </editor> <booktitle> Genetic Programming 1996: Proceedings of the First Annual Conference, </booktitle> <pages> page 3, </pages> <address> Stanford University, CA, USA, 28-31 July 1996. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Andre, Bennet and Koza <ref> [2] </ref> used GP to discover rules for cellular automata which could solve large majority-classification problems. In a system called PADO (Parallel Algorithm Discovery and Orchestration), Teller and Veloso [28] used a combination of GP and linear discrimination to obtain classification programs for signals and images.
Reference: [3] <author> David Andre and John R. Koza. </author> <title> Parallel genetic programming: A scalable implementation using the transputer network architecture. </title> <editor> In Peter J. Angeline and K. E. Kinnear, Jr., editors, </editor> <booktitle> Advances in Genetic Programming 2, chapter 17. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, USA, </address> <year> 1996. </year>
Reference-contexts: Programs are represented in PDGP as graphs with nodes representing functions and terminals and links representing 1 The development of parallel programs should not be confused with the parallel implementations of GP, which are essentially methods of speeding-up the genetic search of standard tree-like programs <ref> [3, 6, 21, 13, 25] </ref>. These methods are usually based on the use of multiple processors, each one handling a separate population, a subset of fitness evaluations or a subset of fitness cases. 2 the flow of control and results.
Reference: [4] <author> Forrest H. Bennett III. </author> <title> Automatic creation of an efficient multi-agent architecture using genetic programming with architecture-altering operations. </title> <editor> In John R. Koza, David E. Goldberg, David B. 31 Fogel, and Rick L. Riolo, editors, </editor> <booktitle> Genetic Programming 1996: Proceedings of the First Annual Conference, </booktitle> <pages> page 30, </pages> <address> Stanford University, CA, USA, 28-31 July 1996. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Also, in conjunction with an interpreter implementing a parallel virtual machine, GP can be used to translate sequential programs into parallel ones [31] or to develop some kinds of parallel programs. 1 For example, Bennett, in <ref> [4] </ref>, used a parallel virtual machine in which several standard tree-like programs (called "agents") would have their nodes executed in parallel with a two stage mechanism simulating parallelism of sensing actions and simple conflict resolution (prioritisation) for actions with side effects.
Reference: [5] <author> Scott Brave. </author> <title> Evolving deterministic finite automata using cellular encoding. </title> <editor> In John R. Koza, David E. Goldberg, David B. Fogel, and Rick L. Riolo, editors, </editor> <booktitle> Genetic Programming 1996: Proceedings of the First Annual Conference, </booktitle> <pages> page 39, </pages> <address> Stanford University, CA, USA, 28-31 July 1996. </address> <publisher> MIT Press. </publisher>
Reference-contexts: terminals would have guaranteed validity of all offspring. 27 The problem on which we applied the full PDGP representation was inducing, from positive and negative example, a deterministic finite state automaton capable of recognising a prefixed language (for more details on the problem and on past and recent results see <ref> [5] </ref>). The language we chose for our tests was the regular language L=a*b*a*b* consisting of all sentences with 0 or more a symbols followed by 0 or more b symbols followed by 0 or more a's followed by 0 or more b's.
Reference: [6] <author> Dimitris C. Dracopoulos and Simon Kent. </author> <title> Speeding up genetic programming: A parallel BSP implementation. </title> <editor> In John R. Koza, David E. Goldberg, David B. Fogel, and Rick L. Riolo, editors, </editor> <booktitle> Genetic Programming 1996: Proceedings of the First Annual Conference, </booktitle> <pages> page 421, </pages> <address> Stanford University, CA, USA, 28-31 July 1996. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Programs are represented in PDGP as graphs with nodes representing functions and terminals and links representing 1 The development of parallel programs should not be confused with the parallel implementations of GP, which are essentially methods of speeding-up the genetic search of standard tree-like programs <ref> [3, 6, 21, 13, 25] </ref>. These methods are usually based on the use of multiple processors, each one handling a separate population, a subset of fitness evaluations or a subset of fitness cases. 2 the flow of control and results.
Reference: [7] <author> Herman Ehrenburg. </author> <title> Improved direct acyclic graph handling and the combine operator in genetic programming. </title> <editor> In John R. Koza, David E. Goldberg, David B. Fogel, and Rick L. Riolo, editors, </editor> <booktitle> Genetic Programming 1996: Proceedings of the First Annual Conference, </booktitle> <pages> page 285, </pages> <address> Stanford University, CA, USA, 28-31 July 1996. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Other work which has some relation with ours is based on the idea, firstly proposed by Handley [12] and later improved by Ehrenburg <ref> [7] </ref>, of storing the population of parse trees as a single directed acyclic graph, rather than as a forest of trees. This leads to considerable savings of memory (structurally identical subtrees are not duplicated) and computation (the value computed by each subtree for each fitness case is cached).
Reference: [8] <author> Chris Gathercole and Peter Ross. </author> <title> An adverse interaction between crossover and restricted tree depth in genetic programming. </title> <editor> In John R. Koza, David E. Goldberg, David B. Fogel, and Rick L. Riolo, editors, </editor> <booktitle> Genetic Programming 1996: Proceedings of the First Annual Conference, </booktitle> <pages> page 291, </pages> <address> Stanford University, CA, USA, </address> <month> 28-31 July </month> <year> 1996. </year> <note> MIT Press. to appear in. </note>
Reference-contexts: 1000 4800 12000 16800 21000 60000 32 1000 18200 27600 56400 114400 N/A 128 1000 18200 64000 193600 N/A N/A Table 2: Computational effort E necessary to solve the MAX-depth-Df*,+gf0.5g problems using SSAAN crossover. 6.5 MAX Problems The MAX problems are a class of problems introduced by Gathercole and Ross <ref> [8] </ref> to study the adverse interaction between crossover and the size limits imposed on the trees developed by standard GP.
Reference: [9] <author> F Gruau and D. Whitley. </author> <title> Adding learning to the cellular development process: a comparative study. </title> <journal> Evolutionary Computation, </journal> <volume> 1(3) </volume> <pages> 213-233, </pages> <year> 1993. </year>
Reference-contexts: When appropriate terminals, functions and/or interpreters are defined, standard GP can go beyond the production of sequential tree-like programs. For example using cellular encoding GP can be used to develop (grow) structures, like neural nets <ref> [9, 10] </ref> or electronic circuits [19, 18], which can be thought 1 of as performing some form of parallel analogue computation. <p> Indirect graph representations, like cellular encoding <ref> [9, 10, 19, 18] </ref> or edge encoding [20], do not suffer from this problem as the standard GP operators can be used on them.
Reference: [10] <author> Frederic Gruau. </author> <title> Genetic micro programming of neural networks. </title> <editor> In Kenneth E. Kinnear, Jr., editor, </editor> <booktitle> Advances in Genetic Programming, chapter 24. </booktitle> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: When appropriate terminals, functions and/or interpreters are defined, standard GP can go beyond the production of sequential tree-like programs. For example using cellular encoding GP can be used to develop (grow) structures, like neural nets <ref> [9, 10] </ref> or electronic circuits [19, 18], which can be thought 1 of as performing some form of parallel analogue computation. <p> Indirect graph representations, like cellular encoding <ref> [9, 10, 19, 18] </ref> or edge encoding [20], do not suffer from this problem as the standard GP operators can be used on them.
Reference: [11] <author> Frederic Gruau, Darrell Whitley, and Larry Pyeatt. </author> <title> A comparison between cellular encoding and direct encoding for genetic neural networks. </title> <editor> In John R. Koza, David E. Goldberg, David B. Fogel, and Rick L. Riolo, editors, </editor> <booktitle> Genetic Programming 1996: Proceedings of the First Annual Conference, </booktitle> <pages> page 81, </pages> <address> Stanford University, CA, USA, 28-31 July 1996. </address> <publisher> MIT Press. </publisher>
Reference-contexts: However, the meta-encoding of the graphs seems to make the search more costly by increasing the total number of individuals that must be evaluated (see for example the comparison between cellular and direct encodings of neural nets in <ref> [11] </ref>). PDGP uses a direct representation of graphs which allows the definition of crossover operators which respect all the criteria listed above (in particular efficiency and offspring validity).
Reference: [12] <author> S. Handley. </author> <title> On the use of a directed acyclic graph to represent a population of computer programs. </title> <booktitle> In Proceedings of the 1994 IEEE World Congress on Computational Intelligence, </booktitle> <pages> pages 154-159, </pages> <address> Orlando, Florida, USA, 27-29 June 1994. </address> <publisher> IEEE Press. </publisher>
Reference-contexts: A form of program representation similar to the one used in PADO (but using nodes without branching conditions) has been recently termed by Teller [29] "neural programming". Other work which has some relation with ours is based on the idea, firstly proposed by Handley <ref> [12] </ref> and later improved by Ehrenburg [7], of storing the population of parse trees as a single directed acyclic graph, rather than as a forest of trees.
Reference: [13] <author> Hugues Juille and Jordan B. Pollack. </author> <title> Massively parallel genetic programming. </title> <editor> In Peter J. Angeline and K. E. Kinnear, Jr., editors, </editor> <booktitle> Advances in Genetic Programming 2, chapter 17. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, USA, </address> <year> 1996. </year>
Reference-contexts: Programs are represented in PDGP as graphs with nodes representing functions and terminals and links representing 1 The development of parallel programs should not be confused with the parallel implementations of GP, which are essentially methods of speeding-up the genetic search of standard tree-like programs <ref> [3, 6, 21, 13, 25] </ref>. These methods are usually based on the use of multiple processors, each one handling a separate population, a subset of fitness evaluations or a subset of fitness cases. 2 the flow of control and results.
Reference: [14] <editor> K. E. Kinnear, Jr., editor. </editor> <booktitle> Advances in Genetic Programming. </booktitle> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: This form of GP has been applied successfully to a large number of difficult problems like automated design, pattern recognition, robot control, symbolic regression, music generation, image compression, image analysis, etc. <ref> [16, 17, 14, 15, 1, 22] </ref>. When appropriate terminals, functions and/or interpreters are defined, standard GP can go beyond the production of sequential tree-like programs.
Reference: [15] <editor> J. R. Koza, D. E. Goldberg, D. B. Fogel, and R. L. Riolo, editors. </editor> <booktitle> Proceedings of the First International Conference on Genetic Programming, </booktitle> <address> Stenford University, July 1996. </address> <publisher> MIT Press. </publisher>
Reference-contexts: This form of GP has been applied successfully to a large number of difficult problems like automated design, pattern recognition, robot control, symbolic regression, music generation, image compression, image analysis, etc. <ref> [16, 17, 14, 15, 1, 22] </ref>. When appropriate terminals, functions and/or interpreters are defined, standard GP can go beyond the production of sequential tree-like programs.
Reference: [16] <author> John R. Koza. </author> <title> Genetic Programming: On the Programming of Computers by Means of Natural Selection. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: 1 Introduction Genetic Programming (GP) is an extension of Genetic Algorithms (GAs) in which the structures that make up the population to be optimised are not fixed-length character strings that encode possible solutions to a problem, but programs that, when executed, are the candidate solutions to the problem <ref> [16, 17] </ref>. Programs are expressed in GP as parse trees, rather than as lines of code. For example, the simple expression max (x * y, 3 + x * y) would be represented as shown in Figure 1. <p> This form of GP has been applied successfully to a large number of difficult problems like automated design, pattern recognition, robot control, symbolic regression, music generation, image compression, image analysis, etc. <ref> [16, 17, 14, 15, 1, 22] </ref>. When appropriate terminals, functions and/or interpreters are defined, standard GP can go beyond the production of sequential tree-like programs. <p> The best results in terms of number of fitness evaluations were obtained using a grid with 8 rows and 8 columns, global mutation (with probability 0.02), "full" initialisation, and SSAAN crossover. With these settings E=25,000 and N=1,625,000. This compares extremely favourably with the results described by Koza <ref> [16, 17] </ref> who obtained E=96,000 for P=16,000 and E=80,000 for P=4,000. This result is particularly interesting considering the small size of the populations used in these experiments (P=1,000). It should be noted that this level of performance is not a statistical accident.
Reference: [17] <author> John R. Koza. </author> <title> Genetic Programming II: Automatic Discovery of Reusable Programs. </title> <publisher> MIT Pres, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction Genetic Programming (GP) is an extension of Genetic Algorithms (GAs) in which the structures that make up the population to be optimised are not fixed-length character strings that encode possible solutions to a problem, but programs that, when executed, are the candidate solutions to the problem <ref> [16, 17] </ref>. Programs are expressed in GP as parse trees, rather than as lines of code. For example, the simple expression max (x * y, 3 + x * y) would be represented as shown in Figure 1. <p> This form of GP has been applied successfully to a large number of difficult problems like automated design, pattern recognition, robot control, symbolic regression, music generation, image compression, image analysis, etc. <ref> [16, 17, 14, 15, 1, 22] </ref>. When appropriate terminals, functions and/or interpreters are defined, standard GP can go beyond the production of sequential tree-like programs. <p> The best results in terms of number of fitness evaluations were obtained using a grid with 8 rows and 8 columns, global mutation (with probability 0.02), "full" initialisation, and SSAAN crossover. With these settings E=25,000 and N=1,625,000. This compares extremely favourably with the results described by Koza <ref> [16, 17] </ref> who obtained E=96,000 for P=16,000 and E=80,000 for P=4,000. This result is particularly interesting considering the small size of the populations used in these experiments (P=1,000). It should be noted that this level of performance is not a statistical accident. <p> In comparative terms, with this grid size, PDGP solves the even-3 parity problem with approximately the same number of fitness evaluations as standard GP while producing solutions with 9 or fewer nodes. As the average complexity of the solutions described in <ref> [17] </ref> was 44.6 nodes, this seems to be a considerable saving. <p> For our experiments we selected a well-known, extensively studied problem: the lawnmower problem. The problem consists of finding a program which can control the movements of a lawnmower so that it cuts all the grass in the lawn <ref> [17, pages 225-273] </ref>. <p> Different plots refer to different combinations of parameters and operators. initialisation method). Different plots refer to different combinations of parameters and operators. 18 In order to make the comparison with the results described in <ref> [17] </ref> possible, we used the terminal set T =fMOW, LEFT, randomg and the function set F =fV8A, FROG, PROG2, PROG3g. The function MOW performs a step forward, mows the new grass patch and returns the pair of coordinates (0; 0). <p> Actually, a comparison between the linear regression plotted in the figure, which has equation 3286 + 26:8 fi L, L being the size of the lawn (regression coefficient r=0.949), and the linear regression for standard GP <ref> [17, page 266] </ref> shows that PDGP scales up about 2,300 times better. Also, as shown in Figure 20, the structural complexity (i.e. the number of nodes) of the solutions does not vary significantly (it really could not) as the complexity of the problem changes. <p> In order to understand whether these levels of performance were due to the ability of the SSIAN crossover or to the particularly narrow grid used, which enforces strongly graph-like (as opposed to tree 8 Unlike in <ref> [17] </ref>, we did not use PROGN for efficiency reasons. 19 20 complexity of the problem (number of cells in the lawn). as a function of the complexity of the problem. 21 like) solutions, we decided to study the behaviour of PDGP on the 64-cell problem as the width of the grid <p> PDGP without ADFs). 9 6.4 Symbolic Regression To investigate further the matter of parametrised vs. non-parametrised reuse, we applied PDGP to a symbolic regression problem on which GP performance where approximately the same with (E=1,440,000) or without (E=1,176,000) ADFs with a population of P=4,000 individuals (see <ref> [17, pages 110-122] </ref>). The problem is to find a function (i.e. a program) which fits 50 data samples obtained from the sextic polynomial p (x) = x 6 2x 4 + x 2 .
Reference: [18] <author> John R. Koza, David Andre, Forrest H. Bennett III, and Martin A. Keane. </author> <title> Use of automatically defined functions and architecture-altering operations in automated circuit synthesis using genetic programming. </title> <editor> In John R. Koza, David E. Goldberg, David B. Fogel, and Rick L. Riolo, editors, </editor> <booktitle> Genetic Programming 1996: Proceedings of the First Annual Conference, </booktitle> <pages> page 132, </pages> <address> Stanford University, CA, USA, 28-31 July 1996. </address> <publisher> MIT Press. </publisher> <pages> 32 </pages>
Reference-contexts: When appropriate terminals, functions and/or interpreters are defined, standard GP can go beyond the production of sequential tree-like programs. For example using cellular encoding GP can be used to develop (grow) structures, like neural nets [9, 10] or electronic circuits <ref> [19, 18] </ref>, which can be thought 1 of as performing some form of parallel analogue computation. <p> Indirect graph representations, like cellular encoding <ref> [9, 10, 19, 18] </ref> or edge encoding [20], do not suffer from this problem as the standard GP operators can be used on them.
Reference: [19] <author> John R. Koza, Forrest H. Bennett III David Andre, and Martin A. Keane. </author> <title> Automated WYWIWYG design of both the topology and component values of electrical circuits using genetic programming. </title> <editor> In John R. Koza, David E. Goldberg, David B. Fogel, and Rick L. Riolo, editors, </editor> <booktitle> Genetic Programming 1996: Proceedings of the First Annual Conference, </booktitle> <pages> page 123, </pages> <address> Stanford University, CA, USA, 28-31 July 1996. </address> <publisher> MIT Press. </publisher>
Reference-contexts: When appropriate terminals, functions and/or interpreters are defined, standard GP can go beyond the production of sequential tree-like programs. For example using cellular encoding GP can be used to develop (grow) structures, like neural nets [9, 10] or electronic circuits <ref> [19, 18] </ref>, which can be thought 1 of as performing some form of parallel analogue computation. <p> Indirect graph representations, like cellular encoding <ref> [9, 10, 19, 18] </ref> or edge encoding [20], do not suffer from this problem as the standard GP operators can be used on them.
Reference: [20] <author> S. Like and L. Spector. </author> <title> Evolving graphs and networks with edge encoding: Preliminary report. </title> <editor> In J. R. Koza, editor, </editor> <booktitle> Late Breaking Papers at the Genetic Programming 1996 Conference, </booktitle> <pages> pages 117-124, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: Indirect graph representations, like cellular encoding [9, 10, 19, 18] or edge encoding <ref> [20] </ref>, do not suffer from this problem as the standard GP operators can be used on them.
Reference: [21] <author> Mouloud Oussaidene, Bastien Chopard, Olivier V. Pictet, and Marco Tomassini. </author> <title> Parallel genetic programming: An application to trading models evolution. </title> <editor> In John R. Koza, David E. Goldberg, David B. Fogel, and Rick L. Riolo, editors, </editor> <booktitle> Genetic Programming 1996: Proceedings of the First Annual Conference, </booktitle> <pages> page 357, </pages> <address> Stanford University, CA, USA, 28-31 July 1996. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Programs are represented in PDGP as graphs with nodes representing functions and terminals and links representing 1 The development of parallel programs should not be confused with the parallel implementations of GP, which are essentially methods of speeding-up the genetic search of standard tree-like programs <ref> [3, 6, 21, 13, 25] </ref>. These methods are usually based on the use of multiple processors, each one handling a separate population, a subset of fitness evaluations or a subset of fitness cases. 2 the flow of control and results.
Reference: [22] <author> Riccardo Poli. </author> <title> Genetic programming for image analysis. </title> <editor> In John R. Koza, David E. Goldberg, David B. Fogel, and Rick L. Riolo, editors, </editor> <booktitle> Genetic Programming 1996: Proceedings of the First Annual Conference, </booktitle> <pages> page 363, </pages> <address> Stanford University, CA, USA, 28-31 July 1996. </address> <publisher> MIT Press. </publisher>
Reference-contexts: This form of GP has been applied successfully to a large number of difficult problems like automated design, pattern recognition, robot control, symbolic regression, music generation, image compression, image analysis, etc. <ref> [16, 17, 14, 15, 1, 22] </ref>. When appropriate terminals, functions and/or interpreters are defined, standard GP can go beyond the production of sequential tree-like programs.
Reference: [23] <author> Riccardo Poli. </author> <title> Some steps towards a form of parallel distributed genetic programming. </title> <booktitle> In Proceedings of the First On-line Workshop on Soft Computing, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: This paper describes Parallel Distributed Genetic Programming (PDGP), a new form of Genetic Programming (GP) which is suitable for the development of programs with a high degree of parallelism and distributedness (some early explorations with PDGP have been described in <ref> [23] </ref>).
Reference: [24] <editor> D.E. Rumelhart and J.L. McClelland, editors. </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> Vol. </volume> <pages> 1-2. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: Finally, we draw some conclusions and describe promising directions for future work in Section 7. 2 Representation As mentioned above, usually programs in GP are represented as parse trees. Taking inspiration from the parallel distributed processing performed in neural nets <ref> [24] </ref>, we decided to explore the possibility of representing programs as graphs with labelled nodes and oriented links. The idea was that nodes could be the functions and terminals used in a program while the links would determine which arguments to use in each function-node when it is next evaluated. <p> They also suggest that, when looking for logic and algebraic solutions, PDGP can compete, in terms of computational effort, with other well established machine learning techniques, like the standard back-propagation algorithm which requires from several hundreds to a few thousands training epochs to solve the XOR problem <ref> [24] </ref>. 15 6.2 Even-3 Parity Problem In this section we report on some results obtained by applying PDGP to the even-3 parity problem.
Reference: [25] <author> Kilian Stoffel and Lee Spector. </author> <title> High-performance, parallel, stack-based genetic programming. </title> <editor> In John R. Koza, David E. Goldberg, David B. Fogel, and Rick L. Riolo, editors, </editor> <booktitle> Genetic Programming 1996: Proceedings of the First Annual Conference, </booktitle> <pages> page 224, </pages> <address> Stanford University, CA, USA, 28-31 July 1996. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Programs are represented in PDGP as graphs with nodes representing functions and terminals and links representing 1 The development of parallel programs should not be confused with the parallel implementations of GP, which are essentially methods of speeding-up the genetic search of standard tree-like programs <ref> [3, 6, 21, 13, 25] </ref>. These methods are usually based on the use of multiple processors, each one handling a separate population, a subset of fitness evaluations or a subset of fitness cases. 2 the flow of control and results.
Reference: [26] <author> Astro Teller. </author> <title> Evolving programmers: The co-evolution of intelligent recombination operators. </title> <editor> In Peter J. Angeline and K. E. Kinnear, Jr., editors, </editor> <booktitle> Advances in Genetic Programming 2, chapter 3. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, USA, </address> <year> 1996. </year>
Reference-contexts: The output of a system is a linear combination of the output of the programs included in the system. Initially PADO programs seemed to use the standard tree-like representation used in GP (see for example [28, page 30]). However, more recent work <ref> [27, 26] </ref> has clarified that systems are represented as directed graphs in which arcs represent possible flows of control and nodes perform actions and take decisions on which of the possible control flows should be followed. <p> Each node includes an action (a terminal, a function, an automatically define function (ADF) or library call), a branching condition, and a stack. The interpretation of PADO programs is performed through a post-order chronological (not structural) graph traversal (see <ref> [26] </ref> for more details). A form of program representation similar to the one used in PADO (but using nodes without branching conditions) has been recently termed by Teller [29] "neural programming".
Reference: [27] <author> Astro Teller and Manuela Veloso. </author> <title> A controlled experiment: Evolution for learning difficult image classification. </title> <booktitle> In Seventh Portuguese Conference On Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: The output of a system is a linear combination of the output of the programs included in the system. Initially PADO programs seemed to use the standard tree-like representation used in GP (see for example [28, page 30]). However, more recent work <ref> [27, 26] </ref> has clarified that systems are represented as directed graphs in which arcs represent possible flows of control and nodes perform actions and take decisions on which of the possible control flows should be followed.
Reference: [28] <author> Astro Teller and Manuela Veloso. </author> <title> PADO: Learning tree structured algorithms for orchestration into an object recognition system. </title> <type> Technical Report CMU-CS-95-101, </type> <institution> Department of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, USA, </address> <year> 1995. </year>
Reference-contexts: Andre, Bennet and Koza [2] used GP to discover rules for cellular automata which could solve large majority-classification problems. In a system called PADO (Parallel Algorithm Discovery and Orchestration), Teller and Veloso <ref> [28] </ref> used a combination of GP and linear discrimination to obtain classification programs for signals and images. Given the slight similarity between the representation for programs used in PADO and the one used in PDGP we will give more details on such a system. <p> The system with the highest output determines the output of PADO. The output of a system is a linear combination of the output of the programs included in the system. Initially PADO programs seemed to use the standard tree-like representation used in GP (see for example <ref> [28, page 30] </ref>). However, more recent work [27, 26] has clarified that systems are represented as directed graphs in which arcs represent possible flows of control and nodes perform actions and take decisions on which of the possible control flows should be followed.
Reference: [29] <author> Astro Teller and Manuela Veloso. </author> <title> Neural programming and an internal reinforcement policy. </title> <editor> In John R. Koza, editor, </editor> <booktitle> Late Breaking Papers at the Genetic Programming 1996 Conference Stan-ford University July 28-31, </booktitle> <year> 1996, </year> <pages> pages 186-192, </pages> <address> Stanford University, CA, USA, 28-31 July 1996. </address> <publisher> Stanford Bookstore. </publisher>
Reference-contexts: The interpretation of PADO programs is performed through a post-order chronological (not structural) graph traversal (see [26] for more details). A form of program representation similar to the one used in PADO (but using nodes without branching conditions) has been recently termed by Teller <ref> [29] </ref> "neural programming". Other work which has some relation with ours is based on the idea, firstly proposed by Handley [12] and later improved by Ehrenburg [7], of storing the population of parse trees as a single directed acyclic graph, rather than as a forest of trees.
Reference: [30] <author> S. Thrun, J. Bala, E. Bloedorn, I. Bratko, B. Cestnik, J. Cheng, K. De Jong, S. Dzeroski, S.E. Fahlman, D. Fisher, R. Hamann, K. Kaufman, S. Keller, I. Kononenko, J. Kreuziger, R.S. Michalski, T. Mitchell, P. Pachowicz, Y. Reich, H. Vafaie, W. Van de Welde, W. Wenzel, J. Wnek, and J. Zhang. </author> <title> The monk's problems a performance comparison of different learning algorithms. </title> <type> Technical Report CMU-CS-91-197, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1991. </year> <note> Available via anonymous ftp from archive.cis.ohio-state.edu, file /pub/neuroprose/thrun.comparison.ps.Z. </note>
Reference-contexts: The MONK's problems are a collection of binary classification problems over a six-attribute discrete domain which have been used extensively to compare different machine learning techniques (the results of a contest between 25 different techniques in described in detail in <ref> [30] </ref>). The ranges for the attributes A i are: A 1 2 f1; 2; 3g, A 2 2 f1; 2; 3g, A 3 2 f1; 2g, A 4 2 f1; 2; 3g, A 5 2 f1; 2; 3; 4g and A 6 2 f1; 2g. <p> However, the overall results shown by PDGP on the MONK's problems are aligned to those produced by the best machine learning techniques: in <ref> [30] </ref> no technique was able to perform 100% correctly on all three MONK's problems. For example neural nets could not avoid over-fitting the noise in the MONK-3 problem. Actually, only five techniques out of 25 could perform 100% correctly on the MONK-3 test set. <p> representation of programs used in PDGP allowed us to develop efficient forms of 11 Pop-11 is extremely good for fast prototyping and for exploring alternative algorithms, but it is also much slower than C/C++. 12 The same form of binarisation of the attribute of the MONK's problems was used in <ref> [30] </ref> to train multi-layer neural nets. 29 30 crossover and mutation. By changing the size, shape and dimensionality of the grid, this representation allows a fine control on the size and shape of the programs being developed.
Reference: [31] <author> Paul Walsh and Conor Ryan. </author> <title> Paragen: A novel technique for the autoparallelisation of sequential programs using genetic programming. </title> <editor> In John R. Koza, David E. Goldberg, David B. Fogel, and Rick L. Riolo, editors, </editor> <booktitle> Genetic Programming 1996: Proceedings of the First Annual Conference, </booktitle> <pages> page 406, </pages> <address> Stanford University, CA, USA, 28-31 July 1996. </address> <publisher> MIT Press. </publisher> <pages> 33 </pages>
Reference-contexts: Also, in conjunction with an interpreter implementing a parallel virtual machine, GP can be used to translate sequential programs into parallel ones <ref> [31] </ref> or to develop some kinds of parallel programs. 1 For example, Bennett, in [4], used a parallel virtual machine in which several standard tree-like programs (called "agents") would have their nodes executed in parallel with a two stage mechanism simulating parallelism of sensing actions and simple conflict resolution (prioritisation) for
References-found: 31

