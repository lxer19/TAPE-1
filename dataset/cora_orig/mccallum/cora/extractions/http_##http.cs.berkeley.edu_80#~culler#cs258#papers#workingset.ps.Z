URL: http://http.cs.berkeley.edu:80/~culler/cs258/papers/workingset.ps.Z
Refering-URL: http://http.cs.berkeley.edu:80/~culler/cs258/
Root-URL: http://www.cs.berkeley.edu
Title: Working Sets, Cache Sizes, and Node Granularity Issues for Large-Scale Multiprocessors  
Author: Edward Rothberg Jaswinder Pal Singh and Anoop Gupta 
Address: 14924 N.W. Greenbrier Parkway Stanford University Beaverton, OR 97006 Stanford, CA 94305  
Affiliation: Intel Supercomputer Systems Division Computer Systems Laboratory  
Abstract: The distribution of resources among processors, memory and caches is a crucial question faced by designers of large-scale parallel machines. If a machine is to solve problems with a certain data set size, should it be built with a large number of processors each with a small amount of memory, or a smaller number of processors each with a large amount of memory? How much cache memory should be provided per processor for cost-effectiveness? And how do these decisions change as larger problems are run on larger machines? In this paper, we explore the above questions based on the characteristics of five important classes of large-scale parallel scientific applications. We first show that all the applications have a hierarchy of well-defined per-processor working sets, whose size, performance impact and scaling characteristics can help determine how large different levels of a multiprocessor's cache hierarchy should be. Then, we use these working sets together with certain other important characteristics of the applications|such as communication to computation ratios, concurrency, and load balancing behavior|to reflect upon the broader question of the granularity of processing nodes in high-performance multiprocessors. We find that very small caches whose sizes do not increase with the problem or machine size are adequate for all but two of the application classes. Even in the two exceptions, the working sets scale quite slowly with problem size, and the cache sizes needed for problems that will be run in the foreseeable future are small. We also find that relatively fine-grained machines, with large numbers of processors and quite small amounts of memory per processor, are appropriate for all the applications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David H. Bailey. </author> <title> FFTs in External or Hierarchical Memories. </title> <journal> Journal of Supercomputing, </journal> <volume> 4 </volume> <pages> 23-25, </pages> <year> 1990. </year>
Reference-contexts: Both the cache usage and the computation to communication ratio can be improved dramatically by increasing the radix of the computation (see <ref> [1] </ref> and [12]). Increasing the radix is equivalent to `unrolling' the butterfly, performing multiple butterfly stages in a single pass through the data. A radix-8 FFT, for example, would combine three butterfly stages into a single stage, where each step in this new stage performs operations on 8 points simultaneously.
Reference: [2] <author> Geoffrey Fox et al. </author> <title> Solving Problems on Concurrent Processors, Volume I: General Techniques and Regular Problems. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: Two details have been shown to be crucial for reducing interpro P0 P1 P2 P6 P7 P8 Column K Row K cessor communication volumes and thus obtaining high performance. First, the blocks of the matrix are assigned to processors using a 2-D scatter decomposition <ref> [2] </ref>. That is, the processors are thought of as a P fi Q grid, and block (I ; J ) in the matrix is assigned to processor (I mod P; J mod Q). A simple 3 fi 3 processor example is shown in Figure 1.
Reference: [3] <author> Lars Hernquist. </author> <title> Hierarchical N-body methods. </title> <journal> Computer Physics Communications, </journal> <volume> 48 </volume> <pages> 107-115, </pages> <year> 1988. </year>
Reference-contexts: It changes slightly only with the order of moments used, and hence with the nature of an individual interactions. The size of the important lev2WS is proportional to the number of interactions computed per particle, which is of order 1 2 log n <ref> [3] </ref>. The lev2WS therefore scales very slowly with the number of particles n, more quickly with the accuracy parameter , and is independent of the number of processors p. The constant of proportionality in the above size expression is about 6 Kbytes.
Reference: [4] <author> H.T. Kung. </author> <title> Memory requirements for balanced computer architectures. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <year> 1986. </year>
Reference-contexts: for computation to com munication ratio of 5 2 log N P derived earlier, a ratio of R requires the number of data points per processor to be N=P = 2 2 exponential growth rate of per-processor memory required to improve computation to communication ratios has been previously noted in <ref> [4] </ref>. The consequences of this growth rate are quite severe. Increasing the computation to communication ratio from 33 to a more easily sustained ratio of 60, for example, would require the per-processor data set to be increased to roughly 270 Mbytes.
Reference: [5] <author> Gordon Moore. </author> <title> VLSI: Some fundamental challenges. </title> <journal> IEEE Spectrum, </journal> <pages> pages 30-37, </pages> <month> April </month> <year> 1979. </year>
Reference-contexts: Many of these reasons may disappear, however, due to continually improving technology and integration levels. Within a decade, we are likely to see chips with more than 100 million transistors each <ref> [5] </ref>. This will allow processors, caches, and memory to reside on the same chip. Decisions about how to partition the transistors on a chip among processor, cache, and memory will then involve entirely different tradeoffs.
Reference: [6] <author> Jason Nieh and Marc Levoy. </author> <title> Volume rendering on scalable shared-memory MIMD architectures. </title> <booktitle> In Proceedings of the Boston Workshop on Volume Visualization, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: Volume visualization techniques are of key importance in the analysis and understanding of multidimensional sampled data. This application, which renders volumes using optimized ray tracing techniques, uses a parallel version of the fastest known sequential algorithm for volume rendering <ref> [6] </ref>. 7.1 Description of Computation The volume to be rendered is represented by a cube of voxels (or volume elements). The outermost loop of the computation is over a series of frames or images. Successive frames correspond to changing angles between the viewer and the volume being rendered. <p> Thus, if the entire voxel data set were replicated in the local memory of every processing node, there would be essentially no communication during rendering (except the small amount of communication generated by the ray-stealing performed to ensure load balancing toward the end of the rendering phase <ref> [6] </ref>). However, such replication would imply either unreasonable amounts of local memory per processor or that large data sets cannot be run. In our shared address space implementation, the data set is not replicated at all in main memory but only to some extent in the caches.
Reference: [7] <author> John K. Salmon. </author> <title> Parallel Hierarchical N-body Methods. </title> <type> PhD thesis, </type> <institution> California Institute of Technology, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Using some curve fitting from <ref> [7] </ref> and some of our own, we find that the communication per processor required to compute forces in a time-step scales as n 1=3 3 p 1=3 log 4=3 p, and that the communication to computation ratio is therefore p 2=3 n 2=3 log n .
Reference: [8] <author> Jaswinder Pal Singh, John L. Hennessy, and Anoop Gupta. </author> <title> Implications of hierarchical N-body techniques for multiprocessor architecture. </title> <type> Technical Report CSL-TR-92-506, </type> <institution> Stanford University, </institution> <year> 1992. </year>
Reference-contexts: The number of particles per processor remains the same, so load balancing is not affected, and the communication to computation ratio either increases extremely slowly (if the accuracy is not scaled as well) or stays constant (if accuracy is scaled) <ref> [8] </ref>. The cache size needed per processor grows, but is still relatively small, as we have seen. However, such memory-constrained scaling to keep the grain size constant causes the execution time to increase very rapidly.
Reference: [9] <author> Jaswinder Pal Singh, John L. Hennessy, and Anoop Gupta. </author> <title> Scaling parallel programs for multiprocessors: Methodology and examples. </title> <journal> IEEE Computer, </journal> <volume> 26(7), </volume> <month> July </month> <year> 1993. </year> <note> To appear. Also Stanford Univeristy Tech. Report no. CSL-TR-92-541, </note> <year> 1992. </year>
Reference-contexts: The TC scaling model, on the other hand, assumes that the user will increase the problem size so that the new problem takes as much time to solve on the new machine as the old problem took on the old machine. For more information about these scaling models, see <ref> [9] </ref>. 2.3 Grain Size Having understood the working sets, we then examine other application characteristics that affect the desirable granularity of processing nodes. In particular, we study the implications of interprocessor communication costs, load balance, and problem concurrency for node granularity. <p> Scaling only n, however, is naive. In practice, all of n, and the time-step resolution t are likely to be scaled simultaneously, in order to scale their contributions to the overall simulation error at the same rate <ref> [9] </ref>. This leads to the following rule: If n is scaled by a factor of s, t must be scaled by a factor of 1 4 p s and by a factor of 1 8 p s when quadrupole moments are used.
Reference: [10] <author> Jaswinder Pal Singh, Chris Holt, Takashi Totsuka, Anoop Gupta, and John L. Hennessy. </author> <title> Load balancing and data locality in hierarchical N-body methods. </title> <journal> Journal of Parallel and Distributed Computing. </journal> <note> To appear. Prelim. version available as Stanford Univeristy Tech. Report no. CSL-TR-92-505, </note> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: The two most prominent hierarchical N-body methods are the Barnes-Hut and Fast Multipole methods. We shall use a three-dimensional galactic Barnes-Hut simulation as our example in this paper <ref> [10] </ref>. 6.1 Description of Computation The computation in N-body problems proceeds over a number of time-steps. Every time-step computes the forces experienced by all bodies, and uses these forces to update the positions and velocities of the bodies.
Reference: [11] <author> R. van de Geijn. </author> <title> Massively parallel LINPACK benchmark on the Intel Touchstone Delta and iPSC/860 systems. </title> <type> Technical Report CS-91-28, </type> <institution> University of Texas at Austin, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: dense Cholesky factorization, dense eigenvalue methods, and in many respects sparse Cholesky factorization. 3.1 Description of Computation Dense LU factorization can be performed extremely efficiently if the dense n fi n matrix A is divided into an N fi N array of B fi B blocks, (n = N B) <ref> [11] </ref>.
Reference: [12] <author> Charles van Loan. </author> <title> Computational Frameworks for the Fast Fourier Transform. </title> <publisher> SIAM, </publisher> <year> 1992. </year>
Reference-contexts: Both the cache usage and the computation to communication ratio can be improved dramatically by increasing the radix of the computation (see [1] and <ref> [12] </ref>). Increasing the radix is equivalent to `unrolling' the butterfly, performing multiple butterfly stages in a single pass through the data. A radix-8 FFT, for example, would combine three butterfly stages into a single stage, where each step in this new stage performs operations on 8 points simultaneously.
References-found: 12

