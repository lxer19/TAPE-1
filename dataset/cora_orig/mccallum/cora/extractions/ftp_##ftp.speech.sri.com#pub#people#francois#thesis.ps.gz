URL: ftp://ftp.speech.sri.com/pub/people/francois/thesis.ps.gz
Refering-URL: http://www.speech.sri.com/people/francois/publications.html
Root-URL: 
Title: TWO-LAYER LINEAR STRUCTURES FOR FAST ADAPTIVE FILTERING  
Author: Fran~coise Beaufays 
Degree: a dissertation submitted to the department of electrical engineering and the committee on graduate studies of stanford university in partial fulfillment of the requirements for the degree of doctor of philosophy By  
Date: June 1995  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> P. H. Ang and D. Auld. </author> <title> Video compression makes big gains. </title> <journal> IEEE Spectrum, </journal> <pages> pages 16-19, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: ADAPTATION ALGORITHMS FOR LINEAR FILTERING 18 where the constant fi 2 <ref> [0; 1] </ref> is generally chosen close to one but slightly smaller for stability reasons. <p> DFT and DCT transforms are used for similar purposes in many other fields. In image compression for example, JPEG encoders contains a 2-D DCT that analyses the input image and outputs a number of frequency components <ref> [1] </ref>. After being quantized, these components are coded with variable length codes to achieve the desired image compression. The DCT preprocessing is thus used to "eliminate" the frequency components that carry little information about the image. <p> To carry the analysis through, we make the assumption that the inputs to the adaptive filter are produced by a first-order Markov system. We demonstrate that if the inputs are Markov-1 with autocorrelation parameter 2 <ref> [0; 1] </ref>, the eigenvalue spread of the autocorrelation matrix after DFT and amplitude normalization tends to (1 + )=(1 ) as the filter length, N , tends to infinity. Similarly, we show that after DCT and amplitude normalization, the asymptotic eigenvalue spread is reduced to (1 + ). <p> This matrix is thus a rank zero perturbation. Similarly, a matrix whose elements are all equal to N ( 2 <ref> [0; 1] </ref>) has its largest eigenvalue max satisfying max N N N ; (4.13) which tends to zero if N is large enough. Clearly, matrices whose elements decrease even faster than N as N increases also have asymptotic rank zero. <p> Such a filter has an impulse response that decreases geometrically with a rate given by the z-plane position of the filter pole. The N fi N autocorrelation matrix of a Markov-1 input signal x k of parameter 2 <ref> [0; 1] </ref> is given by R N = B B B B B @ 1 : : : N2 . . . . . . <p> the matrix whose eigenvalues were computed analytically, and its inverse, f X 1 N , has asymptotically the same eigenvalues as S N . 4.3 Conclusion The previous results can be summarized as follows. * The eigenvalue spread of the autocorrelation matrix of a first-order Markov signal of parameter 2 <ref> [0; 1] </ref> tends to (1 + ) 2 =(1 ) 2 as the size N of the matrix increases. * The eigenvalue spread of the autocorrelation matrix of the same signal after DFT and after amplitude normalization tends to (1 + )=(1 ) as the size N of the matrix increases. <p> We used the eigenvalue spread, i.e. the ratio of the largest to the smallest eigenvalues, as a performance factor for the algorithms. After modeling the algorithms so as to make this study possible, we demonstrated that for Markov-1 inputs with correlation parameter 2 <ref> [0; 1] </ref>, the eigenvalue spread before transformation was equal to (1 + ) 2 =(1 ) 2 , and that after transformation by the DFT or the DCT it reduced to (1 + )=(1 ) or (1 + ) respectively.
Reference: [2] <author> F. Beaufays. </author> <title> Transform domain adaptive filters: an analytical approach. </title> <journal> IEEE Trans. on Signal Processing, </journal> <volume> 43(2) </volume> <pages> 422-431, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: The DFT can thus be seen as a bank of bandpass filters whose central frequencies span the interval <ref> [0; 2] </ref> [43]. Figure 3.3 shows the magnitude of a sample transfer function for a 64-point DFT. At each time k, the input signal x k is decomposed into N signals lying in different frequency bins. <p> For comparison, the eigenvalue spread before any transformation tends to (1 + ) 2 =(1 ) 2 . For highly correlated signals, i.e. for signals with correlation close to one, the DCT preprocessing brings a dramatic improvement over plain LMS 1 . 1 These results were first published in <ref> [2] </ref>. 38 CHAPTER 4. EIGENVALUE SPREAD COMPUTATION 39 4.1 Introduction To determine how well a given transform decorrelates certain classes of input signals and how fast the corresponding transform-domain algorithm converges, one must set the problem in a mathematical framework. <p> The limit of P N (!) for N tending to infinity is equal to zero for all frequencies ! 2 <ref> [0; 2] </ref>. Therefore, E N is a rank zero perturbation and, by virtue of Theorem 2, f D N D N . QED.
Reference: [3] <author> F. Beaufays and B. Widrow. </author> <title> Two-layer linear structures for fast adaptive filtering. </title> <booktitle> In Proc. of the World Congress on Neural Networks, volume III, </booktitle> <pages> pages 87-93, </pages> <address> San Diego, CA, </address> <month> June 6-9 </month> <year> 1994. </year>
Reference-contexts: This transformation, combined with a power normalization step that operates on the learning rate of the algorithm, speeds up the convergence of the LMS filter. The resulting structure is thus a two-layer linear adaptive structure <ref> [3] </ref>, as represented in Fig. 8.1 for the case of DFT-LMS. Because a data-independent transformation cannot be a perfect universal decor-relator, transform-domain algorithms do not quite achieve the same performance as RLS, although they can under certain input conditions come very close to RLS. 100 CHAPTER 8.
Reference: [4] <author> F. Beaufays and B. Widrow. </author> <title> On the advantages of the LMS spectrum analyzer over non-adaptive implementations of the sliding-DFT. </title> <journal> IEEE Trans. on Circuits and Systems, </journal> <volume> 4(42), </volume> <month> April </month> <year> 1995. </year>
Reference-contexts: The two representations are clearly equivalent but they yield slightly different formulae. For ease of reference to the literature, we list both formulae, denoting the first DFT 1 These results were first published in <ref> [4] </ref>. CHAPTER 6. IMPLEMENTATION OF THE SLIDING-DFT 71 DFT " and the second one DFT # , with the arrow symbolizing the direction of time.
Reference: [5] <author> N. J. Bershad. </author> <title> Comparison of RLS and LMS algorithms for tracking a chirped signal. </title> <booktitle> In Proc. of the IEEE Intl Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 896-899, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Its weight update requires only O (N ) computations per iteration, it is by far the simplest algorithm to implement, it is robust to error propagation in limited precision implementations [9], and it tracks non-stationarities better than other adaptation algorithms <ref> [5, 38, 7] </ref>. These properties have greatly contributed to the popularity of LMS, although the major complain about the algorithm remains its slow convergence. In applications where it is critical to achieve fast convergence, LMS is often not a viable solution. <p> Note that choosing fi = 1 does not impair the convergence properties: the convergence speed with RLS in roughly independent of fi. As a counterpart, RLS displays poor tracking capabilities in non-stationary environments <ref> [5, 38, 7] </ref>. Intuitively, the weight vector in RLS is based on all the past history of the input signal.
Reference: [6] <author> N. J. Bershad and P. L. Feintuch. </author> <title> A normalized frequency domain LMS adaptive algorithm. </title> <journal> IEEE Trans. on Acoustics, Speech, and Signal Processing, </journal> <volume> 34(3) </volume> <pages> 452-461, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Typically, this is done by adding to p k (i) a 1 The powers p k (i) can also be estimated based on a sliding rectangular window, or with the help of an arbitrary linear weighting filter. These approaches have been analyzed in details in <ref> [6, 17, 54] </ref>. CHAPTER 3.
Reference: [7] <author> N. J. Bershad and O. M. Macchi. </author> <title> Adaptive recovery of a chirped sinusoid in noise, part 2: Performance of the LMS algorithm. </title> <journal> IEEE Trans. on Acoustics, Speech, and Signal Processing, </journal> <volume> 39(3) </volume> <pages> 595-602, </pages> <month> March </month> <year> 1991. </year> <note> 120 BIBLIOGRAPHY 121 </note>
Reference-contexts: Its weight update requires only O (N ) computations per iteration, it is by far the simplest algorithm to implement, it is robust to error propagation in limited precision implementations [9], and it tracks non-stationarities better than other adaptation algorithms <ref> [5, 38, 7] </ref>. These properties have greatly contributed to the popularity of LMS, although the major complain about the algorithm remains its slow convergence. In applications where it is critical to achieve fast convergence, LMS is often not a viable solution. <p> Note that choosing fi = 1 does not impair the convergence properties: the convergence speed with RLS in roughly independent of fi. As a counterpart, RLS displays poor tracking capabilities in non-stationary environments <ref> [5, 38, 7] </ref>. Intuitively, the weight vector in RLS is based on all the past history of the input signal.
Reference: [8] <author> R. N. Bracewell. </author> <title> The Hartley Transform. </title> <publisher> Oxford University Press, </publisher> <address> New York, NY, </address> <year> 1986. </year>
Reference-contexts: However, by not having to estimate R 1 , this algorithm gains in robustness. Other fixed, data-independent transformations have been considered in the literature [39] to replace the DFT: the discrete cosine transform (DCT) [49], the discrete sine transform (DST) [49], the discrete Hartley transform (DHT) <ref> [8] </ref>, the Walsh-Hadamard transform [58] etc. While no general general proof exists that assesses the superiority of one transform over the others 5 , experiments can be made with various classes of input data to obtain some intuition on which transform performs better on which class of inputs.
Reference: [9] <author> J. M. Cioffi. </author> <title> Limited-precision effects in adaptive filtering. </title> <journal> IEEE Trans. on Circuits and Systems, </journal> <volume> 34(7) </volume> <pages> 821-833, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Its weight update requires only O (N ) computations per iteration, it is by far the simplest algorithm to implement, it is robust to error propagation in limited precision implementations <ref> [9] </ref>, and it tracks non-stationarities better than other adaptation algorithms [5, 38, 7]. These properties have greatly contributed to the popularity of LMS, although the major complain about the algorithm remains its slow convergence. <p> Another characteristics that makes lattice filters very popular is the ease with which their stability can be monitored (see e.g. [23]). It has been observed, however, that finite arithmetic effects can severely degrade the algorithm performance <ref> [9] </ref>. The price for the improvement brought by the lattice structure is the increased complexity of the algorithm in terms of number of equations to be implemented, number of variables to be stored, and general complication of the algebra. <p> Therefore, the issue of limited precision in the implementation must be taken into account. The consequences of the propagation of limited-precision noise in adaptive filters are by now well known. A comprehensive survey in this subject can be found in Cioffi's tutorial <ref> [9] </ref>. Another problem consists in analyzing and reducing the effects of noise propagation in the sliding-DFT. CHAPTER 6. IMPLEMENTATION OF THE SLIDING-DFT 79 The straightforward implementation of the sliding-DFT (see Eq. 6.3 or 6.6) displays very poor performances under limited-precision conditions.
Reference: [10] <author> G. A. Clark, S. K. Mitra, and S. R. Parker. </author> <title> Block implementation of adaptive digital filters. </title> <journal> IEEE Trans. on Circuits and Systems, </journal> <volume> 28 </volume> <pages> 584-592, </pages> <month> June </month> <year> 1981. </year>
Reference-contexts: The properties of complex LMS are very similar to those of real LMS, although slight differences can be observed in terms of mean square convergence and stability performance [24]. 2.2.4 The Block-LMS Algorithm Block LMS is a partially batched extension of LMS <ref> [12, 10] </ref>. The instantaneous error gradient, c r~, is computed at each iteration as in regular non-block LMS, but rather than being used right away to update the weights, it is buffered for a certain number, L, of iterations. <p> It can therefore be implemented efficiently by taking the Fourier transforms of the two signals, computing their product, and inverse transforming the result <ref> [12, 10] </ref>. The computational efficiency of this method counterbalances the slowliness of the weight convergence. <p> Not implementing these constraints results in wrap-around effects that affect the performance of the algorithms (biased optimal solution, extra noise in the steady-state solution, etc.) <ref> [10, 46] </ref>. Two methods have been described in the literature that calculate the linear convolution of two signals by taking the product of their Fourier transforms: the overlap-save method and the overlap-add method [44, 11].
Reference: [11] <author> G. A. Clark, S. R. Parker, and S. K. Mitra. </author> <title> A unified approach to time- and frequency-domain realization of FIR adaptive digital filters. </title> <journal> IEEE Trans. on Acoustics, Speech, and Signal Processing, </journal> <volume> 31(5) </volume> <pages> 1073-1083, </pages> <month> October </month> <year> 1983. </year>
Reference-contexts: Two methods have been described in the literature that calculate the linear convolution of two signals by taking the product of their Fourier transforms: the overlap-save method and the overlap-add method <ref> [44, 11] </ref>. These methods require typically one Fourier transform, one inverse Fourier transform, and a few appropriate vector manipulations (zero-padding, truncation, concatenation of vectors, ...) to calculate one convolution.
Reference: [12] <author> M. Dentino, J. McCool, and B. Widrow. </author> <title> Adaptive filtering in the frequency domain. </title> <journal> Proc. of the IEEE, </journal> <volume> 66(12) </volume> <pages> 1658-1659, </pages> <month> December </month> <year> 1978. </year>
Reference-contexts: The properties of complex LMS are very similar to those of real LMS, although slight differences can be observed in terms of mean square convergence and stability performance [24]. 2.2.4 The Block-LMS Algorithm Block LMS is a partially batched extension of LMS <ref> [12, 10] </ref>. The instantaneous error gradient, c r~, is computed at each iteration as in regular non-block LMS, but rather than being used right away to update the weights, it is buffered for a certain number, L, of iterations. <p> It can therefore be implemented efficiently by taking the Fourier transforms of the two signals, computing their product, and inverse transforming the result <ref> [12, 10] </ref>. The computational efficiency of this method counterbalances the slowliness of the weight convergence.
Reference: [13] <author> L. Elden and L. Wittmeyer-Koch. </author> <title> Numerical Analysis, an Introduction. </title> <publisher> Academic Press, Inc., </publisher> <address> San Diego, CA, </address> <year> 1990. </year>
Reference-contexts: The IEEE standard for single precision representation is specified, and an example showing the conversion of a real number into this standard is given. Most of this material is covered in details in Elden and Wittmeyer-Koch <ref> [13] </ref>. We then describe in section D.3 a simple procedure for simulating a low precision processor on a computer running the above (high precision) IEEE standard.
Reference: [14] <author> E. Eweda and O. Macchi. </author> <title> Convergence of an adaptive linear estimation algorithm. </title> <journal> IEEE Trans. on Circuits and Systems, </journal> <volume> 29(2) </volume> <pages> 119-127, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: However, this has the inconvenient of slowing down the adaptation process. A better solution which is often used in practice consists in starting the adaptation with a large and decreasing it progressively as the weights converge (see <ref> [14] </ref>). The misadjustment of LMS should therefore not be seen as a major limitation of the algorithm. Another way of interpreting the non-zero misadjustment of LMS is to note that the algorithm has no memory.
Reference: [15] <author> E. R. Ferrara, Jr. </author> <title> Frequency-domain adaptive filtering. </title> <editor> In C. F. N. Cowen and P. M. Grant, editors, </editor> <title> Adaptive Filters, </title> <type> ch. 6, </type> <address> Englewood Cliffs, NJ, 1985. </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: The main advantage of these algorithms in addition to their computational efficiency is their potentially very fast convergence. By attributing to each transformed weight a learning rate that is inversely proportional to the energy of the corresponding input, the convergence of the algorithms can be greatly improved <ref> [15, 54] </ref>. 2.4.2 Transform-Domain Non-Block LMS Algorithms This family of algorithms was first introduced by Narayan under the name transform domain LMS algorithms [43].
Reference: [16] <author> A. Feuer. </author> <title> Performance analysis of the block least mean squares algorithm. </title> <journal> IEEE Trans. on Circuits and Systems, </journal> <volume> 32(9) </volume> <pages> 960-963, </pages> <month> September </month> <year> 1985. </year>
Reference-contexts: Also, the maximum learning rate that can be used without encountering stability problems is L times smaller than the one that could be used, under CHAPTER 2. ADAPTATION ALGORITHMS FOR LINEAR FILTERING 17 identical input conditions, with regular LMS <ref> [16] </ref>. This, of course, may adversely influence the convergence speed of the algorithm. The advantage of block-LMS comes from the fact that the block-gradient in Eq. 2.34 can be seen as a linear correlation between the input signal and the output error signal.
Reference: [17] <author> S. Florian and N. J. Bershad. </author> <title> A weighted normalized frequency domain LMS adaptive algorithm. </title> <journal> IEEE Trans. on Acoustics, Speech, and Signal Processing, </journal> <volume> 36(7) </volume> <pages> 1002-1007, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: Typically, this is done by adding to p k (i) a 1 The powers p k (i) can also be estimated based on a sliding rectangular window, or with the help of an arbitrary linear weighting filter. These approaches have been analyzed in details in <ref> [6, 17, 54] </ref>. CHAPTER 3.
Reference: [18] <author> G. F. Franklin, J. D. Powell, and M. L. Workman. </author> <title> Digital Control of Dynamic Systems. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <note> second edition, 1990. BIBLIOGRAPHY 122 </note>
Reference-contexts: Its main disadvantage is its very slow convergence under certain input conditions. As we will show in this thesis, some modifications can be brought to LMS to ameliorate its convergence properties. Another famous adaptation algorithm is the recursive least squares or RLS algorithm (see e.g. <ref> [18, 23] </ref>). In the RLS algorithm, the filter coefficients are made equal at each iteration to the best approximation of the Wiener solution that can be calculated based on all the data the system has so far seen. <p> The computational efficiency of this method counterbalances the slowliness of the weight convergence. We will see in section 2.4 that a whole class of transform-domain algorithms is based on this principle. 2.3 The RLS Algorithm The Recursive Least Squares (RLS) algorithm implements recursively an exact least squares solution <ref> [18, 23] </ref>. We saw previously that the Wiener solution for an adaptive filter of finite length is given by w opt = R 1 p where R is the autocorrelation matrix of the inputs and p is the cross-correlation between inputs and desired ouputs. <p> The filter was constructed as follows. We took a one-pole low-pass filter of frequency ! 0 and transformed it into a band-pass filter using the tranformation <ref> [18] </ref>: p =) Q p + p ; (5.3) where p is the continuous Laplace variable, Q is the quality factor of the band-pass filter.
Reference: [19] <author> G. H. Golub and Ch. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, MD, </address> <note> second edition, </note> <year> 1989. </year>
Reference-contexts: N f D N ) = 0 has the same solutions as det ( f D 1 N R N 3 The generalized eigenvalues of a matrix A N with respect to a matrix B N are the values of that satisfy det (A N B N ) = 0 <ref> [19] </ref>. CHAPTER 4. EIGENVALUE SPREAD COMPUTATION 46 I N ) = 0.
Reference: [20] <author> R. M. Gray. </author> <title> Toeplitz and circulant matrices: II. </title> <type> Technical Report 6504-1, </type> <institution> Information Systems Laboratory, Stanford University, </institution> <address> CA, </address> <month> April </month> <year> 1977. </year>
Reference-contexts: Theory has been developed in the past about the decorrelating ability of the DFT and the DCT (see e.g. <ref> [49, 21, 20] </ref>). For example, it has been proved that, since R N is Toeplitz, the autocorrelation matrix B N obtained after processing with the DFT or the DCT is asymptotically equivalent to a diagonal matrix, where the concept of asymptotic equivalence is defined as follows. <p> CHAPTER 4. EIGENVALUE SPREAD COMPUTATION 40 iff N!1 N!1 where jA N j is the weak norm of A N , that is the square root of the arithmetic average of the singular values of A N . Adopting the notation used by Gray in <ref> [20] </ref>, we will simply write A N ~ B N to refer to the asymptotic equivalence of fAg and fBg. Theorem 1 Let E N = A N B N . <p> Theorem 1 Let E N = A N B N . If A N ~ B N ; then lim N!1 N trace (E N ) = 0: (4.5) This theorem is a direct consequence of Definition 1; its proof is given in <ref> [20] </ref>. <p> N1 N2 : : : 1 C C C C C A For N large (theoretically for N tending to infinity), the eigenvalues of R N are values of the power spectrum of x k evaluated at uniformly distributed points on the frequency axis <ref> [21, 20] </ref>. In particular, the smallest and the largest eigenvalues of R N are given by the minimum and the maximum of the power spectrum of x k . This results directly from the fact that R N is Toeplitz. <p> The results are summarized in Table 5.1. Here are a few observations that can be made: * First, the eigenvalue spread of the "original" matrix, R, increases as the filter size N increases, and decreases as the bandwidth, Bw, increases. This is not surprising since we know <ref> [20] </ref> that the eigenvalues of a Toeplitz matrix lay on the power spectrum of the generating signal, at uniformely distributed abscissas. More points (i.e. <p> Since circulant matrices are a special case of Toeplitz matrices <ref> [20] </ref>, D N is Toeplitz. QED. The matrix B N was defined as B N = F N R N F H N (Eq. 4.19). <p> Alternatively, a circulant matrix can be defined as a matrix whose eigenvectors form a DFT matrix <ref> [20] </ref>. 105 APPENDIX A.
Reference: [21] <author> U. Grenander and G. </author> <title> Szego. Toeplitz Forms and Their Applications. </title> <publisher> Chelsea Publishing Company, </publisher> <address> New York, NY, </address> <note> second edition, </note> <year> 1984. </year>
Reference-contexts: Theory has been developed in the past about the decorrelating ability of the DFT and the DCT (see e.g. <ref> [49, 21, 20] </ref>). For example, it has been proved that, since R N is Toeplitz, the autocorrelation matrix B N obtained after processing with the DFT or the DCT is asymptotically equivalent to a diagonal matrix, where the concept of asymptotic equivalence is defined as follows. <p> N1 N2 : : : 1 C C C C C A For N large (theoretically for N tending to infinity), the eigenvalues of R N are values of the power spectrum of x k evaluated at uniformly distributed points on the frequency axis <ref> [21, 20] </ref>. In particular, the smallest and the largest eigenvalues of R N are given by the minimum and the maximum of the power spectrum of x k . This results directly from the fact that R N is Toeplitz.
Reference: [22] <author> L. J. Griffiths. </author> <title> A simple adaptive algorithm for real-time processing in antenna array processing. </title> <journal> Proc. of the IEEE, </journal> <volume> 57 </volume> <pages> 1696-1704, </pages> <month> August </month> <year> 1969. </year>
Reference-contexts: INTRODUCTION 3 circuits (adaptive line enhancer [56]), parameter estimation systems (adaptive spectrum analyzers, adaptive correlators, ... [64]), beamforming circuits <ref> [67, 22] </ref>, and so forth. As new applications were developed and as the adaptation speed required from existing systems increased, various adaptation algorithms were developed to replace LMS, some based on RLS techniques, some based on LMS itself. In this thesis, we will concentrate on the second category.
Reference: [23] <author> S. Haykin. </author> <title> Adaptive Filter Theory. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <note> second edition, </note> <year> 1991. </year>
Reference-contexts: Introduction The first steps in the field of adaptive filtering can be traced back to the 1930's - 1940's with Wiener's early work on linear estimation of stochastic processes and the formulation of the famous Wiener-Hopf equations (see e.g. <ref> [23, 30] </ref>). These equations allow the determination of the linear filter that best maps, in a least squares sense, an input signal into some target output. <p> Its main disadvantage is its very slow convergence under certain input conditions. As we will show in this thesis, some modifications can be brought to LMS to ameliorate its convergence properties. Another famous adaptation algorithm is the recursive least squares or RLS algorithm (see e.g. <ref> [18, 23] </ref>). In the RLS algorithm, the filter coefficients are made equal at each iteration to the best approximation of the Wiener solution that can be calculated based on all the data the system has so far seen. <p> The early theory of LMS developed by Widrow and Hoff considers the convergence in the mean of the weight vector. Later studies have also included convergence in the mean square <ref> [57, 40, 23] </ref>. For our purposes, the former will suffice, and we will limit ourselves to a summary of Widrow's main results, refering the reader to Widrow's and Haykin's textbooks [69, 23] for more details. <p> Later studies have also included convergence in the mean square [57, 40, 23]. For our purposes, the former will suffice, and we will limit ourselves to a summary of Widrow's main results, refering the reader to Widrow's and Haykin's textbooks <ref> [69, 23] </ref> for more details. Widrow based his analysis on the exact steepest descent algorithm: w k+1 = w k r k : (2.18) The exact error gradient at time k can be expressed as r k = dw k = 2Rw k 2p (2.20) CHAPTER 2. <p> By not accumulating any information about the data, LMS can more easily track a time-varying solution than RLS whose weights are delayed in their evolution by the obsolete information they have accumulated (see e.g. <ref> [23] </ref>). CHAPTER 2. ADAPTATION ALGORITHMS FOR LINEAR FILTERING 15 In conclusion, besides for its slow convergence when the inputs are highly correlated, LMS displays excellent properties. <p> Equivalently, the weight update can be formulated as w k+1 = w k + 2e fl with e k = d k y k = d k w H k x k , where the superscript H denotes the hermitian, i.e. the transpose conjugate <ref> [23] </ref>. The properties of complex LMS are very similar to those of real LMS, although slight differences can be observed in terms of mean square convergence and stability performance [24]. 2.2.4 The Block-LMS Algorithm Block LMS is a partially batched extension of LMS [12, 10]. <p> The computational efficiency of this method counterbalances the slowliness of the weight convergence. We will see in section 2.4 that a whole class of transform-domain algorithms is based on this principle. 2.3 The RLS Algorithm The Recursive Least Squares (RLS) algorithm implements recursively an exact least squares solution <ref> [18, 23] </ref>. We saw previously that the Wiener solution for an adaptive filter of finite length is given by w opt = R 1 p where R is the autocorrelation matrix of the inputs and p is the cross-correlation between inputs and desired ouputs. <p> The asymptotic misadjustment in RLS can be arbitrarily decreased by increasing the parameter fi up to one. Intuitively, fi 1 causes the entries of the matrix R k to grow as k increases (see Eq. 2.35), and forces k (Eq. 2.48) to gradually decrease down to zero (see <ref> [23] </ref> for a more formal justification). Note that choosing fi = 1 does not impair the convergence properties: the convergence speed with RLS in roughly independent of fi. As a counterpart, RLS displays poor tracking capabilities in non-stationary environments [5, 38, 7]. <p> Another characteristics that makes lattice filters very popular is the ease with which their stability can be monitored (see e.g. <ref> [23] </ref>). It has been observed, however, that finite arithmetic effects can severely degrade the algorithm performance [9].
Reference: [24] <author> L. L. Horowitz and K. D. Senne. </author> <title> Performance advantage of complex LMS for controlling narrow-band adaptive arrays. </title> <journal> IEEE Trans. on Acoustics, Speech, and Signal Processing, </journal> <volume> 29 </volume> <pages> 722-736, </pages> <year> 1981. </year>
Reference-contexts: The properties of complex LMS are very similar to those of real LMS, although slight differences can be observed in terms of mean square convergence and stability performance <ref> [24] </ref>. 2.2.4 The Block-LMS Algorithm Block LMS is a partially batched extension of LMS [12, 10].
Reference: [25] <author> G. H. Hostetter. </author> <title> Fourier analysis using spectral observers. </title> <journal> Proc. of the IEEE, </journal> <volume> 68 </volume> <pages> 284-285, </pages> <month> February </month> <year> 1980. </year>
Reference-contexts: B.18, B.20), and exactly when p + q is odd (see Eq. B.18, B.20). As a result, A N diagB N , which is the desired result. QED. Appendix C Ongoing Deadbeat Spectral Observers C.1 Introduction The spectral observer introduced by Hostetter <ref> [25] </ref> is essentially a time-varying state-space observer that estimates the DFT of a discrete-time signal from the observation of the signal samples. First introduced in 1980 [25, 26] for the DFT, the concept was later generalized to other transforms such as the Walsh-Hadamard transform [28]. <p> QED. Appendix C Ongoing Deadbeat Spectral Observers C.1 Introduction The spectral observer introduced by Hostetter [25] is essentially a time-varying state-space observer that estimates the DFT of a discrete-time signal from the observation of the signal samples. First introduced in 1980 <ref> [25, 26] </ref> for the DFT, the concept was later generalized to other transforms such as the Walsh-Hadamard transform [28]. In 1986, Peceli provided a simple way of calculating the coefficients involved in Hostetter's spectral observer [45].
Reference: [26] <author> G. H. Hostetter. </author> <title> Recursive discrete fourier transformation. </title> <journal> IEEE Trans. on Acoustics, Speech, and Signal Processing, </journal> <volume> 28(2) </volume> <pages> 184-190, </pages> <month> April </month> <year> 1980. </year>
Reference-contexts: QED. Appendix C Ongoing Deadbeat Spectral Observers C.1 Introduction The spectral observer introduced by Hostetter [25] is essentially a time-varying state-space observer that estimates the DFT of a discrete-time signal from the observation of the signal samples. First introduced in 1980 <ref> [25, 26] </ref> for the DFT, the concept was later generalized to other transforms such as the Walsh-Hadamard transform [28]. In 1986, Peceli provided a simple way of calculating the coefficients involved in Hostetter's spectral observer [45].
Reference: [27] <author> G. H. Hostetter. </author> <title> Ongoing deadbeat observers for linear time-varying systems. </title> <booktitle> In American Control Conf., </booktitle> <pages> pages 1099-1101, </pages> <month> June </month> <year> 1982. </year>
Reference-contexts: The more specific concept of time-varying observer is discussed in <ref> [27] </ref>. APPENDIX C. ONGOING DEADBEAT SPECTRAL OBSERVERS 114 Peceli showed [45] that such g i 's are of the form g i = c fl With these observer gains, the estimated state vector ^ x N1 is exactly equal to DFT N1 .
Reference: [28] <author> G. H. Hostetter. </author> <title> Recursive discrete Walsh-Hadamard transformation. </title> <journal> Proc. of the IEEE, </journal> <volume> 71(2) </volume> <pages> 271-272, </pages> <month> February </month> <year> 1983. </year>
Reference-contexts: First introduced in 1980 [25, 26] for the DFT, the concept was later generalized to other transforms such as the Walsh-Hadamard transform <ref> [28] </ref>. In 1986, Peceli provided a simple way of calculating the coefficients involved in Hostetter's spectral observer [45]. In this appendix, we will try to present a comprehensive summary of the ideas presented in all four papers.
Reference: [29] <author> T. Kailath. </author> <title> Linear Systems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1980. </year>
Reference-contexts: The matrix inversion lemma states that if A = B + CD 1 C T , then A 1 = B 1 B 1 C (D + C T B 1 C) 1 C T B 1 (see e.g. <ref> [29] </ref>). CHAPTER 2. ADAPTATION ALGORITHMS FOR LINEAR FILTERING 19 Equations 2.43, 2.45, and 2.46 summarize the algorithm. <p> 0 ; g 1 ; : : : ; g N1 satisfy the following condition: N1 Y i = 0: (C.7) 1 The concept of observer of a dynamic system is defined and discussed in the original paper by Luenberger [37], and in various linear control systems textbooks (see e.g. <ref> [29] </ref>). The more specific concept of time-varying observer is discussed in [27]. APPENDIX C.
Reference: [30] <author> T. Kailath. </author> <title> Lectures on Wiener and Kalman Filtering. </title> <publisher> Springer-Verlag, </publisher> <address> Wien - New York, </address> <year> 1981. </year> <note> BIBLIOGRAPHY 123 </note>
Reference-contexts: Introduction The first steps in the field of adaptive filtering can be traced back to the 1930's - 1940's with Wiener's early work on linear estimation of stochastic processes and the formulation of the famous Wiener-Hopf equations (see e.g. <ref> [23, 30] </ref>). These equations allow the determination of the linear filter that best maps, in a least squares sense, an input signal into some target output. <p> we will successively discuss three families of algorithms: the least mean squares (LMS) algorithms, the recursive least squares (RLS) algorithms, and the transform-domain LMS algorithms. 1 The filter that best maps, in a least squares sense, an input signal into a given desired output is in general of infinite length <ref> [30] </ref>. It therefore requires the resolution of an infinite set of equations, which may be impractical in computer implementations. Limiting the number of filter taps to N constrains the solution w opt to be of finite length.
Reference: [31] <author> D. T. L. Lee, M. Morf, and B. Friedlander. </author> <title> Recursive least-squares ladder estimation algorithms. </title> <journal> IEEE Trans. on Circuits and Systems, </journal> <volume> 28 </volume> <pages> 467-481, </pages> <year> 1981. </year>
Reference-contexts: The main motivation for doing so was CHAPTER 2. ADAPTATION ALGORITHMS FOR LINEAR FILTERING 21 to reduce the computational complexity of RLS and improve its robustness while maintaining its convergence characteristics. The most famous algorithms in this family are those based on the so-called lattice structure <ref> [42, 31, 32, 33] </ref>. These algorithms take advantage of the fact that in a Toeplitz matrix such as the autocorrelation matrix R, only N out of N 2 elements are distinct.
Reference: [32] <author> F. Ling, D. Manolakis, and J. G. Proakis. </author> <title> New forms of LS lattice algorithms and an analysis of their round-off error characteristics. </title> <booktitle> In Proc. of the IEEE Intl Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 1739-1742, </pages> <address> Tampa, FL., </address> <year> 1985. </year>
Reference-contexts: The main motivation for doing so was CHAPTER 2. ADAPTATION ALGORITHMS FOR LINEAR FILTERING 21 to reduce the computational complexity of RLS and improve its robustness while maintaining its convergence characteristics. The most famous algorithms in this family are those based on the so-called lattice structure <ref> [42, 31, 32, 33] </ref>. These algorithms take advantage of the fact that in a Toeplitz matrix such as the autocorrelation matrix R, only N out of N 2 elements are distinct.
Reference: [33] <author> F. Ling, D. Manolakis, and J. G. Proakis. </author> <title> Numerically robust least-squares lattice-ladder algorithm with direct updating of the reflection coefficients. </title> <journal> IEEE Trans. on Acoustics, Speech, and Signal Processing, </journal> <volume> 34, </volume> <year> 1986. </year>
Reference-contexts: The main motivation for doing so was CHAPTER 2. ADAPTATION ALGORITHMS FOR LINEAR FILTERING 21 to reduce the computational complexity of RLS and improve its robustness while maintaining its convergence characteristics. The most famous algorithms in this family are those based on the so-called lattice structure <ref> [42, 31, 32, 33] </ref>. These algorithms take advantage of the fact that in a Toeplitz matrix such as the autocorrelation matrix R, only N out of N 2 elements are distinct.
Reference: [34] <author> B. Liu and L. T. Bruton. </author> <title> The two-dimensional complex LMS algorithm applied to the 2-D DFT. </title> <journal> IEEE Trans. on Circuits and Systems, </journal> <volume> 40(5) </volume> <pages> 337-341, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: For this reason, we would like to extend the concept of the LMS spectrum analyzer from the DFT to the DCT. Various extensions of the LMS spectrum analyzer have been published in the literature: a DHT analyzer was described in [35], a 2-D DFT spectrum analyzer was presented in <ref> [34] </ref>, a generalization to different orthogonal transforms was described in [61]. Although the DCT was one of the transformations covered by [61], its definition was quite different from the DCT we defined earlier and used in this thesis.
Reference: [35] <author> J.-C. Liu and T.-P. Lin. </author> <title> LMS-based DHT analyser. </title> <journal> Electronic Letters, </journal> <volume> 24(8) </volume> <pages> 483-485, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: For this reason, we would like to extend the concept of the LMS spectrum analyzer from the DFT to the DCT. Various extensions of the LMS spectrum analyzer have been published in the literature: a DHT analyzer was described in <ref> [35] </ref>, a 2-D DFT spectrum analyzer was presented in [34], a generalization to different orthogonal transforms was described in [61]. Although the DCT was one of the transformations covered by [61], its definition was quite different from the DCT we defined earlier and used in this thesis.
Reference: [36] <author> R. W. </author> <title> Lucky. Automatic equalization for digital communication. </title> <journal> Bell System Technical Journal, </journal> <volume> 44 </volume> <pages> 547-588, </pages> <year> 1965. </year>
Reference-contexts: For many decades, it has been a major component in a large number of engineering systems such as, for example, automatic controllers for linear systems (adaptive modeling filters, adaptive inverse controllers, ... [70]), various telephony and communication devices (adaptive interference and echo cancellors, adaptive equalizers, adaptive pulse code modulators, ... <ref> [36, 55, 47] </ref>), signal detection CHAPTER 1. INTRODUCTION 3 circuits (adaptive line enhancer [56]), parameter estimation systems (adaptive spectrum analyzers, adaptive correlators, ... [64]), beamforming circuits [67, 22], and so forth.
Reference: [37] <author> D. G. Luenberger. </author> <title> An introduction to observers. </title> <journal> IEEE Trans. on Automatic Control, </journal> <volume> 16(6) </volume> <pages> 596-602, </pages> <month> December </month> <year> 1971. </year>
Reference-contexts: iterations) is obtained if and only if the gains g 0 ; g 1 ; : : : ; g N1 satisfy the following condition: N1 Y i = 0: (C.7) 1 The concept of observer of a dynamic system is defined and discussed in the original paper by Luenberger <ref> [37] </ref>, and in various linear control systems textbooks (see e.g. [29]). The more specific concept of time-varying observer is discussed in [27]. APPENDIX C.
Reference: [38] <author> O. M. Macchi and N. J. Bershad. </author> <title> Adaptive recovery of a chirped sinusoid in noise, part 1: Performance of the RLS algorithm. </title> <journal> IEEE Trans. on Acoustics, Speech, and Signal Processing, </journal> <volume> 39(3) </volume> <pages> 583-594, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: Its weight update requires only O (N ) computations per iteration, it is by far the simplest algorithm to implement, it is robust to error propagation in limited precision implementations [9], and it tracks non-stationarities better than other adaptation algorithms <ref> [5, 38, 7] </ref>. These properties have greatly contributed to the popularity of LMS, although the major complain about the algorithm remains its slow convergence. In applications where it is critical to achieve fast convergence, LMS is often not a viable solution. <p> Note that choosing fi = 1 does not impair the convergence properties: the convergence speed with RLS in roughly independent of fi. As a counterpart, RLS displays poor tracking capabilities in non-stationary environments <ref> [5, 38, 7] </ref>. Intuitively, the weight vector in RLS is based on all the past history of the input signal.
Reference: [39] <author> D. F. Marshall, W. K. Jenkins, and J. J. Murphy. </author> <title> The use of orthogonal transforms for improving performance of adaptive filter. </title> <journal> IEEE Trans. on Circuits and Systems, </journal> <volume> 36(4) </volume> <pages> 474-484, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: ADAPTATION ALGORITHMS FOR LINEAR FILTERING 23 will not converge as fast as RLS which uses the inverse covariance matrix of the inputs to decorrelate them. However, by not having to estimate R 1 , this algorithm gains in robustness. Other fixed, data-independent transformations have been considered in the literature <ref> [39] </ref> to replace the DFT: the discrete cosine transform (DCT) [49], the discrete sine transform (DST) [49], the discrete Hartley transform (DHT) [8], the Walsh-Hadamard transform [58] etc. <p> Although the DCT does not separate frequencies the way the DFT does, it is a powerful signal decorrelator as will be demonstrated in the next chapter. 3.2.2 Geometrical Approach The DFT/DCT-LMS algorithm can also be illustrated geometrically <ref> [39] </ref>. The DFT and DCT matrices defined in Table 3.2, C N and F N are unitary, i.e. their rows are orthogonal to one another and have euclidian norm one. Unitary transformations perform only rotations and symmetries, they do not modify the shape of the object they transform.
Reference: [40] <author> J. E. Mazo. </author> <title> On the independence theory of equalizer convergence. </title> <journal> Bell System Technical Journal, </journal> <volume> 58 </volume> <pages> 963-993, </pages> <year> 1979. </year> <note> BIBLIOGRAPHY 124 </note>
Reference-contexts: The early theory of LMS developed by Widrow and Hoff considers the convergence in the mean of the weight vector. Later studies have also included convergence in the mean square <ref> [57, 40, 23] </ref>. For our purposes, the former will suffice, and we will limit ourselves to a summary of Widrow's main results, refering the reader to Widrow's and Haykin's textbooks [69, 23] for more details.
Reference: [41] <author> W. F. McGee. </author> <title> Fundamental relations between the LMS spectrum analyzer and the recursive least squares estimation. </title> <journal> IEEE Trans. on Circuits and Systems, </journal> <volume> 36(1) </volume> <pages> 151-153, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: Let us define the error function ~ as the average of the squared output 3 Starting from a different cost function (an infinite sum of squared errors weighted by an exponentially decaying factor), McGee showed in <ref> [41] </ref> that the LMS spectrum analyzer can be seen as a recursive least squares estimator. This leads to an interesting discussion about the weight behaviors, but it does not justify their one-step convergence. CHAPTER 6.
Reference: [42] <author> M. Morf, A. Vieira, and D. T. Lee. </author> <title> Ladder forms for identification and speech processing. </title> <booktitle> In Proc. of the IEEE Conf. on Decision Control, </booktitle> <pages> pages 1074-1078, </pages> <address> New Orleans, </address> <year> 1977. </year>
Reference-contexts: The main motivation for doing so was CHAPTER 2. ADAPTATION ALGORITHMS FOR LINEAR FILTERING 21 to reduce the computational complexity of RLS and improve its robustness while maintaining its convergence characteristics. The most famous algorithms in this family are those based on the so-called lattice structure <ref> [42, 31, 32, 33] </ref>. These algorithms take advantage of the fact that in a Toeplitz matrix such as the autocorrelation matrix R, only N out of N 2 elements are distinct.
Reference: [43] <author> S. S. Narayan, A. M. Peterson, and M. J. Narasimha. </author> <title> Transform domain LMS algorithm. </title> <journal> IEEE Trans. on Acoustics, Speech, and Signal Processing, </journal> <volume> 31(3) </volume> <pages> 609-615, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: to each transformed weight a learning rate that is inversely proportional to the energy of the corresponding input, the convergence of the algorithms can be greatly improved [15, 54]. 2.4.2 Transform-Domain Non-Block LMS Algorithms This family of algorithms was first introduced by Narayan under the name transform domain LMS algorithms <ref> [43] </ref>. Narayan's structure consists simply of an LMS filter whose inputs are preprocessed by a DFT and whose learning rates (one per weight) are adjusted as a function of the input energy levels (a full description of the algorithm is given in the next chapter). <p> The DFT can thus be seen as a bank of bandpass filters whose central frequencies span the interval [0; 2] <ref> [43] </ref>. Figure 3.3 shows the magnitude of a sample transfer function for a 64-point DFT. At each time k, the input signal x k is decomposed into N signals lying in different frequency bins.
Reference: [44] <author> A. V. Oppenheim and R. W. Schafer. </author> <title> Digital Signal Processing. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1975. </year>
Reference-contexts: Because the product of two Fourier series corresponds in the time domain to a circular convolution rather than a linear one, some constraints must be implemented to restore the linearity of the convolution (see e.g. <ref> [44] </ref>). Not implementing these constraints results in wrap-around effects that affect the performance of the algorithms (biased optimal solution, extra noise in the steady-state solution, etc.) [10, 46]. <p> Two methods have been described in the literature that calculate the linear convolution of two signals by taking the product of their Fourier transforms: the overlap-save method and the overlap-add method <ref> [44, 11] </ref>. These methods require typically one Fourier transform, one inverse Fourier transform, and a few appropriate vector manipulations (zero-padding, truncation, concatenation of vectors, ...) to calculate one convolution.
Reference: [45] <author> G. Peceli. </author> <title> A common structure for recursive discrete transforms. </title> <journal> IEEE Trans. on Circuits and Systems, </journal> <volume> 33(10) </volume> <pages> 1035-1036, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: First introduced in 1980 [25, 26] for the DFT, the concept was later generalized to other transforms such as the Walsh-Hadamard transform [28]. In 1986, Peceli provided a simple way of calculating the coefficients involved in Hostetter's spectral observer <ref> [45] </ref>. In this appendix, we will try to present a comprehensive summary of the ideas presented in all four papers. We will then demonstrate that, although it is proceeding from a completely different approach, the spectral observer performs exactly the same operations as the LMS spectrum analyzer. <p> The more specific concept of time-varying observer is discussed in [27]. APPENDIX C. ONGOING DEADBEAT SPECTRAL OBSERVERS 114 Peceli showed <ref> [45] </ref> that such g i 's are of the form g i = c fl With these observer gains, the estimated state vector ^ x N1 is exactly equal to DFT N1 .
Reference: [46] <author> L. Pelkowitz. </author> <title> Frequency domain analysis of wraparound error in fast convolution algorithms. </title> <journal> IEEE Trans. on Acoustics, Speech, and Signal Processing, </journal> <volume> 29(3) </volume> <pages> 413-422, </pages> <month> June </month> <year> 1981. </year>
Reference-contexts: Not implementing these constraints results in wrap-around effects that affect the performance of the algorithms (biased optimal solution, extra noise in the steady-state solution, etc.) <ref> [10, 46] </ref>. Two methods have been described in the literature that calculate the linear convolution of two signals by taking the product of their Fourier transforms: the overlap-save method and the overlap-add method [44, 11]. <p> A similar effect takes place in transform-domain block-LMS algorithms when the gradient constraints are dropped. In block-LMS algorithms, where the weight update is performed in the frequency-domain, this wrap-around effect turns out to bias the optimal solution of the filter <ref> [46] </ref>. In non-block LMS algorithms such as DFT-LMS and DCT-LMS, it only affects the speed of convergence of the filter; the optimal solution remains unbiased.
Reference: [47] <author> J. G. Proakis and J. H. Miller. </author> <title> An adaptive receiver for digital signaling through channels with intersymbol interference. </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> 15(4) </volume> <pages> 484-497, </pages> <month> July </month> <year> 1969. </year>
Reference-contexts: For many decades, it has been a major component in a large number of engineering systems such as, for example, automatic controllers for linear systems (adaptive modeling filters, adaptive inverse controllers, ... [70]), various telephony and communication devices (adaptive interference and echo cancellors, adaptive equalizers, adaptive pulse code modulators, ... <ref> [36, 55, 47] </ref>), signal detection CHAPTER 1. INTRODUCTION 3 circuits (adaptive line enhancer [56]), parameter estimation systems (adaptive spectrum analyzers, adaptive correlators, ... [64]), beamforming circuits [67, 22], and so forth.
Reference: [48] <author> L. R. Rabiner and B. Gold. </author> <title> Theory and Application of Digital Signal Processing. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1975. </year>
Reference-contexts: To save computations, the transform at time k + 1 can be computed directly from the transform at time k. This recursive implementation of the DFT/DCT is sometimes refered to as the sliding-DFT/DCT <ref> [48] </ref>, and has already been introduced in chapter 3, section 3.2.1. Implemented recursively, the DFT requires only O (N ) operations. In time-varying environments, adaptive filters may have to keep on adjusting their weights for thousands and thousands of iterations.
Reference: [49] <author> K. R. Rao and P. Yip. </author> <title> Discrete Cosine Transform. </title> <publisher> Academic Press, Inc., </publisher> <address> San Diego, CA, </address> <year> 1990. </year>
Reference-contexts: However, by not having to estimate R 1 , this algorithm gains in robustness. Other fixed, data-independent transformations have been considered in the literature [39] to replace the DFT: the discrete cosine transform (DCT) <ref> [49] </ref>, the discrete sine transform (DST) [49], the discrete Hartley transform (DHT) [8], the Walsh-Hadamard transform [58] etc. <p> However, by not having to estimate R 1 , this algorithm gains in robustness. Other fixed, data-independent transformations have been considered in the literature [39] to replace the DFT: the discrete cosine transform (DCT) <ref> [49] </ref>, the discrete sine transform (DST) [49], the discrete Hartley transform (DHT) [8], the Walsh-Hadamard transform [58] etc. <p> tends to make the algebra more tractable), second because it will provide a point of referiment to compare DCT-LMS with. 5 A classic benchmark is the Karhunen-Loeve transform (KLT), which performs an exact decor-relation of the input data by projecting them onto the eigenvectors of their autocorrelation matrix (see e.g. <ref> [49] </ref>). It is thus an optimal transform, but since it requires the a priori knowledge of these eigenvectors it is of little interest in practical applications. Chapter 3 Transform-Domain Algorithms: DFT-LMS and DCT-LMS This chapter gives a general presentation of DFT-LMS and DCT-LMS. <p> TRANSFORM-DOMAIN ALGORITHMS 25 window 1 . If the transform used to preprocess the data is the DFT, complex LMS with square-modulus power normalization is used. The two algorithms, DFT-LMS and DCT-LMS, are summarized in Table 3.1. The DCT used here (refered to as DCT-II in Rao's book <ref> [49] </ref>) was chosen because of its superior performance. Other definitions could be used for C N (i; l), but it is our experience that their decorrelating capabilities are worse than these of the DCT-II. A few remarks should be made at this point. The first one concerns the power estimation. <p> Theory has been developed in the past about the decorrelating ability of the DFT and the DCT (see e.g. <ref> [49, 21, 20] </ref>). For example, it has been proved that, since R N is Toeplitz, the autocorrelation matrix B N obtained after processing with the DFT or the DCT is asymptotically equivalent to a diagonal matrix, where the concept of asymptotic equivalence is defined as follows.
Reference: [50] <editor> David E. Rumelhart and James L. McClelland, editors. </editor> <booktitle> Parallel Distributed Processing. </booktitle> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year> <note> BIBLIOGRAPHY 125 </note>
Reference-contexts: These neural networks, which can be described as layered arrangements of linear adaptive units followed by nonlinear unimodal functions, typically contain a very large number of parameters that must be adapted. The most famous algorithm for adjusting these parameters is the backpropagation algorithm <ref> [62, 50, 63] </ref> which is nothing else than a generalization of LMS to this more complicated structure.
Reference: [51] <author> E. B. Saff and A. D. Snider. </author> <title> Fundamentals of Complex Analysis. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1976. </year>
Reference-contexts: IMPLEMENTATION OF THE SLIDING-DFT 70 iteratively updated to minimize a quadratic cost function [69], and a set of Fourier coefficients may be seen as the best mean squares fitting of an input sequence by a series of complex phasors <ref> [51] </ref>. However, its adaptive nature gives the LMS spectrum analyzer the ability to automatically adjust for possible errors in the DFT outputs.
Reference: [52] <author> J. J. Shynk. </author> <title> Frequency-domain and multirate adaptive filtering. </title> <booktitle> In IEEE Signal Processing Magazine, </booktitle> <pages> pages 14-37, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Since two convolutions are calculated at each weight update (one for the error gradient and one for the filter output), frequency-domain block-LMS algorithms are quite involved (see e.g. <ref> [52] </ref> for a detailed description of the algorithms). The main advantage of these algorithms in addition to their computational efficiency is their potentially very fast convergence.
Reference: [53] <author> J. J. Shynk and B. Widrow. </author> <title> Bandpass adaptive pole-zero filtering. </title> <booktitle> In Proc. of the IEEE Intl Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 2107-2110, </pages> <address> Tokyo, Japan, </address> <month> April </month> <year> 1986. </year>
Reference-contexts: Besides from breaking the pace of the system and increasing the average number of computations per iteration, this reset mechanism would add to the complexity of the circuitry. This problem was first addressed by Shynk <ref> [53] </ref> who proposed to slightly move the poles and zeros of the bandpass filters towards the inside of the unit-circle. This modification stabilizes the filters, but it alters the DFT/DCT outputs and, more importantly for our application, it increases the correlation between nearby frequency bins [53]. <p> was first addressed by Shynk <ref> [53] </ref> who proposed to slightly move the poles and zeros of the bandpass filters towards the inside of the unit-circle. This modification stabilizes the filters, but it alters the DFT/DCT outputs and, more importantly for our application, it increases the correlation between nearby frequency bins [53].
Reference: [54] <author> P. C. W. Sommen, P. J. Van Gerwen, H. J. Kotmans, and A. J. E. M. Janssen. </author> <title> Convergence analysis of a frequency-domain adaptive filter with exponential power averaging and generalized window function. </title> <journal> IEEE Trans. on Circuits and Systems, </journal> <volume> 34(7) </volume> <pages> 788-798, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: The main advantage of these algorithms in addition to their computational efficiency is their potentially very fast convergence. By attributing to each transformed weight a learning rate that is inversely proportional to the energy of the corresponding input, the convergence of the algorithms can be greatly improved <ref> [15, 54] </ref>. 2.4.2 Transform-Domain Non-Block LMS Algorithms This family of algorithms was first introduced by Narayan under the name transform domain LMS algorithms [43]. <p> Typically, this is done by adding to p k (i) a 1 The powers p k (i) can also be estimated based on a sliding rectangular window, or with the help of an arbitrary linear weighting filter. These approaches have been analyzed in details in <ref> [6, 17, 54] </ref>. CHAPTER 3.
Reference: [55] <author> M. M. Sondhi and A. J. Presti. </author> <title> A self-adaptive echo canceller. </title> <journal> Bell System Technical Journal, </journal> <volume> 45 </volume> <pages> 1851-1854, </pages> <month> December </month> <year> 1966. </year>
Reference-contexts: For many decades, it has been a major component in a large number of engineering systems such as, for example, automatic controllers for linear systems (adaptive modeling filters, adaptive inverse controllers, ... [70]), various telephony and communication devices (adaptive interference and echo cancellors, adaptive equalizers, adaptive pulse code modulators, ... <ref> [36, 55, 47] </ref>), signal detection CHAPTER 1. INTRODUCTION 3 circuits (adaptive line enhancer [56]), parameter estimation systems (adaptive spectrum analyzers, adaptive correlators, ... [64]), beamforming circuits [67, 22], and so forth.
Reference: [56] <author> J. R. Treichler. </author> <title> The spectral line enhancer the concept, an implementation, and an application. </title> <type> PhD thesis, </type> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <month> May </month> <year> 1977. </year>
Reference-contexts: INTRODUCTION 3 circuits (adaptive line enhancer <ref> [56] </ref>), parameter estimation systems (adaptive spectrum analyzers, adaptive correlators, ... [64]), beamforming circuits [67, 22], and so forth.
Reference: [57] <author> G. Ungerboeck. </author> <title> Theory on the speed of convergence in adaptive equalizers for digital communication. </title> <journal> IBM Journal on Res. and Dev., </journal> <volume> 16 </volume> <pages> 546-555, </pages> <year> 1972. </year>
Reference-contexts: The early theory of LMS developed by Widrow and Hoff considers the convergence in the mean of the weight vector. Later studies have also included convergence in the mean square <ref> [57, 40, 23] </ref>. For our purposes, the former will suffice, and we will limit ourselves to a summary of Widrow's main results, refering the reader to Widrow's and Haykin's textbooks [69, 23] for more details.
Reference: [58] <author> J. L. Walsh. </author> <title> A closed set of normal orthogonal functions. </title> <journal> American Journal of Mathematics, </journal> <volume> 45(1) </volume> <pages> 5-24, </pages> <year> 1923. </year>
Reference-contexts: Other fixed, data-independent transformations have been considered in the literature [39] to replace the DFT: the discrete cosine transform (DCT) [49], the discrete sine transform (DST) [49], the discrete Hartley transform (DHT) [8], the Walsh-Hadamard transform <ref> [58] </ref> etc. While no general general proof exists that assesses the superiority of one transform over the others 5 , experiments can be made with various classes of input data to obtain some intuition on which transform performs better on which class of inputs.
Reference: [59] <author> E. A. Wan. </author> <title> Finite impulse response neural networks for autoregressive time series prediction. </title> <editor> In A. Weigend and N. Gershenfeld, editors, </editor> <title> Predicting the Future and Understanding the Past, volume XVII. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1993. </year>
Reference-contexts: This mechanism reduces the steady-state misadjustment of the algorithm. It would be interesting and probably rather simple to implement a similar mechanism in DFT/DCT-LMS. * Finally, the DFT/DCT preprocessing could advantageously be implemented in some non-linear neural network structures such as the FIR neural networks introduced by Wan in <ref> [59] </ref>. Generalizing the backpropagation algorithm to this structure would be straightforward if we used the flow-graph approach developed by Wan and Beaufays in [60].
Reference: [60] <author> E. A. Wan and F. Beaufays. </author> <title> Diagrammatic derivation of gradient algorithms for neural networks. </title> <note> submitted to Neural Computation, </note> <year> 1994. </year>
Reference-contexts: Generalizing the backpropagation algorithm to this structure would be straightforward if we used the flow-graph approach developed by Wan and Beaufays in <ref> [60] </ref>.
Reference: [61] <author> S. S. Wang. </author> <title> LMS algorithm and discrete orthogonal transforms. </title> <journal> IEEE Trans. on Circuits and Systems, </journal> <volume> 38(8) </volume> <pages> 949-951, </pages> <month> August </month> <year> 1991. </year> <note> BIBLIOGRAPHY 126 </note>
Reference-contexts: Various extensions of the LMS spectrum analyzer have been published in the literature: a DHT analyzer was described in [35], a 2-D DFT spectrum analyzer was presented in [34], a generalization to different orthogonal transforms was described in <ref> [61] </ref>. Although the DCT was one of the transformations covered by [61], its definition was quite different from the DCT we defined earlier and used in this thesis. <p> Various extensions of the LMS spectrum analyzer have been published in the literature: a DHT analyzer was described in [35], a 2-D DFT spectrum analyzer was presented in [34], a generalization to different orthogonal transforms was described in <ref> [61] </ref>. Although the DCT was one of the transformations covered by [61], its definition was quite different from the DCT we defined earlier and used in this thesis.
Reference: [62] <author> P. J. Werbos. </author> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <address> Cambridge, MA, </address> <month> August </month> <year> 1974. </year>
Reference-contexts: These neural networks, which can be described as layered arrangements of linear adaptive units followed by nonlinear unimodal functions, typically contain a very large number of parameters that must be adapted. The most famous algorithm for adjusting these parameters is the backpropagation algorithm <ref> [62, 50, 63] </ref> which is nothing else than a generalization of LMS to this more complicated structure.
Reference: [63] <author> P .J. Werbos. </author> <title> Backpropagation: Past and future. </title> <booktitle> In Proc. of the IEEE Second Intl Conf. on Neural Networks, </booktitle> <volume> volume I, </volume> <pages> pages 333-353, </pages> <address> San Diego, CA, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: These neural networks, which can be described as layered arrangements of linear adaptive units followed by nonlinear unimodal functions, typically contain a very large number of parameters that must be adapted. The most famous algorithm for adjusting these parameters is the backpropagation algorithm <ref> [62, 50, 63] </ref> which is nothing else than a generalization of LMS to this more complicated structure.
Reference: [64] <author> B. Widrow, Ph. Baudrenghien, M. Vetterli, and P. F. Titchener. </author> <title> Fundamental relations between the LMS algorithm and the DFT. </title> <journal> IEEE Trans. on Circuits and Systems, </journal> <volume> 34(7) </volume> <pages> 814-819, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: INTRODUCTION 3 circuits (adaptive line enhancer [56]), parameter estimation systems (adaptive spectrum analyzers, adaptive correlators, ... <ref> [64] </ref>), beamforming circuits [67, 22], and so forth. As new applications were developed and as the adaptation speed required from existing systems increased, various adaptation algorithms were developed to replace LMS, some based on RLS techniques, some based on LMS itself. <p> The best performance we observe with such algorithms are obtained with DCT-LMS, for low-pass input signals. In order to maintain the computational efficiency and robustness of LMS, we propose that the orthogonalizing transforms, DFT or DCT, be implemented using the so-called LMS spectrum analyzer <ref> [64] </ref>, an adaptive structure that calculates the DFT of a time series efficiently and which is extremely robust to CHAPTER 1. INTRODUCTION 4 error propagation. The outline of the thesis is as follows. <p> The so-called LMS spectrum analyzer <ref> [64] </ref> fullfills these three requirements. The LMS spectrum analyzer is essentially a simple LMS filter whose inputs are a set of N periodic phasors. The inputs, weighted by the adaptive coefficients of the filter, are summed to form an output signal. <p> Widrow et al. <ref> [64] </ref> showed that by setting the learning rate to 1/2, by iterating over k from the initial conditions w 0 = 0, and by using the above-mentioned properties of 2 The symbols d k , w k , etc. in this chapter relate to the LMS filter used for the spectrum
Reference: [65] <author> B. Widrow and M. E. Hoff, Jr. </author> <title> Adaptive switching circuits. </title> <type> Technical Report 1553-1, </type> <institution> Stanford Electronics Laboratories, Stanford University, Stanford, </institution> <address> Ca, </address> <month> June </month> <year> 1960. </year>
Reference-contexts: The algorithms used to adjust the parameters of these adaptive filters are referred to as adaptation algorithms. One such algorithm is the least mean squares or LMS algorithm, which was invented by Widrow and Hoff in the late 1950's early 1960's <ref> [65, 66] </ref>. The principle underlying LMS is extremely simple: it consists of defining an error function as the average square difference between the filter output and its target, and in iteratively minimizing this error function over the filter coefficient space, using a simple gradient-based optimization method. <p> The conclusions drawn from their comparison will lead to the introduction of another family of adaptation algorithms, the transform-domain LMS algorithms, of which DFT-LMS and DCT-LMS are two examples. 2.1 Introduction to Adaptive Filters A discrete-time linear adaptive combiner <ref> [65, 66] </ref> of length N is shown in Fig. 2.1. At time k, a set of N signals, x k (0); x k (1); : : : ; x k (N 1) are inputted in the combiner. <p> In general, it is assumed that N is chosen large to make these effects negligeable. CHAPTER 2. ADAPTATION ALGORITHMS FOR LINEAR FILTERING 10 2.2 The LMS Algorithm The Least Mean Squares (LMS) algorithm invented by Widrow and Hoff <ref> [65, 66] </ref> is the simplest and one of the most widely used adaptation algorithms. In this section, we summarize the main features of LMS, insisting only on the properties that will influence the remaining of this thesis.
Reference: [66] <author> B. Widrow and M. E. Hoff, Jr. </author> <title> Adaptive switching circuits. </title> <booktitle> IRE WESCON Conv. Rec., </booktitle> <volume> pt. 4, </volume> <pages> pages 96-104, </pages> <year> 1960. </year>
Reference-contexts: The algorithms used to adjust the parameters of these adaptive filters are referred to as adaptation algorithms. One such algorithm is the least mean squares or LMS algorithm, which was invented by Widrow and Hoff in the late 1950's early 1960's <ref> [65, 66] </ref>. The principle underlying LMS is extremely simple: it consists of defining an error function as the average square difference between the filter output and its target, and in iteratively minimizing this error function over the filter coefficient space, using a simple gradient-based optimization method. <p> The conclusions drawn from their comparison will lead to the introduction of another family of adaptation algorithms, the transform-domain LMS algorithms, of which DFT-LMS and DCT-LMS are two examples. 2.1 Introduction to Adaptive Filters A discrete-time linear adaptive combiner <ref> [65, 66] </ref> of length N is shown in Fig. 2.1. At time k, a set of N signals, x k (0); x k (1); : : : ; x k (N 1) are inputted in the combiner. <p> In general, it is assumed that N is chosen large to make these effects negligeable. CHAPTER 2. ADAPTATION ALGORITHMS FOR LINEAR FILTERING 10 2.2 The LMS Algorithm The Least Mean Squares (LMS) algorithm invented by Widrow and Hoff <ref> [65, 66] </ref> is the simplest and one of the most widely used adaptation algorithms. In this section, we summarize the main features of LMS, insisting only on the properties that will influence the remaining of this thesis.
Reference: [67] <author> B. Widrow, P. Mantey, and B. Goode. </author> <title> Adaptive antenna systems. </title> <journal> Proc. of the IEEE, </journal> <volume> 55 </volume> <pages> 2143-2159, </pages> <month> December </month> <year> 1967. </year>
Reference-contexts: INTRODUCTION 3 circuits (adaptive line enhancer [56]), parameter estimation systems (adaptive spectrum analyzers, adaptive correlators, ... [64]), beamforming circuits <ref> [67, 22] </ref>, and so forth. As new applications were developed and as the adaptation speed required from existing systems increased, various adaptation algorithms were developed to replace LMS, some based on RLS techniques, some based on LMS itself. In this thesis, we will concentrate on the second category.
Reference: [68] <author> B. Widrow, J. M. McCool, and M. Ball. </author> <title> The complex LMS algorithm. </title> <journal> Proc. of the IEEE, </journal> <volume> 63(4) </volume> <pages> 719-720, </pages> <month> April </month> <year> 1975. </year>
Reference-contexts: In order to facilitate the later description of these transform-domain algorithms, we would like to briefly introduce two extensions of LMS: complex LMS and block-LMS, after which we will turn our attention to RLS algorithms, and then to transform-domain LMS algorithms. 2.2.3 The Complex LMS Algorithm Complex LMS <ref> [68] </ref> is the straightforward extension of real LMS to the case where the inputs, the adaptive weigths, and the desired outputs are allowed to take on complex values. <p> Applying stochastic steepest descent to ~, one finds the following weight update fomula 2 : w k+1 = w k + 2e k x fl 2 A formal derivation of the algorithm is given in <ref> [68] </ref>. An intuitive derivation can be obtained by considering a few particular cases such as real inputs with imaginary desired outputs, etc. CHAPTER 2. <p> The filter weight vector w k is adapted with the complex LMS algorithm <ref> [68] </ref>: w k+1 = w k + 2 e k f fl where is the learning rate and e k is the error signal defined as e k = d k f T k w k .
Reference: [69] <author> B. Widrow and S. D. Stearns. </author> <title> Adaptive Signal Processing. </title> <publisher> Prentice-Hall, </publisher> <address> Engle-wood Cliffs, NJ, </address> <year> 1985. </year>
Reference-contexts: In this section, we summarize the main features of LMS, insisting only on the properties that will influence the remaining of this thesis. For more details, we refer the reader to Widrow's textbook <ref> [69] </ref> and to the original publications mentioned above. 2.2.1 Derivation of the LMS Algorithm The LMS algorithm minimizes the error function ~ using a stochastic steepest descent approach, that is, at each iteration, the weights are updated proportionaly to an estimate of the error gradient. <p> Later studies have also included convergence in the mean square [57, 40, 23]. For our purposes, the former will suffice, and we will limit ourselves to a summary of Widrow's main results, refering the reader to Widrow's and Haykin's textbooks <ref> [69, 23] </ref> for more details. Widrow based his analysis on the exact steepest descent algorithm: w k+1 = w k r k : (2.18) The exact error gradient at time k can be expressed as r k = dw k = 2Rw k 2p (2.20) CHAPTER 2. <p> The steady-state solution found by LMS is thus noisy. Its precision is characterized by a quantity called misadjustment, which is equal to the variance of the steady-state solution normalized by the minimum achievable error ~ min . It can be shown <ref> [69] </ref> that the misadjustment is in first approximation given by Misadjustment = Trace (R): (2.30) This formula shows that improving the precision of the steady-state solution can easily be achieved by decreasing the learning rate . However, this has the inconvenient of slowing down the adaptation process. <p> Two weights, w opt (0) = 2cot (2=6) and w opt (1) = 2csc (2=N), suffice to map x k into d k with zero error (see <ref> [69] </ref>, chapter 2). Adding a third weight to the filter is essentially useless, but let us do it nevertheless to see what happens. The error function ~ (w (0); w (1); w (2)) is a 4-D surface. <p> These truncation effects make the adaptation of an appropriate controller difficult. Filtered-X and filtered* LMS are two adaptation algorithms proposed by the authors to alleviate this problem. Filtered-X LMS is described in details in <ref> [69] </ref>, chapter 11; filtered* LMS is presented in [70], chapter 7. <p> This technique can be intuitively justified by the fact that both the LMS and the DFT are least squares estimators: the adaptive weights in the LMS algorithm are CHAPTER 6. IMPLEMENTATION OF THE SLIDING-DFT 70 iteratively updated to minimize a quadratic cost function <ref> [69] </ref>, and a set of Fourier coefficients may be seen as the best mean squares fitting of an input sequence by a series of complex phasors [51]. However, its adaptive nature gives the LMS spectrum analyzer the ability to automatically adjust for possible errors in the DFT outputs. <p> 76 errors over one period, i.e. over N iterations: ~ k = m=0 km e km : (6.20) Accordingly, the input autocorrelation matrix is defined as R = m=0 m f T and the cross-correlation between inputs and desired outputs is p k = m=0 km = (6.22) In Newton-LMS <ref> [69] </ref>, the weight vector at time k is updated as w k+1 = w k R 1 r k ; (6.23) where the error gradient r k is equal to r k = 2 (Rw k p): (6.24) One-step convergence to the optimum solution is obtained if and only if three <p> C.1 would translate into a generalization of leaky-LMS: w k+1 = A k w k + 2e k x k , where A k would be a full matrix instead of a constant scalar as in leaky-LMS <ref> [69] </ref>. Appendix D Floating Point Representation of Numbers This appendix presents a brief overview of floating point representation of real numbers. The IEEE standard for single precision representation is specified, and an example showing the conversion of a real number into this standard is given.
Reference: [70] <author> B. Widrow and E. Walach. </author> <title> Adaptive Inverse Control. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ in prints, </address> <year> 1995. </year>
Reference-contexts: For many decades, it has been a major component in a large number of engineering systems such as, for example, automatic controllers for linear systems (adaptive modeling filters, adaptive inverse controllers, ... <ref> [70] </ref>), various telephony and communication devices (adaptive interference and echo cancellors, adaptive equalizers, adaptive pulse code modulators, ... [36, 55, 47]), signal detection CHAPTER 1. INTRODUCTION 3 circuits (adaptive line enhancer [56]), parameter estimation systems (adaptive spectrum analyzers, adaptive correlators, ... [64]), beamforming circuits [67, 22], and so forth. <p> The simulations performed with Markov-1 and Markov-2 inputs showed the very good performance of DCT-LMS for these input signals. DCT-LMS was also used by Widrow and Walach in simulations they ran to illustrate some novel techniques in inverse dynamic control <ref> [70] </ref>. In these simulations, DCT-LMS was used as a substitute CHAPTER 5. SIMULATIONS 63 for LMS which, in some of the book set-ups, was too slow to converge within a reasonable amount of time. The applications described in the book involved the so-called filtered-X and filtered * LMS algorithms. <p> These truncation effects make the adaptation of an appropriate controller difficult. Filtered-X and filtered* LMS are two adaptation algorithms proposed by the authors to alleviate this problem. Filtered-X LMS is described in details in [69], chapter 11; filtered* LMS is presented in <ref> [70] </ref>, chapter 7. Both algorithms resemble LMS, besides that the input of one (filtered-X LMS) and the error signal of the other (filtered-*) are filtered by a linear FIR system before being used to adjust the adaptive weights of the controller, using LMS. The simulations reported in [70] showed very large <p> is presented in <ref> [70] </ref>, chapter 7. Both algorithms resemble LMS, besides that the input of one (filtered-X LMS) and the error signal of the other (filtered-*) are filtered by a linear FIR system before being used to adjust the adaptive weights of the controller, using LMS. The simulations reported in [70] showed very large reductions in eigenvalue spreads when DCT-LMS was used instead of LMS. Various experiments were made with CHAPTER 5. <p> Also, the algorithm has proved to be useful in other contexts such as, for example, the inverse control of a dynamic system <ref> [70] </ref>. 8.1 Further Work To conclude, we would like to list a few issues that are related to the work described in this thesis and that we believe would be interesting to further investigate. * Generalize the eigenvalue spread results of chapter 4 to Markov inputs of higher orders. * Determine
References-found: 70

