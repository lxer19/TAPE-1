URL: ftp://ftp.huji.ac.il/users/clag/ijcai95/claudia.ps.gz
Refering-URL: http://www.cs.huji.ac.il/labs/dai/papers.html
Root-URL: 
Email: email: clag@cs.huji.ac.il, jeff@cs.huji.ac.il  
Title: Mutually Supervised Learning in Multiagent Systems sometimes avoid repeatedly coordinating their actions from scratch for
Author: Claudia V. Goldman and Jeffrey S. Rosenschein ph: ---- 
Note: Agents that learn from each other can  
Address: Givat Ram, Jerusalem, Israel  
Affiliation: Computer Science Department Hebrew University  
Abstract: In this paper, we propose several learning rules for agents in a multiagent environment. Each agent acts as the teacher of its partner. The agents are trained by receiving examples from a sample space; they then go through a generalization step during which they have to apply the concept they have learned from their instructor. 
Abstract-found: 1
Intro-found: 1
Reference: [ Anthony and Biggs, 1992 ] <author> Martin Anthony and Nor-man Biggs. </author> <title> Computational Learning Theory. </title> <publisher> Cam-bridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: Given the following concepts and notations <ref> [ Anthony and Biggs, 1992 ] </ref> , we can define the PAC learning algorithm. * An example space X fl , where is the alphabet for describing examples. * A concept space C, c 2 C denotes a learned concept, i.e., c : X ! f0; 1g * A sample of <p> target concept t, where the examples are taken from an example space X (i.e., if x = (x 1 ; : : : ; x m ) then s = ((x 1 ; t (x 1 )); : : : ; (x m ; t (x m ))) Then, following <ref> [ Valiant, 1984; Anthony and Biggs, 1992 ] </ref> , we can state the following definition.
Reference: [ Durfee, 1988 ] <author> Edmund H. Durfee. </author> <title> Coordination of Distributed Problem Solvers. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1988. </year>
Reference: [ Findler, 1991 ] <author> N.V. Findler. </author> <title> Distributed control of collaborating and learning expert systems for street traffic signals. </title> <editor> In Lewis and Stephanon, editors, </editor> <booktitle> IFAC Distributed Intelligence Systems, </booktitle> <pages> pages 125| 130. </pages> <publisher> Pergamon Press, </publisher> <year> 1991. </year>
Reference-contexts: In some experiments an agent played against the fixed TIT-FOR-TAT strategy, while in others it played against another Q-learning agent. The Q-learning algorithm has also been applied to packet routing [ Littman and Boyan, 1993 ] . Each node in the network is a Q-learner. Findler <ref> [ Findler, 1991 ] </ref> has investigated how expert systems can control street traffic signals. In his domain there is only one processor for each intersection. This processor, the expert system, communicates with its four adjacent neighbors.
Reference: [ Goldman and Rosenschein, 1994 ] <author> C. Goldman and J. Rosenschein. </author> <title> Emergent coordination through the use of cooperative state-changing rules. </title> <booktitle> In AAAI94, </booktitle> <pages> pages 408-413, </pages> <year> 1994. </year>
Reference-contexts: Multiagent reactive systems have also been analyzed within DAI, where solutions are arrived at dynamically by reactive agents in multiagent environments. Social laws [ Tennenholtz and Moses, 1989; Shoham and Tennenholtz, 1992 ] and cooperative state-changing rules <ref> [ Goldman and Rosenschein, 1994 ] </ref> have been studied; these conventions give the agents a framework within which to act, to more harmoniously interact with the other agents participating in the same world.
Reference: [ Grosz and Kraus, 1993 ] <author> B. Grosz and S. Kraus. </author> <title> Collaborative plans for group activities. </title> <booktitle> In IJCAI93, </booktitle> <pages> pages 367-373, </pages> <address> Chambery, France, </address> <year> 1993. </year> [ <editor> Kearns and Shapire, 1990 ] Michael J. Kearns and Robert E. Shapire. </editor> <title> Efficient distribution-free learning of probabilistic concepts. </title> <booktitle> In Proceedings of the 31st Anual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 382-391, </pages> <year> 1990. </year>
Reference-contexts: Other DAI researchers have focused more on direct modeling of agents' beliefs and desires, as another way for an agent to decide what action to perform when dealing with others <ref> [ Grosz and Kraus, 1993 ] </ref> . Again, learning rarely enters into this research; while the exploitation of a model of the opponent is studied, the actual derivation of the model rarely is.
Reference: [ Kraus and Wilkenfeld, 1991 ] <author> S. Kraus and J. Wilken-feld. </author> <title> Negotiations over time in a multi agent environment: Preliminary report. </title> <booktitle> In IJCAI91, </booktitle> <pages> pages 56-61, </pages> <address> Sydney, Australia, </address> <month> August </month> <year> 1991. </year>
Reference: [ Littman and Boyan, 1993 ] <author> M. Littman and J. Boyan. </author> <title> A distributed reinforcement learning scheme for network routing. </title> <type> technical report CMU-CS-93-165, </type> <institution> Carnegie Mellon University, </institution> <year> 1993. </year>
Reference-contexts: In some experiments an agent played against the fixed TIT-FOR-TAT strategy, while in others it played against another Q-learning agent. The Q-learning algorithm has also been applied to packet routing <ref> [ Littman and Boyan, 1993 ] </ref> . Each node in the network is a Q-learner. Findler [ Findler, 1991 ] has investigated how expert systems can control street traffic signals. In his domain there is only one processor for each intersection.
Reference: [ Littman, 1994 ] <author> M. L. Littman. </author> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Machine Learning 1994, </booktitle> <pages> pages 157|163, </pages> <year> 1994. </year>
Reference-contexts: Littman proposed the use of Markov games as a framework for multiagent systems <ref> [ Littman, 1994 ] </ref> .
Reference: [ Sandholm and Crites, submitted ] <author> Tuomas W. Sand-holm and Robert H. Crites. </author> <title> Multiagent reinforcement learning in the iterated prisoner's dilemma. </title> <journal> Biosystems Journal Special Issue on the Prisoner's Dilemma, </journal> <note> submitted. </note>
Reference-contexts: Experiments using the Q-learning algorithm were also carried out in [ Sen et al., 1994 ] and in <ref> [ Sandholm and Crites, submitted ] </ref> . In [ Sen et al., 1994 ] , two agents have to push a block to a goal position. Agents learned complementary policies without sharing any information with the other agent. <p> Agents learned complementary policies without sharing any information with the other agent. The agents are actually not even aware of the existence of the other; they learn how to push the block by receiving reinforcement from the environment regarding the distance of the block from the optimal path. <ref> [ Sandholm and Crites, submitted ] </ref> covered an experimental study with the iterated prisoner's dilemma, where the players were Q-learning agents. In some experiments an agent played against the fixed TIT-FOR-TAT strategy, while in others it played against another Q-learning agent.
Reference: [ Sen et al., 1994 ] <author> S. Sen, M. Sekaran, and J. Hale. </author> <title> Learning to coordinate without sharing information. </title> <booktitle> In AAAI94, </booktitle> <pages> pages 426-431, </pages> <note> 1994. </note> [ <author> Shoham and Tennenholtz, 1992 ] Y. Shoham and M. Tennenholtz. </author> <title> Emergent conventions in multi-agent systems: initial experimental results and observations (preliminary report). </title> <booktitle> In KR92, </booktitle> <address> Cambridge, Massachusetts, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Experiments using the Q-learning algorithm were also carried out in <ref> [ Sen et al., 1994 ] </ref> and in [ Sandholm and Crites, submitted ] . In [ Sen et al., 1994 ] , two agents have to push a block to a goal position. Agents learned complementary policies without sharing any information with the other agent. <p> Experiments using the Q-learning algorithm were also carried out in <ref> [ Sen et al., 1994 ] </ref> and in [ Sandholm and Crites, submitted ] . In [ Sen et al., 1994 ] , two agents have to push a block to a goal position. Agents learned complementary policies without sharing any information with the other agent.
Reference: [ Shoham and Tennenholtz, 1994 ] <author> Y. Shoham and M. Tennenholtz. </author> <title> Co-learning and the evolution of social activity. </title> <type> Technical Report STAN-CS-TR-94-1511, </type> <institution> Stanford Univ., </institution> <year> 1994. </year>
Reference-contexts: Learning has been investigated within this framework, particularly in <ref> [ Shoham and Tennenholtz, 1994 ] </ref> , which investigated how conventions can evolve when the Highest Cumulative Reward update rule is used (i.e., agents choose to perform the action that has yielded the highest payoff until then). The advantages of having agents learn within a mul-tiagent environment are clear.
Reference: [ Sian, 1991 ] <author> S. Sian. </author> <title> Adaptation based cooperative learning multi-agent systems. </title> <booktitle> In Decentralized AI 2, </booktitle> <pages> pages 257|272, </pages> <year> 1991. </year>
Reference-contexts: Finally, in Section 5, we conclude and discuss several additional points for further investigation. 2 Background on Learning 2.1 Multiagent Learning An example of a multiagent system in which every agent has a learning module is <ref> [ Sian, 1991 ] </ref> . The agents' evaluate various hypotheses while having only partial knowledge about them. The agents can know more about a specific hypothesis by receiving messages from other agents expressing their level of confidence in that hypothesis.
Reference: [ Smith, 1978 ] <author> Reid G. Smith. </author> <title> A Framework for Problem Solving in a Distributed Processing Environment. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1978. </year>
Reference: [ Tan, 1993 ] <author> M. Tan. </author> <title> Multi-agent reinforcement learning: Independent vs. </title> <booktitle> cooperative agents. In Machine Learning: Proceedings of the Tenth international conference, </booktitle> <pages> pages 330|337, </pages> <year> 1993. </year>
Reference-contexts: Reinforcement learning techniques have also been applied in multiagent scenarios. Agents that learn according to the Q-learning algorithm [ Watkins and Dayan, 1992 ] and also cooperate with other agents by exchanging information (partial solutions, plans of action) can learn more quickly than agents that do not cooperate <ref> [ Tan, 1993 ] </ref> . Littman proposed the use of Markov games as a framework for multiagent systems [ Littman, 1994 ] .
Reference: [ Tennenholtz and Moses, 1989 ] <author> M. Tennenholtz and Y. Moses. </author> <title> On cooperation in a multi-entity model. </title> <booktitle> In IJCAI89, </booktitle> <pages> pages 918-923, </pages> <address> Detroit, Michigan, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: Again, learning rarely enters into this research; while the exploitation of a model of the opponent is studied, the actual derivation of the model rarely is. Multiagent reactive systems have also been analyzed within DAI, where solutions are arrived at dynamically by reactive agents in multiagent environments. Social laws <ref> [ Tennenholtz and Moses, 1989; Shoham and Tennenholtz, 1992 ] </ref> and cooperative state-changing rules [ Goldman and Rosenschein, 1994 ] have been studied; these conventions give the agents a framework within which to act, to more harmoniously interact with the other agents participating in the same world.
Reference: [ Valiant, 1984 ] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11):1134| 1142, </volume> <year> 1984. </year>
Reference-contexts: target concept t, where the examples are taken from an example space X (i.e., if x = (x 1 ; : : : ; x m ) then s = ((x 1 ; t (x 1 )); : : : ; (x m ; t (x m ))) Then, following <ref> [ Valiant, 1984; Anthony and Biggs, 1992 ] </ref> , we can state the following definition.
Reference: [ Watkins and Dayan, 1992 ] <author> Christopher Watkins and Peter Dayan. </author> <note> Technical note Q-learning. Machine Learning, 8:279|292, </note> <year> 1992. </year>
Reference-contexts: The two algorithms differ primarily in the competition step; in the ACG algorithm, agents compete over a group of actions. Reinforcement learning techniques have also been applied in multiagent scenarios. Agents that learn according to the Q-learning algorithm <ref> [ Watkins and Dayan, 1992 ] </ref> and also cooperate with other agents by exchanging information (partial solutions, plans of action) can learn more quickly than agents that do not cooperate [ Tan, 1993 ] .
Reference: [ Wei, 1993 ] <author> Gerhard Wei. </author> <title> Learning to coordinate actions in multi-agent systems. </title> <booktitle> In IJCAI93, </booktitle> <address> Chambery, France, </address> <year> 1993. </year>
Reference-contexts: The general aim of the system is for agents to settle on the best hypothesis they can, given the knowledge they have. The ACE and the ACG learning algorithms were proposed for a multiagent environment in <ref> [ Wei, 1993 ] </ref> . In this case, the agents can learn how to coordinate their actions by iteratively sending to one another bids for every action they can perform. These bids reflect the importance of the specific action relative to the agent's goal.
Reference: [ Zlotkin and Rosenschein, 1993 ] <author> G. Zlotkin and J. S. Rosenschein. </author> <title> A domain theory for task oriented negotiation. </title> <booktitle> In IJCAI93, </booktitle> <pages> pages 416-422, </pages> <address> Chambery, France, </address> <year> 1993. </year>
References-found: 19

