URL: http://www.tns.lcs.mit.edu/~djw/library/scout.TR96-03.ps.gz
Refering-URL: http://www.tns.lcs.mit.edu/~djw/library/
Root-URL: 
Title: Analysis of Techniques to Improve Protocol Processing Latency  
Author: David Mosberger, Larry L. Peterson, Patrick G. Bridges, and Sean O'Malley 
Address: Tucson, AZ 85721  
Affiliation: Department of Computer Science The University of Arizona  
Date: TR 96-03  
Abstract: This paper describes several techniques designed to improve protocol latency, and reports on their effectiveness when measured on a modern RISC machine employing the DEC Alpha processor. We found that the memory systemwhich has long been known to dominate network throughputis also a key factor in protocol latency. In particular, improving instruction cache effectiveness can greatly reduce protocol processing overheads. An important metric in this context is the memory cycles per instructions (mCPI), which is the average number of cycles that an instruction stalls waiting for a memory access to complete. The techniques presented in this paper reduce the mCPI by up to a factor of 5.8. In analyzing the effectiveness of the techniques, we also present a detailed study of the protocol processing behavior of two protocol stacksTCP/IP and RPCon a modern RISC processor. 
Abstract-found: 1
Intro-found: 1
Reference: [AMD] <author> AMD. Am7990: </author> <title> Local Area Network Controller for Ethernet. </title>
Reference-contexts: In BSD-derived implementations, VNET's functionality is part of IP. ETH is the device-independent half of the Ethernet driver, while LANCE is the device driver for the LANCE network adaptor that is present in the DEC 3000/600 workstations. The network adaptor connects to the CPU via the TURBOchannel bus <ref> [AMD, DEC93] </ref>. The RPC stack implements a remote procedure call facility similar to Sprite RPC [OP92]. This test stack is a model for the x-kernel paradigm that encourages decomposing networking functionality into many small protocols. Consequently, as shown in Figure 1, the RPC stack is deeper than the TCP/IP stack. <p> While end-to-end latency improvements are certainly respectable, they are nevertheless fractional on the given test system. It is important to keep in mind, however, that modern high-performance network adaptors have much lower latency than the LANCE Ethernet adaptor present in the DEC 3000 system <ref> [AMD] </ref>. To put this into perspective, consider that a minimum-sized Ethernet packet is 64 bytes long, to which an 8 byte long preamble is added. At the speed of Ethernet (1010 6 bps), transmitting the frame takes 57:6s.
Reference: [BGP + 94] <author> Mary L. Bailey, Burra Gopal, Michael A. Pagels, Larry L. Peterson, and Prasenjit Sarkar. PathFinder: </author> <title> A pattern-based packet classifier. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 115-123, </pages> <year> 1994. </year>
Reference-contexts: The last part requires employing a packet classifier <ref> [BGP + 94, MJ93, YBMM93, EKJ95] </ref>. While path-inlining is easy in principle once indirect function calls have been taken care of, the practical problem is quite difficult. None of the common C compilers are able to inline code across module boundaries (object files). <p> It is important to note that path-inlining requires running a packet classifier on incoming packets since the optimized code is no longer general enough to handle all possible packets. Currently, the best packet classifiers add an overhead of about 1 4s per packet on the tested hardware platform <ref> [BGP + 94, EKJ95] </ref>. However, to separate packet classification performance from the techniques under study here, no packet classifier was used. In this sense, the PIN and ALL measurements should be interpreted as the performance obtained with a zero-overhead packet classifier.
Reference: [CJRS89] <author> David D. Clark, Van Jacobson, John Romkey, and Howard Salwen. </author> <title> An analysis of TCP processing over heads. </title> <journal> IEEE Communications Magazine, </journal> <volume> 27(6) </volume> <pages> 23-29, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Communication latency is often just as important as throughput in distributed systems, and for this reason, researchers have analyzed the latency characteristics of common network protocols, such as TCP/IP <ref> [KP93, CJRS89, Jac93] </ref> and RPC [TL93]. This paper revisits the issue of protocol latency. Our goal is not to optimize a particular protocol stack, but rather, to understand the fundamental limitations on processing overhead. <p> These numbers are based on instruction traces that we collected. 2 This comparison also provides an interesting update of the CISC numbers presented in <ref> [CJRS89] </ref>. Specifically, we traced and measured the time it takes to perform IP and TCP processing for an incoming one byte TCP segment on a connection with bi-directional data flow. Notice that [CJRS89] focused on the case where data flows only in one direction (as is the case for an ftp <p> traces that we collected. 2 This comparison also provides an interesting update of the CISC numbers presented in <ref> [CJRS89] </ref>. Specifically, we traced and measured the time it takes to perform IP and TCP processing for an incoming one byte TCP segment on a connection with bi-directional data flow. Notice that [CJRS89] focused on the case where data flows only in one direction (as is the case for an ftp transfer, for example). <p> First, with a bi-directional connection, both hosts perform sender and receiver-related house-keeping when receiving a packet. In contrast, with a unidirectional connection a host performs either sender or receiver-related house-keeping but not both. Second, the DEC Unix TCP implementation uses header-prediction <ref> [CJRS89] </ref>. This is an optimization targeted at improving latency. However, all the header-prediction implementations we know of work only for uni-directional connections. The result is that rather than improving latency, header prediction slightly worsens latency on a connection with a bidirectional data flow. <p> The result is that rather than improving latency, header prediction slightly worsens latency on a connection with a bidirectional data flow. Fortunately, with less than a dozen additional instructions executed, the slow down is not very large. Architecture: 80386 Alpha TCP/IP implementation: <ref> [CJRS89] </ref>: DEC Unix v3.2c: Improved x-kernel: Number of instruction executed: : : : : : in ipintr: 57 248 : : : in tcp input: 276 406 : : : between IP input and TCP input: 262 437 : : : between TCP input and socket input: 1188 1004 Table 3: <p> number for the x-kernel. 2 The traces are available via anonymous ftp from ftp://cage.cs.arizona.edu/pub/davidm/tcpip. 7 The cost of 276 instructions for TCP input processing on the 80386 was arrived at by adding both the sender side and the receiver side costs, as well as the common path cost reported in <ref> [CJRS89] </ref> (154 instructions for the common path, 15+17 additional instructions for the receive side processing, and 9+20+17+44 additional instructions for the sender side processing). First, we notice that IP processing in DEC Unix appears to be more than a factor of four longer than for the 80386.
Reference: [DBRD91] <author> Richard P. Draves, Brian N. Bershad, Richard F. Rashid, and Randall W. Dean. </author> <title> Using continuations to implement thread management and communication in operating systems. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating System Principles, </booktitle> <pages> pages 122-36. </pages> <institution> Association for Computing Machinery SIGOPS, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: The memory object that is used most often during execution is the program stack. To optimize d-cache behavior, we modified the tread manager to make extensive use of continuations <ref> [DBRD91] </ref>. We also converted stacks to first-class objects. Instead of being statically attached to a given thread, stacks are now dynamically attached to a thread upon demand. Together with continuations, this has the effect that latency sensitive path invocations will normally execute on the same stack.
Reference: [DEC93] <author> DEC. TURBOchannel: </author> <title> Hardware Specification. </title> <institution> Digital Equipment Corp., </institution> <year> 1993. </year> <title> Order number EK 369AA-OD-007B. </title>
Reference-contexts: In BSD-derived implementations, VNET's functionality is part of IP. ETH is the device-independent half of the Ethernet driver, while LANCE is the device driver for the LANCE network adaptor that is present in the DEC 3000/600 workstations. The network adaptor connects to the CPU via the TURBOchannel bus <ref> [AMD, DEC93] </ref>. The RPC stack implements a remote procedure call facility similar to Sprite RPC [OP92]. This test stack is a model for the x-kernel paradigm that encourages decomposing networking functionality into many small protocols. Consequently, as shown in Figure 1, the RPC stack is deeper than the TCP/IP stack.
Reference: [EKJ95] <author> Dawson R. Engler, Frans Kaashoek, and James O'Toole Jr. Exokernel: </author> <title> An operating system architecture for application-level resource management. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating System Principles, </booktitle> <pages> pages 251-266, </pages> <year> 1995. </year>
Reference-contexts: The last part requires employing a packet classifier <ref> [BGP + 94, MJ93, YBMM93, EKJ95] </ref>. While path-inlining is easy in principle once indirect function calls have been taken care of, the practical problem is quite difficult. None of the common C compilers are able to inline code across module boundaries (object files). <p> It is important to note that path-inlining requires running a packet classifier on incoming packets since the optimized code is no longer general enough to handle all possible packets. Currently, the best packet classifiers add an overhead of about 1 4s per packet on the tested hardware platform <ref> [BGP + 94, EKJ95] </ref>. However, to separate packet classification performance from the techniques under study here, no packet classifier was used. In this sense, the PIN and ALL measurements should be interpreted as the performance obtained with a zero-overhead packet classifier.
Reference: [GC90] <author> Rajiv Gupta and Chi-Hung Chi. </author> <title> Improving instruction cache behavior by reducing cache pollution. </title> <booktitle> In Proceedings Supercomputing '90, </booktitle> <pages> pages 82-91. </pages> <publisher> IEEE, </publisher> <year> 1990. </year>
Reference-contexts: Second, the gaps introduced by the micro-positioning approach do cost extra i-cache bandwidth. We have not found a single instance where aligning function entry-points or similar gap-introducing techniques would have improved end-to-end latency. This is in stark contrast with the findings published in <ref> [GC90] </ref>, where i-cache optimization focused on functions with a very high degree of locality. So it may be that micro-positioning suffers because of the i-cache bandwidth wasted on loading gaps. Third, the DEC 3000/600 workstations used in the experiments employ a large second-level cache.
Reference: [Hei94] <author> R. R. Heisch. </author> <title> Trace-directed program restructuring for AIX executables. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 38(9) </volume> <pages> 595-603, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: For example, error handling code could be moved to the end of the function or to the end of the program. Outlining traditionally has been associated with profile-based optimizers <ref> [Hei94, PH90] </ref>. Unfortunately, profile-based optimizers suffer from the problem of being aggressive rather than conservative: any code that is not covered by the collected profile will be outlined.
Reference: [HP91] <author> Norman C. Hutchinson and Larry L. Peterson. </author> <title> The x-kernel: An architecture for implementing network protocols. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1) </volume> <pages> 64-76, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Note that these techniques are applied to existing protocol implementations written in C; we are not proposing a new programming language. Finally, Section 5 offers some concluding remarks. 1 2 Starting Point The work reported in this paper is done in the context of the x-kernel <ref> [HP91] </ref>, a framework that allows flexible experimentation with networking protocols. This section briefly describes the x-kernel and the protocols that we evaluate. <p> The CPU is a 64-bit wide, super-scalar design that can issue up to two instructions per cycle. The memory system interface is 128 bits wide. To achieve maximum control over the experiments, the software was implemented in a minimal stand-alone version of the x-kernel <ref> [HP91] </ref>. The entire test runs in kernel mode (no protection domain crossings) and without virtual memory. The kernel is so small that it fits entirely into the b-cache, and unless forced (as in some of the tests), there are no b-cache conflicts.
Reference: [Jac93] <author> Van Jacobson. </author> <title> A high performance TCP/IP implementation. Presentation at the NRI Gigabit TCP Work shop, </title> <month> March 18th-19th </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Communication latency is often just as important as throughput in distributed systems, and for this reason, researchers have analyzed the latency characteristics of common network protocols, such as TCP/IP <ref> [KP93, CJRS89, Jac93] </ref> and RPC [TL93]. This paper revisits the issue of protocol latency. Our goal is not to optimize a particular protocol stack, but rather, to understand the fundamental limitations on processing overhead.
Reference: [KP93] <author> Jonathan Kay and Joseph Pasquale. </author> <title> The importance of non-data touching processing overheads in TCP/IP. </title> <booktitle> In Proceedings of SIGCOMM '93 Symposium, </booktitle> <volume> volume 23, </volume> <pages> pages 259-268, </pages> <address> San Fransico, Cali-fornia, </address> <month> October </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: 1 Introduction Communication latency is often just as important as throughput in distributed systems, and for this reason, researchers have analyzed the latency characteristics of common network protocols, such as TCP/IP <ref> [KP93, CJRS89, Jac93] </ref> and RPC [TL93]. This paper revisits the issue of protocol latency. Our goal is not to optimize a particular protocol stack, but rather, to understand the fundamental limitations on processing overhead.
Reference: [Mas92] <author> Henry Massalin. </author> <title> Synthesis: An Efficient Implementation of Fundamental Operating System Services. </title> <type> PhD thesis, </type> <institution> Columbia University, </institution> <address> New York, NY 10027, </address> <month> September </month> <year> 1992. </year>
Reference-contexts: For example, if cloning is delayed until a TCP/IP connection is established, most connection state will remain constant and can be used to partially evaluate the cloned function. This achieves similar benefits as code synthesis <ref> [Mas92] </ref>. Just as for inlining, cloning is at odds with locality of reference. Cloning at connection creation time will lead to one cloned copy per connection, while cloning at protocol stack creation time will require only one copy per protocol stack.
Reference: [McF89] <author> Scott McFarling. </author> <title> Program optimization for instruction caches. </title> <booktitle> In Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 183-191. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1989. </year>
Reference-contexts: We experimented extensively with different layout strategies for cloned code. We thought that, ideally, it should be possible to avoid all i-cache conflicts along a critical path of execution. With a direct-mapped i-cache, the starting address of a function determines exactly which i-cache blocks it is going to occupy <ref> [McF89] </ref>. Consequently, by choosing appropriate addresses, it is possible to optimize i-cache behavior for a given path. The cost is that occasionally it is necessary to introduce gaps between two consecutive functions (sometimes it is possible to fill a gap with another function of the appropriate length).
Reference: [MJ93] <author> Steven McCanne and Van Jacobson. </author> <title> The BSD packet filter: A new architecture for user-level packet capture. </title> <booktitle> In 1993 Winter USENIX Conference, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1993. </year> <booktitle> USENIX. </booktitle>
Reference-contexts: The last part requires employing a packet classifier <ref> [BGP + 94, MJ93, YBMM93, EKJ95] </ref>. While path-inlining is easy in principle once indirect function calls have been taken care of, the practical problem is quite difficult. None of the common C compilers are able to inline code across module boundaries (object files).
Reference: [Mog92] <author> Jeffrey C. Mogul. </author> <title> Network locality at the scale of processes. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(2) </volume> <pages> 81-109, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: For example, the x-kernel hash-table manager supports a one-entry cache to exploit locality of network messages <ref> [Mog92] </ref>. Ideally, a hash-table lookup that results in a cache-hit should require only the few instructions needed to compare the key being looked up with the key of the cached entry. However, the hash-table manager supports a general interface that allows for unaligned keys and various key-sizes.
Reference: [OP92] <author> Sean W. O'Malley and Larry L. Peterson. </author> <title> A dynamic network architecture. </title> <journal> ACM Transactions on Com puter Systems, </journal> <volume> 10(2) </volume> <pages> 110-143, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Below are TCP and IP which are the x-kernel versions of the corresponding Internet protocols [Pos81b, Pos81a]. The x-kernel implementation of TCP is based on BSD source code so, except for interface changes, they are identical. VNET is a virtual protocol <ref> [OP92] </ref> that routes outgoing messages to the appropriate network adaptor. In BSD-derived implementations, VNET's functionality is part of IP. ETH is the device-independent half of the Ethernet driver, while LANCE is the device driver for the LANCE network adaptor that is present in the DEC 3000/600 workstations. <p> The network adaptor connects to the CPU via the TURBOchannel bus [AMD, DEC93]. The RPC stack implements a remote procedure call facility similar to Sprite RPC <ref> [OP92] </ref>. This test stack is a model for the x-kernel paradigm that encourages decomposing networking functionality into many small protocols. Consequently, as shown in Figure 1, the RPC stack is deeper than the TCP/IP stack. <p> The test program sends zero-sized RPC requests to the RPC server which responds with a zero-sized reply. MSELECT, VCHAN, CHAN, BID and BLAST together provide RPC semantics. A detailed description of these protocols can be found in <ref> [OP92] </ref>. For the purpose of this discussion, it is sufficient to know that the client side of XRPCTEST performs a call into MSELECT to send a 2 request to the server.
Reference: [OPM94] <author> S. W. O'Malley, T. Proebsting, and A. B. Montz. </author> <title> USC: A universal stub compiler. </title> <booktitle> In Proceedings of SIGCOMM '94 Symposium, </booktitle> <pages> pages 295-306, </pages> <address> London, UK, </address> <note> August 31st - September 2nd 1994. </note>
Reference-contexts: Fortunately, the Universal Stub Compiler (USC) is ideally suited for this taskthe layout of the descriptor can be described independently of the structure of the sparse memory space <ref> [OPM94] </ref>. USC can generate inlined functions that allow direct sparse memory access to any field in the descriptor.
Reference: [PH90] <author> K. Pettis and R. C. Hansen. </author> <title> Profile guided code positioning. </title> <booktitle> In Proceedings of SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <volume> volume 25, </volume> <pages> pages 16-27, </pages> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: For example, error handling code could be moved to the end of the function or to the end of the program. Outlining traditionally has been associated with profile-based optimizers <ref> [Hei94, PH90] </ref>. Unfortunately, profile-based optimizers suffer from the problem of being aggressive rather than conservative: any code that is not covered by the collected profile will be outlined. <p> This layout strategy is so simple that it can be computed easily at runtimethe only dynamic information required is the order in which the functions are invoked. In essence, computing a bipartite layout consists of applying the well-known closest-is-best strategy to the library and path partition individually <ref> [PH90] </ref>. Establishing the performance advantage of this layout scheme relative to the micro-positioning approach is difficult since small changes to the heuristics of the latter approach results in large performance changes. The micro-positioning approach usually performs somewhat worse than a bipartite layout and sometimes almost equally well, but never better.
Reference: [Pos81a] <author> J. Postel. RFC-791: </author> <title> Internet Protocol. </title> <note> Available via ftp from ftp.nisc.sri.com, </note> <month> September </month> <year> 1981. </year>
Reference-contexts: The left part of Figure 1 shows the protocol-stack that was used to test TCP/IP. At the top, TCPTEST is a simple, ping-pong-style test program. Below are TCP and IP which are the x-kernel versions of the corresponding Internet protocols <ref> [Pos81b, Pos81a] </ref>. The x-kernel implementation of TCP is based on BSD source code so, except for interface changes, they are identical. VNET is a virtual protocol [OP92] that routes outgoing messages to the appropriate network adaptor. In BSD-derived implementations, VNET's functionality is part of IP.
Reference: [Pos81b] <author> J. Postel. RFC-793: </author> <title> Transmission Control Protocol. </title> <note> Available via ftp from ftp.nisc.sri.com, </note> <month> September </month> <year> 1981. </year>
Reference-contexts: The left part of Figure 1 shows the protocol-stack that was used to test TCP/IP. At the top, TCPTEST is a simple, ping-pong-style test program. Below are TCP and IP which are the x-kernel versions of the corresponding Internet protocols <ref> [Pos81b, Pos81a] </ref>. The x-kernel implementation of TCP is based on BSD source code so, except for interface changes, they are identical. VNET is a virtual protocol [OP92] that routes outgoing messages to the appropriate network adaptor. In BSD-derived implementations, VNET's functionality is part of IP.
Reference: [Sit92] <author> Richard L. </author> <title> Sites, editor. Alpha Architecture Reference Manual. </title> <publisher> Digital Press, </publisher> <address> Burlington, Massachusetts, </address> <year> 1992. </year> <title> Order number EY-L520E-DP. </title>
Reference-contexts: This change does not affect the operational properties of TCP noticeably. 1 Integer multiplication and division are quite slow on most processors. This is compounded by the fact that the Alpha architecture does not provide an integer division instruction <ref> [Sit92] </ref>. Thus, not only is the operation slow, but the function that implements it also takes up instruction space. <p> These workstations use the 21064 Alpha CPU running at 175MHz <ref> [Sit92] </ref>. The memory system features split primary i- and d-caches of 8KB each, a unified 2MB second-level cache (backup-cache, or b-cache), and 64MB of main memory. All caches are direct-mapped and use 32-byte cache blocks. For the i-cache, this implies that a cache block holds 8 instructions.
Reference: [Sta92] <author> Richard M. Stallman. </author> <title> Using and Porting GNU CC, 1992. Manuscript provided by the Free Software Foundation to document gcc. </title>
Reference-contexts: However, the hash-table manager supports a general interface that allows for unaligned keys and various key-sizes. This introduces the dilemma that the common usage pattern is simple enough to be inlined, but the general case is complicated enough to make inlining counter-productive. Fortunately, GNU C <ref> [Sta92] </ref> supports a builtin (intrinsic) function that evaluates to TRUE if and only if the argument is a compile-time constant.
Reference: [TL93] <author> Chandramohan A. Thekkath and Henry M. Levy. </author> <title> Limits to low-latency communication on high-speed networks. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2) </volume> <pages> 179-203, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Communication latency is often just as important as throughput in distributed systems, and for this reason, researchers have analyzed the latency characteristics of common network protocols, such as TCP/IP [KP93, CJRS89, Jac93] and RPC <ref> [TL93] </ref>. This paper revisits the issue of protocol latency. Our goal is not to optimize a particular protocol stack, but rather, to understand the fundamental limitations on processing overhead. <p> The LANCE overhead of 47:4s is consistent with the 51s figure reported elsewhere for the same controller in an older generation workstation <ref> [TL93] </ref>.
Reference: [YBMM93] <author> Masanobu Yuhara, Brian N. Bershad, Chris Maeda, and J. Eliot B. Moss. </author> <title> Efficient packet demultiplexing for multiple endpoints and large messages. </title> <month> July </month> <year> 1993. </year> <month> 22 </month>
Reference-contexts: The last part requires employing a packet classifier <ref> [BGP + 94, MJ93, YBMM93, EKJ95] </ref>. While path-inlining is easy in principle once indirect function calls have been taken care of, the practical problem is quite difficult. None of the common C compilers are able to inline code across module boundaries (object files).
References-found: 24

