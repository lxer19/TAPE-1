URL: http://www.cs.uoregon.edu/paracomp/proj/papers/mmb95.ps.gz
Refering-URL: http://www.cs.uoregon.edu/paracomp/proj/tau/papers.html
Root-URL: http://www.cs.uoregon.edu
Email: -mohr,malony-@cs.uoregon.edu kesavans@convex.com  
Title: Speedy: An Integrated Performance Extrapolation Tool for pC++ Programs  
Author: Bernd W. Mohr, Allen D. Malony Kesavan Shanmugam 
Keyword: performance prediction, extrapolation, object-parallel programming, trace-driven simulation, performance debugging tools, and modeling.  
Address: OR 97403, USA Richardson, TX 75083, USA  
Affiliation: Department of Computer and Information Science Convex Computer Corp. University of Oregon, Eugene  
Abstract: Performance extrapolation is the process of evaluating the performance of a parallel program in a target execution environment using performance information obtained for the same program in a different environment. Performance extrapolation techniques are suited for rapid performance tuning of parallel programs, particularly when the target environment is unavailable. This paper describes one such technique that was developed for data-parallel C++ programs written in the pC++ language. In pC++, the programmer can distribute a collection of objects to various processors and can have methods invoked on those objects execute in parallel. Using performance extrapolation in the development of pC++ applications allows tuning decisions to be made in advance of detailed execution measurements. The pC++ language system includes t, an integrated environment for analyzing and tuning the performance of pC++ programs. This paper presents speedy, a new addition to t, that predicts the performance of pC++ programs on parallel machines using extrapolation techniques. Speedy applies the existing instrumentation support of t to capture high-level event traces of a n-thread pC++ program run on a uniprocessor machine together with trace-driven simulation to predict the performance of the program run on a target n-processor machine. We describe how speedy works and how it is integrated into t. We also show how speedy can be used to evaluate a pC++ program for a given target environment.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. Bodin, P. Beckman, D. Gannon, S. Yang, S. Kesavan, A. Malony, B. Mohr, </author> <title> Implementing a Parallel C++ Runtime System for Scalable Parallel Systems, </title> <booktitle> Proc. Supercomputing 93, IEEE Computer Society, </booktitle> <pages> pp. 588-597, </pages> <month> November </month> <year> 1993. </year>
Reference: [2] <author> F. Bodin, P. Beckman, D. Gannon, J. Gotwals, S. Narayana, S. Srinivas, B. Winnicka, Sage++: </author> <title> An Object Oriented Toolkit and Class Library for Building Fortran and C++ Restructuring Tools, </title> <booktitle> Proc. </booktitle> <address> Oonski 94, Oregon, </address> <year> 1994. </year>
Reference-contexts: These language-level objects appear in all t tools. By plan, t was designed and developed in concert with the pC++ language system. It leverages off pC++ language technology, especially in its use of the Sage++ toolkit <ref> [2] </ref> as an interface to the pC++ compiler for instrumentation and for accessing properties of program objects. t is also integrated with the pC++ runtime system for profiling and tracing support. Because pC++ is intended to be portable, the tools are built to be portable as well.
Reference: [3] <author> E. A. Brewer, W. E. Weihl, </author> <title> Developing Parallel Applications Using High-Performance Simulation, </title> <booktitle> Proc. ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pp. 158-168, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Similarly, the user will be frustrated if the time taken to generate predicted results is significantly greater than the time taken by measurement-based experiments, a problem often faced by simulation systems that analyze program execution at too low a level. For example, the Proteus system <ref> [3] </ref> and the Wisconsin Wind Tunnel [17] have considerably advanced the efficiency and effectiveness of dynamic prediction techniques for architectural studies, but the overhead is still too high to allow their use for rapid and interactive performance debugging.
Reference: [4] <author> D. Brown, S. Hackstadt, A. Malony, B. Mohr, </author> <title> Program Analysis Environments for Parallel Language Systems: The TAU Environment, </title> <booktitle> Proc. of the Workshop on Environments and Tools For Parallel Scientific Computing, Townsend, Tennessee, </booktitle> <pages> pp. 162-171, </pages> <month> May </month> <year> 1994. </year>
Reference: [5] <author> M. E. Crovella and T. J. LeBlanc, </author> <title> Parallel Performance Prediction Using Lost Cycles Analysis, </title> <booktitle> Proc. Supercomputing 94, IEEE Computer Society, </booktitle> <pages> pp. 600-609, </pages> <month> Nov </month> <year> 1994. </year>
Reference: [6] <author> D. C. Grunwald, </author> <title> A Users Guide to AWESIME: An Object Oriented Parallel Programming and Simulation System, </title> <type> Technical Report 552-91, </type> <institution> Department of Computer Science, University of Colorado at Boulder, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: Paragon, TMC CM-5, IBM SP-1 / SP-2, Sequent Symmetry, SGI Challenge, Onyx, and PowerChallenge, Cray T3D, Meiko CS-2, Convex SPP and homogeneous clusters of UNIX workstations using PVM and MPI. pC++ also has multi-threading support for running applications in a quasi-parallel mode on UNIX workstations; supported thread systems are Awesime <ref> [6] </ref>, Pthreads, LWP, and the AT&T task library. This enables the testing and pre-evaluation of parallel pC++ applications in a familiar desktop environment.
Reference: [7] <author> S. Hackstadt, A. Malony, </author> <title> Next-Generation Parallel Performance Visualization: A Prototyping Environment for Visualization Development, </title> <booktitle> Proc. Parallel Architectures and Languages Europe, (PARLE), </booktitle> <address> Athens, Greece, </address> <year> 1994. </year>
Reference: [8] <author> R. Helm, A. D. Malony and S. F. Fickas, </author> <title> Capturing and Automating Performance Diagnosis: The Poirot Approach, </title> <booktitle> Proc. International Parallel Processing Symposium </booktitle>
Reference-contexts: 1 Introduction One of the foremost challenges for a parallel programmer is to achieve the best possible performance for an application on a parallel machine. For this purpose, the process of performance debugging (the iterative application of performance diagnosis <ref> [8] </ref> and tuning) is applied as an integral part of a parallel program development methodology. Application of performance debugging in practice has invariably required the development of performance tools based on the measurement and analysis of actual parallel program execution.
Reference: [9] <author> V. Herrarte, E. Lusk, </author> <title> Studying Parallel Program Behavior with Upshot, </title> <type> Technical Report ANL-91/15, </type> <institution> Mathematics and Computer Science Division, Argonne Natl. Lab., </institution> <year> 1991. </year>
Reference-contexts: Event tracing is fully operational on all parallel computer systems supported by pC++. This has several advantages for an ExtraP user. The traces used for simulation can be analyzed with all the event trace browsers supported by t (currently easy, Pablo [16], SIMPLE [13], and upshot <ref> [9] </ref>). As the ExtraP model is based on the operational characteristics of pC++ event classes, the user can also generate semantically equivalent traces on real parallel computer systems for comparing or validating the extrapolation results.
Reference: [10] <author> S. Hiranandani, K. Kennedy, C.-W. Tseng, S. Warren, </author> <title> The D Editor: A New Interactive Parallel Programming Tool, </title> <booktitle> Proc. </booktitle> <address> Supercomputing94, </address> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 733-742, </pages> <month> November </month> <year> 1994. </year>
Reference: [11] <author> J. Kohn and W. Williams, </author> <title> ATExpert, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 18, </volume> <year> 1993, </year> <pages> pp. 205-222. </pages>
Reference: [12] <author> A. Malony, B. Mohr, P. Beckman, D. Gannon, S. Yang, F. Bodin, </author> <title> Performance Analysis of pC++: A Portable Data-Parallel Programming System for Scalable Parallel Computers, </title> <booktitle> Proc. 8th Int. Parallel Processing Symb. (IPPS), Mexico, IEEE, </booktitle> <pages> pp. 75-85, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: This experiment tells us that to improve the performance of Poisson, we must tune Cyclic reduction first because it is the bottleneck. Speedy can be used in this way to locate bottlenecks in a program. The performance behavior observed using speedy is consistent with actual results <ref> [12] </ref>. 7 Conclusion The speedy and ExtraP tools are representative of the level of parallel performance evaluation support that is expected to be available for high-level parallel languages and to be integrated in program analysis environments where a performance engineered code development process is desired.
Reference: [13] <author> B. Mohr, </author> <title> Standardization of Event Traces Considered Harmful or Is an Implementation of Object-Independent Event Trace Monitoring and Analysis Systems Possible?, </title> <booktitle> Proc. CNRS-NSF Workshop on Environments and Tools For Parallel Scientific Computing, Elsevier, Advances in Parallel Computing, </booktitle> <volume> Vol. 6, </volume> <pages> pp. 103-124, </pages> <year> 1993. </year>
Reference-contexts: Event tracing is fully operational on all parallel computer systems supported by pC++. This has several advantages for an ExtraP user. The traces used for simulation can be analyzed with all the event trace browsers supported by t (currently easy, Pablo [16], SIMPLE <ref> [13] </ref>, and upshot [9]). As the ExtraP model is based on the operational characteristics of pC++ event classes, the user can also generate semantically equivalent traces on real parallel computer systems for comparing or validating the extrapolation results.
Reference: [14] <author> B. Mohr, D. Brown, A. Malony, </author> <title> TAU: A Portable Parallel Program Analysis Environment for pC++, </title> <booktitle> Proc. of CONPAR 94 - VAPP VI, </booktitle> <address> Linz, Austria, </address> <publisher> Springer Verlag, LNCS 854, </publisher> <pages> pp. 29-40, </pages> <month> September </month> <year> 1994. </year>
Reference: [15] <author> J. Ousterhout, </author> <title> Tcl and the Tk Toolkit, </title> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: Because pC++ is intended to be portable, the tools are built to be portable as well. C++ and C are used to ensure portable and efficient implementation, and similar reasons led us to choose Tcl/Tk <ref> [15] </ref> for the graphical interface. The t tools are implemented as graphical hypertools. While the tools are distinct, providing unique capabilities, they can act in combination to provide enhanced functionality.
Reference: [16] <author> D. A. Reed, R. D. Olson, R. A. Aydt, T. M. Madhyasta, T. Birkett, D. W. Jensen, B. A.A. Nazief, B. K. Totty, </author> <title> Scalable Performance Environments for Parallel Systems. </title> <booktitle> Proc. 6th Distributed Memory Computing Conf., </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 562-569, </pages> <year> 1991. </year>
Reference-contexts: Application of performance debugging in practice has invariably required the development of performance tools based on the measurement and analysis of actual parallel program execution. Parallel performance environments <ref> [16] </ref> support performance debugging through program instrumentation, performance data analysis, and results presentation tools, but have often lacked in their integration with parallel programming systems. <p> Event tracing is fully operational on all parallel computer systems supported by pC++. This has several advantages for an ExtraP user. The traces used for simulation can be analyzed with all the event trace browsers supported by t (currently easy, Pablo <ref> [16] </ref>, SIMPLE [13], and upshot [9]). As the ExtraP model is based on the operational characteristics of pC++ event classes, the user can also generate semantically equivalent traces on real parallel computer systems for comparing or validating the extrapolation results.
Reference: [17] <author> S. K. Reinhardt, M. D. Hill, J. R. Larus, A. R. Lebeck, J. C. Lewis and D. A. Wood, </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers, </title> <booktitle> Proc. ACM SIGMETRICS Conf. on Measurement and Modeling of Comp. Systems, </booktitle> <pages> pp. 48-60, </pages> <year> 1993. </year>
Reference-contexts: For example, the Proteus system [3] and the Wisconsin Wind Tunnel <ref> [17] </ref> have considerably advanced the efficiency and effectiveness of dynamic prediction techniques for architectural studies, but the overhead is still too high to allow their use for rapid and interactive performance debugging.
Reference: [18] <author> K. Shanmugam, </author> <title> Performance Extrapolation of Parallel Programs, </title> <type> Masters Thesis, </type> <institution> Department of Computer and Information Science, University of Oregon, </institution> <month> June </month> <year> 1994. </year>
Reference: [19] <author> K. Shanmugam, A. Malony, </author> <title> Performance Extrapolation of Parallel Programs, </title> <booktitle> Proc. </booktitle> <address> ICPP95. </address>
Reference: [20] <author> K. Shanmugam, A. Malony, B. Mohr, </author> <title> Performance Extrapolation of Parallel Programs, </title> <type> Technical Report CIS-TR-95-14, </type> <institution> University of Oregon, Department of Computer and Information Science, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: The technique is one example of a general prediction methodology called Performance Extrapolation that estimates the performance of a parallel program in a target execution environment by using the performance data obtained from running the program in a different environment. In <ref> [20] </ref>, we demonstrated that performance extrapolation is a viable process for parallel program performance debugging that can be applied effectively in situations where standard measurement techniques are restrictive or costly.
Reference: [21] <author> H. Wabnig and G. Haring, </author> <title> PAPS - The Parallel Program Performance Prediction Toolset, Computer Performance Evaluation - Modelling Techniques and Tools, </title> <publisher> LNCS 794, Springer-Verlag, </publisher> <pages> pp. 284-304, </pages> <year> 1994. </year>
Reference-contexts: The environment would measure only those performance data which is necessary, and use high-level analysis to evaluate different program alternatives under different system configuration scenarios. In this manner, the environment would enable performance-driven parallel program design where algorithm choices could be considered early in the development process <ref> [21] </ref>. The user would demand a level of detail from predicted performance analysis comparable to that provided by measurements; however, static prediction tools often cannot provide this.
References-found: 21

