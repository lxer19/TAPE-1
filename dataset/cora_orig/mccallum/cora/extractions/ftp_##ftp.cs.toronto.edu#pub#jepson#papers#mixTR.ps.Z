URL: ftp://ftp.cs.toronto.edu/pub/jepson/papers/mixTR.ps.Z
Refering-URL: http://www.parc.xerox.com/spl/members/black/mixtureOF.html
Root-URL: 
Title: Mixture Models for Optical Flow Computation  
Author: Allan Jepson and Michael Black 
Keyword: Category: Motion analysis. Keywords: Optical flow, probabilistic mixture models, motion boundaries.  
Affiliation: 1 Canadian Institute for Advanced Research, and Department of Computer Science, University of Toronto 2 Xerox PARC  
Abstract: The computation of optical flow relies on merging information available over an image patch to form an estimate of 2D image velocity at a point. This merging process raises a host of issues, which include the treatment of outliers in component velocity measurements and the modeling of multiple motions within a patch which arise from occlusion boundaries or transparency. We present a new approach which allows us to deal with these issues within a common framework. Our approach is based on the use of a probabilistic mixture model to explicitly represent multiple motions within a patch. We use a simple extension of the EM-algorithm to compute a maximum likelihood estimate for the various motion parameters. Preliminary experiments indicate that this approach is computationally efficient and can provide robust estimates of the optical flow values in the presence of outliers and multiple motions. The basic approach can also be applied to other problems in computational vision, such as the computation of 3D relative motion, which require the integration of several partial constraints to obtain a desired quantity. This paper is the University of Toronto, Department of Computer Science, Technical Report: RBCV-TR-93-44, April 1993. Correspondence should be sent to A. Jepson, Department of Computer Science, University of Toronto, 6 Kings College Road, Toronto, Ontario M5S 1A4, or to jepson@cs.toronto.edu 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. R. Bergen, P. Anandan, K. J. Hanna, and R. Hingorani. </author> <title> Hierarchical model-based motion estimation. </title> <editor> In G. Sandini, editor, </editor> <booktitle> Proc. of Second European Conference on 15 Computer Vision, ECCV-92, volume 588 of LNCS-Series, </booktitle> <pages> pages 237-252. </pages> <publisher> Springer-Verlag, </publisher> <month> May </month> <year> 1992. </year>
Reference-contexts: However, on the positive side, in many situations of interest the desired flow field is spatially coherent over relatively large regions of the image. For example, it has been shown that an affine flow model is a reasonable approximation in many cases <ref> [1] </ref>, such as for a smooth surface having a sufficiently small variation in relative depth.
Reference: [2] <author> J. R. Bergen, P. J. Burt, R. Hingorani, and S. Peleg. </author> <title> A three-frame algorithm for estimating two-component image motion. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 14(9) </volume> <pages> 886-896, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: While such an approach cannot cope with transparency and fragmented occlusion, the area-based regression approach of Bergen et al. <ref> [2] </ref> provides an iterative method for recovering two transparent motions from three frames. For coping with motion boundaries, an alternative approach is provided by regularization, in which the effective integration domain for constraints on a single smooth surface is dictated implicitly through a smoothness model and iteration. <p> Figures 4.5b and c show the results obtained by canceling the effect of the moving print and reflection respectively. These results obtained from two frames compare favorably with those of <ref> [2] </ref> which required three frames to recover the two motions. 4 A large wavelength was necessary to simultaneously compute constraints for both motions since they differ by approximately 2.5 pixels. 14 vision researcher (see text). 5 Conclusion We have examined the problems posed by multiple motions and outliers in the intergration
Reference: [3] <author> M. J. Black. </author> <title> Robust Incremental Optical Flow. </title> <type> PhD thesis, </type> <institution> Yale Univeristy, </institution> <address> New Haven, CT, </address> <year> 1992. </year> <note> Research Report YALEU/DCS/RR-923. </note>
Reference-contexts: We observe that, when multiple motions are present, the motion estimates within a region form distinct clusters. We employ a simple extension of the EM-algorithm [14] to isolate these clusters and estimate their likelihood. This approach has a number of benefits. Like robust regression techniques <ref> [3] </ref>, the approach allows us to robustly estimate the dominant motion within a region. Morevoer, by assuming the motion is due to a mixture of distributions we are able to recover multiple coherent motions and identify outlying measurements which do not correspond to a coherent motion. <p> A host of other techniques fall under the category of robust estimation [10] in which the goal is to recover the dominant motion while treating the inconsistent constraints as outliers 1 See <ref> [3] </ref> for a review. 4 and reducing their influence on the solution. The robust estimation framework introduced by Black [3] has been applied to area-based regression techniques as well as correlation. <p> host of other techniques fall under the category of robust estimation [10] in which the goal is to recover the dominant motion while treating the inconsistent constraints as outliers 1 See <ref> [3] </ref> for a review. 4 and reducing their influence on the solution. The robust estimation framework introduced by Black [3] has been applied to area-based regression techniques as well as correlation. The framework also generalizes the regularization techniques by applying outlier rejection to both the data conservation and spatial smoothness assumptions.
Reference: [4] <author> A. Blake and A. Zisserman. </author> <title> Visual Reconstruction. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mas-sachusetts, </address> <year> 1987. </year>
Reference-contexts: Smoothing over motion discontinuities is avoided either by explicitly introducing boundaries (as line processes) [9] or by using weak continuity constraints <ref> [4] </ref>. To cope with cases of fragmented occlusion, Darrell and Pentland [5] have proposed a method for segmenting the motions into distinct layers. The use of multiple layers within a robust regularization framework is discussed further in the chapter by Madarasmi and Kersten in this volume [13].
Reference: [5] <author> T. Darrell and A. Pentland. </author> <title> Robust estimation of a multi-layer motion representation. </title> <booktitle> In Proc. IEEE Workshop on Visual Motion, </booktitle> <pages> pages 173-178, </pages> <address> Princeton, NJ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Smoothing over motion discontinuities is avoided either by explicitly introducing boundaries (as line processes) [9] or by using weak continuity constraints [4]. To cope with cases of fragmented occlusion, Darrell and Pentland <ref> [5] </ref> have proposed a method for segmenting the motions into distinct layers. The use of multiple layers within a robust regularization framework is discussed further in the chapter by Madarasmi and Kersten in this volume [13].
Reference: [6] <author> D.J. </author> <title> Fleet. Measurement of Image Velocity. </title> <publisher> Kluwer, </publisher> <address> Boston, </address> <year> 1992. </year>
Reference-contexts: This constraint is clearly violated in many natural situations, such as cases with shading variation, highlights, transparency, or occlusion boundaries. While more effort can be spent on developing more sophisticated structure assumptions to deal with some of these cases (as has been done, for example, by Fleet <ref> [6] </ref>), we cannot avoid the need to make some assumption in connecting image structure to the motion of points in the scene. When this assumption is violated we can expect our motion constraint vector, ~c (~x; t), to be meaningless. <p> For the moment assume that the actual velocity is given by ~v n , then an exact constraint vector ~c k would lie on the plane perpendicular to ~v n . In <ref> [6] </ref> it is shown that a reasonable approximation for the distribution of errors in component velocity measurements is given by a roughly Gaussian distribution for the angular error between ~c k and the plane perpendicular to ~v n . <p> An important point about this error distribution is that it is only meant to model the measurements that are reasonably accurate. While the actual error distributions have longer tails than one might expect from such a Gaussian model (see <ref> [6] </ref>), this is not critical for our current situation since we also incorporate a model for outliers as a separate component in the mixture model. In effect, the longer tails are modeled by this outlier process, rather than by the above Gaussian distribution. <p> From the wide range of different measurement strategies for component velocities or, equivalently, for the motion constraint vectors ~c k , we chose a phase-based approach. In previous work a similar method has been shown to provide reasonably accurate component velocities, with a low outlier rate <ref> [6] </ref>. The particular approach we use is based on only two consecutive frames, and the actual component velocity measurement method is similar to the phase-based stereo disparity measurement scheme discussed in [12].
Reference: [7] <author> D.J. Fleet, A.D. Jepson, and M. Jenkin. </author> <title> Phase-based disparity measurement. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 53(2) </volume> <pages> 198-210, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: Points at which the complex convolution response was below 4% of the maximum possible value were discarded (since responses at this low level have been crudely quantized to only a few different gray levels), as were points where the phase and amplitude derivatives failed the test for a singularity neighborhood <ref> [7] </ref>. Finally, in order to bring the two frames into rough alignment, the frames were sometimes spatially shifted by an integral multiple of the filter spacing.
Reference: [8] <author> W.T. Freeman and E.H. Adelson. </author> <title> The design and use of steerable filters. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(9) </volume> <pages> 891-906, </pages> <year> 1991. </year>
Reference-contexts: Briefly, the basic steps in the component velocity measurement are to first convolve each frame with the complex band-pass filter G 2 + iH 2 , for each of four different spatial orientations of the filter kernel <ref> [8] </ref>. This kernel is chosen because it is compact (eg. the fine-scale version is 9 fi 9), has a simple analytical form, and the real and imaginary parts nearly form a quadrature pair.
Reference: [9] <author> S. Geman and D. Geman. </author> <title> Stochastic relaxation, Gibbs distributions and Bayesian restoration of images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-6(6):721-741, </volume> <month> November </month> <year> 1984. </year>
Reference-contexts: For coping with motion boundaries, an alternative approach is provided by regularization, in which the effective integration domain for constraints on a single smooth surface is dictated implicitly through a smoothness model and iteration. Smoothing over motion discontinuities is avoided either by explicitly introducing boundaries (as line processes) <ref> [9] </ref> or by using weak continuity constraints [4]. To cope with cases of fragmented occlusion, Darrell and Pentland [5] have proposed a method for segmenting the motions into distinct layers.
Reference: [10] <author> F. R. Hampel, E. M. Ronchetti, P. J. Rousseeuw, and W. A. Stahel. </author> <title> Robust Statistics: The Approach Based on Influence Functions. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, NY, </address> <year> 1986. </year>
Reference-contexts: The use of multiple layers within a robust regularization framework is discussed further in the chapter by Madarasmi and Kersten in this volume [13]. A host of other techniques fall under the category of robust estimation <ref> [10] </ref> in which the goal is to recover the dominant motion while treating the inconsistent constraints as outliers 1 See [3] for a review. 4 and reducing their influence on the solution.
Reference: [11] <author> B. K. P. Horn. </author> <title> Robot Vision. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: 1 Introduction The computation of optical flow relies on merging information available over an image patch to form an estimate of 2D image velocity at a point. The well known aperture problem for optical flow computations <ref> [11] </ref> states that, given information available from only a small spatial aperture, we can expect to derive only a partial constraint on the image motion. In order to fully constrain the optical flow we need to integrate several such constraints obtained over a larger spatial neighborhood. <p> The motion constraint equation (2.1) provides a single (linear) constraint on the two unknowns v 1 and v 2 , and is therefore insufficient to determine a unique 2D image velocity. This is commonly referred to as the aperture problem <ref> [11] </ref>. As a result, we are faced with collecting several such constraints, say from a spatio-temporal neighborhood of the point (~x; t), in an attempt to infer a particular 2D image velocity.
Reference: [12] <author> A.D. Jepson and M. Jenkin. </author> <title> Fast computation of disparity from phase differences. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition, CVPR-89, </booktitle> <pages> pages 398-403, </pages> <address> San Diego, </address> <year> 1989. </year>
Reference-contexts: The particular approach we use is based on only two consecutive frames, and the actual component velocity measurement method is similar to the phase-based stereo disparity measurement scheme discussed in <ref> [12] </ref>. Briefly, the basic steps in the component velocity measurement are to first convolve each frame with the complex band-pass filter G 2 + iH 2 , for each of four different spatial orientations of the filter kernel [8].
Reference: [13] <author> S. Madarasmi and D. Kersten. </author> <title> The visual perception of surfaces, their properties, and relationships. this volume. </title>
Reference-contexts: To cope with cases of fragmented occlusion, Darrell and Pentland [5] have proposed a method for segmenting the motions into distinct layers. The use of multiple layers within a robust regularization framework is discussed further in the chapter by Madarasmi and Kersten in this volume <ref> [13] </ref>. A host of other techniques fall under the category of robust estimation [10] in which the goal is to recover the dominant motion while treating the inconsistent constraints as outliers 1 See [3] for a review. 4 and reducing their influence on the solution.
Reference: [14] <author> G.J. McLachlan and K.E. Basford. </author> <title> Mixture Models: Inference and Applications to Clustering. </title> <publisher> Marcel Dekker Inc., </publisher> <address> N.Y., </address> <year> 1988. </year>
Reference-contexts: We relax the single-motion assumption and, instead, assume that the motions within any particular region can be described by a probabilistic mixture of distributions. We observe that, when multiple motions are present, the motion estimates within a region form distinct clusters. We employ a simple extension of the EM-algorithm <ref> [14] </ref> to isolate these clusters and estimate their likelihood. This approach has a number of benefits. Like robust regression techniques [3], the approach allows us to robustly estimate the dominant motion within a region. <p> These ownership probabilities are defined by q nk = P N : (3:4) These equations (3.3) for a maximum likelihood fit have been derived by a number of authors; for further details see <ref> [14] </ref>. The first equation (3.3a) comes from the condition that the partial derivative of log L with respect to the mixture proportion m n must be equal to the Lagrange multiplier . This Lagrange multiplier arises by imposing the constraint that the mixture proportions must sum to one. <p> The second equation is obtained simply by requiring that the partial derivative of log L with respect to the parameters ~a n must vanish. These equations suggests an iterative algorithm, known as the EM-algorithm <ref> [14] </ref>, for obtaining a maximum likelihood fit for the parameters m n and ~a n , for n = 0; . . . ; N . Given an initial guess for these parameters we first estimate the ownership probabilities q nk for each constraint belonging to each component.
Reference: [15] <author> M. Nitzberg and D. Mumford. </author> <title> The 2.1-D sketch. </title> <booktitle> In Proc. Int. Conf. on Computer Vision, ICCV-90, </booktitle> <pages> pages 138-144, </pages> <address> Osaka, Japan, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: These situations result in exactly the same type of clusters of constraints. To cope with situations such as this we relax the single motion assumption in two ways. First, we assume that a region may contain multiple coherent motions. We can think of these multiple motions as corresponding layers <ref> [15] </ref> whose spatial extent may be the entire region. Each layer contains a single consistent motion and each layer may be described by different parametric motion models.
Reference: [16] <author> M. Okutomi and T. Kanade. </author> <title> A locally adaptive window for signal matching. </title> <journal> International Journal of Computer Vision, </journal> <volume> 7(2) </volume> <pages> 143-162, </pages> <month> January </month> <year> 1992. </year> <month> 16 </month>
Reference-contexts: be identified and rejected. 2.1 Previous Approaches A host of techniques have been developed to deal with the generalized aperture problem and outliers. 1 Example techniques include the use of "adaptive windows" which adjust their size and shape in an attempt to capture constraints from a single smooth surface patch <ref> [16] </ref>. While such an approach cannot cope with transparency and fragmented occlusion, the area-based regression approach of Bergen et al. [2] provides an iterative method for recovering two transparent motions from three frames.
Reference: [17] <author> B. G. Schunck. </author> <title> Image flow segmentation and estimation by constraint line clustering. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 11(10) </volume> <pages> 1010-1027, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Our approach, as opposed to making the single-motion assumption, explicitly models multiple motions and outliers, and hence is able to capture the more complex structure present in the data. A somewhat different approach, referred to as "constraint line clustering", has been proposed by Schunck <ref> [17] </ref>. The basic idea is that the redundancy in the motion constraint vectors ~c (~x; t) arising from a smooth surface patch should be recognizable from the constraint vectors themselves.
References-found: 17

