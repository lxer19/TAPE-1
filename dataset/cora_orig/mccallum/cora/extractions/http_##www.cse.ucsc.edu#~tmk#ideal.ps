URL: http://www.cse.ucsc.edu/~tmk/ideal.ps
Refering-URL: 
Root-URL: 
Title: Exploring the Bounds of Web Latency Reduction from Caching and Prefetching  
Author: Thomas M. Kroeger Darrell D. E. Long Jeffrey C. Mogul 
Affiliation: Department of Computer Engineering University of California, Santa Cruz  Department of Computer Science University of California, Santa Cruz  Digital Equipment Corporation Western Research Laboratory  
Abstract: Prefetching and caching are techniques commonly used in I/O systems to reduce latency. Many researchers have advocated the use of caching and prefetching to reduce latency in the Web. We derive several bounds on the performance improvements seen from these techniques, and then use traces of Web proxy activity taken at Digital Equipment Corporation to quantify these bounds. We found that for these traces, local proxy caching could reduce latency by at best 26%, prefetching could reduce latency by at best 57%, and a combined caching and prefetching proxy could provide at best a 60% latency reduction. Furthermore, we found that how far in advance a prefetching algorithm was able to prefetch an object was a significant factor in its ability to reduce latency. We note that the latency reduction from caching is significantly limited by the rapid changes of objects in the Web. We conclude that for the workload studied caching offers moderate assistance in reducing latency. Prefetching can offer more than twice the improvement of caching but is still limited in its ability to reduce latency. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Bestavros and C. Cunha, </author> <title> A prefetching protocol using client speculation for the WWW, Tech. </title> <type> Rep. </type> <institution> TR-95-011, Boston University, Department of Computer Science, </institution> <address> Boston, MA 02215, </address> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: 1 Introduction The growth of the Web over the past few years has inspired researchers to investigate prefetching and caching as techniques to reduce latency <ref> [1, 12, 15] </ref>. While such techniques have seen significant success reducing latency in storage systems [7, 8, 9, 14] and in processor y tmk@cse.ucsc.edu. Supported in part by Digital Equipment Corporation and the Office of Naval Research under Grant N0001492J 1807. z darrell@cse.ucsc.edu. <p> In their simulations, they estimated reductions of as much as 45%, but note that such techniques will also double the network traffic. Nevertheless, they show that a reasonable balance of latency reduction and network traffic increase can be found. Bestavros et al. <ref> [1] </ref> have presented a model for the speculative dissemination of World Wide Web data. This work shows that reference patterns from a Web server can be used as an effective source of information to drive prefetching.
Reference: [2] <author> R. C. Burns and D. D. E. </author> <title> Long, Efficient distributed backup with delta c1ompression, </title> <booktitle> in Proceedings of the 1997 I/O in Parallel and Distributed Systems (IOPADS'97), </booktitle> <address> San Jose, CA, USA, </address> <month> Nov. </month> <year> 1997. </year>
Reference-contexts: Again, we caution that these results are highly dependent on the workload and environment modeled. They should be applied with care. Nevertheless these results emphasize the need for improved prefetching techniques as well as additional techniques beyond caching and prefetch-ing. These techniques might include wide-area replication [5], delta encoding <ref> [2, 11] </ref> and persistent TCP connections [10] which has been included in the HTTP/1.1 protocol [4]. (a) Passive Caching (b) Local Prefetching (c) Server-Hint Based without Caching (d) Server-Hint Based with Caching Acknowledgments We are grateful to Digital Equipment Corporation for their support of this work and the use of their
Reference: [3] <author> F. Douglis, A. Feldmann, B. Krishnamurthy, and J. Mogul, </author> <title> Rate of change and other metrics: A live study of the world wide web, </title> <booktitle> in Proceedings of First USENIX Symposium on Internet Technologies and Systems, </booktitle> <month> Dec. </month> <year> 1997. </year>
Reference-contexts: Additionally, we observe that the latency reduction from caching was half of what it would have been for a data set with data objects that did not change. This observation agrees with several studies that show a high rate of change for objects in the web <ref> [11, 6, 3] </ref>. Comparing our results with the external latency we observe that, for the workload examined, Web latency consists of 23% internal latency, 20% external latency that cannot be cached or prefetched and 57% external latency that can be removed through prefetch-ing and caching. <p> Table 2 shows that the latency reduction bound for local prefetching is almost double that for passive caching (see Table 1). The two results differ because the passive cache pays for a miss when an object changes; the high observed rate of change <ref> [3, 11] </ref> is what causes much of the poor performance of passive caching. 4.3 Bounds on Prefetching with Server Based Hints To simulate server-hint based prefetching, we assume that prefetching can only begin after the client has contacted the server for the first time.
Reference: [4] <author> R. T. Fielding, J. Gettys, J. C. Mogul, H. F. Nielsen, and T. Berners-Lee, </author> <title> Hypertext transfer protocol HTTP/1.1, RFC 2068, </title> <institution> HTTP Working Group, </institution> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: They should be applied with care. Nevertheless these results emphasize the need for improved prefetching techniques as well as additional techniques beyond caching and prefetch-ing. These techniques might include wide-area replication [5], delta encoding [2, 11] and persistent TCP connections [10] which has been included in the HTTP/1.1 protocol <ref> [4] </ref>. (a) Passive Caching (b) Local Prefetching (c) Server-Hint Based without Caching (d) Server-Hint Based with Caching Acknowledgments We are grateful to Digital Equipment Corporation for their support of this work and the use of their facilities, the Office of Naval Research for their support, Prof. Mary G.
Reference: [5] <author> J. Gwertzman and M. Seltzer, </author> <title> The case for geographical push caching, </title> <booktitle> in Fifth Annual Workshop on Hot Operating Systems, </booktitle> <address> (Orcas Island, WA), </address> <pages> pp. 5155, </pages> <publisher> IEEE Computer Society, </publisher> <month> May </month> <year> 1995. </year>
Reference-contexts: Again, we caution that these results are highly dependent on the workload and environment modeled. They should be applied with care. Nevertheless these results emphasize the need for improved prefetching techniques as well as additional techniques beyond caching and prefetch-ing. These techniques might include wide-area replication <ref> [5] </ref>, delta encoding [2, 11] and persistent TCP connections [10] which has been included in the HTTP/1.1 protocol [4]. (a) Passive Caching (b) Local Prefetching (c) Server-Hint Based without Caching (d) Server-Hint Based with Caching Acknowledgments We are grateful to Digital Equipment Corporation for their support of this work and the
Reference: [6] <author> J. Gwertzman and M. Seltzer, </author> <title> World wide web cache consistency, </title> <booktitle> in Proceedings of the USENIX 1996 Annual Technical Conference, </booktitle> <address> (San Diego, CA), </address> <pages> pp. 141152, </pages> <publisher> USENIX, </publisher> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: Additionally, we observe that the latency reduction from caching was half of what it would have been for a data set with data objects that did not change. This observation agrees with several studies that show a high rate of change for objects in the web <ref> [11, 6, 3] </ref>. Comparing our results with the external latency we observe that, for the workload examined, Web latency consists of 23% internal latency, 20% external latency that cannot be cached or prefetched and 57% external latency that can be removed through prefetch-ing and caching.
Reference: [7] <author> T. Kimbrel, A. Tomkins, R. H. Patterson, B. Ber-shad, P. Cao, E. W. Felton, G. A. Gibson, A. Kar-lin, and K. Li, </author> <title> A trace-driven comparison of algorithm for parallel prefetching and caching, </title> <booktitle> in Proceedings of Second USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pp. 19 34, </pages> <publisher> USENIX, </publisher> <month> October </month> <year> 1996. </year>
Reference-contexts: 1 Introduction The growth of the Web over the past few years has inspired researchers to investigate prefetching and caching as techniques to reduce latency [1, 12, 15]. While such techniques have seen significant success reducing latency in storage systems <ref> [7, 8, 9, 14] </ref> and in processor y tmk@cse.ucsc.edu. Supported in part by Digital Equipment Corporation and the Office of Naval Research under Grant N0001492J 1807. z darrell@cse.ucsc.edu.
Reference: [8] <author> T. M. Kroeger and D. D. E. </author> <title> Long, Predicting filesystem actions from prior events, </title> <booktitle> in Proceedings of the USENIX 1996 Annual Technical Conference, USENIX, </booktitle> <month> January </month> <year> 1996. </year>
Reference-contexts: 1 Introduction The growth of the Web over the past few years has inspired researchers to investigate prefetching and caching as techniques to reduce latency [1, 12, 15]. While such techniques have seen significant success reducing latency in storage systems <ref> [7, 8, 9, 14] </ref> and in processor y tmk@cse.ucsc.edu. Supported in part by Digital Equipment Corporation and the Office of Naval Research under Grant N0001492J 1807. z darrell@cse.ucsc.edu.
Reference: [9] <author> H. Lei and D. Duchamp, </author> <title> An analytical approach to file prefetching, </title> <booktitle> in Proceedings of USENIX 1997 Annual Technical Conference, USENIX, </booktitle> <month> Jan-uary </month> <year> 1997. </year>
Reference-contexts: 1 Introduction The growth of the Web over the past few years has inspired researchers to investigate prefetching and caching as techniques to reduce latency [1, 12, 15]. While such techniques have seen significant success reducing latency in storage systems <ref> [7, 8, 9, 14] </ref> and in processor y tmk@cse.ucsc.edu. Supported in part by Digital Equipment Corporation and the Office of Naval Research under Grant N0001492J 1807. z darrell@cse.ucsc.edu.
Reference: [10] <author> J. C. </author> <title> Mogul, </title> <booktitle> The case for persistent-connection HTTP, in Proceedings of the 1995 SIGCOMM, </booktitle> <pages> pp. 299313, </pages> <publisher> ACM, </publisher> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: They should be applied with care. Nevertheless these results emphasize the need for improved prefetching techniques as well as additional techniques beyond caching and prefetch-ing. These techniques might include wide-area replication [5], delta encoding [2, 11] and persistent TCP connections <ref> [10] </ref> which has been included in the HTTP/1.1 protocol [4]. (a) Passive Caching (b) Local Prefetching (c) Server-Hint Based without Caching (d) Server-Hint Based with Caching Acknowledgments We are grateful to Digital Equipment Corporation for their support of this work and the use of their facilities, the Office of Naval Research
Reference: [11] <author> J. C. Mogul, F. Douglis, A. Feldmann, , and B. Kr-ishnamurthy, </author> <title> Potential benefits of delta encoding and data compression for HTTP, in Proceedings of the 1997 SIGCOMM, </title> <booktitle> (Cannes, France), </booktitle> <pages> pp. 181194, </pages> <publisher> ACM, </publisher> <month> Sept. </month> <year> 1997. </year>
Reference-contexts: Additionally, we observe that the latency reduction from caching was half of what it would have been for a data set with data objects that did not change. This observation agrees with several studies that show a high rate of change for objects in the web <ref> [11, 6, 3] </ref>. Comparing our results with the external latency we observe that, for the workload examined, Web latency consists of 23% internal latency, 20% external latency that cannot be cached or prefetched and 57% external latency that can be removed through prefetch-ing and caching. <p> Table 2 shows that the latency reduction bound for local prefetching is almost double that for passive caching (see Table 1). The two results differ because the passive cache pays for a miss when an object changes; the high observed rate of change <ref> [3, 11] </ref> is what causes much of the poor performance of passive caching. 4.3 Bounds on Prefetching with Server Based Hints To simulate server-hint based prefetching, we assume that prefetching can only begin after the client has contacted the server for the first time. <p> Again, we caution that these results are highly dependent on the workload and environment modeled. They should be applied with care. Nevertheless these results emphasize the need for improved prefetching techniques as well as additional techniques beyond caching and prefetch-ing. These techniques might include wide-area replication [5], delta encoding <ref> [2, 11] </ref> and persistent TCP connections [10] which has been included in the HTTP/1.1 protocol [4]. (a) Passive Caching (b) Local Prefetching (c) Server-Hint Based without Caching (d) Server-Hint Based with Caching Acknowledgments We are grateful to Digital Equipment Corporation for their support of this work and the use of their
Reference: [12] <author> V. N. Padmanabhan and J. C. Mogul, </author> <title> Using predictive prefetching to improve world wide web latency, </title> <journal> Computer Communications Review, </journal> <volume> vol. 26, </volume> <pages> pp. 2236, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: 1 Introduction The growth of the Web over the past few years has inspired researchers to investigate prefetching and caching as techniques to reduce latency <ref> [1, 12, 15] </ref>. While such techniques have seen significant success reducing latency in storage systems [7, 8, 9, 14] and in processor y tmk@cse.ucsc.edu. Supported in part by Digital Equipment Corporation and the Office of Naval Research under Grant N0001492J 1807. z darrell@cse.ucsc.edu. <p> Padmanabhan and Mogul <ref> [12] </ref> described a server hint-based predictive model.
Reference: [13] <author> A. J. Smith, </author> <title> Cache memories, </title> <journal> ACM Computing Surveys, </journal> <volume> vol. 14, </volume> <pages> pp. 473530, </pages> <month> Sept. </month> <year> 1982. </year>
Reference-contexts: Supported in part by Digital Equipment Corporation and the Office of Naval Research under Grant N0001492J 1807. z darrell@cse.ucsc.edu. Supported in part by the Office of Naval Research under Grant N0001492J1807. x mogul@pa.dec.com. memory hierarchies <ref> [13] </ref>, it remains to be seen how effective such techniques can be within the World Wide Web. We classify caching and prefetching into four different methods and then derive bounds on these methods. Using traces taken over a three week period at Digital Equipment Corporation, we quantify these bounds.
Reference: [14] <author> J. S. Vitter and P. Krishnan, </author> <title> Optimal prefetch-ing via data compression, </title> <journal> Journal of the ACM, </journal> <volume> vol. 43, </volume> <pages> pp. 771793, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: 1 Introduction The growth of the Web over the past few years has inspired researchers to investigate prefetching and caching as techniques to reduce latency [1, 12, 15]. While such techniques have seen significant success reducing latency in storage systems <ref> [7, 8, 9, 14] </ref> and in processor y tmk@cse.ucsc.edu. Supported in part by Digital Equipment Corporation and the Office of Naval Research under Grant N0001492J 1807. z darrell@cse.ucsc.edu.
Reference: [15] <author> S. Williams, M. Abrams, C. R. Standridge, C. Ab-dulla, and E. A. Fox, </author> <title> Removal policies in network caches for world-wide web documents, </title> <booktitle> in Proceedings of the 1996 SIGCOMM, </booktitle> <pages> pp. 293305, </pages> <publisher> ACM, </publisher> <month> July </month> <year> 1996. </year>
Reference-contexts: 1 Introduction The growth of the Web over the past few years has inspired researchers to investigate prefetching and caching as techniques to reduce latency <ref> [1, 12, 15] </ref>. While such techniques have seen significant success reducing latency in storage systems [7, 8, 9, 14] and in processor y tmk@cse.ucsc.edu. Supported in part by Digital Equipment Corporation and the Office of Naval Research under Grant N0001492J 1807. z darrell@cse.ucsc.edu. <p> Also, while the cache hit ratio ranges from 47%52%( 19%28%), the latency reduction is only half of that. This implies that the majority of the requests that saw a cache hit are for objects smaller than the average event, which confirms a similar observation made by Williams et al. <ref> [15] </ref>. That study showed a weak inverse relationship between the likely number of accesses per unchanged response and the response size. In Table 1, the latency reduction, hit ratio and cache size are larger for the entire-trace columns than for the single-week columns. <p> This work shows that reference patterns from a Web server can be used as an effective source of information to drive prefetching. They observed a latency reduction of as much as 50%, but only at the cost of a significant increase in the bandwidth used. Williams et al. <ref> [15] </ref> presented a taxonomy of replacement policies for caching within the Web. Their experiment examined the hit rates for various workloads. The observed hit rates that range from 20% to as high as 98%, with the majority ranging around 50%.
References-found: 15

