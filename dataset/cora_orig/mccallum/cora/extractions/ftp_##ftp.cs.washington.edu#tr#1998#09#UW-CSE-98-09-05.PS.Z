URL: ftp://ftp.cs.washington.edu/tr/1998/09/UW-CSE-98-09-05.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-date.html
Root-URL: http://www.cs.washington.edu
Title: Support for Software Assisted Speculative Execution  
Author: E Christopher Lewis 
Date: September 30, 1998  
Abstract: Computer architects strive to improve machine performance by exploiting parallelism, but control flow and data dependences limit available parallelism. Speculative execution enhances parallelism by selectively ignoring the constraints of control flow and data dependences, thereby executing instructions before it it known whether they are needed or correct. Software assisted speculative execution is a form of this tack where the running program directs the hardware in what instructions should be speculatively executed and how. This report identifies and characterized the fundamental architectural, implementation, and compiler issues of software assisted speculative execution. These issues serve as the basis for describing, comparing and contrasting proposed architectures from the literature. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David I. August, Daniel A. Connors, Scott A. Mahlke, John W. Sias, Kevin M. Crozier, Ben-Chung Cheng, Patrick R. Eaton, Qudus B. Olaniran, and Wen-mei W. Hwu. </author> <title> Integrated predicated and speculative execution in the IMPACT EPIC architecture. </title> <booktitle> In Proceedings of the 25th Annual International Symposium on Computer Architecture (ISCA-25), </booktitle> <pages> pages 22737, </pages> <address> Barcelona, Spain, June 27July 1, </address> <year> 1998. </year>
Reference-contexts: ILS architectures rely on compilers to identify parallelism by statically scheduling a program's instructionssome of them speculativelyin an effort to co-locate independent instructions. The Boosting [35, 33, 36, 32] and IMPACT <ref> [5, 6, 3, 1] </ref> architectures are examples of ILS architectures. A thread-level speculative (TLS) architecture is a small-scale multiprocessor-on-a-chip augmented with support for the speculative execution of threads and for managing interthread dependences. A TLS architecture simultaneously executes different threads on independent processing elements, called nodes. <p> In the process, terminal exception that should be handled may be lost, and spurious transparent exceptions may be unnecessarily handled, impacting performance but not correctness. August et al. find that 13% of transparent exceptions are spurious <ref> [1] </ref>. If a terminal exception is an indicator of an error, exceptions are only lost by programs that contain errors. The General Percolation variant of the IMPACT architecture does not preserve exception behavior [6]. <p> Recovery blocks are separate from the body of the program, thus duplicating speculative instructions. When re-execution is necessary, the hardware executes a particular recovery block. An inline replay mechanism selectively re-executes instructions from a previous point in the code <ref> [1, 6, 23] </ref>. The latter is more space efficient, but it must fetch and potentially execute many more instructions than the former. Compilers for architectures that do not preserve state must ensure that all operands to potentially re-executed instructions are available at re-execution time. <p> Smith et al. argue that recovery blocks never increase code size beyond a factor of two [33, 36], Gallagher et al. experimentally measure an average increase of 15% [13], and August et al. find an average increase of 23% <ref> [1] </ref>. Inline replay does not change the code size at all, 15 but it requires special hardware to determine which instructionsof a potentially long sequencerequire re-execution. Assuming re-execution is not frequently required, the performance tradeoffs of these approaches are not obvious. <p> August et al. find that in the IMPACT architecture, recovery blocks increase the instruction cache miss rate by 40% on average versus inline replay, resulting in a 6% execution-time slowdown <ref> [1] </ref>. Re-execution can be initiated implicitly, explicitly, or spontaneously. Special instructions, such as sentinels, that check for a need to re-execute due to incorrect data dependence speculation or exception delay are provided to support explicit re-execution initiation.
Reference: [2] <author> Scott E. Breach, T. N. Vijaykumar, and Gurindar S. Sohi. </author> <title> The anatomy of the register file in a Multiscalar processor. </title> <booktitle> In Proceedings of the 27th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 181190, </pages> <address> San Jose, California, </address> <month> November 30December 2, </month> <year> 1994. </year>
Reference-contexts: Explicit synchronization mechanisms can be built from nonspeculative write instructions or special signal/wait instructions. Implicit synchronization and communication occur as side effects of other instructions. Note that an explicit communication mechanism can be built from an explicit synchronization mechanism. The Multiscalar architecture provides implicit register forwarding <ref> [2] </ref>, resulting in implicit synchro 16 nization and communication. The last assignment to a register in each thread is forwarded to younger threads. When younger threads read the same register, they will stall until the new value is received from an older thread.
Reference: [3] <author> Roger A. Bringmann, Scott A. Mahlke, Richard E. Hank, John C. Gyllenhaal, and Wen-mei W. Hwu. </author> <title> Speculative execution exception recovery using write-back suppression. </title> <booktitle> In Proceedings of the 26th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 21423, </pages> <address> Austin, Texas, </address> <month> December 13, </month> <year> 1993. </year>
Reference-contexts: ILS architectures rely on compilers to identify parallelism by statically scheduling a program's instructionssome of them speculativelyin an effort to co-locate independent instructions. The Boosting [35, 33, 36, 32] and IMPACT <ref> [5, 6, 3, 1] </ref> architectures are examples of ILS architectures. A thread-level speculative (TLS) architecture is a small-scale multiprocessor-on-a-chip augmented with support for the speculative execution of threads and for managing interthread dependences. A TLS architecture simultaneously executes different threads on independent processing elements, called nodes. <p> Encoding by path has the disadvantage that the architecture limits the maximum speculation distance, though Bringmann et al. find that limiting the distance to only seven is not a severe restriction for data dependence speculation <ref> [3] </ref>. Though path encoding appears to only encode control speculation, data dependence speculation can be piggybacked on control speculation. For example, control speculative memory operations can also be considered data dependence speculative with respect to any preceding memory operations they have moved beyond. <p> In addition, because exceptions are recorded in tags associated with registers, the target register of a speculative instruction must not be spilled. The register allocator must de-speculate speculative instructions that have their target spilled until the problem goes away. Bringmann et al. <ref> [3, 4] </ref> have proposed a modified form of the IMPACT architecture that uses write-back suppression to ease the register pressure caused by extending the live range of register to include a whole restartable interval.
Reference: [4] <author> Roger Alexander Bringmann. </author> <title> Enhancing Instruction Level Parallelism through Compiler-Controlled Speculation. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1995. </year> <month> 25 </month>
Reference-contexts: In addition, because exceptions are recorded in tags associated with registers, the target register of a speculative instruction must not be spilled. The register allocator must de-speculate speculative instructions that have their target spilled until the problem goes away. Bringmann et al. <ref> [3, 4] </ref> have proposed a modified form of the IMPACT architecture that uses write-back suppression to ease the register pressure caused by extending the live range of register to include a whole restartable interval.
Reference: [5] <author> Pohua P. Chang, Scott A. Mahlke, William Y. Chen, Nancy J. Warter, and Wen-mei W. Hwu. </author> <title> IMPACT: An ar-chitectural framework for multiple-instruction-issue processors. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture (ISCA-18), </booktitle> <pages> pages 26675, </pages> <address> Toronto, Ontario, Canada, </address> <month> May 2730, </month> <year> 1991. </year> <title> Published as Computer Architecture News, </title> <type> 19(3), </type> <month> May </month> <year> 1991. </year>
Reference-contexts: ILS architectures rely on compilers to identify parallelism by statically scheduling a program's instructionssome of them speculativelyin an effort to co-locate independent instructions. The Boosting [35, 33, 36, 32] and IMPACT <ref> [5, 6, 3, 1] </ref> architectures are examples of ILS architectures. A thread-level speculative (TLS) architecture is a small-scale multiprocessor-on-a-chip augmented with support for the speculative execution of threads and for managing interthread dependences. A TLS architecture simultaneously executes different threads on independent processing elements, called nodes. <p> Smith et al. show that this last method has performance comparable to the more general monopath approach on a machine with very limited parallel resources [33]. It is unlikely that this remains true on wider issues machines. Architectures that do not preserve register state, such as the IMPACT architecture <ref> [5] </ref>, relegate the responsibility to the compiler. Register pressure increases, because speculation extends the live ranges of registers; Section 5 clarifies the reasons for this. The hardware approach does not suffer from this problem, 11 because the hardware separates speculative and nonspeculative register state, effectively increasing the number of registers.
Reference: [6] <author> Pohua P. Chang, Nancy J. Warter, Scott A. Mahlke, William Y. Chen, and Wen-mei W. Hwu. </author> <title> Three architectural models for compiler-controlled speculative execution. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 44(4):48194, </volume> <month> April </month> <year> 1995. </year>
Reference-contexts: ILS architectures rely on compilers to identify parallelism by statically scheduling a program's instructionssome of them speculativelyin an effort to co-locate independent instructions. The Boosting [35, 33, 36, 32] and IMPACT <ref> [5, 6, 3, 1] </ref> architectures are examples of ILS architectures. A thread-level speculative (TLS) architecture is a small-scale multiprocessor-on-a-chip augmented with support for the speculative execution of threads and for managing interthread dependences. A TLS architecture simultaneously executes different threads on independent processing elements, called nodes. <p> The General Percolation variant of the IMPACT architecture takes this approach <ref> [6] </ref>, thus simplifying hardware at the expense of limiting parallelism. Mahlke et al. find a 7% loss of performance from prohibiting control speculative stores alone [23]. The simplest approach to preserving memory state in machines supporting monopath speculation is to use a modified store buffer. <p> August et al. find that 13% of transparent exceptions are spurious [1]. If a terminal exception is an indicator of an error, exceptions are only lost by programs that contain errors. The General Percolation variant of the IMPACT architecture does not preserve exception behavior <ref> [6] </ref>. An architecture that preserves exception behavior must delay exception handling for speculative instructions until the speculation has been resolved, at which time it is known whether the instruction is needed and correct. If it needed and correct, it is re-executed and the exception is handled. <p> Recovery blocks are separate from the body of the program, thus duplicating speculative instructions. When re-execution is necessary, the hardware executes a particular recovery block. An inline replay mechanism selectively re-executes instructions from a previous point in the code <ref> [1, 6, 23] </ref>. The latter is more space efficient, but it must fetch and potentially execute many more instructions than the former. Compilers for architectures that do not preserve state must ensure that all operands to potentially re-executed instructions are available at re-execution time. <p> width, window size, branch prediction accuracy) and find the crossover points between statically and dynamically scheduled 24 machines. 7 Conclusion Of the architectures and techniques that we have examined, which result in the best performance? Though a couple studies make direct performance comparisons of different TLS [15] and ILS architecture <ref> [6, 23] </ref>, the general trend is that additional architectural supportand the hardware that implements itmodestly improves performance. Computer architects must weigh their performance goals and resource budget when determining the appropriate degree of architectural support.
Reference: [7] <author> Brian L. Deitrich and Wen mei W. Hwu. </author> <title> Speculative hedge: Regulating compile-time speculation against profile variations. </title> <booktitle> In Proceedings of the 29th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 7079, </pages> <address> Paris, France, </address> <month> December 24, </month> <year> 1996. </year>
Reference-contexts: Trace scheduling optimizes only for the chosen trace, so it may result in very inefficient off trace code. This is particularly problematic when static analysis or profile information does not match a program's actual dynamic behavior. The schedulers of Smith et al. [33, 36] and Deitrich and Hwu <ref> [7] </ref> only speculate when it does not have a significant adverse effect on off trace code. Trace scheduling has a complex bookkeeping stage that patches off trace code to compensate for instruction movement within the trace.
Reference: [8] <author> Alain Deutsch. </author> <title> Interprocedural may-alias analysis for pointers: Beyond k-limiting. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on Programming Language Design and Implementation (PLDI '94), </booktitle> <pages> pages 230241, </pages> <address> Orlando, Florida, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Improved memory disambiguation can also improve performance. Synchronizing true dependences reduces thread squashing an restarting due to incorrect data dependence speculation, potentially improving performance. Static techniques for disambiguating array references [14, 24] and arbitrary pointers <ref> [21, 8] </ref> exist with varying degrees of success, but their impact on speculative architectures has not yet been studied.
Reference: [9] <author> Pradeep K. Dubey, Kevin O'Brien, Kathryn O'Brien, and Charles Barton. </author> <title> Single-program speculative mul-tithreading (SPSM) architecture: Compiler-assisted fine-grained multithreading. </title> <booktitle> In Proceedings of the 1995 Conference on Parallel Architectures and Compilation Techniques (PACT '95), </booktitle> <pages> pages 10921, </pages> <address> Limassol, Cyprus, </address> <month> June 2729, </month> <year> 1995. </year>
Reference-contexts: An architectural diagram that characterizes a generic TLS architecture appears in Figure 3. Franklin et al. introduced the first modern TLS architecture, called the expandable split window paradigm and later the Multiscalar architecture [11, 37]. Subsequent TLS architectures from Dubey et al. <ref> [9] </ref>, Tsai et al. [39, 22], Oplinger et al. [28, 16], and Steffan and Mowry [38] support speculation with varying degrees of hardware support. The next section introduces several fundamental concepts that will be used to explore the details of these architectures.
Reference: [10] <author> Joseph A. Fisher. </author> <title> Trace scheduling: A technique for global microcode compaction. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-30(7):478490, </volume> <month> July </month> <year> 1981. </year>
Reference-contexts: Nevertheless, a number of compilation issues for ILS and TLS architecture have been studied, and they are discussed below. 5.1 ILS Compilation Compilers for ILS architectures use variants of trace scheduling <ref> [10, 26] </ref> to discover and identify parallelism. A trace is a part of a likely path through a program, chosen based on static predictions or profile data of branch outcomes.
Reference: [11] <author> Manoj Franklin and Gurindar S. Sohi. </author> <title> The expandable split window paradigm for exploiting fine-grain parallelism. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture (ISCA-19), </booktitle> <pages> pages 5867, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1921, 1992. </year> <title> Published as Computer Architecture News, </title> <type> 20(2), </type> <month> May </month> <year> 1992. </year>
Reference-contexts: An architectural diagram that characterizes a generic TLS architecture appears in Figure 3. Franklin et al. introduced the first modern TLS architecture, called the expandable split window paradigm and later the Multiscalar architecture <ref> [11, 37] </ref>. Subsequent TLS architectures from Dubey et al. [9], Tsai et al. [39, 22], Oplinger et al. [28, 16], and Steffan and Mowry [38] support speculation with varying degrees of hardware support.
Reference: [12] <author> Manoj Franklin and Gurindar S. Sohi. ARB: </author> <title> A hardware mechanism for dynamic reordering of memory references. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 45(5):552571, </volume> <month> May </month> <year> 1996. </year>
Reference-contexts: When a speculation is resolved, its store buffer entries are either committed to the rest of the memory system or invalidated. A variant of the store buffer approach is used by ILS architectures and some TLS architectures <ref> [12, 16] </ref>. Oplinger et al. find that a TLS architecture can exploit a great deal of parallelism with only 300 bytes of buffer per node [28]. Most TLS architectures preserve memory state with local data caches in order to minimize reliance on a centralized structure. <p> Note that hardware that preserves state eliminates the need to track and recover from data dependence violations due to storage conflicts from output and anti-dependences. The Address Resolution Buffer (ARB) was designed for the TLS Multiscalar architecture <ref> [12] </ref>. It contains a number of queues. Each queue tracks memory references to a subset of the address space, allowing for faster operation and parallel access to different queues. Each queue reorders all the memory operations to its subset of the address space.
Reference: [13] <author> David M. Gallagher, William Y. Chen, Scott A. Mahlke, John C. Gyllenhaal, and Wen-mei W. Hwu. </author> <title> Dynamic memory disambiguation using the memory conflict buffer. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <pages> pages 18393, </pages> <address> San Jose, California, </address> <month> October 47, </month> <year> 1994. </year> <journal> Published as SIGPLAN Notices, </journal> <volume> 29(11), </volume> <month> November </month> <year> 1994. </year>
Reference-contexts: The ILS IMPACT architecture [23] uses sentinel encoding for control flow speculation, and it was later augmented to use a similar sentinel encoding for data dependence speculation via the memory conflict buffer (MCB) <ref> [13] </ref>. Path encoding explicitly indicates the control flow path that must be taken to reach a speculative instruction's home location. For example, the home location of the speculative instruction 'add.snnt' is 9 reached by not taking the next two branches and taking the third. This encoding supports polypath specula-tion. <p> At this time, dependence violations no longer need to be tracked and the cache line can be evicted. 14 The ILS IMPACT architecture has been extended to support data dependence speculation via the Mem--ory Conflict Buffer (MCB) <ref> [13] </ref>. It is unique in that it tracks dependence violations without preserving memory state, thus it only allows loads to be speculated with respect to stores. A data speculative load instruction may conflict with subsequent stores up to its corresponding sentinel instruction. A conflict bit is associated with each register. <p> Both these issues result in false conflict reports, which degrade performance. In practice, false conflicts are rare, representing only 1% of all conflicts <ref> [13] </ref>. 4.5 Re-executing Speculative Instructions Speculative instructions may be re-executed via either a recovery block or inline replay. A recovery block is a compiler generated sequence of instructions that may require re-execution [35, 13]. Recovery blocks are separate from the body of the program, thus duplicating speculative instructions. <p> In practice, false conflicts are rare, representing only 1% of all conflicts [13]. 4.5 Re-executing Speculative Instructions Speculative instructions may be re-executed via either a recovery block or inline replay. A recovery block is a compiler generated sequence of instructions that may require re-execution <ref> [35, 13] </ref>. Recovery blocks are separate from the body of the program, thus duplicating speculative instructions. When re-execution is necessary, the hardware executes a particular recovery block. An inline replay mechanism selectively re-executes instructions from a previous point in the code [1, 6, 23]. <p> Smith et al. argue that recovery blocks never increase code size beyond a factor of two [33, 36], Gallagher et al. experimentally measure an average increase of 15% <ref> [13] </ref>, and August et al. find an average increase of 23% [1]. Inline replay does not change the code size at all, 15 but it requires special hardware to determine which instructionsof a potentially long sequencerequire re-execution.
Reference: [14] <author> Gina Goff, Ken Kennedy, and Cheu-Wen Tseng. </author> <title> Practical dependence testing. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation (PLDI '91), </booktitle> <pages> pages 1529, </pages> <address> Toronto, Ontario, Canada, </address> <month> June 2628, </month> <year> 1991. </year>
Reference-contexts: Improved memory disambiguation can also improve performance. Synchronizing true dependences reduces thread squashing an restarting due to incorrect data dependence speculation, potentially improving performance. Static techniques for disambiguating array references <ref> [14, 24] </ref> and arbitrary pointers [21, 8] exist with varying degrees of success, but their impact on speculative architectures has not yet been studied.
Reference: [15] <author> Sridhar Gopal, T. N. Vijaykumar, James E. Smith, and Gurindar S. Sohi. </author> <title> Speculative versioning cache. </title> <booktitle> In Proceedings of the Fourth International Symposium on High-Performance Computer Architecture (HPCA-4), pages 195205, </booktitle> <address> Las Vegas, Nevada, </address> <month> February 14, </month> <year> 1998. </year>
Reference-contexts: Steffan and Mowry find that a 16KB two-way set-associative data cache with a small 4 entry victim cache eliminates nearly all node stalling due to conflicts [38]. Gopal et al. further refine this general approach <ref> [15] </ref>, adding hardware that prevents the need for the local cache to be purged whenever a node starts a new thread. As a result, the cache is warm and the write-back of speculative state does not happen all at once, flooding the memory system. <p> When a node stores to an address that has already been loaded by a younger node, a data dependence violation is signaled. Recent TLS architectures use a more distributed approach to detecting data dependence violation via a per-node data cache <ref> [38, 16, 15] </ref>. Cache lines are tagged when they are speculatively read, and store addresses are broadcast on a bus. When a node sees a store from an older node to an address that the current node has already speculatively read, a violation is signaled. <p> (e.g., clock rate, issue width, window size, branch prediction accuracy) and find the crossover points between statically and dynamically scheduled 24 machines. 7 Conclusion Of the architectures and techniques that we have examined, which result in the best performance? Though a couple studies make direct performance comparisons of different TLS <ref> [15] </ref> and ILS architecture [6, 23], the general trend is that additional architectural supportand the hardware that implements itmodestly improves performance. Computer architects must weigh their performance goals and resource budget when determining the appropriate degree of architectural support.
Reference: [16] <author> Lance Hammond, Mark Willey, and Kunle Olukotun. </author> <title> Data speculation support for chip multiprocessors. </title> <booktitle> In Proceedings of the Eighth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VIII), </booktitle> <address> San Jose, California, </address> <month> October 47, </month> <year> 1998. </year>
Reference-contexts: Franklin et al. introduced the first modern TLS architecture, called the expandable split window paradigm and later the Multiscalar architecture [11, 37]. Subsequent TLS architectures from Dubey et al. [9], Tsai et al. [39, 22], Oplinger et al. <ref> [28, 16] </ref>, and Steffan and Mowry [38] support speculation with varying degrees of hardware support. The next section introduces several fundamental concepts that will be used to explore the details of these architectures. <p> When a speculation is resolved, its store buffer entries are either committed to the rest of the memory system or invalidated. A variant of the store buffer approach is used by ILS architectures and some TLS architectures <ref> [12, 16] </ref>. Oplinger et al. find that a TLS architecture can exploit a great deal of parallelism with only 300 bytes of buffer per node [28]. Most TLS architectures preserve memory state with local data caches in order to minimize reliance on a centralized structure. <p> When a node stores to an address that has already been loaded by a younger node, a data dependence violation is signaled. Recent TLS architectures use a more distributed approach to detecting data dependence violation via a per-node data cache <ref> [38, 16, 15] </ref>. Cache lines are tagged when they are speculatively read, and store addresses are broadcast on a bus. When a node sees a store from an older node to an address that the current node has already speculatively read, a violation is signaled. <p> For procedure-based thread selection, before each procedure call, a thread is spawned to execute the code following the call. Hammond et al. <ref> [16] </ref> find the approach to be impractical, because there is insufficient parallelism between procedures and subsequent code. 5.2.2 Synchronization and Communication Vijaykumar describes compiler support for register forwarding [41] in the Multiscalar architecture, but the techniques are applicable to any architecture supporting register forwarding.
Reference: [17] <author> Wen-mei W. Hwu, Scott A. Mahlke, William Y. Chen, Pohua P. Chang, Nancy J. Warter, Roger A. Bringmann, Roland G. Ouellette, Richard E. Hank, Tokuzo Kiyohara, Grant E. Haab, John G. Holm, and Daniel M. Lavery. </author> <title> The superblock: An effective technique for VLIW and superscalar compilation. </title> <journal> Journal of Supercomputer, </journal> <volume> 7(1-2):22948, </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: It has not been demonstrated that this approach results in improvement versus the IMPACT architecture. 19 5.1.3 Enhancing Instruction Scheduling Hwu et al. <ref> [17] </ref> describe a number of techniques for enhancing the parallelism found by the above techniques. Trace enlargement via loop unrolling gives the scheduler more instructions from which to choose. The authors also describe a number of techniques for eliminating dependences which may constrain parallelism. <p> Trace scheduling has a complex bookkeeping stage that patches off trace code to compensate for instruction movement within the trace. Hwu et al. develop a new compiler structure called the superblock to mitigate this complexity <ref> [17] </ref>. The superblock is a trace that has had all its side entrances removed by a technique called tail duplication.
Reference: [18] <author> Quinn Jacobson, Steve Bennett, Nikhil Sharma, and James E. Smith. </author> <title> Control flow speculation in Multiscalar processors. </title> <booktitle> In Proceedings of the Third International Symposium on High Performance Architecture, </booktitle> <pages> pages 218229, </pages> <address> San Antonio, Texas, </address> <month> February 15, </month> <year> 1997. </year>
Reference-contexts: After thread selection, the compiler constructs a descriptor for each thread that contains the addresses of instructions that may follow the thread. The Multiscalar hardware is aware of these descriptors and uses them to schedule subsequent threads during program execution <ref> [18] </ref>. Rotenberg et al. move these compiler issues to the hardware in an architecture called a trace processor [31]. Loop-based thread selection assigns each iteration of a loop to a different thread.
Reference: [19] <author> Norman P. Jouppi and David W. Wall. </author> <title> Available instruction-level parallelism for superscalar and superpipelined machines. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-III), </booktitle> <pages> pages 27282, </pages> <address> Boston, Massachusetts, </address> <month> April 36, </month> <year> 1989. </year> <title> Published as Computer Architecture News, </title> <type> 17(2), </type> <month> April </month> <year> 1989. </year> <month> 26 </month>
Reference-contexts: Pipelined, superscalar and very large instruction word (VLIW) architectures are well known techniques for taking advantage of parallelism, but their scalability is limited by control flow and data dependences. Control flow divides a program's instructions into basic blocks that are typically very small and contain limited parallelism <ref> [19, 34] </ref>. Wall [43] and Lam and Wilson [20] show that greater parallelism exists between the instructions of different basic blocks.
Reference: [20] <author> Monica S. Lam and Robert P. Wilson. </author> <title> Limits of control flow on parallelism. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture (ISCA-19), </booktitle> <pages> pages 4657, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1921, 1992. </year> <title> Published as Computer Architecture News, </title> <type> 20(2), </type> <month> May </month> <year> 1992. </year>
Reference-contexts: Control flow divides a program's instructions into basic blocks that are typically very small and contain limited parallelism [19, 34]. Wall [43] and Lam and Wilson <ref> [20] </ref> show that greater parallelism exists between the instructions of different basic blocks.
Reference: [21] <author> William Landi and Barbara G. Ryder. </author> <title> A safe approximate algorithm for interprocedural pointer aliasing. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation (PLDI '92), </booktitle> <pages> pages 235248, </pages> <address> San Francisco, California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Improved memory disambiguation can also improve performance. Synchronizing true dependences reduces thread squashing an restarting due to incorrect data dependence speculation, potentially improving performance. Static techniques for disambiguating array references [14, 24] and arbitrary pointers <ref> [21, 8] </ref> exist with varying degrees of success, but their impact on speculative architectures has not yet been studied.
Reference: [22] <author> Zhiyuan Li, Jenn-Yuan Tsai, Xin Wang, Pen-Chung Yes, and Bess Zheng. </author> <title> Compiler techniques for concurrent multithreading with hardware speculation support. </title> <editor> In David Sehr, Uptal Banerjee, David Gelernter, Alex Nicolau, and David Padua, editors, </editor> <booktitle> Proceedings of the Ninth International Workshop on Languages and Compilers for Parallel Computing (LCPC '96), </booktitle> <pages> pages 17591, </pages> <address> San Jose, California, </address> <month> August </month> <year> 1996. </year> <note> Springer-Verlag. </note>
Reference-contexts: An architectural diagram that characterizes a generic TLS architecture appears in Figure 3. Franklin et al. introduced the first modern TLS architecture, called the expandable split window paradigm and later the Multiscalar architecture [11, 37]. Subsequent TLS architectures from Dubey et al. [9], Tsai et al. <ref> [39, 22] </ref>, Oplinger et al. [28, 16], and Steffan and Mowry [38] support speculation with varying degrees of hardware support. The next section introduces several fundamental concepts that will be used to explore the details of these architectures. <p> Oplinger et al. use profile information to decidein the presence of nested loopswhat loops should be parallelized [28]. The five reasons for performance degradation discussed above are all relevant in this context. The bulk of recent research takes the loop-based approach <ref> [22, 28, 38] </ref>, because a significant portion of execution time is spent in loops and the scheduling mechanism for parallelizing loops is very simple: scheduling typically occurs in software via a fork instruction that simply specifies the next loop iteration to execute.
Reference: [23] <author> Scott A. Mahlke, William Y. Chen, Roger A. Bringmann, Richard E. Hank, Wen-mei W. Hwu, B. Ramakrishna Rau, and Michael S. Schlansker. </author> <title> Sentinel scheduling: A model for compiler-controlled speculative execution. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4):376408, </volume> <month> November </month> <year> 1993. </year>
Reference-contexts: In this case, the sentinel is associated with the instruction by naming the target register of the speculative instruction. The ILS IMPACT architecture <ref> [23] </ref> uses sentinel encoding for control flow speculation, and it was later augmented to use a similar sentinel encoding for data dependence speculation via the memory conflict buffer (MCB) [13]. Path encoding explicitly indicates the control flow path that must be taken to reach a speculative instruction's home location. <p> Mahlke et al. find that a machine not preserving register state must have at least 48 registers to compete with a 32 register machine that does <ref> [23] </ref>. Though support for preserving register state effectively increases the number of registers, their use is restricted to preserving state. 4.2.2 Memory State Architectures that prohibit both control speculative stores and memory operation reordering do not need to preserve memory state. <p> The General Percolation variant of the IMPACT architecture takes this approach [6], thus simplifying hardware at the expense of limiting parallelism. Mahlke et al. find a 7% loss of performance from prohibiting control speculative stores alone <ref> [23] </ref>. The simplest approach to preserving memory state in machines supporting monopath speculation is to use a modified store buffer. The store buffer reorders stores according to their nonspeculative order. <p> When that home location is reached, all speculation is resolved, and re-execution is initiated if the exception tag associated with the current block is set. If the 13 home location is not reached, the exception is ignored because the speculation was incorrect. The IMPACT architecture encodes speculation via sentinels <ref> [23] </ref>, so the home location of a speculative instruction is not apparent from the instruction itself. As a result, a special exception tag associated with the target register of a speculative instruction is set when the instruction excepts, and the exception is ignored. <p> Recovery blocks are separate from the body of the program, thus duplicating speculative instructions. When re-execution is necessary, the hardware executes a particular recovery block. An inline replay mechanism selectively re-executes instructions from a previous point in the code <ref> [1, 6, 23] </ref>. The latter is more space efficient, but it must fetch and potentially execute many more instructions than the former. Compilers for architectures that do not preserve state must ensure that all operands to potentially re-executed instructions are available at re-execution time. <p> The address of the recover block or the first instruction for inline replay is either encoded in the instruction or in an auxiliary structure <ref> [23] </ref>. Existing instructions, such as branches, can implicitly initiate re-execution. These instructions examine auxiliary structures to see if replay is necessary and initiate it if it is. <p> Compilers that target architectures that do not preserve state must explicitly preserve operands to speculative instructions until the speculation is resolved. The instruction scheduler and register allocator must 18 work together to generate code that respects this constraint. The IMPACT compiler exemplifies this ap-proach <ref> [23] </ref>. First, the scheduler runs. It must ensure that the operands of all potentially re-executed instructions are not overwritten before speculation is resolved. The scheduler prevents an instruction from being speculatively executed before an instruction that overwrites any of its operands. <p> width, window size, branch prediction accuracy) and find the crossover points between statically and dynamically scheduled 24 machines. 7 Conclusion Of the architectures and techniques that we have examined, which result in the best performance? Though a couple studies make direct performance comparisons of different TLS [15] and ILS architecture <ref> [6, 23] </ref>, the general trend is that additional architectural supportand the hardware that implements itmodestly improves performance. Computer architects must weigh their performance goals and resource budget when determining the appropriate degree of architectural support.
Reference: [24] <author> Dror E. Maydan, John L. Hennessy, and Monica S. Lam. </author> <title> Efficient and exact data dependence analysis. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation (PLDI '91), </booktitle> <pages> pages 114, </pages> <address> Toronto, Ontario, Canada, </address> <month> June 2628, </month> <year> 1991. </year>
Reference-contexts: Improved memory disambiguation can also improve performance. Synchronizing true dependences reduces thread squashing an restarting due to incorrect data dependence speculation, potentially improving performance. Static techniques for disambiguating array references <ref> [14, 24] </ref> and arbitrary pointers [21, 8] exist with varying degrees of success, but their impact on speculative architectures has not yet been studied.
Reference: [25] <author> Andreas Moshovos, Scott E. Breach, T. N. Vijaykumar, and Gurindar S. Sohi. </author> <title> Dynamic speculation and synchronization of data dependences. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture (ISCA-24), </booktitle> <pages> pages 18193, </pages> <address> Denver, CO, </address> <month> June 24, </month> <year> 1997. </year> <title> Published as Computer Architecture News, </title> <type> 25(2), </type> <month> May </month> <year> 1997. </year>
Reference-contexts: Hardware speculative execution immediately benefits existing software, but its resource complexity may limit its success, particularly when using sophisticated heuristics to direct and limit speculation <ref> [25] </ref>. Management of an instruction window of size n is regarded as an O (n 2 ) endeavor, and studies suggest that an instruction window must be very large to exploit a significant amount of parallelism [43].
Reference: [26] <author> Alexandru Nicolau. </author> <title> Percolation scheduling: A parallel compilation technique. </title> <type> Technical report, </type> <institution> Cornell University Department of Computer Science, </institution> <type> TR 85-678, </type> <month> May </month> <year> 1985. </year>
Reference-contexts: Nevertheless, a number of compilation issues for ILS and TLS architecture have been studied, and they are discussed below. 5.1 ILS Compilation Compilers for ILS architectures use variants of trace scheduling <ref> [10, 26] </ref> to discover and identify parallelism. A trace is a part of a likely path through a program, chosen based on static predictions or profile data of branch outcomes.
Reference: [27] <author> Alexandru Nicolau. </author> <title> Run-time disambiguation: Copying with statically unpredictable dependences. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(5):663678, </volume> <month> May </month> <year> 1989. </year>
Reference-contexts: In addition, because it makes static speculation decisions, software assisted speculative execution is more sensitive than a hardware approach to dynamic variations in program behavior. Though limited forms of software assisted speculative execution have been proposed without specific architectural and hardware support <ref> [27, 30] </ref>, their success is limited by the overhead of verifying correctness of speculation. This report identifies the fundamental architectural, implementation, and compiler issues of software assisted speculative execution. Proposed architectures from the literature are evaluated with respect to these issues. This report is organized as follows.
Reference: [28] <author> Jeffrey Oplinger, David Heine, Shih-Wei Liao, Basem A. Nayfeh, Monica S. Lam, and Kunle Olukotun. </author> <title> Software and hardware for exploiting speculative parallelism with a multiprocessor. </title> <type> Technical Report CSL-TR-97-715, </type> <institution> Stanford University Computer Systems Lab, </institution> <month> February </month> <year> 1997. </year>
Reference-contexts: Franklin et al. introduced the first modern TLS architecture, called the expandable split window paradigm and later the Multiscalar architecture [11, 37]. Subsequent TLS architectures from Dubey et al. [9], Tsai et al. [39, 22], Oplinger et al. <ref> [28, 16] </ref>, and Steffan and Mowry [38] support speculation with varying degrees of hardware support. The next section introduces several fundamental concepts that will be used to explore the details of these architectures. <p> The TLS Multiscalar architecture [37] includes, in the program, explicit thread descriptors defining each thread and their relationships (i.e., the potential successor of each thread). Other TLS architectures <ref> [39, 28, 38] </ref> simply use a special fork instruction to spawn new threads. In either case, threads are started in proper execution order so the hardware can track the speculation. Thread encoding supports only monopath speculation, and it can not be made to work with ILS architectures. <p> A variant of the store buffer approach is used by ILS architectures and some TLS architectures [12, 16]. Oplinger et al. find that a TLS architecture can exploit a great deal of parallelism with only 300 bytes of buffer per node <ref> [28] </ref>. Most TLS architectures preserve memory state with local data caches in order to minimize reliance on a centralized structure. They use variants of cache coherence protocols so that writes becomes visible to younger nodes but are hidden from older nodes. <p> Loop-based thread selection assigns each iteration of a loop to a different thread. Unfortunately, portions of code that do not contain loops amenable to speculative parallelization do not benefit from this approach. Oplinger et al. use profile information to decidein the presence of nested loopswhat loops should be parallelized <ref> [28] </ref>. The five reasons for performance degradation discussed above are all relevant in this context. <p> Oplinger et al. use profile information to decidein the presence of nested loopswhat loops should be parallelized [28]. The five reasons for performance degradation discussed above are all relevant in this context. The bulk of recent research takes the loop-based approach <ref> [22, 28, 38] </ref>, because a significant portion of execution time is spent in loops and the scheduling mechanism for parallelizing loops is very simple: scheduling typically occurs in software via a fork instruction that simply specifies the next loop iteration to execute. <p> Static techniques for disambiguating array references [14, 24] and arbitrary pointers [21, 8] exist with varying degrees of success, but their impact on speculative architectures has not yet been studied. Profiling is also useful for identifying operations that frequently result in misspeculation, which would benefit from synchronization <ref> [28] </ref>. 6 Research Directions As a relatively recent advent, software assisted speculative execution permits many opportunities for research.
Reference: [29] <institution> Trimaran Project. Trimaran project homepage. </institution> <note> http:/www.trimaran.org. </note>
Reference-contexts: Hewlett-Packard, the IMPACT group from the University of Illinois at Urbana-Champaign, and the ReaCT-ILP group from New York University have established the Trimaran project to develop an infrastructure for 22 research in instruction-level parallelism <ref> [29] </ref>. This enabling project is a step in the right direction. 6.1 Compiler/Resource Mismatch A deficit of ILS architectures is that they expose resource availability, such as issue width, to the static scheduler in the compiler.
Reference: [30] <author> Lawrence Rauchwerger and David Padua. </author> <title> The LRPD test: Speculative run-time parallelization of loops with privatization and reduction parallelization. </title> <booktitle> In Proceedings of the ACM SIGPLAN '95 Conference on Programming Language Design and Implementation (PLDI '95), </booktitle> <address> La Jolla, California, </address> <month> June 1821, </month> <year> 1995. </year> <journal> Published as SIGPLAN Notices, </journal> <volume> 30(6), </volume> <month> June </month> <year> 1995. </year>
Reference-contexts: In addition, because it makes static speculation decisions, software assisted speculative execution is more sensitive than a hardware approach to dynamic variations in program behavior. Though limited forms of software assisted speculative execution have been proposed without specific architectural and hardware support <ref> [27, 30] </ref>, their success is limited by the overhead of verifying correctness of speculation. This report identifies the fundamental architectural, implementation, and compiler issues of software assisted speculative execution. Proposed architectures from the literature are evaluated with respect to these issues. This report is organized as follows.
Reference: [31] <author> Eric Rotenberg, Quinn Jacobson, Yiannakis Sazeides, and Jim Smith. </author> <title> Trace processors. </title> <booktitle> In Proceedings of the 30th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 13848, </pages> <institution> Research Triangle Park, North Carolina, </institution> <month> December 13, </month> <year> 1997. </year>
Reference-contexts: The Multiscalar hardware is aware of these descriptors and uses them to schedule subsequent threads during program execution [18]. Rotenberg et al. move these compiler issues to the hardware in an architecture called a trace processor <ref> [31] </ref>. Loop-based thread selection assigns each iteration of a loop to a different thread. Unfortunately, portions of code that do not contain loops amenable to speculative parallelization do not benefit from this approach.
Reference: [32] <author> Michael D. Smith. </author> <title> The Interaction of Compilation Technology and Computer Architecture, chapter Architectural Support for Compile-Time Speculation, pages 1349. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, Massachusetts, </address> <year> 1994. </year>
Reference-contexts: ILS architectures rely on compilers to identify parallelism by statically scheduling a program's instructionssome of them speculativelyin an effort to co-locate independent instructions. The Boosting <ref> [35, 33, 36, 32] </ref> and IMPACT [5, 6, 3, 1] architectures are examples of ILS architectures. A thread-level speculative (TLS) architecture is a small-scale multiprocessor-on-a-chip augmented with support for the speculative execution of threads and for managing interthread dependences. <p> These instructions examine auxiliary structures to see if replay is necessary and initiate it if it is. The re-execution address is either held in the same auxiliary structure or it is kept in a table indexed by the address of the implicit initiation instruction <ref> [32] </ref>. On some TLS architectures, particular instructions do not initiate re-execution. <p> A standard trace scheduling algorithm can then be used, but speculative instructions must have their speculation depth encoded in them. The scheduler does not need to ensure that speculative instruction operands are preserved, because the hardware guarantees this. The scheduling algorithm developed by Smith et al. <ref> [33, 36, 32] </ref> for the Boosting architecture takes this approach. Compilers that target architectures that do not preserve state must explicitly preserve operands to speculative instructions until the speculation is resolved. The instruction scheduler and register allocator must 18 work together to generate code that respects this constraint.
Reference: [33] <author> Michael D. Smith, Mark Horowitz, and Monica S. Lam. </author> <title> Efficient superscalar performance through boosting. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <pages> pages 24859, </pages> <address> Boston, Massachusetts, </address> <month> October 1215, </month> <year> 1992. </year> <journal> Published as SIGPLAN Notices, </journal> <volume> 27(9), </volume> <month> September </month> <year> 1992. </year>
Reference-contexts: ILS architectures rely on compilers to identify parallelism by statically scheduling a program's instructionssome of them speculativelyin an effort to co-locate independent instructions. The Boosting <ref> [35, 33, 36, 32] </ref> and IMPACT [5, 6, 3, 1] architectures are examples of ILS architectures. A thread-level speculative (TLS) architecture is a small-scale multiprocessor-on-a-chip augmented with support for the speculative execution of threads and for managing interthread dependences. <p> This approach simulates the general monopath approach with less hardware. Smith et al. show that this last method has performance comparable to the more general monopath approach on a machine with very limited parallel resources <ref> [33] </ref>. It is unlikely that this remains true on wider issues machines. Architectures that do not preserve register state, such as the IMPACT architecture [5], relegate the responsibility to the compiler. Register pressure increases, because speculation extends the live ranges of registers; Section 5 clarifies the reasons for this. <p> While recovery blocks require no special hardware, they consume a significant amount of instruction memory, proportional to the number of speculative instructions. Smith et al. argue that recovery blocks never increase code size beyond a factor of two <ref> [33, 36] </ref>, Gallagher et al. experimentally measure an average increase of 15% [13], and August et al. find an average increase of 23% [1]. Inline replay does not change the code size at all, 15 but it requires special hardware to determine which instructionsof a potentially long sequencerequire re-execution. <p> A standard trace scheduling algorithm can then be used, but speculative instructions must have their speculation depth encoded in them. The scheduler does not need to ensure that speculative instruction operands are preserved, because the hardware guarantees this. The scheduling algorithm developed by Smith et al. <ref> [33, 36, 32] </ref> for the Boosting architecture takes this approach. Compilers that target architectures that do not preserve state must explicitly preserve operands to speculative instructions until the speculation is resolved. The instruction scheduler and register allocator must 18 work together to generate code that respects this constraint. <p> Trace scheduling optimizes only for the chosen trace, so it may result in very inefficient off trace code. This is particularly problematic when static analysis or profile information does not match a program's actual dynamic behavior. The schedulers of Smith et al. <ref> [33, 36] </ref> and Deitrich and Hwu [7] only speculate when it does not have a significant adverse effect on off trace code. Trace scheduling has a complex bookkeeping stage that patches off trace code to compensate for instruction movement within the trace.
Reference: [34] <author> Michael D. Smith, Mike Johnson, and Mark A. Horowitz. </author> <title> Limits on multiple instruction issue. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-III), </booktitle> <pages> pages 290302, </pages> <address> Boston, Massachusetts, </address> <month> April 36, </month> <year> 1989. </year> <title> Published as Computer Architecture News, </title> <type> 17(2), </type> <month> April </month> <year> 1989. </year> <month> 27 </month>
Reference-contexts: Pipelined, superscalar and very large instruction word (VLIW) architectures are well known techniques for taking advantage of parallelism, but their scalability is limited by control flow and data dependences. Control flow divides a program's instructions into basic blocks that are typically very small and contain limited parallelism <ref> [19, 34] </ref>. Wall [43] and Lam and Wilson [20] show that greater parallelism exists between the instructions of different basic blocks.
Reference: [35] <author> Michael D. Smith, Monica S. Lam, and Mark A. Horowitz. </author> <title> Boosting beyond static scheduling in a superscalar processor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture (ISCA-17), </booktitle> <pages> pages 34454, </pages> <address> Seattle, WA, </address> <month> May 2831, </month> <year> 1990. </year>
Reference-contexts: ILS architectures rely on compilers to identify parallelism by statically scheduling a program's instructionssome of them speculativelyin an effort to co-locate independent instructions. The Boosting <ref> [35, 33, 36, 32] </ref> and IMPACT [5, 6, 3, 1] architectures are examples of ILS architectures. A thread-level speculative (TLS) architecture is a small-scale multiprocessor-on-a-chip augmented with support for the speculative execution of threads and for managing interthread dependences. <p> For example, control speculative memory operations can also be considered data dependence speculative with respect to any preceding memory operations they have moved beyond. The ILS Boosting architecture uses path encoding for control speculation <ref> [35] </ref>. 4.2 Preserving State Architectural support for preserving state allows an instructions to be freely speculated without regard for its register and memory side effects. <p> The ILS Boosting architecture preserves register state via a fixed number of shadow register files <ref> [35] </ref>. 10 add v1,v2,v3 div v3,v5,v6 L1: add v7,v8,v3 div.s1 v3,v5,v6 beq.n v4,0,L1 L1: add v7,v8,v3 div.s1 v30,v5,v6 beq.n v4,0,L1 . . . (c) (b) control speculation in an architecture that preserves state, and (c) control speculation in an architecture not preserving register state. <p> At the time of speculation resolution, if the excepting instruction should have executed, the excepting instruction and all dependent instructions are re-executed, as described in a later section. Two methods of exception delay are discussed below. Because the Boosting architecture <ref> [35] </ref> uses a path encoding of speculation, when a speculative instruction excepts, the hardware sets an exception tag in the shadow register file associated with the home location of the instruction and ignores the exception. <p> In practice, false conflicts are rare, representing only 1% of all conflicts [13]. 4.5 Re-executing Speculative Instructions Speculative instructions may be re-executed via either a recovery block or inline replay. A recovery block is a compiler generated sequence of instructions that may require re-execution <ref> [35, 13] </ref>. Recovery blocks are separate from the body of the program, thus duplicating speculative instructions. When re-execution is necessary, the hardware executes a particular recovery block. An inline replay mechanism selectively re-executes instructions from a previous point in the code [1, 6, 23].
Reference: [36] <author> Michael David Smith. </author> <title> Support for Speculative Execution in High-Performance Processors. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: ILS architectures rely on compilers to identify parallelism by statically scheduling a program's instructionssome of them speculativelyin an effort to co-locate independent instructions. The Boosting <ref> [35, 33, 36, 32] </ref> and IMPACT [5, 6, 3, 1] architectures are examples of ILS architectures. A thread-level speculative (TLS) architecture is a small-scale multiprocessor-on-a-chip augmented with support for the speculative execution of threads and for managing interthread dependences. <p> While recovery blocks require no special hardware, they consume a significant amount of instruction memory, proportional to the number of speculative instructions. Smith et al. argue that recovery blocks never increase code size beyond a factor of two <ref> [33, 36] </ref>, Gallagher et al. experimentally measure an average increase of 15% [13], and August et al. find an average increase of 23% [1]. Inline replay does not change the code size at all, 15 but it requires special hardware to determine which instructionsof a potentially long sequencerequire re-execution. <p> A standard trace scheduling algorithm can then be used, but speculative instructions must have their speculation depth encoded in them. The scheduler does not need to ensure that speculative instruction operands are preserved, because the hardware guarantees this. The scheduling algorithm developed by Smith et al. <ref> [33, 36, 32] </ref> for the Boosting architecture takes this approach. Compilers that target architectures that do not preserve state must explicitly preserve operands to speculative instructions until the speculation is resolved. The instruction scheduler and register allocator must 18 work together to generate code that respects this constraint. <p> Trace scheduling optimizes only for the chosen trace, so it may result in very inefficient off trace code. This is particularly problematic when static analysis or profile information does not match a program's actual dynamic behavior. The schedulers of Smith et al. <ref> [33, 36] </ref> and Deitrich and Hwu [7] only speculate when it does not have a significant adverse effect on off trace code. Trace scheduling has a complex bookkeeping stage that patches off trace code to compensate for instruction movement within the trace.
Reference: [37] <author> Gurindar S. Sohi, Scott E. Breach, and T. N. Vijaykumar. </author> <title> Multiscalar processors. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture (ISCA-22), </booktitle> <pages> pages 414425, </pages> <address> Santa Margherita Ligure, Italy, </address> <month> June 2224, </month> <year> 1995. </year>
Reference-contexts: An architectural diagram that characterizes a generic TLS architecture appears in Figure 3. Franklin et al. introduced the first modern TLS architecture, called the expandable split window paradigm and later the Multiscalar architecture <ref> [11, 37] </ref>. Subsequent TLS architectures from Dubey et al. [9], Tsai et al. [39, 22], Oplinger et al. [28, 16], and Steffan and Mowry [38] support speculation with varying degrees of hardware support. <p> The instructions in each thread are potentially speculatively executed with 8 1 3 add.s v1,v2,v3 chk v1 L1: . . . add.s1 v1,v2,v3 . . . (c) respect to the instructions in older threads. The TLS Multiscalar architecture <ref> [37] </ref> includes, in the program, explicit thread descriptors defining each thread and their relationships (i.e., the potential successor of each thread). Other TLS architectures [39, 28, 38] simply use a special fork instruction to spawn new threads. <p> On some TLS architectures, particular instructions do not initiate re-execution. Instead, when the data dependence speculation tracking hardware detects a violation, it spontaneously initiates re-execution in the appropriate thread <ref> [37] </ref>. 4.6 Synchronization Where true data dependences exist and are statically manifest between threads of a TLS architecture, it is important to synchronize the threads to prevent incorrect data dependence speculation.
Reference: [38] <author> J. Gregory Steffan and Todd C. Mowry. </author> <title> The potential for using thread-level data speculation to facilitate automatic parallelization. </title> <booktitle> In Proceedings of the Fourth International Symposium on High-Performance Computer Architecture (HPCA-4), </booktitle> <pages> pages 213, </pages> <address> Las Vegas, Nevada, </address> <month> February 14, </month> <year> 1998. </year>
Reference-contexts: Franklin et al. introduced the first modern TLS architecture, called the expandable split window paradigm and later the Multiscalar architecture [11, 37]. Subsequent TLS architectures from Dubey et al. [9], Tsai et al. [39, 22], Oplinger et al. [28, 16], and Steffan and Mowry <ref> [38] </ref> support speculation with varying degrees of hardware support. The next section introduces several fundamental concepts that will be used to explore the details of these architectures. <p> The TLS Multiscalar architecture [37] includes, in the program, explicit thread descriptors defining each thread and their relationships (i.e., the potential successor of each thread). Other TLS architectures <ref> [39, 28, 38] </ref> simply use a special fork instruction to spawn new threads. In either case, threads are started in proper execution order so the hardware can track the speculation. Thread encoding supports only monopath speculation, and it can not be made to work with ILS architectures. <p> As a result, speculative state can not be evicted from the cache until the node becomes nonspeculative. Steffan and Mowry find that a 16KB two-way set-associative data cache with a small 4 entry victim cache eliminates nearly all node stalling due to conflicts <ref> [38] </ref>. Gopal et al. further refine this general approach [15], adding hardware that prevents the need for the local cache to be purged whenever a node starts a new thread. <p> When a node stores to an address that has already been loaded by a younger node, a data dependence violation is signaled. Recent TLS architectures use a more distributed approach to detecting data dependence violation via a per-node data cache <ref> [38, 16, 15] </ref>. Cache lines are tagged when they are speculatively read, and store addresses are broadcast on a bus. When a node sees a store from an older node to an address that the current node has already speculatively read, a violation is signaled. <p> Oplinger et al. use profile information to decidein the presence of nested loopswhat loops should be parallelized [28]. The five reasons for performance degradation discussed above are all relevant in this context. The bulk of recent research takes the loop-based approach <ref> [22, 28, 38] </ref>, because a significant portion of execution time is spent in loops and the scheduling mechanism for parallelizing loops is very simple: scheduling typically occurs in software via a fork instruction that simply specifies the next loop iteration to execute. <p> In addition, the compiler must insert synchronization to stall instructions that can not be speculated, such as system calls and I/O instructions. 5.2.3 Enhancing Parallelism A TLS compiler can enhance parallelism by eliminating certain classes of dependences, such as those due to induction variables and reductions <ref> [38, 41] </ref>. In addition, dependences between instances of certain library routines can be eliminated by rewriting the library [38]. For example, the C function fgetc (stream) reads the next character at the current position in stream, and it advances the current position. <p> In addition, dependences between instances of certain library routines can be eliminated by rewriting the library <ref> [38] </ref>. For example, the C function fgetc (stream) reads the next character at the current position in stream, and it advances the current position. There is a true data dependence between successive calls to this function through its argument.
Reference: [39] <author> Jenn-Yuan Tsai and Pen-Chung Yew. </author> <title> The superthreaded architecture: Thread pipelining with run-time data dependence checking and control speculation. </title> <booktitle> In Proceedings of the 1996 Conference on Parallel Architectures and Compilation Techniques (PACT '96), </booktitle> <pages> pages 3546, </pages> <address> Boston, MA, </address> <month> October </month> <year> 2023, 1996. </year>
Reference-contexts: An architectural diagram that characterizes a generic TLS architecture appears in Figure 3. Franklin et al. introduced the first modern TLS architecture, called the expandable split window paradigm and later the Multiscalar architecture [11, 37]. Subsequent TLS architectures from Dubey et al. [9], Tsai et al. <ref> [39, 22] </ref>, Oplinger et al. [28, 16], and Steffan and Mowry [38] support speculation with varying degrees of hardware support. The next section introduces several fundamental concepts that will be used to explore the details of these architectures. <p> The TLS Multiscalar architecture [37] includes, in the program, explicit thread descriptors defining each thread and their relationships (i.e., the potential successor of each thread). Other TLS architectures <ref> [39, 28, 38] </ref> simply use a special fork instruction to spawn new threads. In either case, threads are started in proper execution order so the hardware can track the speculation. Thread encoding supports only monopath speculation, and it can not be made to work with ILS architectures. <p> Other than conciseness of code, there is no benefit of this approach over explicit communication, because they both require the same static register use analysis. A TLS architecture proposed by Tsai and Yew provides a similar implicit mechanism for forwarding values in memory <ref> [39] </ref>. At the beginning of every thread, special instructions identify the memory addresses, called target store addresses, that the current thread writes that subsequent threads may read. Each node forwards its target store addresses to subsequent nodes when they are spawned.
Reference: [40] <author> Dean Tullsen, Susan Eggers, and Hank Levy. </author> <title> Simultaneous multithreading: Maximizing on-chip parallelism. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture (ISCA-22), </booktitle> <pages> pages 392403, </pages> <address> Santa Margherita Ligure, Italy, </address> <month> June 2224, </month> <year> 1995. </year>
Reference-contexts: As a result, it is difficult to justify high issue machines when they mostly go under utilized. I propose introducing simultaneous miltithreading (SMT) <ref> [40] </ref> ideas into the software speculative execution arena. Such a union would provide both the single application benefits of ILS architectures and the throughput benefits of the SMT architecture.
Reference: [41] <author> T. N. Vijaykumar. </author> <title> Compiling for the Multiscalar Architecture. </title> <type> PhD thesis, </type> <institution> University of WisconsinMadison, </institution> <year> 1998. </year>
Reference-contexts: The CFG-based approach is the most general, so we discuss it first. Research in compiling for the Multiscalar architecture develops the CFG-based approach to thread selection <ref> [41, 42] </ref>. The nodes of the CFG are partitioned into threads in an effort to minimize the following 20 reasons for performance degradation: (i) control flow misspeculation, (ii) inter-thread true data depen-dences, 1 (iii) memory dependence misspeculation, (iv) load imbalance, and (v) task overhead. <p> Hammond et al. [16] find the approach to be impractical, because there is insufficient parallelism between procedures and subsequent code. 5.2.2 Synchronization and Communication Vijaykumar describes compiler support for register forwarding <ref> [41] </ref> in the Multiscalar architecture, but the techniques are applicable to any architecture supporting register forwarding. For each thread, the compiler must first identify what registers need to be forwarded. Conservatively, this can be all registers written in the thread. <p> In addition, the compiler must insert synchronization to stall instructions that can not be speculated, such as system calls and I/O instructions. 5.2.3 Enhancing Parallelism A TLS compiler can enhance parallelism by eliminating certain classes of dependences, such as those due to induction variables and reductions <ref> [38, 41] </ref>. In addition, dependences between instances of certain library routines can be eliminated by rewriting the library [38]. For example, the C function fgetc (stream) reads the next character at the current position in stream, and it advances the current position.
Reference: [42] <author> T. N. Vijaykumar and Gurindar S. Sohi. </author> <title> Task selection for a Multiscalar processor. </title> <booktitle> In Proceedings of the 31st Annual International Symposium on Microarchitecture, </booktitle> <month> December </month> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: The CFG-based approach is the most general, so we discuss it first. Research in compiling for the Multiscalar architecture develops the CFG-based approach to thread selection <ref> [41, 42] </ref>. The nodes of the CFG are partitioned into threads in an effort to minimize the following 20 reasons for performance degradation: (i) control flow misspeculation, (ii) inter-thread true data depen-dences, 1 (iii) memory dependence misspeculation, (iv) load imbalance, and (v) task overhead.
Reference: [43] <author> David W. Wall. </author> <title> Limits of instruction-level parallelism. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), </booktitle> <pages> pages 17688, </pages> <address> Santa Clara, California, </address> <month> April 811, </month> <year> 1991. </year> <journal> Published as SIGPLAN Notices, </journal> <volume> 26(4), </volume> <month> April </month> <year> 1991. </year>
Reference-contexts: Control flow divides a program's instructions into basic blocks that are typically very small and contain limited parallelism [19, 34]. Wall <ref> [43] </ref> and Lam and Wilson [20] show that greater parallelism exists between the instructions of different basic blocks. <p> Management of an instruction window of size n is regarded as an O (n 2 ) endeavor, and studies suggest that an instruction window must be very large to exploit a significant amount of parallelism <ref> [43] </ref>. Furthermore, hardware speculation is resource inefficient in that a significant portion of the implementation logic is devoted to scheduling instructions, rather than executing them. Alternatively, speculation can be encoded in a program itself.
Reference: [44] <author> Michael Wolfe. </author> <title> High Performance Compilers for Parallel Computing. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, Califor-nia, </address> <year> 1996. </year> <month> 28 </month>
Reference-contexts: The final two sections propose new research directions and give conclusions. 2 Foundations This section introduces basic terminology, the types of speculative execution, the classes of speculative architectures, and a few core architectural issues. 2.1 Terminology Data dependence relations describe ordering constraints that must be preserved between instructions <ref> [44, pages 137138] </ref>. A true data dependence exists between two instructions when the first produces a value read by the other. An anti-dependence exists between two instructions when the first reads a location written by the second. <p> A control dependence also describes an ordering relationship among instructions. Informally, an instruction, i 2 , is control dependent on a conditional branch instruction, i 1 , if one branch of i 1 always leads to i 2 and the other may not <ref> [44, pages 7179] </ref>. In other words, instruction i 1 determines whether or not instruction i 2 may need to be executed. An instruction, i 2 , is speculative when it is executed without regard for an apparent data or control dependence from a prior instruction, i 1 .
References-found: 44

