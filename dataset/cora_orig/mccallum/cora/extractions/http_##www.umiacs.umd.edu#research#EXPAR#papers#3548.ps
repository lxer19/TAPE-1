URL: http://www.umiacs.umd.edu/research/EXPAR/papers/3548.ps
Refering-URL: http://www.umiacs.umd.edu/research/EXPAR/papers/3548.html
Root-URL: 
Email: fdbader, helman, josephg@umiacs.umd.edu  
Title: Practical Parallel Algorithms for Personalized Communication and Integer Sorting  
Author: David A. Bader David R. Helman Joseph JaJa 
Keyword: Parallel Algorithms, Personalized Communication, Integer Sorting, Radix Sort, Communication Primitives, Routing h-Relations, Parallel Performance.  
Date: December 12, 1995  
Address: College Park, MD 20742  
Affiliation: Institute for Advanced Computer Studies, and Department of Electrical Engineering, University of Maryland,  
Abstract: A fundamental challenge for parallel computing is to obtain high-level, architecture independent, algorithms which efficiently execute on general-purpose parallel machines. With the emergence of message passing standards such as MPI, it has become easier to design efficient and portable parallel algorithms by making use of these communication primitives. While existing primitives allow an assortment of collective communication routines, they do not handle an important communication event when most or all processors have non-uniformly sized personalized messages to exchange with each other. We focus in this paper on the h-relation personalized communication whose efficient implementation will allow high performance implementations of a large class of algorithms. While most previous h-relation algorithms use randomization, this paper presents a new deterministic approach for h-relation personalized communication with asymptoticaly optimal complexity for h p 2 . As an application, we present an efficient algorithm for stable integer sorting. The algorithms presented in this paper have been coded in Split-C and run on a variety of platforms, including the Thinking Machines CM-5, IBM SP-1 and SP-2, Cray Research T3D, Meiko Scientific CS-2, and the Intel Paragon. Our experimental results are consistent with the theoretical analysis and illustrate the scalability and efficiency of our algorithms across different platforms. In fact, they seem to outperform all similar algorithms known to the authors on these platforms. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. Abali, F. Ozguner, and A. Bataineh. </author> <title> Balanced Parallel Sort on Hypercube Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(5) </volume> <pages> 572-581, </pages> <year> 1993. </year>
Reference-contexts: In particular, in his "bridging model" for parallel computation, Valiant has identified the h-relation personalized communication as the basis for organizing communication between two consecutive major computation steps. Previous parallel algorithms for personalized communication (typically for a hypercube, e.g. <ref> [28, 40, 37, 12, 13, 10, 1] </ref>, a mesh, e.g. [24, 39, 29, 14, 25], or other circuit switched network machines, e.g. [34, 19, 32, 38]) tend to be network or machine dependent, and thus not efficient when ported to current parallel machines. <p> Fast integer sorting is crucial for solving problems in many domains, and, as such, is used as a kernel in several parallel benchmarks such as NAS 1 [8] and SPLASH [46]. Because of the extensive and irregular communication requirements, previous parallel algorithms for sorting (a hypercube, e.g. <ref> [11, 1] </ref>, or a mesh, e.g. [21, 31]) tend to be network or machine dependent, and therefore not efficient across current parallel machines. In this paper, we present an algorithm for integer sorting which couples the well known parallel radix sort algorithm together with our algorithm for personalized communication.
Reference: [2] <author> A. Alexandrov, M. Ionescu, K. Schauser, and C. Scheiman. LogGP: </author> <title> Incorporating Long Messages into the LogP model One step closer towards a realistic model for parallel computation. </title> <booktitle> In 7th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 95-105, </pages> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: The coefficient of t gives the total number of times collective communication primitives are used, and the coefficient of gives the maximum total amount of data exchanged between a processor and the remaining processors. This communication model is close to a number of similar models (e.g. <ref> [18, 44, 2] </ref>) that have recently appeared in the literature and seems to be well-suited for designing parallel algorithms on current high performance platforms. We define the computation time T comp (n; p) as the maximum time it takes a processor to perform 3 all the local computation steps. <p> Performance of the latter code, which had been optimized for the Meiko CS-2, is given in <ref> [2] </ref>. Note that the AIS implementation is based upon the original version by Dusseau ([20, 18]). Also, all codes in this comparison have been written in the Split-C language [17]. Our algorithm is referred to as BHJ.
Reference: [3] <author> R.H. Arpaci, D.E. Culler, A. Krishnamurthy, S.G. Steinberg, and K. Yelick. </author> <title> Empirical Evaluation of the CRAY-T3D: A Compiler Perspective. </title> <publisher> In ACM Press, </publisher> <editor> editor, </editor> <booktitle> Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 320-331, </pages> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: The Meiko CS-2 Computing Facility was acquired through NSF CISE Infrastructure Grant number CDA-9218202, with support from the College of Engineering and the UCSB Office of Research, for research in parallel computing. Arvind Krishnamurthy provided additional help with his port of Split-C to the Cray Research T3D <ref> [3] </ref>. The Jet Propulsion Lab/Caltech 256-node Cray T3D Supercomputer used in this investigation was provided by funding from the NASA Offices of Mission to Planet Earth, Aeronautics, and Space Science.
Reference: [4] <author> D. Bader. </author> <title> Randomized and Deterministic Routing Algorithms for h-Relations. ENEE 648X Class Report, </title> <address> April 1, </address> <year> 1994. </year>
Reference: [5] <author> D. A. Bader and J. JaJa. </author> <title> Parallel Algorithms for Image Histogramming and Connected Components with an Experimental Study. </title> <institution> Technical Report CS-TR-3384 and UMIACS-TR-94-133, UMIACS and Electrical Engineering, University of Maryland, College Park, MD, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: The primitives gather and scatter are companion primitives whereby scatter divides a single array residing on a processor into equal-sized blocks which are then distributed to the remaining processors, and gather coalesces these blocks residing on the different processors into a single array on one processor. See <ref> [7, 6, 5] </ref> for algorithmic details, performance analyses, and empirical results for these communication primitives. The organization of this paper is as follows. Section 2 presents our computation model for analyzing parallel algorithms. <p> We have used vendor-supplied libraries for collective communication primitives on the IBM SP-2 implementation. The other machines used in this experiment do not have vendor-supported collective communication libraries, and hence we used our generic communication primitives as described in <ref> [7, 6, 5] </ref>. 3.2 Comparison with Single-Phase Algorithms It has been widely believed that an efficient algorithm for personalized communication is a single-phase algorithm in which data travels directly from source to destination with no intermediate routing.
Reference: [6] <author> D. A. Bader and J. JaJa. </author> <title> Parallel Algorithms for Image Histogramming and Connected Components with an Experimental Study. </title> <booktitle> In Fifth ACM SIGPLAN Symposium of Principles and Practice of Parallel Programming, </booktitle> <pages> pages 123-133, </pages> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year> <note> To appear in Journal of Parallel and Distributed Computing. </note>
Reference-contexts: The primitives gather and scatter are companion primitives whereby scatter divides a single array residing on a processor into equal-sized blocks which are then distributed to the remaining processors, and gather coalesces these blocks residing on the different processors into a single array on one processor. See <ref> [7, 6, 5] </ref> for algorithmic details, performance analyses, and empirical results for these communication primitives. The organization of this paper is as follows. Section 2 presents our computation model for analyzing parallel algorithms. <p> The cost of each of the communication primitives will be modeled by O (t + max (m; p)), where m is the maximum amount of data transmitted or received by a processor. Such cost (which is an overestimate) can be justified by using our earlier work <ref> [26, 27, 6, 7] </ref>. Using this cost model, we can evaluate the communication time T comm (n; p) of an algorithm as a function of the input size n, the number of processors p , and the parameters t and . <p> We have used vendor-supplied libraries for collective communication primitives on the IBM SP-2 implementation. The other machines used in this experiment do not have vendor-supported collective communication libraries, and hence we used our generic communication primitives as described in <ref> [7, 6, 5] </ref>. 3.2 Comparison with Single-Phase Algorithms It has been widely believed that an efficient algorithm for personalized communication is a single-phase algorithm in which data travels directly from source to destination with no intermediate routing.
Reference: [7] <author> D. A. Bader and J. JaJa. </author> <title> Practical Parallel Algorithms for Dynamic Data Redistribution, Median Finding, and Selection. </title> <institution> Technical Report CS-TR-3494 and UMIACS-TR-95-74, UMIACS and Electrical Engineering, University of Maryland, College Park, MD, </institution> <month> July </month> <year> 1995. </year> <booktitle> To be presented at the 10th International Parallel Processing Symposium, </booktitle> <address> Honolulu, HI, </address> <month> April 15-19, </month> <year> 1996. </year>
Reference-contexts: The remote read and write typically have both blocking and non-blocking versions. Also, when reading or writing more than a single element, bulk data transports are provided with corresponding bulk read and bulk write primitives. Our collective communication primitives, described in detail in <ref> [7] </ref>, are similar to those of MPI [33], the IBM POWERparallel [9], and the Cray MPP systems [16] and, for example, include the following: transpose, bcast, gather, and scatter. Brief descriptions of these are as follows. <p> The primitives gather and scatter are companion primitives whereby scatter divides a single array residing on a processor into equal-sized blocks which are then distributed to the remaining processors, and gather coalesces these blocks residing on the different processors into a single array on one processor. See <ref> [7, 6, 5] </ref> for algorithmic details, performance analyses, and empirical results for these communication primitives. The organization of this paper is as follows. Section 2 presents our computation model for analyzing parallel algorithms. <p> The organization of this paper is as follows. Section 2 presents our computation model for analyzing parallel algorithms. The Communication Library Primitive operations which are fundamental to the design of high-level algorithms are given in <ref> [7] </ref>. Section 3 introduces a practical algorithm for realizing h-relation personalized communication using these primitives. A parallel radix sort algorithm using the routing of h-relations is presented in Section 4. <p> The cost of each of the communication primitives will be modeled by O (t + max (m; p)), where m is the maximum amount of data transmitted or received by a processor. Such cost (which is an overestimate) can be justified by using our earlier work <ref> [26, 27, 6, 7] </ref>. Using this cost model, we can evaluate the communication time T comm (n; p) of an algorithm as a function of the input size n, the number of processors p , and the parameters t and . <p> By the previous argument, the bin size for the second phase is bounded by binsize h + 2 Overall Complexity of the Algorithm Clearly, all computation in this algorithm can be performed in T comp (n; p) = O (h). The transpose primitive, whose analysis is given in <ref> [7] </ref>, takes T comm (n; p) t + p 2 + p (p 1) in the second step, and T comm (n; p) t + p + p (p 1) in the last step. <p> We have used vendor-supplied libraries for collective communication primitives on the IBM SP-2 implementation. The other machines used in this experiment do not have vendor-supported collective communication libraries, and hence we used our generic communication primitives as described in <ref> [7, 6, 5] </ref>. 3.2 Comparison with Single-Phase Algorithms It has been widely believed that an efficient algorithm for personalized communication is a single-phase algorithm in which data travels directly from source to destination with no intermediate routing. <p> The resulting overall complexity is O ( h 1 + h 2 + t + ). Alternatively for large variances (h 1 h 2 ), we can use our dynamic data redistribution algorithm in <ref> [7] </ref> followed by our deterministic algorithm described earlier. The resulting overall complexity will also be the same. 4 Parallel Integer Sorting Consider the problem of sorting n integer keys in the range [0; M 1] that are distributed equally over a p-processor distributed memory machine.
Reference: [8] <author> D. Bailey, E. Barszcz, J. Barton, D. Browning, R. Carter, L. Dagum, R. Fatoohi, S. Fineberg, P. Frederickson, T. Lasinski, R. Schreiber, H. Simon, V. Venkatakrishnan, and S. Weeratunga. </author> <title> The NAS Parallel Benchmarks. </title> <type> Technical Report RNR-94-007, </type> <institution> Numerical Aerodynamic Simulation Facility, NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: Fast integer sorting is crucial for solving problems in many domains, and, as such, is used as a kernel in several parallel benchmarks such as NAS 1 <ref> [8] </ref> and SPLASH [46]. Because of the extensive and irregular communication requirements, previous parallel algorithms for sorting (a hypercube, e.g. [11, 1], or a mesh, e.g. [21, 31]) tend to be network or machine dependent, and therefore not efficient across current parallel machines. <p> bits per key 5 ; * [S]: random integers with entropy of 6:2 bits per key 6 ; * [C]: keys are consecutive in value (from 0 to n1) and are placed cyclically across the processors; * [N]: this input is taken from the NAS Parallel Benchmark for Integer Sorting <ref> [8] </ref>. Keys are integers in the range [0; 2 19 ), and each key is the average of four consecutive uniformly distributed pseudo-random numbers generated by the following recurrence: x k+1 = ax k (mod 2 46 ) where a = 5 13 and the seed x 0 = 314159265.
Reference: [9] <author> V. Bala, J. Bruck, R. Cypher, P. Elustondo, A. Ho, C.-T. Ho, S. Kipnis, and M. Snir. </author> <title> CCL: A Portable and Tunable Collective Communication Library for Scalable Parallel Computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 6 </volume> <pages> 154-164, </pages> <year> 1995. </year> <month> 20 </month>
Reference-contexts: Also, when reading or writing more than a single element, bulk data transports are provided with corresponding bulk read and bulk write primitives. Our collective communication primitives, described in detail in [7], are similar to those of MPI [33], the IBM POWERparallel <ref> [9] </ref>, and the Cray MPP systems [16] and, for example, include the following: transpose, bcast, gather, and scatter. Brief descriptions of these are as follows.
Reference: [10] <author> D.P. Bertsekas, C. Ozveren, G.D. Stamoulis, P. Tseng, and J.N. Tsitsiklis. </author> <title> Optimal Communi--cation Algorithms for Hypercubes. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 11 </volume> <pages> 263-275, </pages> <year> 1991. </year>
Reference-contexts: In particular, in his "bridging model" for parallel computation, Valiant has identified the h-relation personalized communication as the basis for organizing communication between two consecutive major computation steps. Previous parallel algorithms for personalized communication (typically for a hypercube, e.g. <ref> [28, 40, 37, 12, 13, 10, 1] </ref>, a mesh, e.g. [24, 39, 29, 14, 25], or other circuit switched network machines, e.g. [34, 19, 32, 38]) tend to be network or machine dependent, and thus not efficient when ported to current parallel machines.
Reference: [11] <author> G. E. Blelloch, C. E. Leiserson, B. M. Maggs, C. G. Plaxton, S. J. Smith, and M. Zagha. </author> <title> A Comparison of Sorting Algorithms for the Connection Machine CM-2. </title> <booktitle> In Proceedings of the ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 3-16, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Fast integer sorting is crucial for solving problems in many domains, and, as such, is used as a kernel in several parallel benchmarks such as NAS 1 [8] and SPLASH [46]. Because of the extensive and irregular communication requirements, previous parallel algorithms for sorting (a hypercube, e.g. <ref> [11, 1] </ref>, or a mesh, e.g. [21, 31]) tend to be network or machine dependent, and therefore not efficient across current parallel machines. In this paper, we present an algorithm for integer sorting which couples the well known parallel radix sort algorithm together with our algorithm for personalized communication.
Reference: [12] <author> S.H. Bokhari. </author> <title> Complete Exchange on the iPSC-860. </title> <type> ICASE Report No. 91-4, </type> <institution> ICASE, NASA Langley Research Center, Hampton, VA, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: In particular, in his "bridging model" for parallel computation, Valiant has identified the h-relation personalized communication as the basis for organizing communication between two consecutive major computation steps. Previous parallel algorithms for personalized communication (typically for a hypercube, e.g. <ref> [28, 40, 37, 12, 13, 10, 1] </ref>, a mesh, e.g. [24, 39, 29, 14, 25], or other circuit switched network machines, e.g. [34, 19, 32, 38]) tend to be network or machine dependent, and thus not efficient when ported to current parallel machines.
Reference: [13] <author> S.H. Bokhari. </author> <title> Multiphase Complete Exchange on a Circuit Switched Hypercube. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> pages I-525 - I-529, </address> <month> August </month> <year> 1991. </year> <note> Also appeared as NASA ICASE Report No. 91-5. </note>
Reference-contexts: In particular, in his "bridging model" for parallel computation, Valiant has identified the h-relation personalized communication as the basis for organizing communication between two consecutive major computation steps. Previous parallel algorithms for personalized communication (typically for a hypercube, e.g. <ref> [28, 40, 37, 12, 13, 10, 1] </ref>, a mesh, e.g. [24, 39, 29, 14, 25], or other circuit switched network machines, e.g. [34, 19, 32, 38]) tend to be network or machine dependent, and thus not efficient when ported to current parallel machines.
Reference: [14] <author> S.H. Bokhari and H. Berryman. </author> <title> Complete Exchange on a Circuit Switched Mesh. </title> <booktitle> In Proceedings of Scalable High Performance Computing Conference, </booktitle> <pages> pages 300-306, </pages> <address> Williamsburg, VA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: In particular, in his "bridging model" for parallel computation, Valiant has identified the h-relation personalized communication as the basis for organizing communication between two consecutive major computation steps. Previous parallel algorithms for personalized communication (typically for a hypercube, e.g. [28, 40, 37, 12, 13, 10, 1], a mesh, e.g. <ref> [24, 39, 29, 14, 25] </ref>, or other circuit switched network machines, e.g. [34, 19, 32, 38]) tend to be network or machine dependent, and thus not efficient when ported to current parallel machines.
Reference: [15] <author> W.W. Carlson and J.M. Draper. </author> <title> AC for the T3D. </title> <type> Technical Report SRC-TR-95-141, </type> <institution> Supercomputing Research Center, Bowie, MD, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: We also acknowledge William Carlson and Jesse Draper from the Center for Computing Science (formerly Supercomputing Research Center) for writing the parallel compiler AC (version 2.6) <ref> [15] </ref> on which the T3D port of Split-C has been based. This work also utilized the CM-5 at National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, under grant number ASC960008N. We also thank Jeffrey Hollingsworth from UMCP's Computer Science Department for his suggestions and encouragement.
Reference: [16] <institution> Cray Research, Inc. </institution> <note> SHMEM Technical Note for C, October 1994. Revision 2.3. </note>
Reference-contexts: Also, when reading or writing more than a single element, bulk data transports are provided with corresponding bulk read and bulk write primitives. Our collective communication primitives, described in detail in [7], are similar to those of MPI [33], the IBM POWERparallel [9], and the Cray MPP systems <ref> [16] </ref> and, for example, include the following: transpose, bcast, gather, and scatter. Brief descriptions of these are as follows.
Reference: [17] <author> D.E. Culler, A. Dusseau, S.C. Goldstein, A. Krishnamurthy, S. Lumetta, S. Luna, T. von Eicken, and K. Yelick. </author> <title> Introduction to Split-C. </title> <institution> Computer Science Division - EECS, University of Cali-fornia, Berkeley, </institution> <note> version 1.0 edition, </note> <month> March 6, </month> <year> 1994. </year>
Reference-contexts: In this paper, we present an algorithm for integer sorting which couples the well known parallel radix sort algorithm together with our algorithm for personalized communication. We show that this sorting algorithm is both efficient and scalable across a number of different platforms. Our algorithms are implemented in Split-C <ref> [17] </ref>, an extension of C for distributed memory machines. The algorithms make use of MPI-like communication primitives but do not make any assumptions as to how these primitives are actually implemented. The basic data transport is a read or write operation. <p> Performance of the latter code, which had been optimized for the Meiko CS-2, is given in [2]. Note that the AIS implementation is based upon the original version by Dusseau ([20, 18]). Also, all codes in this comparison have been written in the Split-C language <ref> [17] </ref>. Our algorithm is referred to as BHJ.
Reference: [18] <author> D.E. Culler, R.M. Karp, D.A. Patterson, A. Sahay, K.E. Schauser, E. Santos, R. Subramonian, and T. von Eicken. </author> <title> LogP: Towards a Realistic Model of Parallel Computation. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: The coefficient of t gives the total number of times collective communication primitives are used, and the coefficient of gives the maximum total amount of data exchanged between a processor and the remaining processors. This communication model is close to a number of similar models (e.g. <ref> [18, 44, 2] </ref>) that have recently appeared in the literature and seems to be well-suited for designing parallel algorithms on current high performance platforms. We define the computation time T comp (n; p) as the maximum time it takes a processor to perform 3 all the local computation steps.
Reference: [19] <author> V.V. Dimakopoulos and N.J. Dimopoulos. </author> <title> Optimal Total Exchange in Linear Arrays and Rings. </title> <booktitle> In Proceedings of the 1994 International Symposium on Parallel Architectures, Algorithms, and Networks, </booktitle> <pages> pages 230-237, </pages> <address> Kanazawa, Japan, </address> <month> December </month> <year> 1994. </year>
Reference-contexts: Previous parallel algorithms for personalized communication (typically for a hypercube, e.g. [28, 40, 37, 12, 13, 10, 1], a mesh, e.g. [24, 39, 29, 14, 25], or other circuit switched network machines, e.g. <ref> [34, 19, 32, 38] </ref>) tend to be network or machine dependent, and thus not efficient when ported to current parallel machines. In this paper, we introduce a novel deterministic algorithm that is shown to be both efficient and scalable across a number of different platforms.
Reference: [20] <author> A.C. Dusseau. </author> <title> Modeling Parallel Sorts with LogP on the CM-5. </title> <type> Technical Report UCB//CSD-94-829, </type> <institution> Computer Science Division, University of California, Berkeley, </institution> <year> 1994. </year>
Reference: [21] <author> N. Folwell, S. Guha, and I. Suzuki. </author> <title> A Practical Algorithm for Integer Sorting on a Mesh-Connected Computer. </title> <booktitle> In Proceedings of the High Performance Computing Symposium, </booktitle> <pages> pages 281-291, </pages> <address> Montreal, Canada, </address> <month> July </month> <year> 1995. </year> <note> Preliminary Version. </note>
Reference-contexts: Because of the extensive and irregular communication requirements, previous parallel algorithms for sorting (a hypercube, e.g. [11, 1], or a mesh, e.g. <ref> [21, 31] </ref>) tend to be network or machine dependent, and therefore not efficient across current parallel machines. In this paper, we present an algorithm for integer sorting which couples the well known parallel radix sort algorithm together with our algorithm for personalized communication.
Reference: [22] <author> A.V. Gerbessiotis and L.G. Valiant. </author> <title> Direct Bulk-Synchronous Parallel Algorithms. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(2) </volume> <pages> 251-267, </pages> <year> 1994. </year> <month> 21 </month>
Reference-contexts: In this problem, each processor has possibly different amounts of data to share with some subset of the other processors, such that each processor is the origin or destination of at most h messages. Clearly, such a task is endemic in parallel processing (e.g. <ref> [22, 44, 36] </ref>), and several authors have identified its efficient implementation as a prerequisite to efficient general purpose computing ([44]). In particular, in his "bridging model" for parallel computation, Valiant has identified the h-relation personalized communication as the basis for organizing communication between two consecutive major computation steps.
Reference: [23] <author> S. Heller. </author> <title> Congestion-Free Routing on the CM-5 Data Router. </title> <booktitle> In Proceedings of the First Inter--national Workshop on Parallel Computer Routing and Communication, </booktitle> <pages> pages 176-184, </pages> <address> Seattle, WA, May 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference: [24] <author> S. Hinrichs, C. Kosak, D.R. O'Hallaron, T.M. Strickler, and R. </author> <title> Take. An architecture for optimal all-to-all personalized communication. </title> <type> Technical Report CMU-CS-94-140, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: In particular, in his "bridging model" for parallel computation, Valiant has identified the h-relation personalized communication as the basis for organizing communication between two consecutive major computation steps. Previous parallel algorithms for personalized communication (typically for a hypercube, e.g. [28, 40, 37, 12, 13, 10, 1], a mesh, e.g. <ref> [24, 39, 29, 14, 25] </ref>, or other circuit switched network machines, e.g. [34, 19, 32, 38]) tend to be network or machine dependent, and thus not efficient when ported to current parallel machines.
Reference: [25] <author> T. Horie and K. Hayashi. </author> <title> All-to-All Personalized Communication on a Wrap-around Mesh. </title> <booktitle> In Proceedings of the Second Fujitsu-ANU CAP Workshop, </booktitle> <address> Canberra, Austrailia, </address> <month> November </month> <year> 1991. </year> <pages> 10 pp. </pages>
Reference-contexts: In particular, in his "bridging model" for parallel computation, Valiant has identified the h-relation personalized communication as the basis for organizing communication between two consecutive major computation steps. Previous parallel algorithms for personalized communication (typically for a hypercube, e.g. [28, 40, 37, 12, 13, 10, 1], a mesh, e.g. <ref> [24, 39, 29, 14, 25] </ref>, or other circuit switched network machines, e.g. [34, 19, 32, 38]) tend to be network or machine dependent, and thus not efficient when ported to current parallel machines.
Reference: [26] <author> J. JaJa and K.W. Ryu. </author> <title> The Block Distributed Memory Model. </title> <type> Technical Report CS-TR-3207, </type> <institution> Computer Science Department, University of Maryland, College Park, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: The cost of each of the communication primitives will be modeled by O (t + max (m; p)), where m is the maximum amount of data transmitted or received by a processor. Such cost (which is an overestimate) can be justified by using our earlier work <ref> [26, 27, 6, 7] </ref>. Using this cost model, we can evaluate the communication time T comm (n; p) of an algorithm as a function of the input size n, the number of processors p , and the parameters t and .
Reference: [27] <author> J.F. JaJa and K.W. Ryu. </author> <title> The Block Distributed Memory Model for Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the 8th International Parallel Processing Symposium, </booktitle> <pages> pages 752-756, </pages> <address> Cancun, Mexico, </address> <month> April </month> <year> 1994. </year> <note> (Extended Abstract). </note>
Reference-contexts: The cost of each of the communication primitives will be modeled by O (t + max (m; p)), where m is the maximum amount of data transmitted or received by a processor. Such cost (which is an overestimate) can be justified by using our earlier work <ref> [26, 27, 6, 7] </ref>. Using this cost model, we can evaluate the communication time T comm (n; p) of an algorithm as a function of the input size n, the number of processors p , and the parameters t and .
Reference: [28] <author> S.L. Johnsson and C.-T. Ho. </author> <title> Optimal Broadcasting and Personalized Communication in Hyper-cubes. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(9) </volume> <pages> 1249-1268, </pages> <year> 1989. </year>
Reference-contexts: In particular, in his "bridging model" for parallel computation, Valiant has identified the h-relation personalized communication as the basis for organizing communication between two consecutive major computation steps. Previous parallel algorithms for personalized communication (typically for a hypercube, e.g. <ref> [28, 40, 37, 12, 13, 10, 1] </ref>, a mesh, e.g. [24, 39, 29, 14, 25], or other circuit switched network machines, e.g. [34, 19, 32, 38]) tend to be network or machine dependent, and thus not efficient when ported to current parallel machines.
Reference: [29] <author> M. Kaufmann, J.F. Sibeyn, and T. Suel. </author> <title> Derandomizing Algorithms for Routing and Sorting on Meshes. </title> <booktitle> In Proceedings of the 5th Symposium on Discrete Algorithms, </booktitle> <pages> pages 669-679. ACM-SIAM, </pages> <year> 1994. </year>
Reference-contexts: In particular, in his "bridging model" for parallel computation, Valiant has identified the h-relation personalized communication as the basis for organizing communication between two consecutive major computation steps. Previous parallel algorithms for personalized communication (typically for a hypercube, e.g. [28, 40, 37, 12, 13, 10, 1], a mesh, e.g. <ref> [24, 39, 29, 14, 25] </ref>, or other circuit switched network machines, e.g. [34, 19, 32, 38]) tend to be network or machine dependent, and thus not efficient when ported to current parallel machines. <p> Clearly, the local computation bound is asymptotically optimal. As for the communication bound, t + p is a lower bound as in the worst case a processor sends n p elements and receives h elements. 6 Related Work The overall two-stage approach was independently described by Kaufmann et al. <ref> [29] </ref> and Ranka et al. [35] around the same time our earlier draft ([4]) appeared on our Web page.
Reference: [30] <author> D.E. Knuth. </author> <title> The Art of Computer Programming: Sorting and Searching, volume 3. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1973. </year>
Reference-contexts: An efficient algorithm is radix sort that decomposes each key into groups of r-bit blocks, for a suitably chosen r, and sorts the keys by sorting on each of the r-bit blocks beginning with the block containing the least significant bit positions <ref> [30] </ref>. Let R = 2 r p. Assume (w.l.o.g.) that the number of processors is a power of two, say p = 2 k , and hence R p is an integer = 2 rk 1.
Reference: [31] <author> D. Krizanc. </author> <title> Integer Sorting on a Mesh-Connected Array of Processors. </title> <journal> Information Processing Letters, </journal> <volume> 47(6) </volume> <pages> 283-289, </pages> <year> 1993. </year>
Reference-contexts: Because of the extensive and irregular communication requirements, previous parallel algorithms for sorting (a hypercube, e.g. [11, 1], or a mesh, e.g. <ref> [21, 31] </ref>) tend to be network or machine dependent, and therefore not efficient across current parallel machines. In this paper, we present an algorithm for integer sorting which couples the well known parallel radix sort algorithm together with our algorithm for personalized communication.
Reference: [32] <author> Y.-D. Lyuu and E. Schenfeld. </author> <title> Total Exchange on a Reconfigurable Parallel Architecture. </title> <booktitle> In Proceedings of the Fifth IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 2-10, </pages> <address> Dallas, TX, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: Previous parallel algorithms for personalized communication (typically for a hypercube, e.g. [28, 40, 37, 12, 13, 10, 1], a mesh, e.g. [24, 39, 29, 14, 25], or other circuit switched network machines, e.g. <ref> [34, 19, 32, 38] </ref>) tend to be network or machine dependent, and thus not efficient when ported to current parallel machines. In this paper, we introduce a novel deterministic algorithm that is shown to be both efficient and scalable across a number of different platforms.
Reference: [33] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard. </title> <type> Technical report, </type> <institution> University of Tennessee, Knoxville, TN, </institution> <month> June </month> <year> 1995. </year> <note> Version 1.1. </note>
Reference-contexts: 1 Problem Overview A fundamental challenge for parallel computing is to obtain high-level, architecture independent, algo rithms which efficiently execute on general-purpose parallel machines. This problem has become more tractable with the advent of message passing standards such as MPI <ref> [33] </ref>, which seek to guarantee fl The support by NASA Graduate Student Researcher Fellowship No. NGT-50951 is gratefully acknowledged. y Supported in part by NSF grant No. CCR-9103135 and NSF HPCC/GCAG grant No. BIR-9318183. 1 the availability of efficient implementations of certain basic collective communication routines. <p> Also, when reading or writing more than a single element, bulk data transports are provided with corresponding bulk read and bulk write primitives. Our collective communication primitives, described in detail in [7], are similar to those of MPI <ref> [33] </ref>, the IBM POWERparallel [9], and the Cray MPP systems [16] and, for example, include the following: transpose, bcast, gather, and scatter. Brief descriptions of these are as follows.
Reference: [34] <author> S.R. Ohring and S.K. Das. </author> <title> Efficient Communication in the Foldned Petersen Interconnection Networks. </title> <booktitle> In Proceedings of the Sixth International Parallel Architectures and Languages Europe Conference, </booktitle> <pages> pages 25-36, </pages> <address> Athens, Greece, July 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Previous parallel algorithms for personalized communication (typically for a hypercube, e.g. [28, 40, 37, 12, 13, 10, 1], a mesh, e.g. [24, 39, 29, 14, 25], or other circuit switched network machines, e.g. <ref> [34, 19, 32, 38] </ref>) tend to be network or machine dependent, and thus not efficient when ported to current parallel machines. In this paper, we introduce a novel deterministic algorithm that is shown to be both efficient and scalable across a number of different platforms.
Reference: [35] <author> S. Ranka, R.V. Shankar, and K.A. Alsabti. </author> <title> Many-to-many Personalized Communication with Bounded Traffic. </title> <booktitle> In The Fifth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 20-27, </pages> <address> McLean, VA, </address> <month> February </month> <year> 1995. </year> <month> 22 </month>
Reference-contexts: As for the communication bound, t + p is a lower bound as in the worst case a processor sends n p elements and receives h elements. 6 Related Work The overall two-stage approach was independently described by Kaufmann et al. [29] and Ranka et al. <ref> [35] </ref> around the same time our earlier draft ([4]) appeared on our Web page.
Reference: [36] <author> S. Rao, T. Suel, T. Tsantilas, and M. Goudreau. </author> <title> Efficient Communication Using Total-Exchange. </title> <booktitle> In Proceedings of the 9th International Parallel Processing Symposium, </booktitle> <pages> pages 544-550, </pages> <address> Santa Barbara, CA, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: In this problem, each processor has possibly different amounts of data to share with some subset of the other processors, such that each processor is the origin or destination of at most h messages. Clearly, such a task is endemic in parallel processing (e.g. <ref> [22, 44, 36] </ref>), and several authors have identified its efficient implementation as a prerequisite to efficient general purpose computing ([44]). In particular, in his "bridging model" for parallel computation, Valiant has identified the h-relation personalized communication as the basis for organizing communication between two consecutive major computation steps.
Reference: [37] <author> T. Schmiermund and S.R. Seidel. </author> <title> A Communication Model for the Intel iPSC/2. </title> <type> Technical Report Technical Report CS-TR 9002, </type> <institution> Dept. of Computer Science, Michigan Tech. Univ., </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: In particular, in his "bridging model" for parallel computation, Valiant has identified the h-relation personalized communication as the basis for organizing communication between two consecutive major computation steps. Previous parallel algorithms for personalized communication (typically for a hypercube, e.g. <ref> [28, 40, 37, 12, 13, 10, 1] </ref>, a mesh, e.g. [24, 39, 29, 14, 25], or other circuit switched network machines, e.g. [34, 19, 32, 38]) tend to be network or machine dependent, and thus not efficient when ported to current parallel machines.
Reference: [38] <author> D.S. Scott. </author> <title> Efficient All-to-All Communication Patterns in Hypercube and Mesh Topologies. </title> <booktitle> In Proceedings of the 6th Distributed Memory Computing Conference, </booktitle> <pages> pages 398-403, </pages> <address> Portland, OR, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Previous parallel algorithms for personalized communication (typically for a hypercube, e.g. [28, 40, 37, 12, 13, 10, 1], a mesh, e.g. [24, 39, 29, 14, 25], or other circuit switched network machines, e.g. <ref> [34, 19, 32, 38] </ref>) tend to be network or machine dependent, and thus not efficient when ported to current parallel machines. In this paper, we introduce a novel deterministic algorithm that is shown to be both efficient and scalable across a number of different platforms.
Reference: [39] <author> T. Suel. </author> <title> Routing and Sorting on Meshes with Row and Column Buses. </title> <type> Technical Report UTA//CS-TR-94-09, </type> <institution> Department of Computer Sciences, University of Texas at Austin, </institution> <month> Octo-ber </month> <year> 1994. </year>
Reference-contexts: In particular, in his "bridging model" for parallel computation, Valiant has identified the h-relation personalized communication as the basis for organizing communication between two consecutive major computation steps. Previous parallel algorithms for personalized communication (typically for a hypercube, e.g. [28, 40, 37, 12, 13, 10, 1], a mesh, e.g. <ref> [24, 39, 29, 14, 25] </ref>, or other circuit switched network machines, e.g. [34, 19, 32, 38]) tend to be network or machine dependent, and thus not efficient when ported to current parallel machines.
Reference: [40] <author> R. </author> <title> Take. A Routing Method for All-to-All Burst on Hypercube Networks. </title> <booktitle> In Proceedings of the 35th National Conference of Information Processing Society of Japan, </booktitle> <pages> pages 151-152, </pages> <year> 1987. </year> <title> In Japanese. Translation by personal communication with R. Take. </title>
Reference-contexts: In particular, in his "bridging model" for parallel computation, Valiant has identified the h-relation personalized communication as the basis for organizing communication between two consecutive major computation steps. Previous parallel algorithms for personalized communication (typically for a hypercube, e.g. <ref> [28, 40, 37, 12, 13, 10, 1] </ref>, a mesh, e.g. [24, 39, 29, 14, 25], or other circuit switched network machines, e.g. [34, 19, 32, 38]) tend to be network or machine dependent, and thus not efficient when ported to current parallel machines.
Reference: [41] <author> R. Thakur and A. Choudhary. </author> <title> All-to-All Communication on Meshes with Wormhole Routing. </title> <booktitle> In Proceedings of the 8th International Parallel Processing Symposium, </booktitle> <pages> pages 561-565, </pages> <address> Cancun, Mexico, </address> <month> April </month> <year> 1994. </year>
Reference: [42] <author> R. Thakur, A. Choudhary, and G. Fox. </author> <title> Complete Exchange on a Wormhole Routed Mesh. </title> <type> Report SCCS-505, </type> <institution> Northeast Parallel Architectures Center, Syracuse University, Syracuse, </institution> <address> NY, </address> <month> July </month> <year> 1993. </year>
Reference: [43] <author> R. Thakur, R. Ponnusamy, A. Choudhary, and G. Fox. </author> <title> Complete Exchange on the CM-5 and Touchstone Delta. </title> <journal> Journal of Supercomputing, </journal> <volume> 8 </volume> <pages> 305-328, </pages> <year> 1995. </year> <note> (An earlier version of this paper was presented at Supercomputing '92.). </note>
Reference: [44] <author> L.G. Valiant. </author> <title> A Bridging Model for Parallel Computation. </title> <journal> Communication of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <year> 1990. </year>
Reference-contexts: In this problem, each processor has possibly different amounts of data to share with some subset of the other processors, such that each processor is the origin or destination of at most h messages. Clearly, such a task is endemic in parallel processing (e.g. <ref> [22, 44, 36] </ref>), and several authors have identified its efficient implementation as a prerequisite to efficient general purpose computing ([44]). In particular, in his "bridging model" for parallel computation, Valiant has identified the h-relation personalized communication as the basis for organizing communication between two consecutive major computation steps. <p> The coefficient of t gives the total number of times collective communication primitives are used, and the coefficient of gives the maximum total amount of data exchanged between a processor and the remaining processors. This communication model is close to a number of similar models (e.g. <ref> [18, 44, 2] </ref>) that have recently appeared in the literature and seems to be well-suited for designing parallel algorithms on current high performance platforms. We define the computation time T comp (n; p) as the maximum time it takes a processor to perform 3 all the local computation steps.
Reference: [45] <author> J.-C. Wang, T.-H. Lin, and S. Ranka. </author> <title> Distributed Scheduling of Unstructured Collective Communication on the CM-5. </title> <type> Technical Report CRPC-TR94502, </type> <institution> Syracuse University, Syracuse, </institution> <address> NY, </address> <year> 1994. </year>
Reference: [46] <author> S.C. Woo, M. Ohara, E. Torrie, J.P. Singh, and A. Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <month> June </month> <year> 1995. </year> <month> 23 </month>
Reference-contexts: Fast integer sorting is crucial for solving problems in many domains, and, as such, is used as a kernel in several parallel benchmarks such as NAS 1 [8] and SPLASH <ref> [46] </ref>. Because of the extensive and irregular communication requirements, previous parallel algorithms for sorting (a hypercube, e.g. [11, 1], or a mesh, e.g. [21, 31]) tend to be network or machine dependent, and therefore not efficient across current parallel machines.
References-found: 46

