URL: ftp://ftp.cnl.salk.edu/pub/schraudo/nips93.ps.gz
Refering-URL: http://forum.swarthmore.edu/~jay/learn-game/systems/go-net.html
Root-URL: 
Email: schraudo@salk.edu dayan@salk.edu terry@salk.edu  
Title: Temporal Difference Learning of Position Evaluation in the Game of Go  
Author: Nicol N. Schraudolph Peter Dayan Terrence J. Sejnowski 
Address: San Diego, CA 92186-5800  
Affiliation: Computational Neurobiology Laboratory The Salk Institute for Biological Studies  
Abstract: The game of Go has a high branching factor that defeats the tree search approach used in computer chess, and long-range spa-tiotemporal interactions that make position evaluation extremely difficult. Development of conventional Go programs is hampered by their knowledge-intensive nature. We demonstrate a viable alternative by training networks to evaluate Go positions via temporal difference (TD) learning. Our approach is based on network architectures that reflect the spatial organization of both input and reinforcement signals on the Go board, and training protocols that provide exposure to competent (though unlabelled) play. These techniques yield far better performance than undifferentiated networks trained by self-play alone. A network with less than 500 weights learned within 3,000 games of 9x9 Go a position evaluation function that enables a primitive one-ply search to defeat a commercial Go program at a low playing level.
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A., Sutton, R., and Anderson, C. </author> <year> (1983). </year> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13. </volume>
Reference: <author> Br ugmann, B. </author> <year> (1993). </year> <title> Monte Carlo Go. </title> <note> Manuscript available by Internet anonymous file transfer from bsdserver.ucsf.edu, file Go/comp/mcgo.tex.Z. </note>
Reference-contexts: Conventional Go programs are carefully (and protractedly) tuned expert systems (Fotland, 1993). They are fundamentally limited by their need for human assistance in compiling and integrating domain knowledge, and still play barely above the level of a human beginner a machine learning approach may thus offer considerable advantages. <ref> (Br ugmann, 1993) </ref> has shown that a knowledge-free optimization approach to Go can work in principle: he obtained respectable (though inefficient) play by selecting moves through simulated annealing (Kirkpatrick et al., 1983) over possible continuations of the game. The pattern recognition component inherent in Go is amenable to connectionist methods.
Reference: <author> Dayan, P. </author> <year> (1993). </year> <title> Improving generalization for temporal difference learning: </title> <booktitle> The successor representation. Neural Computation, </booktitle> <volume> 5(4) </volume> <pages> 613-624. </pages>
Reference-contexts: We make this spatial credit assignment accessible to the network by having it predict the fate of every point on the board rather than just the overall score, and evaluate whole positions accordingly. This bears some similarity with the Successor Representation <ref> (Dayan, 1993) </ref> which also integrates over vector rather than scalar destinies. 2 Given the knowledge-based approach of existing Go programs, there is an embarrassment of input features that one might adopt for Go: Wally already uses about 30 of them, stronger programs disproportionately more.
Reference: <author> Enderton, H. D. </author> <year> (1991). </year> <title> The Golem Go program. </title> <type> Technical Report CMU-CS-92-101, </type> <institution> Carnegie Mellon University. </institution> <note> Report available by Internet anonymous file transfer from bsdserver.ucsf.edu, file Go/comp/golem.sh.Z. </note>
Reference: <author> Fotland, D. </author> <year> (1993). </year> <title> Knowledge representation in the Many Faces of Go. </title> <note> Manuscript available by Internet anonymous file transfer from bsdserver.ucsf.edu, file Go/comp/mfg.Z. </note>
Reference-contexts: Humans appear to rely mostly on static evaluation of board positions, aided by highly selective yet deep local lookahead. Conventional Go programs are carefully (and protractedly) tuned expert systems <ref> (Fotland, 1993) </ref>. <p> We use three computer opponents to train our networks: a random move generator, the public-domain program Wally (Newman, 1988), and the commercial program The Many Faces of Go <ref> (Fotland, 1993) </ref>. The random move generator naturally doesn't play Go very well 4 , but it does have the advantages of high speed and ergodicity a few thousand games of random Go proved an effective way to prime our networks at the start of training.
Reference: <author> Geman, S. and Geman, D. </author> <year> (1984). </year> <title> Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6. </volume>
Reference-contexts: This was done by Gibbs sampling <ref> (Geman and Geman, 1984) </ref> over values obtained from single-ply search, annealing the temperature parameter from random towards best-predicted play. <p> We therefore pick moves stochastically by Gibbs sampling <ref> (Geman and Geman, 1984) </ref>, in which the probability of a given move is exponentially related to the predicted value of the position it leads to through a temperature parameter that controls the degree of randomness.
Reference: <author> Kirkpatrick, S., Gelatt Jr., C. D., and Vecchi, M. P. </author> <year> (1983). </year> <title> Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220 </volume> <pages> 671-680. </pages>
Reference-contexts: domain knowledge, and still play barely above the level of a human beginner a machine learning approach may thus offer considerable advantages. (Br ugmann, 1993) has shown that a knowledge-free optimization approach to Go can work in principle: he obtained respectable (though inefficient) play by selecting moves through simulated annealing <ref> (Kirkpatrick et al., 1983) </ref> over possible continuations of the game. The pattern recognition component inherent in Go is amenable to connectionist methods. Supervised backpropagation networks have been applied to the game (Stoutamire, 1991; Enderton, 1991) but face a bottleneck in the scarcity of hand-labelled training data.
Reference: <author> Le Cun, Y., Boser, B., Denker, J., Henderson, D., Howard, R., Hubbard, W., and Jackel, L. </author> <year> (1989). </year> <title> Backpropagation applied to handwritten zip code recognition. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 541-551. </pages>
Reference-contexts: Go positions are also invariant with respect to the eightfold (reflection fi rotation) symmetry of the square. We provided mechanisms for constraining the network to obey this invariance by appropriate weight sharing and summing of derivatives <ref> (Le Cun et al., 1989) </ref>.
Reference: <author> Newman, W. H. </author> <year> (1988). </year> <title> Wally, a Go playing program. Shareware C program available by Internet anonymous file transfer from bsdserver.ucsf.edu, file Go/comp/wally.sh.Z. </title>
Reference-contexts: The output learned to predict the margin of victory or defeat for black. This undifferentiated network did learn to squeak past Wally, a weak public domain program <ref> (Newman, 1988) </ref>, but it took 659,000 games of training to do so. <p> We use three computer opponents to train our networks: a random move generator, the public-domain program Wally <ref> (Newman, 1988) </ref>, and the commercial program The Many Faces of Go (Fotland, 1993).
Reference: <author> Rivest, R. </author> <year> (1993). </year> <title> MIT Press, forthcoming. Invited talk: </title> <booktitle> Computational Learning Theory and Natural Learning Systems, </booktitle> <address> Provincetown, MA. </address>
Reference-contexts: A player may pass at any time; the game ends when both players pass in succession. Unlike most other games of strategy, Go has remained an elusive skill for computers to acquire indeed it has been recognized as a grand challenge of Artificial Intelligence <ref> (Rivest, 1993) </ref>. The game tree search approach used extensively in computer chess is infeasible: the game tree of Go has an average branching factor of around 200, but even beginners may routinely look ahead up to 60 plies in some situations.
Reference: <author> Robertie, B. </author> <year> (1992). </year> <title> Carbon versus silicon: Matching wits with TD-Gammon. Inside Backgammon, </title> <booktitle> 2(2) </booktitle> <pages> 14-22. </pages>
Reference: <author> Samuel, A. L. </author> <year> (1959). </year> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 3 </volume> <pages> 211-229. </pages>
Reference-contexts: This is a particular problem when the strategy for choosing moves during learning is different from the policy adopted for `optimal' network play. <ref> (Samuel, 1959) </ref> found it inadvisable to let his checker program learn from games which it won against an opponent, since its predictions might otherwise reflect poor as well as good play.
Reference: <author> Stoutamire, D. </author> <year> (1991). </year> <title> Machine learning applied to Go. </title> <type> Master's thesis, </type> <institution> Case Western Reserve University. </institution> <note> Reprint available by Internet anonymous file transfer from bsdserver.ucsf.edu, file Go/comp/report.ps.Z. </note>
Reference: <author> Sutton, R. </author> <year> (1984). </year> <title> Temporal Credit Assignment in Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, Amherst. </institution>
Reference: <author> Sutton, R. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44. </pages>
Reference: <author> Tesauro, G. </author> <year> (1992). </year> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 257-278. </pages>
Reference-contexts: We propose an alternative approach based on the TD () predictive learning algorithm (Sutton, 1984; Sutton, 1988; Barto et al., 1983), which has been successfully applied to the game of backgammon by <ref> (Tesauro, 1992) </ref>. His TD-Gammon program uses a backpropagation network to map preselected features of the board position to an output reflecting the probability that the player to move would win. <p> The complexity of this task can be significantly reduced by exploiting a number 2 Sharing information within the network across multiple outputs restricts us to = 0 for efficient implementation of TD (). Note that although <ref> (Tesauro, 1992) </ref> did not have this constraint, he nevertheless found = 0 to be optimal. of constraints that hold a priori in this domain.
Reference: <author> Tesauro, G. </author> <year> (1994). </year> <title> TD-Gammon, a self-teaching backgammon program, achieves master-level play. </title> <journal> Neural Computation, </journal> <volume> 6(2) </volume> <pages> 215-219. </pages>
References-found: 17

