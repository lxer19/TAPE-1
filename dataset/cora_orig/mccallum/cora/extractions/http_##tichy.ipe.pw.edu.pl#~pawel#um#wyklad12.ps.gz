URL: http://tichy.ipe.pw.edu.pl/~pawel/um/wyklad12.ps.gz
Refering-URL: http://tichy.ipe.pw.edu.pl/~pawel/um/index.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Uczenie si maszyn: wykad 12  
Author: Uczenie si ze wzmocnieniem (cz- ) Pawe Cichosz Semestr zimowy / Problem uczenia si ze wzmocnieniem analizy teoretycznej bdzie przedstawiony p-niej. Z praktycznego punktu widzenia istotne jest, e w uczeniu si ze wzmocnieniem dopuszcza 
Note: 1.1 Scenariusz 1.2 rodowisko Matematyczny model -rodowiska potrzebny do  
Abstract: Uczenie si ze wzmocnieniem (reinforcement learning, RL) stanowi daleko idce odej-cie od dotychczas rozwaanych przez nas rodzajw uczenia si. Nie jest to uczenie si indukcyjne, a wic oparte na generalizacji duej liczby przykadw, nie jest to take adna inna forma uczenia si poj ani w ogle adnej wiedzy deklaratywnej, wreszcie nie jest to uczenie si z nadzorem, w ktrym rdem informacji trenujcej jest nauczyciel. Uczenie si ze wzmocnieniem ma na celu uczenie si wiedzy proceduralnej (umiejtno-ci). Jest to obliczeniowe podej-cie do uczenia si celowego zachowania na podstawie interakcji. rdem informacji trenujcej, ktra ma charakter warto-ciujcy jako- dziaania ucznia, jest raczej krytyk ni nauczyciel. Mimo korzeni sigajcych lat 50., dopiero od poowy lat 80. uczenie si ze wzmocnieniem rozwija si w nowoczesnej postaci, w latach 90. uzyskao pozycj jednego z dominujcych nurtw maszynowego uczenia si w USA i obecnie ro-nie take zainteresowanie nim w Europie. Na europejskich i midzynarodowych konferencjach na temat uczenia si maszyn udzia publikacji na temat uczenia si ze wzmocnieniem siga ok. 1020%. Uczeniu si ze wzmocnieniem bd po-wicone dwa kolejne wykady. Ten wykad sformuuje problem, przedstawi podstawy teoretyczne zwizane z procesami Markowa i programowaniem dynamicznym oraz wprowadzi podstawowy algorytm TD, a na nastpnym wykadzie omwione bd praktyczne algorytmy uczenia si ze wzmocnieniem oparte na TD i zagadnienia zwizane z ich stosowaniem. U podstaw uczenia si ze wzmocnieniem le dynamiczne interakcje ucznia ze -rodowiskiem, w ktrym dziaa, realizujc swoje zadanie. Interakcje te odbywaj si dyskretnych (na og) krokach czasu i polegaj na obserwowaniu przez ucznia kolejnych stanw -rodowiska oraz wyko-nywaniu wybranych zgodnie z jego obecn strategi decyzyjn akcji. Po wykonaniu akcji ucze otrzymuje rzeczywistoliczbowe warto-ci wzmocnienia lub nagrody, ktre stanowi pewn miar oceny jako-ci jego dziaania. Wykonanie akcji moe rwnie powodowa zmian stanu -rodowi-ska. Uczenie si polega na modytkacji strategii na podstawie obserwowanych sekwencji stanw, akcji i nagrd. Naszkicowany scenariusz sprecyzowany jest w tablicy 1 w postaci oglnego, abstrakcyjnego algorytmu uczenia si ze wzmocnieniem. Wikszo- znanych praktycznych algo-rytmw mona przedstawi jako jego konkretyzacje. Wprowadza on take konwencje notacyjne do oznaczania stanw, akcji i nagrd w kolejnych krokach czasu, ktre bdziemy dalej stosowa. 
Abstract-found: 1
Intro-found: 1
References-found: 0

