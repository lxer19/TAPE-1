URL: ftp://speech.cse.ogi.edu/pub/docs/icassp97/yan.ps
Refering-URL: http://www.cse.ogi.edu/CSLU/personnel/bios/cole.html
Root-URL: http://www.cse.ogi.edu
Email: fyan, fanty, coleg@cse.ogi.edu  
Title: SPEECH RECOGNITION USING NEURAL NETWORKS WITH FORWARD-BACKWARD PROBABILITY GENERATED TARGETS  for Spoken Language Understanding  
Author: Yonghong Yan Mark Fanty Ron Cole 
Address: P.O. Box 91000, Portland, OR 97291-1000  
Affiliation: Center  Oregon Graduate Institute of Science and Technology  
Abstract: Neural network training targets for speech recognition are estimated using a novel method. Rather than use zero and one, continuous targets are generated using forward-backward probabilities. Each training pattern has more than one class active. Experiments showed that the new method effectively decreased the error rate by 15% in a continuous digits recognition task. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Barnard, R. Cole, M. Fanty, and P. Vermeulen. </author> <title> Real-world speech recognition with neural networks. </title> <booktitle> In Proceedings of the International Symposium on Aerospace/Defense Sensing & Control and Dual-Use Photonics, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: This approach was evaluated on a continuous digit telephone speech database. For comparison, evaluations were also done for a hybrid system using our previous work (trained with Viterbi forced alignment with only 1 fixed non-zero target per training pattern) ( <ref> [1] </ref>) and for a continuous HMM-based recognizer, using the same training, development and testing sets. The results showed more than a 15% error reduction for the hybrid system using the new training method. <p> The outputs of the initialization network are used as the emission probabilities of the Markov states, and the initialization network is retrained using the generated targets (in (12)). 3. The Hybrid System The hybrid system used for this study is very similar to our previous approach described in <ref> [1] </ref>, except within-model state transitions are implemented in this new system. Like most of the other hybrid systems, the NN in our system is used as a state emission probability estimator. A three-layer fully-connected NN was used in this study. The modeling units are phones. <p> Within-phone state transition probabilities were not used in the baseline system. The second system was retrained using forward-backward targets. Within-phone state transition probabilities computed with the forward-backward algorithm were used in this system. 4.3..1 The Baseline The baseline system was based on our previous work ( <ref> [1] </ref>). The NN used was a three-layer fully-connected feed-forward net. It had 56 input nodes, 200 hidden nodes and 209 output nodes. PLP analysis was carried out every 6 ms with a 10 ms window. For each frame, the resulting feature is a 8-dimensional vector (7 PLP coefficients plus energy).
Reference: [2] <author> U. Bodenhausen and S. Manke. </author> <title> Connectionist architectural learning for high performance character and speech recognition. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages I-625-I-628, </pages> <year> 1993. </year>
Reference: [3] <author> R.A. Cole, M. Noel, T. Lander, and T. Durham. </author> <title> New telephone speech corpora at cslu. </title> <booktitle> In Proceedings of Eurospeech'95, </booktitle> <pages> pages 821-824, </pages> <address> Madrid, Spain, </address> <year> 1995. </year>
Reference-contexts: Comparative Experiments 4.1. Database and Task The speech corpus used in this experiment consists of digit sequences taken from the public domain numbers corpus (telephone speech), collected by the Center for Spoken Language Understanding (CSLU) at OGI <ref> [3] </ref>. Each utterance contains 1 to 6 continuously pronounced digit strings. Since callers were recruited through public advertisements, and were instructed to call the data collection phone number at any time or place, the database is close to a real-world application environment.
Reference: [4] <author> M. Fanty and R. A. Cole. </author> <title> Spoken letter recognition. </title> <editor> In R. P. Lippman, J. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference: [5] <author> V. Fontaine, C. Ris, H. Leich, J. Vantieghen, S. Ac-caino, and D. Compernolle. </author> <title> Comparison between two hybrid hmm/mlp approaches in speech recognition. </title> <booktitle> In Proceedings 1996 IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 3362-3365, </pages> <address> Atlanta, USA, </address> <month> May </month> <year> 1996. </year>
Reference: [6] <author> H. Hild and A. Waibel. </author> <title> Connected letter recognition with a multi-state time delay neural network. </title> <editor> In J. Cowan, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 1059 - 1068. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1993. </year>
Reference: [7] <author> H. Hutter. </author> <title> Comparison of a new hybrid connectionist-schmm approach with other hybrid approaches for speech recognition. </title> <booktitle> In Proceedings 1995 IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 3311-3314, </pages> <month> May </month> <year> 1995. </year>
Reference: [8] <author> K. Kasper, H. Reininger, and H. Wust. </author> <title> Strategies for reducing the complexity of a rnn based speech recog-nizer. </title> <booktitle> In Proceedings 1996 IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 3354 - 3357, </pages> <address> Atlanta, USA, </address> <month> May </month> <year> 1996. </year>
Reference: [9] <author> D. Kershaw, T. Robinson, and M. Hochberg. </author> <title> Context-dependent classes in a hybrid recurrent network-hmm speech recognition system. </title> <editor> In D.S. Touretzky, M.C. Mozer, and M.E. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 750 - 756. </pages> <publisher> The MIT Press, </publisher> <year> 1996. </year>
Reference: [10] <author> Y. Komori. </author> <title> A neural fuzzy training approach for continuous speech recognition improvemen t. </title> <booktitle> In Proceedings 1992 IEEE International Conference on Acoustics, Speech, and Signal Processi ng, </booktitle> <pages> pages 405-408, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: A reasonable set of targets should represent this; a soft (probabilistic) decision is more appropriate. Attempts along this direction have been made recently, in [11] a frame work called REMAP was proposed and applied to a transition-based model to estimate the posterior probabilities, and in <ref> [10] </ref> a VQ-like technique was used to estimate the fuzzy likelihoods. This paper describes a method for estimating continuous targets for training patterns of NNs based on the conventional forward-backward algorithm. The targets used to train the neural network are derived from the posterior state occupation probabilities.
Reference: [11] <author> Y. Konig, H. Bourlard, and N. Morgan. </author> <title> Remap-experiments with speech recognition. </title> <booktitle> In Proceedings 1996 IEEE International Conference on Acoustics, Speech, and Signal Processing, pages 3350- 3353, </booktitle> <address> At-lanta, USA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: It is not appropriate to force a hard decision. A reasonable set of targets should represent this; a soft (probabilistic) decision is more appropriate. Attempts along this direction have been made recently, in <ref> [11] </ref> a frame work called REMAP was proposed and applied to a transition-based model to estimate the posterior probabilities, and in [10] a VQ-like technique was used to estimate the fuzzy likelihoods.
Reference: [12] <author> S. Renals, M. Hochberg, and T. Robinson. </author> <title> Learning temporal depdendencies in continuous speech recognition. </title> <editor> In J. Cowan, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 1051 - 1058. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1994. </year>
Reference: [13] <author> J. Tebelskis. </author> <title> Performance through consistency: connectionist large vocabulary continuous speech recognition. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages II-259-II-262, </pages> <year> 1993. </year>
Reference: [14] <author> A. Waibel, T. Hanazawa, G. Hinton, K. Shiano, and K. Lang. </author> <title> Phoneme recognition using time-delay neural networks. </title> <journal> In IEEE Trans. on Acoust., Speech, and Signal Processing, </journal> <volume> volume 37(3), </volume> <pages> pages 328-339, </pages> <year> 1989. </year>
Reference: [15] <author> S.J. Young. </author> <title> Htk: Hidden markov model toolkit v.14. </title> <institution> In Cambridge University Engineering Depart ment, </institution> <year> 1992. </year>
References-found: 15

