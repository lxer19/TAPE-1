URL: http://vismod.www.media.mit.edu/~dkroy/papers/Postscript/icassp98.ps.Z
Refering-URL: http://vismod.www.media.mit.edu/~dkroy/toco.html
Root-URL: http://www.media.mit.edu
Email: (dkroy,sandy)@media.mit.edu  
Title: WORD LEARNING IN A MULTIMODAL ENVIRONMENT  
Author: Deb Roy and Alex Pentland 
Address: 20 Ames Street, Rm. E15-388 Cambridge, MA 01239  
Affiliation: MIT Media Lab  
Abstract: We are creating human machine interfaces which let people communicate with machines using natural modalities including speech and gesture. A problem with current multimodal interfaces is that users are forced to learn the set of words and gestures which the interface understands. We report on a trainable interface which lets the user teach the system words of their choice through natural multimodal interactions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G.W. Furnas, T.K. Landauer, L.M. Gomez, and S.T. Dumais. </author> <title> The vocabulary problem in human-system communications. </title> <journal> Communications of the Association for Compuring Machinery, </journal> <volume> 30 </volume> <pages> 964-972, </pages> <year> 1987. </year>
Reference-contexts: If the user strays from this vocabulary, the system will not respond correctly. The semantics of the words must also be defined by the interface designer but may also not match the expectations of the user <ref> [1] </ref>. In practice it is extremely difficult to predict what vocabulary a person will use in even the most restricted domains [1]. As Zipf's law would predict, people's choice of words varies widely making it nearly impossible for the interface designer to determine which words an individual user will choose. <p> The semantics of the words must also be defined by the interface designer but may also not match the expectations of the user <ref> [1] </ref>. In practice it is extremely difficult to predict what vocabulary a person will use in even the most restricted domains [1]. As Zipf's law would predict, people's choice of words varies widely making it nearly impossible for the interface designer to determine which words an individual user will choose. To compound the problem of predicting vocabulary selection, the semantics of words also vary across users. <p> To compound the problem of predicting vocabulary selection, the semantics of words also vary across users. In some of their experiments, Furnas et. al. found that users will sometimes use the same word to refer to different concepts even within highly limited domains <ref> [1] </ref>. These findings suggest that the vocabulary and associated semantics used in an interface should not be hard wired by the interface designer. 2.
Reference: [2] <author> Allen L. Gorin. </author> <title> On automated language acquisition. </title> <journal> Journal of the Acoustic Society of America, </journal> <volume> 97(6) </volume> <pages> 3441-3461, </pages> <year> 1995. </year>
Reference-contexts: 1. PROBLEM Most current human-machine interfaces which use natural modalities such as speech and gesture force the user to learn which words and gestures the system understands before the system can be used (see [9], [10], or [4]; a notable exception is <ref> [2] </ref>). For example, an interface designer who wishes to use speech input must choose the vocabulary which the system will understand. If the user strays from this vocabulary, the system will not respond correctly. <p> Since the elements of the attributes are binary variables, we are able to use simple smoothed relative frequencies to estimate the probabilities in Equation 4 <ref> [2] </ref>. When Toco first starts running, he has no clusters in memory. When the user first points to an object and utters a word, Toco will create a cluster and initialize it with the phoneme string extracted from the user's speech.
Reference: [3] <author> H. Hermansky and N. Morgan. </author> <title> Rasta processing of speech. </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <month> October </month> <year> 1994. </year>
Reference-contexts: The 3-D hand position is estimated by combining estimates from both cameras. For details of the gesture tracking system see [8]. 4.2. Speech Analysis using Real-time Phoneme Recognition Audio is sampled at 16-bit 16 kHz from a head mounted microphone and processed using the Relative Spectral (RASTA) algorithm <ref> [3] </ref>. The RASTA coefficients are computed on 20ms windows of audio (recomputed every 10ms) and fed into a recurrent neural network (RNN) similar to the system described in [6] to produce a 40-dimensional phoneme probability estimate (39 phonemes and silence) at a rate of 100Hz.
Reference: [4] <author> Sharon Oviatt. </author> <title> Multimodal interfaces for dynamic interactive maps. </title> <booktitle> In Proceedings of the Conference on Human Factors in Computing Systems, </booktitle> <pages> pages 95-102, </pages> <address> New York, 1996. </address> <publisher> ACM Press. </publisher>
Reference-contexts: 1. PROBLEM Most current human-machine interfaces which use natural modalities such as speech and gesture force the user to learn which words and gestures the system understands before the system can be used (see [9], [10], or <ref> [4] </ref>; a notable exception is [2]). For example, an interface designer who wishes to use speech input must choose the vocabulary which the system will understand. If the user strays from this vocabulary, the system will not respond correctly.
Reference: [5] <author> Alex Pentland. Smart desks, desks, and clothes. </author> <booktitle> In Proceedings of ICASSP, </booktitle> <pages> pages 171-174, </pages> <address> Munich, Ger-many, April 1997. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: A MULTIMODAL SENSORY ENVIRONMENT We have created an environment to facilitate development of multimodal adaptive interfaces based on the smart desk environment <ref> [5] </ref>. In its current configuration, the user sits at a desk facing a 70" color projection screen which displays Toco and virtual objects. Toco can sense three types of input: the user's gestures, the user's speech, and information about the objects which currently exist in Toco's virtual environment. 4.1.
Reference: [6] <author> Tony Robinson. </author> <title> An application of recurrent nets to phone probability estimation. </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> 5(3), </volume> <year> 1994. </year>
Reference-contexts: The RASTA coefficients are computed on 20ms windows of audio (recomputed every 10ms) and fed into a recurrent neural network (RNN) similar to the system described in <ref> [6] </ref> to produce a 40-dimensional phoneme probability estimate (39 phonemes and silence) at a rate of 100Hz. The RNN has been trained using back propagation in time on the TIMIT database.
Reference: [7] <author> Richard Rose. </author> <title> Word Spotting from Continuous Speech Utterances, </title> <booktitle> chapter 13, </booktitle> <pages> pages 303-329. </pages> <publisher> Kluwer Academic, </publisher> <year> 1996. </year>
Reference-contexts: This distance metric is used for clustering speech events (see Section 5.1). The reference phoneme string may be thought of as a HMM. We can compute a confidence measure that an event was generated by the HMM following methods developed for keyword spotting confidence measures <ref> [7] </ref> as follows. First we compute the log probability of an event e using a forced Viterbi alignment with phoneme transitions determined by the reference phoneme string. We denote this as log (p (ref erence j e)).
Reference: [8] <author> Deb Roy and Alex Pentland. </author> <title> Multimodal adaptive interfaces. </title> <type> Technical Report 438, </type> <institution> MIT Media Lab Vision and Modeling Group, </institution> <year> 1997. </year>
Reference-contexts: A Gaussian mixture model of skin color is used to locate and track the user's hand at 30 Hz. The 3-D hand position is estimated by combining estimates from both cameras. For details of the gesture tracking system see <ref> [8] </ref>. 4.2. Speech Analysis using Real-time Phoneme Recognition Audio is sampled at 16-bit 16 kHz from a head mounted microphone and processed using the Relative Spectral (RASTA) algorithm [3]. <p> The RNN has been trained using back propagation in time on the TIMIT database. A finite state machine is used to detect and segment speech events (defined to be spoken utterances surrounded by silence) using the silence estimate from the RNN <ref> [8] </ref>. The RNN outputs are treated as emission probabilities within a Hidden Markov Model (HMM) framework. Du ration models and bigram phoneme transition probabilities for a all-phoneme loop HMM have been computed from the TIMIT training data set.
Reference: [9] <author> Kristinn R. Thorisson. </author> <title> Communicative Humanoids: A Computational Model of Psychological Dialogue Skills. </title> <type> PhD thesis, </type> <institution> MIT Department of Media Arts and Sciences, </institution> <year> 1996. </year>
Reference-contexts: 1. PROBLEM Most current human-machine interfaces which use natural modalities such as speech and gesture force the user to learn which words and gestures the system understands before the system can be used (see <ref> [9] </ref>, [10], or [4]; a notable exception is [2]). For example, an interface designer who wishes to use speech input must choose the vocabulary which the system will understand. If the user strays from this vocabulary, the system will not respond correctly.
Reference: [10] <author> Alex Waibel, Minh Tue Vo, Paul Duchnowski, and Stephan Manke. </author> <title> Multimodal interfaces. </title> <journal> Artificial Intelligence Review, </journal> <volume> 10(3-4):299-319, </volume> <year> 1995. </year>
Reference-contexts: 1. PROBLEM Most current human-machine interfaces which use natural modalities such as speech and gesture force the user to learn which words and gestures the system understands before the system can be used (see [9], <ref> [10] </ref>, or [4]; a notable exception is [2]). For example, an interface designer who wishes to use speech input must choose the vocabulary which the system will understand. If the user strays from this vocabulary, the system will not respond correctly.
References-found: 10

