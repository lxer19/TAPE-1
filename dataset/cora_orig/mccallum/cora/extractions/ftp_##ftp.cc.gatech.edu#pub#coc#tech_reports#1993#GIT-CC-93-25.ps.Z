URL: ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1993/GIT-CC-93-25.ps.Z
Refering-URL: http://www.cs.gatech.edu/tech_reports/index.93.html
Root-URL: 
Email: (clemenco@lse.epfl.ch)  (bodhi@watson.ibm.com)  (schwan@cc.gatech.edu)  
Title: Distributed Shared Abstractions (DSA) on Multiprocessors  
Author: Christian Clemen~con Bodhisattwa Mukherjee Karsten Schwan 
Note: This work was supported in part by the Swiss National Science Foundation and by NSF grant CCR-8619886.  
Address: Atlanta, Georgia 30332-0280  
Affiliation: College of Computing Georgia Institute of Technology  
Web: GIT-CC-93/25  
Abstract: Any parallel program has abstractions that are shared by the program's multiple processes, including data structures containing shared data, code implementing operations like global sums or minima, type instances used for process synchronization or communication, etc. Such shared abstractions can considerably affect the performance of parallel programs, on both distributed and shared memory multiprocessors. As a result, their implementation must be efficient, and such efficiency should be achieved without unduly compromising program portability and maintainability. Unfortunately, efficiency and portability can be at cross-purposes, since high performance typically requires changes in the representation of shared abstractions across different parallel machines. The primary contribution of the DSA library presented and evaluated in this paper is its representation of shared abstractions as objects that may be internally distributed across different nodes of a parallel machine. Such distributed shared abstractions (DSA) are encapsulated so that their implementations are easily changed while maintaining program portability across parallel architectures ranging from small-scale multiprocessors, to medium-scale shared and distributed memory machines, and potentially, to networks of computer workstations. The principal results presented in this paper are (1) a demonstration that the fragmentation of object state across different nodes of a multiprocessor machine can significantly improve program performance and (2) that such object fragmentation can be achieved without compromising portability by changing object interfaces. These results are demonstrated using implementations of the DSA library on several medium-scale multiprocessors, including the BBN Butterfly, Kendall Square Research, and SGI shared memory multiprocessors. The DSA library's evaluation uses synthetic workloads and a parallel implementation of a branch-and-bound algorithm for solving the Traveling Salesperson Problem (TSP). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Alverson and D. Notkin. </author> <title> Program structuring for effective parallel portability. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(9) </volume> <pages> 1041-1059, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: Other issues regarding parallel TSP implementations on hypercube machines are discussed in [40], as well as various alternative representations of the work sharing and tour abstractions on distributed memory machines. 29 There has been some earlier work on parallel program portability <ref> [10, 1, 9] </ref>. In [1, 9], authors have proposed portable models analogous to programming languages. Similar to our work, their approach is based on abstractions where interfaces and implementations of libraries are clearly separated. Based on the execution environment, an interface is bound to an efficient implementation. <p> Other issues regarding parallel TSP implementations on hypercube machines are discussed in [40], as well as various alternative representations of the work sharing and tour abstractions on distributed memory machines. 29 There has been some earlier work on parallel program portability [10, 1, 9]. In <ref> [1, 9] </ref>, authors have proposed portable models analogous to programming languages. Similar to our work, their approach is based on abstractions where interfaces and implementations of libraries are clearly separated. Based on the execution environment, an interface is bound to an efficient implementation. <p> Based on the execution environment, an interface is bound to an efficient implementation. Although the DSA run-time system is based on a similar object oriented approach to increase portability, the focus of our work is quite different. While their emphasis is on portability based on models <ref> [1] </ref>, late binding and program annotation [9] , our focus is on an active message style mechanism to implement object fragments to support efficient shared abstractions in parallel programs.
Reference: [2] <author> T.E. Anderson. </author> <title> The performance of spin lock alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: Similarly, the differences in local to remote memory access costs in most larger-scale parallel machines (i.e., the NUMA properties of such machines [28]) require substantive changes in the implementation of synchronization constructs for small-scale parallel machines like Sequents <ref> [2] </ref> or Silicon Graphics multiprocessors to larger-scale parallel machines [31]. The contribution of our work toward increased scalability and portability of parallel programs is the provision of the DSA library for the efficient implementation of objects termed "Distributed Shared Abstractions (DSA)".
Reference: [3] <author> H.E. Bal, M.F. Kaashoek, </author> <title> and A.S. Tanenbaum. Orca: A language for parallel programming of distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 13(3), </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: Such an implementation and performance results attained on a cluster of workstations are described in [27]. Third, ongoing research on distributed objects is contributing language and compiler support for describing objects and then compiling object interactions into efficient runtime invocations, using custom communication protocols <ref> [3] </ref> and/or exploiting active message paradigms [25, 48, 21]. We share with such work the assumption that communications between different object fragments can often benefit from the use of active messages and that the use of active messages can improve the locality of parallel programs [47].
Reference: [4] <author> J.K. Bennett, J.B. Carter, and W. Zwaenepol. Munin: </author> <title> Distributed shared memory based on type-specific memory coherence. </title> <booktitle> In Second Symposium on Principles and Practice of Parallel Programming, ACM, </booktitle> <volume> 23, 5, </volume> <month> March </month> <year> 1990. </year>
Reference-contexts: There are several differences of our research to current work on distributed shared abstractions. First, in contrast to recent research in cache architectures for parallel machines (e.g., the DASH project [16]) and in weakly consistent distributed shared memory <ref> [4, 22, 29] </ref>, we do not assume a fixed model (or limited number of models) of consistency between object fragments. Instead, programmers can implement object-specific protocols for state consistency among object fragments, using the low-level remote invocation mechanism offered by the DSA library. <p> These results and similar results reported for distributed memory machines [13, 40] are our main motivation for rejecting conceptually simpler approaches like distributed shared memory <ref> [4] </ref> for the implementation of shared abstractions in parallel programs. depicting the total number of nodes expanded in order to arrive at a solution. <p> While their emphasis is on portability based on models [1], late binding and program annotation [9] , our focus is on an active message style mechanism to implement object fragments to support efficient shared abstractions in parallel programs. Related research regarding weak memory or fragmented memory in distributed systems <ref> [4, 46] </ref> has already been reviewed in the introductory sections of this paper. 6 Conclusions and Future Research This paper presents the DSA runtime library for the efficient implementation of distributed shared abstractions in medium-scale multiprocessor systems.
Reference: [5] <author> Andrew D. Birrel and Bruce Jay Nelson. </author> <title> Implementing remote procedure calls. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(1) </volume> <pages> 39-59, </pages> <month> Feb. </month> <year> 1984. </year>
Reference-contexts: Portability of the DSA library to different shared memory multiprocessors is due to the portability of the underlying Cthreads library. Portability of DSA-based programs from shared to distributed memory machines (including workstation networks) is due to the use of an easily ported remote invocation mechanism <ref> [5] </ref> for communication among object fragments.
Reference: [6] <author> E.M. Chaves, Jr., </author> <title> P.C. Das, T.L. LeBlanc, B.D. Marsh, and M.L. Scott. Kernel-kernel communication in a shared-memory multiprocessor. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 5(3) </volume> <pages> 171-192, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: For example, for the BBN Butterfly, the ratio of access costs to local vs. remote memory is approximately 1:12, which implies that the costs of executing an operation on a shared abstraction strongly depends on the number of remote references performed by the operation <ref> [6] </ref>. <p> The tradeoffs between remote invocation and remote memory access are empirically evaluated on the BBN Butterfly in <ref> [6, 8] </ref>. Results reported by the authors show that the overheads associated with explicit synchronization and remote references increase with increasing `sizes' of remotely accessed data and code (i.e., with the complexity of remote operations), whereas the overheads associated with remote invocation do not depend on those sizes. <p> Both of these measurements are attained with an invocation block containing a two byte parameter. A kernel-level implementation of remote invocations in immediate mode like the one described in <ref> [6] </ref> (using hardware interrupts instead of Unix signals) would reduce the cost of immediate remote invocations to roughly 500 seconds on the BBN Butterfly.
Reference: [7] <author> E. Cooper and R. Draves. </author> <title> C threads. </title> <type> Technical Report CMU-CS-88-154, </type> <institution> Dept. of Computer Science, Carnegie Mellon University, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: DSA support is implemented as a runtime library layered on top of a Mach-compatible Cthreads package developed by our group <ref> [7, 33] </ref>. As a result, a parallel program written with the DSA library consists of a set of independent threads interacting via DSA objects.
Reference: [8] <author> A. Cox, R. Fowler, and J. Veenstra. </author> <title> Interprocessor invocation on a numa multiprocessor. </title> <type> Technical Report TR 356, </type> <institution> University of Rochester, </institution> <year> 1990. </year>
Reference-contexts: The tradeoffs between remote invocation and remote memory access are empirically evaluated on the BBN Butterfly in <ref> [6, 8] </ref>. Results reported by the authors show that the overheads associated with explicit synchronization and remote references increase with increasing `sizes' of remotely accessed data and code (i.e., with the complexity of remote operations), whereas the overheads associated with remote invocation do not depend on those sizes.
Reference: [9] <author> L. Crowl. </author> <title> Architectural Adaptability in Parallel Programming. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Rochester, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: Other issues regarding parallel TSP implementations on hypercube machines are discussed in [40], as well as various alternative representations of the work sharing and tour abstractions on distributed memory machines. 29 There has been some earlier work on parallel program portability <ref> [10, 1, 9] </ref>. In [1, 9], authors have proposed portable models analogous to programming languages. Similar to our work, their approach is based on abstractions where interfaces and implementations of libraries are clearly separated. Based on the execution environment, an interface is bound to an efficient implementation. <p> Other issues regarding parallel TSP implementations on hypercube machines are discussed in [40], as well as various alternative representations of the work sharing and tour abstractions on distributed memory machines. 29 There has been some earlier work on parallel program portability [10, 1, 9]. In <ref> [1, 9] </ref>, authors have proposed portable models analogous to programming languages. Similar to our work, their approach is based on abstractions where interfaces and implementations of libraries are clearly separated. Based on the execution environment, an interface is bound to an efficient implementation. <p> Although the DSA run-time system is based on a similar object oriented approach to increase portability, the focus of our work is quite different. While their emphasis is on portability based on models [1], late binding and program annotation <ref> [9] </ref> , our focus is on an active message style mechanism to implement object fragments to support efficient shared abstractions in parallel programs.
Reference: [10] <author> D. Eager and J. Zahorjan. </author> <title> Enhanced run-time support for shared memory parallel computing. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(1) </volume> <pages> 1-32, </pages> <month> February </month> <year> 1993. </year> <month> 31 </month>
Reference-contexts: Other issues regarding parallel TSP implementations on hypercube machines are discussed in [40], as well as various alternative representations of the work sharing and tour abstractions on distributed memory machines. 29 There has been some earlier work on parallel program portability <ref> [10, 1, 9] </ref>. In [1, 9], authors have proposed portable models analogous to programming languages. Similar to our work, their approach is based on abstractions where interfaces and implementations of libraries are clearly separated. Based on the execution environment, an interface is bound to an efficient implementation.
Reference: [11] <author> Greg Eisenhauer, Weiming Gu, Thomas Kindler, Karsten Schwan, Dilma Silva, and Jeffrey Vetter. </author> <title> Opportunities and tools for highly interactive distributed and parallel computing. </title> <type> Technical Report GIT-CC-94-58, </type> <institution> Georgia Institute of Technology, College of Computing, </institution> <address> Atlanta, GA 30332-0280, </address> <month> December </month> <year> 1994. </year> <booktitle> Also in Proceedings of The Workshop On Debugging and Tuning for Parallel Computing Systems, </booktitle> <address> Chatham, MA, </address> <month> October, </month> <year> 1994. </year>
Reference-contexts: Third, we are developing application programs able to use such parallel and distributed machines, and we are constructing monitoring support for on-line program viewing and evaluation <ref> [11, 19] </ref>. The DSA library software is available in the public domain by remote FTP access to ftp.cc.gatech.edu.
Reference: [12] <author> Ed Felton. </author> <title> Best-first branch-and-bound on a hypercube. </title> <booktitle> In Third Conference on Hypercube Concurrent Computers and Applications, ACM, </booktitle> <month> Jan. </month> <year> 1988. </year>
Reference-contexts: Specifically, similar to the strategy used by Felten in <ref> [12] </ref>, every two `get' operations by a searcher thread on its local queue trigger a move of the second best node from the local queue to the next subqueue along the ring. As a result, `good' nodes are frequently shared among different searcher threads. <p> Furthermore, his implementations carefully avoid the use of global knowledge. However, we share the notion of `fairness' regarding work distribution among searcher threads with Finkel's work. In <ref> [12] </ref>, issues concerning the implementation of a best-first branch-and-bound algorithm on a hypercube multicomputer are discussed. Felten uses a structure called a `decentralized queue' for storage of the nodes of the search tree. This structure is similar to our work sharing abstractions (variants distributed and distributedQB).
Reference: [13] <author> R. Finkel and U. Manber. </author> <title> Dib a distributed implementation of backtracking. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(2) </volume> <pages> 235-255, </pages> <month> Apr </month> <year> 1987. </year>
Reference-contexts: Sample applications structured in this fashion range from (1) domain-decomposed scientific applications to (2) MultiLisp implementations on parallel machines, where "futures" are entered into queues and removed and processed by available processors [20], to (3) parallel optimization codes <ref> [13, 40] </ref>, and (4) even operating system services like file or I/O servers. The sample parallel program used in our research is a client/server structured application, a parallel branch-and-bound algorithm solving the Traveling Salesperson problem (TSP). <p> First, branch-and-bound algorithms are commonly used in the solution of optimization problems and have therefore, been frequently studied and evaluated on parallel machines. Second, experimental evaluations of the algorithm's implementation on distributed memory platforms [43, 40] and on workstation networks <ref> [13] </ref> have already demonstrated the importance of the work sharing and tour abstractions to parallel program performance, where different implementations of the queue itself and of load balancing among queue fragments significantly effect speedup and scalability. <p> This may be stated as the first important insight from these results: * To attain acceptable performance, it may be critical to use information about program semantics in the implementation of distributed shared abstractions. These results and similar results reported for distributed memory machines <ref> [13, 40] </ref> are our main motivation for rejecting conceptually simpler approaches like distributed shared memory [4] for the implementation of shared abstractions in parallel programs. depicting the total number of nodes expanded in order to arrive at a solution. <p> An issue not discussed above is the storage of node data, which results in performance differences regarding the expansion of locally vs. remotely stored nodes. In this implementation of TSP on the BBN Butterfly machine, such differences are not as significant as in distributed memory implementations <ref> [13, 40] </ref>. Expansion of a locally vs. remotely stored node can be performed in 25 milliseconds vs. 27 milliseconds. <p> Previous work on parallel TSP includes that of Mohan in [32] employing the LMSK algorithm which we adopted for use with the DSA library. Finkel's distributed implementations of branch-and-bound algorithms <ref> [13] </ref> are difficult to compare to ours, because he analyzes the performance of TSP for alternative work distributions rather than for alternative methods of work sharing, for distributed termination, and for fault tolerance. Furthermore, his implementations carefully avoid the use of global knowledge.
Reference: [14] <author> I. Foster, C. Kesselman, and S. Tuecke. </author> <title> Nexus: Runtime support for task-parallel programming languages. </title> <type> Technical report, </type> <institution> Argonne National Laboratory, </institution> <year> 1995. </year>
Reference-contexts: Vertex specification is necessary since it is possible to map multiple vertices of an object to the same processor node (ie., a vertex plays a role somewhat similar to a `context' in Nexus <ref> [14] </ref>). An error status is returned if the specified vertex is not located on the calling thread's node. The `top close' routine breaks the binding associated with `obj handle', but it does not `clean up' object state for future use.
Reference: [15] <author> Geoffrey C. Fox, M. A. Johnson, G. A. Lyzenga, S. W. Otto, J. K. Salmon, and D. W. Walker. </author> <title> Solving Problems On Concurrent Processors. </title> <publisher> Prentice-Hall, </publisher> <year> 1988. </year>
Reference-contexts: For example, a port of a parallel program from a shared memory to a distributed memory machine typically requires the re-implementation of its shared abstractions from using explicit synchronization constructs to sending messages across statically or dynamically defined communication structures linking the program's processes <ref> [41, 15] </ref>. Similarly, the differences in local to remote memory access costs in most larger-scale parallel machines (i.e., the NUMA properties of such machines [28]) require substantive changes in the implementation of synchronization constructs for small-scale parallel machines like Sequents [2] or Silicon Graphics multiprocessors to larger-scale parallel machines [31].
Reference: [16] <author> Kourosh Gharchorloo, Daniel Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: There are several differences of our research to current work on distributed shared abstractions. First, in contrast to recent research in cache architectures for parallel machines (e.g., the DASH project <ref> [16] </ref>) and in weakly consistent distributed shared memory [4, 22, 29], we do not assume a fixed model (or limited number of models) of consistency between object fragments.
Reference: [17] <author> Ahmed Gheith and Karsten Schwan. </author> <title> Chaos-arc kernel support for multi-weight objects, invocations, and atomicity in real-time applications. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(1) </volume> <pages> 33-72, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Second, we will explore additional topics concerning the efficient support and implementation of distributed shared abstractions, including: (1) the customization and parallelization of network protocols for use in DSA implementations [30], 30 (2) the embedding of DSA objects in a more general system for the construction of object--oriented parallel programs <ref> [17] </ref>, including the development of language-level and user interface support for object-based parallel programming, and (3) the interfacing of Cthreads and the DSA library with distributed programs written using existing general frameworks for programming networked, heterogeneous parallel machines, such as Nexus, PVM, or MPI.
Reference: [18] <author> Kaushik Ghosh, Bodhisattwa Mukherjee, and Karsten Schwan. </author> <title> Experimentation with configurable, lightweight threads on a ksr multiprocessor. </title> <type> Technical Report GIT-CC-93/37, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <year> 1993. </year>
Reference-contexts: Programmers do not perceive the memory hierarchy existing in the machine (other than by potentially observing performance penalties). Access to non-local memory results in the corresponding cache line being migrated to the local cache, so that future accesses to that memory element are relatively cheap <ref> [18, 35] </ref>.
Reference: [19] <author> Weiming Gu, Greg Eisenhauer, Eileen Kraemer, Karsten Schwan, John Stasko, Jeffrey Vetter, and Nirupama Mallavarupu. </author> <title> Falcon: On-line monitoring and steering of large-scale parallel programs. </title> <type> Technical Report GIT-CC-94-21, </type> <institution> Georgia Institute of Technology, College of Computing, </institution> <address> Atlanta, GA 30332-0280, </address> <month> April </month> <year> 1994. </year> <note> Also in Frontiers 95. </note>
Reference-contexts: Last, shared abstractions are easily instrumented, evaluated [41, 37, 26], and even dynamically adjusted, us 2 ing custom [35] or library-provided <ref> [19] </ref> mechanisms for on-line program monitoring and without exposing such instrumentation to application programs [35]. The remainder of this paper first presents a sample parallel application and the abstractions shared by its concurrent processes (Section 2). <p> Third, we are developing application programs able to use such parallel and distributed machines, and we are constructing monitoring support for on-line program viewing and evaluation <ref> [11, 19] </ref>. The DSA library software is available in the public domain by remote FTP access to ftp.cc.gatech.edu.
Reference: [20] <author> Robert H. Halstead, Jr. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-538, </pages> <month> Oct. </month> <year> 1985. </year>
Reference-contexts: Sample applications structured in this fashion range from (1) domain-decomposed scientific applications to (2) MultiLisp implementations on parallel machines, where "futures" are entered into queues and removed and processed by available processors <ref> [20] </ref>, to (3) parallel optimization codes [13, 40], and (4) even operating system services like file or I/O servers. The sample parallel program used in our research is a client/server structured application, a parallel branch-and-bound algorithm solving the Traveling Salesperson problem (TSP).
Reference: [21] <author> W.C. Hsieh, K.L. Johnson, M.F. Kaashoek, D.A. Wallach, and W.E. Weihl. </author> <title> Optimistic active messages: A mechanism for scheduling communication with computation. </title> <booktitle> In Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Third, ongoing research on distributed objects is contributing language and compiler support for describing objects and then compiling object interactions into efficient runtime invocations, using custom communication protocols [3] and/or exploiting active message paradigms <ref> [25, 48, 21] </ref>. We share with such work the assumption that communications between different object fragments can often benefit from the use of active messages and that the use of active messages can improve the locality of parallel programs [47].
Reference: [22] <author> Phil W. Hutto and Mustaque Ahamad. </author> <title> Slow memory: Weakening consistency to enhance concurrency in distributed shared memories. </title> <booktitle> In Proceedings of the International Conference on Distributed Computing Systems, </booktitle> <pages> pages 302-311, </pages> <year> 1990. </year>
Reference-contexts: There are several differences of our research to current work on distributed shared abstractions. First, in contrast to recent research in cache architectures for parallel machines (e.g., the DASH project [16]) and in weakly consistent distributed shared memory <ref> [4, 22, 29] </ref>, we do not assume a fixed model (or limited number of models) of consistency between object fragments. Instead, programmers can implement object-specific protocols for state consistency among object fragments, using the low-level remote invocation mechanism offered by the DSA library.
Reference: [23] <author> D. Sweeney J.D. Little, K. Murty and C. Karel. </author> <title> An algorithm for the traveling salesman problem. </title> <journal> Operations Research, </journal> <volume> 11, </volume> <year> 1963. </year> <month> 32 </month>
Reference-contexts: When all leaf nodes have been expanded or pruned, the lowest of all the found tours is the solution of the problem. A more complete description of the algorithm can be found in <ref> [23] </ref>. 2.2 Parallel Implementation of the LMSK Algorithm Our parallel LMSK algorithm is implemented as a collection of asynchronous, cooperating searcher threads each of which independently executes the algorithm's main procedure. The resulting code is outlined in Figure 1 (it implements the LMSK search algorithm as described in Section 2.1).
Reference: [24] <author> Anita K. Jones and Karsten Schwan. </author> <title> Task forces: Distributed software for solving prob-lems of substantial size. </title> <booktitle> In Proceedings of the 4th International Conference on Software Engineering, </booktitle> <address> Munich, </address> <publisher> W. Germany, </publisher> <pages> pages 315-329, </pages> <month> Sept. </month> <year> 1979. </year>
Reference-contexts: Many parallel programs resulting from such decompositions exhibit coordinator/server structures, where coordinator processes generate work units processed by workers <ref> [24] </ref> or at least supervise a number of workers.
Reference: [25] <author> V. Karamcheti and A. Chien. </author> <title> Concert efficient runtime support for concurrent object-oriented programming languages on stock hardware. </title> <booktitle> In Proceedings of Supercomputing, </booktitle> <address> Portland, OR. </address> <publisher> ACM, </publisher> <month> May </month> <year> 1993. </year>
Reference-contexts: Third, ongoing research on distributed objects is contributing language and compiler support for describing objects and then compiling object interactions into efficient runtime invocations, using custom communication protocols [3] and/or exploiting active message paradigms <ref> [25, 48, 21] </ref>. We share with such work the assumption that communications between different object fragments can often benefit from the use of active messages and that the use of active messages can improve the locality of parallel programs [47].
Reference: [26] <author> Carol Kilpatrick and Karsten Schwan. </author> <title> Chaosmon - application-specific monitoring and display of performance information for parallel and distributed systems. </title> <booktitle> In ACM Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pages 57-67, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: In contrast to those implementations, the layering of DSA objects on a basic remote invocation mechanism has resulted in library portability to various target platforms, including the aforementioned shared memory platforms and a recently completed implementation on a network platform [27]. Last, shared abstractions are easily instrumented, evaluated <ref> [41, 37, 26] </ref>, and even dynamically adjusted, us 2 ing custom [35] or library-provided [19] mechanisms for on-line program monitoring and without exposing such instrumentation to application programs [35]. The remainder of this paper first presents a sample parallel application and the abstractions shared by its concurrent processes (Section 2).
Reference: [27] <author> Prince Kohli, Mustaque Ahamad, and Karsten Schwan. Indigo: </author> <title> User-level support for building distributed shared absractions. </title> <booktitle> In Fourth IEEE International Symposium on High-Performance Ditributed Computing (HPDC-4), </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: Such an implementation and performance results attained on a cluster of workstations are described in <ref> [27] </ref>. Third, ongoing research on distributed objects is contributing language and compiler support for describing objects and then compiling object interactions into efficient runtime invocations, using custom communication protocols [3] and/or exploiting active message paradigms [25, 48, 21]. <p> In contrast to those implementations, the layering of DSA objects on a basic remote invocation mechanism has resulted in library portability to various target platforms, including the aforementioned shared memory platforms and a recently completed implementation on a network platform <ref> [27] </ref>. Last, shared abstractions are easily instrumented, evaluated [41, 37, 26], and even dynamically adjusted, us 2 ing custom [35] or library-provided [19] mechanisms for on-line program monitoring and without exposing such instrumentation to application programs [35]. <p> These tradeoffs concern the use of active messages vs. polling for remote fragment invocation. The DSA library's generality is demonstrated by its use on two different target parallel machines, 32-node GP1000 BBN Butterfly and KSR-1 multiprocessors. Additional implementations existing for SGI multiprocessors and for distributed systems <ref> [27] </ref> are not evaluated in this paper. Descriptions of target hardware. <p> In the shared memory implementation, the message associated with the event is transmitted by reference using a low-level mailbox communication facility. In the distributed memory implementation described in [41], event transmission employed modifications of the low-level communication protocol on the iPSC hypercube. The implementation described in <ref> [27] </ref> sends messages between daemon processes when events are transmitted. <p> First, we will explore a variety of implementation techniques for high performance distributed and parallel objects for multiple target platforms. For example, we have already developed a first prototype of a framework for implementation of distributed shared abstractions and of distributed shared memory on networked machines <ref> [27] </ref>. In addition, on the KSR-2 multiprocessor platform, we have evaluated the extension of the underlying Cthreads packages with multiple, heterogeneous schedulers [34], so that one scheduler runs threads that perform event execution and a second scheduler runs application threads.
Reference: [28] <author> T. J. Leblanc. </author> <title> Shared memory versus message-passing in a tightly-coupled multiprocessor: A case study. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 463-466, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Similarly, the differences in local to remote memory access costs in most larger-scale parallel machines (i.e., the NUMA properties of such machines <ref> [28] </ref>) require substantive changes in the implementation of synchronization constructs for small-scale parallel machines like Sequents [2] or Silicon Graphics multiprocessors to larger-scale parallel machines [31]. <p> For shared memory multiprocessors, similar results have been attained for RPC implementations on NUMA machines like the BBN Butterfly multiprocessor <ref> [28] </ref> and are demonstrated in this paper for a program-specific abstraction (i.e., a shared queue) in a parallel branch-and-bound application executed on a 32-node GP1000 BBN Butterfly and a 32-node Kendall Square Research KSR-1 supercomputer.
Reference: [29] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: There are several differences of our research to current work on distributed shared abstractions. First, in contrast to recent research in cache architectures for parallel machines (e.g., the DASH project [16]) and in weakly consistent distributed shared memory <ref> [4, 22, 29] </ref>, we do not assume a fixed model (or limited number of models) of consistency between object fragments. Instead, programmers can implement object-specific protocols for state consistency among object fragments, using the low-level remote invocation mechanism offered by the DSA library.
Reference: [30] <author> Bert Lindgren, Bobby Krupczak, Mostafa Ammar, and Karsten Schwan. </author> <title> Parallel and configurable protocols: Experiences with a prototype and an architectural framework. </title> <booktitle> In International Conference on Network Protocols., </booktitle> <year> 1993. </year> <note> TR# GIT-CC-93/22. </note>
Reference-contexts: Second, we will explore additional topics concerning the efficient support and implementation of distributed shared abstractions, including: (1) the customization and parallelization of network protocols for use in DSA implementations <ref> [30] </ref>, 30 (2) the embedding of DSA objects in a more general system for the construction of object--oriented parallel programs [17], including the development of language-level and user interface support for object-based parallel programming, and (3) the interfacing of Cthreads and the DSA library with distributed programs written using existing general
Reference: [31] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for scalable synchronization on shared-memory multiprocssors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: Similarly, the differences in local to remote memory access costs in most larger-scale parallel machines (i.e., the NUMA properties of such machines [28]) require substantive changes in the implementation of synchronization constructs for small-scale parallel machines like Sequents [2] or Silicon Graphics multiprocessors to larger-scale parallel machines <ref> [31] </ref>. The contribution of our work toward increased scalability and portability of parallel programs is the provision of the DSA library for the efficient implementation of objects termed "Distributed Shared Abstractions (DSA)".
Reference: [32] <author> Joseph Mohan. </author> <title> Experience with two parallel programs solving the parallel salesman problem. </title> <booktitle> In Proceedings of the 12th International Conference on Parallel Processing, </booktitle> <pages> pages 191-193, </pages> <month> Aug. </month> <year> 1983. </year>
Reference-contexts: The sample parallel program used in our research is a client/server structured application, a parallel branch-and-bound algorithm solving the Traveling Salesperson problem (TSP). We employ the algorithm of Little, Murty, Sweeney and Karel (LMSK algorithm)[23], and we use a parallelization first described in <ref> [32] </ref>. <p> Previous work on parallel TSP includes that of Mohan in <ref> [32] </ref> employing the LMSK algorithm which we adopted for use with the DSA library.
Reference: [33] <author> Bodhisattwa Mukherjee. </author> <title> A portable and reconfigurable threads package. </title> <booktitle> In Proceedings of Sun User Group Technical Conference, </booktitle> <pages> pages 101-112, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: DSA support is implemented as a runtime library layered on top of a Mach-compatible Cthreads package developed by our group <ref> [7, 33] </ref>. As a result, a parallel program written with the DSA library consists of a set of independent threads interacting via DSA objects. <p> Performance evaluation is performed on two different target machines: (1) a 32-node GP1000 BBN Butterfly multiprocessor and (2) a 32-node Kendall Square Research KSR-1 supercomputer. The library's portability to those machines is due to its implementation on top of a portable lightweight threads package developed by our group <ref> [33] </ref>. 3 The DSA Library: Implementing Distributed Objects The DSA library facilitates the implementation of shared, distributed abstractions in parallel programs in two ways: 1. at user-level, by providing application programs with a uniform interface to shared ab stractions, and 2. at representation-level and implementation-level, by providing machine-independent mechanisms for the
Reference: [34] <author> Bodhisattwa Mukherjee and Karsten Schwan. </author> <title> Experimentation with a reconfigurable micro-kernel. </title> <booktitle> In Proc. of Second workshop on Microkernels and Other Kernel Architectures, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: For example, we have already developed a first prototype of a framework for implementation of distributed shared abstractions and of distributed shared memory on networked machines [27]. In addition, on the KSR-2 multiprocessor platform, we have evaluated the extension of the underlying Cthreads packages with multiple, heterogeneous schedulers <ref> [34] </ref>, so that one scheduler runs threads that perform event execution and a second scheduler runs application threads.
Reference: [35] <author> Bodhisattwa Mukherjee and Karsten Schwan. </author> <title> Improving performance by use of adaptive objects: Experimentation with a configurable multiprocessor thread package. </title> <booktitle> In Proc. of Second International Symposium on High Performance Distributed Computing (HPDC-2), </booktitle> <pages> pages 59-66, </pages> <month> July </month> <year> 1993. </year> <note> Also TR# GIT-CC-93/17. </note>
Reference-contexts: Last, shared abstractions are easily instrumented, evaluated [41, 37, 26], and even dynamically adjusted, us 2 ing custom <ref> [35] </ref> or library-provided [19] mechanisms for on-line program monitoring and without exposing such instrumentation to application programs [35]. The remainder of this paper first presents a sample parallel application and the abstractions shared by its concurrent processes (Section 2). <p> Last, shared abstractions are easily instrumented, evaluated [41, 37, 26], and even dynamically adjusted, us 2 ing custom <ref> [35] </ref> or library-provided [19] mechanisms for on-line program monitoring and without exposing such instrumentation to application programs [35]. The remainder of this paper first presents a sample parallel application and the abstractions shared by its concurrent processes (Section 2). Next, the performance effects of alternative, shared memory implementations of such shared abstractions are evaluated experimentally on 32-node BBN Butterfly and KSR multiprocessors. <p> Programmers do not perceive the memory hierarchy existing in the machine (other than by potentially observing performance penalties). Access to non-local memory results in the corresponding cache line being migrated to the local cache, so that future accesses to that memory element are relatively cheap <ref> [18, 35] </ref>.
Reference: [36] <author> Bodhisattwa Mukherjee, Dilma Silva, Karsten Schwan, and Ahmed Gheith. Ktk: </author> <title> Kernel support for configurable objects and invocations. </title> <journal> Distributed Systems Engineering Journal, </journal> <pages> pages 259-270, </pages> <month> september </month> <year> 1994. </year> <month> 33 </month>
Reference-contexts: However, we also posit that the compilation techniques and runtime support described in such work should be enhanced to support object fragmentation as well as other well-known techniques for optimizing object performance (e.g., caching, the use of knowledge about object semantics expressed by attributes <ref> [36] </ref>, etc.). Fourth, in contrast to the research of Shapiro on fragmented objects [46], we explicitly consider the communication structure linking object fragments in order to exploit application-specific knowledge of the object's communication patterns.
Reference: [37] <author> David M. Ogle, Karsten Schwan, and Richard Snodgrass. </author> <title> The dynamic monitoring of real--time distributed and parallel systems. </title> <type> Technical report, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <address> ICS-GIT-90/23, Atlanta, GA 30332, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: In contrast to those implementations, the layering of DSA objects on a basic remote invocation mechanism has resulted in library portability to various target platforms, including the aforementioned shared memory platforms and a recently completed implementation on a network platform [27]. Last, shared abstractions are easily instrumented, evaluated <ref> [41, 37, 26] </ref>, and even dynamically adjusted, us 2 ing custom [35] or library-provided [19] mechanisms for on-line program monitoring and without exposing such instrumentation to application programs [35]. The remainder of this paper first presents a sample parallel application and the abstractions shared by its concurrent processes (Section 2).
Reference: [38] <author> M. Satayanarayanan, J. Howard, D. Nichols, R. Sidebotham, A. Spector, and M. West. </author> <title> The itc distributed file system: </title> <booktitle> Principles and design. In Proceedings of the Tenth ACM Symposium on Operating System Principles, </booktitle> <pages> pages 35-50, </pages> <month> Dec. </month> <year> 1985. </year>
Reference-contexts: Performance benefits derived from the fragmentation of objects or object state have been shown possible for many implementations of higher level operating system services in distributed systems (e.g., file systems <ref> [38] </ref>) and for application-specific services on distributed memory machines [41].
Reference: [39] <author> M. Schroeder and M. Burrows. </author> <title> Performance or firefly rpc. </title> <booktitle> In Twelfth ACM Symposium on Operating Systems, </booktitle> <pages> pages 83-90, </pages> <month> Dec. </month> <year> 1989. </year>
Reference: [40] <author> Karsten Schwan, Ben Blake, Win Bo, and John Gawkowski. </author> <title> Global data and control in multicomputers: Operating system primitives and experimentation with a parallel branch-and-bound algorithm. </title> <journal> Concurrency: Practice and Experience, </journal> <pages> pages 191-218, </pages> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: Sample applications structured in this fashion range from (1) domain-decomposed scientific applications to (2) MultiLisp implementations on parallel machines, where "futures" are entered into queues and removed and processed by available processors [20], to (3) parallel optimization codes <ref> [13, 40] </ref>, and (4) even operating system services like file or I/O servers. The sample parallel program used in our research is a client/server structured application, a parallel branch-and-bound algorithm solving the Traveling Salesperson problem (TSP). <p> The sample parallel algorithm described in this paper is interesting for three reasons. First, branch-and-bound algorithms are commonly used in the solution of optimization problems and have therefore, been frequently studied and evaluated on parallel machines. Second, experimental evaluations of the algorithm's implementation on distributed memory platforms <ref> [43, 40] </ref> and on workstation networks [13] have already demonstrated the importance of the work sharing and tour abstractions to parallel program performance, where different implementations of the queue itself and of load balancing among queue fragments significantly effect speedup and scalability. <p> This strategy favors nodes that are likely to lead to good solutions fast. It has been shown useful in other LMSK implementations <ref> [40] </ref>, since it tends to minimize the total number of nodes present in the final search tree. We term a queue implementation consistent if a global priority ordering is maintained among queue elements. <p> This may be stated as the first important insight from these results: * To attain acceptable performance, it may be critical to use information about program semantics in the implementation of distributed shared abstractions. These results and similar results reported for distributed memory machines <ref> [13, 40] </ref> are our main motivation for rejecting conceptually simpler approaches like distributed shared memory [4] for the implementation of shared abstractions in parallel programs. depicting the total number of nodes expanded in order to arrive at a solution. <p> An issue not discussed above is the storage of node data, which results in performance differences regarding the expansion of locally vs. remotely stored nodes. In this implementation of TSP on the BBN Butterfly machine, such differences are not as significant as in distributed memory implementations <ref> [13, 40] </ref>. Expansion of a locally vs. remotely stored node can be performed in 25 milliseconds vs. 27 milliseconds. <p> Other issues regarding parallel TSP implementations on hypercube machines are discussed in <ref> [40] </ref>, as well as various alternative representations of the work sharing and tour abstractions on distributed memory machines. 29 There has been some earlier work on parallel program portability [10, 1, 9]. In [1, 9], authors have proposed portable models analogous to programming languages.
Reference: [41] <author> Karsten Schwan and Win Bo. </author> <title> Topologies distributed objects on multicomputers. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 8(2) </volume> <pages> 111-157, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: For example, a port of a parallel program from a shared memory to a distributed memory machine typically requires the re-implementation of its shared abstractions from using explicit synchronization constructs to sending messages across statically or dynamically defined communication structures linking the program's processes <ref> [41, 15] </ref>. Similarly, the differences in local to remote memory access costs in most larger-scale parallel machines (i.e., the NUMA properties of such machines [28]) require substantive changes in the implementation of synchronization constructs for small-scale parallel machines like Sequents [2] or Silicon Graphics multiprocessors to larger-scale parallel machines [31]. <p> The contribution of our work toward increased scalability and portability of parallel programs is the provision of the DSA library for the efficient implementation of objects termed "Distributed Shared Abstractions (DSA)". Scalability on SMP machines is achieved by implementation of such objects as sets of object fragments <ref> [41, 45] </ref> linked by a user-defined communication structure, which we term a topology. The resulting parallel program's portability is improved by encapsulation of its shared abstractions as objects with operational interfaces that may remain invariant across objects' different implementations for specific target machines. <p> Performance benefits derived from the fragmentation of objects or object state have been shown possible for many implementations of higher level operating system services in distributed systems (e.g., file systems [38]) and for application-specific services on distributed memory machines <ref> [41] </ref>. <p> The fifth difference of our work to other research concerns our previous kernel-level implementation of DSA functionality on hypercube machines <ref> [41] </ref>. In contrast to those implementations, the layering of DSA objects on a basic remote invocation mechanism has resulted in library portability to various target platforms, including the aforementioned shared memory platforms and a recently completed implementation on a network platform [27]. <p> In contrast to those implementations, the layering of DSA objects on a basic remote invocation mechanism has resulted in library portability to various target platforms, including the aforementioned shared memory platforms and a recently completed implementation on a network platform [27]. Last, shared abstractions are easily instrumented, evaluated <ref> [41, 37, 26] </ref>, and even dynamically adjusted, us 2 ing custom [35] or library-provided [19] mechanisms for on-line program monitoring and without exposing such instrumentation to application programs [35]. The remainder of this paper first presents a sample parallel application and the abstractions shared by its concurrent processes (Section 2). <p> We continue to use shared memory for implementation of the "tour" abstraction, since the performance impacts of alternative implementations of this abstraction are small in our application (this may not hold when tours are found more frequently and/or for implementations on distributed memory machines, as described in <ref> [41] </ref>). The work sharing queue implementation for TSP stores the current leaf nodes of the search tree. In a parallel implementation, this abstraction implements: (1) a node selection heuristic, (2) a work distribution strategy, and (3) a protocol for program termination. <p> resulting sharing of nodes with two neighbors may increase performance in larger-scale shared memory multiprocessors, but will (1) decrease performance in the DSA library implementation of TSP due to the required additional remote invocations and (2) may decrease performance in message based systems due to the required additional message operations <ref> [41] </ref>. 6 TSP problems. Each TSP problem has 32 cities and is described by an initial random cost matrix, with costs in the range of 1 to 50. Each TSP problem is executed for each of the three work sharing abstractions, with the same initial cost matrix. <p> Such a generalization of DSA objects can be useful, and is discussed further in <ref> [41] </ref>. 13 structures using hardware-supported broadcast instructions or broadcast trees are commonly used). Figure 8 depicts the directed graph representing the object's internal communication structure and the vertices representing the object identical fragments. Communications among vertices are not visible to object invokers. <p> As an example, consider the computation of a global sum using a `combining tree' for the incremental collection and addition of individual threads' contributions to the sum <ref> [41] </ref>. Here, a service in a vertex at a certain level in the combining tree cannot be scheduled for execution until its bound thread has performed its invocations (i.e., contributed its partial sum) and until tree nodes at the lower levels of the combining tree have contributed their partial sums. <p> In DSA, a remote invocation is initiated by calls to the `top output edges' or `top output vertex' routines. The resulting remote service invocation is similar to active messages, but is actually derived from the active message mechanism employed in our earlier work on distributed objects on hypercube machines <ref> [41] </ref>. A remote invocation is comprised of the following steps: (1) extract a free invocation block from the target vertex' pool using remote references, (2) copy the invocation data into this block, including parameters, and (3) send a request containing the invocation block to the target fragment and processor. <p> These two data structures are protected by a spin lock. In the shared memory implementation, the message associated with the event is transmitted by reference using a low-level mailbox communication facility. In the distributed memory implementation described in <ref> [41] </ref>, event transmission employed modifications of the low-level communication protocol on the iPSC hypercube. The implementation described in [27] sends messages between daemon processes when events are transmitted.
Reference: [42] <author> Karsten Schwan, Harold Forbes, Ahmed Gheith, Bodhisattwa Mukherjee, and Yiannis Samiotakis. </author> <title> A c thread library for multiprocessors. </title> <type> Technical report, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <address> Atlanta, GA 30332, GIT-ICS-91/02, </address> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: Upon reception of a creation event, the event dispatcher calls a setup procedure, which enqueues the transmitted vcb in the local vertex queue. Given these steps, the performance of this call depends primarily on the performance of the threads library's calls for memory allocation (see <ref> [42] </ref>) and on the DSA library's calls for remote fragment invocation. The latter are reviewed below. Object binding. A user thread binds itself to an object's vertex using the `top open' routine.
Reference: [43] <author> Karsten Schwan, John Gawkowski, and Ben Blake. </author> <title> Process and workload migration for a parallel branch-and-bound algorithm on a hypercube multicomputer. </title> <booktitle> In Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 1520-1530, </pages> <month> Jan. </month> <year> 1988. </year>
Reference-contexts: The sample parallel algorithm described in this paper is interesting for three reasons. First, branch-and-bound algorithms are commonly used in the solution of optimization problems and have therefore, been frequently studied and evaluated on parallel machines. Second, experimental evaluations of the algorithm's implementation on distributed memory platforms <ref> [43, 40] </ref> and on workstation networks [13] have already demonstrated the importance of the work sharing and tour abstractions to parallel program performance, where different implementations of the queue itself and of load balancing among queue fragments significantly effect speedup and scalability.
Reference: [44] <author> Karsten Schwan, Hongyi Zhou, and Ahmed Gheith. </author> <title> Real-time threads. </title> <journal> Operating Systems Review, </journal> <volume> 25(4) </volume> <pages> 35-46, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The vertices of an object are numbered from `0' to `N-1'. A specific vertex is identified by its `vertex id', which is the vertex number. Routines able to generate such a matrix at the time of program initialization <ref> [44] </ref> may be used in place of the simple statically defined structure shown in this example. Mapping. The mapping of vertices to physical nodes is described by a table with `N' entries. The indices into this table are the vertex numbers, and the table elements are physical node numbers.
Reference: [45] <author> M. Shapiro. </author> <title> Structure and encapsulation in distributed systems: The proxy principle. </title> <booktitle> In Sixth International Conference on Distributed Computing Systems, </booktitle> <pages> pages 198-204, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: The contribution of our work toward increased scalability and portability of parallel programs is the provision of the DSA library for the efficient implementation of objects termed "Distributed Shared Abstractions (DSA)". Scalability on SMP machines is achieved by implementation of such objects as sets of object fragments <ref> [41, 45] </ref> linked by a user-defined communication structure, which we term a topology. The resulting parallel program's portability is improved by encapsulation of its shared abstractions as objects with operational interfaces that may remain invariant across objects' different implementations for specific target machines.
Reference: [46] <author> M. Shapiro. </author> <title> Object-supporting operating systems. </title> <journal> TCOS Newsletter, </journal> <volume> 5(1) </volume> <pages> 39-42, </pages> <year> 1991. </year>
Reference-contexts: Fourth, in contrast to the research of Shapiro on fragmented objects <ref> [46] </ref>, we explicitly consider the communication structure linking object fragments in order to exploit application-specific knowledge of the object's communication patterns. The fifth difference of our work to other research concerns our previous kernel-level implementation of DSA functionality on hypercube machines [41]. <p> While their emphasis is on portability based on models [1], late binding and program annotation [9] , our focus is on an active message style mechanism to implement object fragments to support efficient shared abstractions in parallel programs. Related research regarding weak memory or fragmented memory in distributed systems <ref> [4, 46] </ref> has already been reviewed in the introductory sections of this paper. 6 Conclusions and Future Research This paper presents the DSA runtime library for the efficient implementation of distributed shared abstractions in medium-scale multiprocessor systems.
Reference: [47] <author> Ellen Spertus and William J. Dally. </author> <title> Evaluating and locality benefits of active messages. </title> <booktitle> In Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: We share with such work the assumption that communications between different object fragments can often benefit from the use of active messages and that the use of active messages can improve the locality of parallel programs <ref> [47] </ref>. However, we also posit that the compilation techniques and runtime support described in such work should be enhanced to support object fragmentation as well as other well-known techniques for optimizing object performance (e.g., caching, the use of knowledge about object semantics expressed by attributes [36], etc.).
Reference: [48] <author> W. Weihl, E. Brewer, A. Colbrook, C. Dellarocas, W. Hsieh, A. Joseph, C. Waldspurger, and P. Wang. </author> <title> Prelude: A system for portable parallel software. </title> <type> Technical report, </type> <institution> MIT Laboratory for Computer Science, </institution> <type> Technical Report MIT/LCS/TR-519, </type> <month> Oct. </month> <year> 1991. </year> <note> Shorter version appears in Proceedings of Parle '92. 34 </note>
Reference-contexts: Third, ongoing research on distributed objects is contributing language and compiler support for describing objects and then compiling object interactions into efficient runtime invocations, using custom communication protocols [3] and/or exploiting active message paradigms <ref> [25, 48, 21] </ref>. We share with such work the assumption that communications between different object fragments can often benefit from the use of active messages and that the use of active messages can improve the locality of parallel programs [47].
References-found: 48

