URL: ftp://ftp.cs.colorado.edu/pub/cs/techreports/schwartz/RD.ResearchProblems.Jour.ps.Z
Refering-URL: http://www.cs.colorado.edu/~zorn/cs7135/index.html
Root-URL: 
Email: mic@transarc.com  danzig@usc.edu  udi@cs.arizona.edu  schwartz@cs.colorado.edu  
Phone: +1 412 338 6752  +1 213 740 4780  +1 602 621 4317  +1 303 492 3902  
Title: Scalable Internet Resource Discovery: Research Problems and Approaches  
Author: C. Mic Bowman Peter B. Danzig Udi Manber Michael F. Schwartz 
Date: February 23, 1994  
Note: 707 Grant  
Address: The Gulf Tower  Street Pittsburgh, PA 15219  941 W. 37th Place Los Angeles, California 90089-0781  Tucson, Arizona 85721  Boulder, Colorado 80309-0430  
Affiliation: Transarc Corp.  Computer Science Department University of Southern California  Computer Science Department University of Arizona  Computer Science Department University of Colorado  
Abstract: Over the past several years, a number of information discovery and access tools have been introduced in the Internet, including Archie, Gopher, Netfind, and WAIS. These tools have become quite popular, and are helping to redefine how people think about wide-area network applications. Yet, they are not well suited to supporting the future information infrastructure, which will be characterized by enormous data volume, rapid growth in the user base, and burgeoning data diversity. In this paper we indicate trends in these three dimensions and survey problems these trends will create for current approaches. We then suggest several promising directions of future resource discovery research, along with some initial results from projects carried out by members of the Internet Research Task Force Research Group on Resource Discovery and Directory Service. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Marc Andreessen. </author> <title> NCSA Mosaic technical summary. </title> <type> Technical report, </type> <institution> National Center for Supercomputing Applications, </institution> <month> MAY </month> <year> 1993. </year>
Reference-contexts: The first type of customization involves tracking a person's search history. For example, recently one of this paper's authors discovered that the New Republic magazine was available online while browsing Gopherspace via Mosaic <ref> [1] </ref>. But only two days later it took him 15 minutes to navigate back to the same place. To address this problem, the user interface can keep track of previous successful and unsuccessful queries, comments a user made on past queries, and browsing paths.
Reference: [2] <institution> ANSI. ANSI Z39.50. American National Standards Institute, </institution> <month> May </month> <year> 1991. </year> <note> Version 2, Third Draft. </note>
Reference-contexts: In contrast, we have built a system called Dynamic WAIS [25], which extends the WAIS paradigm to support information from remote search systems (as opposed to the usual collection of static documents). The prototype supports gateways from WAIS to Archie and to Netfind, using the Z39.50 information retrieval protocol <ref> [2] </ref> to seamlessly integrate the information spaces. The Dynamic WAIS interface to Netfind is shown in Figure 3. The key behind the Dynamic WAIS gateways is the conceptual work of constructing the mappings between the WAIS search-and-retrieve operations, and the underlying Archie and Netfind operations.
Reference: [3] <author> S. Armstrong, A. Freier, and K. Marzullo. </author> <title> RFC 1301: Multicast transport protocol. Internet Request for Comments, </title> <month> February </month> <year> 1992. </year>
Reference-contexts: This is not an easy task because Internet topology changes often, and manually composed maps are never current. While we are developing tools to map the Internet [51], even full network maps will not automate topology-update calculation. Avoiding topology knowledge by using today's multicast protocols <ref> [3, 15] </ref> for data distribution fails for other reasons. First, these protocols are limited to single routing domains or require manually placed tunnels between such domains. Second, Internet multicast attempts to minimize message delay, which is the wrong metric for bulk transport.
Reference: [4] <author> T. Berners-Lee, R. Cailliau, J-F. Groff, and B. Pollermann. </author> <title> World-Wide Web: The information universe. </title> <journal> Electronic Networking: Research, Applications and Policy, </journal> <volume> 1(2) </volume> <pages> 52-58, </pages> <month> Spring </month> <year> 1992. </year>
Reference-contexts: To make effective use of this wealth of information, users need ways to locate information of interest. In the past few years, a number of such resource discovery tools have been created, and have gained wide popular acceptance in the Internet <ref> [4, 17, 18, 27, 31, 35, 44] </ref>. 1 Our goal in the current paper is to examine the impact of scale on resource discovery tools, and place these problems into a coherent framework. <p> On the other hand, users are less prone to disorientation, the searching paradigm can handle change much better, and different services can be connected by searching more easily than by interfacing their organizations. Many current systems, such as WAIS, Gopher, and WorldWideWeb <ref> [4] </ref>, employ an organization that is typically based on the location of the data, with limited per-item or per-location searching facilities. Browsing is the first paradigm that users see 6 , but once a server or an archive is located, some type of searching is also provided.
Reference: [5] <author> A. Birrell, R. Levin, R. M. Needham, and M. D. Schroeder. Grapevine: </author> <title> An exercise in distributed computing. </title> <journal> Communications of the ACM, </journal> <volume> 25(4) </volume> <pages> 260-274, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: Other successful tools that cannot easily partition their data will also require 8 massive replication. We believe massive replication requires additional research. On the one hand, without doubt we know how to replicate and cache data that partitions easily, as in the case of name servers <ref> [5, 33] </ref>. Primary copy replication works well because name servers do not perform associative access, and organizational boundaries limit the size of a domain, allowing a handful of servers to meet performance demands. 4 We have learned many lessons to arrive at this understanding [13, 34, 42].
Reference: [6] <author> C. M. Bowman and C. Dharap. </author> <title> The Enterprise distributed white-pages service. </title> <booktitle> Proceedings of the USENIX Winter Conference, </booktitle> <pages> pages 349-360, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: The agreement algorithm defines a method for handling conflicts between different repositories. For example, Figure 1 illustrates data for the Enterprise <ref> [6] </ref> user directory system, which is built on top of the Univers name service [7]. This figure shows three mapping algorithms that gather information from the NIS database, the ruserd server, and the user's electronic mail, respectively. Several attributes can be generated by more than one mapping algorithm. <p> An important component of any complete solution is a directory of services in which archives describe their interest specialization and keep this definition current. This approach essentially amounts to defining archives in terms of queries <ref> [6, 12] </ref>. A server periodically uses its query to hunt throughout the Internet for relevant data objects. One type of server could, for example, be specialized to scan FTP archives, summarizing files and making these summaries available to yet other services.
Reference: [7] <author> M. Bowman, L. L. Peterson, and A. Yeatts. Univers: </author> <title> An attribute-based name server. </title> <journal> Software Practice& Experience, </journal> <volume> 20(4) </volume> <pages> 403-424, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Doing so involves two parts: mapping algorithms for collecting information, and agreement algorithms for correlating information <ref> [7] </ref>. 3 A mapping algorithm is implemented as a function that collects information from a repository and reformats it. There may be several implementations of mapping algorithms, each customized for an existing repository. The most common mapping algorithms are implemented as clients of an existing service. <p> The agreement algorithm defines a method for handling conflicts between different repositories. For example, Figure 1 illustrates data for the Enterprise [6] user directory system, which is built on top of the Univers name service <ref> [7] </ref>. This figure shows three mapping algorithms that gather information from the NIS database, the ruserd server, and the user's electronic mail, respectively. Several attributes can be generated by more than one mapping algorithm.
Reference: [8] <author> V. Cate. </author> <title> Alex A global filesystem. </title> <booktitle> Proceedings of the Usenix File Systems Workshop, </booktitle> <pages> pages 1-11, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The search is done first on the index and then on the blocks that match the query. Glimpse supports approximate matching, regular expression matching, and many other options. Other examples of similar approaches include the scatter/gather browsing approach [11], the Alex file system's "archia" tool <ref> [8] </ref>, and Veronica. Essence and the MIT Semantic File System do selective indexing of documents, by selecting keywords based on knowledge of the structure of the documents being indexed.
Reference: [9] <author> D. Comer and T. P. Murtaugh. </author> <title> The Tilde file naming scheme. </title> <booktitle> Proceedings of the Sixth International Conference on Distributed Computing Systems, </booktitle> <pages> pages 509-514, </pages> <month> May </month> <year> 1986. </year> <month> 18 </month>
Reference-contexts: Browsing can also lead to navigation problems, and users can get disoriented [10, 23]. To some extent this problem can be alleviated by systems that support multiple views of information <ref> [9, 35] </ref>. Yet, doing so really pushes the problem "up" a level|users must locate appropriate views, which in itself is another discovery problem.
Reference: [10] <author> J.E. Conklin. </author> <title> Hypertext: An introduction and survey. </title> <journal> Computer, </journal> <volume> 20(9) </volume> <pages> 17-41, </pages> <month> September </month> <year> 1987. </year>
Reference-contexts: In fact, the notion of "well organized" is highly subjective and personal. What one user finds clear and easy to browse may be difficult for users who have different needs or backgrounds. Browsing can also lead to navigation problems, and users can get disoriented <ref> [10, 23] </ref>. To some extent this problem can be alleviated by systems that support multiple views of information [9, 35]. Yet, doing so really pushes the problem "up" a level|users must locate appropriate views, which in itself is another discovery problem.
Reference: [11] <author> D. R. Cutting, D. R. Karger, and J. O. Pederson. </author> <title> Constant interaction-time scatter/gather browsing of very large document collections. </title> <booktitle> Proceedings of the ACM-SIGIR conf. on Information Retrieval, </booktitle> <pages> pages 126-134, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: The search is done first on the index and then on the blocks that match the query. Glimpse supports approximate matching, regular expression matching, and many other options. Other examples of similar approaches include the scatter/gather browsing approach <ref> [11] </ref>, the Alex file system's "archia" tool [8], and Veronica. Essence and the MIT Semantic File System do selective indexing of documents, by selecting keywords based on knowledge of the structure of the documents being indexed.
Reference: [12] <author> P. B. Danzig, S.-H. Li, and K. Obraczka. </author> <title> Distributed indexing of autonomous Internet services. </title> <journal> Computing Systems, </journal> <volume> 5(4) </volume> <pages> 433-459, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: Explicit typing is most naturally performed by the user when a file is exported into the resource discovery system. We are exploring this approach in the Nebula file system [16] and Indie discovery tool <ref> [12] </ref>. Many files exist without explicit type information, as in most current anonymous FTP 2 files. An implicit typing mechanism can help in this case. <p> At the very least, we see the need for different routing metrics. Third, more research is needed into reliable, bulk transport multicast that efficiently deals with site failure, network partition, and changes in the replication group. We are exploring an approach to providing massively replicated, loosely consistent services <ref> [12, 37] </ref>. This approach extends ideas presented in Lampson's Global name service [28] to operate in the Internet. Briefly, our approach organizes the replicas of a service into groups, imitating the idea behind the Internet's autonomous routing domains. <p> An important component of any complete solution is a directory of services in which archives describe their interest specialization and keep this definition current. This approach essentially amounts to defining archives in terms of queries <ref> [6, 12] </ref>. A server periodically uses its query to hunt throughout the Internet for relevant data objects. One type of server could, for example, be specialized to scan FTP archives, summarizing files and making these summaries available to yet other services.
Reference: [13] <author> P. B. Danzig, K. Obraczka, and A. Kumar. </author> <title> An analysis of wide-area name server traffic: A study of the Domain Name System. </title> <booktitle> ACM SIGCOMM '92 Conference, </booktitle> <pages> pages 281-292, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Self-instrumented servers could also identify server-client or server-server communication run amok. Large distributed systems frequently suffer from undiagnosed, endless cycle of requests. For 7 example, self-instrumented Internet name servers show that DNS traffic consumes 20 times more bandwidth than it should, because of unanticipated interactions between clients and servers <ref> [13] </ref>. Self-instrumentation could identify problem specifications and implementations before they become widespread and difficult to correct. 3.2 Server Replication Name servers scale well because their data are typically partitioned hierarchically. <p> Primary copy replication works well because name servers do not perform associative access, and organizational boundaries limit the size of a domain, allowing a handful of servers to meet performance demands. 4 We have learned many lessons to arrive at this understanding <ref> [13, 34, 42] </ref>. On the other hand, we have little experience deploying replication and caching algorithms to support massively replicated, flat, yet autonomously managed databases. What do existing replication schemes for wide-area services lack? First, existing replication systems ignore network topology.
Reference: [14] <author> Peter B. Danzig, Michael Schwartz, and Richard Hall. </author> <title> A case for caching file objects inside internetworks. </title> <booktitle> ACM SIGCOMM '93 Conference, </booktitle> <pages> pages 239-248, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: We say this for two reasons. First, people currently use FTP and electronic mail as a cacheless, distributed file system <ref> [14, 38] </ref>. Since the quantity and size of read-mostly data objects grows as we add new information services, an object cache would improve user response times and decrease network load (e.g., we found that one fifth of all NSFNET backbone traffic could be avoided by caching FTP data).
Reference: [15] <author> S. Deering and D. Cheriton. </author> <title> Multicast routing in datagram internetworks and extended lans. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 8(2) </volume> <pages> 85-110, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: This is not an easy task because Internet topology changes often, and manually composed maps are never current. While we are developing tools to map the Internet [51], even full network maps will not automate topology-update calculation. Avoiding topology knowledge by using today's multicast protocols <ref> [3, 15] </ref> for data distribution fails for other reasons. First, these protocols are limited to single routing domains or require manually placed tunnels between such domains. Second, Internet multicast attempts to minimize message delay, which is the wrong metric for bulk transport.
Reference: [16] <author> Chanda Dharap, Rajini Balay, and Mic Bowman. </author> <title> Type structured file systems. </title> <booktitle> Proceedings of the International Workshop on Object-Orientation in Operating Systems, </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: Explicit typing is most naturally performed by the user when a file is exported into the resource discovery system. We are exploring this approach in the Nebula file system <ref> [16] </ref> and Indie discovery tool [12]. Many files exist without explicit type information, as in most current anonymous FTP 2 files. An implicit typing mechanism can help in this case.
Reference: [17] <author> R. E. Droms. </author> <title> Access to heterogeneous directory services. </title> <booktitle> Proceedings of the IEEE INFOCOM '90, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: To make effective use of this wealth of information, users need ways to locate information of interest. In the past few years, a number of such resource discovery tools have been created, and have gained wide popular acceptance in the Internet <ref> [4, 17, 18, 27, 31, 35, 44] </ref>. 1 Our goal in the current paper is to examine the impact of scale on resource discovery tools, and place these problems into a coherent framework.
Reference: [18] <author> A. Emtage and P. Deutsch. Archie: </author> <title> An electronic directory service for the Internet. </title> <booktitle> Proceedings of the Winter 1992 Usenix Conference, </booktitle> <pages> pages 93-110, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: To make effective use of this wealth of information, users need ways to locate information of interest. In the past few years, a number of such resource discovery tools have been created, and have gained wide popular acceptance in the Internet <ref> [4, 17, 18, 27, 31, 35, 44] </ref>. 1 Our goal in the current paper is to examine the impact of scale on resource discovery tools, and place these problems into a coherent framework.
Reference: [19] <author> C. Faloutsos. </author> <title> Access methods for text. </title> <journal> ACM Computing Surveys, </journal> <volume> 17 </volume> <pages> 49-74, </pages> <month> MAR </month> <year> 1985. </year>
Reference-contexts: The scale problems for full texts are much more difficult. Full-text indexes are almost always 14 inverted indexes, which store pointers to every occurrence of every word (possibly excluding some very common words). The main problems with inverted indexes are their size usually 50-150% of the original text <ref> [19] </ref> and the time it takes to build and/or update them. Therefore, maintaining an inverted index of the whole Internet FTP space is probably out of the question.
Reference: [20] <author> S. Foster. </author> <title> About the Veronica service, November, 1992. </title> <journal> Electronic bulletin board posting on the comp.infosystems.gopher newsgroup. </journal>
Reference-contexts: Searching can be comprehensive throughout the archive (for example, WAIS servers provide full-text indexes), or limited to titles. 4.2 Indexing Schemes The importance of searching can be seen by the recent emergence of file system search tools [22, 24, 30, 48] and the Veronica system <ref> [20] </ref> (a search facility for Gopher). Moreover, it is interesting to note that while the Prospero model [36] focuses on organizing and browsing, Prospero has found its most successful application as an interface to the Archie search system. To support efficient searching, various means of indexing data are required.
Reference: [21] <author> James C. French, Anita K. Jones, and John L. Pfaltz. </author> <title> Summary of the final report of the NSF workshop on scientific database management. </title> <booktitle> SIGMOD Record, </booktitle> <volume> 19(4) </volume> <pages> 32-40, </pages> <month> Dec, </month> <year> 1990. </year>
Reference-contexts: Many users already store voluminous data, but do not share it over the Internet because of limited wide-area network bandwidths. For example, earth and space scientists collect sensor data at rates as high as gigabytes per day <ref> [21] </ref>. To share data with colleagues, they send magnetic tapes through the postal mail. As Internet link capacities increase, more scientists will use the Internet to share data, adding several more orders of magnitude to the information space.
Reference: [22] <author> D. K. Gifford, P. Jouvelot, M. A. Sheldon, and J. W. O'Toole Jr. </author> <title> Semantic file systems. </title> <booktitle> In Proceedings of 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 16-25, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: For example, Essence [24] uses a variety of heuristics to recognize files as fitting into certain common classes, such as word processor source text or binary executables. These heuristics include common naming conventions or identifying features in the data. The MIT Semantic File System uses similar techniques <ref> [22] </ref>. Given a typed file, the next step is to extract indexing information. This can most easily be accomplished through automatic content extraction, using a grammar that describes how to extract information from each type of file. <p> Searching can be comprehensive throughout the archive (for example, WAIS servers provide full-text indexes), or limited to titles. 4.2 Indexing Schemes The importance of searching can be seen by the recent emergence of file system search tools <ref> [22, 24, 30, 48] </ref> and the Veronica system [20] (a search facility for Gopher). Moreover, it is interesting to note that while the Prospero model [36] focuses on organizing and browsing, Prospero has found its most successful application as an interface to the Archie search system.
Reference: [23] <author> F.G. </author> <title> Halasz. Reflections on notecards: Seven issues for the next generation of hypermedia. </title> <journal> Communications of the ACM, </journal> <volume> 31(7) </volume> <pages> 836-852, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: In fact, the notion of "well organized" is highly subjective and personal. What one user finds clear and easy to browse may be difficult for users who have different needs or backgrounds. Browsing can also lead to navigation problems, and users can get disoriented <ref> [10, 23] </ref>. To some extent this problem can be alleviated by systems that support multiple views of information [9, 35]. Yet, doing so really pushes the problem "up" a level|users must locate appropriate views, which in itself is another discovery problem.
Reference: [24] <author> D. Hardy and M. F. Schwartz. </author> <title> Essence: A resource discovery system based on semantic file indexing. </title> <booktitle> Proceedings of the USENIX Winter Conference, </booktitle> <pages> pages 361-374, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: We are exploring this approach in the Nebula file system [16] and Indie discovery tool [12]. Many files exist without explicit type information, as in most current anonymous FTP 2 files. An implicit typing mechanism can help in this case. For example, Essence <ref> [24] </ref> uses a variety of heuristics to recognize files as fitting into certain common classes, such as word processor source text or binary executables. These heuristics include common naming conventions or identifying features in the data. The MIT Semantic File System uses similar techniques [22]. <p> Searching can be comprehensive throughout the archive (for example, WAIS servers provide full-text indexes), or limited to titles. 4.2 Indexing Schemes The importance of searching can be seen by the recent emergence of file system search tools <ref> [22, 24, 30, 48] </ref> and the Veronica system [20] (a search facility for Gopher). Moreover, it is interesting to note that while the Prospero model [36] focuses on organizing and browsing, Prospero has found its most successful application as an interface to the Archie search system.
Reference: [25] <author> Darren R. Hardy. </author> <title> Scalable Internet resource discovery among diverse information. </title> <type> Technical Report CU-CS-650-93, </type> <institution> Department of Computer Science, University of Colorado, Boulder, Colorado, </institution> <month> May </month> <year> 1993. </year> <title> M.S. </title> <type> Thesis. 19 </type>
Reference-contexts: In contrast, we have built a system called Dynamic WAIS <ref> [25] </ref>, which extends the WAIS paradigm to support information from remote search systems (as opposed to the usual collection of static documents). The prototype supports gateways from WAIS to Archie and to Netfind, using the Z39.50 information retrieval protocol [2] to seamlessly integrate the information spaces.
Reference: [26] <author> International Organization for Standardization. </author> <title> Information Processing Systems Open Sys tems Interconnection The Directory Overview of Concepts, Models, and Service. </title> <type> Technical report, </type> <institution> International Organization for Standardization and International Electrotechnical Committee, </institution> <month> December </month> <year> 1988. </year> <note> International Standard 9594-1. </note>
Reference-contexts: Because information is maintained by several repositories that perform updates at different times using different algorithms, there will often be conflicts between information in the various repositories. For example, information about account name, address, and phone number may be maintained by both the NIS database and an X.500 <ref> [26] </ref> server. When a user's address or phone number changes, the X.500 service will probably be updated first. However, if the account changes, the NIS database will usually be the first to reflect the change. * It is incomplete.
Reference: [27] <author> B. Khale and A. Medlar. </author> <title> An information system for corporate users: Wide Area Information Servers. </title> <journal> ConneXions The Interoperability Report, </journal> <volume> 5(11) </volume> <pages> 2-9, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: To make effective use of this wealth of information, users need ways to locate information of interest. In the past few years, a number of such resource discovery tools have been created, and have gained wide popular acceptance in the Internet <ref> [4, 17, 18, 27, 31, 35, 44] </ref>. 1 Our goal in the current paper is to examine the impact of scale on resource discovery tools, and place these problems into a coherent framework. <p> This stands in contrast to the common approach (used by WAIS <ref> [27] </ref>, for example) of allowing a free association between query keywords and extracted data. We discuss indexing schemes further in Section 4.2. 2.2 Operation Mapping A gateway between two resource discovery systems translates operations from one system into operations in another system.
Reference: [28] <author> B. Lampson. </author> <title> Designing a global name service. </title> <booktitle> ACM Principles of Distributed Computing, </booktitle> <pages> pages 1-10, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Third, more research is needed into reliable, bulk transport multicast that efficiently deals with site failure, network partition, and changes in the replication group. We are exploring an approach to providing massively replicated, loosely consistent services [12, 37]. This approach extends ideas presented in Lampson's Global name service <ref> [28] </ref> to operate in the Internet. Briefly, our approach organizes the replicas of a service into groups, imitating the idea behind the Internet's autonomous routing domains. Group replicas estimate the physical topology, and then create an update topology between the group members.
Reference: [29] <author> U. Manber. </author> <title> Finding similar files in a large file system. </title> <booktitle> Proceedings of the USENIX Winter Conference, </booktitle> <pages> pages 1-10, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: In addition to one flat database of all file names, it is possible to maintain much smaller slightly limited databases that will be replicated widely. For example, we can detect duplicates (exact and/or similar, see <ref> [29] </ref>), and keep only one copy (e.g., based on locality) in the smaller databases. Most queries will be satisfied with the smaller databases, and only few of them will have to go further. The scale problems for full texts are much more difficult.
Reference: [30] <author> U. Manber and S. Wu. Glimpse: </author> <title> A tool to search through entire file systems. </title> <booktitle> Proceedings of the USENIX Winter Conference, </booktitle> <pages> pages 23-32, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Searching can be comprehensive throughout the archive (for example, WAIS servers provide full-text indexes), or limited to titles. 4.2 Indexing Schemes The importance of searching can be seen by the recent emergence of file system search tools <ref> [22, 24, 30, 48] </ref> and the Veronica system [20] (a search facility for Gopher). Moreover, it is interesting to note that while the Prospero model [36] focuses on organizing and browsing, Prospero has found its most successful application as an interface to the Archie search system. <p> Similar indexes are available for individual Gopher and WWW servers. Some recent advances have lowered the space requirements for full-text indexing, with as low as 2-4% for the index used in Glimpse <ref> [30] </ref>. There is usually (but not always) a time-space tradeoff; systems that use less space require more time for searching. <p> Of course, many issues need to be resolved, including ranking, classification, replication, consistency, data extraction, privacy, and more. An example (at a smaller scale) of the multi-level approach is Glimpse <ref> [30] </ref>, an indexing and searching scheme designed for file systems. Glimpse divides the entire file system into blocks, and in contrast with inverted indexes, it stores for each word only the block numbers containing it.
Reference: [31] <author> M. McCahill. </author> <title> The Internet Gopher: A distributed server information system. </title> <journal> ConneXions The Interoperability Report, </journal> <volume> 6(7) </volume> <pages> 10-14, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: To make effective use of this wealth of information, users need ways to locate information of interest. In the past few years, a number of such resource discovery tools have been created, and have gained wide popular acceptance in the Internet <ref> [4, 17, 18, 27, 31, 35, 44] </ref>. 1 Our goal in the current paper is to examine the impact of scale on resource discovery tools, and place these problems into a coherent framework. <p> Even if two systems support similar operations, building seamless gateways may be hindered by another problem: providing appropriate mappings between operations in the two systems. To illustrate the problem, consider the current interim gateway from Gopher <ref> [31] </ref> to Netfind, illustrated in Figure 2. 3 Because the gateway simply opens a telnet window to a UNIX program that provides the Netfind service, users perceive the boundaries between the two systems.
Reference: [32] <author> Sun Microsystems. </author> <title> SunOS reference manual, </title> <booktitle> Volume II, </booktitle> <month> March </month> <year> 1990. </year>
Reference-contexts: As an example, consider the problem of constructing a user directory. In a typical environment, several existing systems contain information about users. The Sun NIS database [50] usually has information about a user's account name, personal name, address, group memberships, password, and home directory. The ruserd <ref> [32] </ref> server has information about a user's workstation and its idle time. In addition, users often place information in a ".plan" file that might list the user's travel schedule, home address, office hours, and research interests.
Reference: [33] <author> P. Mockapetris. </author> <title> RFC 1034: Domain names concepts and facilities. Internet Request for Comments, </title> <month> November </month> <year> 1987. </year>
Reference-contexts: There may be several implementations of mapping algorithms, each customized for an existing repository. The most common mapping algorithms are implemented as clients of an existing service. For example, Netfind extracts user information from several common Internet services [46], including the Domain Naming System (DNS) <ref> [33] </ref>, the finger service [53] and the Simple Mail Transfer Protocol [40]. The agreement algorithm defines a method for handling conflicts between different repositories. For example, Figure 1 illustrates data for the Enterprise [6] user directory system, which is built on top of the Univers name service [7]. <p> Other successful tools that cannot easily partition their data will also require 8 massive replication. We believe massive replication requires additional research. On the one hand, without doubt we know how to replicate and cache data that partitions easily, as in the case of name servers <ref> [5, 33] </ref>. Primary copy replication works well because name servers do not perform associative access, and organizational boundaries limit the size of a domain, allowing a handful of servers to meet performance demands. 4 We have learned many lessons to arrive at this understanding [13, 34, 42].
Reference: [34] <author> P. Mockapetris and K. Dunlap. </author> <title> Development of the Domain Name System. </title> <booktitle> ACM SIGCOMM '88 Conference, </booktitle> <address> Stanford, California, </address> <pages> pages 11-21, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Primary copy replication works well because name servers do not perform associative access, and organizational boundaries limit the size of a domain, allowing a handful of servers to meet performance demands. 4 We have learned many lessons to arrive at this understanding <ref> [13, 34, 42] </ref>. On the other hand, we have little experience deploying replication and caching algorithms to support massively replicated, flat, yet autonomously managed databases. What do existing replication schemes for wide-area services lack? First, existing replication systems ignore network topology.
Reference: [35] <author> B. C. Neuman. Prospero: </author> <title> A tool for organizing Internet resources. </title> <journal> Electronic Networking: Research, Applications, and Policy, </journal> <volume> 2(1) </volume> <pages> 30-37, </pages> <month> Spring </month> <year> 1992. </year>
Reference-contexts: To make effective use of this wealth of information, users need ways to locate information of interest. In the past few years, a number of such resource discovery tools have been created, and have gained wide popular acceptance in the Internet <ref> [4, 17, 18, 27, 31, 35, 44] </ref>. 1 Our goal in the current paper is to examine the impact of scale on resource discovery tools, and place these problems into a coherent framework. <p> Building seamless gateways can be hindered if one system lacks operations needed by another system's user interface [43]. For example, if would be difficult to provide a seamless gateway from a system (like WAIS) that provides a search interface to users, to a system (like Prospero <ref> [35] </ref>) that only support browsing. Even if two systems support similar operations, building seamless gateways may be hindered by another problem: providing appropriate mappings between operations in the two systems. <p> Browsing can also lead to navigation problems, and users can get disoriented [10, 23]. To some extent this problem can be alleviated by systems that support multiple views of information <ref> [9, 35] </ref>. Yet, doing so really pushes the problem "up" a level|users must locate appropriate views, which in itself is another discovery problem.
Reference: [36] <author> B. Clifford Neuman. </author> <title> The virtual system model: A scalable approach to organizing large systems. </title> <type> Technical Report 90-05-01, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <address> Seattle, Washington, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Moreover, it is interesting to note that while the Prospero model <ref> [36] </ref> focuses on organizing and browsing, Prospero has found its most successful application as an interface to the Archie search system. To support efficient searching, various means of indexing data are required. As illustrated in efficient indexes, but only represent the names of the files or menus that they index.
Reference: [37] <author> K. Obraczka, P. Danzig, D. DeLucia, and N. Alaam. </author> <title> Massive replication of data in autonomous internetworks. </title> <type> Technical Report, </type> <institution> University of Southern California, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: At the very least, we see the need for different routing metrics. Third, more research is needed into reliable, bulk transport multicast that efficiently deals with site failure, network partition, and changes in the replication group. We are exploring an approach to providing massively replicated, loosely consistent services <ref> [12, 37] </ref>. This approach extends ideas presented in Lampson's Global name service [28] to operate in the Internet. Briefly, our approach organizes the replicas of a service into groups, imitating the idea behind the Internet's autonomous routing domains.
Reference: [38] <author> Vern Paxson. </author> <title> Empirically-derived analytic models of wide-area tcp connections: </title> <type> Extended report. Technical Report TR LBL-34986, </type> <institution> Lawrence Berkeley Labs, </institution> <month> June 15, </month> <year> 1993. </year>
Reference-contexts: We say this for two reasons. First, people currently use FTP and electronic mail as a cacheless, distributed file system <ref> [14, 38] </ref>. Since the quantity and size of read-mostly data objects grows as we add new information services, an object cache would improve user response times and decrease network load (e.g., we found that one fifth of all NSFNET backbone traffic could be avoided by caching FTP data).
Reference: [39] <author> J. Postel and J. Reynolds. </author> <title> RFC 959: File transfer protocol (FTP). </title> <type> Technical report, </type> <institution> University of Southern California Information Sciences Institute, </institution> <month> October </month> <year> 1985. </year>
Reference-contexts: One final observation about data type structure is that the index should preserve type information to help identify context during searches. For example, keywords extracted from document 2 FTP is an Internet standard protocol that supports transferring files between interconnected hosts <ref> [39] </ref>.
Reference: [40] <author> J. B. Postel. </author> <title> RFC 821: Simple Mail Transfer Protocol. Internet Request for Comments, </title> <month> August </month> <year> 1982. </year>
Reference-contexts: The most common mapping algorithms are implemented as clients of an existing service. For example, Netfind extracts user information from several common Internet services [46], including the Domain Naming System (DNS) [33], the finger service [53] and the Simple Mail Transfer Protocol <ref> [40] </ref>. The agreement algorithm defines a method for handling conflicts between different repositories. For example, Figure 1 illustrates data for the Enterprise [6] user directory system, which is built on top of the Univers name service [7].
Reference: [41] <author> J. Quarterman. </author> <title> The Matrix Computer Networks and Conferencing Systems Worldwide. </title> <publisher> Digital Press, </publisher> <year> 1990. </year> <month> 20 </month>
Reference-contexts: In essence, they treat all physical links as having equal bandwidth, delay, and reliability, and do not recognize administrative domains. We believe that flooding-based replication algorithms can be extended to work well in the Internet environment. Both Archie and network news <ref> [41] </ref> replicate using flooding algorithms. However, for lack of good tools, administrators of both Archie and network news manually configure the flooding topology over which updates travel, and manually reconfigure this topology when the physical network changes.
Reference: [42] <author> M. Schroeder, A. Birrell, and R. Needham. </author> <title> Experience with Grapevine: The growth of a distributed system. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 2(1) </volume> <pages> 3-23, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: Primary copy replication works well because name servers do not perform associative access, and organizational boundaries limit the size of a domain, allowing a handful of servers to meet performance demands. 4 We have learned many lessons to arrive at this understanding <ref> [13, 34, 42] </ref>. On the other hand, we have little experience deploying replication and caching algorithms to support massively replicated, flat, yet autonomously managed databases. What do existing replication schemes for wide-area services lack? First, existing replication systems ignore network topology.
Reference: [43] <author> M. F. Schwartz, A. Emtage, B. Kahle, and B. C. Neuman. </author> <title> A comparison of Internet resource discovery approaches. </title> <journal> Computing Systems, </journal> <volume> 5(4) </volume> <pages> 461-493, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: Since information about a resource exists in many repositories|within the object 1 The reader interested in an overview of resource discovery systems and their approaches is referred to <ref> [43] </ref>. <p> Ideally, the systems interoperate seamlessly, without the need for users to learn the details of each system. Sometimes, however, users must learn how to use each system separately. Building seamless gateways can be hindered if one system lacks operations needed by another system's user interface <ref> [43] </ref>. For example, if would be difficult to provide a seamless gateway from a system (like WAIS) that provides a search interface to users, to a system (like Prospero [35]) that only support browsing.
Reference: [44] <author> M. F. Schwartz and P. G. Tsirigotis. </author> <title> Experience with a semantically cognizant internet white pages directory tool. </title> <journal> Journal of Internetworking: Research and Experience, </journal> <volume> 2(1) </volume> <pages> 23-50, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: To make effective use of this wealth of information, users need ways to locate information of interest. In the past few years, a number of such resource discovery tools have been created, and have gained wide popular acceptance in the Internet <ref> [4, 17, 18, 27, 31, 35, 44] </ref>. 1 Our goal in the current paper is to examine the impact of scale on resource discovery tools, and place these problems into a coherent framework.
Reference: [45] <author> Michael F. Schwartz, Darren R. Hardy, William K. Heinzman, and Glenn Hirschowitz. </author> <title> Sup porting resource discovery among public internet archives using a spectrum of information quality. </title> <booktitle> Proceedings of the Eleventh International Conference on Distributed Computing Systems, </booktitle> <pages> pages 82-89, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Existing systems record some of this information (e.g., the ability to set "bookmarks" in Gopher and "Hotlists" in Mosaic); saving the whole history can help further. For example, we built a system that records the paths users traverse when browsing FTP directories, and allows this information to be searched <ref> [45] </ref>. It is also helpful to record search history, and allow the searches themselves to be searched. For example, the X-windows interface to agrep [52] translates every command into the corresponding agrep statement, and allows flexible retrieval of this information.
Reference: [46] <author> Michael F. Schwartz and Calton Pu. </author> <title> Applying an information gathering architecture to Netfind: A white pages tool for a changing and growing internet. </title> <type> Technical Report CU-CS-656-93, </type> <institution> Department of Computer Science, University of Colorado, Boulder, Colorado, </institution> <month> December </month> <year> 1993. </year> <note> Submitted for publication. </note>
Reference-contexts: There may be several implementations of mapping algorithms, each customized for an existing repository. The most common mapping algorithms are implemented as clients of an existing service. For example, Netfind extracts user information from several common Internet services <ref> [46] </ref>, including the Domain Naming System (DNS) [33], the finger service [53] and the Simple Mail Transfer Protocol [40]. The agreement algorithm defines a method for handling conflicts between different repositories. <p> The Netfind site database includes keywords for each component of a site's Domain name, plus each keyword included in an organizational description that was constructed based on understanding the semantics of many information sources used to gather that information <ref> [46] </ref>. Whois++ [49] indexes templates that were manually constructed by site administrators wishing to describe the resources at their site. By selecting relatively small but important information such as file names, Archie quickly became a popular search-based Internet resource discovery tool.
Reference: [47] <author> Richard Stallman. </author> <title> GNU Emacs Manual, </title> <booktitle> sixth edition, </booktitle> <month> March </month> <year> 1987. </year>
Reference-contexts: Customizations like the ones discussed here are missing from most current resource discovery client programs because they were not designed to be extensible (like the Emacs text-editor <ref> [47] </ref>). 4 Data Volume The amount of information freely available in the Internet is huge and growing rapidly. New usage modes will contribute additional scale.
Reference: [48] <author> SunSoft. SearchIt 1.0. SunSoft, Inc., </author> <year> 1992. </year> <note> User's Guide. </note>
Reference-contexts: Searching can be comprehensive throughout the archive (for example, WAIS servers provide full-text indexes), or limited to titles. 4.2 Indexing Schemes The importance of searching can be seen by the recent emergence of file system search tools <ref> [22, 24, 30, 48] </ref> and the Veronica system [20] (a search facility for Gopher). Moreover, it is interesting to note that while the Prospero model [36] focuses on organizing and browsing, Prospero has found its most successful application as an interface to the Archie search system.
Reference: [49] <author> Chris Weider, Jim Fullton, and Simon Spero. </author> <title> Architecture of the Whois++ index service. </title> <type> Technical report, </type> <month> November </month> <year> 1992. </year> <note> Available by anonymous FTP from nri.reston.va.us, in internet-drafts/draft-ietf-wnils-whois-00.txt. </note>
Reference-contexts: The Netfind site database includes keywords for each component of a site's Domain name, plus each keyword included in an organizational description that was constructed based on understanding the semantics of many information sources used to gather that information [46]. Whois++ <ref> [49] </ref> indexes templates that were manually constructed by site administrators wishing to describe the resources at their site. By selecting relatively small but important information such as file names, Archie quickly became a popular search-based Internet resource discovery tool. But as we discussed in Section 3.2, Archie has scale problems.
Reference: [50] <author> P. Weiss. </author> <title> Yellow Pages Protocol Specification. Sun Microsystems, </title> <publisher> Inc., </publisher> <year> 1985. </year>
Reference-contexts: As an example, consider the problem of constructing a user directory. In a typical environment, several existing systems contain information about users. The Sun NIS database <ref> [50] </ref> usually has information about a user's account name, personal name, address, group memberships, password, and home directory. The ruserd [32] server has information about a user's workstation and its idle time.
Reference: [51] <author> D. C. M. Wood, S. S. Coleman, and M. F. Schwartz. Fremont: </author> <title> A system for discovering network characteristics and problems. </title> <booktitle> Proceedings of the USENIX Winter Conference, </booktitle> <pages> pages 335-348, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: This is not an easy task because Internet topology changes often, and manually composed maps are never current. While we are developing tools to map the Internet <ref> [51] </ref>, even full network maps will not automate topology-update calculation. Avoiding topology knowledge by using today's multicast protocols [3, 15] for data distribution fails for other reasons. First, these protocols are limited to single routing domains or require manually placed tunnels between such domains.
Reference: [52] <author> Sun Wu and Udi Manber. </author> <title> Fast text searching allowing errors. </title> <journal> Communications of the ACM, </journal> <pages> pages 83-91, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: For example, we built a system that records the paths users traverse when browsing FTP directories, and allows this information to be searched [45]. It is also helpful to record search history, and allow the searches themselves to be searched. For example, the X-windows interface to agrep <ref> [52] </ref> translates every command into the corresponding agrep statement, and allows flexible retrieval of this information.
Reference: [53] <author> D. Zimmerman. </author> <title> RFC 1288: The finger user information protocol. Internet Request for Com ments, </title> <month> November </month> <year> 1990. </year> <month> 21 </month>
Reference-contexts: There may be several implementations of mapping algorithms, each customized for an existing repository. The most common mapping algorithms are implemented as clients of an existing service. For example, Netfind extracts user information from several common Internet services [46], including the Domain Naming System (DNS) [33], the finger service <ref> [53] </ref> and the Simple Mail Transfer Protocol [40]. The agreement algorithm defines a method for handling conflicts between different repositories. For example, Figure 1 illustrates data for the Enterprise [6] user directory system, which is built on top of the Univers name service [7].
References-found: 53

