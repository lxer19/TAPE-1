URL: http://www.cs.jhu.edu/~brill/CompLing95.ps
Refering-URL: http://www.cs.jhu.edu/~brill/acadpubs.html
Root-URL: http://www.cs.jhu.edu
Title: Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part of Speech Tagging  
Author: Eric Brill 
Affiliation: The Johns Hopkins University  
Abstract: Recently, there has been a rebirth of empiricism in the field of natural language processing. Manual encoding of linguistic information is being challenged by automated corpus-based learning as a method of providing a natural language processing system with linguistic knowledge. Although corpus-based approaches have been successful in many different areas of natural language processing, it is often the case that these methods capture the linguistic information they are modelling indirectly in large opaque tables of statistics. This can make it difficult to analyze, understand and improve the ability of these approaches to model underlying linguistic behavior. In this paper, we will describe a simple rule-based approach to automated learning of linguistic knowledge. This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance. We present a detailed case study of this learning method applied to part of speech tagging. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Black, Ezra, Fred Jelinek, John Lafferty, David Magerman, Robert Mercer, and Salim Roukos. </author> <year> 1993. </year> <title> Towards history-based grammars: Using richer models for probabilistic parsing. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics. </booktitle> <address> Columbus, Ohio. </address>
Reference: <author> Black, Ezra, Fred Jelinek, John Lafferty, Robert Mercer, and Salim Roukos. </author> <year> 1992. </year> <title> Decision tree models applied to the labeling of text with parts-of-speech. </title> <booktitle> In Darpa Workshop on Speech and Natural Language. </booktitle> <address> Harriman, N.Y. </address>
Reference: <author> Breiman, Leo, Jerome Friedman, Richard Olshen, and Charles Stone. </author> <year> 1984. </year> <title> Classification and regression trees. </title> <publisher> Wadsworth and Brooks. </publisher>
Reference-contexts: The direct correlation between rules and performance improvement in transformation-based learning can make the learned rules more readily interpretable than decision tree rules for increasing population purity. 5 4 For a discussion on why this is the case, see <ref> (Breiman et al., 1984) </ref>, pages 94-98. 5 For a discussion of other issues regarding these two learning algorithms, see (Ramshaw and Marcus, 1994) Computational Linguistics Volume 21, Number 4 4.
Reference: <author> Brill, Eric. </author> <year> 1992. </year> <title> A simple rule-based part of speech tagger. </title> <booktitle> In Proceedings of the Third Conference on Applied Natural Language Processing, ACL, </booktitle> <address> Trento, Italy. </address>
Reference: <author> Brill, Eric. </author> <year> 1993a. </year> <title> Automatic grammar induction and parsing free text: A transformation-based approach. </title> <booktitle> In Proceedings of the 31st Meeting of the Association of Computational Linguistics, </booktitle> <address> Columbus, Oh. </address>
Reference-contexts: The transformation-based tagger captures linguistic information in a small number of simple non-stochastic rules, as opposed to large numbers of lexical and contextual probabilities. This learning approach has also been applied to a number of other tasks, including prepositional phrase attachment disambiguation (Brill and Resnik, 1994), bracketing text <ref> (Brill, 1993a) </ref> and labeling nonterminal nodes (Brill, 1993c).
Reference: <author> Brill, Eric. </author> <year> 1993b. </year> <title> A Corpus-Based Approach to Language Learning. </title> <type> Ph.D. thesis, </type> <institution> Department of Computer and Information Science, University of Pennsylvania. </institution>
Reference-contexts: See <ref> (Brill, 1993b) </ref>.
Reference: <author> Brill, Eric. </author> <year> 1993c. </year> <title> Transformation-based error-driven parsing. </title> <booktitle> In Proceedings of the Third International Workshop on Parsing Technologies, </booktitle> <address> Tilburg, The Netherlands. </address>
Reference-contexts: This learning approach has also been applied to a number of other tasks, including prepositional phrase attachment disambiguation (Brill and Resnik, 1994), bracketing text (Brill, 1993a) and labeling nonterminal nodes <ref> (Brill, 1993c) </ref>.
Reference: <author> Brill, Eric. </author> <year> 1994. </year> <title> Some advances in rule-based part of speech tagging. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94), </booktitle> <address> Seattle, Wa. </address>
Reference-contexts: As large corpora became available, it became clear that simple Markov 6 All of the programs described herein are freely available with no restrictions on use or redistribution. For information on obtaining the tagger, contact the author. 7 In <ref> (Brill and Resnik, 1994) </ref>, we describe an approach to prepositional phrase attachment disambiguation that obtains highly competitive performance compared to other corpus-based solutions to this problem. <p> This approach has already been successfully applied to a system for prepositional phrase attachment attachment disambiguation <ref> (Brill and Resnik, 1994) </ref>. 4.3 Tagging Unknown Words So far, we have not addressed the problem of unknown words. As stated above, the initial state annotator for tagging assigns all words their most likely tag, as indicated in a training corpus. <p> The transformation-based tagger captures linguistic information in a small number of simple non-stochastic rules, as opposed to large numbers of lexical and contextual probabilities. This learning approach has also been applied to a number of other tasks, including prepositional phrase attachment disambiguation <ref> (Brill and Resnik, 1994) </ref>, bracketing text (Brill, 1993a) and labeling nonterminal nodes (Brill, 1993c).
Reference: <author> Brill, Eric and Philip Resnik. </author> <year> 1994. </year> <title> A transformation-based approach to prepositional phrase attachment disambiguation. </title> <booktitle> In Proceedings of the Fifteenth International Conference on Computational Linguistics (COLING-1994), </booktitle> <address> Kyoto, Japan. </address>
Reference-contexts: As large corpora became available, it became clear that simple Markov 6 All of the programs described herein are freely available with no restrictions on use or redistribution. For information on obtaining the tagger, contact the author. 7 In <ref> (Brill and Resnik, 1994) </ref>, we describe an approach to prepositional phrase attachment disambiguation that obtains highly competitive performance compared to other corpus-based solutions to this problem. <p> This approach has already been successfully applied to a system for prepositional phrase attachment attachment disambiguation <ref> (Brill and Resnik, 1994) </ref>. 4.3 Tagging Unknown Words So far, we have not addressed the problem of unknown words. As stated above, the initial state annotator for tagging assigns all words their most likely tag, as indicated in a training corpus. <p> The transformation-based tagger captures linguistic information in a small number of simple non-stochastic rules, as opposed to large numbers of lexical and contextual probabilities. This learning approach has also been applied to a number of other tasks, including prepositional phrase attachment disambiguation <ref> (Brill and Resnik, 1994) </ref>, bracketing text (Brill, 1993a) and labeling nonterminal nodes (Brill, 1993c).
Reference: <author> Brown, Peter, John Cocke, Stephen Della Pietra, Vincent Della Pietra, Fred Jelinek, John Lafferty, Robert Mercer, and Paul Roossin. </author> <year> 1990. </year> <title> A statistical approach to machine translation. </title> <journal> Computational Linguistics, </journal> <volume> 16(2) </volume> <pages> 79-85. </pages> <note> 34 Eric Brill Transformation-Based Error-Driven Learning Brown, </note> <author> Peter, Jennifer Lai, and Robert Mercer. </author> <year> 1991. </year> <title> Word-sense disambiguation using statistical methods. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Berkeley, Ca. </address>
Reference-contexts: An effort has recently been undertaken to create automated machine translation systems where the linguistic information needed for translation is extracted automatically from aligned text corpora <ref> (Brown et al., 1990) </ref>. These are just a few of the many recent applications of corpus-based techniques in natural language processing.
Reference: <author> Bruce, Rebecca and Janyce Wiebe. </author> <year> 1994. </year> <title> Word-sense disambiguation using decomposable models. </title> <booktitle> In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Las Cruces, NM. </address>
Reference: <author> Charniak, Eugene, Curtis Hendrickson, Neil Jacobson, and Michael Perkowitz. </author> <year> 1993. </year> <title> Equations for part of speech tagging. </title> <booktitle> In Proceedings of the Conference of the American Association for Artificial Intelligence (AAAI-93), </booktitle> <address> Washington DC. </address>
Reference: <author> Church, Kenneth. </author> <year> 1988. </year> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Proceedings of the Second Conference on Applied Natural Language Processing, ACL, </booktitle> <address> Austin, Tx. </address>
Reference: <author> Cutting, Doug, Julian Kupiec, Jan Pedersen, and Penelope Sibun. </author> <year> 1992. </year> <title> A practical part-of-speech tagger. </title> <booktitle> In Proceedings of the Third Conference on Applied Natural Language Processing, ACL, </booktitle> <address> Trento, Italy. </address>
Reference: <author> DeMarcken, Carl. </author> <year> 1990. </year> <title> Parsing the LOB corpus. </title> <booktitle> In Proceedings of the 1990 Conference of the Association for Computational Linguistics, </booktitle> <address> Pittsburgh, Pa. </address>
Reference-contexts: In <ref> (DeMarcken, 1990) </ref>, the test set is included in the training set, and so it is difficult to know how this system would do on fresh text. In (Weischedel et al., 1993), a k-best tag experiment was run on the Wall Street Journal corpus.
Reference: <author> DeRose, Stephen. </author> <year> 1988. </year> <title> Grammatical category disambiguation by statistical optimization. </title> <journal> Computational Linguistics, </journal> <volume> 14. </volume>
Reference: <author> Francis, Winthrop Nelson and Henry Kucera. </author> <year> 1982. </year> <title> Frequency analysis of English usage: Lexicon and grammar. </title> <publisher> Houghton Mi*in, </publisher> <address> Boston. </address>
Reference-contexts: If the classification of to were dependent upon the classification of yet another word, this would have to be built into the decision tree as well. Unlike decision trees, intermediate classification results in transformation 3 The original tagged Brown Corpus <ref> (Francis and Kucera, 1982) </ref> makes this distinction; the Penn Treebank (Marcus, Santorini, and Marcinkiewicz, 1993) does not. 12 Eric Brill Transformation-Based Error-Driven Learning based learning are available and can be used as classification progresses.
Reference: <author> Fujisaki, Tetsu, Fred Jelinek, John Cocke, and Ezra Black. </author> <year> 1989. </year> <title> Probabilistic parsing method for sentence disambiguation. </title> <booktitle> In Proceedings of the International Workshop on Parsing Technologies. </booktitle>
Reference: <author> Gale, William and Kenneth Church. </author> <year> 1991. </year> <title> A program for aligning sentences in bilingual corpora. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Berkeley, Ca. </address>
Reference-contexts: A vast amount of on-line text is now available, with much more becoming available in the future. Useful tools, such as large aligned corpora (eg. the aligned Hansards <ref> (Gale and Church, 1991) </ref>) and semantic word hierarchies (eg. Wordnet (Miller, 1990)), have also recently become available. Corpus-based methods are often able to succeed while ignoring the true complexities of language, banking on the fact that complex linguistic phenomena can often be indirectly observed through simple epiphenomena.
Reference: <author> Gale, William, Kenneth Church, and David Yarowsky. </author> <year> 1992. </year> <title> A method for disambiguating word senses in a large corpus. Computers and the Humanities. </title>
Reference: <author> Harris, Zellig. </author> <year> 1962. </year> <title> String Analysis of Language Structure. Mouton and Co., </title> <booktitle> 35 Computational Linguistics Volume 21, Number 4 The Hague. </booktitle>
Reference: <author> Hindle, D. and M. Rooth. </author> <year> 1993. </year> <title> Structural ambiguity and lexical relations. </title> <journal> Computational Linguistics, </journal> <volume> 19(1) </volume> <pages> 103-120. </pages>
Reference-contexts: Email: brill@cs.jhu.edu. c fl 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 4 cer, 1990; Black et al., 1993) and by computing statistical measures of lexical association <ref> (Hindle and Rooth, 1993) </ref>.
Reference: <author> Hindle, Donald. </author> <year> 1989. </year> <title> Acquiring disambiguation rules from text. </title> <booktitle> In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Vancouver, BC. </address>
Reference: <author> Huang, Caroline, Mark Son-Bell, and David Baggett. </author> <year> 1994. </year> <title> Generation of pronunciations from orthographies using transformation-based error-driven learning. </title> <booktitle> In International Conference on Speech and Language Processing (ICSLP), </booktitle> <address> Yokohama, Japan. </address>
Reference-contexts: tagging could readily translate to progress in other areas of lexical, and perhaps structural, ambiguity, such as word sense disambiguation and prepositional phrase attachment disambiguation. 7 Also, it is possible to cast a number of other useful problems as part of speech tagging problems, such as letter to sound translation <ref> (Huang, Son-Bell, and Baggett, 1994) </ref> and building pronunciation networks for speech recognition. Recently, a method has been proposed for using part of speech tagging techniques as a method for parsing with lexicalized grammars (Joshi and Srinivas, 1994).
Reference: <author> Jelinek, Fred. </author> <year> 1985. </year> <title> Self-Organized Language Modelling for Speech Recognition. Dordrecht. In Impact of Processing Techniques on Communication, </title> <editor> J. Skwirzinski, </editor> <publisher> ed. </publisher>
Reference-contexts: This system was derived in under two hours from the transformation-based part of speech tagger described in this paper. 14 Eric Brill Transformation-Based Error-Driven Learning model based stochastic taggers that were automatically trained could achieve high rates of tagging accuracy <ref> (Jelinek, 1985) </ref>. Markov-model based taggers assign to a sentence the tag sequence that maximizes P rob (wordjtag)flP rob (tagjprevious n tags).
Reference: <author> Joshi, Aravind and B. Srinivas. </author> <year> 1994. </year> <title> Disambiguation of super parts of speech (or supertags): Almost parsing. </title> <booktitle> In Proceedings of the 15th International Conference on Computational Linguistics, </booktitle> <address> Kyoto, Japan. </address>
Reference-contexts: Recently, a method has been proposed for using part of speech tagging techniques as a method for parsing with lexicalized grammars <ref> (Joshi and Srinivas, 1994) </ref>. When automated part of speech tagging was initially explored (Klein and Simmons, 1963; Harris, 1962), people manually engineered rules for tagging, sometimes with the aid of a corpus.
Reference: <author> Klein, Sheldon and Robert Simmons. </author> <year> 1963. </year> <title> A computational approach to grammatical coding of English words. </title> <journal> JACM, </journal> <volume> 10. </volume>
Reference: <author> Kupiec, Julian. </author> <year> 1992. </year> <title> Robust part-of-speech tagging using a hidden Markov model. </title> <booktitle> Computer Speech and Language, </booktitle> <pages> 6. </pages>
Reference-contexts: To remedy this problem, we extend the transformation-based tagger by adding contextual transformations that can make reference to words as well as part of speech tags. 11 Version 0.5 of the Penn Treebank was used in all experiments reported in this paper. 12 In <ref> (Kupiec, 1992) </ref>, a limited amount of lexicalization is introduced by having a stochastic tagger with word states for the 100 most frequent words in the corpus. 19 Computational Linguistics Volume 21, Number 4 Change Tag # From To Condition 1 NN VB Previous tag is TO 2 VBP VB One of
Reference: <author> Leech, Geoffrey, Roger Garside, and Michael Bryant. </author> <year> 1994. </year> <title> Claws4: The tagging of the British National Corpus. </title> <booktitle> In Proceedings of the 15th International Conference on Computational Linguistics, </booktitle> <address> Kyoto, Japan. </address>
Reference: <author> Marcus, Mitchell, Beatrice Santorini, and Maryann Marcinkiewicz. </author> <year> 1993. </year> <title> Building a large annotated corpus of English: the Penn Treebank. </title> <journal> Computational Linguistics, </journal> <volume> 19(2). </volume>
Reference-contexts: Unlike decision trees, intermediate classification results in transformation 3 The original tagged Brown Corpus (Francis and Kucera, 1982) makes this distinction; the Penn Treebank <ref> (Marcus, Santorini, and Marcinkiewicz, 1993) </ref> does not. 12 Eric Brill Transformation-Based Error-Driven Learning based learning are available and can be used as classification progresses. Even if decision trees are applied to a corpus in a left-to-right fashion, they only are allowed one pass in which to properly classify. <p> For a description of a fast index-based training algorithm, see (Ramshaw and Marcus, 1994). In figure 4 we list the first twenty transformations learned from training on the Penn Treebank Wall Street Journal Corpus <ref> (Marcus, Santorini, and Marcinkiewicz, 1993) </ref>. 11 The first transformation states that a noun should be changed to a verb if the previous tag is TO, as in to/TO conflict/NN!VB with. The second transformation fixes a tagging such as: might/MD vanish/VBP!VB. The third fixes might/MD not reply/NN!VB.
Reference: <author> Merialdo, Bernard. </author> <year> 1994. </year> <title> Tagging english text with a probabilistic model. </title> <note> Computational Linguistics. </note>
Reference-contexts: However, it appears to be the case that directly estimating probabilities from even a very small manually tagged corpus gives better results than training a hidden Markov model on a large untagged corpus (see <ref> (Merialdo, 1994) </ref>). 9 Earlier versions of this work were reported in (Brill, 1992; Brill, 1994). 15 Computational Linguistics Volume 21, Number 4 one tag labelled as the most likely. Below we show a lexical entry for the word half in the transformation-based tagger.
Reference: <author> Miller, George. </author> <year> 1990. </year> <title> Wordnet: an on-line lexical database. </title> <journal> International Journal of Lexicography, </journal> <volume> 3(4). </volume>
Reference-contexts: A vast amount of on-line text is now available, with much more becoming available in the future. Useful tools, such as large aligned corpora (eg. the aligned Hansards (Gale and Church, 1991)) and semantic word hierarchies (eg. Wordnet <ref> (Miller, 1990) </ref>), have also recently become available. Corpus-based methods are often able to succeed while ignoring the true complexities of language, banking on the fact that complex linguistic phenomena can often be indirectly observed through simple epiphenomena. <p> Rules 64 K 215 96.7 Rule-Based With Lex. Rules 600 K 447 97.2 Rule-Based w/o Lex. Rules 600 K 378 97.0 Table 1 Comparison of Tagging Accuracy With No Unknown Words 24 Eric Brill Transformation-Based Error-Driven Learning simple. Given any source of word class information, such as WordNet <ref> (Miller, 1990) </ref>, the learner is extended such that a rule is allowed to make reference to parts of speech, words, and word classes, allowing for rules such as Change the tag from X to Y if the following word belongs to word class Z.
Reference: <author> Quinlan, J. Ross. </author> <year> 1986. </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106. </pages>
Reference: <author> Quinlan, J. Ross and Ronald Rivest. </author> <year> 1989. </year> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80. </volume>
Reference: <author> Ramshaw, Lance and Mitchell Marcus. </author> <year> 1994. </year> <title> Exploring the statistical derivation of transformational rule sequences for 36 Eric Brill Transformation-Based Error-Driven Learning part-of-speech tagging. In The Balancing Act: </title> <booktitle> Proceedings of the ACL Workshop on Combining Symbolic and Statistical Approaches to Language, </booktitle> <institution> New Mexico State University. </institution>
Reference-contexts: in transformation-based learning can make the learned rules more readily interpretable than decision tree rules for increasing population purity. 5 4 For a discussion on why this is the case, see (Breiman et al., 1984), pages 94-98. 5 For a discussion of other issues regarding these two learning algorithms, see <ref> (Ramshaw and Marcus, 1994) </ref> Computational Linguistics Volume 21, Number 4 4. <p> For a description of a fast index-based training algorithm, see <ref> (Ramshaw and Marcus, 1994) </ref>.
Reference: <author> Roche, Emmanuel and Yves Schabes. </author> <year> 1995. </year> <title> Deterministic part of speech tagging with finite state transducers. </title> <journal> Computational Linguistics, </journal> <volume> 21(2). </volume>
Reference-contexts: Accuracy is consistent across these corpora and tag sets. In addition to obtaining high rates of accuracy and representing relevant linguistic information in a small set of rules, the part of speech tagger can also be made to run extremely fast. <ref> (Roche and Schabes, 1995) </ref> shows a method for converting a list of tagging 30 Eric Brill Transformation-Based Error-Driven Learning transformations into a deterministic finite state transducer with one state transition taken per word of input, the result being a transformation-based tagger whose tagging speed is about ten times that of the
Reference: <author> Schutze, Hinrich and Yoram Singer. </author> <year> 1994. </year> <title> Part of speech tagging using a variable memory Markov model. </title> <booktitle> In Proceedings of the Association for Computational Linguistics, </booktitle> <address> Las Cruces, New Mexico. </address>
Reference: <author> Sharman, Robert, Fred Jelinek, and Robert Mercer. </author> <year> 1990. </year> <title> Generating a grammar for statistical training. </title> <booktitle> In Proceedings of the 1990 Darpa Speech and Natural Language Workshop. </booktitle>
Reference: <author> Weischedel, Ralph, Marie Meteer, Richard Schwartz, Lance Ramshaw, and Jeff Palmucci. </author> <year> 1993. </year> <title> Coping with ambiguity and unknown words through probabilistic models. </title> <note> Computational Linguistics. </note>
Reference-contexts: A stochastic trigram tagger would have to capture this linguistic information indirectly from frequency counts of all trigrams of the form shown in figure 5 (where a star can match any part of speech tag) and from the fact that P (n 0 tjRB) is fairly high. In <ref> (Weischedel et al., 1993) </ref>, results are given when training and testing a Markov-model based tagger on the Penn Treebank Tagged Wall Street Journal Corpus. They cite results making the closed vocabulary assumption that all possible tags for all words in the test set are known. <p> When transformations are allowed to make reference to words and word pairs, some relevant information is probably missed due to sparse data. We are currently exploring the possibility of incorporating word classes into the rule-based learner in hopes of overcoming this problem. The idea is quite 14 In both <ref> (Weischedel et al., 1993) </ref> and here, the test set was incorporated into the lexicon, but was not used in learning contextual information. Testing with no unknown words might seem like an unrealistic test. <p> We are currently experimenting with this idea. In <ref> (Weischedel et al., 1993) </ref>, a statistical approach to tagging unknown words is shown. In this approach, a number of suffixes and important features are prespecified. <p> In (DeMarcken, 1990), the test set is included in the training set, and so it is difficult to know how this system would do on fresh text. In <ref> (Weischedel et al., 1993) </ref>, a k-best tag experiment was run on the Wall Street Journal corpus. They quote the average number of tags per word for various threshold settings, but do not provide accuracy results. 32 Eric Brill Transformation-Based Error-Driven Learning and semantic representations. A.
Reference: <author> Yarowsky, David. </author> <year> 1992. </year> <title> Word-sense disambiguation using statistical models of Roget's categories trained on large corpora. </title> <booktitle> In Proceedings of COLING-92, </booktitle> <pages> pages 454-460, </pages> <address> Nantes, France, </address> <month> July. 37 </month>
References-found: 40

