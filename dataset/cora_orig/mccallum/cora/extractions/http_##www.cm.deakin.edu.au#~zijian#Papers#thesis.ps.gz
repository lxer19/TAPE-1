URL: http://www.cm.deakin.edu.au/~zijian/Papers/thesis.ps.gz
Refering-URL: http://www.cm.deakin.edu.au/~zijian/research.html
Root-URL: 
Title: Constructing New Attributes for Decision Tree Learning  
Author: by Zijian Zheng 
Degree: A thesis submitted in fulfilment of the requirements for the degree of Doctor of Philosophy  
Date: March, 1996  
Address: Sydney NSW 2006, Australia  
Affiliation: Basser Department of Computer Science The University of  
Abstract-found: 0
Intro-found: 1
Reference: [Aha, 1990] <author> D.W. Aha, </author> <title> A Study of Instance-Based Algorithms for Supervised Learning Tasks: Mathematical, Empirical, and Psychological Evaluations, </title> <type> Technical Report 90-42, Ph.D. Thesis, </type> <institution> Department of Information and Computer Science, University of California, </institution> <address> Irvine, CA. </address>
Reference-contexts: Other theory description languages include neural networks [Rumelhart et al., 1986; Hinton, 1989], linear discriminant functions [James, 1985], and instance-based methods <ref> [Aha, 1990; Aha, Kibler, and Albert, 1991] </ref>. An important advantage of decision trees and production rules, as theory description languages, is that they are relatively easy for humans to understand. Actually, they have been used by human experts to express and process their knowledge in a wide variety of domains.
Reference: [Aha et al., 1991] <author> D.W. Aha, D. Kibler, and M.K. Albert, </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 37-66. </pages>
Reference: [Aha and Bankert, 1994] <author> D.W. Aha and R.L. Bankert, </author> <title> Feature selection for case-based classification of cloud types: an empirical comparison. </title> <editor> In D.W. Aha (ed.), </editor> <booktitle> Case-Based Reasoning: Papers from the 1994 Workshop. </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference: [Almuallim and Dietterich, 1992] <author> H. Almuallim and T.G. Dietterich, </author> <title> Efficient algorithms for identifying relevant features. </title> <booktitle> Proceedings of the Ninth Canadian Conference on Artificial Intelligence, </booktitle> <address> Vancouver, BC: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 38-45. </pages>
Reference: [Baffes and Mooney, 1993] <author> P.T. Baffes and R.J. Mooney, </author> <title> Symbolic revision of theories with M-of-N rules. </title> <booktitle> Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 1135-1140. </pages>
Reference-contexts: Crls is shown to outperform standard rule learning in several medical domains [Spackman, 1988]. Ting [1991] demonstrates the performance advantage of MoN as well as ID2-of-3 over C4.5rules in terms of higher prediction accuracy and smaller theory size in a biology domain (Splice-junction). Neither, a propositional theory refinement system <ref> [Baffes and Mooney, 1993] </ref>, is capable of revising Mof-N rules by modifying the M value of Mof-N representations. <p> When building a decision tree, all of them construct one new attribute for each decision node using the local training set. Instead of building decision trees, Crls [Spackman, 1988] and MoN [Ting, 1994] learn Mof-N rules. The symbolic theory revision system Neither <ref> [Baffes and Mooney, 1993] </ref> refines Mof-N rules. The rule learning algorithms Induce [Michalski, 1978], AQ17-dci, and AQ17-mci [Bloedorn et al., 1993] use the counting operator 22 #VarEQ (x) to construct new attributes that count the number of attributes which take the value x.
Reference: [Bailey and Elkan, 1993] <author> T.L. Bailey and C. Elkan, </author> <title> Estimating the accuracy of learned concepts. </title> <booktitle> Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 895-900. </pages>
Reference-contexts: Recent experimental and theoretical results have shown that 10-fold cross-validation is a good method for accuracy estimation, compared with other methods such as leave-one-out and bootstrap <ref> [Efron, 1983; Bailey and Elkan, 1993; Kohavi, 1995] </ref>. In all the experiments of this thesis, all the algorithms being compared are run with their default option settings, except when otherwise indicated. Pruned trees are always used for all the tree learning algorithms when reporting results.
Reference: [Bloedorn and Michalski, 1991] <author> E. Bloedorn and R.S. Michalski, </author> <title> Data-driven constructive induction in AQ17-pre: a method and experiments. </title> <booktitle> Proceedings of IEEE Third International Conference on Tools for Artificial Intelligence, </booktitle> <address> Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <pages> 30-37. 219 220 </pages>
Reference-contexts: Systems with the data-driven strategy, such as ID2-of-3 [Murphy and Pazzani, 1991] and AQ17-dci <ref> [Bloedorn and Michalski, 1991] </ref>, find relevant patterns from input data directly, while systems with the knowledge-driven strategy apply domain-knowledge to create new attributes. This strategy needs information additional to what can be obtained from the training data. <p> Wnek and Michalski [1994] show the accuracy advantage of AQ17-hci over some rule learning and decision tree learning algorithms in four artificial domains. 17 Examples of this selected class are referred to as positive examples, while examples of the other classes are referred to as negative examples. 52 AQ17-dci <ref> [Bloedorn and Michalski, 1991] </ref> uses the data-driven strategy to construct new attributes for rule learning. Based on a generate and test method, AQ17-dci repeats new attribute generating and rule learning until the rules produced satisfy a rule critic [Bloedorn and Michalski, 1991]. <p> of the other classes are referred to as negative examples. 52 AQ17-dci <ref> [Bloedorn and Michalski, 1991] </ref> uses the data-driven strategy to construct new attributes for rule learning. Based on a generate and test method, AQ17-dci repeats new attribute generating and rule learning until the rules produced satisfy a rule critic [Bloedorn and Michalski, 1991]. For each pair of linear attributes (with a finite number of ordered discrete values), mathematical operators including addition, subtraction, multiplication, integer division, equality, greater-than, and less-than are applied. <p> Those attributes with AQF values higher than the user-defined threshold are retained. The HCI method is reported to be more accurate than rule learning based on primitive attributes in a real-world domain involving the classification of texture images <ref> [Bloedorn and Michalski, 1991] </ref>. AQ17-mci [Bloedorn et al., 1993] is a multistrategy rule learning algorithm. It incorporates AQ17-hci to perform hypothesis-driven new attribute construction, and incorporates AQ17-dci to carry out data-driven new attribute construction.
Reference: [Bloedorn et al., 1993] <author> E. Bloedorn, R.S. Michalski, and J. Wnek, </author> <title> Multistrategy constructive induction: </title> <booktitle> AQ17-mci. Proceedings of the Second International Workshop on Multistrategy Learning, </booktitle> <pages> 188-203. </pages>
Reference-contexts: Nevertheless, Mof-N representations still have binary values. Only a few systems explore methods of constructing new continuous-valued attributes using mathematical operators (e.g. Bacon [Langley, Simon, Bradshaw, and Zytkow, 1987], Induce [Michalski, 1978]) or attribute counting attributes 14 (e.g. Induce, AQ17-dci, and AQ17-mci <ref> [Bloedorn et al., 1993] </ref>). In addition, systems such as Lmdt [Brodley and Utgoff, 1992], Swap1 [Indurkhya and Weiss, 1991], and Ccaf [Yip and Webb, 1994] construct linear machines, linear discriminant functions, or canonical discriminant functions as new attributes. <p> Those attributes with AQF values higher than the user-defined threshold are retained. The HCI method is reported to be more accurate than rule learning based on primitive attributes in a real-world domain involving the classification of texture images [Bloedorn and Michalski, 1991]. AQ17-mci <ref> [Bloedorn et al., 1993] </ref> is a multistrategy rule learning algorithm. It incorporates AQ17-hci to perform hypothesis-driven new attribute construction, and incorporates AQ17-dci to carry out data-driven new attribute construction. <p> It incorporates AQ17-hci to perform hypothesis-driven new attribute construction, and incorporates AQ17-dci to carry out data-driven new attribute construction. AQ17-mci selects constructive strategies and constructive operators to apply by using meta-rules that are provided by the user initially, and are generated from the learning experience later on <ref> [Bloedorn et al., 1993] </ref>. <p> on <ref> [Bloedorn et al., 1993] </ref>. It is shown that the performance of AQ17-mci is better than those of selective rule learning algorithms and unistrategy constructive rule learning algorithms including AQ17-hci and AQ17-dci in terms of higher prediction accuracy and lower ruleset complexity 18 in the Noisy and Irrelevant Monks2 domain [Bloedorn et al., 1993]. The idea of constructing attribute counting attributes are also explored in the Cindi [Callan and Utgoff, 1991] and Zenith [Fawcett and Utgoff, 1992] systems. <p> Some systems generate new continuous-valued attributes using different constructive operators. For example, Bacon [Langley et al., 1987a] and Induce [Michalski, 1978] use mathematical operators such as multiplication and division. Ccaf [Yip and Webb, 1994] adopts canonical discriminant functions. Systems such as Induce, AQ17-dci, and AQ17-mci <ref> [Bloedorn et al., 1993] </ref> construct attribute counting attributes. Very few systems construct new nominal attributes, although Lmdt [Brodley and Utgoff, 1992] that creates linear machines and C4.5 that uses subset-ting are two such systems. <p> Instead of building decision trees, Crls [Spackman, 1988] and MoN [Ting, 1994] learn Mof-N rules. The symbolic theory revision system Neither [Baffes and Mooney, 1993] refines Mof-N rules. The rule learning algorithms Induce [Michalski, 1978], AQ17-dci, and AQ17-mci <ref> [Bloedorn et al., 1993] </ref> use the counting operator 22 #VarEQ (x) to construct new attributes that count the number of attributes which take the value x.
Reference: [Blumer et al., 1987] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M.K. Warmuth, </author> <title> Oc-cam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24, </volume> <pages> 377-380. </pages>
Reference-contexts: For each experiment, a training set and a test set are independently drawn from a uniform distribution. The size of the test sets is 2000. The sizes of training sets, given in Table 3.1, are the result of a VC dimension analysis <ref> [Vapnik and Chervonenkis, 1971; Blumer et al., 1987; Haussler, 1988] </ref> for finding the number of examples which would suffice for an ideal learning algorithm to create a consistent hypothesis with an error rate less than 10% on any test set (see Pagallo [1990] for details).
Reference: [Brazdil et al., 1993] <author> P. Brazdil, J. Gama, and B. Henery, </author> <title> Comparison of ML and statistical approaches using meta level learning. </title> <booktitle> Workshop Notes on Real-World Applications of Machine Learning, European Conference on Machine Learning. </booktitle>
Reference: [Breiman et al., 1984] <author> L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone, </author> <title> Classification And Regression Trees, </title> <address> Belmont, CA: </address> <publisher> Wadsworth. </publisher>
Reference-contexts: This kind of induction is called selective induction. There are many learning systems of this type such as ID3 [Quinlan, 1983; 1986], Cart 8 <ref> [Breiman et al., 1984] </ref>, C4.5 [Quinlan, 1993a], PLS1 [Rendell, 1983], CN2 [Clark and Niblett, 1989], and AQ11 [Michalski and Chilausky, 1980a]. <p> A good test should divide a dataset into subsets as pure as possible, not needing to be divided further. Information-based or probabilistic heuristics can provide effective guidance for test selection. Examples of well-known preference criteria are information gain, information gain ratio [Quinlan, 1986b; 1993a], Gini, and Twoing <ref> [Breiman et al., 1984] </ref>. It has been shown that a greedy algorithm with this kind of heuristic is adequate for many learning tasks [Quinlan, 1983; 1986b; Quin-lan, Compton, Horn, and Lazarus, 1987; Breiman et al., 1984; Pagallo, 1990; Murphy and Pazzani, 1991; Zheng, 1995a]. <p> In this case, a tree perfectly consistent with a training set often has a low accuracy on unseen data. This is called overfitting (or overspecialization) to the training data <ref> [Breiman et al., 1984; Quinlan, 1986b; 1987b; Mingers, 1989b] </ref>. The reason is that some spurious tests selected by algorithms may discriminate few training examples by chance, 3 while these tests have no predictive value when applied to fresh data. <p> One example of this kind of significance test is the chi-square test used in ID3 [Quinlan, 1986a; 1986b]. The problem with this first type of approach is that the stopping criteria (or thresholds) are very hard to get right to produce good trees <ref> [Breiman et al., 1984; Quinlan, 1993a] </ref>. The second type of approach is to keep the simple stopping criterion unchanged. After growing a large tree (called a raw tree), this tree is pruned back to a good size. <p> Several pruning 3 Such tests often occur at nodes near leaves of a tree. 25 methods have been developed and used for decision tree learning, such as cost-complexity pruning <ref> [Breiman et al., 1984] </ref>, reduced error pruning [Quinlan, 1987b], and pessimistic pruning [Quinlan, 1987b; 1993a]. Surveys and empirical comparisons of different pruning methods for decision tree learning can be found in Quinlan [1987b], Mingers [1989b], and Esposito, Malerba, and Semeraro [1993]. <p> In addition, C4.5 has the ability to group the values of a nominal attribute, and creates a test with one outcome for each group of values. 26 Cart <ref> [Breiman et al., 1984] </ref> is also a well-known decision tree building algorithm developed by statisticians. It uses Gini or Twoing [Breiman et al., 1984, pp. 103-108] as the test selection heuristic function when growing trees, and uses cost-complexity pruning [Breiman et al., 1984, pp. 66-81] when pruning trees. <p> In addition, C4.5 has the ability to group the values of a nominal attribute, and creates a test with one outcome for each group of values. 26 Cart [Breiman et al., 1984] is also a well-known decision tree building algorithm developed by statisticians. It uses Gini or Twoing <ref> [Breiman et al., 1984, pp. 103-108] </ref> as the test selection heuristic function when growing trees, and uses cost-complexity pruning [Breiman et al., 1984, pp. 66-81] when pruning trees. <p> It uses Gini or Twoing [Breiman et al., 1984, pp. 103-108] as the test selection heuristic function when growing trees, and uses cost-complexity pruning <ref> [Breiman et al., 1984, pp. 66-81] </ref> when pruning trees. Instead of using supervised test evaluation functions such as those mentioned above, Van de Merckt [1993] introduces unsupervised heuristics for finding cut points of continuous-valued attributes to decision tree learning. <p> However, the problem is that some available data must be reserved for pruning, so the original tree can only be built using a smaller subset. If the dataset is not large, this can lead to an inferior tree. The v-fold cross-validation method <ref> [Stone, 1974; Breiman et al., 1984] </ref> can help to mitigate this problem but with a drawback that v trees, instead of one tree, must be built. Another type of tree evaluation method is to use only the training set that is used to build the raw tree. <p> After a set of rules is created, weakest-link pruning is used to find the right complexity ruleset [Indurkhya and Weiss, 1991]. This idea is very similar to that used by Cart for pruning decision trees <ref> [Breiman et al., 1984] </ref>. Generating rules from decision trees Unlike rule learning algorithms discussed above, some algorithms convert decision trees into production rules. Decision tree learning is usually more efficient than rule learning. Building decision trees can reduce the search space for generating rules. <p> Ragavan and Rendell [1993] show that LFC achieves quite high accuracies on several real-world domains such as Pima Indians Diabetes. Like LFC, Cart <ref> [Breiman et al., 1984] </ref> can also construct new attributes in the form of conjunction or disjunction as tests for decision nodes when building decision trees. <p> Greedy search is used to find the conjunction or disjunction of individual conditions derived from single primitive attributes that maximizes the decrease in impurity of a node. The impurity <ref> [Breiman et al., 1984] </ref> of a node is a measure that has the smallest value when all the instances in the node belong to only one class and has the largest value when all classes are equally mixed together in the node. <p> A good coefficient vector is found by using heuristic hill climbing search and backward attribute elimination <ref> [Breiman et al., 1984] </ref> so that the impurity of the partition defined by the corresponding linear combination is minimized. OC1 [Murthy, Kasif, and Salzberg, 1994] is an extension of Cart in the direction of constructing linear combinations for decision tree learning. <p> Instead of using the results of selective induction to guide their new attribute construction, they create new attributes directly from training data when building decision trees. For example, Cart <ref> [Breiman et al., 1984] </ref> can create one Boolean combination of primitive attributes as a new attribute at each decision node during the process of building decision trees. In addition, Cart can also construct linear discriminants as new binary attributes. <p> Fringe, Symmetric Fringe [Pagallo and Haussler, 1989; 1990; Pagallo, 1990], SymFringe, DCFringe [Yang et al., 1991], and SFringe (see Appendix B), the Citre algorithm [Matheus and Rendell, 1989; Matheus, 1989], the CI algorithms [Zheng, 1992] (also see Chapter 4), the 124 LFC algorithm [Ragavan and Rendell, 1993], the Cart algorithm <ref> [Breiman et al., 1984] </ref>, the ID2-of-3 algorithm [Murphy and Pazzani, 1991], and the Lmdt algorithm [Brodley and Utgoff, 1992]. Like CAT, the Fringe family of algorithms and the CI algorithms use the hypothesis-driven strategy to construct conjunctions and/or disjunctions for decision trees.
Reference: [Brodley and Utgoff, 1992] <author> C.E. Brodley and P.E. Utgoff, </author> <title> Multivariate versus univariate decision trees. </title> <type> COINS Technical Report 92-8, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference-contexts: In this thesis, decision tree learning using constructive induction methods is referred to as constructive decision tree learning, while that using selective induction methods is referred to as selective decision tree learning. Decision trees built by selective decision tree learning algorithms such as C4.5 are called univariate trees <ref> [Brodley and Utgoff, 1992] </ref> since a test at each decision node is based on a single primitive attribute. By contrast, constructive decision tree learning algorithms such as Fringe and ID2-of-3 create multi-variate trees where tests consist of multiple primitive attributes. <p> Only a few systems explore methods of constructing new continuous-valued attributes using mathematical operators (e.g. Bacon [Langley, Simon, Bradshaw, and Zytkow, 1987], Induce [Michalski, 1978]) or attribute counting attributes 14 (e.g. Induce, AQ17-dci, and AQ17-mci [Bloedorn et al., 1993]). In addition, systems such as Lmdt <ref> [Brodley and Utgoff, 1992] </ref>, Swap1 [Indurkhya and Weiss, 1991], and Ccaf [Yip and Webb, 1994] construct linear machines, linear discriminant functions, or canonical discriminant functions as new attributes. Note that linear machines, as tests, have multiple values, one for each class. <p> Brodley and Utgoff [1995] explore and review four methods, including the Cart method, of generating linear combination tests for decision tree learning. It is experimentally demonstrated that constructing linear combination tests generally improves the accuracies of the resulting decision trees over univariate trees. Lmdt <ref> [Brodley and Utgoff, 1992] </ref> learns multivariate trees by training linear machines based on a subset of the primitive attributes. <p> It is capable of finding and eliminating irrelevant attributes. When a linear machine being trained is close to its final set of boundaries, Lmdt deletes, from the linear machine, the attribute that contributes least to discriminating the set of instances at that node based on a dispersion 46 measure <ref> [Brodley and Utgoff, 1992] </ref>. Lmdt then continues training the linear machine. Lmdt uses a linear machine as a new nominal test at each decision node. <p> LFC [Ragavan and Rendell, 1993] uses negation and conjunction as constructive operators. It creates one conjunction for each decision node by using a directed lookahead search method. Another multivariate tree learning algorithm is Lmdt <ref> [Brodley and Utgoff, 1992] </ref> that generates a linear machine at each decision node. 4.7 Conclusions Based on the following three observations, this chapter has proposed a novel fixed rule-based approach to constructing conjunctions as new attributes for decision tree learning. (1) Existing hypothesis-driven constructive decision tree learning algorithms use the fixed <p> 1991], and SFringe (see Appendix B), the Citre algorithm [Matheus and Rendell, 1989; Matheus, 1989], the CI algorithms [Zheng, 1992] (also see Chapter 4), the 124 LFC algorithm [Ragavan and Rendell, 1993], the Cart algorithm [Breiman et al., 1984], the ID2-of-3 algorithm [Murphy and Pazzani, 1991], and the Lmdt algorithm <ref> [Brodley and Utgoff, 1992] </ref>. Like CAT, the Fringe family of algorithms and the CI algorithms use the hypothesis-driven strategy to construct conjunctions and/or disjunctions for decision trees. <p> Ccaf [Yip and Webb, 1994] adopts canonical discriminant functions. Systems such as Induce, AQ17-dci, and AQ17-mci [Bloedorn et al., 1993] construct attribute counting attributes. Very few systems construct new nominal attributes, although Lmdt <ref> [Brodley and Utgoff, 1992] </ref> that creates linear machines and C4.5 that uses subset-ting are two such systems. As mentioned before, different types of attributes (binary, nominal, and continuous-valued) are appropriate for describing different characteristics of examples, and perform differently when used to generate theories. <p> It achieved quite high prediction accuracies in some real-world domains such as Pima Indians Diabetes, but the problem is that it has a sensitive parameter "Lookahead Depth" which needs to be set when applied to a domain. Another multivariate tree learning algorithm is Lmdt <ref> [Brodley and Utgoff, 1992] </ref> that generates a linear machine as a nominal attribute with a fixed number of values at 182 each decision node when building a tree.
Reference: [Brodley and Utgoff, 1995] <author> C.E. Brodley and P.E. Utgoff, </author> <title> Multivariate decision trees. </title> <journal> Machine Learning, </journal> <volume> 19, </volume> <pages> 45-77. </pages>
Reference: [Buntine and Niblett, 1992] <author> W. Buntine and T. Niblett, </author> <title> Technical Note: A further comparison of splitting rules for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 75-85. </pages>
Reference-contexts: Unfortunately, the real world is not perfect. Most real-world datasets 1 Note that the conclusion concerning the random test selection criterion drawn by Mingers [1989a] is not correct as an unsound experimental methodology was used <ref> [Buntine and Niblett, 1992; Liu and White, 1994] </ref>. 2 "Reasonable size", here, means that the training set should contain examples representing most cases in the whole instance space defined by all attributes.
Reference: [Callan and Utgoff, 1991] <author> J.P. Callan and P.E. Utgoff, </author> <title> A transformational approach to constructive induction. </title> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 122-126. </pages>
Reference-contexts: The idea of constructing attribute counting attributes are also explored in the Cindi <ref> [Callan and Utgoff, 1991] </ref> and Zenith [Fawcett and Utgoff, 1992] systems. Cindi performs constructive induction for problem solving by transforming information about a 18 This is the number of conditions in a ruleset. 53 search problem into a set of numeric attributes (terms) that describe search states. <p> The resulting terms are expected to be appropriate for learning an evaluation function over search states for problem solving. Four transformations are used by Cindi to create new terms from statements written in first order predicate calculus: LE, AE, UQ, and EQ <ref> [Callan and Utgoff, 1991] </ref>. As a restricted form of Michalski's counting operators, the UQ transformation generates a numeric term from a Boolean expression beginning with a universal quantifier (8). It produces a UQ term calculating the percentage of permutations of variable bindings satisfying the Boolean expression. <p> Two variants of Michalski's attribute counting operators are used to construct new terms (attributes) for learning evaluation functions over search states in problem solving systems. They are Cindi <ref> [Callan and Utgoff, 1991] </ref> and Zenith [Fawcett and Utgoff, 1992]. The UQ transformation of Cindi creates a numeric attribute from a boolean expression beginning with a universal quantifier.
Reference: [Caruana and Freitag, 1994] <author> R. Caruana and D. Freitag, </author> <title> Greedy attribute selection. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 28-36. </pages>
Reference-contexts: Even worse, some of them might be irrelevant to the target theory. Note that irrelevant attributes usually result in worse performance of inductive learning <ref> [Caruana and Freitag, 1994; Langley, 1994] </ref>.
Reference: [Catlett, 1991a] <author> J. Catlett, </author> <title> On changing continuous attributes into ordered discrete attributes. </title> <booktitle> Proceedings of the Fifth European Working Session on Learning, </booktitle> <address> Berlin: </address> <publisher> Springer-Verlag, </publisher> <pages> 164-178. </pages>
Reference-contexts: Therefore, it should be able to alleviate the fragmentation problem of nominal Xof-N attributes as well. At the moment, XofN uses cut points found by C4.5 to discretize primitive continuous-valued attributes. Other discretization methods that can be used include multi-interval discretization methods <ref> [Catlett, 1991a; Fayyad and Irani, 1993] </ref>, supervised/unsupervised methods [Van de Merckt, 1993], and an entropy method [Ragavan and Rendell, 1993]. Both Catlett [1991a] and Fayyad and Irani [1993] recursively apply a binary splitting procedure 16 to a continuous-valued attribute, but they use different stopping criteria.
Reference: [Catlett, 1991b] <author> J. Catlett, </author> <title> Megainduction: machine learning on very large databases, </title> <type> Ph.D. Thesis, </type> <institution> Basser Department of Computer Science, The University of Sydney. </institution>
Reference-contexts: It allows C4.5 to build decision trees iteratively based on a gradually enlarged window which is a subset of the training set. We do not discuss this technique in detail since it is principally used for overcoming memory limits although it can sometimes lead to faster construction of trees <ref> [Catlett, 1991b] </ref> and/or more accurate trees [Quinlan, 1993a]. In addition to learning decision trees, Quinlan's C4.5 package includes other features such as generating production rules from decision trees and interacting with classification models. For details, please see Quinlan [1993a].
Reference: [Catlett, 1991c] <author> J. Catlett, </author> <title> Megainduction: a test flight. </title> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 596-599. 221 </pages>
Reference: [Cestnik et al., 1987] <author> B. Cestnik, I. Kononenko, and I. Bratko, </author> <title> Assistant 86: a knowledge-elicitation tool for sophisticated users. </title> <editor> In I. Bratko and N. Lavrac (eds.), </editor> <booktitle> Progress in Machine Learning Proceedings of the Second European Working Session on Learning (EWSL87), </booktitle> <address> Wilmslow, UK: </address> <publisher> Sigma Press, </publisher> <pages> 31-45. </pages>
Reference-contexts: Acls [Patterson and Niblett, 1983] generalizes ID3 by allowing attributes to be integer-valued. Expert-ease, Ex-tran, and Rule-Master are commercial derivatives of Acls. Furthermore, based on ID3, Assistant <ref> [Cestnik et al., 1987] </ref> introduces mechanisms to handle real-valued and nominal attributes as well as missing (unknown) values. It can split the values of a nominal attribute into two subsets to create a binary test. For null leaves (leaves without any training examples in them) [Cestnik et al., 1987], Assistant applies <p> Furthermore, based on ID3, Assistant <ref> [Cestnik et al., 1987] </ref> introduces mechanisms to handle real-valued and nominal attributes as well as missing (unknown) values. It can split the values of a nominal attribute into two subsets to create a binary test. For null leaves (leaves without any training examples in them) [Cestnik et al., 1987], Assistant applies the Bayesian classification principle when an unseen example falls into the null leaves. GID3 [Cheng et al., 1988] can build more general trees than ID3. <p> The definition of split information is amended by regarding the examples with unknown values as an additional group. For partitioning training examples, C4.5 uses a probabilistic approach <ref> [Cestnik et al., 1987] </ref>. An example with a known value of the test attribute at a node is processed as usual. An example with an unknown value of the test attribute at a node is partially assigned to each subset.
Reference: [Chatfield, 1978] <author> C. Chatfield, </author> <title> Statistics for Technology: A Course in Applied Statistics, </title> <publisher> London: Chapman and Hall. </publisher>
Reference-contexts: In all the experiments of this thesis, all the algorithms being compared are run with their default option settings, except when otherwise indicated. Pruned trees are always used for all the tree learning algorithms when reporting results. To compare the accuracies of two algorithms, a two-tailed block-based pairwise t-test <ref> [Chatfield, 1978] </ref> is conducted. In the Monks domains, because only one block is available for each problem, a two-tailed instance-based pairwise sign-test [Chatfield, 1978] is used. A difference is considered as significant if the significance level of the t-test or sign-test is above 95%. <p> Pruned trees are always used for all the tree learning algorithms when reporting results. To compare the accuracies of two algorithms, a two-tailed block-based pairwise t-test <ref> [Chatfield, 1978] </ref> is conducted. In the Monks domains, because only one block is available for each problem, a two-tailed instance-based pairwise sign-test [Chatfield, 1978] is used. A difference is considered as significant if the significance level of the t-test or sign-test is above 95%. Learning curves are used to demonstrate how the performances of learning algorithms scale up when the training set size increases.
Reference: [Cheng et al., 1988] <author> J. Cheng, U.M. Fayyad, K.B. Irani, and Z. Qian, </author> <title> Improved decision trees: a generalized version of ID3. </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 100-106. </pages>
Reference-contexts: They use information-based heuristic functions for test selection. Other members of the Tdidt family include Acls [Patterson and Niblett, 1983], Assistant [Cestnik, Kononenko, and Bratko, 1987], Expert-ease, Ex-tran, Rule-Master, and GID3 <ref> [Cheng et al., 1988] </ref>. Acls [Patterson and Niblett, 1983] generalizes ID3 by allowing attributes to be integer-valued. Expert-ease, Ex-tran, and Rule-Master are commercial derivatives of Acls. Furthermore, based on ID3, Assistant [Cestnik et al., 1987] introduces mechanisms to handle real-valued and nominal attributes as well as missing (unknown) values. <p> It can split the values of a nominal attribute into two subsets to create a binary test. For null leaves (leaves without any training examples in them) [Cestnik et al., 1987], Assistant applies the Bayesian classification principle when an unseen example falls into the null leaves. GID3 <ref> [Cheng et al., 1988] </ref> can build more general trees than ID3.
Reference: [Clark and Niblett, 1989] <author> P. Clark and T. Niblett, </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 261-283. </pages>
Reference-contexts: This kind of induction is called selective induction. There are many learning systems of this type such as ID3 [Quinlan, 1983; 1986], Cart 8 [Breiman et al., 1984], C4.5 [Quinlan, 1993a], PLS1 [Rendell, 1983], CN2 <ref> [Clark and Niblett, 1989] </ref>, and AQ11 [Michalski and Chilausky, 1980a]. <p> However, the number of nodes in the OODG is smaller than that in the corresponding decision tree in most of these domains. 2.2.2 Rule learning Most rule learning algorithms such as the A q algorithm [Michalski and Chilausky, 1980b] and the CN2 algorithm <ref> [Clark and Niblett, 1989] </ref> generate a set of rules using a covering approach, also termed separate and conquer by analogy with the divide and conquer approach discussed above. Some other systems such as C4.5rules [Quinlan, 1987a; 1993a] create rules by transforming decision trees. <p> The well-known covering algorithm A q is the basis of the AQ family of systems such as AQ11 [Michalski and Chilausky, 1980b], AQ15 [Michalski et al., 1986], and AQR <ref> [Clark and Niblett, 1989] </ref>. The rules learned by A q are in the form of VL1 (Variable-valued Logic 1), which is a multiple-valued logic propositional calculus with typed variables [Michalski, 1972]. The antecedent of a rule is called a cover. It is the disjunction of complexes. <p> A new seed is then chosen again from the remaining positive examples, and the process is repeated. When all the positive examples are removed, the disjunction of all generated complexes forms a cover of the selected class. Another well-known rule learning algorithm that uses covering method is CN2 <ref> [Clark and Niblett, 1989] </ref>. It incorporates ideas from both the ID3 algorithm and the A q algorithm. Like the A q algorithm, CN2 generates complexes using beam search, but it does not depend on specific examples (seeds). <p> In each iteration, the algorithm searches for a complex that covers a large number of examples of one class, say C, and few examples of other classes. Complexes are evaluated using an entropy evaluation function and a likelihood ratio statistic evaluation function which determine their predictiveness and reliability respectively <ref> [Clark and Niblett, 1989] </ref>. Having created a good complex, the algorithm deletes the examples that it covers, and adds the rule "if &lt;complex&gt; then C" to the end of the rule list. This process is repeated until no satisfactory complexes in terms of the evaluation functions can be created.
Reference: [Clark and Boswell, 1991] <author> P. Clark and R. Boswell, </author> <title> Rule induction with CN2: some recent improvements. </title> <booktitle> Proceedings of the Fifth European Working Session on Learning, </booktitle> <address> Berlin: </address> <publisher> Springer-Verlag, </publisher> <pages> 151-163. </pages>
Reference-contexts: This process is repeated until no satisfactory complexes in terms of the evaluation functions can be created. A later improvement to CN2 allows it to generate unordered rules as well <ref> [Clark and Boswell, 1991] </ref>. It constructs rules for each class in turn by using the examples of the selected class as the positive examples and the examples of other classes as the negative examples. In addition, the Laplace function [Clark and Boswell, 1991] instead of the entropy function is used to <p> improvement to CN2 allows it to generate unordered rules as well <ref> [Clark and Boswell, 1991] </ref>. It constructs rules for each class in turn by using the examples of the selected class as the positive examples and the examples of other classes as the negative examples. In addition, the Laplace function [Clark and Boswell, 1991] instead of the entropy function is used to evaluate rules. 37 The Class algorithm [Webb, 1993] uses the same covering method as the version of CN2 for unordered rules. <p> As far as systematic search is concerned, the closest related work is Opus [Webb, 1993]. It carries out systematic search with pruning over the space of all possible disjuncts at the inner level of a covering rule learning algorithm, Class [Webb, 1993], which is similar to CN2 <ref> [Clark and Boswell, 1991] </ref>. The method of systematic search with pruning used in CAT is very similar to that in Opus. The main difference is that CAT uses information gain as the evaluation function, while Opus employs the Laplace function [Webb, 1993].
Reference: [Cohen, 1993] <author> W.W Cohen, </author> <title> Efficient pruning methods for separate-and-conquer rule learning systems. </title> <booktitle> Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 988-994. </pages>
Reference-contexts: The antecedent of each rule is a conjunction. As for decision tree learning, pruning can be performed after a rule or a ruleset is generated by examining whether some conditions of a rule or some rules of the ruleset can be deleted based on heuristic functions <ref> [Cohen, 1993] </ref>. The well-known covering algorithm A q is the basis of the AQ family of systems such as AQ11 [Michalski and Chilausky, 1980b], AQ15 [Michalski et al., 1986], and AQR [Clark and Niblett, 1989].
Reference: [Dietterich and Bakiri, 1995] <author> T.G. Dietterich and G. Bakiri, </author> <title> Solving multiclass learning problems via error-correcting output codes. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <pages> 263-286. </pages>
Reference: [Dietterich et al., 1990] <author> T.G. Dietterich, H. Hild, and G. Bakiri, </author> <title> A comparative study of ID3 and backpropagation for English text-to-speech mapping. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kauf-mann, </publisher> <pages> 24-31. </pages>
Reference-contexts: Blanks are added before and/or after the word if necessary. The class of an instance is the phoneme, stress, or phoneme-stress pair of the letter in the central position. While some researchers use complex non-mutually exclusive encoding and decoding methods <ref> [Dietterich et al., 1990] </ref> for classes, we use the simple traditional method. All three subdomains have mutually exclusive multiple classes.
Reference: [Drastal et al., 1989] <author> G. Drastal, S. Raatz, G. Czako, and S. Raatz, </author> <title> Induction in an abstraction space: a form of constructive induction. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 708-712. 222 </pages>
Reference: [Duda and Fossum, 1966] <author> R.O. Duda and H. Fossum, </author> <title> Pattern classification by iteratively determined linear and piecewise linear discriminant functions. </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> EC-15, </volume> <pages> 220-232. </pages>
Reference-contexts: Lmdt [Brodley and Utgoff, 1992] learns multivariate trees by training linear machines based on a subset of the primitive attributes. During the generation of a tree, at each decision node, Lmdt uses the absolute error correction rule <ref> [Duda and Fossum, 1966] </ref> to train a linear machine in the form of a set of linear discriminant functions g i (Y ) = W T i Y , where Y is an instance description consisting of a constant value 1 and the numerically encoded attributes, and W i is a
Reference: [Duda and Hart, 1973] <author> R.O. Duda and P.E. Hart, </author> <title> Pattern Classification and Scene Analysis, </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: [Efron, 1983] <author> B. Efron, </author> <title> Estimating the error rate of a prediction rule: improvement on cross-validation. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 78, </volume> <pages> 316-331. </pages>
Reference-contexts: When used to estimate the true prediction accuracy of a theory learned from the entire dataset, leave-one-out is almost unbiased as it allows the largest possible number of training examples while still basing the estimation of accuracy on unseen cases, but it has high variance <ref> [Efron, 1983] </ref>. In addition, leave-one-out is very expensive for execution if the dataset is not small, because an algorithm needs to be run n times where n is equal to the dataset size. <p> Recent experimental and theoretical results have shown that 10-fold cross-validation is a good method for accuracy estimation, compared with other methods such as leave-one-out and bootstrap <ref> [Efron, 1983; Bailey and Elkan, 1993; Kohavi, 1995] </ref>. In all the experiments of this thesis, all the algorithms being compared are run with their default option settings, except when otherwise indicated. Pruned trees are always used for all the tree learning algorithms when reporting results.
Reference: [Esposito et al., 1993] <author> F. Esposito, D. Malerba, and G. Semeraro, </author> <title> Decision tree pruning as a search in the state space. </title> <booktitle> Proceedings of European Conference on Machine Learning, </booktitle> <address> Berlin: </address> <publisher> Springer-Verlag, </publisher> <pages> 165-184. </pages>
Reference: [Fawcett and Utgoff, 1992] <author> T.E. Fawcett and P.E. Utgoff, </author> <title> Automatic feature generation for problem solving systems. </title> <booktitle> Proceedings of the Ninth International Workshop on Machine Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 144-153. </pages>
Reference-contexts: The idea of constructing attribute counting attributes are also explored in the Cindi [Callan and Utgoff, 1991] and Zenith <ref> [Fawcett and Utgoff, 1992] </ref> systems. Cindi performs constructive induction for problem solving by transforming information about a 18 This is the number of conditions in a ruleset. 53 search problem into a set of numeric attributes (terms) that describe search states. <p> Given a domain theory and the ability to solve problems in the domain, Zenith <ref> [Fawcett and Utgoff, 1992] </ref>, a problem solving system, can generate useful new attributes (features) for learning evaluation functions. Domain theories in Zenith are expressed in first order predicate calculus. A new attribute consists of two components: a formula and a variable list. <p> Two variants of Michalski's attribute counting operators are used to construct new terms (attributes) for learning evaluation functions over search states in problem solving systems. They are Cindi [Callan and Utgoff, 1991] and Zenith <ref> [Fawcett and Utgoff, 1992] </ref>. The UQ transformation of Cindi creates a numeric attribute from a boolean expression beginning with a universal quantifier.
Reference: [Fayyad and Irani, 1993] <author> U.M. Fayyad and K.B. Irani, </author> <title> Multi-interval discretization of continuous-valued attributes for classification learning. </title> <booktitle> Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kauf-mann, </publisher> <pages> 1022-1027. </pages>
Reference-contexts: Therefore, it should be able to alleviate the fragmentation problem of nominal Xof-N attributes as well. At the moment, XofN uses cut points found by C4.5 to discretize primitive continuous-valued attributes. Other discretization methods that can be used include multi-interval discretization methods <ref> [Catlett, 1991a; Fayyad and Irani, 1993] </ref>, supervised/unsupervised methods [Van de Merckt, 1993], and an entropy method [Ragavan and Rendell, 1993]. Both Catlett [1991a] and Fayyad and Irani [1993] recursively apply a binary splitting procedure 16 to a continuous-valued attribute, but they use different stopping criteria.
Reference: [Feigenbaum, 1961] <editor> E.A. Feigenbaum, </editor> <booktitle> The simulation of verbal learning behavior. Proceeding of the Western Joint Computer Conference, </booktitle> <pages> 121-132. </pages>
Reference-contexts: given with known labels such as the diagnoses of an illness for patients, the inductive learning is called supervised learning [Quinlan, 1986b; Michalski, 1983; Mitchell, 1982; Kibler and Aha, 1987; Rumelhart, Hinton, and Williams, 1986] in contrast to unsupervised learning where training examples are unlabeled or their memberships are unknown <ref> [Fisher, 1987; Feigenbaum, 1961; Lenat, 1977; Langley, Simon, and Bradshaw, 1987b] </ref>.
Reference: [Feigenbaum and McCorduck, 1983] <editor> E.A. Feigenbaum and P. McCorduck, </editor> <booktitle> The Fifth Generation: Artificial Intelligence and Japan's Computer Challenge to the World, </booktitle> <address> Wok-ingham, UK: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Section 1.4 describes the approach adopted. Finally, Section 1.5 presents the structure of the thesis. 1.1 Motivation Inductive learning is important for artificial intelligence. One reason is that the problem of knowledge acquisition is the critical bottleneck in artificial intelligence <ref> [Feigenbaum and McCorduck, 1983] </ref>, and inductive learning can widen this bottleneck by automating the process of knowledge acquisition when constructing knowledge-based systems. <p> Conventional knowledge acquisition is a painstaking process a series of intense, systematic interviews between knowledge engineers and domain experts to explicate the domain experts' heuristics <ref> [Feigenbaum and McCorduck, 1983; Waterman, 1986] </ref>. It is very common for domain experts to have difficulty fully expressing their knowledge or methods at an abstract level, but it is usually much easier for them to give examples of solving specific problems in their domains.
Reference: [Fisher, 1987] <author> D.H. Fisher, </author> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 139-172. </pages>
Reference-contexts: given with known labels such as the diagnoses of an illness for patients, the inductive learning is called supervised learning [Quinlan, 1986b; Michalski, 1983; Mitchell, 1982; Kibler and Aha, 1987; Rumelhart, Hinton, and Williams, 1986] in contrast to unsupervised learning where training examples are unlabeled or their memberships are unknown <ref> [Fisher, 1987; Feigenbaum, 1961; Lenat, 1977; Langley, Simon, and Bradshaw, 1987b] </ref>.
Reference: [Furnkranz and Widmer, 1994] <author> J. Furnkranz and G. </author> <title> Widmer, Incremental reduced error pruning. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 70-77. </pages>
Reference: [Haussler, 1988] <author> D. Haussler, </author> <title> Quantifying inductive bias: AI learning algorithms and valiant's learning framework. </title> <journal> Artificial Intelligence, </journal> <volume> 36, </volume> <pages> 177-221. 223 </pages>
Reference-contexts: For each experiment, a training set and a test set are independently drawn from a uniform distribution. The size of the test sets is 2000. The sizes of training sets, given in Table 3.1, are the result of a VC dimension analysis <ref> [Vapnik and Chervonenkis, 1971; Blumer et al., 1987; Haussler, 1988] </ref> for finding the number of examples which would suffice for an ideal learning algorithm to create a consistent hypothesis with an error rate less than 10% on any test set (see Pagallo [1990] for details).
Reference: [Hinton, 1989] <author> G.E. Hinton, </author> <title> Connectionist learning procedures. </title> <journal> Artificial Intelligence, </journal> <volume> 40, </volume> <pages> 185-234. </pages> <note> Reprinted in B.G. </note> <editor> Buchanan and D.C. Wilkins (eds.), </editor> <booktitle> Readings in Knowledge Acquisition and Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 362-386, </pages> <year> 1993. </year>
Reference-contexts: Decision trees [Hunt, Marin, and Stone, 1966; Quinlan, 1983; Breiman et al., 1984; Quinlan, 1993a] and production rules [Michalski, 1978; Quinlan, 1987a] are two commonly used theory description languages in supervised learning. Other theory description languages include neural networks <ref> [Rumelhart et al., 1986; Hinton, 1989] </ref>, linear discriminant functions [James, 1985], and instance-based methods [Aha, 1990; Aha, Kibler, and Albert, 1991]. An important advantage of decision trees and production rules, as theory description languages, is that they are relatively easy for humans to understand.
Reference: [Holte, 1993] <author> R.C. Holte, </author> <title> Very simple classification rules perform well on most datasets. </title> <journal> Machine Learning, </journal> <volume> 11, </volume> <pages> 63-90. </pages>
Reference: [Hunt et al., 1966] <author> E.B. Hunt, J. Marin, and P.J. Stone, </author> <title> Experiments in Induction, </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference-contexts: Decision tree learning systems The Tdidt (Top-Down Induction of Decision Trees) family [Quinlan, 1986b] contains a few variations on this basic decision tree learning theme. The patriarch of this family is CLS (Concept Learning System framework) <ref> [Hunt et al., 1966] </ref>, but ID3 [Quinlan, 1983; 1986a; 1986b] and its descendant C4.5 [Quinlan, 1993a] are the best-known. They use information-based heuristic functions for test selection.
Reference: [Indurkhya and Weiss, 1991] <author> N. Indurkhya and S.M. Weiss, </author> <title> Iterative rule induction methods. </title> <journal> Journal of Applied Intelligence, </journal> <volume> 1, </volume> <pages> 43-54. </pages>
Reference-contexts: Bacon [Langley, Simon, Bradshaw, and Zytkow, 1987], Induce [Michalski, 1978]) or attribute counting attributes 14 (e.g. Induce, AQ17-dci, and AQ17-mci [Bloedorn et al., 1993]). In addition, systems such as Lmdt [Brodley and Utgoff, 1992], Swap1 <ref> [Indurkhya and Weiss, 1991] </ref>, and Ccaf [Yip and Webb, 1994] construct linear machines, linear discriminant functions, or canonical discriminant functions as new attributes. Note that linear machines, as tests, have multiple values, one for each class. <p> A part of search space consisting of some candidate rules can be pruned. This makes systematic search efficient in practice [Webb, 1993]. Swap1 <ref> [Indurkhya and Weiss, 1991] </ref> is also a covering rule learning algorithm. One important characteristic of it is the use of Swap search for constructing a single rule. <p> During rule generation, Swap1 considers swapping each condition in the current rule for a condition not in the rule including the removal of each condition in the rule. After a set of rules is created, weakest-link pruning is used to find the right complexity ruleset <ref> [Indurkhya and Weiss, 1991] </ref>. This idea is very similar to that used by Cart for pruning decision trees [Breiman et al., 1984]. Generating rules from decision trees Unlike rule learning algorithms discussed above, some algorithms convert decision trees into production rules. <p> Canonical discriminant functions are generated first, and are transformed into new continuous-valued attributes. Then, the tree learning algorithm C4.5 or the rule learning algorithm Einstein [Webb, 1992] is invoked to learn a decision tree or a set of production rules. Similarly to Caf and Ccaf, Swap1 <ref> [Indurkhya and Weiss, 1991] </ref> can also train linear machines for rule learning. Before generating rules, a linear machine is created, and is transformed into new binary attributes, one for each class. <p> In some situations, such as with a large number of irrelevant attributes and/or noise, the search is very easily trapped in a local optimum so that good new attributes cannot be found. Some other 16 Split one interval into two intervals. 165 search approaches such as Swap <ref> [Indurkhya and Weiss, 1991] </ref> and Random Mutation hill climbing [Skalak, 1994] might be helpful for jumping off the local optimum to find a better solution. XofN with beam search has been tried.
Reference: [James, 1985] <author> M. James, </author> <title> Classification Algorithms, </title> <address> London, UK: </address> <publisher> Collins Professional and Technical Books. </publisher>
Reference-contexts: Decision trees [Hunt, Marin, and Stone, 1966; Quinlan, 1983; Breiman et al., 1984; Quinlan, 1993a] and production rules [Michalski, 1978; Quinlan, 1987a] are two commonly used theory description languages in supervised learning. Other theory description languages include neural networks [Rumelhart et al., 1986; Hinton, 1989], linear discriminant functions <ref> [James, 1985] </ref>, and instance-based methods [Aha, 1990; Aha, Kibler, and Albert, 1991]. An important advantage of decision trees and production rules, as theory description languages, is that they are relatively easy for humans to understand.
Reference: [John et al., 1994] <author> G.H. John, R. Kohavi, and K. Pfleger, </author> <title> Irrelevant features and the subset selection problem. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 121-129. </pages>
Reference: [Kibler and Aha, 1987] <author> D. Kibler and D.W. Aha, </author> <title> Learning representative examples of concepts: an initial case study. </title> <booktitle> Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <address> Irvine, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 24-30. </pages>
Reference: [Kira and Rendell, 1992] <author> K. Kira and L.A. Rendell, </author> <title> The feature selection problem: traditional methods and a new algorithm. </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press/Cambridge, </publisher> <address> MA: </address> <publisher> MIT Press, </publisher> <pages> 129-134. </pages>
Reference: [Kohavi, 1994] <author> R. Kohavi, </author> <title> Bottom-up induction of oblivious, read-once decision graphs. </title> <booktitle> Proceedings of the European Conference on Machine Learning, </booktitle> <address> Berlin: </address> <publisher> Springer-Verlag, </publisher> <pages> 154-169. </pages>
Reference-contexts: All nodes at the same level of an OODG use the same test, so this type of decision graphs is oblivious. The term "read-Once", here, means that each attribute occurs at most once in each path of an OODG. The Hoodg algorithm <ref> [Kohavi, 1994] </ref> constructs OODGs based on a set of ordered attributes using a bottom-up method. The wrapper approach [John, Kohavi, and Pfleger, 1994] and/or an entropy criterion are used to select an attribute subset from task-supplied attributes and an ordering of selected attributes.
Reference: [Kohavi, 1995] <author> R. Kohavi, </author> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection. </title> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 1137-1143. </pages>
Reference-contexts: Recent experimental and theoretical results have shown that 10-fold cross-validation is a good method for accuracy estimation, compared with other methods such as leave-one-out and bootstrap <ref> [Efron, 1983; Bailey and Elkan, 1993; Kohavi, 1995] </ref>. In all the experiments of this thesis, all the algorithms being compared are run with their default option settings, except when otherwise indicated. Pruned trees are always used for all the tree learning algorithms when reporting results.
Reference: [Kohavi and Li, 1995] <author> R. Kohavi and C. Li, </author> <title> Oblivious decision trees, graphs, and top-down pruning. </title> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 1071-1077. 224 </pages>
Reference-contexts: Therefore, decision graphs can be considered as a generalization of decision trees. Since branches of decision graphs can join together, they provide a way to alleviate the replication problem [Pagallo and Haussler, 1990] and the fragmentation problem <ref> [Kohavi and Li, 1995] </ref> of decision trees [Oliver, Dowe, and Wallace, 1992; Oliveira and Sangiovanni-Vincentelli, 1995; Kohavi and Li, 1995]. Oliver et al. [1992] introduce a top-down scheme to build general decision graphs. The algorithm starts from a leaf. <p> Hoodg is shown to be effective for nominal attributes, but it cannot deal with continuous-valued attributes and irrelevant attributes. To improve Hoodg, the Eodg algorithm is proposed <ref> [Kohavi and Li, 1995] </ref>. It first builds an oblivious decision tree in which tests at all nodes of the same level are the same. Information gain is used as the splitting criterion. The oblivious decision tree is then pruned up from the bottom. <p> Finally, nodes at the same levels of the oblivious decision tree are merged to convert the tree to an oblivious decision graph. It is reported that Eodg has approximately the same accuracies as C4.5 in some artificial and real-world domains tested <ref> [Kohavi and Li, 1995] </ref>.
Reference: [Lachenbruch and Mickey, 1968] <author> P.A. Lachenbruch and M.R. Mickey, </author> <title> Estimation of error rates in discriminant analysis. </title> <journal> Technometrics, </journal> <volume> 10, </volume> <pages> 1-11. </pages>
Reference-contexts: The algorithm is run v times. Each example in the dataset is used as a test example (unseen case) exactly once in the v trials. One extreme form of v-fold cross-validation is leave-one-out <ref> [Lachenbruch and Mickey, 1968] </ref> or N-fold cross-validation with v equal to the number of examples in the dataset.
Reference: [Langley et al., 1987a] <author> P. Langley, H.A. Simon, G.L. Bradshaw, and J.M. Zytkow, </author> <title> Scientific Discovery: Computational Explorations of the Creative Processes, </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: For example, Chapter 6 investigates approaches to constructing nominal and continuous-valued attributes. Bacon and Induce also construct new continuous-valued attributes. On the other hand, Induce, AQ17-dci, and AQ17-mci construct attribute counting attributes that have ordered discrete values. The scientific discovery system Bacon <ref> [Langley et al., 1987a; 1987b] </ref> discovers numeric laws. It constructs new attributes (called terms) such as X fi Y , X=Y , X N , log (X), and sin (X) by using mathematical operators, where X and Y are primitive continuous-valued attributes (variables). <p> In addition, to deal with more complex relations between X and Y , Bacon considers transformations of both the independent and the dependent terms, such as inverse (Y ), sin (Y ), and log (Y ), using a simple-minded generate and test strategy 50 <ref> [Langley et al., 1987a] </ref>. To deal with noise, it requires a near constant differential instead of a strict constant to be found. The rule learning algorithm Induce [Larson and Michalski, 1977; Michalski, 1980a; 1983] also uses mathematical operators as constructive operators. <p> They use conjunction, disjunction, negation, or Mof-N as constructive operators. Some systems generate new continuous-valued attributes using different constructive operators. For example, Bacon <ref> [Langley et al., 1987a] </ref> and Induce [Michalski, 1978] use mathematical operators such as multiplication and division. Ccaf [Yip and Webb, 1994] adopts canonical discriminant functions. Systems such as Induce, AQ17-dci, and AQ17-mci [Bloedorn et al., 1993] construct attribute counting attributes. <p> A feature is evaluated in a state by counting the distinct values of its variable list that satisfies the formula. Some systems construct new continuous-valued attributes by using mathematical operators such as multiplication and division. The science discovery system Bacon <ref> [Langley et al., 1987a] </ref> and rule induction system Induce [Michalski, 1978] are two examples.
Reference: [Langley et al., 1987b] <author> P. Langley, H.A. Simon, and G.L. Bradshaw, </author> <title> Heuristics for empirical discovery. </title> <editor> In L. Bolc (ed.), </editor> <booktitle> Computational Models of Learning, </booktitle> <address> Berlin: </address> <publisher> Springer-Verlag, </publisher> <pages> 21-54. </pages>
Reference: [Langley, 1994] <author> P. Langley, </author> <title> Selection of relevant features in machine learning. </title> <booktitle> Proceeding of the AAAI Fall Symposium on Relevance, </booktitle> <address> New Orleans, LA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Even worse, some of them might be irrelevant to the target theory. Note that irrelevant attributes usually result in worse performance of inductive learning <ref> [Caruana and Freitag, 1994; Langley, 1994] </ref>.
Reference: [Langley and Sage, 1994] <author> P. Langley and S. Sage, </author> <title> Oblivious decision trees and abstract cases. </title> <booktitle> Working Notes of the AAAI-94 Workshop on Case-Based Reasoning, </booktitle> <address> Seattle, WA: </address> <publisher> AAAI Press, </publisher> <pages> 113-117. </pages>
Reference: [Langley and Simon, 1995] <author> P. Langley and H.A. Simon, </author> <title> Applications of machine learning and rule induction. </title> <journal> Communications of the ACM, </journal> <volume> 38, </volume> <pages> 55-64. </pages>
Reference: [Larson and Michalski, 1977] <author> J. Larson and R.S. Michalski, </author> <title> Inductive inference of VL decision rules. Proceedings of the Workshop on Pattern Directed Inference Systems, </title> <journal> SIGART Newsletter, </journal> <volume> 63, </volume> <pages> 38-44. </pages>
Reference-contexts: To deal with noise, it requires a near constant differential instead of a strict constant to be found. The rule learning algorithm Induce <ref> [Larson and Michalski, 1977; Michalski, 1980a; 1983] </ref> also uses mathematical operators as constructive operators. It constructs new attributes (descriptors) by applying constructive generalization rules. The detecting descriptor interdependence rule can create new descriptors X=Y and X fi Y using an idea similar to that of Bacon.
Reference: [Lathrop et al., 1987] <author> R. Lathrop, T. Webster, and T. Smith, ADRIADNE: </author> <title> pattern-directed inference and hierarchical abstraction in protein structure recognition. </title> <journal> Communications of the ACM, </journal> <volume> 30, </volume> <pages> 909-921. </pages>
Reference: [Lenat, 1977] <author> D.B. Lenat, </author> <title> The ubiquity of discovery. </title> <journal> Artificial Intelligence, </journal> <volume> 9, </volume> <pages> 257-285. </pages>
Reference-contexts: given with known labels such as the diagnoses of an illness for patients, the inductive learning is called supervised learning [Quinlan, 1986b; Michalski, 1983; Mitchell, 1982; Kibler and Aha, 1987; Rumelhart, Hinton, and Williams, 1986] in contrast to unsupervised learning where training examples are unlabeled or their memberships are unknown <ref> [Fisher, 1987; Feigenbaum, 1961; Lenat, 1977; Langley, Simon, and Bradshaw, 1987b] </ref>.
Reference: [Liu and White, 1994] <author> W.Z. Liu and A.P. White, </author> <title> The importance of attribute selection measures in decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 15, </volume> <pages> 25-41. </pages>
Reference-contexts: Unfortunately, the real world is not perfect. Most real-world datasets 1 Note that the conclusion concerning the random test selection criterion drawn by Mingers [1989a] is not correct as an unsound experimental methodology was used <ref> [Buntine and Niblett, 1992; Liu and White, 1994] </ref>. 2 "Reasonable size", here, means that the training set should contain examples representing most cases in the whole instance space defined by all attributes.
Reference: [Mangasarian and Wolberg, 1990] <author> O.L. Mangasarian and W.H. Wolberg, </author> <title> Cancer diagnosis via linear programming. </title> <journal> SIAM News, </journal> <volume> 23, </volume> <pages> pp. 1 & 18. </pages>
Reference-contexts: The dataset was collected by William H. Wol-berg, University of Wisconsin Hospitals, Madison, and donated by Olvi Mangasarian 61 <ref> [Mangasarian and Wolberg, 1990] </ref>. * Promoters: The objective is to classify a given DNA sequence as a promoter sequence or a non-promoter sequence. The dataset was donated by Jude Shavlik [Towell, Shavlik, and Noordewier, 1990]. * Nettalk: This domain concerns mapping English text into speech.
Reference: [Matheus, 1989] <author> C.J. Matheus, </author> <title> Feature Construction: An Analytic Framework and an Application to Decision Trees, </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL. </institution> <month> 225 </month>
Reference-contexts: trees does not seem to increase accuracies, although the generated multivariate rules are more accurate than the initial univariate trees in most domains, and are more accurate than the univariate rules in some domains. 4.6 Related Work As mentioned before, the closest related work to the CI algorithms is Citre <ref> [Matheus and Rendell, 1989; Matheus, 1989] </ref> and the Fringe family of algorithms including Fringe, Dual Fringe, Symmetric Fringe [Pagallo and Haussler, 1989; 1990; Pagallo, 1990], SymFringe, DCFringe [Yang et al., 1991], and SFringe (see Appendix B). <p> Related Work As far as constructing new attributes for decision tree learning is concerned, related work includes the Fringe family of algorithms such as Fringe, Dual Fringe, Symmetric Fringe [Pagallo and Haussler, 1989; 1990; Pagallo, 1990], SymFringe, DCFringe [Yang et al., 1991], and SFringe (see Appendix B), the Citre algorithm <ref> [Matheus and Rendell, 1989; Matheus, 1989] </ref>, the CI algorithms [Zheng, 1992] (also see Chapter 4), the 124 LFC algorithm [Ragavan and Rendell, 1993], the Cart algorithm [Breiman et al., 1984], the ID2-of-3 algorithm [Murphy and Pazzani, 1991], and the Lmdt algorithm [Brodley and Utgoff, 1992].
Reference: [Matheus and Rendell, 1989] <author> C.J. Matheus and L.A. Rendell, </author> <title> Constructive induction on decision trees. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 645-650. </pages>
Reference-contexts: It is usually referred to as the selective induction component of constructive induction. Constructing new attributes can be thought of as the application of a set of constructive operators to a set of conditions formed by existing attributes, resulting in the construction of one or more new attributes <ref> [Matheus and Rendell, 1989] </ref>. Existing attributes can be either primitive attributes or previously constructed new attributes. <p> Systems with the hypothesis-driven strategy analyze hypotheses learned by selective components to discover patterns. Members of this group are algorithms such as Citre 12 Thanks to Thierry Van de Merckt for the suggestion of presenting the problem in this way. 13 <ref> [Matheus and Rendell, 1989] </ref>, Fringe [Pagallo and Haussler, 1989] and AQ17-hci [Wnek and Michalski, 1994]. <p> Different types of attribute are appropriate for describing different characteristics of examples. Most selective induction algorithms can accept attributes of these three kinds. However, many existing constructive induction algorithms such as Fringe [Pagallo, 1990] and Citre <ref> [Matheus and Rendell, 1989] </ref> only construct new binary attributes by using logical operators such as conjunction, negation, and disjunction. <p> In addition, some systems create Mof-N representations, linear discriminant functions, or linear machines as new attributes. In terms of new attribute construction strategies, almost all these algorithms use either the hypothesis-driven strategy or the data-driven strategy. Very few systems use domain knowledge, although Citre <ref> [Matheus and Rendell, 1989] </ref> is such an example. However, it uses domain knowledge to reduce the search space for new attributes rather than to generate new attributes. The following two subsubsections discuss hypothesis-driven and data-driven approaches to constructing new attributes respectively. <p> They interleave two processes: building decision trees and constructing new attributes. The length of conjunctions or disjunctions constructed from paths in each execution cycle is usually two. More complex new attributes are generated through iteration. Citre <ref> [Matheus and Rendell, 1989] </ref> generates a conjunction for each pair of conditions in a positive path as a new attribute. It can use domain knowledge to filter out less promising candidate new attributes. In addition, new attributes can be generalized by using a generalization operator "changing constants to variables". <p> trees does not seem to increase accuracies, although the generated multivariate rules are more accurate than the initial univariate trees in most domains, and are more accurate than the univariate rules in some domains. 4.6 Related Work As mentioned before, the closest related work to the CI algorithms is Citre <ref> [Matheus and Rendell, 1989; Matheus, 1989] </ref> and the Fringe family of algorithms including Fringe, Dual Fringe, Symmetric Fringe [Pagallo and Haussler, 1989; 1990; Pagallo, 1990], SymFringe, DCFringe [Yang et al., 1991], and SFringe (see Appendix B). <p> Since the number of new attributes constructed in each iteration is often quite large and not all of them are useful, new attribute selection is an essential step. Citre <ref> [Matheus and Rendell, 1989] </ref> uses utility to select attributes. New attributes are ordered by utility, and those attributes with the lowest utilities are deleted so that the total number of primitive attributes and new attributes does not exceed a predecided number. <p> Related Work As far as constructing new attributes for decision tree learning is concerned, related work includes the Fringe family of algorithms such as Fringe, Dual Fringe, Symmetric Fringe [Pagallo and Haussler, 1989; 1990; Pagallo, 1990], SymFringe, DCFringe [Yang et al., 1991], and SFringe (see Appendix B), the Citre algorithm <ref> [Matheus and Rendell, 1989; Matheus, 1989] </ref>, the CI algorithms [Zheng, 1992] (also see Chapter 4), the 124 LFC algorithm [Ragavan and Rendell, 1993], the Cart algorithm [Breiman et al., 1984], the ID2-of-3 algorithm [Murphy and Pazzani, 1991], and the Lmdt algorithm [Brodley and Utgoff, 1992]. <p> Some systems construct new continuous-valued attributes by using mathematical operators such as multiplication and division. The science discovery system Bacon [Langley et al., 1987a] and rule induction system Induce [Michalski, 1978] are two examples. Most hypothesis-driven constructive induction algorithms such as Fringe [Pagallo, 1990], Citre <ref> [Matheus and Rendell, 1989] </ref>, CI [Zheng, 1992] (also see Chapter 4), CAT (see Chapter 5), and AQ17-hci [Wnek and Michalski, 1994] construct and select a set of new attributes based on the entire training set. <p> This strategy has a shortcoming: new attributes that have high values of the evaluation function for the entire training set might have lower values than other unselected new attributes for a training subset after a part of a decision tree or a ruleset has been created <ref> [Matheus and Rendell, 1989] </ref>. To overcome this, XofN, XofN (c), and XofN (cc) construct one new attribute using the local training set for each decision node.
Reference: [Michalski, 1972] <author> R.S. Michalski, </author> <title> A variable-valued logic system as applied to picture description and recognition. </title> <booktitle> Graphic Languages: IFIP Working Conference on Graphic Languages, </booktitle> <address> Amsterdam: </address> <publisher> North-Holland, </publisher> <pages> 20-47. </pages>
Reference-contexts: The rules learned by A q are in the form of VL1 (Variable-valued Logic 1), which is a multiple-valued logic propositional calculus with typed variables <ref> [Michalski, 1972] </ref>. The antecedent of a rule is called a cover. It is the disjunction of complexes. A complex in turn is the conjunction of selectors.
Reference: [Michalski, 1978] <author> R.S. Michalski, </author> <title> Pattern recognition as knowledge-guided computer induction. </title> <type> Technical Reports 927, </type> <institution> Department of Computer Science, The University of Illinois at Urbana-Champaign, Urbana, IL. </institution>
Reference-contexts: For example, a decision tree learning algorithm searches only the space of finite trees that carry out axis-orthogonal splits. Decision trees [Hunt, Marin, and Stone, 1966; Quinlan, 1983; Breiman et al., 1984; Quinlan, 1993a] and production rules <ref> [Michalski, 1978; Quinlan, 1987a] </ref> are two commonly used theory description languages in supervised learning. Other theory description languages include neural networks [Rumelhart et al., 1986; Hinton, 1989], linear discriminant functions [James, 1985], and instance-based methods [Aha, 1990; Aha, Kibler, and Albert, 1991]. <p> Generally, at least one disjunct is needed to represent each peak. 10 representing the target theories using given theory description languages. One method of overcoming this limitation is constructive induction <ref> [Michalski, 1978] </ref>. Constructive induction algorithms construct new attributes from task-supplied attributes, and then build theories based on the new attributes, sometimes together with the task-supplied attributes. By contrast to new attributes, task-supplied attributes are called primitive attributes. <p> Nevertheless, Mof-N representations still have binary values. Only a few systems explore methods of constructing new continuous-valued attributes using mathematical operators (e.g. Bacon [Langley, Simon, Bradshaw, and Zytkow, 1987], Induce <ref> [Michalski, 1978] </ref>) or attribute counting attributes 14 (e.g. Induce, AQ17-dci, and AQ17-mci [Bloedorn et al., 1993]). In addition, systems such as Lmdt [Brodley and Utgoff, 1992], Swap1 [Indurkhya and Weiss, 1991], and Ccaf [Yip and Webb, 1994] construct linear machines, linear discriminant functions, or canonical discriminant functions as new attributes. <p> They use conjunction, disjunction, negation, or Mof-N as constructive operators. Some systems generate new continuous-valued attributes using different constructive operators. For example, Bacon [Langley et al., 1987a] and Induce <ref> [Michalski, 1978] </ref> use mathematical operators such as multiplication and division. Ccaf [Yip and Webb, 1994] adopts canonical discriminant functions. Systems such as Induce, AQ17-dci, and AQ17-mci [Bloedorn et al., 1993] construct attribute counting attributes. <p> Instead of building decision trees, Crls [Spackman, 1988] and MoN [Ting, 1994] learn Mof-N rules. The symbolic theory revision system Neither [Baffes and Mooney, 1993] refines Mof-N rules. The rule learning algorithms Induce <ref> [Michalski, 1978] </ref>, AQ17-dci, and AQ17-mci [Bloedorn et al., 1993] use the counting operator 22 #VarEQ (x) to construct new attributes that count the number of attributes which take the value x. <p> A feature is evaluated in a state by counting the distinct values of its variable list that satisfies the formula. Some systems construct new continuous-valued attributes by using mathematical operators such as multiplication and division. The science discovery system Bacon [Langley et al., 1987a] and rule induction system Induce <ref> [Michalski, 1978] </ref> are two examples.
Reference: [Michalski, 1980] <author> R.S. Michalski, </author> <title> Pattern recognition as rule-guided inductive inference. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 2, </volume> <pages> 349-361. </pages>
Reference: [Michalski and Chilausky, 1980a] <author> R.S. Michalski and R.L. Chilausky, </author> <title> Learning by being told and learning from examples: an experimental comparison of the two methods of knowledge acquisition in the context of developing an expert system for soybean disease diagnosis. </title> <journal> International Journal of Policy Analysis and Information Systems, </journal> <volume> 4, </volume> <pages> 125-160. </pages>
Reference-contexts: This kind of induction is called selective induction. There are many learning systems of this type such as ID3 [Quinlan, 1983; 1986], Cart 8 [Breiman et al., 1984], C4.5 [Quinlan, 1993a], PLS1 [Rendell, 1983], CN2 [Clark and Niblett, 1989], and AQ11 <ref> [Michalski and Chilausky, 1980a] </ref>. For example, decision tree learning algorithms ID3, Cart, and C4.5 partition the instance space defined by task-supplied attributes into regions of locally invariant or the same class membership values by using axis-orthogonal splits, each of which is based on a single attribute.
Reference: [Michalski and Chilausky, 1980b] <author> R.S. Michalski and R.L. Chilausky, </author> <title> Knowledge acquisition by encoding expert rules versus computer induction from examples: a case study involving soybean pathology. </title> <journal> International Journal for Man-Machine Studies, </journal> <volume> 12, </volume> <pages> 63-87. </pages>
Reference-contexts: However, the number of nodes in the OODG is smaller than that in the corresponding decision tree in most of these domains. 2.2.2 Rule learning Most rule learning algorithms such as the A q algorithm <ref> [Michalski and Chilausky, 1980b] </ref> and the CN2 algorithm [Clark and Niblett, 1989] generate a set of rules using a covering approach, also termed separate and conquer by analogy with the divide and conquer approach discussed above. <p> The well-known covering algorithm A q is the basis of the AQ family of systems such as AQ11 <ref> [Michalski and Chilausky, 1980b] </ref>, AQ15 [Michalski et al., 1986], and AQR [Clark and Niblett, 1989]. The rules learned by A q are in the form of VL1 (Variable-valued Logic 1), which is a multiple-valued logic propositional calculus with typed variables [Michalski, 1972]. <p> When a selector contains more than one attribute value, it is called a internal disjunction <ref> [Michalski and Chilausky, 1980b] </ref>. Like the general covering method, the A q algorithm learns a separate cover for each class. All the covers form an unordered set. During the construction of the covers, each class is chosen in turn.
Reference: [Michalski, 1983] <author> R.S. Michalski, </author> <title> A theory and methodology of inductive learning. In R.S. </title> <editor> Michalski, J.G. Carbonell, and T.M. Mitchell (eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach (Vol. I), </booktitle> <address> Palo Alto, CA: </address> <publisher> Tioga Press, </publisher> <pages> 83-134. </pages>
Reference-contexts: In this case, it is natural to consider the examples as positive and negative examples of a concept, and the task of supervised learning can be viewed as that of generating a definition of the concept. Therefore, supervised learning is sometimes called concept learning <ref> [Quinlan, 1983; Michalski, 1983] </ref>. The research presented in this thesis is described in the context of supervised learning for classification problems. Moreover, we only discuss zeroth-order learning where examples are represented using a vector of variables.
Reference: [Michalski et al., 1986] <author> R.S. Michalski, I. Mozetic, J. Hong, and N. Lavrac, </author> <title> The multipurpose incremental learning system AQ15 and its testing application to three medical domains. </title> <booktitle> Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 1041-1045. </pages>
Reference-contexts: The well-known covering algorithm A q is the basis of the AQ family of systems such as AQ11 [Michalski and Chilausky, 1980b], AQ15 <ref> [Michalski et al., 1986] </ref>, and AQR [Clark and Niblett, 1989]. The rules learned by A q are in the form of VL1 (Variable-valued Logic 1), which is a multiple-valued logic propositional calculus with typed variables [Michalski, 1972]. The antecedent of a rule is called a cover.
Reference: [Michie et al., 1994] <editor> D. Michie, D.J. Spiegelhalter, and C.C. Taylor (eds.), </editor> <title> Machine Learning, Neural and Statistical Classification (STATLOG Project), </title> <publisher> Hertfordshire: El-lis Horwood. </publisher> <pages> 226 </pages>
Reference: [Mingers, 1989a] <author> J. Mingers, </author> <title> An empirical comparison of selection measures for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 319-342. </pages>
Reference-contexts: the sorted list to find the best cut-point in terms of the test evaluation function. 8 A cut-point splits the training examples into two sets: those with the value of A less than or equal to the cut-point and those with the value of A greater than the cut-point. 7 <ref> [Mingers, 1989a] </ref>, however, indicates that gain ratio leads to smaller, rather than more accurate, trees. 8 Another commonly used method of finding the best threshold is examining each mid-point of two adjacent values in the sorted list.
Reference: [Mingers, 1989b] <author> J. Mingers, </author> <title> An empirical comparison of pruning methods for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 4, </volume> <pages> 227-243. </pages>
Reference-contexts: In this case, a tree perfectly consistent with a training set often has a low accuracy on unseen data. This is called overfitting (or overspecialization) to the training data <ref> [Breiman et al., 1984; Quinlan, 1986b; 1987b; Mingers, 1989b] </ref>. The reason is that some spurious tests selected by algorithms may discriminate few training examples by chance, 3 while these tests have no predictive value when applied to fresh data.
Reference: [Mitchell, 1980] <author> T.M. Mitchell, </author> <title> The need for biases in learning generalizations. </title> <type> Technical Report CBM-TR-117, </type> <institution> Department of Computer Science, Rutgers University, </institution> <address> New Brunswick, NJ. </address> <note> Reprinted in J.W. </note> <editor> Shavlik and T.G. Dietterich (eds.), </editor> <booktitle> Readings in Machine Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 184-191, </pages> <year> 1990. </year>
Reference-contexts: Its theory complexity is the size of the description of the theory. To induce a theory from a given training set, a learning system needs to make assumptions about the theory to be learned. The assumptions, in machine learning, are usually called biases <ref> [Mitchell, 1980] </ref>. 2 A learning system without any assumptions is un 1 Every example is represented using the same set of attributes. 2 Note that they are different from statistical biases. 4 able to generate a useful theory because the number of theories that are consistent 3 with the training set
Reference: [Mitchell, 1982] <author> T.M. Mitchell, </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18, </volume> <pages> 203-226. </pages>
Reference-contexts: It can be used to predict the classes of examples, either training examples or previously unseen cases. The theory is described using a language, called a theory description language. Such a problem of supervised learning can be viewed as a search problem <ref> [Mitchell, 1982] </ref> involving a large hypothesis space that is the space consisting of all possible theories 3 (hypotheses) under consideration. The search aims to find the best theory with respect to the training examples as well as some prior knowledge and expectations.
Reference: [Mooney et al., 1989] <author> R. Mooney, J. Shavlik, G. Towell, and A. Gove, </author> <title> An experimental comparison of symbolic and connectionist learning algorithms. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> San Mateo, CA: </address> <publisher> Mor-gan Kaufmann, </publisher> <pages> 775-780. </pages>
Reference: [Moore and Lee, 1994] <author> A.W. Moore and M.S. Lee, </author> <title> Efficient algorithms for minimizing cross validation error. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 190-198. </pages>
Reference: [Muggleton and Feng, 1990] <author> S. Muggleton and C. Feng, </author> <title> Efficient induction of logical programs. </title> <booktitle> Proceedings of the First Conference on Algorithmic Learning Theory, </booktitle> <address> Tokyo: </address> <publisher> Ohmsha, </publisher> <pages> 368-381. </pages>
Reference: [Murphy and Aha, 1994] <author> P.M. Murphy and D.W. Aha, </author> <title> UCI Repository of machine learning databases [Machine-readable data repository]. </title> <institution> Department of Information and Computer Science, University of California, </institution> <address> Irvine, CA, </address> <note> Available by anonymous ftp at ics.uci.edu in the pub/machine-learning-databases directory. </note>
Reference-contexts: The ten real-world domains are from the UCI repository of machine learning databases <ref> [Murphy and Aha, 1994] </ref>. In this thesis, these three groups of domains are referred to as the artificial domains, the Monks domains, and the real-world domains respectively. The following three subsections describe them in detail. 3.2.1 Artificial domains Table 3.1 summarizes the characteristics of the artificial domains [Pagallo, 1990].
Reference: [Murphy and Pazzani, 1991] <author> P.M. Murphy and M.J. Pazzani, ID2-of-3: </author> <title> constructive induction of M-of-N concepts for discriminators in decision trees. </title> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 183-187. 227 </pages>
Reference-contexts: Systems with the data-driven strategy, such as ID2-of-3 <ref> [Murphy and Pazzani, 1991] </ref> and AQ17-dci [Bloedorn and Michalski, 1991], find relevant patterns from input data directly, while systems with the knowledge-driven strategy apply domain-knowledge to create new attributes. This strategy needs information additional to what can be obtained from the training data. <p> The two research topics mentioned above explore two novel methods of constructing new attributes for decision tree learning, but they also generate binary attributes by using the existing constructive operators conjunction and negation (implicitly). On the other hand, ID2-of-3 <ref> [Murphy and Pazzani, 1991] </ref> creates Mof-N attributes. The Mof-N representation is more representationally powerful than conjunction and disjunction because the latter are two special cases of the former. Nevertheless, Mof-N representations still have binary values. Only a few systems explore methods of constructing new continuous-valued attributes using mathematical operators (e.g. <p> Before generating rules, a linear machine is created, and is transformed into new binary attributes, one for each class. Each new attribute has the value 1 or 0 depending on whether or not an example is classified to the corresponding class by the linear machine. ID2-of-3 <ref> [Murphy and Pazzani, 1991] </ref> differs from other data-driven constructive decision tree learning algorithms in the way that it constructs Mof-N representations as new attributes. Murphy and Pazzani [1991] show the success of learning Mof-N trees compared with building univariate trees in some domains. <p> Murphy and Pazzani [1991] show the success of learning Mof-N trees compared with building univariate trees in some domains. Since it is used as a comparison algorithm in this thesis, we describe it at length here. As shown in Figure 2.5 <ref> [Murphy and Pazzani, 1991] </ref>, ID2-of-3 uses the same tree growing method as C4.5, except that C4.5 selects the best primitive attribute to form a test for each decision node, while ID2-of-3 constructs one Mof-N attribute to form a binary test and uses the entropy function instead of information gain ratio as <p> training into two subsets D covered and D uncovered RETURN the tree formed by a decision node with the test Mof-N and two subtrees ID2-of-3-Tree (D covered ), ID2-of-3-Tree (D uncovered ) g At each decision node, ID2-of-3 uses its embedded greedy algorithm Generate-Mof-N which is outlined in Figure 2.6 <ref> [Murphy and Pazzani, 1991] </ref> to generate an Mof-N attribute to form a test. Function "Initial-Attribute ()" finds an attribute-value pair that best splits the training examples at the current decision node according to the entropy function ("Eval-Attribute ()"). Generate-Mof-N starts from the attribute-value pair as a 1-of-1 attribute. <p> The test set at every point of a trial is the same as the test set used in the corresponding trial of the 10-fold cross-validation. 3.4 Comparison Algorithms C4.5 [Quinlan, 1993a], 13 an algorithm from the Fringe family, and ID2-of-3 <ref> [Murphy and Pazzani, 1991] </ref> are used as the reference algorithms in the empirical studies of this thesis. This section briefly explains why they are chosen. The descriptions of C4.5 and ID2-of-3 have been given in Chapter 2. <p> Given its reliability and ability to handle general learning tasks (with multiple classes and/or nominal attributes), SFringe, instead of Fringe, Symmetric Fringe, SymFringe, and DCFringe, is used as a comparison algorithm for the empirical studies in this thesis. ID2-of-3 <ref> [Murphy and Pazzani, 1991] </ref> is the work most closely related to the algorithms described in Chapter 6. All these algorithms construct one new attribute for each decision node by using the data-driven constructive strategy when building a decision tree. <p> For example, Cart [Breiman et al., 1984] can create one Boolean combination of primitive attributes as a new attribute at each decision node during the process of building decision trees. In addition, Cart can also construct linear discriminants as new binary attributes. ID2-of-3 <ref> [Murphy and Pazzani, 1991] </ref> constructs a new binary attribute directly from training data for each decision node when building decision trees, but it uses at-least Mof-N representations. LFC [Ragavan and Rendell, 1993] uses negation and conjunction as constructive operators. <p> 1990; Pagallo, 1990], SymFringe, DCFringe [Yang et al., 1991], and SFringe (see Appendix B), the Citre algorithm [Matheus and Rendell, 1989; Matheus, 1989], the CI algorithms [Zheng, 1992] (also see Chapter 4), the 124 LFC algorithm [Ragavan and Rendell, 1993], the Cart algorithm [Breiman et al., 1984], the ID2-of-3 algorithm <ref> [Murphy and Pazzani, 1991] </ref>, and the Lmdt algorithm [Brodley and Utgoff, 1992]. Like CAT, the Fringe family of algorithms and the CI algorithms use the hypothesis-driven strategy to construct conjunctions and/or disjunctions for decision trees. <p> continuous-valued attributes are investigated as well. 127 128 6.2 Introduction Up to now, most constructive induction algorithms that have been discussed focus on constructing new binary attributes, such as the CI (see Chapter 4), CAT (see Chapter 5), Fringe [Pagallo and Haussler, 1990], LFC [Ragavan and Rendell, 1993], and ID2-of-3 <ref> [Murphy and Pazzani, 1991] </ref> algorithms. They use conjunction, disjunction, negation, or Mof-N as constructive operators. Some systems generate new continuous-valued attributes using different constructive operators. For example, Bacon [Langley et al., 1987a] and Induce [Michalski, 1978] use mathematical operators such as multiplication and division. <p> The reason is that subtrees with larger local training sets are explored first. Therefore, good Xof-N attributes are more likely to be constructed earlier, thus being able to be reused at decision nodes with small local training sets. 6.6 Related Work The closest related work is ID2-of-3 <ref> [Murphy and Pazzani, 1991] </ref>. It constructs new binary attributes in the form of Mof-N representations, while XofN, XofN (c), and XofN (cc) construct Xof-N representations. When building a decision tree, all of them construct one new attribute for each decision node using the local training set.
Reference: [Murphy and Pazzani, 1994] <author> P.M. Murphy and M.J. Pazzani, </author> <title> Exploring the decision forest: an empirical investigation of Occam's Razor in decision tree induction. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1, </volume> <pages> 257-275. </pages>
Reference: [Murthy et al., 1994] <author> S.K. Murthy, S. Kasif, and S. Salzberg, </author> <title> A system for induction of oblique decision trees. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <pages> 1-32. </pages>
Reference-contexts: OC1 [Murthy, Kasif, and Salzberg, 1994] is an extension of Cart in the direction of constructing linear combinations for decision tree learning. Murthy et al. [1994] refer to this type of multivariate tree as oblique decision trees. A novel contribution of OC1 is a randomized coefficient perturbation algorithm <ref> [Murthy et al., 1994] </ref>. Similarly to Cart, heuristic hill climbing search is used to find (local) optimum coefficients for linear combinations. To escape local optimum when searching for a linear combination, after the hill climbing search halts, two randomization mechanisms are applied. <p> Otherwise, the second mechanism is used. That is, the hill climbing search starts again with another randomly selected start point. This technique allows multiple local searches. OC1 is shown to be more accurate than univariate tree learning <ref> [Murthy et al., 1994] </ref>. Brodley and Utgoff [1995] explore and review four methods, including the Cart method, of generating linear combination tests for decision tree learning. It is experimentally demonstrated that constructing linear combination tests generally improves the accuracies of the resulting decision trees over univariate trees.
Reference: [Oliveira and Sangiovanni-Vincentelli, 1995] <author> A. Oliveira and A. Sangiovanni-Vincentelli, </author> <title> Inferring reduced ordered decision graphs of minimum description length. </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 421-429. </pages>
Reference-contexts: A RODG learning algorithm, Smog, is described. The algorithm uses a decision tree as the start point, and transforms it into a RODG using the Minimum Description Length (MDL) principle [Rissanen, 1983]. It is shown that the Smog algorithm achieves higher prediction accuracies than C4.5 in many artificial domains <ref> [Oliveira and Sangiovanni-Vincentelli, 1995] </ref>. Kohavi [1994] and Kohavi and Li [1995] explore Oblivious read-Once Decision Graphs (OODG) that are similar to RODGs. All nodes at the same level of an OODG use the same test, so this type of decision graphs is oblivious.
Reference: [Oliver et al., 1992] <author> J.J. Oliver, D.L. Dowe, and C.S. Wallace, </author> <title> Inferring decision graphs using the minimum message length principle. </title> <booktitle> Proceedings of the Fifth Australian Joint Conference on Artificial Intelligence, </booktitle> <address> Singapore: </address> <publisher> World Scientific, </publisher> <pages> 361-367. </pages>
Reference: [Pagallo and Haussler, 1989] <author> G. Pagallo and D. Haussler, </author> <title> Two algorithms that learn DNF by discovering relevant features. </title> <booktitle> Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 119-123. </pages>
Reference-contexts: Systems with the hypothesis-driven strategy analyze hypotheses learned by selective components to discover patterns. Members of this group are algorithms such as Citre 12 Thanks to Thierry Van de Merckt for the suggestion of presenting the problem in this way. 13 [Matheus and Rendell, 1989], Fringe <ref> [Pagallo and Haussler, 1989] </ref> and AQ17-hci [Wnek and Michalski, 1994]. Systems with the data-driven strategy, such as ID2-of-3 [Murphy and Pazzani, 1991] and AQ17-dci [Bloedorn and Michalski, 1991], find relevant patterns from input data directly, while systems with the knowledge-driven strategy apply domain-knowledge to create new attributes. <p> Here, we address a few problems of existing constructive induction methods which use decision trees as the theory description language. There is a group of constructive decision tree learning algorithms such as Fringe <ref> [Pagallo and Haussler, 1989; 1990] </ref> that employ the hypothesis-driven strategy. These 14 algorithms typically use all or some of conjunction, disjunction, and negation as constructive operators, and use decision trees to restrict their new attribute search space. <p> Algorithms of the Fringe family such as Fringe, Dual Fringe, Symmetric Fringe, SymFringe, and DCFringe are similar to Citre in terms of constructing new binary attributes and building decision trees. Their common ground is that conditions near leaves of a tree are used to create new attributes. Fringe <ref> [Pagallo and Haussler, 1989; 1990; Pagallo, 1990] </ref> constructs a new attribute using the conjunction of two conditions at the parent and grandparent nodes of a positive leaf. <p> They interleave a tree learning phase and a new attribute construction process. Algorithms of the Fringe family such as Fringe, Dual Fringe, Symmetric Fringe <ref> [Pagallo and Haussler, 1989; 1990; Pagallo, 1990] </ref>, SymFringe, and DCFringe [Yang et al., 1991] behave similarly. Their main difference is that each algorithm uses a different method to generate new attributes. <p> univariate trees in most domains, and are more accurate than the univariate rules in some domains. 4.6 Related Work As mentioned before, the closest related work to the CI algorithms is Citre [Matheus and Rendell, 1989; Matheus, 1989] and the Fringe family of algorithms including Fringe, Dual Fringe, Symmetric Fringe <ref> [Pagallo and Haussler, 1989; 1990; Pagallo, 1990] </ref>, SymFringe, DCFringe [Yang et al., 1991], and SFringe (see Appendix B). The main difference is that the former use the rule-based approach to construct new attributes, while the latter use the path-based approach. <p> the oversearch-ing phenomenon [Quinlan and Cameron-Jones, 1995] that more search does not always produce better results when solving learning problems. 5.6 Related Work As far as constructing new attributes for decision tree learning is concerned, related work includes the Fringe family of algorithms such as Fringe, Dual Fringe, Symmetric Fringe <ref> [Pagallo and Haussler, 1989; 1990; Pagallo, 1990] </ref>, SymFringe, DCFringe [Yang et al., 1991], and SFringe (see Appendix B), the Citre algorithm [Matheus and Rendell, 1989; Matheus, 1989], the CI algorithms [Zheng, 1992] (also see Chapter 4), the 124 LFC algorithm [Ragavan and Rendell, 1993], the Cart algorithm [Breiman et al., 1984],
Reference: [Pagallo, 1990] <author> G. Pagallo, </author> <title> Adaptive Decision Tree Algorithms for Learning from Examples, </title> <type> Ph.D. Thesis, </type> <institution> University of California at Santa Cruz, </institution> <address> Santa Cruz, CA. </address>
Reference-contexts: As the number of peaks increases, system behavior deteriorates, yet real-world problems such as protein folding exhibit millions of peaks [Rendell, 1988; Lathrop, Webster, and Smith, 1987]. This is a manifestation of the fundamental limitation of selective induction. The "replication" problem of decision trees <ref> [Pagallo and Haussler, 1990; Pagallo, 1990] </ref> is another manifestation of the fundamental limitation of selective induction. <p> Different types of attribute are appropriate for describing different characteristics of examples. Most selective induction algorithms can accept attributes of these three kinds. However, many existing constructive induction algorithms such as Fringe <ref> [Pagallo, 1990] </ref> and Citre [Matheus and Rendell, 1989] only construct new binary attributes by using logical operators such as conjunction, negation, and disjunction. <p> Algorithms of the Fringe family such as Fringe, Dual Fringe, Symmetric Fringe, SymFringe, and DCFringe are similar to Citre in terms of constructing new binary attributes and building decision trees. Their common ground is that conditions near leaves of a tree are used to create new attributes. Fringe <ref> [Pagallo and Haussler, 1989; 1990; Pagallo, 1990] </ref> constructs a new attribute using the conjunction of two conditions at the parent and grandparent nodes of a positive leaf. <p> Again, this limitation of Dual Fringe is due to its 42 specific new attribute construction mechanism rather than the disjunctive representations themselves. Symmetric Fringe <ref> [Pagallo, 1990] </ref> is a combination of Fringe and Dual Fringe. For each leaf, it constructs one new attribute using either conjunction or disjunction depending on whether the leaf is positive or negative. <p> In this thesis, these three groups of domains are referred to as the artificial domains, the Monks domains, and the real-world domains respectively. The following three subsections describe them in detail. 3.2.1 Artificial domains Table 3.1 summarizes the characteristics of the artificial domains <ref> [Pagallo, 1990] </ref>. There are three reasons for choosing them. First, they cover a variety of well-studied artificial concepts in the machine learning community: randomly generated boolean concepts including DNF and CNF concepts, multiplexor concepts [Wilson, 1987], parity concepts, and majority concepts [Subutai and Tesauro, 1988]. <p> Second, all of them contain a large number of irrelevant attributes. By using them, we can see the ability of algorithms to tolerate irrelevant attributes. Third, experimental results for these domains of some existing constructive induction algorithms such as Fringe <ref> [Pagallo, 1990] </ref> are available from the literature. This makes comparisons easier. * DNF1, DNF2, DNF3, and DNF4 are four randomly generated DNF concepts. For completeness, their definitions from Pagallo [1990] are given in Appendix A. <p> They interleave a tree learning phase and a new attribute construction process. Algorithms of the Fringe family such as Fringe, Dual Fringe, Symmetric Fringe <ref> [Pagallo and Haussler, 1989; 1990; Pagallo, 1990] </ref>, SymFringe, and DCFringe [Yang et al., 1991] behave similarly. Their main difference is that each algorithm uses a different method to generate new attributes. <p> These results tentatively show the advantage of the fixed rule-based new attribute construction method (with the root strategy) over the fixed path-based new attribute construction method in noisy domains. 4.2 Introduction As mentioned in Chapter 1, the replication problem <ref> [Pagallo, 1990] </ref> is a manifestation of the fundamental limitation of selective induction in decision tree learning. <p> However, Fringe fails to learn a good multi-variate tree for the CNF concept while Dual Fringe fails to learn a good multivariate tree for the DNF concept because they are biased toward DNF and CNF concepts respectively <ref> [Pagallo, 1990] </ref>. SFringe can solve both the DNF and CNF problems. After two iterations, it learns the tree in Figure 4.3 for the DNF concept and the tree in Figure 4.5 for the CNF concept. <p> The reason is that the previous conclusion is for the fixed path-based new attribute construction approach. With the root strategy, the number of new attributes that can be constructed from paths of a tree is very small. Furthermore, as Pagallo <ref> [Pagallo, 1990, p. 77] </ref> argued, for DNF concepts, a conjunction of conditions from different terms is not good, since as a new attribute, it may have the same value for some positive and some negative examples. <p> univariate trees in most domains, and are more accurate than the univariate rules in some domains. 4.6 Related Work As mentioned before, the closest related work to the CI algorithms is Citre [Matheus and Rendell, 1989; Matheus, 1989] and the Fringe family of algorithms including Fringe, Dual Fringe, Symmetric Fringe <ref> [Pagallo and Haussler, 1989; 1990; Pagallo, 1990] </ref>, SymFringe, DCFringe [Yang et al., 1991], and SFringe (see Appendix B). The main difference is that the former use the rule-based approach to construct new attributes, while the latter use the path-based approach. <p> For this reason, we select new attributes through building a decision tree. All new attributes not used in the decision tree are deleted. Fringe, Dual Fringe, Symmetric Fringe <ref> [Pagallo, 1990] </ref>, and SFringe also use decision trees to select new attributes. 107 Some other decision tree learning algorithms use the data-driven constructive strategy. Instead of using the results of selective induction to guide their new attribute construction, they create new attributes directly from training data when building decision trees. <p> the oversearch-ing phenomenon [Quinlan and Cameron-Jones, 1995] that more search does not always produce better results when solving learning problems. 5.6 Related Work As far as constructing new attributes for decision tree learning is concerned, related work includes the Fringe family of algorithms such as Fringe, Dual Fringe, Symmetric Fringe <ref> [Pagallo and Haussler, 1989; 1990; Pagallo, 1990] </ref>, SymFringe, DCFringe [Yang et al., 1991], and SFringe (see Appendix B), the Citre algorithm [Matheus and Rendell, 1989; Matheus, 1989], the CI algorithms [Zheng, 1992] (also see Chapter 4), the 124 LFC algorithm [Ragavan and Rendell, 1993], the Cart algorithm [Breiman et al., 1984], <p> Some systems construct new continuous-valued attributes by using mathematical operators such as multiplication and division. The science discovery system Bacon [Langley et al., 1987a] and rule induction system Induce [Michalski, 1978] are two examples. Most hypothesis-driven constructive induction algorithms such as Fringe <ref> [Pagallo, 1990] </ref>, Citre [Matheus and Rendell, 1989], CI [Zheng, 1992] (also see Chapter 4), CAT (see Chapter 5), and AQ17-hci [Wnek and Michalski, 1994] construct and select a set of new attributes based on the entire training set. <p> Pagallo claims that in real-world domains, "the hypotheses generated by Fringe were more concise and at least as accurate" as those created by a selective decision tree learning algorithm <ref> [Pagallo, 1990, pp. 67-69] </ref>. SFringe achieves some better results in the real-world domains used in this thesis. 3 B.2 Control Structure of SF RINGE Figure B.1 gives the control structure of the SFringe algorithm. It starts from a training set described using a set of primitive attributes and classes. <p> In the i-th iteration (i from 0), conditions at decision nodes of a tree may contain new attributes created previously, and new attributes constructed from the tree can have a size up to 2 i+1 . SFringe uses Pagallo's Keep Used Once new attribute pruning method <ref> [Pagallo, 1990, p. 73] </ref> to delete irrelevant or redundant new attributes. After new attributes are created, the algorithm builds a temporary tree using primitive and all new attributes. Those newly constructed new attributes that do not occur in the tree are deleted. <p> Remaining attributes including primitive and new attributes are used in the following iterations. As there is a danger of deleting useful new attributes, primitive attributes are not removed. This allows the algorithm to reconstruct some new attributes that were deleted. 4 4 This idea is from Fringe <ref> [Pagallo, 1990, p. 74] </ref>.
Reference: [Pagallo and Haussler, 1990] <author> G. Pagallo and D. Haussler, </author> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 71-100. </pages>
Reference-contexts: As the number of peaks increases, system behavior deteriorates, yet real-world problems such as protein folding exhibit millions of peaks [Rendell, 1988; Lathrop, Webster, and Smith, 1987]. This is a manifestation of the fundamental limitation of selective induction. The "replication" problem of decision trees <ref> [Pagallo and Haussler, 1990; Pagallo, 1990] </ref> is another manifestation of the fundamental limitation of selective induction. <p> Therefore, decision graphs can be considered as a generalization of decision trees. Since branches of decision graphs can join together, they provide a way to alleviate the replication problem <ref> [Pagallo and Haussler, 1990] </ref> and the fragmentation problem [Kohavi and Li, 1995] of decision trees [Oliver, Dowe, and Wallace, 1992; Oliveira and Sangiovanni-Vincentelli, 1995; Kohavi and Li, 1995]. Oliver et al. [1992] introduce a top-down scheme to build general decision graphs. The algorithm starts from a leaf. <p> An even parity function on k attributes has the value true if an even number of the k attributes are true; otherwise, it has the value false. It is known that parity functions are hard problems for top-down and divide-and-conquer learning algorithms <ref> [Pagallo and Haussler, 1990] </ref>. They have been used to test neural network learning algorithms as well. In the complete sample of a parity function, the number of positive examples and the number of negative examples are the same. <p> and disadvantages of Xof-N representations as new nominal attributes and new continuous-valued attributes are investigated as well. 127 128 6.2 Introduction Up to now, most constructive induction algorithms that have been discussed focus on constructing new binary attributes, such as the CI (see Chapter 4), CAT (see Chapter 5), Fringe <ref> [Pagallo and Haussler, 1990] </ref>, LFC [Ragavan and Rendell, 1993], and ID2-of-3 [Murphy and Pazzani, 1991] algorithms. They use conjunction, disjunction, negation, or Mof-N as constructive operators. Some systems generate new continuous-valued attributes using different constructive operators.
Reference: [Patterson and Niblett, 1983] <author> A. Patterson and T. Niblett, </author> <title> Acls user manual, </title> <publisher> Glasgow: Intelligent Terminals Ltd. </publisher>
Reference-contexts: The patriarch of this family is CLS (Concept Learning System framework) [Hunt et al., 1966], but ID3 [Quinlan, 1983; 1986a; 1986b] and its descendant C4.5 [Quinlan, 1993a] are the best-known. They use information-based heuristic functions for test selection. Other members of the Tdidt family include Acls <ref> [Patterson and Niblett, 1983] </ref>, Assistant [Cestnik, Kononenko, and Bratko, 1987], Expert-ease, Ex-tran, Rule-Master, and GID3 [Cheng et al., 1988]. Acls [Patterson and Niblett, 1983] generalizes ID3 by allowing attributes to be integer-valued. Expert-ease, Ex-tran, and Rule-Master are commercial derivatives of Acls. <p> They use information-based heuristic functions for test selection. Other members of the Tdidt family include Acls <ref> [Patterson and Niblett, 1983] </ref>, Assistant [Cestnik, Kononenko, and Bratko, 1987], Expert-ease, Ex-tran, Rule-Master, and GID3 [Cheng et al., 1988]. Acls [Patterson and Niblett, 1983] generalizes ID3 by allowing attributes to be integer-valued. Expert-ease, Ex-tran, and Rule-Master are commercial derivatives of Acls. Furthermore, based on ID3, Assistant [Cestnik et al., 1987] introduces mechanisms to handle real-valued and nominal attributes as well as missing (unknown) values.
Reference: [Pazzani et al., 1991] <author> M. Pazzani, C. Brunk, and G. Silverstein, </author> <title> A knowledge-intensive approach to learning relational concepts. </title> <booktitle> Proceedings of the Eight International Workshop on Machine Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 432-436. </pages>
Reference: [Quinlan, 1983] <author> J.R. Quinlan, </author> <title> Learning efficient classification procedures and their application to chess endgames. In R.S. </title> <editor> Michalski, J.G. Carbonell, and T.M. Mitchell (eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach (Vol. I), </booktitle> <address> Palo Alto, CA: </address> <publisher> Tioga Press, </publisher> <pages> 463-482. 228 </pages>
Reference-contexts: Supervised learning can solve two types of problem: classification problems in which labels are categorical <ref> [Quinlan, 1983; Breiman, Friedman, Olshen, and Stone, 1984] </ref> and regression problems in which labels are continuous [Scheffe, 1959; Breiman et al., 1984; Quinlan, 1993b; Weiss and Indurkhya, 1995]. For classification problems, labels are called classes. <p> In this case, it is natural to consider the examples as positive and negative examples of a concept, and the task of supervised learning can be viewed as that of generating a definition of the concept. Therefore, supervised learning is sometimes called concept learning <ref> [Quinlan, 1983; Michalski, 1983] </ref>. The research presented in this thesis is described in the context of supervised learning for classification problems. Moreover, we only discuss zeroth-order learning where examples are represented using a vector of variables. <p> This kind of induction is called selective induction. There are many learning systems of this type such as ID3 <ref> [Quinlan, 1983; 1986] </ref>, Cart 8 [Breiman et al., 1984], C4.5 [Quinlan, 1993a], PLS1 [Rendell, 1983], CN2 [Clark and Niblett, 1989], and AQ11 [Michalski and Chilausky, 1980a]. <p> Decision tree learning systems The Tdidt (Top-Down Induction of Decision Trees) family [Quinlan, 1986b] contains a few variations on this basic decision tree learning theme. The patriarch of this family is CLS (Concept Learning System framework) [Hunt et al., 1966], but ID3 <ref> [Quinlan, 1983; 1986a; 1986b] </ref> and its descendant C4.5 [Quinlan, 1993a] are the best-known. They use information-based heuristic functions for test selection. Other members of the Tdidt family include Acls [Patterson and Niblett, 1983], Assistant [Cestnik, Kononenko, and Bratko, 1987], Expert-ease, Ex-tran, Rule-Master, and GID3 [Cheng et al., 1988].
Reference: [Quinlan, 1986a] <author> J.R. Quinlan, </author> <title> The effect of noise on concept learning. In R.S. </title> <editor> Michalski, J.G. Carbonell, and T.M. Mitchell (eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach (Vol. II), </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 149-166. </pages>
Reference-contexts: If such a probability for any possible test at a node is higher than a threshold, the node splitting process ends. One example of this kind of significance test is the chi-square test used in ID3 <ref> [Quinlan, 1986a; 1986b] </ref>. The problem with this first type of approach is that the stopping criteria (or thresholds) are very hard to get right to produce good trees [Breiman et al., 1984; Quinlan, 1993a]. The second type of approach is to keep the simple stopping criterion unchanged.
Reference: [Quinlan, 1986b] <author> J.R. Quinlan, </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages> <note> Reprinted in J.W. </note> <editor> Shavlik and T.G. Dietterich (eds.), </editor> <booktitle> Readings in Machine Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year> <note> Reprinted in B.G. </note> <editor> Buchanan and D. Wilkins (eds.), </editor> <booktitle> Readings in Knowledge Acquisition and Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kauf-mann, </publisher> <year> 1992. </year>
Reference-contexts: A good test should divide a dataset into subsets as pure as possible, not needing to be divided further. Information-based or probabilistic heuristics can provide effective guidance for test selection. Examples of well-known preference criteria are information gain, information gain ratio <ref> [Quinlan, 1986b; 1993a] </ref>, Gini, and Twoing [Breiman et al., 1984]. It has been shown that a greedy algorithm with this kind of heuristic is adequate for many learning tasks [Quinlan, 1983; 1986b; Quin-lan, Compton, Horn, and Lazarus, 1987; Breiman et al., 1984; Pagallo, 1990; Murphy and Pazzani, 1991; Zheng, 1995a]. <p> In this case, a tree perfectly consistent with a training set often has a low accuracy on unseen data. This is called overfitting (or overspecialization) to the training data <ref> [Breiman et al., 1984; Quinlan, 1986b; 1987b; Mingers, 1989b] </ref>. The reason is that some spurious tests selected by algorithms may discriminate few training examples by chance, 3 while these tests have no predictive value when applied to fresh data. <p> Surveys and empirical comparisons of different pruning methods for decision tree learning can be found in Quinlan [1987b], Mingers [1989b], and Esposito, Malerba, and Semeraro [1993]. Decision tree learning systems The Tdidt (Top-Down Induction of Decision Trees) family <ref> [Quinlan, 1986b] </ref> contains a few variations on this basic decision tree learning theme. The patriarch of this family is CLS (Concept Learning System framework) [Hunt et al., 1966], but ID3 [Quinlan, 1983; 1986a; 1986b] and its descendant C4.5 [Quinlan, 1993a] are the best-known. <p> However, they accept one training example at a time and build a decision tree incrementally. They keep, at each node, the information needed to compute expected information <ref> [Quinlan, 1986b] </ref> for every possible test at that node. Decision trees built by these incremental algorithms are similar to those learned by ID3. Using the same training set, ID5R builds the same decision tree as that generated by ID3.
Reference: [Quinlan, 1987a] <author> J.R. Quinlan, </author> <title> Generating production rules from decision trees. </title> <booktitle> Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 304-307. </pages>
Reference-contexts: For example, a decision tree learning algorithm searches only the space of finite trees that carry out axis-orthogonal splits. Decision trees [Hunt, Marin, and Stone, 1966; Quinlan, 1983; Breiman et al., 1984; Quinlan, 1993a] and production rules <ref> [Michalski, 1978; Quinlan, 1987a] </ref> are two commonly used theory description languages in supervised learning. Other theory description languages include neural networks [Rumelhart et al., 1986; Hinton, 1989], linear discriminant functions [James, 1985], and instance-based methods [Aha, 1990; Aha, Kibler, and Albert, 1991]. <p> However, some conditions in a path may be irrelevant to the class labeled by the leaf of the path <ref> [Quinlan, 1987a] </ref>. Consequently, new attributes created directly from paths by using the fixed path-based strategy may contain some irrelevant conditions. On the other hand, a system called C4.5rules [Quinlan, 1987a; 1993a] transforms decision trees into production rules. In the process, irrelevant or unimportant conditions are deleted. <p> However, some conditions in a path may be irrelevant to the class labeled by the leaf of the path [Quinlan, 1987a]. Consequently, new attributes created directly from paths by using the fixed path-based strategy may contain some irrelevant conditions. On the other hand, a system called C4.5rules <ref> [Quinlan, 1987a; 1993a] </ref> transforms decision trees into production rules. In the process, irrelevant or unimportant conditions are deleted. Therefore, constructing new attributes from production rules should be better than constructing new attributes directly from the decision tree which the rules are generated from. <p> Some other systems such as C4.5rules <ref> [Quinlan, 1987a; 1993a] </ref> create rules by transforming decision trees. The remainder of this subsection discusses these two types of rule learning method in turn. 35 Generating rules using the covering approach For each class, a covering algorithm generally induces a disjunction of conjunctions as its definition. <p> Decision tree learning is usually more efficient than rule learning. Building decision trees can reduce the search space for generating rules. On the other hand, transforming decision trees into rules provides a way to simplify decision trees [Quinlan, 1987b]. C4.5rules <ref> [Quinlan, 1987a] </ref> in the C4.5 package [Quinlan, 1993a] is such an algorithm. From a decision tree, 12 C4.5rules derives a set of production rules based on the same training set used when building the tree. 13 The process consists of two stages. <p> First, individual rules are extracted from a decision tree. Each path from the root of 12 Raw trees are used to generate production rules. 13 C4.5rules can transform multiple decision trees for the same task into a set of rules <ref> [Quinlan, 1987a] </ref>. 38 the tree to a leaf is transformed into an initial production rule of the form: if Cond 1 ^ Cond 2 ^ ... ^ Cond n then class C, where the Cond i 's are conditions of the path and C is the class labeled by the leaf, <p> rules, and using a series of greedy searches [Quinlan, 1995] otherwise. 15 After sets of rules for all the classes have be selected, they are ordered to minimize 14 The earlier releases of C4.5 use a significance test on a contingency table to decide whether a condition should be eliminated <ref> [Quinlan, 1987a] </ref>. It is still retained as an option in the current release. 15 Release 5 [Quinlan, 1993a] uses simulated annealing. 39 false positive errors (cases that satisfy a rule but really do not belong to its predicted class). The default class is then determined using the heuristic mentioned above. <p> If so, the first such rule is deleted, and the ruleset is checked again. It has been shown that the set of production rules is often both simpler and more accurate on unseen cases than the decision tree from which the ruleset is generated <ref> [Quinlan, 1987a; 1987b; 1993a] </ref>. In addition, from the above discussion, we know that during the transformation of a tree into rules, irrelevant or unimportant conditions in paths are dropped. <p> Based on the observation that converting decision trees into production rules can eliminate irrelevant conditions from paths (see the discussions on page 37) <ref> [Quinlan, 1987a] </ref>, this chapter explores a fixed rule-based approach that constructs conjunctions from production rules. The following section describes three algorithms that implement this idea, followed by experiments in Section 4.4 for evaluating these algorithms and comparing them with C4.5 and SFringe. Section 4.5 discusses some related issues.
Reference: [Quinlan, 1987b] <author> J.R. Quinlan, </author> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27, </volume> <pages> 221-234. </pages>
Reference-contexts: Several pruning 3 Such tests often occur at nodes near leaves of a tree. 25 methods have been developed and used for decision tree learning, such as cost-complexity pruning [Breiman et al., 1984], reduced error pruning <ref> [Quinlan, 1987b] </ref>, and pessimistic pruning [Quinlan, 1987b; 1993a]. Surveys and empirical comparisons of different pruning methods for decision tree learning can be found in Quinlan [1987b], Mingers [1989b], and Esposito, Malerba, and Semeraro [1993]. <p> Several pruning 3 Such tests often occur at nodes near leaves of a tree. 25 methods have been developed and used for decision tree learning, such as cost-complexity pruning [Breiman et al., 1984], reduced error pruning [Quinlan, 1987b], and pessimistic pruning <ref> [Quinlan, 1987b; 1993a] </ref>. Surveys and empirical comparisons of different pruning methods for decision tree learning can be found in Quinlan [1987b], Mingers [1989b], and Esposito, Malerba, and Semeraro [1993]. <p> However, the resubstitution error (on the training set) is not a suitable evaluation function, as pruning always increases resubstitution errors. One solution is the pessimistic error estimate that adjusts the resubstitution error by increasing the number of errors observed at each leaf by 0.5 <ref> [Quinlan, 1987b] </ref>. This is the continuity correction for the binomial distribution [Snedecor and Cochran, 1989, pp. 117ff]. Pruning with this tree evaluation method is called pessimistic pruning [Quinlan, 1987b]. The recent releases of C4.5 use an even more pessimistic estimate. Suppose that a leaf covers I instances with E errors. <p> solution is the pessimistic error estimate that adjusts the resubstitution error by increasing the number of errors observed at each leaf by 0.5 <ref> [Quinlan, 1987b] </ref>. This is the continuity correction for the binomial distribution [Snedecor and Cochran, 1989, pp. 117ff]. Pruning with this tree evaluation method is called pessimistic pruning [Quinlan, 1987b]. The recent releases of C4.5 use an even more pessimistic estimate. Suppose that a leaf covers I instances with E errors. <p> Decision tree learning is usually more efficient than rule learning. Building decision trees can reduce the search space for generating rules. On the other hand, transforming decision trees into rules provides a way to simplify decision trees <ref> [Quinlan, 1987b] </ref>. C4.5rules [Quinlan, 1987a] in the C4.5 package [Quinlan, 1993a] is such an algorithm. From a decision tree, 12 C4.5rules derives a set of production rules based on the same training set used when building the tree. 13 The process consists of two stages.
Reference: [Quinlan et al., 1987] <author> J.R. Quinlan, P.J. Compton, K.A. Horn, and L. Lazarus, </author> <title> Inductive knowledge acquisition: a case study. Application of Expert Systems, </title> <address> Wokingham, UK: </address> <publisher> Addison-Wesley, </publisher> <pages> 157-173. </pages>
Reference: [Quinlan, 1988a] <author> J.R. Quinlan, </author> <title> Decision trees and multi-valued attributes. </title> <journal> Machine Intelligence, </journal> <volume> 11, </volume> <pages> 305-318. </pages>
Reference-contexts: The information gain ratio criterion can compensate for this problem. It has been shown that the gain ratio criterion is robust and typically gives a consistently better choice of test than the gain criterion <ref> [Quinlan, 1988a] </ref>. 7 Note that gain ratio is unstable if a split is near-trivial, as the split information is small in that case.
Reference: [Quinlan, 1988b] <author> J.R. Quinlan, </author> <title> An empirical comparison of genetic and decision-tree classifiers. </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 135-141. </pages>
Reference: [Quinlan, 1989] <author> J.R. Quinlan, </author> <title> Unknown attribute values in induction. </title> <booktitle> Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kauf-mann, </publisher> <pages> 164-168. </pages>
Reference: [Quinlan and Rivest, 1989] <author> J.R. Quinlan and R.L. Rivest, </author> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80, </volume> <pages> 227-248. </pages>
Reference-contexts: For each class in turn, the set of rules for that class is simplified by deleting rules whose removal does not diminish the accuracy of the set of rules as a whole. Based on the MDL principle [Rissanen, 1983], the weighted coding cost <ref> [Quinlan and Rivest, 1989] </ref> is used as the heuristic evaluation function of a ruleset [Quinlan, 1993a; 1995]. <p> Finally, CI1 and CI2 choose the best tree using the tree evaluation function from all the pruned trees built. 4.3.3 Decision tree evaluation function The coding cost of a decision tree <ref> [Quinlan and Rivest, 1989] </ref>, which is based on the MDL principle [Rissanen, 1983], provides a uniform metric for measuring the accuracy 3 The optimum problem cannot be solved completely, but being patient for a while should be better than stopping execution immediately after a worse tree is built. 76 and complexity <p> However, the identified relevant primitive attributes must be specified. It costs L (N primitive ; N rel ; N primitive ) bits, where N primitive is the number of all primitive attributes and N rel is the number of relevant primitive attributes. The function L (n; k; b) <ref> [Quinlan and Rivest, 1989] </ref> equals log 2 (b + 1) + log 2 @ @ k A A . <p> Furthermore, for problems with multiple classes (more than two), this encoding scheme tends to underestimate the number of bits required to encode the exceptions because the true classes of error examples should be specified as well <ref> [Quinlan and Rivest, 1989] </ref>.
Reference: [Quinlan, 1990] <author> J.R. Quinlan, </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 239-266. </pages>
Reference: [Quinlan, 1993a] <author> J.R. Quinlan, C4.5: </author> <title> Programs for Machine Learning, </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher> <pages> 229 </pages>
Reference-contexts: This kind of induction is called selective induction. There are many learning systems of this type such as ID3 [Quinlan, 1983; 1986], Cart 8 [Breiman et al., 1984], C4.5 <ref> [Quinlan, 1993a] </ref>, PLS1 [Rendell, 1983], CN2 [Clark and Niblett, 1989], and AQ11 [Michalski and Chilausky, 1980a]. <p> attributes such as structured attributes may be used, but here we discuss only these three most commonly used types of attribute. 14 These count how many attributes of an example have a specified value. 15 This equals the number of classes of the learning problem. 16 form a new test <ref> [Quinlan, 1993a] </ref>. This can be thought as a method of constructing new nominal attributes. To the best of the author's knowledge, there is no other decision tree learning algorithm that constructs new nominal attributes. <p> One example of this kind of significance test is the chi-square test used in ID3 [Quinlan, 1986a; 1986b]. The problem with this first type of approach is that the stopping criteria (or thresholds) are very hard to get right to produce good trees <ref> [Breiman et al., 1984; Quinlan, 1993a] </ref>. The second type of approach is to keep the simple stopping criterion unchanged. After growing a large tree (called a raw tree), this tree is pruned back to a good size. <p> The patriarch of this family is CLS (Concept Learning System framework) [Hunt et al., 1966], but ID3 [Quinlan, 1983; 1986a; 1986b] and its descendant C4.5 <ref> [Quinlan, 1993a] </ref> are the best-known. They use information-based heuristic functions for test selection. Other members of the Tdidt family include Acls [Patterson and Niblett, 1983], Assistant [Cestnik, Kononenko, and Bratko, 1987], Expert-ease, Ex-tran, Rule-Master, and GID3 [Cheng et al., 1988]. <p> OUTPUT: two decision trees: a raw tree and a pruned tree. C := the majority class in D training Tree.raw := Grow-Tree (Att, D training , C) Tree.pruned := Prune-Tree (Tree.raw, Att, D training ) RETURN Tree C4.5 As shown in Figure 2.1, C4.5 <ref> [Quinlan, 1993a] </ref> builds decision trees by employing a two-stage process for growing and pruning. It produces a raw tree and a pruned tree. We describe these two stages below. Another well-known technique of C4.5 is windowing. <p> We do not discuss this technique in detail since it is principally used for overcoming memory limits although it can sometimes lead to faster construction of trees [Catlett, 1991b] and/or more accurate trees <ref> [Quinlan, 1993a] </ref>. In addition to learning decision trees, Quinlan's C4.5 package includes other features such as generating production rules from decision trees and interacting with classification models. For details, please see Quinlan [1993a]. <p> The test evaluation function can be either information gain or information gain ratio. 6 The information gained by partitioning a training set D using the test Test A is defined as <ref> [Quinlan, 1993a] </ref>: gain (Test A ) = info (D) info Test A (D ); (2.1) info Test A (D ) = n X jD i j fi info (D i ); (2.2) info (D) = k X freq (C j ; D ) jD j freq (C j ; D ) <p> Similarly, the gain ratio is defined as <ref> [Quinlan, 1993a] </ref>: gain ratio (Test A ) = gain (Test A ) split info (Test A ) ; (2.4) split info (Test A ) = n X jD i j fi log 2 ( jD j where split info (Test A ) is the potential information generated by splitting D into <p> Decision tree learning is usually more efficient than rule learning. Building decision trees can reduce the search space for generating rules. On the other hand, transforming decision trees into rules provides a way to simplify decision trees [Quinlan, 1987b]. C4.5rules [Quinlan, 1987a] in the C4.5 package <ref> [Quinlan, 1993a] </ref> is such an algorithm. From a decision tree, 12 C4.5rules derives a set of production rules based on the same training set used when building the tree. 13 The process consists of two stages. First, individual rules are extracted from a decision tree. <p> Based on the MDL principle [Rissanen, 1983], the weighted coding cost [Quinlan and Rivest, 1989] is used as the heuristic evaluation function of a ruleset <ref> [Quinlan, 1993a; 1995] </ref>. <p> It is still retained as an option in the current release. 15 Release 5 <ref> [Quinlan, 1993a] </ref> uses simulated annealing. 39 false positive errors (cases that satisfy a rule but really do not belong to its predicted class). The default class is then determined using the heuristic mentioned above. <p> The test set at every point of a trial is the same as the test set used in the corresponding trial of the 10-fold cross-validation. 3.4 Comparison Algorithms C4.5 <ref> [Quinlan, 1993a] </ref>, 13 an algorithm from the Fringe family, and ID2-of-3 [Murphy and Pazzani, 1991] are used as the reference algorithms in the empirical studies of this thesis. This section briefly explains why they are chosen. The descriptions of C4.5 and ID2-of-3 have been given in Chapter 2. <p> It consists of two parts: cost of encoding a tree and cost of encoding exceptions of the tree on the training set (resubstitution errors). Since the more pessimistic error estimate <ref> [Quinlan, 1993a, pp. 40-41] </ref> gives a better estimate of the true error rate of a tree than the resubstitution error rate, we introduce a heuristic decision tree evaluation function inspired by the MDL principle. <p> Note that the MDL-inspired heuristic function could be a good evaluation function for comparing decision trees as it takes into account both the complexity and the estimate accuracy of a tree. However, it is no longer the real coding cost for communicating class information <ref> [Quinlan, 1993a, p. 51] </ref>. Now, let us describe the encoding scheme used by the MDL-inspired heuristic function. It tries to find a lower limit on the number of bits in any encoding rather than choosing a particular encoding, so the scheme is approximate. <p> Given a decision tree T built from a training set D with more pessimistically estimated errors E p derived from T 's resubstitu-tion errors on the training set <ref> [Quinlan, 1993a] </ref>, the value of the MDL-inspired heuristic function for T consists of three parts: the cost of encoding new attributes occurred in T (if there is any), the cost of encoding T itself, and the cost of encoding E p . <p> In addition, "in practice, encoding schemes often tend to overestimate the number of bits required to encode a theory relative to sets of exceptions" <ref> [Quinlan, 1993a, p. 52] </ref>. <p> At each decision node, XofN creates one nominal Xof-N representation by performing heuristic search. pre-processing, and post-processing parts. The kernel builds a decision tree. During the generation of this tree, the construction of Xof-N attributes occurs. The tree is then pruned using the pruning mechanism of C4.5 <ref> [Quinlan, 1993a] </ref>. The pre-processing part discretizes primitive continuous-valued attributes if there are any. The post-processing part identifies relevant attributes and rebuilds a tree using them. <p> Therefore, it cannot build a correct tree even though decision trees with Xof-N representations as nominal attributes can, in principle, represent CNF concepts. The XofN algorithm takes two approaches to alleviating this problem. One is the subsetting mechanism of C4.5 <ref> [Quinlan, 1993a] </ref> described in Subsection 2.2.1 (see page 31). The other is subranging which is similar to subsetting.
Reference: [Quinlan, 1993b] <author> J.R. Quinlan, </author> <title> Combining instance-based and model-based learning. </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 236-243. </pages>
Reference: [Quinlan, 1995] <author> J.R. Quinlan, </author> <title> MDL and categorical theories (continued). </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 464-470. </pages>
Reference-contexts: C4.5rules finds the best subset of the ruleset for each class that minimizes the total coding cost by using exhaustive search if there are only a few rules, and using a series of greedy searches <ref> [Quinlan, 1995] </ref> otherwise. 15 After sets of rules for all the classes have be selected, they are ordered to minimize 14 The earlier releases of C4.5 use a significance test on a contingency table to decide whether a condition should be eliminated [Quinlan, 1987a].
Reference: [Quinlan and Cameron-Jones, 1995] <author> J.R. Quinlan and R.M. Cameron-Jones, </author> <title> Oversearch-ing and layered search in empirical learning. </title> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 1019-1024. </pages>
Reference-contexts: One reason might be that information gain is used as the heuristic function to evaluate new attributes. Good new attributes cannot always be selected even if they are examined. On the other hand, this can be explained as the oversearch-ing phenomenon <ref> [Quinlan and Cameron-Jones, 1995] </ref> that more search does not always produce better results when solving learning problems. 5.6 Related Work As far as constructing new attributes for decision tree learning is concerned, related work includes the Fringe family of algorithms such as Fringe, Dual Fringe, Symmetric Fringe [Pagallo and Haussler, 1989; <p> It does, however, need much more execution time. This might be explained as the over-searching phenomenon that more search does not always lead to more accurate theories <ref> [Quinlan and Cameron-Jones, 1995] </ref>. In addition, the current XofN algorithm constructs Xof-N representations based only on primitive attributes. It can be extended to using both primitive attributes and previously created new attributes. This would allow XofN to construct more complex new attributes. <p> This gives a hint that execution time might be used as a guide to selection of new attribute type among conjunctive, disjunctive, Mof-N, and Xof-N representations. As a consequence, the oversearching problem <ref> [Quinlan and Cameron-Jones, 1995] </ref> may be alleviated. 7.7 Related Work Almost all papers that describe constructive induction algorithms compare a few new attribute construction methods. For example, Matheus [1989] compares four different constructive operand selection strategies with conjunction as the constructive operator. <p> Some other search methods such as simple and less expensive greedy search are worth exploring, especially considering the oversearching phenomenon <ref> [Quinlan and Cameron-Jones, 1995] </ref>. We have mentioned that when the size of new attributes is automatically decided during the process of construction, overfitting might occur. In this case, just as for building decision trees and rules, pruning is necessary for generating new attributes. This issue deserves study in the future.
Reference: [Ragavan and Rendell, 1993] <author> H. Ragavan and L. Rendell, </author> <title> Lookahead feature construction for learning hard concepts. </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 252-259. </pages>
Reference-contexts: Only one decision tree is built for each run. At each decision node, a new attribute is created, and is used as the split test of the node during the formation of a tree. LFC <ref> [Ragavan and Rendell, 1993] </ref> uses conjunction and negation as constructive operators. Information gain is adopted as the attribute evaluation function. At each decision node, LFC constructs one new attribute by carrying out directed lookahead beam search. <p> In addition, Cart can also construct linear discriminants as new binary attributes. ID2-of-3 [Murphy and Pazzani, 1991] constructs a new binary attribute directly from training data for each decision node when building decision trees, but it uses at-least Mof-N representations. LFC <ref> [Ragavan and Rendell, 1993] </ref> uses negation and conjunction as constructive operators. It creates one conjunction for each decision node by using a directed lookahead search method. <p> family of algorithms such as Fringe, Dual Fringe, Symmetric Fringe [Pagallo and Haussler, 1989; 1990; Pagallo, 1990], SymFringe, DCFringe [Yang et al., 1991], and SFringe (see Appendix B), the Citre algorithm [Matheus and Rendell, 1989; Matheus, 1989], the CI algorithms [Zheng, 1992] (also see Chapter 4), the 124 LFC algorithm <ref> [Ragavan and Rendell, 1993] </ref>, the Cart algorithm [Breiman et al., 1984], the ID2-of-3 algorithm [Murphy and Pazzani, 1991], and the Lmdt algorithm [Brodley and Utgoff, 1992]. Like CAT, the Fringe family of algorithms and the CI algorithms use the hypothesis-driven strategy to construct conjunctions and/or disjunctions for decision trees. <p> as new nominal attributes and new continuous-valued attributes are investigated as well. 127 128 6.2 Introduction Up to now, most constructive induction algorithms that have been discussed focus on constructing new binary attributes, such as the CI (see Chapter 4), CAT (see Chapter 5), Fringe [Pagallo and Haussler, 1990], LFC <ref> [Ragavan and Rendell, 1993] </ref>, and ID2-of-3 [Murphy and Pazzani, 1991] algorithms. They use conjunction, disjunction, negation, or Mof-N as constructive operators. Some systems generate new continuous-valued attributes using different constructive operators. For example, Bacon [Langley et al., 1987a] and Induce [Michalski, 1978] use mathematical operators such as multiplication and division. <p> At the moment, XofN uses cut points found by C4.5 to discretize primitive continuous-valued attributes. Other discretization methods that can be used include multi-interval discretization methods [Catlett, 1991a; Fayyad and Irani, 1993], supervised/unsupervised methods [Van de Merckt, 1993], and an entropy method <ref> [Ragavan and Rendell, 1993] </ref>. Both Catlett [1991a] and Fayyad and Irani [1993] recursively apply a binary splitting procedure 16 to a continuous-valued attribute, but they use different stopping criteria. <p> Like ID2-of-3, XofN, XofN (c), and XofN (cc), LFC <ref> [Ragavan and Rendell, 1993] </ref> is also a data-driven constructive induction algorithm that builds multivariate trees, but it uses negation and conjunction as constructive operators. LFC creates one conjunction for each decision node by using directed lookahead search.
Reference: [Rao et al., 1995] <author> R.B. Rao, D. Gordon, and W. Spears, </author> <title> For every generalization action, is there really an equal and opposite reaction? analysis of the conservation law for generalization performance. </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 471-479. </pages>
Reference-contexts: It is highly improbable that the set of "real-world problems" we face is a uniformly random universe <ref> [Rao et al., 1995] </ref>. <p> Otherwise, this measure is the same as the generalization performance measure. They develop arguments supporting the believe that non-zero expected generalization performance is possible and one learner can be better than another in our universe <ref> [Rao et al., 1995] </ref>. This thesis explores novel approaches to the task of classifier learning with decision trees as the theory description language. The next section motivates the research of the thesis, followed by the objective in Section 1.2. Then, Section 1.3 summarizes the principal claims of the thesis.
Reference: [Reinke and Michalski, 1988] <author> R.E. Reinke and R.S. Michalski, </author> <title> Incremental learning of concept descriptions: a method and experimental results. </title> <journal> Machine Intelligence, </journal> <volume> 11, </volume> <pages> 263-288. </pages>
Reference-contexts: The LEF (Lexicographical Evaluation Function) <ref> [Reinke and Michalski, 1988] </ref> is used as the preference criterion for selecting complexes. At each stage, a positive example is randomly selected as a seed.
Reference: [Rendell, 1983] <author> L.A. Rendell, </author> <title> A new basis for state-space learning systems and a successful implementation. </title> <journal> Artificial Intelligence, </journal> <volume> 20, </volume> <pages> 369-392. </pages>
Reference-contexts: This kind of induction is called selective induction. There are many learning systems of this type such as ID3 [Quinlan, 1983; 1986], Cart 8 [Breiman et al., 1984], C4.5 [Quinlan, 1993a], PLS1 <ref> [Rendell, 1983] </ref>, CN2 [Clark and Niblett, 1989], and AQ11 [Michalski and Chilausky, 1980a]. <p> Instead of using supervised test evaluation functions such as those mentioned above, Van de Merckt [1993] introduces unsupervised heuristics for finding cut points of continuous-valued attributes to decision tree learning. Although PLS1 4 <ref> [Rendell, 1983] </ref> generates hyperrectangles instead of decision trees, it uses the divide-and-conquer method and these two types of concept representation (hyperrectangles and decision trees) are similar. All the systems discussed above are batch algorithms. They require all the training examples to be available at the beginning of learning.
Reference: [Rendell, 1988] <author> L.A. Rendell, </author> <title> Learning hard concepts. </title> <booktitle> Proceedings of the Third European Working Session on Learning, </booktitle> <address> London: </address> <publisher> Pitman, </publisher> <pages> 177-200. </pages>
Reference-contexts: Even worse, some of them might be irrelevant to the target theory. Note that irrelevant attributes usually result in worse performance of inductive learning [Caruana and Freitag, 1994; Langley, 1994]. By viewing theories or concepts as functions over instance spaces, some research results <ref> [Rendell, 1988; Rendell and Cho, 1990] </ref> have shown that methods of selective induction 8 The basic version of Cart forms a test by using a single task-supplied attribute, but it could create a test using a conjunction or disjunction of more than one task-supplied attribute. 9 For example, for the "win <p> To delineate a peak, several examples may be needed. As the number of peaks increases, system behavior deteriorates, yet real-world problems such as protein folding exhibit millions of peaks <ref> [Rendell, 1988; Lathrop, Webster, and Smith, 1987] </ref>. This is a manifestation of the fundamental limitation of selective induction. The "replication" problem of decision trees [Pagallo and Haussler, 1990; Pagallo, 1990] is another manifestation of the fundamental limitation of selective induction.
Reference: [Rendell and Cho, 1990] <author> L.A. Rendell and H. Cho, </author> <title> Empirical learning as a function of concept character. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 267-298. </pages>
Reference-contexts: All paths of a tree are mutually exclusive 3 Consistent, here, means that a theory can correctly classify a dataset. 5 and exhaustive. All regions defined by the paths completely cover the instance space 4 <ref> [Rendell and Cho, 1990] </ref>, and there is no overlap among the regions. For binary-class problems, a leaf with the positive class is called a positive leaf, while a leaf with the negative class is called a negative leaf. <p> Even worse, some of them might be irrelevant to the target theory. Note that irrelevant attributes usually result in worse performance of inductive learning [Caruana and Freitag, 1994; Langley, 1994]. By viewing theories or concepts as functions over instance spaces, some research results <ref> [Rendell, 1988; Rendell and Cho, 1990] </ref> have shown that methods of selective induction 8 The basic version of Cart forms a test by using a single task-supplied attribute, but it could create a test using a conjunction or disjunction of more than one task-supplied attribute. 9 For example, for the "win
Reference: [Rissanen, 1983] <author> J. Rissanen, </author> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> Annals of Statistics, </journal> <volume> 11, </volume> <pages> 416-431. </pages>
Reference-contexts: A RODG learning algorithm, Smog, is described. The algorithm uses a decision tree as the start point, and transforms it into a RODG using the Minimum Description Length (MDL) principle <ref> [Rissanen, 1983] </ref>. It is shown that the Smog algorithm achieves higher prediction accuracies than C4.5 in many artificial domains [Oliveira and Sangiovanni-Vincentelli, 1995]. Kohavi [1994] and Kohavi and Li [1995] explore Oblivious read-Once Decision Graphs (OODG) that are similar to RODGs. <p> For each class in turn, the set of rules for that class is simplified by deleting rules whose removal does not diminish the accuracy of the set of rules as a whole. Based on the MDL principle <ref> [Rissanen, 1983] </ref>, the weighted coding cost [Quinlan and Rivest, 1989] is used as the heuristic evaluation function of a ruleset [Quinlan, 1993a; 1995]. <p> Finally, CI1 and CI2 choose the best tree using the tree evaluation function from all the pruned trees built. 4.3.3 Decision tree evaluation function The coding cost of a decision tree [Quinlan and Rivest, 1989], which is based on the MDL principle <ref> [Rissanen, 1983] </ref>, provides a uniform metric for measuring the accuracy 3 The optimum problem cannot be solved completely, but being patient for a while should be better than stopping execution immediately after a worse tree is built. 76 and complexity of a tree. <p> Moreover, they may use more than one type of new attribute for a single learning task. Different types of new attribute can be used at different decision nodes. A possible start point is using Minimum Description Length principle <ref> [Rissanen, 1983] </ref> as the selection criterion. We can see, from Tables 7.1 and 7.2, the clear correlation between prediction accuracy and theory complexity of Conj, Disj, MofN, and XofN.
Reference: [Rivest, 1987] <author> R.L. Rivest, </author> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 229-246. 230 </pages>
Reference-contexts: CN2 learns a set of ordered rules iteratively, which is also termed decision lists <ref> [Rivest, 1987] </ref>. In each iteration, the algorithm searches for a complex that covers a large number of examples of one class, say C, and few examples of other classes.
Reference: [Rumelhart et al., 1986] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams, </author> <title> Learning internal representations by error propagation. </title> <editor> In D.E. Rumelhart and J.L. McClelland (eds.), </editor> <booktitle> Parallel Distributed Processing (Vol. I), </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <pages> 318-362. </pages>
Reference-contexts: Decision trees [Hunt, Marin, and Stone, 1966; Quinlan, 1983; Breiman et al., 1984; Quinlan, 1993a] and production rules [Michalski, 1978; Quinlan, 1987a] are two commonly used theory description languages in supervised learning. Other theory description languages include neural networks <ref> [Rumelhart et al., 1986; Hinton, 1989] </ref>, linear discriminant functions [James, 1985], and instance-based methods [Aha, 1990; Aha, Kibler, and Albert, 1991]. An important advantage of decision trees and production rules, as theory description languages, is that they are relatively easy for humans to understand.
Reference: [Rymon, 1992] <author> R. Rymon, </author> <title> Search through systematic set enumeration. </title> <booktitle> Proceedings of the Third International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <address> Cambridge MA: </address> <publisher> MIT Press, </publisher> <pages> 539-550. </pages>
Reference: [Rymon, 1993] <author> R. Rymon, </author> <title> An SE-tree based characterization of the induction problem. </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 268-275. </pages>
Reference-contexts: However, Class invokes the Opus algorithm [Webb, 1993] to construct one rule by performing systematic search with pruning <ref> [Webb, 1993; Rymon, 1993; Schlimmer, 1993] </ref>. The rule evaluation function is also the Laplace function. Systematic search has the same result as exhaustive search. It can guarantee that the best rule can be found in terms of the evaluation function.
Reference: [Samuel, 1959] <author> A.L. Samuel, </author> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal of Research and Development, </journal> <note> 3. Reprinted in E.A. </note> <editor> Feigenbaum (ed.), </editor> <booktitle> Computers and Thought, </booktitle> <address> New York: </address> <publisher> McGraw-Hill, </publisher> <year> 1963. </year>
Reference-contexts: Cho, 1990] have shown that methods of selective induction 8 The basic version of Cart forms a test by using a single task-supplied attribute, but it could create a test using a conjunction or disjunction of more than one task-supplied attribute. 9 For example, for the "win in checkers" problem <ref> [Samuel, 1959] </ref>, the contents of squares on the board are low-level attributes. 9 founder if the target concept is complex, for example, having too many peaks. 10 Since selective induction can only learn neighborhoods, it alone cannot predict peaks that are not present in the training set.
Reference: [Schaffer, 1994] <author> C. Schaffer, </author> <title> A conservation law for generalization performance. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 259-265. </pages>
Reference-contexts: In addition, the proof of the conservation law given in Schaffer [1994] applies only to domains with all nominal attributes, although continuous-valued attributes can be converted into nominal attributes by using discretization. Rao, Gordon, and Spears [1995] further study the conservation law <ref> [Schaffer, 1994] </ref>, and illustrate that the way in which the conservation law sums generalization performance over all learning situations 7 is relevant only in a uniformly random universe. It is highly improbable that the set of "real-world problems" we face is a uniformly random universe [Rao et al., 1995]. <p> The objective of doing so is to diminish effects that are created by differences of other factors which might affect the performance of constructive induction. Since generalization performance is concerned when comparing learning algorithms, the conservation law <ref> [Schaffer, 1994] </ref> already discussed on page 6 is worth mentioning here. It implies that no algorithm can be superior to another in terms of generalization performance across all learning situations. However, we are interested in the "real-world problems" rather than all learning situations.
Reference: [Scheffe, 1959] <author> H. Scheffe, </author> <title> The Analysis of Variance, </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: [Schlimmer, 1993] <author> J.C. Schlimmer, </author> <title> Efficiently inducing determinations: a complete and systematic search algorithm that uses optimal pruning. </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 284-290. </pages>
Reference-contexts: However, Class invokes the Opus algorithm [Webb, 1993] to construct one rule by performing systematic search with pruning <ref> [Webb, 1993; Rymon, 1993; Schlimmer, 1993] </ref>. The rule evaluation function is also the Laplace function. Systematic search has the same result as exhaustive search. It can guarantee that the best rule can be found in terms of the evaluation function.
Reference: [Schlimmer and Fisher, 1986] <author> J.C. Schlimmer and D. Fisher, </author> <title> A case study of incremental concept induction. </title> <booktitle> Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 496-501. </pages>
Reference-contexts: All the systems discussed above are batch algorithms. They require all the training examples to be available at the beginning of learning. In order to be able to update the learned decision tree quickly when new examples are available, some incremental algorithms have been developed, such as ID4 <ref> [Schlimmer and Fisher, 1986] </ref>, ID5 [Utgoff, 1988], ID5R [Utgoff, 1989], and ITI [Utgoff, 1994]. Like ID3, all of them use the information-based heuristic to select a test for each decision node and chi-square test for independence to prevent overfitting the training examples.
Reference: [Sejnowski and Rosenberg, 1987] <author> T.J. Sejnowski and C.R. Rosenberg, </author> <title> Parallel networks that learn to pronounce English text. </title> <journal> Complex Systems, </journal> <volume> 1, </volume> <pages> 145-168. </pages>
Reference-contexts: Third, the first two are combined to predict both the phoneme and stress of a letter. Finally, the sequence of phoneme-stress pairs of all the letters in a word forms its speech mapping. The dataset was developed by Terry Sejnowski and Charles Rosenberg, and donated by Terry Sejnowski <ref> [Sejnowski and Rosenberg, 1987] </ref>. Here, we only use the first three subdomains, called Nettalk (Phoneme), Nettalk (Stress), and Nettalk (Letter). We say the prediction of a letter is correct if its phoneme prediction and stress prediction are both correct.
Reference: [Shavlik and Dietterich, 1990] <editor> J.W. Shavlik and T.G. Dietterich (eds.), </editor> <booktitle> Readings in Machine Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Inductive learning systems take these examples and induce general knowledge that can be used in knowledge-based systems. Results of inductive learning research are quite promising. Amongst the research areas of machine learning, "supervised learning is relatively well developed and well understood" <ref> [Shavlik and Dietterich, 1990, p. 1] </ref>. Many applications of supervised classifier learning have been tried successfully on a variety of problems (e.g., [Michalski and Chilausky, 1980b; Quinlan, 1983; Catlett, 1991c; Aha and Bankert, 1994; Michie et al., 1994]). Langley and Simon [1995] give a review of some fielded applications.
Reference: [Skalak, 1994] <author> D.B. Skalak, </author> <title> Prototype and feature selection by sampling and random mutation hill climbing algorithms. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 293-301. 231 </pages>
Reference-contexts: Some other 16 Split one interval into two intervals. 165 search approaches such as Swap [Indurkhya and Weiss, 1991] and Random Mutation hill climbing <ref> [Skalak, 1994] </ref> might be helpful for jumping off the local optimum to find a better solution. XofN with beam search has been tried.
Reference: [Snedecor and Cochran, 1989] <author> G.W. Snedecor and W.G. Cochran, </author> <title> Statistical Methods (eighth edition), </title> <institution> Iowa State University Press. </institution>
Reference-contexts: One solution is the pessimistic error estimate that adjusts the resubstitution error by increasing the number of errors observed at each leaf by 0.5 [Quinlan, 1987b]. This is the continuity correction for the binomial distribution <ref> [Snedecor and Cochran, 1989, pp. 117ff] </ref>. Pruning with this tree evaluation method is called pessimistic pruning [Quinlan, 1987b]. The recent releases of C4.5 use an even more pessimistic estimate. Suppose that a leaf covers I instances with E errors.
Reference: [Spackman, 1988] <author> K.A. Spackman, </author> <title> Learning categorical decision criteria in biomedical domains. </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 36-46. </pages>
Reference-contexts: The operator "M+1-of-N+1" specializes the hypothesis by adding an attribute-value pair and increasing M by one. With these two operators, Generate-Mof-N has the potential of constructing any Mof-N attribute that can be defined by a set of primitive attributes. It is worth mentioning that learning systems such as Crls <ref> [Spackman, 1988] </ref> and MoN [Ting, 1994] also construct Mof-N attributes, but they represent learned theories in the form of production rules (called Mof-N rules) rather than decision trees. <p> Crls gener ates a set of Mof-N rules directly from training data, while MoN constructs Mof-N rules 49 based on production rules generated by C4.5rules. Crls is shown to outperform standard rule learning in several medical domains <ref> [Spackman, 1988] </ref>. Ting [1991] demonstrates the performance advantage of MoN as well as ID2-of-3 over C4.5rules in terms of higher prediction accuracy and smaller theory size in a biology domain (Splice-junction). <p> The ten domains cover the spectrum of properties such as dataset size, attribute types and numbers, the number of different nominal attribute values, and the number of classes. In addition, Mof-N like concepts are expected to be found in some of these domains <ref> [Spackman, 1988] </ref>. The objective of using the test suite with this property is to test whether the algorithms capable of learning this kind of concept (discussed in Chapter 6) can work well in some real-world applications. 10 This domain is artificially generated, but it has a real-world background: Tic-Tac-Toe games. <p> Conjunctions and disjunctions are often used by humans to represent knowledge and appear in many real-world domains. Mof-N-like concepts are used in some real-world domains such as medical domains <ref> [Spackman, 1988] </ref>. Although nominal Xof-N representations are representationally powerful, as discussed in Subsection 6.3.2, they suffer from the fragmentation problem in domains that need many Xof-N representations whose few values are worth distinguishing for splitting examples of different classes. This might lead to a decline in the performance of XofN. <p> It has been found that XofN works quite well in the set of artificial and real-world domains under investigation. We expect that XofN can be applied to domains containing Mof-N concepts such as biomedical domains <ref> [Spackman, 1988] </ref> and linguistic domains 164 as well as domains containing parity concepts found for example in digital logic circuit design. To apply XofN to domains containing complex DNF concepts with long terms, some mechanisms are necessary to overcome the fragmentation problem. Two approaches presented are subsetting and subranging. <p> It constructs new binary attributes in the form of Mof-N representations, while XofN, XofN (c), and XofN (cc) construct Xof-N representations. When building a decision tree, all of them construct one new attribute for each decision node using the local training set. Instead of building decision trees, Crls <ref> [Spackman, 1988] </ref> and MoN [Ting, 1994] learn Mof-N rules. The symbolic theory revision system Neither [Baffes and Mooney, 1993] refines Mof-N rules.
Reference: [Stone, 1974] <author> M. Stone, </author> <title> Cross-validation choice and assessment of statistical predictions. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 36, </volume> <pages> 111-147. </pages>
Reference-contexts: However, the problem is that some available data must be reserved for pruning, so the original tree can only be built using a smaller subset. If the dataset is not large, this can lead to an inferior tree. The v-fold cross-validation method <ref> [Stone, 1974; Breiman et al., 1984] </ref> can help to mitigate this problem but with a drawback that v trees, instead of one tree, must be built. Another type of tree evaluation method is to use only the training set that is used to build the raw tree.
Reference: [Subutai and Tesauro, 1988] <author> A. Subutai and G. Tesauro, </author> <title> Scaling and generalization in neural networks: a case study. </title> <booktitle> Proceedings of IEEE-88 Conference on Neural Information Processing Systems Neural and Synthetic. </booktitle>
Reference-contexts: There are three reasons for choosing them. First, they cover a variety of well-studied artificial concepts in the machine learning community: randomly generated boolean concepts including DNF and CNF concepts, multiplexor concepts [Wilson, 1987], parity concepts, and majority concepts <ref> [Subutai and Tesauro, 1988] </ref>. Note that the majority concepts are a special kind of "at least Mof-N" concept. Second, all of them contain a large number of irrelevant attributes. By using them, we can see the ability of algorithms to tolerate irrelevant attributes.
Reference: [Thrun et al., 1991] <author> S.B. Thrun, J. Bala, E. Bloedorn, I. Bratko, B. Cestnik, J. Cheng, K. De Jong, S. Dzeroski, S.E. Fahlman, D. Fisher, R. Hamann, K. Kaufman, S. Keller, I. Kononenko, J. Kreuziger, R.S. Michalski, T. Mitchell, P. Pachowicz, Y. Reich, H. Vafaie, W. Van de Welde, W. Wenzel, J. Wnek, and J. Zhang, </author> <title> The MONK's problems a performance comparison of different learning algorithms. </title> <type> Technical Report CMU-CD-91-197, </type> <institution> Department of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: The majority functions with a large number of relevant attributes are hard to learn for decision tree learning algorithms because their DNF representations are large. 3.2.2 Monks domains The three Monks domains <ref> [Thrun et al., 1991] </ref> are also chosen because they are well-studied. There are published results for more than nineteen different learning algorithms in these domains. They represent three different types of learning task with binary and nominal attributes. <p> Monks1 and Monks3 have irrelevant attributes. In each of the three domains, the fixed training set (a subset of the whole dataset) and test set (the whole dataset) are given by the problem designers <ref> [Thrun et al., 1991] </ref>. The training set sizes are 124, 169, and 122 for Monks1, Monks2, and Monks3 respectively. The test set sizes are 432 for each of them. There is no noise in the test sets of the three domains. <p> The test set sizes are 432 for each of them. There is no noise in the test sets of the three domains. Only Monks3 has 5% classification noise in its training set. 7 6 In the original description of the Monks problems <ref> [Thrun et al., 1991] </ref>, the six attributes and their values have meaningful names. <p> The root and the decision node underneath the 136 root contain the same continuous-valued Xof-N attribute but with different cut points 0 and 2 respectively. SubtreeC contains some other continuous-valued Xof-Ns. Example 6.3 Monks2 problem The Monks2 problem <ref> [Thrun et al., 1991] </ref> is one of our experimental domains. Its description is in Subsection 3.2.2. The target concept, "exactly two of the six attributes have their first value", can be represented using a nominal Xof-N attribute as in Figure 6.9. <p> The UQ transformation of Cindi creates a numeric attribute from a boolean expression beginning with a universal quantifier. A generated UQ term calculates the percentage 21 Personal communication, 1995. 22 In Induce, it is called #v COND. 23 Generated rules have the form like (#VarEQ (1) 3) <ref> [Thrun et al., 1991, p. 11] </ref>. 181 of permutations of variable bindings that satisfy the boolean expression. New features (attributes) generated by Zenith consist of two components: a formula in the form of a conjunction or a disjunction of terms, and a variable list.
Reference: [Ting, 1994] <author> K.M. Ting, </author> <title> An M-of-N rule induction algorithm and its application to DNA domain. </title> <booktitle> Proceedings of the Twenty-seventh Annual Hawaii International Conference on System Sciences, Volume V: Biotechnology Computing, </booktitle> <address> Los Alamitos, CA: </address> <publisher> IEEE Computer Society Press, </publisher> <pages> 133-140. </pages>
Reference-contexts: With these two operators, Generate-Mof-N has the potential of constructing any Mof-N attribute that can be defined by a set of primitive attributes. It is worth mentioning that learning systems such as Crls [Spackman, 1988] and MoN <ref> [Ting, 1994] </ref> also construct Mof-N attributes, but they represent learned theories in the form of production rules (called Mof-N rules) rather than decision trees. Crls gener ates a set of Mof-N rules directly from training data, while MoN constructs Mof-N rules 49 based on production rules generated by C4.5rules. <p> When building a decision tree, all of them construct one new attribute for each decision node using the local training set. Instead of building decision trees, Crls [Spackman, 1988] and MoN <ref> [Ting, 1994] </ref> learn Mof-N rules. The symbolic theory revision system Neither [Baffes and Mooney, 1993] refines Mof-N rules.
Reference: [Ting, 1995] <author> K.M. Ting, </author> <title> Common Issues in Instance-Based and Naive Bayesian Classifiers, </title> <type> Ph.D. Thesis, </type> <institution> Basser Department of Computer Science, The University of Sydney. </institution>
Reference: [Towell et al., 1990] <author> G. Towell, J. Shavlik, and M. Noordewier, </author> <title> Refinement of approximate domain theories by knowledge-based artificial neural networks. </title> <booktitle> Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press/Cambridge, </publisher> <address> MA: </address> <publisher> MIT Press. </publisher>
Reference: [Towell et al., 1991] <author> G.G. Towell, M.W. Craven, and J.W. Shavlik, </author> <title> Constructive induction in knowledge-based neural networks. </title> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 213-217. 232 </pages>
Reference: [Utgoff, 1986] <author> P.E. Utgoff, </author> <title> Shift of bias of inductive concept learning. In R.S. </title> <editor> Michalski, J.G. Carbonell, and T.M. Mitchell (eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach (Vol. II), </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 107-148. </pages>
Reference: [Utgoff, 1988] <author> P.E. Utgoff, ID5: </author> <title> an incremental ID3. </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 107-120. </pages>
Reference-contexts: They require all the training examples to be available at the beginning of learning. In order to be able to update the learned decision tree quickly when new examples are available, some incremental algorithms have been developed, such as ID4 [Schlimmer and Fisher, 1986], ID5 <ref> [Utgoff, 1988] </ref>, ID5R [Utgoff, 1989], and ITI [Utgoff, 1994]. Like ID3, all of them use the information-based heuristic to select a test for each decision node and chi-square test for independence to prevent overfitting the training examples.
Reference: [Utgoff, 1989] <author> P.E. Utgoff, </author> <title> Incremental induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 4, </volume> <pages> 161-186. </pages>
Reference-contexts: They require all the training examples to be available at the beginning of learning. In order to be able to update the learned decision tree quickly when new examples are available, some incremental algorithms have been developed, such as ID4 [Schlimmer and Fisher, 1986], ID5 [Utgoff, 1988], ID5R <ref> [Utgoff, 1989] </ref>, and ITI [Utgoff, 1994]. Like ID3, all of them use the information-based heuristic to select a test for each decision node and chi-square test for independence to prevent overfitting the training examples. However, they accept one training example at a time and build a decision tree incrementally.
Reference: [Utgoff, 1994] <author> P.E. Utgoff, </author> <title> An improved algorithm for incremental induction of decision trees. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 318-325. </pages>
Reference-contexts: In order to be able to update the learned decision tree quickly when new examples are available, some incremental algorithms have been developed, such as ID4 [Schlimmer and Fisher, 1986], ID5 [Utgoff, 1988], ID5R [Utgoff, 1989], and ITI <ref> [Utgoff, 1994] </ref>. Like ID3, all of them use the information-based heuristic to select a test for each decision node and chi-square test for independence to prevent overfitting the training examples. However, they accept one training example at a time and build a decision tree incrementally. <p> Decision trees built by these incremental algorithms are similar to those learned by ID3. Using the same training set, ID5R builds the same decision tree as that generated by ID3. A later and more robust system, ITI <ref> [Utgoff, 1994] </ref>, has the same basic incremental decision tree learning algorithm as ID5R. However, it is able to deal with both nominal and continuous-valued attributes. Furthermore, it can handle multiple classes (more than two), missing values, and inconsistent examples.
Reference: [Valiant, 1984] <author> L.G. Valiant, </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27, </volume> <pages> 1134-1142. </pages>
Reference-contexts: It is reported to build more accurate and more concise trees than Fringe and Sym-Fringe (very similar to Symmetric Fringe as mentioned above) in concepts [Yang et al., 1991]. A concept is a concept that can be represented using a -expression in which each variable appears at most once <ref> [Valiant, 1984] </ref>. Now, let us use an example to illustrate how algorithms of the Fringe family construct new attributes from decision trees. Figure 2.4 (copied from Figure 1.2) gives a decision tree for a very simple CNF concept.
Reference: [Van de Merckt, 1993] <author> T. Van de Merckt, </author> <title> Decision trees in numerical attribute spaces. </title> <booktitle> Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 1016-1021. </pages>
Reference-contexts: At the moment, XofN uses cut points found by C4.5 to discretize primitive continuous-valued attributes. Other discretization methods that can be used include multi-interval discretization methods [Catlett, 1991a; Fayyad and Irani, 1993], supervised/unsupervised methods <ref> [Van de Merckt, 1993] </ref>, and an entropy method [Ragavan and Rendell, 1993]. Both Catlett [1991a] and Fayyad and Irani [1993] recursively apply a binary splitting procedure 16 to a continuous-valued attribute, but they use different stopping criteria.
Reference: [Van de Velde, 1990] <author> W. Van de Velde, </author> <title> Incremental induction of topologically minimal trees. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 66-74. </pages>
Reference-contexts: However, it is able to deal with both nominal and continuous-valued attributes. Furthermore, it can handle multiple classes (more than two), missing values, and inconsistent examples. Nominal attributes are dynamically encoded as sets of binary attributes, so ITI always generates binary trees. Another incremental decision tree generating algorithm IDL <ref> [Van de Velde, 1990] </ref> has its roots in ID4 and ID5, but it uses a statistical criterion for expanding nodes, and uses a tree topological criterion [Van de Velde, 1990] for transforming decision trees. 4 PLS1 is a complex system, a part of which corresponds to ID3; here I refer to <p> Nominal attributes are dynamically encoded as sets of binary attributes, so ITI always generates binary trees. Another incremental decision tree generating algorithm IDL <ref> [Van de Velde, 1990] </ref> has its roots in ID4 and ID5, but it uses a statistical criterion for expanding nodes, and uses a tree topological criterion [Van de Velde, 1990] for transforming decision trees. 4 PLS1 is a complex system, a part of which corresponds to ID3; here I refer to that part as PLS1. 27 C4.5-Tree (Att, D training ) INPUT: Att: a set of attributes, D training : a set of training examples represented using
Reference: [Vapnik and Chervonenkis, 1971] <author> V.N. Vapnik and A.Y. Chervonenkis, </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theor. Probab. Appl., </journal> <volume> 16, </volume> <pages> 264-280. </pages>
Reference-contexts: For each experiment, a training set and a test set are independently drawn from a uniform distribution. The size of the test sets is 2000. The sizes of training sets, given in Table 3.1, are the result of a VC dimension analysis <ref> [Vapnik and Chervonenkis, 1971; Blumer et al., 1987; Haussler, 1988] </ref> for finding the number of examples which would suffice for an ideal learning algorithm to create a consistent hypothesis with an error rate less than 10% on any test set (see Pagallo [1990] for details).
Reference: [Wallace and Patrick, 1993] <author> C.S. Wallace and J.D. Patrick, </author> <title> Coding decision trees. </title> <journal> Machine Learning, </journal> <volume> 11, </volume> <pages> 7-22. </pages>
Reference-contexts: Oliver et al. [1992] introduce a top-down scheme to build general decision graphs. The algorithm starts from a leaf. It carries out hill climbing search for growing a graph, and uses the Minimum Message Length (MML) principle <ref> [Wallace and Patrick, 1993] </ref> as the graph evaluation function. At each step, either to split a leaf or to join two leaves is performed depending on which operation has the greatest saving on the description length of the decision graph under construction.
Reference: [Waterman, 1986] <author> D.A. Waterman, </author> <title> A Guide to Expert Systems, </title> <address> Wokingham, UK: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Conventional knowledge acquisition is a painstaking process a series of intense, systematic interviews between knowledge engineers and domain experts to explicate the domain experts' heuristics <ref> [Feigenbaum and McCorduck, 1983; Waterman, 1986] </ref>. It is very common for domain experts to have difficulty fully expressing their knowledge or methods at an abstract level, but it is usually much easier for them to give examples of solving specific problems in their domains.
Reference: [Webb, 1992] <author> G.I. Webb, </author> <title> Man-machine collaboration for knowledge acquisition. </title> <booktitle> Proceedings of the Fifth Australian Joint Conference on Artificial Intelligence, </booktitle> <address> Singapore: </address> <publisher> World Scientific, </publisher> <pages> 329-334. 233 </pages>
Reference-contexts: Caf and Ccaf [Yip and Webb, 1994] explores the construction of canonical discriminant functions for both decision tree learning and rule learning. Canonical discriminant functions are generated first, and are transformed into new continuous-valued attributes. Then, the tree learning algorithm C4.5 or the rule learning algorithm Einstein <ref> [Webb, 1992] </ref> is invoked to learn a decision tree or a set of production rules. Similarly to Caf and Ccaf, Swap1 [Indurkhya and Weiss, 1991] can also train linear machines for rule learning.
Reference: [Webb, 1993] <author> G.I. Webb, </author> <title> Systematic search for categorical attribute-value data-driven machine learning. </title> <booktitle> Proceedings of the Sixth Australian Joint Conference on Artificial Intelligence, </booktitle> <address> Singapore: </address> <publisher> World Scientific, </publisher> <pages> 342-347. </pages>
Reference-contexts: From the point of view of constructing new attributes with relevant conditions, instead of generating production rules from a tree and then constructing new attributes from each rule, we can directly search through each path of the tree to find new attributes. A systematic search method with pruning <ref> [Webb, 1993] </ref> using an information-based evaluation function is a reasonable choice, since it can find the best new attribute from a path in terms of the evaluation function. This 15 idea forms the basis of the research reported in Chapter 5. <p> In addition, the Laplace function [Clark and Boswell, 1991] instead of the entropy function is used to evaluate rules. 37 The Class algorithm <ref> [Webb, 1993] </ref> uses the same covering method as the version of CN2 for unordered rules. However, Class invokes the Opus algorithm [Webb, 1993] to construct one rule by performing systematic search with pruning [Webb, 1993; Rymon, 1993; Schlimmer, 1993]. The rule evaluation function is also the Laplace function. <p> In addition, the Laplace function [Clark and Boswell, 1991] instead of the entropy function is used to evaluate rules. 37 The Class algorithm <ref> [Webb, 1993] </ref> uses the same covering method as the version of CN2 for unordered rules. However, Class invokes the Opus algorithm [Webb, 1993] to construct one rule by performing systematic search with pruning [Webb, 1993; Rymon, 1993; Schlimmer, 1993]. The rule evaluation function is also the Laplace function. Systematic search has the same result as exhaustive search. <p> However, Class invokes the Opus algorithm [Webb, 1993] to construct one rule by performing systematic search with pruning <ref> [Webb, 1993; Rymon, 1993; Schlimmer, 1993] </ref>. The rule evaluation function is also the Laplace function. Systematic search has the same result as exhaustive search. It can guarantee that the best rule can be found in terms of the evaluation function. <p> A part of search space consisting of some candidate rules can be pruned. This makes systematic search efficient in practice <ref> [Webb, 1993] </ref>. Swap1 [Indurkhya and Weiss, 1991] is also a covering rule learning algorithm. One important characteristic of it is the use of Swap search for constructing a single rule. <p> The conditions in a path used to construct a new attribute are selected dynamically. The number of conditions used to generate a new attribute can also be decided dynamically. Therefore, this approach is referred to as the dynamic path-based approach. A systematic search method with pruning <ref> [Webb, 1993] </ref> is employed. It considers all possible combinations of conditions in a path and is efficient in practice as some parts of the search space are eliminated during search. This method is due to Rymon [1992] and independently Webb [1993]. The implemented algorithm is called CAT. <p> A condition that can be added to a conjunction must have a higher order than existing conditions of the conjunction. For example, given four conditions and their order as: A, B, C, and D, Figure 5.1 shows a search tree. However, following the idea of Opus <ref> [Webb, 1993] </ref>, CAT uses an information-based evaluation function (different from that of Opus) to order the children of a node when generating them. The objective is to increase the size of the regions that can be pruned. <p> Some parts of the search space are pruned during search. 5 Adding ConjCond to CandNew reduces only its negative cover. 6 Some ideas are from Opus, which is used to learn unordered rules <ref> [Webb, 1993] </ref>. 115 Systematic-Search-Pruning (Path, D training ) INPUT: Path: all conditions in the path and its labeled class, D training : training set. OUTPUT: a conjunction as a new attribute. <p> Unlike CAT that uses paths of previously learned trees to restrict the new attribute search space, all these algorithms search over the space defined by all primitive attributes. As far as systematic search is concerned, the closest related work is Opus <ref> [Webb, 1993] </ref>. It carries out systematic search with pruning over the space of all possible disjuncts at the inner level of a covering rule learning algorithm, Class [Webb, 1993], which is similar to CN2 [Clark and Boswell, 1991]. <p> As far as systematic search is concerned, the closest related work is Opus <ref> [Webb, 1993] </ref>. It carries out systematic search with pruning over the space of all possible disjuncts at the inner level of a covering rule learning algorithm, Class [Webb, 1993], which is similar to CN2 [Clark and Boswell, 1991]. The method of systematic search with pruning used in CAT is very similar to that in Opus. The main difference is that CAT uses information gain as the evaluation function, while Opus employs the Laplace function [Webb, 1993]. <p> learning algorithm, Class <ref> [Webb, 1993] </ref>, which is similar to CN2 [Clark and Boswell, 1991]. The method of systematic search with pruning used in CAT is very similar to that in Opus. The main difference is that CAT uses information gain as the evaluation function, while Opus employs the Laplace function [Webb, 1993]. In addition, CAT searches for a conjunction as a new attribute for learning a decision tree, whereas Opus searches for a conjunction as a rule for learning a set of unordered rules.
Reference: [Weiss and Indurkhya, 1995] <author> S.M. Weiss and N. Indurkhya, </author> <title> Rule-based machine learning methods for function prediction. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 3, </volume> <pages> 383-403. </pages>
Reference: [Weiss and Kapouleas, 1989] <author> S.M. Weiss and I. Kapouleas, </author> <title> An empirical comparison of pattern recognition, neural nets, and machine learning classification methods. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufman, </publisher> <pages> 781-787. </pages>
Reference: [Weiss and Kulikowski, 1990] <author> S.M. Weiss and C.A. </author> <title> Kulikowski, Computer Systems that Learn, </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Wettschereck, 1994] <author> D. Wettschereck, </author> <title> A Study of Distance-Based Machine Learning Algorithms, </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer Science, Oregon State University, Corvallis, </institution> <address> OR. </address>
Reference: [Wilson, 1987] <author> S.W. Wilson, </author> <title> Classifier systems and the animat problem. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 199-228. </pages>
Reference-contexts: There are three reasons for choosing them. First, they cover a variety of well-studied artificial concepts in the machine learning community: randomly generated boolean concepts including DNF and CNF concepts, multiplexor concepts <ref> [Wilson, 1987] </ref>, parity concepts, and majority concepts [Subutai and Tesauro, 1988]. Note that the majority concepts are a special kind of "at least Mof-N" concept. Second, all of them contain a large number of irrelevant attributes. By using them, we can see the ability of algorithms to tolerate irrelevant attributes.
Reference: [Winston, 1992] <author> P.H. Winston, </author> <booktitle> Artificial Intelligence (3rd ed.), </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: At each decision node, the test is evaluated on the example, and the example goes to the branch corresponding to the outcome of the test. When the example reaches a leaf, it is asserted to belong to the class labeled by the leaf. A production rule <ref> [Winston, 1992] </ref> consists of two parts: antecedent and consequent. The antecedent is usually a conjunction of conditions.
Reference: [Wnek and Michalski, 1994] <author> J. Wnek and R.S. Michalski, </author> <title> Hypothesis-driven constructive induction in AQ17-hci: a method and experiments. </title> <journal> Machine Learning, </journal> <volume> 14, </volume> <pages> 139-168. </pages>
Reference-contexts: From this point of view, we can say that the new attributes are more representationally powerful than the primitive attributes. Existing constructive induction systems use different strategies for constructing new attributes. There are three primary strategies: the hypothesis-driven strategy, the data-driven strategy, and the knowledge-driven strategy <ref> [Wnek and Michalski, 1994] </ref>. Systems with the hypothesis-driven strategy analyze hypotheses learned by selective components to discover patterns. <p> Members of this group are algorithms such as Citre 12 Thanks to Thierry Van de Merckt for the suggestion of presenting the problem in this way. 13 [Matheus and Rendell, 1989], Fringe [Pagallo and Haussler, 1989] and AQ17-hci <ref> [Wnek and Michalski, 1994] </ref>. Systems with the data-driven strategy, such as ID2-of-3 [Murphy and Pazzani, 1991] and AQ17-dci [Bloedorn and Michalski, 1991], find relevant patterns from input data directly, while systems with the knowledge-driven strategy apply domain-knowledge to create new attributes. <p> The AQ17-hci, AQ17-dci, and AQ17-mci algorithms are three members of the AQ family. All of them use the A q algorithm to generate rules. Their novel characteristic compared with the other members of the AQ family is the capability to construct new 51 attributes. AQ17-hci <ref> [Wnek and Michalski, 1994] </ref> adopts the hypothesis-driven constructive strategy. It iterates two processes: rule learning and new attribute construction. In each iteration, rules are generated based on existing attributes, and then new attributes are created by analyzing the rules. <p> In each iteration, rules are generated based on existing attributes, and then new attributes are created by analyzing the rules. For each class, 17 a subset of rules (called an admissible ruleset) is selected using a rule evaluation function and a threshold <ref> [Wnek and Michalski, 1994] </ref>. The rule evaluation function is a weighted sum of the number of positive examples covered by a rule and the number of positive examples uniquely covered by the same rule. <p> The science discovery system Bacon [Langley et al., 1987a] and rule induction system Induce [Michalski, 1978] are two examples. Most hypothesis-driven constructive induction algorithms such as Fringe [Pagallo, 1990], Citre [Matheus and Rendell, 1989], CI [Zheng, 1992] (also see Chapter 4), CAT (see Chapter 5), and AQ17-hci <ref> [Wnek and Michalski, 1994] </ref> construct and select a set of new attributes based on the entire training set.
Reference: [Wolpert, 1992] <author> D.H. Wolpert, </author> <title> On the connection between in-sample testing and generalization error. </title> <journal> Complex Systems, </journal> <volume> 6, </volume> <pages> 47-94. </pages>
Reference-contexts: Attributes with a fixed number of ordered discrete values can be specified as either continuous-valued or nominal attributes. Learned theories are often evaluated using two measurements: generalization performance (accuracy) and theory complexity. Generalization performance is the expected prediction accuracy of a theory on unseen cases <ref> [Wolpert, 1992] </ref>. It is usually estimated using prediction accuracy. A theory learned from a training set is used to classify a test set from the same domain.
Reference: [Yang et al., 1991] <author> D. Yang, L. Rendell, and G. Blix, </author> <title> A scheme for feature construction and a comparison of empirical methods. </title> <booktitle> Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 699-704. </pages>
Reference-contexts: For positive leaves, SymFringe creates identical new attributes to Symmetric Fringe. For negative leaves, the new attributes created by SymFringe are equal to the negations of new attributes generated by Symmetric Fringe. SymFringe is also limited to problems with a binary class and binary attributes. Like Fringe, DCFringe <ref> [Yang et al., 1991] </ref> also constructs new attributes only from positive paths, but it makes use of more information about patterns near the fringe of a decision tree, and uses both conjunction and disjunction as constructive operators. 43 Table 2.1: New attributes constructed by some algorithms of the Fringe family Algorithm <p> It can be seen that these patterns are limited to logical concepts. DCFringe produces a subset of all new attributes generated by Symmetric Fringe. It is reported to build more accurate and more concise trees than Fringe and Sym-Fringe (very similar to Symmetric Fringe as mentioned above) in concepts <ref> [Yang et al., 1991] </ref>. A concept is a concept that can be represented using a -expression in which each variable appears at most once [Valiant, 1984]. Now, let us use an example to illustrate how algorithms of the Fringe family construct new attributes from decision trees. <p> They interleave a tree learning phase and a new attribute construction process. Algorithms of the Fringe family such as Fringe, Dual Fringe, Symmetric Fringe [Pagallo and Haussler, 1989; 1990; Pagallo, 1990], SymFringe, and DCFringe <ref> [Yang et al., 1991] </ref> behave similarly. Their main difference is that each algorithm uses a different method to generate new attributes. <p> than the univariate rules in some domains. 4.6 Related Work As mentioned before, the closest related work to the CI algorithms is Citre [Matheus and Rendell, 1989; Matheus, 1989] and the Fringe family of algorithms including Fringe, Dual Fringe, Symmetric Fringe [Pagallo and Haussler, 1989; 1990; Pagallo, 1990], SymFringe, DCFringe <ref> [Yang et al., 1991] </ref>, and SFringe (see Appendix B). The main difference is that the former use the rule-based approach to construct new attributes, while the latter use the path-based approach. <p> search does not always produce better results when solving learning problems. 5.6 Related Work As far as constructing new attributes for decision tree learning is concerned, related work includes the Fringe family of algorithms such as Fringe, Dual Fringe, Symmetric Fringe [Pagallo and Haussler, 1989; 1990; Pagallo, 1990], SymFringe, DCFringe <ref> [Yang et al., 1991] </ref>, and SFringe (see Appendix B), the Citre algorithm [Matheus and Rendell, 1989; Matheus, 1989], the CI algorithms [Zheng, 1992] (also see Chapter 4), the 124 LFC algorithm [Ragavan and Rendell, 1993], the Cart algorithm [Breiman et al., 1984], the ID2-of-3 algorithm [Murphy and Pazzani, 1991], and the <p> The real reason is that Fringe uses positive paths to create conjunctions and Dual Fringe uses negative paths to generate disjunctions. If Fringe is extended to construct conjunctions from both positive and negative paths, like SymFringe <ref> [Yang et al., 1991] </ref> and SFringe, it should be able to attack both DNF and CNF problems. There is no essential difference between conjunctions and disjunctions as constructive operators for decision tree learning at least in domains with binary attributes and two classes. <p> 46 x 48 x 58 _ x 4 x 26 x 38 x 52 _ x 6 x 11 x 36 x 55 _ x 6 x 9 x 10 x 39 x 46 _ 209 210 Appendix B The SFRINGE Algorithm B.1 Overview SFringe follows the idea of SymFringe <ref> [Yang et al., 1991] </ref> with a straightforward extension. For each leaf, it constructs one new attribute using the conjunction of two conditions at the parent and grandparent nodes of the leaf.
Reference: [Yip and Webb, 1994] <author> S.P. Yip and G.I. Webb, </author> <title> Incorporating canonical discriminant attributes in classification learning. </title> <booktitle> Proceedings of the Tenth Canadian Conference on Artificial Intelligence, </booktitle> <address> Vancouver, BC: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 63-70. </pages>
Reference-contexts: Bacon [Langley, Simon, Bradshaw, and Zytkow, 1987], Induce [Michalski, 1978]) or attribute counting attributes 14 (e.g. Induce, AQ17-dci, and AQ17-mci [Bloedorn et al., 1993]). In addition, systems such as Lmdt [Brodley and Utgoff, 1992], Swap1 [Indurkhya and Weiss, 1991], and Ccaf <ref> [Yip and Webb, 1994] </ref> construct linear machines, linear discriminant functions, or canonical discriminant functions as new attributes. Note that linear machines, as tests, have multiple values, one for each class. <p> Results in six real-world domains reported in Brodley and Utgoff [1992] show that Lmdt is significantly more accurate than C4.5 in two domains, and is significantly less accurate than C4.5 in two domains. Caf and Ccaf <ref> [Yip and Webb, 1994] </ref> explores the construction of canonical discriminant functions for both decision tree learning and rule learning. Canonical discriminant functions are generated first, and are transformed into new continuous-valued attributes. <p> They use conjunction, disjunction, negation, or Mof-N as constructive operators. Some systems generate new continuous-valued attributes using different constructive operators. For example, Bacon [Langley et al., 1987a] and Induce [Michalski, 1978] use mathematical operators such as multiplication and division. Ccaf <ref> [Yip and Webb, 1994] </ref> adopts canonical discriminant functions. Systems such as Induce, AQ17-dci, and AQ17-mci [Bloedorn et al., 1993] construct attribute counting attributes. Very few systems construct new nominal attributes, although Lmdt [Brodley and Utgoff, 1992] that creates linear machines and C4.5 that uses subset-ting are two such systems.
Reference: [Zhang, 1992] <author> P. Zhang, </author> <title> On the distributional properties of model selection criteria. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 87, </volume> <pages> 732-737. 234 </pages>
Reference: [Zheng, 1992] <author> Z. Zheng, </author> <title> Constructing conjunctive tests for decision trees. </title> <booktitle> Proceedings of the Fifth Australian Joint Conference on Artificial Intelligence, </booktitle> <address> Singapore: </address> <publisher> World Scientific, </publisher> <pages> 355-360. </pages>
Reference-contexts: decision tree learning is concerned, related work includes the Fringe family of algorithms such as Fringe, Dual Fringe, Symmetric Fringe [Pagallo and Haussler, 1989; 1990; Pagallo, 1990], SymFringe, DCFringe [Yang et al., 1991], and SFringe (see Appendix B), the Citre algorithm [Matheus and Rendell, 1989; Matheus, 1989], the CI algorithms <ref> [Zheng, 1992] </ref> (also see Chapter 4), the 124 LFC algorithm [Ragavan and Rendell, 1993], the Cart algorithm [Breiman et al., 1984], the ID2-of-3 algorithm [Murphy and Pazzani, 1991], and the Lmdt algorithm [Brodley and Utgoff, 1992]. <p> The science discovery system Bacon [Langley et al., 1987a] and rule induction system Induce [Michalski, 1978] are two examples. Most hypothesis-driven constructive induction algorithms such as Fringe [Pagallo, 1990], Citre [Matheus and Rendell, 1989], CI <ref> [Zheng, 1992] </ref> (also see Chapter 4), CAT (see Chapter 5), and AQ17-hci [Wnek and Michalski, 1994] construct and select a set of new attributes based on the entire training set.
Reference: [Zheng, 1995a] <author> Z. Zheng, </author> <title> Constructing nominal Xof-N attributes. </title> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> San Mateo, CA: </address> <publisher> Mor-gan Kaufmann, </publisher> <pages> 1064-1070. </pages>
Reference: [Zheng, 1995b] <author> Z. Zheng, </author> <title> Continuous-valued Xof-N attributes versus nominal Xof-N attributes for constructive induction: a case study. </title> <booktitle> Proceedings of the Fourth International Conference for Young Computer Scientists, </booktitle> <address> Beijing: </address> <publisher> Peking University Press, </publisher> <pages> 566-573. </pages>
References-found: 158

