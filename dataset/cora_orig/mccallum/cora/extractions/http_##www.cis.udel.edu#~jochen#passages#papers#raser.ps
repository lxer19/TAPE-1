URL: http://www.cis.udel.edu/~jochen/passages/papers/raser.ps
Refering-URL: http://www.cis.udel.edu/~jochen/passages/pubs.htm
Root-URL: http://www.cis.udel.edu
Email: norris@cis.udel.edu pollock@cis.udel.edu  
Phone: (302) 831-1953, fax (302) 831-8458  
Title: Register Allocation Sensitive Region Scheduling  
Author: Cindy Norris Lori. L. Pollock 
Address: 19716  
Affiliation: Department of Computer and Information Sciences University of Delaware Newark, DE,  
Abstract: Because of the interdependences between instruction scheduling and register allocation, it is not clear which of these two phases should run first. In this paper, we describe how we modified a global instruction scheduling technique to make it cooperate with a subsequent register allocation phase. In particular, our cooperative global instruction scheduler performs region scheduling transformations on the program dependence graph representation of a program while attempting to prevent an increase in the amount of spill code which will be introduced in the subsequent register allocation phase. Our experimental findings indicate that the cooperative technique does indeed produce more efficient code than noncooperative global instruction scheduling in programs in which an allocation can not be performed without the insertion of spill code. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aiken and A. Nicolau. </author> <title> A development environment for horizontal microcode. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 14(5) </volume> <pages> 584-594, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Although the global scheduling algorithm of Bernstein and Rodeh [5] also operates on the PDG, their approach only achieved modest improvements over local instruction scheduling. Other global scheduling techniques such as trace scheduling [16] and percolation scheduling <ref> [1] </ref> utilize the control flow graph rather than a program dependence graph to rearrange code.
Reference: [2] <author> V. H. Allan, J. Janardhan, R. M. Lee, and M. S-rinivas. </author> <title> Enhanced region scheduling on a program dependence graph. </title> <booktitle> In Proceedings of the twenty-fifth International Symposium on Microarchitec-ture, </booktitle> <pages> pages 72-80, </pages> <address> Portland, OR, </address> <year> 1992. </year>
Reference-contexts: be used by other phases of the compiler as the PDG has been used successfully as the basis for various scalar optimizations [14, 27, 4] as well as for detecting and improving parallelization for vector machines [30, 4], multiple processor machines [32, 3], and architectures that exhibit instruction level parallelism <ref> [20, 5, 2] </ref>. Although the global scheduling algorithm of Bernstein and Rodeh [5] also operates on the PDG, their approach only achieved modest improvements over local instruction scheduling.
Reference: [3] <author> F. E. Allen, M. Burke, P. Charles, R. Cytron, and J. Ferrante. </author> <title> An overview of the PTRAN analysis system for multiprocessing. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 617-640, </pages> <year> 1988. </year>
Reference-contexts: The same representation can be used by other phases of the compiler as the PDG has been used successfully as the basis for various scalar optimizations [14, 27, 4] as well as for detecting and improving parallelization for vector machines [30, 4], multiple processor machines <ref> [32, 3] </ref>, and architectures that exhibit instruction level parallelism [20, 5, 2]. Although the global scheduling algorithm of Bernstein and Rodeh [5] also operates on the PDG, their approach only achieved modest improvements over local instruction scheduling.
Reference: [4] <author> W. Baxter and H. R. Bauer, III. </author> <title> The program dependence graph and vectorization. </title> <booktitle> In Proceedings of the Sixteenth Annual ACM SIGAC-T/SIGPLAN Symposium on Principles of Programming Languages, </booktitle> <address> Austin, TX, </address> <year> 1989. </year>
Reference-contexts: The same representation can be used by other phases of the compiler as the PDG has been used successfully as the basis for various scalar optimizations <ref> [14, 27, 4] </ref> as well as for detecting and improving parallelization for vector machines [30, 4], multiple processor machines [32, 3], and architectures that exhibit instruction level parallelism [20, 5, 2]. <p> The same representation can be used by other phases of the compiler as the PDG has been used successfully as the basis for various scalar optimizations [14, 27, 4] as well as for detecting and improving parallelization for vector machines <ref> [30, 4] </ref>, multiple processor machines [32, 3], and architectures that exhibit instruction level parallelism [20, 5, 2]. Although the global scheduling algorithm of Bernstein and Rodeh [5] also operates on the PDG, their approach only achieved modest improvements over local instruction scheduling.
Reference: [5] <author> David Bernstein and Michael Rodeh. </author> <title> Global in-struction scheduling for superscalar machines. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <address> Toronto, CANADA, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: be used by other phases of the compiler as the PDG has been used successfully as the basis for various scalar optimizations [14, 27, 4] as well as for detecting and improving parallelization for vector machines [30, 4], multiple processor machines [32, 3], and architectures that exhibit instruction level parallelism <ref> [20, 5, 2] </ref>. Although the global scheduling algorithm of Bernstein and Rodeh [5] also operates on the PDG, their approach only achieved modest improvements over local instruction scheduling. <p> Although the global scheduling algorithm of Bernstein and Rodeh <ref> [5] </ref> also operates on the PDG, their approach only achieved modest improvements over local instruction scheduling. Other global scheduling techniques such as trace scheduling [16] and percolation scheduling [1] utilize the control flow graph rather than a program dependence graph to rearrange code.
Reference: [6] <author> David A. Berson, Rajiv Gupta, and Mary Lou Sof-fa. </author> <title> URSA: A unified resource allocator for registers and functional units in VLIW architectures. In IFIP Working Conference on Architectures and Compilation Techniques for Fine and Medium Grain Parallelism, </title> <address> Orlando, Florida, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: There have been several strategies developed to introduce some communication of requirements between the local scheduler and the register allocator such that the two phases can cooperate to generate better code <ref> [19, 8, 28, 25, 6] </ref>. Experimental results from these groups indicate that the cooperative schemes indeed generate more efficient code than a conventional code generator that treats register allocation and instruction scheduling in isolation. <p> A search is made for finding a suitable destination register for an operation that has been identified as movable; however, if a suitable register can not be found among the available registers, the next best movable operation is examined. Berson, Gupta and Soffa <ref> [6] </ref> developed a technique called URSA for allocating both functional units and registers in VLIW machines.
Reference: [7] <author> David A. Berson, Rajiv Gupta, and Mary Lou Sof-fa. </author> <title> Resource spackling: A framework for integrating register allocation in local and global sched-ulers. </title> <booktitle> In PACT `94: International Conference on Parallel Architectures and Compilation Techniques, </booktitle> <address> Montreal, Canada, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: The dependence DAG is modified to reflect scheduling and allocation decisions by adding sequence edges and load and store instructions to spill registers. URSA relies on a program trace to build the DAG thus making it mainly applicable to scientific applications. In a later paper <ref> [7] </ref>, Berson, Gupta and Soffa describe the application of this framework to the problem of integrating register allocation with local instruction scheduling, and then with global instruction scheduling.
Reference: [8] <author> David G. Bradlee, Susan J. Eggers, and Robert R. Henry. </author> <title> Integrating register allocation and instruction scheduling for RISCs. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 122-131, </pages> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: There have been several strategies developed to introduce some communication of requirements between the local scheduler and the register allocator such that the two phases can cooperate to generate better code <ref> [19, 8, 28, 25, 6] </ref>. Experimental results from these groups indicate that the cooperative schemes indeed generate more efficient code than a conventional code generator that treats register allocation and instruction scheduling in isolation. <p> The second FOR loop performs Integrated Prepass Scheduling (IPS) <ref> [19, 8] </ref> on each region. Instead of simply building the list of regions that need more parallelism, RASER also builds a list, max list, of regions in which the number of live variables in that region exceeds the number of physical registers in the target architecture. <p> Finally, RASER calls cleanup pdg to eliminate any completely unrolled loops or region nodes with no descendants. The remainder of this section describes the procedures of RASER in more detail. 5.1 Integrated Prepass Scheduling The procedure ips sched, based on IPS [19] and improved IPS <ref> [8] </ref>, performs local scheduling on each region while keeping track of the number of live variables.
Reference: [9] <author> Preston Briggs, Keith D. Cooper, and Linda Tor-czon. </author> <title> R n Programming Environment Newsletter #44. </title> <institution> Department of Computer Science, Rice University, </institution> <month> September </month> <year> 1987. </year>
Reference-contexts: The transformed PDGs are then used as input to pdg2i which generates the intermediate code, iloc. Iloc is a low-level intermediate code designed at Rice University for the development of optimizing compilers <ref> [9] </ref>. Local scheduling is performed in RASER by executing IPS on each region of the PDG. In order to design a fair comparison, we execut ed a local scheduler on the iloc code generated after performing region scheduling.
Reference: [10] <author> Preston Briggs, Keith D. Cooper, and Linda Torc-zon. </author> <title> Rematerialization. </title> <booktitle> In Proceedings of the SIG-PLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: Register allocation, which allocates registers to values fl This work was partially supported by NSF under grant CCR-9300212. so as to minimize the number of memory accesses at run-time <ref> [13, 12, 11, 29, 10, 21, 26] </ref>, often interferes with the goals of instruction scheduling, creating a dilemma for the compiler writer in deciding which order to perform these two important optimization phases. <p> Thus, our experimental study compared a compilation scheme consisting of region scheduling, basic block scheduling, and register allocation against RASER with IPS incorporated into it, followed by register allocation. Our implementation of the optimistic allocator <ref> [10] </ref> is used to perform register allocation on the iloc code. The iloc code produced is targeted for a hypothetical medium pipelined machine with pipelined functional units.
Reference: [11] <author> David Callahan and Brian Koblenz. </author> <title> Register allocation via hierarchical graph coloring. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 192-203, </pages> <address> Toronto, CANADA, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Register allocation, which allocates registers to values fl This work was partially supported by NSF under grant CCR-9300212. so as to minimize the number of memory accesses at run-time <ref> [13, 12, 11, 29, 10, 21, 26] </ref>, often interferes with the goals of instruction scheduling, creating a dilemma for the compiler writer in deciding which order to perform these two important optimization phases.
Reference: [12] <author> Gregory Chaitin, Marc Auslander, Ashok K. Chan-dra, John Cocke, Martin E. Hopkins, and Peter W. Markstein. </author> <title> Register allocation via coloring. </title> <journal> Computer Languages, </journal> <volume> 6 </volume> <pages> 47-57, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: Register allocation, which allocates registers to values fl This work was partially supported by NSF under grant CCR-9300212. so as to minimize the number of memory accesses at run-time <ref> [13, 12, 11, 29, 10, 21, 26] </ref>, often interferes with the goals of instruction scheduling, creating a dilemma for the compiler writer in deciding which order to perform these two important optimization phases.
Reference: [13] <author> Frederick Chow and John Hennessy. </author> <title> The priority-based coloring approach to register allocation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(4), </volume> <month> October </month> <year> 1990. </year>
Reference-contexts: Register allocation, which allocates registers to values fl This work was partially supported by NSF under grant CCR-9300212. so as to minimize the number of memory accesses at run-time <ref> [13, 12, 11, 29, 10, 21, 26] </ref>, often interferes with the goals of instruction scheduling, creating a dilemma for the compiler writer in deciding which order to perform these two important optimization phases.
Reference: [14] <author> Jeanne Ferrante, Karl J. Ottenstein, and Joe D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <year> 1987. </year>
Reference-contexts: The same representation can be used by other phases of the compiler as the PDG has been used successfully as the basis for various scalar optimizations <ref> [14, 27, 4] </ref> as well as for detecting and improving parallelization for vector machines [30, 4], multiple processor machines [32, 3], and architectures that exhibit instruction level parallelism [20, 5, 2]. <p> Thus far, no experimental results have been obtained to judge the effectiveness of the URSA approach. 3 Program Dependence Graphs The program dependence graph (PDG) <ref> [14] </ref> for a program is a directed graph that represents the relevant control and data dependences between statements in the program. The nodes of the graph are statements and predicate expressions that occur in the program. An edge represents either a control dependence or a data dependence among program components.
Reference: [15] <author> Claude-Nicolas Fiechter. </author> <title> PDG C Compiler. </title> <institution> University of Pittsburgh, </institution> <year> 1992. </year>
Reference-contexts: The front end of these two routines is the pdgcc compiler, developed at the University of Pittsburgh <ref> [15] </ref>, which accepts as input C source code and outputs the corresponding PDG. Our region scheduler accepts as input the PDG representation of the C program, performs region scheduling transformations and outputs the transformed PDG.
Reference: [16] <author> J. A. Fisher. </author> <title> Trace scheduling: A technique for global microcode compaction. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 30(7) </volume> <pages> 478-490, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: Although the global scheduling algorithm of Bernstein and Rodeh [5] also operates on the PDG, their approach only achieved modest improvements over local instruction scheduling. Other global scheduling techniques such as trace scheduling <ref> [16] </ref> and percolation scheduling [1] utilize the control flow graph rather than a program dependence graph to rearrange code.
Reference: [17] <author> S. M. Freudenberger and J. C. Ruttenberg. </author> <title> Phase ordering of register allocation and instruction scheduling. In Code Generation Concepts, Tools, Techniques: </title> <booktitle> Proceedings of the International Workshop on Code Generation, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: In this section, we focus on the prior work in conjunction with global instruction scheduling. Freudenberger <ref> [17] </ref> developed a technique for integrating register assignment with trace scheduling in the MultiFlow compilers. The scheduler drives the register assignment to assign the values in the heavily used traces to registers. The scheduler takes as many registers as it needs as it schedules the crucial traces first.
Reference: [18] <author> P. B. Gibbons and S. S. Muchnick. </author> <title> Efficient instruction scheduling for a pipelined architecture. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <month> June </month> <year> 1986. </year>
Reference-contexts: 1 Introduction In order to effectively exploit the fine-grained parallelism in pipelined, superscalar and VLIW machines, various strategies for carefully scheduling instructions at compile time have been developed <ref> [22, 33, 18, 31] </ref>. A compile-time scheduling phase increases run-time performance by rearranging the code to overlap the execution of low level machine instructions such as memory loads and stores, and integer and floating point operations to hide latencies and reduce possible run-time delays. <p> Local scheduling is performed in RASER by executing IPS on each region of the PDG. In order to design a fair comparison, we execut ed a local scheduler on the iloc code generated after performing region scheduling. The local scheduler is based on the Gibbons and Much-nick <ref> [18] </ref> basic block scheduling algorithm. Thus, our experimental study compared a compilation scheme consisting of region scheduling, basic block scheduling, and register allocation against RASER with IPS incorporated into it, followed by register allocation.
Reference: [19] <author> James R. Goodman and Wei-Chung Hsu. </author> <title> Code scheduling and register allocation in large basic blocks. </title> <booktitle> In Supercomputing '88 Proceedings, </booktitle> <pages> pages 442-452, </pages> <address> Orlando, Florida, </address> <month> November </month> <year> 1988. </year>
Reference-contexts: There have been several strategies developed to introduce some communication of requirements between the local scheduler and the register allocator such that the two phases can cooperate to generate better code <ref> [19, 8, 28, 25, 6] </ref>. Experimental results from these groups indicate that the cooperative schemes indeed generate more efficient code than a conventional code generator that treats register allocation and instruction scheduling in isolation. <p> The second FOR loop performs Integrated Prepass Scheduling (IPS) <ref> [19, 8] </ref> on each region. Instead of simply building the list of regions that need more parallelism, RASER also builds a list, max list, of regions in which the number of live variables in that region exceeds the number of physical registers in the target architecture. <p> Finally, RASER calls cleanup pdg to eliminate any completely unrolled loops or region nodes with no descendants. The remainder of this section describes the procedures of RASER in more detail. 5.1 Integrated Prepass Scheduling The procedure ips sched, based on IPS <ref> [19] </ref> and improved IPS [8], performs local scheduling on each region while keeping track of the number of live variables.
Reference: [20] <author> Rajiv Gupta and Mary Lou Soffa. </author> <title> Region scheduling: An approach for detecting and redistributing parallelism. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(4) </volume> <pages> 421-431, </pages> <year> 1990. </year>
Reference-contexts: This leads one to believe that similar increases in code quality could be attained by developing cooperative strategies between a global scheduler and the register allocator. In this paper, we describe a strategy for modifying a global instruction scheduling technique known as region scheduling <ref> [20, 23] </ref> to take into consideration the requirements of a subsequent register allocation phase. We report on our experimental findings based on our implementation and comparison of standard region scheduling and our Register Allocation Sensitive Region scheduling (RASER). <p> be used by other phases of the compiler as the PDG has been used successfully as the basis for various scalar optimizations [14, 27, 4] as well as for detecting and improving parallelization for vector machines [30, 4], multiple processor machines [32, 3], and architectures that exhibit instruction level parallelism <ref> [20, 5, 2] </ref>. Although the global scheduling algorithm of Bernstein and Rodeh [5] also operates on the PDG, their approach only achieved modest improvements over local instruction scheduling. <p> region node R2 represents the conditions on entering the loop or looping back to possibly execute another iteration of the loop, R3 represents the conditions under which the body of the loop is executed, R4 represents the THEN branch, and R5 represents the ELSE branch. 4 Region Scheduling Region scheduling <ref> [20] </ref> is a global instruction scheduling technique which operates on the PDG attempting to create regions of code in the procedure containing equal amounts of fine-grain parallelism. <p> The order of statement nodes in a region from left to right corresponds to the order of the source program. This enables the statements considered for motion to be examined in the desired order. This is a subset of the extensions proposed by <ref> [20] </ref>. <p> See <ref> [20] </ref> or [23] for a more detailed discussion on how this estimate can be calculated. Note that although not indicated in this algorithm, the estimate need not be recalculated for every region of the PDG after each transformation. <p> A region contains an excess amount of parallelism if the estimate of parallelism in the region is greater than the amount exploitable by the underlying architecture. Like <ref> [20] </ref>, the region scheduling transformations are applied in order of increasing difficulty. List i is the current region being examined for code movement because it was found to have insufficient parallelism.
Reference: [21] <author> Laurie J. Hendren, Guang R. Gao, Erik R. Alt-man, and Chandrika Mukerji. </author> <title> A register allocation framework based on hierarchical cyclic interval graphs. </title> <booktitle> In International Workshop on Compiler Construction, </booktitle> <address> Paderdorn, GERMANY, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Register allocation, which allocates registers to values fl This work was partially supported by NSF under grant CCR-9300212. so as to minimize the number of memory accesses at run-time <ref> [13, 12, 11, 29, 10, 21, 26] </ref>, often interferes with the goals of instruction scheduling, creating a dilemma for the compiler writer in deciding which order to perform these two important optimization phases.
Reference: [22] <author> J. L. Hennessy and Thomas Gross. </author> <title> Postpass code optimization of pipeline constraints. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(3) </volume> <pages> 422-448, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: 1 Introduction In order to effectively exploit the fine-grained parallelism in pipelined, superscalar and VLIW machines, various strategies for carefully scheduling instructions at compile time have been developed <ref> [22, 33, 18, 31] </ref>. A compile-time scheduling phase increases run-time performance by rearranging the code to overlap the execution of low level machine instructions such as memory loads and stores, and integer and floating point operations to hide latencies and reduce possible run-time delays.
Reference: [23] <author> Jayashree Janardhan. </author> <title> Enhanced region scheduling for instruction level parallelism. </title> <institution> Utah State University Master's Thesis, </institution> <year> 1992. </year>
Reference-contexts: This leads one to believe that similar increases in code quality could be attained by developing cooperative strategies between a global scheduler and the register allocator. In this paper, we describe a strategy for modifying a global instruction scheduling technique known as region scheduling <ref> [20, 23] </ref> to take into consideration the requirements of a subsequent register allocation phase. We report on our experimental findings based on our implementation and comparison of standard region scheduling and our Register Allocation Sensitive Region scheduling (RASER). <p> See [20] or <ref> [23] </ref> for a more detailed discussion on how this estimate can be calculated. Note that although not indicated in this algorithm, the estimate need not be recalculated for every region of the PDG after each transformation.
Reference: [24] <author> S-M Moon and K. Ebcioglu. </author> <title> An efficient resource-constrained global scheduling technique for superscalar and VLIW processors. </title> <booktitle> In Proceedings of the twenty-fifth International Symposium on Mi-croarchitecture, </booktitle> <address> Portland, OR, </address> <year> 1992. </year>
Reference-contexts: Information about the register assignment is stored at the entry and exit points of traces, and used to hook up less crucial traces to the earlier scheduled traces, to minimize the amount of data movement code. Moon and Ebcioglu <ref> [24] </ref> presented a resource-constrained code scheduling technique for VLIW and superscalar machines. The scheduling algorithm precomputes the set of available operations that are schedulable and can reach the root VLIW instruction.
Reference: [25] <author> Cindy Norris and Lori L. Pollock. </author> <title> A scheduler-sensitive global register allocator. </title> <booktitle> In Supercomputing '93 Proceedings, </booktitle> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: There have been several strategies developed to introduce some communication of requirements between the local scheduler and the register allocator such that the two phases can cooperate to generate better code <ref> [19, 8, 28, 25, 6] </ref>. Experimental results from these groups indicate that the cooperative schemes indeed generate more efficient code than a conventional code generator that treats register allocation and instruction scheduling in isolation. <p> Pdg2i may generate several iloc statements with dependences between them for each high level intermediate code in the PDG. Another pass of IPS on the basic blocks of the iloc code prior to register allocation or executing scheduler-sensitive global register allocation <ref> [25] </ref> would likely improve the performance of RASER. We found by isolating the effects of live value reduction and the sensitive transformations, that the live value reduction sometimes produces a decrease in the amount of spill code inserted by the subsequent register allocation phase and sometimes increases the amount.
Reference: [26] <author> Cindy Norris and Lori L. Pollock. </author> <title> Register allocation over the program dependence graph. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: Register allocation, which allocates registers to values fl This work was partially supported by NSF under grant CCR-9300212. so as to minimize the number of memory accesses at run-time <ref> [13, 12, 11, 29, 10, 21, 26] </ref>, often interferes with the goals of instruction scheduling, creating a dilemma for the compiler writer in deciding which order to perform these two important optimization phases.
Reference: [27] <author> K. J. Ottenstein. </author> <title> An intermediate program form based on a cyclic data-dependence graph. </title> <type> Technical Report 81-1, </type> <institution> Department of Computer Science, Michigan Tech. University, </institution> <year> 1981. </year>
Reference-contexts: The same representation can be used by other phases of the compiler as the PDG has been used successfully as the basis for various scalar optimizations <ref> [14, 27, 4] </ref> as well as for detecting and improving parallelization for vector machines [30, 4], multiple processor machines [32, 3], and architectures that exhibit instruction level parallelism [20, 5, 2].
Reference: [28] <author> S. S. Pinter. </author> <title> Register allocation with instruction scheduling: a new approach. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: There have been several strategies developed to introduce some communication of requirements between the local scheduler and the register allocator such that the two phases can cooperate to generate better code <ref> [19, 8, 28, 25, 6] </ref>. Experimental results from these groups indicate that the cooperative schemes indeed generate more efficient code than a conventional code generator that treats register allocation and instruction scheduling in isolation.
Reference: [29] <author> Todd A. Proebsting and Charles N. Fischer. </author> <title> Probabilistic register allocation. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 300-310, </pages> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Register allocation, which allocates registers to values fl This work was partially supported by NSF under grant CCR-9300212. so as to minimize the number of memory accesses at run-time <ref> [13, 12, 11, 29, 10, 21, 26] </ref>, often interferes with the goals of instruction scheduling, creating a dilemma for the compiler writer in deciding which order to perform these two important optimization phases.
Reference: [30] <author> J. Warren. </author> <title> A hierarchical basis for reordering transformations. </title> <booktitle> In Proceedings of the Eleventh Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 272-282, </pages> <year> 1984. </year>
Reference-contexts: The same representation can be used by other phases of the compiler as the PDG has been used successfully as the basis for various scalar optimizations [14, 27, 4] as well as for detecting and improving parallelization for vector machines <ref> [30, 4] </ref>, multiple processor machines [32, 3], and architectures that exhibit instruction level parallelism [20, 5, 2]. Although the global scheduling algorithm of Bernstein and Rodeh [5] also operates on the PDG, their approach only achieved modest improvements over local instruction scheduling.
Reference: [31] <author> S. Weiss and J. E. Smith. </author> <title> A study of scalar compilation techniques for pipelined supercomputers. </title> <booktitle> In Second International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1987. </year>
Reference-contexts: 1 Introduction In order to effectively exploit the fine-grained parallelism in pipelined, superscalar and VLIW machines, various strategies for carefully scheduling instructions at compile time have been developed <ref> [22, 33, 18, 31] </ref>. A compile-time scheduling phase increases run-time performance by rearranging the code to overlap the execution of low level machine instructions such as memory loads and stores, and integer and floating point operations to hide latencies and reduce possible run-time delays.
Reference: [32] <author> M. J. Wolfe. </author> <booktitle> Research Monographs in Parallel and Distributed Computing. </booktitle> <publisher> The MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: The same representation can be used by other phases of the compiler as the PDG has been used successfully as the basis for various scalar optimizations [14, 27, 4] as well as for detecting and improving parallelization for vector machines [30, 4], multiple processor machines <ref> [32, 3] </ref>, and architectures that exhibit instruction level parallelism [20, 5, 2]. Although the global scheduling algorithm of Bernstein and Rodeh [5] also operates on the PDG, their approach only achieved modest improvements over local instruction scheduling.
Reference: [33] <author> H. Young. </author> <title> Evaluation of a decoupled computer architecture and the design of a vector extension. </title> <note> Computer Sciences Technical Report 603, 21(4), </note> <month> July </month> <year> 1985. </year>
Reference-contexts: 1 Introduction In order to effectively exploit the fine-grained parallelism in pipelined, superscalar and VLIW machines, various strategies for carefully scheduling instructions at compile time have been developed <ref> [22, 33, 18, 31] </ref>. A compile-time scheduling phase increases run-time performance by rearranging the code to overlap the execution of low level machine instructions such as memory loads and stores, and integer and floating point operations to hide latencies and reduce possible run-time delays.
References-found: 33

