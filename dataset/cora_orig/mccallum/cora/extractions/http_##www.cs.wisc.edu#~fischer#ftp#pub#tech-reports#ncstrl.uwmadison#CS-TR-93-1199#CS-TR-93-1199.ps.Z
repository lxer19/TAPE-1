URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-93-1199/CS-TR-93-1199.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-93-1199/
Root-URL: http://www.cs.wisc.edu
Title: Specifying System Requirements for Memory Consistency Models  
Author: Kourosh Gharachorloo Sarita V. Adve Anoop Gupta John L. Hennessy and Mark D. Hill 
Date: #1199  
Address: Stanford, CA 94305  Madison, Wisconsin 53706  
Affiliation: Computer System Laboratory Stanford University  Computer Sciences Department University of Wisconsin  Stanford University  University of Wisconsin-Madison Computer Sciences  
Pubnum: Technical Report CSL-TR-93-594  Technical Report  
Abstract: The memory consistency model of a shared-memory system determines the order in which memory accesses can be executed by the system, and greatly affects the implementation and performance of the system. To aid system designers, memory models either directly specify, or are accompanied by, a set of low-level system conditions that can be easily translated into a correct implementation. These sufficient conditions play a key role in helping the designer determine the architecture and compiler optimizations that may be safely exploited under a specific model. Therefore, these conditions should obey three important properties. First, they should be unambiguous. Second, they should be feasibly aggressive; i.e., they should not prohibit practical optimizations that do not violate the semantics of the model. Third, it should be relatively straightforward to convert the conditions into efficient implementations, and conversely, to verify if an implementation obeys the conditions. Most previous approaches in specifying system requirements for a model are lacking in at least one of the above aspects. This paper presents a methodology for specifying the system conditions for a memory model that satisfies the above goals. A key attribute of our methodology is the exclusion of ordering constraints among memory operations to different locations by observing that such constraints are unnecessary for maintaining the semantics of a model. To demonstrate the flexibility of our approach, we specify the conditions for several proposed memory models within this framework. Compared to the original specification for each model, the new specification allows more optimizations without violating the original semantics and, in many cases, is more precise. 
Abstract-found: 1
Intro-found: 1
Reference: [ABM89] <author> Y. Afek, G. Brown, and M. Merritt. </author> <title> A lazy cache algorithm. </title> <booktitle> In Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 209-222, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: While the difference between the two sets of conditions is not discernable by the programmer, from a system design perspective, the aggressive conditions allow more flexibility with respect to optimizations that are possible. For example, more aggressive implementations of SC such as the lazy caching technique <ref> [ABM89] </ref> do not directly satisfy conditions in Figure 2, but can be shown to directly satisfy the aggressive conditions in Figure 4. We further explore such implications on system implementation in Section 3. 2.3.2 Requirements for Other Models The previous section presented system requirements specific to sequential consistency. <p> Below, we discuss implementations proposed in the literature that maintain the multiprocessor dependence condition more aggressively. We discuss the first implementation in some detail, and only give the key ideas for the others. The first implementation is the lazy caching technique proposed by Afek et al. <ref> [ABM89] </ref>. This implementation uses a bus-based update protocol to keep the caches coherent. Sequential consistency is maintained in an aggressive way as explained below.
Reference: [Adv93] <author> Sarita V. Adve. </author> <title> Designing Memory Consistency Models for Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Computer Sciences Technical Report #1198, University of Wisconsin - Madison, </institution> <month> December </month> <year> 1993. </year> <title> 25 Our abstraction is meant to be used by system designers and we don't expect the programmer to reason with this specification. We strongly believe programmers should reason with the high-level abstraction presented by programmer-centric models. </title> <type> 22 </type>
Reference-contexts: Our use of the rch ! relation for WO, RCsc, and RCpc is a straightforward adaptation of the condition developed for PLpc. The formal proof that this type of a condition is sufficient for providing sequentially consistent results for a general class of programmer-centric models will appear in <ref> [Adv93] </ref>.
Reference: [AF92] <author> Hagit Attiya and Roy Friedman. </author> <title> A correctness condition for high-performance multiprocessors. </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 679-690, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Another way of abstracting the system is to represent it in terms of execution histories [HW90] (see <ref> [AF92] </ref> for another example where this abstraction is used). Effectively, a history represents one processor's view of all memory operations or a combined view of different processors. This type of abstraction is in essence similar to Collier's abstraction and shares the same advantages and disadvantages.
Reference: [AGG + 93] <author> Sarita V. Adve, Kourosh Gharachorloo, Anoop Gupta, John L. Hennessy, and Mark D. Hill. </author> <title> Sufficient system requirements for supporting the PLpc memory model. </title> <type> Technical Report #1200, </type> <institution> Computer Sciences, University of Wisconsin - Madison, </institution> <month> December </month> <year> 1993. </year> <note> Also available as Stanford University Technical Report CSL-TR-93-595. </note>
Reference-contexts: spo sco spo atomic read-modify-write (AR,AW): if W conflicts with AR,AW, then either W sxo sxo Conditions on xo Initiation condition holds. sxo ! condition: if X sxo ! Y, then X (i) xo Termination condition holds for W rel sub-operations. sufficient system requirements for programmer-centric models such as PLpc <ref> [AGG + 93] </ref>. on the execution order are present among conflicting sub-operations only. 11 We use R acq and W rel to denote a read acquire and a write release, respectively, and R and W still denote any read or write, including acquires and releases. <p> More recently, we have jointly formalized an aggressive form of this condition as part of specifying sufficient conditions for PLpc <ref> [AGG + 93] </ref> and have proven that PLpc programs provide sequentially consistent results on models such as RCpc. Our use of the rch ! relation for WO, RCsc, and RCpc is a straightforward adaptation of the condition developed for PLpc. <p> order and is related to W by certain program order ( po !) arcs (e.g., spo !) that are used in the sxo ! relation. 14 The formal definition of the rch ! relation is provided in Appendix A and is a straightforward adaptation of the conditions developed for PLpc <ref> [AGG + 93] </ref>. This formulation represents the most aggressive condition developed so far and allows for several optimizations while disallowing the anomalous executions (e.g., speculative reads are allowed; writes that do not depend on the outcome of a branch are allowed to execute before the branch is resolved). <p> We provide the specification for several of the hardware-centric models in Appendix B. We have also used our framework for specifying the system requirements for programmer-centric models such as PLpc <ref> [AGG + 93] </ref>. Given the generality of our framework, it would be interesting to use it to specify the system requirements for other models that we have not discussed in this paper.
Reference: [AH90a] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Implementing sequential consistency in cache-based systems. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <pages> pages I: 47-50, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Collier proposes a similar implementation for ring-based systems where one node in the ring acts as the "root" that serializes all writes [Col92]. Adve and Hill <ref> [AH90a] </ref> also propose an implementation of SC for cache-based systems that in certain cases allows a processor that has issued a write to proceed as soon as the write is serialized (as opposed to delaying the processor for the write to take affect in all cache copies).
Reference: [AH90b] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak ordering Anew definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Programmer-centric models provide a higher level system abstraction to the programmer, thus relieving the programmer from directly reasoning with memory access optimizations. Instead, the programmer can reason with SC and is only required to provide certain information about memory accesses (e.g., which accesses may be involved in a race <ref> [AH90b, GLL + 90] </ref>). This information is in turn used to allow optimizations without violating SC. DRF0 [AH90b], DRF1 [AH93], PL [GLL + 90], and PLpc [GAG + 92] are example programmer-centric models that allow optimizations similar to the hardware-centric models. <p> Instead, the programmer can reason with SC and is only required to provide certain information about memory accesses (e.g., which accesses may be involved in a race [AH90b, GLL + 90]). This information is in turn used to allow optimizations without violating SC. DRF0 <ref> [AH90b] </ref>, DRF1 [AH93], PL [GLL + 90], and PLpc [GAG + 92] are example programmer-centric models that allow optimizations similar to the hardware-centric models. To correctly and efficiently implement a memory model, the system designer must identify the hardware and software optimizations that are allowed by that model. <p> On the other hand, such anomalous executions could occur with models such as RCsc, RCpc, and WO if the rch ! relation is not included in spo !. However, these models are intended to appear SC to a class of well-behaved programs <ref> [AH90b, GLL + 90, AH93, GAG + 92] </ref>, and executions such as the one described above violate this intent. Therefore, it is necessary to give precise conditions that would prohibit the above type of anomalous executions. <p> In most previous work, such conditions were either implicitly assumed or assumed to be imposed by informal descriptions such as "intra-processor dependencies are preserved" <ref> [AH90b] </ref> or "uniprocessor control and data dependences are respected" [GLL + 90]. Some proofs of correctness (e.g., proof of correctness for PL programs executing on the RCsc model [GLL + 90]) formalized certain aspects of this condition, but the full condition was never presented in precise terms. <p> While this does not directly satisfy the original conditions for RC [GLL + 90], it can be easily shown to satisfy the aggressive conditions for RC presented in Appendix B. Adve and Hill's implementations for the data-race-free-0 <ref> [AH90b] </ref> and data-race-free-1 [AH93] programmer-centric models also use an aggressive form of the multiprocessor dependence chain condition to provide more aggressive implementations than allowed by WO and RCsc.
Reference: [AH92] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Sufficient conditions for implementing the data-race-free-1 memory model. </title> <type> Technical Report #1107, </type> <institution> Computer Sciences, University of Wisconsin - Madison, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: This paper presents a framework for specifying sufficient system constraints for a memory model that meets the above criteria. Our framework extends previous work by Collier [Col92], Sindhu et al. [SFC91], and Adve and Hill <ref> [AH92] </ref>. The framework consists of two parts: an abstraction of a shared-memory system, and a specification methodology for system requirements based on that abstraction. <p> Our abstraction is influenced by the abstractions of Collier [Col92] and that of Sindhu et al. [SFC91, SUN91]. Some of the formalism and terminology are derived from work by Adve and Hill <ref> [AH92] </ref>. 2.1 Abstraction of the System processor has a complete copy of the shared (writable) memory, denoted as M i for P i . A read operation reads a specific memory location and a write operation modifies a location. <p> The first two features of a complete memory copy per processor and several atomic sub-operations for writes are based directly on the abstraction by Collier [Col92] and have been previously used as the basis for specifying system requirements for memory models (e.g., DRF1 <ref> [AH92] </ref>). Dubois et al.'s "perform with respect to" abstraction [DSB86, SD87], though different in flavor, also effectively captures these concepts. <p> The multiple sub-operations attributed to each write operation model the fact that updating multiple copies of a location may be non-atomic. Adve and Hill <ref> [AH92] </ref> explain the correspondence to real systems as follows (slightly paraphrased). "While there may be no distinct physical entity in a real system that corresponds to a certain sub-operation, a logically distinct sub-operation may be associated with every operation and a memory copy. <p> For this reason, we may often ignore the set I of the execution, and use only S and the program order relation on S to describe the execution. Our definition of execution is influenced by the work by Adve and Hill <ref> [AH92] </ref> and by Sindhu et al. [SFC91] and combines concepts from both. <p> As before, we denote the program order by po ! and the execution order by xo !. A condition like "X (i) xo ! Y (j) for all i,j" implicitly refers to pairs of values for i and j for which both X (i) and Y (j) are defined <ref> [AH92] </ref>. We also implicitly assume that X (i) and Y (j) appear in the execution for all such pairs. <p> Recent work by Adve and Hill specifies this condition more explicitly in the context of the DRF1 model <ref> [AH92] </ref> and proves that it is sufficient for ensuring sequentially consistent results to data-race-free programs on models such as WO and RCsc. <p> Therefore, none of the above methodologies expose the optimizations that become possible when only the order among conflicting operations is constrained. Adve and Hill's specification of sufficient conditions for satisfying DRF1 <ref> [AH92] </ref> is one of the few specifications that presents ordering restrictions among conflicting memory operations only. However, parts of these conditions are too general to be easily convertible to an implementation.
Reference: [AH93] <author> Sarita V. Adve and Mark D. Hill. </author> <title> A unified formalization of four shared-memory models. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(6) </volume> <pages> 613-624, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Instead, the programmer can reason with SC and is only required to provide certain information about memory accesses (e.g., which accesses may be involved in a race [AH90b, GLL + 90]). This information is in turn used to allow optimizations without violating SC. DRF0 [AH90b], DRF1 <ref> [AH93] </ref>, PL [GLL + 90], and PLpc [GAG + 92] are example programmer-centric models that allow optimizations similar to the hardware-centric models. To correctly and efficiently implement a memory model, the system designer must identify the hardware and software optimizations that are allowed by that model. <p> On the other hand, such anomalous executions could occur with models such as RCsc, RCpc, and WO if the rch ! relation is not included in spo !. However, these models are intended to appear SC to a class of well-behaved programs <ref> [AH90b, GLL + 90, AH93, GAG + 92] </ref>, and executions such as the one described above violate this intent. Therefore, it is necessary to give precise conditions that would prohibit the above type of anomalous executions. <p> While this does not directly satisfy the original conditions for RC [GLL + 90], it can be easily shown to satisfy the aggressive conditions for RC presented in Appendix B. Adve and Hill's implementations for the data-race-free-0 [AH90b] and data-race-free-1 <ref> [AH93] </ref> programmer-centric models also use an aggressive form of the multiprocessor dependence chain condition to provide more aggressive implementations than allowed by WO and RCsc.
Reference: [ASU86] <author> A. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference: [BZ91] <author> B. Bershad and M. Zekauskas. Midway: </author> <title> Shared memory parallel programming with entry consistency for distributed memory multiprocessors. </title> <type> Technical Report CMU-CS-91-170, </type> <institution> Carnegie-Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Zucker [Zuc92] makes a similar observation in the context of software cache coherence, and finally, similar observations have led to more efficient software distributed shared memory implementations <ref> [BZ91, KCZ92] </ref>. The implementations we have described above exploit the fact that maintaining the order among the intermediate operations in a multiprocessor dependence chain is not necessary for correctness. Our framework for specifying the system requirements for a model exposes this optimization. <p> The termination condition is especially relevant for systems that replicate data (e.g., through caching). Cache-based systems that use hardware protocols for coherence usually satisfy the termination condition for all sub-operations through the coherence protocol. Other systems such as Munin [CBZ91] and Midway <ref> [BZ91] </ref> which use software consistency management need to explicitly ensure that the required sub-operations are executed. Note here that the termination condition interacts in a subtle way with the uniprocessor correctness condition (Condition 1).
Reference: [CBZ91] <author> J. B. Carter, J. K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The termination condition is especially relevant for systems that replicate data (e.g., through caching). Cache-based systems that use hardware protocols for coherence usually satisfy the termination condition for all sub-operations through the coherence protocol. Other systems such as Munin <ref> [CBZ91] </ref> and Midway [BZ91] which use software consistency management need to explicitly ensure that the required sub-operations are executed. Note here that the termination condition interacts in a subtle way with the uniprocessor correctness condition (Condition 1).
Reference: [Col92] <author> William W. Collier. </author> <title> Reasoning about Parallel Architectures. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1992. </year>
Reference-contexts: This paper presents a framework for specifying sufficient system constraints for a memory model that meets the above criteria. Our framework extends previous work by Collier <ref> [Col92] </ref>, Sindhu et al. [SFC91], and Adve and Hill [AH92]. The framework consists of two parts: an abstraction of a shared-memory system, and a specification methodology for system requirements based on that abstraction. <p> We then formally define the notion of an execution and describe how conditions may be imposed on an execution in order to satisfy the semantics of various memory models. Our abstraction is influenced by the abstractions of Collier <ref> [Col92] </ref> and that of Sindhu et al. [SFC91, SUN91]. Some of the formalism and terminology are derived from work by Adve and Hill [AH92]. 2.1 Abstraction of the System processor has a complete copy of the shared (writable) memory, denoted as M i for P i . <p> The first two features of a complete memory copy per processor and several atomic sub-operations for writes are based directly on the abstraction by Collier <ref> [Col92] </ref> and have been previously used as the basis for specifying system requirements for memory models (e.g., DRF1 [AH92]). Dubois et al.'s "perform with respect to" abstraction [DSB86, SD87], though different in flavor, also effectively captures these concepts. <p> The above observation has been previously made by others as well. For example, Shasha and Snir [SS88] exploit a similar observation in identifying a minimal set of orders (derived from the program order) that are sufficient for achieving sequential consistency for a given program. Collier <ref> [Col92] </ref> also uses this observation for proving equivalences between different sets of ordering constraints. However, previous specifications of memory models have not exploited this observation to its full potential. <p> Collier proposes a similar implementation for ring-based systems where one node in the ring acts as the "root" that serializes all writes <ref> [Col92] </ref>. <p> abstraction is that it does not seem to be powerful enough to capture the optimization where the processor is allowed to read the value of its own write before the write takes effect in any memory copy (as discussed in Sections 2 and 3.1.1). 23 The abstractions proposed by Collier <ref> [Col92] </ref>, Sindhu et al. [SFC91], and Gibbons et al. [GMG91, GM92] were also referred to in Section 2. Collier's abstraction is formal and captures replication of data and the non-atomicity of writes. <p> This disallows the type of aggressive implementations discussed in Section 3.1.2. This same limitation exists with the specification by Sindhu et al. [SFC91] for TSO and PSO and the specification by Gibbons et al. [GMG91, GM92] for release consistency. Collier <ref> [Col92] </ref> does observe that two sets of conditions are indistinguishable if they maintain the same order among conflicting accesses, yet his methodology for specifying conditions constrains order among non-conflicting operations just like the other schemes.
Reference: [DSB86] <author> Michel Dubois, Christoph Scheurich, and Faye Briggs. </author> <title> Memory access buffering in multiprocessors. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 434-442, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Bell Laboratories, Cray Research Foundation and Digital Equipment Corporation. Sarita Adve is also supported by an IBM graduate fellowship. 1 consistency (SC) [Lam79]. While SC provides an intuitive model for the programmer, it restricts many architec-ture and compiler optimizations that exploit the reordering and overlap of memory accesses <ref> [DSB86, MPC89] </ref>. <p> This has led researchers to propose alternate models that allow more optimizations: e.g., processor consistency (PC) [GLL + 90], total store ordering (TSO) [SUN91], partial store ordering (PSO) [SUN91], weak ordering (WO) <ref> [DSB86] </ref>, and release consistency (RCsc/RCpc) [GLL + 90]. 1 These models are referred to as hardware-centric because they are defined in terms of relatively low-level hardware constraints on the ordering of memory accesses. <p> Dubois et al.'s "perform with respect to" abstraction <ref> [DSB86, SD87] </ref>, though different in flavor, also effectively captures these concepts. Providing each processor with a copy of memory serves to model the multiple copies of a datum that are present in real systems due to the replication and caching of shared data. <p> For the system abstraction, we are interested in whether it is flexible enough for expressing the desirable optimizations. For the specification methodology, we compare the approaches based on aggressiveness of the constraints. Dubois et al. <ref> [DSB86, SD87] </ref> present an abstraction based on the various stages that a memory request goes through and use this abstraction to present specifications for both sequential consistency and weak ordering. <p> We now consider the methods for specifying system requirements used with the above abstractions. Dubois et al.'s specification style <ref> [DSB86, SD87] </ref> leads to unnecessary constraints on memory ordering since it constrains the execution order among accesses to different locations in a similar way to the conservative conditions for SC in Figure 2. This disallows the type of aggressive implementations discussed in Section 3.1.2.
Reference: [DWB + 91] <author> M. Dubois, J.-C. Wang, L.A. Barrosso, K. Lee, and Y.-S. Chen. </author> <title> Delayed consistency and its effects on the miss rate of parallel programs. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 197-206, </pages> <year> 1991. </year>
Reference-contexts: Landin et al. extend their observations for SC on race-free networks to aggressive implementations of PC on such networks. Again, this implementation satisfies the aggressive conditions for PC 16 (provided in Appendix B). 19 Dubois et al.'s delayed consistency implementation of release consistency <ref> [DWB + 91] </ref> delays doing the invalidations at a destination cache until the destination processor reaches an acquire as opposed to performing the invalidations before the release on the source processor.
Reference: [FOW87] <author> Jeanne Ferrante, Karl J. Ottenstein, and Joe D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference: [GAG + 92] <author> Kourosh Gharachorloo, Sarita V. Adve, Anoop Gupta, John L. Hennessy, and Mark D. Hill. </author> <title> Programming for different memory consistency models. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15(4) </volume> <pages> 399-407, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: This information is in turn used to allow optimizations without violating SC. DRF0 [AH90b], DRF1 [AH93], PL [GLL + 90], and PLpc <ref> [GAG + 92] </ref> are example programmer-centric models that allow optimizations similar to the hardware-centric models. To correctly and efficiently implement a memory model, the system designer must identify the hardware and software optimizations that are allowed by that model. <p> However, consider the 11 For simplicity, we treat all read and write nsync operations [GLL + 90] as acquires and releases, respectively. Appendix B shows the conditions without this simplification. 12 This termination condition is sufficient for properly-labeled programs (PL [GLL + 90] and PLpc <ref> [GAG + 92] </ref>) where all races are identified. <p> On the other hand, such anomalous executions could occur with models such as RCsc, RCpc, and WO if the rch ! relation is not included in spo !. However, these models are intended to appear SC to a class of well-behaved programs <ref> [AH90b, GLL + 90, AH93, GAG + 92] </ref>, and executions such as the one described above violate this intent. Therefore, it is necessary to give precise conditions that would prohibit the above type of anomalous executions.
Reference: [GGH91] <author> Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> Performance evaluation of memory consistency models for shared-memory multiprocessors. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 245-257, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: While the above relaxed models provide substantial performance improvements over SC <ref> [GGH91, GGH92, ZB92] </ref>, they are difficult for programmers to reason with. To remedy this, another category of models, referred to as programmer-centric, have been proposed. Programmer-centric models provide a higher level system abstraction to the programmer, thus relieving the programmer from directly reasoning with memory access optimizations.
Reference: [GGH92] <author> Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> Hiding memory latency using dynamic scheduling in shared-memory multiprocessors. </title> <booktitle> In Proceeding of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 22-33, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: While the above relaxed models provide substantial performance improvements over SC <ref> [GGH91, GGH92, ZB92] </ref>, they are difficult for programmers to reason with. To remedy this, another category of models, referred to as programmer-centric, have been proposed. Programmer-centric models provide a higher level system abstraction to the programmer, thus relieving the programmer from directly reasoning with memory access optimizations.
Reference: [GGH93] <author> Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> Revision to "Memory consistency and event ordering in scalable shared-memory multiprocessors". </title> <type> Technical Report CSL-TR-93-568, </type> <institution> Stan-ford University, </institution> <month> April </month> <year> 1993. </year> <month> 23 </month>
Reference-contexts: The definitions for PC and RCsc/RCpc given in [GLL + 90] are modified in a minor way as explained in <ref> [GGH93] </ref>. 2 - P 1 i M n M W (j), j = 1..n R (i) R (i) response W (i) from other P's = 1 i n Network . . . . . . - 2 Our Framework This section describes our framework for specifying system requirements that are imposed <p> Compared to the original conditions for RCpc <ref> [GLL + 90, GGH93] </ref>, the conditions presented here are more complete (with respect to precisely specifying data and control dependences) and expose more aggressive opti 13 Similar to the uniprocessor correctness condition, the reach condition requires considering all instructions (not just memory instructions) to determine whether the relation holds between a <p> The implementation without coherence does not satisfy the semantics of PC as we have been using it and as defined in <ref> [GLL + 90, GGH93] </ref>. 17 3.2 Compiler Issues The previous section described the impact of the specified system requirements on the implementation of the architecture. These system requirements have an impact on the compiler as well. <p> However, parts of these conditions are too general to be easily convertible to an implementation. While they provide a second set of conditions that 23 See <ref> [GGH93] </ref> for a discussion on this limitation of Dubois et al.'s framework and an extension to it that allows this optimization to be expressed. 24 Gibbons et al.'s specification [GMG91, GM92] involves more events than we use in our specification and inherently depends on states and state transitions. 21 translates more
Reference: [GLL + 90] <author> Kourosh Gharachorloo, Dan Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hen--nessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: While SC provides an intuitive model for the programmer, it restricts many architec-ture and compiler optimizations that exploit the reordering and overlap of memory accesses [DSB86, MPC89]. This has led researchers to propose alternate models that allow more optimizations: e.g., processor consistency (PC) <ref> [GLL + 90] </ref>, total store ordering (TSO) [SUN91], partial store ordering (PSO) [SUN91], weak ordering (WO) [DSB86], and release consistency (RCsc/RCpc) [GLL + 90]. 1 These models are referred to as hardware-centric because they are defined in terms of relatively low-level hardware constraints on the ordering of memory accesses. <p> This has led researchers to propose alternate models that allow more optimizations: e.g., processor consistency (PC) <ref> [GLL + 90] </ref>, total store ordering (TSO) [SUN91], partial store ordering (PSO) [SUN91], weak ordering (WO) [DSB86], and release consistency (RCsc/RCpc) [GLL + 90]. 1 These models are referred to as hardware-centric because they are defined in terms of relatively low-level hardware constraints on the ordering of memory accesses. While the above relaxed models provide substantial performance improvements over SC [GGH91, GGH92, ZB92], they are difficult for programmers to reason with. <p> Programmer-centric models provide a higher level system abstraction to the programmer, thus relieving the programmer from directly reasoning with memory access optimizations. Instead, the programmer can reason with SC and is only required to provide certain information about memory accesses (e.g., which accesses may be involved in a race <ref> [AH90b, GLL + 90] </ref>). This information is in turn used to allow optimizations without violating SC. DRF0 [AH90b], DRF1 [AH93], PL [GLL + 90], and PLpc [GAG + 92] are example programmer-centric models that allow optimizations similar to the hardware-centric models. <p> Instead, the programmer can reason with SC and is only required to provide certain information about memory accesses (e.g., which accesses may be involved in a race [AH90b, GLL + 90]). This information is in turn used to allow optimizations without violating SC. DRF0 [AH90b], DRF1 [AH93], PL <ref> [GLL + 90] </ref>, and PLpc [GAG + 92] are example programmer-centric models that allow optimizations similar to the hardware-centric models. To correctly and efficiently implement a memory model, the system designer must identify the hardware and software optimizations that are allowed by that model. <p> The definitions for PC and RCsc/RCpc given in <ref> [GLL + 90] </ref> are modified in a minor way as explained in [GGH93]. 2 - P 1 i M n M W (j), j = 1..n R (i) R (i) response W (i) from other P's = 1 i n Network . . . . . . - 2 Our Framework <p> Therefore, given the read and write operations to Flag are labeled as release and acquire, respectively, the read of A will always return the value 1. However, consider the 11 For simplicity, we treat all read and write nsync operations <ref> [GLL + 90] </ref> as acquires and releases, respectively. Appendix B shows the conditions without this simplification. 12 This termination condition is sufficient for properly-labeled programs (PL [GLL + 90] and PLpc [GAG + 92]) where all races are identified. <p> However, consider the 11 For simplicity, we treat all read and write nsync operations <ref> [GLL + 90] </ref> as acquires and releases, respectively. Appendix B shows the conditions without this simplification. 12 This termination condition is sufficient for properly-labeled programs (PL [GLL + 90] and PLpc [GAG + 92]) where all races are identified. <p> On the other hand, such anomalous executions could occur with models such as RCsc, RCpc, and WO if the rch ! relation is not included in spo !. However, these models are intended to appear SC to a class of well-behaved programs <ref> [AH90b, GLL + 90, AH93, GAG + 92] </ref>, and executions such as the one described above violate this intent. Therefore, it is necessary to give precise conditions that would prohibit the above type of anomalous executions. <p> In most previous work, such conditions were either implicitly assumed or assumed to be imposed by informal descriptions such as "intra-processor dependencies are preserved" [AH90b] or "uniprocessor control and data dependences are respected" <ref> [GLL + 90] </ref>. Some proofs of correctness (e.g., proof of correctness for PL programs executing on the RCsc model [GLL + 90]) formalized certain aspects of this condition, but the full condition was never presented in precise terms. <p> previous work, such conditions were either implicitly assumed or assumed to be imposed by informal descriptions such as "intra-processor dependencies are preserved" [AH90b] or "uniprocessor control and data dependences are respected" <ref> [GLL + 90] </ref>. Some proofs of correctness (e.g., proof of correctness for PL programs executing on the RCsc model [GLL + 90]) formalized certain aspects of this condition, but the full condition was never presented in precise terms. <p> Compared to the original conditions for RCpc <ref> [GLL + 90, GGH93] </ref>, the conditions presented here are more complete (with respect to precisely specifying data and control dependences) and expose more aggressive opti 13 Similar to the uniprocessor correctness condition, the reach condition requires considering all instructions (not just memory instructions) to determine whether the relation holds between a <p> This is typically achieved through a logical serialization point per location where writes to that location are serialized (e.g., at the home directory in a directory based coherence protocol). Techniques for maintaining coherence are also well understood and have been covered in the literature (e.g., see <ref> [GLL + 90, LLG + 90] </ref> for a description of the Dash coherence protocol). Interaction between value, initiation, uniprocessor dependence, and coherence conditions There is a subtle interaction between the value, initiation, uniprocessor dependence, and coherence conditions when all of them are to be satisfied by operations to a location. <p> For consecutive operations related by spo !, we can maintain the program order such that A spo ensures A (i) xo ! B (j) for all i,j. This can be achieved by delaying sub-operations of B until all sub-operations of A are completed. ( <ref> [GLL + 90] </ref> gives a set of fence operations that provide one general way of achieving this delay; Appendix A further discusses the particular case when the delay is due to the rch ! component of spo The above is sufficient to maintain the required order for chains that do not <p> One way of making a write appear atomic is to ensure that no read is allowed to return the value of the write until all old copies of the line being written are either invalidated or updated with the new value <ref> [SD87, GLL + 90] </ref>. Aggressive implementations of multiprocessor dependence chain condition The primary reason our aggressive conditions lead to more flexible implementations is because we constrain the execution order only among conflicting accesses. This is most important for the specification of the orders implied by the multiprocessor dependence chain. <p> While this does not directly satisfy the original conditions for RC <ref> [GLL + 90] </ref>, it can be easily shown to satisfy the aggressive conditions for RC presented in Appendix B. <p> The implementation without coherence does not satisfy the semantics of PC as we have been using it and as defined in <ref> [GLL + 90, GGH93] </ref>. 17 3.2 Compiler Issues The previous section described the impact of the specified system requirements on the implementation of the architecture. These system requirements have an impact on the compiler as well.
Reference: [GM92] <author> Phillip B. Gibbons and Michael Merritt. </author> <title> Specifying nonblocking shared memories. </title> <booktitle> In Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 158-168, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: However, their abstraction is limited in two aspects: they assume the processor stalls on a read operation until a value is returned and they model writes with a single sub-operation. Thus, they do not model out-of-order reads and non-atomic writes. The abstraction used by Gibbons et al. <ref> [GMG91, GM92] </ref> to formalize the system requirements for properly-labeled (PL) programs and release consistency meets the criteria we have discussed above; i.e., it models the existence of multiple copies, the non-atomicity of writes, the out-of-order execution of memory operations, and allowing the processor to read its own write before the write <p> out-of-order execution of memory operations, and allowing the processor to read its own write before the write is issued to the memory system. (The M base abstraction in [GMG91] is limited because it does not model out-of-order read operations from the same processor; however, the non-blocking M base abstraction in <ref> [GM92] </ref> removes this restriction.) While their abstraction meets our goals, the methodology used in [GMG91, GM92] to specify system requirements is overconstraining and does not fully expose the range of possible optimizations. We will further discuss their abstraction and framework and contrast it to our approach in Section 4. <p> before the write is issued to the memory system. (The M base abstraction in [GMG91] is limited because it does not model out-of-order read operations from the same processor; however, the non-blocking M base abstraction in [GM92] removes this restriction.) While their abstraction meets our goals, the methodology used in <ref> [GMG91, GM92] </ref> to specify system requirements is overconstraining and does not fully expose the range of possible optimizations. We will further discuss their abstraction and framework and contrast it to our approach in Section 4. <p> powerful enough to capture the optimization where the processor is allowed to read the value of its own write before the write takes effect in any memory copy (as discussed in Sections 2 and 3.1.1). 23 The abstractions proposed by Collier [Col92], Sindhu et al. [SFC91], and Gibbons et al. <ref> [GMG91, GM92] </ref> were also referred to in Section 2. Collier's abstraction is formal and captures replication of data and the non-atomicity of writes. <p> Effectively, a history represents one processor's view of all memory operations or a combined view of different processors. This type of abstraction is in essence similar to Collier's abstraction and shares the same advantages and disadvantages. Gibbons et al.'s abstraction <ref> [GMG91, GM92] </ref> is the only one that captures replication and non-atomicity of writes and is flexible enough for specifying models such as TSO. <p> This disallows the type of aggressive implementations discussed in Section 3.1.2. This same limitation exists with the specification by Sindhu et al. [SFC91] for TSO and PSO and the specification by Gibbons et al. <ref> [GMG91, GM92] </ref> for release consistency. Collier [Col92] does observe that two sets of conditions are indistinguishable if they maintain the same order among conflicting accesses, yet his methodology for specifying conditions constrains order among non-conflicting operations just like the other schemes. <p> While they provide a second set of conditions that 23 See [GGH93] for a discussion on this limitation of Dubois et al.'s framework and an extension to it that allows this optimization to be expressed. 24 Gibbons et al.'s specification <ref> [GMG91, GM92] </ref> involves more events than we use in our specification and inherently depends on states and state transitions. 21 translates more easily to an implementation, this latter set of conditions are not as aggressive and restrict orders among operations to different locations.
Reference: [GMG91] <author> Phillip B. Gibbons, Michael Merritt, and Kourosh Gharachorloo. </author> <title> Proving sequential consistency of high-performance shared memories. </title> <booktitle> In Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 292-303, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: However, their abstraction is limited in two aspects: they assume the processor stalls on a read operation until a value is returned and they model writes with a single sub-operation. Thus, they do not model out-of-order reads and non-atomic writes. The abstraction used by Gibbons et al. <ref> [GMG91, GM92] </ref> to formalize the system requirements for properly-labeled (PL) programs and release consistency meets the criteria we have discussed above; i.e., it models the existence of multiple copies, the non-atomicity of writes, the out-of-order execution of memory operations, and allowing the processor to read its own write before the write <p> release consistency meets the criteria we have discussed above; i.e., it models the existence of multiple copies, the non-atomicity of writes, the out-of-order execution of memory operations, and allowing the processor to read its own write before the write is issued to the memory system. (The M base abstraction in <ref> [GMG91] </ref> is limited because it does not model out-of-order read operations from the same processor; however, the non-blocking M base abstraction in [GM92] removes this restriction.) While their abstraction meets our goals, the methodology used in [GMG91, GM92] to specify system requirements is overconstraining and does not fully expose the range <p> before the write is issued to the memory system. (The M base abstraction in [GMG91] is limited because it does not model out-of-order read operations from the same processor; however, the non-blocking M base abstraction in [GM92] removes this restriction.) While their abstraction meets our goals, the methodology used in <ref> [GMG91, GM92] </ref> to specify system requirements is overconstraining and does not fully expose the range of possible optimizations. We will further discuss their abstraction and framework and contrast it to our approach in Section 4. <p> powerful enough to capture the optimization where the processor is allowed to read the value of its own write before the write takes effect in any memory copy (as discussed in Sections 2 and 3.1.1). 23 The abstractions proposed by Collier [Col92], Sindhu et al. [SFC91], and Gibbons et al. <ref> [GMG91, GM92] </ref> were also referred to in Section 2. Collier's abstraction is formal and captures replication of data and the non-atomicity of writes. <p> Effectively, a history represents one processor's view of all memory operations or a combined view of different processors. This type of abstraction is in essence similar to Collier's abstraction and shares the same advantages and disadvantages. Gibbons et al.'s abstraction <ref> [GMG91, GM92] </ref> is the only one that captures replication and non-atomicity of writes and is flexible enough for specifying models such as TSO. <p> This disallows the type of aggressive implementations discussed in Section 3.1.2. This same limitation exists with the specification by Sindhu et al. [SFC91] for TSO and PSO and the specification by Gibbons et al. <ref> [GMG91, GM92] </ref> for release consistency. Collier [Col92] does observe that two sets of conditions are indistinguishable if they maintain the same order among conflicting accesses, yet his methodology for specifying conditions constrains order among non-conflicting operations just like the other schemes. <p> While they provide a second set of conditions that 23 See [GGH93] for a discussion on this limitation of Dubois et al.'s framework and an extension to it that allows this optimization to be expressed. 24 Gibbons et al.'s specification <ref> [GMG91, GM92] </ref> involves more events than we use in our specification and inherently depends on states and state transitions. 21 translates more easily to an implementation, this latter set of conditions are not as aggressive and restrict orders among operations to different locations.
Reference: [Goo91] <author> James R. Goodman. </author> <title> Cache consistency and sequential consistency. </title> <type> Technical Report Computer Sciences #1006, </type> <institution> University of Wisconsin, Madison, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: The detailed specification for several models with the corresponding correctness proofs are provided in Appendices B and C; Appendices A and D provide some of the other conditions that are required for correctness. 1 The processor consistency model considered in this paper is different from that proposed by Goodman <ref> [Goo91] </ref>.
Reference: [HW90] <author> M. P. Herlihy and J. M. Wing. </author> <title> Linearizability: A correctness condition for concurrent objects. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(3) </volume> <pages> 463-492, </pages> <year> 1990. </year>
Reference-contexts: Note here that in forming I' and S', we should not include speculative instructions or sub-operations that may have been generated but are not yet committed in the system. 9 This would not be possible for models such as linearizability <ref> [HW90] </ref> where correctness depends on the real time occurrence of events. However, for the models we have been discussing, the real time occurrence of events is assumed to be unobservable. 7 define sxo sxo 1. <p> Considering other abstractions, Sindhu et al.'s abstraction [SFC91] was designed to describe TSO and PSO and therefore handles the optimization described above, but is not flexible enough to capture the non-atomicity of writes. Another way of abstracting the system is to represent it in terms of execution histories <ref> [HW90] </ref> (see [AF92] for another example where this abstraction is used). Effectively, a history represents one processor's view of all memory operations or a combined view of different processors. This type of abstraction is in essence similar to Collier's abstraction and shares the same advantages and disadvantages.
Reference: [KCZ92] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Zucker [Zuc92] makes a similar observation in the context of software cache coherence, and finally, similar observations have led to more efficient software distributed shared memory implementations <ref> [BZ91, KCZ92] </ref>. The implementations we have described above exploit the fact that maintaining the order among the intermediate operations in a multiprocessor dependence chain is not necessary for correctness. Our framework for specifying the system requirements for a model exposes this optimization.
Reference: [Lam79] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: The University of Wisconsin authors are supported in part by a National Science Foundation Presidential Young Investigator Award (MIPS-8957278) with matching funds from A.T. & T. Bell Laboratories, Cray Research Foundation and Digital Equipment Corporation. Sarita Adve is also supported by an IBM graduate fellowship. 1 consistency (SC) <ref> [Lam79] </ref>. While SC provides an intuitive model for the programmer, it restricts many architec-ture and compiler optimizations that exploit the reordering and overlap of memory accesses [DSB86, MPC89]. <p> Since atomic read-modify-write operations (e.g., test-and-set and fetch-and-increment) are common in shared-memory machines, we include them in our framework as well. Given an atomic read-modify-write operation, RMW denotes the operation itself and AR and AW denote the individual read and write operations, respectively. We assume AR po Lamport <ref> [Lam79] </ref> defines sequential consistency as follows: A system is sequentially consistent "if the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its
Reference: [LHH91] <author> A. Landin, E. Hagersten, and S. Haridi. </author> <title> Race-free interconnection networks and multiprocessor consistency. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 27-30, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: A number of other aggressive implementations have been proposed for SC. Scheurich proposes a similar idea to lazy caching in his thesis, but his description of the implementation is very informal [Sch89]. Landin et al. <ref> [LHH91] </ref> propose an implementation of SC based on "race-free networks." Such networks maintain certain transactions in order, and this inherent order allows a processor to continue issuing operations as soon as its previous write reaches the appropriate "root" in the network (as opposed to waiting until the write reaches its destination
Reference: [LLG + 90] <author> Dan Lenoski, James Laudon, Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: This is typically achieved through a logical serialization point per location where writes to that location are serialized (e.g., at the home directory in a directory based coherence protocol). Techniques for maintaining coherence are also well understood and have been covered in the literature (e.g., see <ref> [GLL + 90, LLG + 90] </ref> for a description of the Dash coherence protocol). Interaction between value, initiation, uniprocessor dependence, and coherence conditions There is a subtle interaction between the value, initiation, uniprocessor dependence, and coherence conditions when all of them are to be satisfied by operations to a location. <p> Some systems like DASH <ref> [LLG + 90] </ref> use a hybrid scheme that is a merge of the two techniques discussed above.
Reference: [MPC89] <author> Samuel Midkiff, David Padua, and Ron Cytron. </author> <title> Compiling programs with user parallelism. </title> <booktitle> In Proceedings of the Second Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1989. </year>
Reference-contexts: Bell Laboratories, Cray Research Foundation and Digital Equipment Corporation. Sarita Adve is also supported by an IBM graduate fellowship. 1 consistency (SC) [Lam79]. While SC provides an intuitive model for the programmer, it restricts many architec-ture and compiler optimizations that exploit the reordering and overlap of memory accesses <ref> [DSB86, MPC89] </ref>.
Reference: [PBG + 85] <author> G. F. Pfister, W. C. Brantley, D. A. George, S. L. Harvey, W. J. Kleinfelder, K. P. McAuliffe, E. A. Melton, V. A. Norton, and J. Weiss. </author> <title> The IBM research parallel processor prototype (RP3): Introduction and architecture. </title> <booktitle> In Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <pages> pages 764-771, </pages> <year> 1985. </year>
Reference-contexts: For a similar reason, the coherence condition may not be relevant to the compiler. (An exception is a system that implements software-based cache coherence using instructions such as cache invalidate (as in the RP3 <ref> [PBG + 85] </ref>).) Below, we discuss the impact of the other conditions that are relevant to the compiler. We first discuss how to conceptually reason about optimizations such as register allocation.
Reference: [Sch89] <author> Christoph Scheurich. </author> <title> Access Ordering and Coherence in Shared Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Southern California, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: A number of other aggressive implementations have been proposed for SC. Scheurich proposes a similar idea to lazy caching in his thesis, but his description of the implementation is very informal <ref> [Sch89] </ref>.
Reference: [SD87] <author> C. Scheurich and M. Dubois. </author> <title> Correct memory operation of cache-based multiprocessors. </title> <booktitle> In Proceedings of the 14th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 234-243, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: Dubois et al.'s "perform with respect to" abstraction <ref> [DSB86, SD87] </ref>, though different in flavor, also effectively captures these concepts. Providing each processor with a copy of memory serves to model the multiple copies of a datum that are present in real systems due to the replication and caching of shared data. <p> While this definition specifies the possible outcomes for a program, it does not directly translate into an implementation of SC. Therefore, to aid designers, we need to specify a set of conditions that more closely relate to an implementation. For example, Dubois et al. <ref> [SD87] </ref> have shown that to satisfy SC, it is sufficient to maintain program order and make writes appear atomic in an implementation. <p> One way of making a write appear atomic is to ensure that no read is allowed to return the value of the write until all old copies of the line being written are either invalidated or updated with the new value <ref> [SD87, GLL + 90] </ref>. Aggressive implementations of multiprocessor dependence chain condition The primary reason our aggressive conditions lead to more flexible implementations is because we constrain the execution order only among conflicting accesses. This is most important for the specification of the orders implied by the multiprocessor dependence chain. <p> Therefore, the implementation does not directly satisfy the conservative conditions for SC specified in Figure 2 or the conditions specified by Dubois et al. <ref> [SD87] </ref>, since X po ! Y does not necessarily imply X (i) xo ! Y (j) for all i,j. <p> For the system abstraction, we are interested in whether it is flexible enough for expressing the desirable optimizations. For the specification methodology, we compare the approaches based on aggressiveness of the constraints. Dubois et al. <ref> [DSB86, SD87] </ref> present an abstraction based on the various stages that a memory request goes through and use this abstraction to present specifications for both sequential consistency and weak ordering. <p> We now consider the methods for specifying system requirements used with the above abstractions. Dubois et al.'s specification style <ref> [DSB86, SD87] </ref> leads to unnecessary constraints on memory ordering since it constrains the execution order among accesses to different locations in a similar way to the conservative conditions for SC in Figure 2. This disallows the type of aggressive implementations discussed in Section 3.1.2.
Reference: [SFC91] <author> P. S. Sindhu, J.-M. Frailong, and M. Cekleov. </author> <title> Formal specification of memory models. </title> <type> Technical Report (PARC) CSL-91-11, </type> <institution> Xerox Corporation, Palo Alto Research Center, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: This paper presents a framework for specifying sufficient system constraints for a memory model that meets the above criteria. Our framework extends previous work by Collier [Col92], Sindhu et al. <ref> [SFC91] </ref>, and Adve and Hill [AH92]. The framework consists of two parts: an abstraction of a shared-memory system, and a specification methodology for system requirements based on that abstraction. <p> We then formally define the notion of an execution and describe how conditions may be imposed on an execution in order to satisfy the semantics of various memory models. Our abstraction is influenced by the abstractions of Collier [Col92] and that of Sindhu et al. <ref> [SFC91, SUN91] </ref>. Some of the formalism and terminology are derived from work by Adve and Hill [AH92]. 2.1 Abstraction of the System processor has a complete copy of the shared (writable) memory, denoted as M i for P i . <p> scenario can occur in a cache-coherent multiprocessor if a processor does a write to a location that requires exclusive ownership to be requested and allows a subsequent read (issued by itself) to that location to return the new value while the ownership request is pending. 4 Sindhu et al.'s abstraction <ref> [SFC91] </ref> models this effect through a conceptual write buffer and allowing the read to return the value of a write before it is retired from this buffer. <p> For this reason, we may often ignore the set I of the execution, and use only S and the program order relation on S to describe the execution. Our definition of execution is influenced by the work by Adve and Hill [AH92] and by Sindhu et al. <ref> [SFC91] </ref> and combines concepts from both. Adve and Hill define an execution similar to the above except that they do not allow a read, R (i), to return the value of a write whose sub-operation, W (i), occurs after the read in the execution order. <p> does not seem to be powerful enough to capture the optimization where the processor is allowed to read the value of its own write before the write takes effect in any memory copy (as discussed in Sections 2 and 3.1.1). 23 The abstractions proposed by Collier [Col92], Sindhu et al. <ref> [SFC91] </ref>, and Gibbons et al. [GMG91, GM92] were also referred to in Section 2. Collier's abstraction is formal and captures replication of data and the non-atomicity of writes. <p> However, these two notions are important for properly capturing the mentioned optimization. Considering other abstractions, Sindhu et al.'s abstraction <ref> [SFC91] </ref> was designed to describe TSO and PSO and therefore handles the optimization described above, but is not flexible enough to capture the non-atomicity of writes. <p> This disallows the type of aggressive implementations discussed in Section 3.1.2. This same limitation exists with the specification by Sindhu et al. <ref> [SFC91] </ref> for TSO and PSO and the specification by Gibbons et al. [GMG91, GM92] for release consistency.
Reference: [SS88] <author> Dennis Shasha and Marc Snir. </author> <title> Efficient and correct execution of parallel programs that share memory. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(2) </volume> <pages> 282-312, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: The program order relation is a partial order on the instruction instances, where the partial order is total for the instruction instances issued by the same processor and is determined by the program text for that processor <ref> [SS88] </ref>. In addition, the execution consists of a set of shared-memory sub-operations that correspond to the instruction instances that access shared-memory. Note that read and write sub-operations to private memory are not included in this set. The notion of an execution is formalized in Definition 1. <p> We then present specific system constraints for satisfying sequential consistency and other more relaxed memory models. We use the following terminology below. We denote program order by po !. Two operations (or sub-operations) conflict if both are to the same location and at least one is a write <ref> [SS88] </ref>. We use R (or Ri, Rj, Rk, etc.) and W (or Wi, Wj, Wk, etc.) to denote any read and write operations respectively. RW denotes either a read or a write. For RCpc, we use R acq and W rel to denote acquire and release operations, respectively. <p> The above observation has been previously made by others as well. For example, Shasha and Snir <ref> [SS88] </ref> exploit a similar observation in identifying a minimal set of orders (derived from the program order) that are sufficient for achieving sequential consistency for a given program. Collier [Col92] also uses this observation for proving equivalences between different sets of ordering constraints. <p> As mentioned before, two operations conflict if both are to the same location and at least one is a write <ref> [SS88] </ref>. Two sub-operations conflict if their corresponding operations conflict with one another. The conflict order relation (denoted co ! ) is defined as follows. For an execution order xo ! and two conflicting operations X and Y, X co xo ! Y (i) holds for some i.
Reference: [SUN91] <institution> The SPARC Architecture Manual. Sun Microsystems Inc., </institution> <month> January </month> <year> 1991. </year> <note> No. 800-199-12, Version 8. </note>
Reference-contexts: This has led researchers to propose alternate models that allow more optimizations: e.g., processor consistency (PC) [GLL + 90], total store ordering (TSO) <ref> [SUN91] </ref>, partial store ordering (PSO) [SUN91], weak ordering (WO) [DSB86], and release consistency (RCsc/RCpc) [GLL + 90]. 1 These models are referred to as hardware-centric because they are defined in terms of relatively low-level hardware constraints on the ordering of memory accesses. <p> This has led researchers to propose alternate models that allow more optimizations: e.g., processor consistency (PC) [GLL + 90], total store ordering (TSO) <ref> [SUN91] </ref>, partial store ordering (PSO) [SUN91], weak ordering (WO) [DSB86], and release consistency (RCsc/RCpc) [GLL + 90]. 1 These models are referred to as hardware-centric because they are defined in terms of relatively low-level hardware constraints on the ordering of memory accesses. <p> We then formally define the notion of an execution and describe how conditions may be imposed on an execution in order to satisfy the semantics of various memory models. Our abstraction is influenced by the abstractions of Collier [Col92] and that of Sindhu et al. <ref> [SFC91, SUN91] </ref>. Some of the formalism and terminology are derived from work by Adve and Hill [AH92]. 2.1 Abstraction of the System processor has a complete copy of the shared (writable) memory, denoted as M i for P i .
Reference: [ZB92] <author> Richard N. Zucker and Jean-Loup Baer. </author> <title> A performance study of memory consistency models. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-12, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: While the above relaxed models provide substantial performance improvements over SC <ref> [GGH91, GGH92, ZB92] </ref>, they are difficult for programmers to reason with. To remedy this, another category of models, referred to as programmer-centric, have been proposed. Programmer-centric models provide a higher level system abstraction to the programmer, thus relieving the programmer from directly reasoning with memory access optimizations.
Reference: [Zuc92] <author> Richard N. Zucker. </author> <title> Relaxed Consistency and Synchronization in Parallel Processors. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <month> December </month> <year> 1992. </year> <note> Also available as Technical Report No. 92-12-05. 24 </note>
Reference-contexts: Adve and Hill's implementations for the data-race-free-0 [AH90b] and data-race-free-1 [AH93] programmer-centric models also use an aggressive form of the multiprocessor dependence chain condition to provide more aggressive implementations than allowed by WO and RCsc. Zucker <ref> [Zuc92] </ref> makes a similar observation in the context of software cache coherence, and finally, similar observations have led to more efficient software distributed shared memory implementations [BZ91, KCZ92].
References-found: 37

