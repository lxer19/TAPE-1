URL: ftp://ftp.cs.cmu.edu/user/mootaz/papers/thesis.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/mootaz/ftp/html/pub.html
Root-URL: 
Title: Manetho: Fault Tolerance in Distributed Systems Using Rollback-Recovery and Process Replication  
Author: by Elmootazbellah Nabil Elnozahy John Bennett 
Degree: A Thesis Submitted in Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy Approved, Thesis Committee: Willy Zwaenepoel, Chairman Professor of Computer Science Robert Cartwright Professor of Computer Science  Assistant Professor of Electrical and  
Date: October, 1993  
Address: Houston, Texas  
Affiliation: RICE UNIVERSITY  Computer Engineering  
Abstract-found: 0
Intro-found: 1
Reference: [ABB + 86] <author> M. Accetta, R. Baron, W. Bolosky, D. Golub, R. Rashid, A. Tevanian, and M. Young. </author> <title> Mach: A new kernel foundation for UNIX development. </title> <booktitle> In Proceedings of the Summer Usenix Conference, </booktitle> <pages> pages 93-111, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: Collectively, they implement the four protocols described in Chapter 2. Recovery units based on rollback-recovery include support for the communication, checkpoint-ing, and recovery protocols. Recovery units based on replication also include support fl A recovery unit is equivalent to a Mach task <ref> [ABB + 86] </ref>. 49 for the multicast protocol. Due to the added support for Manetho, the source code files for the kernel and system servers grew by about 26% and 40%, respectively.
Reference: [ADKM92] <author> Y. Amir, D. Dolev, S. Kramer, and D. Malki. Transis: </author> <title> A communication subsystem for high availability. </title> <booktitle> In Proceedings of the 22nd International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 76-84, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Several copies of a server program are run on different machines. One copy is distinguished and is called the leader, while the remaining copies are called cohorts. During failure-free operation, replicas of a server program must receive the same set of messages in the same order <ref> [ADKM92, Bir85, BSS91, MSMA90, PBS89, VRB89, Woo93] </ref>. Otherwise, their executions will not remain identical. For this purpose, Manetho features a multicast protocol that ensures ordered delivery of application messages among all replicas. Leaders represent their corresponding replicated servers when interacting with application programs or other servers. <p> The latency of message delivery in Manetho is small regardless of the rate of incoming application multicasts. Transis Transis is a protocol that combines ideas from the Trans/Total protocol with some ideas from ISIS <ref> [ADKM92] </ref>. The purpose is to exploit the broadcast facility commonly offered by local area networks to provide an efficient implementation that supports communication facilities similar to ISIS's. Manetho's multicast offers equivalent semantics to Transis's safe multicast, in the context of process replication.
Reference: [ADL90] <author> M. Ahamad, P. Dasgupta, and R.J. LeBlanc. </author> <title> Fault-tolerant atomic computations in an object-based distributed system. </title> <journal> Distributed Computing, </journal> <volume> 4 </volume> <pages> 69-80, </pages> <year> 1990. </year>
Reference-contexts: Rollback-recovery also provides fault tolerance transparently. By embodying state saving and restoration techniques in the operating system, the provision of rollback-recovery is automatic and no change to application programs is required. 5 1.3.2 Process Replication Manetho uses process replication to provide high availability to servers <ref> [ADL90, BF83, Bir85, Coo84, Coo85, CPR + 92, SES + 92] </ref>. Several copies of a server program are run on different machines. One copy is distinguished and is called the leader, while the remaining copies are called cohorts. <p> Manetho is similar to CHORUS in adopting the leader/cohort model, but Manetho's cohorts are in sync with the leader most of the time, while CHORUS's cohorts are always one processing step behind. Clouds The Clouds system uses transactions to support process replication <ref> [ADL90] </ref>. The computation to be replicated is written as a sequence of short transactions. For each transaction, the replicas execute the required operations in parallel. At commit time, only one replica succeeds while the remaining cohorts abort. The next transaction in program order is then run.
Reference: [AL89] <author> M. Ahamad and L. Lin. </author> <title> Using checkpoints to localize the effects of faults in distributed systems. </title> <booktitle> In Proceedings of the 8th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 1-11, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Distributed Checkpointing Most previous work in distributed checkpointing has concentrated on issues such as reducing the number of messages required to coordinate a checkpoint [BL88, BCS84, CJ91, LY87, LNP91, SK86, TKT89, VRL87], or limiting the number of hosts that have to participate in taking the checkpoint or in rolling back <ref> [AL89, KYA86, IM89, KT87, KT87, WF92a] </ref>. Manetho is the first system to use coordinated checkpointing with message logging. This combination can be viewed as an enhancement of coordinated checkpointing protocols to enable them to interact more efficiently with the outside world.
Reference: [BBG + 89] <author> A. Borg, W. Blau, W. Graetsch, F. Herrmann, and W. Oberle. </author> <title> Fault tolerance under UNIX. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(1) </volume> <pages> 1-24, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: Furthermore, as checkpoints accumulate on stable storage, the system needs to invoke a garbage collection procedure. 70 Coordinated Checkpointing Manetho's checkpointing protocol was chosen as a representative of coordinated check-pointing protocols. Receiver-Based Optimistic Message Logging (RBML) In RBML, processes use message logging in addition to independent checkpoint-ing <ref> [BBG + 89, JV91, JZ88, PP83, SW89, SY85] </ref>. The processes that participate in the distributed computation log the messages that they receive during failure-free operation on stable storage. Asynchronous logging is used to avoid the overhead of synchronous stable storage access. <p> These advantages result from using the antecedence graph. The performance measurements show that the overhead of maintaining the antecedence graph is negligible. Pessimistic Message Logging In pessimistic logging, processes log recovery information synchronously on stable storage <ref> [BBG + 89, Jal89, PP83] </ref>. These systems usually use hardware support to mitigate the effects of synchronous logging.
Reference: [BCS84] <author> D. Briatico, A. Ciuffoletti, and L. Simoncini. </author> <title> A distributed domino-effect free recovery algorithm. </title> <booktitle> In Proceedings of the 4th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 207-215, </pages> <month> October </month> <year> 1984. </year>
Reference-contexts: This number tags every message exchanged under Manetho's communication protocol (see Figure 2.5). The CCN enables the protocol to run in the presence of message reordering or loss <ref> [BCS84, LY87] </ref>. Additionally, each RU includes in the checkpoint the state interval index (P SI ) during which the RU took the checkpoint. Thus, if an RU p fails, it will restart at state interval p:P SI. The protocol proceeds as follows: 1. <p> Since this message was sent after its sender has started participating in the consistent checkpoint, the receiver must save its state before receiving this message to maintain the consistency of the global checkpoint <ref> [BCS84, LY87] </ref>. 3. After the tentative checkpoint has been written on stable storage, the RU sends an acknowledgment message to the coordinator. 4. The coordinator collects the responses from all RU's, and if all tentative checkpoints have been successful, it broadcasts a success message; otherwise, it broadcasts an abort message. <p> The antecedence graph also provides support for a limited form of nondeterministic execution, which is not possible with sender-based message logging. 5.1.2 Distributed Checkpointing Most previous work in distributed checkpointing has concentrated on issues such as reducing the number of messages required to coordinate a checkpoint <ref> [BL88, BCS84, CJ91, LY87, LNP91, SK86, TKT89, VRL87] </ref>, or limiting the number of hosts that have to participate in taking the checkpoint or in rolling back [AL89, KYA86, IM89, KT87, KT87, WF92a]. Manetho is the first system to use coordinated checkpointing with message logging.
Reference: [BF83] <author> J.S. Banino and J.C. Fabre. </author> <title> Distributed and coupled actors: A CHORUS proposal for reliability. </title> <booktitle> In Proceedings of the 3rd International Conference on Distributed Computing Systems, </booktitle> <pages> pages 128-134, </pages> <month> October </month> <year> 1983. </year>
Reference-contexts: Rollback-recovery also provides fault tolerance transparently. By embodying state saving and restoration techniques in the operating system, the provision of rollback-recovery is automatic and no change to application programs is required. 5 1.3.2 Process Replication Manetho uses process replication to provide high availability to servers <ref> [ADL90, BF83, Bir85, Coo84, Coo85, CPR + 92, SES + 92] </ref>. Several copies of a server program are run on different machines. One copy is distinguished and is called the leader, while the remaining copies are called cohorts. <p> This transaction would deadlock if two members of the troupe received messages in different orders. Committing this transaction required at least r additional multicasts. In contrast, Manetho provides ordered multicast delivery with only one overhead multicast per application multicast. CHORUS The CHORUS system provides actors among its supported facilities <ref> [BF83] </ref>. An actor is a sequential process whose execution is divided by the programmer into processing steps that are executed sequentially. A processing step starts by receiving a message from another actor, and ends by sending messages to other actors. To achieve fault tolerance, CHORUS uses the coupled-actors mechanism.
Reference: [BHG87] <author> P.A. Bernstein, V. Hadzilacos, and N. Goodman. </author> <title> Concurrency Control and Recovery in Database Systems. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mas-sachusetts, </address> <year> 1987. </year> <month> 104 </month>
Reference-contexts: The design of the stable storage server is therefore optimized for the common case of writing files of large sizes. In particular, the stable storage server organizes the disk as a sequential log to eliminate most disk seeks <ref> [BHG87] </ref>. Read and write operations occur in multiples of disk blocks, up to eight blocks in one operation. The disk block size is eight Kbytes, which is also the size of a memory page on a Sun 3/60.
Reference: [BHV + 90] <author> P.A. Barrett, A.M. Hilborne, P. Verissimo, L. Rodrigues, P.G. Bond, D.T. Seaton, and N.A. Speirs. </author> <title> The Delta-4 extra performance architecture XPA. </title> <booktitle> In Proceedings of the 20th International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 481-488, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Such support can be implemented as in the Delta-4 project <ref> [BHV + 90] </ref>. 3.3 Implementation of Checkpointing Each machine based on rollback-recovery includes a checkpoint server that participates in the checkpointing protocol as described in Section 2.5. This server is responsible for controlling the actions required to take a checkpoint of the recovery unit. <p> Sequencer-Based Multicasts Having a sequencer define the receipt order of a multicast was used in the mul-ticast protocol of Chang and Maxemchuck [CM84], the implementation of ISIS's ABCAST [BJ87b, BSS91], Amoeba's atomic broadcast protocol [KT91], and the Delta-4 system <ref> [BHV + 90, CPR + 92, VRB89] </ref>. The r-resilient protocol of Chang and Maxemchuck relies on negative acknowledgment and leadership transfer to achieve reliable total ordering. However, the protocol cannot deliver a multicast before r 1 subsequent leadership transfers occur.
Reference: [Bir85] <author> K.P. Birman. </author> <title> Replication and fault-tolerance in the ISIS system. </title> <booktitle> In Proceedings of the 10th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 79-86, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: Rollback-recovery also provides fault tolerance transparently. By embodying state saving and restoration techniques in the operating system, the provision of rollback-recovery is automatic and no change to application programs is required. 5 1.3.2 Process Replication Manetho uses process replication to provide high availability to servers <ref> [ADL90, BF83, Bir85, Coo84, Coo85, CPR + 92, SES + 92] </ref>. Several copies of a server program are run on different machines. One copy is distinguished and is called the leader, while the remaining copies are called cohorts. <p> Several copies of a server program are run on different machines. One copy is distinguished and is called the leader, while the remaining copies are called cohorts. During failure-free operation, replicas of a server program must receive the same set of messages in the same order <ref> [ADKM92, Bir85, BSS91, MSMA90, PBS89, VRB89, Woo93] </ref>. Otherwise, their executions will not remain identical. For this purpose, Manetho features a multicast protocol that ensures ordered delivery of application messages among all replicas. Leaders represent their corresponding replicated servers when interacting with application programs or other servers. <p> The purpose of message logging becomes the provision for faster output commit, rather than the elimination of checkpoint coordination. This observation is one of the contributions of this dissertation. 2.6 Manetho's Multicast Protocol Manetho uses active replication to provide high availability to server applications. Replication follows the leader/cohort model <ref> [Bir85, BMST92] </ref>, where an r-resilient replicated RU consists of a leader and r cohorts that execute the same application program. Each replica maintains its own copy of the RU's data structures described in Section 2.4. Following the terminology used in CIRCUS [Coo85], a replicated RU is also called a troupe.
Reference: [BJ87a] <author> K.P. Birman and T.A. Joseph. </author> <title> Exploiting virtual synchrony in distributed systems. </title> <booktitle> In Proceedings of the 11th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 123-138, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: This is an instance of the problem of ordering the perception of failures with respect to application messages <ref> [BJ87a, SY85] </ref>. To solve this problem, each RU maintains an incarnation number . Each time an RU starts recovery from a failure, it increments its incarnation number and reliably sends it to all RU's before restarting execution. Each application message is tagged with the current incarnation number of the sender.
Reference: [BJ87b] <author> K.P. Birman and T.A. Joseph. </author> <title> Reliable communication in the presence of failures. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(1) </volume> <pages> 47-76, </pages> <month> February </month> <year> 1987. </year>
Reference-contexts: In positive acknowledgment protocols, such as the original implementation of ABCAST of ISIS <ref> [BJ87b] </ref>, the receivers run an agreement protocol to determine the receipt order of each application multicast. The multicast is delivered as soon as its receipt order is agreed on, at the expense of the overhead caused by the control messages necessary to reach agreement. <p> Sequencer-Based Multicasts Having a sequencer define the receipt order of a multicast was used in the mul-ticast protocol of Chang and Maxemchuck [CM84], the implementation of ISIS's ABCAST <ref> [BJ87b, BSS91] </ref>, Amoeba's atomic broadcast protocol [KT91], and the Delta-4 system [BHV + 90, CPR + 92, VRB89]. The r-resilient protocol of Chang and Maxemchuck relies on negative acknowledgment and leadership transfer to achieve reliable total ordering.
Reference: [BL88] <author> B. Bhargava and S-R. Lian. </author> <title> Independent checkpointing and concurrent rollback recovery for distributed systems | an optimistic approach. </title> <booktitle> In Proceedings of the 7th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 3-12, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: An empirical performance evaluation using the prototype implementation confirms that the advantages of the system are realized in practice. In addition, the evaluation reveals several results that are applicable to Manetho and are relevant to any rollback recovery or multicast protocol: 1. Contrary to previous assumptions <ref> [BL88, SY85] </ref>, coordinated checkpointing is an efficient technique for implementing rollback-recovery. Moreover, the implementation shows that using coordinated checkpointing in message logging protocols improves performance and reduces the complexity of garbage collec tion. 2. Concurrent and incremental checkpointing [LNP90] are essential for low failure free performance overhead. 3. <p> A brief description of each of these protocols follows. Independent Checkpointing With independent checkpointing, each process takes a checkpoint of its state without coordination with other processes <ref> [BL88, WF92a, WF92b] </ref>. If a failure occurs, the recovery algorithm will roll back the computation to a set of checkpoints on stable storage that forms the most recent consistent state. <p> Analysis 1. There is no considerable difference in performance between independent and coordinated checkpointing. This result contradicts many previous claims that portrayed coordinated checkpointing as inferior because of the overhead of coordination <ref> [BL88] </ref>. In fact, the overhead of coordinated checkpointing reported here is not due to coordination messages, but rather a result of having all the machines "gang up" on the same stable storage server at the same time to save their states [EJZ92]. <p> The antecedence graph also provides support for a limited form of nondeterministic execution, which is not possible with sender-based message logging. 5.1.2 Distributed Checkpointing Most previous work in distributed checkpointing has concentrated on issues such as reducing the number of messages required to coordinate a checkpoint <ref> [BL88, BCS84, CJ91, LY87, LNP91, SK86, TKT89, VRL87] </ref>, or limiting the number of hosts that have to participate in taking the checkpoint or in rolling back [AL89, KYA86, IM89, KT87, KT87, WF92a]. Manetho is the first system to use coordinated checkpointing with message logging.
Reference: [BLL90] <author> B. Bhargava, S-R. Lian, and P-J. Leu. </author> <title> Experimental evaluation of concurrent checkpointing and rollback-recovery algorithms. </title> <booktitle> In Proceedings of the International Conference on Data Engineering, </booktitle> <pages> pages 182-189, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Without message logging or dependence tracking, a protocol based solely on coordinated checkpointing must take a global checkpoint before releasing output to the outside world. The resulting latency makes such techniques unsuitable for interactive applications. 92 Independent Versus Coordinated Checkpointing Bhargava et al. <ref> [BLL90] </ref> reported on the performance of independent checkpointing using simulation. They concluded that the messages used for synchronizing a checkpoint cause a large overhead. Their conclusion differs from the results presented here for two reasons. First, they use application programs with small address spaces (4 to 48 Kbytes).
Reference: [BMST92] <author> N. Budhiraja, K. Marzullo, F.B. Schneider, and S. Toueg. </author> <title> Optimal primary-backup protocols. </title> <type> Technical Report TR92-1299, </type> <institution> Cornell University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: The purpose of message logging becomes the provision for faster output commit, rather than the elimination of checkpoint coordination. This observation is one of the contributions of this dissertation. 2.6 Manetho's Multicast Protocol Manetho uses active replication to provide high availability to server applications. Replication follows the leader/cohort model <ref> [Bir85, BMST92] </ref>, where an r-resilient replicated RU consists of a leader and r cohorts that execute the same application program. Each replica maintains its own copy of the RU's data structures described in Section 2.4. Following the terminology used in CIRCUS [Coo85], a replicated RU is also called a troupe.
Reference: [BN84] <author> A.D. Birrell and B.J. Nelson. </author> <title> Implementing remote procedure calls. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(1) </volume> <pages> 39-59, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: Copying a message takes place immediately after issuing the command to the network interface to send it over the network. Therefore, logging a message is not in the critical path of interprocess communication. For the common case of a remote procedure call where the sender is blocked <ref> [BN84] </ref>, logging the message does not affect the communication latency. If all messages sent from a recovery unit between two consecutive checkpoints fit in the volatile log, the overhead of message logging will be small. Otherwise, the volatile log must be flushed to secondary storage to free up space.
Reference: [BSS91] <author> K. Birman, A. Schiper, and P. Stephenson. </author> <title> Lightweight causal and atomic group multicast. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(3) </volume> <pages> 272-314, </pages> <month> August </month> <year> 1991. </year> <month> 105 </month>
Reference-contexts: Several copies of a server program are run on different machines. One copy is distinguished and is called the leader, while the remaining copies are called cohorts. During failure-free operation, replicas of a server program must receive the same set of messages in the same order <ref> [ADKM92, Bir85, BSS91, MSMA90, PBS89, VRB89, Woo93] </ref>. Otherwise, their executions will not remain identical. For this purpose, Manetho features a multicast protocol that ensures ordered delivery of application messages among all replicas. Leaders represent their corresponding replicated servers when interacting with application programs or other servers. <p> Using the antecedence graph also leads to several advantages with respect to process replication. Manetho's multicast protocol offers high throughput and low latency message delivery. Having both these features was not possible in previous multicast protocols without relying on application semantics <ref> [BSS91, PBS89] </ref>. In contrast, Manetho's multicast is application-transparent. A prototype for Manetho has been implemented on a network multicomputer consisting of a 10 Mbits/second Ethernet network connecting 16 Sun-3/60 workstations. <p> It is not clear how the problem can be 78 solved, but it suggests that a multicast protocol should not be implemented or designed independently from the flow control algorithm. Analogous experience was reported in ISIS <ref> [BSS91] </ref>. Multicast Latency The second metric of the performance of a multicast protocol is the latency. Latency is measured by the round trip time required by a null remote procedure call (RPC) issued by a client process and executed on a server. <p> Sequencer-Based Multicasts Having a sequencer define the receipt order of a multicast was used in the mul-ticast protocol of Chang and Maxemchuck [CM84], the implementation of ISIS's ABCAST <ref> [BJ87b, BSS91] </ref>, Amoeba's atomic broadcast protocol [KT91], and the Delta-4 system [BHV + 90, CPR + 92, VRB89]. The r-resilient protocol of Chang and Maxemchuck relies on negative acknowledgment and leadership transfer to achieve reliable total ordering. <p> The effect of this delay on the application performance can be substantial. Manetho and the new implementation of ISIS's ABCAST <ref> [BSS91] </ref> rely on a single site to define the multicast's receipt order. ABCAST relies on an underlying transport protocol that guarantees reliable message delivery in FIFO order. This transport protocol is a major source of overhead in ISIS [BSS91]. In addition, ISIS is not application-transparent. <p> Manetho and the new implementation of ISIS's ABCAST <ref> [BSS91] </ref> rely on a single site to define the multicast's receipt order. ABCAST relies on an underlying transport protocol that guarantees reliable message delivery in FIFO order. This transport protocol is a major source of overhead in ISIS [BSS91]. In addition, ISIS is not application-transparent. In contrast, Manetho adopts weaker assumption about network reliability, leading to better performance. It is also application-transparent. Amoeba's atomic broadcast uses negative acknowledgment for the 0-resilient version, and positive acknowledgment otherwise.
Reference: [Che88] <author> D.R. Cheriton. </author> <title> The V distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 31(3) </volume> <pages> 314-333, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Each workstation has a 20 MHz MC68020 microprocessor and four Mbytes of physical memory. The workstations run a version of the V-System <ref> [Che88] </ref>, to which Manetho's mechanisms are added. The application programs running on this multicomputer use the distributed processing facilities provided by the V-System. Examples include interprocess communication primitives, transparent remote execution, and libraries to support construction of distributed programs [VDG86].
Reference: [CJ91] <author> F. Cristian and F. Jahanian. </author> <title> A timestamp-based checkpointing protocol for long-lived distributed computations. </title> <booktitle> In Proceedings of the 10th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 12-20, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: The antecedence graph also provides support for a limited form of nondeterministic execution, which is not possible with sender-based message logging. 5.1.2 Distributed Checkpointing Most previous work in distributed checkpointing has concentrated on issues such as reducing the number of messages required to coordinate a checkpoint <ref> [BL88, BCS84, CJ91, LY87, LNP91, SK86, TKT89, VRL87] </ref>, or limiting the number of hosts that have to participate in taking the checkpoint or in rolling back [AL89, KYA86, IM89, KT87, KT87, WF92a]. Manetho is the first system to use coordinated checkpointing with message logging.
Reference: [CL85] <author> K.M. Chandy and L. Lamport. </author> <title> Distributed snapshots: Determining global states of distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(1) </volume> <pages> 63-75, </pages> <month> February </month> <year> 1985. </year>
Reference-contexts: A global consistent state is one that could have occurred during a failure-free execution of the application program <ref> [CL85] </ref>. Finding this consistent state may entail rolling back processes that have survived the failure [JZ90, KT87, SY85]. Rollback-recovery has the potential for providing fault tolerance with low overhead in resources and performance. Stable storage devices typically use magnetic disks and are inexpensive. <p> If the leader fails, the surviving cohorts will elect a new leader among themselves and operation will continue immediately, ensuring high availability. 1.3.3 The Antecedence Graph When a process fails, the recovery protocol may need to roll back several processes to restart from a global consistent state <ref> [CL85, SY85] </ref>. It is however undesirable to roll back replicated processes, because the services that they offer will not be available during recovery. Therefore, to allow rollback-recovery to co-exist with process replication, the recovery protocol must restart the system from a consistent state without rolling back the replicated processes. <p> Therefore, to allow rollback-recovery to co-exist with process replication, the recovery protocol must restart the system from a consistent state without rolling back the replicated processes. Many rollback-recovery protocols cannot give such a guarantee <ref> [CL85, JZ88, KT87, SY85] </ref>. Combining rollback-recovery and process replication in Manetho is made possible by using an antecedence graph. This data structure tracks nondeterministic events that occur during failure-free operation. <p> All processes in the distributed computation participate in the protocol. A checkpointing protocol: Processes that use rollback-recovery periodically use this protocol to store their states. The protocol ensures that a consistent <ref> [CL85] </ref> set of process states is always available on stable storage. After a failure occurs, the failed processes restore their states from stable storage and restart execution. <p> These checkpoints reduce the amount of execution replay that would be necessary should a failure occur. The RU's coordinate their checkpoints to form a system wide consistent state. A consistent state is one that could have occurred during some failure-free execution of the application program <ref> [CL85] </ref>. One distinguished RU acts as a coordinator and sends marker messages to all RU's to start a consistent checkpoint. Each RU maintains one permanent checkpoint, belonging to the most recent consistent checkpoint. <p> Thus, if an RU p fails, it will restart at state interval p:P SI. The protocol proceeds as follows: 1. The coordinator starts a new consistent checkpoint by incrementing CCN and sending marker messages <ref> [CL85] </ref> that contain CCN to each RU in the system. 2. Upon receiving a marker message, an RU starts a tentative checkpoint, if it has not started one already. The RU also starts a tentative checkpoint when it receives an application message whose CCN is greater than the local CCN. <p> In this case, q detects that the incarnation number tagging the message is old, and rejects it. The final safety property of the protocol concerns the consistency between any two RU's <ref> [CL85] </ref>. Lemma 2.4 shows that before p issues any UpdateInc () call, the antecedence graph at any RU q does not contain any state interval p i where i &gt; m.
Reference: [CM84] <author> J. Chang and N.F. Maxemchuck. </author> <title> Reliable broadcast protocols. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(3) </volume> <pages> 251-273, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: On the other hand, negative acknowledgment protocols reduce the number of control messages by piggybacking the ordering information on application multi-casts <ref> [CM84, Kaa92, MSMA90] </ref>. However, reducing the number of control messages or eliminating them altogether introduces latency in achieving agreement on the receipt order of an application multicast, delaying the delivery of the multicast to the application program. <p> For example, the r-resilient protocol by Chang and Maxemchuck requires only one overhead message per application multicast <ref> [CM84] </ref>. However, it cannot deliver a message to the application program until r 1 "token transfers" have occurred, each requiring one application message to be received or a timeout to expire. Thus, an application message cannot be delivered until r 1 subsequent application messages are received [CM84]. <p> message per application multicast <ref> [CM84] </ref>. However, it cannot deliver a message to the application program until r 1 "token transfers" have occurred, each requiring one application message to be received or a timeout to expire. Thus, an application message cannot be delivered until r 1 subsequent application messages are received [CM84]. Manetho exploits the fact that active replication does not need the full generality and functionality of existing multicast protocols. Like negative acknowledgment multicast protocols, Manetho reduces the overhead during failure-free operation. A cohort does not acknowledge receiving application multicasts and it acknowledges the sequence multicasts only during synchronization. <p> The second part of the performance study compares Manetho's multicast with two well known, efficient multicast protocols, the Amoeba r-resilient multicast protocol [Kaa92], and the protocol by Chang and Maxemchuck <ref> [CM84] </ref>. They respectively represent the positive and negative acknowledgment families of multicast protocols. Results show that for supporting process replication, Manetho outperforms the other two protocols in throughput and latency. The study confirms the conventional wisdom that negative acknowledgment protocols have higher throughput and latency than positive acknowledgment protocols. <p> the same site as the leader of a troupe, so that a message is delivered to the application as soon as the sequencer receives all acknowledgments. 4.3.2 Chang and Maxemchuck The protocol of Chang and Maxemchuck was selected to represent the negative acknowledgment family because of its simplicity and efficiency <ref> [CM84] </ref>. The receivers of a multicast in this protocol form a logical ring and exchange a token along the ring's links. <p> In addition, Manetho does not require every host in the system to receive every message or to maintain the entire history graph of the system. Sequencer-Based Multicasts Having a sequencer define the receipt order of a multicast was used in the mul-ticast protocol of Chang and Maxemchuck <ref> [CM84] </ref>, the implementation of ISIS's ABCAST [BJ87b, BSS91], Amoeba's atomic broadcast protocol [KT91], and the Delta-4 system [BHV + 90, CPR + 92, VRB89]. The r-resilient protocol of Chang and Maxemchuck relies on negative acknowledgment and leadership transfer to achieve reliable total ordering.
Reference: [Coo84] <author> E.C. Cooper. </author> <title> Replicated procedure call. </title> <booktitle> In Proceedings of the 3rd Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 220-232, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: Rollback-recovery also provides fault tolerance transparently. By embodying state saving and restoration techniques in the operating system, the provision of rollback-recovery is automatic and no change to application programs is required. 5 1.3.2 Process Replication Manetho uses process replication to provide high availability to servers <ref> [ADL90, BF83, Bir85, Coo84, Coo85, CPR + 92, SES + 92] </ref>. Several copies of a server program are run on different machines. One copy is distinguished and is called the leader, while the remaining copies are called cohorts. <p> The comparison here considers only systems that operate in asynchronous distributed environments and provide replication for reliability. Systems that assume real-time 94 requirements [SES + 92], or use replication for enhancing performance [Gol91] are not considered. CIRCUS CIRCUS supported process replication in an asynchronous network <ref> [Coo84, Coo85] </ref>. Replicated remote procedure calls were used to implement inter-troupe communication. If identical receipt order at each replica was not required, a many-to-many RPC incurred between r + 1 to 2r multicasts. Identical receipt order was possible by structuring the many-to-many RPC as a transaction.
Reference: [Coo85] <author> E.C. Cooper. </author> <title> Replicated distributed programs. </title> <booktitle> In Proceedings of the 10th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 63-78, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: Rollback-recovery also provides fault tolerance transparently. By embodying state saving and restoration techniques in the operating system, the provision of rollback-recovery is automatic and no change to application programs is required. 5 1.3.2 Process Replication Manetho uses process replication to provide high availability to servers <ref> [ADL90, BF83, Bir85, Coo84, Coo85, CPR + 92, SES + 92] </ref>. Several copies of a server program are run on different machines. One copy is distinguished and is called the leader, while the remaining copies are called cohorts. <p> Replication follows the leader/cohort model [Bir85, BMST92], where an r-resilient replicated RU consists of a leader and r cohorts that execute the same application program. Each replica maintains its own copy of the RU's data structures described in Section 2.4. Following the terminology used in CIRCUS <ref> [Coo85] </ref>, a replicated RU is also called a troupe. The system converts an application message sent to a troupe into a multicast that is sent to every replica. The sender of the message is not aware that it is communicating with a troupe. <p> The comparison here considers only systems that operate in asynchronous distributed environments and provide replication for reliability. Systems that assume real-time 94 requirements [SES + 92], or use replication for enhancing performance [Gol91] are not considered. CIRCUS CIRCUS supported process replication in an asynchronous network <ref> [Coo84, Coo85] </ref>. Replicated remote procedure calls were used to implement inter-troupe communication. If identical receipt order at each replica was not required, a many-to-many RPC incurred between r + 1 to 2r multicasts. Identical receipt order was possible by structuring the many-to-many RPC as a transaction.
Reference: [CPR + 92] <author> M. Chereque, D. Powell, P. Reynier, J-L. Richier, and J. Voiron. </author> <title> Active replication in Delta-4. </title> <booktitle> In Proceedings of the 22nd International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 28-37, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Rollback-recovery also provides fault tolerance transparently. By embodying state saving and restoration techniques in the operating system, the provision of rollback-recovery is automatic and no change to application programs is required. 5 1.3.2 Process Replication Manetho uses process replication to provide high availability to servers <ref> [ADL90, BF83, Bir85, Coo84, Coo85, CPR + 92, SES + 92] </ref>. Several copies of a server program are run on different machines. One copy is distinguished and is called the leader, while the remaining copies are called cohorts. <p> Sequencer-Based Multicasts Having a sequencer define the receipt order of a multicast was used in the mul-ticast protocol of Chang and Maxemchuck [CM84], the implementation of ISIS's ABCAST [BJ87b, BSS91], Amoeba's atomic broadcast protocol [KT91], and the Delta-4 system <ref> [BHV + 90, CPR + 92, VRB89] </ref>. The r-resilient protocol of Chang and Maxemchuck relies on negative acknowledgment and leadership transfer to achieve reliable total ordering. However, the protocol cannot deliver a multicast before r 1 subsequent leadership transfers occur.
Reference: [EJZ92] <author> E.N. Elnozahy, D.B. Johnson, and W. Zwaenepoel. </author> <title> The performance of consistent checkpointing. </title> <booktitle> In Proceedings of the 11th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 39-47, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: In fact, the overhead of coordinated checkpointing reported here is not due to coordination messages, but rather a result of having all the machines "gang up" on the same stable storage server at the same time to save their states <ref> [EJZ92] </ref>. With a larger stable storage bandwidth and after accounting for the overhead of garbage collection of independent checkpointing, the performance difference between the two methods is likely to decrease further. 2. <p> In addition, if the application program quickly modifies additional pages of the address space, it may have to be suspended to complete the checkpoint. The overhead introduced by copy-on-write is always less than or equal to that introduced by precopying <ref> [EJZ92] </ref>. 93 Li et al. [LNP90] described several checkpointing methods for programs executing on shared memory multiprocessors. Their results showed that concurrent checkpoint-ing reduces the checkpointing overhead for programs running on shared memory multiprocessors. The results presented here extend their results to distributed systems.
Reference: [EZ92a] <author> E.N. Elnozahy and W. Zwaenepoel. Manetho: </author> <title> Transparent rollback-recovery with low overhead, limited rollback, and fast output commit. </title> <journal> IEEE Transactions on Computers Special Issue On Fault-Tolerant Computing, </journal> <volume> 41(5) </volume> <pages> 526-531, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: For the other applications, the message logs do not overflow and therefore copy-on-write was not used. 4.2.4 Comparison with Another Implementation of Manetho The performance of the seven application programs was measured for an implementation of an earlier design of Manetho that uses independent checkpointing <ref> [EZ92a, EZ92b] </ref>. In this design, each recovery unit flushes its message log to stable storage whenever it takes a checkpoint. If the message log is not flushed, a domino effect may occur if two or more recovery units fail.
Reference: [EZ92b] <author> E.N. Elnozahy and W. Zwaenepoel. </author> <title> Replicated distributed processes in Manetho. </title> <booktitle> In Proceedings of the 22nd International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 18-27, </pages> <month> July </month> <year> 1992. </year> <month> 106 </month>
Reference-contexts: For the other applications, the message logs do not overflow and therefore copy-on-write was not used. 4.2.4 Comparison with Another Implementation of Manetho The performance of the seven application programs was measured for an implementation of an earlier design of Manetho that uses independent checkpointing <ref> [EZ92a, EZ92b] </ref>. In this design, each recovery unit flushes its message log to stable storage whenever it takes a checkpoint. If the message log is not flushed, a domino effect may occur if two or more recovery units fail.
Reference: [FR86] <author> R. Fitzgerald and R.F. Rashid. </author> <title> The integration of virtual memory management and interprocess communication in Accent. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(2) </volume> <pages> 147-177, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: Secondary storage here acts as an extension of the volatile message log similar to swap space being an extension of a machine's main memory. 51 are write-protected within the address space of the application using the memory management hardware <ref> [FR86] </ref>. The application program is then allowed to continue. The kernel will block the application program only if it modifies the protected data. This technique is especially effective in applications where the communication load consists of infrequent large bursts of messages. <p> Concurrent Checkpointing The kernel uses copy-on-write protection to allow the application program to continue execution while its checkpoint is being written on stable storage. At the start of an incremental checkpoint, the pages to be written on stable storage are write-protected using the memory management hardware <ref> [FR86] </ref>. The application program blocks only while initializing the protection of the address space. After writing a page on stable storage, the kernel removes the protection on it. If the application program attempts to modify one of these pages while it is still protected, a memory protection fault is generated.
Reference: [GGL + 90] <author> A. Goldberg, A. Gopal, K. Li, R. Strom, and D. Bacon. </author> <title> Transparent recovery of Mach applications. </title> <booktitle> In Proceedings of the Usenix Mach Workshop, </booktitle> <pages> pages 169-184, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: The descriptor's type field indicates a synchronization operation. Remaining fields contain the recovery unit's identifier, the thread identifier, and the state interval index. Similar techniques have been used in Instant Replay for debugging parallel programs [LMC87] and in an implementation of optimistic recovery <ref> [GGL + 90] </ref>. 54 System Calls The V-System implements system calls by local remote procedure calls to the kernel and system servers. Unlike monolithic kernels, the number of system calls available to application programs is limited.
Reference: [GM82] <author> H. Garcia-Molina. </author> <title> Elections in a distributed computing system. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 31(1) </volume> <pages> 48-59, </pages> <month> January </month> <year> 1982. </year>
Reference-contexts: Leader Election If the leader of a troupe fails, the cohorts will use the following protocol to elect a new leader. This protocol is an adaptation of the invitation protocol <ref> [GM82] </ref> in which the winner of the election is the cohort that has the highest state interval index. 35 * A cohort starts leader election by sending a recovery-multicast to the other cohorts in the troupe. <p> The leader also informs the cohorts of the new incarnation number during synchronization. If there is at least one surviving troupe member, the protocol elects a single leader and terminates <ref> [GM82] </ref>. If the initiator of the protocol fails, the protocol is simply restarted. 2.8.4 Formal Description procedure Recover (). The procedure's argument is the recovering RU's identifier.
Reference: [Gol91] <author> A. Goldberg. </author> <title> Optimistic Algorithms for Distributed Transparent Process Replication. </title> <type> PhD thesis, </type> <institution> University of California, </institution> <address> Los Angeles, </address> <year> 1991. </year>
Reference-contexts: The comparison here considers only systems that operate in asynchronous distributed environments and provide replication for reliability. Systems that assume real-time 94 requirements [SES + 92], or use replication for enhancing performance <ref> [Gol91] </ref> are not considered. CIRCUS CIRCUS supported process replication in an asynchronous network [Coo84, Coo85]. Replicated remote procedure calls were used to implement inter-troupe communication. If identical receipt order at each replica was not required, a many-to-many RPC incurred between r + 1 to 2r multicasts.
Reference: [IM89] <author> S. Israel and D. Morris. </author> <title> A non-intrusive checkpointing protocol. </title> <booktitle> In The Phoenix Conference on Communications and Computers, </booktitle> <pages> pages 413-421, </pages> <year> 1989. </year>
Reference-contexts: Distributed Checkpointing Most previous work in distributed checkpointing has concentrated on issues such as reducing the number of messages required to coordinate a checkpoint [BL88, BCS84, CJ91, LY87, LNP91, SK86, TKT89, VRL87], or limiting the number of hosts that have to participate in taking the checkpoint or in rolling back <ref> [AL89, KYA86, IM89, KT87, KT87, WF92a] </ref>. Manetho is the first system to use coordinated checkpointing with message logging. This combination can be viewed as an enhancement of coordinated checkpointing protocols to enable them to interact more efficiently with the outside world.
Reference: [Jal89] <author> P. Jalote. </author> <title> Fault tolerant processes. </title> <journal> Distributed Computing, </journal> <volume> 3 </volume> <pages> 187-195, </pages> <year> 1989. </year>
Reference-contexts: These advantages result from using the antecedence graph. The performance measurements show that the overhead of maintaining the antecedence graph is negligible. Pessimistic Message Logging In pessimistic logging, processes log recovery information synchronously on stable storage <ref> [BBG + 89, Jal89, PP83] </ref>. These systems usually use hardware support to mitigate the effects of synchronous logging.
Reference: [Joh89] <author> D.B. Johnson. </author> <title> Distributed System Fault Tolerance Using Message Logging and Checkpointing. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: The overhead is low because no synchronous logging to stable storage takes place except during output commit. Therefore, the protocol behaves like optimistic rollback-recovery systems that use asynchronous logging <ref> [Joh89, SY85] </ref>. Unlike these systems, an output commit does not require several synchronization messages or several stable storage accesses. Committing output in Manetho only requires flushing the local antecedence graph in a single access to stable storage. <p> A simple solution is to block the execution of the application program while the checkpoint is being taken [KMBT92, TS84]. This policy, however, can have a severe effect on the performance and thus saving a checkpoint in Manetho is both incremental and concurrent <ref> [Joh89, LNP90] </ref>. Incremental Checkpointing Instead of writing the entire address space on stable storage during each checkpoint, only the memory pages that have been modified since the previous checkpoint are written. <p> The maximum number of versions a file may have is limited to 255. Thus, the stable storage server can support systems where several checkpoints may be maintained for each recovery unit, such as in optimistic recovery <ref> [Joh89, SY85] </ref>. When a checkpoint is to be discarded, the corresponding version is purged. The maximum throughput for writing to the server is 860 Kbytes/second. This maximum occurs when both server and client reside on the same machine and is included only as a reference. <p> Several techniques exist for recovery, all based on computing the maximum recoverable state using the checkpoints and message logs available on stable storage <ref> [Joh89] </ref>. The performance study uses an implementation of the technique suggested by Johnson and Zwaenepoel [JZ88]. In this technique, the sender adds O (1) dependence information on each message it sends. However, the implementation departs from the original design with respect to output commit. <p> The resulting protocol tolerates an arbitrary number of failures. In the implementation, each sender adds an O (1) dependence information on each message it sends, as suggested by Johnson <ref> [Joh89] </ref>. Again, the implementation of output commit logs the receipt order instead of the entire messages, for the same reasons as in the RBML implementation. 71 Measurements Table 4.5 shows the percentage increase in running time for four applications under the five rollback-recovery protocols. <p> The other two message logging protocols require a multihost protocol to commit output, which entails exchanging several messages and performing several I/O operations on stable storage <ref> [Joh89, Joh93, SW89] </ref>. <p> Therefore, the combination of coordinated checkpointing with message logging leads to better performance than the traditional combination of message logging with independent checkpointing, unlike what is widely believed. 90 Optimistic Message Logging In traditional optimistic message logging, processes take independent checkpoints and log the messages asynchronously to stable storage <ref> [Joh89, JZ90, JV91, SBY88, SY85, SW89] </ref>. The optimistic assumption is that each message will be logged on stable storage before a failure occurs and thus will be available for recovery. If a failure occurs, the processes will roll back to the maximum recoverable state and resume operation [Joh89]. <p> The optimistic assumption is that each message will be logged on stable storage before a failure occurs and thus will be available for recovery. If a failure occurs, the processes will roll back to the maximum recoverable state and resume operation <ref> [Joh89] </ref>. The failure-free overhead of optimistic message logging is low because it does not require synchronous access to stable storage. However, there are several drawbacks. The effects of failures are not confined to the processes that fail. <p> one checkpoint must be maintained by each process, failures do not affect live processes, and output can be committed without multihost coordination. 91 Sender-Based Message Logging Sender-based message logging is a protocol in which each message is logged in volatile memory on the machine from which the message is sent <ref> [JZ87, Joh89] </ref>. When a process receives a message, it returns to the sender a receipt sequence number (RSN) that indicates the order in which the message was received. <p> Second, it requires intricate garbage collection and recovery protocols, complicating the implementation and causing more overhead in processing time and in the required space on stable storage. Implementation techniques Johnson used a technique called precopying in implementing checkpointing <ref> [Joh89, TLC85] </ref>. In this technique, the pages to be written on stable storage are first copied to a separate area in memory if their number is below some threshold.
Reference: [Joh93] <author> D.B. Johnson. </author> <title> Efficient transparent optimistic rollback recovery for distributed application programs. </title> <booktitle> In Proceedings of the 12th Symposium on Reliable Distributed Systems, </booktitle> <month> October </month> <year> 1993. </year>
Reference-contexts: The other two message logging protocols require a multihost protocol to commit output, which entails exchanging several messages and performing several I/O operations on stable storage <ref> [Joh89, Joh93, SW89] </ref>.
Reference: [JV91] <author> T. Juang and S. Venkatesan. </author> <title> Crash recovery with little overhead. </title> <booktitle> In Proceedings of the 11th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 454-461, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Concurrent and incremental checkpointing [LNP90] are essential for low failure free performance overhead. 3. Sender-based message logging [JZ87, SBY88] has proved to be superior to receiver-based logging <ref> [JV91, JZ90, SW89, SY85] </ref>. 4. The implementation provides empirical evidence supporting the conventional wisdom that positive acknowledgment multicast protocols provide lower latency than positive acknowledgment protocols at the expense of a lower throughput. The performance evaluation also includes a comparative study between Manetho and other systems. <p> Furthermore, as checkpoints accumulate on stable storage, the system needs to invoke a garbage collection procedure. 70 Coordinated Checkpointing Manetho's checkpointing protocol was chosen as a representative of coordinated check-pointing protocols. Receiver-Based Optimistic Message Logging (RBML) In RBML, processes use message logging in addition to independent checkpoint-ing <ref> [BBG + 89, JV91, JZ88, PP83, SW89, SY85] </ref>. The processes that participate in the distributed computation log the messages that they receive during failure-free operation on stable storage. Asynchronous logging is used to avoid the overhead of synchronous stable storage access. <p> Therefore, the combination of coordinated checkpointing with message logging leads to better performance than the traditional combination of message logging with independent checkpointing, unlike what is widely believed. 90 Optimistic Message Logging In traditional optimistic message logging, processes take independent checkpoints and log the messages asynchronously to stable storage <ref> [Joh89, JZ90, JV91, SBY88, SY85, SW89] </ref>. The optimistic assumption is that each message will be logged on stable storage before a failure occurs and thus will be available for recovery. If a failure occurs, the processes will roll back to the maximum recoverable state and resume operation [Joh89].
Reference: [JZ87] <author> D.B. Johnson and W. Zwaenepoel. </author> <title> Sender-based message logging. </title> <booktitle> In Proceedings of the 17th International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 14-19, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: Moreover, the implementation shows that using coordinated checkpointing in message logging protocols improves performance and reduces the complexity of garbage collec tion. 2. Concurrent and incremental checkpointing [LNP90] are essential for low failure free performance overhead. 3. Sender-based message logging <ref> [JZ87, SBY88] </ref> has proved to be superior to receiver-based logging [JV91, JZ90, SW89, SY85]. 4. The implementation provides empirical evidence supporting the conventional wisdom that positive acknowledgment multicast protocols provide lower latency than positive acknowledgment protocols at the expense of a lower throughput. <p> Instead, the implementation just logs the receipt order of the messages. Sender-Based Message Logging (SBML) The SBML protocol implemented here is modeled after the protocol of Strom et al, in which the processes log the messages at the sender and log the receipt order at the receiver <ref> [JZ87, SBY88] </ref>. The resulting protocol tolerates an arbitrary number of failures. In the implementation, each sender adds an O (1) dependence information on each message it sends, as suggested by Johnson [Joh89]. <p> one checkpoint must be maintained by each process, failures do not affect live processes, and output can be committed without multihost coordination. 91 Sender-Based Message Logging Sender-based message logging is a protocol in which each message is logged in volatile memory on the machine from which the message is sent <ref> [JZ87, Joh89] </ref>. When a process receives a message, it returns to the sender a receipt sequence number (RSN) that indicates the order in which the message was received.
Reference: [JZ88] <author> D.B. Johnson and W. Zwaenepoel. </author> <title> Recovery in distributed systems using optimistic message logging and checkpointing. </title> <booktitle> In Proceedings of the 7th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 171-181, </pages> <month> August </month> <year> 1988. </year> <month> 107 </month>
Reference-contexts: Therefore, to allow rollback-recovery to co-exist with process replication, the recovery protocol must restart the system from a consistent state without rolling back the replicated processes. Many rollback-recovery protocols cannot give such a guarantee <ref> [CL85, JZ88, KT87, SY85] </ref>. Combining rollback-recovery and process replication in Manetho is made possible by using an antecedence graph. This data structure tracks nondeterministic events that occur during failure-free operation. <p> Furthermore, as checkpoints accumulate on stable storage, the system needs to invoke a garbage collection procedure. 70 Coordinated Checkpointing Manetho's checkpointing protocol was chosen as a representative of coordinated check-pointing protocols. Receiver-Based Optimistic Message Logging (RBML) In RBML, processes use message logging in addition to independent checkpoint-ing <ref> [BBG + 89, JV91, JZ88, PP83, SW89, SY85] </ref>. The processes that participate in the distributed computation log the messages that they receive during failure-free operation on stable storage. Asynchronous logging is used to avoid the overhead of synchronous stable storage access. <p> Several techniques exist for recovery, all based on computing the maximum recoverable state using the checkpoints and message logs available on stable storage [Joh89]. The performance study uses an implementation of the technique suggested by Johnson and Zwaenepoel <ref> [JZ88] </ref>. In this technique, the sender adds O (1) dependence information on each message it sends. However, the implementation departs from the original design with respect to output commit. The original design runs a multihost protocol that logs the necessary messages on stable storage.
Reference: [JZ90] <author> D.B. Johnson and W. Zwaenepoel. </author> <title> Recovery in distributed systems using optimistic message logging and checkpointing. </title> <journal> Journal of Algorithms, </journal> <volume> 11(3) </volume> <pages> 462-491, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: A global consistent state is one that could have occurred during a failure-free execution of the application program [CL85]. Finding this consistent state may entail rolling back processes that have survived the failure <ref> [JZ90, KT87, SY85] </ref>. Rollback-recovery has the potential for providing fault tolerance with low overhead in resources and performance. Stable storage devices typically use magnetic disks and are inexpensive. The performance overhead results from saving recovery information on stable storage during failure-free operation. <p> Concurrent and incremental checkpointing [LNP90] are essential for low failure free performance overhead. 3. Sender-based message logging [JZ87, SBY88] has proved to be superior to receiver-based logging <ref> [JV91, JZ90, SW89, SY85] </ref>. 4. The implementation provides empirical evidence supporting the conventional wisdom that positive acknowledgment multicast protocols provide lower latency than positive acknowledgment protocols at the expense of a lower throughput. The performance evaluation also includes a comparative study between Manetho and other systems. <p> Therefore, the combination of coordinated checkpointing with message logging leads to better performance than the traditional combination of message logging with independent checkpointing, unlike what is widely believed. 90 Optimistic Message Logging In traditional optimistic message logging, processes take independent checkpoints and log the messages asynchronously to stable storage <ref> [Joh89, JZ90, JV91, SBY88, SY85, SW89] </ref>. The optimistic assumption is that each message will be logged on stable storage before a failure occurs and thus will be available for recovery. If a failure occurs, the processes will roll back to the maximum recoverable state and resume operation [Joh89].
Reference: [Kaa92] <author> M.F. Kaashoek. </author> <title> Group Communication in Distributed Computer Systems. </title> <type> PhD thesis, </type> <institution> Vrije Universiteit, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: On the other hand, negative acknowledgment protocols reduce the number of control messages by piggybacking the ordering information on application multi-casts <ref> [CM84, Kaa92, MSMA90] </ref>. However, reducing the number of control messages or eliminating them altogether introduces latency in achieving agreement on the receipt order of an application multicast, delaying the delivery of the multicast to the application program. <p> Other issues include a comparison between coordinated checkpointing and message logging in general, and another comparison between independent and coordinated checkpointing. The second part of the performance study compares Manetho's multicast with two well known, efficient multicast protocols, the Amoeba r-resilient multicast protocol <ref> [Kaa92] </ref>, and the protocol by Chang and Maxemchuck [CM84]. They respectively represent the positive and negative acknowledgment families of multicast protocols. Results show that for supporting process replication, Manetho outperforms the other two protocols in throughput and latency. <p> support active replication outperforms general purpose multicast protocols when used for this purpose. 4.3.1 The Amoeba Broadcast Protocol The r-resilient Amoeba broadcast protocol was selected as a representative of the positive acknowledgment family of multicast protocols because it is one of the fastest implementations of that kind known to date <ref> [Kaa92] </ref>. It uses a two-phase agreement algorithm to determine the receipt order. The receivers of the multicast form a group, with a distinguished member called the sequencer. When the sequencer receives a message, it defines the receipt order and sends it with the message to the receiving group. <p> Their results showed that concurrent checkpoint-ing reduces the checkpointing overhead for programs running on shared memory multiprocessors. The results presented here extend their results to distributed systems. They did not implement incremental checkpointing which proved to be an important optimization. Kaashoek et al. <ref> [Kaa92, KMBT92] </ref> implemented consistent checkpointing to add fault tolerance to Orca, a distributed object-oriented language. Their implementation uses a broadcast protocol to order marker messages with application messages. Processes block while their checkpoints are being written on stable storage.
Reference: [KMBT92] <author> M.F. Kaashoek, R. Michiels, H.E. Bal, </author> <title> and A.S. Tanenbaum. Transparent fault-tolerance in parallel orca programs. </title> <booktitle> In Symposium on Experiences with Distributed and Multiprocessor Systems III, </booktitle> <pages> pages 297-312, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The second component results from the requirement that the saved state in the checkpoint file must represent an instantaneous snapshot of the 56 recovery unit's state at some point in time. A simple solution is to block the execution of the application program while the checkpoint is being taken <ref> [KMBT92, TS84] </ref>. This policy, however, can have a severe effect on the performance and thus saving a checkpoint in Manetho is both incremental and concurrent [Joh89, LNP90]. <p> Their results showed that concurrent checkpoint-ing reduces the checkpointing overhead for programs running on shared memory multiprocessors. The results presented here extend their results to distributed systems. They did not implement incremental checkpointing which proved to be an important optimization. Kaashoek et al. <ref> [Kaa92, KMBT92] </ref> implemented consistent checkpointing to add fault tolerance to Orca, a distributed object-oriented language. Their implementation uses a broadcast protocol to order marker messages with application messages. Processes block while their checkpoints are being written on stable storage.
Reference: [KT87] <author> R. Koo and S. Toueg. </author> <title> Checkpointing and rollback-recovery for distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-13(1):23-31, </volume> <month> January </month> <year> 1987. </year>
Reference-contexts: A global consistent state is one that could have occurred during a failure-free execution of the application program [CL85]. Finding this consistent state may entail rolling back processes that have survived the failure <ref> [JZ90, KT87, SY85] </ref>. Rollback-recovery has the potential for providing fault tolerance with low overhead in resources and performance. Stable storage devices typically use magnetic disks and are inexpensive. The performance overhead results from saving recovery information on stable storage during failure-free operation. <p> Therefore, to allow rollback-recovery to co-exist with process replication, the recovery protocol must restart the system from a consistent state without rolling back the replicated processes. Many rollback-recovery protocols cannot give such a guarantee <ref> [CL85, JZ88, KT87, SY85] </ref>. Combining rollback-recovery and process replication in Manetho is made possible by using an antecedence graph. This data structure tracks nondeterministic events that occur during failure-free operation. <p> Each RU maintains one permanent checkpoint, belonging to the most recent consistent checkpoint. During each run of the protocol, each RU takes a tentative checkpoint, which replaces the permanent one only if the protocol terminates successfully <ref> [KT87] </ref>. Each RU also maintains a consistent check 24 procedure Input (m; r) s m:SENDER ; r:SI r:SI + 1; AddNewNode (r:SI ; r:AG ; s; m); Deliver (m; s); return; point number (CCN ), which is incremented by one each time a consistent checkpoint is started. <p> Distributed Checkpointing Most previous work in distributed checkpointing has concentrated on issues such as reducing the number of messages required to coordinate a checkpoint [BL88, BCS84, CJ91, LY87, LNP91, SK86, TKT89, VRL87], or limiting the number of hosts that have to participate in taking the checkpoint or in rolling back <ref> [AL89, KYA86, IM89, KT87, KT87, WF92a] </ref>. Manetho is the first system to use coordinated checkpointing with message logging. This combination can be viewed as an enhancement of coordinated checkpointing protocols to enable them to interact more efficiently with the outside world.
Reference: [KT91] <author> M.F. Kaashoek and A.S. Tanenbaum. </author> <title> Group communication in the Amoeba distributed operating system. </title> <booktitle> In Proceedings of the 11th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 222-230, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Sequencer-Based Multicasts Having a sequencer define the receipt order of a multicast was used in the mul-ticast protocol of Chang and Maxemchuck [CM84], the implementation of ISIS's ABCAST [BJ87b, BSS91], Amoeba's atomic broadcast protocol <ref> [KT91] </ref>, and the Delta-4 system [BHV + 90, CPR + 92, VRB89]. The r-resilient protocol of Chang and Maxemchuck relies on negative acknowledgment and leadership transfer to achieve reliable total ordering. However, the protocol cannot deliver a multicast before r 1 subsequent leadership transfers occur.
Reference: [KYA86] <author> K.H. Kim, J.H. You, and A. Aboulnaga. </author> <title> A scheme for coordinated execution of independently designed recoverable distributed processes. </title> <booktitle> In Proceedings of the 16th International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 130-135, </pages> <year> 1986. </year>
Reference-contexts: Distributed Checkpointing Most previous work in distributed checkpointing has concentrated on issues such as reducing the number of messages required to coordinate a checkpoint [BL88, BCS84, CJ91, LY87, LNP91, SK86, TKT89, VRL87], or limiting the number of hosts that have to participate in taking the checkpoint or in rolling back <ref> [AL89, KYA86, IM89, KT87, KT87, WF92a] </ref>. Manetho is the first system to use coordinated checkpointing with message logging. This combination can be viewed as an enhancement of coordinated checkpointing protocols to enable them to interact more efficiently with the outside world.
Reference: [Lad89] <author> R. Ladin. </author> <title> A Method for Constructing Highly Available Services and a Technique for Distributed Garbage Collection. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: On the other hand, the application must be structured as a sequence of transactions. In comparison, Manetho limits the allowed nondeterminism, but is completely transparent to the application program. 95 Lazy Replication Lazy replication advocates using the application semantics for efficient replication support <ref> [Lad89, LLSG92] </ref>. The programmer classifies the primitives of a replicated service according to their types. Non-conflicting operations are allowed to proceed in parallel at different replicas possibly in different orders.
Reference: [Lam78] <author> L. Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: receiver of a message determines from the incarnation number tagging it whether it originated during the current incarnation of the sender or not. 2.3 The Antecedence Graph 2.3.1 Description The antecedence graph of a state interval p i contains a summary of the system's execution that happened before p i <ref> [Lam78] </ref>. It contains a node representing p node for each state interval that happened before p i . The edges of the graph represent the happened before relation between the state intervals. <p> The reader should bear in mind though that all multicast protocols described below are not limited to supporting active replication. Psync and Consul The context graph of Psync [PBS89] represents a variation on Lamport's happened-before relation <ref> [Lam78] </ref>. A group of processes participates in a conversation in which every process receives every message sent by any participant. The processes maintain a context graph that orders messages according to the context relation.
Reference: [LF90] <author> C.C. Li and W.K. Fuchs. </author> <title> CATCH: Compiler-assisted techniques for checkpointing. </title> <booktitle> In Proceedings of the 20th International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 74-81, </pages> <year> 1990. </year>
Reference-contexts: A limited form of incremental checkpointing is used: the application code is written to the checkpoint only once, but the entire data space is written out on each checkpoint, whether modified or not. A different technique for checkpointing relies on compiler support <ref> [LF90, LFA92] </ref>. Routines to save the state of the process are automatically inserted by the compiler in the application program. The checkpointing interval is determined statically by compiler analysis.
Reference: [LFA92] <author> J. Long, W.K. Fuchs, and J.A. Abraham. </author> <title> Compiler-assisted static checkpoint insertion. </title> <booktitle> In Proceedings of the 22nd International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 58-65, </pages> <month> July </month> <year> 1992. </year> <month> 108 </month>
Reference-contexts: A limited form of incremental checkpointing is used: the application code is written to the checkpoint only once, but the entire data space is written out on each checkpoint, whether modified or not. A different technique for checkpointing relies on compiler support <ref> [LF90, LFA92] </ref>. Routines to save the state of the process are automatically inserted by the compiler in the application program. The checkpointing interval is determined statically by compiler analysis.
Reference: [LLSG92] <author> R. Ladin, B. Liskov, L. Shrira, and S. Ghemawat. </author> <title> Providing high availability using lazy replication. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10 </volume> <pages> 360-391, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: On the other hand, the application must be structured as a sequence of transactions. In comparison, Manetho limits the allowed nondeterminism, but is completely transparent to the application program. 95 Lazy Replication Lazy replication advocates using the application semantics for efficient replication support <ref> [Lad89, LLSG92] </ref>. The programmer classifies the primitives of a replicated service according to their types. Non-conflicting operations are allowed to proceed in parallel at different replicas possibly in different orders.
Reference: [LMC87] <author> T.J. LeBlanc and J.M. Mellor-Crummey. </author> <title> Debugging parallel programs with Instant Replay. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(4):471-482, </volume> <month> April </month> <year> 1987. </year>
Reference-contexts: The descriptor's type field indicates a synchronization operation. Remaining fields contain the recovery unit's identifier, the thread identifier, and the state interval index. Similar techniques have been used in Instant Replay for debugging parallel programs <ref> [LMC87] </ref> and in an implementation of optimistic recovery [GGL + 90]. 54 System Calls The V-System implements system calls by local remote procedure calls to the kernel and system servers. Unlike monolithic kernels, the number of system calls available to application programs is limited.
Reference: [LNP90] <author> K. Li, J.F. Naughton, and J.S. Plank. </author> <title> Real-time, concurrent checkpoint for parallel programs. </title> <booktitle> In Proceedings of the 1990 Conference on the Principles and Practice of Parallel Programming, </booktitle> <pages> pages 79-88, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Contrary to previous assumptions [BL88, SY85], coordinated checkpointing is an efficient technique for implementing rollback-recovery. Moreover, the implementation shows that using coordinated checkpointing in message logging protocols improves performance and reduces the complexity of garbage collec tion. 2. Concurrent and incremental checkpointing <ref> [LNP90] </ref> are essential for low failure free performance overhead. 3. Sender-based message logging [JZ87, SBY88] has proved to be superior to receiver-based logging [JV91, JZ90, SW89, SY85]. 4. <p> A simple solution is to block the execution of the application program while the checkpoint is being taken [KMBT92, TS84]. This policy, however, can have a severe effect on the performance and thus saving a checkpoint in Manetho is both incremental and concurrent <ref> [Joh89, LNP90] </ref>. Incremental Checkpointing Instead of writing the entire address space on stable storage during each checkpoint, only the memory pages that have been modified since the previous checkpoint are written. <p> If no memory is available to allocate a new page for handling the copy-on-write fault, the application program is blocked until memory can be allocated. This scheme is similar to that used by Li et al. in their implementation of checkpointing on shared memory multiprocessors <ref> [LNP90] </ref>. The interference of concurrent checkpointing with the progress of the application program is small. Blocking the application occurs only while initializing the protection on the address space, which takes only a few tens of microseconds. <p> In addition, if the application program quickly modifies additional pages of the address space, it may have to be suspended to complete the checkpoint. The overhead introduced by copy-on-write is always less than or equal to that introduced by precopying [EJZ92]. 93 Li et al. <ref> [LNP90] </ref> described several checkpointing methods for programs executing on shared memory multiprocessors. Their results showed that concurrent checkpoint-ing reduces the checkpointing overhead for programs running on shared memory multiprocessors. The results presented here extend their results to distributed systems.
Reference: [LNP91] <author> K. Li, J.F. Naughton, and J.S. Plank. </author> <title> Checkpointing multicomputer applications. </title> <booktitle> In Proceedings of the 10th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 1-10, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The antecedence graph also provides support for a limited form of nondeterministic execution, which is not possible with sender-based message logging. 5.1.2 Distributed Checkpointing Most previous work in distributed checkpointing has concentrated on issues such as reducing the number of messages required to coordinate a checkpoint <ref> [BL88, BCS84, CJ91, LY87, LNP91, SK86, TKT89, VRL87] </ref>, or limiting the number of hosts that have to participate in taking the checkpoint or in rolling back [AL89, KYA86, IM89, KT87, KT87, WF92a]. Manetho is the first system to use coordinated checkpointing with message logging.
Reference: [LS79] <author> B.W. Lampson and H.E. Sturgis. </author> <title> Crash recovery in a distributed data storage system. </title> <type> Technical report, </type> <institution> Xerox Palo Alto Research Center, </institution> <month> April </month> <year> 1979. </year>
Reference-contexts: The communication subsystem is assumed to be unreliable and asynchronous: a message may be lost, duplicated, or arbitrarily delayed. However, corrupted messages are detected and suppressed. 5. Each RU has access to a highly-available, stable storage device that survives failures <ref> [LS79] </ref>. 6. Messages sent by an RU to the outside world may produce external effects that cannot be reversed. Furthermore, the interface to the outside world cannot be relied on to resend input messages for recovery purposes. 7. <p> It is also assumed that updating the volatile version is much faster than updating the persistent one <ref> [LS79] </ref>. Thus, performance considerations prohibit updating the persistent version whenever the volatile version changes. For convenience, the notation p:x is used to refer to data structure x of RU p. A description of all data structures follows: ID : The identifier of the RU.
Reference: [LY87] <author> T.H. Lai and T.H. Yang. </author> <title> On distributed snapshots. </title> <journal> Information Processing Letters, </journal> <volume> 25 </volume> <pages> 153-158, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: This number tags every message exchanged under Manetho's communication protocol (see Figure 2.5). The CCN enables the protocol to run in the presence of message reordering or loss <ref> [BCS84, LY87] </ref>. Additionally, each RU includes in the checkpoint the state interval index (P SI ) during which the RU took the checkpoint. Thus, if an RU p fails, it will restart at state interval p:P SI. The protocol proceeds as follows: 1. <p> Since this message was sent after its sender has started participating in the consistent checkpoint, the receiver must save its state before receiving this message to maintain the consistency of the global checkpoint <ref> [BCS84, LY87] </ref>. 3. After the tentative checkpoint has been written on stable storage, the RU sends an acknowledgment message to the coordinator. 4. The coordinator collects the responses from all RU's, and if all tentative checkpoints have been successful, it broadcasts a success message; otherwise, it broadcasts an abort message. <p> The antecedence graph also provides support for a limited form of nondeterministic execution, which is not possible with sender-based message logging. 5.1.2 Distributed Checkpointing Most previous work in distributed checkpointing has concentrated on issues such as reducing the number of messages required to coordinate a checkpoint <ref> [BL88, BCS84, CJ91, LY87, LNP91, SK86, TKT89, VRL87] </ref>, or limiting the number of hosts that have to participate in taking the checkpoint or in rolling back [AL89, KYA86, IM89, KT87, KT87, WF92a]. Manetho is the first system to use coordinated checkpointing with message logging.
Reference: [MS92] <author> S. Mishra and R. Schlichting. </author> <title> Abstractions for constructing dependable distributed systems. </title> <type> Technical Report TR02-19, </type> <institution> University of Arizona, </institution> <year> 1992. </year>
Reference-contexts: The Consul system has been implemented on top of Psync and offers services similar to those provided by Manetho, such as rollback-recovery for client processes and replication support for server processes <ref> [MS92] </ref>. Consul is designed to exploit the semantics of the application program, so that messages can be delivered according to Psync's context order whenever possible. The context order does not guarantee that the replicas of a server receive the messages in the same order.
Reference: [MSMA90] <author> P.M. Melliar-Smith, L.E. Moser, and V. Agrawala. </author> <title> Broadcast protocols for distributed systems. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 17-25, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Several copies of a server program are run on different machines. One copy is distinguished and is called the leader, while the remaining copies are called cohorts. During failure-free operation, replicas of a server program must receive the same set of messages in the same order <ref> [ADKM92, Bir85, BSS91, MSMA90, PBS89, VRB89, Woo93] </ref>. Otherwise, their executions will not remain identical. For this purpose, Manetho features a multicast protocol that ensures ordered delivery of application messages among all replicas. Leaders represent their corresponding replicated servers when interacting with application programs or other servers. <p> On the other hand, negative acknowledgment protocols reduce the number of control messages by piggybacking the ordering information on application multi-casts <ref> [CM84, Kaa92, MSMA90] </ref>. However, reducing the number of control messages or eliminating them altogether introduces latency in achieving agreement on the receipt order of an application multicast, delaying the delivery of the multicast to the application program. <p> Delta-4 relies on a special network adapter to provide the ordering and reliability, and to mask the overhead of acknowledgment messages from the application program. In contrast, Manetho does not depend on special network support. Trans/Total The atomic broadcast protocol of Melliar-Smith et al. <ref> [MSMA90] </ref> uses no control messages during normal operation. Eliminating control messages results in a delivery delay that depends mainly on the rate of incoming application broadcasts. A performance study shows that an application message may wait for seven subsequent application messages on average before it is delivered [MSMA90]. <p> Melliar-Smith et al. <ref> [MSMA90] </ref> uses no control messages during normal operation. Eliminating control messages results in a delivery delay that depends mainly on the rate of incoming application broadcasts. A performance study shows that an application message may wait for seven subsequent application messages on average before it is delivered [MSMA90]. Therefore, the approach used in Trans/Total is only suitable when the rate of application multicasts is extremely high. The latency of message delivery in Manetho is small regardless of the rate of incoming application multicasts.
Reference: [OCD + 88] <author> J.K. Ousterhout, A.R. Cherenson, F. Douglis, M.N. Nelson, and B.B. Welch. </author> <title> The Sprite network operating system. </title> <journal> IEEE Computer, </journal> <volume> 21(2) </volume> <pages> 23-36, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Read and write operations occur in multiples of disk blocks, up to eight blocks in one operation. The disk block size is eight Kbytes, which is also the size of a memory page on a Sun 3/60. The stable storage server does not maintain a conventional disk cache <ref> [OCD + 88] </ref>, since it would be useless given the expected workload. To provide efficient support for incremental checkpointing, the stable storage server supports file versioning. Versions of a single file share disk blocks common between them.
Reference: [OCH + 85] <author> J.K. Ousterhout, H. Da Costa, D. Harrison, J.A. Kunze, M. Kupfer, and J.G. Thompson. </author> <title> A trace-driven analysis of the UNIX 4.2 BSD file system. </title> <booktitle> In Proceedings of the 10th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 15-24, </pages> <year> 1985. </year> <month> 109 </month>
Reference-contexts: This alternative has the advantages of simplicity and generality. However, it has a performance disadvantage. Ordinary file servers are optimized for situations where most files are small and short lived, and where the workload is dominated 61 by reads <ref> [OCH + 85] </ref>. Such optimizations would be of little value for a stable storage server in a rollback-recovery system, where the workload is write-dominated and the logs are large, long-lived, and seldom read.
Reference: [Pau88] <author> Randy Pausch. </author> <title> Adding Input and Output to the Transactional Model. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <month> August </month> <year> 1988. </year>
Reference-contexts: The distributed computation may interact with the outside world by sending and receiving messages. However, the issues of implementing the interface to the outside world are outside the scope of this dissertation. For a study of these issues, the reader is referred to the work of Pausch <ref> [Pau88] </ref>. 2.2.2 Assumptions The following is assumed about the distributed system: 1. The RU's do not have access to a common time base or synchronized clocks. 2. The RU's are fail-stop [SS83].
Reference: [PBS89] <author> L.L. Peterson, N.C. Bucholz, and R.D. Schlichting. </author> <title> Preserving and using context information in interprocess communication. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(3) </volume> <pages> 217-246, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Several copies of a server program are run on different machines. One copy is distinguished and is called the leader, while the remaining copies are called cohorts. During failure-free operation, replicas of a server program must receive the same set of messages in the same order <ref> [ADKM92, Bir85, BSS91, MSMA90, PBS89, VRB89, Woo93] </ref>. Otherwise, their executions will not remain identical. For this purpose, Manetho features a multicast protocol that ensures ordered delivery of application messages among all replicas. Leaders represent their corresponding replicated servers when interacting with application programs or other servers. <p> Using the antecedence graph also leads to several advantages with respect to process replication. Manetho's multicast protocol offers high throughput and low latency message delivery. Having both these features was not possible in previous multicast protocols without relying on application semantics <ref> [BSS91, PBS89] </ref>. In contrast, Manetho's multicast is application-transparent. A prototype for Manetho has been implemented on a network multicomputer consisting of a 10 Mbits/second Ethernet network connecting 16 Sun-3/60 workstations. <p> The reader should bear in mind though that all multicast protocols described below are not limited to supporting active replication. Psync and Consul The context graph of Psync <ref> [PBS89] </ref> represents a variation on Lamport's happened-before relation [Lam78]. A group of processes participates in a conversation in which every process receives every message sent by any participant. The processes maintain a context graph that orders messages according to the context relation. <p> This deterministic filter introduces delay in delivering the messages. Specifically, if a message m is received, it cannot be delivered until it becomes a member of a "stable" wave, which requires that each process in the system send a message in the context of m <ref> [PBS89] </ref>. The delay thus is generally linear in the number of the processes in the conversation. * The context graph requires that each process receive every message. Thus, a process is forced to pay the overhead of receiving all the messages whether it is the intended receiver or not.
Reference: [PP83] <author> M.L. Powell and D.L. Presotto. </author> <title> Publishing: A reliable broadcast communication mechanism. </title> <booktitle> In Proceedings of the 9th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 100-109, </pages> <month> October </month> <year> 1983. </year>
Reference-contexts: Furthermore, as checkpoints accumulate on stable storage, the system needs to invoke a garbage collection procedure. 70 Coordinated Checkpointing Manetho's checkpointing protocol was chosen as a representative of coordinated check-pointing protocols. Receiver-Based Optimistic Message Logging (RBML) In RBML, processes use message logging in addition to independent checkpoint-ing <ref> [BBG + 89, JV91, JZ88, PP83, SW89, SY85] </ref>. The processes that participate in the distributed computation log the messages that they receive during failure-free operation on stable storage. Asynchronous logging is used to avoid the overhead of synchronous stable storage access. <p> These advantages result from using the antecedence graph. The performance measurements show that the overhead of maintaining the antecedence graph is negligible. Pessimistic Message Logging In pessimistic logging, processes log recovery information synchronously on stable storage <ref> [BBG + 89, Jal89, PP83] </ref>. These systems usually use hardware support to mitigate the effects of synchronous logging.
Reference: [Ran75] <author> B. Randell. </author> <title> System structure for software fault tolerance. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-1(2):220-232, </volume> <month> June </month> <year> 1975. </year>
Reference-contexts: This technique does not guarantee that a consistent set of checkpoints exists on stable storage, and therefore the computation may roll back to the initial state (the domino effect) <ref> [Ran75] </ref>. Furthermore, as checkpoints accumulate on stable storage, the system needs to invoke a garbage collection procedure. 70 Coordinated Checkpointing Manetho's checkpointing protocol was chosen as a representative of coordinated check-pointing protocols. <p> The study in Chapter 4 shows that the difference in performance between independent and coordinated checkpointing is marginal, and that independent check-pointing causes two important problems. First, it is susceptible to the domino effect <ref> [Ran75, Rus80] </ref>, where a failure may roll back the computation to the initial state losing all work done. Second, it requires intricate garbage collection and recovery protocols, complicating the implementation and causing more overhead in processing time and in the required space on stable storage.
Reference: [Rus80] <author> D.L. Russell. </author> <title> State restoration in systems of communicating processes. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-6(2):183-194, </volume> <month> March </month> <year> 1980. </year>
Reference-contexts: The study in Chapter 4 shows that the difference in performance between independent and coordinated checkpointing is marginal, and that independent check-pointing causes two important problems. First, it is susceptible to the domino effect <ref> [Ran75, Rus80] </ref>, where a failure may roll back the computation to the initial state losing all work done. Second, it requires intricate garbage collection and recovery protocols, complicating the implementation and causing more overhead in processing time and in the required space on stable storage.
Reference: [SBY88] <author> R.E. Strom, D.F. Bacon, and S.A. Yemini. </author> <title> Volatile logging in n-fault-tolerant distributed systems. </title> <booktitle> In Proceedings of the 18th International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 44-49, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Moreover, the implementation shows that using coordinated checkpointing in message logging protocols improves performance and reduces the complexity of garbage collec tion. 2. Concurrent and incremental checkpointing [LNP90] are essential for low failure free performance overhead. 3. Sender-based message logging <ref> [JZ87, SBY88] </ref> has proved to be superior to receiver-based logging [JV91, JZ90, SW89, SY85]. 4. The implementation provides empirical evidence supporting the conventional wisdom that positive acknowledgment multicast protocols provide lower latency than positive acknowledgment protocols at the expense of a lower throughput. <p> Instead, the implementation just logs the receipt order of the messages. Sender-Based Message Logging (SBML) The SBML protocol implemented here is modeled after the protocol of Strom et al, in which the processes log the messages at the sender and log the receipt order at the receiver <ref> [JZ87, SBY88] </ref>. The resulting protocol tolerates an arbitrary number of failures. In the implementation, each sender adds an O (1) dependence information on each message it sends, as suggested by Johnson [Joh89]. <p> Therefore, the combination of coordinated checkpointing with message logging leads to better performance than the traditional combination of message logging with independent checkpointing, unlike what is widely believed. 90 Optimistic Message Logging In traditional optimistic message logging, processes take independent checkpoints and log the messages asynchronously to stable storage <ref> [Joh89, JZ90, JV91, SBY88, SY85, SW89] </ref>. The optimistic assumption is that each message will be logged on stable storage before a failure occurs and thus will be available for recovery. If a failure occurs, the processes will roll back to the maximum recoverable state and resume operation [Joh89].
Reference: [SES + 92] <author> S.K. Shrivastava, P.D. Ezhilchelvan, N.A. Speirs, S. Tao, and A. Tully. </author> <title> Principal features of the VOLTAN family of reliable node architectures for distributed systems. </title> <journal> IEEE Transactions on Computers Special Issue On Fault-Tolerant Computing, </journal> <volume> 41(5) </volume> <pages> 542-549, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Rollback-recovery also provides fault tolerance transparently. By embodying state saving and restoration techniques in the operating system, the provision of rollback-recovery is automatic and no change to application programs is required. 5 1.3.2 Process Replication Manetho uses process replication to provide high availability to servers <ref> [ADL90, BF83, Bir85, Coo84, Coo85, CPR + 92, SES + 92] </ref>. Several copies of a server program are run on different machines. One copy is distinguished and is called the leader, while the remaining copies are called cohorts. <p> The comparison here considers only systems that operate in asynchronous distributed environments and provide replication for reliability. Systems that assume real-time 94 requirements <ref> [SES + 92] </ref>, or use replication for enhancing performance [Gol91] are not considered. CIRCUS CIRCUS supported process replication in an asynchronous network [Coo84, Coo85]. Replicated remote procedure calls were used to implement inter-troupe communication.
Reference: [SK86] <author> M. Spezialetti and P. Kearns. </author> <title> Efficient distributed snapshots. </title> <booktitle> In Proceedings of the 6th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 382-388, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: The antecedence graph also provides support for a limited form of nondeterministic execution, which is not possible with sender-based message logging. 5.1.2 Distributed Checkpointing Most previous work in distributed checkpointing has concentrated on issues such as reducing the number of messages required to coordinate a checkpoint <ref> [BL88, BCS84, CJ91, LY87, LNP91, SK86, TKT89, VRL87] </ref>, or limiting the number of hosts that have to participate in taking the checkpoint or in rolling back [AL89, KYA86, IM89, KT87, KT87, WF92a]. Manetho is the first system to use coordinated checkpointing with message logging.
Reference: [SS83] <author> R.D. Schlichting and F.B. Schneider. </author> <title> Fail-stop processors: An approach to designing fault-tolerant computing systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 1(3) </volume> <pages> 222-238, </pages> <month> August </month> <year> 1983. </year>
Reference-contexts: For a study of these issues, the reader is referred to the work of Pausch [Pau88]. 2.2.2 Assumptions The following is assumed about the distributed system: 1. The RU's do not have access to a common time base or synchronized clocks. 2. The RU's are fail-stop <ref> [SS83] </ref>. An RU that uses rollback-recovery fails by losing its volatile state and destroying its threads without transmitting incorrect messages.
Reference: [SW89] <author> A.P. Sistla and J.L. Welch. </author> <title> Efficient distributed recovery using message logging. </title> <booktitle> In Proceedings of the 8th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 223-238, </pages> <month> August </month> <year> 1989. </year> <month> 110 </month>
Reference-contexts: Concurrent and incremental checkpointing [LNP90] are essential for low failure free performance overhead. 3. Sender-based message logging [JZ87, SBY88] has proved to be superior to receiver-based logging <ref> [JV91, JZ90, SW89, SY85] </ref>. 4. The implementation provides empirical evidence supporting the conventional wisdom that positive acknowledgment multicast protocols provide lower latency than positive acknowledgment protocols at the expense of a lower throughput. The performance evaluation also includes a comparative study between Manetho and other systems. <p> Furthermore, as checkpoints accumulate on stable storage, the system needs to invoke a garbage collection procedure. 70 Coordinated Checkpointing Manetho's checkpointing protocol was chosen as a representative of coordinated check-pointing protocols. Receiver-Based Optimistic Message Logging (RBML) In RBML, processes use message logging in addition to independent checkpoint-ing <ref> [BBG + 89, JV91, JZ88, PP83, SW89, SY85] </ref>. The processes that participate in the distributed computation log the messages that they receive during failure-free operation on stable storage. Asynchronous logging is used to avoid the overhead of synchronous stable storage access. <p> The three protocols based on message logging (RBML, SBML, and Manetho) have higher overhead than the protocols based only on checkpointing. The overhead of message logging and dependence tracking outweighs the overhead of coordinated checkpointing, contrary to previous claims <ref> [SW89, SY85] </ref>. 3. Independent checkpointing performed worse on gauss than consistent check-pointing. This anomaly is due to the tight synchronization nature of the gauss program. Each iteration of gauss requires global communication among the recovery units to distribute the next pivot element and pivot column. <p> The other two message logging protocols require a multihost protocol to commit output, which entails exchanging several messages and performing several I/O operations on stable storage <ref> [Joh89, Joh93, SW89] </ref>. <p> Therefore, the combination of coordinated checkpointing with message logging leads to better performance than the traditional combination of message logging with independent checkpointing, unlike what is widely believed. 90 Optimistic Message Logging In traditional optimistic message logging, processes take independent checkpoints and log the messages asynchronously to stable storage <ref> [Joh89, JZ90, JV91, SBY88, SY85, SW89] </ref>. The optimistic assumption is that each message will be logged on stable storage before a failure occurs and thus will be available for recovery. If a failure occurs, the processes will roll back to the maximum recoverable state and resume operation [Joh89].
Reference: [SY85] <author> R.E. Strom and S.A. Yemini. </author> <title> Optimistic recovery in distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(3) </volume> <pages> 204-226, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: A global consistent state is one that could have occurred during a failure-free execution of the application program [CL85]. Finding this consistent state may entail rolling back processes that have survived the failure <ref> [JZ90, KT87, SY85] </ref>. Rollback-recovery has the potential for providing fault tolerance with low overhead in resources and performance. Stable storage devices typically use magnetic disks and are inexpensive. The performance overhead results from saving recovery information on stable storage during failure-free operation. <p> If the leader fails, the surviving cohorts will elect a new leader among themselves and operation will continue immediately, ensuring high availability. 1.3.3 The Antecedence Graph When a process fails, the recovery protocol may need to roll back several processes to restart from a global consistent state <ref> [CL85, SY85] </ref>. It is however undesirable to roll back replicated processes, because the services that they offer will not be available during recovery. Therefore, to allow rollback-recovery to co-exist with process replication, the recovery protocol must restart the system from a consistent state without rolling back the replicated processes. <p> Therefore, to allow rollback-recovery to co-exist with process replication, the recovery protocol must restart the system from a consistent state without rolling back the replicated processes. Many rollback-recovery protocols cannot give such a guarantee <ref> [CL85, JZ88, KT87, SY85] </ref>. Combining rollback-recovery and process replication in Manetho is made possible by using an antecedence graph. This data structure tracks nondeterministic events that occur during failure-free operation. <p> Typically, the recovery protocol logs information on stable storage before sending the message and thus introduces some delay. The act of logging information before sending messages to the outside world has been called "output commit" <ref> [SY85] </ref>. In Manetho, output commit is a low latency operation. A process committing output only needs to write its antecedence graph on stable storage. Unlike many other protocols, no multihost coordination is necessary. Using the antecedence graph also leads to several advantages with respect to process replication. <p> An empirical performance evaluation using the prototype implementation confirms that the advantages of the system are realized in practice. In addition, the evaluation reveals several results that are applicable to Manetho and are relevant to any rollback recovery or multicast protocol: 1. Contrary to previous assumptions <ref> [BL88, SY85] </ref>, coordinated checkpointing is an efficient technique for implementing rollback-recovery. Moreover, the implementation shows that using coordinated checkpointing in message logging protocols improves performance and reduces the complexity of garbage collec tion. 2. Concurrent and incremental checkpointing [LNP90] are essential for low failure free performance overhead. 3. <p> Concurrent and incremental checkpointing [LNP90] are essential for low failure free performance overhead. 3. Sender-based message logging [JZ87, SBY88] has proved to be superior to receiver-based logging <ref> [JV91, JZ90, SW89, SY85] </ref>. 4. The implementation provides empirical evidence supporting the conventional wisdom that positive acknowledgment multicast protocols provide lower latency than positive acknowledgment protocols at the expense of a lower throughput. The performance evaluation also includes a comparative study between Manetho and other systems. <p> Section 2.8 describes the recovery protocol and Section 2.9 provides proofs for its correctness. Section 2.10 summarizes the chapter. 2.2 System Model 2.2.1 Overview A distributed system consists of a number of recovery units (RU's) that communicate only through messages <ref> [SY85] </ref>. An RU is the unit of failure and recovery in the system. It can correspond to a process, a machine, or any unit of failure as conveniently defined by a particular implementation. Abstractly, it represents a volatile state manipulated by a number of threads according to some application program. <p> This assumption simplifies the presentation and makes the design independent of the communication protocol. 2.2.3 Nondeterministic Events and State Intervals The execution of an RU consists of a sequence of piecewise deterministic state intervals <ref> [SY85] </ref>. Each state interval starts by the occurrence of a nondeterministic event. <p> This is an instance of the problem of ordering the perception of failures with respect to application messages <ref> [BJ87a, SY85] </ref>. To solve this problem, each RU maintains an incarnation number . Each time an RU starts recovery from a failure, it increments its incarnation number and reliably sends it to all RU's before restarting execution. Each application message is tagged with the current incarnation number of the sender. <p> The overhead is low because no synchronous logging to stable storage takes place except during output commit. Therefore, the protocol behaves like optimistic rollback-recovery systems that use asynchronous logging <ref> [Joh89, SY85] </ref>. Unlike these systems, an output commit does not require several synchronization messages or several stable storage accesses. Committing output in Manetho only requires flushing the local antecedence graph in a single access to stable storage. <p> Such systems exclusively used independent checkpointing in combination with message logging, in the belief that the cost of coordination is prohibitively expensive. In fact, the original reason for advocating message logging was to avoid the overhead of coordinating the checkpoints in a distributed system <ref> [SY85] </ref>. The decision to use coordinated checkpointing in Manetho was motivated by two important factors. First, a performance study based on actual implementation has shown that the overhead of coordinating the checkpoints is negligible. This performance study is reported in Chapter 4. <p> The maximum number of versions a file may have is limited to 255. Thus, the stable storage server can support systems where several checkpoints may be maintained for each recovery unit, such as in optimistic recovery <ref> [Joh89, SY85] </ref>. When a checkpoint is to be discarded, the corresponding version is purged. The maximum throughput for writing to the server is 860 Kbytes/second. This maximum occurs when both server and client reside on the same machine and is included only as a reference. <p> Furthermore, as checkpoints accumulate on stable storage, the system needs to invoke a garbage collection procedure. 70 Coordinated Checkpointing Manetho's checkpointing protocol was chosen as a representative of coordinated check-pointing protocols. Receiver-Based Optimistic Message Logging (RBML) In RBML, processes use message logging in addition to independent checkpoint-ing <ref> [BBG + 89, JV91, JZ88, PP83, SW89, SY85] </ref>. The processes that participate in the distributed computation log the messages that they receive during failure-free operation on stable storage. Asynchronous logging is used to avoid the overhead of synchronous stable storage access. <p> The three protocols based on message logging (RBML, SBML, and Manetho) have higher overhead than the protocols based only on checkpointing. The overhead of message logging and dependence tracking outweighs the overhead of coordinated checkpointing, contrary to previous claims <ref> [SW89, SY85] </ref>. 3. Independent checkpointing performed worse on gauss than consistent check-pointing. This anomaly is due to the tight synchronization nature of the gauss program. Each iteration of gauss requires global communication among the recovery units to distribute the next pivot element and pivot column. <p> Therefore, the combination of coordinated checkpointing with message logging leads to better performance than the traditional combination of message logging with independent checkpointing, unlike what is widely believed. 90 Optimistic Message Logging In traditional optimistic message logging, processes take independent checkpoints and log the messages asynchronously to stable storage <ref> [Joh89, JZ90, JV91, SBY88, SY85, SW89] </ref>. The optimistic assumption is that each message will be logged on stable storage before a failure occurs and thus will be available for recovery. If a failure occurs, the processes will roll back to the maximum recoverable state and resume operation [Joh89]. <p> However, there are several drawbacks. The effects of failures are not confined to the processes that fail. A process that continues to operate on a functioning machine may become an orphan and have to roll back during the recovery of other processes <ref> [SY85] </ref>. In addition, each process must maintain several checkpoints, and a garbage collection protocol is required to reclaim the space used by the message logs. Optimistic recovery also introduces delays when communicating with the outside world.
Reference: [Tan88] <author> A.S. Tanenbaum. </author> <title> Computer Networks. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1988. </year>
Reference-contexts: The system includes a membership protocol that maintains a list of the func tioning RU's. The communication subsystem does not guarantee the reliable, ordered message delivery commonly needed by application programs. These properties are easily provided by a standard end-to-end protocol <ref> [Tan88] </ref>. The Manetho runtime system treats control messages used by the end-to-end protocol as ordinary application messages. Examples of such control messages include acknowledgments, messages maintaining the connection between two machines, and messages belonging to the flow control algorithm.
Reference: [TKT89] <author> Z. Tong, R.Y. Kain, and W.T. Tsai. </author> <title> A lower overhead checkpointing and rollback recovery scheme for distributed systems. </title> <booktitle> In Proceedings of the 8th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 12-20, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: The antecedence graph also provides support for a limited form of nondeterministic execution, which is not possible with sender-based message logging. 5.1.2 Distributed Checkpointing Most previous work in distributed checkpointing has concentrated on issues such as reducing the number of messages required to coordinate a checkpoint <ref> [BL88, BCS84, CJ91, LY87, LNP91, SK86, TKT89, VRL87] </ref>, or limiting the number of hosts that have to participate in taking the checkpoint or in rolling back [AL89, KYA86, IM89, KT87, KT87, WF92a]. Manetho is the first system to use coordinated checkpointing with message logging.
Reference: [TLC85] <author> M. Theimer, K. Lantz, and D.R. Cheriton. </author> <title> Preemptable remote execution facilities in the V-system. </title> <booktitle> In Proceedings of the 10th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 2-12, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: Examples include interprocess communication primitives, transparent remote execution, and libraries to support construction of distributed programs [VDG86]. In this environment, a recovery unit consists of an address space and the threads that manipulate it (a V-System's logical host <ref> [TLC85] </ref>). fl It does not include the kernel or system servers, as there is no attempt to make them recoverable. A recovery unit interacts with the outside world by receiving input from the keyboard and producing output on a workstation's display. <p> A recovery unit interacts with the outside world by receiving input from the keyboard and producing output on a workstation's display. If a failure occurs, the V-System addressing mechanism allows a recovery unit to be reinstalled and run on any available machine on the network <ref> [TLC85] </ref>. Support for Manetho consists of several user-level servers and kernel routines. Collectively, they implement the four protocols described in Chapter 2. Recovery units based on rollback-recovery include support for the communication, checkpoint-ing, and recovery protocols. <p> Second, it requires intricate garbage collection and recovery protocols, complicating the implementation and causing more overhead in processing time and in the required space on stable storage. Implementation techniques Johnson used a technique called precopying in implementing checkpointing <ref> [Joh89, TLC85] </ref>. In this technique, the pages to be written on stable storage are first copied to a separate area in memory if their number is below some threshold.
Reference: [TS84] <author> Y. Tamir and C.H. Sequin. </author> <title> Error recovery in multicomputers using global checkpoints. </title> <booktitle> In 1984 International Conference on Parallel Processing, </booktitle> <pages> pages 32-41, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: The second component results from the requirement that the saved state in the checkpoint file must represent an instantaneous snapshot of the 56 recovery unit's state at some point in time. A simple solution is to block the execution of the application program while the checkpoint is being taken <ref> [KMBT92, TS84] </ref>. This policy, however, can have a severe effect on the performance and thus saving a checkpoint in Manetho is both incremental and concurrent [Joh89, LNP90].
Reference: [VDG86] <institution> V-System Development Group. V-System 6.0 Reference Manual. Computer Systems Laboratory, Department of Computer Science and Electrical Engineering, Stanford University, </institution> <year> 1986. </year>
Reference-contexts: The workstations run a version of the V-System [Che88], to which Manetho's mechanisms are added. The application programs running on this multicomputer use the distributed processing facilities provided by the V-System. Examples include interprocess communication primitives, transparent remote execution, and libraries to support construction of distributed programs <ref> [VDG86] </ref>. In this environment, a recovery unit consists of an address space and the threads that manipulate it (a V-System's logical host [TLC85]). fl It does not include the kernel or system servers, as there is no attempt to make them recoverable.
Reference: [VRB89] <author> P. Ver issimo, L. Rodrigues, and M. Baptista. </author> <title> A highly parallel atomic multicast protocol. </title> <booktitle> In Proceedings of the Sigcomm '89 Symposium, </booktitle> <pages> pages 83-93, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Several copies of a server program are run on different machines. One copy is distinguished and is called the leader, while the remaining copies are called cohorts. During failure-free operation, replicas of a server program must receive the same set of messages in the same order <ref> [ADKM92, Bir85, BSS91, MSMA90, PBS89, VRB89, Woo93] </ref>. Otherwise, their executions will not remain identical. For this purpose, Manetho features a multicast protocol that ensures ordered delivery of application messages among all replicas. Leaders represent their corresponding replicated servers when interacting with application programs or other servers. <p> Sequencer-Based Multicasts Having a sequencer define the receipt order of a multicast was used in the mul-ticast protocol of Chang and Maxemchuck [CM84], the implementation of ISIS's ABCAST [BJ87b, BSS91], Amoeba's atomic broadcast protocol [KT91], and the Delta-4 system <ref> [BHV + 90, CPR + 92, VRB89] </ref>. The r-resilient protocol of Chang and Maxemchuck relies on negative acknowledgment and leadership transfer to achieve reliable total ordering. However, the protocol cannot deliver a multicast before r 1 subsequent leadership transfers occur.
Reference: [VRL87] <author> K. Venkatesh, T. Radhakrishnan, and H.F. Li. </author> <title> Optimal checkpointing and local recording for domino-free rollback recovery. </title> <journal> Information Processing Letters, </journal> <volume> 25 </volume> <pages> 295-303, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: The antecedence graph also provides support for a limited form of nondeterministic execution, which is not possible with sender-based message logging. 5.1.2 Distributed Checkpointing Most previous work in distributed checkpointing has concentrated on issues such as reducing the number of messages required to coordinate a checkpoint <ref> [BL88, BCS84, CJ91, LY87, LNP91, SK86, TKT89, VRL87] </ref>, or limiting the number of hosts that have to participate in taking the checkpoint or in rolling back [AL89, KYA86, IM89, KT87, KT87, WF92a]. Manetho is the first system to use coordinated checkpointing with message logging.
Reference: [Wel84] <author> T.A. Welch. </author> <title> A technique for high performance data compression. </title> <journal> IEEE Computer, </journal> <volume> 17(6) </volume> <pages> 8-19, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: To examine the effect of compression on message logging, the volatile logs were flushed to secondary storage, and a data compression algorithm was applied post mortem on the log files. The experiment used the compress program that is widely available on UNIX systems <ref> [Wel84] </ref>. Experiments showed that data compression is not effective with the applications studied.
Reference: [WF92a] <author> Y.-M. Wang and W.K. Fuchs. </author> <title> Optimistic message logging for independent checkpointing in message-passing systems. </title> <booktitle> In Proceedings of the 11th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 147-154, </pages> <month> Octo-ber </month> <year> 1992. </year> <month> 111 </month>
Reference-contexts: A brief description of each of these protocols follows. Independent Checkpointing With independent checkpointing, each process takes a checkpoint of its state without coordination with other processes <ref> [BL88, WF92a, WF92b] </ref>. If a failure occurs, the recovery algorithm will roll back the computation to a set of checkpoints on stable storage that forms the most recent consistent state. <p> Distributed Checkpointing Most previous work in distributed checkpointing has concentrated on issues such as reducing the number of messages required to coordinate a checkpoint [BL88, BCS84, CJ91, LY87, LNP91, SK86, TKT89, VRL87], or limiting the number of hosts that have to participate in taking the checkpoint or in rolling back <ref> [AL89, KYA86, IM89, KT87, KT87, WF92a] </ref>. Manetho is the first system to use coordinated checkpointing with message logging. This combination can be viewed as an enhancement of coordinated checkpointing protocols to enable them to interact more efficiently with the outside world.
Reference: [WF92b] <author> Y.-M. Wang and W.K. Fuchs. </author> <title> Scheduling message processing for reducing rollback propagation. </title> <booktitle> In Proceedings of the 22nd International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 204-211, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: A brief description of each of these protocols follows. Independent Checkpointing With independent checkpointing, each process takes a checkpoint of its state without coordination with other processes <ref> [BL88, WF92a, WF92b] </ref>. If a failure occurs, the recovery algorithm will roll back the computation to a set of checkpoints on stable storage that forms the most recent consistent state.
Reference: [Woo93] <author> M. Wood. </author> <title> Replicated RPC using Amoeba closed group communication. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Distributed Computing Systems, </booktitle> <pages> pages 499-507, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Several copies of a server program are run on different machines. One copy is distinguished and is called the leader, while the remaining copies are called cohorts. During failure-free operation, replicas of a server program must receive the same set of messages in the same order <ref> [ADKM92, Bir85, BSS91, MSMA90, PBS89, VRB89, Woo93] </ref>. Otherwise, their executions will not remain identical. For this purpose, Manetho features a multicast protocol that ensures ordered delivery of application messages among all replicas. Leaders represent their corresponding replicated servers when interacting with application programs or other servers. <p> It is also application-transparent. Amoeba's atomic broadcast uses negative acknowledgment for the 0-resilient version, and positive acknowledgment otherwise. A replicated RPC library has been implemented on top of the broadcast protocol to provide higher level support for applications <ref> [Woo93] </ref>. Amoeba's protocol is highly tuned for the 0-resilient operation mode. The r-resilient version of Amoeba's requires r 1 overhead messages for each application multicast. Manetho does not require such overhead messages.
References-found: 80

