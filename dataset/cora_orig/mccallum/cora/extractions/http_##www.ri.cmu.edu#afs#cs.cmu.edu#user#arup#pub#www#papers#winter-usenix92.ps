URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/arup/pub/www/papers/winter-usenix92.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/arup/pub/www/resume.html
Root-URL: 
Title: Issues in Implementation of Cache-Affinity Scheduling  
Author: Murthy Devarakonda Arup Mukherjee 
Affiliation: IBM Thomas J. Watson Research Center  
Abstract: In a shared memory multiprocessor, a thread may have an affinity to a processor because of the data remaining in the processor's cache from a previous dispatch. We show that two basic problems should be addressed in a Unix-like system to exploit cache affinity for improved performance: First, the limitation of the Unix dispatcher model ("processor seeking a thread"); Second, pseudo-affinity caused by low-cost waiting techniques used in a threads package such as C Threads. We demonstrate that the affinity scheduling is most effective when used in a threads package that supports multiplexing of user threads on kernel threads.
Abstract-found: 1
Intro-found: 1
Reference: [Accetta86] <author> M. J. Accetta, et al, </author> <title> "Mach: A New Kernel Foundation for UNIX Development," </title> <booktitle> Proc. of the Summer Usenix, </booktitle> <month> July </month> <year> 1986. </year>
Reference: [Cooper87] <author> E. Cooper and R. Draves, </author> <title> "C Threads", </title> <type> Tech. Report CMU-CS-88-154, </type> <institution> Computer Science Dept., Carnegie Mellon University, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: Exposing and resolving these issues is, therefore, the main focus of this paper. We present and discuss these issues through a description of our experiences in implementing affinity scheduling on an Encore Multimax running Mach 2.5. (We used the thread primitives of the C Threads package <ref> [Cooper87] </ref>.) First, we implemented a simple yet effective affinity scheme in the Mach kernel. <p> For the second level cache, each cache line is eight bytes long. allocated for each thread. Subsequently, all threads execute an identical loop, which consists of accessing data in the footprint and then synchronizing at the barrier. The original C Threads library <ref> [Cooper87] </ref>, as supplied with Mach 2.5, is used to create the threads and to synchronize them at the end of each iteration. The number of threads and the footprint size are the key parameters to the program.
Reference: [Fox88] <author> G. Fox, et al, </author> <title> Solving Problems on Concurrent Processors, Vol. I, </title> <publisher> Prentice-Hall, </publisher> <year> 1988. </year>
Reference-contexts: The effect of affinity remains unchanged for most numbers of threads and input sizes. Parallel merge sort: The parallel sorting program is along the lines described in <ref> [Fox88] </ref>. Given a linear array of numbers for sorting, the thread works as follows: If the number of elements is smaller than a predefined value, N , then it sorts the numbers using the bubble sort and returns.
Reference: [Golub90] <author> D. Golub, et al, </author> <title> "Unix as an Application Program," </title> <booktitle> Proc. of the Summer Usenix, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: This solution is clearly superior to changing the kernel since such a threads package can provide several other benefits such as a mechanism for allocating processors in a multiprogrammed environment [Tucker89], a large number of user threads <ref> [Golub90] </ref>, and light-weight threads [Powell91]. We also made measurements using two real parallel applications: A thinning program for digital patterns, and a parallel merge sort program. <p> END &lt;repeat loop&gt; END &lt;parallel code&gt; Print out time spent in parallel section. Several papers have described the use and implementation of multiplexed threads packages: [Tucker89] for efficient scheduling of multiprogrammed, multi-threaded programs; <ref> [Golub90] </ref> to provide a large number of threads per application; [Powell91] for controlling concurrency and for light-weight. Here we use the multiplexed user threads for successful exploitation of affinity. 3 Kernel-level implementation This section provides a detailed description of our experiences in implementing cache affinity at the kernel-level.
Reference: [Gupta91] <author> A. Gupta, A. Tucker, and S. Urushibara, </author> <title> "The Impact of Operating System Scheduling Policies and Synchronization Methods on the Performance of Parallel Applications," </title> <booktitle> Proc. of the 1991 ACM SIGMETRICS Conf., </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: Increasing cache sizes and a growing disparity between the access times of cache and main memories make it an attractive optimization opportunity. While the modeling study of [Squillante90] has shown potential performance gain from using such cache affinity, the simulated-environment based study of <ref> [Gupta91] </ref> has shown only small gains. A more recent (concurrent) study [Vaswani91] has concluded that the kernel-level affinity scheduling has a negligible effect for multiprogrammed, multi-threaded applications. <p> Our measurements show that even a simple scheme can produce significant benefit given a suitable application and proper implementation strategy. Two other studies have considered cache affinity. <ref> [Gupta91] </ref> describes experiments using an affinity scheme derived from the Minimum Intervening method of [Squillante90] in a simulated multiprocessor environment. The study reports a small positive effect on performance for a benchmark suite.
Reference: [Lu86] <author> H. E. Lu and P. S. P. Wang, </author> <title> "A Comment on a Fast Parallel Algorithm for Thinning Digital Patterns," </title> <journal> Comm. of ACM, </journal> <month> March </month> <year> 1986. </year>
Reference-contexts: The goal is to retain the structure of the image while getting rid of the clutter. We implemented the parallel algorithm described in <ref> [Lu86] </ref> and [Zhang84] with minor changes. The algorithm works iteratively, terminating when no more pixels can be turned off. The matrix is divided into n 2 sub-blocks, and each thread of the program works on a sub-block.
Reference: [Powell91] <editor> M. L. Powell, et al, </editor> <booktitle> "SunOS Multi-thread Architecture," Proc. of the Winter Usenix, </booktitle> <month> January </month> <year> 1991. </year>
Reference-contexts: This solution is clearly superior to changing the kernel since such a threads package can provide several other benefits such as a mechanism for allocating processors in a multiprogrammed environment [Tucker89], a large number of user threads [Golub90], and light-weight threads <ref> [Powell91] </ref>. We also made measurements using two real parallel applications: A thinning program for digital patterns, and a parallel merge sort program. <p> END &lt;repeat loop&gt; END &lt;parallel code&gt; Print out time spent in parallel section. Several papers have described the use and implementation of multiplexed threads packages: [Tucker89] for efficient scheduling of multiprogrammed, multi-threaded programs; [Golub90] to provide a large number of threads per application; <ref> [Powell91] </ref> for controlling concurrency and for light-weight. Here we use the multiplexed user threads for successful exploitation of affinity. 3 Kernel-level implementation This section provides a detailed description of our experiences in implementing cache affinity at the kernel-level.
Reference: [Squillante90] <author> M. Squillante and E. Lazowska, </author> <title> "Using Processor-Cache Affinity Information in Shared-Memory Multiprocessor Scheduling," </title> <type> Research Report RC 17128, </type> <institution> IBM T. J. Watson Research Center, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Increasing cache sizes and a growing disparity between the access times of cache and main memories make it an attractive optimization opportunity. While the modeling study of <ref> [Squillante90] </ref> has shown potential performance gain from using such cache affinity, the simulated-environment based study of [Gupta91] has shown only small gains. A more recent (concurrent) study [Vaswani91] has concluded that the kernel-level affinity scheduling has a negligible effect for multiprogrammed, multi-threaded applications. <p> Implications of these results are discussed later. The next section briefly reviews the related work. Sections 3 and 4 describe and discuss kernel and user-level affinity implementations respectively. Section 5 presents results for the real applications. Finally section 6 concludes the paper. 2 Related Work In <ref> [Squillante90] </ref>, several affinity scheduling algorithms have been proposed, and the algorithms have been evaluated using queueing models. One of the two simple schemes implemented in our user-level threads package, the FCFS Affinity, is quite similar to the Last Processor scheme of [Squillante90]. <p> section 6 concludes the paper. 2 Related Work In <ref> [Squillante90] </ref>, several affinity scheduling algorithms have been proposed, and the algorithms have been evaluated using queueing models. One of the two simple schemes implemented in our user-level threads package, the FCFS Affinity, is quite similar to the Last Processor scheme of [Squillante90]. Our measurements show that even a simple scheme can produce significant benefit given a suitable application and proper implementation strategy. Two other studies have considered cache affinity. [Gupta91] describes experiments using an affinity scheme derived from the Minimum Intervening method of [Squillante90] in a simulated multiprocessor environment. <p> quite similar to the Last Processor scheme of <ref> [Squillante90] </ref>. Our measurements show that even a simple scheme can produce significant benefit given a suitable application and proper implementation strategy. Two other studies have considered cache affinity. [Gupta91] describes experiments using an affinity scheme derived from the Minimum Intervening method of [Squillante90] in a simulated multiprocessor environment. The study reports a small positive effect on performance for a benchmark suite. Three factors limit generalization of results from this study: suitability of the benchmarks for affinity scheduling, small cache size, and the simulated environment. <p> A multiplexed user-threads package is not only a general solution for this problem, but also an efficient one (as shown in this paper). The avoidance of pseudo-affinity is a concern in any threads package when affinity can't be quantified. Perhaps hardware support <ref> [Squillante90] </ref> can be used to quantify affinity and hence to deal with pseudo-affinity. Until then the dispatcher and synchronization primitives must avoid scenarios causing pseudo-affinity. Is cache affinity scheduling worth the trouble? As caches become larger and faster relative to main memory, the benefits of cache affinity scheduling will increase.
Reference: [Tucker89] <author> A. Tucker and A. Gupta, </author> <title> "Process Control and Scheduling Issues for Multiprogrammed Shared-Memory Multiprocessors," </title> <booktitle> Proc. of the 12th ACM Symp. on Operating Systems Principles, </booktitle> <month> December </month> <year> 1989. </year>
Reference-contexts: This solution is clearly superior to changing the kernel since such a threads package can provide several other benefits such as a mechanism for allocating processors in a multiprogrammed environment <ref> [Tucker89] </ref>, a large number of user threads [Golub90], and light-weight threads [Powell91]. We also made measurements using two real parallel applications: A thinning program for digital patterns, and a parallel merge sort program. <p> END &lt;repeat loop&gt; END &lt;parallel code&gt; Print out time spent in parallel section. Several papers have described the use and implementation of multiplexed threads packages: <ref> [Tucker89] </ref> for efficient scheduling of multiprogrammed, multi-threaded programs; [Golub90] to provide a large number of threads per application; [Powell91] for controlling concurrency and for light-weight.
Reference: [Vaswani91] <author> R. Vaswani and J. Zahorjan, </author> <title> "The Implications of Cache Affinity on Processor Scheduling for Multiprogrammed, Shared Memory Multiprocessors," </title> <booktitle> Proc. of the 13th ACM Symp. on Operating Systems Principles, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: While the modeling study of [Squillante90] has shown potential performance gain from using such cache affinity, the simulated-environment based study of [Gupta91] has shown only small gains. A more recent (concurrent) study <ref> [Vaswani91] </ref> has concluded that the kernel-level affinity scheduling has a negligible effect for multiprogrammed, multi-threaded applications. <p> The study reports a small positive effect on performance for a benchmark suite. Three factors limit generalization of results from this study: suitability of the benchmarks for affinity scheduling, small cache size, and the simulated environment. Using a mix of measurements on a test-bed and modeling, <ref> [Vaswani91] </ref> studied the conflict between space-sharing (dividing processors among parallel programs) and affinity-scheduling policies in a shared memory multiprocessor. The study concluded that affinity scheduling has negligible effects on performance for the current hardware.

References-found: 10

