URL: ftp://ftp.eecs.umich.edu/people/durfee/IA-v2.ps.Z
Refering-URL: http://ai.eecs.umich.edu/diag/INFOECON.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fjmvidal,durfeeg@umich.edu  
Title: Using Recursive Agent Models Effectively  
Author: Jose M. Vidal and Edmund H. Durfee 
Address: 1101 Beal Avenue, Ann Arbor, Michigan 48109-2110, USA  
Affiliation: Artificial Intelligence Laboratory, University of Michigan  
Abstract: We present an algorithm that an agent can use for determining which of its nested, recursive models of other agents are important to consider when choosing an action. Pruning away less important models allows an agent to take its best action in a timely manner, given its knowledge, computational capabilities, and time constraints. We describe a theoretical framework, based on situations, for talking about recursive agent models and the strategies and expected strategies associated with them. This framework allows us to rigorously define the gain of continuing deliberation versus taking action. The expected gain of computational actions is used to guide the pruning of the nested model structure. We have implemented our approach on a canonical multi-agent problem, the pursuit task, to illustrate how real-time, multi-agent decision-making can be based on a principled, combinatorial model. Test results show a marked decrease in deliberation time while maintaining a good performance level.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Edmund H. Durfee. </author> <title> Blissful ignorance: Knowing just enough to coordinate well. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems, </booktitle> <pages> pages 406-413, </pages> <year> 1995. </year>
Reference-contexts: A1 knows something about what A2 expects A1 to do in the situation but not how A2 represents A1's thinking, then A1 could associate with A2 a sub-intentional model of A1 2 There are, of course, other approximation techniques that increase practicality which rely on various abstractions of the problem <ref> [1] </ref>. Z Z A A p=.8 0 0 A2 3 4 p=.2 Sub-intentional Model p=.8 4 2 2 b a b c a b c a c ZK c Fig. 1. This is an example RMM hierarchy for two agents A1 and A2. <p> Allowing agents to jump to assumptions about common knowledge can simplify coordination reasoning, since agents can use fixed-point equilibrium notions and axioms based on mutual beliefs and plans. While leaping to such assumptions can, at times, be a viable method for keeping coordination practical <ref> [1] </ref>, it also introduces risks that we wish to avoid. A situation has both a physical and a mental component.
Reference: 2. <author> Edmund H. Durfee, Piotr J. Gmytrasiewicz, and Jeffrey S. Rosenschein. </author> <title> The utility of embedded communications and the emergence of protocols. </title> <booktitle> In Proceedings of the 13th International Distributed Artificial Intelligence Workshop, </booktitle> <year> 1994. </year>
Reference-contexts: While each of these approaches has its merits, each also involves some cost or risk: effective communication requires careful decision-making about what to say, when to say it, and whether to trust what is heard <ref> [2] </ref> [9]; relying on learned patterns of action [15] risks jumping to incorrect expectations when environmental variations occur; and using deeper models of other agents can be more accurate but extremely time-consuming [7]. In this paper, we concentrate on coordinated decision-making using deeper, nested models of agents. <p> section presents the results of an implementation of our algorithm, from which we derive some conclusions and directions for further work, discussed in the Conclusion. 1.1 The Recursive Modeling Method The basic modeling primitives we use are based on the Recursive Modeling Method (RMM) (see [8] in this Volume) [6] <ref> [2] </ref> [3]. RMM provides a theoretical framework for representing and using the knowledge that an agent has about its expected payoffs and those of others.
Reference: 3. <author> Edmund H. Durfee, Jaeho Lee, and Piotr J. Gmytrasiewicz. </author> <title> Overeager reciprocal rationality and mixed strategy equilibria. </title> <booktitle> In Proceedings of the eleventh National Conference on Artificial Intelligence, </booktitle> <year> 1993. </year>
Reference-contexts: presents the results of an implementation of our algorithm, from which we derive some conclusions and directions for further work, discussed in the Conclusion. 1.1 The Recursive Modeling Method The basic modeling primitives we use are based on the Recursive Modeling Method (RMM) (see [8] in this Volume) [6] [2] <ref> [3] </ref>. RMM provides a theoretical framework for representing and using the knowledge that an agent has about its expected payoffs and those of others.
Reference: 4. <author> Ronald Fagin, Joseph Y. Halpern, Yoram Moses, and Moshe Y. Vardi. </author> <title> Reasoning About Knowledge. </title> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Conversely, cycles or loops of reference between situations imply the presence of common knowledge. They are, therefore, disallowed since common knowledge can not be achieved in many common situations of interest to us, such as in asynchronous message-passing system <ref> [4] </ref>. Allowing agents to jump to assumptions about common knowledge can simplify coordination reasoning, since agents can use fixed-point equilibrium notions and axioms based on mutual beliefs and plans.
Reference: 5. <author> Les Gasser, Nicholas F. Rouquetter, Randall W. Hill, and John Lieb. </author> <title> Representing and using organizational knowledge in distributed ai systems. </title> <editor> In Les Gasser and Michael N Huhns, editors, </editor> <booktitle> Distributed Artificial Intelligence, </booktitle> <volume> volume 2, </volume> <pages> pages 55-78. </pages> <publisher> Morgan Kauffman Publishers, </publisher> <year> 1989. </year>
Reference-contexts: The second value is 5 k , where k is the number of quadrants around the prey that have a predator in them in the new situation. The quadrants are defined by drawing two diagonal lines across the prey <ref> [5, 11] </ref>. Since the matrices take into account all possible combinations of moves (5 for each predator and 4 for the prey), they are five-dimensional with a total of 2500 payoff entries.
Reference: 6. <author> Piotr J. Gmytrasiewics and Edmund H. Durfee. </author> <title> A rigorous, operational formalization of recursive modeling. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems, </booktitle> <pages> pages 125-132, </pages> <year> 1995. </year>
Reference-contexts: Implementation section presents the results of an implementation of our algorithm, from which we derive some conclusions and directions for further work, discussed in the Conclusion. 1.1 The Recursive Modeling Method The basic modeling primitives we use are based on the Recursive Modeling Method (RMM) (see [8] in this Volume) <ref> [6] </ref> [2] [3]. RMM provides a theoretical framework for representing and using the knowledge that an agent has about its expected payoffs and those of others.
Reference: 7. <author> Piotr J. Gmytrasiewicz. </author> <title> A Decision-Theoretic Model of Coordination and Communication in Autonomous Systems (Reasoning Systems). </title> <type> PhD thesis, </type> <institution> University of Michigan, </institution> <year> 1992. </year>
Reference-contexts: requires careful decision-making about what to say, when to say it, and whether to trust what is heard [2] [9]; relying on learned patterns of action [15] risks jumping to incorrect expectations when environmental variations occur; and using deeper models of other agents can be more accurate but extremely time-consuming <ref> [7] </ref>. In this paper, we concentrate on coordinated decision-making using deeper, nested models of agents. Because these models allow an agent to fully represent and use all of its available knowledge about itself and others, the quality of its decision-making can be high.
Reference: 8. <author> Piotr. J. Gmytrasiewicz. </author> <title> On reasoning about other agents. </title> <editor> In M. Wooldridge, J. P. Muller, and M. Tambe, editors, </editor> <booktitle> Intelligent Agents Volume II Proceedings of the 1995 Workshop on Agent Theories, Architectures, and Languages (ATAL-95), Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1996. </year> <note> (In this volume). </note>
Reference-contexts: The Implementation section presents the results of an implementation of our algorithm, from which we derive some conclusions and directions for further work, discussed in the Conclusion. 1.1 The Recursive Modeling Method The basic modeling primitives we use are based on the Recursive Modeling Method (RMM) (see <ref> [8] </ref> in this Volume) [6] [2] [3]. RMM provides a theoretical framework for representing and using the knowledge that an agent has about its expected payoffs and those of others. <p> This is an example RMM hierarchy for two agents A1 and A2. The leaves of the tree will either be Zero Knowledge (ZK) strategy or a sub-intentional model. Note that we consider the ZK strategy and the NO-INFO model in <ref> [8] </ref>, as equivalent. that summarizes A1's likely actions.
Reference: 9. <author> Piotr J. Gmytrasiewicz and Edmund H. Durfee. </author> <title> Toward a theory of honesty and trust among communicating autonomous agents. Group Decision and Negotiation, </title> <booktitle> 2 </booktitle> <pages> 237-258, </pages> <year> 1993. </year>
Reference-contexts: While each of these approaches has its merits, each also involves some cost or risk: effective communication requires careful decision-making about what to say, when to say it, and whether to trust what is heard [2] <ref> [9] </ref>; relying on learned patterns of action [15] risks jumping to incorrect expectations when environmental variations occur; and using deeper models of other agents can be more accurate but extremely time-consuming [7]. In this paper, we concentrate on coordinated decision-making using deeper, nested models of agents.
Reference: 10. <author> Richard E. Korf. </author> <title> A simple solution to pursuit games. </title> <booktitle> In Proceedings of the 11th International Distributed Artificial Intelligence Workshop, </booktitle> <year> 1992. </year>
Reference-contexts: The pursuit task has been investigated in Distributed AI (DAI) and many different methods have been devised for solving it <ref> [10] </ref> [17] [11]. These either impose specific roles on the predators, spend much time computing, or fail because of lack of coordination.
Reference: 11. <author> Ran Levy and Jeffrey S. Rosenschein. </author> <title> A game theoretic approach to the pursuit problem. </title> <booktitle> In Proceedings of the 11th International Distributed Artificial Intelligence Workshop, </booktitle> <year> 1992. </year>
Reference-contexts: The pursuit task has been investigated in Distributed AI (DAI) and many different methods have been devised for solving it [10] [17] <ref> [11] </ref>. These either impose specific roles on the predators, spend much time computing, or fail because of lack of coordination. <p> The second value is 5 k , where k is the number of quadrants around the prey that have a predator in them in the new situation. The quadrants are defined by drawing two diagonal lines across the prey <ref> [5, 11] </ref>. Since the matrices take into account all possible combinations of moves (5 for each predator and 4 for the prey), they are five-dimensional with a total of 2500 payoff entries.
Reference: 12. <author> T. A. Montgomery and Edmund H. Durfee. </author> <title> Using mice to study intelligent dynamic coordination. </title> <booktitle> In Proceedings of IEEE Conference on Tools for AI, </booktitle> <year> 1990. </year>
Reference-contexts: The intuition was that agents that are far from the prey do not influence the other agents' move decision. 4 Implementation of Pursuit Task The original algorithm has been implemented in a simulation of the pursuit task, using the MICE system <ref> [12] </ref>. The experiments showed some promising results.
Reference: 13. <author> Anand S. Rao and Michael P. Georgeff. </author> <title> BDI agents: From theory to practice. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems, </booktitle> <pages> pages 312-319, </pages> <year> 1995. </year>
Reference-contexts: We are currently trying to apply the ideas distilled from this work and form a more general approach that can work with different agent modeling techniques such as those based on Beliefs, Desires and Intentions logics <ref> [13] </ref>.
Reference: 14. <author> Stuart Russell and Eric Wefald. </author> <title> Do The Right Thing. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mas-sachusetts, </address> <year> 1991. </year>
Reference-contexts: Finally, If A1 believes that A2 has no knowledge of A1, this can be captures in a ZK strategy, as shown by the rightmost branch. 1.2 Limited Rationality Our algorithm is based on ideas borrowed from limited rationality as defined in Russell and Wefald's book <ref> [14] </ref> which, in turn, was inspired by the work done by Simon [16]. Russell and Wefald define the agent's thinking as the execution of a series of computational actions, and use metalevel thinking techniques to determine which computational action, if any, the agent should execute next. <p> The value of r can either be a situation (r 2 S) or, if the modeling agent has no more knowledge, it can be the Zero Knowledge strategy (r = ZK). 2.1 Notation and Formalisms Our implementation of limited rationality and our notation closely parallels Russell and Wefald's work <ref> [14] </ref>, although with some major differences as will be pointed out later. We define a partially expanded situation as a subset of a situation where only some of the nodes have been expanded. <p> This reasoning is better understood with the help of an example. We contrast our problem with the case of using minimax for playing a board game, as studied in <ref> [14] </ref>. There, if a player realizes that taking move A will lead to a set of possible outcomes, none of which have higher utility than he gets when taking move B, then the player should not consider move A anymore. Within our framework, however, this result does not apply.
Reference: 15. <author> Sandip Sen and Edmund H. Durfee. </author> <title> Adaptive surrogate agents. </title> <booktitle> In Proceedings of the 13th International Distributed Artificial Intelligence Workshop, </booktitle> <year> 1994. </year>
Reference-contexts: While each of these approaches has its merits, each also involves some cost or risk: effective communication requires careful decision-making about what to say, when to say it, and whether to trust what is heard [2] [9]; relying on learned patterns of action <ref> [15] </ref> risks jumping to incorrect expectations when environmental variations occur; and using deeper models of other agents can be more accurate but extremely time-consuming [7]. In this paper, we concentrate on coordinated decision-making using deeper, nested models of agents.
Reference: 16. <author> Herbert A. Simon. </author> <title> Models of Bounded Rationality, volume 2. </title> <publisher> MIT Press, </publisher> <year> 1982. </year>
Reference-contexts: no knowledge of A1, this can be captures in a ZK strategy, as shown by the rightmost branch. 1.2 Limited Rationality Our algorithm is based on ideas borrowed from limited rationality as defined in Russell and Wefald's book [14] which, in turn, was inspired by the work done by Simon <ref> [16] </ref>. Russell and Wefald define the agent's thinking as the execution of a series of computational actions, and use metalevel thinking techniques to determine which computational action, if any, the agent should execute next. We have taken this approach and applied it to the problem of using recursive agent models.
Reference: 17. <author> Larry M. Stephens and Matthias B. Merx. </author> <title> The effect of agent control strategy on the performance of a DAI pursuit problem. </title> <booktitle> In Proceedings of the 9th International Distributed Artificial Intelligence Workshop, </booktitle> <year> 1990. </year>
Reference-contexts: The pursuit task has been investigated in Distributed AI (DAI) and many different methods have been devised for solving it [10] <ref> [17] </ref> [11]. These either impose specific roles on the predators, spend much time computing, or fail because of lack of coordination.
Reference: 18. <author> M. Tambe and P. S. Rosenbloom. </author> <title> Agent tracking in real-time dynamic environments. </title> <editor> In M. Wooldridge, J. P. Muller, and M. Tambe, editors, </editor> <booktitle> Intelligent Agents Volume II Proceedings of the 1995 Workshop on Agent Theories, Architectures, and Languages (ATAL-95), Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1996. </year> <note> (In this volume). </note>
Reference-contexts: From our experience with the algorithm, the test results, and our ongoing work, we arrived at some important conclusions. First of all, recursive models appear very frequently when working with Multi-Agent Systems (for another example see <ref> [18] </ref>). They are not just artifacts of our modeling techniques but are, in fact, dictated by the fact that smart agents when interacting with other agents, will usually chose to build models of them.
Reference: 19. <author> Jose M. Vidal and Edmund H. Durfee. </author> <title> Agent modeling methods using limited rationality. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> page 1495, </pages> <year> 1994. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
References-found: 19

